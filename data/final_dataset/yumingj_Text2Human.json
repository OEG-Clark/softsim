{"home.repos.pwc.inspect_result.yumingj_Text2Human.None.sample_from_pose.main": [[15, 49], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "utils.options.parse", "utils.util.make_exp_dirs", "os.join", "utils.logger.get_root_logger", "utils.logger.get_root_logger.info", "utils.options.dict_to_nonedict", "utils.logger.get_root_logger.info", "utils.util.set_random_seed", "data.pose_attr_dataset.DeepFashionAttrPoseDataset", "torch.utils.data.DataLoader", "utils.logger.get_root_logger.info", "models.create_model", "models.create_model.inference", "utils.options.dict2str", "random.randint", "len"], "function", ["home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.parse", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.make_exp_dirs", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.dict_to_nonedict", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.set_random_seed", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.__init__.create_model", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.inference", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.dict2str"], ["def", "main", "(", ")", ":", "\n", "# options", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'-opt'", ",", "type", "=", "str", ",", "help", "=", "'Path to option YAML file.'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "opt", "=", "parse", "(", "args", ".", "opt", ",", "is_train", "=", "False", ")", "\n", "\n", "# mkdir and loggers", "\n", "make_exp_dirs", "(", "opt", ")", "\n", "log_file", "=", "osp", ".", "join", "(", "opt", "[", "'path'", "]", "[", "'log'", "]", ",", "f\"test_{opt['name']}.log\"", ")", "\n", "logger", "=", "get_root_logger", "(", "\n", "logger_name", "=", "'base'", ",", "log_level", "=", "logging", ".", "INFO", ",", "log_file", "=", "log_file", ")", "\n", "logger", ".", "info", "(", "dict2str", "(", "opt", ")", ")", "\n", "\n", "# convert to NoneDict, which returns None for missing keys", "\n", "opt", "=", "dict_to_nonedict", "(", "opt", ")", "\n", "\n", "# random seed", "\n", "seed", "=", "opt", "[", "'manual_seed'", "]", "\n", "if", "seed", "is", "None", ":", "\n", "        ", "seed", "=", "random", ".", "randint", "(", "1", ",", "10000", ")", "\n", "", "logger", ".", "info", "(", "f'Random seed: {seed}'", ")", "\n", "set_random_seed", "(", "seed", ")", "\n", "\n", "test_dataset", "=", "DeepFashionAttrPoseDataset", "(", "\n", "pose_dir", "=", "opt", "[", "'pose_dir'", "]", ",", "\n", "texture_ann_dir", "=", "opt", "[", "'texture_ann_file'", "]", ",", "\n", "shape_ann_path", "=", "opt", "[", "'shape_ann_path'", "]", ")", "\n", "test_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "test_dataset", ",", "batch_size", "=", "4", ",", "shuffle", "=", "False", ")", "\n", "logger", ".", "info", "(", "f'Number of test set: {len(test_dataset)}.'", ")", "\n", "\n", "model", "=", "create_model", "(", "opt", ")", "\n", "_", "=", "model", ".", "inference", "(", "test_loader", ",", "opt", "[", "'path'", "]", "[", "'results_root'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.train_sampler.main": [[17, 119], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "utils.options.parse", "utils.util.make_exp_dirs", "os.join", "utils.logger.get_root_logger", "utils.logger.get_root_logger.info", "utils.options.dict_to_nonedict", "data.segm_attr_dataset.DeepFashionAttrSegmDataset", "torch.utils.data.DataLoader", "utils.logger.get_root_logger.info", "data.segm_attr_dataset.DeepFashionAttrSegmDataset", "torch.utils.data.DataLoader", "utils.logger.get_root_logger.info", "data.segm_attr_dataset.DeepFashionAttrSegmDataset", "torch.utils.data.DataLoader", "utils.logger.get_root_logger.info", "models.create_model", "utils.logger.MessageLogger", "range", "utils.options.dict2str", "utils.logger.init_tb_logger", "models.create_model.update_learning_rate", "enumerate", "len", "models.create_model.feed_data", "models.create_model.optimize_parameters", "time.time", "time.time", "os.makedirs", "os.makedirs", "models.create_model.inference", "os.makedirs", "os.makedirs", "models.create_model.inference", "models.create_model.save_network", "len", "len", "len", "time.time", "time.time", "log_vars.update", "log_vars.update", "log_vars.update", "utils.logger.MessageLogger.", "models.create_model.get_current_log"], "function", ["home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.parse", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.make_exp_dirs", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.dict_to_nonedict", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.__init__.create_model", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.dict2str", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.logger.init_tb_logger", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.update_learning_rate", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.feed_data", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.optimize_parameters", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.inference", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.inference", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.save_network", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.get_current_log"], ["def", "main", "(", ")", ":", "\n", "# options", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'-opt'", ",", "type", "=", "str", ",", "help", "=", "'Path to option YAML file.'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "opt", "=", "parse", "(", "args", ".", "opt", ",", "is_train", "=", "True", ")", "\n", "\n", "# mkdir and loggers", "\n", "make_exp_dirs", "(", "opt", ")", "\n", "log_file", "=", "osp", ".", "join", "(", "opt", "[", "'path'", "]", "[", "'log'", "]", ",", "f\"train_{opt['name']}.log\"", ")", "\n", "logger", "=", "get_root_logger", "(", "\n", "logger_name", "=", "'base'", ",", "log_level", "=", "logging", ".", "INFO", ",", "log_file", "=", "log_file", ")", "\n", "logger", ".", "info", "(", "dict2str", "(", "opt", ")", ")", "\n", "# initialize tensorboard logger", "\n", "tb_logger", "=", "None", "\n", "if", "opt", "[", "'use_tb_logger'", "]", "and", "'debug'", "not", "in", "opt", "[", "'name'", "]", ":", "\n", "        ", "tb_logger", "=", "init_tb_logger", "(", "log_dir", "=", "'./tb_logger/'", "+", "opt", "[", "'name'", "]", ")", "\n", "\n", "# convert to NoneDict, which returns None for missing keys", "\n", "", "opt", "=", "dict_to_nonedict", "(", "opt", ")", "\n", "\n", "# set up data loader", "\n", "train_dataset", "=", "DeepFashionAttrSegmDataset", "(", "\n", "img_dir", "=", "opt", "[", "'train_img_dir'", "]", ",", "\n", "segm_dir", "=", "opt", "[", "'segm_dir'", "]", ",", "\n", "pose_dir", "=", "opt", "[", "'pose_dir'", "]", ",", "\n", "ann_dir", "=", "opt", "[", "'train_ann_file'", "]", ",", "\n", "xflip", "=", "True", ")", "\n", "train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "train_dataset", ",", "\n", "batch_size", "=", "opt", "[", "'batch_size'", "]", ",", "\n", "shuffle", "=", "True", ",", "\n", "num_workers", "=", "opt", "[", "'num_workers'", "]", ",", "\n", "persistent_workers", "=", "True", ",", "\n", "drop_last", "=", "True", ")", "\n", "logger", ".", "info", "(", "f'Number of train set: {len(train_dataset)}.'", ")", "\n", "opt", "[", "'max_iters'", "]", "=", "opt", "[", "'num_epochs'", "]", "*", "len", "(", "\n", "train_dataset", ")", "//", "opt", "[", "'batch_size'", "]", "\n", "\n", "val_dataset", "=", "DeepFashionAttrSegmDataset", "(", "\n", "img_dir", "=", "opt", "[", "'train_img_dir'", "]", ",", "\n", "segm_dir", "=", "opt", "[", "'segm_dir'", "]", ",", "\n", "pose_dir", "=", "opt", "[", "'pose_dir'", "]", ",", "\n", "ann_dir", "=", "opt", "[", "'val_ann_file'", "]", ")", "\n", "val_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "val_dataset", ",", "batch_size", "=", "opt", "[", "'batch_size'", "]", ",", "shuffle", "=", "False", ")", "\n", "logger", ".", "info", "(", "f'Number of val set: {len(val_dataset)}.'", ")", "\n", "\n", "test_dataset", "=", "DeepFashionAttrSegmDataset", "(", "\n", "img_dir", "=", "opt", "[", "'test_img_dir'", "]", ",", "\n", "segm_dir", "=", "opt", "[", "'segm_dir'", "]", ",", "\n", "pose_dir", "=", "opt", "[", "'pose_dir'", "]", ",", "\n", "ann_dir", "=", "opt", "[", "'test_ann_file'", "]", ")", "\n", "test_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "test_dataset", ",", "batch_size", "=", "opt", "[", "'batch_size'", "]", ",", "shuffle", "=", "False", ")", "\n", "logger", ".", "info", "(", "f'Number of test set: {len(test_dataset)}.'", ")", "\n", "\n", "current_iter", "=", "0", "\n", "\n", "model", "=", "create_model", "(", "opt", ")", "\n", "\n", "data_time", ",", "iter_time", "=", "0", ",", "0", "\n", "current_iter", "=", "0", "\n", "\n", "# create message logger (formatted outputs)", "\n", "msg_logger", "=", "MessageLogger", "(", "opt", ",", "current_iter", ",", "tb_logger", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "opt", "[", "'num_epochs'", "]", ")", ":", "\n", "        ", "lr", "=", "model", ".", "update_learning_rate", "(", "epoch", ",", "current_iter", ")", "\n", "\n", "for", "_", ",", "batch_data", "in", "enumerate", "(", "train_loader", ")", ":", "\n", "            ", "data_time", "=", "time", ".", "time", "(", ")", "-", "data_time", "\n", "\n", "current_iter", "+=", "1", "\n", "\n", "model", ".", "feed_data", "(", "batch_data", ")", "\n", "model", ".", "optimize_parameters", "(", ")", "\n", "\n", "iter_time", "=", "time", ".", "time", "(", ")", "-", "iter_time", "\n", "if", "current_iter", "%", "opt", "[", "'print_freq'", "]", "==", "0", ":", "\n", "                ", "log_vars", "=", "{", "'epoch'", ":", "epoch", ",", "'iter'", ":", "current_iter", "}", "\n", "log_vars", ".", "update", "(", "{", "'lrs'", ":", "[", "lr", "]", "}", ")", "\n", "log_vars", ".", "update", "(", "{", "'time'", ":", "iter_time", ",", "'data_time'", ":", "data_time", "}", ")", "\n", "log_vars", ".", "update", "(", "model", ".", "get_current_log", "(", ")", ")", "\n", "msg_logger", "(", "log_vars", ")", "\n", "\n", "", "data_time", "=", "time", ".", "time", "(", ")", "\n", "iter_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "if", "epoch", "%", "opt", "[", "'val_freq'", "]", "==", "0", "and", "epoch", "!=", "0", ":", "\n", "            ", "save_dir", "=", "f'{opt[\"path\"][\"visualization\"]}/valset/epoch_{epoch:03d}'", "# noqa", "\n", "os", ".", "makedirs", "(", "save_dir", ",", "exist_ok", "=", "opt", "[", "'debug'", "]", ")", "\n", "model", ".", "inference", "(", "val_loader", ",", "save_dir", ")", "\n", "\n", "save_dir", "=", "f'{opt[\"path\"][\"visualization\"]}/testset/epoch_{epoch:03d}'", "# noqa", "\n", "os", ".", "makedirs", "(", "save_dir", ",", "exist_ok", "=", "opt", "[", "'debug'", "]", ")", "\n", "model", ".", "inference", "(", "test_loader", ",", "save_dir", ")", "\n", "\n", "# save model", "\n", "model", ".", "save_network", "(", "\n", "model", ".", "_denoise_fn", ",", "\n", "f'{opt[\"path\"][\"models\"]}/sampler_epoch{epoch}.pth'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.train_parsing_token.main": [[17, 119], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "utils.options.parse", "utils.util.make_exp_dirs", "os.join", "utils.logger.get_root_logger", "utils.logger.get_root_logger.info", "utils.options.dict_to_nonedict", "data.mask_dataset.MaskDataset", "torch.utils.data.DataLoader", "utils.logger.get_root_logger.info", "data.mask_dataset.MaskDataset", "torch.utils.data.DataLoader", "utils.logger.get_root_logger.info", "data.mask_dataset.MaskDataset", "torch.utils.data.DataLoader", "utils.logger.get_root_logger.info", "models.create_model", "utils.logger.MessageLogger", "range", "utils.options.dict2str", "utils.logger.init_tb_logger", "models.create_model.update_learning_rate", "enumerate", "len", "models.create_model.optimize_parameters", "time.time", "time.time", "os.makedirs", "os.makedirs", "models.create_model.inference", "os.makedirs", "os.makedirs", "models.create_model.inference", "utils.logger.get_root_logger.info", "utils.logger.get_root_logger.info", "models.create_model.save_network", "len", "len", "len", "time.time", "time.time", "log_vars.update", "log_vars.update", "log_vars.update", "utils.logger.MessageLogger.", "models.create_model.get_current_log"], "function", ["home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.parse", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.make_exp_dirs", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.dict_to_nonedict", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.__init__.create_model", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.dict2str", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.logger.init_tb_logger", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.update_learning_rate", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.optimize_parameters", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.inference", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.inference", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.save_network", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.get_current_log"], ["def", "main", "(", ")", ":", "\n", "# options", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'-opt'", ",", "type", "=", "str", ",", "help", "=", "'Path to option YAML file.'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "opt", "=", "parse", "(", "args", ".", "opt", ",", "is_train", "=", "True", ")", "\n", "\n", "# mkdir and loggers", "\n", "make_exp_dirs", "(", "opt", ")", "\n", "log_file", "=", "osp", ".", "join", "(", "opt", "[", "'path'", "]", "[", "'log'", "]", ",", "f\"train_{opt['name']}.log\"", ")", "\n", "logger", "=", "get_root_logger", "(", "\n", "logger_name", "=", "'base'", ",", "log_level", "=", "logging", ".", "INFO", ",", "log_file", "=", "log_file", ")", "\n", "logger", ".", "info", "(", "dict2str", "(", "opt", ")", ")", "\n", "# initialize tensorboard logger", "\n", "tb_logger", "=", "None", "\n", "if", "opt", "[", "'use_tb_logger'", "]", "and", "'debug'", "not", "in", "opt", "[", "'name'", "]", ":", "\n", "        ", "tb_logger", "=", "init_tb_logger", "(", "log_dir", "=", "'./tb_logger/'", "+", "opt", "[", "'name'", "]", ")", "\n", "\n", "# convert to NoneDict, which returns None for missing keys", "\n", "", "opt", "=", "dict_to_nonedict", "(", "opt", ")", "\n", "\n", "# set up data loader", "\n", "train_dataset", "=", "MaskDataset", "(", "\n", "segm_dir", "=", "opt", "[", "'segm_dir'", "]", ",", "ann_dir", "=", "opt", "[", "'train_ann_file'", "]", ",", "xflip", "=", "True", ")", "\n", "train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "train_dataset", ",", "\n", "batch_size", "=", "opt", "[", "'batch_size'", "]", ",", "\n", "shuffle", "=", "True", ",", "\n", "num_workers", "=", "opt", "[", "'num_workers'", "]", ",", "\n", "persistent_workers", "=", "True", ",", "\n", "drop_last", "=", "True", ")", "\n", "logger", ".", "info", "(", "f'Number of train set: {len(train_dataset)}.'", ")", "\n", "opt", "[", "'max_iters'", "]", "=", "opt", "[", "'num_epochs'", "]", "*", "len", "(", "\n", "train_dataset", ")", "//", "opt", "[", "'batch_size'", "]", "\n", "\n", "val_dataset", "=", "MaskDataset", "(", "\n", "segm_dir", "=", "opt", "[", "'segm_dir'", "]", ",", "ann_dir", "=", "opt", "[", "'val_ann_file'", "]", ")", "\n", "val_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "val_dataset", ",", "batch_size", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "logger", ".", "info", "(", "f'Number of val set: {len(val_dataset)}.'", ")", "\n", "\n", "test_dataset", "=", "MaskDataset", "(", "\n", "segm_dir", "=", "opt", "[", "'segm_dir'", "]", ",", "ann_dir", "=", "opt", "[", "'test_ann_file'", "]", ")", "\n", "test_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "test_dataset", ",", "batch_size", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "logger", ".", "info", "(", "f'Number of test set: {len(test_dataset)}.'", ")", "\n", "\n", "current_iter", "=", "0", "\n", "best_epoch", "=", "None", "\n", "best_loss", "=", "100000", "\n", "\n", "model", "=", "create_model", "(", "opt", ")", "\n", "\n", "data_time", ",", "iter_time", "=", "0", ",", "0", "\n", "current_iter", "=", "0", "\n", "\n", "# create message logger (formatted outputs)", "\n", "msg_logger", "=", "MessageLogger", "(", "opt", ",", "current_iter", ",", "tb_logger", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "opt", "[", "'num_epochs'", "]", ")", ":", "\n", "        ", "lr", "=", "model", ".", "update_learning_rate", "(", "epoch", ")", "\n", "\n", "for", "_", ",", "batch_data", "in", "enumerate", "(", "train_loader", ")", ":", "\n", "            ", "data_time", "=", "time", ".", "time", "(", ")", "-", "data_time", "\n", "\n", "current_iter", "+=", "1", "\n", "\n", "model", ".", "optimize_parameters", "(", "batch_data", ",", "current_iter", ")", "\n", "\n", "iter_time", "=", "time", ".", "time", "(", ")", "-", "iter_time", "\n", "if", "current_iter", "%", "opt", "[", "'print_freq'", "]", "==", "0", ":", "\n", "                ", "log_vars", "=", "{", "'epoch'", ":", "epoch", ",", "'iter'", ":", "current_iter", "}", "\n", "log_vars", ".", "update", "(", "{", "'lrs'", ":", "[", "lr", "]", "}", ")", "\n", "log_vars", ".", "update", "(", "{", "'time'", ":", "iter_time", ",", "'data_time'", ":", "data_time", "}", ")", "\n", "log_vars", ".", "update", "(", "model", ".", "get_current_log", "(", ")", ")", "\n", "msg_logger", "(", "log_vars", ")", "\n", "\n", "", "data_time", "=", "time", ".", "time", "(", ")", "\n", "iter_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "if", "epoch", "%", "opt", "[", "'val_freq'", "]", "==", "0", ":", "\n", "            ", "save_dir", "=", "f'{opt[\"path\"][\"visualization\"]}/valset/epoch_{epoch:03d}'", "# noqa", "\n", "os", ".", "makedirs", "(", "save_dir", ",", "exist_ok", "=", "opt", "[", "'debug'", "]", ")", "\n", "val_loss_total", ",", "_", ",", "_", "=", "model", ".", "inference", "(", "val_loader", ",", "save_dir", ")", "\n", "\n", "save_dir", "=", "f'{opt[\"path\"][\"visualization\"]}/testset/epoch_{epoch:03d}'", "# noqa", "\n", "os", ".", "makedirs", "(", "save_dir", ",", "exist_ok", "=", "opt", "[", "'debug'", "]", ")", "\n", "test_loss_total", ",", "_", ",", "_", "=", "model", ".", "inference", "(", "test_loader", ",", "save_dir", ")", "\n", "\n", "logger", ".", "info", "(", "f'Epoch: {epoch}, '", "\n", "f'val_loss_total: {val_loss_total}, '", "\n", "f'test_loss_total: {test_loss_total}.'", ")", "\n", "\n", "if", "test_loss_total", "<", "best_loss", ":", "\n", "                ", "best_epoch", "=", "epoch", "\n", "best_loss", "=", "test_loss_total", "\n", "\n", "", "logger", ".", "info", "(", "f'Best epoch: {best_epoch}, '", "\n", "f'Best test loss: {best_loss: .4f}.'", ")", "\n", "\n", "# save model", "\n", "model", ".", "save_network", "(", "f'{opt[\"path\"][\"models\"]}/epoch{epoch}.pth'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.train_parsing_gen.main": [[18, 133], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "utils.options.parse", "utils.util.make_exp_dirs", "os.join", "utils.logger.get_root_logger", "utils.logger.get_root_logger.info", "utils.options.dict_to_nonedict", "data.parsing_generation_segm_attr_dataset.ParsingGenerationDeepFashionAttrSegmDataset", "torch.utils.data.DataLoader", "utils.logger.get_root_logger.info", "data.parsing_generation_segm_attr_dataset.ParsingGenerationDeepFashionAttrSegmDataset", "torch.utils.data.DataLoader", "utils.logger.get_root_logger.info", "data.parsing_generation_segm_attr_dataset.ParsingGenerationDeepFashionAttrSegmDataset", "torch.utils.data.DataLoader", "utils.logger.get_root_logger.info", "models.create_model", "utils.logger.MessageLogger", "range", "utils.options.dict2str", "utils.logger.init_tb_logger", "models.create_model.update_learning_rate", "enumerate", "len", "models.create_model.feed_data", "models.create_model.optimize_parameters", "time.time", "time.time", "os.makedirs", "os.makedirs", "models.create_model.inference", "os.makedirs", "os.makedirs", "models.create_model.inference", "utils.logger.get_root_logger.info", "utils.logger.get_root_logger.info", "models.create_model.save_network", "len", "len", "len", "time.time", "time.time", "log_vars.update", "log_vars.update", "log_vars.update", "utils.logger.MessageLogger.", "models.create_model.get_current_log"], "function", ["home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.parse", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.make_exp_dirs", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.dict_to_nonedict", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.__init__.create_model", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.dict2str", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.logger.init_tb_logger", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.update_learning_rate", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.feed_data", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.optimize_parameters", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.inference", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.inference", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.save_network", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.get_current_log"], ["def", "main", "(", ")", ":", "\n", "# options", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'-opt'", ",", "type", "=", "str", ",", "help", "=", "'Path to option YAML file.'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "opt", "=", "parse", "(", "args", ".", "opt", ",", "is_train", "=", "True", ")", "\n", "\n", "# mkdir and loggers", "\n", "make_exp_dirs", "(", "opt", ")", "\n", "log_file", "=", "osp", ".", "join", "(", "opt", "[", "'path'", "]", "[", "'log'", "]", ",", "f\"train_{opt['name']}.log\"", ")", "\n", "logger", "=", "get_root_logger", "(", "\n", "logger_name", "=", "'base'", ",", "log_level", "=", "logging", ".", "INFO", ",", "log_file", "=", "log_file", ")", "\n", "logger", ".", "info", "(", "dict2str", "(", "opt", ")", ")", "\n", "# initialize tensorboard logger", "\n", "tb_logger", "=", "None", "\n", "if", "opt", "[", "'use_tb_logger'", "]", "and", "'debug'", "not", "in", "opt", "[", "'name'", "]", ":", "\n", "        ", "tb_logger", "=", "init_tb_logger", "(", "log_dir", "=", "'./tb_logger/'", "+", "opt", "[", "'name'", "]", ")", "\n", "\n", "# convert to NoneDict, which returns None for missing keys", "\n", "", "opt", "=", "dict_to_nonedict", "(", "opt", ")", "\n", "\n", "# set up data loader", "\n", "train_dataset", "=", "ParsingGenerationDeepFashionAttrSegmDataset", "(", "\n", "segm_dir", "=", "opt", "[", "'segm_dir'", "]", ",", "\n", "pose_dir", "=", "opt", "[", "'pose_dir'", "]", ",", "\n", "ann_file", "=", "opt", "[", "'train_ann_file'", "]", ")", "\n", "train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "train_dataset", ",", "\n", "batch_size", "=", "opt", "[", "'batch_size'", "]", ",", "\n", "shuffle", "=", "True", ",", "\n", "num_workers", "=", "opt", "[", "'num_workers'", "]", ",", "\n", "drop_last", "=", "True", ")", "\n", "logger", ".", "info", "(", "f'Number of train set: {len(train_dataset)}.'", ")", "\n", "opt", "[", "'max_iters'", "]", "=", "opt", "[", "'num_epochs'", "]", "*", "len", "(", "\n", "train_dataset", ")", "//", "opt", "[", "'batch_size'", "]", "\n", "\n", "val_dataset", "=", "ParsingGenerationDeepFashionAttrSegmDataset", "(", "\n", "segm_dir", "=", "opt", "[", "'segm_dir'", "]", ",", "\n", "pose_dir", "=", "opt", "[", "'pose_dir'", "]", ",", "\n", "ann_file", "=", "opt", "[", "'val_ann_file'", "]", ")", "\n", "val_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "val_dataset", ",", "\n", "batch_size", "=", "1", ",", "\n", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "opt", "[", "'num_workers'", "]", ")", "\n", "logger", ".", "info", "(", "f'Number of val set: {len(val_dataset)}.'", ")", "\n", "\n", "test_dataset", "=", "ParsingGenerationDeepFashionAttrSegmDataset", "(", "\n", "segm_dir", "=", "opt", "[", "'segm_dir'", "]", ",", "\n", "pose_dir", "=", "opt", "[", "'pose_dir'", "]", ",", "\n", "ann_file", "=", "opt", "[", "'test_ann_file'", "]", ")", "\n", "test_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "test_dataset", ",", "\n", "batch_size", "=", "1", ",", "\n", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "opt", "[", "'num_workers'", "]", ")", "\n", "logger", ".", "info", "(", "f'Number of test set: {len(test_dataset)}.'", ")", "\n", "\n", "current_iter", "=", "0", "\n", "best_epoch", "=", "None", "\n", "best_acc", "=", "0", "\n", "\n", "model", "=", "create_model", "(", "opt", ")", "\n", "\n", "data_time", ",", "iter_time", "=", "0", ",", "0", "\n", "current_iter", "=", "0", "\n", "\n", "# create message logger (formatted outputs)", "\n", "msg_logger", "=", "MessageLogger", "(", "opt", ",", "current_iter", ",", "tb_logger", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "opt", "[", "'num_epochs'", "]", ")", ":", "\n", "        ", "lr", "=", "model", ".", "update_learning_rate", "(", "epoch", ")", "\n", "\n", "for", "_", ",", "batch_data", "in", "enumerate", "(", "train_loader", ")", ":", "\n", "            ", "data_time", "=", "time", ".", "time", "(", ")", "-", "data_time", "\n", "\n", "current_iter", "+=", "1", "\n", "\n", "model", ".", "feed_data", "(", "batch_data", ")", "\n", "model", ".", "optimize_parameters", "(", ")", "\n", "\n", "iter_time", "=", "time", ".", "time", "(", ")", "-", "iter_time", "\n", "if", "current_iter", "%", "opt", "[", "'print_freq'", "]", "==", "0", ":", "\n", "                ", "log_vars", "=", "{", "'epoch'", ":", "epoch", ",", "'iter'", ":", "current_iter", "}", "\n", "log_vars", ".", "update", "(", "{", "'lrs'", ":", "[", "lr", "]", "}", ")", "\n", "log_vars", ".", "update", "(", "{", "'time'", ":", "iter_time", ",", "'data_time'", ":", "data_time", "}", ")", "\n", "log_vars", ".", "update", "(", "model", ".", "get_current_log", "(", ")", ")", "\n", "msg_logger", "(", "log_vars", ")", "\n", "\n", "", "data_time", "=", "time", ".", "time", "(", ")", "\n", "iter_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "if", "epoch", "%", "opt", "[", "'val_freq'", "]", "==", "0", ":", "\n", "            ", "save_dir", "=", "f'{opt[\"path\"][\"visualization\"]}/valset/epoch_{epoch:03d}'", "\n", "os", ".", "makedirs", "(", "save_dir", ",", "exist_ok", "=", "opt", "[", "'debug'", "]", ")", "\n", "val_acc", "=", "model", ".", "inference", "(", "val_loader", ",", "save_dir", ")", "\n", "\n", "save_dir", "=", "f'{opt[\"path\"][\"visualization\"]}/testset/epoch_{epoch:03d}'", "\n", "os", ".", "makedirs", "(", "save_dir", ",", "exist_ok", "=", "opt", "[", "'debug'", "]", ")", "\n", "test_acc", "=", "model", ".", "inference", "(", "test_loader", ",", "save_dir", ")", "\n", "\n", "logger", ".", "info", "(", "f'Epoch: {epoch}, '", "\n", "f'val_acc: {val_acc: .4f}, '", "\n", "f'test_acc: {test_acc: .4f}.'", ")", "\n", "\n", "if", "test_acc", ">", "best_acc", ":", "\n", "                ", "best_epoch", "=", "epoch", "\n", "best_acc", "=", "test_acc", "\n", "\n", "", "logger", ".", "info", "(", "f'Best epoch: {best_epoch}, '", "\n", "f'Best test acc: {best_acc: .4f}.'", ")", "\n", "\n", "# save model", "\n", "model", ".", "save_network", "(", "\n", "f'{opt[\"path\"][\"models\"]}/parsing_generation_epoch{epoch}.pth'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.train_vqvae.main": [[17, 129], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "utils.options.parse", "utils.util.make_exp_dirs", "os.join", "utils.logger.get_root_logger", "utils.logger.get_root_logger.info", "utils.options.dict_to_nonedict", "data.segm_attr_dataset.DeepFashionAttrSegmDataset", "torch.utils.data.DataLoader", "utils.logger.get_root_logger.info", "data.segm_attr_dataset.DeepFashionAttrSegmDataset", "torch.utils.data.DataLoader", "utils.logger.get_root_logger.info", "data.segm_attr_dataset.DeepFashionAttrSegmDataset", "torch.utils.data.DataLoader", "utils.logger.get_root_logger.info", "models.create_model", "utils.logger.MessageLogger", "range", "utils.options.dict2str", "utils.logger.init_tb_logger", "models.create_model.update_learning_rate", "enumerate", "len", "models.create_model.optimize_parameters", "time.time", "time.time", "os.makedirs", "os.makedirs", "models.create_model.inference", "os.makedirs", "os.makedirs", "models.create_model.inference", "utils.logger.get_root_logger.info", "utils.logger.get_root_logger.info", "models.create_model.save_network", "len", "len", "len", "time.time", "time.time", "log_vars.update", "log_vars.update", "log_vars.update", "utils.logger.MessageLogger.", "models.create_model.get_current_log"], "function", ["home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.parse", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.make_exp_dirs", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.dict_to_nonedict", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.__init__.create_model", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.dict2str", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.logger.init_tb_logger", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.update_learning_rate", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.optimize_parameters", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.inference", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.inference", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.save_network", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.get_current_log"], ["def", "main", "(", ")", ":", "\n", "# options", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'-opt'", ",", "type", "=", "str", ",", "help", "=", "'Path to option YAML file.'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "opt", "=", "parse", "(", "args", ".", "opt", ",", "is_train", "=", "True", ")", "\n", "\n", "# mkdir and loggers", "\n", "make_exp_dirs", "(", "opt", ")", "\n", "log_file", "=", "osp", ".", "join", "(", "opt", "[", "'path'", "]", "[", "'log'", "]", ",", "f\"train_{opt['name']}.log\"", ")", "\n", "logger", "=", "get_root_logger", "(", "\n", "logger_name", "=", "'base'", ",", "log_level", "=", "logging", ".", "INFO", ",", "log_file", "=", "log_file", ")", "\n", "logger", ".", "info", "(", "dict2str", "(", "opt", ")", ")", "\n", "# initialize tensorboard logger", "\n", "tb_logger", "=", "None", "\n", "if", "opt", "[", "'use_tb_logger'", "]", "and", "'debug'", "not", "in", "opt", "[", "'name'", "]", ":", "\n", "        ", "tb_logger", "=", "init_tb_logger", "(", "log_dir", "=", "'./tb_logger/'", "+", "opt", "[", "'name'", "]", ")", "\n", "\n", "# convert to NoneDict, which returns None for missing keys", "\n", "", "opt", "=", "dict_to_nonedict", "(", "opt", ")", "\n", "\n", "# set up data loader", "\n", "train_dataset", "=", "DeepFashionAttrSegmDataset", "(", "\n", "img_dir", "=", "opt", "[", "'train_img_dir'", "]", ",", "\n", "segm_dir", "=", "opt", "[", "'segm_dir'", "]", ",", "\n", "pose_dir", "=", "opt", "[", "'pose_dir'", "]", ",", "\n", "ann_dir", "=", "opt", "[", "'train_ann_file'", "]", ",", "\n", "xflip", "=", "True", ")", "\n", "train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "train_dataset", ",", "\n", "batch_size", "=", "opt", "[", "'batch_size'", "]", ",", "\n", "shuffle", "=", "True", ",", "\n", "num_workers", "=", "opt", "[", "'num_workers'", "]", ",", "\n", "persistent_workers", "=", "True", ",", "\n", "drop_last", "=", "True", ")", "\n", "logger", ".", "info", "(", "f'Number of train set: {len(train_dataset)}.'", ")", "\n", "opt", "[", "'max_iters'", "]", "=", "opt", "[", "'num_epochs'", "]", "*", "len", "(", "\n", "train_dataset", ")", "//", "opt", "[", "'batch_size'", "]", "\n", "\n", "val_dataset", "=", "DeepFashionAttrSegmDataset", "(", "\n", "img_dir", "=", "opt", "[", "'train_img_dir'", "]", ",", "\n", "segm_dir", "=", "opt", "[", "'segm_dir'", "]", ",", "\n", "pose_dir", "=", "opt", "[", "'pose_dir'", "]", ",", "\n", "ann_dir", "=", "opt", "[", "'val_ann_file'", "]", ")", "\n", "val_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "val_dataset", ",", "batch_size", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "logger", ".", "info", "(", "f'Number of val set: {len(val_dataset)}.'", ")", "\n", "\n", "test_dataset", "=", "DeepFashionAttrSegmDataset", "(", "\n", "img_dir", "=", "opt", "[", "'test_img_dir'", "]", ",", "\n", "segm_dir", "=", "opt", "[", "'segm_dir'", "]", ",", "\n", "pose_dir", "=", "opt", "[", "'pose_dir'", "]", ",", "\n", "ann_dir", "=", "opt", "[", "'test_ann_file'", "]", ")", "\n", "test_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "test_dataset", ",", "batch_size", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "logger", ".", "info", "(", "f'Number of test set: {len(test_dataset)}.'", ")", "\n", "\n", "current_iter", "=", "0", "\n", "best_epoch", "=", "None", "\n", "best_loss", "=", "100000", "\n", "\n", "model", "=", "create_model", "(", "opt", ")", "\n", "\n", "data_time", ",", "iter_time", "=", "0", ",", "0", "\n", "current_iter", "=", "0", "\n", "\n", "# create message logger (formatted outputs)", "\n", "msg_logger", "=", "MessageLogger", "(", "opt", ",", "current_iter", ",", "tb_logger", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "opt", "[", "'num_epochs'", "]", ")", ":", "\n", "        ", "lr", "=", "model", ".", "update_learning_rate", "(", "epoch", ")", "\n", "\n", "for", "_", ",", "batch_data", "in", "enumerate", "(", "train_loader", ")", ":", "\n", "            ", "data_time", "=", "time", ".", "time", "(", ")", "-", "data_time", "\n", "\n", "current_iter", "+=", "1", "\n", "\n", "model", ".", "optimize_parameters", "(", "batch_data", ",", "current_iter", ")", "\n", "\n", "iter_time", "=", "time", ".", "time", "(", ")", "-", "iter_time", "\n", "if", "current_iter", "%", "opt", "[", "'print_freq'", "]", "==", "0", ":", "\n", "                ", "log_vars", "=", "{", "'epoch'", ":", "epoch", ",", "'iter'", ":", "current_iter", "}", "\n", "log_vars", ".", "update", "(", "{", "'lrs'", ":", "[", "lr", "]", "}", ")", "\n", "log_vars", ".", "update", "(", "{", "'time'", ":", "iter_time", ",", "'data_time'", ":", "data_time", "}", ")", "\n", "log_vars", ".", "update", "(", "model", ".", "get_current_log", "(", ")", ")", "\n", "msg_logger", "(", "log_vars", ")", "\n", "\n", "", "data_time", "=", "time", ".", "time", "(", ")", "\n", "iter_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "if", "epoch", "%", "opt", "[", "'val_freq'", "]", "==", "0", ":", "\n", "            ", "save_dir", "=", "f'{opt[\"path\"][\"visualization\"]}/valset/epoch_{epoch:03d}'", "# noqa", "\n", "os", ".", "makedirs", "(", "save_dir", ",", "exist_ok", "=", "opt", "[", "'debug'", "]", ")", "\n", "val_loss_total", "=", "model", ".", "inference", "(", "val_loader", ",", "save_dir", ")", "\n", "\n", "save_dir", "=", "f'{opt[\"path\"][\"visualization\"]}/testset/epoch_{epoch:03d}'", "# noqa", "\n", "os", ".", "makedirs", "(", "save_dir", ",", "exist_ok", "=", "opt", "[", "'debug'", "]", ")", "\n", "test_loss_total", "=", "model", ".", "inference", "(", "test_loader", ",", "save_dir", ")", "\n", "\n", "logger", ".", "info", "(", "f'Epoch: {epoch}, '", "\n", "f'val_loss_total: {val_loss_total}, '", "\n", "f'test_loss_total: {test_loss_total}.'", ")", "\n", "\n", "if", "test_loss_total", "<", "best_loss", ":", "\n", "                ", "best_epoch", "=", "epoch", "\n", "best_loss", "=", "test_loss_total", "\n", "\n", "", "logger", ".", "info", "(", "f'Best epoch: {best_epoch}, '", "\n", "f'Best test loss: {best_loss: .4f}.'", ")", "\n", "\n", "# save model", "\n", "model", ".", "save_network", "(", "f'{opt[\"path\"][\"models\"]}/epoch{epoch}.pth'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.__init__": [[29, 68], ["QWidget.__init__", "ui_demo.Ex.setupUi", "ui_demo.Ex.show", "QGraphicsScene", "ui_demo.Ex.graphicsView.setScene", "ui_demo.Ex.graphicsView.setAlignment", "ui_demo.Ex.graphicsView.setVerticalScrollBarPolicy", "ui_demo.Ex.graphicsView.setHorizontalScrollBarPolicy", "ui.mouse_event.GraphicsScene", "ui_demo.Ex.graphicsView_2.setScene", "ui_demo.Ex.graphicsView_2.setAlignment", "ui_demo.Ex.graphicsView_2.setVerticalScrollBarPolicy", "ui_demo.Ex.graphicsView_2.setHorizontalScrollBarPolicy", "QGraphicsScene", "ui_demo.Ex.graphicsView_3.setScene", "ui_demo.Ex.graphicsView_3.setAlignment", "ui_demo.Ex.graphicsView_3.setVerticalScrollBarPolicy", "ui_demo.Ex.graphicsView_3.setHorizontalScrollBarPolicy", "QColorDialog", "models.sample_model.SampleFromPoseModel"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__", "home.repos.pwc.inspect_result.yumingj_Text2Human.ui.ui.Ui_Form.setupUi"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "Ex", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "setupUi", "(", "self", ")", "\n", "self", ".", "show", "(", ")", "\n", "\n", "self", ".", "output_img", "=", "None", "\n", "\n", "self", ".", "mat_img", "=", "None", "\n", "\n", "self", ".", "mode", "=", "0", "\n", "self", ".", "size", "=", "6", "\n", "self", ".", "mask", "=", "None", "\n", "self", ".", "mask_m", "=", "None", "\n", "self", ".", "img", "=", "None", "\n", "\n", "# about UI", "\n", "self", ".", "mouse_clicked", "=", "False", "\n", "self", ".", "scene", "=", "QGraphicsScene", "(", ")", "\n", "self", ".", "graphicsView", ".", "setScene", "(", "self", ".", "scene", ")", "\n", "self", ".", "graphicsView", ".", "setAlignment", "(", "Qt", ".", "AlignTop", "|", "Qt", ".", "AlignLeft", ")", "\n", "self", ".", "graphicsView", ".", "setVerticalScrollBarPolicy", "(", "Qt", ".", "ScrollBarAlwaysOff", ")", "\n", "self", ".", "graphicsView", ".", "setHorizontalScrollBarPolicy", "(", "Qt", ".", "ScrollBarAlwaysOff", ")", "\n", "\n", "self", ".", "ref_scene", "=", "GraphicsScene", "(", "self", ".", "mode", ",", "self", ".", "size", ")", "\n", "self", ".", "graphicsView_2", ".", "setScene", "(", "self", ".", "ref_scene", ")", "\n", "self", ".", "graphicsView_2", ".", "setAlignment", "(", "Qt", ".", "AlignTop", "|", "Qt", ".", "AlignLeft", ")", "\n", "self", ".", "graphicsView_2", ".", "setVerticalScrollBarPolicy", "(", "Qt", ".", "ScrollBarAlwaysOff", ")", "\n", "self", ".", "graphicsView_2", ".", "setHorizontalScrollBarPolicy", "(", "Qt", ".", "ScrollBarAlwaysOff", ")", "\n", "\n", "self", ".", "result_scene", "=", "QGraphicsScene", "(", ")", "\n", "self", ".", "graphicsView_3", ".", "setScene", "(", "self", ".", "result_scene", ")", "\n", "self", ".", "graphicsView_3", ".", "setAlignment", "(", "Qt", ".", "AlignTop", "|", "Qt", ".", "AlignLeft", ")", "\n", "self", ".", "graphicsView_3", ".", "setVerticalScrollBarPolicy", "(", "Qt", ".", "ScrollBarAlwaysOff", ")", "\n", "self", ".", "graphicsView_3", ".", "setHorizontalScrollBarPolicy", "(", "Qt", ".", "ScrollBarAlwaysOff", ")", "\n", "\n", "self", ".", "dlg", "=", "QColorDialog", "(", "self", ".", "graphicsView", ")", "\n", "self", ".", "color", "=", "None", "\n", "\n", "self", ".", "sample_model", "=", "SampleFromPoseModel", "(", "opt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.open_densepose": [[69, 101], ["QFileDialog.getOpenFileName", "QDir.currentPath", "QPixmap", "PIL.Image.open", "PIL.Image.open.copy", "image.scaled.scaled.isNull", "image.scaled.scaled.scaled", "ui_demo.Ex.scene.addPixmap", "ui_demo.Ex.ref_scene.clear", "ui_demo.Ex.result_scene.clear", "[].transpose().astype", "torch.from_numpy().unsqueeze", "ui_demo.Ex.sample_model.feed_pose_data", "QMessageBox.information", "ui_demo.Ex.graphicsView.size", "len", "ui_demo.Ex.scene.removeItem", "ui_demo.Ex.scene.items", "[].transpose", "torch.from_numpy", "ui_demo.Ex.scene.items", "numpy.array", "ui_demo.Ex.pose_img.resize"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.clear", "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.clear", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.feed_pose_data"], ["", "def", "open_densepose", "(", "self", ")", ":", "\n", "        ", "fileName", ",", "_", "=", "QFileDialog", ".", "getOpenFileName", "(", "self", ",", "\"Open File\"", ",", "\n", "QDir", ".", "currentPath", "(", ")", ")", "\n", "if", "fileName", ":", "\n", "            ", "image", "=", "QPixmap", "(", "fileName", ")", "\n", "mat_img", "=", "Image", ".", "open", "(", "fileName", ")", "\n", "self", ".", "pose_img", "=", "mat_img", ".", "copy", "(", ")", "\n", "if", "image", ".", "isNull", "(", ")", ":", "\n", "                ", "QMessageBox", ".", "information", "(", "self", ",", "\"Image Viewer\"", ",", "\n", "\"Cannot load %s.\"", "%", "fileName", ")", "\n", "return", "\n", "", "image", "=", "image", ".", "scaled", "(", "self", ".", "graphicsView", ".", "size", "(", ")", ",", "\n", "Qt", ".", "IgnoreAspectRatio", ")", "\n", "\n", "if", "len", "(", "self", ".", "scene", ".", "items", "(", ")", ")", ">", "0", ":", "\n", "                ", "self", ".", "scene", ".", "removeItem", "(", "self", ".", "scene", ".", "items", "(", ")", "[", "-", "1", "]", ")", "\n", "", "self", ".", "scene", ".", "addPixmap", "(", "image", ")", "\n", "\n", "self", ".", "ref_scene", ".", "clear", "(", ")", "\n", "self", ".", "result_scene", ".", "clear", "(", ")", "\n", "\n", "# load pose to model", "\n", "self", ".", "pose_img", "=", "np", ".", "array", "(", "\n", "self", ".", "pose_img", ".", "resize", "(", "\n", "size", "=", "(", "256", ",", "512", ")", ",", "\n", "resample", "=", "Image", ".", "LANCZOS", ")", ")", "[", ":", ",", ":", ",", "2", ":", "]", ".", "transpose", "(", "\n", "2", ",", "0", ",", "1", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "self", ".", "pose_img", "=", "self", ".", "pose_img", "/", "12.", "-", "1", "\n", "\n", "self", ".", "pose_img", "=", "torch", ".", "from_numpy", "(", "self", ".", "pose_img", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "self", ".", "sample_model", ".", "feed_pose_data", "(", "self", ".", "pose_img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.generate_parsing": [[102, 135], ["ui_demo.Ex.ref_scene.reset_items", "ui_demo.Ex.ref_scene.reset", "ui_demo.Ex.message_box_1.text", "utils.language_utils.generate_shape_attributes", "torch.LongTensor().unsqueeze", "ui_demo.Ex.sample_model.feed_shape_attributes", "ui_demo.Ex.sample_model.generate_parsing_map", "ui_demo.Ex.sample_model.generate_quantized_segm", "ui_demo.Ex.sample_model.palette_result", "cv2.cvtColor", "QImage", "QPixmap.fromImage", "image.scaled.scaled.scaled", "ui_demo.Ex.ref_scene.addPixmap", "ui_demo.Ex.result_scene.clear", "ui_demo.Ex.sample_model.segm[].cpu", "cv2.cvtColor", "ui_demo.Ex.colored_segm.data.tobytes", "ui_demo.Ex.graphicsView.size", "len", "ui_demo.Ex.ref_scene.removeItem", "torch.LongTensor", "ui_demo.Ex.ref_scene.items", "ui_demo.Ex.ref_scene.items"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.ui.mouse_event.GraphicsScene.reset_items", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.reset", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.language_utils.generate_shape_attributes", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.feed_shape_attributes", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.generate_parsing_map", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.generate_quantized_segm", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.palette_result", "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.clear"], ["", "", "def", "generate_parsing", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "reset_items", "(", ")", "\n", "self", ".", "ref_scene", ".", "reset", "(", ")", "\n", "\n", "shape_texts", "=", "self", ".", "message_box_1", ".", "text", "(", ")", "\n", "\n", "shape_attributes", "=", "generate_shape_attributes", "(", "shape_texts", ")", "\n", "shape_attributes", "=", "torch", ".", "LongTensor", "(", "shape_attributes", ")", ".", "unsqueeze", "(", "0", ")", "\n", "self", ".", "sample_model", ".", "feed_shape_attributes", "(", "shape_attributes", ")", "\n", "\n", "self", ".", "sample_model", ".", "generate_parsing_map", "(", ")", "\n", "self", ".", "sample_model", ".", "generate_quantized_segm", "(", ")", "\n", "\n", "self", ".", "colored_segm", "=", "self", ".", "sample_model", ".", "palette_result", "(", "\n", "self", ".", "sample_model", ".", "segm", "[", "0", "]", ".", "cpu", "(", ")", ")", "\n", "\n", "self", ".", "mask_m", "=", "cv2", ".", "cvtColor", "(", "\n", "cv2", ".", "cvtColor", "(", "self", ".", "colored_segm", ",", "cv2", ".", "COLOR_RGB2BGR", ")", ",", "\n", "cv2", ".", "COLOR_BGR2RGB", ")", "\n", "\n", "qim", "=", "QImage", "(", "self", ".", "colored_segm", ".", "data", ".", "tobytes", "(", ")", ",", "\n", "self", ".", "colored_segm", ".", "shape", "[", "1", "]", ",", "self", ".", "colored_segm", ".", "shape", "[", "0", "]", ",", "\n", "QImage", ".", "Format_RGB888", ")", "\n", "\n", "image", "=", "QPixmap", ".", "fromImage", "(", "qim", ")", "\n", "\n", "image", "=", "image", ".", "scaled", "(", "self", ".", "graphicsView", ".", "size", "(", ")", ",", "Qt", ".", "IgnoreAspectRatio", ")", "\n", "\n", "if", "len", "(", "self", ".", "ref_scene", ".", "items", "(", ")", ")", ">", "0", ":", "\n", "            ", "self", ".", "ref_scene", ".", "removeItem", "(", "self", ".", "ref_scene", ".", "items", "(", ")", "[", "-", "1", "]", ")", "\n", "", "self", ".", "ref_scene", ".", "addPixmap", "(", "image", ")", "\n", "\n", "self", ".", "result_scene", ".", "clear", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.generate_human": [[136, 180], ["range", "numpy.full", "enumerate", "torch.from_numpy().unsqueeze().unsqueeze().to", "ui_demo.Ex.sample_model.generate_quantized_segm", "ui_demo.Ex.message_box_2.text", "utils.language_utils.generate_texture_attributes", "torch.LongTensor", "ui_demo.Ex.sample_model.feed_texture_attributes", "ui_demo.Ex.sample_model.generate_texture_map", "ui_demo.Ex.sample_model.sample_and_refine", "numpy.asarray.permute", "numpy.asarray.detach().cpu().numpy", "numpy.asarray", "QImage", "QPixmap.fromImage", "image.scaled.scaled.scaled", "ui_demo.Ex.result_scene.addPixmap", "ui_demo.Ex.make_mask", "numpy.asarray.data.tobytes", "ui_demo.Ex.graphicsView.size", "len", "ui_demo.Ex.result_scene.removeItem", "torch.from_numpy().unsqueeze().unsqueeze", "numpy.asarray.detach().cpu", "ui_demo.Ex.result_scene.items", "ui_demo.Ex.result_scene.items", "numpy.sum", "torch.from_numpy().unsqueeze", "numpy.asarray.detach", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.generate_quantized_segm", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.language_utils.generate_texture_attributes", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.feed_texture_attributes", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.generate_texture_map", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.sample_and_refine", "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.make_mask"], ["", "def", "generate_human", "(", "self", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "24", ")", ":", "\n", "            ", "self", ".", "mask_m", "=", "self", ".", "make_mask", "(", "self", ".", "mask_m", ",", "\n", "self", ".", "ref_scene", ".", "mask_points", "[", "i", "]", ",", "\n", "self", ".", "ref_scene", ".", "size_points", "[", "i", "]", ",", "\n", "color_list", "[", "i", "]", ")", "\n", "\n", "", "seg_map", "=", "np", ".", "full", "(", "self", ".", "mask_m", ".", "shape", "[", ":", "-", "1", "]", ",", "-", "1", ")", "\n", "\n", "# convert rgb to num", "\n", "for", "index", ",", "color", "in", "enumerate", "(", "color_list", ")", ":", "\n", "            ", "seg_map", "[", "np", ".", "sum", "(", "self", ".", "mask_m", "==", "color", ",", "axis", "=", "2", ")", "==", "3", "]", "=", "index", "\n", "", "assert", "(", "seg_map", "!=", "-", "1", ")", ".", "all", "(", ")", "\n", "\n", "self", ".", "sample_model", ".", "segm", "=", "torch", ".", "from_numpy", "(", "seg_map", ")", ".", "unsqueeze", "(", "\n", "0", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "self", ".", "sample_model", ".", "device", ")", "\n", "self", ".", "sample_model", ".", "generate_quantized_segm", "(", ")", "\n", "\n", "texture_texts", "=", "self", ".", "message_box_2", ".", "text", "(", ")", "\n", "texture_attributes", "=", "generate_texture_attributes", "(", "texture_texts", ")", "\n", "\n", "texture_attributes", "=", "torch", ".", "LongTensor", "(", "texture_attributes", ")", "\n", "\n", "self", ".", "sample_model", ".", "feed_texture_attributes", "(", "texture_attributes", ")", "\n", "\n", "self", ".", "sample_model", ".", "generate_texture_map", "(", ")", "\n", "result", "=", "self", ".", "sample_model", ".", "sample_and_refine", "(", ")", "\n", "result", "=", "result", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "\n", "result", "=", "result", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "result", "=", "result", "*", "255", "\n", "\n", "result", "=", "np", ".", "asarray", "(", "result", "[", "0", ",", ":", ",", ":", ",", ":", "]", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "\n", "self", ".", "output_img", "=", "result", "\n", "\n", "qim", "=", "QImage", "(", "result", ".", "data", ".", "tobytes", "(", ")", ",", "result", ".", "shape", "[", "1", "]", ",", "result", ".", "shape", "[", "0", "]", ",", "\n", "QImage", ".", "Format_RGB888", ")", "\n", "image", "=", "QPixmap", ".", "fromImage", "(", "qim", ")", "\n", "\n", "image", "=", "image", ".", "scaled", "(", "self", ".", "graphicsView", ".", "size", "(", ")", ",", "Qt", ".", "IgnoreAspectRatio", ")", "\n", "\n", "if", "len", "(", "self", ".", "result_scene", ".", "items", "(", ")", ")", ">", "0", ":", "\n", "            ", "self", ".", "result_scene", ".", "removeItem", "(", "self", ".", "result_scene", ".", "items", "(", ")", "[", "-", "1", "]", ")", "\n", "", "self", ".", "result_scene", ".", "addPixmap", "(", "image", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.top_mode": [[181, 183], ["None"], "methods", ["None"], ["", "def", "top_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.skin_mode": [[184, 186], ["None"], "methods", ["None"], ["", "def", "skin_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "15", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.outer_mode": [[187, 189], ["None"], "methods", ["None"], ["", "def", "outer_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.face_mode": [[190, 192], ["None"], "methods", ["None"], ["", "def", "face_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "14", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.skirt_mode": [[193, 195], ["None"], "methods", ["None"], ["", "def", "skirt_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "3", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.hair_mode": [[196, 198], ["None"], "methods", ["None"], ["", "def", "hair_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "13", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.dress_mode": [[199, 201], ["None"], "methods", ["None"], ["", "def", "dress_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "4", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.headwear_mode": [[202, 204], ["None"], "methods", ["None"], ["", "def", "headwear_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "7", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.pants_mode": [[205, 207], ["None"], "methods", ["None"], ["", "def", "pants_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "5", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.eyeglass_mode": [[208, 210], ["None"], "methods", ["None"], ["", "def", "eyeglass_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "8", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.rompers_mode": [[211, 213], ["None"], "methods", ["None"], ["", "def", "rompers_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "21", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.footwear_mode": [[214, 216], ["None"], "methods", ["None"], ["", "def", "footwear_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "11", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.leggings_mode": [[217, 219], ["None"], "methods", ["None"], ["", "def", "leggings_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "6", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.ring_mode": [[220, 222], ["None"], "methods", ["None"], ["", "def", "ring_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "16", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.belt_mode": [[223, 225], ["None"], "methods", ["None"], ["", "def", "belt_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "10", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.neckwear_mode": [[226, 228], ["None"], "methods", ["None"], ["", "def", "neckwear_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "9", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.wrist_mode": [[229, 231], ["None"], "methods", ["None"], ["", "def", "wrist_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "17", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.socks_mode": [[232, 234], ["None"], "methods", ["None"], ["", "def", "socks_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "18", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.tie_mode": [[235, 237], ["None"], "methods", ["None"], ["", "def", "tie_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "23", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.earstuds_mode": [[238, 240], ["None"], "methods", ["None"], ["", "def", "earstuds_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "22", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.necklace_mode": [[241, 243], ["None"], "methods", ["None"], ["", "def", "necklace_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "20", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.bag_mode": [[244, 246], ["None"], "methods", ["None"], ["", "def", "bag_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "12", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.glove_mode": [[247, 249], ["None"], "methods", ["None"], ["", "def", "glove_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "19", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.background_mode": [[250, 252], ["None"], "methods", ["None"], ["", "def", "background_mode", "(", "self", ")", ":", "\n", "        ", "self", ".", "ref_scene", ".", "mode", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.make_mask": [[253, 258], ["len", "enumerate", "cv2.line"], "methods", ["None"], ["", "def", "make_mask", "(", "self", ",", "mask", ",", "pts", ",", "sizes", ",", "color", ")", ":", "\n", "        ", "if", "len", "(", "pts", ")", ">", "0", ":", "\n", "            ", "for", "idx", ",", "pt", "in", "enumerate", "(", "pts", ")", ":", "\n", "                ", "cv2", ".", "line", "(", "mask", ",", "pt", "[", "'prev'", "]", ",", "pt", "[", "'curr'", "]", ",", "color", ",", "sizes", "[", "idx", "]", ")", "\n", "", "", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.save_img": [[259, 264], ["type", "QFileDialog.getSaveFileName", "cv2.imwrite", "QDir.currentPath"], "methods", ["None"], ["", "def", "save_img", "(", "self", ")", ":", "\n", "        ", "if", "type", "(", "self", ".", "output_img", ")", ":", "\n", "            ", "fileName", ",", "_", "=", "QFileDialog", ".", "getSaveFileName", "(", "self", ",", "\"Save File\"", ",", "\n", "QDir", ".", "currentPath", "(", ")", ")", "\n", "cv2", ".", "imwrite", "(", "fileName", "+", "'.png'", ",", "self", ".", "output_img", "[", ":", ",", ":", ",", ":", ":", "-", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.undo": [[265, 267], ["ui_demo.Ex.scene.undo"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.ui.mouse_event.GraphicsScene.undo"], ["", "", "def", "undo", "(", "self", ")", ":", "\n", "        ", "self", ".", "scene", ".", "undo", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.clear": [[268, 276], ["ui_demo.Ex.ref_scene.reset_items", "ui_demo.Ex.ref_scene.reset", "ui_demo.Ex.ref_scene.clear", "ui_demo.Ex.result_scene.clear"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.ui.mouse_event.GraphicsScene.reset_items", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.reset", "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.clear", "home.repos.pwc.inspect_result.yumingj_Text2Human.None.ui_demo.Ex.clear"], ["", "def", "clear", "(", "self", ")", ":", "\n", "\n", "        ", "self", ".", "ref_scene", ".", "reset_items", "(", ")", "\n", "self", ".", "ref_scene", ".", "reset", "(", ")", "\n", "\n", "self", ".", "ref_scene", ".", "clear", "(", ")", "\n", "\n", "self", ".", "result_scene", ".", "clear", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.sample_from_parsing.main": [[15, 50], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "utils.options.parse", "utils.util.make_exp_dirs", "os.join", "utils.logger.get_root_logger", "utils.logger.get_root_logger.info", "utils.options.dict_to_nonedict", "utils.logger.get_root_logger.info", "utils.util.set_random_seed", "data.segm_attr_dataset.DeepFashionAttrSegmDataset", "torch.utils.data.DataLoader", "utils.logger.get_root_logger.info", "models.create_model", "models.create_model.inference", "utils.options.dict2str", "random.randint", "len"], "function", ["home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.parse", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.make_exp_dirs", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.dict_to_nonedict", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.set_random_seed", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.__init__.create_model", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.inference", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.dict2str"], ["def", "main", "(", ")", ":", "\n", "# options", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'-opt'", ",", "type", "=", "str", ",", "help", "=", "'Path to option YAML file.'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "opt", "=", "parse", "(", "args", ".", "opt", ",", "is_train", "=", "False", ")", "\n", "\n", "# mkdir and loggers", "\n", "make_exp_dirs", "(", "opt", ")", "\n", "log_file", "=", "osp", ".", "join", "(", "opt", "[", "'path'", "]", "[", "'log'", "]", ",", "f\"test_{opt['name']}.log\"", ")", "\n", "logger", "=", "get_root_logger", "(", "\n", "logger_name", "=", "'base'", ",", "log_level", "=", "logging", ".", "INFO", ",", "log_file", "=", "log_file", ")", "\n", "logger", ".", "info", "(", "dict2str", "(", "opt", ")", ")", "\n", "\n", "# convert to NoneDict, which returns None for missing keys", "\n", "opt", "=", "dict_to_nonedict", "(", "opt", ")", "\n", "\n", "# random seed", "\n", "seed", "=", "opt", "[", "'manual_seed'", "]", "\n", "if", "seed", "is", "None", ":", "\n", "        ", "seed", "=", "random", ".", "randint", "(", "1", ",", "10000", ")", "\n", "", "logger", ".", "info", "(", "f'Random seed: {seed}'", ")", "\n", "set_random_seed", "(", "seed", ")", "\n", "\n", "test_dataset", "=", "DeepFashionAttrSegmDataset", "(", "\n", "img_dir", "=", "opt", "[", "'test_img_dir'", "]", ",", "\n", "segm_dir", "=", "opt", "[", "'segm_dir'", "]", ",", "\n", "pose_dir", "=", "opt", "[", "'pose_dir'", "]", ",", "\n", "ann_dir", "=", "opt", "[", "'test_ann_file'", "]", ")", "\n", "test_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "test_dataset", ",", "batch_size", "=", "4", ",", "shuffle", "=", "False", ")", "\n", "logger", ".", "info", "(", "f'Number of test set: {len(test_dataset)}.'", ")", "\n", "\n", "model", "=", "create_model", "(", "opt", ")", "\n", "_", "=", "model", ".", "inference", "(", "test_loader", ",", "opt", "[", "'path'", "]", "[", "'results_root'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.None.train_index_prediction.main": [[17, 130], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "utils.options.parse", "utils.util.make_exp_dirs", "os.join", "utils.logger.get_root_logger", "utils.logger.get_root_logger.info", "utils.options.dict_to_nonedict", "data.segm_attr_dataset.DeepFashionAttrSegmDataset", "torch.utils.data.DataLoader", "utils.logger.get_root_logger.info", "data.segm_attr_dataset.DeepFashionAttrSegmDataset", "torch.utils.data.DataLoader", "utils.logger.get_root_logger.info", "data.segm_attr_dataset.DeepFashionAttrSegmDataset", "torch.utils.data.DataLoader", "utils.logger.get_root_logger.info", "models.create_model", "utils.logger.MessageLogger", "range", "utils.options.dict2str", "utils.logger.init_tb_logger", "models.create_model.update_learning_rate", "enumerate", "len", "models.create_model.feed_data", "models.create_model.optimize_parameters", "time.time", "time.time", "os.makedirs", "os.makedirs", "models.create_model.inference", "os.makedirs", "os.makedirs", "models.create_model.inference", "utils.logger.get_root_logger.info", "utils.logger.get_root_logger.info", "models.create_model.save_network", "len", "len", "len", "time.time", "time.time", "log_vars.update", "log_vars.update", "log_vars.update", "utils.logger.MessageLogger.", "models.create_model.get_current_log"], "function", ["home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.parse", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.make_exp_dirs", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.dict_to_nonedict", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.__init__.create_model", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.dict2str", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.logger.init_tb_logger", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.update_learning_rate", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.feed_data", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.optimize_parameters", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.inference", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.inference", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.save_network", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.get_current_log"], ["def", "main", "(", ")", ":", "\n", "# options", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'-opt'", ",", "type", "=", "str", ",", "help", "=", "'Path to option YAML file.'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "opt", "=", "parse", "(", "args", ".", "opt", ",", "is_train", "=", "True", ")", "\n", "\n", "# mkdir and loggers", "\n", "make_exp_dirs", "(", "opt", ")", "\n", "log_file", "=", "osp", ".", "join", "(", "opt", "[", "'path'", "]", "[", "'log'", "]", ",", "f\"train_{opt['name']}.log\"", ")", "\n", "logger", "=", "get_root_logger", "(", "\n", "logger_name", "=", "'base'", ",", "log_level", "=", "logging", ".", "INFO", ",", "log_file", "=", "log_file", ")", "\n", "logger", ".", "info", "(", "dict2str", "(", "opt", ")", ")", "\n", "# initialize tensorboard logger", "\n", "tb_logger", "=", "None", "\n", "if", "opt", "[", "'use_tb_logger'", "]", "and", "'debug'", "not", "in", "opt", "[", "'name'", "]", ":", "\n", "        ", "tb_logger", "=", "init_tb_logger", "(", "log_dir", "=", "'./tb_logger/'", "+", "opt", "[", "'name'", "]", ")", "\n", "\n", "# convert to NoneDict, which returns None for missing keys", "\n", "", "opt", "=", "dict_to_nonedict", "(", "opt", ")", "\n", "\n", "# set up data loader", "\n", "train_dataset", "=", "DeepFashionAttrSegmDataset", "(", "\n", "img_dir", "=", "opt", "[", "'train_img_dir'", "]", ",", "\n", "segm_dir", "=", "opt", "[", "'segm_dir'", "]", ",", "\n", "pose_dir", "=", "opt", "[", "'pose_dir'", "]", ",", "\n", "ann_dir", "=", "opt", "[", "'train_ann_file'", "]", ",", "\n", "xflip", "=", "True", ")", "\n", "train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "train_dataset", ",", "\n", "batch_size", "=", "opt", "[", "'batch_size'", "]", ",", "\n", "shuffle", "=", "True", ",", "\n", "num_workers", "=", "opt", "[", "'num_workers'", "]", ",", "\n", "drop_last", "=", "True", ")", "\n", "logger", ".", "info", "(", "f'Number of train set: {len(train_dataset)}.'", ")", "\n", "opt", "[", "'max_iters'", "]", "=", "opt", "[", "'num_epochs'", "]", "*", "len", "(", "\n", "train_dataset", ")", "//", "opt", "[", "'batch_size'", "]", "\n", "\n", "val_dataset", "=", "DeepFashionAttrSegmDataset", "(", "\n", "img_dir", "=", "opt", "[", "'train_img_dir'", "]", ",", "\n", "segm_dir", "=", "opt", "[", "'segm_dir'", "]", ",", "\n", "pose_dir", "=", "opt", "[", "'pose_dir'", "]", ",", "\n", "ann_dir", "=", "opt", "[", "'val_ann_file'", "]", ")", "\n", "val_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "val_dataset", ",", "batch_size", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "logger", ".", "info", "(", "f'Number of val set: {len(val_dataset)}.'", ")", "\n", "\n", "test_dataset", "=", "DeepFashionAttrSegmDataset", "(", "\n", "img_dir", "=", "opt", "[", "'test_img_dir'", "]", ",", "\n", "segm_dir", "=", "opt", "[", "'segm_dir'", "]", ",", "\n", "pose_dir", "=", "opt", "[", "'pose_dir'", "]", ",", "\n", "ann_dir", "=", "opt", "[", "'test_ann_file'", "]", ")", "\n", "test_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "test_dataset", ",", "batch_size", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "logger", ".", "info", "(", "f'Number of test set: {len(test_dataset)}.'", ")", "\n", "\n", "current_iter", "=", "0", "\n", "best_epoch", "=", "None", "\n", "best_acc", "=", "0", "\n", "\n", "model", "=", "create_model", "(", "opt", ")", "\n", "\n", "data_time", ",", "iter_time", "=", "0", ",", "0", "\n", "current_iter", "=", "0", "\n", "\n", "# create message logger (formatted outputs)", "\n", "msg_logger", "=", "MessageLogger", "(", "opt", ",", "current_iter", ",", "tb_logger", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "opt", "[", "'num_epochs'", "]", ")", ":", "\n", "        ", "lr", "=", "model", ".", "update_learning_rate", "(", "epoch", ")", "\n", "\n", "for", "_", ",", "batch_data", "in", "enumerate", "(", "train_loader", ")", ":", "\n", "            ", "data_time", "=", "time", ".", "time", "(", ")", "-", "data_time", "\n", "\n", "current_iter", "+=", "1", "\n", "\n", "model", ".", "feed_data", "(", "batch_data", ")", "\n", "model", ".", "optimize_parameters", "(", ")", "\n", "\n", "iter_time", "=", "time", ".", "time", "(", ")", "-", "iter_time", "\n", "if", "current_iter", "%", "opt", "[", "'print_freq'", "]", "==", "0", ":", "\n", "                ", "log_vars", "=", "{", "'epoch'", ":", "epoch", ",", "'iter'", ":", "current_iter", "}", "\n", "log_vars", ".", "update", "(", "{", "'lrs'", ":", "[", "lr", "]", "}", ")", "\n", "log_vars", ".", "update", "(", "{", "'time'", ":", "iter_time", ",", "'data_time'", ":", "data_time", "}", ")", "\n", "log_vars", ".", "update", "(", "model", ".", "get_current_log", "(", ")", ")", "\n", "msg_logger", "(", "log_vars", ")", "\n", "\n", "", "data_time", "=", "time", ".", "time", "(", ")", "\n", "iter_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "if", "epoch", "%", "opt", "[", "'val_freq'", "]", "==", "0", ":", "\n", "            ", "save_dir", "=", "f'{opt[\"path\"][\"visualization\"]}/valset/epoch_{epoch:03d}'", "# noqa", "\n", "os", ".", "makedirs", "(", "save_dir", ",", "exist_ok", "=", "opt", "[", "'debug'", "]", ")", "\n", "val_acc", "=", "model", ".", "inference", "(", "val_loader", ",", "save_dir", ")", "\n", "\n", "save_dir", "=", "f'{opt[\"path\"][\"visualization\"]}/testset/epoch_{epoch:03d}'", "# noqa", "\n", "os", ".", "makedirs", "(", "save_dir", ",", "exist_ok", "=", "opt", "[", "'debug'", "]", ")", "\n", "test_acc", "=", "model", ".", "inference", "(", "test_loader", ",", "save_dir", ")", "\n", "\n", "logger", ".", "info", "(", "\n", "f'Epoch: {epoch}, val_acc: {val_acc: .4f}, test_acc: {test_acc: .4f}.'", "\n", ")", "\n", "\n", "if", "test_acc", ">", "best_acc", ":", "\n", "                ", "best_epoch", "=", "epoch", "\n", "best_acc", "=", "test_acc", "\n", "\n", "", "logger", ".", "info", "(", "f'Best epoch: {best_epoch}, '", "\n", "f'Best test acc: {best_acc: .4f}.'", ")", "\n", "\n", "# save model", "\n", "model", ".", "save_network", "(", "\n", "f'{opt[\"path\"][\"models\"]}/models_epoch{epoch}.pth'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.ui_util.config.Config.__init__": [[10, 20], ["os.path.exists", "logger.info", "logger.info", "open", "yaml.load", "logger.error"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "filename", "=", "None", ")", ":", "\n", "        ", "assert", "os", ".", "path", ".", "exists", "(", "filename", ")", ",", "\"ERROR: Config File doesn't exist.\"", "\n", "try", ":", "\n", "            ", "with", "open", "(", "filename", ",", "'r'", ")", "as", "f", ":", "\n", "                ", "self", ".", "_cfg_dict", "=", "yaml", ".", "load", "(", "f", ")", "\n", "# parent of IOError, OSError *and* WindowsError where available", "\n", "", "", "except", "EnvironmentError", ":", "\n", "            ", "logger", ".", "error", "(", "'Please check the file with name of \"%s\"'", ",", "filename", ")", "\n", "", "logger", ".", "info", "(", "' APP CONFIG '", ".", "center", "(", "80", ",", "'-'", ")", ")", "\n", "logger", ".", "info", "(", "''", ".", "center", "(", "80", ",", "'-'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.ui_util.config.Config.__getattr__": [[21, 26], ["isinstance", "DictAsMember"], "methods", ["None"], ["", "def", "__getattr__", "(", "self", ",", "name", ")", ":", "\n", "        ", "value", "=", "self", ".", "_cfg_dict", "[", "name", "]", "\n", "if", "isinstance", "(", "value", ",", "dict", ")", ":", "\n", "            ", "value", "=", "DictAsMember", "(", "value", ")", "\n", "", "return", "value", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.ui.mouse_event.GraphicsScene.__init__": [[38, 59], ["QGraphicsScene.__init__", "range", "range", "len", "mouse_event.GraphicsScene.mask_points.append", "len", "mouse_event.GraphicsScene.size_points.append"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "mode", ",", "size", ",", "parent", "=", "None", ")", ":", "\n", "        ", "QGraphicsScene", ".", "__init__", "(", "self", ",", "parent", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "size", "=", "size", "\n", "self", ".", "mouse_clicked", "=", "False", "\n", "self", ".", "prev_pt", "=", "None", "\n", "\n", "# self.masked_image = None", "\n", "\n", "# save the points", "\n", "self", ".", "mask_points", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "color_list", ")", ")", ":", "\n", "            ", "self", ".", "mask_points", ".", "append", "(", "[", "]", ")", "\n", "\n", "# save the size of points", "\n", "", "self", ".", "size_points", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "color_list", ")", ")", ":", "\n", "            ", "self", ".", "size_points", ".", "append", "(", "[", "]", ")", "\n", "\n", "# save the history of edit", "\n", "", "self", ".", "history", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.ui.mouse_event.GraphicsScene.reset": [[60, 74], ["range", "range", "len", "mouse_event.GraphicsScene.mask_points.append", "len", "mouse_event.GraphicsScene.size_points.append"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "# save the points", "\n", "        ", "self", ".", "mask_points", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "color_list", ")", ")", ":", "\n", "            ", "self", ".", "mask_points", ".", "append", "(", "[", "]", ")", "\n", "# save the size of points", "\n", "", "self", ".", "size_points", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "color_list", ")", ")", ":", "\n", "            ", "self", ".", "size_points", ".", "append", "(", "[", "]", ")", "\n", "# save the history of edit", "\n", "", "self", ".", "history", "=", "[", "]", "\n", "\n", "self", ".", "mode", "=", "0", "\n", "self", ".", "prev_pt", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.ui.mouse_event.GraphicsScene.mousePressEvent": [[75, 77], ["None"], "methods", ["None"], ["", "def", "mousePressEvent", "(", "self", ",", "event", ")", ":", "\n", "        ", "self", ".", "mouse_clicked", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.ui.mouse_event.GraphicsScene.mouseReleaseEvent": [[78, 81], ["None"], "methods", ["None"], ["", "def", "mouseReleaseEvent", "(", "self", ",", "event", ")", ":", "\n", "        ", "self", ".", "prev_pt", "=", "None", "\n", "self", ".", "mouse_clicked", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.ui.mouse_event.GraphicsScene.mouseMoveEvent": [[82, 98], ["mouse_event.GraphicsScene.drawMask", "mouse_event.GraphicsScene.size_points[].append", "mouse_event.GraphicsScene.mask_points[].append", "mouse_event.GraphicsScene.history.append", "event.scenePos", "event.scenePos", "event.scenePos", "int", "int", "int", "int", "mouse_event.GraphicsScene.prev_pt.x", "mouse_event.GraphicsScene.prev_pt.y", "event.scenePos().x", "event.scenePos().y", "event.scenePos", "event.scenePos"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.ui.mouse_event.GraphicsScene.drawMask"], ["", "def", "mouseMoveEvent", "(", "self", ",", "event", ")", ":", "# drawing", "\n", "        ", "if", "self", ".", "mouse_clicked", ":", "\n", "            ", "if", "self", ".", "prev_pt", ":", "\n", "                ", "self", ".", "drawMask", "(", "self", ".", "prev_pt", ",", "event", ".", "scenePos", "(", ")", ",", "\n", "color_list", "[", "self", ".", "mode", "]", ",", "self", ".", "size", ")", "\n", "pts", "=", "{", "}", "\n", "pts", "[", "'prev'", "]", "=", "(", "int", "(", "self", ".", "prev_pt", ".", "x", "(", ")", ")", ",", "int", "(", "self", ".", "prev_pt", ".", "y", "(", ")", ")", ")", "\n", "pts", "[", "'curr'", "]", "=", "(", "int", "(", "event", ".", "scenePos", "(", ")", ".", "x", "(", ")", ")", ",", "\n", "int", "(", "event", ".", "scenePos", "(", ")", ".", "y", "(", ")", ")", ")", "\n", "\n", "self", ".", "size_points", "[", "self", ".", "mode", "]", ".", "append", "(", "self", ".", "size", ")", "\n", "self", ".", "mask_points", "[", "self", ".", "mode", "]", ".", "append", "(", "pts", ")", "\n", "self", ".", "history", ".", "append", "(", "self", ".", "mode", ")", "\n", "self", ".", "prev_pt", "=", "event", ".", "scenePos", "(", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "prev_pt", "=", "event", ".", "scenePos", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.ui.mouse_event.GraphicsScene.drawMask": [[99, 103], ["QGraphicsLineItem", "QGraphicsLineItem.setPen", "mouse_event.GraphicsScene.addItem", "QLineF", "QPen"], "methods", ["None"], ["", "", "", "def", "drawMask", "(", "self", ",", "prev_pt", ",", "curr_pt", ",", "color", ",", "size", ")", ":", "\n", "        ", "lineItem", "=", "QGraphicsLineItem", "(", "QLineF", "(", "prev_pt", ",", "curr_pt", ")", ")", "\n", "lineItem", ".", "setPen", "(", "QPen", "(", "color", ",", "size", ",", "Qt", ".", "SolidLine", ")", ")", "# rect", "\n", "self", ".", "addItem", "(", "lineItem", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.ui.mouse_event.GraphicsScene.erase_prev_pt": [[104, 106], ["None"], "methods", ["None"], ["", "def", "erase_prev_pt", "(", "self", ")", ":", "\n", "        ", "self", ".", "prev_pt", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.ui.mouse_event.GraphicsScene.reset_items": [[107, 111], ["range", "len", "mouse_event.GraphicsScene.removeItem", "mouse_event.GraphicsScene.items", "mouse_event.GraphicsScene.items"], "methods", ["None"], ["", "def", "reset_items", "(", "self", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "items", "(", ")", ")", ")", ":", "\n", "            ", "item", "=", "self", ".", "items", "(", ")", "[", "0", "]", "\n", "self", ".", "removeItem", "(", "item", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.ui.mouse_event.GraphicsScene.undo": [[112, 130], ["len", "mouse_event.GraphicsScene.items", "len", "range", "range", "mouse_event.GraphicsScene.items", "mouse_event.GraphicsScene.removeItem", "mouse_event.GraphicsScene.removeItem", "mouse_event.GraphicsScene.items", "mouse_event.GraphicsScene.mask_points[].pop", "mouse_event.GraphicsScene.size_points[].pop", "mouse_event.GraphicsScene.history.pop", "len", "mouse_event.GraphicsScene.items", "mouse_event.GraphicsScene.mask_points[].pop", "mouse_event.GraphicsScene.size_points[].pop", "mouse_event.GraphicsScene.history.pop", "mouse_event.GraphicsScene.items"], "methods", ["None"], ["", "", "def", "undo", "(", "self", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "items", "(", ")", ")", ">", "1", ":", "\n", "            ", "if", "len", "(", "self", ".", "items", "(", ")", ")", ">=", "9", ":", "\n", "                ", "for", "i", "in", "range", "(", "8", ")", ":", "\n", "                    ", "item", "=", "self", ".", "items", "(", ")", "[", "0", "]", "\n", "self", ".", "removeItem", "(", "item", ")", "\n", "if", "self", ".", "history", "[", "-", "1", "]", "==", "self", ".", "mode", ":", "\n", "                        ", "self", ".", "mask_points", "[", "self", ".", "mode", "]", ".", "pop", "(", ")", "\n", "self", ".", "size_points", "[", "self", ".", "mode", "]", ".", "pop", "(", ")", "\n", "self", ".", "history", ".", "pop", "(", ")", "\n", "", "", "", "else", ":", "\n", "                ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "items", "(", ")", ")", "-", "1", ")", ":", "\n", "                    ", "item", "=", "self", ".", "items", "(", ")", "[", "0", "]", "\n", "self", ".", "removeItem", "(", "item", ")", "\n", "if", "self", ".", "history", "[", "-", "1", "]", "==", "self", ".", "mode", ":", "\n", "                        ", "self", ".", "mask_points", "[", "self", ".", "mode", "]", ".", "pop", "(", ")", "\n", "self", ".", "size_points", "[", "self", ".", "mode", "]", ".", "pop", "(", ")", "\n", "self", ".", "history", ".", "pop", "(", ")", "\n", "", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.ui.ui.Ui_Form.setupUi": [[9, 295], ["Form.setObjectName", "Form.resize", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_2.setGeometry", "ui.Ui_Form.pushButton_2.setObjectName", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_6.setGeometry", "ui.Ui_Form.pushButton_6.setObjectName", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_0.setGeometry", "ui.Ui_Form.pushButton_0.setObjectName", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_1.setGeometry", "ui.Ui_Form.pushButton_1.setObjectName", "PyQt5.QtWidgets.QLabel", "ui.Ui_Form.label_heading_1.setText", "ui.Ui_Form.label_heading_1.setObjectName", "ui.Ui_Form.label_heading_1.setGeometry", "PyQt5.QtWidgets.QLineEdit", "ui.Ui_Form.message_box_1.setGeometry", "ui.Ui_Form.message_box_1.setObjectName", "ui.Ui_Form.message_box_1.setAlignment", "PyQt5.QtWidgets.QLabel", "ui.Ui_Form.label_heading_2.setText", "ui.Ui_Form.label_heading_2.setObjectName", "ui.Ui_Form.label_heading_2.setGeometry", "PyQt5.QtWidgets.QLineEdit", "ui.Ui_Form.message_box_2.setGeometry", "ui.Ui_Form.message_box_2.setObjectName", "ui.Ui_Form.message_box_2.setAlignment", "PyQt5.QtWidgets.QLabel", "ui.Ui_Form.title_icon.setGeometry", "ui.Ui_Form.title_icon.setPixmap", "PyQt5.QtWidgets.QLabel", "ui.Ui_Form.palette_icon.setGeometry", "ui.Ui_Form.palette_icon.setPixmap", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_8.setGeometry", "ui.Ui_Form.pushButton_8.setObjectName", "ui.Ui_Form.pushButton_8.setStyleSheet", "ui.Ui_Form.pushButton_8.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_9.setGeometry", "ui.Ui_Form.pushButton_9.setObjectName", "ui.Ui_Form.pushButton_9.setStyleSheet", "ui.Ui_Form.pushButton_9.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_10.setGeometry", "ui.Ui_Form.pushButton_10.setObjectName", "ui.Ui_Form.pushButton_10.setStyleSheet", "ui.Ui_Form.pushButton_10.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_11.setGeometry", "ui.Ui_Form.pushButton_11.setObjectName", "ui.Ui_Form.pushButton_11.setStyleSheet", "ui.Ui_Form.pushButton_11.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_12.setGeometry", "ui.Ui_Form.pushButton_12.setObjectName", "ui.Ui_Form.pushButton_12.setStyleSheet", "ui.Ui_Form.pushButton_12.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_13.setGeometry", "ui.Ui_Form.pushButton_13.setObjectName", "ui.Ui_Form.pushButton_13.setStyleSheet", "ui.Ui_Form.pushButton_13.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_14.setGeometry", "ui.Ui_Form.pushButton_14.setObjectName", "ui.Ui_Form.pushButton_14.setStyleSheet", "ui.Ui_Form.pushButton_14.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_15.setGeometry", "ui.Ui_Form.pushButton_15.setObjectName", "ui.Ui_Form.pushButton_15.setStyleSheet", "ui.Ui_Form.pushButton_15.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_16.setGeometry", "ui.Ui_Form.pushButton_16.setObjectName", "ui.Ui_Form.pushButton_16.setStyleSheet", "ui.Ui_Form.pushButton_16.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_17.setGeometry", "ui.Ui_Form.pushButton_17.setObjectName", "ui.Ui_Form.pushButton_17.setStyleSheet", "ui.Ui_Form.pushButton_17.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_18.setGeometry", "ui.Ui_Form.pushButton_18.setObjectName", "ui.Ui_Form.pushButton_18.setStyleSheet", "ui.Ui_Form.pushButton_18.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_19.setGeometry", "ui.Ui_Form.pushButton_19.setObjectName", "ui.Ui_Form.pushButton_19.setStyleSheet", "ui.Ui_Form.pushButton_19.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_20.setGeometry", "ui.Ui_Form.pushButton_20.setObjectName", "ui.Ui_Form.pushButton_20.setStyleSheet", "ui.Ui_Form.pushButton_20.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_21.setGeometry", "ui.Ui_Form.pushButton_21.setObjectName", "ui.Ui_Form.pushButton_21.setStyleSheet", "ui.Ui_Form.pushButton_21.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_22.setGeometry", "ui.Ui_Form.pushButton_22.setObjectName", "ui.Ui_Form.pushButton_22.setStyleSheet", "ui.Ui_Form.pushButton_22.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_23.setGeometry", "ui.Ui_Form.pushButton_23.setObjectName", "ui.Ui_Form.pushButton_23.setStyleSheet", "ui.Ui_Form.pushButton_23.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_24.setGeometry", "ui.Ui_Form.pushButton_24.setObjectName", "ui.Ui_Form.pushButton_24.setStyleSheet", "ui.Ui_Form.pushButton_24.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_25.setGeometry", "ui.Ui_Form.pushButton_25.setObjectName", "ui.Ui_Form.pushButton_25.setStyleSheet", "ui.Ui_Form.pushButton_25.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_26.setGeometry", "ui.Ui_Form.pushButton_26.setObjectName", "ui.Ui_Form.pushButton_26.setStyleSheet", "ui.Ui_Form.pushButton_26.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_27.setGeometry", "ui.Ui_Form.pushButton_27.setObjectName", "ui.Ui_Form.pushButton_27.setStyleSheet", "ui.Ui_Form.pushButton_27.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_28.setGeometry", "ui.Ui_Form.pushButton_28.setObjectName", "ui.Ui_Form.pushButton_28.setStyleSheet", "ui.Ui_Form.pushButton_28.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_29.setGeometry", "ui.Ui_Form.pushButton_29.setObjectName", "ui.Ui_Form.pushButton_29.setStyleSheet", "ui.Ui_Form.pushButton_29.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_30.setGeometry", "ui.Ui_Form.pushButton_30.setObjectName", "ui.Ui_Form.pushButton_30.setStyleSheet", "ui.Ui_Form.pushButton_30.setIcon", "PyQt5.QtWidgets.QPushButton", "ui.Ui_Form.pushButton_31.setGeometry", "ui.Ui_Form.pushButton_31.setObjectName", "ui.Ui_Form.pushButton_31.setStyleSheet", "ui.Ui_Form.pushButton_31.setIcon", "PyQt5.QtWidgets.QGraphicsView", "ui.Ui_Form.graphicsView.setGeometry", "ui.Ui_Form.graphicsView.setObjectName", "PyQt5.QtWidgets.QGraphicsView", "ui.Ui_Form.graphicsView_2.setGeometry", "ui.Ui_Form.graphicsView_2.setObjectName", "PyQt5.QtWidgets.QGraphicsView", "ui.Ui_Form.graphicsView_3.setGeometry", "ui.Ui_Form.graphicsView_3.setObjectName", "ui.Ui_Form.retranslateUi", "ui.Ui_Form.pushButton_2.clicked.connect", "ui.Ui_Form.pushButton_6.clicked.connect", "ui.Ui_Form.pushButton_8.clicked.connect", "ui.Ui_Form.pushButton_9.clicked.connect", "ui.Ui_Form.pushButton_10.clicked.connect", "ui.Ui_Form.pushButton_11.clicked.connect", "ui.Ui_Form.pushButton_12.clicked.connect", "ui.Ui_Form.pushButton_13.clicked.connect", "ui.Ui_Form.pushButton_14.clicked.connect", "ui.Ui_Form.pushButton_15.clicked.connect", "ui.Ui_Form.pushButton_16.clicked.connect", "ui.Ui_Form.pushButton_17.clicked.connect", "ui.Ui_Form.pushButton_18.clicked.connect", "ui.Ui_Form.pushButton_19.clicked.connect", "ui.Ui_Form.pushButton_20.clicked.connect", "ui.Ui_Form.pushButton_21.clicked.connect", "ui.Ui_Form.pushButton_22.clicked.connect", "ui.Ui_Form.pushButton_23.clicked.connect", "ui.Ui_Form.pushButton_24.clicked.connect", "ui.Ui_Form.pushButton_25.clicked.connect", "ui.Ui_Form.pushButton_26.clicked.connect", "ui.Ui_Form.pushButton_27.clicked.connect", "ui.Ui_Form.pushButton_28.clicked.connect", "ui.Ui_Form.pushButton_29.clicked.connect", "ui.Ui_Form.pushButton_30.clicked.connect", "ui.Ui_Form.pushButton_31.clicked.connect", "ui.Ui_Form.pushButton_0.clicked.connect", "ui.Ui_Form.pushButton_1.clicked.connect", "PyQt5.QtCore.QMetaObject.connectSlotsByName", "PyQt5.QtCore.QRect", "PyQt5.QtCore.QRect", "PyQt5.QtCore.QRect", "PyQt5.QtCore.QRect", "PyQt5.QtCore.QRect", "PyQt5.QtCore.QRect", "PyQt5.QtCore.QRect", "PyQt5.QtCore.QRect", "PyQt5.QtCore.QRect", "PyQt5.QtGui.QPixmap().scaledToWidth", "PyQt5.QtCore.QRect", "PyQt5.QtGui.QPixmap().scaledToWidth", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "QIcon", "PyQt5.QtCore.QRect", "PyQt5.QtCore.QRect", "PyQt5.QtCore.QRect", "PyQt5.QtGui.QPixmap", "PyQt5.QtGui.QPixmap"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.ui.ui.Ui_Form.retranslateUi"], ["    ", "def", "setupUi", "(", "self", ",", "Form", ")", ":", "\n", "        ", "Form", ".", "setObjectName", "(", "\"Form\"", ")", "\n", "Form", ".", "resize", "(", "1250", ",", "670", ")", "\n", "\n", "self", ".", "pushButton_2", "=", "QtWidgets", ".", "QPushButton", "(", "Form", ")", "\n", "self", ".", "pushButton_2", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "20", ",", "60", ",", "97", ",", "27", ")", ")", "\n", "self", ".", "pushButton_2", ".", "setObjectName", "(", "\"pushButton_2\"", ")", "\n", "\n", "self", ".", "pushButton_6", "=", "QtWidgets", ".", "QPushButton", "(", "Form", ")", "\n", "self", ".", "pushButton_6", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "20", ",", "100", ",", "97", ",", "27", ")", ")", "\n", "self", ".", "pushButton_6", ".", "setObjectName", "(", "\"pushButton_6\"", ")", "\n", "\n", "# Generate Parsing", "\n", "self", ".", "pushButton_0", "=", "QtWidgets", ".", "QPushButton", "(", "Form", ")", "\n", "self", ".", "pushButton_0", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "126", ",", "60", ",", "150", ",", "27", ")", ")", "\n", "self", ".", "pushButton_0", ".", "setObjectName", "(", "\"pushButton_0\"", ")", "\n", "\n", "# Generate Human", "\n", "self", ".", "pushButton_1", "=", "QtWidgets", ".", "QPushButton", "(", "Form", ")", "\n", "self", ".", "pushButton_1", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "126", ",", "100", ",", "150", ",", "27", ")", ")", "\n", "self", ".", "pushButton_1", ".", "setObjectName", "(", "\"pushButton_1\"", ")", "\n", "\n", "# shape text box", "\n", "self", ".", "label_heading_1", "=", "QtWidgets", ".", "QLabel", "(", "Form", ")", "\n", "self", ".", "label_heading_1", ".", "setText", "(", "'Describe the shape.'", ")", "\n", "self", ".", "label_heading_1", ".", "setObjectName", "(", "\"label_heading_1\"", ")", "\n", "self", ".", "label_heading_1", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "320", ",", "20", ",", "200", ",", "20", ")", ")", "\n", "\n", "self", ".", "message_box_1", "=", "QtWidgets", ".", "QLineEdit", "(", "Form", ")", "\n", "self", ".", "message_box_1", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "320", ",", "50", ",", "256", ",", "80", ")", ")", "\n", "self", ".", "message_box_1", ".", "setObjectName", "(", "\"message_box_1\"", ")", "\n", "self", ".", "message_box_1", ".", "setAlignment", "(", "Qt", ".", "AlignTop", ")", "\n", "\n", "# texture text box", "\n", "self", ".", "label_heading_2", "=", "QtWidgets", ".", "QLabel", "(", "Form", ")", "\n", "self", ".", "label_heading_2", ".", "setText", "(", "'Describe the textures.'", ")", "\n", "self", ".", "label_heading_2", ".", "setObjectName", "(", "\"label_heading_2\"", ")", "\n", "self", ".", "label_heading_2", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "620", ",", "20", ",", "200", ",", "20", ")", ")", "\n", "\n", "self", ".", "message_box_2", "=", "QtWidgets", ".", "QLineEdit", "(", "Form", ")", "\n", "self", ".", "message_box_2", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "620", ",", "50", ",", "256", ",", "80", ")", ")", "\n", "self", ".", "message_box_2", ".", "setObjectName", "(", "\"message_box_2\"", ")", "\n", "self", ".", "message_box_2", ".", "setAlignment", "(", "Qt", ".", "AlignTop", ")", "\n", "\n", "# title icon", "\n", "self", ".", "title_icon", "=", "QtWidgets", ".", "QLabel", "(", "Form", ")", "\n", "self", ".", "title_icon", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "30", ",", "10", ",", "200", ",", "50", ")", ")", "\n", "self", ".", "title_icon", ".", "setPixmap", "(", "\n", "QtGui", ".", "QPixmap", "(", "'./ui/icons/icon_title.png'", ")", ".", "scaledToWidth", "(", "200", ")", ")", "\n", "\n", "# palette icon", "\n", "self", ".", "palette_icon", "=", "QtWidgets", ".", "QLabel", "(", "Form", ")", "\n", "self", ".", "palette_icon", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "950", ",", "10", ",", "256", ",", "128", ")", ")", "\n", "self", ".", "palette_icon", ".", "setPixmap", "(", "\n", "QtGui", ".", "QPixmap", "(", "'./ui/icons/icon_palette.png'", ")", ".", "scaledToWidth", "(", "256", ")", ")", "\n", "\n", "# top", "\n", "self", ".", "pushButton_8", "=", "QtWidgets", ".", "QPushButton", "(", "'   top'", ",", "Form", ")", "\n", "self", ".", "pushButton_8", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "940", ",", "120", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_8", ".", "setObjectName", "(", "\"pushButton_8\"", ")", "\n", "self", ".", "pushButton_8", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_8", ".", "setIcon", "(", "QIcon", "(", "'./ui/color_blocks/class_top.png'", ")", ")", "\n", "# skin", "\n", "self", ".", "pushButton_9", "=", "QtWidgets", ".", "QPushButton", "(", "'   skin'", ",", "Form", ")", "\n", "self", ".", "pushButton_9", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "940", ",", "165", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_9", ".", "setObjectName", "(", "\"pushButton_9\"", ")", "\n", "self", ".", "pushButton_9", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_9", ".", "setIcon", "(", "QIcon", "(", "'./ui/color_blocks/class_skin.png'", ")", ")", "\n", "# outer", "\n", "self", ".", "pushButton_10", "=", "QtWidgets", ".", "QPushButton", "(", "'   outer'", ",", "Form", ")", "\n", "self", ".", "pushButton_10", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "940", ",", "210", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_10", ".", "setObjectName", "(", "\"pushButton_10\"", ")", "\n", "self", ".", "pushButton_10", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_10", ".", "setIcon", "(", "QIcon", "(", "'./ui/color_blocks/class_outer.png'", ")", ")", "\n", "# face", "\n", "self", ".", "pushButton_11", "=", "QtWidgets", ".", "QPushButton", "(", "'   face'", ",", "Form", ")", "\n", "self", ".", "pushButton_11", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "940", ",", "255", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_11", ".", "setObjectName", "(", "\"pushButton_11\"", ")", "\n", "self", ".", "pushButton_11", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_11", ".", "setIcon", "(", "QIcon", "(", "'./ui/color_blocks/class_face.png'", ")", ")", "\n", "# skirt", "\n", "self", ".", "pushButton_12", "=", "QtWidgets", ".", "QPushButton", "(", "'   skirt'", ",", "Form", ")", "\n", "self", ".", "pushButton_12", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "940", ",", "300", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_12", ".", "setObjectName", "(", "\"pushButton_12\"", ")", "\n", "self", ".", "pushButton_12", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_12", ".", "setIcon", "(", "QIcon", "(", "'./ui/color_blocks/class_skirt.png'", ")", ")", "\n", "# hair", "\n", "self", ".", "pushButton_13", "=", "QtWidgets", ".", "QPushButton", "(", "'   hair'", ",", "Form", ")", "\n", "self", ".", "pushButton_13", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "940", ",", "345", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_13", ".", "setObjectName", "(", "\"pushButton_13\"", ")", "\n", "self", ".", "pushButton_13", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_13", ".", "setIcon", "(", "QIcon", "(", "'./ui/color_blocks/class_hair.png'", ")", ")", "\n", "# dress", "\n", "self", ".", "pushButton_14", "=", "QtWidgets", ".", "QPushButton", "(", "'   dress'", ",", "Form", ")", "\n", "self", ".", "pushButton_14", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "940", ",", "390", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_14", ".", "setObjectName", "(", "\"pushButton_14\"", ")", "\n", "self", ".", "pushButton_14", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_14", ".", "setIcon", "(", "QIcon", "(", "'./ui/color_blocks/class_dress.png'", ")", ")", "\n", "# headwear", "\n", "self", ".", "pushButton_15", "=", "QtWidgets", ".", "QPushButton", "(", "'   headwear'", ",", "Form", ")", "\n", "self", ".", "pushButton_15", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "940", ",", "435", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_15", ".", "setObjectName", "(", "\"pushButton_15\"", ")", "\n", "self", ".", "pushButton_15", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_15", ".", "setIcon", "(", "\n", "QIcon", "(", "'./ui/color_blocks/class_headwear.png'", ")", ")", "\n", "# pants", "\n", "self", ".", "pushButton_16", "=", "QtWidgets", ".", "QPushButton", "(", "'   pants'", ",", "Form", ")", "\n", "self", ".", "pushButton_16", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "940", ",", "480", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_16", ".", "setObjectName", "(", "\"pushButton_16\"", ")", "\n", "self", ".", "pushButton_16", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_16", ".", "setIcon", "(", "QIcon", "(", "'./ui/color_blocks/class_pants.png'", ")", ")", "\n", "# eyeglasses", "\n", "self", ".", "pushButton_17", "=", "QtWidgets", ".", "QPushButton", "(", "'   eyeglass'", ",", "Form", ")", "\n", "self", ".", "pushButton_17", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "940", ",", "525", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_17", ".", "setObjectName", "(", "\"pushButton_17\"", ")", "\n", "self", ".", "pushButton_17", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_17", ".", "setIcon", "(", "\n", "QIcon", "(", "'./ui/color_blocks/class_eyeglass.png'", ")", ")", "\n", "# rompers", "\n", "self", ".", "pushButton_18", "=", "QtWidgets", ".", "QPushButton", "(", "'   rompers'", ",", "Form", ")", "\n", "self", ".", "pushButton_18", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "940", ",", "570", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_18", ".", "setObjectName", "(", "\"pushButton_18\"", ")", "\n", "self", ".", "pushButton_18", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_18", ".", "setIcon", "(", "\n", "QIcon", "(", "'./ui/color_blocks/class_rompers.png'", ")", ")", "\n", "# footwear", "\n", "self", ".", "pushButton_19", "=", "QtWidgets", ".", "QPushButton", "(", "'   footwear'", ",", "Form", ")", "\n", "self", ".", "pushButton_19", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "940", ",", "615", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_19", ".", "setObjectName", "(", "\"pushButton_19\"", ")", "\n", "self", ".", "pushButton_19", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_19", ".", "setIcon", "(", "\n", "QIcon", "(", "'./ui/color_blocks/class_footwear.png'", ")", ")", "\n", "\n", "# leggings", "\n", "self", ".", "pushButton_20", "=", "QtWidgets", ".", "QPushButton", "(", "'   leggings'", ",", "Form", ")", "\n", "self", ".", "pushButton_20", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "1100", ",", "120", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_20", ".", "setObjectName", "(", "\"pushButton_10\"", ")", "\n", "self", ".", "pushButton_20", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_20", ".", "setIcon", "(", "\n", "QIcon", "(", "'./ui/color_blocks/class_leggings.png'", ")", ")", "\n", "\n", "# ring", "\n", "self", ".", "pushButton_21", "=", "QtWidgets", ".", "QPushButton", "(", "'   ring'", ",", "Form", ")", "\n", "self", ".", "pushButton_21", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "1100", ",", "165", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_21", ".", "setObjectName", "(", "\"pushButton_2`0`\"", ")", "\n", "self", ".", "pushButton_21", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_21", ".", "setIcon", "(", "QIcon", "(", "'./ui/color_blocks/class_ring.png'", ")", ")", "\n", "\n", "# belt", "\n", "self", ".", "pushButton_22", "=", "QtWidgets", ".", "QPushButton", "(", "'   belt'", ",", "Form", ")", "\n", "self", ".", "pushButton_22", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "1100", ",", "210", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_22", ".", "setObjectName", "(", "\"pushButton_2`0`\"", ")", "\n", "self", ".", "pushButton_22", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_22", ".", "setIcon", "(", "QIcon", "(", "'./ui/color_blocks/class_belt.png'", ")", ")", "\n", "\n", "# neckwear", "\n", "self", ".", "pushButton_23", "=", "QtWidgets", ".", "QPushButton", "(", "'   neckwear'", ",", "Form", ")", "\n", "self", ".", "pushButton_23", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "1100", ",", "255", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_23", ".", "setObjectName", "(", "\"pushButton_2`0`\"", ")", "\n", "self", ".", "pushButton_23", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_23", ".", "setIcon", "(", "\n", "QIcon", "(", "'./ui/color_blocks/class_neckwear.png'", ")", ")", "\n", "\n", "# wrist", "\n", "self", ".", "pushButton_24", "=", "QtWidgets", ".", "QPushButton", "(", "'   wrist'", ",", "Form", ")", "\n", "self", ".", "pushButton_24", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "1100", ",", "300", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_24", ".", "setObjectName", "(", "\"pushButton_2`0`\"", ")", "\n", "self", ".", "pushButton_24", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_24", ".", "setIcon", "(", "QIcon", "(", "'./ui/color_blocks/class_wrist.png'", ")", ")", "\n", "\n", "# socks", "\n", "self", ".", "pushButton_25", "=", "QtWidgets", ".", "QPushButton", "(", "'   socks'", ",", "Form", ")", "\n", "self", ".", "pushButton_25", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "1100", ",", "345", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_25", ".", "setObjectName", "(", "\"pushButton_2`0`\"", ")", "\n", "self", ".", "pushButton_25", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_25", ".", "setIcon", "(", "QIcon", "(", "'./ui/color_blocks/class_socks.png'", ")", ")", "\n", "\n", "# tie", "\n", "self", ".", "pushButton_26", "=", "QtWidgets", ".", "QPushButton", "(", "'   tie'", ",", "Form", ")", "\n", "self", ".", "pushButton_26", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "1100", ",", "390", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_26", ".", "setObjectName", "(", "\"pushButton_2`0`\"", ")", "\n", "self", ".", "pushButton_26", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_26", ".", "setIcon", "(", "QIcon", "(", "'./ui/color_blocks/class_tie.png'", ")", ")", "\n", "\n", "# earstuds", "\n", "self", ".", "pushButton_27", "=", "QtWidgets", ".", "QPushButton", "(", "'   necklace'", ",", "Form", ")", "\n", "self", ".", "pushButton_27", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "1100", ",", "435", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_27", ".", "setObjectName", "(", "\"pushButton_2`0`\"", ")", "\n", "self", ".", "pushButton_27", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_27", ".", "setIcon", "(", "\n", "QIcon", "(", "'./ui/color_blocks/class_necklace.png'", ")", ")", "\n", "\n", "# necklace", "\n", "self", ".", "pushButton_28", "=", "QtWidgets", ".", "QPushButton", "(", "'   earstuds'", ",", "Form", ")", "\n", "self", ".", "pushButton_28", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "1100", ",", "480", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_28", ".", "setObjectName", "(", "\"pushButton_2`0`\"", ")", "\n", "self", ".", "pushButton_28", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_28", ".", "setIcon", "(", "\n", "QIcon", "(", "'./ui/color_blocks/class_earstuds.png'", ")", ")", "\n", "\n", "# bag", "\n", "self", ".", "pushButton_29", "=", "QtWidgets", ".", "QPushButton", "(", "'   bag'", ",", "Form", ")", "\n", "self", ".", "pushButton_29", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "1100", ",", "525", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_29", ".", "setObjectName", "(", "\"pushButton_2`0`\"", ")", "\n", "self", ".", "pushButton_29", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_29", ".", "setIcon", "(", "QIcon", "(", "'./ui/color_blocks/class_bag.png'", ")", ")", "\n", "\n", "# glove", "\n", "self", ".", "pushButton_30", "=", "QtWidgets", ".", "QPushButton", "(", "'   glove'", ",", "Form", ")", "\n", "self", ".", "pushButton_30", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "1100", ",", "570", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_30", ".", "setObjectName", "(", "\"pushButton_2`0`\"", ")", "\n", "self", ".", "pushButton_30", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_30", ".", "setIcon", "(", "QIcon", "(", "'./ui/color_blocks/class_glove.png'", ")", ")", "\n", "\n", "# background", "\n", "self", ".", "pushButton_31", "=", "QtWidgets", ".", "QPushButton", "(", "'   background'", ",", "Form", ")", "\n", "self", ".", "pushButton_31", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "1100", ",", "615", ",", "120", ",", "27", ")", ")", "\n", "self", ".", "pushButton_31", ".", "setObjectName", "(", "\"pushButton_2`0`\"", ")", "\n", "self", ".", "pushButton_31", ".", "setStyleSheet", "(", "\n", "\"text-align: left; padding-left: 10px;\"", ")", "\n", "self", ".", "pushButton_31", ".", "setIcon", "(", "QIcon", "(", "'./ui/color_blocks/class_bg.png'", ")", ")", "\n", "\n", "self", ".", "graphicsView", "=", "QtWidgets", ".", "QGraphicsView", "(", "Form", ")", "\n", "self", ".", "graphicsView", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "20", ",", "140", ",", "256", ",", "512", ")", ")", "\n", "self", ".", "graphicsView", ".", "setObjectName", "(", "\"graphicsView\"", ")", "\n", "self", ".", "graphicsView_2", "=", "QtWidgets", ".", "QGraphicsView", "(", "Form", ")", "\n", "self", ".", "graphicsView_2", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "320", ",", "140", ",", "256", ",", "512", ")", ")", "\n", "self", ".", "graphicsView_2", ".", "setObjectName", "(", "\"graphicsView_2\"", ")", "\n", "self", ".", "graphicsView_3", "=", "QtWidgets", ".", "QGraphicsView", "(", "Form", ")", "\n", "self", ".", "graphicsView_3", ".", "setGeometry", "(", "QtCore", ".", "QRect", "(", "620", ",", "140", ",", "256", ",", "512", ")", ")", "\n", "self", ".", "graphicsView_3", ".", "setObjectName", "(", "\"graphicsView_3\"", ")", "\n", "\n", "self", ".", "retranslateUi", "(", "Form", ")", "\n", "self", ".", "pushButton_2", ".", "clicked", ".", "connect", "(", "Form", ".", "open_densepose", ")", "\n", "self", ".", "pushButton_6", ".", "clicked", ".", "connect", "(", "Form", ".", "save_img", ")", "\n", "self", ".", "pushButton_8", ".", "clicked", ".", "connect", "(", "Form", ".", "top_mode", ")", "\n", "self", ".", "pushButton_9", ".", "clicked", ".", "connect", "(", "Form", ".", "skin_mode", ")", "\n", "self", ".", "pushButton_10", ".", "clicked", ".", "connect", "(", "Form", ".", "outer_mode", ")", "\n", "self", ".", "pushButton_11", ".", "clicked", ".", "connect", "(", "Form", ".", "face_mode", ")", "\n", "self", ".", "pushButton_12", ".", "clicked", ".", "connect", "(", "Form", ".", "skirt_mode", ")", "\n", "self", ".", "pushButton_13", ".", "clicked", ".", "connect", "(", "Form", ".", "hair_mode", ")", "\n", "self", ".", "pushButton_14", ".", "clicked", ".", "connect", "(", "Form", ".", "dress_mode", ")", "\n", "self", ".", "pushButton_15", ".", "clicked", ".", "connect", "(", "Form", ".", "headwear_mode", ")", "\n", "self", ".", "pushButton_16", ".", "clicked", ".", "connect", "(", "Form", ".", "pants_mode", ")", "\n", "self", ".", "pushButton_17", ".", "clicked", ".", "connect", "(", "Form", ".", "eyeglass_mode", ")", "\n", "self", ".", "pushButton_18", ".", "clicked", ".", "connect", "(", "Form", ".", "rompers_mode", ")", "\n", "self", ".", "pushButton_19", ".", "clicked", ".", "connect", "(", "Form", ".", "footwear_mode", ")", "\n", "self", ".", "pushButton_20", ".", "clicked", ".", "connect", "(", "Form", ".", "leggings_mode", ")", "\n", "self", ".", "pushButton_21", ".", "clicked", ".", "connect", "(", "Form", ".", "ring_mode", ")", "\n", "self", ".", "pushButton_22", ".", "clicked", ".", "connect", "(", "Form", ".", "belt_mode", ")", "\n", "self", ".", "pushButton_23", ".", "clicked", ".", "connect", "(", "Form", ".", "neckwear_mode", ")", "\n", "self", ".", "pushButton_24", ".", "clicked", ".", "connect", "(", "Form", ".", "wrist_mode", ")", "\n", "self", ".", "pushButton_25", ".", "clicked", ".", "connect", "(", "Form", ".", "socks_mode", ")", "\n", "self", ".", "pushButton_26", ".", "clicked", ".", "connect", "(", "Form", ".", "tie_mode", ")", "\n", "self", ".", "pushButton_27", ".", "clicked", ".", "connect", "(", "Form", ".", "earstuds_mode", ")", "\n", "self", ".", "pushButton_28", ".", "clicked", ".", "connect", "(", "Form", ".", "necklace_mode", ")", "\n", "self", ".", "pushButton_29", ".", "clicked", ".", "connect", "(", "Form", ".", "bag_mode", ")", "\n", "self", ".", "pushButton_30", ".", "clicked", ".", "connect", "(", "Form", ".", "glove_mode", ")", "\n", "self", ".", "pushButton_31", ".", "clicked", ".", "connect", "(", "Form", ".", "background_mode", ")", "\n", "self", ".", "pushButton_0", ".", "clicked", ".", "connect", "(", "Form", ".", "generate_parsing", ")", "\n", "self", ".", "pushButton_1", ".", "clicked", ".", "connect", "(", "Form", ".", "generate_human", ")", "\n", "\n", "QtCore", ".", "QMetaObject", ".", "connectSlotsByName", "(", "Form", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.ui.ui.Ui_Form.retranslateUi": [[296, 304], ["Form.setWindowTitle", "ui.Ui_Form.pushButton_2.setText", "ui.Ui_Form.pushButton_6.setText", "ui.Ui_Form.pushButton_0.setText", "ui.Ui_Form.pushButton_1.setText", "_translate", "_translate", "_translate", "_translate", "_translate"], "methods", ["None"], ["", "def", "retranslateUi", "(", "self", ",", "Form", ")", ":", "\n", "        ", "_translate", "=", "QtCore", ".", "QCoreApplication", ".", "translate", "\n", "Form", ".", "setWindowTitle", "(", "_translate", "(", "\"Form\"", ",", "\"Text2Human\"", ")", ")", "\n", "self", ".", "pushButton_2", ".", "setText", "(", "_translate", "(", "\"Form\"", ",", "\"Load Pose\"", ")", ")", "\n", "self", ".", "pushButton_6", ".", "setText", "(", "_translate", "(", "\"Form\"", ",", "\"Save Image\"", ")", ")", "\n", "\n", "self", ".", "pushButton_0", ".", "setText", "(", "_translate", "(", "\"Form\"", ",", "\"Generate Parsing\"", ")", ")", "\n", "self", ".", "pushButton_1", ".", "setText", "(", "_translate", "(", "\"Form\"", ",", "\"Generate Human\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.NoneDict.__missing__": [[108, 110], ["None"], "methods", ["None"], ["def", "__missing__", "(", "self", ",", "key", ")", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.ordered_yaml": [[8, 31], ["Dumper.add_representer", "Loader.add_constructor", "dumper.represent_dict", "collections.OrderedDict", "data.items", "loader.construct_pairs"], "function", ["None"], ["def", "ordered_yaml", "(", ")", ":", "\n", "    ", "\"\"\"Support OrderedDict for yaml.\n\n    Returns:\n        yaml Loader and Dumper.\n    \"\"\"", "\n", "try", ":", "\n", "        ", "from", "yaml", "import", "CDumper", "as", "Dumper", "\n", "from", "yaml", "import", "CLoader", "as", "Loader", "\n", "", "except", "ImportError", ":", "\n", "        ", "from", "yaml", "import", "Dumper", ",", "Loader", "\n", "\n", "", "_mapping_tag", "=", "yaml", ".", "resolver", ".", "BaseResolver", ".", "DEFAULT_MAPPING_TAG", "\n", "\n", "def", "dict_representer", "(", "dumper", ",", "data", ")", ":", "\n", "        ", "return", "dumper", ".", "represent_dict", "(", "data", ".", "items", "(", ")", ")", "\n", "\n", "", "def", "dict_constructor", "(", "loader", ",", "node", ")", ":", "\n", "        ", "return", "OrderedDict", "(", "loader", ".", "construct_pairs", "(", "node", ")", ")", "\n", "\n", "", "Dumper", ".", "add_representer", "(", "OrderedDict", ",", "dict_representer", ")", "\n", "Loader", ".", "add_constructor", "(", "_mapping_tag", ",", "dict_constructor", ")", "\n", "return", "Loader", ",", "Dumper", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.parse": [[33, 82], ["yaml.load.get", "os.abspath", "open", "options.ordered_yaml", "yaml.load", "print", "print", "os.join", "os.join", "os.join", "os.join", "os.join", "os.join", "str"], "function", ["home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.ordered_yaml"], ["", "def", "parse", "(", "opt_path", ",", "is_train", "=", "True", ")", ":", "\n", "    ", "\"\"\"Parse option file.\n\n    Args:\n        opt_path (str): Option file path.\n        is_train (str): Indicate whether in training or not. Default: True.\n\n    Returns:\n        (dict): Options.\n    \"\"\"", "\n", "with", "open", "(", "opt_path", ",", "mode", "=", "'r'", ")", "as", "f", ":", "\n", "        ", "Loader", ",", "_", "=", "ordered_yaml", "(", ")", "\n", "opt", "=", "yaml", ".", "load", "(", "f", ",", "Loader", "=", "Loader", ")", "\n", "\n", "", "gpu_list", "=", "','", ".", "join", "(", "str", "(", "x", ")", "for", "x", "in", "opt", "[", "'gpu_ids'", "]", ")", "\n", "if", "opt", ".", "get", "(", "'set_CUDA_VISIBLE_DEVICES'", ",", "None", ")", ":", "\n", "        ", "os", ".", "environ", "[", "'CUDA_VISIBLE_DEVICES'", "]", "=", "gpu_list", "\n", "print", "(", "'export CUDA_VISIBLE_DEVICES='", "+", "gpu_list", ",", "flush", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "'gpu_list: '", ",", "gpu_list", ",", "flush", "=", "True", ")", "\n", "\n", "", "opt", "[", "'is_train'", "]", "=", "is_train", "\n", "\n", "# paths", "\n", "opt", "[", "'path'", "]", "=", "{", "}", "\n", "opt", "[", "'path'", "]", "[", "'root'", "]", "=", "osp", ".", "abspath", "(", "\n", "osp", ".", "join", "(", "__file__", ",", "osp", ".", "pardir", ",", "osp", ".", "pardir", ")", ")", "\n", "if", "is_train", ":", "\n", "        ", "experiments_root", "=", "osp", ".", "join", "(", "opt", "[", "'path'", "]", "[", "'root'", "]", ",", "'experiments'", ",", "\n", "opt", "[", "'name'", "]", ")", "\n", "opt", "[", "'path'", "]", "[", "'experiments_root'", "]", "=", "experiments_root", "\n", "opt", "[", "'path'", "]", "[", "'models'", "]", "=", "osp", ".", "join", "(", "experiments_root", ",", "'models'", ")", "\n", "opt", "[", "'path'", "]", "[", "'log'", "]", "=", "experiments_root", "\n", "opt", "[", "'path'", "]", "[", "'visualization'", "]", "=", "osp", ".", "join", "(", "experiments_root", ",", "\n", "'visualization'", ")", "\n", "\n", "# change some options for debug mode", "\n", "if", "'debug'", "in", "opt", "[", "'name'", "]", ":", "\n", "            ", "opt", "[", "'debug'", "]", "=", "True", "\n", "opt", "[", "'val_freq'", "]", "=", "1", "\n", "opt", "[", "'print_freq'", "]", "=", "1", "\n", "opt", "[", "'save_checkpoint_freq'", "]", "=", "1", "\n", "", "", "else", ":", "# test", "\n", "        ", "results_root", "=", "osp", ".", "join", "(", "opt", "[", "'path'", "]", "[", "'root'", "]", ",", "'results'", ",", "opt", "[", "'name'", "]", ")", "\n", "opt", "[", "'path'", "]", "[", "'results_root'", "]", "=", "results_root", "\n", "opt", "[", "'path'", "]", "[", "'log'", "]", "=", "results_root", "\n", "opt", "[", "'path'", "]", "[", "'visualization'", "]", "=", "osp", ".", "join", "(", "results_root", ",", "'visualization'", ")", "\n", "\n", "", "return", "opt", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.dict2str": [[84, 103], ["opt.items", "isinstance", "options.dict2str", "str"], "function", ["home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.dict2str"], ["", "def", "dict2str", "(", "opt", ",", "indent_level", "=", "1", ")", ":", "\n", "    ", "\"\"\"dict to string for printing options.\n\n    Args:\n        opt (dict): Option dict.\n        indent_level (int): Indent level. Default: 1.\n\n    Return:\n        (str): Option string for printing.\n    \"\"\"", "\n", "msg", "=", "''", "\n", "for", "k", ",", "v", "in", "opt", ".", "items", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "v", ",", "dict", ")", ":", "\n", "            ", "msg", "+=", "' '", "*", "(", "indent_level", "*", "2", ")", "+", "k", "+", "':[\\n'", "\n", "msg", "+=", "dict2str", "(", "v", ",", "indent_level", "+", "1", ")", "\n", "msg", "+=", "' '", "*", "(", "indent_level", "*", "2", ")", "+", "']\\n'", "\n", "", "else", ":", "\n", "            ", "msg", "+=", "' '", "*", "(", "indent_level", "*", "2", ")", "+", "k", "+", "': '", "+", "str", "(", "v", ")", "+", "'\\n'", "\n", "", "", "return", "msg", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.dict_to_nonedict": [[112, 130], ["isinstance", "dict", "opt.items", "options.NoneDict", "isinstance", "options.dict_to_nonedict", "options.dict_to_nonedict"], "function", ["home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.dict_to_nonedict", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.options.dict_to_nonedict"], ["", "", "def", "dict_to_nonedict", "(", "opt", ")", ":", "\n", "    ", "\"\"\"Convert to NoneDict, which returns None for missing keys.\n\n    Args:\n        opt (dict): Option dict.\n\n    Returns:\n        (dict): NoneDict for options.\n    \"\"\"", "\n", "if", "isinstance", "(", "opt", ",", "dict", ")", ":", "\n", "        ", "new_opt", "=", "dict", "(", ")", "\n", "for", "key", ",", "sub_opt", "in", "opt", ".", "items", "(", ")", ":", "\n", "            ", "new_opt", "[", "key", "]", "=", "dict_to_nonedict", "(", "sub_opt", ")", "\n", "", "return", "NoneDict", "(", "**", "new_opt", ")", "\n", "", "elif", "isinstance", "(", "opt", ",", "list", ")", ":", "\n", "        ", "return", "[", "dict_to_nonedict", "(", "sub_opt", ")", "for", "sub_opt", "in", "opt", "]", "\n", "", "else", ":", "\n", "        ", "return", "opt", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.language_utils.generate_shape_attributes": [[93, 278], ["sentence_transformers.SentenceTransformer", "user_shape_texts.split", "len", "enumerate", "sentence_transformers.SentenceTransformer.encode", "sentence_transformers.SentenceTransformer.encode", "sentence_transformers.util.dot_score", "torch.argmax().item", "sentence_transformers.SentenceTransformer.encode", "sentence_transformers.util.dot_score", "torch.argmax().item", "sentence_transformers.SentenceTransformer.encode", "sentence_transformers.util.dot_score", "torch.argmax().item", "sentence_transformers.SentenceTransformer.encode", "sentence_transformers.util.dot_score", "sentence_transformers.SentenceTransformer.encode", "sentence_transformers.util.dot_score", "torch.argmax().item", "sentence_transformers.SentenceTransformer.encode", "sentence_transformers.util.dot_score", "torch.argmax().item", "sentence_transformers.SentenceTransformer.encode", "sentence_transformers.util.dot_score", "torch.argmax().item", "sentence_transformers.SentenceTransformer.encode", "sentence_transformers.util.dot_score", "sentence_transformers.util.dot_score", "sentence_transformers.SentenceTransformer.encode", "sentence_transformers.util.dot_score", "torch.max().item", "sentence_transformers.SentenceTransformer.encode", "sentence_transformers.util.dot_score", "torch.max().item", "sentence_transformers.SentenceTransformer.encode", "sentence_transformers.util.dot_score", "torch.argmax().item", "sentence_transformers.SentenceTransformer.encode", "sentence_transformers.util.dot_score", "sentence_transformers.util.dot_score", "sentence_transformers.SentenceTransformer.encode", "sentence_transformers.util.dot_score", "torch.max().item", "sentence_transformers.SentenceTransformer.encode", "sentence_transformers.SentenceTransformer.encode", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.max", "torch.max", "torch.argmax", "torch.max"], "function", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.encode", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.encode", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.encode", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.encode", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.encode", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.encode", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.encode", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.encode", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.encode", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.encode", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.encode", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.encode", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.encode", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.encode", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.encode", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.encode"], ["def", "generate_shape_attributes", "(", "user_shape_texts", ")", ":", "\n", "    ", "model", "=", "SentenceTransformer", "(", "'all-MiniLM-L6-v2'", ")", "\n", "parsed_texts", "=", "user_shape_texts", ".", "split", "(", "','", ")", "\n", "\n", "text_num", "=", "len", "(", "parsed_texts", ")", "\n", "\n", "human_attr", "=", "[", "0", ",", "0", "]", "\n", "attr", "=", "[", "1", ",", "3", ",", "0", ",", "0", ",", "0", ",", "3", ",", "1", ",", "1", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", "\n", "\n", "changed", "=", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", "\n", "for", "text_id", ",", "text", "in", "enumerate", "(", "parsed_texts", ")", ":", "\n", "        ", "user_embeddings", "=", "model", ".", "encode", "(", "text", ")", "\n", "if", "(", "'man'", "in", "text", ")", "and", "(", "text_id", "==", "0", ")", ":", "\n", "            ", "human_attr", "[", "0", "]", "=", "0", "\n", "human_attr", "[", "1", "]", "=", "0", "\n", "\n", "", "if", "(", "'woman'", "in", "text", "or", "'lady'", "in", "text", ")", "and", "(", "text_id", "==", "0", ")", ":", "\n", "            ", "human_attr", "[", "0", "]", "=", "1", "\n", "human_attr", "[", "1", "]", "=", "2", "\n", "\n", "", "if", "(", "not", "changed", "[", "0", "]", ")", "and", "(", "text_id", "==", "1", ")", ":", "\n", "# upper length", "\n", "            ", "predefined_embeddings", "=", "model", ".", "encode", "(", "upper_length_text", ")", "\n", "similarities", "=", "util", ".", "dot_score", "(", "user_embeddings", ",", "\n", "predefined_embeddings", ")", "\n", "arg_idx", "=", "torch", ".", "argmax", "(", "similarities", ")", ".", "item", "(", ")", "\n", "attr", "[", "0", "]", "=", "upper_length_attr", "[", "upper_length_text", "[", "arg_idx", "]", "]", "\n", "changed", "[", "0", "]", "=", "1", "\n", "\n", "", "if", "(", "not", "changed", "[", "1", "]", ")", "and", "(", "(", "text_num", "==", "2", "and", "text_id", "==", "1", ")", "or", "\n", "(", "text_num", ">", "2", "and", "text_id", "==", "2", ")", ")", ":", "\n", "# lower length", "\n", "            ", "predefined_embeddings", "=", "model", ".", "encode", "(", "lower_length_text", ")", "\n", "similarities", "=", "util", ".", "dot_score", "(", "user_embeddings", ",", "\n", "predefined_embeddings", ")", "\n", "arg_idx", "=", "torch", ".", "argmax", "(", "similarities", ")", ".", "item", "(", ")", "\n", "attr", "[", "1", "]", "=", "lower_length_attr", "[", "lower_length_text", "[", "arg_idx", "]", "]", "\n", "changed", "[", "1", "]", "=", "1", "\n", "\n", "", "if", "(", "not", "changed", "[", "2", "]", ")", "and", "(", "text_id", ">", "2", ")", ":", "\n", "# socks length", "\n", "            ", "predefined_embeddings", "=", "model", ".", "encode", "(", "socks_length_text", ")", "\n", "similarities", "=", "util", ".", "dot_score", "(", "user_embeddings", ",", "\n", "predefined_embeddings", ")", "\n", "arg_idx", "=", "torch", ".", "argmax", "(", "similarities", ")", ".", "item", "(", ")", "\n", "if", "similarities", "[", "0", "]", "[", "arg_idx", "]", ">", "0.7", ":", "\n", "                ", "attr", "[", "2", "]", "=", "arg_idx", "+", "1", "\n", "changed", "[", "2", "]", "=", "1", "\n", "\n", "", "", "if", "(", "not", "changed", "[", "3", "]", ")", "and", "(", "text_id", ">", "2", ")", ":", "\n", "# hat", "\n", "            ", "predefined_embeddings", "=", "model", ".", "encode", "(", "hat_text", ")", "\n", "similarities", "=", "util", ".", "dot_score", "(", "user_embeddings", ",", "\n", "predefined_embeddings", ")", "\n", "if", "similarities", "[", "0", "]", "[", "0", "]", ">", "0.7", ":", "\n", "                ", "attr", "[", "3", "]", "=", "1", "\n", "changed", "[", "3", "]", "=", "1", "\n", "\n", "", "", "if", "(", "not", "changed", "[", "4", "]", ")", "and", "(", "text_id", ">", "2", ")", ":", "\n", "# glasses", "\n", "            ", "predefined_embeddings", "=", "model", ".", "encode", "(", "eyeglasses_text", ")", "\n", "similarities", "=", "util", ".", "dot_score", "(", "user_embeddings", ",", "\n", "predefined_embeddings", ")", "\n", "arg_idx", "=", "torch", ".", "argmax", "(", "similarities", ")", ".", "item", "(", ")", "\n", "if", "similarities", "[", "0", "]", "[", "arg_idx", "]", ">", "0.7", ":", "\n", "                ", "attr", "[", "4", "]", "=", "arg_idx", "+", "1", "\n", "changed", "[", "4", "]", "=", "1", "\n", "\n", "", "", "if", "(", "not", "changed", "[", "5", "]", ")", "and", "(", "text_id", ">", "2", ")", ":", "\n", "# belt", "\n", "            ", "predefined_embeddings", "=", "model", ".", "encode", "(", "belt_text", ")", "\n", "similarities", "=", "util", ".", "dot_score", "(", "user_embeddings", ",", "\n", "predefined_embeddings", ")", "\n", "arg_idx", "=", "torch", ".", "argmax", "(", "similarities", ")", ".", "item", "(", ")", "\n", "if", "similarities", "[", "0", "]", "[", "arg_idx", "]", ">", "0.7", ":", "\n", "                ", "attr", "[", "5", "]", "=", "arg_idx", "+", "1", "\n", "changed", "[", "5", "]", "=", "1", "\n", "\n", "", "", "if", "(", "not", "changed", "[", "6", "]", ")", "and", "(", "text_id", "==", "3", ")", ":", "\n", "# outer coverage", "\n", "            ", "predefined_embeddings", "=", "model", ".", "encode", "(", "outer_shape_text", ")", "\n", "similarities", "=", "util", ".", "dot_score", "(", "user_embeddings", ",", "\n", "predefined_embeddings", ")", "\n", "arg_idx", "=", "torch", ".", "argmax", "(", "similarities", ")", ".", "item", "(", ")", "\n", "if", "similarities", "[", "0", "]", "[", "arg_idx", "]", ">", "0.7", ":", "\n", "                ", "attr", "[", "6", "]", "=", "arg_idx", "\n", "changed", "[", "6", "]", "=", "1", "\n", "\n", "", "", "if", "(", "not", "changed", "[", "10", "]", ")", "and", "(", "text_num", "==", "2", "and", "text_id", "==", "1", ")", ":", "\n", "# dress_types", "\n", "            ", "predefined_embeddings", "=", "model", ".", "encode", "(", "dress_types", ")", "\n", "similarities", "=", "util", ".", "dot_score", "(", "user_embeddings", ",", "\n", "predefined_embeddings", ")", "\n", "similarity_skirt", "=", "util", ".", "dot_score", "(", "user_embeddings", ",", "\n", "model", ".", "encode", "(", "skirt_types", ")", ")", "\n", "if", "similarities", "[", "0", "]", "[", "0", "]", ">", "0.5", "and", "similarities", "[", "0", "]", "[", "\n", "0", "]", ">", "similarity_skirt", "[", "0", "]", "[", "0", "]", ":", "\n", "                ", "attr", "[", "10", "]", "=", "1", "\n", "attr", "[", "7", "]", "=", "0", "\n", "attr", "[", "8", "]", "=", "0", "\n", "attr", "[", "9", "]", "=", "0", "\n", "attr", "[", "11", "]", "=", "0", "\n", "attr", "[", "12", "]", "=", "0", "\n", "\n", "changed", "[", "0", "]", "=", "1", "\n", "changed", "[", "10", "]", "=", "1", "\n", "changed", "[", "7", "]", "=", "1", "\n", "changed", "[", "8", "]", "=", "1", "\n", "changed", "[", "9", "]", "=", "1", "\n", "changed", "[", "11", "]", "=", "1", "\n", "changed", "[", "12", "]", "=", "1", "\n", "\n", "", "", "if", "(", "not", "changed", "[", "12", "]", ")", "and", "(", "text_num", "==", "2", "and", "text_id", "==", "1", ")", ":", "\n", "# rompers_types", "\n", "            ", "predefined_embeddings", "=", "model", ".", "encode", "(", "rompers_types", ")", "\n", "similarities", "=", "util", ".", "dot_score", "(", "user_embeddings", ",", "\n", "predefined_embeddings", ")", "\n", "max_similarity", "=", "torch", ".", "max", "(", "similarities", ")", ".", "item", "(", ")", "\n", "if", "max_similarity", ">", "0.6", ":", "\n", "                ", "attr", "[", "12", "]", "=", "1", "\n", "attr", "[", "7", "]", "=", "0", "\n", "attr", "[", "8", "]", "=", "0", "\n", "attr", "[", "9", "]", "=", "0", "\n", "attr", "[", "10", "]", "=", "0", "\n", "attr", "[", "11", "]", "=", "0", "\n", "\n", "changed", "[", "12", "]", "=", "1", "\n", "changed", "[", "7", "]", "=", "1", "\n", "changed", "[", "8", "]", "=", "1", "\n", "changed", "[", "9", "]", "=", "1", "\n", "changed", "[", "10", "]", "=", "1", "\n", "changed", "[", "11", "]", "=", "1", "\n", "\n", "", "", "if", "(", "not", "changed", "[", "7", "]", ")", "and", "(", "text_num", ">", "2", "and", "text_id", "==", "1", ")", ":", "\n", "# upper_types", "\n", "            ", "predefined_embeddings", "=", "model", ".", "encode", "(", "upper_types", ")", "\n", "similarities", "=", "util", ".", "dot_score", "(", "user_embeddings", ",", "\n", "predefined_embeddings", ")", "\n", "max_similarity", "=", "torch", ".", "max", "(", "similarities", ")", ".", "item", "(", ")", "\n", "if", "max_similarity", ">", "0.6", ":", "\n", "                ", "attr", "[", "7", "]", "=", "1", "\n", "changed", "[", "7", "]", "=", "1", "\n", "\n", "", "", "if", "(", "not", "changed", "[", "8", "]", ")", "and", "(", "text_id", "==", "3", ")", ":", "\n", "# outer_types", "\n", "            ", "predefined_embeddings", "=", "model", ".", "encode", "(", "outer_types", ")", "\n", "similarities", "=", "util", ".", "dot_score", "(", "user_embeddings", ",", "\n", "predefined_embeddings", ")", "\n", "arg_idx", "=", "torch", ".", "argmax", "(", "similarities", ")", ".", "item", "(", ")", "\n", "if", "similarities", "[", "0", "]", "[", "arg_idx", "]", ">", "0.7", ":", "\n", "                ", "attr", "[", "6", "]", "=", "outer_shape_attr", "[", "outer_shape_text", "[", "arg_idx", "]", "]", "\n", "attr", "[", "8", "]", "=", "1", "\n", "changed", "[", "8", "]", "=", "1", "\n", "\n", "", "", "if", "(", "not", "changed", "[", "9", "]", ")", "and", "(", "text_num", ">", "2", "and", "text_id", "==", "2", ")", ":", "\n", "# skirt_types", "\n", "            ", "predefined_embeddings", "=", "model", ".", "encode", "(", "skirt_types", ")", "\n", "similarity_skirt", "=", "util", ".", "dot_score", "(", "user_embeddings", ",", "\n", "predefined_embeddings", ")", "\n", "similarity_dress", "=", "util", ".", "dot_score", "(", "user_embeddings", ",", "\n", "model", ".", "encode", "(", "dress_types", ")", ")", "\n", "if", "similarity_skirt", "[", "0", "]", "[", "0", "]", ">", "0.7", "and", "similarity_skirt", "[", "0", "]", "[", "\n", "0", "]", ">", "similarity_dress", "[", "0", "]", "[", "0", "]", ":", "\n", "                ", "attr", "[", "9", "]", "=", "1", "\n", "attr", "[", "10", "]", "=", "0", "\n", "changed", "[", "9", "]", "=", "1", "\n", "changed", "[", "10", "]", "=", "1", "\n", "\n", "", "", "if", "(", "not", "changed", "[", "11", "]", ")", "and", "(", "text_num", ">", "2", "and", "text_id", "==", "2", ")", ":", "\n", "# pant_types", "\n", "            ", "predefined_embeddings", "=", "model", ".", "encode", "(", "pant_types", ")", "\n", "similarities", "=", "util", ".", "dot_score", "(", "user_embeddings", ",", "\n", "predefined_embeddings", ")", "\n", "max_similarity", "=", "torch", ".", "max", "(", "similarities", ")", ".", "item", "(", ")", "\n", "if", "max_similarity", ">", "0.6", ":", "\n", "                ", "attr", "[", "11", "]", "=", "1", "\n", "attr", "[", "9", "]", "=", "0", "\n", "attr", "[", "10", "]", "=", "0", "\n", "attr", "[", "12", "]", "=", "0", "\n", "changed", "[", "11", "]", "=", "1", "\n", "changed", "[", "9", "]", "=", "1", "\n", "changed", "[", "10", "]", "=", "1", "\n", "changed", "[", "12", "]", "=", "1", "\n", "\n", "", "", "", "return", "human_attr", "+", "attr", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.language_utils.generate_texture_attributes": [[280, 306], ["user_text.split", "len", "attr.append", "attr.append", "len", "attr.append", "attr.append", "attr.append", "attr.append", "attr.append", "attr.append", "attr.append"], "function", ["None"], ["", "def", "generate_texture_attributes", "(", "user_text", ")", ":", "\n", "    ", "parsed_texts", "=", "user_text", ".", "split", "(", "','", ")", "\n", "\n", "attr", "=", "[", "]", "\n", "for", "text", "in", "parsed_texts", ":", "\n", "        ", "if", "(", "'pure color'", "in", "text", ")", "or", "(", "'solid color'", "in", "text", ")", ":", "\n", "            ", "attr", ".", "append", "(", "4", ")", "\n", "", "elif", "(", "'spline'", "in", "text", ")", "or", "(", "'stripe'", "in", "text", ")", ":", "\n", "            ", "attr", ".", "append", "(", "3", ")", "\n", "", "elif", "(", "'plaid'", "in", "text", ")", "or", "(", "'lattice'", "in", "text", ")", ":", "\n", "            ", "attr", ".", "append", "(", "5", ")", "\n", "", "elif", "'floral'", "in", "text", ":", "\n", "            ", "attr", ".", "append", "(", "1", ")", "\n", "", "elif", "'denim'", "in", "text", ":", "\n", "            ", "attr", ".", "append", "(", "0", ")", "\n", "", "else", ":", "\n", "            ", "attr", ".", "append", "(", "17", ")", "\n", "\n", "", "", "if", "len", "(", "attr", ")", "==", "1", ":", "\n", "        ", "attr", ".", "append", "(", "attr", "[", "0", "]", ")", "\n", "attr", ".", "append", "(", "17", ")", "\n", "\n", "", "if", "len", "(", "attr", ")", "==", "2", ":", "\n", "        ", "attr", ".", "append", "(", "17", ")", "\n", "\n", "", "return", "attr", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.logger.MessageLogger.__init__": [[19, 28], ["time.time", "logger.get_root_logger"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.utils.logger.get_root_logger"], ["def", "__init__", "(", "self", ",", "opt", ",", "start_iter", "=", "1", ",", "tb_logger", "=", "None", ")", ":", "\n", "        ", "self", ".", "exp_name", "=", "opt", "[", "'name'", "]", "\n", "self", ".", "interval", "=", "opt", "[", "'print_freq'", "]", "\n", "self", ".", "start_iter", "=", "start_iter", "\n", "self", ".", "max_iters", "=", "opt", "[", "'max_iters'", "]", "\n", "self", ".", "use_tb_logger", "=", "opt", "[", "'use_tb_logger'", "]", "\n", "self", ".", "tb_logger", "=", "tb_logger", "\n", "self", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "logger", "=", "get_root_logger", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.logger.MessageLogger.__call__": [[29, 72], ["log_vars.pop", "log_vars.pop", "log_vars.pop", "log_vars.items", "logger.MessageLogger.logger.info", "log_vars.keys", "log_vars.pop", "log_vars.pop", "str", "time.time", "datetime.timedelta", "logger.MessageLogger.tb_logger.add_scalar", "int"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "log_vars", ")", ":", "\n", "        ", "\"\"\"Format logging message.\n\n        Args:\n            log_vars (dict): It contains the following keys:\n                epoch (int): Epoch number.\n                iter (int): Current iter.\n                lrs (list): List for learning rates.\n\n                time (float): Iter time.\n                data_time (float): Data time for each iter.\n        \"\"\"", "\n", "# epoch, iter, learning rates", "\n", "epoch", "=", "log_vars", ".", "pop", "(", "'epoch'", ")", "\n", "current_iter", "=", "log_vars", ".", "pop", "(", "'iter'", ")", "\n", "lrs", "=", "log_vars", ".", "pop", "(", "'lrs'", ")", "\n", "\n", "message", "=", "(", "f'[{self.exp_name[:5]}..][epoch:{epoch:3d}, '", "\n", "f'iter:{current_iter:8,d}, lr:('", ")", "\n", "for", "v", "in", "lrs", ":", "\n", "            ", "message", "+=", "f'{v:.3e},'", "\n", "", "message", "+=", "')] '", "\n", "\n", "# time and estimated time", "\n", "if", "'time'", "in", "log_vars", ".", "keys", "(", ")", ":", "\n", "            ", "iter_time", "=", "log_vars", ".", "pop", "(", "'time'", ")", "\n", "data_time", "=", "log_vars", ".", "pop", "(", "'data_time'", ")", "\n", "\n", "total_time", "=", "time", ".", "time", "(", ")", "-", "self", ".", "start_time", "\n", "time_sec_avg", "=", "total_time", "/", "(", "current_iter", "-", "self", ".", "start_iter", "+", "1", ")", "\n", "eta_sec", "=", "time_sec_avg", "*", "(", "self", ".", "max_iters", "-", "current_iter", "-", "1", ")", "\n", "eta_str", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "eta_sec", ")", ")", ")", "\n", "message", "+=", "f'[eta: {eta_str}, '", "\n", "message", "+=", "f'time: {iter_time:.3f}, data_time: {data_time:.3f}] '", "\n", "\n", "# other items, especially losses", "\n", "", "for", "k", ",", "v", "in", "log_vars", ".", "items", "(", ")", ":", "\n", "            ", "message", "+=", "f'{k}: {v:.4e} '", "\n", "# tensorboard logger", "\n", "if", "self", ".", "use_tb_logger", "and", "'debug'", "not", "in", "self", ".", "exp_name", ":", "\n", "                ", "self", ".", "tb_logger", ".", "add_scalar", "(", "k", ",", "v", ",", "current_iter", ")", "\n", "\n", "", "", "self", ".", "logger", ".", "info", "(", "message", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.logger.init_tb_logger": [[74, 78], ["SummaryWriter"], "function", ["None"], ["", "", "def", "init_tb_logger", "(", "log_dir", ")", ":", "\n", "    ", "from", "torch", ".", "utils", ".", "tensorboard", "import", "SummaryWriter", "\n", "tb_logger", "=", "SummaryWriter", "(", "log_dir", "=", "log_dir", ")", "\n", "return", "tb_logger", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.logger.get_root_logger": [[80, 113], ["logging.getLogger", "logging.getLogger.hasHandlers", "logging.basicConfig", "logging.FileHandler", "logging.FileHandler.setFormatter", "logging.FileHandler.setLevel", "logging.getLogger.addHandler", "logging.Formatter"], "function", ["None"], ["", "def", "get_root_logger", "(", "logger_name", "=", "'base'", ",", "log_level", "=", "logging", ".", "INFO", ",", "log_file", "=", "None", ")", ":", "\n", "    ", "\"\"\"Get the root logger.\n\n    The logger will be initialized if it has not been initialized. By default a\n    StreamHandler will be added. If `log_file` is specified, a FileHandler will\n    also be added.\n\n    Args:\n        logger_name (str): root logger name. Default: base.\n        log_file (str | None): The log filename. If specified, a FileHandler\n            will be added to the root logger.\n        log_level (int): The root logger level. Note that only the process of\n            rank 0 is affected, while other processes will set the level to\n            \"Error\" and be silent most of the time.\n\n    Returns:\n        logging.Logger: The root logger.\n    \"\"\"", "\n", "logger", "=", "logging", ".", "getLogger", "(", "logger_name", ")", "\n", "# if the logger has been initialized, just return it", "\n", "if", "logger", ".", "hasHandlers", "(", ")", ":", "\n", "        ", "return", "logger", "\n", "\n", "", "format_str", "=", "'%(asctime)s.%(msecs)03d - %(levelname)s: %(message)s'", "\n", "logging", ".", "basicConfig", "(", "format", "=", "format_str", ",", "level", "=", "log_level", ")", "\n", "\n", "if", "log_file", "is", "not", "None", ":", "\n", "        ", "file_handler", "=", "logging", ".", "FileHandler", "(", "log_file", ",", "'w'", ")", "\n", "file_handler", ".", "setFormatter", "(", "logging", ".", "Formatter", "(", "format_str", ")", ")", "\n", "file_handler", ".", "setLevel", "(", "log_level", ")", "\n", "logger", ".", "addHandler", "(", "file_handler", ")", "\n", "\n", "", "return", "logger", "\n", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.ProgressBar.__init__": [[41, 49], ["util.ProgressBar._get_max_bar_width", "util.ProgressBar.start"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.ProgressBar._get_max_bar_width", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.ProgressBar.start"], ["def", "__init__", "(", "self", ",", "task_num", "=", "0", ",", "bar_width", "=", "50", ",", "start", "=", "True", ")", ":", "\n", "        ", "self", ".", "task_num", "=", "task_num", "\n", "max_bar_width", "=", "self", ".", "_get_max_bar_width", "(", ")", "\n", "self", ".", "bar_width", "=", "(", "\n", "bar_width", "if", "bar_width", "<=", "max_bar_width", "else", "max_bar_width", ")", "\n", "self", ".", "completed", "=", "0", "\n", "if", "start", ":", "\n", "            ", "self", ".", "start", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.ProgressBar._get_max_bar_width": [[50, 59], ["shutil.get_terminal_size", "min", "int", "print"], "methods", ["None"], ["", "", "def", "_get_max_bar_width", "(", "self", ")", ":", "\n", "        ", "terminal_width", ",", "_", "=", "get_terminal_size", "(", ")", "\n", "max_bar_width", "=", "min", "(", "int", "(", "terminal_width", "*", "0.6", ")", ",", "terminal_width", "-", "50", ")", "\n", "if", "max_bar_width", "<", "10", ":", "\n", "            ", "print", "(", "f'terminal width is too small ({terminal_width}), '", "\n", "'please consider widen the terminal for better '", "\n", "'progressbar visualization'", ")", "\n", "max_bar_width", "=", "10", "\n", "", "return", "max_bar_width", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.ProgressBar.start": [[60, 68], ["sys.stdout.flush", "time.time", "sys.stdout.write", "sys.stdout.write"], "methods", ["None"], ["", "def", "start", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "task_num", ">", "0", ":", "\n", "            ", "sys", ".", "stdout", ".", "write", "(", "f\"[{' ' * self.bar_width}] 0/{self.task_num}, \"", "\n", "f'elapsed: 0s, ETA:\\nStart...\\n'", ")", "\n", "", "else", ":", "\n", "            ", "sys", ".", "stdout", ".", "write", "(", "'completed: 0, elapsed: 0s'", ")", "\n", "", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "self", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.ProgressBar.update": [[69, 91], ["sys.stdout.flush", "time.time", "int", "int", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "float", "int", "int"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "msg", "=", "'In progress...'", ")", ":", "\n", "        ", "self", ".", "completed", "+=", "1", "\n", "elapsed", "=", "time", ".", "time", "(", ")", "-", "self", ".", "start_time", "\n", "fps", "=", "self", ".", "completed", "/", "elapsed", "\n", "if", "self", ".", "task_num", ">", "0", ":", "\n", "            ", "percentage", "=", "self", ".", "completed", "/", "float", "(", "self", ".", "task_num", ")", "\n", "eta", "=", "int", "(", "elapsed", "*", "(", "1", "-", "percentage", ")", "/", "percentage", "+", "0.5", ")", "\n", "mark_width", "=", "int", "(", "self", ".", "bar_width", "*", "percentage", ")", "\n", "bar_chars", "=", "'>'", "*", "mark_width", "+", "'-'", "*", "(", "self", ".", "bar_width", "-", "mark_width", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "'\\033[2F'", ")", "# cursor up 2 lines", "\n", "sys", ".", "stdout", ".", "write", "(", "\n", "'\\033[J'", "\n", ")", "# clean the output (remove extra chars since last display)", "\n", "sys", ".", "stdout", ".", "write", "(", "\n", "f'[{bar_chars}] {self.completed}/{self.task_num}, '", "\n", "f'{fps:.1f} task/s, elapsed: {int(elapsed + 0.5)}s, '", "\n", "f'ETA: {eta:5}s\\n{msg}\\n'", ")", "\n", "", "else", ":", "\n", "            ", "sys", ".", "stdout", ".", "write", "(", "\n", "f'completed: {self.completed}, elapsed: {int(elapsed + 0.5)}s, '", "\n", "f'{fps:.1f} tasks/s'", ")", "\n", "", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.__init__": [[100, 102], ["util.AverageMeter.reset"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.reset"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.reset": [[103, 108], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "val", "=", "0", "\n", "self", ".", "avg", "=", "0", "# running average = running sum / running count", "\n", "self", ".", "sum", "=", "0", "# running sum", "\n", "self", ".", "count", "=", "0", "# running count", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.update": [[109, 124], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "val", ",", "n", "=", "1", ")", ":", "\n", "# n = batch_size", "\n", "\n", "# val = batch accuracy for an attribute", "\n", "# self.val = val", "\n", "\n", "# sum = 100 * accumulative correct predictions for this attribute", "\n", "        ", "self", ".", "sum", "+=", "val", "*", "n", "\n", "\n", "# count = total samples so far", "\n", "self", ".", "count", "+=", "n", "\n", "\n", "# avg = 100 * avg accuracy for this attribute", "\n", "# for all the batches so far", "\n", "self", ".", "avg", "=", "self", ".", "sum", "/", "self", ".", "count", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.make_exp_dirs": [[14, 23], ["opt[].copy", "os.makedirs", "os.makedirs", "os.makedirs", "opt[].copy.pop", "opt[].copy.pop", "opt[].copy.pop"], "function", ["None"], ["def", "make_exp_dirs", "(", "opt", ")", ":", "\n", "    ", "\"\"\"Make dirs for experiments.\"\"\"", "\n", "path_opt", "=", "opt", "[", "'path'", "]", ".", "copy", "(", ")", "\n", "if", "opt", "[", "'is_train'", "]", ":", "\n", "        ", "overwrite", "=", "True", "if", "'debug'", "in", "opt", "[", "'name'", "]", "else", "False", "\n", "os", ".", "makedirs", "(", "path_opt", ".", "pop", "(", "'experiments_root'", ")", ",", "exist_ok", "=", "overwrite", ")", "\n", "os", ".", "makedirs", "(", "path_opt", ".", "pop", "(", "'models'", ")", ",", "exist_ok", "=", "overwrite", ")", "\n", "", "else", ":", "\n", "        ", "os", ".", "makedirs", "(", "path_opt", ".", "pop", "(", "'results_root'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.set_random_seed": [[25, 32], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed_all"], "function", ["None"], ["", "", "def", "set_random_seed", "(", "seed", ")", ":", "\n", "    ", "\"\"\"Set random seeds.\"\"\"", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.parsing_gen_model.ParsingGenModel.__init__": [[23, 55], ["torch.device", "models.archs.shape_attr_embedding_arch.ShapeAttrEmbedding().to", "models.archs.unet_arch.ShapeUNet().to", "models.archs.fcn_arch.FCNHead().to", "parsing_gen_model.ParsingGenModel.init_training_settings", "models.archs.shape_attr_embedding_arch.ShapeAttrEmbedding", "models.archs.unet_arch.ShapeUNet", "models.archs.fcn_arch.FCNHead"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.init_training_settings"], ["def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "self", ".", "opt", "=", "opt", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "'cuda'", ")", "\n", "self", ".", "is_train", "=", "opt", "[", "'is_train'", "]", "\n", "\n", "self", ".", "attr_embedder", "=", "ShapeAttrEmbedding", "(", "\n", "dim", "=", "opt", "[", "'embedder_dim'", "]", ",", "\n", "out_dim", "=", "opt", "[", "'embedder_out_dim'", "]", ",", "\n", "cls_num_list", "=", "opt", "[", "'attr_class_num'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "parsing_encoder", "=", "ShapeUNet", "(", "\n", "in_channels", "=", "opt", "[", "'encoder_in_channels'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "parsing_decoder", "=", "FCNHead", "(", "\n", "in_channels", "=", "opt", "[", "'fc_in_channels'", "]", ",", "\n", "in_index", "=", "opt", "[", "'fc_in_index'", "]", ",", "\n", "channels", "=", "opt", "[", "'fc_channels'", "]", ",", "\n", "num_convs", "=", "opt", "[", "'fc_num_convs'", "]", ",", "\n", "concat_input", "=", "opt", "[", "'fc_concat_input'", "]", ",", "\n", "dropout_ratio", "=", "opt", "[", "'fc_dropout_ratio'", "]", ",", "\n", "num_classes", "=", "opt", "[", "'fc_num_classes'", "]", ",", "\n", "align_corners", "=", "opt", "[", "'fc_align_corners'", "]", ",", "\n", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "self", ".", "init_training_settings", "(", ")", "\n", "\n", "self", ".", "palette", "=", "[", "[", "0", ",", "0", ",", "0", "]", ",", "[", "255", ",", "250", ",", "250", "]", ",", "[", "220", ",", "220", ",", "220", "]", ",", "\n", "[", "250", ",", "235", ",", "215", "]", ",", "[", "255", ",", "250", ",", "205", "]", ",", "[", "211", ",", "211", ",", "211", "]", ",", "\n", "[", "70", ",", "130", ",", "180", "]", ",", "[", "127", ",", "255", ",", "212", "]", ",", "[", "0", ",", "100", ",", "0", "]", ",", "\n", "[", "50", ",", "205", ",", "50", "]", ",", "[", "255", ",", "255", ",", "0", "]", ",", "[", "245", ",", "222", ",", "179", "]", ",", "\n", "[", "255", ",", "140", ",", "0", "]", ",", "[", "255", ",", "0", ",", "0", "]", ",", "[", "16", ",", "78", ",", "139", "]", ",", "\n", "[", "144", ",", "238", ",", "144", "]", ",", "[", "50", ",", "205", ",", "174", "]", ",", "[", "50", ",", "155", ",", "250", "]", ",", "\n", "[", "160", ",", "140", ",", "88", "]", ",", "[", "213", ",", "140", ",", "88", "]", ",", "[", "90", ",", "140", ",", "90", "]", ",", "\n", "[", "185", ",", "210", ",", "205", "]", ",", "[", "130", ",", "165", ",", "180", "]", ",", "[", "225", ",", "141", ",", "151", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.parsing_gen_model.ParsingGenModel.init_training_settings": [[56, 74], ["parsing_gen_model.ParsingGenModel.attr_embedder.parameters", "parsing_gen_model.ParsingGenModel.parsing_encoder.parameters", "parsing_gen_model.ParsingGenModel.parsing_decoder.parameters", "torch.optim.Adam", "collections.OrderedDict", "models.losses.cross_entropy_loss.CrossEntropyLoss().to", "optim_params.append", "optim_params.append", "optim_params.append", "models.losses.cross_entropy_loss.CrossEntropyLoss"], "methods", ["None"], ["", "def", "init_training_settings", "(", "self", ")", ":", "\n", "        ", "optim_params", "=", "[", "]", "\n", "for", "v", "in", "self", ".", "attr_embedder", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "v", ".", "requires_grad", ":", "\n", "                ", "optim_params", ".", "append", "(", "v", ")", "\n", "", "", "for", "v", "in", "self", ".", "parsing_encoder", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "v", ".", "requires_grad", ":", "\n", "                ", "optim_params", ".", "append", "(", "v", ")", "\n", "", "", "for", "v", "in", "self", ".", "parsing_decoder", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "v", ".", "requires_grad", ":", "\n", "                ", "optim_params", ".", "append", "(", "v", ")", "\n", "# set up optimizers", "\n", "", "", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "\n", "optim_params", ",", "\n", "self", ".", "opt", "[", "'lr'", "]", ",", "\n", "weight_decay", "=", "self", ".", "opt", "[", "'weight_decay'", "]", ")", "\n", "self", ".", "log_dict", "=", "OrderedDict", "(", ")", "\n", "self", ".", "entropy_loss", "=", "CrossEntropyLoss", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.parsing_gen_model.ParsingGenModel.feed_data": [[75, 79], ["data[].to", "data[].to", "data[].to"], "methods", ["None"], ["", "def", "feed_data", "(", "self", ",", "data", ")", ":", "\n", "        ", "self", ".", "pose", "=", "data", "[", "'densepose'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "attr", "=", "data", "[", "'attr'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "segm", "=", "data", "[", "'segm'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.parsing_gen_model.ParsingGenModel.optimize_parameters": [[80, 96], ["parsing_gen_model.ParsingGenModel.attr_embedder.train", "parsing_gen_model.ParsingGenModel.parsing_encoder.train", "parsing_gen_model.ParsingGenModel.parsing_decoder.train", "parsing_gen_model.ParsingGenModel.attr_embedder", "parsing_gen_model.ParsingGenModel.parsing_encoder", "parsing_gen_model.ParsingGenModel.parsing_decoder", "parsing_gen_model.ParsingGenModel.entropy_loss", "parsing_gen_model.ParsingGenModel.optimizer.zero_grad", "parsing_gen_model.ParsingGenModel.backward", "parsing_gen_model.ParsingGenModel.optimizer.step"], "methods", ["None"], ["", "def", "optimize_parameters", "(", "self", ")", ":", "\n", "        ", "self", ".", "attr_embedder", ".", "train", "(", ")", "\n", "self", ".", "parsing_encoder", ".", "train", "(", ")", "\n", "self", ".", "parsing_decoder", ".", "train", "(", ")", "\n", "\n", "self", ".", "attr_embedding", "=", "self", ".", "attr_embedder", "(", "self", ".", "attr", ")", "\n", "self", ".", "pose_enc", "=", "self", ".", "parsing_encoder", "(", "self", ".", "pose", ",", "self", ".", "attr_embedding", ")", "\n", "self", ".", "seg_logits", "=", "self", ".", "parsing_decoder", "(", "self", ".", "pose_enc", ")", "\n", "\n", "loss", "=", "self", ".", "entropy_loss", "(", "self", ".", "seg_logits", ",", "self", ".", "segm", ")", "\n", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "self", ".", "log_dict", "[", "'loss_total'", "]", "=", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.parsing_gen_model.ParsingGenModel.get_vis": [[97, 107], ["torch.cat().detach", "img_cat.clamp_.clamp_.clamp_", "torchvision.utils.save_image", "torch.cat"], "methods", ["None"], ["", "def", "get_vis", "(", "self", ",", "save_path", ")", ":", "\n", "        ", "img_cat", "=", "torch", ".", "cat", "(", "[", "\n", "self", ".", "pose", ",", "\n", "self", ".", "segm", ",", "\n", "]", ",", "dim", "=", "3", ")", ".", "detach", "(", ")", "\n", "img_cat", "=", "(", "(", "img_cat", "+", "1", ")", "/", "2", ")", "\n", "\n", "img_cat", "=", "img_cat", ".", "clamp_", "(", "0", ",", "1", ")", "\n", "\n", "save_image", "(", "img_cat", ",", "save_path", ",", "nrow", "=", "1", ",", "padding", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.parsing_gen_model.ParsingGenModel.inference": [[108, 144], ["parsing_gen_model.ParsingGenModel.attr_embedder.eval", "parsing_gen_model.ParsingGenModel.parsing_encoder.eval", "parsing_gen_model.ParsingGenModel.parsing_decoder.eval", "enumerate", "parsing_gen_model.ParsingGenModel.attr_embedder.train", "parsing_gen_model.ParsingGenModel.parsing_encoder.train", "parsing_gen_model.ParsingGenModel.parsing_decoder.train", "data[].to", "data[].to", "data[].to", "data[].to.size", "parsing_gen_model.ParsingGenModel.argmax", "models.losses.accuracy.accuracy", "parsing_gen_model.ParsingGenModel.palette_result", "parsing_gen_model.ParsingGenModel.palette_result", "numpy.concatenate", "mmcv.imwrite", "torch.no_grad", "parsing_gen_model.ParsingGenModel.attr_embedder", "parsing_gen_model.ParsingGenModel.parsing_encoder", "parsing_gen_model.ParsingGenModel.parsing_decoder", "data[].to.cpu().numpy", "parsing_gen_model.ParsingGenModel.argmax.cpu().numpy", "data[].to.cpu", "parsing_gen_model.ParsingGenModel.argmax.cpu", "pose[].size", "pose[].size"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.losses.accuracy.accuracy", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.palette_result", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.palette_result"], ["", "def", "inference", "(", "self", ",", "data_loader", ",", "save_dir", ")", ":", "\n", "        ", "self", ".", "attr_embedder", ".", "eval", "(", ")", "\n", "self", ".", "parsing_encoder", ".", "eval", "(", ")", "\n", "self", ".", "parsing_decoder", ".", "eval", "(", ")", "\n", "\n", "acc", "=", "0", "\n", "num", "=", "0", "\n", "\n", "for", "_", ",", "data", "in", "enumerate", "(", "data_loader", ")", ":", "\n", "            ", "pose", "=", "data", "[", "'densepose'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "attr", "=", "data", "[", "'attr'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "segm", "=", "data", "[", "'segm'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "img_name", "=", "data", "[", "'img_name'", "]", "\n", "\n", "num", "+=", "pose", ".", "size", "(", "0", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "attr_embedding", "=", "self", ".", "attr_embedder", "(", "attr", ")", "\n", "pose_enc", "=", "self", ".", "parsing_encoder", "(", "pose", ",", "attr_embedding", ")", "\n", "seg_logits", "=", "self", ".", "parsing_decoder", "(", "pose_enc", ")", "\n", "", "seg_pred", "=", "seg_logits", ".", "argmax", "(", "dim", "=", "1", ")", "\n", "acc", "+=", "accuracy", "(", "seg_logits", ",", "segm", ")", "\n", "palette_label", "=", "self", ".", "palette_result", "(", "segm", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "palette_pred", "=", "self", ".", "palette_result", "(", "seg_pred", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "pose_numpy", "=", "(", "(", "pose", "[", "0", "]", "+", "1", ")", "/", "2.", "*", "255.", ")", ".", "expand", "(", "\n", "3", ",", "\n", "pose", "[", "0", "]", ".", "size", "(", "1", ")", ",", "\n", "pose", "[", "0", "]", ".", "size", "(", "2", ")", ",", "\n", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "clip", "(", "0", ",", "255", ")", ".", "astype", "(", "np", ".", "uint8", ")", ".", "transpose", "(", "1", ",", "2", ",", "0", ")", "\n", "concat_result", "=", "np", ".", "concatenate", "(", "\n", "(", "pose_numpy", ",", "palette_pred", ",", "palette_label", ")", ",", "axis", "=", "1", ")", "\n", "mmcv", ".", "imwrite", "(", "concat_result", ",", "f'{save_dir}/{img_name[0]}'", ")", "\n", "\n", "", "self", ".", "attr_embedder", ".", "train", "(", ")", "\n", "self", ".", "parsing_encoder", ".", "train", "(", ")", "\n", "self", ".", "parsing_decoder", ".", "train", "(", ")", "\n", "return", "(", "acc", "/", "num", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.parsing_gen_model.ParsingGenModel.get_current_log": [[145, 147], ["None"], "methods", ["None"], ["", "def", "get_current_log", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "log_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.parsing_gen_model.ParsingGenModel.update_learning_rate": [[148, 184], ["math.cos", "ValueError", "int"], "methods", ["None"], ["", "def", "update_learning_rate", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "\"\"\"Update learning rate.\n\n        Args:\n            current_iter (int): Current iteration.\n            warmup_iter (int): Warmup iter numbers. -1 for no warmup.\n                Default: -1.\n        \"\"\"", "\n", "lr", "=", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "\n", "if", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'step'", ":", "\n", "            ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "(", "\n", "self", ".", "opt", "[", "'gamma'", "]", "**", "(", "epoch", "//", "self", ".", "opt", "[", "'step'", "]", ")", ")", "\n", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'cos'", ":", "\n", "            ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "(", "\n", "1", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "epoch", "/", "self", ".", "opt", "[", "'num_epochs'", "]", ")", ")", "/", "2", "\n", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'linear'", ":", "\n", "            ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "(", "1", "-", "epoch", "/", "self", ".", "opt", "[", "'num_epochs'", "]", ")", "\n", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'linear2exp'", ":", "\n", "            ", "if", "epoch", "<", "self", ".", "opt", "[", "'turning_point'", "]", "+", "1", ":", "\n", "# learning rate decay as 95%", "\n", "# at the turning point (1 / 95% = 1.0526)", "\n", "                ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "(", "\n", "1", "-", "epoch", "/", "int", "(", "self", ".", "opt", "[", "'turning_point'", "]", "*", "1.0526", ")", ")", "\n", "", "else", ":", "\n", "                ", "lr", "*=", "self", ".", "opt", "[", "'gamma'", "]", "\n", "", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'schedule'", ":", "\n", "            ", "if", "epoch", "in", "self", ".", "opt", "[", "'schedule'", "]", ":", "\n", "                ", "lr", "*=", "self", ".", "opt", "[", "'gamma'", "]", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Unknown lr mode {}'", ".", "format", "(", "self", ".", "opt", "[", "'lr_decay'", "]", ")", ")", "\n", "# set learning rate", "\n", "", "for", "param_group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "            ", "param_group", "[", "'lr'", "]", "=", "lr", "\n", "\n", "", "return", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.parsing_gen_model.ParsingGenModel.save_network": [[185, 195], ["parsing_gen_model.ParsingGenModel.attr_embedder.state_dict", "parsing_gen_model.ParsingGenModel.parsing_encoder.state_dict", "parsing_gen_model.ParsingGenModel.parsing_decoder.state_dict", "torch.save"], "methods", ["None"], ["", "def", "save_network", "(", "self", ",", "save_path", ")", ":", "\n", "        ", "\"\"\"Save networks.\n        \"\"\"", "\n", "\n", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'embedder'", "]", "=", "self", ".", "attr_embedder", ".", "state_dict", "(", ")", "\n", "save_dict", "[", "'encoder'", "]", "=", "self", ".", "parsing_encoder", ".", "state_dict", "(", ")", "\n", "save_dict", "[", "'decoder'", "]", "=", "self", ".", "parsing_decoder", ".", "state_dict", "(", ")", "\n", "\n", "torch", ".", "save", "(", "save_dict", ",", "save_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.parsing_gen_model.ParsingGenModel.load_network": [[196, 209], ["torch.load", "parsing_gen_model.ParsingGenModel.attr_embedder.load_state_dict", "parsing_gen_model.ParsingGenModel.attr_embedder.eval", "parsing_gen_model.ParsingGenModel.parsing_encoder.load_state_dict", "parsing_gen_model.ParsingGenModel.parsing_encoder.eval", "parsing_gen_model.ParsingGenModel.parsing_decoder.load_state_dict", "parsing_gen_model.ParsingGenModel.parsing_decoder.eval"], "methods", ["None"], ["", "def", "load_network", "(", "self", ")", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "self", ".", "opt", "[", "'pretrained_parsing_gen'", "]", ")", "\n", "\n", "self", ".", "attr_embedder", ".", "load_state_dict", "(", "checkpoint", "[", "'embedder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "attr_embedder", ".", "eval", "(", ")", "\n", "\n", "self", ".", "parsing_encoder", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'encoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "parsing_encoder", ".", "eval", "(", ")", "\n", "\n", "self", ".", "parsing_decoder", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'decoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "parsing_decoder", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.parsing_gen_model.ParsingGenModel.palette_result": [[210, 221], ["numpy.array", "numpy.zeros", "enumerate", "len"], "methods", ["None"], ["", "def", "palette_result", "(", "self", ",", "result", ")", ":", "\n", "        ", "seg", "=", "result", "[", "0", "]", "\n", "palette", "=", "np", ".", "array", "(", "self", ".", "palette", ")", "\n", "assert", "palette", ".", "shape", "[", "1", "]", "==", "3", "\n", "assert", "len", "(", "palette", ".", "shape", ")", "==", "2", "\n", "color_seg", "=", "np", ".", "zeros", "(", "(", "seg", ".", "shape", "[", "0", "]", ",", "seg", ".", "shape", "[", "1", "]", ",", "3", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "for", "label", ",", "color", "in", "enumerate", "(", "palette", ")", ":", "\n", "            ", "color_seg", "[", "seg", "==", "label", ",", ":", "]", "=", "color", "\n", "# convert to BGR", "\n", "", "color_seg", "=", "color_seg", "[", "...", ",", ":", ":", "-", "1", "]", "\n", "return", "color_seg", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.__init__": [[22, 106], ["torch.device", "torch.device", "torch.device", "torch.device", "models.archs.vqgan_arch.Encoder().to", "models.archs.vqgan_arch.Decoder().to", "models.archs.vqgan_arch.VectorQuantizerTexture().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.load_top_pretrain_models", "models.archs.vqgan_arch.Encoder().to", "models.archs.vqgan_arch.DecoderRes().to", "models.archs.vqgan_arch.VectorQuantizerSpatialTextureAware().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.load_bot_pretrain_network", "models.archs.unet_arch.UNet().to", "models.archs.fcn_arch.MultiHeadFCNHead().to", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.init_training_settings", "models.archs.vqgan_arch.Encoder", "models.archs.vqgan_arch.Decoder", "models.archs.vqgan_arch.VectorQuantizerTexture", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "models.archs.vqgan_arch.Encoder", "models.archs.vqgan_arch.DecoderRes", "models.archs.vqgan_arch.VectorQuantizerSpatialTextureAware", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "models.archs.unet_arch.UNet", "models.archs.fcn_arch.MultiHeadFCNHead"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.load_top_pretrain_models", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.load_bot_pretrain_network", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.init_training_settings"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "self", ".", "opt", "=", "opt", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "'cuda'", ")", "\n", "self", ".", "is_train", "=", "opt", "[", "'is_train'", "]", "\n", "\n", "self", ".", "top_encoder", "=", "Encoder", "(", "\n", "ch", "=", "opt", "[", "'top_ch'", "]", ",", "\n", "num_res_blocks", "=", "opt", "[", "'top_num_res_blocks'", "]", ",", "\n", "attn_resolutions", "=", "opt", "[", "'top_attn_resolutions'", "]", ",", "\n", "ch_mult", "=", "opt", "[", "'top_ch_mult'", "]", ",", "\n", "in_channels", "=", "opt", "[", "'top_in_channels'", "]", ",", "\n", "resolution", "=", "opt", "[", "'top_resolution'", "]", ",", "\n", "z_channels", "=", "opt", "[", "'top_z_channels'", "]", ",", "\n", "double_z", "=", "opt", "[", "'top_double_z'", "]", ",", "\n", "dropout", "=", "opt", "[", "'top_dropout'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "decoder", "=", "Decoder", "(", "\n", "in_channels", "=", "opt", "[", "'top_in_channels'", "]", ",", "\n", "resolution", "=", "opt", "[", "'top_resolution'", "]", ",", "\n", "z_channels", "=", "opt", "[", "'top_z_channels'", "]", ",", "\n", "ch", "=", "opt", "[", "'top_ch'", "]", ",", "\n", "out_ch", "=", "opt", "[", "'top_out_ch'", "]", ",", "\n", "num_res_blocks", "=", "opt", "[", "'top_num_res_blocks'", "]", ",", "\n", "attn_resolutions", "=", "opt", "[", "'top_attn_resolutions'", "]", ",", "\n", "ch_mult", "=", "opt", "[", "'top_ch_mult'", "]", ",", "\n", "dropout", "=", "opt", "[", "'top_dropout'", "]", ",", "\n", "resamp_with_conv", "=", "True", ",", "\n", "give_pre_end", "=", "False", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "top_quantize", "=", "VectorQuantizerTexture", "(", "\n", "1024", ",", "opt", "[", "'embed_dim'", "]", ",", "beta", "=", "0.25", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "top_quant_conv", "=", "torch", ".", "nn", ".", "Conv2d", "(", "opt", "[", "\"top_z_channels\"", "]", ",", "\n", "opt", "[", "'embed_dim'", "]", ",", "\n", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "top_post_quant_conv", "=", "torch", ".", "nn", ".", "Conv2d", "(", "opt", "[", "'embed_dim'", "]", ",", "\n", "opt", "[", "\"top_z_channels\"", "]", ",", "\n", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "load_top_pretrain_models", "(", ")", "\n", "\n", "self", ".", "bot_encoder", "=", "Encoder", "(", "\n", "ch", "=", "opt", "[", "'bot_ch'", "]", ",", "\n", "num_res_blocks", "=", "opt", "[", "'bot_num_res_blocks'", "]", ",", "\n", "attn_resolutions", "=", "opt", "[", "'bot_attn_resolutions'", "]", ",", "\n", "ch_mult", "=", "opt", "[", "'bot_ch_mult'", "]", ",", "\n", "in_channels", "=", "opt", "[", "'bot_in_channels'", "]", ",", "\n", "resolution", "=", "opt", "[", "'bot_resolution'", "]", ",", "\n", "z_channels", "=", "opt", "[", "'bot_z_channels'", "]", ",", "\n", "double_z", "=", "opt", "[", "'bot_double_z'", "]", ",", "\n", "dropout", "=", "opt", "[", "'bot_dropout'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "bot_decoder_res", "=", "DecoderRes", "(", "\n", "in_channels", "=", "opt", "[", "'bot_in_channels'", "]", ",", "\n", "resolution", "=", "opt", "[", "'bot_resolution'", "]", ",", "\n", "z_channels", "=", "opt", "[", "'bot_z_channels'", "]", ",", "\n", "ch", "=", "opt", "[", "'bot_ch'", "]", ",", "\n", "num_res_blocks", "=", "opt", "[", "'bot_num_res_blocks'", "]", ",", "\n", "ch_mult", "=", "opt", "[", "'bot_ch_mult'", "]", ",", "\n", "dropout", "=", "opt", "[", "'bot_dropout'", "]", ",", "\n", "give_pre_end", "=", "False", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "bot_quantize", "=", "VectorQuantizerSpatialTextureAware", "(", "\n", "opt", "[", "'bot_n_embed'", "]", ",", "\n", "opt", "[", "'embed_dim'", "]", ",", "\n", "beta", "=", "0.25", ",", "\n", "spatial_size", "=", "opt", "[", "'codebook_spatial_size'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "bot_quant_conv", "=", "torch", ".", "nn", ".", "Conv2d", "(", "opt", "[", "\"bot_z_channels\"", "]", ",", "\n", "opt", "[", "'embed_dim'", "]", ",", "\n", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "bot_post_quant_conv", "=", "torch", ".", "nn", ".", "Conv2d", "(", "opt", "[", "'embed_dim'", "]", ",", "\n", "opt", "[", "\"bot_z_channels\"", "]", ",", "\n", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "self", ".", "load_bot_pretrain_network", "(", ")", "\n", "\n", "self", ".", "guidance_encoder", "=", "UNet", "(", "\n", "in_channels", "=", "opt", "[", "'encoder_in_channels'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "index_decoder", "=", "MultiHeadFCNHead", "(", "\n", "in_channels", "=", "opt", "[", "'fc_in_channels'", "]", ",", "\n", "in_index", "=", "opt", "[", "'fc_in_index'", "]", ",", "\n", "channels", "=", "opt", "[", "'fc_channels'", "]", ",", "\n", "num_convs", "=", "opt", "[", "'fc_num_convs'", "]", ",", "\n", "concat_input", "=", "opt", "[", "'fc_concat_input'", "]", ",", "\n", "dropout_ratio", "=", "opt", "[", "'fc_dropout_ratio'", "]", ",", "\n", "num_classes", "=", "opt", "[", "'fc_num_classes'", "]", ",", "\n", "align_corners", "=", "opt", "[", "'fc_align_corners'", "]", ",", "\n", "num_head", "=", "18", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "self", ".", "init_training_settings", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.init_training_settings": [[107, 130], ["hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.guidance_encoder.parameters", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.index_decoder.parameters", "collections.OrderedDict", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "models.losses.cross_entropy_loss.CrossEntropyLoss().to", "optim_params.append", "optim_params.append", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "models.losses.cross_entropy_loss.CrossEntropyLoss"], "methods", ["None"], ["", "def", "init_training_settings", "(", "self", ")", ":", "\n", "        ", "optim_params", "=", "[", "]", "\n", "for", "v", "in", "self", ".", "guidance_encoder", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "v", ".", "requires_grad", ":", "\n", "                ", "optim_params", ".", "append", "(", "v", ")", "\n", "", "", "for", "v", "in", "self", ".", "index_decoder", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "v", ".", "requires_grad", ":", "\n", "                ", "optim_params", ".", "append", "(", "v", ")", "\n", "# set up optimizers", "\n", "", "", "if", "self", ".", "opt", "[", "'optimizer'", "]", "==", "'Adam'", ":", "\n", "            ", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "\n", "optim_params", ",", "\n", "self", ".", "opt", "[", "'lr'", "]", ",", "\n", "weight_decay", "=", "self", ".", "opt", "[", "'weight_decay'", "]", ")", "\n", "", "elif", "self", ".", "opt", "[", "'optimizer'", "]", "==", "'SGD'", ":", "\n", "            ", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "\n", "optim_params", ",", "\n", "self", ".", "opt", "[", "'lr'", "]", ",", "\n", "momentum", "=", "self", ".", "opt", "[", "'momentum'", "]", ",", "\n", "weight_decay", "=", "self", ".", "opt", "[", "'weight_decay'", "]", ")", "\n", "", "self", ".", "log_dict", "=", "OrderedDict", "(", ")", "\n", "if", "self", ".", "opt", "[", "'loss_function'", "]", "==", "'cross_entropy'", ":", "\n", "            ", "self", ".", "loss_func", "=", "CrossEntropyLoss", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.load_top_pretrain_models": [[131, 148], ["torch.load", "torch.load", "torch.load", "torch.load", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.top_encoder.load_state_dict", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.decoder.load_state_dict", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.top_quantize.load_state_dict", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.top_quant_conv.load_state_dict", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.top_post_quant_conv.load_state_dict", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.top_encoder.eval", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.top_quantize.eval", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.top_quant_conv.eval", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.top_post_quant_conv.eval"], "methods", ["None"], ["", "", "def", "load_top_pretrain_models", "(", "self", ")", ":", "\n", "# load pretrained vqgan for segmentation mask", "\n", "        ", "top_vae_checkpoint", "=", "torch", ".", "load", "(", "self", ".", "opt", "[", "'top_vae_path'", "]", ")", "\n", "self", ".", "top_encoder", ".", "load_state_dict", "(", "\n", "top_vae_checkpoint", "[", "'encoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "decoder", ".", "load_state_dict", "(", "\n", "top_vae_checkpoint", "[", "'decoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "top_quantize", ".", "load_state_dict", "(", "\n", "top_vae_checkpoint", "[", "'quantize'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "top_quant_conv", ".", "load_state_dict", "(", "\n", "top_vae_checkpoint", "[", "'quant_conv'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "top_post_quant_conv", ".", "load_state_dict", "(", "\n", "top_vae_checkpoint", "[", "'post_quant_conv'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "top_encoder", ".", "eval", "(", ")", "\n", "self", ".", "top_quantize", ".", "eval", "(", ")", "\n", "self", ".", "top_quant_conv", ".", "eval", "(", ")", "\n", "self", ".", "top_post_quant_conv", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.load_bot_pretrain_network": [[149, 169], ["torch.load", "torch.load", "torch.load", "torch.load", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.bot_encoder.load_state_dict", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.bot_decoder_res.load_state_dict", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.decoder.load_state_dict", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.bot_quantize.load_state_dict", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.bot_quant_conv.load_state_dict", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.bot_post_quant_conv.load_state_dict", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.bot_encoder.eval", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.bot_decoder_res.eval", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.decoder.eval", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.bot_quantize.eval", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.bot_quant_conv.eval", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.bot_post_quant_conv.eval"], "methods", ["None"], ["", "def", "load_bot_pretrain_network", "(", "self", ")", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "self", ".", "opt", "[", "'bot_vae_path'", "]", ")", "\n", "self", ".", "bot_encoder", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'bot_encoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "bot_decoder_res", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'bot_decoder_res'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "decoder", ".", "load_state_dict", "(", "checkpoint", "[", "'decoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "bot_quantize", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'bot_quantize'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "bot_quant_conv", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'bot_quant_conv'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "bot_post_quant_conv", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'bot_post_quant_conv'", "]", ",", "strict", "=", "True", ")", "\n", "\n", "self", ".", "bot_encoder", ".", "eval", "(", ")", "\n", "self", ".", "bot_decoder_res", ".", "eval", "(", ")", "\n", "self", ".", "decoder", ".", "eval", "(", ")", "\n", "self", ".", "bot_quantize", ".", "eval", "(", ")", "\n", "self", ".", "bot_quant_conv", ".", "eval", "(", ")", "\n", "self", ".", "bot_post_quant_conv", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.top_encode": [[170, 177], ["hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.top_encoder", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.top_quant_conv", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.top_quantize", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.top_post_quant_conv"], "methods", ["None"], ["", "def", "top_encode", "(", "self", ",", "x", ",", "mask", ")", ":", "\n", "        ", "h", "=", "self", ".", "top_encoder", "(", "x", ")", "\n", "h", "=", "self", ".", "top_quant_conv", "(", "h", ")", "\n", "quant", ",", "_", ",", "_", "=", "self", ".", "top_quantize", "(", "h", ",", "mask", ")", "\n", "quant", "=", "self", ".", "top_post_quant_conv", "(", "quant", ")", "\n", "\n", "return", "quant", ",", "quant", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.feed_data": [[178, 186], ["data[].to", "data[].float().to", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.get_gt_indices", "torch.interpolate().view().long", "torch.interpolate().view().long", "data[].float", "torch.interpolate().view", "torch.interpolate().view", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.image.size", "torch.interpolate", "torch.interpolate"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.get_gt_indices"], ["", "def", "feed_data", "(", "self", ",", "data", ")", ":", "\n", "        ", "self", ".", "image", "=", "data", "[", "'image'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "texture_mask", "=", "data", "[", "'texture_mask'", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "get_gt_indices", "(", ")", "\n", "\n", "self", ".", "texture_tokens", "=", "F", ".", "interpolate", "(", "\n", "self", ".", "texture_mask", ",", "size", "=", "(", "32", ",", "16", ")", ",", "\n", "mode", "=", "'nearest'", ")", ".", "view", "(", "self", ".", "image", ".", "size", "(", "0", ")", ",", "-", "1", ")", ".", "long", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.bot_encode": [[187, 193], ["hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.bot_encoder", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.bot_quant_conv", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.bot_quantize"], "methods", ["None"], ["", "def", "bot_encode", "(", "self", ",", "x", ",", "mask", ")", ":", "\n", "        ", "h", "=", "self", ".", "bot_encoder", "(", "x", ")", "\n", "h", "=", "self", ".", "bot_quant_conv", "(", "h", ")", "\n", "_", ",", "_", ",", "(", "_", ",", "_", ",", "indices_list", ")", "=", "self", ".", "bot_quantize", "(", "h", ",", "mask", ")", "\n", "\n", "return", "indices_list", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.get_gt_indices": [[194, 198], ["hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.top_encode", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.bot_encode"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.top_encode", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_encode"], ["", "def", "get_gt_indices", "(", "self", ")", ":", "\n", "        ", "self", ".", "quant_t", ",", "self", ".", "feature_t", "=", "self", ".", "top_encode", "(", "self", ".", "image", ",", "\n", "self", ".", "texture_mask", ")", "\n", "self", ".", "gt_indices_list", "=", "self", ".", "bot_encode", "(", "self", ".", "image", ",", "self", ".", "texture_mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.index_to_image": [[199, 211], ["hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.bot_quantize.get_codebook_entry", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.bot_post_quant_conv", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.bot_decoder_res", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.decoder", "index_bottom_list[].size", "index_bottom_list[].size", "index_bottom_list[].size"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.VectorQuantizerSpatialTextureAware.get_codebook_entry"], ["", "def", "index_to_image", "(", "self", ",", "index_bottom_list", ",", "texture_mask", ")", ":", "\n", "        ", "quant_b", "=", "self", ".", "bot_quantize", ".", "get_codebook_entry", "(", "\n", "index_bottom_list", ",", "texture_mask", ",", "\n", "(", "index_bottom_list", "[", "0", "]", ".", "size", "(", "0", ")", ",", "index_bottom_list", "[", "0", "]", ".", "size", "(", "1", ")", ",", "\n", "index_bottom_list", "[", "0", "]", ".", "size", "(", "2", ")", ",", "\n", "self", ".", "opt", "[", "\"bot_z_channels\"", "]", ")", ")", "#.permute(0, 3, 1, 2)", "\n", "quant_b", "=", "self", ".", "bot_post_quant_conv", "(", "quant_b", ")", "\n", "bot_dec_res", "=", "self", ".", "bot_decoder_res", "(", "quant_b", ")", "\n", "\n", "dec", "=", "self", ".", "decoder", "(", "self", ".", "quant_t", ",", "bot_h", "=", "bot_dec_res", ")", "\n", "\n", "return", "dec", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.get_vis": [[212, 226], ["hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.index_to_image", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.index_to_image", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.decoder", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "img_cat.clamp_.clamp_.clamp_", "torchvision.utils.save_image", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.index_to_image", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.index_to_image"], ["", "def", "get_vis", "(", "self", ",", "pred_img_index", ",", "rec_img_index", ",", "texture_mask", ",", "save_path", ")", ":", "\n", "        ", "rec_img", "=", "self", ".", "index_to_image", "(", "rec_img_index", ",", "texture_mask", ")", "\n", "pred_img", "=", "self", ".", "index_to_image", "(", "pred_img_index", ",", "texture_mask", ")", "\n", "\n", "base_img", "=", "self", ".", "decoder", "(", "self", ".", "quant_t", ")", "\n", "img_cat", "=", "torch", ".", "cat", "(", "[", "\n", "self", ".", "image", ",", "\n", "rec_img", ",", "\n", "base_img", ",", "\n", "pred_img", ",", "\n", "]", ",", "dim", "=", "3", ")", ".", "detach", "(", ")", "\n", "img_cat", "=", "(", "(", "img_cat", "+", "1", ")", "/", "2", ")", "\n", "img_cat", "=", "img_cat", ".", "clamp_", "(", "0", ",", "1", ")", "\n", "save_image", "(", "img_cat", ",", "save_path", ",", "nrow", "=", "1", ",", "padding", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.optimize_parameters": [[227, 246], ["hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.guidance_encoder.train", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.index_decoder.train", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.guidance_encoder", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.index_decoder", "range", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.optimizer.zero_grad", "loss.backward", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.optimizer.step", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.loss_func"], "methods", ["None"], ["", "def", "optimize_parameters", "(", "self", ")", ":", "\n", "        ", "self", ".", "guidance_encoder", ".", "train", "(", ")", "\n", "self", ".", "index_decoder", ".", "train", "(", ")", "\n", "\n", "self", ".", "feature_enc", "=", "self", ".", "guidance_encoder", "(", "self", ".", "feature_t", ")", "\n", "self", ".", "memory_logits_list", "=", "self", ".", "index_decoder", "(", "self", ".", "feature_enc", ")", "\n", "\n", "loss", "=", "0", "\n", "for", "i", "in", "range", "(", "18", ")", ":", "\n", "            ", "loss", "+=", "self", ".", "loss_func", "(", "\n", "self", ".", "memory_logits_list", "[", "i", "]", ",", "\n", "self", ".", "gt_indices_list", "[", "i", "]", ",", "\n", "ignore_index", "=", "-", "1", ")", "\n", "\n", "", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "self", ".", "log_dict", "[", "'loss_total'", "]", "=", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.inference": [[247, 299], ["hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.guidance_encoder.eval", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.index_decoder.eval", "enumerate", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.guidance_encoder.train", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.index_decoder.train", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.feed_data", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.image.size", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.texture_tokens.view", "enumerate", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.get_vis", "torch.full", "torch.full", "torch.full", "torch.full", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.guidance_encoder", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.index_decoder", "min_encodings_indices.view", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.image.size", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.size", "range", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "memory_logits.argmax().view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.gt_indices_list[].size", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.gt_indices_list[].numel", "memory_logits.argmax", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.gt_indices_list[].view"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.feed_data", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.get_vis"], ["", "def", "inference", "(", "self", ",", "data_loader", ",", "save_dir", ")", ":", "\n", "        ", "self", ".", "guidance_encoder", ".", "eval", "(", ")", "\n", "self", ".", "index_decoder", ".", "eval", "(", ")", "\n", "\n", "acc", "=", "0", "\n", "num", "=", "0", "\n", "\n", "for", "_", ",", "data", "in", "enumerate", "(", "data_loader", ")", ":", "\n", "            ", "self", ".", "feed_data", "(", "data", ")", "\n", "img_name", "=", "data", "[", "'img_name'", "]", "\n", "\n", "num", "+=", "self", ".", "image", ".", "size", "(", "0", ")", "\n", "\n", "texture_mask_flatten", "=", "self", ".", "texture_tokens", ".", "view", "(", "-", "1", ")", "\n", "min_encodings_indices_list", "=", "[", "\n", "torch", ".", "full", "(", "\n", "texture_mask_flatten", ".", "size", "(", ")", ",", "\n", "fill_value", "=", "-", "1", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "texture_mask_flatten", ".", "device", ")", "for", "_", "in", "range", "(", "18", ")", "\n", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "self", ".", "feature_enc", "=", "self", ".", "guidance_encoder", "(", "self", ".", "feature_t", ")", "\n", "memory_logits_list", "=", "self", ".", "index_decoder", "(", "self", ".", "feature_enc", ")", "\n", "# memory_indices_pred = memory_logits.argmax(dim=1)", "\n", "", "batch_acc", "=", "0", "\n", "for", "codebook_idx", ",", "memory_logits", "in", "enumerate", "(", "memory_logits_list", ")", ":", "\n", "                ", "region_of_interest", "=", "texture_mask_flatten", "==", "codebook_idx", "\n", "if", "torch", ".", "sum", "(", "region_of_interest", ")", ">", "0", ":", "\n", "                    ", "memory_indices_pred", "=", "memory_logits", ".", "argmax", "(", "dim", "=", "1", ")", ".", "view", "(", "-", "1", ")", "\n", "batch_acc", "+=", "torch", ".", "sum", "(", "\n", "memory_indices_pred", "[", "region_of_interest", "]", "==", "\n", "self", ".", "gt_indices_list", "[", "codebook_idx", "]", ".", "view", "(", "\n", "-", "1", ")", "[", "region_of_interest", "]", ")", "\n", "memory_indices_pred", "=", "memory_indices_pred", "\n", "min_encodings_indices_list", "[", "codebook_idx", "]", "[", "\n", "region_of_interest", "]", "=", "memory_indices_pred", "[", "\n", "region_of_interest", "]", "\n", "", "", "min_encodings_indices_return_list", "=", "[", "\n", "min_encodings_indices", ".", "view", "(", "self", ".", "gt_indices_list", "[", "0", "]", ".", "size", "(", ")", ")", "\n", "for", "min_encodings_indices", "in", "min_encodings_indices_list", "\n", "]", "\n", "batch_acc", "=", "batch_acc", "/", "self", ".", "gt_indices_list", "[", "codebook_idx", "]", ".", "numel", "(", "\n", ")", "*", "self", ".", "image", ".", "size", "(", "0", ")", "\n", "acc", "+=", "batch_acc", "\n", "self", ".", "get_vis", "(", "min_encodings_indices_return_list", ",", "\n", "self", ".", "gt_indices_list", ",", "self", ".", "texture_mask", ",", "\n", "f'{save_dir}/{img_name[0]}'", ")", "\n", "\n", "", "self", ".", "guidance_encoder", ".", "train", "(", ")", "\n", "self", ".", "index_decoder", ".", "train", "(", ")", "\n", "return", "(", "acc", "/", "num", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.load_network": [[300, 309], ["torch.load", "torch.load", "torch.load", "torch.load", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.guidance_encoder.load_state_dict", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.guidance_encoder.eval", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.index_decoder.load_state_dict", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.index_decoder.eval"], "methods", ["None"], ["", "def", "load_network", "(", "self", ")", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "self", ".", "opt", "[", "'pretrained_models'", "]", ")", "\n", "self", ".", "guidance_encoder", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'guidance_encoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "guidance_encoder", ".", "eval", "(", ")", "\n", "\n", "self", ".", "index_decoder", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'index_decoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "index_decoder", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.save_network": [[310, 324], ["hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.guidance_encoder.state_dict", "hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.index_decoder.state_dict", "torch.save", "torch.save", "torch.save", "torch.save"], "methods", ["None"], ["", "def", "save_network", "(", "self", ",", "save_path", ")", ":", "\n", "        ", "\"\"\"Save networks.\n\n        Args:\n            net (nn.Module): Network to be saved.\n            net_label (str): Network label.\n            current_iter (int): Current iter number.\n        \"\"\"", "\n", "\n", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'guidance_encoder'", "]", "=", "self", ".", "guidance_encoder", ".", "state_dict", "(", ")", "\n", "save_dict", "[", "'index_decoder'", "]", "=", "self", ".", "index_decoder", ".", "state_dict", "(", ")", "\n", "\n", "torch", ".", "save", "(", "save_dict", ",", "save_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.update_learning_rate": [[325, 361], ["math.cos", "ValueError", "int"], "methods", ["None"], ["", "def", "update_learning_rate", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "\"\"\"Update learning rate.\n\n        Args:\n            current_iter (int): Current iteration.\n            warmup_iter (int): Warmup iter numbers. -1 for no warmup.\n                Default: -1.\n        \"\"\"", "\n", "lr", "=", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "\n", "if", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'step'", ":", "\n", "            ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "(", "\n", "self", ".", "opt", "[", "'gamma'", "]", "**", "(", "epoch", "//", "self", ".", "opt", "[", "'step'", "]", ")", ")", "\n", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'cos'", ":", "\n", "            ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "(", "\n", "1", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "epoch", "/", "self", ".", "opt", "[", "'num_epochs'", "]", ")", ")", "/", "2", "\n", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'linear'", ":", "\n", "            ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "(", "1", "-", "epoch", "/", "self", ".", "opt", "[", "'num_epochs'", "]", ")", "\n", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'linear2exp'", ":", "\n", "            ", "if", "epoch", "<", "self", ".", "opt", "[", "'turning_point'", "]", "+", "1", ":", "\n", "# learning rate decay as 95%", "\n", "# at the turning point (1 / 95% = 1.0526)", "\n", "                ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "(", "\n", "1", "-", "epoch", "/", "int", "(", "self", ".", "opt", "[", "'turning_point'", "]", "*", "1.0526", ")", ")", "\n", "", "else", ":", "\n", "                ", "lr", "*=", "self", ".", "opt", "[", "'gamma'", "]", "\n", "", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'schedule'", ":", "\n", "            ", "if", "epoch", "in", "self", ".", "opt", "[", "'schedule'", "]", ":", "\n", "                ", "lr", "*=", "self", ".", "opt", "[", "'gamma'", "]", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Unknown lr mode {}'", ".", "format", "(", "self", ".", "opt", "[", "'lr_decay'", "]", ")", ")", "\n", "# set learning rate", "\n", "", "for", "param_group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "            ", "param_group", "[", "'lr'", "]", "=", "lr", "\n", "\n", "", "return", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_inference_model.VQGANTextureAwareSpatialHierarchyInferenceModel.get_current_log": [[362, 364], ["None"], "methods", ["None"], ["", "def", "get_current_log", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "log_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.__init__": [[22, 108], ["torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "models.archs.vqgan_arch.Encoder().to", "models.archs.vqgan_arch.Decoder().to", "models.archs.vqgan_arch.VectorQuantizerTexture().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "transformer_model.TransformerTextureAwareModel.load_pretrained_image_vae", "models.archs.vqgan_arch.Encoder().to", "models.archs.vqgan_arch.VectorQuantizer().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "transformer_model.TransformerTextureAwareModel.load_pretrained_segm_vae", "models.archs.transformer_arch.TransformerMultiHead().to", "tuple", "transformer_model.TransformerTextureAwareModel.init_training_settings", "models.archs.vqgan_arch.Encoder", "models.archs.vqgan_arch.Decoder", "models.archs.vqgan_arch.VectorQuantizerTexture", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "models.archs.vqgan_arch.Encoder", "models.archs.vqgan_arch.VectorQuantizer", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "models.archs.transformer_arch.TransformerMultiHead"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.load_pretrained_image_vae", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.load_pretrained_segm_vae", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.init_training_settings"], ["def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "self", ".", "opt", "=", "opt", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "'cuda'", ")", "\n", "self", ".", "is_train", "=", "opt", "[", "'is_train'", "]", "\n", "\n", "# VQVAE for image", "\n", "self", ".", "img_encoder", "=", "Encoder", "(", "\n", "ch", "=", "opt", "[", "'img_ch'", "]", ",", "\n", "num_res_blocks", "=", "opt", "[", "'img_num_res_blocks'", "]", ",", "\n", "attn_resolutions", "=", "opt", "[", "'img_attn_resolutions'", "]", ",", "\n", "ch_mult", "=", "opt", "[", "'img_ch_mult'", "]", ",", "\n", "in_channels", "=", "opt", "[", "'img_in_channels'", "]", ",", "\n", "resolution", "=", "opt", "[", "'img_resolution'", "]", ",", "\n", "z_channels", "=", "opt", "[", "'img_z_channels'", "]", ",", "\n", "double_z", "=", "opt", "[", "'img_double_z'", "]", ",", "\n", "dropout", "=", "opt", "[", "'img_dropout'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "img_decoder", "=", "Decoder", "(", "\n", "in_channels", "=", "opt", "[", "'img_in_channels'", "]", ",", "\n", "resolution", "=", "opt", "[", "'img_resolution'", "]", ",", "\n", "z_channels", "=", "opt", "[", "'img_z_channels'", "]", ",", "\n", "ch", "=", "opt", "[", "'img_ch'", "]", ",", "\n", "out_ch", "=", "opt", "[", "'img_out_ch'", "]", ",", "\n", "num_res_blocks", "=", "opt", "[", "'img_num_res_blocks'", "]", ",", "\n", "attn_resolutions", "=", "opt", "[", "'img_attn_resolutions'", "]", ",", "\n", "ch_mult", "=", "opt", "[", "'img_ch_mult'", "]", ",", "\n", "dropout", "=", "opt", "[", "'img_dropout'", "]", ",", "\n", "resamp_with_conv", "=", "True", ",", "\n", "give_pre_end", "=", "False", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "img_quantizer", "=", "VectorQuantizerTexture", "(", "\n", "opt", "[", "'img_n_embed'", "]", ",", "opt", "[", "'img_embed_dim'", "]", ",", "\n", "beta", "=", "0.25", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "img_quant_conv", "=", "torch", ".", "nn", ".", "Conv2d", "(", "opt", "[", "\"img_z_channels\"", "]", ",", "\n", "opt", "[", "'img_embed_dim'", "]", ",", "\n", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "img_post_quant_conv", "=", "torch", ".", "nn", ".", "Conv2d", "(", "opt", "[", "'img_embed_dim'", "]", ",", "\n", "opt", "[", "\"img_z_channels\"", "]", ",", "\n", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "load_pretrained_image_vae", "(", ")", "\n", "\n", "# VAE for segmentation mask", "\n", "self", ".", "segm_encoder", "=", "Encoder", "(", "\n", "ch", "=", "opt", "[", "'segm_ch'", "]", ",", "\n", "num_res_blocks", "=", "opt", "[", "'segm_num_res_blocks'", "]", ",", "\n", "attn_resolutions", "=", "opt", "[", "'segm_attn_resolutions'", "]", ",", "\n", "ch_mult", "=", "opt", "[", "'segm_ch_mult'", "]", ",", "\n", "in_channels", "=", "opt", "[", "'segm_in_channels'", "]", ",", "\n", "resolution", "=", "opt", "[", "'segm_resolution'", "]", ",", "\n", "z_channels", "=", "opt", "[", "'segm_z_channels'", "]", ",", "\n", "double_z", "=", "opt", "[", "'segm_double_z'", "]", ",", "\n", "dropout", "=", "opt", "[", "'segm_dropout'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "segm_quantizer", "=", "VectorQuantizer", "(", "\n", "opt", "[", "'segm_n_embed'", "]", ",", "\n", "opt", "[", "'segm_embed_dim'", "]", ",", "\n", "beta", "=", "0.25", ",", "\n", "sane_index_shape", "=", "True", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "segm_quant_conv", "=", "torch", ".", "nn", ".", "Conv2d", "(", "opt", "[", "\"segm_z_channels\"", "]", ",", "\n", "opt", "[", "'segm_embed_dim'", "]", ",", "\n", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "load_pretrained_segm_vae", "(", ")", "\n", "\n", "# define sampler", "\n", "self", ".", "_denoise_fn", "=", "TransformerMultiHead", "(", "\n", "codebook_size", "=", "opt", "[", "'codebook_size'", "]", ",", "\n", "segm_codebook_size", "=", "opt", "[", "'segm_codebook_size'", "]", ",", "\n", "texture_codebook_size", "=", "opt", "[", "'texture_codebook_size'", "]", ",", "\n", "bert_n_emb", "=", "opt", "[", "'bert_n_emb'", "]", ",", "\n", "bert_n_layers", "=", "opt", "[", "'bert_n_layers'", "]", ",", "\n", "bert_n_head", "=", "opt", "[", "'bert_n_head'", "]", ",", "\n", "block_size", "=", "opt", "[", "'block_size'", "]", ",", "\n", "latent_shape", "=", "opt", "[", "'latent_shape'", "]", ",", "\n", "embd_pdrop", "=", "opt", "[", "'embd_pdrop'", "]", ",", "\n", "resid_pdrop", "=", "opt", "[", "'resid_pdrop'", "]", ",", "\n", "attn_pdrop", "=", "opt", "[", "'attn_pdrop'", "]", ",", "\n", "num_head", "=", "opt", "[", "'num_head'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "self", ".", "num_classes", "=", "opt", "[", "'codebook_size'", "]", "\n", "self", ".", "shape", "=", "tuple", "(", "opt", "[", "'latent_shape'", "]", ")", "\n", "self", ".", "num_timesteps", "=", "1000", "\n", "\n", "self", ".", "mask_id", "=", "opt", "[", "'codebook_size'", "]", "\n", "self", ".", "loss_type", "=", "opt", "[", "'loss_type'", "]", "\n", "self", ".", "mask_schedule", "=", "opt", "[", "'mask_schedule'", "]", "\n", "\n", "self", ".", "sample_steps", "=", "opt", "[", "'sample_steps'", "]", "\n", "\n", "self", ".", "init_training_settings", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.load_pretrained_image_vae": [[109, 127], ["torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "transformer_model.TransformerTextureAwareModel.img_encoder.load_state_dict", "transformer_model.TransformerTextureAwareModel.img_decoder.load_state_dict", "transformer_model.TransformerTextureAwareModel.img_quantizer.load_state_dict", "transformer_model.TransformerTextureAwareModel.img_quant_conv.load_state_dict", "transformer_model.TransformerTextureAwareModel.img_post_quant_conv.load_state_dict", "transformer_model.TransformerTextureAwareModel.img_encoder.eval", "transformer_model.TransformerTextureAwareModel.img_decoder.eval", "transformer_model.TransformerTextureAwareModel.img_quantizer.eval", "transformer_model.TransformerTextureAwareModel.img_quant_conv.eval", "transformer_model.TransformerTextureAwareModel.img_post_quant_conv.eval"], "methods", ["None"], ["", "def", "load_pretrained_image_vae", "(", "self", ")", ":", "\n", "# load pretrained vqgan for segmentation mask", "\n", "        ", "img_ae_checkpoint", "=", "torch", ".", "load", "(", "self", ".", "opt", "[", "'img_ae_path'", "]", ")", "\n", "self", ".", "img_encoder", ".", "load_state_dict", "(", "\n", "img_ae_checkpoint", "[", "'encoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "img_decoder", ".", "load_state_dict", "(", "\n", "img_ae_checkpoint", "[", "'decoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "img_quantizer", ".", "load_state_dict", "(", "\n", "img_ae_checkpoint", "[", "'quantize'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "img_quant_conv", ".", "load_state_dict", "(", "\n", "img_ae_checkpoint", "[", "'quant_conv'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "img_post_quant_conv", ".", "load_state_dict", "(", "\n", "img_ae_checkpoint", "[", "'post_quant_conv'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "img_encoder", ".", "eval", "(", ")", "\n", "self", ".", "img_decoder", ".", "eval", "(", ")", "\n", "self", ".", "img_quantizer", ".", "eval", "(", ")", "\n", "self", ".", "img_quant_conv", ".", "eval", "(", ")", "\n", "self", ".", "img_post_quant_conv", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.load_pretrained_segm_vae": [[128, 140], ["torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "transformer_model.TransformerTextureAwareModel.segm_encoder.load_state_dict", "transformer_model.TransformerTextureAwareModel.segm_quantizer.load_state_dict", "transformer_model.TransformerTextureAwareModel.segm_quant_conv.load_state_dict", "transformer_model.TransformerTextureAwareModel.segm_encoder.eval", "transformer_model.TransformerTextureAwareModel.segm_quantizer.eval", "transformer_model.TransformerTextureAwareModel.segm_quant_conv.eval"], "methods", ["None"], ["", "def", "load_pretrained_segm_vae", "(", "self", ")", ":", "\n", "# load pretrained vqgan for segmentation mask", "\n", "        ", "segm_ae_checkpoint", "=", "torch", ".", "load", "(", "self", ".", "opt", "[", "'segm_ae_path'", "]", ")", "\n", "self", ".", "segm_encoder", ".", "load_state_dict", "(", "\n", "segm_ae_checkpoint", "[", "'encoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "segm_quantizer", ".", "load_state_dict", "(", "\n", "segm_ae_checkpoint", "[", "'quantize'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "segm_quant_conv", ".", "load_state_dict", "(", "\n", "segm_ae_checkpoint", "[", "'quant_conv'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "segm_encoder", ".", "eval", "(", ")", "\n", "self", ".", "segm_quantizer", ".", "eval", "(", ")", "\n", "self", ".", "segm_quant_conv", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.init_training_settings": [[141, 152], ["transformer_model.TransformerTextureAwareModel._denoise_fn.parameters", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "collections.OrderedDict", "optim_params.append"], "methods", ["None"], ["", "def", "init_training_settings", "(", "self", ")", ":", "\n", "        ", "optim_params", "=", "[", "]", "\n", "for", "v", "in", "self", ".", "_denoise_fn", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "v", ".", "requires_grad", ":", "\n", "                ", "optim_params", ".", "append", "(", "v", ")", "\n", "# set up optimizer", "\n", "", "", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "\n", "optim_params", ",", "\n", "self", ".", "opt", "[", "'lr'", "]", ",", "\n", "weight_decay", "=", "self", ".", "opt", "[", "'weight_decay'", "]", ")", "\n", "self", ".", "log_dict", "=", "OrderedDict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.get_quantized_img": [[153, 171], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "transformer_model.TransformerTextureAwareModel.img_encoder", "transformer_model.TransformerTextureAwareModel.img_quant_conv", "transformer_model.TransformerTextureAwareModel.img_quantizer", "image.size", "img_tokens_input.view.view.view", "img_tokens_gt.view"], "methods", ["None"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "get_quantized_img", "(", "self", ",", "image", ",", "texture_mask", ")", ":", "\n", "        ", "encoded_img", "=", "self", ".", "img_encoder", "(", "image", ")", "\n", "encoded_img", "=", "self", ".", "img_quant_conv", "(", "encoded_img", ")", "\n", "\n", "# img_tokens_input is the continual index for the input of transformer", "\n", "# img_tokens_gt_list is the index for 18 texture-aware codebooks respectively", "\n", "_", ",", "_", ",", "[", "_", ",", "img_tokens_input", ",", "img_tokens_gt_list", "\n", "]", "=", "self", ".", "img_quantizer", "(", "encoded_img", ",", "texture_mask", ")", "\n", "\n", "# reshape the tokens", "\n", "b", "=", "image", ".", "size", "(", "0", ")", "\n", "img_tokens_input", "=", "img_tokens_input", ".", "view", "(", "b", ",", "-", "1", ")", "\n", "img_tokens_gt_return_list", "=", "[", "\n", "img_tokens_gt", ".", "view", "(", "b", ",", "-", "1", ")", "for", "img_tokens_gt", "in", "img_tokens_gt_list", "\n", "]", "\n", "\n", "return", "img_tokens_input", ",", "img_tokens_gt_return_list", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.decode": [[172, 177], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "transformer_model.TransformerTextureAwareModel.img_post_quant_conv", "transformer_model.TransformerTextureAwareModel.img_decoder"], "methods", ["None"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "decode", "(", "self", ",", "quant", ")", ":", "\n", "        ", "quant", "=", "self", ".", "img_post_quant_conv", "(", "quant", ")", "\n", "dec", "=", "self", ".", "img_decoder", "(", "quant", ")", "\n", "return", "dec", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.decode_image_indices": [[178, 187], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "transformer_model.TransformerTextureAwareModel.img_quantizer.get_codebook_entry", "transformer_model.TransformerTextureAwareModel.decode", "indices_list[].size"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.VectorQuantizerSpatialTextureAware.get_codebook_entry", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.decode"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "decode_image_indices", "(", "self", ",", "indices_list", ",", "texture_mask", ")", ":", "\n", "        ", "quant", "=", "self", ".", "img_quantizer", ".", "get_codebook_entry", "(", "\n", "indices_list", ",", "texture_mask", ",", "\n", "(", "indices_list", "[", "0", "]", ".", "size", "(", "0", ")", ",", "self", ".", "shape", "[", "0", "]", ",", "self", ".", "shape", "[", "1", "]", ",", "\n", "self", ".", "opt", "[", "\"img_z_channels\"", "]", ")", ")", "\n", "dec", "=", "self", ".", "decode", "(", "quant", ")", "\n", "\n", "return", "dec", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.sample_time": [[188, 211], ["torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "pt_all.gather", "transformer_model.TransformerTextureAwareModel.sample_time", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "Lt_sqrt.sum", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.ones_like().float", "torch.ones_like().float", "torch.ones_like().float", "torch.ones_like().float", "torch.ones_like().float", "torch.ones_like().float", "torch.ones_like().float", "torch.ones_like().float", "torch.ones_like().float", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.sample_time"], ["", "def", "sample_time", "(", "self", ",", "b", ",", "device", ",", "method", "=", "'uniform'", ")", ":", "\n", "        ", "if", "method", "==", "'importance'", ":", "\n", "            ", "if", "not", "(", "self", ".", "Lt_count", ">", "10", ")", ".", "all", "(", ")", ":", "\n", "                ", "return", "self", ".", "sample_time", "(", "b", ",", "device", ",", "method", "=", "'uniform'", ")", "\n", "\n", "", "Lt_sqrt", "=", "torch", ".", "sqrt", "(", "self", ".", "Lt_history", "+", "1e-10", ")", "+", "0.0001", "\n", "Lt_sqrt", "[", "0", "]", "=", "Lt_sqrt", "[", "1", "]", "# Overwrite decoder term with L1.", "\n", "pt_all", "=", "Lt_sqrt", "/", "Lt_sqrt", ".", "sum", "(", ")", "\n", "\n", "t", "=", "torch", ".", "multinomial", "(", "pt_all", ",", "num_samples", "=", "b", ",", "replacement", "=", "True", ")", "\n", "\n", "pt", "=", "pt_all", ".", "gather", "(", "dim", "=", "0", ",", "index", "=", "t", ")", "\n", "\n", "return", "t", ",", "pt", "\n", "\n", "", "elif", "method", "==", "'uniform'", ":", "\n", "            ", "t", "=", "torch", ".", "randint", "(", "\n", "1", ",", "self", ".", "num_timesteps", "+", "1", ",", "(", "b", ",", ")", ",", "device", "=", "device", ")", ".", "long", "(", ")", "\n", "pt", "=", "torch", ".", "ones_like", "(", "t", ")", ".", "float", "(", ")", "/", "self", ".", "num_timesteps", "\n", "return", "t", ",", "pt", "\n", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.q_sample": [[212, 231], ["x_0.clone", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "x_0_gt.clone", "x_0_gt_ignore_list.append", "x_0.clone.float", "t.float().unsqueeze", "torch.bitwise_not", "torch.bitwise_not", "torch.bitwise_not", "torch.bitwise_not", "torch.bitwise_not", "torch.bitwise_not", "torch.bitwise_not", "torch.bitwise_not", "torch.bitwise_not", "t.float"], "methods", ["None"], ["", "", "def", "q_sample", "(", "self", ",", "x_0", ",", "x_0_gt_list", ",", "t", ")", ":", "\n", "# samples q(x_t | x_0)", "\n", "# randomly set token to mask with probability t/T", "\n", "# x_t, x_0_ignore = x_0.clone(), x_0.clone()", "\n", "        ", "x_t", "=", "x_0", ".", "clone", "(", ")", "\n", "\n", "mask", "=", "torch", ".", "rand_like", "(", "x_t", ".", "float", "(", ")", ")", "<", "(", "\n", "t", ".", "float", "(", ")", ".", "unsqueeze", "(", "-", "1", ")", "/", "self", ".", "num_timesteps", ")", "\n", "x_t", "[", "mask", "]", "=", "self", ".", "mask_id", "\n", "# x_0_ignore[torch.bitwise_not(mask)] = -1", "\n", "\n", "# for every gt token list, we also need to do the mask", "\n", "x_0_gt_ignore_list", "=", "[", "]", "\n", "for", "x_0_gt", "in", "x_0_gt_list", ":", "\n", "            ", "x_0_gt_ignore", "=", "x_0_gt", ".", "clone", "(", ")", "\n", "x_0_gt_ignore", "[", "torch", ".", "bitwise_not", "(", "mask", ")", "]", "=", "-", "1", "\n", "x_0_gt_ignore_list", ".", "append", "(", "x_0_gt_ignore", ")", "\n", "\n", "", "return", "x_t", ",", "x_0_gt_ignore_list", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel._train_loss": [[232, 275], ["transformer_model.TransformerTextureAwareModel.sample_time", "transformer_model.TransformerTextureAwareModel._denoise_fn", "zip", "x_0.size", "transformer_model.TransformerTextureAwareModel.q_sample", "torch.cross_entropy().sum", "torch.cross_entropy().sum", "torch.cross_entropy().sum", "loss.mean", "vb_loss.mean", "math.log", "x_0.shape[].numel", "mask.float().sum", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "x_0_hat_logits.permute", "mask.float", "math.log", "x_0.shape[].numel"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.sample_time", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.q_sample", "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.cross_entropy_loss.cross_entropy", "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.cross_entropy_loss.cross_entropy", "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.cross_entropy_loss.cross_entropy"], ["", "def", "_train_loss", "(", "self", ",", "x_0", ",", "x_0_gt_list", ")", ":", "\n", "        ", "b", ",", "device", "=", "x_0", ".", "size", "(", "0", ")", ",", "x_0", ".", "device", "\n", "\n", "# choose what time steps to compute loss at", "\n", "t", ",", "pt", "=", "self", ".", "sample_time", "(", "b", ",", "device", ",", "'uniform'", ")", "\n", "\n", "# make x noisy and denoise", "\n", "if", "self", ".", "mask_schedule", "==", "'random'", ":", "\n", "            ", "x_t", ",", "x_0_gt_ignore_list", ",", "mask", "=", "self", ".", "q_sample", "(", "\n", "x_0", "=", "x_0", ",", "x_0_gt_list", "=", "x_0_gt_list", ",", "t", "=", "t", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "# sample p(x_0 | x_t)", "\n", "", "x_0_hat_logits_list", "=", "self", ".", "_denoise_fn", "(", "\n", "x_t", ",", "self", ".", "segm_tokens", ",", "self", ".", "texture_tokens", ",", "t", "=", "t", ")", "\n", "\n", "# Always compute ELBO for comparison purposes", "\n", "cross_entropy_loss", "=", "0", "\n", "for", "x_0_hat_logits", ",", "x_0_gt_ignore", "in", "zip", "(", "x_0_hat_logits_list", ",", "\n", "x_0_gt_ignore_list", ")", ":", "\n", "            ", "cross_entropy_loss", "+=", "F", ".", "cross_entropy", "(", "\n", "x_0_hat_logits", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ",", "\n", "x_0_gt_ignore", ",", "\n", "ignore_index", "=", "-", "1", ",", "\n", "reduction", "=", "'none'", ")", ".", "sum", "(", "1", ")", "\n", "", "vb_loss", "=", "cross_entropy_loss", "/", "t", "\n", "vb_loss", "=", "vb_loss", "/", "pt", "\n", "vb_loss", "=", "vb_loss", "/", "(", "math", ".", "log", "(", "2", ")", "*", "x_0", ".", "shape", "[", "1", ":", "]", ".", "numel", "(", ")", ")", "\n", "if", "self", ".", "loss_type", "==", "'elbo'", ":", "\n", "            ", "loss", "=", "vb_loss", "\n", "", "elif", "self", ".", "loss_type", "==", "'mlm'", ":", "\n", "            ", "denom", "=", "mask", ".", "float", "(", ")", ".", "sum", "(", "1", ")", "\n", "denom", "[", "denom", "==", "0", "]", "=", "1", "# prevent divide by 0 errors.", "\n", "loss", "=", "cross_entropy_loss", "/", "denom", "\n", "", "elif", "self", ".", "loss_type", "==", "'reweighted_elbo'", ":", "\n", "            ", "weight", "=", "(", "1", "-", "(", "t", "/", "self", ".", "num_timesteps", ")", ")", "\n", "loss", "=", "weight", "*", "cross_entropy_loss", "\n", "loss", "=", "loss", "/", "(", "math", ".", "log", "(", "2", ")", "*", "x_0", ".", "shape", "[", "1", ":", "]", ".", "numel", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "\n", "\n", "", "return", "loss", ".", "mean", "(", ")", ",", "vb_loss", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.feed_data": [[276, 289], ["data[].to", "data[].to", "data[].to", "transformer_model.TransformerTextureAwareModel.get_quantized_img", "torch.interpolate().view().long", "torch.interpolate().view().long", "torch.interpolate().view().long", "transformer_model.TransformerTextureAwareModel.get_quantized_segm", "transformer_model.TransformerTextureAwareModel.segm_tokens.view", "transformer_model.TransformerTextureAwareModel.image.size", "torch.interpolate().view", "torch.interpolate().view", "torch.interpolate().view", "transformer_model.TransformerTextureAwareModel.image.size", "torch.interpolate", "torch.interpolate", "torch.interpolate"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.get_quantized_img", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.get_quantized_segm"], ["", "def", "feed_data", "(", "self", ",", "data", ")", ":", "\n", "        ", "self", ".", "image", "=", "data", "[", "'image'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "segm", "=", "data", "[", "'segm'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "texture_mask", "=", "data", "[", "'texture_mask'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "input_indices", ",", "self", ".", "gt_indices_list", "=", "self", ".", "get_quantized_img", "(", "\n", "self", ".", "image", ",", "self", ".", "texture_mask", ")", "\n", "\n", "self", ".", "texture_tokens", "=", "F", ".", "interpolate", "(", "\n", "self", ".", "texture_mask", ",", "size", "=", "self", ".", "shape", ",", "\n", "mode", "=", "'nearest'", ")", ".", "view", "(", "self", ".", "image", ".", "size", "(", "0", ")", ",", "-", "1", ")", ".", "long", "(", ")", "\n", "\n", "self", ".", "segm_tokens", "=", "self", ".", "get_quantized_segm", "(", "self", ".", "segm", ")", "\n", "self", ".", "segm_tokens", "=", "self", ".", "segm_tokens", ".", "view", "(", "self", ".", "image", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.optimize_parameters": [[290, 304], ["transformer_model.TransformerTextureAwareModel._denoise_fn.train", "transformer_model.TransformerTextureAwareModel._train_loss", "transformer_model.TransformerTextureAwareModel.optimizer.zero_grad", "loss.backward", "transformer_model.TransformerTextureAwareModel.optimizer.step", "transformer_model.TransformerTextureAwareModel._denoise_fn.eval"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel._train_loss"], ["", "def", "optimize_parameters", "(", "self", ")", ":", "\n", "        ", "self", ".", "_denoise_fn", ".", "train", "(", ")", "\n", "\n", "loss", ",", "vb_loss", "=", "self", ".", "_train_loss", "(", "self", ".", "input_indices", ",", "\n", "self", ".", "gt_indices_list", ")", "\n", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "self", ".", "log_dict", "[", "'loss'", "]", "=", "loss", "\n", "self", ".", "log_dict", "[", "'vb_loss'", "]", "=", "vb_loss", "\n", "\n", "self", ".", "_denoise_fn", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.get_quantized_segm": [[305, 316], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.one_hot().permute().to().float", "torch.one_hot().permute().to().float", "torch.one_hot().permute().to().float", "transformer_model.TransformerTextureAwareModel.segm_encoder", "transformer_model.TransformerTextureAwareModel.segm_quant_conv", "transformer_model.TransformerTextureAwareModel.segm_quantizer", "torch.one_hot().permute().to", "torch.one_hot().permute().to", "torch.one_hot().permute().to", "torch.one_hot().permute", "torch.one_hot().permute", "torch.one_hot().permute", "torch.one_hot", "torch.one_hot", "torch.one_hot", "segm.squeeze().long", "segm.squeeze"], "methods", ["None"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "get_quantized_segm", "(", "self", ",", "segm", ")", ":", "\n", "        ", "segm_one_hot", "=", "F", ".", "one_hot", "(", "\n", "segm", ".", "squeeze", "(", "1", ")", ".", "long", "(", ")", ",", "\n", "num_classes", "=", "self", ".", "opt", "[", "'segm_num_segm_classes'", "]", ")", ".", "permute", "(", "\n", "0", ",", "3", ",", "1", ",", "2", ")", ".", "to", "(", "memory_format", "=", "torch", ".", "contiguous_format", ")", ".", "float", "(", ")", "\n", "encoded_segm_mask", "=", "self", ".", "segm_encoder", "(", "segm_one_hot", ")", "\n", "encoded_segm_mask", "=", "self", ".", "segm_quant_conv", "(", "encoded_segm_mask", ")", "\n", "_", ",", "_", ",", "[", "_", ",", "_", ",", "segm_tokens", "]", "=", "self", ".", "segm_quantizer", "(", "encoded_segm_mask", ")", "\n", "\n", "return", "segm_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.sample_fn": [[317, 385], ["transformer_model.TransformerTextureAwareModel._denoise_fn.eval", "torch.zeros_like().bool", "torch.zeros_like().bool", "torch.zeros_like().bool", "torch.zeros_like().bool", "torch.zeros_like().bool", "torch.zeros_like().bool", "torch.zeros_like().bool", "torch.zeros_like().bool", "torch.zeros_like().bool", "list", "transformer_model.TransformerTextureAwareModel.texture_tokens.view", "reversed", "transformer_model.TransformerTextureAwareModel._denoise_fn.train", "transformer_model.TransformerTextureAwareModel.image.size", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "range", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "print", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.bitwise_xor", "torch.bitwise_xor", "torch.bitwise_xor", "torch.bitwise_xor", "torch.bitwise_xor", "torch.bitwise_xor", "torch.bitwise_xor", "torch.bitwise_xor", "torch.bitwise_xor", "torch.bitwise_or", "torch.bitwise_or", "torch.bitwise_or", "torch.bitwise_or", "torch.bitwise_or", "torch.bitwise_or", "torch.bitwise_or", "torch.bitwise_or", "torch.bitwise_or", "transformer_model.TransformerTextureAwareModel._denoise_fn", "torch.bitwise_xor.view", "torch.bitwise_xor.view", "torch.bitwise_xor.view", "x_t.view.view.view", "enumerate", "x_t.view.view.view", "min_encodings_indices.view", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "transformer_model.TransformerTextureAwareModel.size", "range", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.full.float().unsqueeze", "torch.full.float().unsqueeze", "torch.full.float().unsqueeze", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.Categorical", "torch.Categorical", "torch.Categorical", "torch.Categorical.sample().long", "x_0_hat.view.view.view", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "numpy.prod", "torch.full.float", "torch.full.float", "torch.full.float", "torch.Categorical.sample"], "methods", ["None"], ["", "def", "sample_fn", "(", "self", ",", "temp", "=", "1.0", ",", "sample_steps", "=", "None", ")", ":", "\n", "        ", "self", ".", "_denoise_fn", ".", "eval", "(", ")", "\n", "\n", "b", ",", "device", "=", "self", ".", "image", ".", "size", "(", "0", ")", ",", "'cuda'", "\n", "x_t", "=", "torch", ".", "ones", "(", "\n", "(", "b", ",", "np", ".", "prod", "(", "self", ".", "shape", ")", ")", ",", "device", "=", "device", ")", ".", "long", "(", ")", "*", "self", ".", "mask_id", "\n", "unmasked", "=", "torch", ".", "zeros_like", "(", "x_t", ",", "device", "=", "device", ")", ".", "bool", "(", ")", "\n", "sample_steps", "=", "list", "(", "range", "(", "1", ",", "sample_steps", "+", "1", ")", ")", "\n", "\n", "texture_mask_flatten", "=", "self", ".", "texture_tokens", ".", "view", "(", "-", "1", ")", "\n", "\n", "# min_encodings_indices_list would be used to visualize the image", "\n", "min_encodings_indices_list", "=", "[", "\n", "torch", ".", "full", "(", "\n", "texture_mask_flatten", ".", "size", "(", ")", ",", "\n", "fill_value", "=", "-", "1", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "texture_mask_flatten", ".", "device", ")", "for", "_", "in", "range", "(", "18", ")", "\n", "]", "\n", "\n", "for", "t", "in", "reversed", "(", "sample_steps", ")", ":", "\n", "            ", "print", "(", "f'Sample timestep {t:4d}'", ",", "end", "=", "'\\r'", ")", "\n", "t", "=", "torch", ".", "full", "(", "(", "b", ",", ")", ",", "t", ",", "device", "=", "device", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "# where to unmask", "\n", "changes", "=", "torch", ".", "rand", "(", "\n", "x_t", ".", "shape", ",", "device", "=", "device", ")", "<", "1", "/", "t", ".", "float", "(", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# don't unmask somewhere already unmasked", "\n", "changes", "=", "torch", ".", "bitwise_xor", "(", "changes", ",", "\n", "torch", ".", "bitwise_and", "(", "changes", ",", "unmasked", ")", ")", "\n", "# update mask with changes", "\n", "unmasked", "=", "torch", ".", "bitwise_or", "(", "unmasked", ",", "changes", ")", "\n", "\n", "x_0_logits_list", "=", "self", ".", "_denoise_fn", "(", "\n", "x_t", ",", "self", ".", "segm_tokens", ",", "self", ".", "texture_tokens", ",", "t", "=", "t", ")", "\n", "\n", "changes_flatten", "=", "changes", ".", "view", "(", "-", "1", ")", "\n", "ori_shape", "=", "x_t", ".", "shape", "# [b, h*w]", "\n", "x_t", "=", "x_t", ".", "view", "(", "-", "1", ")", "# [b*h*w]", "\n", "for", "codebook_idx", ",", "x_0_logits", "in", "enumerate", "(", "x_0_logits_list", ")", ":", "\n", "                ", "if", "torch", ".", "sum", "(", "texture_mask_flatten", "[", "changes_flatten", "]", "==", "\n", "codebook_idx", ")", ">", "0", ":", "\n", "# scale by temperature", "\n", "                    ", "x_0_logits", "=", "x_0_logits", "/", "temp", "\n", "x_0_dist", "=", "dists", ".", "Categorical", "(", "logits", "=", "x_0_logits", ")", "\n", "x_0_hat", "=", "x_0_dist", ".", "sample", "(", ")", ".", "long", "(", ")", "\n", "x_0_hat", "=", "x_0_hat", ".", "view", "(", "-", "1", ")", "\n", "\n", "# only replace the changed indices with corresponding codebook_idx", "\n", "changes_segm", "=", "torch", ".", "bitwise_and", "(", "\n", "changes_flatten", ",", "texture_mask_flatten", "==", "codebook_idx", ")", "\n", "\n", "# x_t would be the input to the transformer, so the index range should be continual one", "\n", "x_t", "[", "changes_segm", "]", "=", "x_0_hat", "[", "\n", "changes_segm", "]", "+", "1024", "*", "codebook_idx", "\n", "min_encodings_indices_list", "[", "codebook_idx", "]", "[", "\n", "changes_segm", "]", "=", "x_0_hat", "[", "changes_segm", "]", "\n", "\n", "", "", "x_t", "=", "x_t", ".", "view", "(", "ori_shape", ")", "# [b, h*w]", "\n", "\n", "", "min_encodings_indices_return_list", "=", "[", "\n", "min_encodings_indices", ".", "view", "(", "ori_shape", ")", "\n", "for", "min_encodings_indices", "in", "min_encodings_indices_list", "\n", "]", "\n", "\n", "self", ".", "_denoise_fn", ".", "train", "(", ")", "\n", "\n", "return", "min_encodings_indices_return_list", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.get_vis": [[386, 400], ["transformer_model.TransformerTextureAwareModel.decode_image_indices", "transformer_model.TransformerTextureAwareModel.decode_image_indices", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "img_cat.clamp_.clamp_.clamp_", "torchvision.utils.save_image", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.decode_image_indices", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.decode_image_indices"], ["", "def", "get_vis", "(", "self", ",", "image", ",", "gt_indices", ",", "predicted_indices", ",", "texture_mask", ",", "\n", "save_path", ")", ":", "\n", "# original image", "\n", "        ", "ori_img", "=", "self", ".", "decode_image_indices", "(", "gt_indices", ",", "texture_mask", ")", "\n", "# pred image", "\n", "pred_img", "=", "self", ".", "decode_image_indices", "(", "predicted_indices", ",", "texture_mask", ")", "\n", "img_cat", "=", "torch", ".", "cat", "(", "[", "\n", "image", ",", "\n", "ori_img", ",", "\n", "pred_img", ",", "\n", "]", ",", "dim", "=", "3", ")", ".", "detach", "(", ")", "\n", "img_cat", "=", "(", "(", "img_cat", "+", "1", ")", "/", "2", ")", "\n", "img_cat", "=", "img_cat", ".", "clamp_", "(", "0", ",", "1", ")", "\n", "save_image", "(", "img_cat", ",", "save_path", ",", "nrow", "=", "1", ",", "padding", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.inference": [[401, 422], ["transformer_model.TransformerTextureAwareModel._denoise_fn.eval", "enumerate", "transformer_model.TransformerTextureAwareModel._denoise_fn.train", "transformer_model.TransformerTextureAwareModel.feed_data", "transformer_model.TransformerTextureAwareModel.image.size", "range", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "transformer_model.TransformerTextureAwareModel.sample_fn", "transformer_model.TransformerTextureAwareModel.get_vis"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.feed_data", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.sample_fn", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.get_vis"], ["", "def", "inference", "(", "self", ",", "data_loader", ",", "save_dir", ")", ":", "\n", "        ", "self", ".", "_denoise_fn", ".", "eval", "(", ")", "\n", "\n", "for", "_", ",", "data", "in", "enumerate", "(", "data_loader", ")", ":", "\n", "            ", "img_name", "=", "data", "[", "'img_name'", "]", "\n", "self", ".", "feed_data", "(", "data", ")", "\n", "b", "=", "self", ".", "image", ".", "size", "(", "0", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "sampled_indices_list", "=", "self", ".", "sample_fn", "(", "\n", "temp", "=", "1", ",", "sample_steps", "=", "self", ".", "sample_steps", ")", "\n", "", "for", "idx", "in", "range", "(", "b", ")", ":", "\n", "                ", "self", ".", "get_vis", "(", "self", ".", "image", "[", "idx", ":", "idx", "+", "1", "]", ",", "[", "\n", "gt_indices", "[", "idx", ":", "idx", "+", "1", "]", "\n", "for", "gt_indices", "in", "self", ".", "gt_indices_list", "\n", "]", ",", "[", "\n", "sampled_indices", "[", "idx", ":", "idx", "+", "1", "]", "\n", "for", "sampled_indices", "in", "sampled_indices_list", "\n", "]", ",", "self", ".", "texture_mask", "[", "idx", ":", "idx", "+", "1", "]", ",", "\n", "f'{save_dir}/{img_name[idx]}'", ")", "\n", "\n", "", "", "self", ".", "_denoise_fn", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.get_current_log": [[423, 425], ["None"], "methods", ["None"], ["", "def", "get_current_log", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "log_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.update_learning_rate": [[426, 467], ["math.cos", "ValueError", "int", "float"], "methods", ["None"], ["", "def", "update_learning_rate", "(", "self", ",", "epoch", ",", "iters", "=", "None", ")", ":", "\n", "        ", "\"\"\"Update learning rate.\n\n        Args:\n            current_iter (int): Current iteration.\n            warmup_iter (int): Warmup iter numbers. -1 for no warmup.\n                Default: -1.\n        \"\"\"", "\n", "lr", "=", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "\n", "if", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'step'", ":", "\n", "            ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "(", "\n", "self", ".", "opt", "[", "'gamma'", "]", "**", "(", "epoch", "//", "self", ".", "opt", "[", "'step'", "]", ")", ")", "\n", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'cos'", ":", "\n", "            ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "(", "\n", "1", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "epoch", "/", "self", ".", "opt", "[", "'num_epochs'", "]", ")", ")", "/", "2", "\n", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'linear'", ":", "\n", "            ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "(", "1", "-", "epoch", "/", "self", ".", "opt", "[", "'num_epochs'", "]", ")", "\n", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'linear2exp'", ":", "\n", "            ", "if", "epoch", "<", "self", ".", "opt", "[", "'turning_point'", "]", "+", "1", ":", "\n", "# learning rate decay as 95%", "\n", "# at the turning point (1 / 95% = 1.0526)", "\n", "                ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "(", "\n", "1", "-", "epoch", "/", "int", "(", "self", ".", "opt", "[", "'turning_point'", "]", "*", "1.0526", ")", ")", "\n", "", "else", ":", "\n", "                ", "lr", "*=", "self", ".", "opt", "[", "'gamma'", "]", "\n", "", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'schedule'", ":", "\n", "            ", "if", "epoch", "in", "self", ".", "opt", "[", "'schedule'", "]", ":", "\n", "                ", "lr", "*=", "self", ".", "opt", "[", "'gamma'", "]", "\n", "", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'warm_up'", ":", "\n", "            ", "if", "iters", "<=", "self", ".", "opt", "[", "'warmup_iters'", "]", ":", "\n", "                ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "float", "(", "iters", ")", "/", "self", ".", "opt", "[", "'warmup_iters'", "]", "\n", "", "else", ":", "\n", "                ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Unknown lr mode {}'", ".", "format", "(", "self", ".", "opt", "[", "'lr_decay'", "]", ")", ")", "\n", "# set learning rate", "\n", "", "for", "param_group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "            ", "param_group", "[", "'lr'", "]", "=", "lr", "\n", "\n", "", "return", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.save_network": [[468, 478], ["net.state_dict", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save"], "methods", ["None"], ["", "def", "save_network", "(", "self", ",", "net", ",", "save_path", ")", ":", "\n", "        ", "\"\"\"Save networks.\n\n        Args:\n            net (nn.Module): Network to be saved.\n            net_label (str): Network label.\n            current_iter (int): Current iter number.\n        \"\"\"", "\n", "state_dict", "=", "net", ".", "state_dict", "(", ")", "\n", "torch", ".", "save", "(", "state_dict", ",", "save_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.transformer_model.TransformerTextureAwareModel.load_network": [[479, 483], ["torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "transformer_model.TransformerTextureAwareModel._denoise_fn.load_state_dict", "transformer_model.TransformerTextureAwareModel._denoise_fn.eval"], "methods", ["None"], ["", "def", "load_network", "(", "self", ")", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "self", ".", "opt", "[", "'pretrained_sampler'", "]", ")", "\n", "self", ".", "_denoise_fn", ".", "load_state_dict", "(", "checkpoint", ",", "strict", "=", "True", ")", "\n", "self", ".", "_denoise_fn", ".", "eval", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.__init__": [[24, 123], ["torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "models.archs.vqgan_arch.Decoder().to", "models.archs.vqgan_arch.VectorQuantizerTexture().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "sample_model.BaseSampleModel.load_top_pretrain_models", "models.archs.vqgan_arch.DecoderRes().to", "models.archs.vqgan_arch.VectorQuantizerSpatialTextureAware().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "sample_model.BaseSampleModel.load_bot_pretrain_network", "models.archs.unet_arch.UNet().to", "models.archs.fcn_arch.MultiHeadFCNHead().to", "sample_model.BaseSampleModel.load_index_pred_network", "models.archs.vqgan_arch.Encoder().to", "models.archs.vqgan_arch.VectorQuantizer().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "sample_model.BaseSampleModel.load_pretrained_segm_token", "models.archs.transformer_arch.TransformerMultiHead().to", "sample_model.BaseSampleModel.load_sampler_pretrained_network", "tuple", "models.archs.vqgan_arch.Decoder", "models.archs.vqgan_arch.VectorQuantizerTexture", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "models.archs.vqgan_arch.DecoderRes", "models.archs.vqgan_arch.VectorQuantizerSpatialTextureAware", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "models.archs.unet_arch.UNet", "models.archs.fcn_arch.MultiHeadFCNHead", "models.archs.vqgan_arch.Encoder", "models.archs.vqgan_arch.VectorQuantizer", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "models.archs.transformer_arch.TransformerMultiHead"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.load_top_pretrain_models", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.load_bot_pretrain_network", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.load_index_pred_network", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.load_pretrained_segm_token", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.load_sampler_pretrained_network"], ["def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "self", ".", "opt", "=", "opt", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "'cuda'", ")", "\n", "\n", "# hierarchical VQVAE", "\n", "self", ".", "decoder", "=", "Decoder", "(", "\n", "in_channels", "=", "opt", "[", "'top_in_channels'", "]", ",", "\n", "resolution", "=", "opt", "[", "'top_resolution'", "]", ",", "\n", "z_channels", "=", "opt", "[", "'top_z_channels'", "]", ",", "\n", "ch", "=", "opt", "[", "'top_ch'", "]", ",", "\n", "out_ch", "=", "opt", "[", "'top_out_ch'", "]", ",", "\n", "num_res_blocks", "=", "opt", "[", "'top_num_res_blocks'", "]", ",", "\n", "attn_resolutions", "=", "opt", "[", "'top_attn_resolutions'", "]", ",", "\n", "ch_mult", "=", "opt", "[", "'top_ch_mult'", "]", ",", "\n", "dropout", "=", "opt", "[", "'top_dropout'", "]", ",", "\n", "resamp_with_conv", "=", "True", ",", "\n", "give_pre_end", "=", "False", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "top_quantize", "=", "VectorQuantizerTexture", "(", "\n", "1024", ",", "opt", "[", "'embed_dim'", "]", ",", "beta", "=", "0.25", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "top_post_quant_conv", "=", "torch", ".", "nn", ".", "Conv2d", "(", "opt", "[", "'embed_dim'", "]", ",", "\n", "opt", "[", "\"top_z_channels\"", "]", ",", "\n", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "load_top_pretrain_models", "(", ")", "\n", "\n", "self", ".", "bot_decoder_res", "=", "DecoderRes", "(", "\n", "in_channels", "=", "opt", "[", "'bot_in_channels'", "]", ",", "\n", "resolution", "=", "opt", "[", "'bot_resolution'", "]", ",", "\n", "z_channels", "=", "opt", "[", "'bot_z_channels'", "]", ",", "\n", "ch", "=", "opt", "[", "'bot_ch'", "]", ",", "\n", "num_res_blocks", "=", "opt", "[", "'bot_num_res_blocks'", "]", ",", "\n", "ch_mult", "=", "opt", "[", "'bot_ch_mult'", "]", ",", "\n", "dropout", "=", "opt", "[", "'bot_dropout'", "]", ",", "\n", "give_pre_end", "=", "False", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "bot_quantize", "=", "VectorQuantizerSpatialTextureAware", "(", "\n", "opt", "[", "'bot_n_embed'", "]", ",", "\n", "opt", "[", "'embed_dim'", "]", ",", "\n", "beta", "=", "0.25", ",", "\n", "spatial_size", "=", "opt", "[", "'bot_codebook_spatial_size'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "bot_post_quant_conv", "=", "torch", ".", "nn", ".", "Conv2d", "(", "opt", "[", "'embed_dim'", "]", ",", "\n", "opt", "[", "\"bot_z_channels\"", "]", ",", "\n", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "load_bot_pretrain_network", "(", ")", "\n", "\n", "# top -> bot prediction", "\n", "self", ".", "index_pred_guidance_encoder", "=", "UNet", "(", "\n", "in_channels", "=", "opt", "[", "'index_pred_encoder_in_channels'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "index_pred_decoder", "=", "MultiHeadFCNHead", "(", "\n", "in_channels", "=", "opt", "[", "'index_pred_fc_in_channels'", "]", ",", "\n", "in_index", "=", "opt", "[", "'index_pred_fc_in_index'", "]", ",", "\n", "channels", "=", "opt", "[", "'index_pred_fc_channels'", "]", ",", "\n", "num_convs", "=", "opt", "[", "'index_pred_fc_num_convs'", "]", ",", "\n", "concat_input", "=", "opt", "[", "'index_pred_fc_concat_input'", "]", ",", "\n", "dropout_ratio", "=", "opt", "[", "'index_pred_fc_dropout_ratio'", "]", ",", "\n", "num_classes", "=", "opt", "[", "'index_pred_fc_num_classes'", "]", ",", "\n", "align_corners", "=", "opt", "[", "'index_pred_fc_align_corners'", "]", ",", "\n", "num_head", "=", "18", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "load_index_pred_network", "(", ")", "\n", "\n", "# VAE for segmentation mask", "\n", "self", ".", "segm_encoder", "=", "Encoder", "(", "\n", "ch", "=", "opt", "[", "'segm_ch'", "]", ",", "\n", "num_res_blocks", "=", "opt", "[", "'segm_num_res_blocks'", "]", ",", "\n", "attn_resolutions", "=", "opt", "[", "'segm_attn_resolutions'", "]", ",", "\n", "ch_mult", "=", "opt", "[", "'segm_ch_mult'", "]", ",", "\n", "in_channels", "=", "opt", "[", "'segm_in_channels'", "]", ",", "\n", "resolution", "=", "opt", "[", "'segm_resolution'", "]", ",", "\n", "z_channels", "=", "opt", "[", "'segm_z_channels'", "]", ",", "\n", "double_z", "=", "opt", "[", "'segm_double_z'", "]", ",", "\n", "dropout", "=", "opt", "[", "'segm_dropout'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "segm_quantizer", "=", "VectorQuantizer", "(", "\n", "opt", "[", "'segm_n_embed'", "]", ",", "\n", "opt", "[", "'segm_embed_dim'", "]", ",", "\n", "beta", "=", "0.25", ",", "\n", "sane_index_shape", "=", "True", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "segm_quant_conv", "=", "torch", ".", "nn", ".", "Conv2d", "(", "opt", "[", "\"segm_z_channels\"", "]", ",", "\n", "opt", "[", "'segm_embed_dim'", "]", ",", "\n", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "load_pretrained_segm_token", "(", ")", "\n", "\n", "# define sampler", "\n", "self", ".", "sampler_fn", "=", "TransformerMultiHead", "(", "\n", "codebook_size", "=", "opt", "[", "'codebook_size'", "]", ",", "\n", "segm_codebook_size", "=", "opt", "[", "'segm_codebook_size'", "]", ",", "\n", "texture_codebook_size", "=", "opt", "[", "'texture_codebook_size'", "]", ",", "\n", "bert_n_emb", "=", "opt", "[", "'bert_n_emb'", "]", ",", "\n", "bert_n_layers", "=", "opt", "[", "'bert_n_layers'", "]", ",", "\n", "bert_n_head", "=", "opt", "[", "'bert_n_head'", "]", ",", "\n", "block_size", "=", "opt", "[", "'block_size'", "]", ",", "\n", "latent_shape", "=", "opt", "[", "'latent_shape'", "]", ",", "\n", "embd_pdrop", "=", "opt", "[", "'embd_pdrop'", "]", ",", "\n", "resid_pdrop", "=", "opt", "[", "'resid_pdrop'", "]", ",", "\n", "attn_pdrop", "=", "opt", "[", "'attn_pdrop'", "]", ",", "\n", "num_head", "=", "opt", "[", "'num_head'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "load_sampler_pretrained_network", "(", ")", "\n", "\n", "self", ".", "shape", "=", "tuple", "(", "opt", "[", "'latent_shape'", "]", ")", "\n", "\n", "self", ".", "mask_id", "=", "opt", "[", "'codebook_size'", "]", "\n", "self", ".", "sample_steps", "=", "opt", "[", "'sample_steps'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.load_top_pretrain_models": [[124, 138], ["torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "sample_model.BaseSampleModel.decoder.load_state_dict", "sample_model.BaseSampleModel.top_quantize.load_state_dict", "sample_model.BaseSampleModel.top_post_quant_conv.load_state_dict", "sample_model.BaseSampleModel.decoder.eval", "sample_model.BaseSampleModel.top_quantize.eval", "sample_model.BaseSampleModel.top_post_quant_conv.eval"], "methods", ["None"], ["", "def", "load_top_pretrain_models", "(", "self", ")", ":", "\n", "# load pretrained vqgan", "\n", "        ", "top_vae_checkpoint", "=", "torch", ".", "load", "(", "self", ".", "opt", "[", "'top_vae_path'", "]", ")", "\n", "\n", "self", ".", "decoder", ".", "load_state_dict", "(", "\n", "top_vae_checkpoint", "[", "'decoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "top_quantize", ".", "load_state_dict", "(", "\n", "top_vae_checkpoint", "[", "'quantize'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "top_post_quant_conv", ".", "load_state_dict", "(", "\n", "top_vae_checkpoint", "[", "'post_quant_conv'", "]", ",", "strict", "=", "True", ")", "\n", "\n", "self", ".", "decoder", ".", "eval", "(", ")", "\n", "self", ".", "top_quantize", ".", "eval", "(", ")", "\n", "self", ".", "top_post_quant_conv", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.load_bot_pretrain_network": [[139, 153], ["torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "sample_model.BaseSampleModel.bot_decoder_res.load_state_dict", "sample_model.BaseSampleModel.decoder.load_state_dict", "sample_model.BaseSampleModel.bot_quantize.load_state_dict", "sample_model.BaseSampleModel.bot_post_quant_conv.load_state_dict", "sample_model.BaseSampleModel.bot_decoder_res.eval", "sample_model.BaseSampleModel.decoder.eval", "sample_model.BaseSampleModel.bot_quantize.eval", "sample_model.BaseSampleModel.bot_post_quant_conv.eval"], "methods", ["None"], ["", "def", "load_bot_pretrain_network", "(", "self", ")", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "self", ".", "opt", "[", "'bot_vae_path'", "]", ")", "\n", "self", ".", "bot_decoder_res", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'bot_decoder_res'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "decoder", ".", "load_state_dict", "(", "checkpoint", "[", "'decoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "bot_quantize", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'bot_quantize'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "bot_post_quant_conv", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'bot_post_quant_conv'", "]", ",", "strict", "=", "True", ")", "\n", "\n", "self", ".", "bot_decoder_res", ".", "eval", "(", ")", "\n", "self", ".", "decoder", ".", "eval", "(", ")", "\n", "self", ".", "bot_quantize", ".", "eval", "(", ")", "\n", "self", ".", "bot_post_quant_conv", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.load_pretrained_segm_token": [[154, 167], ["torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "sample_model.BaseSampleModel.segm_encoder.load_state_dict", "sample_model.BaseSampleModel.segm_quantizer.load_state_dict", "sample_model.BaseSampleModel.segm_quant_conv.load_state_dict", "sample_model.BaseSampleModel.segm_encoder.eval", "sample_model.BaseSampleModel.segm_quantizer.eval", "sample_model.BaseSampleModel.segm_quant_conv.eval"], "methods", ["None"], ["", "def", "load_pretrained_segm_token", "(", "self", ")", ":", "\n", "# load pretrained vqgan for segmentation mask", "\n", "        ", "segm_token_checkpoint", "=", "torch", ".", "load", "(", "self", ".", "opt", "[", "'segm_token_path'", "]", ")", "\n", "self", ".", "segm_encoder", ".", "load_state_dict", "(", "\n", "segm_token_checkpoint", "[", "'encoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "segm_quantizer", ".", "load_state_dict", "(", "\n", "segm_token_checkpoint", "[", "'quantize'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "segm_quant_conv", ".", "load_state_dict", "(", "\n", "segm_token_checkpoint", "[", "'quant_conv'", "]", ",", "strict", "=", "True", ")", "\n", "\n", "self", ".", "segm_encoder", ".", "eval", "(", ")", "\n", "self", ".", "segm_quantizer", ".", "eval", "(", ")", "\n", "self", ".", "segm_quant_conv", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.load_index_pred_network": [[168, 177], ["torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "sample_model.BaseSampleModel.index_pred_guidance_encoder.load_state_dict", "sample_model.BaseSampleModel.index_pred_decoder.load_state_dict", "sample_model.BaseSampleModel.index_pred_guidance_encoder.eval", "sample_model.BaseSampleModel.index_pred_decoder.eval"], "methods", ["None"], ["", "def", "load_index_pred_network", "(", "self", ")", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "self", ".", "opt", "[", "'pretrained_index_network'", "]", ")", "\n", "self", ".", "index_pred_guidance_encoder", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'guidance_encoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "index_pred_decoder", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'index_decoder'", "]", ",", "strict", "=", "True", ")", "\n", "\n", "self", ".", "index_pred_guidance_encoder", ".", "eval", "(", ")", "\n", "self", ".", "index_pred_decoder", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.load_sampler_pretrained_network": [[178, 182], ["torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "sample_model.BaseSampleModel.sampler_fn.load_state_dict", "sample_model.BaseSampleModel.sampler_fn.eval"], "methods", ["None"], ["", "def", "load_sampler_pretrained_network", "(", "self", ")", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "self", ".", "opt", "[", "'pretrained_sampler'", "]", ")", "\n", "self", ".", "sampler_fn", ".", "load_state_dict", "(", "checkpoint", ",", "strict", "=", "True", ")", "\n", "self", ".", "sampler_fn", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.bot_index_prediction": [[183, 216], ["sample_model.BaseSampleModel.index_pred_guidance_encoder.eval", "sample_model.BaseSampleModel.index_pred_decoder.eval", "torch.interpolate().view().long", "torch.interpolate().view().long", "torch.interpolate().view().long", "torch.interpolate().view().long.view", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "sample_model.BaseSampleModel.index_pred_guidance_encoder", "sample_model.BaseSampleModel.index_pred_decoder", "enumerate", "torch.interpolate().view", "torch.interpolate().view", "torch.interpolate().view", "F.interpolate().view().long.view.size", "range", "min_encodings_indices.view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "memory_logits.argmax().view", "torch.interpolate", "torch.interpolate", "torch.interpolate", "memory_logits.argmax"], "methods", ["None"], ["", "def", "bot_index_prediction", "(", "self", ",", "feature_top", ",", "texture_mask", ")", ":", "\n", "        ", "self", ".", "index_pred_guidance_encoder", ".", "eval", "(", ")", "\n", "self", ".", "index_pred_decoder", ".", "eval", "(", ")", "\n", "\n", "texture_tokens", "=", "F", ".", "interpolate", "(", "\n", "texture_mask", ",", "(", "32", ",", "16", ")", ",", "mode", "=", "'nearest'", ")", ".", "view", "(", "self", ".", "batch_size", ",", "\n", "-", "1", ")", ".", "long", "(", ")", "\n", "\n", "texture_mask_flatten", "=", "texture_tokens", ".", "view", "(", "-", "1", ")", "\n", "min_encodings_indices_list", "=", "[", "\n", "torch", ".", "full", "(", "\n", "texture_mask_flatten", ".", "size", "(", ")", ",", "\n", "fill_value", "=", "-", "1", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "texture_mask_flatten", ".", "device", ")", "for", "_", "in", "range", "(", "18", ")", "\n", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "feature_enc", "=", "self", ".", "index_pred_guidance_encoder", "(", "feature_top", ")", "\n", "memory_logits_list", "=", "self", ".", "index_pred_decoder", "(", "feature_enc", ")", "\n", "for", "codebook_idx", ",", "memory_logits", "in", "enumerate", "(", "memory_logits_list", ")", ":", "\n", "                ", "region_of_interest", "=", "texture_mask_flatten", "==", "codebook_idx", "\n", "if", "torch", ".", "sum", "(", "region_of_interest", ")", ">", "0", ":", "\n", "                    ", "memory_indices_pred", "=", "memory_logits", ".", "argmax", "(", "dim", "=", "1", ")", ".", "view", "(", "-", "1", ")", "\n", "memory_indices_pred", "=", "memory_indices_pred", "\n", "min_encodings_indices_list", "[", "codebook_idx", "]", "[", "\n", "region_of_interest", "]", "=", "memory_indices_pred", "[", "\n", "region_of_interest", "]", "\n", "", "", "min_encodings_indices_return_list", "=", "[", "\n", "min_encodings_indices", ".", "view", "(", "(", "1", ",", "32", ",", "16", ")", ")", "\n", "for", "min_encodings_indices", "in", "min_encodings_indices_list", "\n", "]", "\n", "\n", "", "return", "min_encodings_indices_return_list", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.sample_and_refine": [[217, 257], ["sample_model.BaseSampleModel.sample_fn", "range", "sample_model.BaseSampleModel.top_quantize.get_codebook_entry", "sample_model.BaseSampleModel.top_post_quant_conv", "sample_model.BaseSampleModel.bot_index_prediction", "sample_model.BaseSampleModel.bot_quantize.get_codebook_entry", "sample_model.BaseSampleModel.bot_post_quant_conv", "sample_model.BaseSampleModel.bot_decoder_res", "sample_model.BaseSampleModel.decoder", "dec.clamp_.clamp_.clamp_", "torchvision.utils.save_image", "sample_indices[].size", "bot_indices_list[].size", "bot_indices_list[].size", "bot_indices_list[].size"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.sample_fn", "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.VectorQuantizerSpatialTextureAware.get_codebook_entry", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.bot_index_prediction", "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.VectorQuantizerSpatialTextureAware.get_codebook_entry"], ["", "def", "sample_and_refine", "(", "self", ",", "save_dir", "=", "None", ",", "img_name", "=", "None", ")", ":", "\n", "# sample 32x16 features indices", "\n", "        ", "sampled_top_indices_list", "=", "self", ".", "sample_fn", "(", "\n", "temp", "=", "1", ",", "sample_steps", "=", "self", ".", "sample_steps", ")", "\n", "\n", "for", "sample_idx", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "            ", "sample_indices", "=", "[", "\n", "sampled_indices_cur", "[", "sample_idx", ":", "sample_idx", "+", "1", "]", "\n", "for", "sampled_indices_cur", "in", "sampled_top_indices_list", "\n", "]", "\n", "top_quant", "=", "self", ".", "top_quantize", ".", "get_codebook_entry", "(", "\n", "sample_indices", ",", "self", ".", "texture_mask", "[", "sample_idx", ":", "sample_idx", "+", "1", "]", ",", "\n", "(", "sample_indices", "[", "0", "]", ".", "size", "(", "0", ")", ",", "self", ".", "shape", "[", "0", "]", ",", "self", ".", "shape", "[", "1", "]", ",", "\n", "self", ".", "opt", "[", "\"top_z_channels\"", "]", ")", ")", "\n", "\n", "top_quant", "=", "self", ".", "top_post_quant_conv", "(", "top_quant", ")", "\n", "\n", "bot_indices_list", "=", "self", ".", "bot_index_prediction", "(", "\n", "top_quant", ",", "self", ".", "texture_mask", "[", "sample_idx", ":", "sample_idx", "+", "1", "]", ")", "\n", "\n", "quant_bot", "=", "self", ".", "bot_quantize", ".", "get_codebook_entry", "(", "\n", "bot_indices_list", ",", "self", ".", "texture_mask", "[", "sample_idx", ":", "sample_idx", "+", "1", "]", ",", "\n", "(", "bot_indices_list", "[", "0", "]", ".", "size", "(", "0", ")", ",", "bot_indices_list", "[", "0", "]", ".", "size", "(", "1", ")", ",", "\n", "bot_indices_list", "[", "0", "]", ".", "size", "(", "2", ")", ",", "\n", "self", ".", "opt", "[", "\"bot_z_channels\"", "]", ")", ")", "#.permute(0, 3, 1, 2)", "\n", "quant_bot", "=", "self", ".", "bot_post_quant_conv", "(", "quant_bot", ")", "\n", "bot_dec_res", "=", "self", ".", "bot_decoder_res", "(", "quant_bot", ")", "\n", "\n", "dec", "=", "self", ".", "decoder", "(", "top_quant", ",", "bot_h", "=", "bot_dec_res", ")", "\n", "\n", "dec", "=", "(", "(", "dec", "+", "1", ")", "/", "2", ")", "\n", "dec", "=", "dec", ".", "clamp_", "(", "0", ",", "1", ")", "\n", "if", "save_dir", "is", "None", "and", "img_name", "is", "None", ":", "\n", "                ", "return", "dec", "\n", "", "else", ":", "\n", "                ", "save_image", "(", "\n", "dec", ",", "\n", "f'{save_dir}/{img_name[sample_idx]}'", ",", "\n", "nrow", "=", "1", ",", "\n", "padding", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.sample_fn": [[258, 331], ["sample_model.BaseSampleModel.sampler_fn.eval", "torch.zeros_like().bool", "torch.zeros_like().bool", "torch.zeros_like().bool", "torch.zeros_like().bool", "torch.zeros_like().bool", "torch.zeros_like().bool", "torch.zeros_like().bool", "torch.zeros_like().bool", "torch.zeros_like().bool", "list", "torch.interpolate().view().long", "torch.interpolate().view().long", "torch.interpolate().view().long", "torch.interpolate().view().long.view", "reversed", "sample_model.BaseSampleModel.sampler_fn.train", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "range", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.bitwise_xor", "torch.bitwise_xor", "torch.bitwise_xor", "torch.bitwise_xor", "torch.bitwise_xor", "torch.bitwise_xor", "torch.bitwise_xor", "torch.bitwise_xor", "torch.bitwise_xor", "torch.bitwise_or", "torch.bitwise_or", "torch.bitwise_or", "torch.bitwise_or", "torch.bitwise_or", "torch.bitwise_or", "torch.bitwise_or", "torch.bitwise_or", "torch.bitwise_or", "sample_model.BaseSampleModel.sampler_fn", "torch.bitwise_xor.view", "torch.bitwise_xor.view", "torch.bitwise_xor.view", "x_t.view.view.view", "enumerate", "x_t.view.view.view", "min_encodings_indices.view", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.interpolate().view", "torch.interpolate().view", "torch.interpolate().view", "F.interpolate().view().long.view.size", "range", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.full.float().unsqueeze", "torch.full.float().unsqueeze", "torch.full.float().unsqueeze", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.Categorical", "torch.Categorical", "torch.Categorical", "torch.Categorical.sample().long", "x_0_hat.view.view.view", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.interpolate", "torch.interpolate", "torch.interpolate", "numpy.prod", "torch.full.float", "torch.full.float", "torch.full.float", "torch.Categorical.sample"], "methods", ["None"], ["", "", "", "def", "sample_fn", "(", "self", ",", "temp", "=", "1.0", ",", "sample_steps", "=", "None", ")", ":", "\n", "        ", "self", ".", "sampler_fn", ".", "eval", "(", ")", "\n", "\n", "x_t", "=", "torch", ".", "ones", "(", "(", "self", ".", "batch_size", ",", "np", ".", "prod", "(", "self", ".", "shape", ")", ")", ",", "\n", "device", "=", "self", ".", "device", ")", ".", "long", "(", ")", "*", "self", ".", "mask_id", "\n", "unmasked", "=", "torch", ".", "zeros_like", "(", "x_t", ",", "device", "=", "self", ".", "device", ")", ".", "bool", "(", ")", "\n", "sample_steps", "=", "list", "(", "range", "(", "1", ",", "sample_steps", "+", "1", ")", ")", "\n", "\n", "texture_tokens", "=", "F", ".", "interpolate", "(", "\n", "self", ".", "texture_mask", ",", "(", "32", ",", "16", ")", ",", "\n", "mode", "=", "'nearest'", ")", ".", "view", "(", "self", ".", "batch_size", ",", "-", "1", ")", ".", "long", "(", ")", "\n", "\n", "texture_mask_flatten", "=", "texture_tokens", ".", "view", "(", "-", "1", ")", "\n", "\n", "# min_encodings_indices_list would be used to visualize the image", "\n", "min_encodings_indices_list", "=", "[", "\n", "torch", ".", "full", "(", "\n", "texture_mask_flatten", ".", "size", "(", ")", ",", "\n", "fill_value", "=", "-", "1", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "texture_mask_flatten", ".", "device", ")", "for", "_", "in", "range", "(", "18", ")", "\n", "]", "\n", "\n", "for", "t", "in", "reversed", "(", "sample_steps", ")", ":", "\n", "            ", "t", "=", "torch", ".", "full", "(", "(", "self", ".", "batch_size", ",", ")", ",", "\n", "t", ",", "\n", "device", "=", "self", ".", "device", ",", "\n", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "# where to unmask", "\n", "changes", "=", "torch", ".", "rand", "(", "\n", "x_t", ".", "shape", ",", "device", "=", "self", ".", "device", ")", "<", "1", "/", "t", ".", "float", "(", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# don't unmask somewhere already unmasked", "\n", "changes", "=", "torch", ".", "bitwise_xor", "(", "changes", ",", "\n", "torch", ".", "bitwise_and", "(", "changes", ",", "unmasked", ")", ")", "\n", "# update mask with changes", "\n", "unmasked", "=", "torch", ".", "bitwise_or", "(", "unmasked", ",", "changes", ")", "\n", "\n", "x_0_logits_list", "=", "self", ".", "sampler_fn", "(", "\n", "x_t", ",", "self", ".", "segm_tokens", ",", "texture_tokens", ",", "t", "=", "t", ")", "\n", "\n", "changes_flatten", "=", "changes", ".", "view", "(", "-", "1", ")", "\n", "ori_shape", "=", "x_t", ".", "shape", "# [b, h*w]", "\n", "x_t", "=", "x_t", ".", "view", "(", "-", "1", ")", "# [b*h*w]", "\n", "for", "codebook_idx", ",", "x_0_logits", "in", "enumerate", "(", "x_0_logits_list", ")", ":", "\n", "                ", "if", "torch", ".", "sum", "(", "texture_mask_flatten", "[", "changes_flatten", "]", "==", "\n", "codebook_idx", ")", ">", "0", ":", "\n", "# scale by temperature", "\n", "                    ", "x_0_logits", "=", "x_0_logits", "/", "temp", "\n", "x_0_dist", "=", "dists", ".", "Categorical", "(", "logits", "=", "x_0_logits", ")", "\n", "x_0_hat", "=", "x_0_dist", ".", "sample", "(", ")", ".", "long", "(", ")", "\n", "x_0_hat", "=", "x_0_hat", ".", "view", "(", "-", "1", ")", "\n", "\n", "# only replace the changed indices with corresponding codebook_idx", "\n", "changes_segm", "=", "torch", ".", "bitwise_and", "(", "\n", "changes_flatten", ",", "texture_mask_flatten", "==", "codebook_idx", ")", "\n", "\n", "# x_t would be the input to the transformer, so the index range should be continual one", "\n", "x_t", "[", "changes_segm", "]", "=", "x_0_hat", "[", "\n", "changes_segm", "]", "+", "1024", "*", "codebook_idx", "\n", "min_encodings_indices_list", "[", "codebook_idx", "]", "[", "\n", "changes_segm", "]", "=", "x_0_hat", "[", "changes_segm", "]", "\n", "\n", "", "", "x_t", "=", "x_t", ".", "view", "(", "ori_shape", ")", "# [b, h*w]", "\n", "\n", "", "min_encodings_indices_return_list", "=", "[", "\n", "min_encodings_indices", ".", "view", "(", "ori_shape", ")", "\n", "for", "min_encodings_indices", "in", "min_encodings_indices_list", "\n", "]", "\n", "\n", "self", ".", "sampler_fn", ".", "train", "(", ")", "\n", "\n", "return", "min_encodings_indices_return_list", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.get_quantized_segm": [[332, 343], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.one_hot().permute().to().float", "torch.one_hot().permute().to().float", "torch.one_hot().permute().to().float", "sample_model.BaseSampleModel.segm_encoder", "sample_model.BaseSampleModel.segm_quant_conv", "sample_model.BaseSampleModel.segm_quantizer", "torch.one_hot().permute().to", "torch.one_hot().permute().to", "torch.one_hot().permute().to", "torch.one_hot().permute", "torch.one_hot().permute", "torch.one_hot().permute", "torch.one_hot", "torch.one_hot", "torch.one_hot", "segm.squeeze().long", "segm.squeeze"], "methods", ["None"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "get_quantized_segm", "(", "self", ",", "segm", ")", ":", "\n", "        ", "segm_one_hot", "=", "F", ".", "one_hot", "(", "\n", "segm", ".", "squeeze", "(", "1", ")", ".", "long", "(", ")", ",", "\n", "num_classes", "=", "self", ".", "opt", "[", "'segm_num_segm_classes'", "]", ")", ".", "permute", "(", "\n", "0", ",", "3", ",", "1", ",", "2", ")", ".", "to", "(", "memory_format", "=", "torch", ".", "contiguous_format", ")", ".", "float", "(", ")", "\n", "encoded_segm_mask", "=", "self", ".", "segm_encoder", "(", "segm_one_hot", ")", "\n", "encoded_segm_mask", "=", "self", ".", "segm_quant_conv", "(", "encoded_segm_mask", ")", "\n", "_", ",", "_", ",", "[", "_", ",", "_", ",", "segm_tokens", "]", "=", "self", ".", "segm_quantizer", "(", "encoded_segm_mask", ")", "\n", "\n", "return", "segm_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromParsingModel.feed_data": [[349, 356], ["data[].to", "data[].to", "sample_model.SampleFromParsingModel.segm.size", "sample_model.SampleFromParsingModel.get_quantized_segm", "sample_model.SampleFromParsingModel.segm_tokens.view"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.get_quantized_segm"], ["def", "feed_data", "(", "self", ",", "data", ")", ":", "\n", "        ", "self", ".", "segm", "=", "data", "[", "'segm'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "texture_mask", "=", "data", "[", "'texture_mask'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "batch_size", "=", "self", ".", "segm", ".", "size", "(", "0", ")", "\n", "\n", "self", ".", "segm_tokens", "=", "self", ".", "get_quantized_segm", "(", "self", ".", "segm", ")", "\n", "self", ".", "segm_tokens", "=", "self", ".", "segm_tokens", ".", "view", "(", "self", ".", "batch_size", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromParsingModel.inference": [[357, 363], ["enumerate", "sample_model.SampleFromParsingModel.feed_data", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "sample_model.SampleFromParsingModel.sample_and_refine"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.feed_data", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.sample_and_refine"], ["", "def", "inference", "(", "self", ",", "data_loader", ",", "save_dir", ")", ":", "\n", "        ", "for", "_", ",", "data", "in", "enumerate", "(", "data_loader", ")", ":", "\n", "            ", "img_name", "=", "data", "[", "'img_name'", "]", "\n", "self", ".", "feed_data", "(", "data", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "self", ".", "sample_and_refine", "(", "save_dir", ",", "img_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.__init__": [[369, 398], ["sample_model.BaseSampleModel.__init__", "models.archs.shape_attr_embedding_arch.ShapeAttrEmbedding().to", "models.archs.unet_arch.ShapeUNet().to", "models.archs.fcn_arch.FCNHead().to", "sample_model.SampleFromPoseModel.load_shape_generation_models", "models.archs.shape_attr_embedding_arch.ShapeAttrEmbedding", "models.archs.unet_arch.ShapeUNet", "models.archs.fcn_arch.FCNHead"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.load_shape_generation_models"], ["def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "opt", ")", "\n", "# pose-to-parsing", "\n", "self", ".", "shape_attr_embedder", "=", "ShapeAttrEmbedding", "(", "\n", "dim", "=", "opt", "[", "'shape_embedder_dim'", "]", ",", "\n", "out_dim", "=", "opt", "[", "'shape_embedder_out_dim'", "]", ",", "\n", "cls_num_list", "=", "opt", "[", "'shape_attr_class_num'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "shape_parsing_encoder", "=", "ShapeUNet", "(", "\n", "in_channels", "=", "opt", "[", "'shape_encoder_in_channels'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "shape_parsing_decoder", "=", "FCNHead", "(", "\n", "in_channels", "=", "opt", "[", "'shape_fc_in_channels'", "]", ",", "\n", "in_index", "=", "opt", "[", "'shape_fc_in_index'", "]", ",", "\n", "channels", "=", "opt", "[", "'shape_fc_channels'", "]", ",", "\n", "num_convs", "=", "opt", "[", "'shape_fc_num_convs'", "]", ",", "\n", "concat_input", "=", "opt", "[", "'shape_fc_concat_input'", "]", ",", "\n", "dropout_ratio", "=", "opt", "[", "'shape_fc_dropout_ratio'", "]", ",", "\n", "num_classes", "=", "opt", "[", "'shape_fc_num_classes'", "]", ",", "\n", "align_corners", "=", "opt", "[", "'shape_fc_align_corners'", "]", ",", "\n", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "load_shape_generation_models", "(", ")", "\n", "\n", "self", ".", "palette", "=", "[", "[", "0", ",", "0", ",", "0", "]", ",", "[", "255", ",", "250", ",", "250", "]", ",", "[", "220", ",", "220", ",", "220", "]", ",", "\n", "[", "250", ",", "235", ",", "215", "]", ",", "[", "255", ",", "250", ",", "205", "]", ",", "[", "211", ",", "211", ",", "211", "]", ",", "\n", "[", "70", ",", "130", ",", "180", "]", ",", "[", "127", ",", "255", ",", "212", "]", ",", "[", "0", ",", "100", ",", "0", "]", ",", "\n", "[", "50", ",", "205", ",", "50", "]", ",", "[", "255", ",", "255", ",", "0", "]", ",", "[", "245", ",", "222", ",", "179", "]", ",", "\n", "[", "255", ",", "140", ",", "0", "]", ",", "[", "255", ",", "0", ",", "0", "]", ",", "[", "16", ",", "78", ",", "139", "]", ",", "\n", "[", "144", ",", "238", ",", "144", "]", ",", "[", "50", ",", "205", ",", "174", "]", ",", "[", "50", ",", "155", ",", "250", "]", ",", "\n", "[", "160", ",", "140", ",", "88", "]", ",", "[", "213", ",", "140", ",", "88", "]", ",", "[", "90", ",", "140", ",", "90", "]", ",", "\n", "[", "185", ",", "210", ",", "205", "]", ",", "[", "130", ",", "165", ",", "180", "]", ",", "[", "225", ",", "141", ",", "151", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.load_shape_generation_models": [[399, 413], ["torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "sample_model.SampleFromPoseModel.shape_attr_embedder.load_state_dict", "sample_model.SampleFromPoseModel.shape_attr_embedder.eval", "sample_model.SampleFromPoseModel.shape_parsing_encoder.load_state_dict", "sample_model.SampleFromPoseModel.shape_parsing_encoder.eval", "sample_model.SampleFromPoseModel.shape_parsing_decoder.load_state_dict", "sample_model.SampleFromPoseModel.shape_parsing_decoder.eval"], "methods", ["None"], ["", "def", "load_shape_generation_models", "(", "self", ")", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "self", ".", "opt", "[", "'pretrained_parsing_gen'", "]", ")", "\n", "\n", "self", ".", "shape_attr_embedder", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'embedder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "shape_attr_embedder", ".", "eval", "(", ")", "\n", "\n", "self", ".", "shape_parsing_encoder", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'encoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "shape_parsing_encoder", ".", "eval", "(", ")", "\n", "\n", "self", ".", "shape_parsing_decoder", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'decoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "shape_parsing_decoder", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.feed_data": [[414, 422], ["data[].to", "sample_model.SampleFromPoseModel.pose.size", "data[].to", "data[].to", "data[].to", "data[].to"], "methods", ["None"], ["", "def", "feed_data", "(", "self", ",", "data", ")", ":", "\n", "        ", "self", ".", "pose", "=", "data", "[", "'densepose'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "batch_size", "=", "self", ".", "pose", ".", "size", "(", "0", ")", "\n", "\n", "self", ".", "shape_attr", "=", "data", "[", "'shape_attr'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "upper_fused_attr", "=", "data", "[", "'upper_fused_attr'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "lower_fused_attr", "=", "data", "[", "'lower_fused_attr'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "outer_fused_attr", "=", "data", "[", "'outer_fused_attr'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.inference": [[423, 432], ["enumerate", "sample_model.SampleFromPoseModel.feed_data", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "sample_model.SampleFromPoseModel.generate_parsing_map", "sample_model.SampleFromPoseModel.generate_quantized_segm", "sample_model.SampleFromPoseModel.generate_texture_map", "sample_model.SampleFromPoseModel.sample_and_refine"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.feed_data", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.generate_parsing_map", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.generate_quantized_segm", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.generate_texture_map", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.sample_and_refine"], ["", "def", "inference", "(", "self", ",", "data_loader", ",", "save_dir", ")", ":", "\n", "        ", "for", "_", ",", "data", "in", "enumerate", "(", "data_loader", ")", ":", "\n", "            ", "img_name", "=", "data", "[", "'img_name'", "]", "\n", "self", ".", "feed_data", "(", "data", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "self", ".", "generate_parsing_map", "(", ")", "\n", "self", ".", "generate_quantized_segm", "(", ")", "\n", "self", ".", "generate_texture_map", "(", ")", "\n", "self", ".", "sample_and_refine", "(", "save_dir", ",", "img_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.generate_parsing_map": [[433, 440], ["sample_model.SampleFromPoseModel.argmax", "sample_model.SampleFromPoseModel.segm.unsqueeze", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "sample_model.SampleFromPoseModel.shape_attr_embedder", "sample_model.SampleFromPoseModel.shape_parsing_encoder", "sample_model.SampleFromPoseModel.shape_parsing_decoder"], "methods", ["None"], ["", "", "", "def", "generate_parsing_map", "(", "self", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "attr_embedding", "=", "self", ".", "shape_attr_embedder", "(", "self", ".", "shape_attr", ")", "\n", "pose_enc", "=", "self", ".", "shape_parsing_encoder", "(", "self", ".", "pose", ",", "attr_embedding", ")", "\n", "seg_logits", "=", "self", ".", "shape_parsing_decoder", "(", "pose_enc", ")", "\n", "", "self", ".", "segm", "=", "seg_logits", ".", "argmax", "(", "dim", "=", "1", ")", "\n", "self", ".", "segm", "=", "self", ".", "segm", ".", "unsqueeze", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.generate_quantized_segm": [[441, 444], ["sample_model.SampleFromPoseModel.get_quantized_segm", "sample_model.SampleFromPoseModel.segm_tokens.view"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.BaseSampleModel.get_quantized_segm"], ["", "def", "generate_quantized_segm", "(", "self", ")", ":", "\n", "        ", "self", ".", "segm_tokens", "=", "self", ".", "get_quantized_segm", "(", "self", ".", "segm", ")", "\n", "self", ".", "segm_tokens", "=", "self", ".", "segm_tokens", ".", "view", "(", "self", ".", "batch_size", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.generate_texture_map": [[445, 470], ["range", "torch.stack().to", "torch.stack().to", "torch.stack().to", "torch.stack().to", "torch.stack().to", "torch.stack().to", "torch.stack().to", "torch.stack().to", "torch.stack().to", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "mask_batch.append", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["None"], ["", "def", "generate_texture_map", "(", "self", ")", ":", "\n", "        ", "upper_cls", "=", "[", "1.", ",", "4.", "]", "\n", "lower_cls", "=", "[", "3.", ",", "5.", ",", "21.", "]", "\n", "outer_cls", "=", "[", "2.", "]", "\n", "\n", "mask_batch", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "            ", "mask", "=", "torch", ".", "zeros_like", "(", "self", ".", "segm", "[", "idx", "]", ")", "\n", "upper_fused_attr", "=", "self", ".", "upper_fused_attr", "[", "idx", "]", "\n", "lower_fused_attr", "=", "self", ".", "lower_fused_attr", "[", "idx", "]", "\n", "outer_fused_attr", "=", "self", ".", "outer_fused_attr", "[", "idx", "]", "\n", "if", "upper_fused_attr", "!=", "17", ":", "\n", "                ", "for", "cls", "in", "upper_cls", ":", "\n", "                    ", "mask", "[", "self", ".", "segm", "[", "idx", "]", "==", "cls", "]", "=", "upper_fused_attr", "+", "1", "\n", "\n", "", "", "if", "lower_fused_attr", "!=", "17", ":", "\n", "                ", "for", "cls", "in", "lower_cls", ":", "\n", "                    ", "mask", "[", "self", ".", "segm", "[", "idx", "]", "==", "cls", "]", "=", "lower_fused_attr", "+", "1", "\n", "\n", "", "", "if", "outer_fused_attr", "!=", "17", ":", "\n", "                ", "for", "cls", "in", "outer_cls", ":", "\n", "                    ", "mask", "[", "self", ".", "segm", "[", "idx", "]", "==", "cls", "]", "=", "outer_fused_attr", "+", "1", "\n", "\n", "", "", "mask_batch", ".", "append", "(", "mask", ")", "\n", "", "self", ".", "texture_mask", "=", "torch", ".", "stack", "(", "mask_batch", ",", "dim", "=", "0", ")", ".", "to", "(", "torch", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.feed_pose_data": [[471, 476], ["pose_img.to", "sample_model.SampleFromPoseModel.pose.size"], "methods", ["None"], ["", "def", "feed_pose_data", "(", "self", ",", "pose_img", ")", ":", "\n", "# for ui demo", "\n", "\n", "        ", "self", ".", "pose", "=", "pose_img", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "batch_size", "=", "self", ".", "pose", ".", "size", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.feed_shape_attributes": [[477, 481], ["shape_attr.to"], "methods", ["None"], ["", "def", "feed_shape_attributes", "(", "self", ",", "shape_attr", ")", ":", "\n", "# for ui demo", "\n", "\n", "        ", "self", ".", "shape_attr", "=", "shape_attr", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.feed_texture_attributes": [[482, 488], ["texture_attr[].unsqueeze().to", "texture_attr[].unsqueeze().to", "texture_attr[].unsqueeze().to", "texture_attr[].unsqueeze", "texture_attr[].unsqueeze", "texture_attr[].unsqueeze"], "methods", ["None"], ["", "def", "feed_texture_attributes", "(", "self", ",", "texture_attr", ")", ":", "\n", "# for ui demo", "\n", "\n", "        ", "self", ".", "upper_fused_attr", "=", "texture_attr", "[", "0", "]", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "lower_fused_attr", "=", "texture_attr", "[", "1", "]", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "outer_fused_attr", "=", "texture_attr", "[", "2", "]", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.sample_model.SampleFromPoseModel.palette_result": [[489, 501], ["numpy.array", "numpy.zeros", "enumerate", "len"], "methods", ["None"], ["", "def", "palette_result", "(", "self", ",", "result", ")", ":", "\n", "\n", "        ", "seg", "=", "result", "[", "0", "]", "\n", "palette", "=", "np", ".", "array", "(", "self", ".", "palette", ")", "\n", "assert", "palette", ".", "shape", "[", "1", "]", "==", "3", "\n", "assert", "len", "(", "palette", ".", "shape", ")", "==", "2", "\n", "color_seg", "=", "np", ".", "zeros", "(", "(", "seg", ".", "shape", "[", "0", "]", ",", "seg", ".", "shape", "[", "1", "]", ",", "3", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "for", "label", ",", "color", "in", "enumerate", "(", "palette", ")", ":", "\n", "            ", "color_seg", "[", "seg", "==", "label", ",", ":", "]", "=", "color", "\n", "# convert to BGR", "\n", "# color_seg = color_seg[..., ::-1]", "\n", "", "return", "color_seg", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.__init__.create_model": [[21, 43], ["getattr.", "logging.getLogger", "logging.getLogger.info", "getattr", "ValueError"], "function", ["None"], []], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.__init__": [[21, 104], ["torch.device", "torch.device", "torch.device", "torch.device", "models.archs.vqgan_arch.Encoder().to", "models.archs.vqgan_arch.Decoder().to", "models.archs.vqgan_arch.VectorQuantizerTexture().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.load_top_pretrain_models", "models.archs.vqgan_arch.Encoder().to", "models.archs.vqgan_arch.DecoderRes().to", "models.archs.vqgan_arch.VectorQuantizerSpatialTextureAware().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "models.archs.vqgan_arch.Discriminator().to", "lpips.LPIPS().to", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.load_discriminator_models", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.disc.train", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.init_training_settings", "models.archs.vqgan_arch.Encoder", "models.archs.vqgan_arch.Decoder", "models.archs.vqgan_arch.VectorQuantizerTexture", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "models.archs.vqgan_arch.Encoder", "models.archs.vqgan_arch.DecoderRes", "models.archs.vqgan_arch.VectorQuantizerSpatialTextureAware", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "models.archs.vqgan_arch.Discriminator", "lpips.LPIPS"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.load_top_pretrain_models", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.load_discriminator_models", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.init_training_settings"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "self", ".", "opt", "=", "opt", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "'cuda'", ")", "\n", "self", ".", "top_encoder", "=", "Encoder", "(", "\n", "ch", "=", "opt", "[", "'top_ch'", "]", ",", "\n", "num_res_blocks", "=", "opt", "[", "'top_num_res_blocks'", "]", ",", "\n", "attn_resolutions", "=", "opt", "[", "'top_attn_resolutions'", "]", ",", "\n", "ch_mult", "=", "opt", "[", "'top_ch_mult'", "]", ",", "\n", "in_channels", "=", "opt", "[", "'top_in_channels'", "]", ",", "\n", "resolution", "=", "opt", "[", "'top_resolution'", "]", ",", "\n", "z_channels", "=", "opt", "[", "'top_z_channels'", "]", ",", "\n", "double_z", "=", "opt", "[", "'top_double_z'", "]", ",", "\n", "dropout", "=", "opt", "[", "'top_dropout'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "decoder", "=", "Decoder", "(", "\n", "in_channels", "=", "opt", "[", "'top_in_channels'", "]", ",", "\n", "resolution", "=", "opt", "[", "'top_resolution'", "]", ",", "\n", "z_channels", "=", "opt", "[", "'top_z_channels'", "]", ",", "\n", "ch", "=", "opt", "[", "'top_ch'", "]", ",", "\n", "out_ch", "=", "opt", "[", "'top_out_ch'", "]", ",", "\n", "num_res_blocks", "=", "opt", "[", "'top_num_res_blocks'", "]", ",", "\n", "attn_resolutions", "=", "opt", "[", "'top_attn_resolutions'", "]", ",", "\n", "ch_mult", "=", "opt", "[", "'top_ch_mult'", "]", ",", "\n", "dropout", "=", "opt", "[", "'top_dropout'", "]", ",", "\n", "resamp_with_conv", "=", "True", ",", "\n", "give_pre_end", "=", "False", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "top_quantize", "=", "VectorQuantizerTexture", "(", "\n", "1024", ",", "opt", "[", "'embed_dim'", "]", ",", "beta", "=", "0.25", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "top_quant_conv", "=", "torch", ".", "nn", ".", "Conv2d", "(", "opt", "[", "\"top_z_channels\"", "]", ",", "\n", "opt", "[", "'embed_dim'", "]", ",", "\n", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "top_post_quant_conv", "=", "torch", ".", "nn", ".", "Conv2d", "(", "opt", "[", "'embed_dim'", "]", ",", "\n", "opt", "[", "\"top_z_channels\"", "]", ",", "\n", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "load_top_pretrain_models", "(", ")", "\n", "\n", "self", ".", "bot_encoder", "=", "Encoder", "(", "\n", "ch", "=", "opt", "[", "'bot_ch'", "]", ",", "\n", "num_res_blocks", "=", "opt", "[", "'bot_num_res_blocks'", "]", ",", "\n", "attn_resolutions", "=", "opt", "[", "'bot_attn_resolutions'", "]", ",", "\n", "ch_mult", "=", "opt", "[", "'bot_ch_mult'", "]", ",", "\n", "in_channels", "=", "opt", "[", "'bot_in_channels'", "]", ",", "\n", "resolution", "=", "opt", "[", "'bot_resolution'", "]", ",", "\n", "z_channels", "=", "opt", "[", "'bot_z_channels'", "]", ",", "\n", "double_z", "=", "opt", "[", "'bot_double_z'", "]", ",", "\n", "dropout", "=", "opt", "[", "'bot_dropout'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "bot_decoder_res", "=", "DecoderRes", "(", "\n", "in_channels", "=", "opt", "[", "'bot_in_channels'", "]", ",", "\n", "resolution", "=", "opt", "[", "'bot_resolution'", "]", ",", "\n", "z_channels", "=", "opt", "[", "'bot_z_channels'", "]", ",", "\n", "ch", "=", "opt", "[", "'bot_ch'", "]", ",", "\n", "num_res_blocks", "=", "opt", "[", "'bot_num_res_blocks'", "]", ",", "\n", "ch_mult", "=", "opt", "[", "'bot_ch_mult'", "]", ",", "\n", "dropout", "=", "opt", "[", "'bot_dropout'", "]", ",", "\n", "give_pre_end", "=", "False", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "bot_quantize", "=", "VectorQuantizerSpatialTextureAware", "(", "\n", "opt", "[", "'bot_n_embed'", "]", ",", "\n", "opt", "[", "'embed_dim'", "]", ",", "\n", "beta", "=", "0.25", ",", "\n", "spatial_size", "=", "opt", "[", "'codebook_spatial_size'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "bot_quant_conv", "=", "torch", ".", "nn", ".", "Conv2d", "(", "opt", "[", "\"bot_z_channels\"", "]", ",", "\n", "opt", "[", "'embed_dim'", "]", ",", "\n", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "bot_post_quant_conv", "=", "torch", ".", "nn", ".", "Conv2d", "(", "opt", "[", "'embed_dim'", "]", ",", "\n", "opt", "[", "\"bot_z_channels\"", "]", ",", "\n", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "self", ".", "disc", "=", "Discriminator", "(", "\n", "opt", "[", "'n_channels'", "]", ",", "opt", "[", "'ndf'", "]", ",", "\n", "n_layers", "=", "opt", "[", "'disc_layers'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "perceptual", "=", "lpips", ".", "LPIPS", "(", "net", "=", "\"vgg\"", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "perceptual_weight", "=", "opt", "[", "'perceptual_weight'", "]", "\n", "self", ".", "disc_start_step", "=", "opt", "[", "'disc_start_step'", "]", "\n", "self", ".", "disc_weight_max", "=", "opt", "[", "'disc_weight_max'", "]", "\n", "self", ".", "diff_aug", "=", "opt", "[", "'diff_aug'", "]", "\n", "self", ".", "policy", "=", "\"color,translation\"", "\n", "\n", "self", ".", "load_discriminator_models", "(", ")", "\n", "\n", "self", ".", "disc", ".", "train", "(", ")", "\n", "\n", "self", ".", "fix_decoder", "=", "opt", "[", "'fix_decoder'", "]", "\n", "\n", "self", ".", "init_training_settings", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.load_top_pretrain_models": [[105, 122], ["torch.load", "torch.load", "torch.load", "torch.load", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.top_encoder.load_state_dict", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.decoder.load_state_dict", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.top_quantize.load_state_dict", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.top_quant_conv.load_state_dict", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.top_post_quant_conv.load_state_dict", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.top_encoder.eval", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.top_quantize.eval", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.top_quant_conv.eval", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.top_post_quant_conv.eval"], "methods", ["None"], ["", "def", "load_top_pretrain_models", "(", "self", ")", ":", "\n", "# load pretrained vqgan for segmentation mask", "\n", "        ", "top_vae_checkpoint", "=", "torch", ".", "load", "(", "self", ".", "opt", "[", "'top_vae_path'", "]", ")", "\n", "self", ".", "top_encoder", ".", "load_state_dict", "(", "\n", "top_vae_checkpoint", "[", "'encoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "decoder", ".", "load_state_dict", "(", "\n", "top_vae_checkpoint", "[", "'decoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "top_quantize", ".", "load_state_dict", "(", "\n", "top_vae_checkpoint", "[", "'quantize'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "top_quant_conv", ".", "load_state_dict", "(", "\n", "top_vae_checkpoint", "[", "'quant_conv'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "top_post_quant_conv", ".", "load_state_dict", "(", "\n", "top_vae_checkpoint", "[", "'post_quant_conv'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "top_encoder", ".", "eval", "(", ")", "\n", "self", ".", "top_quantize", ".", "eval", "(", ")", "\n", "self", ".", "top_quant_conv", ".", "eval", "(", ")", "\n", "self", ".", "top_post_quant_conv", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.init_training_settings": [[123, 126], ["collections.OrderedDict", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.configure_optimizers"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.configure_optimizers"], ["", "def", "init_training_settings", "(", "self", ")", ":", "\n", "        ", "self", ".", "log_dict", "=", "OrderedDict", "(", ")", "\n", "self", ".", "configure_optimizers", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.configure_optimizers": [[127, 160], ["hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_encoder.parameters", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_decoder_res.parameters", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_quantize.parameters", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_quant_conv.parameters", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_post_quant_conv.parameters", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.decoder.named_parameters", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.disc.parameters", "optim_params.append", "optim_params.append", "optim_params.append", "optim_params.append", "optim_params.append", "optim_params.append", "optim_params.append", "optim_params.append", "optim_params.append"], "methods", ["None"], ["", "def", "configure_optimizers", "(", "self", ")", ":", "\n", "        ", "optim_params", "=", "[", "]", "\n", "for", "v", "in", "self", ".", "bot_encoder", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "v", ".", "requires_grad", ":", "\n", "                ", "optim_params", ".", "append", "(", "v", ")", "\n", "", "", "for", "v", "in", "self", ".", "bot_decoder_res", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "v", ".", "requires_grad", ":", "\n", "                ", "optim_params", ".", "append", "(", "v", ")", "\n", "", "", "for", "v", "in", "self", ".", "bot_quantize", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "v", ".", "requires_grad", ":", "\n", "                ", "optim_params", ".", "append", "(", "v", ")", "\n", "", "", "for", "v", "in", "self", ".", "bot_quant_conv", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "v", ".", "requires_grad", ":", "\n", "                ", "optim_params", ".", "append", "(", "v", ")", "\n", "", "", "for", "v", "in", "self", ".", "bot_post_quant_conv", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "v", ".", "requires_grad", ":", "\n", "                ", "optim_params", ".", "append", "(", "v", ")", "\n", "", "", "if", "not", "self", ".", "fix_decoder", ":", "\n", "            ", "for", "name", ",", "v", "in", "self", ".", "decoder", ".", "named_parameters", "(", ")", ":", "\n", "                ", "if", "v", ".", "requires_grad", ":", "\n", "                    ", "if", "'up.0'", "in", "name", ":", "\n", "                        ", "optim_params", ".", "append", "(", "v", ")", "\n", "", "if", "'up.1'", "in", "name", ":", "\n", "                        ", "optim_params", ".", "append", "(", "v", ")", "\n", "", "if", "'up.2'", "in", "name", ":", "\n", "                        ", "optim_params", ".", "append", "(", "v", ")", "\n", "", "if", "'up.3'", "in", "name", ":", "\n", "                        ", "optim_params", ".", "append", "(", "v", ")", "\n", "\n", "", "", "", "", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "optim_params", ",", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", ")", "\n", "\n", "self", ".", "disc_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "\n", "self", ".", "disc", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.load_discriminator_models": [[161, 166], ["torch.load", "torch.load", "torch.load", "torch.load", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.disc.load_state_dict"], "methods", ["None"], ["", "def", "load_discriminator_models", "(", "self", ")", ":", "\n", "# load pretrained vqgan for segmentation mask", "\n", "        ", "top_vae_checkpoint", "=", "torch", ".", "load", "(", "self", ".", "opt", "[", "'top_vae_path'", "]", ")", "\n", "self", ".", "disc", ".", "load_state_dict", "(", "\n", "top_vae_checkpoint", "[", "'discriminator'", "]", ",", "strict", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.save_network": [[167, 181], ["hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_encoder.state_dict", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_decoder_res.state_dict", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.decoder.state_dict", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_quantize.state_dict", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_quant_conv.state_dict", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_post_quant_conv.state_dict", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.disc.state_dict", "torch.save", "torch.save", "torch.save", "torch.save"], "methods", ["None"], ["", "def", "save_network", "(", "self", ",", "save_path", ")", ":", "\n", "        ", "\"\"\"Save networks.\n        \"\"\"", "\n", "\n", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'bot_encoder'", "]", "=", "self", ".", "bot_encoder", ".", "state_dict", "(", ")", "\n", "save_dict", "[", "'bot_decoder_res'", "]", "=", "self", ".", "bot_decoder_res", ".", "state_dict", "(", ")", "\n", "save_dict", "[", "'decoder'", "]", "=", "self", ".", "decoder", ".", "state_dict", "(", ")", "\n", "save_dict", "[", "'bot_quantize'", "]", "=", "self", ".", "bot_quantize", ".", "state_dict", "(", ")", "\n", "save_dict", "[", "'bot_quant_conv'", "]", "=", "self", ".", "bot_quant_conv", ".", "state_dict", "(", ")", "\n", "save_dict", "[", "'bot_post_quant_conv'", "]", "=", "self", ".", "bot_post_quant_conv", ".", "state_dict", "(", "\n", ")", "\n", "save_dict", "[", "'discriminator'", "]", "=", "self", ".", "disc", ".", "state_dict", "(", ")", "\n", "torch", ".", "save", "(", "save_dict", ",", "save_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.load_network": [[182, 195], ["torch.load", "torch.load", "torch.load", "torch.load", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_encoder.load_state_dict", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_decoder_res.load_state_dict", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.decoder.load_state_dict", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_quantize.load_state_dict", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_quant_conv.load_state_dict", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_post_quant_conv.load_state_dict"], "methods", ["None"], ["", "def", "load_network", "(", "self", ")", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "self", ".", "opt", "[", "'pretrained_models'", "]", ")", "\n", "self", ".", "bot_encoder", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'bot_encoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "bot_decoder_res", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'bot_decoder_res'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "decoder", ".", "load_state_dict", "(", "checkpoint", "[", "'decoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "bot_quantize", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'bot_quantize'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "bot_quant_conv", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'bot_quant_conv'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "bot_post_quant_conv", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'bot_post_quant_conv'", "]", ",", "strict", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.optimize_parameters": [[196, 214], ["hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_encoder.train", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_decoder_res.train", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_quantize.train", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_quant_conv.train", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_post_quant_conv.train", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.training_step", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.optimizer.zero_grad", "loss.backward", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.optimizer.step", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.decoder.train", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.disc_optimizer.zero_grad", "d_loss.backward", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.disc_optimizer.step"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.training_step"], ["", "def", "optimize_parameters", "(", "self", ",", "data", ",", "step", ")", ":", "\n", "        ", "self", ".", "bot_encoder", ".", "train", "(", ")", "\n", "self", ".", "bot_decoder_res", ".", "train", "(", ")", "\n", "if", "not", "self", ".", "fix_decoder", ":", "\n", "            ", "self", ".", "decoder", ".", "train", "(", ")", "\n", "", "self", ".", "bot_quantize", ".", "train", "(", ")", "\n", "self", ".", "bot_quant_conv", ".", "train", "(", ")", "\n", "self", ".", "bot_post_quant_conv", ".", "train", "(", ")", "\n", "\n", "loss", ",", "d_loss", "=", "self", ".", "training_step", "(", "data", ",", "step", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "step", ">", "self", ".", "disc_start_step", ":", "\n", "            ", "self", ".", "disc_optimizer", ".", "zero_grad", "(", ")", "\n", "d_loss", ".", "backward", "(", ")", "\n", "self", ".", "disc_optimizer", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.top_encode": [[215, 221], ["hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.top_encoder", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.top_quant_conv", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.top_quantize", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.top_post_quant_conv"], "methods", ["None"], ["", "", "def", "top_encode", "(", "self", ",", "x", ",", "mask", ")", ":", "\n", "        ", "h", "=", "self", ".", "top_encoder", "(", "x", ")", "\n", "h", "=", "self", ".", "top_quant_conv", "(", "h", ")", "\n", "quant", ",", "_", ",", "_", "=", "self", ".", "top_quantize", "(", "h", ",", "mask", ")", "\n", "quant", "=", "self", ".", "top_post_quant_conv", "(", "quant", ")", "\n", "return", "quant", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_encode": [[222, 229], ["hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_encoder", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_quant_conv", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_quantize", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_post_quant_conv", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_decoder_res"], "methods", ["None"], ["", "def", "bot_encode", "(", "self", ",", "x", ",", "mask", ")", ":", "\n", "        ", "h", "=", "self", ".", "bot_encoder", "(", "x", ")", "\n", "h", "=", "self", ".", "bot_quant_conv", "(", "h", ")", "\n", "quant", ",", "emb_loss", ",", "info", "=", "self", ".", "bot_quantize", "(", "h", ",", "mask", ")", "\n", "quant", "=", "self", ".", "bot_post_quant_conv", "(", "quant", ")", "\n", "bot_dec_res", "=", "self", ".", "bot_decoder_res", "(", "quant", ")", "\n", "return", "bot_dec_res", ",", "emb_loss", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.decode": [[230, 233], ["hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.decoder"], "methods", ["None"], ["", "def", "decode", "(", "self", ",", "quant_top", ",", "bot_dec_res", ")", ":", "\n", "        ", "dec", "=", "self", ".", "decoder", "(", "quant_top", ",", "bot_h", "=", "bot_dec_res", ")", "\n", "return", "dec", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.forward_step": [[234, 240], ["hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_encode", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.decode", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.top_encode"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_encode", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.decode", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.top_encode"], ["", "def", "forward_step", "(", "self", ",", "input", ",", "mask", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "quant_top", "=", "self", ".", "top_encode", "(", "input", ",", "mask", ")", "\n", "", "bot_dec_res", ",", "diff", ",", "_", "=", "self", ".", "bot_encode", "(", "input", ",", "mask", ")", "\n", "dec", "=", "self", ".", "decode", "(", "quant_top", ",", "bot_dec_res", ")", "\n", "return", "dec", ",", "diff", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.feed_data": [[241, 246], ["data[].float().to", "data[].float().to", "data[].float", "data[].float"], "methods", ["None"], ["", "def", "feed_data", "(", "self", ",", "data", ")", ":", "\n", "        ", "x", "=", "data", "[", "'image'", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "mask", "=", "data", "[", "'texture_mask'", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "return", "x", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.training_step": [[247, 292], ["hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.feed_data", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.forward_step", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.perceptual", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.disc", "models.losses.vqgan_loss.calculate_adaptive_weight", "models.losses.vqgan_loss.adopt_weight", "torch.abs.mean().item", "torch.abs.mean().item", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.mean().item", "torch.mean.item", "torch.mean.item", "g_loss.item", "codebook_loss.item", "x.contiguous", "models.losses.vqgan_loss.DiffAugment.contiguous", "models.losses.vqgan_loss.DiffAugment", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.disc", "models.losses.vqgan_loss.hinge_d_loss", "x.contiguous", "models.losses.vqgan_loss.DiffAugment.contiguous", "torch.abs.mean", "torch.abs.mean", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.mean", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.disc", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.disc", "models.losses.vqgan_loss.DiffAugment.contiguous().detach", "models.losses.vqgan_loss.DiffAugment", "x.contiguous().detach", "x.contiguous().detach", "models.losses.vqgan_loss.DiffAugment.contiguous", "x.contiguous", "x.contiguous"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.feed_data", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.forward_step", "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.calculate_adaptive_weight", "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.adopt_weight", "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.DiffAugment", "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.hinge_d_loss", "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.DiffAugment"], ["", "def", "training_step", "(", "self", ",", "data", ",", "step", ")", ":", "\n", "        ", "x", ",", "mask", "=", "self", ".", "feed_data", "(", "data", ")", "\n", "xrec", ",", "codebook_loss", "=", "self", ".", "forward_step", "(", "x", ",", "mask", ")", "\n", "\n", "# get recon/perceptual loss", "\n", "recon_loss", "=", "torch", ".", "abs", "(", "x", ".", "contiguous", "(", ")", "-", "xrec", ".", "contiguous", "(", ")", ")", "\n", "p_loss", "=", "self", ".", "perceptual", "(", "x", ".", "contiguous", "(", ")", ",", "xrec", ".", "contiguous", "(", ")", ")", "\n", "nll_loss", "=", "recon_loss", "+", "self", ".", "perceptual_weight", "*", "p_loss", "\n", "nll_loss", "=", "torch", ".", "mean", "(", "nll_loss", ")", "\n", "\n", "# augment for input to discriminator", "\n", "if", "self", ".", "diff_aug", ":", "\n", "            ", "xrec", "=", "DiffAugment", "(", "xrec", ",", "policy", "=", "self", ".", "policy", ")", "\n", "\n", "# update generator", "\n", "", "logits_fake", "=", "self", ".", "disc", "(", "xrec", ")", "\n", "g_loss", "=", "-", "torch", ".", "mean", "(", "logits_fake", ")", "\n", "last_layer", "=", "self", ".", "decoder", ".", "conv_out", ".", "weight", "\n", "d_weight", "=", "calculate_adaptive_weight", "(", "nll_loss", ",", "g_loss", ",", "last_layer", ",", "\n", "self", ".", "disc_weight_max", ")", "\n", "d_weight", "*=", "adopt_weight", "(", "1", ",", "step", ",", "self", ".", "disc_start_step", ")", "\n", "loss", "=", "nll_loss", "+", "d_weight", "*", "g_loss", "+", "codebook_loss", "\n", "\n", "self", ".", "log_dict", "[", "\"loss\"", "]", "=", "loss", "\n", "self", ".", "log_dict", "[", "\"l1\"", "]", "=", "recon_loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "log_dict", "[", "\"perceptual\"", "]", "=", "p_loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "log_dict", "[", "\"nll_loss\"", "]", "=", "nll_loss", ".", "item", "(", ")", "\n", "self", ".", "log_dict", "[", "\"g_loss\"", "]", "=", "g_loss", ".", "item", "(", ")", "\n", "self", ".", "log_dict", "[", "\"d_weight\"", "]", "=", "d_weight", "\n", "self", ".", "log_dict", "[", "\"codebook_loss\"", "]", "=", "codebook_loss", ".", "item", "(", ")", "\n", "\n", "if", "step", ">", "self", ".", "disc_start_step", ":", "\n", "            ", "if", "self", ".", "diff_aug", ":", "\n", "                ", "logits_real", "=", "self", ".", "disc", "(", "\n", "DiffAugment", "(", "x", ".", "contiguous", "(", ")", ".", "detach", "(", ")", ",", "policy", "=", "self", ".", "policy", ")", ")", "\n", "", "else", ":", "\n", "                ", "logits_real", "=", "self", ".", "disc", "(", "x", ".", "contiguous", "(", ")", ".", "detach", "(", ")", ")", "\n", "", "logits_fake", "=", "self", ".", "disc", "(", "xrec", ".", "contiguous", "(", ")", ".", "detach", "(", "\n", ")", ")", "# detach so that generator isn\"t also updated", "\n", "d_loss", "=", "hinge_d_loss", "(", "logits_real", ",", "logits_fake", ")", "\n", "self", ".", "log_dict", "[", "\"d_loss\"", "]", "=", "d_loss", "\n", "", "else", ":", "\n", "            ", "d_loss", "=", "None", "\n", "\n", "", "return", "loss", ",", "d_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.inference": [[293, 335], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_encoder.eval", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_decoder_res.eval", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.decoder.eval", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_quantize.eval", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_quant_conv.eval", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.bot_post_quant_conv.eval", "enumerate", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.feed_data", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.forward_step", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.perceptual", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.size", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "img_cat.clamp_.clamp_.clamp_", "torchvision.utils.save_image", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.contiguous", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.contiguous", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.one_hot", "torch.one_hot", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.squeeze().permute().float", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.to_rgb", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.to_rgb", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.contiguous", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.contiguous", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.squeeze().permute", "hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.squeeze"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.feed_data", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.forward_step", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQSegmentationModel.to_rgb", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQSegmentationModel.to_rgb"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "inference", "(", "self", ",", "data_loader", ",", "save_dir", ")", ":", "\n", "        ", "self", ".", "bot_encoder", ".", "eval", "(", ")", "\n", "self", ".", "bot_decoder_res", ".", "eval", "(", ")", "\n", "self", ".", "decoder", ".", "eval", "(", ")", "\n", "self", ".", "bot_quantize", ".", "eval", "(", ")", "\n", "self", ".", "bot_quant_conv", ".", "eval", "(", ")", "\n", "self", ".", "bot_post_quant_conv", ".", "eval", "(", ")", "\n", "\n", "loss_total", "=", "0", "\n", "num", "=", "0", "\n", "\n", "for", "_", ",", "data", "in", "enumerate", "(", "data_loader", ")", ":", "\n", "            ", "img_name", "=", "data", "[", "'img_name'", "]", "[", "0", "]", "\n", "x", ",", "mask", "=", "self", ".", "feed_data", "(", "data", ")", "\n", "xrec", ",", "_", "=", "self", ".", "forward_step", "(", "x", ",", "mask", ")", "\n", "\n", "recon_loss", "=", "torch", ".", "abs", "(", "x", ".", "contiguous", "(", ")", "-", "xrec", ".", "contiguous", "(", ")", ")", "\n", "p_loss", "=", "self", ".", "perceptual", "(", "x", ".", "contiguous", "(", ")", ",", "xrec", ".", "contiguous", "(", ")", ")", "\n", "nll_loss", "=", "recon_loss", "+", "self", ".", "perceptual_weight", "*", "p_loss", "\n", "nll_loss", "=", "torch", ".", "mean", "(", "nll_loss", ")", "\n", "loss_total", "+=", "nll_loss", "\n", "\n", "num", "+=", "x", ".", "size", "(", "0", ")", "\n", "\n", "if", "x", ".", "shape", "[", "1", "]", ">", "3", ":", "\n", "# colorize with random projection", "\n", "                ", "assert", "xrec", ".", "shape", "[", "1", "]", ">", "3", "\n", "# convert logits to indices", "\n", "xrec", "=", "torch", ".", "argmax", "(", "xrec", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "xrec", "=", "F", ".", "one_hot", "(", "xrec", ",", "num_classes", "=", "x", ".", "shape", "[", "1", "]", ")", "\n", "xrec", "=", "xrec", ".", "squeeze", "(", "1", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "float", "(", ")", "\n", "x", "=", "self", ".", "to_rgb", "(", "x", ")", "\n", "xrec", "=", "self", ".", "to_rgb", "(", "xrec", ")", "\n", "\n", "", "img_cat", "=", "torch", ".", "cat", "(", "[", "x", ",", "xrec", "]", ",", "dim", "=", "3", ")", ".", "detach", "(", ")", "\n", "img_cat", "=", "(", "(", "img_cat", "+", "1", ")", "/", "2", ")", "\n", "img_cat", "=", "img_cat", ".", "clamp_", "(", "0", ",", "1", ")", "\n", "save_image", "(", "\n", "img_cat", ",", "f'{save_dir}/{img_name}.png'", ",", "nrow", "=", "1", ",", "padding", "=", "4", ")", "\n", "\n", "", "return", "(", "loss_total", "/", "num", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.get_current_log": [[336, 338], ["None"], "methods", ["None"], ["", "def", "get_current_log", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "log_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.hierarchy_vqgan_model.HierarchyVQSpatialTextureAwareModel.update_learning_rate": [[339, 375], ["math.cos", "ValueError", "int"], "methods", ["None"], ["", "def", "update_learning_rate", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "\"\"\"Update learning rate.\n\n        Args:\n            current_iter (int): Current iteration.\n            warmup_iter (int): Warmup iter numbers. -1 for no warmup.\n                Default: -1.\n        \"\"\"", "\n", "lr", "=", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "\n", "if", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'step'", ":", "\n", "            ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "(", "\n", "self", ".", "opt", "[", "'gamma'", "]", "**", "(", "epoch", "//", "self", ".", "opt", "[", "'step'", "]", ")", ")", "\n", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'cos'", ":", "\n", "            ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "(", "\n", "1", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "epoch", "/", "self", ".", "opt", "[", "'num_epochs'", "]", ")", ")", "/", "2", "\n", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'linear'", ":", "\n", "            ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "(", "1", "-", "epoch", "/", "self", ".", "opt", "[", "'num_epochs'", "]", ")", "\n", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'linear2exp'", ":", "\n", "            ", "if", "epoch", "<", "self", ".", "opt", "[", "'turning_point'", "]", "+", "1", ":", "\n", "# learning rate decay as 95%", "\n", "# at the turning point (1 / 95% = 1.0526)", "\n", "                ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "(", "\n", "1", "-", "epoch", "/", "int", "(", "self", ".", "opt", "[", "'turning_point'", "]", "*", "1.0526", ")", ")", "\n", "", "else", ":", "\n", "                ", "lr", "*=", "self", ".", "opt", "[", "'gamma'", "]", "\n", "", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'schedule'", ":", "\n", "            ", "if", "epoch", "in", "self", ".", "opt", "[", "'schedule'", "]", ":", "\n", "                ", "lr", "*=", "self", ".", "opt", "[", "'gamma'", "]", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Unknown lr mode {}'", ".", "format", "(", "self", ".", "opt", "[", "'lr_decay'", "]", ")", ")", "\n", "# set learning rate", "\n", "", "for", "param_group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "            ", "param_group", "[", "'lr'", "]", "=", "lr", "\n", "\n", "", "return", "lr", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.__init__": [[20, 53], ["super().__init__", "torch.device", "torch.device", "torch.device", "torch.device", "models.archs.vqgan_arch.Encoder().to", "models.archs.vqgan_arch.Decoder().to", "models.archs.vqgan_arch.VectorQuantizer().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "models.archs.vqgan_arch.Encoder", "models.archs.vqgan_arch.Decoder", "models.archs.vqgan_arch.VectorQuantizer", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "opt", "=", "opt", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "'cuda'", ")", "\n", "self", ".", "encoder", "=", "Encoder", "(", "\n", "ch", "=", "opt", "[", "'ch'", "]", ",", "\n", "num_res_blocks", "=", "opt", "[", "'num_res_blocks'", "]", ",", "\n", "attn_resolutions", "=", "opt", "[", "'attn_resolutions'", "]", ",", "\n", "ch_mult", "=", "opt", "[", "'ch_mult'", "]", ",", "\n", "in_channels", "=", "opt", "[", "'in_channels'", "]", ",", "\n", "resolution", "=", "opt", "[", "'resolution'", "]", ",", "\n", "z_channels", "=", "opt", "[", "'z_channels'", "]", ",", "\n", "double_z", "=", "opt", "[", "'double_z'", "]", ",", "\n", "dropout", "=", "opt", "[", "'dropout'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "decoder", "=", "Decoder", "(", "\n", "in_channels", "=", "opt", "[", "'in_channels'", "]", ",", "\n", "resolution", "=", "opt", "[", "'resolution'", "]", ",", "\n", "z_channels", "=", "opt", "[", "'z_channels'", "]", ",", "\n", "ch", "=", "opt", "[", "'ch'", "]", ",", "\n", "out_ch", "=", "opt", "[", "'out_ch'", "]", ",", "\n", "num_res_blocks", "=", "opt", "[", "'num_res_blocks'", "]", ",", "\n", "attn_resolutions", "=", "opt", "[", "'attn_resolutions'", "]", ",", "\n", "ch_mult", "=", "opt", "[", "'ch_mult'", "]", ",", "\n", "dropout", "=", "opt", "[", "'dropout'", "]", ",", "\n", "resamp_with_conv", "=", "True", ",", "\n", "give_pre_end", "=", "False", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "quantize", "=", "VectorQuantizer", "(", "\n", "opt", "[", "'n_embed'", "]", ",", "opt", "[", "'embed_dim'", "]", ",", "beta", "=", "0.25", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "quant_conv", "=", "torch", ".", "nn", ".", "Conv2d", "(", "opt", "[", "\"z_channels\"", "]", ",", "opt", "[", "'embed_dim'", "]", ",", "\n", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "post_quant_conv", "=", "torch", ".", "nn", ".", "Conv2d", "(", "opt", "[", "'embed_dim'", "]", ",", "\n", "opt", "[", "\"z_channels\"", "]", ",", "\n", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.init_training_settings": [[54, 58], ["models.losses.segmentation_loss.BCELossWithQuant", "collections.OrderedDict", "vqgan_model.VQModel.configure_optimizers"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.configure_optimizers"], ["", "def", "init_training_settings", "(", "self", ")", ":", "\n", "        ", "self", ".", "loss", "=", "BCELossWithQuant", "(", ")", "\n", "self", ".", "log_dict", "=", "OrderedDict", "(", ")", "\n", "self", ".", "configure_optimizers", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.save_network": [[59, 76], ["vqgan_model.VQModel.encoder.state_dict", "vqgan_model.VQModel.decoder.state_dict", "vqgan_model.VQModel.quantize.state_dict", "vqgan_model.VQModel.quant_conv.state_dict", "vqgan_model.VQModel.post_quant_conv.state_dict", "vqgan_model.VQModel.disc.state_dict", "torch.save", "torch.save", "torch.save", "torch.save"], "methods", ["None"], ["", "def", "save_network", "(", "self", ",", "save_path", ")", ":", "\n", "        ", "\"\"\"Save networks.\n\n        Args:\n            net (nn.Module): Network to be saved.\n            net_label (str): Network label.\n            current_iter (int): Current iter number.\n        \"\"\"", "\n", "\n", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'encoder'", "]", "=", "self", ".", "encoder", ".", "state_dict", "(", ")", "\n", "save_dict", "[", "'decoder'", "]", "=", "self", ".", "decoder", ".", "state_dict", "(", ")", "\n", "save_dict", "[", "'quantize'", "]", "=", "self", ".", "quantize", ".", "state_dict", "(", ")", "\n", "save_dict", "[", "'quant_conv'", "]", "=", "self", ".", "quant_conv", ".", "state_dict", "(", ")", "\n", "save_dict", "[", "'post_quant_conv'", "]", "=", "self", ".", "post_quant_conv", ".", "state_dict", "(", ")", "\n", "save_dict", "[", "'discriminator'", "]", "=", "self", ".", "disc", ".", "state_dict", "(", ")", "\n", "torch", ".", "save", "(", "save_dict", ",", "save_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.load_network": [[77, 85], ["torch.load", "torch.load", "torch.load", "torch.load", "vqgan_model.VQModel.encoder.load_state_dict", "vqgan_model.VQModel.decoder.load_state_dict", "vqgan_model.VQModel.quantize.load_state_dict", "vqgan_model.VQModel.quant_conv.load_state_dict", "vqgan_model.VQModel.post_quant_conv.load_state_dict"], "methods", ["None"], ["", "def", "load_network", "(", "self", ")", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "self", ".", "opt", "[", "'pretrained_models'", "]", ")", "\n", "self", ".", "encoder", ".", "load_state_dict", "(", "checkpoint", "[", "'encoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "decoder", ".", "load_state_dict", "(", "checkpoint", "[", "'decoder'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "quantize", ".", "load_state_dict", "(", "checkpoint", "[", "'quantize'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "quant_conv", ".", "load_state_dict", "(", "checkpoint", "[", "'quant_conv'", "]", ",", "strict", "=", "True", ")", "\n", "self", ".", "post_quant_conv", ".", "load_state_dict", "(", "\n", "checkpoint", "[", "'post_quant_conv'", "]", ",", "strict", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.optimize_parameters": [[86, 97], ["vqgan_model.VQModel.encoder.train", "vqgan_model.VQModel.decoder.train", "vqgan_model.VQModel.quantize.train", "vqgan_model.VQModel.quant_conv.train", "vqgan_model.VQModel.post_quant_conv.train", "vqgan_model.VQModel.training_step", "vqgan_model.VQModel.optimizer.zero_grad", "vqgan_model.VQModel.backward", "vqgan_model.VQModel.optimizer.step"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.training_step"], ["", "def", "optimize_parameters", "(", "self", ",", "data", ",", "current_iter", ")", ":", "\n", "        ", "self", ".", "encoder", ".", "train", "(", ")", "\n", "self", ".", "decoder", ".", "train", "(", ")", "\n", "self", ".", "quantize", ".", "train", "(", ")", "\n", "self", ".", "quant_conv", ".", "train", "(", ")", "\n", "self", ".", "post_quant_conv", ".", "train", "(", ")", "\n", "\n", "loss", "=", "self", ".", "training_step", "(", "data", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.encode": [[98, 103], ["vqgan_model.VQModel.encoder", "vqgan_model.VQModel.quant_conv", "vqgan_model.VQModel.quantize"], "methods", ["None"], ["", "def", "encode", "(", "self", ",", "x", ")", ":", "\n", "        ", "h", "=", "self", ".", "encoder", "(", "x", ")", "\n", "h", "=", "self", ".", "quant_conv", "(", "h", ")", "\n", "quant", ",", "emb_loss", ",", "info", "=", "self", ".", "quantize", "(", "h", ")", "\n", "return", "quant", ",", "emb_loss", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.decode": [[104, 108], ["vqgan_model.VQModel.post_quant_conv", "vqgan_model.VQModel.decoder"], "methods", ["None"], ["", "def", "decode", "(", "self", ",", "quant", ")", ":", "\n", "        ", "quant", "=", "self", ".", "post_quant_conv", "(", "quant", ")", "\n", "dec", "=", "self", ".", "decoder", "(", "quant", ")", "\n", "return", "dec", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.decode_code": [[109, 113], ["vqgan_model.VQModel.quantize.embed_code", "vqgan_model.VQModel.decode"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.decode"], ["", "def", "decode_code", "(", "self", ",", "code_b", ")", ":", "\n", "        ", "quant_b", "=", "self", ".", "quantize", ".", "embed_code", "(", "code_b", ")", "\n", "dec", "=", "self", ".", "decode", "(", "quant_b", ")", "\n", "return", "dec", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.forward_step": [[114, 118], ["vqgan_model.VQModel.encode", "vqgan_model.VQModel.decode"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.encode", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.decode"], ["", "def", "forward_step", "(", "self", ",", "input", ")", ":", "\n", "        ", "quant", ",", "diff", ",", "_", "=", "self", ".", "encode", "(", "input", ")", "\n", "dec", "=", "self", ".", "decode", "(", "quant", ")", "\n", "return", "dec", ",", "diff", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.feed_data": [[119, 127], ["torch.one_hot", "torch.one_hot", "x.permute().to.permute().to.permute().to", "x.permute().to.permute().to.float().to", "len", "x.permute().to.permute().to.permute", "x.permute().to.permute().to.float"], "methods", ["None"], ["", "def", "feed_data", "(", "self", ",", "data", ")", ":", "\n", "        ", "x", "=", "data", "[", "'segm'", "]", "\n", "x", "=", "F", ".", "one_hot", "(", "x", ",", "num_classes", "=", "self", ".", "opt", "[", "'num_segm_classes'", "]", ")", "\n", "\n", "if", "len", "(", "x", ".", "shape", ")", "==", "3", ":", "\n", "            ", "x", "=", "x", "[", "...", ",", "None", "]", "\n", "", "x", "=", "x", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "to", "(", "memory_format", "=", "torch", ".", "contiguous_format", ")", "\n", "return", "x", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.get_current_log": [[128, 130], ["None"], "methods", ["None"], ["", "def", "get_current_log", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "log_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQModel.update_learning_rate": [[131, 167], ["math.cos", "ValueError", "int"], "methods", ["None"], ["", "def", "update_learning_rate", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "\"\"\"Update learning rate.\n\n        Args:\n            current_iter (int): Current iteration.\n            warmup_iter (int): Warmup iter numbers. -1 for no warmup.\n                Default: -1.\n        \"\"\"", "\n", "lr", "=", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "\n", "if", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'step'", ":", "\n", "            ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "(", "\n", "self", ".", "opt", "[", "'gamma'", "]", "**", "(", "epoch", "//", "self", ".", "opt", "[", "'step'", "]", ")", ")", "\n", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'cos'", ":", "\n", "            ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "(", "\n", "1", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "epoch", "/", "self", ".", "opt", "[", "'num_epochs'", "]", ")", ")", "/", "2", "\n", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'linear'", ":", "\n", "            ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "(", "1", "-", "epoch", "/", "self", ".", "opt", "[", "'num_epochs'", "]", ")", "\n", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'linear2exp'", ":", "\n", "            ", "if", "epoch", "<", "self", ".", "opt", "[", "'turning_point'", "]", "+", "1", ":", "\n", "# learning rate decay as 95%", "\n", "# at the turning point (1 / 95% = 1.0526)", "\n", "                ", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", "*", "(", "\n", "1", "-", "epoch", "/", "int", "(", "self", ".", "opt", "[", "'turning_point'", "]", "*", "1.0526", ")", ")", "\n", "", "else", ":", "\n", "                ", "lr", "*=", "self", ".", "opt", "[", "'gamma'", "]", "\n", "", "", "elif", "self", ".", "opt", "[", "'lr_decay'", "]", "==", "'schedule'", ":", "\n", "            ", "if", "epoch", "in", "self", ".", "opt", "[", "'schedule'", "]", ":", "\n", "                ", "lr", "*=", "self", ".", "opt", "[", "'gamma'", "]", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Unknown lr mode {}'", ".", "format", "(", "self", ".", "opt", "[", "'lr_decay'", "]", ")", ")", "\n", "# set learning rate", "\n", "", "for", "param_group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "            ", "param_group", "[", "'lr'", "]", "=", "lr", "\n", "\n", "", "return", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQSegmentationModel.__init__": [[171, 177], ["vqgan_model.VQModel.__init__", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.randn().to", "vqgan_model.VQSegmentationModel.init_training_settings", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.init_training_settings"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "opt", ")", "\n", "self", ".", "colorize", "=", "torch", ".", "randn", "(", "3", ",", "opt", "[", "'num_segm_classes'", "]", ",", "1", ",", "\n", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "self", ".", "init_training_settings", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQSegmentationModel.configure_optimizers": [[178, 186], ["torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "list", "list", "vqgan_model.VQSegmentationModel.post_quant_conv.parameters", "list", "vqgan_model.VQSegmentationModel.quant_conv.parameters", "list", "list", "vqgan_model.VQSegmentationModel.quantize.parameters", "vqgan_model.VQSegmentationModel.encoder.parameters", "vqgan_model.VQSegmentationModel.decoder.parameters"], "methods", ["None"], ["", "def", "configure_optimizers", "(", "self", ")", ":", "\n", "        ", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "\n", "list", "(", "self", ".", "encoder", ".", "parameters", "(", ")", ")", "+", "list", "(", "self", ".", "decoder", ".", "parameters", "(", ")", ")", "+", "\n", "list", "(", "self", ".", "quantize", ".", "parameters", "(", ")", ")", "+", "\n", "list", "(", "self", ".", "quant_conv", ".", "parameters", "(", ")", ")", "+", "\n", "list", "(", "self", ".", "post_quant_conv", ".", "parameters", "(", ")", ")", ",", "\n", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", ",", "\n", "betas", "=", "(", "0.5", ",", "0.9", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQSegmentationModel.training_step": [[187, 193], ["vqgan_model.VQSegmentationModel.feed_data", "vqgan_model.VQSegmentationModel.forward_step", "vqgan_model.VQSegmentationModel.loss", "vqgan_model.VQSegmentationModel.log_dict.update"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.feed_data", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.forward_step", "home.repos.pwc.inspect_result.yumingj_Text2Human.utils.util.AverageMeter.update"], ["", "def", "training_step", "(", "self", ",", "data", ")", ":", "\n", "        ", "x", "=", "self", ".", "feed_data", "(", "data", ")", "\n", "xrec", ",", "qloss", "=", "self", ".", "forward_step", "(", "x", ")", "\n", "aeloss", ",", "log_dict_ae", "=", "self", ".", "loss", "(", "qloss", ",", "x", ",", "xrec", ",", "split", "=", "\"train\"", ")", "\n", "self", ".", "log_dict", ".", "update", "(", "log_dict_ae", ")", "\n", "return", "aeloss", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQSegmentationModel.to_rgb": [[194, 198], ["torch.conv2d", "torch.conv2d", "torch.conv2d.max", "torch.conv2d.min", "torch.conv2d.min"], "methods", ["None"], ["", "def", "to_rgb", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "F", ".", "conv2d", "(", "x", ",", "weight", "=", "self", ".", "colorize", ")", "\n", "x", "=", "2.", "*", "(", "x", "-", "x", ".", "min", "(", ")", ")", "/", "(", "x", ".", "max", "(", ")", "-", "x", ".", "min", "(", ")", ")", "-", "1.", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQSegmentationModel.inference": [[199, 243], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "vqgan_model.VQSegmentationModel.encoder.eval", "vqgan_model.VQSegmentationModel.decoder.eval", "vqgan_model.VQSegmentationModel.quantize.eval", "vqgan_model.VQSegmentationModel.quant_conv.eval", "vqgan_model.VQSegmentationModel.post_quant_conv.eval", "enumerate", "vqgan_model.VQSegmentationModel.feed_data", "vqgan_model.VQSegmentationModel.forward_step", "vqgan_model.VQSegmentationModel.loss", "vqgan_model.VQSegmentationModel.size", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "img_cat.clamp_.clamp_.clamp_", "torchvision.utils.save_image", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.one_hot", "torch.one_hot", "vqgan_model.VQSegmentationModel.squeeze().permute().float", "vqgan_model.VQSegmentationModel.to_rgb", "vqgan_model.VQSegmentationModel.to_rgb", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "vqgan_model.VQSegmentationModel.squeeze().permute", "vqgan_model.VQSegmentationModel.squeeze"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.feed_data", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.forward_step", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQSegmentationModel.to_rgb", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQSegmentationModel.to_rgb"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "inference", "(", "self", ",", "data_loader", ",", "save_dir", ")", ":", "\n", "        ", "self", ".", "encoder", ".", "eval", "(", ")", "\n", "self", ".", "decoder", ".", "eval", "(", ")", "\n", "self", ".", "quantize", ".", "eval", "(", ")", "\n", "self", ".", "quant_conv", ".", "eval", "(", ")", "\n", "self", ".", "post_quant_conv", ".", "eval", "(", ")", "\n", "\n", "loss_total", "=", "0", "\n", "loss_bce", "=", "0", "\n", "loss_quant", "=", "0", "\n", "num", "=", "0", "\n", "\n", "for", "_", ",", "data", "in", "enumerate", "(", "data_loader", ")", ":", "\n", "            ", "img_name", "=", "data", "[", "'img_name'", "]", "[", "0", "]", "\n", "x", "=", "self", ".", "feed_data", "(", "data", ")", "\n", "xrec", ",", "qloss", "=", "self", ".", "forward_step", "(", "x", ")", "\n", "_", ",", "log_dict_ae", "=", "self", ".", "loss", "(", "qloss", ",", "x", ",", "xrec", ",", "split", "=", "\"val\"", ")", "\n", "\n", "loss_total", "+=", "log_dict_ae", "[", "'val/total_loss'", "]", "\n", "loss_bce", "+=", "log_dict_ae", "[", "'val/bce_loss'", "]", "\n", "loss_quant", "+=", "log_dict_ae", "[", "'val/quant_loss'", "]", "\n", "\n", "num", "+=", "x", ".", "size", "(", "0", ")", "\n", "\n", "if", "x", ".", "shape", "[", "1", "]", ">", "3", ":", "\n", "# colorize with random projection", "\n", "                ", "assert", "xrec", ".", "shape", "[", "1", "]", ">", "3", "\n", "# convert logits to indices", "\n", "xrec", "=", "torch", ".", "argmax", "(", "xrec", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "xrec", "=", "F", ".", "one_hot", "(", "xrec", ",", "num_classes", "=", "x", ".", "shape", "[", "1", "]", ")", "\n", "xrec", "=", "xrec", ".", "squeeze", "(", "1", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "float", "(", ")", "\n", "x", "=", "self", ".", "to_rgb", "(", "x", ")", "\n", "xrec", "=", "self", ".", "to_rgb", "(", "xrec", ")", "\n", "\n", "", "img_cat", "=", "torch", ".", "cat", "(", "[", "x", ",", "xrec", "]", ",", "dim", "=", "3", ")", ".", "detach", "(", ")", "\n", "img_cat", "=", "(", "(", "img_cat", "+", "1", ")", "/", "2", ")", "\n", "img_cat", "=", "img_cat", ".", "clamp_", "(", "0", ",", "1", ")", "\n", "save_image", "(", "\n", "img_cat", ",", "f'{save_dir}/{img_name}.png'", ",", "nrow", "=", "1", ",", "padding", "=", "4", ")", "\n", "\n", "", "return", "(", "loss_total", "/", "num", ")", ".", "item", "(", ")", ",", "(", "loss_bce", "/", "\n", "num", ")", ".", "item", "(", ")", ",", "(", "loss_quant", "/", "\n", "num", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.__init__": [[247, 262], ["vqgan_model.VQModel.__init__", "models.archs.vqgan_arch.Discriminator().to", "lpips.LPIPS().to", "vqgan_model.VQImageModel.disc.train", "vqgan_model.VQImageModel.init_training_settings", "models.archs.vqgan_arch.Discriminator", "lpips.LPIPS"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.init_training_settings"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "opt", ")", "\n", "self", ".", "disc", "=", "Discriminator", "(", "\n", "opt", "[", "'n_channels'", "]", ",", "opt", "[", "'ndf'", "]", ",", "\n", "n_layers", "=", "opt", "[", "'disc_layers'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "perceptual", "=", "lpips", ".", "LPIPS", "(", "net", "=", "\"vgg\"", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "perceptual_weight", "=", "opt", "[", "'perceptual_weight'", "]", "\n", "self", ".", "disc_start_step", "=", "opt", "[", "'disc_start_step'", "]", "\n", "self", ".", "disc_weight_max", "=", "opt", "[", "'disc_weight_max'", "]", "\n", "self", ".", "diff_aug", "=", "opt", "[", "'diff_aug'", "]", "\n", "self", ".", "policy", "=", "\"color,translation\"", "\n", "\n", "self", ".", "disc", ".", "train", "(", ")", "\n", "\n", "self", ".", "init_training_settings", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.feed_data": [[263, 267], ["x.float().to", "x.float"], "methods", ["None"], ["", "def", "feed_data", "(", "self", ",", "data", ")", ":", "\n", "        ", "x", "=", "data", "[", "'image'", "]", "\n", "\n", "return", "x", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.init_training_settings": [[268, 271], ["collections.OrderedDict", "vqgan_model.VQImageModel.configure_optimizers"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.configure_optimizers"], ["", "def", "init_training_settings", "(", "self", ")", ":", "\n", "        ", "self", ".", "log_dict", "=", "OrderedDict", "(", ")", "\n", "self", ".", "configure_optimizers", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.configure_optimizers": [[272, 282], ["torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "vqgan_model.VQImageModel.disc.parameters", "list", "list", "vqgan_model.VQImageModel.post_quant_conv.parameters", "list", "vqgan_model.VQImageModel.quant_conv.parameters", "list", "list", "vqgan_model.VQImageModel.quantize.parameters", "vqgan_model.VQImageModel.encoder.parameters", "vqgan_model.VQImageModel.decoder.parameters"], "methods", ["None"], ["", "def", "configure_optimizers", "(", "self", ")", ":", "\n", "        ", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "\n", "list", "(", "self", ".", "encoder", ".", "parameters", "(", ")", ")", "+", "list", "(", "self", ".", "decoder", ".", "parameters", "(", ")", ")", "+", "\n", "list", "(", "self", ".", "quantize", ".", "parameters", "(", ")", ")", "+", "\n", "list", "(", "self", ".", "quant_conv", ".", "parameters", "(", ")", ")", "+", "\n", "list", "(", "self", ".", "post_quant_conv", ".", "parameters", "(", ")", ")", ",", "\n", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", ")", "\n", "\n", "self", ".", "disc_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "\n", "self", ".", "disc", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "opt", "[", "'lr'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.training_step": [[283, 328], ["vqgan_model.VQImageModel.feed_data", "vqgan_model.VQImageModel.forward_step", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "vqgan_model.VQImageModel.perceptual", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "vqgan_model.VQImageModel.disc", "models.losses.vqgan_loss.calculate_adaptive_weight", "models.losses.vqgan_loss.adopt_weight", "torch.abs.mean().item", "torch.abs.mean().item", "vqgan_model.VQImageModel.mean().item", "torch.mean.item", "torch.mean.item", "g_loss.item", "codebook_loss.item", "vqgan_model.VQImageModel.contiguous", "models.losses.vqgan_loss.DiffAugment.contiguous", "models.losses.vqgan_loss.DiffAugment", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "vqgan_model.VQImageModel.disc", "models.losses.vqgan_loss.hinge_d_loss", "vqgan_model.VQImageModel.contiguous", "models.losses.vqgan_loss.DiffAugment.contiguous", "torch.abs.mean", "torch.abs.mean", "vqgan_model.VQImageModel.mean", "vqgan_model.VQImageModel.disc", "vqgan_model.VQImageModel.disc", "models.losses.vqgan_loss.DiffAugment.contiguous().detach", "models.losses.vqgan_loss.DiffAugment", "vqgan_model.VQImageModel.contiguous().detach", "vqgan_model.VQImageModel.contiguous().detach", "models.losses.vqgan_loss.DiffAugment.contiguous", "vqgan_model.VQImageModel.contiguous", "vqgan_model.VQImageModel.contiguous"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.feed_data", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.forward_step", "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.calculate_adaptive_weight", "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.adopt_weight", "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.DiffAugment", "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.hinge_d_loss", "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.DiffAugment"], ["", "def", "training_step", "(", "self", ",", "data", ",", "step", ")", ":", "\n", "        ", "x", "=", "self", ".", "feed_data", "(", "data", ")", "\n", "xrec", ",", "codebook_loss", "=", "self", ".", "forward_step", "(", "x", ")", "\n", "\n", "# get recon/perceptual loss", "\n", "recon_loss", "=", "torch", ".", "abs", "(", "x", ".", "contiguous", "(", ")", "-", "xrec", ".", "contiguous", "(", ")", ")", "\n", "p_loss", "=", "self", ".", "perceptual", "(", "x", ".", "contiguous", "(", ")", ",", "xrec", ".", "contiguous", "(", ")", ")", "\n", "nll_loss", "=", "recon_loss", "+", "self", ".", "perceptual_weight", "*", "p_loss", "\n", "nll_loss", "=", "torch", ".", "mean", "(", "nll_loss", ")", "\n", "\n", "# augment for input to discriminator", "\n", "if", "self", ".", "diff_aug", ":", "\n", "            ", "xrec", "=", "DiffAugment", "(", "xrec", ",", "policy", "=", "self", ".", "policy", ")", "\n", "\n", "# update generator", "\n", "", "logits_fake", "=", "self", ".", "disc", "(", "xrec", ")", "\n", "g_loss", "=", "-", "torch", ".", "mean", "(", "logits_fake", ")", "\n", "last_layer", "=", "self", ".", "decoder", ".", "conv_out", ".", "weight", "\n", "d_weight", "=", "calculate_adaptive_weight", "(", "nll_loss", ",", "g_loss", ",", "last_layer", ",", "\n", "self", ".", "disc_weight_max", ")", "\n", "d_weight", "*=", "adopt_weight", "(", "1", ",", "step", ",", "self", ".", "disc_start_step", ")", "\n", "loss", "=", "nll_loss", "+", "d_weight", "*", "g_loss", "+", "codebook_loss", "\n", "\n", "self", ".", "log_dict", "[", "\"loss\"", "]", "=", "loss", "\n", "self", ".", "log_dict", "[", "\"l1\"", "]", "=", "recon_loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "log_dict", "[", "\"perceptual\"", "]", "=", "p_loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "log_dict", "[", "\"nll_loss\"", "]", "=", "nll_loss", ".", "item", "(", ")", "\n", "self", ".", "log_dict", "[", "\"g_loss\"", "]", "=", "g_loss", ".", "item", "(", ")", "\n", "self", ".", "log_dict", "[", "\"d_weight\"", "]", "=", "d_weight", "\n", "self", ".", "log_dict", "[", "\"codebook_loss\"", "]", "=", "codebook_loss", ".", "item", "(", ")", "\n", "\n", "if", "step", ">", "self", ".", "disc_start_step", ":", "\n", "            ", "if", "self", ".", "diff_aug", ":", "\n", "                ", "logits_real", "=", "self", ".", "disc", "(", "\n", "DiffAugment", "(", "x", ".", "contiguous", "(", ")", ".", "detach", "(", ")", ",", "policy", "=", "self", ".", "policy", ")", ")", "\n", "", "else", ":", "\n", "                ", "logits_real", "=", "self", ".", "disc", "(", "x", ".", "contiguous", "(", ")", ".", "detach", "(", ")", ")", "\n", "", "logits_fake", "=", "self", ".", "disc", "(", "xrec", ".", "contiguous", "(", ")", ".", "detach", "(", "\n", ")", ")", "# detach so that generator isn\"t also updated", "\n", "d_loss", "=", "hinge_d_loss", "(", "logits_real", ",", "logits_fake", ")", "\n", "self", ".", "log_dict", "[", "\"d_loss\"", "]", "=", "d_loss", "\n", "", "else", ":", "\n", "            ", "d_loss", "=", "None", "\n", "\n", "", "return", "loss", ",", "d_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.optimize_parameters": [[329, 345], ["vqgan_model.VQImageModel.encoder.train", "vqgan_model.VQImageModel.decoder.train", "vqgan_model.VQImageModel.quantize.train", "vqgan_model.VQImageModel.quant_conv.train", "vqgan_model.VQImageModel.post_quant_conv.train", "vqgan_model.VQImageModel.training_step", "vqgan_model.VQImageModel.optimizer.zero_grad", "loss.backward", "vqgan_model.VQImageModel.optimizer.step", "vqgan_model.VQImageModel.disc_optimizer.zero_grad", "d_loss.backward", "vqgan_model.VQImageModel.disc_optimizer.step"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.training_step"], ["", "def", "optimize_parameters", "(", "self", ",", "data", ",", "step", ")", ":", "\n", "        ", "self", ".", "encoder", ".", "train", "(", ")", "\n", "self", ".", "decoder", ".", "train", "(", ")", "\n", "self", ".", "quantize", ".", "train", "(", ")", "\n", "self", ".", "quant_conv", ".", "train", "(", ")", "\n", "self", ".", "post_quant_conv", ".", "train", "(", ")", "\n", "\n", "loss", ",", "d_loss", "=", "self", ".", "training_step", "(", "data", ",", "step", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "step", ">", "self", ".", "disc_start_step", ":", "\n", "            ", "self", ".", "disc_optimizer", ".", "zero_grad", "(", ")", "\n", "d_loss", ".", "backward", "(", ")", "\n", "self", ".", "disc_optimizer", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.inference": [[346, 387], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "vqgan_model.VQImageModel.encoder.eval", "vqgan_model.VQImageModel.decoder.eval", "vqgan_model.VQImageModel.quantize.eval", "vqgan_model.VQImageModel.quant_conv.eval", "vqgan_model.VQImageModel.post_quant_conv.eval", "enumerate", "vqgan_model.VQImageModel.feed_data", "vqgan_model.VQImageModel.forward_step", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "vqgan_model.VQImageModel.perceptual", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "vqgan_model.VQImageModel.size", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "img_cat.clamp_.clamp_.clamp_", "torchvision.utils.save_image", "vqgan_model.VQImageModel.contiguous", "vqgan_model.VQImageModel.contiguous", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.one_hot", "torch.one_hot", "vqgan_model.VQImageModel.squeeze().permute().float", "vqgan_model.VQImageModel.to_rgb", "vqgan_model.VQImageModel.to_rgb", "vqgan_model.VQImageModel.contiguous", "vqgan_model.VQImageModel.contiguous", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "vqgan_model.VQImageModel.squeeze().permute", "vqgan_model.VQImageModel.squeeze"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.feed_data", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.forward_step", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQSegmentationModel.to_rgb", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQSegmentationModel.to_rgb"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "inference", "(", "self", ",", "data_loader", ",", "save_dir", ")", ":", "\n", "        ", "self", ".", "encoder", ".", "eval", "(", ")", "\n", "self", ".", "decoder", ".", "eval", "(", ")", "\n", "self", ".", "quantize", ".", "eval", "(", ")", "\n", "self", ".", "quant_conv", ".", "eval", "(", ")", "\n", "self", ".", "post_quant_conv", ".", "eval", "(", ")", "\n", "\n", "loss_total", "=", "0", "\n", "num", "=", "0", "\n", "\n", "for", "_", ",", "data", "in", "enumerate", "(", "data_loader", ")", ":", "\n", "            ", "img_name", "=", "data", "[", "'img_name'", "]", "[", "0", "]", "\n", "x", "=", "self", ".", "feed_data", "(", "data", ")", "\n", "xrec", ",", "_", "=", "self", ".", "forward_step", "(", "x", ")", "\n", "\n", "recon_loss", "=", "torch", ".", "abs", "(", "x", ".", "contiguous", "(", ")", "-", "xrec", ".", "contiguous", "(", ")", ")", "\n", "p_loss", "=", "self", ".", "perceptual", "(", "x", ".", "contiguous", "(", ")", ",", "xrec", ".", "contiguous", "(", ")", ")", "\n", "nll_loss", "=", "recon_loss", "+", "self", ".", "perceptual_weight", "*", "p_loss", "\n", "nll_loss", "=", "torch", ".", "mean", "(", "nll_loss", ")", "\n", "loss_total", "+=", "nll_loss", "\n", "\n", "num", "+=", "x", ".", "size", "(", "0", ")", "\n", "\n", "if", "x", ".", "shape", "[", "1", "]", ">", "3", ":", "\n", "# colorize with random projection", "\n", "                ", "assert", "xrec", ".", "shape", "[", "1", "]", ">", "3", "\n", "# convert logits to indices", "\n", "xrec", "=", "torch", ".", "argmax", "(", "xrec", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "xrec", "=", "F", ".", "one_hot", "(", "xrec", ",", "num_classes", "=", "x", ".", "shape", "[", "1", "]", ")", "\n", "xrec", "=", "xrec", ".", "squeeze", "(", "1", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "float", "(", ")", "\n", "x", "=", "self", ".", "to_rgb", "(", "x", ")", "\n", "xrec", "=", "self", ".", "to_rgb", "(", "xrec", ")", "\n", "\n", "", "img_cat", "=", "torch", ".", "cat", "(", "[", "x", ",", "xrec", "]", ",", "dim", "=", "3", ")", ".", "detach", "(", ")", "\n", "img_cat", "=", "(", "(", "img_cat", "+", "1", ")", "/", "2", ")", "\n", "img_cat", "=", "img_cat", ".", "clamp_", "(", "0", ",", "1", ")", "\n", "save_image", "(", "\n", "img_cat", ",", "f'{save_dir}/{img_name}.png'", ",", "nrow", "=", "1", ",", "padding", "=", "4", ")", "\n", "\n", "", "return", "(", "loss_total", "/", "num", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.__init__": [[391, 437], ["torch.device", "torch.device", "torch.device", "torch.device", "models.archs.vqgan_arch.Encoder().to", "models.archs.vqgan_arch.Decoder().to", "models.archs.vqgan_arch.VectorQuantizerTexture().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "torch.nn.Conv2d().to", "models.archs.vqgan_arch.Discriminator().to", "lpips.LPIPS().to", "vqgan_model.VQImageSegmTextureModel.disc.train", "vqgan_model.VQImageSegmTextureModel.init_training_settings", "models.archs.vqgan_arch.Encoder", "models.archs.vqgan_arch.Decoder", "models.archs.vqgan_arch.VectorQuantizerTexture", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "models.archs.vqgan_arch.Discriminator", "lpips.LPIPS"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageModel.init_training_settings"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "self", ".", "opt", "=", "opt", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "'cuda'", ")", "\n", "self", ".", "encoder", "=", "Encoder", "(", "\n", "ch", "=", "opt", "[", "'ch'", "]", ",", "\n", "num_res_blocks", "=", "opt", "[", "'num_res_blocks'", "]", ",", "\n", "attn_resolutions", "=", "opt", "[", "'attn_resolutions'", "]", ",", "\n", "ch_mult", "=", "opt", "[", "'ch_mult'", "]", ",", "\n", "in_channels", "=", "opt", "[", "'in_channels'", "]", ",", "\n", "resolution", "=", "opt", "[", "'resolution'", "]", ",", "\n", "z_channels", "=", "opt", "[", "'z_channels'", "]", ",", "\n", "double_z", "=", "opt", "[", "'double_z'", "]", ",", "\n", "dropout", "=", "opt", "[", "'dropout'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "decoder", "=", "Decoder", "(", "\n", "in_channels", "=", "opt", "[", "'in_channels'", "]", ",", "\n", "resolution", "=", "opt", "[", "'resolution'", "]", ",", "\n", "z_channels", "=", "opt", "[", "'z_channels'", "]", ",", "\n", "ch", "=", "opt", "[", "'ch'", "]", ",", "\n", "out_ch", "=", "opt", "[", "'out_ch'", "]", ",", "\n", "num_res_blocks", "=", "opt", "[", "'num_res_blocks'", "]", ",", "\n", "attn_resolutions", "=", "opt", "[", "'attn_resolutions'", "]", ",", "\n", "ch_mult", "=", "opt", "[", "'ch_mult'", "]", ",", "\n", "dropout", "=", "opt", "[", "'dropout'", "]", ",", "\n", "resamp_with_conv", "=", "True", ",", "\n", "give_pre_end", "=", "False", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "quantize", "=", "VectorQuantizerTexture", "(", "\n", "opt", "[", "'n_embed'", "]", ",", "opt", "[", "'embed_dim'", "]", ",", "beta", "=", "0.25", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "quant_conv", "=", "torch", ".", "nn", ".", "Conv2d", "(", "opt", "[", "\"z_channels\"", "]", ",", "opt", "[", "'embed_dim'", "]", ",", "\n", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "post_quant_conv", "=", "torch", ".", "nn", ".", "Conv2d", "(", "opt", "[", "'embed_dim'", "]", ",", "\n", "opt", "[", "\"z_channels\"", "]", ",", "\n", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "self", ".", "disc", "=", "Discriminator", "(", "\n", "opt", "[", "'n_channels'", "]", ",", "opt", "[", "'ndf'", "]", ",", "\n", "n_layers", "=", "opt", "[", "'disc_layers'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "perceptual", "=", "lpips", ".", "LPIPS", "(", "net", "=", "\"vgg\"", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "perceptual_weight", "=", "opt", "[", "'perceptual_weight'", "]", "\n", "self", ".", "disc_start_step", "=", "opt", "[", "'disc_start_step'", "]", "\n", "self", ".", "disc_weight_max", "=", "opt", "[", "'disc_weight_max'", "]", "\n", "self", ".", "diff_aug", "=", "opt", "[", "'diff_aug'", "]", "\n", "self", ".", "policy", "=", "\"color,translation\"", "\n", "\n", "self", ".", "disc", ".", "train", "(", ")", "\n", "\n", "self", ".", "init_training_settings", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.feed_data": [[438, 443], ["data[].float().to", "data[].float().to", "data[].float", "data[].float"], "methods", ["None"], ["", "def", "feed_data", "(", "self", ",", "data", ")", ":", "\n", "        ", "x", "=", "data", "[", "'image'", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "mask", "=", "data", "[", "'texture_mask'", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "return", "x", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.training_step": [[444, 489], ["vqgan_model.VQImageSegmTextureModel.feed_data", "vqgan_model.VQImageSegmTextureModel.forward_step", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "vqgan_model.VQImageSegmTextureModel.perceptual", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "vqgan_model.VQImageSegmTextureModel.disc", "models.losses.vqgan_loss.calculate_adaptive_weight", "models.losses.vqgan_loss.adopt_weight", "torch.abs.mean().item", "torch.abs.mean().item", "vqgan_model.VQImageSegmTextureModel.mean().item", "torch.mean.item", "torch.mean.item", "g_loss.item", "codebook_loss.item", "x.contiguous", "models.losses.vqgan_loss.DiffAugment.contiguous", "models.losses.vqgan_loss.DiffAugment", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "vqgan_model.VQImageSegmTextureModel.disc", "models.losses.vqgan_loss.hinge_d_loss", "x.contiguous", "models.losses.vqgan_loss.DiffAugment.contiguous", "torch.abs.mean", "torch.abs.mean", "vqgan_model.VQImageSegmTextureModel.mean", "vqgan_model.VQImageSegmTextureModel.disc", "vqgan_model.VQImageSegmTextureModel.disc", "models.losses.vqgan_loss.DiffAugment.contiguous().detach", "models.losses.vqgan_loss.DiffAugment", "x.contiguous().detach", "x.contiguous().detach", "models.losses.vqgan_loss.DiffAugment.contiguous", "x.contiguous", "x.contiguous"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.feed_data", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.forward_step", "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.calculate_adaptive_weight", "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.adopt_weight", "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.DiffAugment", "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.hinge_d_loss", "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.DiffAugment"], ["", "def", "training_step", "(", "self", ",", "data", ",", "step", ")", ":", "\n", "        ", "x", ",", "mask", "=", "self", ".", "feed_data", "(", "data", ")", "\n", "xrec", ",", "codebook_loss", "=", "self", ".", "forward_step", "(", "x", ",", "mask", ")", "\n", "\n", "# get recon/perceptual loss", "\n", "recon_loss", "=", "torch", ".", "abs", "(", "x", ".", "contiguous", "(", ")", "-", "xrec", ".", "contiguous", "(", ")", ")", "\n", "p_loss", "=", "self", ".", "perceptual", "(", "x", ".", "contiguous", "(", ")", ",", "xrec", ".", "contiguous", "(", ")", ")", "\n", "nll_loss", "=", "recon_loss", "+", "self", ".", "perceptual_weight", "*", "p_loss", "\n", "nll_loss", "=", "torch", ".", "mean", "(", "nll_loss", ")", "\n", "\n", "# augment for input to discriminator", "\n", "if", "self", ".", "diff_aug", ":", "\n", "            ", "xrec", "=", "DiffAugment", "(", "xrec", ",", "policy", "=", "self", ".", "policy", ")", "\n", "\n", "# update generator", "\n", "", "logits_fake", "=", "self", ".", "disc", "(", "xrec", ")", "\n", "g_loss", "=", "-", "torch", ".", "mean", "(", "logits_fake", ")", "\n", "last_layer", "=", "self", ".", "decoder", ".", "conv_out", ".", "weight", "\n", "d_weight", "=", "calculate_adaptive_weight", "(", "nll_loss", ",", "g_loss", ",", "last_layer", ",", "\n", "self", ".", "disc_weight_max", ")", "\n", "d_weight", "*=", "adopt_weight", "(", "1", ",", "step", ",", "self", ".", "disc_start_step", ")", "\n", "loss", "=", "nll_loss", "+", "d_weight", "*", "g_loss", "+", "codebook_loss", "\n", "\n", "self", ".", "log_dict", "[", "\"loss\"", "]", "=", "loss", "\n", "self", ".", "log_dict", "[", "\"l1\"", "]", "=", "recon_loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "log_dict", "[", "\"perceptual\"", "]", "=", "p_loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "log_dict", "[", "\"nll_loss\"", "]", "=", "nll_loss", ".", "item", "(", ")", "\n", "self", ".", "log_dict", "[", "\"g_loss\"", "]", "=", "g_loss", ".", "item", "(", ")", "\n", "self", ".", "log_dict", "[", "\"d_weight\"", "]", "=", "d_weight", "\n", "self", ".", "log_dict", "[", "\"codebook_loss\"", "]", "=", "codebook_loss", ".", "item", "(", ")", "\n", "\n", "if", "step", ">", "self", ".", "disc_start_step", ":", "\n", "            ", "if", "self", ".", "diff_aug", ":", "\n", "                ", "logits_real", "=", "self", ".", "disc", "(", "\n", "DiffAugment", "(", "x", ".", "contiguous", "(", ")", ".", "detach", "(", ")", ",", "policy", "=", "self", ".", "policy", ")", ")", "\n", "", "else", ":", "\n", "                ", "logits_real", "=", "self", ".", "disc", "(", "x", ".", "contiguous", "(", ")", ".", "detach", "(", ")", ")", "\n", "", "logits_fake", "=", "self", ".", "disc", "(", "xrec", ".", "contiguous", "(", ")", ".", "detach", "(", "\n", ")", ")", "# detach so that generator isn\"t also updated", "\n", "d_loss", "=", "hinge_d_loss", "(", "logits_real", ",", "logits_fake", ")", "\n", "self", ".", "log_dict", "[", "\"d_loss\"", "]", "=", "d_loss", "\n", "", "else", ":", "\n", "            ", "d_loss", "=", "None", "\n", "\n", "", "return", "loss", ",", "d_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.inference": [[490, 531], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "vqgan_model.VQImageSegmTextureModel.encoder.eval", "vqgan_model.VQImageSegmTextureModel.decoder.eval", "vqgan_model.VQImageSegmTextureModel.quantize.eval", "vqgan_model.VQImageSegmTextureModel.quant_conv.eval", "vqgan_model.VQImageSegmTextureModel.post_quant_conv.eval", "enumerate", "vqgan_model.VQImageSegmTextureModel.feed_data", "vqgan_model.VQImageSegmTextureModel.forward_step", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "vqgan_model.VQImageSegmTextureModel.perceptual", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "vqgan_model.VQImageSegmTextureModel.size", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "img_cat.clamp_.clamp_.clamp_", "torchvision.utils.save_image", "vqgan_model.VQImageSegmTextureModel.contiguous", "vqgan_model.VQImageSegmTextureModel.contiguous", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.one_hot", "torch.one_hot", "vqgan_model.VQImageSegmTextureModel.squeeze().permute().float", "vqgan_model.VQImageSegmTextureModel.to_rgb", "vqgan_model.VQImageSegmTextureModel.to_rgb", "vqgan_model.VQImageSegmTextureModel.contiguous", "vqgan_model.VQImageSegmTextureModel.contiguous", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "vqgan_model.VQImageSegmTextureModel.squeeze().permute", "vqgan_model.VQImageSegmTextureModel.squeeze"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.feed_data", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.forward_step", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQSegmentationModel.to_rgb", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQSegmentationModel.to_rgb"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "inference", "(", "self", ",", "data_loader", ",", "save_dir", ")", ":", "\n", "        ", "self", ".", "encoder", ".", "eval", "(", ")", "\n", "self", ".", "decoder", ".", "eval", "(", ")", "\n", "self", ".", "quantize", ".", "eval", "(", ")", "\n", "self", ".", "quant_conv", ".", "eval", "(", ")", "\n", "self", ".", "post_quant_conv", ".", "eval", "(", ")", "\n", "\n", "loss_total", "=", "0", "\n", "num", "=", "0", "\n", "\n", "for", "_", ",", "data", "in", "enumerate", "(", "data_loader", ")", ":", "\n", "            ", "img_name", "=", "data", "[", "'img_name'", "]", "[", "0", "]", "\n", "x", ",", "mask", "=", "self", ".", "feed_data", "(", "data", ")", "\n", "xrec", ",", "_", "=", "self", ".", "forward_step", "(", "x", ",", "mask", ")", "\n", "\n", "recon_loss", "=", "torch", ".", "abs", "(", "x", ".", "contiguous", "(", ")", "-", "xrec", ".", "contiguous", "(", ")", ")", "\n", "p_loss", "=", "self", ".", "perceptual", "(", "x", ".", "contiguous", "(", ")", ",", "xrec", ".", "contiguous", "(", ")", ")", "\n", "nll_loss", "=", "recon_loss", "+", "self", ".", "perceptual_weight", "*", "p_loss", "\n", "nll_loss", "=", "torch", ".", "mean", "(", "nll_loss", ")", "\n", "loss_total", "+=", "nll_loss", "\n", "\n", "num", "+=", "x", ".", "size", "(", "0", ")", "\n", "\n", "if", "x", ".", "shape", "[", "1", "]", ">", "3", ":", "\n", "# colorize with random projection", "\n", "                ", "assert", "xrec", ".", "shape", "[", "1", "]", ">", "3", "\n", "# convert logits to indices", "\n", "xrec", "=", "torch", ".", "argmax", "(", "xrec", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "xrec", "=", "F", ".", "one_hot", "(", "xrec", ",", "num_classes", "=", "x", ".", "shape", "[", "1", "]", ")", "\n", "xrec", "=", "xrec", ".", "squeeze", "(", "1", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "float", "(", ")", "\n", "x", "=", "self", ".", "to_rgb", "(", "x", ")", "\n", "xrec", "=", "self", ".", "to_rgb", "(", "xrec", ")", "\n", "\n", "", "img_cat", "=", "torch", ".", "cat", "(", "[", "x", ",", "xrec", "]", ",", "dim", "=", "3", ")", ".", "detach", "(", ")", "\n", "img_cat", "=", "(", "(", "img_cat", "+", "1", ")", "/", "2", ")", "\n", "img_cat", "=", "img_cat", ".", "clamp_", "(", "0", ",", "1", ")", "\n", "save_image", "(", "\n", "img_cat", ",", "f'{save_dir}/{img_name}.png'", ",", "nrow", "=", "1", ",", "padding", "=", "4", ")", "\n", "\n", "", "return", "(", "loss_total", "/", "num", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.encode": [[532, 537], ["vqgan_model.VQImageSegmTextureModel.encoder", "vqgan_model.VQImageSegmTextureModel.quant_conv", "vqgan_model.VQImageSegmTextureModel.quantize"], "methods", ["None"], ["", "def", "encode", "(", "self", ",", "x", ",", "mask", ")", ":", "\n", "        ", "h", "=", "self", ".", "encoder", "(", "x", ")", "\n", "h", "=", "self", ".", "quant_conv", "(", "h", ")", "\n", "quant", ",", "emb_loss", ",", "info", "=", "self", ".", "quantize", "(", "h", ",", "mask", ")", "\n", "return", "quant", ",", "emb_loss", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.decode": [[538, 542], ["vqgan_model.VQImageSegmTextureModel.post_quant_conv", "vqgan_model.VQImageSegmTextureModel.decoder"], "methods", ["None"], ["", "def", "decode", "(", "self", ",", "quant", ")", ":", "\n", "        ", "quant", "=", "self", ".", "post_quant_conv", "(", "quant", ")", "\n", "dec", "=", "self", ".", "decoder", "(", "quant", ")", "\n", "return", "dec", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.decode_code": [[543, 547], ["vqgan_model.VQImageSegmTextureModel.quantize.embed_code", "vqgan_model.VQImageSegmTextureModel.decode"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.decode"], ["", "def", "decode_code", "(", "self", ",", "code_b", ")", ":", "\n", "        ", "quant_b", "=", "self", ".", "quantize", ".", "embed_code", "(", "code_b", ")", "\n", "dec", "=", "self", ".", "decode", "(", "quant_b", ")", "\n", "return", "dec", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.forward_step": [[548, 552], ["vqgan_model.VQImageSegmTextureModel.encode", "vqgan_model.VQImageSegmTextureModel.decode"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.encode", "home.repos.pwc.inspect_result.yumingj_Text2Human.models.vqgan_model.VQImageSegmTextureModel.decode"], ["", "def", "forward_step", "(", "self", ",", "input", ",", "mask", ")", ":", "\n", "        ", "quant", ",", "diff", ",", "_", "=", "self", ".", "encode", "(", "input", ",", "mask", ")", "\n", "dec", "=", "self", ".", "decode", "(", "quant", ")", "\n", "return", "dec", ",", "diff", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.unet_arch.UpConvBlock.__init__": [[50, 99], ["dict", "dict", "dict", "torch.Module.__init__", "conv_block", "mmcv.cnn.build_upsample_layer", "mmcv.cnn.ConvModule"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["def", "__init__", "(", "self", ",", "\n", "conv_block", ",", "\n", "in_channels", ",", "\n", "skip_channels", ",", "\n", "out_channels", ",", "\n", "num_convs", "=", "2", ",", "\n", "stride", "=", "1", ",", "\n", "dilation", "=", "1", ",", "\n", "with_cp", "=", "False", ",", "\n", "conv_cfg", "=", "None", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN'", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ")", ",", "\n", "upsample_cfg", "=", "dict", "(", "type", "=", "'InterpConv'", ")", ",", "\n", "dcn", "=", "None", ",", "\n", "plugins", "=", "None", ")", ":", "\n", "        ", "super", "(", "UpConvBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "dcn", "is", "None", ",", "'Not implemented yet.'", "\n", "assert", "plugins", "is", "None", ",", "'Not implemented yet.'", "\n", "\n", "self", ".", "conv_block", "=", "conv_block", "(", "\n", "in_channels", "=", "2", "*", "skip_channels", ",", "\n", "out_channels", "=", "out_channels", ",", "\n", "num_convs", "=", "num_convs", ",", "\n", "stride", "=", "stride", ",", "\n", "dilation", "=", "dilation", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ",", "\n", "dcn", "=", "None", ",", "\n", "plugins", "=", "None", ")", "\n", "if", "upsample_cfg", "is", "not", "None", ":", "\n", "            ", "self", ".", "upsample", "=", "build_upsample_layer", "(", "\n", "cfg", "=", "upsample_cfg", ",", "\n", "in_channels", "=", "in_channels", ",", "\n", "out_channels", "=", "skip_channels", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "upsample", "=", "ConvModule", "(", "\n", "in_channels", ",", "\n", "skip_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.unet_arch.UpConvBlock.forward": [[100, 108], ["unet_arch.UpConvBlock.upsample", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "unet_arch.UpConvBlock.conv_block"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "skip", ",", "x", ")", ":", "\n", "        ", "\"\"\"Forward function.\"\"\"", "\n", "\n", "x", "=", "self", ".", "upsample", "(", "x", ")", "\n", "out", "=", "torch", ".", "cat", "(", "[", "skip", ",", "x", "]", ",", "dim", "=", "1", ")", "\n", "out", "=", "self", ".", "conv_block", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.unet_arch.BasicConvBlock.__init__": [[140, 172], ["dict", "dict", "torch.Module.__init__", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "convs.append", "mmcv.cnn.ConvModule"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "num_convs", "=", "2", ",", "\n", "stride", "=", "1", ",", "\n", "dilation", "=", "1", ",", "\n", "with_cp", "=", "False", ",", "\n", "conv_cfg", "=", "None", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN'", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ")", ",", "\n", "dcn", "=", "None", ",", "\n", "plugins", "=", "None", ")", ":", "\n", "        ", "super", "(", "BasicConvBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "dcn", "is", "None", ",", "'Not implemented yet.'", "\n", "assert", "plugins", "is", "None", ",", "'Not implemented yet.'", "\n", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "convs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_convs", ")", ":", "\n", "            ", "convs", ".", "append", "(", "\n", "ConvModule", "(", "\n", "in_channels", "=", "in_channels", "if", "i", "==", "0", "else", "out_channels", ",", "\n", "out_channels", "=", "out_channels", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "stride", "if", "i", "==", "0", "else", "1", ",", "\n", "dilation", "=", "1", "if", "i", "==", "0", "else", "dilation", ",", "\n", "padding", "=", "1", "if", "i", "==", "0", "else", "dilation", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ")", ")", "\n", "\n", "", "self", ".", "convs", "=", "nn", ".", "Sequential", "(", "*", "convs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.unet_arch.BasicConvBlock.forward": [[173, 181], ["torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "unet_arch.BasicConvBlock.convs"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Forward function.\"\"\"", "\n", "\n", "if", "self", ".", "with_cp", "and", "x", ".", "requires_grad", ":", "\n", "            ", "out", "=", "cp", ".", "checkpoint", "(", "self", ".", "convs", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "self", ".", "convs", "(", "x", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.unet_arch.DeconvModule.__init__": [[201, 232], ["dict", "dict", "torch.Module.__init__", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "mmcv.cnn.build_norm_layer", "mmcv.cnn.build_activation_layer", "torch.Sequential", "torch.Sequential", "torch.Sequential"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "with_cp", "=", "False", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN'", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ")", ",", "\n", "*", ",", "\n", "kernel_size", "=", "4", ",", "\n", "scale_factor", "=", "2", ")", ":", "\n", "        ", "super", "(", "DeconvModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "(", "kernel_size", "-", "scale_factor", ">=", "0", ")", "and", "(", "kernel_size", "-", "scale_factor", ")", "%", "2", "==", "0", ",", "f'kernel_size should be greater than or equal to scale_factor '", "f'and (kernel_size - scale_factor) should be even numbers, '", "f'while the kernel size is {kernel_size} and scale_factor is '", "f'{scale_factor}.'", "\n", "\n", "stride", "=", "scale_factor", "\n", "padding", "=", "(", "kernel_size", "-", "scale_factor", ")", "//", "2", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "deconv", "=", "nn", ".", "ConvTranspose2d", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "stride", "=", "stride", ",", "\n", "padding", "=", "padding", ")", "\n", "\n", "norm_name", ",", "norm", "=", "build_norm_layer", "(", "norm_cfg", ",", "out_channels", ")", "\n", "activate", "=", "build_activation_layer", "(", "act_cfg", ")", "\n", "self", ".", "deconv_upsamping", "=", "nn", ".", "Sequential", "(", "deconv", ",", "norm", ",", "activate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.unet_arch.DeconvModule.forward": [[233, 241], ["torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "unet_arch.DeconvModule.deconv_upsamping"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Forward function.\"\"\"", "\n", "\n", "if", "self", ".", "with_cp", "and", "x", ".", "requires_grad", ":", "\n", "            ", "out", "=", "cp", ".", "checkpoint", "(", "self", ".", "deconv_upsamping", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "self", ".", "deconv_upsamping", "(", "x", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.unet_arch.InterpConv.__init__": [[275, 306], ["dict", "dict", "dict", "torch.Module.__init__", "mmcv.cnn.ConvModule", "torch.Upsample", "torch.Upsample", "torch.Upsample", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "with_cp", "=", "False", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN'", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ")", ",", "\n", "*", ",", "\n", "conv_cfg", "=", "None", ",", "\n", "conv_first", "=", "False", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "upsampe_cfg", "=", "dict", "(", "\n", "scale_factor", "=", "2", ",", "mode", "=", "'bilinear'", ",", "align_corners", "=", "False", ")", ")", ":", "\n", "        ", "super", "(", "InterpConv", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "conv", "=", "ConvModule", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "stride", "=", "stride", ",", "\n", "padding", "=", "padding", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ")", "\n", "upsample", "=", "nn", ".", "Upsample", "(", "**", "upsampe_cfg", ")", "\n", "if", "conv_first", ":", "\n", "            ", "self", ".", "interp_upsample", "=", "nn", ".", "Sequential", "(", "conv", ",", "upsample", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "interp_upsample", "=", "nn", ".", "Sequential", "(", "upsample", ",", "conv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.unet_arch.InterpConv.forward": [[307, 315], ["torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "unet_arch.InterpConv.interp_upsample"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Forward function.\"\"\"", "\n", "\n", "if", "self", ".", "with_cp", "and", "x", ".", "requires_grad", ":", "\n", "            ", "out", "=", "cp", ".", "checkpoint", "(", "self", ".", "interp_upsample", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "self", ".", "interp_upsample", "(", "x", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.unet_arch.UNet.__init__": [[372, 469], ["dict", "dict", "dict", "torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "len", "len", "len", "len", "len", "len", "enc_conv_block.append", "unet_arch.UNet.encoder.append", "len", "len", "len", "len", "len", "len", "unet_arch.UNet.decoder.append", "unet_arch.BasicConvBlock", "torch.Sequential", "torch.Sequential", "torch.Sequential", "enc_conv_block.append", "unet_arch.UpConvBlock", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", "=", "3", ",", "\n", "base_channels", "=", "64", ",", "\n", "num_stages", "=", "5", ",", "\n", "strides", "=", "(", "1", ",", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "enc_num_convs", "=", "(", "2", ",", "2", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "dec_num_convs", "=", "(", "2", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "downsamples", "=", "(", "True", ",", "True", ",", "True", ",", "True", ")", ",", "\n", "enc_dilations", "=", "(", "1", ",", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "dec_dilations", "=", "(", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "with_cp", "=", "False", ",", "\n", "conv_cfg", "=", "None", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN'", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ")", ",", "\n", "upsample_cfg", "=", "dict", "(", "type", "=", "'InterpConv'", ")", ",", "\n", "norm_eval", "=", "False", ",", "\n", "dcn", "=", "None", ",", "\n", "plugins", "=", "None", ")", ":", "\n", "        ", "super", "(", "UNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "dcn", "is", "None", ",", "'Not implemented yet.'", "\n", "assert", "plugins", "is", "None", ",", "'Not implemented yet.'", "\n", "assert", "len", "(", "strides", ")", "==", "num_stages", ",", "'The length of strides should be equal to num_stages, '", "f'while the strides is {strides}, the length of '", "f'strides is {len(strides)}, and the num_stages is '", "f'{num_stages}.'", "\n", "assert", "len", "(", "enc_num_convs", ")", "==", "num_stages", ",", "'The length of enc_num_convs should be equal to num_stages, '", "f'while the enc_num_convs is {enc_num_convs}, the length of '", "f'enc_num_convs is {len(enc_num_convs)}, and the num_stages is '", "f'{num_stages}.'", "\n", "assert", "len", "(", "dec_num_convs", ")", "==", "(", "num_stages", "-", "1", ")", ",", "'The length of dec_num_convs should be equal to (num_stages-1), '", "f'while the dec_num_convs is {dec_num_convs}, the length of '", "f'dec_num_convs is {len(dec_num_convs)}, and the num_stages is '", "f'{num_stages}.'", "\n", "assert", "len", "(", "downsamples", ")", "==", "(", "num_stages", "-", "1", ")", ",", "'The length of downsamples should be equal to (num_stages-1), '", "f'while the downsamples is {downsamples}, the length of '", "f'downsamples is {len(downsamples)}, and the num_stages is '", "f'{num_stages}.'", "\n", "assert", "len", "(", "enc_dilations", ")", "==", "num_stages", ",", "'The length of enc_dilations should be equal to num_stages, '", "f'while the enc_dilations is {enc_dilations}, the length of '", "f'enc_dilations is {len(enc_dilations)}, and the num_stages is '", "f'{num_stages}.'", "\n", "assert", "len", "(", "dec_dilations", ")", "==", "(", "num_stages", "-", "1", ")", ",", "'The length of dec_dilations should be equal to (num_stages-1), '", "f'while the dec_dilations is {dec_dilations}, the length of '", "f'dec_dilations is {len(dec_dilations)}, and the num_stages is '", "f'{num_stages}.'", "\n", "self", ".", "num_stages", "=", "num_stages", "\n", "self", ".", "strides", "=", "strides", "\n", "self", ".", "downsamples", "=", "downsamples", "\n", "self", ".", "norm_eval", "=", "norm_eval", "\n", "\n", "self", ".", "encoder", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "decoder", "=", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "num_stages", ")", ":", "\n", "            ", "enc_conv_block", "=", "[", "]", "\n", "if", "i", "!=", "0", ":", "\n", "                ", "if", "strides", "[", "i", "]", "==", "1", "and", "downsamples", "[", "i", "-", "1", "]", ":", "\n", "                    ", "enc_conv_block", ".", "append", "(", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "2", ")", ")", "\n", "", "upsample", "=", "(", "strides", "[", "i", "]", "!=", "1", "or", "downsamples", "[", "i", "-", "1", "]", ")", "\n", "self", ".", "decoder", ".", "append", "(", "\n", "UpConvBlock", "(", "\n", "conv_block", "=", "BasicConvBlock", ",", "\n", "in_channels", "=", "base_channels", "*", "2", "**", "i", ",", "\n", "skip_channels", "=", "base_channels", "*", "2", "**", "(", "i", "-", "1", ")", ",", "\n", "out_channels", "=", "base_channels", "*", "2", "**", "(", "i", "-", "1", ")", ",", "\n", "num_convs", "=", "dec_num_convs", "[", "i", "-", "1", "]", ",", "\n", "stride", "=", "1", ",", "\n", "dilation", "=", "dec_dilations", "[", "i", "-", "1", "]", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ",", "\n", "upsample_cfg", "=", "upsample_cfg", "if", "upsample", "else", "None", ",", "\n", "dcn", "=", "None", ",", "\n", "plugins", "=", "None", ")", ")", "\n", "\n", "", "enc_conv_block", ".", "append", "(", "\n", "BasicConvBlock", "(", "\n", "in_channels", "=", "in_channels", ",", "\n", "out_channels", "=", "base_channels", "*", "2", "**", "i", ",", "\n", "num_convs", "=", "enc_num_convs", "[", "i", "]", ",", "\n", "stride", "=", "strides", "[", "i", "]", ",", "\n", "dilation", "=", "enc_dilations", "[", "i", "]", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ",", "\n", "dcn", "=", "None", ",", "\n", "plugins", "=", "None", ")", ")", "\n", "self", ".", "encoder", ".", "append", "(", "(", "nn", ".", "Sequential", "(", "*", "enc_conv_block", ")", ")", ")", "\n", "in_channels", "=", "base_channels", "*", "2", "**", "i", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.unet_arch.UNet.forward": [[470, 482], ["reversed", "enc", "enc_outs.append", "range", "dec_outs.append", "len"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "enc_outs", "=", "[", "]", "\n", "\n", "for", "enc", "in", "self", ".", "encoder", ":", "\n", "            ", "x", "=", "enc", "(", "x", ")", "\n", "enc_outs", ".", "append", "(", "x", ")", "\n", "", "dec_outs", "=", "[", "x", "]", "\n", "for", "i", "in", "reversed", "(", "range", "(", "len", "(", "self", ".", "decoder", ")", ")", ")", ":", "\n", "            ", "x", "=", "self", ".", "decoder", "[", "i", "]", "(", "enc_outs", "[", "i", "]", ",", "x", ")", "\n", "dec_outs", ".", "append", "(", "x", ")", "\n", "\n", "", "return", "dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.unet_arch.UNet.init_weights": [[483, 501], ["isinstance", "mmseg.utils.get_root_logger", "mmcv.runner.load_checkpoint", "unet_arch.UNet.modules", "TypeError", "isinstance", "mmcv.cnn.kaiming_init", "isinstance", "mmcv.cnn.constant_init"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.utils.logger.get_root_logger"], ["", "def", "init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initialize the weights in backbone.\n\n        Args:\n            pretrained (str, optional): Path to pre-trained weights.\n                Defaults to None.\n        \"\"\"", "\n", "if", "isinstance", "(", "pretrained", ",", "str", ")", ":", "\n", "            ", "logger", "=", "get_root_logger", "(", ")", "\n", "load_checkpoint", "(", "self", ",", "pretrained", ",", "strict", "=", "False", ",", "logger", "=", "logger", ")", "\n", "", "elif", "pretrained", "is", "None", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                    ", "kaiming_init", "(", "m", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "(", "_BatchNorm", ",", "nn", ".", "GroupNorm", ")", ")", ":", "\n", "                    ", "constant_init", "(", "m", ",", "1", ")", "\n", "", "", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'pretrained must be a str or None'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.unet_arch.ShapeUNet.__init__": [[558, 656], ["dict", "dict", "dict", "torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "len", "len", "len", "len", "len", "len", "enc_conv_block.append", "unet_arch.ShapeUNet.encoder.append", "len", "len", "len", "len", "len", "len", "unet_arch.ShapeUNet.decoder.append", "unet_arch.BasicConvBlock", "torch.Sequential", "torch.Sequential", "torch.Sequential", "enc_conv_block.append", "unet_arch.UpConvBlock", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", "=", "3", ",", "\n", "base_channels", "=", "64", ",", "\n", "num_stages", "=", "5", ",", "\n", "attr_embedding", "=", "128", ",", "\n", "strides", "=", "(", "1", ",", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "enc_num_convs", "=", "(", "2", ",", "2", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "dec_num_convs", "=", "(", "2", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "downsamples", "=", "(", "True", ",", "True", ",", "True", ",", "True", ")", ",", "\n", "enc_dilations", "=", "(", "1", ",", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "dec_dilations", "=", "(", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "with_cp", "=", "False", ",", "\n", "conv_cfg", "=", "None", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN'", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ")", ",", "\n", "upsample_cfg", "=", "dict", "(", "type", "=", "'InterpConv'", ")", ",", "\n", "norm_eval", "=", "False", ",", "\n", "dcn", "=", "None", ",", "\n", "plugins", "=", "None", ")", ":", "\n", "        ", "super", "(", "ShapeUNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "dcn", "is", "None", ",", "'Not implemented yet.'", "\n", "assert", "plugins", "is", "None", ",", "'Not implemented yet.'", "\n", "assert", "len", "(", "strides", ")", "==", "num_stages", ",", "'The length of strides should be equal to num_stages, '", "f'while the strides is {strides}, the length of '", "f'strides is {len(strides)}, and the num_stages is '", "f'{num_stages}.'", "\n", "assert", "len", "(", "enc_num_convs", ")", "==", "num_stages", ",", "'The length of enc_num_convs should be equal to num_stages, '", "f'while the enc_num_convs is {enc_num_convs}, the length of '", "f'enc_num_convs is {len(enc_num_convs)}, and the num_stages is '", "f'{num_stages}.'", "\n", "assert", "len", "(", "dec_num_convs", ")", "==", "(", "num_stages", "-", "1", ")", ",", "'The length of dec_num_convs should be equal to (num_stages-1), '", "f'while the dec_num_convs is {dec_num_convs}, the length of '", "f'dec_num_convs is {len(dec_num_convs)}, and the num_stages is '", "f'{num_stages}.'", "\n", "assert", "len", "(", "downsamples", ")", "==", "(", "num_stages", "-", "1", ")", ",", "'The length of downsamples should be equal to (num_stages-1), '", "f'while the downsamples is {downsamples}, the length of '", "f'downsamples is {len(downsamples)}, and the num_stages is '", "f'{num_stages}.'", "\n", "assert", "len", "(", "enc_dilations", ")", "==", "num_stages", ",", "'The length of enc_dilations should be equal to num_stages, '", "f'while the enc_dilations is {enc_dilations}, the length of '", "f'enc_dilations is {len(enc_dilations)}, and the num_stages is '", "f'{num_stages}.'", "\n", "assert", "len", "(", "dec_dilations", ")", "==", "(", "num_stages", "-", "1", ")", ",", "'The length of dec_dilations should be equal to (num_stages-1), '", "f'while the dec_dilations is {dec_dilations}, the length of '", "f'dec_dilations is {len(dec_dilations)}, and the num_stages is '", "f'{num_stages}.'", "\n", "self", ".", "num_stages", "=", "num_stages", "\n", "self", ".", "strides", "=", "strides", "\n", "self", ".", "downsamples", "=", "downsamples", "\n", "self", ".", "norm_eval", "=", "norm_eval", "\n", "\n", "self", ".", "encoder", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "decoder", "=", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "num_stages", ")", ":", "\n", "            ", "enc_conv_block", "=", "[", "]", "\n", "if", "i", "!=", "0", ":", "\n", "                ", "if", "strides", "[", "i", "]", "==", "1", "and", "downsamples", "[", "i", "-", "1", "]", ":", "\n", "                    ", "enc_conv_block", ".", "append", "(", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "2", ")", ")", "\n", "", "upsample", "=", "(", "strides", "[", "i", "]", "!=", "1", "or", "downsamples", "[", "i", "-", "1", "]", ")", "\n", "self", ".", "decoder", ".", "append", "(", "\n", "UpConvBlock", "(", "\n", "conv_block", "=", "BasicConvBlock", ",", "\n", "in_channels", "=", "base_channels", "*", "2", "**", "i", ",", "\n", "skip_channels", "=", "base_channels", "*", "2", "**", "(", "i", "-", "1", ")", ",", "\n", "out_channels", "=", "base_channels", "*", "2", "**", "(", "i", "-", "1", ")", ",", "\n", "num_convs", "=", "dec_num_convs", "[", "i", "-", "1", "]", ",", "\n", "stride", "=", "1", ",", "\n", "dilation", "=", "dec_dilations", "[", "i", "-", "1", "]", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ",", "\n", "upsample_cfg", "=", "upsample_cfg", "if", "upsample", "else", "None", ",", "\n", "dcn", "=", "None", ",", "\n", "plugins", "=", "None", ")", ")", "\n", "\n", "", "enc_conv_block", ".", "append", "(", "\n", "BasicConvBlock", "(", "\n", "in_channels", "=", "in_channels", "+", "attr_embedding", ",", "\n", "out_channels", "=", "base_channels", "*", "2", "**", "i", ",", "\n", "num_convs", "=", "enc_num_convs", "[", "i", "]", ",", "\n", "stride", "=", "strides", "[", "i", "]", ",", "\n", "dilation", "=", "enc_dilations", "[", "i", "]", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ",", "\n", "dcn", "=", "None", ",", "\n", "plugins", "=", "None", ")", ")", "\n", "self", ".", "encoder", ".", "append", "(", "(", "nn", ".", "Sequential", "(", "*", "enc_conv_block", ")", ")", ")", "\n", "in_channels", "=", "base_channels", "*", "2", "**", "i", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.unet_arch.ShapeUNet.forward": [[657, 675], ["attr_embedding.size", "reversed", "enc.size", "enc", "enc_outs.append", "range", "dec_outs.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "attr_embedding.view().expand", "attr_embedding.view"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "attr_embedding", ")", ":", "\n", "        ", "enc_outs", "=", "[", "]", "\n", "Be", ",", "Ce", "=", "attr_embedding", ".", "size", "(", ")", "\n", "for", "enc", "in", "self", ".", "encoder", ":", "\n", "            ", "_", ",", "_", ",", "H", ",", "W", "=", "x", ".", "size", "(", ")", "\n", "x", "=", "enc", "(", "\n", "torch", ".", "cat", "(", "[", "\n", "x", ",", "\n", "attr_embedding", ".", "view", "(", "Be", ",", "Ce", ",", "1", ",", "1", ")", ".", "expand", "(", "(", "Be", ",", "Ce", ",", "H", ",", "W", ")", ")", "\n", "]", ",", "\n", "dim", "=", "1", ")", ")", "\n", "enc_outs", ".", "append", "(", "x", ")", "\n", "", "dec_outs", "=", "[", "x", "]", "\n", "for", "i", "in", "reversed", "(", "range", "(", "len", "(", "self", ".", "decoder", ")", ")", ")", ":", "\n", "            ", "x", "=", "self", ".", "decoder", "[", "i", "]", "(", "enc_outs", "[", "i", "]", ",", "x", ")", "\n", "dec_outs", ".", "append", "(", "x", ")", "\n", "\n", "", "return", "dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.unet_arch.ShapeUNet.init_weights": [[676, 694], ["isinstance", "mmseg.utils.get_root_logger", "mmcv.runner.load_checkpoint", "unet_arch.ShapeUNet.modules", "TypeError", "isinstance", "mmcv.cnn.kaiming_init", "isinstance", "mmcv.cnn.constant_init"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.utils.logger.get_root_logger"], ["", "def", "init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initialize the weights in backbone.\n\n        Args:\n            pretrained (str, optional): Path to pre-trained weights.\n                Defaults to None.\n        \"\"\"", "\n", "if", "isinstance", "(", "pretrained", ",", "str", ")", ":", "\n", "            ", "logger", "=", "get_root_logger", "(", ")", "\n", "load_checkpoint", "(", "self", ",", "pretrained", ",", "strict", "=", "False", ",", "logger", "=", "logger", ")", "\n", "", "elif", "pretrained", "is", "None", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                    ", "kaiming_init", "(", "m", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "(", "_BatchNorm", ",", "nn", ".", "GroupNorm", ")", ")", ":", "\n", "                    ", "constant_init", "(", "m", ",", "1", ")", "\n", "", "", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'pretrained must be a str or None'", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.transformer_arch.CausalSelfAttention.__init__": [[16, 36], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "numpy.prod", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "transformer_arch.CausalSelfAttention.register_buffer", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.tril.view", "torch.tril.view", "torch.tril.view"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["def", "__init__", "(", "self", ",", "bert_n_emb", ",", "bert_n_head", ",", "attn_pdrop", ",", "resid_pdrop", ",", "\n", "latent_shape", ",", "sampler", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "bert_n_emb", "%", "bert_n_head", "==", "0", "\n", "# key, query, value projections for all heads", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "bert_n_emb", ",", "bert_n_emb", ")", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "bert_n_emb", ",", "bert_n_emb", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "bert_n_emb", ",", "bert_n_emb", ")", "\n", "# regularization", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_pdrop", ")", "\n", "self", ".", "resid_drop", "=", "nn", ".", "Dropout", "(", "resid_pdrop", ")", "\n", "# output projection", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "bert_n_emb", ",", "bert_n_emb", ")", "\n", "self", ".", "n_head", "=", "bert_n_head", "\n", "self", ".", "causal", "=", "True", "if", "sampler", "==", "'autoregressive'", "else", "False", "\n", "if", "self", ".", "causal", ":", "\n", "            ", "block_size", "=", "np", ".", "prod", "(", "latent_shape", ")", "\n", "mask", "=", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "block_size", ",", "block_size", ")", ")", "\n", "self", ".", "register_buffer", "(", "\"mask\"", ",", "mask", ".", "view", "(", "1", ",", "1", ",", "block_size", ",", "\n", "block_size", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.transformer_arch.CausalSelfAttention.forward": [[37, 72], ["x.size", "transformer_arch.CausalSelfAttention.key().view().transpose", "transformer_arch.CausalSelfAttention.query().view().transpose", "transformer_arch.CausalSelfAttention.value().view().transpose", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.softmax", "torch.softmax", "torch.softmax", "transformer_arch.CausalSelfAttention.attn_drop", "transformer_arch.CausalSelfAttention.transpose().contiguous().view", "transformer_arch.CausalSelfAttention.resid_drop", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "att.masked_fill.masked_fill.masked_fill", "transformer_arch.CausalSelfAttention.proj", "transformer_arch.CausalSelfAttention.key().view", "transformer_arch.CausalSelfAttention.query().view", "transformer_arch.CausalSelfAttention.value().view", "torch.cat.transpose", "torch.cat.transpose", "torch.cat.transpose", "math.sqrt", "float", "transformer_arch.CausalSelfAttention.transpose().contiguous", "torch.cat.size", "torch.cat.size", "torch.cat.size", "transformer_arch.CausalSelfAttention.key", "transformer_arch.CausalSelfAttention.query", "transformer_arch.CausalSelfAttention.value", "transformer_arch.CausalSelfAttention.transpose"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "layer_past", "=", "None", ")", ":", "\n", "        ", "B", ",", "T", ",", "C", "=", "x", ".", "size", "(", ")", "\n", "\n", "# calculate query, key, values for all heads in batch and move head forward to be the batch dim", "\n", "k", "=", "self", ".", "key", "(", "x", ")", ".", "view", "(", "B", ",", "T", ",", "self", ".", "n_head", ",", "\n", "C", "//", "self", ".", "n_head", ")", ".", "transpose", "(", "1", ",", "\n", "2", ")", "# (B, nh, T, hs)", "\n", "q", "=", "self", ".", "query", "(", "x", ")", ".", "view", "(", "B", ",", "T", ",", "self", ".", "n_head", ",", "\n", "C", "//", "self", ".", "n_head", ")", ".", "transpose", "(", "1", ",", "\n", "2", ")", "# (B, nh, T, hs)", "\n", "v", "=", "self", ".", "value", "(", "x", ")", ".", "view", "(", "B", ",", "T", ",", "self", ".", "n_head", ",", "\n", "C", "//", "self", ".", "n_head", ")", ".", "transpose", "(", "1", ",", "\n", "2", ")", "# (B, nh, T, hs)", "\n", "\n", "present", "=", "torch", ".", "stack", "(", "(", "k", ",", "v", ")", ")", "\n", "if", "self", ".", "causal", "and", "layer_past", "is", "not", "None", ":", "\n", "            ", "past_key", ",", "past_value", "=", "layer_past", "\n", "k", "=", "torch", ".", "cat", "(", "(", "past_key", ",", "k", ")", ",", "dim", "=", "-", "2", ")", "\n", "v", "=", "torch", ".", "cat", "(", "(", "past_value", ",", "v", ")", ",", "dim", "=", "-", "2", ")", "\n", "\n", "# causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)", "\n", "", "att", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "(", "1.0", "/", "math", ".", "sqrt", "(", "k", ".", "size", "(", "-", "1", ")", ")", ")", "\n", "\n", "if", "self", ".", "causal", "and", "layer_past", "is", "None", ":", "\n", "            ", "att", "=", "att", ".", "masked_fill", "(", "self", ".", "mask", "[", ":", ",", ":", ",", ":", "T", ",", ":", "T", "]", "==", "0", ",", "float", "(", "'-inf'", ")", ")", "\n", "\n", "", "att", "=", "F", ".", "softmax", "(", "att", ",", "dim", "=", "-", "1", ")", "\n", "att", "=", "self", ".", "attn_drop", "(", "att", ")", "\n", "y", "=", "att", "@", "v", "# (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)", "\n", "# re-assemble all head outputs side by side", "\n", "y", "=", "y", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "B", ",", "T", ",", "C", ")", "\n", "\n", "# output projection", "\n", "y", "=", "self", ".", "resid_drop", "(", "self", ".", "proj", "(", "y", ")", ")", "\n", "return", "y", ",", "present", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.transformer_arch.Block.__init__": [[77, 89], ["torch.Module.__init__", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "transformer_arch.CausalSelfAttention", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.GELU", "torch.GELU", "torch.GELU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["def", "__init__", "(", "self", ",", "bert_n_emb", ",", "resid_pdrop", ",", "bert_n_head", ",", "attn_pdrop", ",", "\n", "latent_shape", ",", "sampler", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "ln1", "=", "nn", ".", "LayerNorm", "(", "bert_n_emb", ")", "\n", "self", ".", "ln2", "=", "nn", ".", "LayerNorm", "(", "bert_n_emb", ")", "\n", "self", ".", "attn", "=", "CausalSelfAttention", "(", "bert_n_emb", ",", "bert_n_head", ",", "attn_pdrop", ",", "\n", "resid_pdrop", ",", "latent_shape", ",", "sampler", ")", "\n", "self", ".", "mlp", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "bert_n_emb", ",", "4", "*", "bert_n_emb", ")", ",", "\n", "nn", ".", "GELU", "(", ")", ",", "# nice", "\n", "nn", ".", "Linear", "(", "4", "*", "bert_n_emb", ",", "bert_n_emb", ")", ",", "\n", "nn", ".", "Dropout", "(", "resid_pdrop", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.transformer_arch.Block.forward": [[91, 100], ["transformer_arch.Block.attn", "transformer_arch.Block.ln1", "transformer_arch.Block.mlp", "transformer_arch.Block.ln2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "layer_past", "=", "None", ",", "return_present", "=", "False", ")", ":", "\n", "\n", "        ", "attn", ",", "present", "=", "self", ".", "attn", "(", "self", ".", "ln1", "(", "x", ")", ",", "layer_past", ")", "\n", "x", "=", "x", "+", "attn", "\n", "x", "=", "x", "+", "self", ".", "mlp", "(", "self", ".", "ln2", "(", "x", ")", ")", "\n", "\n", "if", "layer_past", "is", "not", "None", "or", "return_present", ":", "\n", "            ", "return", "x", ",", "present", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.transformer_arch.Transformer.__init__": [[105, 144], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.Linear", "torch.Linear", "torch.Linear", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "transformer_arch.Block", "range"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["def", "__init__", "(", "self", ",", "\n", "codebook_size", ",", "\n", "segm_codebook_size", ",", "\n", "bert_n_emb", ",", "\n", "bert_n_layers", ",", "\n", "bert_n_head", ",", "\n", "block_size", ",", "\n", "latent_shape", ",", "\n", "embd_pdrop", ",", "\n", "resid_pdrop", ",", "\n", "attn_pdrop", ",", "\n", "sampler", "=", "'absorbing'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "vocab_size", "=", "codebook_size", "+", "1", "\n", "self", ".", "n_embd", "=", "bert_n_emb", "\n", "self", ".", "block_size", "=", "block_size", "\n", "self", ".", "n_layers", "=", "bert_n_layers", "\n", "self", ".", "codebook_size", "=", "codebook_size", "\n", "self", ".", "segm_codebook_size", "=", "segm_codebook_size", "\n", "self", ".", "causal", "=", "sampler", "==", "'autoregressive'", "\n", "if", "self", ".", "causal", ":", "\n", "            ", "self", ".", "vocab_size", "=", "codebook_size", "\n", "\n", "", "self", ".", "tok_emb", "=", "nn", ".", "Embedding", "(", "self", ".", "vocab_size", ",", "self", ".", "n_embd", ")", "\n", "self", ".", "pos_emb", "=", "nn", ".", "Parameter", "(", "\n", "torch", ".", "zeros", "(", "1", ",", "self", ".", "block_size", ",", "self", ".", "n_embd", ")", ")", "\n", "self", ".", "segm_emb", "=", "nn", ".", "Embedding", "(", "self", ".", "segm_codebook_size", ",", "self", ".", "n_embd", ")", "\n", "self", ".", "start_tok", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "self", ".", "n_embd", ")", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "embd_pdrop", ")", "\n", "\n", "# transformer", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "Block", "(", "bert_n_emb", ",", "resid_pdrop", ",", "bert_n_head", ",", "attn_pdrop", ",", "\n", "latent_shape", ",", "sampler", ")", "for", "_", "in", "range", "(", "self", ".", "n_layers", ")", "\n", "]", ")", "\n", "# decoder head", "\n", "self", ".", "ln_f", "=", "nn", ".", "LayerNorm", "(", "self", ".", "n_embd", ")", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "n_embd", ",", "self", ".", "codebook_size", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.transformer_arch.Transformer.get_block_size": [[145, 147], ["None"], "methods", ["None"], ["", "def", "get_block_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "block_size", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.transformer_arch.Transformer._init_weights": [[148, 156], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["", "def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "0.02", ")", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.transformer_arch.Transformer.forward": [[157, 182], ["transformer_arch.Transformer.tok_emb", "transformer_arch.Transformer.segm_emb", "transformer_arch.Transformer.drop", "transformer_arch.Transformer.ln_f", "transformer_arch.Transformer.head", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "block", "transformer_arch.Transformer.start_tok.repeat", "torch.cat.size", "torch.cat.size", "torch.cat.size"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "idx", ",", "segm_tokens", ",", "t", "=", "None", ")", ":", "\n", "# each index maps to a (learnable) vector", "\n", "        ", "token_embeddings", "=", "self", ".", "tok_emb", "(", "idx", ")", "\n", "\n", "segm_embeddings", "=", "self", ".", "segm_emb", "(", "segm_tokens", ")", "\n", "\n", "if", "self", ".", "causal", ":", "\n", "            ", "token_embeddings", "=", "torch", ".", "cat", "(", "(", "self", ".", "start_tok", ".", "repeat", "(", "\n", "token_embeddings", ".", "size", "(", "0", ")", ",", "1", ",", "1", ")", ",", "token_embeddings", ")", ",", "\n", "dim", "=", "1", ")", "\n", "\n", "", "t", "=", "token_embeddings", ".", "shape", "[", "1", "]", "\n", "assert", "t", "<=", "self", ".", "block_size", ",", "\"Cannot forward, model block size is exhausted.\"", "\n", "# each position maps to a (learnable) vector", "\n", "\n", "position_embeddings", "=", "self", ".", "pos_emb", "[", ":", ",", ":", "t", ",", ":", "]", "\n", "\n", "x", "=", "token_embeddings", "+", "position_embeddings", "+", "segm_embeddings", "\n", "x", "=", "self", ".", "drop", "(", "x", ")", "\n", "for", "block", "in", "self", ".", "blocks", ":", "\n", "            ", "x", "=", "block", "(", "x", ")", "\n", "", "x", "=", "self", ".", "ln_f", "(", "x", ")", "\n", "logits", "=", "self", ".", "head", "(", "x", ")", "\n", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.transformer_arch.TransformerMultiHead.__init__": [[187, 235], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Linear", "torch.Linear", "torch.Linear", "transformer_arch.Block", "range", "range"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["def", "__init__", "(", "self", ",", "\n", "codebook_size", ",", "\n", "segm_codebook_size", ",", "\n", "texture_codebook_size", ",", "\n", "bert_n_emb", ",", "\n", "bert_n_layers", ",", "\n", "bert_n_head", ",", "\n", "block_size", ",", "\n", "latent_shape", ",", "\n", "embd_pdrop", ",", "\n", "resid_pdrop", ",", "\n", "attn_pdrop", ",", "\n", "num_head", ",", "\n", "sampler", "=", "'absorbing'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "vocab_size", "=", "codebook_size", "+", "1", "\n", "self", ".", "n_embd", "=", "bert_n_emb", "\n", "self", ".", "block_size", "=", "block_size", "\n", "self", ".", "n_layers", "=", "bert_n_layers", "\n", "self", ".", "codebook_size", "=", "codebook_size", "\n", "self", ".", "segm_codebook_size", "=", "segm_codebook_size", "\n", "self", ".", "texture_codebook_size", "=", "texture_codebook_size", "\n", "self", ".", "causal", "=", "sampler", "==", "'autoregressive'", "\n", "if", "self", ".", "causal", ":", "\n", "            ", "self", ".", "vocab_size", "=", "codebook_size", "\n", "\n", "", "self", ".", "tok_emb", "=", "nn", ".", "Embedding", "(", "self", ".", "vocab_size", ",", "self", ".", "n_embd", ")", "\n", "self", ".", "pos_emb", "=", "nn", ".", "Parameter", "(", "\n", "torch", ".", "zeros", "(", "1", ",", "self", ".", "block_size", ",", "self", ".", "n_embd", ")", ")", "\n", "self", ".", "segm_emb", "=", "nn", ".", "Embedding", "(", "self", ".", "segm_codebook_size", ",", "self", ".", "n_embd", ")", "\n", "self", ".", "texture_emb", "=", "nn", ".", "Embedding", "(", "self", ".", "texture_codebook_size", ",", "\n", "self", ".", "n_embd", ")", "\n", "self", ".", "start_tok", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "self", ".", "n_embd", ")", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "embd_pdrop", ")", "\n", "\n", "# transformer", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "Block", "(", "bert_n_emb", ",", "resid_pdrop", ",", "bert_n_head", ",", "attn_pdrop", ",", "\n", "latent_shape", ",", "sampler", ")", "for", "_", "in", "range", "(", "self", ".", "n_layers", ")", "\n", "]", ")", "\n", "# decoder head", "\n", "self", ".", "num_head", "=", "num_head", "\n", "self", ".", "head_class_num", "=", "codebook_size", "//", "self", ".", "num_head", "\n", "self", ".", "ln_f", "=", "nn", ".", "LayerNorm", "(", "self", ".", "n_embd", ")", "\n", "self", ".", "head_list", "=", "nn", ".", "ModuleList", "(", "[", "\n", "nn", ".", "Linear", "(", "self", ".", "n_embd", ",", "self", ".", "head_class_num", ",", "bias", "=", "False", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "num_head", ")", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.transformer_arch.TransformerMultiHead.get_block_size": [[237, 239], ["None"], "methods", ["None"], ["", "def", "get_block_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "block_size", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.transformer_arch.TransformerMultiHead._init_weights": [[240, 248], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["", "def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "0.02", ")", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.transformer_arch.TransformerMultiHead.forward": [[249, 274], ["transformer_arch.TransformerMultiHead.tok_emb", "transformer_arch.TransformerMultiHead.segm_emb", "transformer_arch.TransformerMultiHead.texture_emb", "transformer_arch.TransformerMultiHead.drop", "transformer_arch.TransformerMultiHead.ln_f", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "block", "range", "transformer_arch.TransformerMultiHead.start_tok.repeat", "torch.cat.size", "torch.cat.size", "torch.cat.size"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "idx", ",", "segm_tokens", ",", "texture_tokens", ",", "t", "=", "None", ")", ":", "\n", "# each index maps to a (learnable) vector", "\n", "        ", "token_embeddings", "=", "self", ".", "tok_emb", "(", "idx", ")", "\n", "segm_embeddings", "=", "self", ".", "segm_emb", "(", "segm_tokens", ")", "\n", "texture_embeddings", "=", "self", ".", "texture_emb", "(", "texture_tokens", ")", "\n", "\n", "if", "self", ".", "causal", ":", "\n", "            ", "token_embeddings", "=", "torch", ".", "cat", "(", "(", "self", ".", "start_tok", ".", "repeat", "(", "\n", "token_embeddings", ".", "size", "(", "0", ")", ",", "1", ",", "1", ")", ",", "token_embeddings", ")", ",", "\n", "dim", "=", "1", ")", "\n", "\n", "", "t", "=", "token_embeddings", ".", "shape", "[", "1", "]", "\n", "assert", "t", "<=", "self", ".", "block_size", ",", "\"Cannot forward, model block size is exhausted.\"", "\n", "# each position maps to a (learnable) vector", "\n", "\n", "position_embeddings", "=", "self", ".", "pos_emb", "[", ":", ",", ":", "t", ",", ":", "]", "\n", "\n", "x", "=", "token_embeddings", "+", "position_embeddings", "+", "segm_embeddings", "+", "texture_embeddings", "\n", "x", "=", "self", ".", "drop", "(", "x", ")", "\n", "for", "block", "in", "self", ".", "blocks", ":", "\n", "            ", "x", "=", "block", "(", "x", ")", "\n", "", "x", "=", "self", ".", "ln_f", "(", "x", ")", "\n", "logits_list", "=", "[", "self", ".", "head_list", "[", "i", "]", "(", "x", ")", "for", "i", "in", "range", "(", "self", ".", "num_head", ")", "]", "\n", "\n", "return", "logits_list", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.shape_attr_embedding_arch.ShapeAttrEmbedding.__init__": [[8, 22], ["torch.nn.Module.__init__", "enumerate", "len", "torch.nn.Sequential", "torch.nn.Sequential", "setattr", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.LeakyReLU", "torch.nn.LeakyReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.LeakyReLU", "torch.nn.LeakyReLU", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ",", "out_dim", ",", "cls_num_list", ")", ":", "\n", "        ", "super", "(", "ShapeAttrEmbedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "for", "idx", ",", "cls_num", "in", "enumerate", "(", "cls_num_list", ")", ":", "\n", "            ", "setattr", "(", "\n", "self", ",", "f'attr_{idx}'", ",", "\n", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "cls_num", ",", "dim", ")", ",", "nn", ".", "LeakyReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", ")", ")", "\n", "", "self", ".", "cls_num_list", "=", "cls_num_list", "\n", "self", ".", "attr_num", "=", "len", "(", "cls_num_list", ")", "\n", "self", ".", "fusion", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "dim", "*", "self", ".", "attr_num", ",", "out_dim", ")", ",", "nn", ".", "LeakyReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "out_dim", ",", "out_dim", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.shape_attr_embedding_arch.ShapeAttrEmbedding.forward": [[23, 36], ["range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "shape_attr_embedding_arch.ShapeAttrEmbedding.fusion", "getattr", "attr_embedding_list.append", "getattr.", "torch.one_hot().to", "torch.one_hot().to", "torch.one_hot", "torch.one_hot"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "attr", ")", ":", "\n", "        ", "attr_embedding_list", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "self", ".", "attr_num", ")", ":", "\n", "            ", "attr_embed_fc", "=", "getattr", "(", "self", ",", "f'attr_{idx}'", ")", "\n", "attr_embedding_list", ".", "append", "(", "\n", "attr_embed_fc", "(", "\n", "F", ".", "one_hot", "(", "\n", "attr", "[", ":", ",", "idx", "]", ",", "\n", "num_classes", "=", "self", ".", "cls_num_list", "[", "idx", "]", ")", ".", "to", "(", "torch", ".", "float32", ")", ")", ")", "\n", "", "attr_embedding", "=", "torch", ".", "cat", "(", "attr_embedding_list", ",", "dim", "=", "1", ")", "\n", "attr_embedding", "=", "self", ".", "fusion", "(", "attr_embedding", ")", "\n", "\n", "return", "attr_embedding", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.VectorQuantizer.__init__": [[21, 52], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "vqgan_arch.VectorQuantizer.embedding.weight.data.uniform_", "vqgan_arch.VectorQuantizer.register_buffer", "print", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "numpy.load"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["def", "__init__", "(", "self", ",", "\n", "n_e", ",", "\n", "e_dim", ",", "\n", "beta", ",", "\n", "remap", "=", "None", ",", "\n", "unknown_index", "=", "\"random\"", ",", "\n", "sane_index_shape", "=", "False", ",", "\n", "legacy", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "n_e", "=", "n_e", "\n", "self", ".", "e_dim", "=", "e_dim", "\n", "self", ".", "beta", "=", "beta", "\n", "self", ".", "legacy", "=", "legacy", "\n", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "self", ".", "n_e", ",", "self", ".", "e_dim", ")", "\n", "self", ".", "embedding", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "1.0", "/", "self", ".", "n_e", ",", "1.0", "/", "self", ".", "n_e", ")", "\n", "\n", "self", ".", "remap", "=", "remap", "\n", "if", "self", ".", "remap", "is", "not", "None", ":", "\n", "            ", "self", ".", "register_buffer", "(", "\"used\"", ",", "torch", ".", "tensor", "(", "np", ".", "load", "(", "self", ".", "remap", ")", ")", ")", "\n", "self", ".", "re_embed", "=", "self", ".", "used", ".", "shape", "[", "0", "]", "\n", "self", ".", "unknown_index", "=", "unknown_index", "# \"random\" or \"extra\" or integer", "\n", "if", "self", ".", "unknown_index", "==", "\"extra\"", ":", "\n", "                ", "self", ".", "unknown_index", "=", "self", ".", "re_embed", "\n", "self", ".", "re_embed", "=", "self", ".", "re_embed", "+", "1", "\n", "", "print", "(", "f\"Remapping {self.n_e} indices to {self.re_embed} indices. \"", "\n", "f\"Using {self.unknown_index} for unknown indices.\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "re_embed", "=", "n_e", "\n", "\n", "", "self", ".", "sane_index_shape", "=", "sane_index_shape", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.VectorQuantizer.remap_to_used": [[53, 68], ["inds.reshape.reshape.reshape", "vqgan_arch.VectorQuantizer.used.to", "match.argmax", "match.argmax.reshape", "len", "match.sum", "torch.randint().to", "torch.randint().to", "torch.randint().to", "torch.randint().to", "torch.randint().to", "torch.randint().to", "torch.randint().to", "torch.randint().to", "torch.randint().to", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint"], "methods", ["None"], ["", "def", "remap_to_used", "(", "self", ",", "inds", ")", ":", "\n", "        ", "ishape", "=", "inds", ".", "shape", "\n", "assert", "len", "(", "ishape", ")", ">", "1", "\n", "inds", "=", "inds", ".", "reshape", "(", "ishape", "[", "0", "]", ",", "-", "1", ")", "\n", "used", "=", "self", ".", "used", ".", "to", "(", "inds", ")", "\n", "match", "=", "(", "inds", "[", ":", ",", ":", ",", "None", "]", "==", "used", "[", "None", ",", "None", ",", "...", "]", ")", ".", "long", "(", ")", "\n", "new", "=", "match", ".", "argmax", "(", "-", "1", ")", "\n", "unknown", "=", "match", ".", "sum", "(", "2", ")", "<", "1", "\n", "if", "self", ".", "unknown_index", "==", "\"random\"", ":", "\n", "            ", "new", "[", "unknown", "]", "=", "torch", ".", "randint", "(", "\n", "0", ",", "self", ".", "re_embed", ",", "\n", "size", "=", "new", "[", "unknown", "]", ".", "shape", ")", ".", "to", "(", "device", "=", "new", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "new", "[", "unknown", "]", "=", "self", ".", "unknown_index", "\n", "", "return", "new", ".", "reshape", "(", "ishape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.VectorQuantizer.unmap_to_all": [[69, 78], ["inds.reshape.reshape.reshape", "vqgan_arch.VectorQuantizer.used.to", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather.reshape", "torch.gather.reshape", "torch.gather.reshape", "len"], "methods", ["None"], ["", "def", "unmap_to_all", "(", "self", ",", "inds", ")", ":", "\n", "        ", "ishape", "=", "inds", ".", "shape", "\n", "assert", "len", "(", "ishape", ")", ">", "1", "\n", "inds", "=", "inds", ".", "reshape", "(", "ishape", "[", "0", "]", ",", "-", "1", ")", "\n", "used", "=", "self", ".", "used", ".", "to", "(", "inds", ")", "\n", "if", "self", ".", "re_embed", ">", "self", ".", "used", ".", "shape", "[", "0", "]", ":", "# extra token", "\n", "            ", "inds", "[", "inds", ">=", "self", ".", "used", ".", "shape", "[", "0", "]", "]", "=", "0", "# simply set to zero", "\n", "", "back", "=", "torch", ".", "gather", "(", "used", "[", "None", ",", ":", "]", "[", "inds", ".", "shape", "[", "0", "]", "*", "[", "0", "]", ",", ":", "]", ",", "1", ",", "inds", ")", "\n", "return", "back", ".", "reshape", "(", "ishape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.VectorQuantizer.forward": [[79, 123], ["einops.rearrange().contiguous", "einops.rearrange().contiguous.view", "torch.argmin", "torch.argmin", "torch.argmin", "torch.argmin", "torch.argmin", "torch.argmin", "torch.argmin", "torch.argmin", "torch.argmin", "vqgan_arch.VectorQuantizer.embedding().view", "einops.rearrange().contiguous", "min_encoding_indices.reshape.reshape.reshape", "vqgan_arch.VectorQuantizer.remap_to_used", "min_encoding_indices.reshape.reshape.reshape", "min_encoding_indices.reshape.reshape.reshape", "einops.rearrange", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "vqgan_arch.VectorQuantizer.embedding", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "einops.rearrange", "einops.rearrange", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "einops.rearrange().contiguous.detach", "einops.rearrange().contiguous.detach", "einops.rearrange().contiguous.detach", "einops.rearrange().contiguous.detach"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.VectorQuantizerTexture.remap_to_used"], ["", "def", "forward", "(", "self", ",", "z", ",", "temp", "=", "None", ",", "rescale_logits", "=", "False", ",", "return_logits", "=", "False", ")", ":", "\n", "        ", "assert", "temp", "is", "None", "or", "temp", "==", "1.0", ",", "\"Only for interface compatible with Gumbel\"", "\n", "assert", "rescale_logits", "==", "False", ",", "\"Only for interface compatible with Gumbel\"", "\n", "assert", "return_logits", "==", "False", ",", "\"Only for interface compatible with Gumbel\"", "\n", "# reshape z -> (batch, height, width, channel) and flatten", "\n", "z", "=", "rearrange", "(", "z", ",", "'b c h w -> b h w c'", ")", ".", "contiguous", "(", ")", "\n", "z_flattened", "=", "z", ".", "view", "(", "-", "1", ",", "self", ".", "e_dim", ")", "\n", "# distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z", "\n", "\n", "d", "=", "torch", ".", "sum", "(", "z_flattened", "**", "2", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "+", "torch", ".", "sum", "(", "self", ".", "embedding", ".", "weight", "**", "2", ",", "dim", "=", "1", ")", "-", "2", "*", "torch", ".", "einsum", "(", "'bd,dn->bn'", ",", "z_flattened", ",", "rearrange", "(", "self", ".", "embedding", ".", "weight", ",", "'n d -> d n'", ")", ")", "\n", "\n", "min_encoding_indices", "=", "torch", ".", "argmin", "(", "d", ",", "dim", "=", "1", ")", "\n", "z_q", "=", "self", ".", "embedding", "(", "min_encoding_indices", ")", ".", "view", "(", "z", ".", "shape", ")", "\n", "perplexity", "=", "None", "\n", "min_encodings", "=", "None", "\n", "\n", "# compute loss for embedding", "\n", "if", "not", "self", ".", "legacy", ":", "\n", "            ", "loss", "=", "self", ".", "beta", "*", "torch", ".", "mean", "(", "(", "z_q", ".", "detach", "(", ")", "-", "z", ")", "**", "2", ")", "+", "torch", ".", "mean", "(", "(", "z_q", "-", "z", ".", "detach", "(", ")", ")", "**", "2", ")", "\n", "", "else", ":", "\n", "            ", "loss", "=", "torch", ".", "mean", "(", "(", "z_q", ".", "detach", "(", ")", "-", "z", ")", "**", "2", ")", "+", "self", ".", "beta", "*", "torch", ".", "mean", "(", "(", "z_q", "-", "z", ".", "detach", "(", ")", ")", "**", "2", ")", "\n", "\n", "# preserve gradients", "\n", "", "z_q", "=", "z", "+", "(", "z_q", "-", "z", ")", ".", "detach", "(", ")", "\n", "\n", "# reshape back to match original input shape", "\n", "z_q", "=", "rearrange", "(", "z_q", ",", "'b h w c -> b c h w'", ")", ".", "contiguous", "(", ")", "\n", "\n", "if", "self", ".", "remap", "is", "not", "None", ":", "\n", "            ", "min_encoding_indices", "=", "min_encoding_indices", ".", "reshape", "(", "\n", "z", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "# add batch axis", "\n", "min_encoding_indices", "=", "self", ".", "remap_to_used", "(", "min_encoding_indices", ")", "\n", "min_encoding_indices", "=", "min_encoding_indices", ".", "reshape", "(", "-", "1", ",", "\n", "1", ")", "# flatten", "\n", "\n", "", "if", "self", ".", "sane_index_shape", ":", "\n", "            ", "min_encoding_indices", "=", "min_encoding_indices", ".", "reshape", "(", "\n", "z_q", ".", "shape", "[", "0", "]", ",", "z_q", ".", "shape", "[", "2", "]", ",", "z_q", ".", "shape", "[", "3", "]", ")", "\n", "\n", "", "return", "z_q", ",", "loss", ",", "(", "perplexity", ",", "min_encodings", ",", "min_encoding_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.VectorQuantizer.get_codebook_entry": [[124, 140], ["vqgan_arch.VectorQuantizer.embedding", "indices.reshape.reshape.reshape", "vqgan_arch.VectorQuantizer.unmap_to_all", "indices.reshape.reshape.reshape", "z_q.permute().contiguous.permute().contiguous.view", "z_q.permute().contiguous.permute().contiguous.permute().contiguous", "z_q.permute().contiguous.permute().contiguous.permute"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.VectorQuantizerTexture.unmap_to_all"], ["", "def", "get_codebook_entry", "(", "self", ",", "indices", ",", "shape", ")", ":", "\n", "# shape specifying (batch, height, width, channel)", "\n", "        ", "if", "self", ".", "remap", "is", "not", "None", ":", "\n", "            ", "indices", "=", "indices", ".", "reshape", "(", "shape", "[", "0", "]", ",", "-", "1", ")", "# add batch axis", "\n", "indices", "=", "self", ".", "unmap_to_all", "(", "indices", ")", "\n", "indices", "=", "indices", ".", "reshape", "(", "-", "1", ")", "# flatten again", "\n", "\n", "# get quantized latent vectors", "\n", "", "z_q", "=", "self", ".", "embedding", "(", "indices", ")", "\n", "\n", "if", "shape", "is", "not", "None", ":", "\n", "            ", "z_q", "=", "z_q", ".", "view", "(", "shape", ")", "\n", "# reshape back to match original input shape", "\n", "z_q", "=", "z_q", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "\n", "", "return", "z_q", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.VectorQuantizerTexture.__init__": [[151, 185], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "embedding.weight.data.uniform_", "vqgan_arch.VectorQuantizerTexture.register_buffer", "print", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range", "numpy.load"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["def", "__init__", "(", "self", ",", "\n", "n_e", ",", "\n", "e_dim", ",", "\n", "beta", ",", "\n", "remap", "=", "None", ",", "\n", "unknown_index", "=", "\"random\"", ",", "\n", "sane_index_shape", "=", "False", ",", "\n", "legacy", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "n_e", "=", "n_e", "\n", "self", ".", "e_dim", "=", "e_dim", "\n", "self", ".", "beta", "=", "beta", "\n", "self", ".", "legacy", "=", "legacy", "\n", "\n", "# TODO: decide number of embeddings", "\n", "self", ".", "embedding_list", "=", "nn", ".", "ModuleList", "(", "\n", "[", "nn", ".", "Embedding", "(", "self", ".", "n_e", ",", "self", ".", "e_dim", ")", "for", "i", "in", "range", "(", "18", ")", "]", ")", "\n", "for", "embedding", "in", "self", ".", "embedding_list", ":", "\n", "            ", "embedding", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "1.0", "/", "self", ".", "n_e", ",", "1.0", "/", "self", ".", "n_e", ")", "\n", "\n", "", "self", ".", "remap", "=", "remap", "\n", "if", "self", ".", "remap", "is", "not", "None", ":", "\n", "            ", "self", ".", "register_buffer", "(", "\"used\"", ",", "torch", ".", "tensor", "(", "np", ".", "load", "(", "self", ".", "remap", ")", ")", ")", "\n", "self", ".", "re_embed", "=", "self", ".", "used", ".", "shape", "[", "0", "]", "\n", "self", ".", "unknown_index", "=", "unknown_index", "# \"random\" or \"extra\" or integer", "\n", "if", "self", ".", "unknown_index", "==", "\"extra\"", ":", "\n", "                ", "self", ".", "unknown_index", "=", "self", ".", "re_embed", "\n", "self", ".", "re_embed", "=", "self", ".", "re_embed", "+", "1", "\n", "", "print", "(", "f\"Remapping {self.n_e} indices to {self.re_embed} indices. \"", "\n", "f\"Using {self.unknown_index} for unknown indices.\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "re_embed", "=", "n_e", "\n", "\n", "", "self", ".", "sane_index_shape", "=", "sane_index_shape", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.VectorQuantizerTexture.remap_to_used": [[186, 201], ["inds.reshape.reshape.reshape", "vqgan_arch.VectorQuantizerTexture.used.to", "match.argmax", "match.argmax.reshape", "len", "match.sum", "torch.randint().to", "torch.randint().to", "torch.randint().to", "torch.randint().to", "torch.randint().to", "torch.randint().to", "torch.randint().to", "torch.randint().to", "torch.randint().to", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint"], "methods", ["None"], ["", "def", "remap_to_used", "(", "self", ",", "inds", ")", ":", "\n", "        ", "ishape", "=", "inds", ".", "shape", "\n", "assert", "len", "(", "ishape", ")", ">", "1", "\n", "inds", "=", "inds", ".", "reshape", "(", "ishape", "[", "0", "]", ",", "-", "1", ")", "\n", "used", "=", "self", ".", "used", ".", "to", "(", "inds", ")", "\n", "match", "=", "(", "inds", "[", ":", ",", ":", ",", "None", "]", "==", "used", "[", "None", ",", "None", ",", "...", "]", ")", ".", "long", "(", ")", "\n", "new", "=", "match", ".", "argmax", "(", "-", "1", ")", "\n", "unknown", "=", "match", ".", "sum", "(", "2", ")", "<", "1", "\n", "if", "self", ".", "unknown_index", "==", "\"random\"", ":", "\n", "            ", "new", "[", "unknown", "]", "=", "torch", ".", "randint", "(", "\n", "0", ",", "self", ".", "re_embed", ",", "\n", "size", "=", "new", "[", "unknown", "]", ".", "shape", ")", ".", "to", "(", "device", "=", "new", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "new", "[", "unknown", "]", "=", "self", ".", "unknown_index", "\n", "", "return", "new", ".", "reshape", "(", "ishape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.VectorQuantizerTexture.unmap_to_all": [[202, 211], ["inds.reshape.reshape.reshape", "vqgan_arch.VectorQuantizerTexture.used.to", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather.reshape", "torch.gather.reshape", "torch.gather.reshape", "len"], "methods", ["None"], ["", "def", "unmap_to_all", "(", "self", ",", "inds", ")", ":", "\n", "        ", "ishape", "=", "inds", ".", "shape", "\n", "assert", "len", "(", "ishape", ")", ">", "1", "\n", "inds", "=", "inds", ".", "reshape", "(", "ishape", "[", "0", "]", ",", "-", "1", ")", "\n", "used", "=", "self", ".", "used", ".", "to", "(", "inds", ")", "\n", "if", "self", ".", "re_embed", ">", "self", ".", "used", ".", "shape", "[", "0", "]", ":", "# extra token", "\n", "            ", "inds", "[", "inds", ">=", "self", ".", "used", ".", "shape", "[", "0", "]", "]", "=", "0", "# simply set to zero", "\n", "", "back", "=", "torch", ".", "gather", "(", "used", "[", "None", ",", ":", "]", "[", "inds", ".", "shape", "[", "0", "]", "*", "[", "0", "]", ",", ":", "]", ",", "1", ",", "inds", ")", "\n", "return", "back", ".", "reshape", "(", "ishape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.VectorQuantizerTexture.forward": [[212, 288], ["torch.interpolate", "torch.interpolate", "torch.interpolate", "einops.rearrange().contiguous", "einops.rearrange().contiguous.view", "torch.interpolate.view", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "range", "min_encoding_indices_continual.reshape.reshape.reshape", "einops.rearrange().contiguous.view", "einops.rearrange().contiguous", "F.interpolate.view.size", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "min_encoding_indices.reshape.reshape.reshape", "min_encoding_indices_list.append", "einops.rearrange", "F.interpolate.view.size", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.argmin", "torch.argmin", "torch.argmin", "torch.argmin", "torch.argmin", "torch.argmin", "torch.argmin", "torch.argmin", "torch.argmin", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "einops.rearrange", "einops.rearrange().contiguous.size", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "einops.rearrange", "einops.rearrange().contiguous.detach", "einops.rearrange().contiguous.detach", "einops.rearrange().contiguous.detach", "einops.rearrange().contiguous.detach"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "\n", "z", ",", "\n", "segm_map", ",", "\n", "temp", "=", "None", ",", "\n", "rescale_logits", "=", "False", ",", "\n", "return_logits", "=", "False", ")", ":", "\n", "        ", "assert", "temp", "is", "None", "or", "temp", "==", "1.0", ",", "\"Only for interface compatible with Gumbel\"", "\n", "assert", "rescale_logits", "==", "False", ",", "\"Only for interface compatible with Gumbel\"", "\n", "assert", "return_logits", "==", "False", ",", "\"Only for interface compatible with Gumbel\"", "\n", "\n", "segm_map", "=", "F", ".", "interpolate", "(", "segm_map", ",", "size", "=", "z", ".", "size", "(", ")", "[", "2", ":", "]", ",", "mode", "=", "'nearest'", ")", "\n", "# reshape z -> (batch, height, width, channel) and flatten", "\n", "z", "=", "rearrange", "(", "z", ",", "'b c h w -> b h w c'", ")", ".", "contiguous", "(", ")", "\n", "z_flattened", "=", "z", ".", "view", "(", "-", "1", ",", "self", ".", "e_dim", ")", "\n", "\n", "# flatten segm_map (b, h, w)", "\n", "segm_map_flatten", "=", "segm_map", ".", "view", "(", "-", "1", ")", "\n", "\n", "z_q", "=", "torch", ".", "zeros_like", "(", "z_flattened", ")", "\n", "min_encoding_indices_list", "=", "[", "]", "\n", "min_encoding_indices_continual", "=", "torch", ".", "full", "(", "\n", "segm_map_flatten", ".", "size", "(", ")", ",", "\n", "fill_value", "=", "-", "1", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "segm_map_flatten", ".", "device", ")", "\n", "for", "codebook_idx", "in", "range", "(", "18", ")", ":", "\n", "            ", "min_encoding_indices", "=", "torch", ".", "full", "(", "\n", "segm_map_flatten", ".", "size", "(", ")", ",", "\n", "fill_value", "=", "-", "1", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "segm_map_flatten", ".", "device", ")", "\n", "if", "torch", ".", "sum", "(", "segm_map_flatten", "==", "codebook_idx", ")", ">", "0", ":", "\n", "                ", "z_selected", "=", "z_flattened", "[", "segm_map_flatten", "==", "codebook_idx", "]", "\n", "# distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z", "\n", "d_selected", "=", "torch", ".", "sum", "(", "\n", "z_selected", "**", "2", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "+", "torch", ".", "sum", "(", "\n", "self", ".", "embedding_list", "[", "codebook_idx", "]", ".", "weight", "**", "2", ",", "\n", "dim", "=", "1", ")", "-", "2", "*", "torch", ".", "einsum", "(", "\n", "'bd,dn->bn'", ",", "z_selected", ",", "\n", "rearrange", "(", "self", ".", "embedding_list", "[", "codebook_idx", "]", ".", "weight", ",", "\n", "'n d -> d n'", ")", ")", "\n", "min_encoding_indices_selected", "=", "torch", ".", "argmin", "(", "d_selected", ",", "dim", "=", "1", ")", "\n", "z_q_selected", "=", "self", ".", "embedding_list", "[", "codebook_idx", "]", "(", "\n", "min_encoding_indices_selected", ")", "\n", "z_q", "[", "segm_map_flatten", "==", "codebook_idx", "]", "=", "z_q_selected", "\n", "min_encoding_indices", "[", "\n", "segm_map_flatten", "==", "\n", "codebook_idx", "]", "=", "min_encoding_indices_selected", "\n", "min_encoding_indices_continual", "[", "\n", "segm_map_flatten", "==", "\n", "codebook_idx", "]", "=", "min_encoding_indices_selected", "+", "1024", "*", "codebook_idx", "\n", "", "min_encoding_indices", "=", "min_encoding_indices", ".", "reshape", "(", "\n", "z", ".", "shape", "[", "0", "]", ",", "z", ".", "shape", "[", "1", "]", ",", "z", ".", "shape", "[", "2", "]", ")", "\n", "min_encoding_indices_list", ".", "append", "(", "min_encoding_indices", ")", "\n", "\n", "", "min_encoding_indices_continual", "=", "min_encoding_indices_continual", ".", "reshape", "(", "\n", "z", ".", "shape", "[", "0", "]", ",", "z", ".", "shape", "[", "1", "]", ",", "z", ".", "shape", "[", "2", "]", ")", "\n", "z_q", "=", "z_q", ".", "view", "(", "z", ".", "shape", ")", "\n", "perplexity", "=", "None", "\n", "\n", "# compute loss for embedding", "\n", "if", "not", "self", ".", "legacy", ":", "\n", "            ", "loss", "=", "self", ".", "beta", "*", "torch", ".", "mean", "(", "(", "z_q", ".", "detach", "(", ")", "-", "z", ")", "**", "2", ")", "+", "torch", ".", "mean", "(", "(", "z_q", "-", "z", ".", "detach", "(", ")", ")", "**", "2", ")", "\n", "", "else", ":", "\n", "            ", "loss", "=", "torch", ".", "mean", "(", "(", "z_q", ".", "detach", "(", ")", "-", "z", ")", "**", "2", ")", "+", "self", ".", "beta", "*", "torch", ".", "mean", "(", "(", "z_q", "-", "z", ".", "detach", "(", ")", ")", "**", "2", ")", "\n", "\n", "# preserve gradients", "\n", "", "z_q", "=", "z", "+", "(", "z_q", "-", "z", ")", ".", "detach", "(", ")", "\n", "\n", "# reshape back to match original input shape", "\n", "z_q", "=", "rearrange", "(", "z_q", ",", "'b h w c -> b c h w'", ")", ".", "contiguous", "(", ")", "\n", "\n", "return", "z_q", ",", "loss", ",", "(", "perplexity", ",", "min_encoding_indices_continual", ",", "\n", "min_encoding_indices_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.VectorQuantizerTexture.get_codebook_entry": [[289, 310], ["torch.interpolate", "torch.interpolate", "torch.interpolate", "torch.interpolate.view", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "range", "z_q.permute().contiguous.permute().contiguous.view", "z_q.permute().contiguous.permute().contiguous.permute().contiguous", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "z_q.permute().contiguous.permute().contiguous.permute", "indices_list[].view"], "methods", ["None"], ["", "def", "get_codebook_entry", "(", "self", ",", "indices_list", ",", "segm_map", ",", "shape", ")", ":", "\n", "# flatten segm_map (b, h, w)", "\n", "        ", "segm_map", "=", "F", ".", "interpolate", "(", "\n", "segm_map", ",", "size", "=", "(", "shape", "[", "1", "]", ",", "shape", "[", "2", "]", ")", ",", "mode", "=", "'nearest'", ")", "\n", "segm_map_flatten", "=", "segm_map", ".", "view", "(", "-", "1", ")", "\n", "\n", "z_q", "=", "torch", ".", "zeros", "(", "(", "shape", "[", "0", "]", "*", "shape", "[", "1", "]", "*", "shape", "[", "2", "]", ")", ",", "\n", "self", ".", "e_dim", ")", ".", "to", "(", "segm_map", ".", "device", ")", "\n", "for", "codebook_idx", "in", "range", "(", "18", ")", ":", "\n", "            ", "if", "torch", ".", "sum", "(", "segm_map_flatten", "==", "codebook_idx", ")", ">", "0", ":", "\n", "                ", "min_encoding_indices_selected", "=", "indices_list", "[", "\n", "codebook_idx", "]", ".", "view", "(", "-", "1", ")", "[", "segm_map_flatten", "==", "codebook_idx", "]", "\n", "z_q_selected", "=", "self", ".", "embedding_list", "[", "codebook_idx", "]", "(", "\n", "min_encoding_indices_selected", ")", "\n", "z_q", "[", "segm_map_flatten", "==", "codebook_idx", "]", "=", "z_q_selected", "\n", "\n", "", "", "z_q", "=", "z_q", ".", "view", "(", "shape", ")", "\n", "# reshape back to match original input shape", "\n", "z_q", "=", "z_q", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "\n", "return", "z_q", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.VectorQuantizerSpatialTextureAware.__init__": [[338, 374], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "embedding.weight.data.uniform_", "vqgan_arch.VectorQuantizerSpatialTextureAware.register_buffer", "print", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range", "numpy.load"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["def", "__init__", "(", "self", ",", "\n", "n_e", ",", "\n", "e_dim", ",", "\n", "beta", ",", "\n", "spatial_size", ",", "\n", "remap", "=", "None", ",", "\n", "unknown_index", "=", "\"random\"", ",", "\n", "sane_index_shape", "=", "False", ",", "\n", "legacy", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "n_e", "=", "n_e", "\n", "self", ".", "e_dim", "=", "e_dim", "*", "spatial_size", "*", "spatial_size", "\n", "self", ".", "beta", "=", "beta", "\n", "self", ".", "legacy", "=", "legacy", "\n", "self", ".", "spatial_size", "=", "spatial_size", "\n", "\n", "# TODO: decide number of embeddings", "\n", "self", ".", "embedding_list", "=", "nn", ".", "ModuleList", "(", "\n", "[", "nn", ".", "Embedding", "(", "self", ".", "n_e", ",", "self", ".", "e_dim", ")", "for", "i", "in", "range", "(", "18", ")", "]", ")", "\n", "for", "embedding", "in", "self", ".", "embedding_list", ":", "\n", "            ", "embedding", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "1.0", "/", "self", ".", "n_e", ",", "1.0", "/", "self", ".", "n_e", ")", "\n", "\n", "", "self", ".", "remap", "=", "remap", "\n", "if", "self", ".", "remap", "is", "not", "None", ":", "\n", "            ", "self", ".", "register_buffer", "(", "\"used\"", ",", "torch", ".", "tensor", "(", "np", ".", "load", "(", "self", ".", "remap", ")", ")", ")", "\n", "self", ".", "re_embed", "=", "self", ".", "used", ".", "shape", "[", "0", "]", "\n", "self", ".", "unknown_index", "=", "unknown_index", "# \"random\" or \"extra\" or integer", "\n", "if", "self", ".", "unknown_index", "==", "\"extra\"", ":", "\n", "                ", "self", ".", "unknown_index", "=", "self", ".", "re_embed", "\n", "self", ".", "re_embed", "=", "self", ".", "re_embed", "+", "1", "\n", "", "print", "(", "f\"Remapping {self.n_e} indices to {self.re_embed} indices. \"", "\n", "f\"Using {self.unknown_index} for unknown indices.\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "re_embed", "=", "n_e", "\n", "\n", "", "self", ".", "sane_index_shape", "=", "sane_index_shape", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.VectorQuantizerSpatialTextureAware.forward": [[375, 462], ["torch.interpolate", "torch.interpolate", "torch.interpolate", "sample_patches().permute", "sample_patches().permute.reshape", "torch.interpolate.view", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "range", "torch.fold", "torch.fold", "torch.fold", "F.interpolate.view.size", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "min_encoding_indices.reshape.reshape.reshape", "min_encoding_indices_list.append", "torch.fold.view().permute", "vqgan_arch.sample_patches", "F.interpolate.view.size", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.argmin", "torch.argmin", "torch.argmin", "torch.argmin", "torch.argmin", "torch.argmin", "torch.argmin", "torch.argmin", "torch.argmin", "z.size", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.fold.view", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "z.size", "z.size", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "einops.rearrange", "z.detach", "torch.fold.detach", "torch.fold.detach", "z.detach"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.sample_patches"], ["", "def", "forward", "(", "self", ",", "\n", "z", ",", "\n", "segm_map", ",", "\n", "temp", "=", "None", ",", "\n", "rescale_logits", "=", "False", ",", "\n", "return_logits", "=", "False", ")", ":", "\n", "        ", "assert", "temp", "is", "None", "or", "temp", "==", "1.0", ",", "\"Only for interface compatible with Gumbel\"", "\n", "assert", "rescale_logits", "==", "False", ",", "\"Only for interface compatible with Gumbel\"", "\n", "assert", "return_logits", "==", "False", ",", "\"Only for interface compatible with Gumbel\"", "\n", "\n", "segm_map", "=", "F", ".", "interpolate", "(", "\n", "segm_map", ",", "\n", "size", "=", "(", "z", ".", "size", "(", "2", ")", "//", "self", ".", "spatial_size", ",", "\n", "z", ".", "size", "(", "3", ")", "//", "self", ".", "spatial_size", ")", ",", "\n", "mode", "=", "'nearest'", ")", "\n", "\n", "# reshape z -> (batch, height, width, channel) and flatten", "\n", "# z = rearrange(z, 'b c h w -> b h w c').contiguous() ?", "\n", "z_patches", "=", "sample_patches", "(", "\n", "z", ",", "patch_size", "=", "self", ".", "spatial_size", ",", "\n", "stride", "=", "self", ".", "spatial_size", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "z_patches_flattened", "=", "z_patches", ".", "reshape", "(", "-", "1", ",", "self", ".", "e_dim", ")", "\n", "# distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z", "\n", "\n", "# flatten segm_map (b, h, w)", "\n", "segm_map_flatten", "=", "segm_map", ".", "view", "(", "-", "1", ")", "\n", "\n", "z_q", "=", "torch", ".", "zeros_like", "(", "z_patches_flattened", ")", "\n", "min_encoding_indices_list", "=", "[", "]", "\n", "min_encoding_indices_continual", "=", "torch", ".", "full", "(", "\n", "segm_map_flatten", ".", "size", "(", ")", ",", "\n", "fill_value", "=", "-", "1", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "segm_map_flatten", ".", "device", ")", "\n", "\n", "for", "codebook_idx", "in", "range", "(", "18", ")", ":", "\n", "            ", "min_encoding_indices", "=", "torch", ".", "full", "(", "\n", "segm_map_flatten", ".", "size", "(", ")", ",", "\n", "fill_value", "=", "-", "1", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "segm_map_flatten", ".", "device", ")", "\n", "if", "torch", ".", "sum", "(", "segm_map_flatten", "==", "codebook_idx", ")", ">", "0", ":", "\n", "                ", "z_selected", "=", "z_patches_flattened", "[", "segm_map_flatten", "==", "\n", "codebook_idx", "]", "\n", "# distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z", "\n", "d_selected", "=", "torch", ".", "sum", "(", "\n", "z_selected", "**", "2", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "+", "torch", ".", "sum", "(", "\n", "self", ".", "embedding_list", "[", "codebook_idx", "]", ".", "weight", "**", "2", ",", "\n", "dim", "=", "1", ")", "-", "2", "*", "torch", ".", "einsum", "(", "\n", "'bd,dn->bn'", ",", "z_selected", ",", "\n", "rearrange", "(", "self", ".", "embedding_list", "[", "codebook_idx", "]", ".", "weight", ",", "\n", "'n d -> d n'", ")", ")", "\n", "min_encoding_indices_selected", "=", "torch", ".", "argmin", "(", "d_selected", ",", "dim", "=", "1", ")", "\n", "z_q_selected", "=", "self", ".", "embedding_list", "[", "codebook_idx", "]", "(", "\n", "min_encoding_indices_selected", ")", "\n", "z_q", "[", "segm_map_flatten", "==", "codebook_idx", "]", "=", "z_q_selected", "\n", "min_encoding_indices", "[", "\n", "segm_map_flatten", "==", "\n", "codebook_idx", "]", "=", "min_encoding_indices_selected", "\n", "min_encoding_indices_continual", "[", "\n", "segm_map_flatten", "==", "\n", "codebook_idx", "]", "=", "min_encoding_indices_selected", "+", "self", ".", "n_e", "*", "codebook_idx", "\n", "", "min_encoding_indices", "=", "min_encoding_indices", ".", "reshape", "(", "\n", "z_patches", ".", "shape", "[", "0", "]", ",", "segm_map", ".", "shape", "[", "2", "]", ",", "segm_map", ".", "shape", "[", "3", "]", ")", "\n", "min_encoding_indices_list", ".", "append", "(", "min_encoding_indices", ")", "\n", "\n", "", "z_q", "=", "F", ".", "fold", "(", "\n", "z_q", ".", "view", "(", "z_patches", ".", "shape", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ",", "\n", "z", ".", "size", "(", ")", "[", "2", ":", "]", ",", "\n", "kernel_size", "=", "(", "self", ".", "spatial_size", ",", "self", ".", "spatial_size", ")", ",", "\n", "stride", "=", "self", ".", "spatial_size", ")", "\n", "\n", "perplexity", "=", "None", "\n", "\n", "# compute loss for embedding", "\n", "if", "not", "self", ".", "legacy", ":", "\n", "            ", "loss", "=", "self", ".", "beta", "*", "torch", ".", "mean", "(", "(", "z_q", ".", "detach", "(", ")", "-", "z", ")", "**", "2", ")", "+", "torch", ".", "mean", "(", "(", "z_q", "-", "z", ".", "detach", "(", ")", ")", "**", "2", ")", "\n", "", "else", ":", "\n", "            ", "loss", "=", "torch", ".", "mean", "(", "(", "z_q", ".", "detach", "(", ")", "-", "z", ")", "**", "2", ")", "+", "self", ".", "beta", "*", "torch", ".", "mean", "(", "(", "z_q", "-", "z", ".", "detach", "(", ")", ")", "**", "2", ")", "\n", "\n", "# preserve gradients", "\n", "", "z_q", "=", "z", "+", "(", "z_q", "-", "z", ")", ".", "detach", "(", ")", "\n", "\n", "return", "z_q", ",", "loss", ",", "(", "perplexity", ",", "min_encoding_indices_continual", ",", "\n", "min_encoding_indices_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.VectorQuantizerSpatialTextureAware.get_codebook_entry": [[463, 487], ["torch.interpolate", "torch.interpolate", "torch.interpolate", "torch.interpolate.view", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "range", "torch.fold", "torch.fold", "torch.fold", "torch.fold.view().permute", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "indices_list[].view", "torch.fold.view"], "methods", ["None"], ["", "def", "get_codebook_entry", "(", "self", ",", "indices_list", ",", "segm_map", ",", "shape", ")", ":", "\n", "# flatten segm_map (b, h, w)", "\n", "        ", "segm_map", "=", "F", ".", "interpolate", "(", "\n", "segm_map", ",", "size", "=", "(", "shape", "[", "1", "]", ",", "shape", "[", "2", "]", ")", ",", "mode", "=", "'nearest'", ")", "\n", "segm_map_flatten", "=", "segm_map", ".", "view", "(", "-", "1", ")", "\n", "\n", "z_q", "=", "torch", ".", "zeros", "(", "(", "shape", "[", "0", "]", "*", "shape", "[", "1", "]", "*", "shape", "[", "2", "]", ")", ",", "\n", "self", ".", "e_dim", ")", ".", "to", "(", "segm_map", ".", "device", ")", "\n", "for", "codebook_idx", "in", "range", "(", "18", ")", ":", "\n", "            ", "if", "torch", ".", "sum", "(", "segm_map_flatten", "==", "codebook_idx", ")", ">", "0", ":", "\n", "                ", "min_encoding_indices_selected", "=", "indices_list", "[", "\n", "codebook_idx", "]", ".", "view", "(", "-", "1", ")", "[", "segm_map_flatten", "==", "codebook_idx", "]", "\n", "z_q_selected", "=", "self", ".", "embedding_list", "[", "codebook_idx", "]", "(", "\n", "min_encoding_indices_selected", ")", "\n", "z_q", "[", "segm_map_flatten", "==", "codebook_idx", "]", "=", "z_q_selected", "\n", "\n", "", "", "z_q", "=", "F", ".", "fold", "(", "\n", "z_q", ".", "view", "(", "(", "(", "shape", "[", "0", "]", ",", "shape", "[", "1", "]", "*", "shape", "[", "2", "]", ",", "\n", "self", ".", "e_dim", ")", ")", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ",", "\n", "(", "shape", "[", "1", "]", "*", "self", ".", "spatial_size", ",", "shape", "[", "2", "]", "*", "self", ".", "spatial_size", ")", ",", "\n", "kernel_size", "=", "(", "self", ".", "spatial_size", ",", "self", ".", "spatial_size", ")", ",", "\n", "stride", "=", "self", ".", "spatial_size", ")", "\n", "\n", "return", "z_q", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Upsample.__init__": [[522, 528], ["torch.Module.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "with_conv", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "with_conv", "=", "with_conv", "\n", "if", "self", ".", "with_conv", ":", "\n", "            ", "self", ".", "conv", "=", "torch", ".", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "in_channels", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Upsample.forward": [[529, 535], ["torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "vqgan_arch.Upsample.conv"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "torch", ".", "nn", ".", "functional", ".", "interpolate", "(", "\n", "x", ",", "scale_factor", "=", "2.0", ",", "mode", "=", "\"nearest\"", ")", "\n", "if", "self", ".", "with_conv", ":", "\n", "            ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Downsample.__init__": [[539, 546], ["torch.Module.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "with_conv", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "with_conv", "=", "with_conv", "\n", "if", "self", ".", "with_conv", ":", "\n", "# no asymmetric padding in torch conv, must do it ourselves", "\n", "            ", "self", ".", "conv", "=", "torch", ".", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "in_channels", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Downsample.forward": [[547, 555], ["torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "vqgan_arch.Downsample.conv", "torch.nn.functional.avg_pool2d", "torch.nn.functional.avg_pool2d", "torch.nn.functional.avg_pool2d", "torch.nn.functional.avg_pool2d", "torch.nn.functional.avg_pool2d", "torch.nn.functional.avg_pool2d", "torch.nn.functional.avg_pool2d", "torch.nn.functional.avg_pool2d", "torch.nn.functional.avg_pool2d"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "with_conv", ":", "\n", "            ", "pad", "=", "(", "0", ",", "1", ",", "0", ",", "1", ")", "\n", "x", "=", "torch", ".", "nn", ".", "functional", ".", "pad", "(", "x", ",", "pad", ",", "mode", "=", "\"constant\"", ",", "value", "=", "0", ")", "\n", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "torch", ".", "nn", ".", "functional", ".", "avg_pool2d", "(", "x", ",", "kernel_size", "=", "2", ",", "stride", "=", "2", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.ResnetBlock.__init__": [[559, 596], ["torch.Module.__init__", "vqgan_arch.Normalize", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "vqgan_arch.Normalize", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__", "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Normalize", "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Normalize"], ["    ", "def", "__init__", "(", "self", ",", "\n", "*", ",", "\n", "in_channels", ",", "\n", "out_channels", "=", "None", ",", "\n", "conv_shortcut", "=", "False", ",", "\n", "dropout", ",", "\n", "temb_channels", "=", "512", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "out_channels", "=", "in_channels", "if", "out_channels", "is", "None", "else", "out_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "use_conv_shortcut", "=", "conv_shortcut", "\n", "\n", "self", ".", "norm1", "=", "Normalize", "(", "in_channels", ")", "\n", "self", ".", "conv1", "=", "torch", ".", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "if", "temb_channels", ">", "0", ":", "\n", "            ", "self", ".", "temb_proj", "=", "torch", ".", "nn", ".", "Linear", "(", "temb_channels", ",", "out_channels", ")", "\n", "", "self", ".", "norm2", "=", "Normalize", "(", "out_channels", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "conv2", "=", "torch", ".", "nn", ".", "Conv2d", "(", "\n", "out_channels", ",", "out_channels", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "if", "self", ".", "in_channels", "!=", "self", ".", "out_channels", ":", "\n", "            ", "if", "self", ".", "use_conv_shortcut", ":", "\n", "                ", "self", ".", "conv_shortcut", "=", "torch", ".", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "1", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "nin_shortcut", "=", "torch", ".", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.ResnetBlock.forward": [[597, 618], ["vqgan_arch.ResnetBlock.norm1", "vqgan_arch.nonlinearity", "vqgan_arch.ResnetBlock.conv1", "vqgan_arch.ResnetBlock.norm2", "vqgan_arch.nonlinearity", "vqgan_arch.ResnetBlock.dropout", "vqgan_arch.ResnetBlock.conv2", "vqgan_arch.ResnetBlock.conv_shortcut", "vqgan_arch.ResnetBlock.nin_shortcut", "vqgan_arch.ResnetBlock.temb_proj", "vqgan_arch.nonlinearity"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.nonlinearity", "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.nonlinearity", "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.nonlinearity"], ["", "", "", "def", "forward", "(", "self", ",", "x", ",", "temb", ")", ":", "\n", "        ", "h", "=", "x", "\n", "h", "=", "self", ".", "norm1", "(", "h", ")", "\n", "h", "=", "nonlinearity", "(", "h", ")", "\n", "h", "=", "self", ".", "conv1", "(", "h", ")", "\n", "\n", "if", "temb", "is", "not", "None", ":", "\n", "            ", "h", "=", "h", "+", "self", ".", "temb_proj", "(", "nonlinearity", "(", "temb", ")", ")", "[", ":", ",", ":", ",", "None", ",", "None", "]", "\n", "\n", "", "h", "=", "self", ".", "norm2", "(", "h", ")", "\n", "h", "=", "nonlinearity", "(", "h", ")", "\n", "h", "=", "self", ".", "dropout", "(", "h", ")", "\n", "h", "=", "self", ".", "conv2", "(", "h", ")", "\n", "\n", "if", "self", ".", "in_channels", "!=", "self", ".", "out_channels", ":", "\n", "            ", "if", "self", ".", "use_conv_shortcut", ":", "\n", "                ", "x", "=", "self", ".", "conv_shortcut", "(", "x", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "self", ".", "nin_shortcut", "(", "x", ")", "\n", "\n", "", "", "return", "x", "+", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.AttnBlock.__init__": [[622, 635], ["torch.Module.__init__", "vqgan_arch.Normalize", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__", "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Normalize"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "\n", "self", ".", "norm", "=", "Normalize", "(", "in_channels", ")", "\n", "self", ".", "q", "=", "torch", ".", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "in_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ")", "\n", "self", ".", "k", "=", "torch", ".", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "in_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ")", "\n", "self", ".", "v", "=", "torch", ".", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "in_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ")", "\n", "self", ".", "proj_out", "=", "torch", ".", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "in_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.AttnBlock.forward": [[636, 662], ["vqgan_arch.AttnBlock.norm", "vqgan_arch.AttnBlock.q", "vqgan_arch.AttnBlock.k", "vqgan_arch.AttnBlock.v", "q.permute.permute.reshape", "q.permute.permute.permute", "k.reshape.reshape.reshape", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "v.reshape.reshape.reshape", "w_.permute.permute.permute", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "vqgan_arch.AttnBlock.reshape", "vqgan_arch.AttnBlock.proj_out", "int"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "h_", "=", "x", "\n", "h_", "=", "self", ".", "norm", "(", "h_", ")", "\n", "q", "=", "self", ".", "q", "(", "h_", ")", "\n", "k", "=", "self", ".", "k", "(", "h_", ")", "\n", "v", "=", "self", ".", "v", "(", "h_", ")", "\n", "\n", "# compute attention", "\n", "b", ",", "c", ",", "h", ",", "w", "=", "q", ".", "shape", "\n", "q", "=", "q", ".", "reshape", "(", "b", ",", "c", ",", "h", "*", "w", ")", "\n", "q", "=", "q", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "# b,hw,c", "\n", "k", "=", "k", ".", "reshape", "(", "b", ",", "c", ",", "h", "*", "w", ")", "# b,c,hw", "\n", "w_", "=", "torch", ".", "bmm", "(", "q", ",", "k", ")", "# b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]", "\n", "w_", "=", "w_", "*", "(", "int", "(", "c", ")", "**", "(", "-", "0.5", ")", ")", "\n", "w_", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "w_", ",", "dim", "=", "2", ")", "\n", "\n", "# attend to values", "\n", "v", "=", "v", ".", "reshape", "(", "b", ",", "c", ",", "h", "*", "w", ")", "\n", "w_", "=", "w_", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "# b,hw,hw (first hw of k, second of q)", "\n", "h_", "=", "torch", ".", "bmm", "(", "\n", "v", ",", "w_", ")", "# b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]", "\n", "h_", "=", "h_", ".", "reshape", "(", "b", ",", "c", ",", "h", ",", "w", ")", "\n", "\n", "h_", "=", "self", ".", "proj_out", "(", "h_", ")", "\n", "\n", "return", "x", "+", "h_", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Model.__init__": [[666, 770], ["torch.Module.__init__", "len", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.Module", "torch.Module", "torch.Module", "vqgan_arch.ResnetBlock", "vqgan_arch.AttnBlock", "vqgan_arch.ResnetBlock", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "reversed", "vqgan_arch.Normalize", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.Module", "torch.Module", "torch.Module", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "tuple", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.Module", "torch.Module", "torch.Module", "vqgan_arch.Model.down.append", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.Module", "torch.Module", "torch.Module", "vqgan_arch.Model.up.insert", "torch.ModuleList.append", "vqgan_arch.Downsample", "torch.ModuleList.append", "vqgan_arch.Upsample", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "vqgan_arch.ResnetBlock", "torch.ModuleList.append", "vqgan_arch.ResnetBlock", "torch.ModuleList.append", "vqgan_arch.AttnBlock", "vqgan_arch.AttnBlock"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__", "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Normalize"], ["    ", "def", "__init__", "(", "self", ",", "\n", "*", ",", "\n", "ch", ",", "\n", "out_ch", ",", "\n", "ch_mult", "=", "(", "1", ",", "2", ",", "4", ",", "8", ")", ",", "\n", "num_res_blocks", ",", "\n", "attn_resolutions", ",", "\n", "dropout", "=", "0.0", ",", "\n", "resamp_with_conv", "=", "True", ",", "\n", "in_channels", ",", "\n", "resolution", ",", "\n", "use_timestep", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "ch", "=", "ch", "\n", "self", ".", "temb_ch", "=", "self", ".", "ch", "*", "4", "\n", "self", ".", "num_resolutions", "=", "len", "(", "ch_mult", ")", "\n", "self", ".", "num_res_blocks", "=", "num_res_blocks", "\n", "self", ".", "resolution", "=", "resolution", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "\n", "self", ".", "use_timestep", "=", "use_timestep", "\n", "if", "self", ".", "use_timestep", ":", "\n", "# timestep embedding", "\n", "            ", "self", ".", "temb", "=", "nn", ".", "Module", "(", ")", "\n", "self", ".", "temb", ".", "dense", "=", "nn", ".", "ModuleList", "(", "[", "\n", "torch", ".", "nn", ".", "Linear", "(", "self", ".", "ch", ",", "self", ".", "temb_ch", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "self", ".", "temb_ch", ",", "self", ".", "temb_ch", ")", ",", "\n", "]", ")", "\n", "\n", "# downsampling", "\n", "", "self", ".", "conv_in", "=", "torch", ".", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "self", ".", "ch", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "\n", "curr_res", "=", "resolution", "\n", "in_ch_mult", "=", "(", "1", ",", ")", "+", "tuple", "(", "ch_mult", ")", "\n", "self", ".", "down", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i_level", "in", "range", "(", "self", ".", "num_resolutions", ")", ":", "\n", "            ", "block", "=", "nn", ".", "ModuleList", "(", ")", "\n", "attn", "=", "nn", ".", "ModuleList", "(", ")", "\n", "block_in", "=", "ch", "*", "in_ch_mult", "[", "i_level", "]", "\n", "block_out", "=", "ch", "*", "ch_mult", "[", "i_level", "]", "\n", "for", "i_block", "in", "range", "(", "self", ".", "num_res_blocks", ")", ":", "\n", "                ", "block", ".", "append", "(", "\n", "ResnetBlock", "(", "\n", "in_channels", "=", "block_in", ",", "\n", "out_channels", "=", "block_out", ",", "\n", "temb_channels", "=", "self", ".", "temb_ch", ",", "\n", "dropout", "=", "dropout", ")", ")", "\n", "block_in", "=", "block_out", "\n", "if", "curr_res", "in", "attn_resolutions", ":", "\n", "                    ", "attn", ".", "append", "(", "AttnBlock", "(", "block_in", ")", ")", "\n", "", "", "down", "=", "nn", ".", "Module", "(", ")", "\n", "down", ".", "block", "=", "block", "\n", "down", ".", "attn", "=", "attn", "\n", "if", "i_level", "!=", "self", ".", "num_resolutions", "-", "1", ":", "\n", "                ", "down", ".", "downsample", "=", "Downsample", "(", "block_in", ",", "resamp_with_conv", ")", "\n", "curr_res", "=", "curr_res", "//", "2", "\n", "", "self", ".", "down", ".", "append", "(", "down", ")", "\n", "\n", "# middle", "\n", "", "self", ".", "mid", "=", "nn", ".", "Module", "(", ")", "\n", "self", ".", "mid", ".", "block_1", "=", "ResnetBlock", "(", "\n", "in_channels", "=", "block_in", ",", "\n", "out_channels", "=", "block_in", ",", "\n", "temb_channels", "=", "self", ".", "temb_ch", ",", "\n", "dropout", "=", "dropout", ")", "\n", "self", ".", "mid", ".", "attn_1", "=", "AttnBlock", "(", "block_in", ")", "\n", "self", ".", "mid", ".", "block_2", "=", "ResnetBlock", "(", "\n", "in_channels", "=", "block_in", ",", "\n", "out_channels", "=", "block_in", ",", "\n", "temb_channels", "=", "self", ".", "temb_ch", ",", "\n", "dropout", "=", "dropout", ")", "\n", "\n", "# upsampling", "\n", "self", ".", "up", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i_level", "in", "reversed", "(", "range", "(", "self", ".", "num_resolutions", ")", ")", ":", "\n", "            ", "block", "=", "nn", ".", "ModuleList", "(", ")", "\n", "attn", "=", "nn", ".", "ModuleList", "(", ")", "\n", "block_out", "=", "ch", "*", "ch_mult", "[", "i_level", "]", "\n", "skip_in", "=", "ch", "*", "ch_mult", "[", "i_level", "]", "\n", "for", "i_block", "in", "range", "(", "self", ".", "num_res_blocks", "+", "1", ")", ":", "\n", "                ", "if", "i_block", "==", "self", ".", "num_res_blocks", ":", "\n", "                    ", "skip_in", "=", "ch", "*", "in_ch_mult", "[", "i_level", "]", "\n", "", "block", ".", "append", "(", "\n", "ResnetBlock", "(", "\n", "in_channels", "=", "block_in", "+", "skip_in", ",", "\n", "out_channels", "=", "block_out", ",", "\n", "temb_channels", "=", "self", ".", "temb_ch", ",", "\n", "dropout", "=", "dropout", ")", ")", "\n", "block_in", "=", "block_out", "\n", "if", "curr_res", "in", "attn_resolutions", ":", "\n", "                    ", "attn", ".", "append", "(", "AttnBlock", "(", "block_in", ")", ")", "\n", "", "", "up", "=", "nn", ".", "Module", "(", ")", "\n", "up", ".", "block", "=", "block", "\n", "up", ".", "attn", "=", "attn", "\n", "if", "i_level", "!=", "0", ":", "\n", "                ", "up", ".", "upsample", "=", "Upsample", "(", "block_in", ",", "resamp_with_conv", ")", "\n", "curr_res", "=", "curr_res", "*", "2", "\n", "", "self", ".", "up", ".", "insert", "(", "0", ",", "up", ")", "# prepend to get consistent order", "\n", "\n", "# end", "\n", "", "self", ".", "norm_out", "=", "Normalize", "(", "block_in", ")", "\n", "self", ".", "conv_out", "=", "torch", ".", "nn", ".", "Conv2d", "(", "\n", "block_in", ",", "out_ch", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Model.forward": [[771, 816], ["range", "vqgan_arch.Model.mid.block_1", "vqgan_arch.Model.mid.attn_1", "vqgan_arch.Model.mid.block_2", "reversed", "vqgan_arch.Model.norm_out", "vqgan_arch.nonlinearity", "vqgan_arch.Model.conv_out", "vqgan_arch.get_timestep_embedding", "vqgan_arch.nonlinearity", "vqgan_arch.Model.conv_in", "range", "range", "range", "hs.append", "hs.append", "vqgan_arch.Model.up[].upsample", "len", "vqgan_arch.Model.down[].downsample", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "hs.pop"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.nonlinearity", "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.get_timestep_embedding", "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.nonlinearity"], ["", "def", "forward", "(", "self", ",", "x", ",", "t", "=", "None", ")", ":", "\n", "#assert x.shape[2] == x.shape[3] == self.resolution", "\n", "\n", "        ", "if", "self", ".", "use_timestep", ":", "\n", "# timestep embedding", "\n", "            ", "assert", "t", "is", "not", "None", "\n", "temb", "=", "get_timestep_embedding", "(", "t", ",", "self", ".", "ch", ")", "\n", "temb", "=", "self", ".", "temb", ".", "dense", "[", "0", "]", "(", "temb", ")", "\n", "temb", "=", "nonlinearity", "(", "temb", ")", "\n", "temb", "=", "self", ".", "temb", ".", "dense", "[", "1", "]", "(", "temb", ")", "\n", "", "else", ":", "\n", "            ", "temb", "=", "None", "\n", "\n", "# downsampling", "\n", "", "hs", "=", "[", "self", ".", "conv_in", "(", "x", ")", "]", "\n", "for", "i_level", "in", "range", "(", "self", ".", "num_resolutions", ")", ":", "\n", "            ", "for", "i_block", "in", "range", "(", "self", ".", "num_res_blocks", ")", ":", "\n", "                ", "h", "=", "self", ".", "down", "[", "i_level", "]", ".", "block", "[", "i_block", "]", "(", "hs", "[", "-", "1", "]", ",", "temb", ")", "\n", "if", "len", "(", "self", ".", "down", "[", "i_level", "]", ".", "attn", ")", ">", "0", ":", "\n", "                    ", "h", "=", "self", ".", "down", "[", "i_level", "]", ".", "attn", "[", "i_block", "]", "(", "h", ")", "\n", "", "hs", ".", "append", "(", "h", ")", "\n", "", "if", "i_level", "!=", "self", ".", "num_resolutions", "-", "1", ":", "\n", "                ", "hs", ".", "append", "(", "self", ".", "down", "[", "i_level", "]", ".", "downsample", "(", "hs", "[", "-", "1", "]", ")", ")", "\n", "\n", "# middle", "\n", "", "", "h", "=", "hs", "[", "-", "1", "]", "\n", "h", "=", "self", ".", "mid", ".", "block_1", "(", "h", ",", "temb", ")", "\n", "h", "=", "self", ".", "mid", ".", "attn_1", "(", "h", ")", "\n", "h", "=", "self", ".", "mid", ".", "block_2", "(", "h", ",", "temb", ")", "\n", "\n", "# upsampling", "\n", "for", "i_level", "in", "reversed", "(", "range", "(", "self", ".", "num_resolutions", ")", ")", ":", "\n", "            ", "for", "i_block", "in", "range", "(", "self", ".", "num_res_blocks", "+", "1", ")", ":", "\n", "                ", "h", "=", "self", ".", "up", "[", "i_level", "]", ".", "block", "[", "i_block", "]", "(", "torch", ".", "cat", "(", "[", "h", ",", "hs", ".", "pop", "(", ")", "]", ",", "\n", "dim", "=", "1", ")", ",", "temb", ")", "\n", "if", "len", "(", "self", ".", "up", "[", "i_level", "]", ".", "attn", ")", ">", "0", ":", "\n", "                    ", "h", "=", "self", ".", "up", "[", "i_level", "]", ".", "attn", "[", "i_block", "]", "(", "h", ")", "\n", "", "", "if", "i_level", "!=", "0", ":", "\n", "                ", "h", "=", "self", ".", "up", "[", "i_level", "]", ".", "upsample", "(", "h", ")", "\n", "\n", "# end", "\n", "", "", "h", "=", "self", ".", "norm_out", "(", "h", ")", "\n", "h", "=", "nonlinearity", "(", "h", ")", "\n", "h", "=", "self", ".", "conv_out", "(", "h", ")", "\n", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Encoder.__init__": [[820, 891], ["torch.Module.__init__", "len", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.Module", "torch.Module", "torch.Module", "vqgan_arch.ResnetBlock", "vqgan_arch.AttnBlock", "vqgan_arch.ResnetBlock", "vqgan_arch.Normalize", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "tuple", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.Module", "torch.Module", "torch.Module", "vqgan_arch.Encoder.down.append", "torch.ModuleList.append", "vqgan_arch.Downsample", "vqgan_arch.ResnetBlock", "torch.ModuleList.append", "vqgan_arch.AttnBlock"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__", "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Normalize"], ["    ", "def", "__init__", "(", "self", ",", "\n", "ch", ",", "\n", "num_res_blocks", ",", "\n", "attn_resolutions", ",", "\n", "in_channels", ",", "\n", "resolution", ",", "\n", "z_channels", ",", "\n", "ch_mult", "=", "(", "1", ",", "2", ",", "4", ",", "8", ")", ",", "\n", "dropout", "=", "0.0", ",", "\n", "resamp_with_conv", "=", "True", ",", "\n", "double_z", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "ch", "=", "ch", "\n", "self", ".", "temb_ch", "=", "0", "\n", "self", ".", "num_resolutions", "=", "len", "(", "ch_mult", ")", "\n", "self", ".", "num_res_blocks", "=", "num_res_blocks", "\n", "self", ".", "resolution", "=", "resolution", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "\n", "# downsampling", "\n", "self", ".", "conv_in", "=", "torch", ".", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "self", ".", "ch", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "\n", "curr_res", "=", "resolution", "\n", "in_ch_mult", "=", "(", "1", ",", ")", "+", "tuple", "(", "ch_mult", ")", "\n", "self", ".", "down", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i_level", "in", "range", "(", "self", ".", "num_resolutions", ")", ":", "\n", "            ", "block", "=", "nn", ".", "ModuleList", "(", ")", "\n", "attn", "=", "nn", ".", "ModuleList", "(", ")", "\n", "block_in", "=", "ch", "*", "in_ch_mult", "[", "i_level", "]", "\n", "block_out", "=", "ch", "*", "ch_mult", "[", "i_level", "]", "\n", "for", "i_block", "in", "range", "(", "self", ".", "num_res_blocks", ")", ":", "\n", "                ", "block", ".", "append", "(", "\n", "ResnetBlock", "(", "\n", "in_channels", "=", "block_in", ",", "\n", "out_channels", "=", "block_out", ",", "\n", "temb_channels", "=", "self", ".", "temb_ch", ",", "\n", "dropout", "=", "dropout", ")", ")", "\n", "block_in", "=", "block_out", "\n", "if", "curr_res", "in", "attn_resolutions", ":", "\n", "                    ", "attn", ".", "append", "(", "AttnBlock", "(", "block_in", ")", ")", "\n", "", "", "down", "=", "nn", ".", "Module", "(", ")", "\n", "down", ".", "block", "=", "block", "\n", "down", ".", "attn", "=", "attn", "\n", "if", "i_level", "!=", "self", ".", "num_resolutions", "-", "1", ":", "\n", "                ", "down", ".", "downsample", "=", "Downsample", "(", "block_in", ",", "resamp_with_conv", ")", "\n", "curr_res", "=", "curr_res", "//", "2", "\n", "", "self", ".", "down", ".", "append", "(", "down", ")", "\n", "\n", "# middle", "\n", "", "self", ".", "mid", "=", "nn", ".", "Module", "(", ")", "\n", "self", ".", "mid", ".", "block_1", "=", "ResnetBlock", "(", "\n", "in_channels", "=", "block_in", ",", "\n", "out_channels", "=", "block_in", ",", "\n", "temb_channels", "=", "self", ".", "temb_ch", ",", "\n", "dropout", "=", "dropout", ")", "\n", "self", ".", "mid", ".", "attn_1", "=", "AttnBlock", "(", "block_in", ")", "\n", "self", ".", "mid", ".", "block_2", "=", "ResnetBlock", "(", "\n", "in_channels", "=", "block_in", ",", "\n", "out_channels", "=", "block_in", ",", "\n", "temb_channels", "=", "self", ".", "temb_ch", ",", "\n", "dropout", "=", "dropout", ")", "\n", "\n", "# end", "\n", "self", ".", "norm_out", "=", "Normalize", "(", "block_in", ")", "\n", "self", ".", "conv_out", "=", "torch", ".", "nn", ".", "Conv2d", "(", "\n", "block_in", ",", "\n", "2", "*", "z_channels", "if", "double_z", "else", "z_channels", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Encoder.forward": [[892, 920], ["range", "vqgan_arch.Encoder.mid.block_1", "vqgan_arch.Encoder.mid.attn_1", "vqgan_arch.Encoder.mid.block_2", "vqgan_arch.Encoder.norm_out", "vqgan_arch.nonlinearity", "vqgan_arch.Encoder.conv_out", "vqgan_arch.Encoder.conv_in", "range", "hs.append", "hs.append", "len", "vqgan_arch.Encoder.down[].downsample"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.nonlinearity"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "#assert x.shape[2] == x.shape[3] == self.resolution, \"{}, {}, {}\".format(x.shape[2], x.shape[3], self.resolution)", "\n", "\n", "# timestep embedding", "\n", "        ", "temb", "=", "None", "\n", "\n", "# downsampling", "\n", "hs", "=", "[", "self", ".", "conv_in", "(", "x", ")", "]", "\n", "for", "i_level", "in", "range", "(", "self", ".", "num_resolutions", ")", ":", "\n", "            ", "for", "i_block", "in", "range", "(", "self", ".", "num_res_blocks", ")", ":", "\n", "                ", "h", "=", "self", ".", "down", "[", "i_level", "]", ".", "block", "[", "i_block", "]", "(", "hs", "[", "-", "1", "]", ",", "temb", ")", "\n", "if", "len", "(", "self", ".", "down", "[", "i_level", "]", ".", "attn", ")", ">", "0", ":", "\n", "                    ", "h", "=", "self", ".", "down", "[", "i_level", "]", ".", "attn", "[", "i_block", "]", "(", "h", ")", "\n", "", "hs", ".", "append", "(", "h", ")", "\n", "", "if", "i_level", "!=", "self", ".", "num_resolutions", "-", "1", ":", "\n", "                ", "hs", ".", "append", "(", "self", ".", "down", "[", "i_level", "]", ".", "downsample", "(", "hs", "[", "-", "1", "]", ")", ")", "\n", "\n", "# middle", "\n", "", "", "h", "=", "hs", "[", "-", "1", "]", "\n", "h", "=", "self", ".", "mid", ".", "block_1", "(", "h", ",", "temb", ")", "\n", "h", "=", "self", ".", "mid", ".", "attn_1", "(", "h", ")", "\n", "h", "=", "self", ".", "mid", ".", "block_2", "(", "h", ",", "temb", ")", "\n", "\n", "# end", "\n", "h", "=", "self", ".", "norm_out", "(", "h", ")", "\n", "h", "=", "nonlinearity", "(", "h", ")", "\n", "h", "=", "self", ".", "conv_out", "(", "h", ")", "\n", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Decoder.__init__": [[924, 999], ["torch.Module.__init__", "len", "print", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.Module", "torch.Module", "torch.Module", "vqgan_arch.ResnetBlock", "vqgan_arch.AttnBlock", "vqgan_arch.ResnetBlock", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "reversed", "vqgan_arch.Normalize", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "tuple", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.Module", "torch.Module", "torch.Module", "vqgan_arch.Decoder.up.insert", "numpy.prod", "torch.ModuleList.append", "vqgan_arch.Upsample", "vqgan_arch.ResnetBlock", "torch.ModuleList.append", "vqgan_arch.AttnBlock"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__", "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Normalize"], ["    ", "def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "resolution", ",", "\n", "z_channels", ",", "\n", "ch", ",", "\n", "out_ch", ",", "\n", "num_res_blocks", ",", "\n", "attn_resolutions", ",", "\n", "ch_mult", "=", "(", "1", ",", "2", ",", "4", ",", "8", ")", ",", "\n", "dropout", "=", "0.0", ",", "\n", "resamp_with_conv", "=", "True", ",", "\n", "give_pre_end", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "ch", "=", "ch", "\n", "self", ".", "temb_ch", "=", "0", "\n", "self", ".", "num_resolutions", "=", "len", "(", "ch_mult", ")", "\n", "self", ".", "num_res_blocks", "=", "num_res_blocks", "\n", "self", ".", "resolution", "=", "resolution", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "give_pre_end", "=", "give_pre_end", "\n", "\n", "# compute in_ch_mult, block_in and curr_res at lowest res", "\n", "in_ch_mult", "=", "(", "1", ",", ")", "+", "tuple", "(", "ch_mult", ")", "\n", "block_in", "=", "ch", "*", "ch_mult", "[", "self", ".", "num_resolutions", "-", "1", "]", "\n", "curr_res", "=", "resolution", "//", "2", "**", "(", "self", ".", "num_resolutions", "-", "1", ")", "\n", "self", ".", "z_shape", "=", "(", "1", ",", "z_channels", ",", "curr_res", ",", "curr_res", "//", "2", ")", "\n", "print", "(", "\"Working with z of shape {} = {} dimensions.\"", ".", "format", "(", "\n", "self", ".", "z_shape", ",", "np", ".", "prod", "(", "self", ".", "z_shape", ")", ")", ")", "\n", "\n", "# z to block_in", "\n", "self", ".", "conv_in", "=", "torch", ".", "nn", ".", "Conv2d", "(", "\n", "z_channels", ",", "block_in", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "\n", "# middle", "\n", "self", ".", "mid", "=", "nn", ".", "Module", "(", ")", "\n", "self", ".", "mid", ".", "block_1", "=", "ResnetBlock", "(", "\n", "in_channels", "=", "block_in", ",", "\n", "out_channels", "=", "block_in", ",", "\n", "temb_channels", "=", "self", ".", "temb_ch", ",", "\n", "dropout", "=", "dropout", ")", "\n", "self", ".", "mid", ".", "attn_1", "=", "AttnBlock", "(", "block_in", ")", "\n", "self", ".", "mid", ".", "block_2", "=", "ResnetBlock", "(", "\n", "in_channels", "=", "block_in", ",", "\n", "out_channels", "=", "block_in", ",", "\n", "temb_channels", "=", "self", ".", "temb_ch", ",", "\n", "dropout", "=", "dropout", ")", "\n", "\n", "# upsampling", "\n", "self", ".", "up", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i_level", "in", "reversed", "(", "range", "(", "self", ".", "num_resolutions", ")", ")", ":", "\n", "            ", "block", "=", "nn", ".", "ModuleList", "(", ")", "\n", "attn", "=", "nn", ".", "ModuleList", "(", ")", "\n", "block_out", "=", "ch", "*", "ch_mult", "[", "i_level", "]", "\n", "for", "i_block", "in", "range", "(", "self", ".", "num_res_blocks", "+", "1", ")", ":", "\n", "                ", "block", ".", "append", "(", "\n", "ResnetBlock", "(", "\n", "in_channels", "=", "block_in", ",", "\n", "out_channels", "=", "block_out", ",", "\n", "temb_channels", "=", "self", ".", "temb_ch", ",", "\n", "dropout", "=", "dropout", ")", ")", "\n", "block_in", "=", "block_out", "\n", "if", "curr_res", "in", "attn_resolutions", ":", "\n", "                    ", "attn", ".", "append", "(", "AttnBlock", "(", "block_in", ")", ")", "\n", "", "", "up", "=", "nn", ".", "Module", "(", ")", "\n", "up", ".", "block", "=", "block", "\n", "up", ".", "attn", "=", "attn", "\n", "if", "i_level", "!=", "0", ":", "\n", "                ", "up", ".", "upsample", "=", "Upsample", "(", "block_in", ",", "resamp_with_conv", ")", "\n", "curr_res", "=", "curr_res", "*", "2", "\n", "", "self", ".", "up", ".", "insert", "(", "0", ",", "up", ")", "# prepend to get consistent order", "\n", "\n", "# end", "\n", "", "self", ".", "norm_out", "=", "Normalize", "(", "block_in", ")", "\n", "self", ".", "conv_out", "=", "torch", ".", "nn", ".", "Conv2d", "(", "\n", "block_in", ",", "out_ch", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Decoder.forward": [[1000, 1034], ["vqgan_arch.Decoder.conv_in", "vqgan_arch.Decoder.mid.block_1", "vqgan_arch.Decoder.mid.attn_1", "vqgan_arch.Decoder.mid.block_2", "reversed", "vqgan_arch.Decoder.norm_out", "vqgan_arch.nonlinearity", "vqgan_arch.Decoder.conv_out", "range", "range", "vqgan_arch.Decoder.up[].upsample", "len"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.nonlinearity"], ["", "def", "forward", "(", "self", ",", "z", ",", "bot_h", "=", "None", ")", ":", "\n", "#assert z.shape[1:] == self.z_shape[1:]", "\n", "        ", "self", ".", "last_z_shape", "=", "z", ".", "shape", "\n", "\n", "# timestep embedding", "\n", "temb", "=", "None", "\n", "\n", "# z to block_in", "\n", "h", "=", "self", ".", "conv_in", "(", "z", ")", "\n", "\n", "# middle", "\n", "h", "=", "self", ".", "mid", ".", "block_1", "(", "h", ",", "temb", ")", "\n", "h", "=", "self", ".", "mid", ".", "attn_1", "(", "h", ")", "\n", "h", "=", "self", ".", "mid", ".", "block_2", "(", "h", ",", "temb", ")", "\n", "\n", "# upsampling", "\n", "for", "i_level", "in", "reversed", "(", "range", "(", "self", ".", "num_resolutions", ")", ")", ":", "\n", "            ", "for", "i_block", "in", "range", "(", "self", ".", "num_res_blocks", "+", "1", ")", ":", "\n", "                ", "h", "=", "self", ".", "up", "[", "i_level", "]", ".", "block", "[", "i_block", "]", "(", "h", ",", "temb", ")", "\n", "if", "len", "(", "self", ".", "up", "[", "i_level", "]", ".", "attn", ")", ">", "0", ":", "\n", "                    ", "h", "=", "self", ".", "up", "[", "i_level", "]", ".", "attn", "[", "i_block", "]", "(", "h", ")", "\n", "", "", "if", "i_level", "!=", "0", ":", "\n", "                ", "h", "=", "self", ".", "up", "[", "i_level", "]", ".", "upsample", "(", "h", ")", "\n", "", "if", "i_level", "==", "4", "and", "bot_h", "is", "not", "None", ":", "\n", "                ", "h", "+=", "bot_h", "\n", "\n", "# end", "\n", "", "", "if", "self", ".", "give_pre_end", ":", "\n", "            ", "return", "h", "\n", "\n", "", "h", "=", "self", ".", "norm_out", "(", "h", ")", "\n", "h", "=", "nonlinearity", "(", "h", ")", "\n", "h", "=", "self", ".", "conv_out", "(", "h", ")", "\n", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Decoder.get_feature_top": [[1035, 1060], ["vqgan_arch.Decoder.conv_in", "vqgan_arch.Decoder.mid.block_1", "vqgan_arch.Decoder.mid.attn_1", "vqgan_arch.Decoder.mid.block_2", "reversed", "range", "range", "vqgan_arch.Decoder.up[].upsample", "len"], "methods", ["None"], ["", "def", "get_feature_top", "(", "self", ",", "z", ")", ":", "\n", "#assert z.shape[1:] == self.z_shape[1:]", "\n", "        ", "self", ".", "last_z_shape", "=", "z", ".", "shape", "\n", "\n", "# timestep embedding", "\n", "temb", "=", "None", "\n", "\n", "# z to block_in", "\n", "h", "=", "self", ".", "conv_in", "(", "z", ")", "\n", "\n", "# middle", "\n", "h", "=", "self", ".", "mid", ".", "block_1", "(", "h", ",", "temb", ")", "\n", "h", "=", "self", ".", "mid", ".", "attn_1", "(", "h", ")", "\n", "h", "=", "self", ".", "mid", ".", "block_2", "(", "h", ",", "temb", ")", "\n", "\n", "# upsampling", "\n", "for", "i_level", "in", "reversed", "(", "range", "(", "self", ".", "num_resolutions", ")", ")", ":", "\n", "            ", "for", "i_block", "in", "range", "(", "self", ".", "num_res_blocks", "+", "1", ")", ":", "\n", "                ", "h", "=", "self", ".", "up", "[", "i_level", "]", ".", "block", "[", "i_block", "]", "(", "h", ",", "temb", ")", "\n", "if", "len", "(", "self", ".", "up", "[", "i_level", "]", ".", "attn", ")", ">", "0", ":", "\n", "                    ", "h", "=", "self", ".", "up", "[", "i_level", "]", ".", "attn", "[", "i_block", "]", "(", "h", ")", "\n", "", "", "if", "i_level", "!=", "0", ":", "\n", "                ", "h", "=", "self", ".", "up", "[", "i_level", "]", ".", "upsample", "(", "h", ")", "\n", "", "if", "i_level", "==", "4", ":", "\n", "                ", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Decoder.get_feature_middle": [[1061, 1088], ["vqgan_arch.Decoder.conv_in", "vqgan_arch.Decoder.mid.block_1", "vqgan_arch.Decoder.mid.attn_1", "vqgan_arch.Decoder.mid.block_2", "reversed", "range", "range", "vqgan_arch.Decoder.up[].upsample", "len"], "methods", ["None"], ["", "", "", "def", "get_feature_middle", "(", "self", ",", "z", ",", "mid_h", ")", ":", "\n", "#assert z.shape[1:] == self.z_shape[1:]", "\n", "        ", "self", ".", "last_z_shape", "=", "z", ".", "shape", "\n", "\n", "# timestep embedding", "\n", "temb", "=", "None", "\n", "\n", "# z to block_in", "\n", "h", "=", "self", ".", "conv_in", "(", "z", ")", "\n", "\n", "# middle", "\n", "h", "=", "self", ".", "mid", ".", "block_1", "(", "h", ",", "temb", ")", "\n", "h", "=", "self", ".", "mid", ".", "attn_1", "(", "h", ")", "\n", "h", "=", "self", ".", "mid", ".", "block_2", "(", "h", ",", "temb", ")", "\n", "\n", "# upsampling", "\n", "for", "i_level", "in", "reversed", "(", "range", "(", "self", ".", "num_resolutions", ")", ")", ":", "\n", "            ", "for", "i_block", "in", "range", "(", "self", ".", "num_res_blocks", "+", "1", ")", ":", "\n", "                ", "h", "=", "self", ".", "up", "[", "i_level", "]", ".", "block", "[", "i_block", "]", "(", "h", ",", "temb", ")", "\n", "if", "len", "(", "self", ".", "up", "[", "i_level", "]", ".", "attn", ")", ">", "0", ":", "\n", "                    ", "h", "=", "self", ".", "up", "[", "i_level", "]", ".", "attn", "[", "i_block", "]", "(", "h", ")", "\n", "", "", "if", "i_level", "!=", "0", ":", "\n", "                ", "h", "=", "self", ".", "up", "[", "i_level", "]", ".", "upsample", "(", "h", ")", "\n", "", "if", "i_level", "==", "4", ":", "\n", "                ", "h", "+=", "mid_h", "\n", "", "if", "i_level", "==", "3", ":", "\n", "                ", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.DecoderRes.__init__": [[1092, 1135], ["torch.Module.__init__", "len", "print", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.Module", "torch.Module", "torch.Module", "vqgan_arch.ResnetBlock", "vqgan_arch.AttnBlock", "vqgan_arch.ResnetBlock", "tuple", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "resolution", ",", "\n", "z_channels", ",", "\n", "ch", ",", "\n", "num_res_blocks", ",", "\n", "ch_mult", "=", "(", "1", ",", "2", ",", "4", ",", "8", ")", ",", "\n", "dropout", "=", "0.0", ",", "\n", "give_pre_end", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "ch", "=", "ch", "\n", "self", ".", "temb_ch", "=", "0", "\n", "self", ".", "num_resolutions", "=", "len", "(", "ch_mult", ")", "\n", "self", ".", "num_res_blocks", "=", "num_res_blocks", "\n", "self", ".", "resolution", "=", "resolution", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "give_pre_end", "=", "give_pre_end", "\n", "\n", "# compute in_ch_mult, block_in and curr_res at lowest res", "\n", "in_ch_mult", "=", "(", "1", ",", ")", "+", "tuple", "(", "ch_mult", ")", "\n", "block_in", "=", "ch", "*", "ch_mult", "[", "self", ".", "num_resolutions", "-", "1", "]", "\n", "curr_res", "=", "resolution", "//", "2", "**", "(", "self", ".", "num_resolutions", "-", "1", ")", "\n", "self", ".", "z_shape", "=", "(", "1", ",", "z_channels", ",", "curr_res", ",", "curr_res", "//", "2", ")", "\n", "print", "(", "\"Working with z of shape {} = {} dimensions.\"", ".", "format", "(", "\n", "self", ".", "z_shape", ",", "np", ".", "prod", "(", "self", ".", "z_shape", ")", ")", ")", "\n", "\n", "# z to block_in", "\n", "self", ".", "conv_in", "=", "torch", ".", "nn", ".", "Conv2d", "(", "\n", "z_channels", ",", "block_in", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "\n", "# middle", "\n", "self", ".", "mid", "=", "nn", ".", "Module", "(", ")", "\n", "self", ".", "mid", ".", "block_1", "=", "ResnetBlock", "(", "\n", "in_channels", "=", "block_in", ",", "\n", "out_channels", "=", "block_in", ",", "\n", "temb_channels", "=", "self", ".", "temb_ch", ",", "\n", "dropout", "=", "dropout", ")", "\n", "self", ".", "mid", ".", "attn_1", "=", "AttnBlock", "(", "block_in", ")", "\n", "self", ".", "mid", ".", "block_2", "=", "ResnetBlock", "(", "\n", "in_channels", "=", "block_in", ",", "\n", "out_channels", "=", "block_in", ",", "\n", "temb_channels", "=", "self", ".", "temb_ch", ",", "\n", "dropout", "=", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.DecoderRes.forward": [[1136, 1152], ["vqgan_arch.DecoderRes.conv_in", "vqgan_arch.DecoderRes.mid.block_1", "vqgan_arch.DecoderRes.mid.attn_1", "vqgan_arch.DecoderRes.mid.block_2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "z", ")", ":", "\n", "#assert z.shape[1:] == self.z_shape[1:]", "\n", "        ", "self", ".", "last_z_shape", "=", "z", ".", "shape", "\n", "\n", "# timestep embedding", "\n", "temb", "=", "None", "\n", "\n", "# z to block_in", "\n", "h", "=", "self", ".", "conv_in", "(", "z", ")", "\n", "\n", "# middle", "\n", "h", "=", "self", ".", "mid", ".", "block_1", "(", "h", ",", "temb", ")", "\n", "h", "=", "self", ".", "mid", ".", "attn_1", "(", "h", ")", "\n", "h", "=", "self", ".", "mid", ".", "block_2", "(", "h", ",", "temb", ")", "\n", "\n", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Discriminator.__init__": [[1157, 1201], ["torch.Module.__init__", "range", "min", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU", "min", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "nc", ",", "ndf", ",", "n_layers", "=", "3", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "layers", "=", "[", "\n", "nn", ".", "Conv2d", "(", "nc", ",", "ndf", ",", "kernel_size", "=", "4", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", "\n", "]", "\n", "ndf_mult", "=", "1", "\n", "ndf_mult_prev", "=", "1", "\n", "for", "n", "in", "range", "(", "1", ",", "\n", "n_layers", ")", ":", "# gradually increase the number of filters", "\n", "            ", "ndf_mult_prev", "=", "ndf_mult", "\n", "ndf_mult", "=", "min", "(", "2", "**", "n", ",", "8", ")", "\n", "layers", "+=", "[", "\n", "nn", ".", "Conv2d", "(", "\n", "ndf", "*", "ndf_mult_prev", ",", "\n", "ndf", "*", "ndf_mult", ",", "\n", "kernel_size", "=", "4", ",", "\n", "stride", "=", "2", ",", "\n", "padding", "=", "1", ",", "\n", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "ndf", "*", "ndf_mult", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", "\n", "]", "\n", "\n", "", "ndf_mult_prev", "=", "ndf_mult", "\n", "ndf_mult", "=", "min", "(", "2", "**", "n_layers", ",", "8", ")", "\n", "\n", "layers", "+=", "[", "\n", "nn", ".", "Conv2d", "(", "\n", "ndf", "*", "ndf_mult_prev", ",", "\n", "ndf", "*", "ndf_mult", ",", "\n", "kernel_size", "=", "4", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "1", ",", "\n", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "ndf", "*", "ndf_mult", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", "\n", "]", "\n", "\n", "layers", "+=", "[", "\n", "nn", ".", "Conv2d", "(", "ndf", "*", "ndf_mult", ",", "1", ",", "kernel_size", "=", "4", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "]", "# output 1 channel prediction map", "\n", "self", ".", "main", "=", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Discriminator.forward": [[1202, 1204], ["vqgan_arch.Discriminator.main"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.None.train_index_prediction.main"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "main", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.sample_patches": [[312, 327], ["torch.unfold"], "function", ["None"], ["", "", "def", "sample_patches", "(", "inputs", ",", "patch_size", "=", "3", ",", "stride", "=", "1", ")", ":", "\n", "    ", "\"\"\"Extract sliding local patches from an input feature tensor.\n    The sampled pathes are row-major.\n    Args:\n        inputs (Tensor): the input feature maps, shape: (n, c, h, w).\n        patch_size (int): the spatial size of sampled patches. Default: 3.\n        stride (int): the stride of sampling. Default: 1.\n    Returns:\n        patches (Tensor): extracted patches, shape: (n, c *  patch_size *\n            patch_size, n_patches).\n    \"\"\"", "\n", "\n", "patches", "=", "F", ".", "unfold", "(", "inputs", ",", "(", "patch_size", ",", "patch_size", ")", ",", "stride", "=", "stride", ")", "\n", "\n", "return", "patches", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.get_timestep_embedding": [[489, 508], ["torch.exp", "torch.exp", "torch.exp", "torch.nn.functional.pad.to", "torch.cat", "torch.cat", "torch.cat", "len", "math.log", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.arange", "torch.arange", "torch.arange", "timesteps.float", "torch.sin", "torch.sin", "torch.sin", "torch.cos", "torch.cos", "torch.cos"], "function", ["None"], ["", "", "def", "get_timestep_embedding", "(", "timesteps", ",", "embedding_dim", ")", ":", "\n", "    ", "\"\"\"\n    This matches the implementation in Denoising Diffusion Probabilistic Models:\n    From Fairseq.\n    Build sinusoidal embeddings.\n    This matches the implementation in tensor2tensor, but differs slightly\n    from the description in Section 3.5 of \"Attention Is All You Need\".\n    \"\"\"", "\n", "assert", "len", "(", "timesteps", ".", "shape", ")", "==", "1", "\n", "\n", "half_dim", "=", "embedding_dim", "//", "2", "\n", "emb", "=", "math", ".", "log", "(", "10000", ")", "/", "(", "half_dim", "-", "1", ")", "\n", "emb", "=", "torch", ".", "exp", "(", "torch", ".", "arange", "(", "half_dim", ",", "dtype", "=", "torch", ".", "float32", ")", "*", "-", "emb", ")", "\n", "emb", "=", "emb", ".", "to", "(", "device", "=", "timesteps", ".", "device", ")", "\n", "emb", "=", "timesteps", ".", "float", "(", ")", "[", ":", ",", "None", "]", "*", "emb", "[", "None", ",", ":", "]", "\n", "emb", "=", "torch", ".", "cat", "(", "[", "torch", ".", "sin", "(", "emb", ")", ",", "torch", ".", "cos", "(", "emb", ")", "]", ",", "dim", "=", "1", ")", "\n", "if", "embedding_dim", "%", "2", "==", "1", ":", "# zero pad", "\n", "        ", "emb", "=", "torch", ".", "nn", ".", "functional", ".", "pad", "(", "emb", ",", "(", "0", ",", "1", ",", "0", ",", "0", ")", ")", "\n", "", "return", "emb", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.nonlinearity": [[510, 513], ["torch.sigmoid", "torch.sigmoid", "torch.sigmoid"], "function", ["None"], ["", "def", "nonlinearity", "(", "x", ")", ":", "\n", "# swish", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.vqgan_arch.Normalize": [[515, 518], ["torch.nn.GroupNorm", "torch.nn.GroupNorm", "torch.nn.GroupNorm"], "function", ["None"], ["", "def", "Normalize", "(", "in_channels", ")", ":", "\n", "    ", "return", "torch", ".", "nn", ".", "GroupNorm", "(", "\n", "num_groups", "=", "32", ",", "num_channels", "=", "in_channels", ",", "eps", "=", "1e-6", ",", "affine", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.fcn_arch.BaseDecodeHead.__init__": [[39, 70], ["dict", "dict", "torch.Module.__init__", "fcn_arch.BaseDecodeHead._init_inputs", "torch.Conv2d", "torch.Conv2d", "torch.Dropout2d", "torch.Dropout2d"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__", "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.fcn_arch.MultiHeadFCNHead._init_inputs"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "channels", ",", "\n", "*", ",", "\n", "num_classes", ",", "\n", "dropout_ratio", "=", "0.1", ",", "\n", "conv_cfg", "=", "None", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN'", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ")", ",", "\n", "in_index", "=", "-", "1", ",", "\n", "input_transform", "=", "None", ",", "\n", "ignore_index", "=", "255", ",", "\n", "align_corners", "=", "False", ")", ":", "\n", "        ", "super", "(", "BaseDecodeHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_init_inputs", "(", "in_channels", ",", "in_index", ",", "input_transform", ")", "\n", "self", ".", "channels", "=", "channels", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "dropout_ratio", "=", "dropout_ratio", "\n", "self", ".", "conv_cfg", "=", "conv_cfg", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "act_cfg", "=", "act_cfg", "\n", "self", ".", "in_index", "=", "in_index", "\n", "\n", "self", ".", "ignore_index", "=", "ignore_index", "\n", "self", ".", "align_corners", "=", "align_corners", "\n", "\n", "self", ".", "conv_seg", "=", "nn", ".", "Conv2d", "(", "channels", ",", "num_classes", ",", "kernel_size", "=", "1", ")", "\n", "if", "dropout_ratio", ">", "0", ":", "\n", "            ", "self", ".", "dropout", "=", "nn", ".", "Dropout2d", "(", "dropout_ratio", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.fcn_arch.BaseDecodeHead.extra_repr": [[71, 77], ["None"], "methods", ["None"], ["", "", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "\"\"\"Extra repr.\"\"\"", "\n", "s", "=", "f'input_transform={self.input_transform}, '", "f'ignore_index={self.ignore_index}, '", "f'align_corners={self.align_corners}'", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.fcn_arch.BaseDecodeHead._init_inputs": [[78, 115], ["isinstance", "isinstance", "isinstance", "isinstance", "len", "len", "sum"], "methods", ["None"], ["", "def", "_init_inputs", "(", "self", ",", "in_channels", ",", "in_index", ",", "input_transform", ")", ":", "\n", "        ", "\"\"\"Check and initialize input transforms.\n\n        The in_channels, in_index and input_transform must match.\n        Specifically, when input_transform is None, only single feature map\n        will be selected. So in_channels and in_index must be of type int.\n        When input_transform\n\n        Args:\n            in_channels (int|Sequence[int]): Input channels.\n            in_index (int|Sequence[int]): Input feature index.\n            input_transform (str|None): Transformation type of input features.\n                Options: 'resize_concat', 'multiple_select', None.\n                'resize_concat': Multiple feature maps will be resize to the\n                    same size as first one and than concat together.\n                    Usually used in FCN head of HRNet.\n                'multiple_select': Multiple feature maps will be bundle into\n                    a list and passed into decode head.\n                None: Only one select feature map is allowed.\n        \"\"\"", "\n", "\n", "if", "input_transform", "is", "not", "None", ":", "\n", "            ", "assert", "input_transform", "in", "[", "'resize_concat'", ",", "'multiple_select'", "]", "\n", "", "self", ".", "input_transform", "=", "input_transform", "\n", "self", ".", "in_index", "=", "in_index", "\n", "if", "input_transform", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "in_channels", ",", "(", "list", ",", "tuple", ")", ")", "\n", "assert", "isinstance", "(", "in_index", ",", "(", "list", ",", "tuple", ")", ")", "\n", "assert", "len", "(", "in_channels", ")", "==", "len", "(", "in_index", ")", "\n", "if", "input_transform", "==", "'resize_concat'", ":", "\n", "                ", "self", ".", "in_channels", "=", "sum", "(", "in_channels", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "in_channels", "=", "in_channels", "\n", "", "", "else", ":", "\n", "            ", "assert", "isinstance", "(", "in_channels", ",", "int", ")", "\n", "assert", "isinstance", "(", "in_index", ",", "int", ")", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.fcn_arch.BaseDecodeHead.init_weights": [[116, 119], ["mmcv.cnn.normal_init"], "methods", ["None"], ["", "", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initialize weights of classification layer.\"\"\"", "\n", "normal_init", "(", "self", ".", "conv_seg", ",", "mean", "=", "0", ",", "std", "=", "0.01", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.fcn_arch.BaseDecodeHead._transform_inputs": [[120, 146], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "mmseg.ops.resize"], "methods", ["None"], ["", "def", "_transform_inputs", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "\"\"\"Transform inputs for decoder.\n\n        Args:\n            inputs (list[Tensor]): List of multi-level img features.\n\n        Returns:\n            Tensor: The transformed inputs\n        \"\"\"", "\n", "\n", "if", "self", ".", "input_transform", "==", "'resize_concat'", ":", "\n", "            ", "inputs", "=", "[", "inputs", "[", "i", "]", "for", "i", "in", "self", ".", "in_index", "]", "\n", "upsampled_inputs", "=", "[", "\n", "resize", "(", "\n", "input", "=", "x", ",", "\n", "size", "=", "inputs", "[", "0", "]", ".", "shape", "[", "2", ":", "]", ",", "\n", "mode", "=", "'bilinear'", ",", "\n", "align_corners", "=", "self", ".", "align_corners", ")", "for", "x", "in", "inputs", "\n", "]", "\n", "inputs", "=", "torch", ".", "cat", "(", "upsampled_inputs", ",", "dim", "=", "1", ")", "\n", "", "elif", "self", ".", "input_transform", "==", "'multiple_select'", ":", "\n", "            ", "inputs", "=", "[", "inputs", "[", "i", "]", "for", "i", "in", "self", ".", "in_index", "]", "\n", "", "else", ":", "\n", "            ", "inputs", "=", "inputs", "[", "self", ".", "in_index", "]", "\n", "\n", "", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.fcn_arch.BaseDecodeHead.forward": [[147, 150], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "\"\"\"Placeholder of forward function.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.fcn_arch.BaseDecodeHead.cls_seg": [[151, 157], ["fcn_arch.BaseDecodeHead.conv_seg", "fcn_arch.BaseDecodeHead.dropout"], "methods", ["None"], ["", "def", "cls_seg", "(", "self", ",", "feat", ")", ":", "\n", "        ", "\"\"\"Classify each pixel.\"\"\"", "\n", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "feat", "=", "self", ".", "dropout", "(", "feat", ")", "\n", "", "output", "=", "self", ".", "conv_seg", "(", "feat", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.fcn_arch.FCNHead.__init__": [[171, 217], ["fcn_arch.BaseDecodeHead.__init__", "convs.append", "range", "mmcv.cnn.ConvModule", "convs.append", "torch.Identity", "torch.Identity", "torch.Sequential", "torch.Sequential", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["def", "__init__", "(", "self", ",", "\n", "num_convs", "=", "2", ",", "\n", "kernel_size", "=", "3", ",", "\n", "concat_input", "=", "True", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "assert", "num_convs", ">=", "0", "\n", "self", ".", "num_convs", "=", "num_convs", "\n", "self", ".", "concat_input", "=", "concat_input", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "super", "(", "FCNHead", ",", "self", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "if", "num_convs", "==", "0", ":", "\n", "            ", "assert", "self", ".", "in_channels", "==", "self", ".", "channels", "\n", "\n", "", "convs", "=", "[", "]", "\n", "convs", ".", "append", "(", "\n", "ConvModule", "(", "\n", "self", ".", "in_channels", ",", "\n", "self", ".", "channels", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "padding", "=", "kernel_size", "//", "2", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", ")", "\n", "for", "i", "in", "range", "(", "num_convs", "-", "1", ")", ":", "\n", "            ", "convs", ".", "append", "(", "\n", "ConvModule", "(", "\n", "self", ".", "channels", ",", "\n", "self", ".", "channels", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "padding", "=", "kernel_size", "//", "2", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", ")", "\n", "", "if", "num_convs", "==", "0", ":", "\n", "            ", "self", ".", "convs", "=", "nn", ".", "Identity", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "convs", "=", "nn", ".", "Sequential", "(", "*", "convs", ")", "\n", "", "if", "self", ".", "concat_input", ":", "\n", "            ", "self", ".", "conv_cat", "=", "ConvModule", "(", "\n", "self", ".", "in_channels", "+", "self", ".", "channels", ",", "\n", "self", ".", "channels", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "padding", "=", "kernel_size", "//", "2", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.fcn_arch.FCNHead.forward": [[218, 226], ["fcn_arch.FCNHead._transform_inputs", "fcn_arch.FCNHead.convs", "fcn_arch.FCNHead.cls_seg", "fcn_arch.FCNHead.conv_cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.archs.fcn_arch.MultiHeadFCNHead._transform_inputs", "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.fcn_arch.BaseDecodeHead.cls_seg"], ["", "", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "\"\"\"Forward function.\"\"\"", "\n", "x", "=", "self", ".", "_transform_inputs", "(", "inputs", ")", "\n", "output", "=", "self", ".", "convs", "(", "x", ")", "\n", "if", "self", ".", "concat_input", ":", "\n", "            ", "output", "=", "self", ".", "conv_cat", "(", "torch", ".", "cat", "(", "[", "x", ",", "output", "]", ",", "dim", "=", "1", ")", ")", "\n", "", "output", "=", "self", ".", "cls_seg", "(", "output", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.fcn_arch.MultiHeadFCNHead.__init__": [[240, 332], ["dict", "dict", "torch.Module.__init__", "fcn_arch.MultiHeadFCNHead._init_inputs", "range", "torch.ModuleList", "torch.ModuleList", "fcn_arch.MultiHeadFCNHead.init_weights", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Dropout2d", "torch.Dropout2d", "conv_seg_head_list.append", "convs.append", "range", "torch.Conv2d", "torch.Conv2d", "mmcv.cnn.ConvModule", "convs.append", "convs_list.append", "convs_list.append", "conv_cat_list.append", "mmcv.cnn.ConvModule", "torch.Identity", "torch.Identity", "torch.Sequential", "torch.Sequential", "mmcv.cnn.ConvModule"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__", "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.fcn_arch.MultiHeadFCNHead._init_inputs", "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.fcn_arch.MultiHeadFCNHead.init_weights"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "channels", ",", "\n", "*", ",", "\n", "num_classes", ",", "\n", "dropout_ratio", "=", "0.1", ",", "\n", "conv_cfg", "=", "None", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN'", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ")", ",", "\n", "in_index", "=", "-", "1", ",", "\n", "input_transform", "=", "None", ",", "\n", "ignore_index", "=", "255", ",", "\n", "align_corners", "=", "False", ",", "\n", "num_convs", "=", "2", ",", "\n", "kernel_size", "=", "3", ",", "\n", "concat_input", "=", "True", ",", "\n", "num_head", "=", "18", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "MultiHeadFCNHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "num_convs", ">=", "0", "\n", "self", ".", "num_convs", "=", "num_convs", "\n", "self", ".", "concat_input", "=", "concat_input", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "_init_inputs", "(", "in_channels", ",", "in_index", ",", "input_transform", ")", "\n", "self", ".", "channels", "=", "channels", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "dropout_ratio", "=", "dropout_ratio", "\n", "self", ".", "conv_cfg", "=", "conv_cfg", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "act_cfg", "=", "act_cfg", "\n", "self", ".", "in_index", "=", "in_index", "\n", "self", ".", "num_head", "=", "num_head", "\n", "\n", "self", ".", "ignore_index", "=", "ignore_index", "\n", "self", ".", "align_corners", "=", "align_corners", "\n", "\n", "if", "dropout_ratio", ">", "0", ":", "\n", "            ", "self", ".", "dropout", "=", "nn", ".", "Dropout2d", "(", "dropout_ratio", ")", "\n", "\n", "", "conv_seg_head_list", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "self", ".", "num_head", ")", ":", "\n", "            ", "conv_seg_head_list", ".", "append", "(", "\n", "nn", ".", "Conv2d", "(", "channels", ",", "num_classes", ",", "kernel_size", "=", "1", ")", ")", "\n", "\n", "", "self", ".", "conv_seg_head_list", "=", "nn", ".", "ModuleList", "(", "conv_seg_head_list", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n", "if", "num_convs", "==", "0", ":", "\n", "            ", "assert", "self", ".", "in_channels", "==", "self", ".", "channels", "\n", "\n", "", "convs_list", "=", "[", "]", "\n", "conv_cat_list", "=", "[", "]", "\n", "\n", "for", "_", "in", "range", "(", "self", ".", "num_head", ")", ":", "\n", "            ", "convs", "=", "[", "]", "\n", "convs", ".", "append", "(", "\n", "ConvModule", "(", "\n", "self", ".", "in_channels", ",", "\n", "self", ".", "channels", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "padding", "=", "kernel_size", "//", "2", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", ")", "\n", "for", "_", "in", "range", "(", "num_convs", "-", "1", ")", ":", "\n", "                ", "convs", ".", "append", "(", "\n", "ConvModule", "(", "\n", "self", ".", "channels", ",", "\n", "self", ".", "channels", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "padding", "=", "kernel_size", "//", "2", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", ")", "\n", "", "if", "num_convs", "==", "0", ":", "\n", "                ", "convs_list", ".", "append", "(", "nn", ".", "Identity", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "convs_list", ".", "append", "(", "nn", ".", "Sequential", "(", "*", "convs", ")", ")", "\n", "", "if", "self", ".", "concat_input", ":", "\n", "                ", "conv_cat_list", ".", "append", "(", "\n", "ConvModule", "(", "\n", "self", ".", "in_channels", "+", "self", ".", "channels", ",", "\n", "self", ".", "channels", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "padding", "=", "kernel_size", "//", "2", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", ")", "\n", "\n", "", "", "self", ".", "convs_list", "=", "nn", ".", "ModuleList", "(", "convs_list", ")", "\n", "self", ".", "conv_cat_list", "=", "nn", ".", "ModuleList", "(", "conv_cat_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.fcn_arch.MultiHeadFCNHead.forward": [[333, 349], ["fcn_arch.MultiHeadFCNHead._transform_inputs", "range", "output_list.append", "fcn_arch.MultiHeadFCNHead.dropout", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.archs.fcn_arch.MultiHeadFCNHead._transform_inputs"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "\"\"\"Forward function.\"\"\"", "\n", "x", "=", "self", ".", "_transform_inputs", "(", "inputs", ")", "\n", "\n", "output_list", "=", "[", "]", "\n", "for", "head_idx", "in", "range", "(", "self", ".", "num_head", ")", ":", "\n", "            ", "output", "=", "self", ".", "convs_list", "[", "head_idx", "]", "(", "x", ")", "\n", "if", "self", ".", "concat_input", ":", "\n", "                ", "output", "=", "self", ".", "conv_cat_list", "[", "head_idx", "]", "(", "\n", "torch", ".", "cat", "(", "[", "x", ",", "output", "]", ",", "dim", "=", "1", ")", ")", "\n", "", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "                ", "output", "=", "self", ".", "dropout", "(", "output", ")", "\n", "", "output", "=", "self", ".", "conv_seg_head_list", "[", "head_idx", "]", "(", "output", ")", "\n", "output_list", ".", "append", "(", "output", ")", "\n", "\n", "", "return", "output_list", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.fcn_arch.MultiHeadFCNHead._init_inputs": [[350, 387], ["isinstance", "isinstance", "isinstance", "isinstance", "len", "len", "sum"], "methods", ["None"], ["", "def", "_init_inputs", "(", "self", ",", "in_channels", ",", "in_index", ",", "input_transform", ")", ":", "\n", "        ", "\"\"\"Check and initialize input transforms.\n\n        The in_channels, in_index and input_transform must match.\n        Specifically, when input_transform is None, only single feature map\n        will be selected. So in_channels and in_index must be of type int.\n        When input_transform\n\n        Args:\n            in_channels (int|Sequence[int]): Input channels.\n            in_index (int|Sequence[int]): Input feature index.\n            input_transform (str|None): Transformation type of input features.\n                Options: 'resize_concat', 'multiple_select', None.\n                'resize_concat': Multiple feature maps will be resize to the\n                    same size as first one and than concat together.\n                    Usually used in FCN head of HRNet.\n                'multiple_select': Multiple feature maps will be bundle into\n                    a list and passed into decode head.\n                None: Only one select feature map is allowed.\n        \"\"\"", "\n", "\n", "if", "input_transform", "is", "not", "None", ":", "\n", "            ", "assert", "input_transform", "in", "[", "'resize_concat'", ",", "'multiple_select'", "]", "\n", "", "self", ".", "input_transform", "=", "input_transform", "\n", "self", ".", "in_index", "=", "in_index", "\n", "if", "input_transform", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "in_channels", ",", "(", "list", ",", "tuple", ")", ")", "\n", "assert", "isinstance", "(", "in_index", ",", "(", "list", ",", "tuple", ")", ")", "\n", "assert", "len", "(", "in_channels", ")", "==", "len", "(", "in_index", ")", "\n", "if", "input_transform", "==", "'resize_concat'", ":", "\n", "                ", "self", ".", "in_channels", "=", "sum", "(", "in_channels", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "in_channels", "=", "in_channels", "\n", "", "", "else", ":", "\n", "            ", "assert", "isinstance", "(", "in_channels", ",", "int", ")", "\n", "assert", "isinstance", "(", "in_index", ",", "int", ")", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.fcn_arch.MultiHeadFCNHead.init_weights": [[388, 392], ["mmcv.cnn.normal_init"], "methods", ["None"], ["", "", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initialize weights of classification layer.\"\"\"", "\n", "for", "conv_seg_head", "in", "self", ".", "conv_seg_head_list", ":", "\n", "            ", "normal_init", "(", "conv_seg_head", ",", "mean", "=", "0", ",", "std", "=", "0.01", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.archs.fcn_arch.MultiHeadFCNHead._transform_inputs": [[393, 419], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "mmseg.ops.resize"], "methods", ["None"], ["", "", "def", "_transform_inputs", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "\"\"\"Transform inputs for decoder.\n\n        Args:\n            inputs (list[Tensor]): List of multi-level img features.\n\n        Returns:\n            Tensor: The transformed inputs\n        \"\"\"", "\n", "\n", "if", "self", ".", "input_transform", "==", "'resize_concat'", ":", "\n", "            ", "inputs", "=", "[", "inputs", "[", "i", "]", "for", "i", "in", "self", ".", "in_index", "]", "\n", "upsampled_inputs", "=", "[", "\n", "resize", "(", "\n", "input", "=", "x", ",", "\n", "size", "=", "inputs", "[", "0", "]", ".", "shape", "[", "2", ":", "]", ",", "\n", "mode", "=", "'bilinear'", ",", "\n", "align_corners", "=", "self", ".", "align_corners", ")", "for", "x", "in", "inputs", "\n", "]", "\n", "inputs", "=", "torch", ".", "cat", "(", "upsampled_inputs", ",", "dim", "=", "1", ")", "\n", "", "elif", "self", ".", "input_transform", "==", "'multiple_select'", ":", "\n", "            ", "inputs", "=", "[", "inputs", "[", "i", "]", "for", "i", "in", "self", ".", "in_index", "]", "\n", "", "else", ":", "\n", "            ", "inputs", "=", "inputs", "[", "self", ".", "in_index", "]", "\n", "\n", "", "return", "inputs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.segmentation_loss.BCELoss.forward": [[7, 10], ["torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "prediction", ",", "target", ")", ":", "\n", "        ", "loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "prediction", ",", "target", ")", "\n", "return", "loss", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.segmentation_loss.BCELossWithQuant.__init__": [[14, 17], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "codebook_weight", "=", "1.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "codebook_weight", "=", "codebook_weight", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.segmentation_loss.BCELossWithQuant.forward": [[18, 25], ["torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "loss.clone().detach().mean", "torch.binary_cross_entropy_with_logits.detach().mean", "qloss.detach().mean", "loss.clone().detach", "torch.binary_cross_entropy_with_logits.detach", "qloss.detach", "loss.clone"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "qloss", ",", "target", ",", "prediction", ",", "split", ")", ":", "\n", "        ", "bce_loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "prediction", ",", "target", ")", "\n", "loss", "=", "bce_loss", "+", "self", ".", "codebook_weight", "*", "qloss", "\n", "return", "loss", ",", "{", "\n", "\"{}/total_loss\"", ".", "format", "(", "split", ")", ":", "loss", ".", "clone", "(", ")", ".", "detach", "(", ")", ".", "mean", "(", ")", ",", "\n", "\"{}/bce_loss\"", ".", "format", "(", "split", ")", ":", "bce_loss", ".", "detach", "(", ")", ".", "mean", "(", ")", ",", "\n", "\"{}/quant_loss\"", ".", "format", "(", "split", ")", ":", "qloss", ".", "detach", "(", ")", ".", "mean", "(", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.calculate_adaptive_weight": [[5, 13], ["torch.clamp().detach", "torch.clamp().detach", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.clamp", "torch.clamp"], "function", ["None"], ["def", "calculate_adaptive_weight", "(", "recon_loss", ",", "g_loss", ",", "last_layer", ",", "disc_weight_max", ")", ":", "\n", "    ", "recon_grads", "=", "torch", ".", "autograd", ".", "grad", "(", "\n", "recon_loss", ",", "last_layer", ",", "retain_graph", "=", "True", ")", "[", "0", "]", "\n", "g_grads", "=", "torch", ".", "autograd", ".", "grad", "(", "g_loss", ",", "last_layer", ",", "retain_graph", "=", "True", ")", "[", "0", "]", "\n", "\n", "d_weight", "=", "torch", ".", "norm", "(", "recon_grads", ")", "/", "(", "torch", ".", "norm", "(", "g_grads", ")", "+", "1e-4", ")", "\n", "d_weight", "=", "torch", ".", "clamp", "(", "d_weight", ",", "0.0", ",", "disc_weight_max", ")", ".", "detach", "(", ")", "\n", "return", "d_weight", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.adopt_weight": [[15, 19], ["None"], "function", ["None"], ["", "def", "adopt_weight", "(", "weight", ",", "global_step", ",", "threshold", "=", "0", ",", "value", "=", "0.", ")", ":", "\n", "    ", "if", "global_step", "<", "threshold", ":", "\n", "        ", "weight", "=", "value", "\n", "", "return", "weight", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.hinge_d_loss": [[21, 27], ["torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.relu", "torch.relu"], "function", ["None"], ["", "@", "torch", ".", "jit", ".", "script", "\n", "def", "hinge_d_loss", "(", "logits_real", ",", "logits_fake", ")", ":", "\n", "    ", "loss_real", "=", "torch", ".", "mean", "(", "F", ".", "relu", "(", "1.", "-", "logits_real", ")", ")", "\n", "loss_fake", "=", "torch", ".", "mean", "(", "F", ".", "relu", "(", "1.", "+", "logits_fake", ")", ")", "\n", "d_loss", "=", "0.5", "*", "(", "loss_real", "+", "loss_fake", ")", "\n", "return", "d_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.DiffAugment": [[29, 40], ["policy.split", "f.contiguous", "f.permute", "f.permute", "f"], "function", ["None"], ["", "def", "DiffAugment", "(", "x", ",", "policy", "=", "''", ",", "channels_first", "=", "True", ")", ":", "\n", "    ", "if", "policy", ":", "\n", "        ", "if", "not", "channels_first", ":", "\n", "            ", "x", "=", "x", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "", "for", "p", "in", "policy", ".", "split", "(", "','", ")", ":", "\n", "            ", "for", "f", "in", "AUGMENT_FNS", "[", "p", "]", ":", "\n", "                ", "x", "=", "f", "(", "x", ")", "\n", "", "", "if", "not", "channels_first", ":", "\n", "            ", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "\n", "", "x", "=", "x", ".", "contiguous", "(", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.rand_brightness": [[42, 46], ["torch.rand", "torch.rand", "x.size"], "function", ["None"], ["", "def", "rand_brightness", "(", "x", ")", ":", "\n", "    ", "x", "=", "x", "+", "(", "\n", "torch", ".", "rand", "(", "x", ".", "size", "(", "0", ")", ",", "1", ",", "1", ",", "1", ",", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "-", "0.5", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.rand_saturation": [[48, 53], ["x.mean", "torch.rand", "torch.rand", "x.size"], "function", ["None"], ["", "def", "rand_saturation", "(", "x", ")", ":", "\n", "    ", "x_mean", "=", "x", ".", "mean", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "x", "=", "(", "x", "-", "x_mean", ")", "*", "(", "torch", ".", "rand", "(", "\n", "x", ".", "size", "(", "0", ")", ",", "1", ",", "1", ",", "1", ",", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "*", "2", ")", "+", "x_mean", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.rand_contrast": [[55, 60], ["x.mean", "torch.rand", "torch.rand", "x.size"], "function", ["None"], ["", "def", "rand_contrast", "(", "x", ")", ":", "\n", "    ", "x_mean", "=", "x", ".", "mean", "(", "dim", "=", "[", "1", ",", "2", ",", "3", "]", ",", "keepdim", "=", "True", ")", "\n", "x", "=", "(", "x", "-", "x_mean", ")", "*", "(", "torch", ".", "rand", "(", "\n", "x", ".", "size", "(", "0", ")", ",", "1", ",", "1", ",", "1", ",", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "+", "0.5", ")", "+", "x_mean", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.rand_translation": [[62, 80], ["torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.meshgrid", "torch.meshgrid", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.pad", "[].permute", "int", "int", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "[].permute.size", "[].permute.size", "[].permute.size", "[].permute.size", "[].permute.size", "[].permute.size", "[].permute.size", "F.pad.permute().contiguous", "[].permute.size", "[].permute.size", "F.pad.permute"], "function", ["None"], ["", "def", "rand_translation", "(", "x", ",", "ratio", "=", "0.125", ")", ":", "\n", "    ", "shift_x", ",", "shift_y", "=", "int", "(", "x", ".", "size", "(", "2", ")", "*", "ratio", "+", "\n", "0.5", ")", ",", "int", "(", "x", ".", "size", "(", "3", ")", "*", "ratio", "+", "0.5", ")", "\n", "translation_x", "=", "torch", ".", "randint", "(", "\n", "-", "shift_x", ",", "shift_x", "+", "1", ",", "size", "=", "[", "x", ".", "size", "(", "0", ")", ",", "1", ",", "1", "]", ",", "device", "=", "x", ".", "device", ")", "\n", "translation_y", "=", "torch", ".", "randint", "(", "\n", "-", "shift_y", ",", "shift_y", "+", "1", ",", "size", "=", "[", "x", ".", "size", "(", "0", ")", ",", "1", ",", "1", "]", ",", "device", "=", "x", ".", "device", ")", "\n", "grid_batch", ",", "grid_x", ",", "grid_y", "=", "torch", ".", "meshgrid", "(", "\n", "torch", ".", "arange", "(", "x", ".", "size", "(", "0", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "x", ".", "device", ")", ",", "\n", "torch", ".", "arange", "(", "x", ".", "size", "(", "2", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "x", ".", "device", ")", ",", "\n", "torch", ".", "arange", "(", "x", ".", "size", "(", "3", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "x", ".", "device", ")", ",", "\n", ")", "\n", "grid_x", "=", "torch", ".", "clamp", "(", "grid_x", "+", "translation_x", "+", "1", ",", "0", ",", "x", ".", "size", "(", "2", ")", "+", "1", ")", "\n", "grid_y", "=", "torch", ".", "clamp", "(", "grid_y", "+", "translation_y", "+", "1", ",", "0", ",", "x", ".", "size", "(", "3", ")", "+", "1", ")", "\n", "x_pad", "=", "F", ".", "pad", "(", "x", ",", "[", "1", ",", "1", ",", "1", ",", "1", ",", "0", ",", "0", ",", "0", ",", "0", "]", ")", "\n", "x", "=", "x_pad", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ".", "contiguous", "(", ")", "[", "grid_batch", ",", "grid_x", ",", "\n", "grid_y", "]", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.vqgan_loss.rand_cutout": [[82, 108], ["torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.meshgrid", "torch.meshgrid", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.ones", "torch.ones", "int", "int", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "x.size", "x.size", "x.size", "torch.ones.unsqueeze", "x.size", "x.size", "x.size", "x.size", "x.size", "x.size", "x.size", "x.size", "x.size"], "function", ["None"], ["", "def", "rand_cutout", "(", "x", ",", "ratio", "=", "0.5", ")", ":", "\n", "    ", "cutout_size", "=", "int", "(", "x", ".", "size", "(", "2", ")", "*", "ratio", "+", "0.5", ")", ",", "int", "(", "x", ".", "size", "(", "3", ")", "*", "ratio", "+", "0.5", ")", "\n", "offset_x", "=", "torch", ".", "randint", "(", "\n", "0", ",", "\n", "x", ".", "size", "(", "2", ")", "+", "(", "1", "-", "cutout_size", "[", "0", "]", "%", "2", ")", ",", "\n", "size", "=", "[", "x", ".", "size", "(", "0", ")", ",", "1", ",", "1", "]", ",", "\n", "device", "=", "x", ".", "device", ")", "\n", "offset_y", "=", "torch", ".", "randint", "(", "\n", "0", ",", "\n", "x", ".", "size", "(", "3", ")", "+", "(", "1", "-", "cutout_size", "[", "1", "]", "%", "2", ")", ",", "\n", "size", "=", "[", "x", ".", "size", "(", "0", ")", ",", "1", ",", "1", "]", ",", "\n", "device", "=", "x", ".", "device", ")", "\n", "grid_batch", ",", "grid_x", ",", "grid_y", "=", "torch", ".", "meshgrid", "(", "\n", "torch", ".", "arange", "(", "x", ".", "size", "(", "0", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "x", ".", "device", ")", ",", "\n", "torch", ".", "arange", "(", "cutout_size", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "x", ".", "device", ")", ",", "\n", "torch", ".", "arange", "(", "cutout_size", "[", "1", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "x", ".", "device", ")", ",", "\n", ")", "\n", "grid_x", "=", "torch", ".", "clamp", "(", "\n", "grid_x", "+", "offset_x", "-", "cutout_size", "[", "0", "]", "//", "2", ",", "min", "=", "0", ",", "max", "=", "x", ".", "size", "(", "2", ")", "-", "1", ")", "\n", "grid_y", "=", "torch", ".", "clamp", "(", "\n", "grid_y", "+", "offset_y", "-", "cutout_size", "[", "1", "]", "//", "2", ",", "min", "=", "0", ",", "max", "=", "x", ".", "size", "(", "3", ")", "-", "1", ")", "\n", "mask", "=", "torch", ".", "ones", "(", "\n", "x", ".", "size", "(", "0", ")", ",", "x", ".", "size", "(", "2", ")", ",", "x", ".", "size", "(", "3", ")", ",", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "\n", "mask", "[", "grid_batch", ",", "grid_x", ",", "grid_y", "]", "=", "0", "\n", "x", "=", "x", "*", "mask", ".", "unsqueeze", "(", "1", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.accuracy.accuracy": [[1, 47], ["isinstance", "isinstance", "max", "pred.topk", "pred_label.transpose.transpose", "pred_label.transpose.eq", "pred.size", "pred.size", "target.size", "pred.size", "target.unsqueeze().expand_as", "correct[].view().float().sum", "res.append", "pred.new_tensor", "pred.size", "correct[].view().float().sum.mul_", "range", "target.unsqueeze", "correct[].view().float", "len", "target.numel", "correct[].view"], "function", ["None"], ["def", "accuracy", "(", "pred", ",", "target", ",", "topk", "=", "1", ",", "thresh", "=", "None", ")", ":", "\n", "    ", "\"\"\"Calculate accuracy according to the prediction and target.\n\n    Args:\n        pred (torch.Tensor): The model prediction, shape (N, num_class, ...)\n        target (torch.Tensor): The target of each prediction, shape (N, , ...)\n        topk (int | tuple[int], optional): If the predictions in ``topk``\n            matches the target, the predictions will be regarded as\n            correct ones. Defaults to 1.\n        thresh (float, optional): If not None, predictions with scores under\n            this threshold are considered incorrect. Default to None.\n\n    Returns:\n        float | tuple[float]: If the input ``topk`` is a single integer,\n            the function will return a single float as accuracy. If\n            ``topk`` is a tuple containing multiple integers, the\n            function will return a tuple containing accuracies of\n            each ``topk`` number.\n    \"\"\"", "\n", "assert", "isinstance", "(", "topk", ",", "(", "int", ",", "tuple", ")", ")", "\n", "if", "isinstance", "(", "topk", ",", "int", ")", ":", "\n", "        ", "topk", "=", "(", "topk", ",", ")", "\n", "return_single", "=", "True", "\n", "", "else", ":", "\n", "        ", "return_single", "=", "False", "\n", "\n", "", "maxk", "=", "max", "(", "topk", ")", "\n", "if", "pred", ".", "size", "(", "0", ")", "==", "0", ":", "\n", "        ", "accu", "=", "[", "pred", ".", "new_tensor", "(", "0.", ")", "for", "i", "in", "range", "(", "len", "(", "topk", ")", ")", "]", "\n", "return", "accu", "[", "0", "]", "if", "return_single", "else", "accu", "\n", "", "assert", "pred", ".", "ndim", "==", "target", ".", "ndim", "+", "1", "\n", "assert", "pred", ".", "size", "(", "0", ")", "==", "target", ".", "size", "(", "0", ")", "\n", "assert", "maxk", "<=", "pred", ".", "size", "(", "1", ")", ",", "f'maxk {maxk} exceeds pred dimension {pred.size(1)}'", "\n", "pred_value", ",", "pred_label", "=", "pred", ".", "topk", "(", "maxk", ",", "dim", "=", "1", ")", "\n", "# transpose to shape (maxk, N, ...)", "\n", "pred_label", "=", "pred_label", ".", "transpose", "(", "0", ",", "1", ")", "\n", "correct", "=", "pred_label", ".", "eq", "(", "target", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "pred_label", ")", ")", "\n", "if", "thresh", "is", "not", "None", ":", "\n", "# Only prediction values larger than thresh are counted as correct", "\n", "        ", "correct", "=", "correct", "&", "(", "pred_value", ">", "thresh", ")", ".", "t", "(", ")", "\n", "", "res", "=", "[", "]", "\n", "for", "k", "in", "topk", ":", "\n", "        ", "correct_k", "=", "correct", "[", ":", "k", "]", ".", "view", "(", "-", "1", ")", ".", "float", "(", ")", ".", "sum", "(", "0", ",", "keepdim", "=", "True", ")", "\n", "res", ".", "append", "(", "correct_k", ".", "mul_", "(", "100.0", "/", "target", ".", "numel", "(", ")", ")", ")", "\n", "", "return", "res", "[", "0", "]", "if", "return_single", "else", "res", "\n", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.cross_entropy_loss.CrossEntropyLoss.__init__": [[202, 222], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__"], ["def", "__init__", "(", "self", ",", "\n", "use_sigmoid", "=", "False", ",", "\n", "use_mask", "=", "False", ",", "\n", "reduction", "=", "'mean'", ",", "\n", "class_weight", "=", "None", ",", "\n", "loss_weight", "=", "1.0", ")", ":", "\n", "        ", "super", "(", "CrossEntropyLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "(", "use_sigmoid", "is", "False", ")", "or", "(", "use_mask", "is", "False", ")", "\n", "self", ".", "use_sigmoid", "=", "use_sigmoid", "\n", "self", ".", "use_mask", "=", "use_mask", "\n", "self", ".", "reduction", "=", "reduction", "\n", "self", ".", "loss_weight", "=", "loss_weight", "\n", "self", ".", "class_weight", "=", "class_weight", "\n", "\n", "if", "self", ".", "use_sigmoid", ":", "\n", "            ", "self", ".", "cls_criterion", "=", "binary_cross_entropy", "\n", "", "elif", "self", ".", "use_mask", ":", "\n", "            ", "self", ".", "cls_criterion", "=", "mask_cross_entropy", "\n", "", "else", ":", "\n", "            ", "self", ".", "cls_criterion", "=", "cross_entropy", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.cross_entropy_loss.CrossEntropyLoss.forward": [[223, 247], ["cls_score.new_tensor", "cross_entropy_loss.CrossEntropyLoss.cls_criterion"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "\n", "cls_score", ",", "\n", "label", ",", "\n", "weight", "=", "None", ",", "\n", "avg_factor", "=", "None", ",", "\n", "reduction_override", "=", "None", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Forward function.\"\"\"", "\n", "assert", "reduction_override", "in", "(", "None", ",", "'none'", ",", "'mean'", ",", "'sum'", ")", "\n", "reduction", "=", "(", "\n", "reduction_override", "if", "reduction_override", "else", "self", ".", "reduction", ")", "\n", "if", "self", ".", "class_weight", "is", "not", "None", ":", "\n", "            ", "class_weight", "=", "cls_score", ".", "new_tensor", "(", "self", ".", "class_weight", ")", "\n", "", "else", ":", "\n", "            ", "class_weight", "=", "None", "\n", "", "loss_cls", "=", "self", ".", "loss_weight", "*", "self", ".", "cls_criterion", "(", "\n", "cls_score", ",", "\n", "label", ",", "\n", "weight", ",", "\n", "class_weight", "=", "class_weight", ",", "\n", "reduction", "=", "reduction", ",", "\n", "avg_factor", "=", "avg_factor", ",", "\n", "**", "kwargs", ")", "\n", "return", "loss_cls", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.cross_entropy_loss.reduce_loss": [[6, 24], ["torch._Reduction.get_enum", "loss.mean", "loss.sum"], "function", ["None"], ["def", "reduce_loss", "(", "loss", ",", "reduction", ")", ":", "\n", "    ", "\"\"\"Reduce loss as specified.\n\n    Args:\n        loss (Tensor): Elementwise loss tensor.\n        reduction (str): Options are \"none\", \"mean\" and \"sum\".\n\n    Return:\n        Tensor: Reduced loss tensor.\n    \"\"\"", "\n", "reduction_enum", "=", "F", ".", "_Reduction", ".", "get_enum", "(", "reduction", ")", "\n", "# none: 0, elementwise_mean:1, sum: 2", "\n", "if", "reduction_enum", "==", "0", ":", "\n", "        ", "return", "loss", "\n", "", "elif", "reduction_enum", "==", "1", ":", "\n", "        ", "return", "loss", ".", "mean", "(", ")", "\n", "", "elif", "reduction_enum", "==", "2", ":", "\n", "        ", "return", "loss", ".", "sum", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.cross_entropy_loss.weight_reduce_loss": [[26, 56], ["cross_entropy_loss.reduce_loss", "weight.dim", "reduce_loss.dim", "weight.dim", "reduce_loss.sum", "ValueError", "weight.size", "weight.size", "reduce_loss.size"], "function", ["home.repos.pwc.inspect_result.yumingj_Text2Human.losses.cross_entropy_loss.reduce_loss"], ["", "", "def", "weight_reduce_loss", "(", "loss", ",", "weight", "=", "None", ",", "reduction", "=", "'mean'", ",", "avg_factor", "=", "None", ")", ":", "\n", "    ", "\"\"\"Apply element-wise weight and reduce loss.\n\n    Args:\n        loss (Tensor): Element-wise loss.\n        weight (Tensor): Element-wise weights.\n        reduction (str): Same as built-in losses of PyTorch.\n        avg_factor (float): Avarage factor when computing the mean of losses.\n\n    Returns:\n        Tensor: Processed loss values.\n    \"\"\"", "\n", "# if weight is specified, apply element-wise weight", "\n", "if", "weight", "is", "not", "None", ":", "\n", "        ", "assert", "weight", ".", "dim", "(", ")", "==", "loss", ".", "dim", "(", ")", "\n", "if", "weight", ".", "dim", "(", ")", ">", "1", ":", "\n", "            ", "assert", "weight", ".", "size", "(", "1", ")", "==", "1", "or", "weight", ".", "size", "(", "1", ")", "==", "loss", ".", "size", "(", "1", ")", "\n", "", "loss", "=", "loss", "*", "weight", "\n", "\n", "# if avg_factor is not specified, just reduce the loss", "\n", "", "if", "avg_factor", "is", "None", ":", "\n", "        ", "loss", "=", "reduce_loss", "(", "loss", ",", "reduction", ")", "\n", "", "else", ":", "\n", "# if reduction is mean, then average the loss by avg_factor", "\n", "        ", "if", "reduction", "==", "'mean'", ":", "\n", "            ", "loss", "=", "loss", ".", "sum", "(", ")", "/", "avg_factor", "\n", "# if reduction is 'none', then do nothing, otherwise raise an error", "\n", "", "elif", "reduction", "!=", "'none'", ":", "\n", "            ", "raise", "ValueError", "(", "'avg_factor can not be used with reduction=\"sum\"'", ")", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.cross_entropy_loss.cross_entropy": [[58, 82], ["torch.cross_entropy", "cross_entropy_loss.weight_reduce_loss", "weight.float.float"], "function", ["home.repos.pwc.inspect_result.yumingj_Text2Human.losses.cross_entropy_loss.cross_entropy", "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.cross_entropy_loss.weight_reduce_loss"], ["", "def", "cross_entropy", "(", "pred", ",", "\n", "label", ",", "\n", "weight", "=", "None", ",", "\n", "class_weight", "=", "None", ",", "\n", "reduction", "=", "'mean'", ",", "\n", "avg_factor", "=", "None", ",", "\n", "ignore_index", "=", "-", "100", ")", ":", "\n", "    ", "\"\"\"The wrapper function for :func:`F.cross_entropy`\"\"\"", "\n", "# class_weight is a manual rescaling weight given to each class.", "\n", "# If given, has to be a Tensor of size C element-wise losses", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "\n", "pred", ",", "\n", "label", ",", "\n", "weight", "=", "class_weight", ",", "\n", "reduction", "=", "'none'", ",", "\n", "ignore_index", "=", "ignore_index", ")", "\n", "\n", "# apply weights and do the reduction", "\n", "if", "weight", "is", "not", "None", ":", "\n", "        ", "weight", "=", "weight", ".", "float", "(", ")", "\n", "", "loss", "=", "weight_reduce_loss", "(", "\n", "loss", ",", "weight", "=", "weight", ",", "reduction", "=", "reduction", ",", "avg_factor", "=", "avg_factor", ")", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.cross_entropy_loss._expand_onehot_labels": [[84, 104], ["labels.new_zeros", "torch.nonzero", "torch.nonzero", "torch.nonzero", "valid_mask.unsqueeze().expand().float.unsqueeze().expand().float", "inds[].numel", "label_weights.unsqueeze().expand", "labels.dim", "valid_mask.unsqueeze().expand().float.unsqueeze().expand", "label_weights.unsqueeze", "valid_mask.unsqueeze().expand().float.unsqueeze"], "function", ["None"], ["", "def", "_expand_onehot_labels", "(", "labels", ",", "label_weights", ",", "target_shape", ",", "ignore_index", ")", ":", "\n", "    ", "\"\"\"Expand onehot labels to match the size of prediction.\"\"\"", "\n", "bin_labels", "=", "labels", ".", "new_zeros", "(", "target_shape", ")", "\n", "valid_mask", "=", "(", "labels", ">=", "0", ")", "&", "(", "labels", "!=", "ignore_index", ")", "\n", "inds", "=", "torch", ".", "nonzero", "(", "valid_mask", ",", "as_tuple", "=", "True", ")", "\n", "\n", "if", "inds", "[", "0", "]", ".", "numel", "(", ")", ">", "0", ":", "\n", "        ", "if", "labels", ".", "dim", "(", ")", "==", "3", ":", "\n", "            ", "bin_labels", "[", "inds", "[", "0", "]", ",", "labels", "[", "valid_mask", "]", ",", "inds", "[", "1", "]", ",", "inds", "[", "2", "]", "]", "=", "1", "\n", "", "else", ":", "\n", "            ", "bin_labels", "[", "inds", "[", "0", "]", ",", "labels", "[", "valid_mask", "]", "]", "=", "1", "\n", "\n", "", "", "valid_mask", "=", "valid_mask", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "target_shape", ")", ".", "float", "(", ")", "\n", "if", "label_weights", "is", "None", ":", "\n", "        ", "bin_label_weights", "=", "valid_mask", "\n", "", "else", ":", "\n", "        ", "bin_label_weights", "=", "label_weights", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "target_shape", ")", "\n", "bin_label_weights", "*=", "valid_mask", "\n", "\n", "", "return", "bin_labels", ",", "bin_label_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.cross_entropy_loss.binary_cross_entropy": [[106, 147], ["torch.binary_cross_entropy_with_logits", "cross_entropy_loss.weight_reduce_loss", "pred.dim", "label.dim", "cross_entropy_loss._expand_onehot_labels", "weight.float.float", "label.float", "pred.dim", "label.dim", "pred.dim", "label.dim"], "function", ["home.repos.pwc.inspect_result.yumingj_Text2Human.losses.cross_entropy_loss.weight_reduce_loss", "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.cross_entropy_loss._expand_onehot_labels"], ["", "def", "binary_cross_entropy", "(", "pred", ",", "\n", "label", ",", "\n", "weight", "=", "None", ",", "\n", "reduction", "=", "'mean'", ",", "\n", "avg_factor", "=", "None", ",", "\n", "class_weight", "=", "None", ",", "\n", "ignore_index", "=", "255", ")", ":", "\n", "    ", "\"\"\"Calculate the binary CrossEntropy loss.\n\n    Args:\n        pred (torch.Tensor): The prediction with shape (N, 1).\n        label (torch.Tensor): The learning label of the prediction.\n        weight (torch.Tensor, optional): Sample-wise loss weight.\n        reduction (str, optional): The method used to reduce the loss.\n            Options are \"none\", \"mean\" and \"sum\".\n        avg_factor (int, optional): Average factor that is used to average\n            the loss. Defaults to None.\n        class_weight (list[float], optional): The weight for each class.\n        ignore_index (int | None): The label index to be ignored. Default: 255\n\n    Returns:\n        torch.Tensor: The calculated loss\n    \"\"\"", "\n", "if", "pred", ".", "dim", "(", ")", "!=", "label", ".", "dim", "(", ")", ":", "\n", "        ", "assert", "(", "pred", ".", "dim", "(", ")", "==", "2", "and", "label", ".", "dim", "(", ")", "==", "1", ")", "or", "(", "\n", "pred", ".", "dim", "(", ")", "==", "4", "and", "label", ".", "dim", "(", ")", "==", "3", ")", ",", "'Only pred shape [N, C], label shape [N] or pred shape [N, C, '", "'H, W], label shape [N, H, W] are supported'", "\n", "label", ",", "weight", "=", "_expand_onehot_labels", "(", "label", ",", "weight", ",", "pred", ".", "shape", ",", "\n", "ignore_index", ")", "\n", "\n", "# weighted element-wise losses", "\n", "", "if", "weight", "is", "not", "None", ":", "\n", "        ", "weight", "=", "weight", ".", "float", "(", ")", "\n", "", "loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "\n", "pred", ",", "label", ".", "float", "(", ")", ",", "pos_weight", "=", "class_weight", ",", "reduction", "=", "'none'", ")", "\n", "# do the reduction for the weighted loss", "\n", "loss", "=", "weight_reduce_loss", "(", "\n", "loss", ",", "weight", ",", "reduction", "=", "reduction", ",", "avg_factor", "=", "avg_factor", ")", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.losses.cross_entropy_loss.mask_cross_entropy": [[149, 185], ["torch.arange", "torch.arange", "torch.arange", "pred[].squeeze", "pred.size", "torch.binary_cross_entropy_with_logits"], "function", ["None"], ["", "def", "mask_cross_entropy", "(", "pred", ",", "\n", "target", ",", "\n", "label", ",", "\n", "reduction", "=", "'mean'", ",", "\n", "avg_factor", "=", "None", ",", "\n", "class_weight", "=", "None", ",", "\n", "ignore_index", "=", "None", ")", ":", "\n", "    ", "\"\"\"Calculate the CrossEntropy loss for masks.\n\n    Args:\n        pred (torch.Tensor): The prediction with shape (N, C), C is the number\n            of classes.\n        target (torch.Tensor): The learning label of the prediction.\n        label (torch.Tensor): ``label`` indicates the class label of the mask'\n            corresponding object. This will be used to select the mask in the\n            of the class which the object belongs to when the mask prediction\n            if not class-agnostic.\n        reduction (str, optional): The method used to reduce the loss.\n            Options are \"none\", \"mean\" and \"sum\".\n        avg_factor (int, optional): Average factor that is used to average\n            the loss. Defaults to None.\n        class_weight (list[float], optional): The weight for each class.\n        ignore_index (None): Placeholder, to be consistent with other loss.\n            Default: None.\n\n    Returns:\n        torch.Tensor: The calculated loss\n    \"\"\"", "\n", "assert", "ignore_index", "is", "None", ",", "'BCE loss does not support ignore_index'", "\n", "# TODO: handle these two reserved arguments", "\n", "assert", "reduction", "==", "'mean'", "and", "avg_factor", "is", "None", "\n", "num_rois", "=", "pred", ".", "size", "(", ")", "[", "0", "]", "\n", "inds", "=", "torch", ".", "arange", "(", "0", ",", "num_rois", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "pred", ".", "device", ")", "\n", "pred_slice", "=", "pred", "[", "inds", ",", "label", "]", ".", "squeeze", "(", "1", ")", "\n", "return", "F", ".", "binary_cross_entropy_with_logits", "(", "\n", "pred_slice", ",", "target", ",", "weight", "=", "class_weight", ",", "reduction", "=", "'mean'", ")", "[", "None", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.parsing_generation_segm_attr_dataset.ParsingGenerationDeepFashionAttrSegmDataset.__init__": [[12, 26], ["os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "open", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "row.split", "parsing_generation_segm_attr_dataset.ParsingGenerationDeepFashionAttrSegmDataset._image_fnames.append", "parsing_generation_segm_attr_dataset.ParsingGenerationDeepFashionAttrSegmDataset.attrs.append", "int"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "segm_dir", ",", "pose_dir", ",", "ann_file", ",", "downsample_factor", "=", "2", ")", ":", "\n", "        ", "self", ".", "_densepose_path", "=", "pose_dir", "\n", "self", ".", "_segm_path", "=", "segm_dir", "\n", "self", ".", "_image_fnames", "=", "[", "]", "\n", "self", ".", "attrs", "=", "[", "]", "\n", "\n", "self", ".", "downsample_factor", "=", "downsample_factor", "\n", "\n", "# training, ground-truth available", "\n", "assert", "os", ".", "path", ".", "exists", "(", "ann_file", ")", "\n", "for", "row", "in", "open", "(", "os", ".", "path", ".", "join", "(", "ann_file", ")", ",", "'r'", ")", ":", "\n", "            ", "annotations", "=", "row", ".", "split", "(", ")", "\n", "self", ".", "_image_fnames", ".", "append", "(", "annotations", "[", "0", "]", ")", "\n", "self", ".", "attrs", ".", "append", "(", "[", "int", "(", "i", ")", "for", "i", "in", "annotations", "[", "1", ":", "]", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.parsing_generation_segm_attr_dataset.ParsingGenerationDeepFashionAttrSegmDataset._open_file": [[27, 29], ["open", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "methods", ["None"], ["", "", "def", "_open_file", "(", "self", ",", "path_prefix", ",", "fname", ")", ":", "\n", "        ", "return", "open", "(", "os", ".", "path", ".", "join", "(", "path_prefix", ",", "fname", ")", ",", "'rb'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.parsing_generation_segm_attr_dataset.ParsingGenerationDeepFashionAttrSegmDataset._load_densepose": [[30, 44], ["densepose.resize.resize.astype", "parsing_generation_segm_attr_dataset.ParsingGenerationDeepFashionAttrSegmDataset._open_file", "PIL.Image.open", "[].transpose", "densepose.resize.resize.resize", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset._open_file"], ["", "def", "_load_densepose", "(", "self", ",", "raw_idx", ")", ":", "\n", "        ", "fname", "=", "self", ".", "_image_fnames", "[", "raw_idx", "]", "\n", "fname", "=", "f'{fname[:-4]}_densepose.png'", "\n", "with", "self", ".", "_open_file", "(", "self", ".", "_densepose_path", ",", "fname", ")", "as", "f", ":", "\n", "            ", "densepose", "=", "Image", ".", "open", "(", "f", ")", "\n", "if", "self", ".", "downsample_factor", "!=", "1", ":", "\n", "                ", "width", ",", "height", "=", "densepose", ".", "size", "\n", "width", "=", "width", "//", "self", ".", "downsample_factor", "\n", "height", "=", "height", "//", "self", ".", "downsample_factor", "\n", "densepose", "=", "densepose", ".", "resize", "(", "\n", "size", "=", "(", "width", ",", "height", ")", ",", "resample", "=", "Image", ".", "NEAREST", ")", "\n", "# channel-wise IUV order, [3, H, W]", "\n", "", "densepose", "=", "np", ".", "array", "(", "densepose", ")", "[", ":", ",", ":", ",", "2", ":", "]", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", "\n", "", "return", "densepose", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.parsing_generation_segm_attr_dataset.ParsingGenerationDeepFashionAttrSegmDataset._load_segm": [[45, 58], ["segm.resize.resize.astype", "parsing_generation_segm_attr_dataset.ParsingGenerationDeepFashionAttrSegmDataset._open_file", "PIL.Image.open", "numpy.array", "segm.resize.resize.resize"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset._open_file"], ["", "def", "_load_segm", "(", "self", ",", "raw_idx", ")", ":", "\n", "        ", "fname", "=", "self", ".", "_image_fnames", "[", "raw_idx", "]", "\n", "fname", "=", "f'{fname[:-4]}_segm.png'", "\n", "with", "self", ".", "_open_file", "(", "self", ".", "_segm_path", ",", "fname", ")", "as", "f", ":", "\n", "            ", "segm", "=", "Image", ".", "open", "(", "f", ")", "\n", "if", "self", ".", "downsample_factor", "!=", "1", ":", "\n", "                ", "width", ",", "height", "=", "segm", ".", "size", "\n", "width", "=", "width", "//", "self", ".", "downsample_factor", "\n", "height", "=", "height", "//", "self", ".", "downsample_factor", "\n", "segm", "=", "segm", ".", "resize", "(", "\n", "size", "=", "(", "width", ",", "height", ")", ",", "resample", "=", "Image", ".", "NEAREST", ")", "\n", "", "segm", "=", "np", ".", "array", "(", "segm", ")", "\n", "", "return", "segm", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.parsing_generation_segm_attr_dataset.ParsingGenerationDeepFashionAttrSegmDataset.__getitem__": [[59, 78], ["parsing_generation_segm_attr_dataset.ParsingGenerationDeepFashionAttrSegmDataset._load_densepose", "parsing_generation_segm_attr_dataset.ParsingGenerationDeepFashionAttrSegmDataset._load_segm", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.segm_attr_dataset.DeepFashionAttrSegmDataset._load_densepose", "home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset._load_segm"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "pose", "=", "self", ".", "_load_densepose", "(", "index", ")", "\n", "segm", "=", "self", ".", "_load_segm", "(", "index", ")", "\n", "attr", "=", "self", ".", "attrs", "[", "index", "]", "\n", "\n", "pose", "=", "torch", ".", "from_numpy", "(", "pose", ")", "\n", "segm", "=", "torch", ".", "LongTensor", "(", "segm", ")", "\n", "attr", "=", "torch", ".", "LongTensor", "(", "attr", ")", "\n", "\n", "pose", "=", "pose", "/", "12.", "-", "1", "\n", "\n", "return_dict", "=", "{", "\n", "'densepose'", ":", "pose", ",", "\n", "'segm'", ":", "segm", ",", "\n", "'attr'", ":", "attr", ",", "\n", "'img_name'", ":", "self", ".", "_image_fnames", "[", "index", "]", "\n", "}", "\n", "\n", "return", "return_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.parsing_generation_segm_attr_dataset.ParsingGenerationDeepFashionAttrSegmDataset.__len__": [[79, 81], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_image_fnames", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.pose_attr_dataset.DeepFashionAttrPoseDataset.__init__": [[13, 64], ["os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "enumerate", "os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "enumerate", "os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "enumerate", "os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "enumerate", "open", "row.split", "pose_attr_dataset.DeepFashionAttrPoseDataset._image_fnames_target.append", "pose_attr_dataset.DeepFashionAttrPoseDataset._image_fnames.append", "pose_attr_dataset.DeepFashionAttrPoseDataset.upper_fused_attrs.append", "len", "len", "open", "row.split", "pose_attr_dataset.DeepFashionAttrPoseDataset.lower_fused_attrs.append", "len", "len", "open", "row.split", "pose_attr_dataset.DeepFashionAttrPoseDataset.outer_fused_attrs.append", "len", "len", "open", "row.split", "pose_attr_dataset.DeepFashionAttrPoseDataset.shape_attrs.append", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "int", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "int", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "int", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "int", "annotations[].split"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "\n", "pose_dir", ",", "\n", "texture_ann_dir", ",", "\n", "shape_ann_path", ",", "\n", "downsample_factor", "=", "2", ",", "\n", "xflip", "=", "False", ")", ":", "\n", "        ", "self", ".", "_densepose_path", "=", "pose_dir", "\n", "self", ".", "_image_fnames_target", "=", "[", "]", "\n", "self", ".", "_image_fnames", "=", "[", "]", "\n", "self", ".", "upper_fused_attrs", "=", "[", "]", "\n", "self", ".", "lower_fused_attrs", "=", "[", "]", "\n", "self", ".", "outer_fused_attrs", "=", "[", "]", "\n", "self", ".", "shape_attrs", "=", "[", "]", "\n", "\n", "self", ".", "downsample_factor", "=", "downsample_factor", "\n", "self", ".", "xflip", "=", "xflip", "\n", "\n", "# load attributes", "\n", "assert", "os", ".", "path", ".", "exists", "(", "f'{texture_ann_dir}/upper_fused.txt'", ")", "\n", "for", "idx", ",", "row", "in", "enumerate", "(", "\n", "open", "(", "os", ".", "path", ".", "join", "(", "f'{texture_ann_dir}/upper_fused.txt'", ")", ",", "'r'", ")", ")", ":", "\n", "            ", "annotations", "=", "row", ".", "split", "(", ")", "\n", "self", ".", "_image_fnames_target", ".", "append", "(", "annotations", "[", "0", "]", ")", "\n", "self", ".", "_image_fnames", ".", "append", "(", "f'{annotations[0].split(\".\")[0]}.png'", ")", "\n", "self", ".", "upper_fused_attrs", ".", "append", "(", "int", "(", "annotations", "[", "1", "]", ")", ")", "\n", "\n", "", "assert", "len", "(", "self", ".", "_image_fnames_target", ")", "==", "len", "(", "self", ".", "upper_fused_attrs", ")", "\n", "\n", "assert", "os", ".", "path", ".", "exists", "(", "f'{texture_ann_dir}/lower_fused.txt'", ")", "\n", "for", "idx", ",", "row", "in", "enumerate", "(", "\n", "open", "(", "os", ".", "path", ".", "join", "(", "f'{texture_ann_dir}/lower_fused.txt'", ")", ",", "'r'", ")", ")", ":", "\n", "            ", "annotations", "=", "row", ".", "split", "(", ")", "\n", "assert", "self", ".", "_image_fnames_target", "[", "idx", "]", "==", "annotations", "[", "0", "]", "\n", "self", ".", "lower_fused_attrs", ".", "append", "(", "int", "(", "annotations", "[", "1", "]", ")", ")", "\n", "\n", "", "assert", "len", "(", "self", ".", "_image_fnames_target", ")", "==", "len", "(", "self", ".", "lower_fused_attrs", ")", "\n", "\n", "assert", "os", ".", "path", ".", "exists", "(", "f'{texture_ann_dir}/outer_fused.txt'", ")", "\n", "for", "idx", ",", "row", "in", "enumerate", "(", "\n", "open", "(", "os", ".", "path", ".", "join", "(", "f'{texture_ann_dir}/outer_fused.txt'", ")", ",", "'r'", ")", ")", ":", "\n", "            ", "annotations", "=", "row", ".", "split", "(", ")", "\n", "assert", "self", ".", "_image_fnames_target", "[", "idx", "]", "==", "annotations", "[", "0", "]", "\n", "self", ".", "outer_fused_attrs", ".", "append", "(", "int", "(", "annotations", "[", "1", "]", ")", ")", "\n", "\n", "", "assert", "len", "(", "self", ".", "_image_fnames_target", ")", "==", "len", "(", "self", ".", "outer_fused_attrs", ")", "\n", "\n", "assert", "os", ".", "path", ".", "exists", "(", "shape_ann_path", ")", "\n", "for", "idx", ",", "row", "in", "enumerate", "(", "open", "(", "os", ".", "path", ".", "join", "(", "shape_ann_path", ")", ",", "'r'", ")", ")", ":", "\n", "            ", "annotations", "=", "row", ".", "split", "(", ")", "\n", "assert", "self", ".", "_image_fnames_target", "[", "idx", "]", "==", "annotations", "[", "0", "]", "\n", "self", ".", "shape_attrs", ".", "append", "(", "[", "int", "(", "i", ")", "for", "i", "in", "annotations", "[", "1", ":", "]", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.pose_attr_dataset.DeepFashionAttrPoseDataset._open_file": [[65, 67], ["open", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "methods", ["None"], ["", "", "def", "_open_file", "(", "self", ",", "path_prefix", ",", "fname", ")", ":", "\n", "        ", "return", "open", "(", "os", ".", "path", ".", "join", "(", "path_prefix", ",", "fname", ")", ",", "'rb'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.pose_attr_dataset.DeepFashionAttrPoseDataset._load_densepose": [[68, 82], ["densepose.resize.resize.astype", "pose_attr_dataset.DeepFashionAttrPoseDataset._open_file", "PIL.Image.open", "[].transpose", "densepose.resize.resize.resize", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset._open_file"], ["", "def", "_load_densepose", "(", "self", ",", "raw_idx", ")", ":", "\n", "        ", "fname", "=", "self", ".", "_image_fnames", "[", "raw_idx", "]", "\n", "fname", "=", "f'{fname[:-4]}_densepose.png'", "\n", "with", "self", ".", "_open_file", "(", "self", ".", "_densepose_path", ",", "fname", ")", "as", "f", ":", "\n", "            ", "densepose", "=", "Image", ".", "open", "(", "f", ")", "\n", "if", "self", ".", "downsample_factor", "!=", "1", ":", "\n", "                ", "width", ",", "height", "=", "densepose", ".", "size", "\n", "width", "=", "width", "//", "self", ".", "downsample_factor", "\n", "height", "=", "height", "//", "self", ".", "downsample_factor", "\n", "densepose", "=", "densepose", ".", "resize", "(", "\n", "size", "=", "(", "width", ",", "height", ")", ",", "resample", "=", "Image", ".", "NEAREST", ")", "\n", "# channel-wise IUV order, [3, H, W]", "\n", "", "densepose", "=", "np", ".", "array", "(", "densepose", ")", "[", ":", ",", ":", ",", "2", ":", "]", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", "\n", "", "return", "densepose", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.pose_attr_dataset.DeepFashionAttrPoseDataset.__getitem__": [[83, 107], ["pose_attr_dataset.DeepFashionAttrPoseDataset._load_densepose", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "pose[].copy", "random.random"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.segm_attr_dataset.DeepFashionAttrSegmDataset._load_densepose"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "pose", "=", "self", ".", "_load_densepose", "(", "index", ")", "\n", "shape_attr", "=", "self", ".", "shape_attrs", "[", "index", "]", "\n", "shape_attr", "=", "torch", ".", "LongTensor", "(", "shape_attr", ")", "\n", "\n", "if", "self", ".", "xflip", "and", "random", ".", "random", "(", ")", ">", "0.5", ":", "\n", "            ", "pose", "=", "pose", "[", ":", ",", ":", ",", ":", ":", "-", "1", "]", ".", "copy", "(", ")", "\n", "\n", "", "upper_fused_attr", "=", "self", ".", "upper_fused_attrs", "[", "index", "]", "\n", "lower_fused_attr", "=", "self", ".", "lower_fused_attrs", "[", "index", "]", "\n", "outer_fused_attr", "=", "self", ".", "outer_fused_attrs", "[", "index", "]", "\n", "\n", "pose", "=", "pose", "/", "12.", "-", "1", "\n", "\n", "return_dict", "=", "{", "\n", "'densepose'", ":", "pose", ",", "\n", "'img_name'", ":", "self", ".", "_image_fnames_target", "[", "index", "]", ",", "\n", "'shape_attr'", ":", "shape_attr", ",", "\n", "'upper_fused_attr'", ":", "upper_fused_attr", ",", "\n", "'lower_fused_attr'", ":", "lower_fused_attr", ",", "\n", "'outer_fused_attr'", ":", "outer_fused_attr", ",", "\n", "}", "\n", "\n", "return", "return_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.pose_attr_dataset.DeepFashionAttrPoseDataset.__len__": [[108, 110], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_image_fnames", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.segm_attr_dataset.DeepFashionAttrSegmDataset.__init__": [[13, 69], ["os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "enumerate", "os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "enumerate", "os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "enumerate", "open", "row.split", "segm_attr_dataset.DeepFashionAttrSegmDataset._image_fnames.append", "segm_attr_dataset.DeepFashionAttrSegmDataset.upper_fused_attrs.append", "len", "len", "open", "row.split", "segm_attr_dataset.DeepFashionAttrSegmDataset.lower_fused_attrs.append", "len", "len", "open", "row.split", "segm_attr_dataset.DeepFashionAttrSegmDataset.outer_fused_attrs.append", "len", "len", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "int", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "int", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "int"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "\n", "img_dir", ",", "\n", "segm_dir", ",", "\n", "pose_dir", ",", "\n", "ann_dir", ",", "\n", "downsample_factor", "=", "2", ",", "\n", "xflip", "=", "False", ")", ":", "\n", "        ", "self", ".", "_img_path", "=", "img_dir", "\n", "self", ".", "_densepose_path", "=", "pose_dir", "\n", "self", ".", "_segm_path", "=", "segm_dir", "\n", "self", ".", "_image_fnames", "=", "[", "]", "\n", "self", ".", "upper_fused_attrs", "=", "[", "]", "\n", "self", ".", "lower_fused_attrs", "=", "[", "]", "\n", "self", ".", "outer_fused_attrs", "=", "[", "]", "\n", "\n", "self", ".", "downsample_factor", "=", "downsample_factor", "\n", "self", ".", "xflip", "=", "xflip", "\n", "\n", "# load attributes", "\n", "assert", "os", ".", "path", ".", "exists", "(", "f'{ann_dir}/upper_fused.txt'", ")", "\n", "for", "idx", ",", "row", "in", "enumerate", "(", "\n", "open", "(", "os", ".", "path", ".", "join", "(", "f'{ann_dir}/upper_fused.txt'", ")", ",", "'r'", ")", ")", ":", "\n", "            ", "annotations", "=", "row", ".", "split", "(", ")", "\n", "self", ".", "_image_fnames", ".", "append", "(", "annotations", "[", "0", "]", ")", "\n", "# assert self._image_fnames[idx] == annotations[0]", "\n", "self", ".", "upper_fused_attrs", ".", "append", "(", "int", "(", "annotations", "[", "1", "]", ")", ")", "\n", "\n", "", "assert", "len", "(", "self", ".", "_image_fnames", ")", "==", "len", "(", "self", ".", "upper_fused_attrs", ")", "\n", "\n", "assert", "os", ".", "path", ".", "exists", "(", "f'{ann_dir}/lower_fused.txt'", ")", "\n", "for", "idx", ",", "row", "in", "enumerate", "(", "\n", "open", "(", "os", ".", "path", ".", "join", "(", "f'{ann_dir}/lower_fused.txt'", ")", ",", "'r'", ")", ")", ":", "\n", "            ", "annotations", "=", "row", ".", "split", "(", ")", "\n", "assert", "self", ".", "_image_fnames", "[", "idx", "]", "==", "annotations", "[", "0", "]", "\n", "self", ".", "lower_fused_attrs", ".", "append", "(", "int", "(", "annotations", "[", "1", "]", ")", ")", "\n", "\n", "", "assert", "len", "(", "self", ".", "_image_fnames", ")", "==", "len", "(", "self", ".", "lower_fused_attrs", ")", "\n", "\n", "assert", "os", ".", "path", ".", "exists", "(", "f'{ann_dir}/outer_fused.txt'", ")", "\n", "for", "idx", ",", "row", "in", "enumerate", "(", "\n", "open", "(", "os", ".", "path", ".", "join", "(", "f'{ann_dir}/outer_fused.txt'", ")", ",", "'r'", ")", ")", ":", "\n", "            ", "annotations", "=", "row", ".", "split", "(", ")", "\n", "assert", "self", ".", "_image_fnames", "[", "idx", "]", "==", "annotations", "[", "0", "]", "\n", "self", ".", "outer_fused_attrs", ".", "append", "(", "int", "(", "annotations", "[", "1", "]", ")", ")", "\n", "\n", "", "assert", "len", "(", "self", ".", "_image_fnames", ")", "==", "len", "(", "self", ".", "outer_fused_attrs", ")", "\n", "\n", "# remove the overlapping item between upper cls and lower cls", "\n", "# cls 21 can appear with upper clothes", "\n", "# cls 4 can appear with lower clothes", "\n", "self", ".", "upper_cls", "=", "[", "1.", ",", "4.", "]", "\n", "self", ".", "lower_cls", "=", "[", "3.", ",", "5.", ",", "21.", "]", "\n", "self", ".", "outer_cls", "=", "[", "2.", "]", "\n", "self", ".", "other_cls", "=", "[", "\n", "11.", ",", "18.", ",", "7.", ",", "8.", ",", "9.", ",", "10.", ",", "12.", ",", "16.", ",", "17.", ",", "19.", ",", "20.", ",", "22.", ",", "23.", ",", "15.", ",", "\n", "14.", ",", "13.", ",", "0.", ",", "6.", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.segm_attr_dataset.DeepFashionAttrSegmDataset._open_file": [[71, 73], ["open", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "methods", ["None"], ["", "def", "_open_file", "(", "self", ",", "path_prefix", ",", "fname", ")", ":", "\n", "        ", "return", "open", "(", "os", ".", "path", ".", "join", "(", "path_prefix", ",", "fname", ")", ",", "'rb'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.segm_attr_dataset.DeepFashionAttrSegmDataset._load_raw_image": [[74, 89], ["image.resize.resize.transpose", "segm_attr_dataset.DeepFashionAttrSegmDataset._open_file", "PIL.Image.open", "numpy.array", "image.resize.resize.resize"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset._open_file"], ["", "def", "_load_raw_image", "(", "self", ",", "raw_idx", ")", ":", "\n", "        ", "fname", "=", "self", ".", "_image_fnames", "[", "raw_idx", "]", "\n", "with", "self", ".", "_open_file", "(", "self", ".", "_img_path", ",", "fname", ")", "as", "f", ":", "\n", "            ", "image", "=", "Image", ".", "open", "(", "f", ")", "\n", "if", "self", ".", "downsample_factor", "!=", "1", ":", "\n", "                ", "width", ",", "height", "=", "image", ".", "size", "\n", "width", "=", "width", "//", "self", ".", "downsample_factor", "\n", "height", "=", "height", "//", "self", ".", "downsample_factor", "\n", "image", "=", "image", ".", "resize", "(", "\n", "size", "=", "(", "width", ",", "height", ")", ",", "resample", "=", "Image", ".", "LANCZOS", ")", "\n", "", "image", "=", "np", ".", "array", "(", "image", ")", "\n", "", "if", "image", ".", "ndim", "==", "2", ":", "\n", "            ", "image", "=", "image", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", "# HW => HWC", "\n", "", "image", "=", "image", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", "# HWC => CHW", "\n", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.segm_attr_dataset.DeepFashionAttrSegmDataset._load_densepose": [[90, 104], ["densepose.resize.resize.astype", "segm_attr_dataset.DeepFashionAttrSegmDataset._open_file", "PIL.Image.open", "[].transpose", "densepose.resize.resize.resize", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset._open_file"], ["", "def", "_load_densepose", "(", "self", ",", "raw_idx", ")", ":", "\n", "        ", "fname", "=", "self", ".", "_image_fnames", "[", "raw_idx", "]", "\n", "fname", "=", "f'{fname[:-4]}_densepose.png'", "\n", "with", "self", ".", "_open_file", "(", "self", ".", "_densepose_path", ",", "fname", ")", "as", "f", ":", "\n", "            ", "densepose", "=", "Image", ".", "open", "(", "f", ")", "\n", "if", "self", ".", "downsample_factor", "!=", "1", ":", "\n", "                ", "width", ",", "height", "=", "densepose", ".", "size", "\n", "width", "=", "width", "//", "self", ".", "downsample_factor", "\n", "height", "=", "height", "//", "self", ".", "downsample_factor", "\n", "densepose", "=", "densepose", ".", "resize", "(", "\n", "size", "=", "(", "width", ",", "height", ")", ",", "resample", "=", "Image", ".", "NEAREST", ")", "\n", "# channel-wise IUV order, [3, H, W]", "\n", "", "densepose", "=", "np", ".", "array", "(", "densepose", ")", "[", ":", ",", ":", ",", "2", ":", "]", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", "\n", "", "return", "densepose", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.segm_attr_dataset.DeepFashionAttrSegmDataset._load_segm": [[105, 119], ["segm[].transpose", "segm.resize.resize.astype", "segm_attr_dataset.DeepFashionAttrSegmDataset._open_file", "PIL.Image.open", "numpy.array", "segm.resize.resize.resize"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset._open_file"], ["", "def", "_load_segm", "(", "self", ",", "raw_idx", ")", ":", "\n", "        ", "fname", "=", "self", ".", "_image_fnames", "[", "raw_idx", "]", "\n", "fname", "=", "f'{fname[:-4]}_segm.png'", "\n", "with", "self", ".", "_open_file", "(", "self", ".", "_segm_path", ",", "fname", ")", "as", "f", ":", "\n", "            ", "segm", "=", "Image", ".", "open", "(", "f", ")", "\n", "if", "self", ".", "downsample_factor", "!=", "1", ":", "\n", "                ", "width", ",", "height", "=", "segm", ".", "size", "\n", "width", "=", "width", "//", "self", ".", "downsample_factor", "\n", "height", "=", "height", "//", "self", ".", "downsample_factor", "\n", "segm", "=", "segm", ".", "resize", "(", "\n", "size", "=", "(", "width", ",", "height", ")", ",", "resample", "=", "Image", ".", "NEAREST", ")", "\n", "", "segm", "=", "np", ".", "array", "(", "segm", ")", "\n", "", "segm", "=", "segm", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", "\n", "return", "segm", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.segm_attr_dataset.DeepFashionAttrSegmDataset.__getitem__": [[120, 165], ["segm_attr_dataset.DeepFashionAttrSegmDataset._load_raw_image", "segm_attr_dataset.DeepFashionAttrSegmDataset._load_densepose", "segm_attr_dataset.DeepFashionAttrSegmDataset._load_segm", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "image[].copy", "pose[].copy", "segm[].copy", "random.random"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.segm_attr_dataset.DeepFashionAttrSegmDataset._load_raw_image", "home.repos.pwc.inspect_result.yumingj_Text2Human.data.segm_attr_dataset.DeepFashionAttrSegmDataset._load_densepose", "home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset._load_segm"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "image", "=", "self", ".", "_load_raw_image", "(", "index", ")", "\n", "pose", "=", "self", ".", "_load_densepose", "(", "index", ")", "\n", "segm", "=", "self", ".", "_load_segm", "(", "index", ")", "\n", "\n", "if", "self", ".", "xflip", "and", "random", ".", "random", "(", ")", ">", "0.5", ":", "\n", "            ", "assert", "image", ".", "ndim", "==", "3", "# CHW", "\n", "image", "=", "image", "[", ":", ",", ":", ",", ":", ":", "-", "1", "]", ".", "copy", "(", ")", "\n", "pose", "=", "pose", "[", ":", ",", ":", ",", ":", ":", "-", "1", "]", ".", "copy", "(", ")", "\n", "segm", "=", "segm", "[", ":", ",", ":", ",", ":", ":", "-", "1", "]", ".", "copy", "(", ")", "\n", "\n", "", "image", "=", "torch", ".", "from_numpy", "(", "image", ")", "\n", "segm", "=", "torch", ".", "from_numpy", "(", "segm", ")", "\n", "\n", "upper_fused_attr", "=", "self", ".", "upper_fused_attrs", "[", "index", "]", "\n", "lower_fused_attr", "=", "self", ".", "lower_fused_attrs", "[", "index", "]", "\n", "outer_fused_attr", "=", "self", ".", "outer_fused_attrs", "[", "index", "]", "\n", "\n", "# mask 0: denotes the common codebook,", "\n", "# mask (attr + 1): denotes the texture-specific codebook", "\n", "mask", "=", "torch", ".", "zeros_like", "(", "segm", ")", "\n", "if", "upper_fused_attr", "!=", "17", ":", "\n", "            ", "for", "cls", "in", "self", ".", "upper_cls", ":", "\n", "                ", "mask", "[", "segm", "==", "cls", "]", "=", "upper_fused_attr", "+", "1", "\n", "\n", "", "", "if", "lower_fused_attr", "!=", "17", ":", "\n", "            ", "for", "cls", "in", "self", ".", "lower_cls", ":", "\n", "                ", "mask", "[", "segm", "==", "cls", "]", "=", "lower_fused_attr", "+", "1", "\n", "\n", "", "", "if", "outer_fused_attr", "!=", "17", ":", "\n", "            ", "for", "cls", "in", "self", ".", "outer_cls", ":", "\n", "                ", "mask", "[", "segm", "==", "cls", "]", "=", "outer_fused_attr", "+", "1", "\n", "\n", "", "", "pose", "=", "pose", "/", "12.", "-", "1", "\n", "image", "=", "image", "/", "127.5", "-", "1", "\n", "\n", "return_dict", "=", "{", "\n", "'image'", ":", "image", ",", "\n", "'densepose'", ":", "pose", ",", "\n", "'segm'", ":", "segm", ",", "\n", "'texture_mask'", ":", "mask", ",", "\n", "'img_name'", ":", "self", ".", "_image_fnames", "[", "index", "]", "\n", "}", "\n", "\n", "return", "return_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.segm_attr_dataset.DeepFashionAttrSegmDataset.__len__": [[166, 168], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_image_fnames", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__init__": [[13, 27], ["os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "enumerate", "open", "row.split", "mask_dataset.MaskDataset._image_fnames.append", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "segm_dir", ",", "ann_dir", ",", "downsample_factor", "=", "2", ",", "xflip", "=", "False", ")", ":", "\n", "\n", "        ", "self", ".", "_segm_path", "=", "segm_dir", "\n", "self", ".", "_image_fnames", "=", "[", "]", "\n", "\n", "self", ".", "downsample_factor", "=", "downsample_factor", "\n", "self", ".", "xflip", "=", "xflip", "\n", "\n", "# load attributes", "\n", "assert", "os", ".", "path", ".", "exists", "(", "f'{ann_dir}/upper_fused.txt'", ")", "\n", "for", "idx", ",", "row", "in", "enumerate", "(", "\n", "open", "(", "os", ".", "path", ".", "join", "(", "f'{ann_dir}/upper_fused.txt'", ")", ",", "'r'", ")", ")", ":", "\n", "            ", "annotations", "=", "row", ".", "split", "(", ")", "\n", "self", ".", "_image_fnames", ".", "append", "(", "annotations", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset._open_file": [[28, 30], ["open", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "methods", ["None"], ["", "", "def", "_open_file", "(", "self", ",", "path_prefix", ",", "fname", ")", ":", "\n", "        ", "return", "open", "(", "os", ".", "path", ".", "join", "(", "path_prefix", ",", "fname", ")", ",", "'rb'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset._load_segm": [[31, 45], ["segm.resize.resize.astype", "mask_dataset.MaskDataset._open_file", "PIL.Image.open", "numpy.array", "segm.resize.resize.resize"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset._open_file"], ["", "def", "_load_segm", "(", "self", ",", "raw_idx", ")", ":", "\n", "        ", "fname", "=", "self", ".", "_image_fnames", "[", "raw_idx", "]", "\n", "fname", "=", "f'{fname[:-4]}_segm.png'", "\n", "with", "self", ".", "_open_file", "(", "self", ".", "_segm_path", ",", "fname", ")", "as", "f", ":", "\n", "            ", "segm", "=", "Image", ".", "open", "(", "f", ")", "\n", "if", "self", ".", "downsample_factor", "!=", "1", ":", "\n", "                ", "width", ",", "height", "=", "segm", ".", "size", "\n", "width", "=", "width", "//", "self", ".", "downsample_factor", "\n", "height", "=", "height", "//", "self", ".", "downsample_factor", "\n", "segm", "=", "segm", ".", "resize", "(", "\n", "size", "=", "(", "width", ",", "height", ")", ",", "resample", "=", "Image", ".", "NEAREST", ")", "\n", "", "segm", "=", "np", ".", "array", "(", "segm", ")", "\n", "# segm = segm[:, :, np.newaxis].transpose(2, 0, 1)", "\n", "", "return", "segm", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__getitem__": [[46, 57], ["mask_dataset.MaskDataset._load_segm", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "segm[].copy", "random.random", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset._load_segm"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "segm", "=", "self", ".", "_load_segm", "(", "index", ")", "\n", "\n", "if", "self", ".", "xflip", "and", "random", ".", "random", "(", ")", ">", "0.5", ":", "\n", "            ", "segm", "=", "segm", "[", ":", ",", ":", ":", "-", "1", "]", ".", "copy", "(", ")", "\n", "\n", "", "segm", "=", "torch", ".", "from_numpy", "(", "segm", ")", ".", "long", "(", ")", "\n", "\n", "return_dict", "=", "{", "'segm'", ":", "segm", ",", "'img_name'", ":", "self", ".", "_image_fnames", "[", "index", "]", "}", "\n", "\n", "return", "return_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.yumingj_Text2Human.data.mask_dataset.MaskDataset.__len__": [[58, 60], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_image_fnames", ")", "\n", "", "", ""]]}