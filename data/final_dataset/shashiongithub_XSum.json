{"home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-Topic-ConvS2S.generate.main": [[17, 159], ["print", "print", "fairseq.utils.load_ensemble_for_inference", "print", "print", "print", "fairseq.utils.load_align_dict", "min", "data.load_raw_text_dataset.eval_dataloader", "fairseq.meters.StopwatchMeter", "fairseq.bleu.Scorer", "print", "torch.cuda.is_available", "fairseq.data.load_dataset", "fairseq.data.load_raw_text_dataset", "model.make_generation_fast_", "fairseq.data.sharded_iterator", "fairseq.sequence_scorer.SequenceScorer", "fairseq.sequence_generator.SequenceGenerator", "fairseq.sequence_generator.SequenceGenerator.cuda", "data.load_raw_text_dataset.dst_dict.pad", "data.load_raw_text_dataset.dst_dict.eos", "data.load_raw_text_dataset.dst_dict.unk", "fairseq.progress_bar.build_progress_bar", "fairseq.meters.TimeMeter", "print", "len", "len", "len", "model.max_encoder_positions", "ValueError", "fairseq.sequence_generator.SequenceGenerator.score_batched_itr", "fairseq.sequence_generator.SequenceGenerator.generate_batched_itr", "enumerate", "fairseq.meters.TimeMeter.update", "t.log", "tokenizer.Tokenizer.tokenize.int().cpu", "data.load_raw_text_dataset.splits[].src.get_original_text", "data.load_raw_text_dataset.splits[].dst.get_original_text", "data.load_raw_text_dataset.src_dict.string", "print", "fairseq.utils.post_process_prediction", "src_tokens.size", "bleu.Scorer.result_string", "data.load_raw_text_dataset.dst_dict.string", "print", "print", "print", "print", "bleu.Scorer.add", "round", "tokenizer.Tokenizer.tokenize.int", "min", "hypo[].int().cpu", "hypo[].int().cpu", "fairseq.tokenizer.Tokenizer.tokenize", "len", "hypo[].int", "hypo[].int", "map", "map", "hypo[].tolist", "str", "fairseq.utils.item"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.load_ensemble_for_inference", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.load_align_dict", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.LanguageDatasets.eval_dataloader", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.load_dataset", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.load_raw_text_dataset", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.make_generation_fast_", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator.cuda", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.max_encoder_positions", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_scorer.SequenceScorer.score_batched_itr", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator.generate_batched_itr", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.log", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDatasetDOCTOPICS.get_original_text", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDatasetDOCTOPICS.get_original_text", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.string", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.post_process_prediction", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.result_string", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.string", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.add", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.tokenizer.Tokenizer.tokenize", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.item"], ["def", "main", "(", "args", ")", ":", "\n", "    ", "print", "(", "args", ")", "\n", "\n", "use_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "cpu", "\n", "\n", "# Load dataset", "\n", "if", "args", ".", "replace_unk", "is", "None", ":", "\n", "        ", "dataset", "=", "data", ".", "load_dataset", "(", "\n", "args", ".", "data", ",", "\n", "[", "args", ".", "gen_subset", "]", ",", "\n", "args", ".", "source_lang", ",", "\n", "args", ".", "target_lang", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "dataset", "=", "data", ".", "load_raw_text_dataset", "(", "\n", "args", ".", "data", ",", "\n", "[", "args", ".", "gen_subset", "]", ",", "\n", "args", ".", "source_lang", ",", "\n", "args", ".", "target_lang", ",", "\n", "args", ".", "doctopics", ",", "args", ".", "encoder_embed_dim", ",", "\n", ")", "\n", "", "if", "args", ".", "source_lang", "is", "None", "or", "args", ".", "target_lang", "is", "None", ":", "\n", "# record inferred languages in args", "\n", "        ", "args", ".", "source_lang", ",", "args", ".", "target_lang", "=", "dataset", ".", "src", ",", "dataset", ".", "dst", "\n", "\n", "# Load ensemble", "\n", "", "print", "(", "'| loading model(s) from {}'", ".", "format", "(", "', '", ".", "join", "(", "args", ".", "path", ")", ")", ")", "\n", "models", ",", "_", "=", "utils", ".", "load_ensemble_for_inference", "(", "args", ".", "path", ",", "dataset", ".", "src_dict", ",", "dataset", ".", "dst_dict", ")", "\n", "\n", "print", "(", "'| [{}] dictionary: {} types'", ".", "format", "(", "dataset", ".", "src", ",", "len", "(", "dataset", ".", "src_dict", ")", ")", ")", "\n", "print", "(", "'| [{}] dictionary: {} types'", ".", "format", "(", "dataset", ".", "dst", ",", "len", "(", "dataset", ".", "dst_dict", ")", ")", ")", "\n", "print", "(", "'| {} {} {} examples'", ".", "format", "(", "args", ".", "data", ",", "args", ".", "gen_subset", ",", "len", "(", "dataset", ".", "splits", "[", "args", ".", "gen_subset", "]", ")", ")", ")", "\n", "\n", "# Optimize ensemble for generation", "\n", "for", "model", "in", "models", ":", "\n", "        ", "model", ".", "make_generation_fast_", "(", "\n", "beamable_mm_beam_size", "=", "None", "if", "args", ".", "no_beamable_mm", "else", "args", ".", "beam", ",", "\n", ")", "\n", "\n", "# Load alignment dictionary for unknown word replacement", "\n", "# (None if no unknown word replacement, empty if no path to align dictionary)", "\n", "", "align_dict", "=", "utils", ".", "load_align_dict", "(", "args", ".", "replace_unk", ")", "\n", "\n", "# Load dataset (possibly sharded)", "\n", "max_positions", "=", "min", "(", "model", ".", "max_encoder_positions", "(", ")", "for", "model", "in", "models", ")", "\n", "itr", "=", "dataset", ".", "eval_dataloader", "(", "\n", "args", ".", "gen_subset", ",", "\n", "max_sentences", "=", "args", ".", "max_sentences", ",", "\n", "max_positions", "=", "max_positions", ",", "\n", "skip_invalid_size_inputs_valid_test", "=", "args", ".", "skip_invalid_size_inputs_valid_test", ",", "\n", ")", "\n", "if", "args", ".", "num_shards", ">", "1", ":", "\n", "        ", "if", "args", ".", "shard_id", "<", "0", "or", "args", ".", "shard_id", ">=", "args", ".", "num_shards", ":", "\n", "            ", "raise", "ValueError", "(", "'--shard-id must be between 0 and num_shards'", ")", "\n", "", "itr", "=", "data", ".", "sharded_iterator", "(", "itr", ",", "args", ".", "num_shards", ",", "args", ".", "shard_id", ")", "\n", "\n", "# print(\"SHASHI: I AM HERE\")", "\n", "\n", "# Initialize generator", "\n", "", "gen_timer", "=", "StopwatchMeter", "(", ")", "\n", "if", "args", ".", "score_reference", ":", "\n", "        ", "translator", "=", "SequenceScorer", "(", "models", ")", "\n", "", "else", ":", "\n", "        ", "translator", "=", "SequenceGenerator", "(", "\n", "models", ",", "beam_size", "=", "args", ".", "beam", ",", "stop_early", "=", "(", "not", "args", ".", "no_early_stop", ")", ",", "\n", "normalize_scores", "=", "(", "not", "args", ".", "unnormalized", ")", ",", "len_penalty", "=", "args", ".", "lenpen", ",", "\n", "unk_penalty", "=", "args", ".", "unkpen", ")", "\n", "", "if", "use_cuda", ":", "\n", "        ", "translator", ".", "cuda", "(", ")", "\n", "\n", "# Generate and compute BLEU score", "\n", "", "scorer", "=", "bleu", ".", "Scorer", "(", "dataset", ".", "dst_dict", ".", "pad", "(", ")", ",", "dataset", ".", "dst_dict", ".", "eos", "(", ")", ",", "dataset", ".", "dst_dict", ".", "unk", "(", ")", ")", "\n", "num_sentences", "=", "0", "\n", "has_target", "=", "True", "\n", "with", "progress_bar", ".", "build_progress_bar", "(", "args", ",", "itr", ")", "as", "t", ":", "\n", "        ", "if", "args", ".", "score_reference", ":", "\n", "            ", "translations", "=", "translator", ".", "score_batched_itr", "(", "t", ",", "cuda", "=", "use_cuda", ",", "timer", "=", "gen_timer", ")", "\n", "", "else", ":", "\n", "            ", "translations", "=", "translator", ".", "generate_batched_itr", "(", "\n", "t", ",", "maxlen_a", "=", "args", ".", "max_len_a", ",", "maxlen_b", "=", "args", ".", "max_len_b", ",", "\n", "cuda", "=", "use_cuda", ",", "timer", "=", "gen_timer", ",", "prefix_size", "=", "args", ".", "prefix_size", ")", "\n", "", "wps_meter", "=", "TimeMeter", "(", ")", "\n", "for", "sample_id", ",", "src_tokens", ",", "target_tokens", ",", "hypos", "in", "translations", ":", "\n", "# Process input and ground truth", "\n", "            ", "has_target", "=", "target_tokens", "is", "not", "None", "\n", "target_tokens", "=", "target_tokens", ".", "int", "(", ")", ".", "cpu", "(", ")", "if", "has_target", "else", "None", "\n", "# Either retrieve the original sentences or regenerate them from tokens.", "\n", "if", "align_dict", "is", "not", "None", ":", "\n", "                ", "src_str", "=", "dataset", ".", "splits", "[", "args", ".", "gen_subset", "]", ".", "src", ".", "get_original_text", "(", "sample_id", ")", "\n", "target_str", "=", "dataset", ".", "splits", "[", "args", ".", "gen_subset", "]", ".", "dst", ".", "get_original_text", "(", "sample_id", ")", "\n", "", "else", ":", "\n", "                ", "src_str", "=", "dataset", ".", "src_dict", ".", "string", "(", "src_tokens", ",", "args", ".", "remove_bpe", ")", "\n", "target_str", "=", "dataset", ".", "dst_dict", ".", "string", "(", "target_tokens", ",", "\n", "args", ".", "remove_bpe", ",", "\n", "escape_unk", "=", "True", ")", "if", "has_target", "else", "''", "\n", "\n", "", "if", "not", "args", ".", "quiet", ":", "\n", "                ", "print", "(", "'S-{}\\t{}'", ".", "format", "(", "sample_id", ",", "src_str", ")", ")", "\n", "if", "has_target", ":", "\n", "                    ", "print", "(", "'T-{}\\t{}'", ".", "format", "(", "sample_id", ",", "target_str", ")", ")", "\n", "\n", "# Process top predictions", "\n", "", "", "for", "i", ",", "hypo", "in", "enumerate", "(", "hypos", "[", ":", "min", "(", "len", "(", "hypos", ")", ",", "args", ".", "nbest", ")", "]", ")", ":", "\n", "                ", "hypo_tokens", ",", "hypo_str", ",", "alignment", "=", "utils", ".", "post_process_prediction", "(", "\n", "hypo_tokens", "=", "hypo", "[", "'tokens'", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", ",", "\n", "src_str", "=", "src_str", ",", "\n", "alignment", "=", "hypo", "[", "'alignment'", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", ",", "\n", "align_dict", "=", "align_dict", ",", "\n", "dst_dict", "=", "dataset", ".", "dst_dict", ",", "\n", "remove_bpe", "=", "args", ".", "remove_bpe", ",", "\n", ")", "\n", "\n", "if", "not", "args", ".", "quiet", ":", "\n", "                    ", "print", "(", "'H-{}\\t{}\\t{}'", ".", "format", "(", "sample_id", ",", "hypo", "[", "'score'", "]", ",", "hypo_str", ")", ")", "\n", "print", "(", "'P-{}\\t{}'", ".", "format", "(", "\n", "sample_id", ",", "\n", "' '", ".", "join", "(", "map", "(", "\n", "lambda", "x", ":", "'{:.4f}'", ".", "format", "(", "x", ")", ",", "\n", "hypo", "[", "'positional_scores'", "]", ".", "tolist", "(", ")", ",", "\n", ")", ")", "\n", ")", ")", "\n", "print", "(", "'A-{}\\t{}'", ".", "format", "(", "\n", "sample_id", ",", "\n", "' '", ".", "join", "(", "map", "(", "lambda", "x", ":", "str", "(", "utils", ".", "item", "(", "x", ")", ")", ",", "alignment", ")", ")", "\n", ")", ")", "\n", "\n", "# Score only the top hypothesis", "\n", "", "if", "has_target", "and", "i", "==", "0", ":", "\n", "                    ", "if", "align_dict", "is", "not", "None", "or", "args", ".", "remove_bpe", "is", "not", "None", ":", "\n", "# Convert back to tokens for evaluation with unk replacement and/or without BPE", "\n", "                        ", "target_tokens", "=", "tokenizer", ".", "Tokenizer", ".", "tokenize", "(", "\n", "target_str", ",", "dataset", ".", "dst_dict", ",", "add_if_not_exist", "=", "True", ")", "\n", "", "scorer", ".", "add", "(", "target_tokens", ",", "hypo_tokens", ")", "\n", "\n", "", "", "wps_meter", ".", "update", "(", "src_tokens", ".", "size", "(", "0", ")", ")", "\n", "t", ".", "log", "(", "{", "'wps'", ":", "round", "(", "wps_meter", ".", "avg", ")", "}", ")", "\n", "num_sentences", "+=", "1", "\n", "\n", "", "", "print", "(", "'| Translated {} sentences ({} tokens) in {:.1f}s ({:.2f} tokens/s)'", ".", "format", "(", "\n", "num_sentences", ",", "gen_timer", ".", "n", ",", "gen_timer", ".", "sum", ",", "1.", "/", "gen_timer", ".", "avg", ")", ")", "\n", "if", "has_target", ":", "\n", "        ", "print", "(", "'| Generate {} with beam={}: {}'", ".", "format", "(", "args", ".", "gen_subset", ",", "args", ".", "beam", ",", "scorer", ".", "result_string", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-Topic-ConvS2S.singleprocess_train.main": [[22, 104], ["print", "torch.cuda.set_device", "torch.manual_seed", "fairseq.data.has_binary_files", "print", "print", "print", "fairseq.models.build_model", "fairseq.criterions.build_criterion", "print", "print", "fairseq.trainer.Trainer", "print", "print", "os.makedirs", "os.path.join", "fairseq.trainer.Trainer.load_checkpoint", "fairseq.trainer.Trainer.get_lr", "fairseq.meters.StopwatchMeter", "fairseq.meters.StopwatchMeter.start", "fairseq.meters.StopwatchMeter.stop", "print", "torch.cuda.is_available", "NotImplementedError", "fairseq.data.load_dataset", "fairseq.data.load_raw_text_dataset", "print", "print", "singleprocess_train.train", "enumerate", "len", "len", "len", "sum", "fairseq.trainer.Trainer.lr_step", "args.valid_subset.split", "singleprocess_train.validate", "len", "fairseq.trainer.Trainer.lr_step", "p.data.numel", "singleprocess_train.save_checkpoint", "models.build_model.parameters"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.has_binary_files", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.__init__.build_model", "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.__init__.build_criterion", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.load_checkpoint", "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.get_lr", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.stop", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.load_dataset", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.load_raw_text_dataset", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.train", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.lr_step", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.validate", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.lr_step", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.save_checkpoint"], ["def", "main", "(", "args", ")", ":", "\n", "    ", "print", "(", "args", ")", "\n", "\n", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "'Training on CPU is not supported'", ")", "\n", "", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "device_id", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "\n", "# Load dataset", "\n", "splits", "=", "[", "'train'", ",", "'valid'", "]", "\n", "if", "data", ".", "has_binary_files", "(", "args", ".", "data", ",", "splits", ")", ":", "\n", "        ", "dataset", "=", "data", ".", "load_dataset", "(", "\n", "args", ".", "data", ",", "splits", ",", "args", ".", "source_lang", ",", "args", ".", "target_lang", ")", "\n", "", "else", ":", "\n", "        ", "dataset", "=", "data", ".", "load_raw_text_dataset", "(", "\n", "args", ".", "data", ",", "splits", ",", "args", ".", "source_lang", ",", "args", ".", "target_lang", ",", "args", ".", "doctopics", ",", "args", ".", "encoder_embed_dim", ")", "\n", "", "if", "args", ".", "source_lang", "is", "None", "or", "args", ".", "target_lang", "is", "None", ":", "\n", "# record inferred languages in args, so that it's saved in checkpoints", "\n", "        ", "args", ".", "source_lang", ",", "args", ".", "target_lang", "=", "dataset", ".", "src", ",", "dataset", ".", "dst", "\n", "", "print", "(", "'| [{}] dictionary: {} types'", ".", "format", "(", "dataset", ".", "src", ",", "len", "(", "dataset", ".", "src_dict", ")", ")", ")", "\n", "print", "(", "'| [{}] dictionary: {} types'", ".", "format", "(", "dataset", ".", "dst", ",", "len", "(", "dataset", ".", "dst_dict", ")", ")", ")", "\n", "print", "(", "'| [{}-lemma-topic] dictionary: {} types'", ".", "format", "(", "dataset", ".", "src", ",", "len", "(", "dataset", ".", "src_lemma_topic_dict", ")", ")", ")", "\n", "for", "split", "in", "splits", ":", "\n", "        ", "print", "(", "'| {} {} {} examples'", ".", "format", "(", "args", ".", "data", ",", "split", ",", "len", "(", "dataset", ".", "splits", "[", "split", "]", ")", ")", ")", "\n", "\n", "# exit(0)", "\n", "\n", "# Build model and criterion", "\n", "", "model", "=", "models", ".", "build_model", "(", "args", ",", "dataset", ".", "src_dict", ",", "dataset", ".", "dst_dict", ")", "\n", "criterion", "=", "criterions", ".", "build_criterion", "(", "args", ",", "dataset", ".", "src_dict", ",", "dataset", ".", "dst_dict", ")", "\n", "print", "(", "'| model {}, criterion {}'", ".", "format", "(", "args", ".", "arch", ",", "criterion", ".", "__class__", ".", "__name__", ")", ")", "\n", "print", "(", "'| num. model params: {}'", ".", "format", "(", "sum", "(", "p", ".", "data", ".", "numel", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", ")", ")", ")", "\n", "\n", "# Build trainer", "\n", "trainer", "=", "Trainer", "(", "args", ",", "model", ",", "criterion", ")", "\n", "print", "(", "'| training on {} GPUs'", ".", "format", "(", "args", ".", "distributed_world_size", ")", ")", "\n", "print", "(", "'| max tokens per GPU = {} and max sentences per GPU = {}'", ".", "format", "(", "\n", "args", ".", "max_tokens", ",", "\n", "args", ".", "max_sentences", ",", "\n", ")", ")", "\n", "\n", "# exit(0)", "\n", "\n", "# Load the latest checkpoint if one is available", "\n", "os", ".", "makedirs", "(", "args", ".", "save_dir", ",", "exist_ok", "=", "True", ")", "\n", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save_dir", ",", "args", ".", "restore_file", ")", "\n", "extra_state", "=", "trainer", ".", "load_checkpoint", "(", "checkpoint_path", ")", "\n", "if", "extra_state", "is", "not", "None", ":", "\n", "        ", "epoch", "=", "extra_state", "[", "'epoch'", "]", "\n", "batch_offset", "=", "extra_state", "[", "'batch_offset'", "]", "\n", "print", "(", "'| loaded checkpoint {} (epoch {})'", ".", "format", "(", "checkpoint_path", ",", "epoch", ")", ")", "\n", "if", "batch_offset", "==", "0", ":", "\n", "            ", "trainer", ".", "lr_step", "(", "epoch", ")", "\n", "epoch", "+=", "1", "\n", "", "", "else", ":", "\n", "        ", "epoch", ",", "batch_offset", "=", "1", ",", "0", "\n", "\n", "# Train until the learning rate gets too small", "\n", "", "max_epoch", "=", "args", ".", "max_epoch", "or", "math", ".", "inf", "\n", "lr", "=", "trainer", ".", "get_lr", "(", ")", "\n", "train_meter", "=", "StopwatchMeter", "(", ")", "\n", "train_meter", ".", "start", "(", ")", "\n", "while", "lr", ">", "args", ".", "min_lr", "and", "epoch", "<=", "max_epoch", ":", "\n", "# train for one epoch", "\n", "        ", "train", "(", "args", ",", "trainer", ",", "dataset", ",", "epoch", ",", "batch_offset", ")", "\n", "\n", "# evaluate on validate set", "\n", "for", "k", ",", "subset", "in", "enumerate", "(", "args", ".", "valid_subset", ".", "split", "(", "','", ")", ")", ":", "\n", "            ", "val_loss", "=", "validate", "(", "args", ",", "trainer", ",", "dataset", ",", "subset", ",", "epoch", ")", "\n", "if", "k", "==", "0", ":", "\n", "# only use first validation loss to update the learning schedule", "\n", "                ", "lr", "=", "trainer", ".", "lr_step", "(", "epoch", ",", "val_loss", ")", "\n", "\n", "# save checkpoint", "\n", "if", "not", "args", ".", "no_save", ":", "\n", "                    ", "save_checkpoint", "(", "trainer", ",", "args", ",", "epoch", ",", "0", ",", "val_loss", ")", "\n", "\n", "", "", "", "epoch", "+=", "1", "\n", "batch_offset", "=", "0", "\n", "", "train_meter", ".", "stop", "(", ")", "\n", "\n", "print", "(", "'| done training in {:.1f} seconds'", ".", "format", "(", "train_meter", ".", "sum", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-Topic-ConvS2S.singleprocess_train.train": [[106, 176], ["torch.manual_seed", "dataset.train_dataloader", "fairseq.progress_bar.build_progress_bar", "itertools.islice", "collections.defaultdict", "enumerate", "singleprocess_train.get_training_stats", "collections.defaultdict.items", "progress_bar.build_progress_bar.print", "min", "min", "trainer.get_meter", "trainer.train_step", "singleprocess_train.get_training_stats", "trainer.train_step.items", "progress_bar.build_progress_bar.log", "trainer.get_model().max_encoder_positions", "trainer.get_model().max_decoder_positions", "trainer.get_meter.reset", "fairseq.meters.AverageMeter", "extra_meters[].update", "trainer.get_meter().reset", "singleprocess_train.save_checkpoint", "trainer.get_model", "trainer.get_model", "trainer.get_meter", "trainer.get_num_updates"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.LanguageDatasets.train_dataloader", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.get_training_stats", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.train_step", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.get_training_stats", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.log", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.max_encoder_positions", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.max_decoder_positions", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.reset", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.reset", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.save_checkpoint", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_model", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_model", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_num_updates"], ["", "def", "train", "(", "args", ",", "trainer", ",", "dataset", ",", "epoch", ",", "batch_offset", ")", ":", "\n", "    ", "\"\"\"Train the model for one epoch.\"\"\"", "\n", "\n", "# Set seed based on args.seed and the epoch number so that we get", "\n", "# reproducible results when resuming from checkpoints", "\n", "seed", "=", "args", ".", "seed", "+", "epoch", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "\n", "# The max number of positions can be different for train and valid", "\n", "# e.g., RNNs may support more positions at test time than seen in training", "\n", "max_positions_train", "=", "(", "\n", "min", "(", "args", ".", "max_source_positions", ",", "trainer", ".", "get_model", "(", ")", ".", "max_encoder_positions", "(", ")", ")", ",", "\n", "min", "(", "args", ".", "max_target_positions", ",", "trainer", ".", "get_model", "(", ")", ".", "max_decoder_positions", "(", ")", ")", "\n", ")", "\n", "\n", "# Initialize dataloader, starting at batch_offset", "\n", "itr", "=", "dataset", ".", "train_dataloader", "(", "\n", "args", ".", "train_subset", ",", "\n", "max_tokens", "=", "args", ".", "max_tokens", ",", "\n", "max_sentences", "=", "args", ".", "max_sentences", ",", "\n", "max_positions", "=", "max_positions_train", ",", "\n", "seed", "=", "seed", ",", "\n", "epoch", "=", "epoch", ",", "\n", "sample_without_replacement", "=", "args", ".", "sample_without_replacement", ",", "\n", "sort_by_source_size", "=", "(", "epoch", "<=", "args", ".", "curriculum", ")", ",", "\n", "shard_id", "=", "args", ".", "distributed_rank", ",", "\n", "num_shards", "=", "args", ".", "distributed_world_size", ",", "\n", ")", "\n", "progress", "=", "progress_bar", ".", "build_progress_bar", "(", "args", ",", "itr", ",", "epoch", ",", "no_progress_bar", "=", "'simple'", ")", "\n", "itr", "=", "itertools", ".", "islice", "(", "progress", ",", "batch_offset", ",", "None", ")", "\n", "\n", "# reset training meters", "\n", "for", "k", "in", "[", "'train_loss'", ",", "'train_nll_loss'", ",", "'wps'", ",", "'ups'", ",", "'wpb'", ",", "'bsz'", ",", "'clip'", "]", ":", "\n", "        ", "meter", "=", "trainer", ".", "get_meter", "(", "k", ")", "\n", "if", "meter", "is", "not", "None", ":", "\n", "            ", "meter", ".", "reset", "(", ")", "\n", "\n", "", "", "extra_meters", "=", "collections", ".", "defaultdict", "(", "lambda", ":", "AverageMeter", "(", ")", ")", "\n", "for", "i", ",", "sample", "in", "enumerate", "(", "itr", ",", "start", "=", "batch_offset", ")", ":", "\n", "# print(sample)", "\n", "# print(sample['net_input']['src_doctopic'][1])", "\n", "# print(sample['net_input']['src_wordtopics'][1][0], sample['net_input']['src_wordtopics'][1][1], sample['net_input']['src_wordtopics'][1][2],", "\n", "#       sample['net_input']['src_wordtopics'][1][3], sample['net_input']['src_wordtopics'][1][4])", "\n", "\n", "        ", "log_output", "=", "trainer", ".", "train_step", "(", "sample", ")", "\n", "\n", "# print(log_output)", "\n", "# exit(0)", "\n", "\n", "# log mid-epoch stats", "\n", "stats", "=", "get_training_stats", "(", "trainer", ")", "\n", "for", "k", ",", "v", "in", "log_output", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", "in", "[", "'loss'", ",", "'nll_loss'", "]", ":", "\n", "                ", "continue", "# these are already logged above", "\n", "", "extra_meters", "[", "k", "]", ".", "update", "(", "v", ")", "\n", "stats", "[", "k", "]", "=", "extra_meters", "[", "k", "]", ".", "avg", "\n", "", "progress", ".", "log", "(", "stats", ")", "\n", "\n", "# save mid-epoch checkpoints", "\n", "if", "i", "==", "batch_offset", ":", "\n", "# ignore the first mini-batch in words-per-second calculation", "\n", "            ", "trainer", ".", "get_meter", "(", "'wps'", ")", ".", "reset", "(", ")", "\n", "", "if", "args", ".", "save_interval", ">", "0", "and", "trainer", ".", "get_num_updates", "(", ")", "%", "args", ".", "save_interval", "==", "0", ":", "\n", "            ", "save_checkpoint", "(", "trainer", ",", "args", ",", "epoch", ",", "i", "+", "1", ")", "\n", "\n", "# log end-of-epoch stats", "\n", "", "", "stats", "=", "get_training_stats", "(", "trainer", ")", "\n", "for", "k", ",", "meter", "in", "extra_meters", ".", "items", "(", ")", ":", "\n", "        ", "stats", "[", "k", "]", "=", "meter", ".", "avg", "\n", "", "progress", ".", "print", "(", "stats", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-Topic-ConvS2S.singleprocess_train.get_training_stats": [[178, 197], ["collections.OrderedDict", "singleprocess_train.get_perplexity", "round", "round", "round", "trainer.get_num_updates", "trainer.get_lr", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.get_perplexity", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_num_updates", "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.get_lr", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter"], ["", "def", "get_training_stats", "(", "trainer", ")", ":", "\n", "    ", "stats", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "stats", "[", "'loss'", "]", "=", "'{:.3f}'", ".", "format", "(", "trainer", ".", "get_meter", "(", "'train_loss'", ")", ".", "avg", ")", "\n", "if", "trainer", ".", "get_meter", "(", "'train_nll_loss'", ")", ".", "count", ">", "0", ":", "\n", "        ", "nll_loss", "=", "trainer", ".", "get_meter", "(", "'train_nll_loss'", ")", ".", "avg", "\n", "stats", "[", "'nll_loss'", "]", "=", "'{:.3f}'", ".", "format", "(", "nll_loss", ")", "\n", "", "else", ":", "\n", "        ", "nll_loss", "=", "trainer", ".", "get_meter", "(", "'train_loss'", ")", ".", "avg", "\n", "", "stats", "[", "'ppl'", "]", "=", "get_perplexity", "(", "nll_loss", ")", "\n", "stats", "[", "'wps'", "]", "=", "round", "(", "trainer", ".", "get_meter", "(", "'wps'", ")", ".", "avg", ")", "\n", "stats", "[", "'ups'", "]", "=", "'{:.1f}'", ".", "format", "(", "trainer", ".", "get_meter", "(", "'ups'", ")", ".", "avg", ")", "\n", "stats", "[", "'wpb'", "]", "=", "round", "(", "trainer", ".", "get_meter", "(", "'wpb'", ")", ".", "avg", ")", "\n", "stats", "[", "'bsz'", "]", "=", "round", "(", "trainer", ".", "get_meter", "(", "'bsz'", ")", ".", "avg", ")", "\n", "stats", "[", "'num_updates'", "]", "=", "trainer", ".", "get_num_updates", "(", ")", "\n", "stats", "[", "'lr'", "]", "=", "trainer", ".", "get_lr", "(", ")", "\n", "stats", "[", "'gnorm'", "]", "=", "'{:.3f}'", ".", "format", "(", "trainer", ".", "get_meter", "(", "'gnorm'", ")", ".", "avg", ")", "\n", "stats", "[", "'clip'", "]", "=", "'{:.0%}'", ".", "format", "(", "trainer", ".", "get_meter", "(", "'clip'", ")", ".", "avg", ")", "\n", "stats", "[", "'oom'", "]", "=", "trainer", ".", "get_meter", "(", "'oom'", ")", ".", "avg", "\n", "return", "stats", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-Topic-ConvS2S.singleprocess_train.validate": [[199, 249], ["dataset.eval_dataloader", "fairseq.progress_bar.build_progress_bar", "collections.defaultdict", "singleprocess_train.get_valid_stats", "collections.defaultdict.items", "progress_bar.build_progress_bar.print", "trainer.get_model().max_encoder_positions", "trainer.get_model().max_decoder_positions", "trainer.get_meter", "trainer.valid_step", "singleprocess_train.get_valid_stats", "trainer.valid_step.items", "progress_bar.build_progress_bar.log", "trainer.get_meter.reset", "fairseq.meters.AverageMeter", "extra_meters[].update", "trainer.get_model", "trainer.get_model"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.LanguageDatasets.eval_dataloader", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.get_valid_stats", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.max_encoder_positions", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.max_decoder_positions", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.valid_step", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.get_valid_stats", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.log", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.reset", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_model", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_model"], ["", "def", "validate", "(", "args", ",", "trainer", ",", "dataset", ",", "subset", ",", "epoch", ")", ":", "\n", "    ", "\"\"\"Evaluate the model on the validation set and return the average loss.\"\"\"", "\n", "\n", "# Initialize dataloader", "\n", "max_positions_valid", "=", "(", "\n", "trainer", ".", "get_model", "(", ")", ".", "max_encoder_positions", "(", ")", ",", "\n", "trainer", ".", "get_model", "(", ")", ".", "max_decoder_positions", "(", ")", ",", "\n", ")", "\n", "itr", "=", "dataset", ".", "eval_dataloader", "(", "\n", "subset", ",", "\n", "max_tokens", "=", "args", ".", "max_tokens", ",", "\n", "max_sentences", "=", "args", ".", "max_sentences_valid", ",", "\n", "max_positions", "=", "max_positions_valid", ",", "\n", "skip_invalid_size_inputs_valid_test", "=", "args", ".", "skip_invalid_size_inputs_valid_test", ",", "\n", "descending", "=", "True", ",", "# largest batch first to warm the caching allocator", "\n", "shard_id", "=", "args", ".", "distributed_rank", ",", "\n", "num_shards", "=", "args", ".", "distributed_world_size", ",", "\n", ")", "\n", "progress", "=", "progress_bar", ".", "build_progress_bar", "(", "\n", "args", ",", "itr", ",", "epoch", ",", "\n", "prefix", "=", "'valid on \\'{}\\' subset'", ".", "format", "(", "subset", ")", ",", "\n", "no_progress_bar", "=", "'simple'", "\n", ")", "\n", "\n", "# reset validation loss meters", "\n", "for", "k", "in", "[", "'valid_loss'", ",", "'valid_nll_loss'", "]", ":", "\n", "        ", "meter", "=", "trainer", ".", "get_meter", "(", "k", ")", "\n", "if", "meter", "is", "not", "None", ":", "\n", "            ", "meter", ".", "reset", "(", ")", "\n", "\n", "", "", "extra_meters", "=", "collections", ".", "defaultdict", "(", "lambda", ":", "AverageMeter", "(", ")", ")", "\n", "for", "sample", "in", "progress", ":", "\n", "        ", "log_output", "=", "trainer", ".", "valid_step", "(", "sample", ")", "\n", "\n", "# log mid-validation stats", "\n", "stats", "=", "get_valid_stats", "(", "trainer", ")", "\n", "for", "k", ",", "v", "in", "log_output", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", "in", "[", "'loss'", ",", "'nll_loss'", "]", ":", "\n", "                ", "continue", "\n", "", "extra_meters", "[", "k", "]", ".", "update", "(", "v", ")", "\n", "stats", "[", "k", "]", "=", "extra_meters", "[", "k", "]", ".", "avg", "\n", "", "progress", ".", "log", "(", "stats", ")", "\n", "\n", "# log validation stats", "\n", "", "stats", "=", "get_valid_stats", "(", "trainer", ")", "\n", "for", "k", ",", "meter", "in", "extra_meters", ".", "items", "(", ")", ":", "\n", "        ", "stats", "[", "k", "]", "=", "meter", ".", "avg", "\n", "", "progress", ".", "print", "(", "stats", ")", "\n", "\n", "return", "stats", "[", "'valid_loss'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-Topic-ConvS2S.singleprocess_train.get_valid_stats": [[251, 261], ["collections.OrderedDict", "singleprocess_train.get_perplexity", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.get_perplexity", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter"], ["", "def", "get_valid_stats", "(", "trainer", ")", ":", "\n", "    ", "stats", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "stats", "[", "'valid_loss'", "]", "=", "trainer", ".", "get_meter", "(", "'valid_loss'", ")", ".", "avg", "\n", "if", "trainer", ".", "get_meter", "(", "'valid_nll_loss'", ")", ".", "count", ">", "0", ":", "\n", "        ", "nll_loss", "=", "trainer", ".", "get_meter", "(", "'valid_nll_loss'", ")", ".", "avg", "\n", "stats", "[", "'valid_nll_loss'", "]", "=", "nll_loss", "\n", "", "else", ":", "\n", "        ", "nll_loss", "=", "trainer", ".", "get_meter", "(", "'valid_loss'", ")", ".", "avg", "\n", "", "stats", "[", "'valid_ppl'", "]", "=", "get_perplexity", "(", "nll_loss", ")", "\n", "return", "stats", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-Topic-ConvS2S.singleprocess_train.get_perplexity": [[263, 268], ["math.pow", "float"], "function", ["None"], ["", "def", "get_perplexity", "(", "loss", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "return", "'{:.2f}'", ".", "format", "(", "math", ".", "pow", "(", "2", ",", "loss", ")", ")", "\n", "", "except", "OverflowError", ":", "\n", "        ", "return", "float", "(", "'inf'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-Topic-ConvS2S.singleprocess_train.save_checkpoint": [[270, 294], ["os.path.join", "trainer.save_checkpoint", "os.path.join", "trainer.save_checkpoint", "os.path.join", "trainer.save_checkpoint", "os.path.join", "trainer.save_checkpoint", "hasattr"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.save_checkpoint", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.save_checkpoint", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.save_checkpoint", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.save_checkpoint"], ["", "", "def", "save_checkpoint", "(", "trainer", ",", "args", ",", "epoch", ",", "batch_offset", ",", "val_loss", "=", "None", ")", ":", "\n", "    ", "extra_state", "=", "{", "\n", "'epoch'", ":", "epoch", ",", "\n", "'batch_offset'", ":", "batch_offset", ",", "\n", "'val_loss'", ":", "val_loss", ",", "\n", "}", "\n", "\n", "if", "batch_offset", "==", "0", ":", "\n", "        ", "if", "not", "args", ".", "no_epoch_checkpoints", ":", "\n", "            ", "epoch_filename", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save_dir", ",", "'checkpoint{}.pt'", ".", "format", "(", "epoch", ")", ")", "\n", "trainer", ".", "save_checkpoint", "(", "epoch_filename", ",", "extra_state", ")", "\n", "\n", "", "assert", "val_loss", "is", "not", "None", "\n", "if", "not", "hasattr", "(", "save_checkpoint", ",", "'best'", ")", "or", "val_loss", "<", "save_checkpoint", ".", "best", ":", "\n", "            ", "save_checkpoint", ".", "best", "=", "val_loss", "\n", "best_filename", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save_dir", ",", "'checkpoint_best.pt'", ")", "\n", "trainer", ".", "save_checkpoint", "(", "best_filename", ",", "extra_state", ")", "\n", "", "", "elif", "not", "args", ".", "no_epoch_checkpoints", ":", "\n", "        ", "epoch_filename", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "save_dir", ",", "'checkpoint{}_{}.pt'", ".", "format", "(", "epoch", ",", "batch_offset", ")", ")", "\n", "trainer", ".", "save_checkpoint", "(", "epoch_filename", ",", "extra_state", ")", "\n", "\n", "", "last_filename", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save_dir", ",", "'checkpoint_last.pt'", ")", "\n", "trainer", ".", "save_checkpoint", "(", "last_filename", ",", "extra_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-Topic-ConvS2S.interactive.main": [[17, 74], ["print", "print", "fairseq.utils.load_ensemble_for_inference", "print", "print", "fairseq.sequence_generator.SequenceGenerator", "fairseq.utils.load_align_dict", "print", "torch.cuda.is_available", "model.make_generation_fast_", "fairseq.sequence_generator.SequenceGenerator.cuda", "src_str.strip.strip", "fairseq.tokenizer.Tokenizer.tokenize().long", "src_tokens.cuda.new", "fairseq.sequence_generator.SequenceGenerator.generate", "print", "len", "len", "src_tokens.cuda.cuda", "torch.autograd.Variable", "torch.autograd.Variable", "fairseq.utils.post_process_prediction", "print", "print", "fairseq.tokenizer.Tokenizer.tokenize", "src_tokens.cuda.numel", "src_tokens.cuda.view", "src_tokens.new.view", "min", "len", "hypo[].int().cpu", "hypo[].int().cpu", "map", "hypo[].int", "hypo[].int"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.load_ensemble_for_inference", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.load_align_dict", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.make_generation_fast_", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator.cuda", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator.cuda", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.post_process_prediction", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.tokenizer.Tokenizer.tokenize"], ["def", "main", "(", "args", ")", ":", "\n", "    ", "print", "(", "args", ")", "\n", "\n", "use_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "cpu", "\n", "\n", "# Load ensemble", "\n", "print", "(", "'| loading model(s) from {}'", ".", "format", "(", "', '", ".", "join", "(", "args", ".", "path", ")", ")", ")", "\n", "models", ",", "model_args", "=", "utils", ".", "load_ensemble_for_inference", "(", "args", ".", "path", ",", "data_dir", "=", "args", ".", "data", ")", "\n", "src_dict", ",", "dst_dict", "=", "models", "[", "0", "]", ".", "src_dict", ",", "models", "[", "0", "]", ".", "dst_dict", "\n", "\n", "print", "(", "'| [{}] dictionary: {} types'", ".", "format", "(", "model_args", ".", "source_lang", ",", "len", "(", "src_dict", ")", ")", ")", "\n", "print", "(", "'| [{}] dictionary: {} types'", ".", "format", "(", "model_args", ".", "target_lang", ",", "len", "(", "dst_dict", ")", ")", ")", "\n", "\n", "# Optimize ensemble for generation", "\n", "for", "model", "in", "models", ":", "\n", "        ", "model", ".", "make_generation_fast_", "(", "\n", "beamable_mm_beam_size", "=", "None", "if", "args", ".", "no_beamable_mm", "else", "args", ".", "beam", ",", "\n", ")", "\n", "\n", "# Initialize generator", "\n", "", "translator", "=", "SequenceGenerator", "(", "\n", "models", ",", "beam_size", "=", "args", ".", "beam", ",", "stop_early", "=", "(", "not", "args", ".", "no_early_stop", ")", ",", "\n", "normalize_scores", "=", "(", "not", "args", ".", "unnormalized", ")", ",", "len_penalty", "=", "args", ".", "lenpen", ",", "\n", "unk_penalty", "=", "args", ".", "unkpen", ")", "\n", "if", "use_cuda", ":", "\n", "        ", "translator", ".", "cuda", "(", ")", "\n", "\n", "# Load alignment dictionary for unknown word replacement", "\n", "# (None if no unknown word replacement, empty if no path to align dictionary)", "\n", "", "align_dict", "=", "utils", ".", "load_align_dict", "(", "args", ".", "replace_unk", ")", "\n", "\n", "print", "(", "'| Type the input sentence and press return:'", ")", "\n", "for", "src_str", "in", "sys", ".", "stdin", ":", "\n", "        ", "src_str", "=", "src_str", ".", "strip", "(", ")", "\n", "src_tokens", "=", "tokenizer", ".", "Tokenizer", ".", "tokenize", "(", "src_str", ",", "src_dict", ",", "add_if_not_exist", "=", "False", ")", ".", "long", "(", ")", "\n", "if", "use_cuda", ":", "\n", "            ", "src_tokens", "=", "src_tokens", ".", "cuda", "(", ")", "\n", "", "src_lengths", "=", "src_tokens", ".", "new", "(", "[", "src_tokens", ".", "numel", "(", ")", "]", ")", "\n", "translations", "=", "translator", ".", "generate", "(", "\n", "Variable", "(", "src_tokens", ".", "view", "(", "1", ",", "-", "1", ")", ")", ",", "\n", "Variable", "(", "src_lengths", ".", "view", "(", "-", "1", ")", ")", ",", "\n", ")", "\n", "hypos", "=", "translations", "[", "0", "]", "\n", "print", "(", "'O\\t{}'", ".", "format", "(", "src_str", ")", ")", "\n", "\n", "# Process top predictions", "\n", "for", "hypo", "in", "hypos", "[", ":", "min", "(", "len", "(", "hypos", ")", ",", "args", ".", "nbest", ")", "]", ":", "\n", "            ", "hypo_tokens", ",", "hypo_str", ",", "alignment", "=", "utils", ".", "post_process_prediction", "(", "\n", "hypo_tokens", "=", "hypo", "[", "'tokens'", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", ",", "\n", "src_str", "=", "src_str", ",", "\n", "alignment", "=", "hypo", "[", "'alignment'", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", ",", "\n", "align_dict", "=", "align_dict", ",", "\n", "dst_dict", "=", "dst_dict", ",", "\n", "remove_bpe", "=", "args", ".", "remove_bpe", ",", "\n", ")", "\n", "print", "(", "'H\\t{}\\t{}'", ".", "format", "(", "hypo", "[", "'score'", "]", ",", "hypo_str", ")", ")", "\n", "print", "(", "'A\\t{}'", ".", "format", "(", "' '", ".", "join", "(", "map", "(", "str", ",", "alignment", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-Topic-ConvS2S.train.main": [[16, 24], ["distributed_train.main", "multiprocessing_train.main", "singleprocess_train.main"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.score.main", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.score.main", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.score.main"], ["def", "main", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "distributed_port", ">", "0", "or", "args", ".", "distributed_init_method", "is", "not", "None", ":", "\n", "        ", "distributed_main", "(", "args", ")", "\n", "", "elif", "args", ".", "distributed_world_size", ">", "1", ":", "\n", "        ", "multiprocessing_main", "(", "args", ")", "\n", "", "else", ":", "\n", "        ", "singleprocess_main", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-Topic-ConvS2S.multiprocessing_train.ErrorHandler.__init__": [[59, 67], ["threading.Thread", "multiprocessing_train.ErrorHandler.error_thread.start", "signal.signal"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.start"], ["def", "__init__", "(", "self", ",", "error_queue", ")", ":", "\n", "        ", "import", "signal", "\n", "import", "threading", "\n", "self", ".", "error_queue", "=", "error_queue", "\n", "self", ".", "children_pids", "=", "[", "]", "\n", "self", ".", "error_thread", "=", "threading", ".", "Thread", "(", "target", "=", "self", ".", "error_listener", ",", "daemon", "=", "True", ")", "\n", "self", ".", "error_thread", ".", "start", "(", ")", "\n", "signal", ".", "signal", "(", "signal", ".", "SIGUSR1", ",", "self", ".", "signal_handler", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-Topic-ConvS2S.multiprocessing_train.ErrorHandler.add_child": [[68, 70], ["multiprocessing_train.ErrorHandler.children_pids.append"], "methods", ["None"], ["", "def", "add_child", "(", "self", ",", "pid", ")", ":", "\n", "        ", "self", ".", "children_pids", ".", "append", "(", "pid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-Topic-ConvS2S.multiprocessing_train.ErrorHandler.error_listener": [[71, 75], ["multiprocessing_train.ErrorHandler.error_queue.get", "multiprocessing_train.ErrorHandler.error_queue.put", "os.kill", "os.getpid"], "methods", ["None"], ["", "def", "error_listener", "(", "self", ")", ":", "\n", "        ", "(", "rank", ",", "original_trace", ")", "=", "self", ".", "error_queue", ".", "get", "(", ")", "\n", "self", ".", "error_queue", ".", "put", "(", "(", "rank", ",", "original_trace", ")", ")", "\n", "os", ".", "kill", "(", "os", ".", "getpid", "(", ")", ",", "signal", ".", "SIGUSR1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-Topic-ConvS2S.multiprocessing_train.ErrorHandler.signal_handler": [[76, 83], ["multiprocessing_train.ErrorHandler.error_queue.get", "Exception", "os.kill"], "methods", ["None"], ["", "def", "signal_handler", "(", "self", ",", "signalnum", ",", "stackframe", ")", ":", "\n", "        ", "for", "pid", "in", "self", ".", "children_pids", ":", "\n", "            ", "os", ".", "kill", "(", "pid", ",", "signal", ".", "SIGINT", ")", "# kill children processes", "\n", "", "(", "rank", ",", "original_trace", ")", "=", "self", ".", "error_queue", ".", "get", "(", ")", "\n", "msg", "=", "\"\\n\\n-- Tracebacks above this line can probably be ignored --\\n\\n\"", "\n", "msg", "+=", "original_trace", "\n", "raise", "Exception", "(", "msg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-Topic-ConvS2S.multiprocessing_train.main": [[19, 41], ["torch.cuda.device_count", "torch.multiprocessing.get_context", "torch.multiprocessing.get_context.SimpleQueue", "multiprocessing_train.ErrorHandler", "range", "procs.append", "procs[].start", "multiprocessing_train.ErrorHandler.add_child", "p.join", "random.randint", "torch.multiprocessing.get_context.Process"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.add_child"], ["def", "main", "(", "args", ")", ":", "\n", "# Set distributed training parameters for a single node.", "\n", "    ", "args", ".", "distributed_world_size", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "args", ".", "distributed_init_method", "=", "'tcp://localhost:{port}'", ".", "format", "(", "\n", "port", "=", "random", ".", "randint", "(", "10000", ",", "20000", ")", ")", "\n", "\n", "mp", "=", "torch", ".", "multiprocessing", ".", "get_context", "(", "'spawn'", ")", "\n", "\n", "# Create a thread to listen for errors in the child processes.", "\n", "error_queue", "=", "mp", ".", "SimpleQueue", "(", ")", "\n", "error_handler", "=", "ErrorHandler", "(", "error_queue", ")", "\n", "\n", "# Train with multiprocessing.", "\n", "procs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "args", ".", "distributed_world_size", ")", ":", "\n", "        ", "args", ".", "distributed_rank", "=", "i", "\n", "args", ".", "device_id", "=", "i", "\n", "procs", ".", "append", "(", "mp", ".", "Process", "(", "target", "=", "run", ",", "args", "=", "(", "args", ",", "error_queue", ",", ")", ",", "daemon", "=", "True", ")", ")", "\n", "procs", "[", "i", "]", ".", "start", "(", ")", "\n", "error_handler", ".", "add_child", "(", "procs", "[", "i", "]", ".", "pid", ")", "\n", "", "for", "p", "in", "procs", ":", "\n", "        ", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-Topic-ConvS2S.multiprocessing_train.run": [[43, 53], ["fairseq.distributed_utils.distributed_init", "singleprocess_train.main", "error_queue.put", "traceback.format_exc"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.distributed_utils.distributed_init", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.score.main"], ["", "", "def", "run", "(", "args", ",", "error_queue", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "args", ".", "distributed_rank", "=", "distributed_utils", ".", "distributed_init", "(", "args", ")", "\n", "single_process_main", "(", "args", ")", "\n", "", "except", "KeyboardInterrupt", ":", "\n", "        ", "pass", "# killed by parent, do nothing", "\n", "", "except", "Exception", ":", "\n", "# propagate exception to parent process, keeping original traceback", "\n", "        ", "import", "traceback", "\n", "error_queue", ".", "put", "(", "(", "args", ".", "distributed_rank", ",", "traceback", ".", "format_exc", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-Topic-ConvS2S.preprocess.get_parser": [[21, 44], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Data pre-processing: Create dictionary and store data in binary format'", ")", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--source-lang'", ",", "default", "=", "None", ",", "metavar", "=", "'SRC'", ",", "help", "=", "'source language'", ")", "\n", "parser", ".", "add_argument", "(", "'-t'", ",", "'--target-lang'", ",", "default", "=", "None", ",", "metavar", "=", "'TARGET'", ",", "help", "=", "'target language'", ")", "\n", "parser", ".", "add_argument", "(", "'--trainpref'", ",", "metavar", "=", "'FP'", ",", "default", "=", "None", ",", "help", "=", "'target language'", ")", "\n", "parser", ".", "add_argument", "(", "'--validpref'", ",", "metavar", "=", "'FP'", ",", "default", "=", "None", ",", "help", "=", "'comma separated, valid language prefixes'", ")", "\n", "parser", ".", "add_argument", "(", "'--testpref'", ",", "metavar", "=", "'FP'", ",", "default", "=", "None", ",", "help", "=", "'comma separated, test language prefixes'", ")", "\n", "parser", ".", "add_argument", "(", "'--destdir'", ",", "metavar", "=", "'DIR'", ",", "default", "=", "'data-bin'", ",", "help", "=", "'destination dir'", ")", "\n", "parser", ".", "add_argument", "(", "'--thresholdtgt'", ",", "metavar", "=", "'N'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "'map words appearing less than threshold times to unknown'", ")", "\n", "parser", ".", "add_argument", "(", "'--thresholdsrc'", ",", "metavar", "=", "'N'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "'map words appearing less than threshold times to unknown'", ")", "\n", "parser", ".", "add_argument", "(", "'--tgtdict'", ",", "metavar", "=", "'FP'", ",", "help", "=", "'reuse given target dictionary'", ")", "\n", "parser", ".", "add_argument", "(", "'--srcdict'", ",", "metavar", "=", "'FP'", ",", "help", "=", "'reuse given source dictionary'", ")", "\n", "parser", ".", "add_argument", "(", "'--nwordstgt'", ",", "metavar", "=", "'N'", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "help", "=", "'number of target words to retain'", ")", "\n", "parser", ".", "add_argument", "(", "'--nwordssrc'", ",", "metavar", "=", "'N'", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "help", "=", "'number of source words to retain'", ")", "\n", "parser", ".", "add_argument", "(", "'--alignfile'", ",", "metavar", "=", "'ALIGN'", ",", "default", "=", "None", ",", "help", "=", "'an alignment file (optional)'", ")", "\n", "parser", ".", "add_argument", "(", "'--output-format'", ",", "metavar", "=", "'FORMAT'", ",", "default", "=", "'binary'", ",", "choices", "=", "[", "'binary'", ",", "'raw'", "]", ",", "\n", "help", "=", "'output format (optional)'", ")", "\n", "parser", ".", "add_argument", "(", "'--joined-dictionary'", ",", "action", "=", "'store_true'", ",", "help", "=", "'Generate joined dictionary'", ")", "\n", "parser", ".", "add_argument", "(", "'--only-source'", ",", "action", "=", "'store_true'", ",", "help", "=", "'Only process the source language'", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-Topic-ConvS2S.preprocess.main": [[46, 171], ["print", "os.makedirs", "Tokenizer.build_dictionary.save", "exit", "preprocess.main.make_all"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.save"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "print", "(", "args", ")", "\n", "os", ".", "makedirs", "(", "args", ".", "destdir", ",", "exist_ok", "=", "True", ")", "\n", "target", "=", "not", "args", ".", "only_source", "\n", "\n", "if", "args", ".", "joined_dictionary", ":", "\n", "        ", "assert", "not", "args", ".", "srcdict", ",", "'cannot combine --srcdict and --joined-dictionary'", "\n", "assert", "not", "args", ".", "tgtdict", ",", "'cannot combine --tgtdict and --joined-dictionary'", "\n", "src_dict", "=", "dictionary", ".", "Dictionary", "(", ")", "\n", "for", "lang", "in", "[", "args", ".", "source_lang", ",", "args", ".", "target_lang", "]", ":", "\n", "            ", "Tokenizer", ".", "add_file_to_dictionary", "(", "\n", "filename", "=", "'{}.{}'", ".", "format", "(", "args", ".", "trainpref", ",", "lang", ")", ",", "\n", "dict", "=", "src_dict", ",", "\n", "tokenize", "=", "tokenize_line", ",", "\n", ")", "\n", "", "src_dict", ".", "finalize", "(", ")", "\n", "tgt_dict", "=", "src_dict", "\n", "", "else", ":", "\n", "        ", "if", "args", ".", "srcdict", ":", "\n", "            ", "src_dict", "=", "dictionary", ".", "Dictionary", ".", "load", "(", "args", ".", "srcdict", ")", "\n", "", "else", ":", "\n", "            ", "assert", "args", ".", "trainpref", ",", "\"--trainpref must be set if --srcdict is not specified\"", "\n", "src_dict", "=", "Tokenizer", ".", "build_dictionary", "(", "filename", "=", "'{}.{}'", ".", "format", "(", "args", ".", "trainpref", ",", "args", ".", "source_lang", ")", ")", "\n", "", "if", "target", ":", "\n", "            ", "if", "args", ".", "tgtdict", ":", "\n", "                ", "tgt_dict", "=", "dictionary", ".", "Dictionary", ".", "load", "(", "args", ".", "tgtdict", ")", "\n", "", "else", ":", "\n", "                ", "assert", "args", ".", "trainpref", ",", "\"--trainpref must be set if --tgtdict is not specified\"", "\n", "tgt_dict", "=", "Tokenizer", ".", "build_dictionary", "(", "filename", "=", "'{}.{}'", ".", "format", "(", "args", ".", "trainpref", ",", "args", ".", "target_lang", ")", ")", "\n", "\n", "", "", "", "src_dict", ".", "save", "(", "os", ".", "path", ".", "join", "(", "args", ".", "destdir", ",", "'dict.{}.txt'", ".", "format", "(", "args", ".", "source_lang", ")", ")", ",", "\n", "threshold", "=", "args", ".", "thresholdsrc", ",", "nwords", "=", "args", ".", "nwordssrc", ")", "\n", "if", "target", ":", "\n", "        ", "tgt_dict", ".", "save", "(", "os", ".", "path", ".", "join", "(", "args", ".", "destdir", ",", "'dict.{}.txt'", ".", "format", "(", "args", ".", "target_lang", ")", ")", ",", "\n", "threshold", "=", "args", ".", "thresholdtgt", ",", "nwords", "=", "args", ".", "nwordstgt", ")", "\n", "\n", "", "exit", "(", "0", ")", "\n", "\n", "def", "make_binary_dataset", "(", "input_prefix", ",", "output_prefix", ",", "lang", ")", ":", "\n", "        ", "dict", "=", "dictionary", ".", "Dictionary", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "destdir", ",", "'dict.{}.txt'", ".", "format", "(", "lang", ")", ")", ")", "\n", "print", "(", "'| [{}] Dictionary: {} types'", ".", "format", "(", "lang", ",", "len", "(", "dict", ")", "-", "1", ")", ")", "\n", "\n", "ds", "=", "indexed_dataset", ".", "IndexedDatasetBuilder", "(", "\n", "'{}/{}.{}-{}.{}.bin'", ".", "format", "(", "args", ".", "destdir", ",", "output_prefix", ",", "args", ".", "source_lang", ",", "\n", "args", ".", "target_lang", ",", "lang", ")", "\n", ")", "\n", "\n", "def", "consumer", "(", "tensor", ")", ":", "\n", "            ", "ds", ".", "add_item", "(", "tensor", ")", "\n", "\n", "", "input_file", "=", "'{}.{}'", ".", "format", "(", "input_prefix", ",", "lang", ")", "\n", "res", "=", "Tokenizer", ".", "binarize", "(", "input_file", ",", "dict", ",", "consumer", ")", "\n", "print", "(", "'| [{}] {}: {} sents, {} tokens, {:.3}% replaced by {}'", ".", "format", "(", "\n", "lang", ",", "input_file", ",", "res", "[", "'nseq'", "]", ",", "res", "[", "'ntok'", "]", ",", "\n", "100", "*", "res", "[", "'nunk'", "]", "/", "res", "[", "'ntok'", "]", ",", "dict", ".", "unk_word", ")", ")", "\n", "ds", ".", "finalize", "(", "'{}/{}.{}-{}.{}.idx'", ".", "format", "(", "\n", "args", ".", "destdir", ",", "output_prefix", ",", "\n", "args", ".", "source_lang", ",", "args", ".", "target_lang", ",", "lang", ")", ")", "\n", "\n", "", "def", "make_dataset", "(", "input_prefix", ",", "output_prefix", ",", "lang", ",", "output_format", "=", "'binary'", ")", ":", "\n", "        ", "print", "(", "input_prefix", ",", "output_prefix", ",", "lang", ")", "\n", "exit", "(", "0", ")", "\n", "if", "output_format", "==", "'binary'", ":", "\n", "            ", "make_binary_dataset", "(", "input_prefix", ",", "output_prefix", ",", "lang", ")", "\n", "", "elif", "output_format", "==", "'raw'", ":", "\n", "# Copy original text file to destination folder", "\n", "            ", "output_text_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "destdir", ",", "'{}.{}'", ".", "format", "(", "output_prefix", ",", "lang", ")", ")", "\n", "shutil", ".", "copyfile", "(", "'{}.{}'", ".", "format", "(", "input_prefix", ",", "lang", ")", ",", "output_text_file", ")", "\n", "\n", "", "", "def", "make_all", "(", "args", ",", "make_dataset", ",", "lang", ")", ":", "\n", "        ", "if", "args", ".", "trainpref", ":", "\n", "            ", "make_dataset", "(", "args", ".", "trainpref", ",", "'train'", ",", "lang", ",", "args", ".", "output_format", ")", "\n", "", "if", "args", ".", "validpref", ":", "\n", "            ", "for", "k", ",", "validpref", "in", "enumerate", "(", "args", ".", "validpref", ".", "split", "(", "','", ")", ")", ":", "\n", "                ", "outprefix", "=", "'valid{}'", ".", "format", "(", "k", ")", "if", "k", ">", "0", "else", "'valid'", "\n", "make_dataset", "(", "validpref", ",", "outprefix", ",", "lang", ",", "args", ".", "output_format", ")", "\n", "", "", "if", "args", ".", "testpref", ":", "\n", "            ", "for", "k", ",", "testpref", "in", "enumerate", "(", "args", ".", "testpref", ".", "split", "(", "','", ")", ")", ":", "\n", "                ", "outprefix", "=", "'test{}'", ".", "format", "(", "k", ")", "if", "k", ">", "0", "else", "'test'", "\n", "make_dataset", "(", "testpref", ",", "outprefix", ",", "lang", ",", "args", ".", "output_format", ")", "\n", "\n", "", "", "", "make_all", "(", "args", ",", "make_dataset", ",", "args", ".", "source_lang", ")", "\n", "if", "target", ":", "\n", "        ", "make_all", "(", "args", ",", "make_dataset", ",", "args", ".", "target_lang", ")", "\n", "", "print", "(", "'| Wrote preprocessed data to {}'", ".", "format", "(", "args", ".", "destdir", ")", ")", "\n", "\n", "\n", "if", "args", ".", "alignfile", ":", "\n", "        ", "assert", "args", ".", "trainpref", ",", "\"--trainpref must be set if --alignfile is specified\"", "\n", "src_file_name", "=", "'{}.{}'", ".", "format", "(", "args", ".", "trainpref", ",", "args", ".", "source_lang", ")", "\n", "tgt_file_name", "=", "'{}.{}'", ".", "format", "(", "args", ".", "trainpref", ",", "args", ".", "target_lang", ")", "\n", "src_dict", "=", "dictionary", ".", "Dictionary", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "destdir", ",", "'dict.{}.txt'", ".", "format", "(", "args", ".", "source_lang", ")", ")", ")", "\n", "tgt_dict", "=", "dictionary", ".", "Dictionary", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "destdir", ",", "'dict.{}.txt'", ".", "format", "(", "args", ".", "target_lang", ")", ")", ")", "\n", "freq_map", "=", "{", "}", "\n", "with", "open", "(", "args", ".", "alignfile", ",", "'r'", ")", "as", "align_file", ":", "\n", "            ", "with", "open", "(", "src_file_name", ",", "'r'", ")", "as", "src_file", ":", "\n", "                ", "with", "open", "(", "tgt_file_name", ",", "'r'", ")", "as", "tgt_file", ":", "\n", "                    ", "for", "a", ",", "s", ",", "t", "in", "zip_longest", "(", "align_file", ",", "src_file", ",", "tgt_file", ")", ":", "\n", "                        ", "si", "=", "Tokenizer", ".", "tokenize", "(", "s", ",", "src_dict", ",", "add_if_not_exist", "=", "False", ")", "\n", "ti", "=", "Tokenizer", ".", "tokenize", "(", "t", ",", "tgt_dict", ",", "add_if_not_exist", "=", "False", ")", "\n", "ai", "=", "list", "(", "map", "(", "lambda", "x", ":", "tuple", "(", "x", ".", "split", "(", "'-'", ")", ")", ",", "a", ".", "split", "(", ")", ")", ")", "\n", "for", "sai", ",", "tai", "in", "ai", ":", "\n", "                            ", "srcidx", "=", "si", "[", "int", "(", "sai", ")", "]", "\n", "tgtidx", "=", "ti", "[", "int", "(", "tai", ")", "]", "\n", "if", "srcidx", "!=", "src_dict", ".", "unk", "(", ")", "and", "tgtidx", "!=", "tgt_dict", ".", "unk", "(", ")", ":", "\n", "                                ", "assert", "srcidx", "!=", "src_dict", ".", "pad", "(", ")", "\n", "assert", "srcidx", "!=", "src_dict", ".", "eos", "(", ")", "\n", "assert", "tgtidx", "!=", "tgt_dict", ".", "pad", "(", ")", "\n", "assert", "tgtidx", "!=", "tgt_dict", ".", "eos", "(", ")", "\n", "\n", "if", "srcidx", "not", "in", "freq_map", ":", "\n", "                                    ", "freq_map", "[", "srcidx", "]", "=", "{", "}", "\n", "", "if", "tgtidx", "not", "in", "freq_map", "[", "srcidx", "]", ":", "\n", "                                    ", "freq_map", "[", "srcidx", "]", "[", "tgtidx", "]", "=", "1", "\n", "", "else", ":", "\n", "                                    ", "freq_map", "[", "srcidx", "]", "[", "tgtidx", "]", "+=", "1", "\n", "\n", "", "", "", "", "", "", "", "align_dict", "=", "{", "}", "\n", "for", "srcidx", "in", "freq_map", ".", "keys", "(", ")", ":", "\n", "            ", "align_dict", "[", "srcidx", "]", "=", "max", "(", "freq_map", "[", "srcidx", "]", ",", "key", "=", "freq_map", "[", "srcidx", "]", ".", "get", ")", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "destdir", ",", "'alignment.{}-{}.txt'", ".", "format", "(", "\n", "args", ".", "source_lang", ",", "args", ".", "target_lang", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "for", "k", ",", "v", "in", "align_dict", ".", "items", "(", ")", ":", "\n", "                ", "print", "(", "'{} {}'", ".", "format", "(", "src_dict", "[", "k", "]", ",", "tgt_dict", "[", "v", "]", ")", ",", "file", "=", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-Topic-ConvS2S.distributed_train.main": [[17, 40], ["fairseq.distributed_utils.distributed_init", "print", "singleprocess_train.main", "os.environ.get", "ValueError", "socket.gethostname", "subprocess.check_output", "int", "int", "os.environ.get", "os.environ.get", "[].decode", "subprocess.check_output.split"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.distributed_utils.distributed_init", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.score.main"], ["def", "main", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "distributed_init_method", "is", "None", "and", "args", ".", "distributed_port", ">", "0", ":", "\n", "# We can determine the init method automatically for Slurm.", "\n", "        ", "node_list", "=", "os", ".", "environ", ".", "get", "(", "'SLURM_JOB_NODELIST'", ")", "\n", "if", "node_list", "is", "not", "None", ":", "\n", "            ", "try", ":", "\n", "                ", "hostnames", "=", "subprocess", ".", "check_output", "(", "[", "'scontrol'", ",", "'show'", ",", "'hostnames'", ",", "node_list", "]", ")", "\n", "args", ".", "distributed_init_method", "=", "'tcp://{host}:{port}'", ".", "format", "(", "\n", "host", "=", "hostnames", ".", "split", "(", ")", "[", "0", "]", ".", "decode", "(", "'utf-8'", ")", ",", "\n", "port", "=", "args", ".", "distributed_port", ")", "\n", "args", ".", "distributed_rank", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'SLURM_PROCID'", ")", ")", "\n", "args", ".", "device_id", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'SLURM_LOCALID'", ")", ")", "\n", "", "except", "subprocess", ".", "CalledProcessError", "as", "e", ":", "# scontrol failed", "\n", "                ", "raise", "e", "\n", "", "except", "FileNotFoundError", "as", "e", ":", "# Slurm is not installed", "\n", "                ", "pass", "\n", "", "", "", "if", "args", ".", "distributed_init_method", "is", "None", ":", "\n", "        ", "raise", "ValueError", "(", "'--distributed-init-method or --distributed-port '", "\n", "'must be specified for distributed training'", ")", "\n", "\n", "", "args", ".", "distributed_rank", "=", "distributed_utils", ".", "distributed_init", "(", "args", ")", "\n", "print", "(", "'| initialized host {} as rank {}'", ".", "format", "(", "socket", ".", "gethostname", "(", ")", ",", "args", ".", "distributed_rank", ")", ")", "\n", "single_process_main", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-Topic-ConvS2S.score.main": [[17, 56], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "os.path.exists", "fairseq.dictionary.Dictionary", "os.path.exists", "fd.readlines", "score.main.score"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.exists", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.exists", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.score"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Command-line script for BLEU scoring.'", ")", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--sys'", ",", "default", "=", "'-'", ",", "help", "=", "'system output'", ")", "\n", "parser", ".", "add_argument", "(", "'-r'", ",", "'--ref'", ",", "required", "=", "True", ",", "help", "=", "'references'", ")", "\n", "parser", ".", "add_argument", "(", "'-o'", ",", "'--order'", ",", "default", "=", "4", ",", "metavar", "=", "'N'", ",", "\n", "type", "=", "int", ",", "help", "=", "'consider ngrams up to this order'", ")", "\n", "parser", ".", "add_argument", "(", "'--ignore-case'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'case-insensitive scoring'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "print", "(", "args", ")", "\n", "\n", "assert", "args", ".", "sys", "==", "'-'", "or", "os", ".", "path", ".", "exists", "(", "args", ".", "sys", ")", ",", "\"System output file {} does not exist\"", ".", "format", "(", "args", ".", "sys", ")", "\n", "assert", "os", ".", "path", ".", "exists", "(", "args", ".", "ref", ")", ",", "\"Reference file {} does not exist\"", ".", "format", "(", "args", ".", "ref", ")", "\n", "\n", "dict", "=", "dictionary", ".", "Dictionary", "(", ")", "\n", "\n", "def", "readlines", "(", "fd", ")", ":", "\n", "        ", "for", "line", "in", "fd", ".", "readlines", "(", ")", ":", "\n", "            ", "if", "args", ".", "ignore_case", ":", "\n", "                ", "yield", "line", ".", "lower", "(", ")", "\n", "", "yield", "line", "\n", "\n", "", "", "def", "score", "(", "fdsys", ")", ":", "\n", "        ", "with", "open", "(", "args", ".", "ref", ")", "as", "fdref", ":", "\n", "            ", "scorer", "=", "bleu", ".", "Scorer", "(", "dict", ".", "pad", "(", ")", ",", "dict", ".", "eos", "(", ")", ",", "dict", ".", "unk", "(", ")", ")", "\n", "for", "sys_tok", ",", "ref_tok", "in", "zip", "(", "readlines", "(", "fdsys", ")", ",", "readlines", "(", "fdref", ")", ")", ":", "\n", "                ", "sys_tok", "=", "tokenizer", ".", "Tokenizer", ".", "tokenize", "(", "sys_tok", ",", "dict", ")", "\n", "ref_tok", "=", "tokenizer", ".", "Tokenizer", ".", "tokenize", "(", "ref_tok", ",", "dict", ")", "\n", "scorer", ".", "add", "(", "ref_tok", ",", "sys_tok", ")", "\n", "", "print", "(", "scorer", ".", "result_string", "(", "args", ".", "order", ")", ")", "\n", "\n", "", "", "if", "args", ".", "sys", "==", "'-'", ":", "\n", "        ", "score", "(", "sys", ".", "stdin", ")", "\n", "", "else", ":", "\n", "        ", "with", "open", "(", "args", ".", "sys", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "score", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.options.get_training_parser": [[18, 26], ["options.get_parser", "options.add_dataset_args", "options.add_distributed_training_args", "options.add_model_args", "options.add_optimization_args", "options.add_checkpoint_args"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.preprocess.get_parser", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.options.add_dataset_args", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.options.add_distributed_training_args", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.options.add_model_args", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.options.add_optimization_args", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.options.add_checkpoint_args"], ["from", "fairseq", ".", "optim", ".", "lr_scheduler", "import", "LR_SCHEDULER_REGISTRY", "\n", "\n", "\n", "def", "get_training_parser", "(", ")", ":", "\n", "    ", "parser", "=", "get_parser", "(", "'Trainer'", ")", "\n", "add_dataset_args", "(", "parser", ",", "train", "=", "True", ")", "\n", "add_distributed_training_args", "(", "parser", ")", "\n", "add_model_args", "(", "parser", ")", "\n", "add_optimization_args", "(", "parser", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.options.get_generation_parser": [[28, 33], ["options.get_parser", "options.add_dataset_args", "options.add_generation_args"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.preprocess.get_parser", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.options.add_dataset_args", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.options.add_generation_args"], ["return", "parser", "\n", "\n", "\n", "", "def", "get_generation_parser", "(", ")", ":", "\n", "    ", "parser", "=", "get_parser", "(", "'Generation'", ")", "\n", "add_dataset_args", "(", "parser", ",", "gen", "=", "True", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.options.parse_args_and_arch": [[35, 67], ["parser.parse_known_args", "parser.add_argument_group", "ARCH_MODEL_REGISTRY[].add_args", "CRITERION_REGISTRY[].add_args", "OPTIMIZER_REGISTRY[].add_args", "LR_SCHEDULER_REGISTRY[].add_args", "parser.parse_args", "list", "map", "parser.parse_args.lr.split"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.add_args", "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.add_args", "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.add_args", "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.add_args"], ["return", "parser", "\n", "\n", "\n", "", "def", "parse_args_and_arch", "(", "parser", ",", "_args", "=", "None", ")", ":", "\n", "# The parser doesn't know about model/criterion/optimizer-specific args, so", "\n", "# we parse twice. First we parse the model/criterion/optimizer, then we", "\n", "# parse a second time after adding the *-specific arguments.", "\n", "    ", "args", ",", "_", "=", "parser", ".", "parse_known_args", "(", "_args", ")", "\n", "\n", "# Add model-specific args to parser.", "\n", "model_specific_group", "=", "parser", ".", "add_argument_group", "(", "\n", "'Model-specific configuration'", ",", "\n", "# Only include attributes which are explicitly given as command-line", "\n", "# arguments or which have default values.", "\n", "argument_default", "=", "argparse", ".", "SUPPRESS", ",", "\n", ")", "\n", "ARCH_MODEL_REGISTRY", "[", "args", ".", "arch", "]", ".", "add_args", "(", "model_specific_group", ")", "\n", "\n", "# Add *-specific args to parser.", "\n", "CRITERION_REGISTRY", "[", "args", ".", "criterion", "]", ".", "add_args", "(", "parser", ")", "\n", "OPTIMIZER_REGISTRY", "[", "args", ".", "optimizer", "]", ".", "add_args", "(", "parser", ")", "\n", "LR_SCHEDULER_REGISTRY", "[", "args", ".", "lr_scheduler", "]", ".", "add_args", "(", "parser", ")", "\n", "\n", "# Parse a second time.", "\n", "args", "=", "parser", ".", "parse_args", "(", "_args", ")", "\n", "\n", "# Post-process args.", "\n", "args", ".", "lr", "=", "list", "(", "map", "(", "float", ",", "args", ".", "lr", ".", "split", "(", "','", ")", ")", ")", "\n", "if", "args", ".", "max_sentences_valid", "is", "None", ":", "\n", "        ", "args", ".", "max_sentences_valid", "=", "args", ".", "max_sentences", "\n", "\n", "# Apply architecture configuration.", "\n", "", "ARCH_CONFIG_REGISTRY", "[", "args", ".", "arch", "]", "(", "args", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.options.get_parser": [[69, 80], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["return", "args", "\n", "\n", "\n", "", "def", "get_parser", "(", "desc", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Facebook AI Research Sequence-to-Sequence Toolkit -- '", "+", "desc", ")", "\n", "parser", ".", "add_argument", "(", "'--no-progress-bar'", ",", "action", "=", "'store_true'", ",", "help", "=", "'disable progress bar'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-interval'", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'log progress every N batches (when progress bar is disabled)'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-format'", ",", "default", "=", "None", ",", "help", "=", "'log format to use'", ",", "\n", "choices", "=", "[", "'json'", ",", "'none'", ",", "'simple'", ",", "'tqdm'", "]", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.options.add_dataset_args": [[82, 118], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["return", "parser", "\n", "\n", "\n", "", "def", "add_dataset_args", "(", "parser", ",", "train", "=", "False", ",", "gen", "=", "False", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Dataset and data loading'", ")", "\n", "group", ".", "add_argument", "(", "'data'", ",", "metavar", "=", "'DIR'", ",", "\n", "help", "=", "'path to data directory'", ")", "\n", "group", ".", "add_argument", "(", "'-s'", ",", "'--source-lang'", ",", "default", "=", "None", ",", "metavar", "=", "'SRC'", ",", "\n", "help", "=", "'source language'", ")", "\n", "group", ".", "add_argument", "(", "'-t'", ",", "'--target-lang'", ",", "default", "=", "None", ",", "metavar", "=", "'TARGET'", ",", "\n", "help", "=", "'target language'", ")", "\n", "# SHASHI #", "\n", "# group.add_argument('--numtopics', default=50, type=int, metavar='NUMTOPICS',", "\n", "#                    help='Number of topics')", "\n", "group", ".", "add_argument", "(", "'--doctopics'", ",", "default", "=", "None", ",", "metavar", "=", "'DOCTOPICS'", ",", "\n", "help", "=", "'LDA Document Topics'", ")", "\n", "# group.add_argument('--wordtopics', default=None, metavar='WORDTOPICS',", "\n", "#                    help='LDA WORD Topics')", "\n", "##########", "\n", "\n", "group", ".", "add_argument", "(", "'--max-source-positions'", ",", "default", "=", "1024", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'max number of tokens in the source sequence'", ")", "\n", "group", ".", "add_argument", "(", "'--max-target-positions'", ",", "default", "=", "1024", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'max number of tokens in the target sequence'", ")", "\n", "group", ".", "add_argument", "(", "'--skip-invalid-size-inputs-valid-test'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Ignore too long or too short lines in valid and test set'", ")", "\n", "group", ".", "add_argument", "(", "'--max-tokens'", ",", "default", "=", "6000", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'maximum number of tokens in a batch'", ")", "\n", "group", ".", "add_argument", "(", "'--max-sentences'", ",", "'--batch-size'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'maximum number of sentences in a batch'", ")", "\n", "if", "train", ":", "\n", "        ", "group", ".", "add_argument", "(", "'--train-subset'", ",", "default", "=", "'train'", ",", "metavar", "=", "'SPLIT'", ",", "\n", "choices", "=", "[", "'train'", ",", "'valid'", ",", "'test'", "]", ",", "\n", "help", "=", "'data subset to use for training (train, valid, test)'", ")", "\n", "group", ".", "add_argument", "(", "'--valid-subset'", ",", "default", "=", "'valid'", ",", "metavar", "=", "'SPLIT'", ",", "\n", "help", "=", "'comma separated list of data subsets to use for validation'", "\n", "' (train, valid, valid1,test, test1)'", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.options.add_distributed_training_args": [[120, 137], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "torch.cuda.device_count"], "function", ["None"], ["help", "=", "'maximum number of sentences in a validation batch'", "\n", "' (defaults to --max-sentences)'", ")", "\n", "", "if", "gen", ":", "\n", "        ", "group", ".", "add_argument", "(", "'--gen-subset'", ",", "default", "=", "'test'", ",", "metavar", "=", "'SPLIT'", ",", "\n", "help", "=", "'data subset to generate (train, valid, test)'", ")", "\n", "group", ".", "add_argument", "(", "'--num-shards'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'shard generation over N shards'", ")", "\n", "group", ".", "add_argument", "(", "'--shard-id'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'ID'", ",", "\n", "help", "=", "'id of the shard to generate (id < num_shards)'", ")", "\n", "", "return", "group", "\n", "\n", "\n", "", "def", "add_distributed_training_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Distributed training'", ")", "\n", "group", ".", "add_argument", "(", "'--distributed-world-size'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "default", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", ",", "\n", "help", "=", "'total number of GPUs across all nodes (default: all visible GPUs)'", ")", "\n", "group", ".", "add_argument", "(", "'--distributed-rank'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.options.add_optimization_args": [[139, 177], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "fairseq.optim.OPTIMIZER_REGISTRY.keys", "fairseq.optim.OPTIMIZER_REGISTRY.keys", "fairseq.optim.lr_scheduler.LR_SCHEDULER_REGISTRY.keys"], "function", ["None"], ["group", ".", "add_argument", "(", "'--distributed-backend'", ",", "default", "=", "'nccl'", ",", "type", "=", "str", ",", "\n", "help", "=", "'distributed backend'", ")", "\n", "group", ".", "add_argument", "(", "'--distributed-init-method'", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "'typically tcp://hostname:port that will be used to '", "\n", "'establish initial connetion'", ")", "\n", "group", ".", "add_argument", "(", "'--distributed-port'", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "'port number (not required if using --distributed-init-method)'", ")", "\n", "group", ".", "add_argument", "(", "'--device-id'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "'which GPU to use (usually configured automatically)'", ")", "\n", "return", "group", "\n", "\n", "\n", "", "def", "add_optimization_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Optimization'", ")", "\n", "group", ".", "add_argument", "(", "'--max-epoch'", ",", "'--me'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'force stop training at specified epoch'", ")", "\n", "group", ".", "add_argument", "(", "'--clip-norm'", ",", "default", "=", "25", ",", "type", "=", "float", ",", "metavar", "=", "'NORM'", ",", "\n", "help", "=", "'clip threshold of gradients'", ")", "\n", "group", ".", "add_argument", "(", "'--sentence-avg'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'normalize gradients by the number of sentences in a batch'", "\n", "' (default is to normalize by number of tokens)'", ")", "\n", "\n", "# Optimizer definitions can be found under fairseq/optim/", "\n", "group", ".", "add_argument", "(", "'--optimizer'", ",", "default", "=", "'nag'", ",", "metavar", "=", "'OPT'", ",", "\n", "choices", "=", "OPTIMIZER_REGISTRY", ".", "keys", "(", ")", ",", "\n", "help", "=", "'optimizer: {} (default: nag)'", ".", "format", "(", "', '", ".", "join", "(", "OPTIMIZER_REGISTRY", ".", "keys", "(", ")", ")", ")", ")", "\n", "group", ".", "add_argument", "(", "'--lr'", ",", "'--learning-rate'", ",", "default", "=", "'0.25'", ",", "metavar", "=", "'LR_1,LR_2,...,LR_N'", ",", "\n", "help", "=", "'learning rate for the first N epochs; all epochs >N using LR_N'", "\n", "' (note: this may be interpreted differently depending on --lr-scheduler)'", ")", "\n", "group", ".", "add_argument", "(", "'--momentum'", ",", "default", "=", "0.99", ",", "type", "=", "float", ",", "metavar", "=", "'M'", ",", "\n", "help", "=", "'momentum factor'", ")", "\n", "group", ".", "add_argument", "(", "'--weight-decay'", ",", "'--wd'", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "metavar", "=", "'WD'", ",", "\n", "help", "=", "'weight decay'", ")", "\n", "\n", "# Learning rate schedulers can be found under fairseq/optim/lr_scheduler/", "\n", "group", ".", "add_argument", "(", "'--lr-scheduler'", ",", "default", "=", "'reduce_lr_on_plateau'", ",", "\n", "help", "=", "'learning rate scheduler: {} (default: reduce_lr_on_plateau)'", ".", "format", "(", "\n", "', '", ".", "join", "(", "LR_SCHEDULER_REGISTRY", ".", "keys", "(", ")", ")", ")", ")", "\n", "group", ".", "add_argument", "(", "'--lr-shrink'", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "metavar", "=", "'LS'", ",", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.options.add_checkpoint_args": [[179, 192], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["group", ".", "add_argument", "(", "'--min-lr'", ",", "default", "=", "1e-5", ",", "type", "=", "float", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'minimum learning rate'", ")", "\n", "\n", "group", ".", "add_argument", "(", "'--sample-without-replacement'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'If bigger than 0, use that number of mini-batches for each epoch,'", "\n", "' where each sample is drawn randomly without replacement from the'", "\n", "' dataset'", ")", "\n", "group", ".", "add_argument", "(", "'--curriculum'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'sort batches by source length for first N epochs'", ")", "\n", "return", "group", "\n", "\n", "\n", "", "def", "add_checkpoint_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Checkpointing'", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.options.add_generation_args": [[194, 232], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["help", "=", "'path to save checkpoints'", ")", "\n", "group", ".", "add_argument", "(", "'--restore-file'", ",", "default", "=", "'checkpoint_last.pt'", ",", "\n", "help", "=", "'filename in save-dir from which to load checkpoint'", ")", "\n", "group", ".", "add_argument", "(", "'--save-interval'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'save a checkpoint every N updates'", ")", "\n", "group", ".", "add_argument", "(", "'--no-save'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'don\\'t save models and checkpoints'", ")", "\n", "group", ".", "add_argument", "(", "'--no-epoch-checkpoints'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'only store last and best checkpoints'", ")", "\n", "return", "group", "\n", "\n", "\n", "", "def", "add_generation_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Generation'", ")", "\n", "group", ".", "add_argument", "(", "'--path'", ",", "metavar", "=", "'FILE'", ",", "required", "=", "True", ",", "action", "=", "'append'", ",", "\n", "help", "=", "'path(s) to model file(s)'", ")", "\n", "group", ".", "add_argument", "(", "'--beam'", ",", "default", "=", "5", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'beam size'", ")", "\n", "group", ".", "add_argument", "(", "'--nbest'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of hypotheses to output'", ")", "\n", "group", ".", "add_argument", "(", "'--max-len-a'", ",", "default", "=", "0", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "(", "'generate sequences of maximum length ax + b, '", "\n", "'where x is the source length'", ")", ")", "\n", "group", ".", "add_argument", "(", "'--max-len-b'", ",", "default", "=", "200", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "(", "'generate sequences of maximum length ax + b, '", "\n", "'where x is the source length'", ")", ")", "\n", "group", ".", "add_argument", "(", "'--remove-bpe'", ",", "nargs", "=", "'?'", ",", "const", "=", "'@@ '", ",", "default", "=", "None", ",", "\n", "help", "=", "'remove BPE tokens before scoring'", ")", "\n", "group", ".", "add_argument", "(", "'--no-early-stop'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "(", "'continue searching even after finalizing k=beam '", "\n", "'hypotheses; this is more correct, but increases '", "\n", "'generation time by 50%%'", ")", ")", "\n", "group", ".", "add_argument", "(", "'--unnormalized'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'compare unnormalized hypothesis scores'", ")", "\n", "group", ".", "add_argument", "(", "'--cpu'", ",", "action", "=", "'store_true'", ",", "help", "=", "'generate on CPU'", ")", "\n", "group", ".", "add_argument", "(", "'--no-beamable-mm'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'don\\'t use BeamableMM in attention layers'", ")", "\n", "group", ".", "add_argument", "(", "'--lenpen'", ",", "default", "=", "1", ",", "type", "=", "float", ",", "\n", "help", "=", "'length penalty: <1.0 favors shorter, >1.0 favors longer sentences'", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.options.add_model_args": [[234, 260], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "fairseq.models.ARCH_MODEL_REGISTRY.keys", "fairseq.criterions.CRITERION_REGISTRY.keys", "fairseq.models.ARCH_MODEL_REGISTRY.keys", "fairseq.criterions.CRITERION_REGISTRY.keys"], "function", ["None"], ["help", "=", "'unknown word penalty: <0 produces more unks, >0 produces fewer'", ")", "\n", "group", ".", "add_argument", "(", "'--replace-unk'", ",", "nargs", "=", "'?'", ",", "const", "=", "True", ",", "default", "=", "None", ",", "\n", "help", "=", "'perform unknown replacement (optionally with alignment dictionary)'", ")", "\n", "group", ".", "add_argument", "(", "'--quiet'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'only print final scores'", ")", "\n", "group", ".", "add_argument", "(", "'--score-reference'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'just score the reference translation'", ")", "\n", "group", ".", "add_argument", "(", "'--prefix-size'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'PS'", ",", "\n", "help", "=", "(", "'initialize generation by target prefix of given length'", ")", ")", "\n", "\n", "# SHASHI: It should be same as embedding used in training, used to map topic vector to same dimension", "\n", "group", ".", "add_argument", "(", "'--encoder-embed-dim'", ",", "default", "=", "512", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "\n", "return", "group", "\n", "\n", "\n", "", "def", "add_model_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Model configuration'", ")", "\n", "\n", "# Model definitions can be found under fairseq/models/", "\n", "#", "\n", "# The model architecture can be specified in several ways.", "\n", "# In increasing order of priority:", "\n", "# 1) model defaults (lowest priority)", "\n", "# 2) --arch argument", "\n", "# 3) --encoder/decoder-* arguments (highest priority)", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.distributed_utils.distributed_init": [[14, 34], ["distributed_utils.suppress_output.print", "args.distributed_init_method.startswith", "torch.distributed.get_rank", "ValueError", "torch.distributed.init_process_group", "torch.distributed.init_process_group", "distributed_utils.suppress_output"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.distributed_utils.suppress_output"], ["def", "distributed_init", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "distributed_world_size", "==", "1", ":", "\n", "        ", "raise", "ValueError", "(", "'Cannot initialize distributed with distributed_world_size=1'", ")", "\n", "\n", "", "print", "(", "'| distributed init (rank {}): {}'", ".", "format", "(", "\n", "args", ".", "distributed_rank", ",", "args", ".", "distributed_init_method", ")", ",", "flush", "=", "True", ")", "\n", "if", "args", ".", "distributed_init_method", ".", "startswith", "(", "'tcp://'", ")", ":", "\n", "        ", "torch", ".", "distributed", ".", "init_process_group", "(", "\n", "backend", "=", "args", ".", "distributed_backend", ",", "init_method", "=", "args", ".", "distributed_init_method", ",", "\n", "world_size", "=", "args", ".", "distributed_world_size", ",", "rank", "=", "args", ".", "distributed_rank", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "distributed", ".", "init_process_group", "(", "\n", "backend", "=", "args", ".", "distributed_backend", ",", "init_method", "=", "args", ".", "distributed_init_method", ",", "\n", "world_size", "=", "args", ".", "distributed_world_size", ")", "\n", "\n", "", "args", ".", "distributed_rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", ")", "\n", "if", "args", ".", "distributed_rank", "!=", "0", ":", "\n", "        ", "suppress_output", "(", ")", "\n", "\n", "", "return", "args", ".", "distributed_rank", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.distributed_utils.suppress_output": [[36, 48], ["kwargs.pop", "builtin_print"], "function", ["None"], ["", "def", "suppress_output", "(", ")", ":", "\n", "    ", "\"\"\"Suppress printing on the current device. Force printing with `force=True`.\"\"\"", "\n", "import", "builtins", "as", "__builtin__", "\n", "builtin_print", "=", "__builtin__", ".", "print", "\n", "\n", "def", "print", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "'force'", "in", "kwargs", ":", "\n", "            ", "force", "=", "kwargs", ".", "pop", "(", "'force'", ")", "\n", "if", "force", ":", "\n", "                ", "builtin_print", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "", "", "", "__builtin__", ".", "print", "=", "print", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.distributed_utils.all_reduce_and_rescale_tensors": [[50, 100], ["tensors[].new().zero_", "torch.distributed.all_reduce", "tensors[].new().zero_.div_", "len", "distributed_utils.all_reduce_and_rescale_tensors.all_reduce_buffer"], "function", ["None"], ["", "def", "all_reduce_and_rescale_tensors", "(", "tensors", ",", "rescale_denom", ",", "buffer_size", "=", "10485760", ")", ":", "\n", "    ", "\"\"\"All-reduce and rescale tensors in chunks of the specified size.\n\n    Args:\n        tensors: list of Tensors to all-reduce\n        rescale_denom: denominator for rescaling summed Tensors\n        buffer_size: all-reduce chunk size in bytes\n    \"\"\"", "\n", "# buffer size is in bytes, determine equiv. # of elements based on data type", "\n", "buffer_t", "=", "tensors", "[", "0", "]", ".", "new", "(", "math", ".", "ceil", "(", "buffer_size", "/", "tensors", "[", "0", "]", ".", "element_size", "(", ")", ")", ")", ".", "zero_", "(", ")", "\n", "buffer", "=", "[", "]", "\n", "\n", "def", "all_reduce_buffer", "(", ")", ":", "\n", "# copy tensors into buffer_t", "\n", "        ", "offset", "=", "0", "\n", "for", "t", "in", "buffer", ":", "\n", "            ", "numel", "=", "t", ".", "numel", "(", ")", "\n", "buffer_t", "[", "offset", ":", "offset", "+", "numel", "]", ".", "copy_", "(", "t", ".", "view", "(", "-", "1", ")", ")", "\n", "offset", "+=", "numel", "\n", "\n", "# all-reduce and rescale", "\n", "", "torch", ".", "distributed", ".", "all_reduce", "(", "buffer_t", "[", ":", "offset", "]", ")", "\n", "buffer_t", ".", "div_", "(", "rescale_denom", ")", "\n", "\n", "# copy all-reduced buffer back into tensors", "\n", "offset", "=", "0", "\n", "for", "t", "in", "buffer", ":", "\n", "            ", "numel", "=", "t", ".", "numel", "(", ")", "\n", "t", ".", "view", "(", "-", "1", ")", ".", "copy_", "(", "buffer_t", "[", "offset", ":", "offset", "+", "numel", "]", ")", "\n", "offset", "+=", "numel", "\n", "\n", "", "", "filled", "=", "0", "\n", "for", "t", "in", "tensors", ":", "\n", "        ", "sz", "=", "t", ".", "numel", "(", ")", "*", "t", ".", "element_size", "(", ")", "\n", "if", "sz", ">", "buffer_size", ":", "\n", "# tensor is bigger than buffer, all-reduce and rescale directly", "\n", "            ", "torch", ".", "distributed", ".", "all_reduce", "(", "t", ")", "\n", "t", ".", "div_", "(", "rescale_denom", ")", "\n", "", "elif", "filled", "+", "sz", ">", "buffer_size", ":", "\n", "# buffer is full, all-reduce and replace buffer with grad", "\n", "            ", "all_reduce_buffer", "(", ")", "\n", "buffer", "=", "[", "t", "]", "\n", "filled", "=", "sz", "\n", "", "else", ":", "\n", "# add tensor to buffer", "\n", "            ", "buffer", ".", "append", "(", "t", ")", "\n", "filled", "+=", "sz", "\n", "\n", "", "", "if", "len", "(", "buffer", ")", ">", "0", ":", "\n", "        ", "all_reduce_buffer", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.distributed_utils.all_gather_list": [[102, 131], ["torch.distributed.get_world_size", "pickle.dumps", "len", "torch.ByteTensor", "torch.distributed.all_gather", "range", "torch.ByteTensor", "len", "ValueError", "list", "in_buffer.cuda", "result.append", "hasattr", "all_gather_list._in_buffer.size", "torch.cuda.ByteTensor", "pickle.loads", "range", "len", "len", "bytes", "out_buffer[].tolist"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator.cuda"], ["", "", "def", "all_gather_list", "(", "data", ",", "max_size", "=", "4096", ")", ":", "\n", "    ", "\"\"\"Gathers arbitrary data from all nodes into a list.\"\"\"", "\n", "world_size", "=", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "if", "not", "hasattr", "(", "all_gather_list", ",", "'_in_buffer'", ")", "or", "max_size", "!=", "all_gather_list", ".", "_in_buffer", ".", "size", "(", ")", ":", "\n", "        ", "all_gather_list", ".", "_in_buffer", "=", "torch", ".", "ByteTensor", "(", "max_size", ")", "\n", "all_gather_list", ".", "_out_buffers", "=", "[", "\n", "torch", ".", "cuda", ".", "ByteTensor", "(", "max_size", ")", "\n", "for", "i", "in", "range", "(", "world_size", ")", "\n", "]", "\n", "", "in_buffer", "=", "all_gather_list", ".", "_in_buffer", "\n", "out_buffers", "=", "all_gather_list", ".", "_out_buffers", "\n", "\n", "enc", "=", "pickle", ".", "dumps", "(", "data", ")", "\n", "if", "len", "(", "enc", ")", ">=", "max_size", ":", "\n", "        ", "raise", "ValueError", "(", "'encoded data exceeds max_size: {}'", ".", "format", "(", "len", "(", "enc", ")", ")", ")", "\n", "", "in_buffer", "[", "0", "]", "=", "len", "(", "enc", ")", "\n", "in_buffer", "[", "1", ":", "len", "(", "enc", ")", "+", "1", "]", "=", "torch", ".", "ByteTensor", "(", "list", "(", "enc", ")", ")", "\n", "\n", "torch", ".", "distributed", ".", "all_gather", "(", "out_buffers", ",", "in_buffer", ".", "cuda", "(", ")", ")", "\n", "\n", "result", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "world_size", ")", ":", "\n", "        ", "out_buffer", "=", "out_buffers", "[", "i", "]", "\n", "size", "=", "out_buffer", "[", "0", "]", "\n", "result", ".", "append", "(", "\n", "pickle", ".", "loads", "(", "bytes", "(", "out_buffer", "[", "1", ":", "size", "+", "1", "]", ".", "tolist", "(", ")", ")", ")", "\n", ")", "\n", "", "return", "result", "\n", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.multiprocessing_pdb.MultiprocessingPdb.__init__": [[24, 26], ["pdb.Pdb.__init__"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "pdb", ".", "Pdb", ".", "__init__", "(", "self", ",", "nosigint", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.multiprocessing_pdb.MultiprocessingPdb._cmdloop": [[27, 37], ["multiprocessing_pdb.MultiprocessingPdb.cmdloop", "os.fdopen"], "methods", ["None"], ["", "def", "_cmdloop", "(", "self", ")", ":", "\n", "        ", "stdin_bak", "=", "sys", ".", "stdin", "\n", "with", "self", ".", "_stdin_lock", ":", "\n", "            ", "try", ":", "\n", "                ", "if", "not", "self", ".", "_stdin", ":", "\n", "                    ", "self", ".", "_stdin", "=", "os", ".", "fdopen", "(", "self", ".", "_stdin_fd", ")", "\n", "", "sys", ".", "stdin", "=", "self", ".", "_stdin", "\n", "self", ".", "cmdloop", "(", ")", "\n", "", "finally", ":", "\n", "                ", "sys", ".", "stdin", "=", "stdin_bak", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.progress_bar.__init__": [[44, 52], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "iterable", ",", "epoch", "=", "None", ",", "prefix", "=", "None", ")", ":", "\n", "        ", "self", ".", "iterable", "=", "iterable", "\n", "self", ".", "epoch", "=", "epoch", "\n", "self", ".", "prefix", "=", "''", "\n", "if", "epoch", "is", "not", "None", ":", "\n", "            ", "self", ".", "prefix", "+=", "'| epoch {:03d}'", ".", "format", "(", "epoch", ")", "\n", "", "if", "prefix", "is", "not", "None", ":", "\n", "            ", "self", ".", "prefix", "+=", "' | {}'", ".", "format", "(", "prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.progress_bar.__enter__": [[53, 55], ["None"], "methods", ["None"], ["", "", "def", "__enter__", "(", "self", ")", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.progress_bar.__exit__": [[56, 58], ["None"], "methods", ["None"], ["", "def", "__exit__", "(", "self", ",", "*", "exc", ")", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.progress_bar.__iter__": [[59, 61], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.progress_bar.log": [[62, 65], ["None"], "methods", ["None"], ["", "def", "log", "(", "self", ",", "stats", ")", ":", "\n", "        ", "\"\"\"Log intermediate stats according to log_interval.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.progress_bar.print": [[66, 69], ["None"], "methods", ["None"], ["", "def", "print", "(", "self", ",", "stats", ")", ":", "\n", "        ", "\"\"\"Print end-of-epoch stats.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.progress_bar._str_commas": [[70, 73], ["stats[].strip", "stats.keys"], "methods", ["None"], ["", "def", "_str_commas", "(", "self", ",", "stats", ")", ":", "\n", "        ", "return", "', '", ".", "join", "(", "key", "+", "'='", "+", "stats", "[", "key", "]", ".", "strip", "(", ")", "\n", "for", "key", "in", "stats", ".", "keys", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.progress_bar._str_pipes": [[74, 77], ["stats[].strip", "stats.keys"], "methods", ["None"], ["", "def", "_str_pipes", "(", "self", ",", "stats", ")", ":", "\n", "        ", "return", "' | '", ".", "join", "(", "key", "+", "' '", "+", "stats", "[", "key", "]", ".", "strip", "(", ")", "\n", "for", "key", "in", "stats", ".", "keys", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.progress_bar._format_stats": [[78, 94], ["collections.OrderedDict", "collections.OrderedDict.keys", "isinstance", "isinstance", "isinstance", "str"], "methods", ["None"], ["", "def", "_format_stats", "(", "self", ",", "stats", ")", ":", "\n", "        ", "postfix", "=", "OrderedDict", "(", "stats", ")", "\n", "# Preprocess stats according to datatype", "\n", "for", "key", "in", "postfix", ".", "keys", "(", ")", ":", "\n", "# Number: limit the length of the string", "\n", "            ", "if", "isinstance", "(", "postfix", "[", "key", "]", ",", "Number", ")", ":", "\n", "                ", "postfix", "[", "key", "]", "=", "'{:g}'", ".", "format", "(", "postfix", "[", "key", "]", ")", "\n", "# Meter: display both current and average value", "\n", "", "elif", "isinstance", "(", "postfix", "[", "key", "]", ",", "AverageMeter", ")", ":", "\n", "                ", "postfix", "[", "key", "]", "=", "'{:.2f} ({:.2f})'", ".", "format", "(", "\n", "postfix", "[", "key", "]", ".", "val", ",", "postfix", "[", "key", "]", ".", "avg", ")", "\n", "# Else for any other type, try to get the string conversion", "\n", "", "elif", "not", "isinstance", "(", "postfix", "[", "key", "]", ",", "str", ")", ":", "\n", "                ", "postfix", "[", "key", "]", "=", "str", "(", "postfix", "[", "key", "]", ")", "\n", "# Else if it's a string, don't need to preprocess anything", "\n", "", "", "return", "postfix", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.json_progress_bar.__init__": [[99, 103], ["progress_bar.progress_bar.__init__"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["def", "__init__", "(", "self", ",", "iterable", ",", "epoch", "=", "None", ",", "prefix", "=", "None", ",", "log_interval", "=", "1000", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "iterable", ",", "epoch", ",", "prefix", ")", "\n", "self", ".", "log_interval", "=", "log_interval", "\n", "self", ".", "stats", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.json_progress_bar.__iter__": [[104, 113], ["float", "enumerate", "len", "progress_bar.json_progress_bar._format_stats", "progress_bar.json_progress_bar.print"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.json_progress_bar._format_stats", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "size", "=", "float", "(", "len", "(", "self", ".", "iterable", ")", ")", "\n", "for", "i", ",", "obj", "in", "enumerate", "(", "self", ".", "iterable", ")", ":", "\n", "            ", "yield", "obj", "\n", "if", "self", ".", "stats", "is", "not", "None", "and", "i", ">", "0", "and", "self", ".", "log_interval", "is", "not", "None", "and", "i", "%", "self", ".", "log_interval", "==", "0", ":", "\n", "                ", "update", "=", "self", ".", "epoch", "+", "float", "(", "i", "/", "size", ")", "if", "self", ".", "epoch", "is", "not", "None", "else", "None", "\n", "stats", "=", "self", ".", "_format_stats", "(", "self", ".", "stats", ",", "epoch", "=", "self", ".", "epoch", ",", "update", "=", "update", ")", "\n", "print", "(", "'sweep_log: '", "+", "json", ".", "dumps", "(", "stats", ")", ",", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.json_progress_bar.log": [[114, 117], ["None"], "methods", ["None"], ["", "", "", "def", "log", "(", "self", ",", "stats", ")", ":", "\n", "        ", "\"\"\"Log intermediate stats according to log_interval.\"\"\"", "\n", "self", ".", "stats", "=", "stats", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.json_progress_bar.print": [[118, 122], ["progress_bar.json_progress_bar._format_stats", "progress_bar.json_progress_bar.print"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.json_progress_bar._format_stats", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print"], ["", "def", "print", "(", "self", ",", "stats", ")", ":", "\n", "        ", "\"\"\"Print end-of-epoch stats.\"\"\"", "\n", "stats", "=", "self", ".", "_format_stats", "(", "self", ".", "stats", ",", "epoch", "=", "self", ".", "epoch", ")", "\n", "print", "(", "\"sweep_log: \"", "+", "json", ".", "dumps", "(", "stats", ")", ",", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.json_progress_bar._format_stats": [[123, 138], ["collections.OrderedDict", "stats.keys", "isinstance"], "methods", ["None"], ["", "def", "_format_stats", "(", "self", ",", "stats", ",", "epoch", "=", "None", ",", "update", "=", "None", ")", ":", "\n", "        ", "postfix", "=", "OrderedDict", "(", ")", "\n", "if", "epoch", "is", "not", "None", ":", "\n", "            ", "postfix", "[", "'epoch'", "]", "=", "epoch", "\n", "", "if", "update", "is", "not", "None", ":", "\n", "            ", "postfix", "[", "'update'", "]", "=", "update", "\n", "# Preprocess stats according to datatype", "\n", "", "for", "key", "in", "stats", ".", "keys", "(", ")", ":", "\n", "# Meter: display both current and average value", "\n", "            ", "if", "isinstance", "(", "stats", "[", "key", "]", ",", "AverageMeter", ")", ":", "\n", "                ", "postfix", "[", "key", "]", "=", "stats", "[", "key", "]", ".", "val", "\n", "postfix", "[", "key", "+", "'_avg'", "]", "=", "stats", "[", "key", "]", ".", "avg", "\n", "", "else", ":", "\n", "                ", "postfix", "[", "key", "]", "=", "stats", "[", "key", "]", "\n", "", "", "return", "postfix", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.noop_progress_bar.__init__": [[143, 145], ["progress_bar.progress_bar.__init__"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["def", "__init__", "(", "self", ",", "iterable", ",", "epoch", "=", "None", ",", "prefix", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "iterable", ",", "epoch", ",", "prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.noop_progress_bar.__iter__": [[146, 149], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "obj", "in", "self", ".", "iterable", ":", "\n", "            ", "yield", "obj", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.noop_progress_bar.log": [[150, 153], ["None"], "methods", ["None"], ["", "", "def", "log", "(", "self", ",", "stats", ")", ":", "\n", "        ", "\"\"\"Log intermediate stats according to log_interval.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.noop_progress_bar.print": [[154, 157], ["None"], "methods", ["None"], ["", "def", "print", "(", "self", ",", "stats", ")", ":", "\n", "        ", "\"\"\"Print end-of-epoch stats.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.simple_progress_bar.__init__": [[162, 166], ["progress_bar.progress_bar.__init__"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["def", "__init__", "(", "self", ",", "iterable", ",", "epoch", "=", "None", ",", "prefix", "=", "None", ",", "log_interval", "=", "1000", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "iterable", ",", "epoch", ",", "prefix", ")", "\n", "self", ".", "log_interval", "=", "log_interval", "\n", "self", ".", "stats", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.simple_progress_bar.__iter__": [[167, 176], ["len", "enumerate", "progress_bar.simple_progress_bar._str_commas", "progress_bar.simple_progress_bar.print"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.progress_bar._str_commas", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "size", "=", "len", "(", "self", ".", "iterable", ")", "\n", "for", "i", ",", "obj", "in", "enumerate", "(", "self", ".", "iterable", ")", ":", "\n", "            ", "yield", "obj", "\n", "if", "self", ".", "stats", "is", "not", "None", "and", "i", ">", "0", "and", "self", ".", "log_interval", "is", "not", "None", "and", "i", "%", "self", ".", "log_interval", "==", "0", ":", "\n", "                ", "postfix", "=", "self", ".", "_str_commas", "(", "self", ".", "stats", ")", "\n", "print", "(", "'{}:  {:5d} / {:d} {}'", ".", "format", "(", "self", ".", "prefix", ",", "i", ",", "size", ",", "postfix", ")", ",", "\n", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.simple_progress_bar.log": [[177, 180], ["progress_bar.simple_progress_bar._format_stats"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.json_progress_bar._format_stats"], ["", "", "", "def", "log", "(", "self", ",", "stats", ")", ":", "\n", "        ", "\"\"\"Log intermediate stats according to log_interval.\"\"\"", "\n", "self", ".", "stats", "=", "self", ".", "_format_stats", "(", "stats", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.simple_progress_bar.print": [[181, 185], ["progress_bar.simple_progress_bar._str_pipes", "progress_bar.simple_progress_bar.print"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.progress_bar._str_pipes", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print"], ["", "def", "print", "(", "self", ",", "stats", ")", ":", "\n", "        ", "\"\"\"Print end-of-epoch stats.\"\"\"", "\n", "postfix", "=", "self", ".", "_str_pipes", "(", "self", ".", "_format_stats", "(", "stats", ")", ")", "\n", "print", "(", "'{} | {}'", ".", "format", "(", "self", ".", "prefix", ",", "postfix", ")", ",", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.__init__": [[190, 193], ["progress_bar.progress_bar.__init__", "tqdm.tqdm.tqdm"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["def", "__init__", "(", "self", ",", "iterable", ",", "epoch", "=", "None", ",", "prefix", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "iterable", ",", "epoch", ",", "prefix", ")", "\n", "self", ".", "tqdm", "=", "tqdm", "(", "iterable", ",", "self", ".", "prefix", ",", "leave", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.__iter__": [[194, 196], ["iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "iter", "(", "self", ".", "tqdm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.log": [[197, 200], ["progress_bar.tqdm_progress_bar.tqdm.set_postfix", "progress_bar.tqdm_progress_bar._format_stats"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.json_progress_bar._format_stats"], ["", "def", "log", "(", "self", ",", "stats", ")", ":", "\n", "        ", "\"\"\"Log intermediate stats according to log_interval.\"\"\"", "\n", "self", ".", "tqdm", ".", "set_postfix", "(", "self", ".", "_format_stats", "(", "stats", ")", ",", "refresh", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print": [[201, 205], ["progress_bar.tqdm_progress_bar._str_pipes", "progress_bar.tqdm_progress_bar.tqdm.write", "progress_bar.tqdm_progress_bar._format_stats"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.progress_bar._str_pipes", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.json_progress_bar._format_stats"], ["", "def", "print", "(", "self", ",", "stats", ")", ":", "\n", "        ", "\"\"\"Print end-of-epoch stats.\"\"\"", "\n", "postfix", "=", "self", ".", "_str_pipes", "(", "self", ".", "_format_stats", "(", "stats", ")", ")", "\n", "self", ".", "tqdm", ".", "write", "(", "'{} | {}'", ".", "format", "(", "self", ".", "tqdm", ".", "desc", ",", "postfix", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.build_progress_bar": [[22, 40], ["progress_bar.json_progress_bar", "sys.stderr.isatty", "progress_bar.noop_progress_bar", "progress_bar.simple_progress_bar", "progress_bar.tqdm_progress_bar", "ValueError"], "function", ["None"], ["def", "build_progress_bar", "(", "args", ",", "iterator", ",", "epoch", "=", "None", ",", "prefix", "=", "None", ",", "default", "=", "'tqdm'", ",", "no_progress_bar", "=", "'none'", ")", ":", "\n", "    ", "if", "args", ".", "log_format", "is", "None", ":", "\n", "        ", "args", ".", "log_format", "=", "no_progress_bar", "if", "args", ".", "no_progress_bar", "else", "default", "\n", "\n", "", "if", "args", ".", "log_format", "==", "'tqdm'", "and", "not", "sys", ".", "stderr", ".", "isatty", "(", ")", ":", "\n", "        ", "args", ".", "log_format", "=", "'simple'", "\n", "\n", "", "if", "args", ".", "log_format", "==", "'json'", ":", "\n", "        ", "bar", "=", "json_progress_bar", "(", "iterator", ",", "epoch", ",", "prefix", ",", "args", ".", "log_interval", ")", "\n", "", "elif", "args", ".", "log_format", "==", "'none'", ":", "\n", "        ", "bar", "=", "noop_progress_bar", "(", "iterator", ",", "epoch", ",", "prefix", ")", "\n", "", "elif", "args", ".", "log_format", "==", "'simple'", ":", "\n", "        ", "bar", "=", "simple_progress_bar", "(", "iterator", ",", "epoch", ",", "prefix", ",", "args", ".", "log_interval", ")", "\n", "", "elif", "args", ".", "log_format", "==", "'tqdm'", ":", "\n", "        ", "bar", "=", "tqdm_progress_bar", "(", "iterator", ",", "epoch", ",", "prefix", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Unknown log format: {}'", ".", "format", "(", "args", ".", "log_format", ")", ")", "\n", "", "return", "bar", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_scorer.SequenceScorer.__init__": [[14, 18], ["models[].dst_dict.pad", "all", "m.dst_dict.pad"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.pad"], ["def", "__init__", "(", "self", ",", "models", ")", ":", "\n", "        ", "self", ".", "models", "=", "models", "\n", "self", ".", "pad", "=", "models", "[", "0", "]", ".", "dst_dict", ".", "pad", "(", ")", "\n", "assert", "all", "(", "m", ".", "dst_dict", ".", "pad", "(", ")", "==", "self", ".", "pad", "for", "m", "in", "self", ".", "models", "[", "1", ":", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_scorer.SequenceScorer.cuda": [[19, 23], ["model.cuda"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator.cuda"], ["", "def", "cuda", "(", "self", ")", ":", "\n", "        ", "for", "model", "in", "self", ".", "models", ":", "\n", "            ", "model", ".", "cuda", "(", ")", "\n", "", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_scorer.SequenceScorer.score_batched_itr": [[24, 51], ["fairseq.utils.make_variable", "sequence_scorer.SequenceScorer.score", "enumerate", "timer.start", "timer.stop", "fairseq.utils.strip_pad", "fairseq.utils.strip_pad.numel", "attn_i.max", "pos_scores_i.sum"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.make_variable", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.score", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.stop", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.strip_pad"], ["", "def", "score_batched_itr", "(", "self", ",", "data_itr", ",", "cuda", "=", "False", ",", "timer", "=", "None", ")", ":", "\n", "        ", "\"\"\"Iterate over a batched dataset and yield scored translations.\"\"\"", "\n", "for", "sample", "in", "data_itr", ":", "\n", "            ", "s", "=", "utils", ".", "make_variable", "(", "sample", ",", "volatile", "=", "True", ",", "cuda", "=", "cuda", ")", "\n", "if", "timer", "is", "not", "None", ":", "\n", "                ", "timer", ".", "start", "(", ")", "\n", "", "pos_scores", ",", "attn", "=", "self", ".", "score", "(", "s", ")", "\n", "if", "timer", "is", "not", "None", ":", "\n", "                ", "timer", ".", "stop", "(", "s", "[", "'ntokens'", "]", ")", "\n", "", "for", "i", ",", "id", "in", "enumerate", "(", "s", "[", "'id'", "]", ".", "data", ")", ":", "\n", "                ", "src", "=", "s", "[", "'net_input'", "]", "[", "'src_tokens'", "]", ".", "data", "[", "i", ",", ":", "]", "\n", "# remove padding from ref", "\n", "ref", "=", "utils", ".", "strip_pad", "(", "s", "[", "'target'", "]", ".", "data", "[", "i", ",", ":", "]", ",", "self", ".", "pad", ")", "\n", "tgt_len", "=", "ref", ".", "numel", "(", ")", "\n", "pos_scores_i", "=", "pos_scores", "[", "i", "]", "[", ":", "tgt_len", "]", "\n", "score_i", "=", "pos_scores_i", ".", "sum", "(", ")", "/", "tgt_len", "\n", "attn_i", "=", "attn", "[", "i", "]", "\n", "_", ",", "alignment", "=", "attn_i", ".", "max", "(", "dim", "=", "0", ")", "\n", "hypos", "=", "[", "{", "\n", "'tokens'", ":", "ref", ",", "\n", "'score'", ":", "score_i", ",", "\n", "'attention'", ":", "attn_i", ",", "\n", "'alignment'", ":", "alignment", ",", "\n", "'positional_scores'", ":", "pos_scores_i", ",", "\n", "}", "]", "\n", "# return results in the same format as SequenceGenerator", "\n", "yield", "id", ",", "src", ",", "ref", ",", "hypos", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_scorer.SequenceScorer.score": [[52, 90], ["avg_probs.gather.gather.div_", "avg_probs.gather.gather.log_", "avg_probs.gather.gather.gather", "len", "avg_attn.div_", "avg_probs.gather.gather.squeeze", "fairseq.utils.maybe_no_grad", "model.eval", "model.encoder", "model.decoder", "model.get_normalized_probs", "avg_probs.gather.gather.add_", "len", "sample[].data.unsqueeze", "avg_attn.add_"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.maybe_no_grad", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_decoder.FairseqDecoder.get_normalized_probs"], ["", "", "", "def", "score", "(", "self", ",", "sample", ")", ":", "\n", "        ", "\"\"\"Score a batch of translations.\"\"\"", "\n", "net_input", "=", "sample", "[", "'net_input'", "]", "\n", "\n", "# compute scores for each model in the ensemble", "\n", "avg_probs", "=", "None", "\n", "avg_attn", "=", "None", "\n", "for", "model", "in", "self", ".", "models", ":", "\n", "            ", "with", "utils", ".", "maybe_no_grad", "(", ")", ":", "\n", "                ", "model", ".", "eval", "(", ")", "\n", "encoder_out", "=", "model", ".", "encoder", "(", "\n", "net_input", "[", "'src_tokens'", "]", ",", "\n", "net_input", "[", "'src_lengths'", "]", ",", "\n", ")", "\n", "decoder_out", ",", "attn", "=", "model", ".", "decoder", "(", "\n", "net_input", "[", "'prev_output_tokens'", "]", ",", "\n", "encoder_out", ",", "\n", ")", "\n", "", "probs", "=", "model", ".", "get_normalized_probs", "(", "decoder_out", ",", "log_probs", "=", "False", ")", ".", "data", "\n", "if", "avg_probs", "is", "None", ":", "\n", "                ", "avg_probs", "=", "probs", "\n", "", "else", ":", "\n", "                ", "avg_probs", ".", "add_", "(", "probs", ")", "\n", "", "if", "attn", "is", "not", "None", ":", "\n", "                ", "attn", "=", "attn", ".", "data", "\n", "if", "avg_attn", "is", "None", ":", "\n", "                    ", "avg_attn", "=", "attn", "\n", "", "else", ":", "\n", "                    ", "avg_attn", ".", "add_", "(", "attn", ")", "\n", "", "", "", "avg_probs", ".", "div_", "(", "len", "(", "self", ".", "models", ")", ")", "\n", "avg_probs", ".", "log_", "(", ")", "\n", "if", "avg_attn", "is", "not", "None", ":", "\n", "            ", "avg_attn", ".", "div_", "(", "len", "(", "self", ".", "models", ")", ")", "\n", "", "avg_probs", "=", "avg_probs", ".", "gather", "(", "\n", "dim", "=", "2", ",", "\n", "index", "=", "sample", "[", "'target'", "]", ".", "data", ".", "unsqueeze", "(", "-", "1", ")", ",", "\n", ")", "\n", "return", "avg_probs", ".", "squeeze", "(", "2", ")", ",", "avg_attn", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.__init__": [[14, 25], ["dictionary.Dictionary.add_symbol", "dictionary.Dictionary.add_symbol", "dictionary.Dictionary.add_symbol", "dictionary.Dictionary.add_symbol", "len"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.add_symbol"], ["def", "__init__", "(", "self", ",", "pad", "=", "'<pad>'", ",", "eos", "=", "'</s>'", ",", "unk", "=", "'<unk>'", ")", ":", "\n", "        ", "self", ".", "unk_word", ",", "self", ".", "pad_word", ",", "self", ".", "eos_word", "=", "unk", ",", "pad", ",", "eos", "\n", "self", ".", "symbols", "=", "[", "]", "\n", "self", ".", "count", "=", "[", "]", "\n", "self", ".", "indices", "=", "{", "}", "\n", "# dictionary indexing starts at 1 for consistency with Lua", "\n", "self", ".", "add_symbol", "(", "'<Lua heritage>'", ")", "\n", "self", ".", "pad_index", "=", "self", ".", "add_symbol", "(", "pad", ")", "\n", "self", ".", "eos_index", "=", "self", ".", "add_symbol", "(", "eos", ")", "\n", "self", ".", "unk_index", "=", "self", ".", "add_symbol", "(", "unk", ")", "\n", "self", ".", "nspecial", "=", "len", "(", "self", ".", "symbols", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.__getitem__": [[26, 30], ["len"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "if", "idx", "<", "len", "(", "self", ".", "symbols", ")", ":", "\n", "            ", "return", "self", ".", "symbols", "[", "idx", "]", "\n", "", "return", "self", ".", "unk_word", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.__len__": [[31, 34], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the number of symbols in the dictionary\"\"\"", "\n", "return", "len", "(", "self", ".", "symbols", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.index": [[35, 40], ["None"], "methods", ["None"], ["", "def", "index", "(", "self", ",", "sym", ")", ":", "\n", "        ", "\"\"\"Returns the index of the specified symbol\"\"\"", "\n", "if", "sym", "in", "self", ".", "indices", ":", "\n", "            ", "return", "self", ".", "indices", "[", "sym", "]", "\n", "", "return", "self", ".", "unk_index", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.string": [[41, 59], ["torch.is_tensor", "sent.replace.replace.replace", "tensor.dim", "dictionary.Dictionary.unk", "dictionary.Dictionary.unk_string", "dictionary.Dictionary.string.token_string"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.unk_string"], ["", "def", "string", "(", "self", ",", "tensor", ",", "bpe_symbol", "=", "None", ",", "escape_unk", "=", "False", ")", ":", "\n", "        ", "\"\"\"Helper for converting a tensor of token indices to a string.\n\n        Can optionally remove BPE symbols or escape <unk> words.\n        \"\"\"", "\n", "if", "torch", ".", "is_tensor", "(", "tensor", ")", "and", "tensor", ".", "dim", "(", ")", "==", "2", ":", "\n", "            ", "return", "'\\n'", ".", "join", "(", "self", ".", "string", "(", "t", ")", "for", "t", "in", "tensor", ")", "\n", "\n", "", "def", "token_string", "(", "i", ")", ":", "\n", "            ", "if", "i", "==", "self", ".", "unk", "(", ")", ":", "\n", "                ", "return", "self", ".", "unk_string", "(", "escape_unk", ")", "\n", "", "else", ":", "\n", "                ", "return", "self", "[", "i", "]", "\n", "\n", "", "", "sent", "=", "' '", ".", "join", "(", "token_string", "(", "i", ")", "for", "i", "in", "tensor", "if", "i", "!=", "self", ".", "eos", "(", ")", ")", "\n", "if", "bpe_symbol", "is", "not", "None", ":", "\n", "            ", "sent", "=", "sent", ".", "replace", "(", "bpe_symbol", ",", "''", ")", "\n", "", "return", "sent", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.unk_string": [[60, 66], ["None"], "methods", ["None"], ["", "def", "unk_string", "(", "self", ",", "escape", "=", "False", ")", ":", "\n", "        ", "\"\"\"Return unknown string, optionally escaped as: <<unk>>\"\"\"", "\n", "if", "escape", ":", "\n", "            ", "return", "'<{}>'", ".", "format", "(", "self", ".", "unk_word", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "unk_word", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.add_symbol": [[67, 79], ["len", "dictionary.Dictionary.symbols.append", "dictionary.Dictionary.count.append"], "methods", ["None"], ["", "", "def", "add_symbol", "(", "self", ",", "word", ",", "n", "=", "1", ")", ":", "\n", "        ", "\"\"\"Adds a word to the dictionary\"\"\"", "\n", "if", "word", "in", "self", ".", "indices", ":", "\n", "            ", "idx", "=", "self", ".", "indices", "[", "word", "]", "\n", "self", ".", "count", "[", "idx", "]", "=", "self", ".", "count", "[", "idx", "]", "+", "n", "\n", "return", "idx", "\n", "", "else", ":", "\n", "            ", "idx", "=", "len", "(", "self", ".", "symbols", ")", "\n", "self", ".", "indices", "[", "word", "]", "=", "idx", "\n", "self", ".", "symbols", ".", "append", "(", "word", ")", "\n", "self", ".", "count", ".", "append", "(", "n", ")", "\n", "return", "idx", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.finalize": [[80, 86], ["zip", "sorted", "zip"], "methods", ["None"], ["", "", "def", "finalize", "(", "self", ")", ":", "\n", "        ", "\"\"\"Sort symbols by frequency in descending order, ignoring special ones.\"\"\"", "\n", "self", ".", "count", ",", "self", ".", "symbols", "=", "zip", "(", "\n", "*", "sorted", "(", "zip", "(", "self", ".", "count", ",", "self", ".", "symbols", ")", ",", "\n", "key", "=", "(", "lambda", "x", ":", "math", ".", "inf", "if", "self", ".", "indices", "[", "x", "[", "1", "]", "]", "<", "self", ".", "nspecial", "else", "x", "[", "0", "]", ")", ",", "\n", "reverse", "=", "True", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.pad": [[88, 91], ["None"], "methods", ["None"], ["", "def", "pad", "(", "self", ")", ":", "\n", "        ", "\"\"\"Helper to get index of pad symbol\"\"\"", "\n", "return", "self", ".", "pad_index", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.eos": [[92, 95], ["None"], "methods", ["None"], ["", "def", "eos", "(", "self", ")", ":", "\n", "        ", "\"\"\"Helper to get index of end-of-sentence symbol\"\"\"", "\n", "return", "self", ".", "eos_index", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.unk": [[96, 99], ["None"], "methods", ["None"], ["", "def", "unk", "(", "self", ")", ":", "\n", "        ", "\"\"\"Helper to get index of unk symbol\"\"\"", "\n", "return", "self", ".", "unk_index", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.load": [[100, 130], ["isinstance", "dictionary.Dictionary", "f.readlines", "line.rfind", "int", "len", "Dictionary.symbols.append", "object.count", "open", "dictionary.Dictionary.load", "Exception"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.load"], ["", "@", "staticmethod", "\n", "def", "load", "(", "f", ")", ":", "\n", "        ", "\"\"\"Loads the dictionary from a text file with the format:\n\n        ```\n        <symbol0> <count0>\n        <symbol1> <count1>\n        ...\n        ```\n        \"\"\"", "\n", "\n", "if", "isinstance", "(", "f", ",", "str", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "with", "open", "(", "f", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "fd", ":", "\n", "                    ", "return", "Dictionary", ".", "load", "(", "fd", ")", "\n", "", "", "except", "FileNotFoundError", "as", "fnfe", ":", "\n", "                ", "raise", "fnfe", "\n", "", "except", "Exception", ":", "\n", "                ", "raise", "Exception", "(", "\"Incorrect encoding detected in {}, please \"", "\n", "\"rebuild the dataset\"", ".", "format", "(", "f", ")", ")", "\n", "\n", "", "", "d", "=", "Dictionary", "(", ")", "\n", "for", "line", "in", "f", ".", "readlines", "(", ")", ":", "\n", "            ", "idx", "=", "line", ".", "rfind", "(", "' '", ")", "\n", "word", "=", "line", "[", ":", "idx", "]", "\n", "count", "=", "int", "(", "line", "[", "idx", "+", "1", ":", "]", ")", "\n", "d", ".", "indices", "[", "word", "]", "=", "len", "(", "d", ".", "symbols", ")", "\n", "d", ".", "symbols", ".", "append", "(", "word", ")", "\n", "d", ".", "count", ".", "append", "(", "count", ")", "\n", "", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.save": [[131, 142], ["isinstance", "enumerate", "zip", "open", "dictionary.Dictionary.save", "print"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.save", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print"], ["", "def", "save", "(", "self", ",", "f", ",", "threshold", "=", "3", ",", "nwords", "=", "-", "1", ")", ":", "\n", "        ", "\"\"\"Stores dictionary into a text file\"\"\"", "\n", "if", "isinstance", "(", "f", ",", "str", ")", ":", "\n", "            ", "with", "open", "(", "f", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "fd", ":", "\n", "                ", "return", "self", ".", "save", "(", "fd", ",", "threshold", ",", "nwords", ")", "\n", "", "", "cnt", "=", "0", "\n", "for", "i", ",", "t", "in", "enumerate", "(", "zip", "(", "self", ".", "symbols", ",", "self", ".", "count", ")", ")", ":", "\n", "            ", "if", "i", ">=", "self", ".", "nspecial", "and", "t", "[", "1", "]", ">=", "threshold", "and", "(", "nwords", "<", "0", "or", "cnt", "<", "nwords", ")", ":", "\n", "                ", "print", "(", "'{} {}'", ".", "format", "(", "t", "[", "0", "]", ",", "t", "[", "1", "]", ")", ",", "file", "=", "f", ")", "\n", "cnt", "+=", "1", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.tokenizer.Tokenizer.build_dictionary": [[27, 33], ["fairseq.dictionary.Dictionary", "tokenizer.Tokenizer.add_file_to_dictionary", "fairseq.dictionary.Dictionary.finalize"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.tokenizer.Tokenizer.add_file_to_dictionary", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDatasetBuilder.finalize"], ["    ", "@", "staticmethod", "\n", "def", "build_dictionary", "(", "filename", ",", "tokenize", "=", "tokenize_line", ")", ":", "\n", "        ", "dict", "=", "dictionary", ".", "Dictionary", "(", ")", "\n", "Tokenizer", ".", "add_file_to_dictionary", "(", "filename", ",", "dict", ",", "tokenize", ")", "\n", "dict", ".", "finalize", "(", ")", "\n", "return", "dict", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.tokenizer.Tokenizer.add_file_to_dictionary": [[34, 41], ["open", "tokenizer.Tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.tokenizer.Tokenizer.tokenize"], ["", "@", "staticmethod", "\n", "def", "add_file_to_dictionary", "(", "filename", ",", "dict", ",", "tokenize", ")", ":", "\n", "        ", "with", "open", "(", "filename", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "for", "word", "in", "tokenize", "(", "line", ")", ":", "\n", "                    ", "dict", ".", "add_symbol", "(", "word", ")", "\n", "", "dict", ".", "add_symbol", "(", "dict", ".", "eos_word", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.tokenizer.Tokenizer.binarize": [[42, 59], ["collections.Counter", "open", "sum", "len", "collections.Counter.update", "tokenizer.Tokenizer.tokenize", "consumer", "len", "collections.Counter.values"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.tokenizer.Tokenizer.tokenize"], ["", "", "", "@", "staticmethod", "\n", "def", "binarize", "(", "filename", ",", "dict", ",", "consumer", ",", "tokenize", "=", "tokenize_line", ")", ":", "\n", "        ", "nseq", ",", "ntok", "=", "0", ",", "0", "\n", "replaced", "=", "Counter", "(", ")", "\n", "\n", "def", "replaced_consumer", "(", "word", ",", "idx", ")", ":", "\n", "            ", "if", "idx", "==", "dict", ".", "unk_index", "and", "word", "!=", "dict", ".", "unk_word", ":", "\n", "                ", "replaced", ".", "update", "(", "[", "word", "]", ")", "\n", "\n", "", "", "with", "open", "(", "filename", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "ids", "=", "Tokenizer", ".", "tokenize", "(", "line", ",", "dict", ",", "tokenize", ",", "add_if_not_exist", "=", "False", ",", "consumer", "=", "replaced_consumer", ")", "\n", "nseq", "+=", "1", "\n", "\n", "consumer", "(", "ids", ")", "\n", "ntok", "+=", "len", "(", "ids", ")", "\n", "", "", "return", "{", "'nseq'", ":", "nseq", ",", "'nunk'", ":", "sum", "(", "replaced", ".", "values", "(", ")", ")", ",", "'ntok'", ":", "ntok", ",", "'replaced'", ":", "len", "(", "replaced", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.tokenizer.Tokenizer.tokenize": [[60, 75], ["tokenizer.Tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.tokenizer.Tokenizer.tokenize"], ["", "@", "staticmethod", "\n", "def", "tokenize", "(", "line", ",", "dict", ",", "tokenize", "=", "tokenize_line", ",", "add_if_not_exist", "=", "True", ",", "consumer", "=", "None", ")", ":", "\n", "        ", "words", "=", "tokenize", "(", "line", ")", "\n", "nwords", "=", "len", "(", "words", ")", "\n", "ids", "=", "torch", ".", "IntTensor", "(", "nwords", "+", "1", ")", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "            ", "if", "add_if_not_exist", ":", "\n", "                ", "idx", "=", "dict", ".", "add_symbol", "(", "word", ")", "\n", "", "else", ":", "\n", "                ", "idx", "=", "dict", ".", "index", "(", "word", ")", "\n", "", "if", "consumer", "is", "not", "None", ":", "\n", "                ", "consumer", "(", "word", ",", "idx", ")", "\n", "", "ids", "[", "i", "]", "=", "idx", "\n", "", "ids", "[", "nwords", "]", "=", "dict", ".", "eos_index", "\n", "return", "ids", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.tokenizer.tokenize_line": [[19, 23], ["SPACE_NORMALIZER.sub", "line.strip.strip", "line.strip.split"], "function", ["None"], ["def", "tokenize_line", "(", "line", ")", ":", "\n", "    ", "line", "=", "SPACE_NORMALIZER", ".", "sub", "(", "\" \"", ",", "line", ")", "\n", "line", "=", "line", ".", "strip", "(", ")", "\n", "return", "line", ".", "split", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.__init__": [[46, 59], ["indexed_dataset.IndexedDataset.read_data", "open", "f.read", "f.read", "struct.unpack", "struct.unpack", "indexed_dataset.read_longs", "indexed_dataset.read_longs", "indexed_dataset.read_longs", "struct.unpack", "f.read", "f.read"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDatasetDOCTOPICS.read_data", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.read_longs", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.read_longs", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.read_longs"], ["    ", "\"\"\"Loader for TorchNet IndexedDataset\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "path", ")", ":", "\n", "        ", "with", "open", "(", "path", "+", "'.idx'", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "magic", "=", "f", ".", "read", "(", "8", ")", "\n", "assert", "magic", "==", "b'TNTIDX\\x00\\x00'", "\n", "version", "=", "f", ".", "read", "(", "8", ")", "\n", "assert", "struct", ".", "unpack", "(", "'<Q'", ",", "version", ")", "==", "(", "1", ",", ")", "\n", "code", ",", "self", ".", "element_size", "=", "struct", ".", "unpack", "(", "'<QQ'", ",", "f", ".", "read", "(", "16", ")", ")", "\n", "self", ".", "dtype", "=", "dtypes", "[", "code", "]", "\n", "self", ".", "size", ",", "self", ".", "s", "=", "struct", ".", "unpack", "(", "'<QQ'", ",", "f", ".", "read", "(", "16", ")", ")", "\n", "self", ".", "dim_offsets", "=", "read_longs", "(", "f", ",", "self", ".", "size", "+", "1", ")", "\n", "self", ".", "data_offsets", "=", "read_longs", "(", "f", ",", "self", ".", "size", "+", "1", ")", "\n", "self", ".", "sizes", "=", "read_longs", "(", "f", ",", "self", ".", "s", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.read_data": [[60, 62], ["open"], "methods", ["None"], ["", "self", ".", "read_data", "(", "path", ")", "\n", "\n", "", "def", "read_data", "(", "self", ",", "path", ")", ":", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.check_index": [[63, 66], ["IndexError"], "methods", ["None"], ["        ", "self", ".", "data_file", "=", "open", "(", "path", "+", "'.bin'", ",", "'rb'", ",", "buffering", "=", "0", ")", "\n", "\n", "", "def", "check_index", "(", "self", ",", "i", ")", ":", "\n", "        ", "if", "i", "<", "0", "or", "i", ">=", "self", ".", "size", ":", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.__del__": [[67, 69], ["indexed_dataset.IndexedDataset.data_file.close"], "methods", ["None"], ["            ", "raise", "IndexError", "(", "'index out of range'", ")", "\n", "\n", "", "", "def", "__del__", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.__getitem__": [[70, 77], ["indexed_dataset.IndexedDataset.check_index", "numpy.empty", "indexed_dataset.IndexedDataset.data_file.seek", "indexed_dataset.IndexedDataset.data_file.readinto", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.check_index"], ["        ", "self", ".", "data_file", ".", "close", "(", ")", "\n", "\n", "", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "self", ".", "check_index", "(", "i", ")", "\n", "tensor_size", "=", "self", ".", "sizes", "[", "self", ".", "dim_offsets", "[", "i", "]", ":", "self", ".", "dim_offsets", "[", "i", "+", "1", "]", "]", "\n", "a", "=", "np", ".", "empty", "(", "tensor_size", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "self", ".", "data_file", ".", "seek", "(", "self", ".", "data_offsets", "[", "i", "]", "*", "self", ".", "element_size", ")", "\n", "self", ".", "data_file", ".", "readinto", "(", "a", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.__len__": [[78, 80], ["None"], "methods", ["None"], ["return", "torch", ".", "from_numpy", "(", "a", ")", "\n", "\n", "", "def", "__len__", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.exists": [[81, 84], ["os.path.exists"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.exists"], ["        ", "return", "self", ".", "size", "\n", "\n", "", "@", "staticmethod", "\n", "def", "exists", "(", "path", ")", ":", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedInMemoryDataset.read_data": [[89, 94], ["open", "numpy.empty", "indexed_dataset.IndexedInMemoryDataset.data_file.readinto", "indexed_dataset.IndexedInMemoryDataset.data_file.close"], "methods", ["None"], ["    ", "\"\"\"Loader for TorchNet IndexedDataset, keeps all the data in memory\"\"\"", "\n", "\n", "def", "read_data", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "data_file", "=", "open", "(", "path", "+", "'.bin'", ",", "'rb'", ")", "\n", "self", ".", "buffer", "=", "np", ".", "empty", "(", "self", ".", "data_offsets", "[", "-", "1", "]", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "self", ".", "data_file", ".", "readinto", "(", "self", ".", "buffer", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedInMemoryDataset.__del__": [[95, 97], ["None"], "methods", ["None"], ["self", ".", "data_file", ".", "close", "(", ")", "\n", "\n", "", "def", "__del__", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedInMemoryDataset.__getitem__": [[98, 104], ["indexed_dataset.IndexedInMemoryDataset.check_index", "numpy.empty", "numpy.copyto", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.check_index"], ["        ", "pass", "\n", "\n", "", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "self", ".", "check_index", "(", "i", ")", "\n", "tensor_size", "=", "self", ".", "sizes", "[", "self", ".", "dim_offsets", "[", "i", "]", ":", "self", ".", "dim_offsets", "[", "i", "+", "1", "]", "]", "\n", "a", "=", "np", ".", "empty", "(", "tensor_size", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "np", ".", "copyto", "(", "a", ",", "self", ".", "buffer", "[", "self", ".", "data_offsets", "[", "i", "]", ":", "self", ".", "data_offsets", "[", "i", "+", "1", "]", "]", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDataset.__init__": [[110, 116], ["indexed_dataset.IndexedRawTextDataset.read_data", "len"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDatasetDOCTOPICS.read_data"], ["\n", "\n", "def", "__init__", "(", "self", ",", "path", ",", "dictionary", ")", ":", "\n", "        ", "self", ".", "tokens_list", "=", "[", "]", "\n", "self", ".", "lines", "=", "[", "]", "\n", "self", ".", "sizes", "=", "[", "]", "\n", "print", "(", "\"Loading \"", ",", "path", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDataset.read_data": [[117, 126], ["numpy.array", "open", "indexed_dataset.IndexedRawTextDataset.lines.append", "indexed_dataset.IndexedRawTextDataset.tokens_list.append", "indexed_dataset.IndexedRawTextDataset.sizes.append", "line.strip", "fairseq.tokenizer.Tokenizer.tokenize", "len"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.tokenizer.Tokenizer.tokenize"], ["self", ".", "read_data", "(", "path", ",", "dictionary", ")", "\n", "print", "(", "\"Done!\"", ")", "\n", "self", ".", "size", "=", "len", "(", "self", ".", "tokens_list", ")", "\n", "\n", "", "def", "read_data", "(", "self", ",", "path", ",", "dictionary", ")", ":", "\n", "        ", "with", "open", "(", "path", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "self", ".", "lines", ".", "append", "(", "line", ".", "strip", "(", "'\\n'", ")", ")", "\n", "# +1 for Lua compatibility", "\n", "tokens", "=", "Tokenizer", ".", "tokenize", "(", "line", ",", "dictionary", ",", "add_if_not_exist", "=", "False", ")", "+", "1", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDataset.__getitem__": [[127, 130], ["indexed_dataset.IndexedRawTextDataset.check_index"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.check_index"], ["self", ".", "tokens_list", ".", "append", "(", "tokens", ")", "\n", "self", ".", "sizes", ".", "append", "(", "len", "(", "tokens", ")", ")", "\n", "", "", "self", ".", "sizes", "=", "np", ".", "array", "(", "self", ".", "sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDataset.get_original_text": [[131, 134], ["indexed_dataset.IndexedRawTextDataset.check_index"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.check_index"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "self", ".", "check_index", "(", "i", ")", "\n", "return", "self", ".", "tokens_list", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDataset.__del__": [[135, 137], ["None"], "methods", ["None"], ["", "def", "get_original_text", "(", "self", ",", "i", ")", ":", "\n", "        ", "self", ".", "check_index", "(", "i", ")", "\n", "return", "self", ".", "lines", "[", "i", "]", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDataset.__len__": [[138, 140], ["None"], "methods", ["None"], ["\n", "", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "pass", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDatasetLEMMA.__init__": [[149, 156], ["print", "indexed_dataset.IndexedRawTextDatasetLEMMA.read_data", "print", "len"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDatasetDOCTOPICS.read_data", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print"], ["def", "__init__", "(", "self", ",", "lemma_path", ")", ":", "\n", "        ", "self", ".", "tokens_list", "=", "[", "]", "\n", "self", ".", "sizes", "=", "[", "]", "\n", "print", "(", "\"Loading \"", ",", "lemma_path", ")", "\n", "self", ".", "read_data", "(", "lemma_path", ")", "\n", "print", "(", "\"Done!\"", ")", "\n", "self", ".", "size", "=", "len", "(", "self", ".", "tokens_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDatasetLEMMA.read_data": [[157, 166], ["numpy.array", "open", "line.split", "line.split.append", "indexed_dataset.IndexedRawTextDatasetLEMMA.tokens_list.append", "indexed_dataset.IndexedRawTextDatasetLEMMA.sizes.append", "len"], "methods", ["None"], ["", "def", "read_data", "(", "self", ",", "path", ")", ":", "\n", "        ", "with", "open", "(", "path", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "tokens", "=", "line", ".", "split", "(", ")", "\n", "# End for end of document", "\n", "tokens", ".", "append", "(", "\"UNK\"", ")", "\n", "self", ".", "tokens_list", ".", "append", "(", "tokens", ")", "\n", "self", ".", "sizes", ".", "append", "(", "len", "(", "tokens", ")", ")", "\n", "", "", "self", ".", "sizes", "=", "np", ".", "array", "(", "self", ".", "sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDatasetLEMMA.__getitem__": [[167, 170], ["indexed_dataset.IndexedRawTextDatasetLEMMA.check_index"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.check_index"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "self", ".", "check_index", "(", "i", ")", "\n", "return", "self", ".", "tokens_list", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDatasetLEMMA.get_original_text": [[171, 174], ["indexed_dataset.IndexedRawTextDatasetLEMMA.check_index"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.check_index"], ["", "def", "get_original_text", "(", "self", ",", "i", ")", ":", "\n", "        ", "self", ".", "check_index", "(", "i", ")", "\n", "return", "\" \"", ".", "join", "(", "self", ".", "tokens_list", "[", "i", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDatasetLEMMA.__del__": [[175, 177], ["None"], "methods", ["None"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDatasetLEMMA.__len__": [[178, 180], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDatasetDOCTOPICS.__init__": [[185, 191], ["print", "indexed_dataset.IndexedRawTextDatasetDOCTOPICS.read_data", "print", "len"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDatasetDOCTOPICS.read_data", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print"], ["def", "__init__", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "doctopics_list", "=", "[", "]", "\n", "print", "(", "\"Loading \"", ",", "path", ")", "\n", "self", ".", "read_data", "(", "path", ")", "\n", "print", "(", "\"Done!\"", ")", "\n", "self", ".", "size", "=", "len", "(", "self", ".", "doctopics_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDatasetDOCTOPICS.read_data": [[192, 208], ["open", "len", "indexed_dataset.IndexedRawTextDatasetDOCTOPICS.doctopics_list.append", "indexed_dataset.IndexedRawTextDatasetDOCTOPICS.doctopics_list.append", "print", "line.strip", "item.split", "float", "line.strip().split", "line.strip"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print"], ["", "def", "read_data", "(", "self", ",", "path", ")", ":", "\n", "        ", "count", "=", "0", "\n", "with", "open", "(", "path", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "if", "len", "(", "line", ".", "strip", "(", ")", ")", "==", "0", ":", "\n", "                    ", "self", ".", "doctopics_list", ".", "append", "(", "[", "]", ")", "\n", "", "else", ":", "\n", "# print(line)", "\n", "                    ", "doctopics", "=", "[", "item", ".", "split", "(", "\":\"", ")", "for", "item", "in", "line", ".", "strip", "(", "'\\n'", ")", ".", "split", "(", "\",\"", ")", "]", "\n", "doctopics", "=", "[", "float", "(", "item", "[", "1", "]", ")", "for", "item", "in", "doctopics", "]", "\n", "self", ".", "doctopics_list", ".", "append", "(", "doctopics", ")", "\n", "# print(doctopics)", "\n", "\n", "", "count", "+=", "1", "\n", "if", "count", "%", "10000", "==", "0", ":", "\n", "                    ", "print", "(", "\"Reading doc topics: \"", ",", "count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDatasetDOCTOPICS.__getitem__": [[210, 213], ["indexed_dataset.IndexedRawTextDatasetDOCTOPICS.check_index"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.check_index"], ["", "", "", "", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "self", ".", "check_index", "(", "i", ")", "\n", "return", "self", ".", "doctopics_list", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDatasetDOCTOPICS.get_original_text": [[214, 217], ["indexed_dataset.IndexedRawTextDatasetDOCTOPICS.check_index"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.check_index"], ["", "def", "get_original_text", "(", "self", ",", "i", ")", ":", "\n", "        ", "self", ".", "check_index", "(", "i", ")", "\n", "return", "self", ".", "doctopics_list", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDatasetDOCTOPICS.__del__": [[218, 220], ["None"], "methods", ["None"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDatasetDOCTOPICS.__len__": [[221, 223], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDatasetBuilder.__init__": [[154, 161], ["open"], "methods", ["None"], ["print", "(", "\"Done!\"", ")", "\n", "self", ".", "size", "=", "len", "(", "self", ".", "tokens_list", ")", "\n", "\n", "", "def", "read_data", "(", "self", ",", "path", ")", ":", "\n", "        ", "with", "open", "(", "path", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "tokens", "=", "line", ".", "split", "(", ")", "\n", "# End for end of document", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDatasetBuilder.add_item": [[162, 169], ["indexed_dataset.IndexedDatasetBuilder.out_file.write", "indexed_dataset.IndexedDatasetBuilder.data_offsets.append", "tensor.size", "indexed_dataset.IndexedDatasetBuilder.dim_offsets.append", "numpy.array", "indexed_dataset.IndexedDatasetBuilder.sizes.append", "len", "tensor.numpy", "tensor.size"], "methods", ["None"], ["tokens", ".", "append", "(", "\"UNK\"", ")", "\n", "self", ".", "tokens_list", ".", "append", "(", "tokens", ")", "\n", "self", ".", "sizes", ".", "append", "(", "len", "(", "tokens", ")", ")", "\n", "", "", "self", ".", "sizes", "=", "np", ".", "array", "(", "self", ".", "sizes", ")", "\n", "\n", "", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "self", ".", "check_index", "(", "i", ")", "\n", "return", "self", ".", "tokens_list", "[", "i", "]", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDatasetBuilder.finalize": [[170, 183], ["indexed_dataset.IndexedDatasetBuilder.out_file.close", "open", "open.write", "open.write", "open.write", "open.write", "indexed_dataset.write_longs", "indexed_dataset.write_longs", "indexed_dataset.write_longs", "open.close", "struct.pack", "struct.pack", "struct.pack", "indexed_dataset.code", "len", "len"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.write_longs", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.write_longs", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.write_longs", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.code"], ["\n", "", "def", "get_original_text", "(", "self", ",", "i", ")", ":", "\n", "        ", "self", ".", "check_index", "(", "i", ")", "\n", "return", "\" \"", ".", "join", "(", "self", ".", "tokens_list", "[", "i", "]", ")", "\n", "\n", "", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "size", "\n", "\n", "", "", "class", "IndexedRawTextDatasetDOCTOPICS", "(", "IndexedDataset", ")", ":", "\n", "    ", "\"\"\"Takes a text file with Doc Topics as input and binarizes it in memory at instantiation.\n    Original lines are also kept in memory\"\"\"", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.read_longs": [[16, 20], ["numpy.empty", "f.readinto"], "function", ["None"], ["\n", "\n", "def", "read_longs", "(", "f", ",", "n", ")", ":", "\n", "    ", "a", "=", "np", ".", "empty", "(", "n", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "f", ".", "readinto", "(", "a", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.write_longs": [[22, 24], ["f.write", "numpy.array"], "function", ["None"], ["\n", "\n", "", "def", "write_longs", "(", "f", ",", "a", ")", ":", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.code": [[37, 41], ["dtypes.keys"], "function", ["None"], ["\n", "\n", "def", "code", "(", "dtype", ")", ":", "\n", "    ", "for", "k", "in", "dtypes", ".", "keys", "(", ")", ":", "\n", "        ", "if", "dtypes", "[", "k", "]", "==", "dtype", ":", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.__init__": [[39, 45], ["bleu.BleuStat", "bleu.Scorer.reset"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.reset"], ["    ", "def", "__init__", "(", "self", ",", "pad", ",", "eos", ",", "unk", ")", ":", "\n", "        ", "self", ".", "stat", "=", "BleuStat", "(", ")", "\n", "self", ".", "pad", "=", "pad", "\n", "self", ".", "eos", "=", "eos", "\n", "self", ".", "unk", "=", "unk", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.reset": [[46, 51], ["C.bleu_one_init", "C.bleu_zero_init", "ctypes.byref", "ctypes.byref"], "methods", ["None"], ["", "def", "reset", "(", "self", ",", "one_init", "=", "False", ")", ":", "\n", "        ", "if", "one_init", ":", "\n", "            ", "C", ".", "bleu_one_init", "(", "ctypes", ".", "byref", "(", "self", ".", "stat", ")", ")", "\n", "", "else", ":", "\n", "            ", "C", ".", "bleu_zero_init", "(", "ctypes", ".", "byref", "(", "self", ".", "stat", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.add": [[52, 75], ["ref.clone", "rref.contiguous().view.contiguous().view.apply_", "rref.contiguous().view.contiguous().view.contiguous().view", "pred.contiguous().view.contiguous().view.contiguous().view", "C.bleu_add", "isinstance", "TypeError", "isinstance", "TypeError", "ctypes.byref", "ctypes.c_size_t", "ctypes.c_void_p", "ctypes.c_size_t", "ctypes.c_void_p", "ctypes.c_int", "ctypes.c_int", "rref.contiguous().view.contiguous().view.contiguous", "pred.contiguous().view.contiguous().view.contiguous", "rref.contiguous().view.contiguous().view.size", "rref.contiguous().view.contiguous().view.data_ptr", "pred.contiguous().view.contiguous().view.size", "pred.contiguous().view.contiguous().view.data_ptr", "type", "type"], "methods", ["None"], ["", "", "def", "add", "(", "self", ",", "ref", ",", "pred", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "ref", ",", "torch", ".", "IntTensor", ")", ":", "\n", "            ", "raise", "TypeError", "(", "'ref must be a torch.IntTensor (got {})'", "\n", ".", "format", "(", "type", "(", "ref", ")", ")", ")", "\n", "", "if", "not", "isinstance", "(", "pred", ",", "torch", ".", "IntTensor", ")", ":", "\n", "            ", "raise", "TypeError", "(", "'pred must be a torch.IntTensor(got {})'", "\n", ".", "format", "(", "type", "(", "pred", ")", ")", ")", "\n", "\n", "", "assert", "self", ".", "unk", ">", "0", ",", "'unknown token index must be >0'", "\n", "rref", "=", "ref", ".", "clone", "(", ")", "\n", "rref", ".", "apply_", "(", "lambda", "x", ":", "x", "if", "x", "!=", "self", ".", "unk", "else", "-", "x", ")", "\n", "\n", "rref", "=", "rref", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "pred", "=", "pred", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "C", ".", "bleu_add", "(", "\n", "ctypes", ".", "byref", "(", "self", ".", "stat", ")", ",", "\n", "ctypes", ".", "c_size_t", "(", "rref", ".", "size", "(", "0", ")", ")", ",", "\n", "ctypes", ".", "c_void_p", "(", "rref", ".", "data_ptr", "(", ")", ")", ",", "\n", "ctypes", ".", "c_size_t", "(", "pred", ".", "size", "(", "0", ")", ")", ",", "\n", "ctypes", ".", "c_void_p", "(", "pred", ".", "data_ptr", "(", ")", ")", ",", "\n", "ctypes", ".", "c_int", "(", "self", ".", "pad", ")", ",", "\n", "ctypes", ".", "c_int", "(", "self", ".", "eos", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.score": [[76, 80], ["sum", "bleu.Scorer.brevity", "math.exp", "math.log", "float", "bleu.Scorer.precision"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.brevity", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.log", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.precision"], ["", "def", "score", "(", "self", ",", "order", "=", "4", ")", ":", "\n", "        ", "psum", "=", "sum", "(", "math", ".", "log", "(", "p", ")", "if", "p", ">", "0", "else", "float", "(", "'-Inf'", ")", "\n", "for", "p", "in", "self", ".", "precision", "(", ")", "[", ":", "order", "]", ")", "\n", "return", "self", ".", "brevity", "(", ")", "*", "math", ".", "exp", "(", "psum", "/", "order", ")", "*", "100", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.precision": [[81, 90], ["bleu.Scorer.precision.ratio"], "methods", ["None"], ["", "def", "precision", "(", "self", ")", ":", "\n", "        ", "def", "ratio", "(", "a", ",", "b", ")", ":", "\n", "            ", "return", "a", "/", "b", "if", "b", ">", "0", "else", "0", "\n", "\n", "", "return", "[", "\n", "ratio", "(", "self", ".", "stat", ".", "match1", ",", "self", ".", "stat", ".", "count1", ")", ",", "\n", "ratio", "(", "self", ".", "stat", ".", "match2", ",", "self", ".", "stat", ".", "count2", ")", ",", "\n", "ratio", "(", "self", ".", "stat", ".", "match3", ",", "self", ".", "stat", ".", "count3", ")", ",", "\n", "ratio", "(", "self", ".", "stat", ".", "match4", ",", "self", ".", "stat", ".", "count4", ")", ",", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.brevity": [[92, 95], ["min", "math.exp"], "methods", ["None"], ["", "def", "brevity", "(", "self", ")", ":", "\n", "        ", "r", "=", "self", ".", "stat", ".", "reflen", "/", "self", ".", "stat", ".", "predlen", "\n", "return", "min", "(", "1", ",", "math", ".", "exp", "(", "1", "-", "r", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.result_string": [[96, 106], ["range", "fmt.format", "bleu.Scorer.score", "bleu.Scorer.brevity", "bleu.Scorer.precision"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.score", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.brevity", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.precision"], ["", "def", "result_string", "(", "self", ",", "order", "=", "4", ")", ":", "\n", "        ", "assert", "order", "<=", "4", ",", "\"BLEU scores for order > 4 aren't supported\"", "\n", "fmt", "=", "'BLEU{} = {:2.2f}, {:2.1f}'", "\n", "for", "_", "in", "range", "(", "1", ",", "order", ")", ":", "\n", "            ", "fmt", "+=", "'/{:2.1f}'", "\n", "", "fmt", "+=", "' (BP={:.3f}, ratio={:.3f}, syslen={}, reflen={})'", "\n", "bleup", "=", "[", "p", "*", "100", "for", "p", "in", "self", ".", "precision", "(", ")", "[", ":", "order", "]", "]", "\n", "return", "fmt", ".", "format", "(", "order", ",", "self", ".", "score", "(", "order", "=", "order", ")", ",", "*", "bleup", ",", "\n", "self", ".", "brevity", "(", ")", ",", "self", ".", "stat", ".", "predlen", "/", "self", ".", "stat", ".", "reflen", ",", "\n", "self", ".", "stat", ".", "predlen", ",", "self", ".", "stat", ".", "reflen", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.__init__": [[29, 60], ["model.cuda", "criterion.cuda", "fairseq.optim.build_optimizer", "fairseq.optim.lr_scheduler.build_lr_scheduler", "collections.OrderedDict", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "fairseq.meters.TimeMeter", "fairseq.meters.TimeMeter", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "torch.cuda.is_available", "NotImplementedError", "trainer.Trainer.model.parameters"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator.cuda", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator.cuda", "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.__init__.build_optimizer", "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.__init__.build_lr_scheduler"], ["\n", "\n", "def", "__init__", "(", "self", ",", "args", ",", "model", ",", "criterion", ")", ":", "\n", "\n", "        ", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Training on CPU is not supported'", ")", "\n", "\n", "", "self", ".", "args", "=", "args", "\n", "\n", "# copy model and criterion to current device", "\n", "self", ".", "model", "=", "model", ".", "cuda", "(", ")", "\n", "self", ".", "criterion", "=", "criterion", ".", "cuda", "(", ")", "\n", "\n", "# initialize optimizer and LR scheduler", "\n", "self", ".", "optimizer", "=", "optim", ".", "build_optimizer", "(", "self", ".", "args", ",", "self", ".", "model", ".", "parameters", "(", ")", ")", "\n", "self", ".", "lr_scheduler", "=", "lr_scheduler", ".", "build_lr_scheduler", "(", "self", ".", "args", ",", "self", ".", "optimizer", ")", "\n", "\n", "# initialize meters", "\n", "self", ".", "meters", "=", "OrderedDict", "(", ")", "\n", "self", ".", "meters", "[", "'train_loss'", "]", "=", "AverageMeter", "(", ")", "\n", "self", ".", "meters", "[", "'train_nll_loss'", "]", "=", "AverageMeter", "(", ")", "\n", "self", ".", "meters", "[", "'valid_loss'", "]", "=", "AverageMeter", "(", ")", "\n", "self", ".", "meters", "[", "'valid_nll_loss'", "]", "=", "AverageMeter", "(", ")", "\n", "self", ".", "meters", "[", "'wps'", "]", "=", "TimeMeter", "(", ")", "# words per second", "\n", "self", ".", "meters", "[", "'ups'", "]", "=", "TimeMeter", "(", ")", "# updates per second", "\n", "self", ".", "meters", "[", "'wpb'", "]", "=", "AverageMeter", "(", ")", "# words per batch", "\n", "self", ".", "meters", "[", "'bsz'", "]", "=", "AverageMeter", "(", ")", "# sentences per batch", "\n", "self", ".", "meters", "[", "'gnorm'", "]", "=", "AverageMeter", "(", ")", "# gradient norm", "\n", "self", ".", "meters", "[", "'clip'", "]", "=", "AverageMeter", "(", ")", "# % of updates clipped", "\n", "self", ".", "meters", "[", "'oom'", "]", "=", "AverageMeter", "(", ")", "# out of memory", "\n", "\n", "self", ".", "_max_bsz_seen", "=", "0", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.save_checkpoint": [[61, 66], ["fairseq.utils.save_state"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.save_state"], ["self", ".", "_num_updates", "=", "0", "\n", "\n", "", "def", "save_checkpoint", "(", "self", ",", "filename", ",", "extra_state", ")", ":", "\n", "        ", "\"\"\"Save all training state in a checkpoint file.\"\"\"", "\n", "if", "self", ".", "args", ".", "distributed_rank", "==", "0", ":", "# only save one checkpoint", "\n", "            ", "utils", ".", "save_state", "(", "filename", ",", "self", ".", "args", ",", "self", ".", "model", ",", "self", ".", "criterion", ",", "self", ".", "optimizer", ",", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.load_checkpoint": [[67, 87], ["fairseq.utils.load_model_state", "fairseq.optim.build_optimizer", "fairseq.optim.lr_scheduler.build_lr_scheduler", "torch.cuda.current_device", "trainer.Trainer.model.parameters", "trainer.Trainer.lr_scheduler.load_state_dict", "trainer.Trainer.optimizer.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.load_model_state", "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.__init__.build_optimizer", "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.__init__.build_lr_scheduler", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.load_state_dict", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.load_state_dict"], ["self", ".", "lr_scheduler", ",", "self", ".", "_num_updates", ",", "self", ".", "_optim_history", ",", "extra_state", ")", "\n", "\n", "", "", "def", "load_checkpoint", "(", "self", ",", "filename", ")", ":", "\n", "        ", "\"\"\"Load all training state from a checkpoint file.\"\"\"", "\n", "extra_state", ",", "self", ".", "_optim_history", ",", "last_optim_state", "=", "utils", ".", "load_model_state", "(", "\n", "filename", ",", "self", ".", "model", ",", "cuda_device", "=", "torch", ".", "cuda", ".", "current_device", "(", ")", ")", "\n", "\n", "if", "last_optim_state", "is", "not", "None", ":", "\n", "# rebuild optimizer after loading model, since params may have changed", "\n", "            ", "self", ".", "optimizer", "=", "optim", ".", "build_optimizer", "(", "self", ".", "args", ",", "self", ".", "model", ".", "parameters", "(", ")", ")", "\n", "self", ".", "lr_scheduler", "=", "lr_scheduler", ".", "build_lr_scheduler", "(", "self", ".", "args", ",", "self", ".", "optimizer", ")", "\n", "\n", "# only reload optimizer and lr_scheduler if they match", "\n", "last_optim", "=", "self", ".", "_optim_history", "[", "-", "1", "]", "\n", "if", "last_optim", "[", "'criterion_name'", "]", "==", "self", ".", "criterion", ".", "__class__", ".", "__name__", ":", "\n", "                ", "self", ".", "lr_scheduler", ".", "load_state_dict", "(", "last_optim", "[", "'lr_scheduler_state'", "]", ")", "\n", "if", "last_optim", "[", "'optimizer_name'", "]", "==", "self", ".", "optimizer", ".", "__class__", ".", "__name__", ":", "\n", "                    ", "self", ".", "optimizer", ".", "load_state_dict", "(", "last_optim_state", ")", "\n", "\n", "", "", "self", ".", "_num_updates", "=", "last_optim", "[", "'num_updates'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.train_step": [[88, 122], ["trainer.Trainer._prepare_sample", "trainer.Trainer._forward", "sum", "sum", "trainer.Trainer.criterion.__class__.grad_denom", "trainer.Trainer.criterion.__class__.aggregate_logging_outputs", "trainer.Trainer._backward_and_opt", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "log.get", "log.get"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer._prepare_sample", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer._forward", "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.fairseq_criterion.FairseqCriterion.grad_denom", "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.aggregate_logging_outputs", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer._backward_and_opt", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update"], ["", "return", "extra_state", "\n", "\n", "", "def", "train_step", "(", "self", ",", "sample", ")", ":", "\n", "        ", "\"\"\"Do forward, backward and parameter update.\"\"\"", "\n", "\n", "sample", "=", "self", ".", "_prepare_sample", "(", "sample", ",", "volatile", "=", "False", ")", "\n", "\n", "# forward pass", "\n", "loss", ",", "sample_sizes", ",", "logging_outputs", ",", "ooms_fwd", "=", "self", ".", "_forward", "(", "sample", ")", "\n", "\n", "# aggregate stats and logging outputs", "\n", "ntokens", "=", "sum", "(", "log", ".", "get", "(", "'ntokens'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "nsentences", "=", "sum", "(", "log", ".", "get", "(", "'nsentences'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "grad_denom", "=", "self", ".", "criterion", ".", "__class__", ".", "grad_denom", "(", "sample_sizes", ")", "\n", "agg_logging_output", "=", "self", ".", "criterion", ".", "__class__", ".", "aggregate_logging_outputs", "(", "logging_outputs", ")", "\n", "\n", "# backward pass, all-reduce gradients and take an optimization step", "\n", "grad_norm", ",", "ooms_bwd", "=", "self", ".", "_backward_and_opt", "(", "loss", ",", "grad_denom", ")", "\n", "\n", "# update meters", "\n", "self", ".", "meters", "[", "'wps'", "]", ".", "update", "(", "ntokens", ")", "\n", "self", ".", "meters", "[", "'ups'", "]", ".", "update", "(", "1.", ")", "\n", "self", ".", "meters", "[", "'wpb'", "]", ".", "update", "(", "ntokens", ")", "\n", "self", ".", "meters", "[", "'bsz'", "]", ".", "update", "(", "nsentences", ")", "\n", "self", ".", "meters", "[", "'gnorm'", "]", ".", "update", "(", "grad_norm", ")", "\n", "self", ".", "meters", "[", "'clip'", "]", ".", "update", "(", "1.", "if", "grad_norm", ">", "self", ".", "args", ".", "clip_norm", "else", "0.", ")", "\n", "self", ".", "meters", "[", "'oom'", "]", ".", "update", "(", "ooms_fwd", "+", "ooms_bwd", ")", "\n", "\n", "# update loss meters for training", "\n", "if", "'loss'", "in", "agg_logging_output", ":", "\n", "            ", "self", ".", "meters", "[", "'train_loss'", "]", ".", "update", "(", "agg_logging_output", "[", "'loss'", "]", ",", "grad_denom", ")", "\n", "# criterions can optionally log the NLL loss too", "\n", "", "if", "'nll_loss'", "in", "agg_logging_output", ":", "\n", "            ", "self", ".", "meters", "[", "'train_nll_loss'", "]", ".", "update", "(", "agg_logging_output", "[", "'nll_loss'", "]", ",", "ntokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer._forward": [[123, 165], ["trainer.Trainer.model.eval", "trainer.Trainer.model.train", "trainer.Trainer.optimizer.zero_grad", "zip", "sum", "sample[].size", "fairseq.utils.maybe_no_grad", "trainer.Trainer.criterion", "logging_output.update", "list", "print", "hasattr", "fairseq.distributed_utils.all_gather_list", "str", "torch.cuda.empty_cache"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.train", "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.zero_grad", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.maybe_no_grad", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.distributed_utils.all_gather_list"], ["", "return", "agg_logging_output", "\n", "\n", "", "def", "_forward", "(", "self", ",", "sample", ",", "eval", "=", "False", ")", ":", "\n", "# prepare model and optimizer", "\n", "        ", "if", "eval", ":", "\n", "            ", "self", ".", "model", ".", "eval", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "model", ".", "train", "(", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "", "loss", "=", "None", "\n", "sample_size", "=", "0", "\n", "logging_output", "=", "{", "\n", "'ntokens'", ":", "sample", "[", "'ntokens'", "]", "if", "sample", "is", "not", "None", "else", "0", ",", "\n", "'nsentences'", ":", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", "if", "sample", "is", "not", "None", "else", "0", ",", "\n", "}", "\n", "oom", "=", "0", "\n", "if", "sample", "is", "not", "None", ":", "\n", "            ", "try", ":", "\n", "                ", "with", "utils", ".", "maybe_no_grad", "(", "eval", ")", ":", "\n", "# calculate loss and sample size", "\n", "                    ", "loss", ",", "sample_size", ",", "logging_output_", "=", "self", ".", "criterion", "(", "self", ".", "model", ",", "sample", ")", "\n", "logging_output", ".", "update", "(", "logging_output_", ")", "\n", "", "", "except", "RuntimeError", "as", "e", ":", "\n", "                ", "if", "not", "eval", "and", "'out of memory'", "in", "str", "(", "e", ")", ":", "\n", "                    ", "print", "(", "'| WARNING: ran out of memory, skipping batch'", ")", "\n", "oom", "=", "1", "\n", "loss", "=", "None", "\n", "if", "hasattr", "(", "torch", ".", "cuda", ",", "'empty_cache'", ")", ":", "\n", "                        ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "", "", "else", ":", "\n", "                    ", "raise", "e", "\n", "\n", "# synchronize logging outputs for multi-GPU training", "\n", "", "", "", "if", "self", ".", "args", ".", "distributed_world_size", ">", "1", ":", "\n", "            ", "sample_sizes", ",", "logging_outputs", ",", "ooms", "=", "zip", "(", "*", "list", "(", "\n", "distributed_utils", ".", "all_gather_list", "(", "(", "sample_size", ",", "logging_output", ",", "oom", ")", ")", ")", ")", "\n", "ooms", "=", "sum", "(", "ooms", ")", "\n", "", "else", ":", "\n", "            ", "sample_sizes", "=", "[", "sample_size", "]", "\n", "logging_outputs", "=", "[", "logging_output", "]", "\n", "ooms", "=", "oom", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer._backward_and_opt": [[166, 205], ["trainer.Trainer.optimizer.step", "trainer.Trainer.lr_scheduler.step_update", "fairseq.distributed_utils.all_reduce_and_rescale_tensors", "trainer.Trainer.model.parameters", "fairseq.utils.item", "math.sqrt", "loss.backward", "torch.nn.utils.clip_grad_norm", "sum", "trainer.Trainer.model.parameters", "p.grad.data.div_", "trainer.Trainer.model.parameters", "str", "print", "hasattr", "trainer.Trainer.optimizer.zero_grad", "torch.cuda.empty_cache", "p.grad.data.norm", "trainer.Trainer.model.parameters"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fixed_schedule.FixedSchedule.step", "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.step_update", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.distributed_utils.all_reduce_and_rescale_tensors", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.item", "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.grad_multiply.GradMultiply.backward", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.zero_grad"], ["", "return", "loss", ",", "sample_sizes", ",", "logging_outputs", ",", "ooms", "\n", "\n", "", "def", "_backward_and_opt", "(", "self", ",", "loss", ",", "grad_denom", ")", ":", "\n", "        ", "oom", "=", "0", "\n", "if", "loss", "is", "not", "None", ":", "\n", "            ", "try", ":", "\n", "# backward pass", "\n", "                ", "loss", ".", "backward", "(", ")", "\n", "", "except", "RuntimeError", "as", "e", ":", "\n", "                ", "if", "'out of memory'", "in", "str", "(", "e", ")", ":", "\n", "                    ", "print", "(", "'| WARNING: ran out of memory, skipping batch'", ")", "\n", "oom", "=", "1", "\n", "if", "hasattr", "(", "torch", ".", "cuda", ",", "'empty_cache'", ")", ":", "\n", "                        ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "e", "\n", "\n", "# all-reduce grads and rescale by grad_denom", "\n", "", "", "", "if", "self", ".", "args", ".", "distributed_world_size", ">", "1", ":", "\n", "            ", "grads", "=", "[", "p", ".", "grad", ".", "data", "for", "p", "in", "self", ".", "model", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", "]", "\n", "distributed_utils", ".", "all_reduce_and_rescale_tensors", "(", "grads", ",", "grad_denom", ")", "\n", "", "else", ":", "\n", "            ", "for", "p", "in", "self", ".", "model", ".", "parameters", "(", ")", ":", "\n", "                ", "if", "p", ".", "requires_grad", ":", "\n", "                    ", "p", ".", "grad", ".", "data", ".", "div_", "(", "grad_denom", ")", "\n", "\n", "# clip grads", "\n", "", "", "", "if", "self", ".", "args", ".", "clip_norm", ">", "0", ":", "\n", "            ", "grad_norm", "=", "utils", ".", "item", "(", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "self", ".", "args", ".", "clip_norm", ")", ")", "\n", "", "else", ":", "\n", "            ", "grad_norm", "=", "math", ".", "sqrt", "(", "sum", "(", "p", ".", "grad", ".", "data", ".", "norm", "(", ")", "**", "2", "for", "p", "in", "self", ".", "model", ".", "parameters", "(", ")", ")", ")", "\n", "\n", "# take an optimization step", "\n", "", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "self", ".", "_num_updates", "+=", "1", "\n", "\n", "# update learning rate", "\n", "self", ".", "lr_scheduler", ".", "step_update", "(", "self", ".", "_num_updates", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.valid_step": [[206, 228], ["trainer.Trainer._prepare_sample", "trainer.Trainer._forward", "sum", "trainer.Trainer.criterion.__class__.grad_denom", "trainer.Trainer.criterion.__class__.aggregate_logging_outputs", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "log.get"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer._prepare_sample", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer._forward", "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.fairseq_criterion.FairseqCriterion.grad_denom", "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.aggregate_logging_outputs", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update"], ["return", "grad_norm", ",", "oom", "\n", "\n", "", "def", "valid_step", "(", "self", ",", "sample", ")", ":", "\n", "        ", "\"\"\"Do forward pass in evaluation mode.\"\"\"", "\n", "\n", "sample", "=", "self", ".", "_prepare_sample", "(", "sample", ",", "volatile", "=", "True", ")", "\n", "\n", "# forward pass", "\n", "loss", ",", "sample_sizes", ",", "logging_outputs", ",", "ooms_fwd", "=", "self", ".", "_forward", "(", "sample", ",", "eval", "=", "True", ")", "\n", "assert", "not", "ooms_fwd", ",", "'Ran out of memory during validation'", "\n", "\n", "# aggregate stats and logging outputs", "\n", "ntokens", "=", "sum", "(", "log", ".", "get", "(", "'ntokens'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "grad_denom", "=", "self", ".", "criterion", ".", "__class__", ".", "grad_denom", "(", "sample_sizes", ")", "\n", "agg_logging_output", "=", "self", ".", "criterion", ".", "__class__", ".", "aggregate_logging_outputs", "(", "logging_outputs", ")", "\n", "\n", "# update loss meters for validation", "\n", "if", "'loss'", "in", "agg_logging_output", ":", "\n", "            ", "self", ".", "meters", "[", "'valid_loss'", "]", ".", "update", "(", "agg_logging_output", "[", "'loss'", "]", ",", "grad_denom", ")", "\n", "# criterions can optionally log the NLL loss too", "\n", "", "if", "'nll_loss'", "in", "agg_logging_output", ":", "\n", "            ", "self", ".", "meters", "[", "'valid_nll_loss'", "]", ".", "update", "(", "agg_logging_output", "[", "'nll_loss'", "]", ",", "ntokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.lr_step": [[229, 232], ["trainer.Trainer.lr_scheduler.step"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fixed_schedule.FixedSchedule.step"], ["", "return", "agg_logging_output", "\n", "\n", "", "def", "lr_step", "(", "self", ",", "epoch", ",", "val_loss", "=", "None", ")", ":", "\n", "        ", "\"\"\"Adjust the learning rate based on the validation loss.\"\"\"", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_lr": [[233, 236], ["trainer.Trainer.optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.get_lr"], ["return", "self", ".", "lr_scheduler", ".", "step", "(", "epoch", ",", "val_loss", ")", "\n", "\n", "", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the current learning rate.\"\"\"", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_model": [[237, 240], ["None"], "methods", ["None"], ["return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "\n", "", "def", "get_model", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the model replica.\"\"\"", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter": [[241, 246], ["None"], "methods", ["None"], ["return", "self", ".", "model", "\n", "\n", "", "def", "get_meter", "(", "self", ",", "name", ")", ":", "\n", "        ", "\"\"\"Get a specific meter by name.\"\"\"", "\n", "if", "name", "not", "in", "self", ".", "meters", ":", "\n", "            ", "return", "None", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_num_updates": [[247, 250], ["None"], "methods", ["None"], ["", "return", "self", ".", "meters", "[", "name", "]", "\n", "\n", "", "def", "get_num_updates", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the number of parameters updates.\"\"\"", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer._prepare_sample": [[251, 260], ["hasattr", "fairseq.utils.make_variable", "len", "sample[].size", "sample[].size", "torch.cuda.empty_cache"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.make_variable"], ["return", "self", ".", "_num_updates", "\n", "\n", "", "def", "_prepare_sample", "(", "self", ",", "sample", ",", "volatile", ")", ":", "\n", "        ", "if", "sample", "is", "None", "or", "len", "(", "sample", ")", "==", "0", ":", "\n", "            ", "return", "None", "\n", "", "if", "hasattr", "(", "torch", ".", "cuda", ",", "'empty_cache'", ")", ":", "\n", "# clear the caching allocator if this is the largest sample we've seen", "\n", "            ", "if", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", ">", "self", ".", "_max_bsz_seen", ":", "\n", "                ", "self", ".", "_max_bsz_seen", "=", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.LanguageDatasets.__init__": [[126, 136], ["data.LanguageDatasets.src_dict.pad", "data.LanguageDatasets.dst_dict.pad", "data.LanguageDatasets.src_dict.eos", "data.LanguageDatasets.dst_dict.eos", "data.LanguageDatasets.src_dict.unk", "data.LanguageDatasets.dst_dict.unk"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.unk"], ["assert", "(", "src", "is", "not", "None", ")", "and", "(", "dst", "is", "not", "None", ")", "and", "(", "doctopic", "is", "not", "None", ")", ",", "'Source language, target language and doc topic should be provided'", "\n", "\n", "src_dict", ",", "dst_dict", "=", "load_dictionaries", "(", "path", ",", "src", ",", "dst", ")", "\n", "src_lemma_topic_dict", "=", "load_src_lemma_topic_dictionaries", "(", "path", ",", "src", ")", "\n", "\n", "dataset", "=", "LanguageDatasets", "(", "src", ",", "dst", ",", "doctopic", ",", "src_dict", ",", "dst_dict", ",", "src_lemma_topic_dict", ")", "\n", "\n", "# Load dataset from raw text files", "\n", "for", "split", "in", "load_splits", ":", "\n", "        ", "src_path", "=", "os", ".", "path", ".", "join", "(", "path", ",", "'{}.{}'", ".", "format", "(", "split", ",", "src", ")", ")", "\n", "dst_path", "=", "os", ".", "path", ".", "join", "(", "path", ",", "'{}.{}'", ".", "format", "(", "split", ",", "dst", ")", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.LanguageDatasets.train_dataloader": [[137, 152], ["torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "data.numpy_seed", "data.shuffled_batches_by_size", "data.mask_batches"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.numpy_seed", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.shuffled_batches_by_size", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.mask_batches"], ["src_lemma_path", "=", "os", ".", "path", ".", "join", "(", "path", ",", "'{}.{}-lemma'", ".", "format", "(", "split", ",", "src", ")", ")", "\n", "doctopic_path", "=", "os", ".", "path", ".", "join", "(", "path", ",", "'{}.{}'", ".", "format", "(", "split", ",", "doctopic", ")", ")", "\n", "\n", "dataset", ".", "splits", "[", "split", "]", "=", "LanguagePairDataset", "(", "\n", "IndexedRawTextDataset", "(", "src_path", ",", "src_dict", ")", ",", "\n", "IndexedRawTextDataset", "(", "dst_path", ",", "dst_dict", ")", ",", "\n", "IndexedRawTextDatasetLEMMA", "(", "src_lemma_path", ")", ",", "\n", "IndexedRawTextDatasetDOCTOPICS", "(", "doctopic_path", ")", ",", "\n", "src_lemma_topic_dict", ",", "\n", "pad_idx", "=", "dataset", ".", "src_dict", ".", "pad", "(", ")", ",", "\n", "eos_idx", "=", "dataset", ".", "src_dict", ".", "eos", "(", ")", ",", "\n", "embed_dim", "=", "embed_dim", ",", "\n", ")", "\n", "\n", "# print(dataset.splits[split].__getitem__(0)) ", "\n", "", "return", "dataset", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.LanguageDatasets.eval_dataloader": [[153, 167], ["data.batches_by_size", "data.mask_batches", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.batches_by_size", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.mask_batches"], ["\n", "\n", "", "class", "LanguageDatasets", "(", "object", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "src", ",", "dst", ",", "doctopic", ",", "src_dict", ",", "dst_dict", ",", "src_lemma_topic_dict", ")", ":", "\n", "        ", "self", ".", "src", "=", "src", "\n", "self", ".", "dst", "=", "dst", "\n", "self", ".", "doctopic", "=", "doctopic", "\n", "\n", "self", ".", "src_dict", "=", "src_dict", "\n", "self", ".", "dst_dict", "=", "dst_dict", "\n", "self", ".", "src_lemma_topic_dict", "=", "src_lemma_topic_dict", "\n", "\n", "self", ".", "splits", "=", "{", "}", "\n", "\n", "assert", "self", ".", "src_dict", ".", "pad", "(", ")", "==", "self", ".", "dst_dict", ".", "pad", "(", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.sharded_iterator.__init__": [[171, 176], ["None"], "methods", ["None"], ["", "def", "train_dataloader", "(", "self", ",", "split", ",", "max_tokens", "=", "None", ",", "\n", "max_sentences", "=", "None", ",", "max_positions", "=", "(", "1024", ",", "1024", ")", ",", "\n", "seed", "=", "None", ",", "epoch", "=", "1", ",", "sample_without_replacement", "=", "0", ",", "\n", "sort_by_source_size", "=", "False", ",", "shard_id", "=", "0", ",", "num_shards", "=", "1", ")", ":", "\n", "        ", "dataset", "=", "self", ".", "splits", "[", "split", "]", "\n", "with", "numpy_seed", "(", "seed", ")", ":", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.sharded_iterator.__len__": [[177, 179], ["len"], "methods", ["None"], ["            ", "batch_sampler", "=", "shuffled_batches_by_size", "(", "\n", "dataset", ".", "src", ",", "dataset", ".", "dst", ",", "dataset", ".", "src_lemma", ",", "dataset", ".", "src_doctopic", ",", "\n", "max_tokens", "=", "max_tokens", ",", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.sharded_iterator.__iter__": [[180, 184], ["enumerate"], "methods", ["None"], ["max_sentences", "=", "max_sentences", ",", "epoch", "=", "epoch", ",", "\n", "sample", "=", "sample_without_replacement", ",", "max_positions", "=", "max_positions", ",", "\n", "sort_by_source_size", "=", "sort_by_source_size", ")", "\n", "batch_sampler", "=", "mask_batches", "(", "batch_sampler", ",", "shard_id", "=", "shard_id", ",", "num_shards", "=", "num_shards", ")", "\n", "", "return", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.LanguagePairDataset.__init__": [[192, 197], ["None"], "methods", ["None"], ["        ", "dataset", "=", "self", ".", "splits", "[", "split", "]", "\n", "batch_sampler", "=", "batches_by_size", "(", "\n", "dataset", ".", "src", ",", "dataset", ".", "dst", ",", "dataset", ".", "src_lemma", ",", "dataset", ".", "src_doctopic", ",", "\n", "max_tokens", ",", "max_sentences", ",", "\n", "max_positions", "=", "max_positions", ",", "\n", "ignore_invalid_inputs", "=", "skip_invalid_size_inputs_valid_test", ",", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.LanguagePairDataset.__getitem__": [[198, 206], ["data.LanguagePairDataset.src[].long", "data.LanguagePairDataset.dst[].long"], "methods", ["None"], ["descending", "=", "descending", ")", "\n", "batch_sampler", "=", "mask_batches", "(", "batch_sampler", ",", "shard_id", "=", "shard_id", ",", "num_shards", "=", "num_shards", ")", "\n", "return", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "num_workers", "=", "num_workers", ",", "collate_fn", "=", "dataset", ".", "collater", ",", "\n", "batch_sampler", "=", "batch_sampler", ")", "\n", "\n", "\n", "", "", "class", "sharded_iterator", "(", "object", ")", ":", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.LanguagePairDataset.__len__": [[207, 209], ["len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "itr", ",", "num_shards", ",", "shard_id", ")", ":", "\n", "        ", "assert", "shard_id", ">=", "0", "and", "shard_id", "<", "num_shards", "\n", "self", ".", "itr", "=", "itr", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.LanguagePairDataset.collater": [[210, 212], ["data.LanguagePairDataset.collate"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.LanguagePairDataset.collate"], ["self", ".", "num_shards", "=", "num_shards", "\n", "self", ".", "shard_id", "=", "shard_id", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.LanguagePairDataset.collate": [[213, 257], ["torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "data.LanguagePairDataset.collate.merge"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "itr", ")", "\n", "\n", "", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "i", ",", "v", "in", "enumerate", "(", "self", ".", "itr", ")", ":", "\n", "            ", "if", "i", "%", "self", ".", "num_shards", "==", "self", ".", "shard_id", ":", "\n", "                ", "yield", "v", "\n", "\n", "\n", "", "", "", "", "class", "LanguagePairDataset", "(", "torch", ".", "utils", ".", "data", ".", "Dataset", ")", ":", "\n", "\n", "# padding constants", "\n", "    ", "LEFT_PAD_SOURCE", "=", "True", "\n", "LEFT_PAD_TARGET", "=", "False", "\n", "\n", "def", "__init__", "(", "self", ",", "src", ",", "dst", ",", "src_lemma", ",", "src_doctopic", ",", "src_lemma_topic_dict", ",", "pad_idx", ",", "eos_idx", ",", "embed_dim", "=", "512", ")", ":", "\n", "        ", "self", ".", "src", "=", "src", "\n", "self", ".", "dst", "=", "dst", "\n", "self", ".", "src_lemma", "=", "src_lemma", "\n", "self", ".", "src_doctopic", "=", "src_doctopic", "\n", "self", ".", "src_lemma_topic_dict", "=", "src_lemma_topic_dict", "\n", "\n", "self", ".", "pad_idx", "=", "pad_idx", "\n", "self", ".", "eos_idx", "=", "eos_idx", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "\n", "", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "# subtract 1 for 0-based indexing", "\n", "        ", "source", "=", "self", ".", "src", "[", "i", "]", ".", "long", "(", ")", "-", "1", "\n", "res", "=", "{", "'id'", ":", "i", ",", "'source'", ":", "source", "}", "\n", "if", "self", ".", "dst", ":", "\n", "            ", "res", "[", "'target'", "]", "=", "self", ".", "dst", "[", "i", "]", ".", "long", "(", ")", "-", "1", "\n", "", "res", "[", "'doctopic'", "]", "=", "self", ".", "src_doctopic", "[", "i", "]", "\n", "res", "[", "'wordtopics'", "]", "=", "[", "self", ".", "src_lemma_topic_dict", "[", "lemma", "]", "for", "lemma", "in", "self", ".", "src_lemma", "[", "i", "]", "]", "\n", "\n", "return", "res", "\n", "\n", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "src", ")", "\n", "\n", "", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "return", "LanguagePairDataset", ".", "collate", "(", "samples", ",", "self", ".", "pad_idx", ",", "self", ".", "eos_idx", ",", "self", ".", "embed_dim", ",", "self", ".", "dst", "is", "not", "None", ")", "\n", "\n", "", "@", "staticmethod", "\n", "def", "collate", "(", "samples", ",", "pad_idx", ",", "eos_idx", ",", "embed_dim", ",", "has_target", "=", "True", ")", ":", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.LanguagePairDataset.collate_tokens": [[259, 279], ["max", "values[].new().fill_", "enumerate", "v.size", "values[].new", "dst.numel", "src.numel", "dst.copy_", "data.LanguagePairDataset.collate_tokens.copy_tensor"], "methods", ["None"], ["            ", "return", "{", "}", "\n", "\n", "", "def", "merge", "(", "key", ",", "left_pad", ",", "move_eos_to_beginning", "=", "False", ")", ":", "\n", "            ", "tokens", "=", "[", "s", "[", "key", "]", "for", "s", "in", "samples", "]", "\n", "doctopic", "=", "None", "\n", "wordtopics", "=", "None", "\n", "if", "key", "==", "\"source\"", ":", "\n", "                ", "doctopic", "=", "[", "s", "[", "'doctopic'", "]", "for", "s", "in", "samples", "]", "\n", "wordtopics", "=", "[", "s", "[", "'wordtopics'", "]", "for", "s", "in", "samples", "]", "\n", "", "return", "LanguagePairDataset", ".", "collate_tokens", "(", "tokens", ",", "doctopic", ",", "wordtopics", ",", "pad_idx", ",", "eos_idx", ",", "embed_dim", ",", "left_pad", ",", "move_eos_to_beginning", ")", "\n", "\n", "", "id", "=", "torch", ".", "LongTensor", "(", "[", "s", "[", "'id'", "]", "for", "s", "in", "samples", "]", ")", "\n", "src_tokens", ",", "src_doctopic", ",", "src_wordtopics", "=", "merge", "(", "'source'", ",", "left_pad", "=", "LanguagePairDataset", ".", "LEFT_PAD_SOURCE", ")", "\n", "# sort by descending source length", "\n", "src_lengths", "=", "torch", ".", "LongTensor", "(", "[", "s", "[", "'source'", "]", ".", "numel", "(", ")", "for", "s", "in", "samples", "]", ")", "\n", "src_lengths", ",", "sort_order", "=", "src_lengths", ".", "sort", "(", "descending", "=", "True", ")", "\n", "id", "=", "id", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "src_tokens", "=", "src_tokens", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "src_doctopic", "=", "src_doctopic", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "src_wordtopics", "=", "src_wordtopics", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.has_binary_files": [[22, 27], ["len", "glob.glob", "os.path.join"], "function", ["None"], ["from", "fairseq", ".", "indexed_dataset", "import", "IndexedDataset", ",", "IndexedInMemoryDataset", ",", "IndexedRawTextDataset", ",", "IndexedRawTextDatasetDOCTOPICS", ",", "IndexedRawTextDatasetLEMMA", "\n", "\n", "\n", "def", "has_binary_files", "(", "data_dir", ",", "splits", ")", ":", "\n", "    ", "for", "split", "in", "splits", ":", "\n", "        ", "if", "len", "(", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'{}.*-*.*.bin'", ".", "format", "(", "split", ")", ")", ")", ")", "<", "2", ":", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.infer_language_pair": [[29, 39], ["os.listdir", "filename.split", "parts[].split"], "function", ["None"], ["", "", "return", "True", "\n", "\n", "\n", "", "def", "infer_language_pair", "(", "path", ",", "splits", ")", ":", "\n", "    ", "\"\"\"Infer language pair from filename: <split>.<lang1>-<lang2>.(...).idx\"\"\"", "\n", "src", ",", "dst", "=", "None", ",", "None", "\n", "for", "filename", "in", "os", ".", "listdir", "(", "path", ")", ":", "\n", "        ", "parts", "=", "filename", ".", "split", "(", "'.'", ")", "\n", "for", "split", "in", "splits", ":", "\n", "            ", "if", "parts", "[", "0", "]", "==", "split", "and", "parts", "[", "-", "1", "]", "==", "'idx'", ":", "\n", "                ", "src", ",", "dst", "=", "parts", "[", "1", "]", ".", "split", "(", "'-'", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.load_dictionaries": [[41, 46], ["fairseq.dictionary.Dictionary.load", "fairseq.dictionary.Dictionary.load", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.load"], ["", "", "", "return", "src", ",", "dst", "\n", "\n", "\n", "", "def", "load_dictionaries", "(", "path", ",", "src_lang", ",", "dst_lang", ")", ":", "\n", "    ", "\"\"\"Load dictionaries for a given language pair.\"\"\"", "\n", "src_dict", "=", "Dictionary", ".", "load", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'dict.{}.txt'", ".", "format", "(", "src_lang", ")", ")", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.load_src_lemma_topic_dictionaries": [[51, 65], ["print", "print", "os.path.join", "open", "os.path.join", "line.split", "float"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print"], ["", "def", "load_src_lemma_topic_dictionaries", "(", "path", ",", "src_lang", ")", ":", "\n", "    ", "\"\"\"\n    SHASHI\n    \"\"\"", "\n", "print", "(", "\"Loading \"", ",", "os", ".", "path", ".", "join", "(", "path", ",", "'dict.{}-lemma.lda.txt'", ".", "format", "(", "src_lang", ")", ")", ")", "\n", "src_lemma_topic_dict", "=", "{", "}", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'dict.{}-lemma.lda.txt'", ".", "format", "(", "src_lang", ")", ")", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "ldata", "=", "line", ".", "split", "(", ")", "\n", "src_lemma_topic_dict", "[", "ldata", "[", "0", "]", "]", "=", "[", "float", "(", "item", ")", "for", "item", "in", "ldata", "[", "1", ":", "]", "]", "\n", "# if len(src_lemma_topic_dict[ldata[0]]) != 512:", "\n", "#     print(\"some problem\")", "\n", "", "", "print", "(", "\"Done!\"", ")", "\n", "return", "src_lemma_topic_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.load_dataset": [[48, 99], ["data.load_dictionaries", "data.LanguageDatasets", "data.load_dataset.all_splits_exist"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.load_dictionaries"], ["return", "src_dict", ",", "dst_dict", "\n", "\n", "\n", "", "def", "load_src_lemma_topic_dictionaries", "(", "path", ",", "src_lang", ")", ":", "\n", "    ", "\"\"\"\n    SHASHI\n    \"\"\"", "\n", "print", "(", "\"Loading \"", ",", "os", ".", "path", ".", "join", "(", "path", ",", "'dict.{}-lemma.lda.txt'", ".", "format", "(", "src_lang", ")", ")", ")", "\n", "src_lemma_topic_dict", "=", "{", "}", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'dict.{}-lemma.lda.txt'", ".", "format", "(", "src_lang", ")", ")", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "ldata", "=", "line", ".", "split", "(", ")", "\n", "src_lemma_topic_dict", "[", "ldata", "[", "0", "]", "]", "=", "[", "float", "(", "item", ")", "for", "item", "in", "ldata", "[", "1", ":", "]", "]", "\n", "# if len(src_lemma_topic_dict[ldata[0]]) != 512:", "\n", "#     print(\"some problem\")", "\n", "", "", "print", "(", "\"Done!\"", ")", "\n", "return", "src_lemma_topic_dict", "\n", "\n", "\n", "", "def", "load_dataset", "(", "path", ",", "load_splits", ",", "src", "=", "None", ",", "dst", "=", "None", ")", ":", "\n", "    ", "\"\"\"Loads specified data splits (e.g., test, train or valid) from the\n    specified folder and check that files exist.\"\"\"", "\n", "if", "src", "is", "None", "and", "dst", "is", "None", ":", "\n", "# find language pair automatically", "\n", "        ", "src", ",", "dst", "=", "infer_language_pair", "(", "path", ",", "load_splits", ")", "\n", "", "assert", "src", "is", "not", "None", "and", "dst", "is", "not", "None", ",", "'Source and target languages should be provided'", "\n", "\n", "src_dict", ",", "dst_dict", "=", "load_dictionaries", "(", "path", ",", "src", ",", "dst", ")", "\n", "dataset", "=", "LanguageDatasets", "(", "src", ",", "dst", ",", "src_dict", ",", "dst_dict", ")", "\n", "\n", "# Load dataset from binary files", "\n", "def", "all_splits_exist", "(", "src", ",", "dst", ",", "lang", ")", ":", "\n", "        ", "for", "split", "in", "load_splits", ":", "\n", "            ", "filename", "=", "'{0}.{1}-{2}.{3}.idx'", ".", "format", "(", "split", ",", "src", ",", "dst", ",", "lang", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "path", ",", "filename", ")", ")", ":", "\n", "                ", "return", "False", "\n", "", "", "return", "True", "\n", "\n", "# infer langcode", "\n", "", "if", "all_splits_exist", "(", "src", ",", "dst", ",", "src", ")", ":", "\n", "        ", "langcode", "=", "'{}-{}'", ".", "format", "(", "src", ",", "dst", ")", "\n", "", "elif", "all_splits_exist", "(", "dst", ",", "src", ",", "src", ")", ":", "\n", "        ", "langcode", "=", "'{}-{}'", ".", "format", "(", "dst", ",", "src", ")", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "'Dataset cannot be loaded from path: '", "+", "path", ")", "\n", "\n", "", "def", "fmt_path", "(", "fmt", ",", "*", "args", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "path", ",", "fmt", ".", "format", "(", "*", "args", ")", ")", "\n", "\n", "", "for", "split", "in", "load_splits", ":", "\n", "        ", "for", "k", "in", "itertools", ".", "count", "(", ")", ":", "\n", "            ", "prefix", "=", "\"{}{}\"", ".", "format", "(", "split", ",", "k", "if", "k", ">", "0", "else", "''", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.load_raw_text_dataset": [[101, 123], ["data.load_dictionaries", "data.LanguageDatasets", "data.infer_language_pair", "os.path.join", "os.path.join", "data.LanguagePairDataset", "fairseq.indexed_dataset.IndexedRawTextDataset", "fairseq.indexed_dataset.IndexedRawTextDataset", "LanguageDatasets.src_dict.pad", "LanguageDatasets.src_dict.eos"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.load_dictionaries", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.infer_language_pair", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.eos"], ["dst_path", "=", "fmt_path", "(", "'{}.{}.{}'", ",", "prefix", ",", "langcode", ",", "dst", ")", "\n", "\n", "if", "not", "IndexedInMemoryDataset", ".", "exists", "(", "src_path", ")", ":", "\n", "                ", "break", "\n", "\n", "", "target_dataset", "=", "None", "\n", "if", "IndexedInMemoryDataset", ".", "exists", "(", "dst_path", ")", ":", "\n", "                ", "target_dataset", "=", "IndexedInMemoryDataset", "(", "dst_path", ")", "\n", "\n", "", "dataset", ".", "splits", "[", "prefix", "]", "=", "LanguagePairDataset", "(", "\n", "IndexedInMemoryDataset", "(", "src_path", ")", ",", "\n", "target_dataset", ",", "\n", "pad_idx", "=", "dataset", ".", "src_dict", ".", "pad", "(", ")", ",", "\n", "eos_idx", "=", "dataset", ".", "src_dict", ".", "eos", "(", ")", ",", "\n", ")", "\n", "\n", "", "", "return", "dataset", "\n", "\n", "\n", "", "def", "load_raw_text_dataset", "(", "path", ",", "load_splits", ",", "src", "=", "None", ",", "dst", "=", "None", ",", "doctopic", "=", "None", ",", "embed_dim", "=", "512", ")", ":", "\n", "    ", "\"\"\"Loads specified data splits (e.g., test, train or valid) from raw text\n    files in the specified folder.\"\"\"", "\n", "# if src is None and dst is None or doctopic is None:", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data._valid_size": [[281, 291], ["isinstance"], "function", ["None"], ["target", "=", "None", "\n", "ntokens", "=", "None", "\n", "if", "has_target", ":", "\n", "            ", "target", ",", "_", ",", "_", "=", "merge", "(", "'target'", ",", "left_pad", "=", "LanguagePairDataset", ".", "LEFT_PAD_TARGET", ")", "\n", "# we create a shifted version of targets for feeding the", "\n", "# previous output token(s) into the next decoder step", "\n", "prev_output_tokens", ",", "_", ",", "_", "=", "merge", "(", "\n", "'target'", ",", "\n", "left_pad", "=", "LanguagePairDataset", ".", "LEFT_PAD_TARGET", ",", "\n", "move_eos_to_beginning", "=", "True", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data._make_batches": [[293, 338], ["map", "max", "data._make_batches.yield_batch"], "function", ["None"], ["target", "=", "target", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "ntokens", "=", "sum", "(", "len", "(", "s", "[", "'target'", "]", ")", "for", "s", "in", "samples", ")", "\n", "\n", "", "return", "{", "\n", "'id'", ":", "id", ",", "\n", "'ntokens'", ":", "ntokens", ",", "\n", "'net_input'", ":", "{", "\n", "'src_tokens'", ":", "src_tokens", ",", "\n", "'src_lengths'", ":", "src_lengths", ",", "\n", "'src_doctopic'", ":", "src_doctopic", ",", "\n", "'src_wordtopics'", ":", "src_wordtopics", ",", "\n", "'prev_output_tokens'", ":", "prev_output_tokens", ",", "\n", "}", ",", "\n", "'target'", ":", "target", ",", "\n", "}", "\n", "\n", "", "@", "staticmethod", "\n", "def", "collate_tokens", "(", "values", ",", "values_doctopic", ",", "values_wordtopics", ",", "pad_idx", ",", "eos_idx", ",", "embed_dim", ",", "left_pad", ",", "move_eos_to_beginning", "=", "False", ")", ":", "\n", "        ", "size", "=", "max", "(", "v", ".", "size", "(", "0", ")", "for", "v", "in", "values", ")", "\n", "res", "=", "values", "[", "0", "]", ".", "new", "(", "len", "(", "values", ")", ",", "size", ")", ".", "fill_", "(", "pad_idx", ")", "\n", "res_doctopic", "=", "None", "\n", "res_wordtopics", "=", "None", "\n", "if", "values_doctopic", "and", "values_wordtopics", ":", "\n", "            ", "tmp_values_doctopic", "=", "[", "]", "\n", "for", "doctopic", "in", "values_doctopic", ":", "\n", "                ", "tmp_tensor", "=", "torch", ".", "FloatTensor", "(", "doctopic", ")", "\n", "tmp_values_doctopic", ".", "append", "(", "tmp_tensor", ")", "\n", "", "tmp_values_wordtopics", "=", "[", "]", "\n", "for", "wordtopics", "in", "values_wordtopics", ":", "\n", "                ", "tmp_tensor", "=", "torch", ".", "FloatTensor", "(", "wordtopics", ")", "\n", "tmp_values_wordtopics", ".", "append", "(", "tmp_tensor", ")", "\n", "", "res_doctopic", "=", "tmp_values_wordtopics", "[", "0", "]", ".", "new", "(", "len", "(", "values", ")", ",", "embed_dim", ")", ".", "fill_", "(", "0.0", ")", "\n", "res_wordtopics", "=", "tmp_values_wordtopics", "[", "0", "]", ".", "new", "(", "len", "(", "values", ")", ",", "size", ",", "embed_dim", ")", ".", "fill_", "(", "0.0", ")", "\n", "\n", "# print(values[0], len(values[0]))", "\n", "# # print(values_doctopic[0])", "\n", "# print(tmp_values_doctopic[0])", "\n", "# # print(values_wordtopics[0])", "\n", "# print(tmp_values_wordtopics[0])", "\n", "# print(\"Shashi\")", "\n", "\n", "", "def", "copy_tensor", "(", "src", ",", "dst", ")", ":", "\n", "            ", "assert", "dst", ".", "numel", "(", ")", "==", "src", ".", "numel", "(", ")", "\n", "if", "move_eos_to_beginning", ":", "\n", "                ", "assert", "src", "[", "-", "1", "]", "==", "eos_idx", "\n", "dst", "[", "0", "]", "=", "eos_idx", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.batches_by_size": [[340, 356], ["numpy.argsort", "list", "isinstance", "float", "float", "numpy.flip", "data._make_batches", "isinstance"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data._make_batches"], ["", "else", ":", "\n", "                ", "dst", ".", "copy_", "(", "src", ")", "\n", "\n", "", "", "def", "copy_tensor_srconly", "(", "src", ",", "dst", ")", ":", "\n", "            ", "assert", "dst", ".", "numel", "(", ")", "==", "src", ".", "numel", "(", ")", "\n", "dst", ".", "copy_", "(", "src", ")", "\n", "\n", "", "for", "i", ",", "v", "in", "enumerate", "(", "values", ")", ":", "\n", "            ", "if", "left_pad", ":", "\n", "                ", "copy_tensor", "(", "v", ",", "res", "[", "i", "]", "[", "size", "-", "len", "(", "v", ")", ":", "]", ")", "\n", "if", "values_doctopic", "and", "values_wordtopics", ":", "\n", "# Source only", "\n", "                    ", "copy_tensor_srconly", "(", "tmp_values_doctopic", "[", "i", "]", ",", "res_doctopic", "[", "i", "]", ")", "\n", "copy_tensor_srconly", "(", "tmp_values_wordtopics", "[", "i", "]", ",", "res_wordtopics", "[", "i", "]", "[", "size", "-", "len", "(", "tmp_values_wordtopics", "[", "i", "]", ")", ":", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "copy_tensor", "(", "v", ",", "res", "[", "i", "]", "[", ":", "len", "(", "v", ")", "]", ")", "\n", "if", "values_doctopic", "and", "values_wordtopics", ":", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.shuffled_batches_by_size": [[358, 399], ["numpy.random.permutation", "list", "isinstance", "isinstance", "float", "float", "len", "data._make_batches", "numpy.random.shuffle", "numpy.argsort", "numpy.argsort", "len", "numpy.random.shuffle", "len", "len", "numpy.random.shuffle", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data._make_batches"], ["                    ", "copy_tensor_srconly", "(", "tmp_values_doctopic", "[", "i", "]", ",", "res_doctopic", "[", "i", "]", ")", "\n", "copy_tensor_srconly", "(", "tmp_values_wordtopics", "[", "i", "]", ",", "res_wordtopics", "[", "i", "]", "[", ":", "len", "(", "tmp_values_wordtopics", "[", "i", "]", ")", "]", ")", "\n", "", "", "", "return", "res", ",", "res_doctopic", ",", "res_wordtopics", "\n", "\n", "\n", "", "", "def", "_valid_size", "(", "src_size", ",", "dst_size", ",", "max_positions", ")", ":", "\n", "    ", "if", "isinstance", "(", "max_positions", ",", "numbers", ".", "Number", ")", ":", "\n", "        ", "max_src_positions", ",", "max_dst_positions", "=", "max_positions", ",", "max_positions", "\n", "", "else", ":", "\n", "        ", "max_src_positions", ",", "max_dst_positions", "=", "max_positions", "\n", "", "if", "src_size", "<", "2", "or", "src_size", ">", "max_src_positions", ":", "\n", "        ", "return", "False", "\n", "", "if", "dst_size", "is", "not", "None", "and", "(", "dst_size", "<", "2", "or", "dst_size", ">", "max_dst_positions", ")", ":", "\n", "        ", "return", "False", "\n", "", "return", "True", "\n", "\n", "\n", "", "def", "_make_batches", "(", "src", ",", "dst", ",", "src_lemma", ",", "src_doctopic", ",", "\n", "indices", ",", "max_tokens", ",", "max_sentences", ",", "max_positions", ",", "\n", "ignore_invalid_inputs", "=", "False", ",", "allow_different_src_lens", "=", "False", ")", ":", "\n", "    ", "batch", "=", "[", "]", "\n", "\n", "def", "yield_batch", "(", "next_idx", ",", "num_tokens", ")", ":", "\n", "        ", "if", "len", "(", "batch", ")", "==", "0", ":", "\n", "            ", "return", "False", "\n", "", "if", "len", "(", "batch", ")", "==", "max_sentences", ":", "\n", "            ", "return", "True", "\n", "", "if", "num_tokens", ">", "max_tokens", ":", "\n", "            ", "return", "True", "\n", "", "if", "not", "allow_different_src_lens", "and", "(", "src", ".", "sizes", "[", "batch", "[", "0", "]", "]", "!=", "src", ".", "sizes", "[", "next_idx", "]", ")", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "sample_len", "=", "0", "\n", "ignored", "=", "[", "]", "\n", "for", "idx", "in", "map", "(", "int", ",", "indices", ")", ":", "\n", "        ", "src_size", "=", "src", ".", "sizes", "[", "idx", "]", "\n", "dst_size", "=", "dst", ".", "sizes", "[", "idx", "]", "if", "dst", "else", "src_size", "\n", "if", "not", "_valid_size", "(", "src_size", ",", "dst_size", ",", "max_positions", ")", ":", "\n", "            ", "if", "ignore_invalid_inputs", ":", "\n", "                ", "ignored", ".", "append", "(", "idx", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.mask_batches": [[401, 411], ["int", "math.ceil", "enumerate", "len", "len"], "function", ["None"], ["", "raise", "Exception", "(", "(", "\n", "\"Sample #{} has size (src={}, dst={}) but max size is {}.\"", "\n", "\" Skip this example with --skip-invalid-size-inputs-valid-test\"", "\n", ")", ".", "format", "(", "idx", ",", "src_size", ",", "dst_size", ",", "max_positions", ")", ")", "\n", "\n", "", "sample_len", "=", "max", "(", "sample_len", ",", "src_size", ",", "dst_size", ")", "\n", "num_tokens", "=", "(", "len", "(", "batch", ")", "+", "1", ")", "*", "sample_len", "\n", "if", "yield_batch", "(", "idx", ",", "num_tokens", ")", ":", "\n", "            ", "yield", "batch", "\n", "batch", "=", "[", "]", "\n", "sample_len", "=", "max", "(", "src_size", ",", "dst_size", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.numpy_seed": [[413, 426], ["numpy.random.get_state", "numpy.random.seed", "numpy.random.set_state"], "function", ["None"], ["", "batch", ".", "append", "(", "idx", ")", "\n", "\n", "", "if", "len", "(", "batch", ")", ">", "0", ":", "\n", "        ", "yield", "batch", "\n", "\n", "", "if", "len", "(", "ignored", ")", ">", "0", ":", "\n", "        ", "print", "(", "\"Warning! {} samples are either too short or too long \"", "\n", "\"and will be ignored, first few sample ids={}\"", ".", "format", "(", "len", "(", "ignored", ")", ",", "ignored", "[", ":", "10", "]", ")", ")", "\n", "\n", "\n", "", "", "def", "batches_by_size", "(", "src", ",", "dst", ",", "src_lemma", ",", "src_doctopic", ",", "\n", "max_tokens", "=", "None", ",", "max_sentences", "=", "None", ",", "\n", "max_positions", "=", "(", "1024", ",", "1024", ")", ",", "ignore_invalid_inputs", "=", "False", ",", "\n", "descending", "=", "False", ")", ":", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.AverageMeter.__init__": [[13, 15], ["meters.AverageMeter.reset"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.reset"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.AverageMeter.reset": [[16, 21], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "val", "=", "0", "\n", "self", ".", "avg", "=", "0", "\n", "self", ".", "sum", "=", "0", "\n", "self", ".", "count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.AverageMeter.update": [[22, 27], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "val", ",", "n", "=", "1", ")", ":", "\n", "        ", "self", ".", "val", "=", "val", "\n", "self", ".", "sum", "+=", "val", "*", "n", "\n", "self", ".", "count", "+=", "n", "\n", "self", ".", "avg", "=", "self", ".", "sum", "/", "self", ".", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.__init__": [[31, 33], ["meters.TimeMeter.reset"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.reset"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.reset": [[34, 37], ["time.time"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "start", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "n", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update": [[38, 40], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "val", "=", "1", ")", ":", "\n", "        ", "self", ".", "n", "+=", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.avg": [[41, 45], ["time.time"], "methods", ["None"], ["", "@", "property", "\n", "def", "avg", "(", "self", ")", ":", "\n", "        ", "delta", "=", "time", ".", "time", "(", ")", "-", "self", ".", "start", "\n", "return", "self", ".", "n", "/", "delta", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.elapsed_time": [[46, 49], ["time.time"], "methods", ["None"], ["", "@", "property", "\n", "def", "elapsed_time", "(", "self", ")", ":", "\n", "        ", "return", "time", ".", "time", "(", ")", "-", "self", ".", "start", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.__init__": [[53, 55], ["meters.StopwatchMeter.reset"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.reset"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.start": [[56, 58], ["time.time"], "methods", ["None"], ["", "def", "start", "(", "self", ")", ":", "\n", "        ", "self", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.stop": [[59, 65], ["time.time"], "methods", ["None"], ["", "def", "stop", "(", "self", ",", "n", "=", "1", ")", ":", "\n", "        ", "if", "self", ".", "start_time", "is", "not", "None", ":", "\n", "            ", "delta", "=", "time", ".", "time", "(", ")", "-", "self", ".", "start_time", "\n", "self", ".", "sum", "+=", "delta", "\n", "self", ".", "n", "+=", "n", "\n", "self", ".", "start_time", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.reset": [[66, 70], ["None"], "methods", ["None"], ["", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "sum", "=", "0", "\n", "self", ".", "n", "=", "0", "\n", "self", ".", "start_time", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.avg": [[71, 74], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "avg", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "sum", "/", "self", ".", "n", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator.__init__": [[16, 46], ["models[].dst_dict.pad", "models[].dst_dict.unk", "models[].dst_dict.eos", "all", "all", "all", "len", "min", "min", "m.max_decoder_positions", "m.dst_dict.pad", "m.dst_dict.unk", "m.dst_dict.eos"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.max_decoder_positions", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.eos"], ["    ", "def", "__init__", "(", "self", ",", "models", ",", "beam_size", "=", "1", ",", "minlen", "=", "1", ",", "maxlen", "=", "None", ",", "\n", "stop_early", "=", "True", ",", "normalize_scores", "=", "True", ",", "len_penalty", "=", "1", ",", "\n", "unk_penalty", "=", "0", ",", "retain_dropout", "=", "False", ")", ":", "\n", "        ", "\"\"\"Generates translations of a given source sentence.\n\n        Args:\n            min/maxlen: The length of the generated output will be bounded by\n                minlen and maxlen (not including the end-of-sentence marker).\n            stop_early: Stop generation immediately after we finalize beam_size\n                hypotheses, even though longer hypotheses might have better\n                normalized scores.\n            normalize_scores: Normalize scores by the length of the output.\n        \"\"\"", "\n", "self", ".", "models", "=", "models", "\n", "self", ".", "pad", "=", "models", "[", "0", "]", ".", "dst_dict", ".", "pad", "(", ")", "\n", "self", ".", "unk", "=", "models", "[", "0", "]", ".", "dst_dict", ".", "unk", "(", ")", "\n", "self", ".", "eos", "=", "models", "[", "0", "]", ".", "dst_dict", ".", "eos", "(", ")", "\n", "assert", "all", "(", "m", ".", "dst_dict", ".", "pad", "(", ")", "==", "self", ".", "pad", "for", "m", "in", "self", ".", "models", "[", "1", ":", "]", ")", "\n", "assert", "all", "(", "m", ".", "dst_dict", ".", "unk", "(", ")", "==", "self", ".", "unk", "for", "m", "in", "self", ".", "models", "[", "1", ":", "]", ")", "\n", "assert", "all", "(", "m", ".", "dst_dict", ".", "eos", "(", ")", "==", "self", ".", "eos", "for", "m", "in", "self", ".", "models", "[", "1", ":", "]", ")", "\n", "self", ".", "vocab_size", "=", "len", "(", "models", "[", "0", "]", ".", "dst_dict", ")", "\n", "self", ".", "beam_size", "=", "beam_size", "\n", "self", ".", "minlen", "=", "minlen", "\n", "max_decoder_len", "=", "min", "(", "[", "m", ".", "max_decoder_positions", "(", ")", "for", "m", "in", "self", ".", "models", "]", ")", "\n", "self", ".", "maxlen", "=", "max_decoder_len", "if", "maxlen", "is", "None", "else", "min", "(", "maxlen", ",", "max_decoder_len", ")", "\n", "self", ".", "stop_early", "=", "stop_early", "\n", "self", ".", "normalize_scores", "=", "normalize_scores", "\n", "self", ".", "len_penalty", "=", "len_penalty", "\n", "self", ".", "unk_penalty", "=", "unk_penalty", "\n", "self", ".", "retain_dropout", "=", "retain_dropout", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator.cuda": [[47, 51], ["model.cuda"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator.cuda"], ["", "def", "cuda", "(", "self", ")", ":", "\n", "        ", "for", "model", "in", "self", ".", "models", ":", "\n", "            ", "model", ".", "cuda", "(", ")", "\n", "", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator.generate_batched_itr": [[52, 86], ["fairseq.utils.make_variable", "input[].size", "enumerate", "timer.start", "fairseq.utils.maybe_no_grad", "sequence_generator.SequenceGenerator.generate", "timer.stop", "sum", "fairseq.utils.strip_pad", "int", "len"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.make_variable", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.maybe_no_grad", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.stop", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.strip_pad"], ["", "def", "generate_batched_itr", "(", "self", ",", "data_itr", ",", "beam_size", "=", "None", ",", "maxlen_a", "=", "0.0", ",", "maxlen_b", "=", "None", ",", "\n", "cuda", "=", "False", ",", "timer", "=", "None", ",", "prefix_size", "=", "0", ")", ":", "\n", "        ", "\"\"\"Iterate over a batched dataset and yield individual translations.\n\n        Args:\n            maxlen_a/b: generate sequences of maximum length ax + b,\n                where x is the source sentence length.\n            cuda: use GPU for generation\n            timer: StopwatchMeter for timing generations.\n        \"\"\"", "\n", "if", "maxlen_b", "is", "None", ":", "\n", "            ", "maxlen_b", "=", "self", ".", "maxlen", "\n", "\n", "", "for", "sample", "in", "data_itr", ":", "\n", "            ", "s", "=", "utils", ".", "make_variable", "(", "sample", ",", "volatile", "=", "True", ",", "cuda", "=", "cuda", ")", "\n", "input", "=", "s", "[", "'net_input'", "]", "\n", "srclen", "=", "input", "[", "'src_tokens'", "]", ".", "size", "(", "1", ")", "\n", "if", "timer", "is", "not", "None", ":", "\n", "                ", "timer", ".", "start", "(", ")", "\n", "", "with", "utils", ".", "maybe_no_grad", "(", ")", ":", "\n", "                ", "hypos", "=", "self", ".", "generate", "(", "\n", "input", "[", "'src_tokens'", "]", ",", "\n", "input", "[", "'src_lengths'", "]", ",", "\n", "input", "[", "'src_doctopic'", "]", ",", "\n", "input", "[", "'src_wordtopics'", "]", ",", "\n", "beam_size", "=", "beam_size", ",", "\n", "maxlen", "=", "int", "(", "maxlen_a", "*", "srclen", "+", "maxlen_b", ")", ",", "\n", "prefix_tokens", "=", "s", "[", "'target'", "]", "[", ":", ",", ":", "prefix_size", "]", "if", "prefix_size", ">", "0", "else", "None", ",", "\n", ")", "\n", "", "if", "timer", "is", "not", "None", ":", "\n", "                ", "timer", ".", "stop", "(", "sum", "(", "[", "len", "(", "h", "[", "0", "]", "[", "'tokens'", "]", ")", "for", "h", "in", "hypos", "]", ")", ")", "\n", "", "for", "i", ",", "id", "in", "enumerate", "(", "s", "[", "'id'", "]", ".", "data", ")", ":", "\n", "                ", "src", "=", "input", "[", "'src_tokens'", "]", ".", "data", "[", "i", ",", ":", "]", "\n", "# remove padding from ref", "\n", "ref", "=", "utils", ".", "strip_pad", "(", "s", "[", "'target'", "]", ".", "data", "[", "i", ",", ":", "]", ",", "self", ".", "pad", ")", "if", "s", "[", "'target'", "]", "is", "not", "None", "else", "None", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator.generate": [[87, 91], ["fairseq.utils.maybe_no_grad", "sequence_generator.SequenceGenerator._generate"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.maybe_no_grad", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator._generate"], ["yield", "id", ",", "src", ",", "ref", ",", "hypos", "[", "i", "]", "\n", "\n", "", "", "", "def", "generate", "(", "self", ",", "src_tokens", ",", "src_lengths", ",", "src_doctopic", ",", "src_wordtopics", ",", "beam_size", "=", "None", ",", "maxlen", "=", "None", ",", "prefix_tokens", "=", "None", ")", ":", "\n", "        ", "\"\"\"Generate a batch of translations.\"\"\"", "\n", "with", "utils", ".", "maybe_no_grad", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator._generate": [[92, 404], ["src_tokens.size", "min", "src_tokens.data.new().float().fill_", "scores.type_as.type_as.clone", "src_tokens.data.new().fill_", "src_tokens.data.new().fill_.clone", "scores.type_as.type_as.new", "scores.type_as.new.clone", "torch.arange().type_as", "range", "range", "min", "isinstance", "model.encoder", "encoder_outs.append", "src_tokens.size", "src_tokens.data.new().fill_.index_select", "set", "enumerate", "sequence_generator.SequenceGenerator._decode", "attn[].copy_", "sequence_generator.SequenceGenerator._generate.buffer"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator._decode"], ["            ", "return", "self", ".", "_generate", "(", "src_tokens", ",", "src_lengths", ",", "src_doctopic", ",", "src_wordtopics", ",", "beam_size", ",", "maxlen", ",", "prefix_tokens", ")", "\n", "\n", "", "", "def", "_generate", "(", "self", ",", "src_tokens", ",", "src_lengths", ",", "src_doctopic", ",", "src_wordtopics", ",", "beam_size", "=", "None", ",", "maxlen", "=", "None", ",", "prefix_tokens", "=", "None", ")", ":", "\n", "        ", "bsz", ",", "srclen", "=", "src_tokens", ".", "size", "(", ")", "\n", "bsz_1", ",", "emb_dim", "=", "src_doctopic", ".", "size", "(", ")", "\n", "bsz_2", ",", "srclen_1", ",", "emb_dim_1", ",", "=", "src_wordtopics", ".", "size", "(", ")", "\n", "\n", "# Shashi: Quick check", "\n", "assert", "(", "bsz", "==", "bsz_1", ")", "and", "(", "bsz", "==", "bsz_2", ")", "and", "(", "emb_dim", "==", "emb_dim_1", ")", "and", "(", "srclen", "==", "srclen_1", ")", "\n", "# assert (1 == 2), (\"%d %d %d %d %d %d %d\")%(bsz, bsz_1, bsz_2, emb_dim, emb_dim_1, srclen, srclen_1)", "\n", "\n", "maxlen", "=", "min", "(", "maxlen", ",", "self", ".", "maxlen", ")", "if", "maxlen", "is", "not", "None", "else", "self", ".", "maxlen", "\n", "\n", "# the max beam size is the dictionary size - 1, since we never select pad", "\n", "beam_size", "=", "beam_size", "if", "beam_size", "is", "not", "None", "else", "self", ".", "beam_size", "\n", "beam_size", "=", "min", "(", "beam_size", ",", "self", ".", "vocab_size", "-", "1", ")", "\n", "\n", "# Reshape inputs ", "\n", "src_tokens_reshaped", "=", "src_tokens", ".", "repeat", "(", "1", ",", "beam_size", ")", ".", "view", "(", "-", "1", ",", "srclen", ")", "\n", "src_lengths_reshaped", "=", "src_lengths", ".", "repeat", "(", "beam_size", ")", "\n", "src_doctopic_reshaped", "=", "src_doctopic", ".", "repeat", "(", "beam_size", ",", "1", ")", ".", "view", "(", "-", "1", ",", "emb_dim", ")", "\n", "src_wordtopics_reshaped", "=", "src_wordtopics", ".", "repeat", "(", "1", ",", "beam_size", ",", "1", ")", ".", "view", "(", "-", "1", ",", "srclen", ",", "emb_dim", ")", "\n", "# print(src_tokens_reshaped.size(),src_lengths_reshaped.size(),src_doctopic_reshaped.size(),src_wordtopics_reshaped.size())", "\n", "\n", "encoder_outs", "=", "[", "]", "\n", "incremental_states", "=", "{", "}", "\n", "for", "model", "in", "self", ".", "models", ":", "\n", "            ", "if", "not", "self", ".", "retain_dropout", ":", "\n", "                ", "model", ".", "eval", "(", ")", "\n", "", "if", "isinstance", "(", "model", ".", "decoder", ",", "FairseqIncrementalDecoder", ")", ":", "\n", "                ", "incremental_states", "[", "model", "]", "=", "{", "}", "\n", "", "else", ":", "\n", "                ", "incremental_states", "[", "model", "]", "=", "None", "\n", "\n", "# compute the encoder output for each beam", "\n", "\n", "# encoder_out = model.encoder(", "\n", "#     src_tokens.repeat(1, beam_size).view(-1, srclen),", "\n", "#     src_lengths.repeat(beam_size),", "\n", "# )", "\n", "\n", "", "encoder_out", "=", "model", ".", "encoder", "(", "\n", "src_tokens_reshaped", ",", "\n", "src_lengths_reshaped", ",", "\n", "src_doctopic_reshaped", ",", "\n", "src_wordtopics_reshaped", ",", "\n", ")", "\n", "\n", "# print(encoder_out.size())", "\n", "encoder_outs", ".", "append", "(", "encoder_out", ")", "\n", "\n", "# initialize buffers", "\n", "", "scores", "=", "src_tokens", ".", "data", ".", "new", "(", "bsz", "*", "beam_size", ",", "maxlen", "+", "1", ")", ".", "float", "(", ")", ".", "fill_", "(", "0", ")", "\n", "scores_buf", "=", "scores", ".", "clone", "(", ")", "\n", "tokens", "=", "src_tokens", ".", "data", ".", "new", "(", "bsz", "*", "beam_size", ",", "maxlen", "+", "2", ")", ".", "fill_", "(", "self", ".", "pad", ")", "\n", "tokens_buf", "=", "tokens", ".", "clone", "(", ")", "\n", "tokens", "[", ":", ",", "0", "]", "=", "self", ".", "eos", "\n", "attn", "=", "scores", ".", "new", "(", "bsz", "*", "beam_size", ",", "src_tokens", ".", "size", "(", "1", ")", ",", "maxlen", "+", "2", ")", "\n", "attn_buf", "=", "attn", ".", "clone", "(", ")", "\n", "\n", "# list of completed sentences", "\n", "finalized", "=", "[", "[", "]", "for", "i", "in", "range", "(", "bsz", ")", "]", "\n", "finished", "=", "[", "False", "for", "i", "in", "range", "(", "bsz", ")", "]", "\n", "worst_finalized", "=", "[", "{", "'idx'", ":", "None", ",", "'score'", ":", "-", "math", ".", "inf", "}", "for", "i", "in", "range", "(", "bsz", ")", "]", "\n", "num_remaining_sent", "=", "bsz", "\n", "\n", "# number of candidate hypos per step", "\n", "cand_size", "=", "2", "*", "beam_size", "# 2 x beam size in case half are EOS", "\n", "\n", "# offset arrays for converting between different indexing schemes", "\n", "bbsz_offsets", "=", "(", "torch", ".", "arange", "(", "0", ",", "bsz", ")", "*", "beam_size", ")", ".", "unsqueeze", "(", "1", ")", ".", "type_as", "(", "tokens", ")", "\n", "cand_offsets", "=", "torch", ".", "arange", "(", "0", ",", "cand_size", ")", ".", "type_as", "(", "tokens", ")", "\n", "\n", "# helper function for allocating buffers on the fly", "\n", "buffers", "=", "{", "}", "\n", "def", "buffer", "(", "name", ",", "type_of", "=", "tokens", ")", ":", "# noqa", "\n", "            ", "if", "name", "not", "in", "buffers", ":", "\n", "                ", "buffers", "[", "name", "]", "=", "type_of", ".", "new", "(", ")", "\n", "", "return", "buffers", "[", "name", "]", "\n", "\n", "", "def", "is_finished", "(", "sent", ",", "step", ",", "unfinalized_scores", "=", "None", ")", ":", "\n", "            ", "\"\"\"\n            Check whether we've finished generation for a given sentence, by\n            comparing the worst score among finalized hypotheses to the best\n            possible score among unfinalized hypotheses.\n            \"\"\"", "\n", "assert", "len", "(", "finalized", "[", "sent", "]", ")", "<=", "beam_size", "\n", "if", "len", "(", "finalized", "[", "sent", "]", ")", "==", "beam_size", ":", "\n", "                ", "if", "self", ".", "stop_early", "or", "step", "==", "maxlen", "or", "unfinalized_scores", "is", "None", ":", "\n", "                    ", "return", "True", "\n", "# stop if the best unfinalized score is worse than the worst", "\n", "# finalized one", "\n", "", "best_unfinalized_score", "=", "unfinalized_scores", "[", "sent", "]", ".", "max", "(", ")", "\n", "if", "self", ".", "normalize_scores", ":", "\n", "                    ", "best_unfinalized_score", "/=", "maxlen", "\n", "", "if", "worst_finalized", "[", "sent", "]", "[", "'score'", "]", ">=", "best_unfinalized_score", ":", "\n", "                    ", "return", "True", "\n", "", "", "return", "False", "\n", "\n", "", "def", "finalize_hypos", "(", "step", ",", "bbsz_idx", ",", "eos_scores", ",", "unfinalized_scores", "=", "None", ")", ":", "\n", "            ", "\"\"\"\n            Finalize the given hypotheses at this step, while keeping the total\n            number of finalized hypotheses per sentence <= beam_size.\n\n            Note: the input must be in the desired finalization order, so that\n            hypotheses that appear earlier in the input are preferred to those\n            that appear later.\n\n            Args:\n                step: current time step\n                bbsz_idx: A vector of indices in the range [0, bsz*beam_size),\n                    indicating which hypotheses to finalize\n                eos_scores: A vector of the same size as bbsz_idx containing\n                    scores for each hypothesis\n                unfinalized_scores: A vector containing scores for all\n                    unfinalized hypotheses\n            \"\"\"", "\n", "assert", "bbsz_idx", ".", "numel", "(", ")", "==", "eos_scores", ".", "numel", "(", ")", "\n", "\n", "# clone relevant token and attention tensors", "\n", "tokens_clone", "=", "tokens", ".", "index_select", "(", "0", ",", "bbsz_idx", ")", "\n", "tokens_clone", "=", "tokens_clone", "[", ":", ",", "1", ":", "step", "+", "2", "]", "# skip the first index, which is EOS", "\n", "tokens_clone", "[", ":", ",", "step", "]", "=", "self", ".", "eos", "\n", "attn_clone", "=", "attn", ".", "index_select", "(", "0", ",", "bbsz_idx", ")", "[", ":", ",", ":", ",", "1", ":", "step", "+", "2", "]", "\n", "\n", "# compute scores per token position", "\n", "pos_scores", "=", "scores", ".", "index_select", "(", "0", ",", "bbsz_idx", ")", "[", ":", ",", ":", "step", "+", "1", "]", "\n", "pos_scores", "[", ":", ",", "step", "]", "=", "eos_scores", "\n", "# convert from cumulative to per-position scores", "\n", "pos_scores", "[", ":", ",", "1", ":", "]", "=", "pos_scores", "[", ":", ",", "1", ":", "]", "-", "pos_scores", "[", ":", ",", ":", "-", "1", "]", "\n", "\n", "# normalize sentence-level scores", "\n", "if", "self", ".", "normalize_scores", ":", "\n", "                ", "eos_scores", "/=", "(", "step", "+", "1", ")", "**", "self", ".", "len_penalty", "\n", "\n", "", "sents_seen", "=", "set", "(", ")", "\n", "for", "i", ",", "(", "idx", ",", "score", ")", "in", "enumerate", "(", "zip", "(", "bbsz_idx", ".", "tolist", "(", ")", ",", "eos_scores", ".", "tolist", "(", ")", ")", ")", ":", "\n", "                ", "sent", "=", "idx", "//", "beam_size", "\n", "sents_seen", ".", "add", "(", "sent", ")", "\n", "\n", "def", "get_hypo", "(", ")", ":", "\n", "                    ", "_", ",", "alignment", "=", "attn_clone", "[", "i", "]", ".", "max", "(", "dim", "=", "0", ")", "\n", "return", "{", "\n", "'tokens'", ":", "tokens_clone", "[", "i", "]", ",", "\n", "'score'", ":", "score", ",", "\n", "'attention'", ":", "attn_clone", "[", "i", "]", ",", "# src_len x tgt_len", "\n", "'alignment'", ":", "alignment", ",", "\n", "'positional_scores'", ":", "pos_scores", "[", "i", "]", ",", "\n", "}", "\n", "\n", "", "if", "len", "(", "finalized", "[", "sent", "]", ")", "<", "beam_size", ":", "\n", "                    ", "finalized", "[", "sent", "]", ".", "append", "(", "get_hypo", "(", ")", ")", "\n", "", "elif", "not", "self", ".", "stop_early", "and", "score", ">", "worst_finalized", "[", "sent", "]", "[", "'score'", "]", ":", "\n", "# replace worst hypo for this sentence with new/better one", "\n", "                    ", "worst_idx", "=", "worst_finalized", "[", "sent", "]", "[", "'idx'", "]", "\n", "if", "worst_idx", "is", "not", "None", ":", "\n", "                        ", "finalized", "[", "sent", "]", "[", "worst_idx", "]", "=", "get_hypo", "(", ")", "\n", "\n", "# find new worst finalized hypo for this sentence", "\n", "", "idx", ",", "s", "=", "min", "(", "enumerate", "(", "finalized", "[", "sent", "]", ")", ",", "key", "=", "lambda", "r", ":", "r", "[", "1", "]", "[", "'score'", "]", ")", "\n", "worst_finalized", "[", "sent", "]", "=", "{", "\n", "'score'", ":", "s", "[", "'score'", "]", ",", "\n", "'idx'", ":", "idx", ",", "\n", "}", "\n", "\n", "# return number of hypotheses finished this step", "\n", "", "", "num_finished", "=", "0", "\n", "for", "sent", "in", "sents_seen", ":", "\n", "# check termination conditions for this sentence", "\n", "                ", "if", "not", "finished", "[", "sent", "]", "and", "is_finished", "(", "sent", ",", "step", ",", "unfinalized_scores", ")", ":", "\n", "                    ", "finished", "[", "sent", "]", "=", "True", "\n", "num_finished", "+=", "1", "\n", "", "", "return", "num_finished", "\n", "\n", "", "reorder_state", "=", "None", "\n", "for", "step", "in", "range", "(", "maxlen", "+", "1", ")", ":", "# one extra step for EOS marker", "\n", "# reorder decoder internal states based on the prev choice of beams", "\n", "            ", "if", "reorder_state", "is", "not", "None", ":", "\n", "                ", "for", "model", "in", "self", ".", "models", ":", "\n", "                    ", "if", "isinstance", "(", "model", ".", "decoder", ",", "FairseqIncrementalDecoder", ")", ":", "\n", "                        ", "model", ".", "decoder", ".", "reorder_incremental_state", "(", "\n", "incremental_states", "[", "model", "]", ",", "reorder_state", ")", "\n", "\n", "", "", "", "probs", ",", "avg_attn_scores", "=", "self", ".", "_decode", "(", "\n", "tokens", "[", ":", ",", ":", "step", "+", "1", "]", ",", "encoder_outs", ",", "src_doctopic_reshaped", ",", "incremental_states", ")", "\n", "if", "step", "==", "0", ":", "\n", "# at the first step all hypotheses are equally likely, so use", "\n", "# only the first beam", "\n", "                ", "probs", "=", "probs", ".", "unfold", "(", "0", ",", "1", ",", "beam_size", ")", ".", "squeeze", "(", "2", ")", ".", "contiguous", "(", ")", "\n", "scores", "=", "scores", ".", "type_as", "(", "probs", ")", "\n", "scores_buf", "=", "scores_buf", ".", "type_as", "(", "probs", ")", "\n", "", "else", ":", "\n", "# make probs contain cumulative scores for each hypothesis", "\n", "                ", "probs", ".", "add_", "(", "scores", "[", ":", ",", "step", "-", "1", "]", ".", "view", "(", "-", "1", ",", "1", ")", ")", "\n", "", "probs", "[", ":", ",", "self", ".", "pad", "]", "=", "-", "math", ".", "inf", "# never select pad", "\n", "probs", "[", ":", ",", "self", ".", "unk", "]", "-=", "self", ".", "unk_penalty", "# apply unk penalty", "\n", "\n", "# Record attention scores", "\n", "attn", "[", ":", ",", ":", ",", "step", "+", "1", "]", ".", "copy_", "(", "avg_attn_scores", ")", "\n", "\n", "cand_scores", "=", "buffer", "(", "'cand_scores'", ",", "type_of", "=", "scores", ")", "\n", "cand_indices", "=", "buffer", "(", "'cand_indices'", ")", "\n", "cand_beams", "=", "buffer", "(", "'cand_beams'", ")", "\n", "eos_bbsz_idx", "=", "buffer", "(", "'eos_bbsz_idx'", ")", "\n", "eos_scores", "=", "buffer", "(", "'eos_scores'", ",", "type_of", "=", "scores", ")", "\n", "if", "step", "<", "maxlen", ":", "\n", "                ", "if", "prefix_tokens", "is", "not", "None", "and", "step", "<", "prefix_tokens", ".", "size", "(", "1", ")", ":", "\n", "                    ", "probs_slice", "=", "probs", ".", "view", "(", "bsz", ",", "-", "1", ",", "probs", ".", "size", "(", "-", "1", ")", ")", "[", ":", ",", "0", ",", ":", "]", "\n", "cand_scores", "=", "torch", ".", "gather", "(", "\n", "probs_slice", ",", "dim", "=", "1", ",", "\n", "index", "=", "prefix_tokens", "[", ":", ",", "step", "]", ".", "view", "(", "-", "1", ",", "1", ")", ".", "data", "\n", ")", ".", "expand", "(", "-", "1", ",", "cand_size", ")", "\n", "cand_indices", "=", "prefix_tokens", "[", ":", ",", "step", "]", ".", "view", "(", "-", "1", ",", "1", ")", ".", "expand", "(", "bsz", ",", "cand_size", ")", ".", "data", "\n", "cand_beams", ".", "resize_as_", "(", "cand_indices", ")", ".", "fill_", "(", "0", ")", "\n", "", "else", ":", "\n", "# take the best 2 x beam_size predictions. We'll choose the first", "\n", "# beam_size of these which don't predict eos to continue with.", "\n", "                    ", "torch", ".", "topk", "(", "\n", "probs", ".", "view", "(", "bsz", ",", "-", "1", ")", ",", "\n", "k", "=", "min", "(", "cand_size", ",", "probs", ".", "view", "(", "bsz", ",", "-", "1", ")", ".", "size", "(", "1", ")", "-", "1", ")", ",", "# -1 so we never select pad", "\n", "out", "=", "(", "cand_scores", ",", "cand_indices", ")", ",", "\n", ")", "\n", "torch", ".", "div", "(", "cand_indices", ",", "self", ".", "vocab_size", ",", "out", "=", "cand_beams", ")", "\n", "cand_indices", ".", "fmod_", "(", "self", ".", "vocab_size", ")", "\n", "", "", "else", ":", "\n", "# finalize all active hypotheses once we hit maxlen", "\n", "# pick the hypothesis with the highest prob of EOS right now", "\n", "                ", "torch", ".", "sort", "(", "\n", "probs", "[", ":", ",", "self", ".", "eos", "]", ",", "\n", "descending", "=", "True", ",", "\n", "out", "=", "(", "eos_scores", ",", "eos_bbsz_idx", ")", ",", "\n", ")", "\n", "num_remaining_sent", "-=", "finalize_hypos", "(", "\n", "step", ",", "eos_bbsz_idx", ",", "eos_scores", ")", "\n", "assert", "num_remaining_sent", "==", "0", "\n", "break", "\n", "\n", "# cand_bbsz_idx contains beam indices for the top candidate", "\n", "# hypotheses, with a range of values: [0, bsz*beam_size),", "\n", "# and dimensions: [bsz, cand_size]", "\n", "", "cand_bbsz_idx", "=", "cand_beams", ".", "add_", "(", "bbsz_offsets", ")", "\n", "\n", "# finalize hypotheses that end in eos", "\n", "eos_mask", "=", "cand_indices", ".", "eq", "(", "self", ".", "eos", ")", "\n", "if", "step", ">=", "self", ".", "minlen", ":", "\n", "# only consider eos when it's among the top beam_size indices", "\n", "                ", "torch", ".", "masked_select", "(", "\n", "cand_bbsz_idx", "[", ":", ",", ":", "beam_size", "]", ",", "\n", "mask", "=", "eos_mask", "[", ":", ",", ":", "beam_size", "]", ",", "\n", "out", "=", "eos_bbsz_idx", ",", "\n", ")", "\n", "if", "eos_bbsz_idx", ".", "numel", "(", ")", ">", "0", ":", "\n", "                    ", "torch", ".", "masked_select", "(", "\n", "cand_scores", "[", ":", ",", ":", "beam_size", "]", ",", "\n", "mask", "=", "eos_mask", "[", ":", ",", ":", "beam_size", "]", ",", "\n", "out", "=", "eos_scores", ",", "\n", ")", "\n", "num_remaining_sent", "-=", "finalize_hypos", "(", "\n", "step", ",", "eos_bbsz_idx", ",", "eos_scores", ",", "cand_scores", ")", "\n", "\n", "", "", "assert", "num_remaining_sent", ">=", "0", "\n", "if", "num_remaining_sent", "==", "0", ":", "\n", "                ", "break", "\n", "", "assert", "step", "<", "maxlen", "\n", "\n", "# set active_mask so that values > cand_size indicate eos hypos", "\n", "# and values < cand_size indicate candidate active hypos.", "\n", "# After, the min values per row are the top candidate active hypos", "\n", "active_mask", "=", "buffer", "(", "'active_mask'", ")", "\n", "torch", ".", "add", "(", "\n", "eos_mask", ".", "type_as", "(", "cand_offsets", ")", "*", "cand_size", ",", "\n", "cand_offsets", "[", ":", "eos_mask", ".", "size", "(", "1", ")", "]", ",", "\n", "out", "=", "active_mask", ",", "\n", ")", "\n", "\n", "# get the top beam_size active hypotheses, which are just the hypos", "\n", "# with the smallest values in active_mask", "\n", "active_hypos", ",", "_ignore", "=", "buffer", "(", "'active_hypos'", ")", ",", "buffer", "(", "'_ignore'", ")", "\n", "torch", ".", "topk", "(", "\n", "active_mask", ",", "k", "=", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "False", ",", "\n", "out", "=", "(", "_ignore", ",", "active_hypos", ")", "\n", ")", "\n", "active_bbsz_idx", "=", "buffer", "(", "'active_bbsz_idx'", ")", "\n", "torch", ".", "gather", "(", "\n", "cand_bbsz_idx", ",", "dim", "=", "1", ",", "index", "=", "active_hypos", ",", "\n", "out", "=", "active_bbsz_idx", ",", "\n", ")", "\n", "active_scores", "=", "torch", ".", "gather", "(", "\n", "cand_scores", ",", "dim", "=", "1", ",", "index", "=", "active_hypos", ",", "\n", "out", "=", "scores", "[", ":", ",", "step", "]", ".", "view", "(", "bsz", ",", "beam_size", ")", ",", "\n", ")", "\n", "active_bbsz_idx", "=", "active_bbsz_idx", ".", "view", "(", "-", "1", ")", "\n", "active_scores", "=", "active_scores", ".", "view", "(", "-", "1", ")", "\n", "\n", "# copy tokens and scores for active hypotheses", "\n", "torch", ".", "index_select", "(", "\n", "tokens", "[", ":", ",", ":", "step", "+", "1", "]", ",", "dim", "=", "0", ",", "index", "=", "active_bbsz_idx", ",", "\n", "out", "=", "tokens_buf", "[", ":", ",", ":", "step", "+", "1", "]", ",", "\n", ")", "\n", "torch", ".", "gather", "(", "\n", "cand_indices", ",", "dim", "=", "1", ",", "index", "=", "active_hypos", ",", "\n", "out", "=", "tokens_buf", ".", "view", "(", "bsz", ",", "beam_size", ",", "-", "1", ")", "[", ":", ",", ":", ",", "step", "+", "1", "]", ",", "\n", ")", "\n", "if", "step", ">", "0", ":", "\n", "                ", "torch", ".", "index_select", "(", "\n", "scores", "[", ":", ",", ":", "step", "]", ",", "dim", "=", "0", ",", "index", "=", "active_bbsz_idx", ",", "\n", "out", "=", "scores_buf", "[", ":", ",", ":", "step", "]", ",", "\n", ")", "\n", "", "torch", ".", "gather", "(", "\n", "cand_scores", ",", "dim", "=", "1", ",", "index", "=", "active_hypos", ",", "\n", "out", "=", "scores_buf", ".", "view", "(", "bsz", ",", "beam_size", ",", "-", "1", ")", "[", ":", ",", ":", ",", "step", "]", ",", "\n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator._decode": [[405, 431], ["fairseq.utils.volatile_variable", "zip", "avg_probs.div_", "avg_probs.log_", "len", "avg_attn.div_", "fairseq.utils.maybe_no_grad", "model.decoder", "model.get_normalized_probs", "avg_probs.add_", "len", "avg_attn.add_"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.volatile_variable", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.maybe_no_grad", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_decoder.FairseqDecoder.get_normalized_probs"], ["# copy attention for active hypotheses", "\n", "torch", ".", "index_select", "(", "\n", "attn", "[", ":", ",", ":", ",", ":", "step", "+", "2", "]", ",", "dim", "=", "0", ",", "index", "=", "active_bbsz_idx", ",", "\n", "out", "=", "attn_buf", "[", ":", ",", ":", ",", ":", "step", "+", "2", "]", ",", "\n", ")", "\n", "\n", "# swap buffers", "\n", "old_tokens", "=", "tokens", "\n", "tokens", "=", "tokens_buf", "\n", "tokens_buf", "=", "old_tokens", "\n", "old_scores", "=", "scores", "\n", "scores", "=", "scores_buf", "\n", "scores_buf", "=", "old_scores", "\n", "old_attn", "=", "attn", "\n", "attn", "=", "attn_buf", "\n", "attn_buf", "=", "old_attn", "\n", "\n", "# reorder incremental state in decoder", "\n", "reorder_state", "=", "active_bbsz_idx", "\n", "\n", "# sort by score descending", "\n", "", "for", "sent", "in", "range", "(", "bsz", ")", ":", "\n", "            ", "finalized", "[", "sent", "]", "=", "sorted", "(", "finalized", "[", "sent", "]", ",", "key", "=", "lambda", "r", ":", "r", "[", "'score'", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "", "return", "finalized", "\n", "\n", "", "def", "_decode", "(", "self", ",", "tokens", ",", "encoder_outs", ",", "src_doctopic_reshaped", ",", "incremental_states", ")", ":", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.torch_persistent_save": [[21, 28], ["range", "torch.save", "logging.error", "traceback.format_exc"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.save"], ["def", "torch_persistent_save", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "torch", ".", "save", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "Exception", ":", "\n", "            ", "if", "i", "==", "2", ":", "\n", "                ", "logging", ".", "error", "(", "traceback", ".", "format_exc", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.save_state": [[30, 51], ["utils.torch_persistent_save", "model.state_dict", "optimizer.state_dict", "lr_scheduler.state_dict"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.torch_persistent_save", "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.state_dict", "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.state_dict", "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.state_dict"], ["", "", "", "", "def", "save_state", "(", "filename", ",", "args", ",", "model", ",", "criterion", ",", "optimizer", ",", "lr_scheduler", ",", "\n", "num_updates", ",", "optim_history", "=", "None", ",", "extra_state", "=", "None", ")", ":", "\n", "    ", "if", "optim_history", "is", "None", ":", "\n", "        ", "optim_history", "=", "[", "]", "\n", "", "if", "extra_state", "is", "None", ":", "\n", "        ", "extra_state", "=", "{", "}", "\n", "", "state_dict", "=", "{", "\n", "'args'", ":", "args", ",", "\n", "'model'", ":", "model", ".", "state_dict", "(", ")", ",", "\n", "'optimizer_history'", ":", "optim_history", "+", "[", "\n", "{", "\n", "'criterion_name'", ":", "criterion", ".", "__class__", ".", "__name__", ",", "\n", "'optimizer_name'", ":", "optimizer", ".", "__class__", ".", "__name__", ",", "\n", "'lr_scheduler_state'", ":", "lr_scheduler", ".", "state_dict", "(", ")", ",", "\n", "'num_updates'", ":", "num_updates", ",", "\n", "}", "\n", "]", ",", "\n", "'last_optimizer_state'", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "'extra_state'", ":", "extra_state", ",", "\n", "}", "\n", "torch_persistent_save", "(", "state_dict", ",", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.load_model_state": [[53, 74], ["utils._upgrade_state_dict", "model.upgrade_state_dict", "os.path.exists", "torch.load", "torch.load", "model.load_state_dict", "Exception", "torch.serialization.default_restore_location"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils._upgrade_state_dict", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_decoder.FairseqDecoder.upgrade_state_dict", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.exists", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.load_state_dict"], ["", "def", "load_model_state", "(", "filename", ",", "model", ",", "cuda_device", "=", "None", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "filename", ")", ":", "\n", "        ", "return", "None", ",", "[", "]", ",", "None", "\n", "", "if", "cuda_device", "is", "None", ":", "\n", "        ", "state", "=", "torch", ".", "load", "(", "filename", ")", "\n", "", "else", ":", "\n", "        ", "state", "=", "torch", ".", "load", "(", "\n", "filename", ",", "\n", "map_location", "=", "lambda", "s", ",", "l", ":", "default_restore_location", "(", "s", ",", "'cuda:{}'", ".", "format", "(", "cuda_device", ")", ")", "\n", ")", "\n", "", "state", "=", "_upgrade_state_dict", "(", "state", ")", "\n", "state", "[", "'model'", "]", "=", "model", ".", "upgrade_state_dict", "(", "state", "[", "'model'", "]", ")", "\n", "\n", "# load model parameters", "\n", "try", ":", "\n", "        ", "model", ".", "load_state_dict", "(", "state", "[", "'model'", "]", ")", "\n", "", "except", "Exception", ":", "\n", "        ", "raise", "Exception", "(", "'Cannot load model parameters from checkpoint, '", "\n", "'please ensure that the architectures match'", ")", "\n", "\n", "", "return", "state", "[", "'extra_state'", "]", ",", "state", "[", "'optimizer_history'", "]", ",", "state", "[", "'last_optimizer_state'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils._upgrade_state_dict": [[76, 117], ["None"], "function", ["None"], ["", "def", "_upgrade_state_dict", "(", "state", ")", ":", "\n", "    ", "\"\"\"Helper for upgrading old model checkpoints.\"\"\"", "\n", "# add optimizer_history", "\n", "if", "'optimizer_history'", "not", "in", "state", ":", "\n", "        ", "state", "[", "'optimizer_history'", "]", "=", "[", "\n", "{", "\n", "'criterion_name'", ":", "'CrossEntropyCriterion'", ",", "\n", "'best_loss'", ":", "state", "[", "'best_loss'", "]", ",", "\n", "}", ",", "\n", "]", "\n", "state", "[", "'last_optimizer_state'", "]", "=", "state", "[", "'optimizer'", "]", "\n", "del", "state", "[", "'optimizer'", "]", "\n", "del", "state", "[", "'best_loss'", "]", "\n", "# move extra_state into sub-dictionary", "\n", "", "if", "'epoch'", "in", "state", "and", "'extra_state'", "not", "in", "state", ":", "\n", "        ", "state", "[", "'extra_state'", "]", "=", "{", "\n", "'epoch'", ":", "state", "[", "'epoch'", "]", ",", "\n", "'batch_offset'", ":", "state", "[", "'batch_offset'", "]", ",", "\n", "'val_loss'", ":", "state", "[", "'val_loss'", "]", ",", "\n", "}", "\n", "del", "state", "[", "'epoch'", "]", "\n", "del", "state", "[", "'batch_offset'", "]", "\n", "del", "state", "[", "'val_loss'", "]", "\n", "# reduce optimizer history's memory usage (only keep the last state)", "\n", "", "if", "'optimizer'", "in", "state", "[", "'optimizer_history'", "]", "[", "-", "1", "]", ":", "\n", "        ", "state", "[", "'last_optimizer_state'", "]", "=", "state", "[", "'optimizer_history'", "]", "[", "-", "1", "]", "[", "'optimizer'", "]", "\n", "for", "optim_hist", "in", "state", "[", "'optimizer_history'", "]", ":", "\n", "            ", "del", "optim_hist", "[", "'optimizer'", "]", "\n", "# record the optimizer class name", "\n", "", "", "if", "'optimizer_name'", "not", "in", "state", "[", "'optimizer_history'", "]", "[", "-", "1", "]", ":", "\n", "        ", "state", "[", "'optimizer_history'", "]", "[", "-", "1", "]", "[", "'optimizer_name'", "]", "=", "'FairseqNAG'", "\n", "# move best_loss into lr_scheduler_state", "\n", "", "if", "'lr_scheduler_state'", "not", "in", "state", "[", "'optimizer_history'", "]", "[", "-", "1", "]", ":", "\n", "        ", "state", "[", "'optimizer_history'", "]", "[", "-", "1", "]", "[", "'lr_scheduler_state'", "]", "=", "{", "\n", "'best'", ":", "state", "[", "'optimizer_history'", "]", "[", "-", "1", "]", "[", "'best_loss'", "]", ",", "\n", "}", "\n", "del", "state", "[", "'optimizer_history'", "]", "[", "-", "1", "]", "[", "'best_loss'", "]", "\n", "# keep track of number of updates", "\n", "", "if", "'num_updates'", "not", "in", "state", "[", "'optimizer_history'", "]", "[", "-", "1", "]", ":", "\n", "        ", "state", "[", "'optimizer_history'", "]", "[", "-", "1", "]", "[", "'num_updates'", "]", "=", "0", "\n", "", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.load_ensemble_for_inference": [[119, 149], ["utils._upgrade_args", "states.append", "data.load_dictionaries", "models.build_model", "models.build_model.load_state_dict", "ensemble.append", "os.path.exists", "IOError", "torch.load", "torch.serialization.default_restore_location"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils._upgrade_args", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.load_dictionaries", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.__init__.build_model", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.load_state_dict", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.exists", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.load"], ["", "def", "load_ensemble_for_inference", "(", "filenames", ",", "src_dict", "=", "None", ",", "dst_dict", "=", "None", ",", "data_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"Load an ensemble of models for inference.\n\n    The source and target dictionaries can be given explicitly, or loaded from\n    the `data_dir` directory.\n    \"\"\"", "\n", "from", "fairseq", "import", "data", ",", "models", "\n", "\n", "# load model architectures and weights", "\n", "states", "=", "[", "]", "\n", "for", "filename", "in", "filenames", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "filename", ")", ":", "\n", "            ", "raise", "IOError", "(", "'Model file not found: {}'", ".", "format", "(", "filename", ")", ")", "\n", "", "states", ".", "append", "(", "\n", "torch", ".", "load", "(", "filename", ",", "map_location", "=", "lambda", "s", ",", "l", ":", "default_restore_location", "(", "s", ",", "'cpu'", ")", ")", "\n", ")", "\n", "", "args", "=", "states", "[", "0", "]", "[", "'args'", "]", "\n", "args", "=", "_upgrade_args", "(", "args", ")", "\n", "\n", "if", "src_dict", "is", "None", "or", "dst_dict", "is", "None", ":", "\n", "        ", "assert", "data_dir", "is", "not", "None", "\n", "src_dict", ",", "dst_dict", "=", "data", ".", "load_dictionaries", "(", "data_dir", ",", "args", ".", "source_lang", ",", "args", ".", "target_lang", ")", "\n", "\n", "# build ensemble", "\n", "", "ensemble", "=", "[", "]", "\n", "for", "state", "in", "states", ":", "\n", "        ", "model", "=", "models", ".", "build_model", "(", "args", ",", "src_dict", ",", "dst_dict", ")", "\n", "model", ".", "load_state_dict", "(", "state", "[", "'model'", "]", ")", "\n", "ensemble", ".", "append", "(", "model", ")", "\n", "", "return", "ensemble", ",", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils._upgrade_args": [[151, 158], ["hasattr", "hasattr"], "function", ["None"], ["", "def", "_upgrade_args", "(", "args", ")", ":", "\n", "    ", "if", "not", "hasattr", "(", "args", ",", "'max_source_positions'", ")", ":", "\n", "        ", "args", ".", "max_source_positions", "=", "args", ".", "max_positions", "\n", "args", ".", "max_target_positions", "=", "args", ".", "max_positions", "\n", "", "if", "not", "hasattr", "(", "args", ",", "'share_input_output_embed'", ")", ":", "\n", "        ", "args", ".", "share_input_output_embed", "=", "False", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.maybe_no_grad": [[160, 165], ["contextlib.ExitStack", "hasattr", "torch.no_grad"], "function", ["None"], ["", "def", "maybe_no_grad", "(", "condition", "=", "True", ")", ":", "\n", "    ", "if", "hasattr", "(", "torch", ",", "'no_grad'", ")", "and", "condition", ":", "\n", "        ", "return", "torch", ".", "no_grad", "(", ")", "\n", "# no-op context manager", "\n", "", "return", "contextlib", ".", "ExitStack", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.volatile_variable": [[167, 173], ["hasattr", "torch.autograd.Variable", "torch.autograd.Variable"], "function", ["None"], ["", "def", "volatile_variable", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "hasattr", "(", "torch", ",", "'no_grad'", ")", ":", "\n", "# volatile has been deprecated, use the no_grad context manager instead", "\n", "        ", "return", "Variable", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "        ", "return", "Variable", "(", "*", "args", ",", "**", "kwargs", ",", "volatile", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.make_variable": [[175, 200], ["utils.make_variable._make_variable"], "function", ["None"], ["", "", "def", "make_variable", "(", "sample", ",", "volatile", "=", "False", ",", "cuda", "=", "False", ")", ":", "\n", "    ", "\"\"\"Wrap input tensors in Variable class.\"\"\"", "\n", "\n", "if", "len", "(", "sample", ")", "==", "0", ":", "\n", "        ", "return", "{", "}", "\n", "\n", "", "def", "_make_variable", "(", "maybe_tensor", ")", ":", "\n", "        ", "if", "torch", ".", "is_tensor", "(", "maybe_tensor", ")", ":", "\n", "            ", "if", "cuda", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                ", "maybe_tensor", "=", "maybe_tensor", ".", "cuda", "(", ")", "\n", "", "if", "volatile", ":", "\n", "                ", "return", "volatile_variable", "(", "maybe_tensor", ")", "\n", "", "else", ":", "\n", "                ", "return", "Variable", "(", "maybe_tensor", ")", "\n", "", "", "elif", "isinstance", "(", "maybe_tensor", ",", "dict", ")", ":", "\n", "            ", "return", "{", "\n", "key", ":", "_make_variable", "(", "value", ")", "\n", "for", "key", ",", "value", "in", "maybe_tensor", ".", "items", "(", ")", "\n", "}", "\n", "", "elif", "isinstance", "(", "maybe_tensor", ",", "list", ")", ":", "\n", "            ", "return", "[", "_make_variable", "(", "x", ")", "for", "x", "in", "maybe_tensor", "]", "\n", "", "else", ":", "\n", "            ", "return", "maybe_tensor", "\n", "\n", "", "", "return", "_make_variable", "(", "sample", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils._get_full_incremental_state_key": [[205, 215], ["hasattr"], "function", ["None"], ["def", "_get_full_incremental_state_key", "(", "module_instance", ",", "key", ")", ":", "\n", "    ", "module_name", "=", "module_instance", ".", "__class__", ".", "__name__", "\n", "\n", "# assign a unique ID to each module instance, so that incremental state is", "\n", "# not shared across module instances", "\n", "if", "not", "hasattr", "(", "module_instance", ",", "'_fairseq_instance_id'", ")", ":", "\n", "        ", "INCREMENTAL_STATE_INSTANCE_ID", "[", "module_name", "]", "+=", "1", "\n", "module_instance", ".", "_fairseq_instance_id", "=", "INCREMENTAL_STATE_INSTANCE_ID", "[", "module_name", "]", "\n", "\n", "", "return", "'{}.{}.{}'", ".", "format", "(", "module_name", ",", "module_instance", ".", "_fairseq_instance_id", ",", "key", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.get_incremental_state": [[217, 223], ["utils._get_full_incremental_state_key"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils._get_full_incremental_state_key"], ["", "def", "get_incremental_state", "(", "module", ",", "incremental_state", ",", "key", ")", ":", "\n", "    ", "\"\"\"Helper for getting incremental state for an nn.Module.\"\"\"", "\n", "full_key", "=", "_get_full_incremental_state_key", "(", "module", ",", "key", ")", "\n", "if", "incremental_state", "is", "None", "or", "full_key", "not", "in", "incremental_state", ":", "\n", "        ", "return", "None", "\n", "", "return", "incremental_state", "[", "full_key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.set_incremental_state": [[225, 230], ["utils._get_full_incremental_state_key"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils._get_full_incremental_state_key"], ["", "def", "set_incremental_state", "(", "module", ",", "incremental_state", ",", "key", ",", "value", ")", ":", "\n", "    ", "\"\"\"Helper for setting incremental state for an nn.Module.\"\"\"", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "        ", "full_key", "=", "_get_full_incremental_state_key", "(", "module", ",", "key", ")", "\n", "incremental_state", "[", "full_key", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.load_align_dict": [[232, 247], ["isinstance", "open", "line.split"], "function", ["None"], ["", "", "def", "load_align_dict", "(", "replace_unk", ")", ":", "\n", "    ", "if", "replace_unk", "is", "None", ":", "\n", "        ", "align_dict", "=", "None", "\n", "", "elif", "isinstance", "(", "replace_unk", ",", "str", ")", ":", "\n", "# Load alignment dictionary for unknown word replacement if it was passed as an argument.", "\n", "        ", "align_dict", "=", "{", "}", "\n", "with", "open", "(", "replace_unk", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "cols", "=", "line", ".", "split", "(", ")", "\n", "align_dict", "[", "cols", "[", "0", "]", "]", "=", "cols", "[", "1", "]", "\n", "", "", "", "else", ":", "\n", "# No alignment dictionary provided but we still want to perform unknown word replacement by copying the", "\n", "# original source word.", "\n", "        ", "align_dict", "=", "{", "}", "\n", "", "return", "align_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.replace_unk": [[249, 260], ["fairseq.tokenizer.tokenize_line", "enumerate", "fairseq.tokenizer.tokenize_line", "align_dict.get"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.tokenizer.tokenize_line", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.tokenizer.tokenize_line"], ["", "def", "replace_unk", "(", "hypo_str", ",", "src_str", ",", "alignment", ",", "align_dict", ",", "unk", ")", ":", "\n", "# Tokens are strings here", "\n", "    ", "hypo_tokens", "=", "tokenizer", ".", "tokenize_line", "(", "hypo_str", ")", "\n", "# TODO: Very rare cases where the replacement is '<eos>' should be handled gracefully", "\n", "src_tokens", "=", "tokenizer", ".", "tokenize_line", "(", "src_str", ")", "+", "[", "'<eos>'", "]", "\n", "for", "i", ",", "ht", "in", "enumerate", "(", "hypo_tokens", ")", ":", "\n", "        ", "if", "ht", "==", "unk", ":", "\n", "            ", "src_token", "=", "src_tokens", "[", "alignment", "[", "i", "]", "]", "\n", "# Either take the corresponding value in the aligned dictionary or just copy the original value.", "\n", "hypo_tokens", "[", "i", "]", "=", "align_dict", ".", "get", "(", "src_token", ",", "src_token", ")", "\n", "", "", "return", "' '", ".", "join", "(", "hypo_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.post_process_prediction": [[262, 271], ["dst_dict.string", "utils.replace_unk", "fairseq.tokenizer.Tokenizer.tokenize", "dst_dict.unk_string"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.string", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.replace_unk", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.tokenizer.Tokenizer.tokenize", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.unk_string"], ["", "def", "post_process_prediction", "(", "hypo_tokens", ",", "src_str", ",", "alignment", ",", "align_dict", ",", "dst_dict", ",", "remove_bpe", ")", ":", "\n", "    ", "hypo_str", "=", "dst_dict", ".", "string", "(", "hypo_tokens", ",", "remove_bpe", ")", "\n", "if", "align_dict", "is", "not", "None", ":", "\n", "        ", "hypo_str", "=", "replace_unk", "(", "hypo_str", ",", "src_str", ",", "alignment", ",", "align_dict", ",", "dst_dict", ".", "unk_string", "(", ")", ")", "\n", "", "if", "align_dict", "is", "not", "None", "or", "remove_bpe", "is", "not", "None", ":", "\n", "# Convert back to tokens for evaluating with unk replacement or without BPE", "\n", "# Note that the dictionary can be modified inside the method.", "\n", "        ", "hypo_tokens", "=", "tokenizer", ".", "Tokenizer", ".", "tokenize", "(", "hypo_str", ",", "dst_dict", ",", "add_if_not_exist", "=", "True", ")", "\n", "", "return", "hypo_tokens", ",", "hypo_str", ",", "alignment", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.strip_pad": [[273, 275], ["tensor.ne"], "function", ["None"], ["", "def", "strip_pad", "(", "tensor", ",", "pad", ")", ":", "\n", "    ", "return", "tensor", "[", "tensor", ".", "ne", "(", "pad", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.buffered_arange": [[277, 283], ["hasattr", "torch.LongTensor", "buffered_arange.buf.numel", "torch.arange"], "function", ["None"], ["", "def", "buffered_arange", "(", "max", ")", ":", "\n", "    ", "if", "not", "hasattr", "(", "buffered_arange", ",", "'buf'", ")", ":", "\n", "        ", "buffered_arange", ".", "buf", "=", "torch", ".", "LongTensor", "(", ")", "\n", "", "if", "max", ">", "buffered_arange", ".", "buf", ".", "numel", "(", ")", ":", "\n", "        ", "torch", ".", "arange", "(", "max", ",", "out", "=", "buffered_arange", ".", "buf", ")", "\n", "", "return", "buffered_arange", ".", "buf", "[", ":", "max", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.convert_padding_direction": [[285, 305], ["src_tokens.eq", "src_tokens.size", "buffered_arange().type_as().expand_as", "src_tokens.eq.long().sum", "src_tokens.gather", "src_tokens.eq.max", "torch.remainder", "torch.remainder", "buffered_arange().type_as", "src_tokens.eq.long", "utils.buffered_arange"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.buffered_arange"], ["", "def", "convert_padding_direction", "(", "\n", "src_tokens", ",", "\n", "src_lengths", ",", "\n", "padding_idx", ",", "\n", "right_to_left", "=", "False", ",", "\n", "left_to_right", "=", "False", ",", "\n", ")", ":", "\n", "    ", "assert", "right_to_left", "^", "left_to_right", "\n", "pad_mask", "=", "src_tokens", ".", "eq", "(", "padding_idx", ")", "\n", "if", "pad_mask", ".", "max", "(", ")", "==", "0", ":", "\n", "# no padding, return early", "\n", "        ", "return", "src_tokens", "\n", "", "max_len", "=", "src_tokens", ".", "size", "(", "1", ")", "\n", "range", "=", "buffered_arange", "(", "max_len", ")", ".", "type_as", "(", "src_tokens", ")", ".", "expand_as", "(", "src_tokens", ")", "\n", "num_pads", "=", "pad_mask", ".", "long", "(", ")", ".", "sum", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "if", "right_to_left", ":", "\n", "        ", "index", "=", "torch", ".", "remainder", "(", "range", "-", "num_pads", ",", "max_len", ")", "\n", "", "else", ":", "\n", "        ", "index", "=", "torch", ".", "remainder", "(", "range", "+", "num_pads", ",", "max_len", ")", "\n", "", "return", "src_tokens", ".", "gather", "(", "1", ",", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.item": [[306, 312], ["hasattr", "hasattr", "tensor.item"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.item"], ["", "def", "item", "(", "tensor", ")", ":", "\n", "    ", "if", "hasattr", "(", "tensor", ",", "'item'", ")", ":", "\n", "        ", "return", "tensor", ".", "item", "(", ")", "\n", "", "if", "hasattr", "(", "tensor", ",", "'__getitem__'", ")", ":", "\n", "        ", "return", "tensor", "[", "0", "]", "\n", "", "return", "tensor", "\n", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.adagrad.Adagrad.__init__": [[15, 18], ["FairseqOptimizer.__init__", "torch.optim.Adagrad"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "self", ".", "_optimizer", "=", "torch", ".", "optim", ".", "Adagrad", "(", "params", ",", "**", "self", ".", "optimizer_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.adagrad.Adagrad.optimizer_config": [[19, 30], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"", "\n", "return", "{", "\n", "'lr'", ":", "self", ".", "args", ".", "lr", "[", "0", "]", ",", "\n", "'weight_decay'", ":", "self", ".", "args", ".", "weight_decay", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.__init__": [[13, 17], ["object.__init__"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "params", "=", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.add_args": [[18, 22], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add optimizer-specific arguments to the parser.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.optimizer": [[23, 31], ["hasattr", "isinstance", "ValueError"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return a torch.optim.optimizer.Optimizer instance.\"\"\"", "\n", "if", "not", "hasattr", "(", "self", ",", "'_optimizer'", ")", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "if", "not", "isinstance", "(", "self", ".", "_optimizer", ",", "torch", ".", "optim", ".", "Optimizer", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'_optimizer must be an instance of torch.optim.Optimizer'", ")", "\n", "", "return", "self", ".", "_optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.optimizer_config": [[32, 41], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.get_lr": [[42, 45], ["None"], "methods", ["None"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the current learning rate.\"\"\"", "\n", "return", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.set_lr": [[46, 50], ["None"], "methods", ["None"], ["", "def", "set_lr", "(", "self", ",", "lr", ")", ":", "\n", "        ", "\"\"\"Set the learning rate.\"\"\"", "\n", "for", "param_group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "            ", "param_group", "[", "'lr'", "]", "=", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.state_dict": [[51, 54], ["fairseq_optimizer.FairseqOptimizer.optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.state_dict"], ["", "", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the optimizer's state dict.\"\"\"", "\n", "return", "self", ".", "optimizer", ".", "state_dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.load_state_dict": [[55, 68], ["fairseq_optimizer.FairseqOptimizer.optimizer.load_state_dict", "group.update"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.load_state_dict", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "\"\"\"Load an optimizer state dict.\n\n        In general we should prefer the configuration of the existing optimizer\n        instance (e.g., learning rate) over that found in the state_dict. This\n        allows us to resume training from a checkpoint using a new set of\n        optimizer args.\n        \"\"\"", "\n", "self", ".", "optimizer", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n", "# override learning rate, momentum, etc. with latest values", "\n", "for", "group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "            ", "group", ".", "update", "(", "self", ".", "optimizer_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.step": [[69, 72], ["fairseq_optimizer.FairseqOptimizer.optimizer.step"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fixed_schedule.FixedSchedule.step"], ["", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\"\"\"", "\n", "return", "self", ".", "optimizer", ".", "step", "(", "closure", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.zero_grad": [[73, 76], ["fairseq_optimizer.FairseqOptimizer.optimizer.zero_grad"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.zero_grad"], ["", "def", "zero_grad", "(", "self", ")", ":", "\n", "        ", "\"\"\"Clears the gradients of all optimized parameters.\"\"\"", "\n", "return", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.adam.FairseqAdam.__init__": [[17, 20], ["FairseqOptimizer.__init__", "adam.Adam"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "self", ".", "_optimizer", "=", "Adam", "(", "params", ",", "**", "self", ".", "optimizer_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.adam.FairseqAdam.add_args": [[21, 26], ["parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add optimizer-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--adam-betas'", ",", "default", "=", "'(0.9, 0.999)'", ",", "metavar", "=", "'B'", ",", "\n", "help", "=", "'betas for Adam optimizer'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.adam.FairseqAdam.optimizer_config": [[27, 39], ["eval"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"", "\n", "return", "{", "\n", "'lr'", ":", "self", ".", "args", ".", "lr", "[", "0", "]", ",", "\n", "'betas'", ":", "eval", "(", "self", ".", "args", ".", "adam_betas", ")", ",", "\n", "'weight_decay'", ":", "self", ".", "args", ".", "weight_decay", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.adam.Adam.__init__": [[69, 74], ["dict", "super().__init__"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "amsgrad", "=", "False", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "amsgrad", "=", "amsgrad", ")", "\n", "super", "(", "Adam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.adam.Adam.step": [[75, 136], ["closure", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "p.data.addcdiv_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.max", "torch.max", "torch.max", "torch.max", "max_exp_avg_sq.sqrt().add_", "exp_avg_sq.sqrt().add_", "p.data.add_", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "exp_avg.mul_", "exp_avg_sq.mul_", "math.sqrt", "max_exp_avg_sq.sqrt", "exp_avg_sq.sqrt"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "", "amsgrad", "=", "group", "[", "'amsgrad'", "]", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "if", "amsgrad", ":", "\n", "                    ", "max_exp_avg_sq", "=", "state", "[", "'max_exp_avg_sq'", "]", "\n", "", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "                    ", "torch", ".", "max", "(", "max_exp_avg_sq", ",", "exp_avg_sq", ",", "out", "=", "max_exp_avg_sq", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "\n", "", "bias_correction1", "=", "1", "-", "beta1", "**", "state", "[", "'step'", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "state", "[", "'step'", "]", "\n", "step_size", "=", "group", "[", "'lr'", "]", "*", "math", ".", "sqrt", "(", "bias_correction2", ")", "/", "bias_correction1", "\n", "\n", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p", ".", "data", ".", "add_", "(", "-", "group", "[", "'weight_decay'", "]", "*", "group", "[", "'lr'", "]", ",", "p", ".", "data", ")", "\n", "\n", "", "p", ".", "data", ".", "addcdiv_", "(", "-", "step_size", ",", "exp_avg", ",", "denom", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.__init__.build_optimizer": [[18, 20], ["None"], "function", ["None"], []], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.__init__.register_optimizer": [[22, 39], ["OPTIMIZER_CLASS_NAMES.add", "ValueError", "issubclass", "ValueError", "ValueError"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.add"], []], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.nag.FairseqNAG.__init__": [[15, 18], ["FairseqOptimizer.__init__", "nag.NAG"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "self", ".", "_optimizer", "=", "NAG", "(", "params", ",", "**", "self", ".", "optimizer_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.nag.FairseqNAG.optimizer_config": [[19, 31], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"", "\n", "return", "{", "\n", "'lr'", ":", "self", ".", "args", ".", "lr", "[", "0", "]", ",", "\n", "'momentum'", ":", "self", ".", "args", ".", "momentum", ",", "\n", "'weight_decay'", ":", "self", ".", "args", ".", "weight_decay", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.nag.NAG.__init__": [[35, 38], ["dict", "torch.optim.optimizer.Optimizer.__init__"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "required", ",", "momentum", "=", "0", ",", "weight_decay", "=", "0", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "lr_old", "=", "lr", ",", "momentum", "=", "momentum", ",", "weight_decay", "=", "weight_decay", ")", "\n", "super", "(", "NAG", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.nag.NAG.step": [[39, 78], ["closure", "group.get", "p.data.add_", "p.data.add_", "buf.mul_().add_", "d_p.clone().zero_", "p.data.mul_", "buf.mul_", "d_p.clone"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "weight_decay", "=", "group", "[", "'weight_decay'", "]", "\n", "momentum", "=", "group", "[", "'momentum'", "]", "\n", "lr", "=", "group", "[", "'lr'", "]", "\n", "lr_old", "=", "group", ".", "get", "(", "'lr_old'", ",", "lr", ")", "\n", "lr_correct", "=", "lr", "/", "lr_old", "\n", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "d_p", "=", "p", ".", "grad", ".", "data", "\n", "param_state", "=", "self", ".", "state", "[", "p", "]", "\n", "if", "'momentum_buffer'", "not", "in", "param_state", ":", "\n", "                    ", "param_state", "[", "'momentum_buffer'", "]", "=", "d_p", ".", "clone", "(", ")", ".", "zero_", "(", ")", "\n", "\n", "", "buf", "=", "param_state", "[", "'momentum_buffer'", "]", "\n", "\n", "if", "weight_decay", "!=", "0", ":", "\n", "                    ", "p", ".", "data", ".", "mul_", "(", "1", "-", "lr", "*", "weight_decay", ")", "\n", "", "p", ".", "data", ".", "add_", "(", "momentum", "*", "momentum", "*", "lr_correct", ",", "buf", ")", "\n", "p", ".", "data", ".", "add_", "(", "-", "(", "1", "+", "momentum", ")", "*", "lr", ",", "d_p", ")", "\n", "\n", "buf", ".", "mul_", "(", "momentum", "*", "lr_correct", ")", ".", "add_", "(", "-", "lr", ",", "d_p", ")", "\n", "\n", "", "group", "[", "'lr_old'", "]", "=", "lr", "\n", "\n", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.sgd.SGD.__init__": [[15, 18], ["FairseqOptimizer.__init__", "torch.optim.SGD"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "self", ".", "_optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "params", ",", "**", "self", ".", "optimizer_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.sgd.SGD.optimizer_config": [[19, 31], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"", "\n", "return", "{", "\n", "'lr'", ":", "self", ".", "args", ".", "lr", "[", "0", "]", ",", "\n", "'momentum'", ":", "self", ".", "args", ".", "momentum", ",", "\n", "'weight_decay'", ":", "self", ".", "args", ".", "weight_decay", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.reduce_lr_on_plateau.ReduceLROnPlateau.__init__": [[17, 26], ["FairseqLRScheduler.__init__", "torch.optim.lr_scheduler.ReduceLROnPlateau", "len", "ValueError"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "optimizer", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "optimizer", ")", "\n", "if", "len", "(", "args", ".", "lr", ")", ">", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'Cannot use a fixed learning rate schedule with reduce_lr_on_plateau.'", "\n", "' Consider --lr-scheduler=fixed instead.'", "\n", ")", "\n", "", "self", ".", "lr_scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "ReduceLROnPlateau", "(", "\n", "self", ".", "optimizer", ".", "optimizer", ",", "patience", "=", "0", ",", "factor", "=", "args", ".", "lr_shrink", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.reduce_lr_on_plateau.ReduceLROnPlateau.state_dict": [[27, 32], ["None"], "methods", ["None"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the LR scheduler state dict.\"\"\"", "\n", "return", "{", "\n", "'best'", ":", "self", ".", "lr_scheduler", ".", "best", ",", "\n", "'last_epoch'", ":", "self", ".", "lr_scheduler", ".", "last_epoch", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.reduce_lr_on_plateau.ReduceLROnPlateau.load_state_dict": [[34, 39], ["None"], "methods", ["None"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "\"\"\"Load an LR scheduler state dict.\"\"\"", "\n", "self", ".", "lr_scheduler", ".", "best", "=", "state_dict", "[", "'best'", "]", "\n", "if", "'last_epoch'", "in", "state_dict", ":", "\n", "            ", "self", ".", "lr_scheduler", ".", "last_epoch", "=", "state_dict", "[", "'last_epoch'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.reduce_lr_on_plateau.ReduceLROnPlateau.step": [[40, 47], ["reduce_lr_on_plateau.ReduceLROnPlateau.optimizer.get_lr", "reduce_lr_on_plateau.ReduceLROnPlateau.lr_scheduler.step"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.get_lr", "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fixed_schedule.FixedSchedule.step"], ["", "", "def", "step", "(", "self", ",", "epoch", ",", "val_loss", "=", "None", ")", ":", "\n", "        ", "\"\"\"Update the learning rate at the end of the given epoch.\"\"\"", "\n", "if", "val_loss", "is", "not", "None", ":", "\n", "            ", "self", ".", "lr_scheduler", ".", "step", "(", "val_loss", ",", "epoch", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "lr_scheduler", ".", "last_epoch", "=", "epoch", "\n", "", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.inverse_square_root_schedule.InverseSquareRootSchedule.__init__": [[34, 54], ["FairseqLRScheduler.__init__", "inverse_square_root_schedule.InverseSquareRootSchedule.optimizer.set_lr", "len", "ValueError"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__", "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.set_lr"], ["def", "__init__", "(", "self", ",", "args", ",", "optimizer", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "optimizer", ")", "\n", "if", "len", "(", "args", ".", "lr", ")", ">", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'Cannot use a fixed learning rate schedule with inverse_sqrt.'", "\n", "' Consider --lr-scheduler=fixed instead.'", "\n", ")", "\n", "", "warmup_end_lr", "=", "args", ".", "lr", "[", "0", "]", "\n", "if", "args", ".", "warmup_init_lr", "<", "0", ":", "\n", "            ", "args", ".", "warmup_init_lr", "=", "warmup_end_lr", "\n", "\n", "# linearly warmup for the first args.warmup_updates", "\n", "", "self", ".", "lr_step", "=", "(", "warmup_end_lr", "-", "args", ".", "warmup_init_lr", ")", "/", "args", ".", "warmup_updates", "\n", "\n", "# then, decay prop. to the inverse square root of the update number", "\n", "self", ".", "decay_factor", "=", "warmup_end_lr", "*", "args", ".", "warmup_updates", "**", "0.5", "\n", "\n", "# initial learning rate", "\n", "self", ".", "lr", "=", "args", ".", "warmup_init_lr", "\n", "self", ".", "optimizer", ".", "set_lr", "(", "self", ".", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.inverse_square_root_schedule.InverseSquareRootSchedule.add_args": [[55, 62], ["parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add arguments to the parser for this LR scheduler.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--warmup-updates'", ",", "default", "=", "4000", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'warmup the learning rate linearly for the first N updates'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-init-lr'", ",", "default", "=", "-", "1", ",", "type", "=", "float", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'initial learning rate during warmup phase; default is args.lr'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.inverse_square_root_schedule.InverseSquareRootSchedule.step": [[63, 68], ["super().step", "inverse_square_root_schedule.InverseSquareRootSchedule.optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fixed_schedule.FixedSchedule.step", "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.get_lr"], ["", "def", "step", "(", "self", ",", "epoch", ",", "val_loss", "=", "None", ")", ":", "\n", "        ", "\"\"\"Update the learning rate at the end of the given epoch.\"\"\"", "\n", "super", "(", ")", ".", "step", "(", "epoch", ",", "val_loss", ")", "\n", "# we don't change the learning rate at epoch boundaries", "\n", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.inverse_square_root_schedule.InverseSquareRootSchedule.step_update": [[69, 77], ["inverse_square_root_schedule.InverseSquareRootSchedule.optimizer.set_lr"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.set_lr"], ["", "def", "step_update", "(", "self", ",", "num_updates", ")", ":", "\n", "        ", "\"\"\"Update the learning rate after each update.\"\"\"", "\n", "if", "num_updates", "<", "self", ".", "args", ".", "warmup_updates", ":", "\n", "            ", "self", ".", "lr", "+=", "self", ".", "lr_step", "\n", "", "else", ":", "\n", "            ", "self", ".", "lr", "=", "self", ".", "decay_factor", "*", "num_updates", "**", "-", "0.5", "\n", "", "self", ".", "optimizer", ".", "set_lr", "(", "self", ".", "lr", ")", "\n", "return", "self", ".", "lr", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.__init__": [[13, 20], ["object.__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "optimizer", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "optimizer", ",", "FairseqOptimizer", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'optimizer must be an instance of FairseqOptimizer'", ")", "\n", "", "self", ".", "args", "=", "args", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "best", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.add_args": [[21, 25], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add arguments to the parser for this LR scheduler.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.state_dict": [[26, 29], ["None"], "methods", ["None"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the LR scheduler state dict.\"\"\"", "\n", "return", "{", "'best'", ":", "self", ".", "best", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict": [[30, 33], ["None"], "methods", ["None"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "\"\"\"Load an LR scheduler state dict.\"\"\"", "\n", "self", ".", "best", "=", "state_dict", "[", "'best'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.step": [[34, 41], ["min"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "epoch", ",", "val_loss", "=", "None", ")", ":", "\n", "        ", "\"\"\"Update the learning rate at the end of the given epoch.\"\"\"", "\n", "if", "val_loss", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "best", "is", "None", ":", "\n", "                ", "self", ".", "best", "=", "val_loss", "\n", "", "else", ":", "\n", "                ", "self", ".", "best", "=", "min", "(", "self", ".", "best", ",", "val_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.step_update": [[42, 45], ["fairseq_lr_scheduler.FairseqLRScheduler.optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.get_lr"], ["", "", "", "def", "step_update", "(", "self", ",", "num_updates", ")", ":", "\n", "        ", "\"\"\"Update the learning rate after each update.\"\"\"", "\n", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fixed_schedule.FixedSchedule.__init__": [[17, 21], ["FairseqLRScheduler.__init__", "torch.optim.lr_scheduler.LambdaLR"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "optimizer", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "optimizer", ")", "\n", "self", ".", "lr_scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "LambdaLR", "(", "\n", "self", ".", "optimizer", ".", "optimizer", ",", "self", ".", "anneal", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fixed_schedule.FixedSchedule.add_args": [[22, 27], ["parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add arguments to the parser for this LR scheduler.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--force-anneal'", ",", "'--fa'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'force annealing at specified epoch'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fixed_schedule.FixedSchedule.anneal": [[28, 37], ["min", "len"], "methods", ["None"], ["", "def", "anneal", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "lrs", "=", "self", ".", "args", ".", "lr", "\n", "if", "self", ".", "args", ".", "force_anneal", "is", "None", "or", "epoch", "<", "self", ".", "args", ".", "force_anneal", ":", "\n", "# use fixed LR schedule", "\n", "            ", "next_lr", "=", "lrs", "[", "min", "(", "epoch", ",", "len", "(", "lrs", ")", "-", "1", ")", "]", "\n", "", "else", ":", "\n", "# annneal based on lr_shrink", "\n", "            ", "next_lr", "=", "lrs", "[", "-", "1", "]", "*", "self", ".", "args", ".", "lr_shrink", "**", "(", "epoch", "+", "1", "-", "self", ".", "args", ".", "force_anneal", ")", "\n", "", "return", "next_lr", "/", "lrs", "[", "0", "]", "# correct for scaling from LambdaLR", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fixed_schedule.FixedSchedule.step": [[38, 43], ["super().step", "fixed_schedule.FixedSchedule.lr_scheduler.step", "fixed_schedule.FixedSchedule.optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fixed_schedule.FixedSchedule.step", "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.fixed_schedule.FixedSchedule.step", "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.get_lr"], ["", "def", "step", "(", "self", ",", "epoch", ",", "val_loss", "=", "None", ")", ":", "\n", "        ", "\"\"\"Update the learning rate at the end of the given epoch.\"\"\"", "\n", "super", "(", ")", ".", "step", "(", "epoch", ",", "val_loss", ")", "\n", "self", ".", "lr_scheduler", ".", "step", "(", "epoch", ")", "\n", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.__init__.build_lr_scheduler": [[17, 19], ["None"], "function", ["None"], []], "home.repos.pwc.inspect_result.shashiongithub_XSum.lr_scheduler.__init__.register_lr_scheduler": [[21, 33], ["ValueError", "issubclass", "ValueError"], "function", ["None"], []], "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.grad_multiply.GradMultiply.forward": [[12, 18], ["x.new", "ctx.mark_shared_storage"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ",", "scale", ")", ":", "\n", "        ", "ctx", ".", "scale", "=", "scale", "\n", "res", "=", "x", ".", "new", "(", "x", ")", "\n", "ctx", ".", "mark_shared_storage", "(", "(", "x", ",", "res", ")", ")", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.grad_multiply.GradMultiply.backward": [[19, 22], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad", ")", ":", "\n", "        ", "return", "grad", "*", "ctx", ".", "scale", ",", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.learned_positional_embedding.LearnedPositionalEmbedding.__init__": [[20, 23], ["torch.Embedding.__init__"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["def", "__init__", "(", "self", ",", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ",", "left_pad", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", "\n", "self", ".", "left_pad", "=", "left_pad", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.learned_positional_embedding.LearnedPositionalEmbedding.forward": [[24, 33], ["super().forward", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "input.data.new().fill_", "learned_positional_embedding.LearnedPositionalEmbedding.make_positions", "input.data.new", "input.size"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.forward", "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.learned_positional_embedding.LearnedPositionalEmbedding.make_positions"], ["", "def", "forward", "(", "self", ",", "input", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "\"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "# positions is the same for every token when decoding a single step", "\n", "            ", "positions", "=", "Variable", "(", "\n", "input", ".", "data", ".", "new", "(", "1", ",", "1", ")", ".", "fill_", "(", "self", ".", "padding_idx", "+", "input", ".", "size", "(", "1", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "positions", "=", "Variable", "(", "self", ".", "make_positions", "(", "input", ".", "data", ")", ")", "\n", "", "return", "super", "(", ")", ".", "forward", "(", "positions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.learned_positional_embedding.LearnedPositionalEmbedding.max_positions": [[34, 37], ["None"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum number of supported positions.\"\"\"", "\n", "return", "self", ".", "num_embeddings", "-", "self", ".", "padding_idx", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.learned_positional_embedding.LearnedPositionalEmbedding.make_positions": [[38, 52], ["input.size", "input.ne", "learned_positional_embedding.LearnedPositionalEmbedding.range_buf[].expand_as", "input.clone().masked_scatter_", "hasattr", "input.new", "learned_positional_embedding.LearnedPositionalEmbedding.range_buf.numel", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "input.ne.long().sum().unsqueeze", "input.clone", "input.ne.size", "input.ne.long().sum", "input.ne.long"], "methods", ["None"], ["", "def", "make_positions", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\"Replace non-padding symbols with their position numbers.\"\"\"", "\n", "if", "not", "hasattr", "(", "self", ",", "'range_buf'", ")", ":", "\n", "            ", "self", ".", "range_buf", "=", "input", ".", "new", "(", ")", "\n", "", "seqlen", "=", "input", ".", "size", "(", "1", ")", "\n", "if", "self", ".", "range_buf", ".", "numel", "(", ")", "<", "seqlen", ":", "\n", "# offset positions by the padding index", "\n", "            ", "torch", ".", "arange", "(", "self", ".", "padding_idx", "+", "1", ",", "self", ".", "padding_idx", "+", "1", "+", "seqlen", ",", "\n", "out", "=", "self", ".", "range_buf", ")", "\n", "", "mask", "=", "input", ".", "ne", "(", "self", ".", "padding_idx", ")", "\n", "positions", "=", "self", ".", "range_buf", "[", ":", "seqlen", "]", ".", "expand_as", "(", "input", ")", "\n", "if", "self", ".", "left_pad", ":", "\n", "            ", "positions", "=", "positions", "-", "mask", ".", "size", "(", "1", ")", "+", "mask", ".", "long", "(", ")", ".", "sum", "(", "dim", "=", "1", ")", ".", "unsqueeze", "(", "1", ")", "\n", "", "return", "input", ".", "clone", "(", ")", ".", "masked_scatter_", "(", "mask", ",", "positions", "[", "mask", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.beamable_mm.BeamableMM.__init__": [[20, 23], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["def", "__init__", "(", "self", ",", "beam_size", "=", "None", ")", ":", "\n", "        ", "super", "(", "BeamableMM", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "beam_size", "=", "beam_size", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.beamable_mm.BeamableMM.forward": [[24, 47], ["input1[].unfold().transpose", "input1[].unfold().transpose.bmm.view", "input1[].unfold().transpose.bmm", "input1[].unfold().transpose.dim", "input1[].unfold().transpose.size", "input1[].unfold().transpose.size", "input2.unfold", "input1[].unfold().transpose.size", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "input1[].unfold().transpose.bmm", "input1[].unfold"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input1", ",", "input2", ")", ":", "\n", "        ", "if", "(", "\n", "not", "self", ".", "training", "and", "# test mode", "\n", "self", ".", "beam_size", "is", "not", "None", "and", "# beam size is set", "\n", "input1", ".", "dim", "(", ")", "==", "3", "and", "# only support batched input", "\n", "input1", ".", "size", "(", "1", ")", "==", "1", "# single time step update", "\n", ")", ":", "\n", "            ", "bsz", ",", "beam", "=", "input1", ".", "size", "(", "0", ")", ",", "self", ".", "beam_size", "\n", "\n", "# bsz x 1 x nhu --> bsz/beam x beam x nhu", "\n", "input1", "=", "input1", "[", ":", ",", "0", ",", ":", "]", ".", "unfold", "(", "0", ",", "beam", ",", "beam", ")", ".", "transpose", "(", "2", ",", "1", ")", "\n", "\n", "# bsz x sz2 x nhu --> bsz/beam x sz2 x nhu", "\n", "input2", "=", "input2", ".", "unfold", "(", "0", ",", "beam", ",", "beam", ")", "[", ":", ",", ":", ",", ":", ",", "0", "]", "\n", "\n", "# use non batched operation if bsz = beam", "\n", "if", "input1", ".", "size", "(", "0", ")", "==", "1", ":", "\n", "                ", "output", "=", "torch", ".", "mm", "(", "input1", "[", "0", ",", ":", ",", ":", "]", ",", "input2", "[", "0", ",", ":", ",", ":", "]", ")", "\n", "", "else", ":", "\n", "                ", "output", "=", "input1", ".", "bmm", "(", "input2", ")", "\n", "", "return", "output", ".", "view", "(", "bsz", ",", "1", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "return", "input1", ".", "bmm", "(", "input2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.beamable_mm.BeamableMM.set_beam_size": [[48, 50], ["None"], "methods", ["None"], ["", "", "def", "set_beam_size", "(", "self", ",", "beam_size", ")", ":", "\n", "        ", "self", ".", "beam_size", "=", "beam_size", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.conv_tbc.ConvTBC.__init__": [[20, 30], ["super().__init__", "torch.nn.modules.utils._single", "torch.nn.modules.utils._single", "torch.nn.Parameter", "torch.nn.Parameter", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "padding", "=", "0", ")", ":", "\n", "        ", "super", "(", "ConvTBC", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "kernel_size", "=", "_single", "(", "kernel_size", ")", "\n", "self", ".", "padding", "=", "_single", "(", "padding", ")", "\n", "\n", "self", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "\n", "self", ".", "kernel_size", "[", "0", "]", ",", "in_channels", ",", "out_channels", ")", ")", "\n", "self", ".", "bias", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "out_channels", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.conv_tbc.ConvTBC.forward": [[31, 33], ["input.contiguous().conv_tbc", "input.contiguous"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "return", "input", ".", "contiguous", "(", ")", ".", "conv_tbc", "(", "self", ".", "weight", ",", "self", ".", "bias", ",", "self", ".", "padding", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.conv_tbc.ConvTBC.__repr__": [[34, 41], ["s.format"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "s", "=", "(", "'{name}({in_channels}, {out_channels}, kernel_size={kernel_size}'", "\n", "', padding={padding}'", ")", "\n", "if", "self", ".", "bias", "is", "None", ":", "\n", "            ", "s", "+=", "', bias=False'", "\n", "", "s", "+=", "')'", "\n", "return", "s", ".", "format", "(", "name", "=", "self", ".", "__class__", ".", "__name__", ",", "**", "self", ".", "__dict__", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.linearized_convolution.LinearizedConvolution.__init__": [[24, 28], ["conv_tbc.ConvTBC.__init__", "linearized_convolution.LinearizedConvolution.register_backward_hook"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "self", ".", "_linearized_weight", "=", "None", "\n", "self", ".", "register_backward_hook", "(", "self", ".", "_clear_linearized_weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.linearized_convolution.LinearizedConvolution.forward": [[29, 66], ["linearized_convolution.LinearizedConvolution._get_linearized_weight", "fairseq.utils.volatile_variable.size", "torch.linear.view", "super().forward", "linearized_convolution.LinearizedConvolution._get_input_buffer", "fairseq.utils.volatile_variable", "fairseq.utils.maybe_no_grad", "torch.linear", "fairseq.utils.volatile_variable.new().zero_", "linearized_convolution.LinearizedConvolution._set_input_buffer", "input_buffer[].clone", "fairseq.utils.volatile_variable.view", "fairseq.utils.volatile_variable.new", "fairseq.utils.volatile_variable.size"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.modules.linearized_convolution.LinearizedConvolution._get_linearized_weight", "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.forward", "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.linearized_convolution.LinearizedConvolution._get_input_buffer", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.volatile_variable", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.maybe_no_grad", "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.linearized_convolution.LinearizedConvolution._set_input_buffer"], ["", "def", "forward", "(", "self", ",", "input", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Input:\n            Time x Batch x Channel during training\n            Batch x Time x Channel during inference\n        Args:\n            incremental_state: Used to buffer signal; if not None, then input is\n                expected to contain a single frame. If the input order changes\n                between time steps, call reorder_incremental_state.\n        \"\"\"", "\n", "if", "incremental_state", "is", "None", ":", "\n", "            ", "output", "=", "super", "(", ")", ".", "forward", "(", "input", ")", "\n", "if", "self", ".", "kernel_size", "[", "0", "]", ">", "1", "and", "self", ".", "padding", "[", "0", "]", ">", "0", ":", "\n", "# remove future timesteps added by padding", "\n", "                ", "output", "=", "output", "[", ":", "-", "self", ".", "padding", "[", "0", "]", ",", ":", ",", ":", "]", "\n", "", "return", "output", "\n", "\n", "# reshape weight", "\n", "", "weight", "=", "self", ".", "_get_linearized_weight", "(", ")", "\n", "kw", "=", "self", ".", "kernel_size", "[", "0", "]", "\n", "\n", "bsz", "=", "input", ".", "size", "(", "0", ")", "# input: bsz x len x dim", "\n", "if", "kw", ">", "1", ":", "\n", "            ", "input", "=", "input", ".", "data", "\n", "input_buffer", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "input_buffer", "is", "None", ":", "\n", "                ", "input_buffer", "=", "input", ".", "new", "(", "bsz", ",", "kw", ",", "input", ".", "size", "(", "2", ")", ")", ".", "zero_", "(", ")", "\n", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "input_buffer", ")", "\n", "", "else", ":", "\n", "# shift buffer", "\n", "                ", "input_buffer", "[", ":", ",", ":", "-", "1", ",", ":", "]", "=", "input_buffer", "[", ":", ",", "1", ":", ",", ":", "]", ".", "clone", "(", ")", "\n", "# append next input", "\n", "", "input_buffer", "[", ":", ",", "-", "1", ",", ":", "]", "=", "input", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "input", "=", "utils", ".", "volatile_variable", "(", "input_buffer", ")", "\n", "", "with", "utils", ".", "maybe_no_grad", "(", ")", ":", "\n", "            ", "output", "=", "F", ".", "linear", "(", "input", ".", "view", "(", "bsz", ",", "-", "1", ")", ",", "weight", ",", "self", ".", "bias", ")", "\n", "", "return", "output", ".", "view", "(", "bsz", ",", "1", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.linearized_convolution.LinearizedConvolution.reorder_incremental_state": [[67, 72], ["linearized_convolution.LinearizedConvolution._get_input_buffer", "input_buffer.index_select.index_select.index_select", "linearized_convolution.LinearizedConvolution._set_input_buffer"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.modules.linearized_convolution.LinearizedConvolution._get_input_buffer", "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.linearized_convolution.LinearizedConvolution._set_input_buffer"], ["", "def", "reorder_incremental_state", "(", "self", ",", "incremental_state", ",", "new_order", ")", ":", "\n", "        ", "input_buffer", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "input_buffer", "is", "not", "None", ":", "\n", "            ", "input_buffer", "=", "input_buffer", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "input_buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.linearized_convolution.LinearizedConvolution._get_input_buffer": [[73, 75], ["fairseq.utils.get_incremental_state"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.get_incremental_state"], ["", "", "def", "_get_input_buffer", "(", "self", ",", "incremental_state", ")", ":", "\n", "        ", "return", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'input_buffer'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.linearized_convolution.LinearizedConvolution._set_input_buffer": [[76, 78], ["fairseq.utils.set_incremental_state"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.set_incremental_state"], ["", "def", "_set_input_buffer", "(", "self", ",", "incremental_state", ",", "new_buffer", ")", ":", "\n", "        ", "return", "utils", ".", "set_incremental_state", "(", "self", ",", "incremental_state", ",", "'input_buffer'", ",", "new_buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.linearized_convolution.LinearizedConvolution._get_linearized_weight": [[79, 86], ["linearized_convolution.LinearizedConvolution.weight.transpose().transpose().contiguous", "linearized_convolution.LinearizedConvolution.view", "linearized_convolution.LinearizedConvolution.size", "linearized_convolution.LinearizedConvolution.weight.transpose().transpose", "linearized_convolution.LinearizedConvolution.weight.transpose"], "methods", ["None"], ["", "def", "_get_linearized_weight", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_linearized_weight", "is", "None", ":", "\n", "            ", "kw", "=", "self", ".", "kernel_size", "[", "0", "]", "\n", "weight", "=", "self", ".", "weight", ".", "transpose", "(", "2", ",", "1", ")", ".", "transpose", "(", "1", ",", "0", ")", ".", "contiguous", "(", ")", "\n", "assert", "weight", ".", "size", "(", ")", "==", "(", "self", ".", "out_channels", ",", "kw", ",", "self", ".", "in_channels", ")", "\n", "self", ".", "_linearized_weight", "=", "weight", ".", "view", "(", "self", ".", "out_channels", ",", "-", "1", ")", "\n", "", "return", "self", ".", "_linearized_weight", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.modules.linearized_convolution.LinearizedConvolution._clear_linearized_weight": [[87, 89], ["None"], "methods", ["None"], ["", "def", "_clear_linearized_weight", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "self", ".", "_linearized_weight", "=", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_encoder.FairseqEncoder.__init__": [[14, 17], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["    ", "\"\"\"Base class for encoders.\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_encoder.FairseqEncoder.forward": [[18, 20], ["None"], "methods", ["None"], ["self", ".", "dictionary", "=", "dictionary", "\n", "\n", "", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ",", "src_doctopic", ",", "src_wordtopics", ")", ":", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_encoder.FairseqEncoder.max_positions": [[21, 24], ["None"], "methods", ["None"], ["        ", "raise", "NotImplementedError", "\n", "\n", "", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the encoder.\"\"\"", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_encoder.FairseqEncoder.upgrade_state_dict": [[25, 27], ["None"], "methods", ["None"], ["raise", "NotImplementedError", "\n", "\n", "", "def", "upgrade_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_incremental_decoder.FairseqIncrementalDecoder.__init__": [[14, 16], ["FairseqDecoder.__init__"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["def", "__init__", "(", "self", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_incremental_decoder.FairseqIncrementalDecoder.forward": [[17, 19], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_incremental_decoder.FairseqIncrementalDecoder.reorder_incremental_state": [[20, 31], ["fairseq_incremental_decoder.FairseqIncrementalDecoder.apply", "hasattr", "module.reorder_incremental_state"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.LSTMDecoder.reorder_incremental_state"], ["", "def", "reorder_incremental_state", "(", "self", ",", "incremental_state", ",", "new_order", ")", ":", "\n", "        ", "\"\"\"Reorder incremental state.\n\n        This should be called when the order of the input has changed from the\n        previous time step. A typical use case is beam search, where the input\n        order changes between time steps based on the selection of beams.\n        \"\"\"", "\n", "def", "apply_reorder_incremental_state", "(", "module", ")", ":", "\n", "            ", "if", "module", "!=", "self", "and", "hasattr", "(", "module", ",", "'reorder_incremental_state'", ")", ":", "\n", "                ", "module", ".", "reorder_incremental_state", "(", "incremental_state", ",", "new_order", ")", "\n", "", "", "self", ".", "apply", "(", "apply_reorder_incremental_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_incremental_decoder.FairseqIncrementalDecoder.set_beam_size": [[32, 40], ["getattr", "fairseq_incremental_decoder.FairseqIncrementalDecoder.apply", "hasattr", "module.set_beam_size"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_incremental_decoder.FairseqIncrementalDecoder.set_beam_size"], ["", "def", "set_beam_size", "(", "self", ",", "beam_size", ")", ":", "\n", "        ", "\"\"\"Sets the beam size in the decoder and all children.\"\"\"", "\n", "if", "getattr", "(", "self", ",", "'_beam_size'", ",", "-", "1", ")", "!=", "beam_size", ":", "\n", "            ", "def", "apply_set_beam_size", "(", "module", ")", ":", "\n", "                ", "if", "module", "!=", "self", "and", "hasattr", "(", "module", ",", "'set_beam_size'", ")", ":", "\n", "                    ", "module", ".", "set_beam_size", "(", "beam_size", ")", "\n", "", "", "self", ".", "apply", "(", "apply_set_beam_size", ")", "\n", "self", ".", "_beam_size", "=", "beam_size", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.LSTMModel.__init__": [[21, 23], ["FairseqModel.__init__"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.LSTMModel.add_args": [[24, 51], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of encoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of decoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-out-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder output embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention'", ",", "type", "=", "str", ",", "metavar", "=", "'BOOL'", ",", "\n", "help", "=", "'decoder attention'", ")", "\n", "\n", "# Granular dropout settings (if not specified these default to --dropout)", "\n", "parser", ".", "add_argument", "(", "'--encoder-dropout-in'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for encoder input embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-dropout-out'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for encoder output'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-dropout-in'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for decoder input embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-dropout-out'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for decoder output'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.LSTMModel.build_model": [[52, 73], ["lstm.LSTMEncoder", "lstm.LSTMDecoder", "cls", "bool"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "src_dict", ",", "dst_dict", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "encoder", "=", "LSTMEncoder", "(", "\n", "src_dict", ",", "\n", "embed_dim", "=", "args", ".", "encoder_embed_dim", ",", "\n", "num_layers", "=", "args", ".", "encoder_layers", ",", "\n", "dropout_in", "=", "args", ".", "encoder_dropout_in", ",", "\n", "dropout_out", "=", "args", ".", "encoder_dropout_out", ",", "\n", ")", "\n", "decoder", "=", "LSTMDecoder", "(", "\n", "dst_dict", ",", "\n", "encoder_embed_dim", "=", "args", ".", "encoder_embed_dim", ",", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", ",", "\n", "out_embed_dim", "=", "args", ".", "decoder_out_embed_dim", ",", "\n", "num_layers", "=", "args", ".", "decoder_layers", ",", "\n", "attention", "=", "bool", "(", "args", ".", "decoder_attention", ")", ",", "\n", "dropout_in", "=", "args", ".", "decoder_dropout_in", ",", "\n", "dropout_out", "=", "args", ".", "decoder_dropout_out", ",", "\n", ")", "\n", "return", "cls", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.LSTMEncoder.__init__": [[77, 94], ["FairseqEncoder.__init__", "len", "dictionary.pad", "lstm.Embedding", "lstm.LSTM"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Embedding", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.LSTM"], ["def", "__init__", "(", "self", ",", "dictionary", ",", "embed_dim", "=", "512", ",", "num_layers", "=", "1", ",", "dropout_in", "=", "0.1", ",", "\n", "dropout_out", "=", "0.1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "dropout_in", "=", "dropout_in", "\n", "self", ".", "dropout_out", "=", "dropout_out", "\n", "\n", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "self", ".", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "self", ".", "embed_tokens", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "self", ".", "padding_idx", ")", "\n", "\n", "self", ".", "lstm", "=", "LSTM", "(", "\n", "input_size", "=", "embed_dim", ",", "\n", "hidden_size", "=", "embed_dim", ",", "\n", "num_layers", "=", "num_layers", ",", "\n", "dropout", "=", "self", ".", "dropout_out", ",", "\n", "bidirectional", "=", "False", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.LSTMEncoder.forward": [[96, 133], ["fairseq.utils.convert_padding_direction.size", "lstm.LSTMEncoder.embed_tokens", "torch.dropout", "torch.dropout", "torch.dropout", "torch.dropout.size", "torch.dropout.transpose", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "lstm.LSTMEncoder.lstm", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.dropout", "torch.dropout", "torch.dropout", "fairseq.utils.convert_padding_direction", "src_lengths.data.tolist", "torch.dropout.data.new().zero_", "torch.dropout.data.new().zero_", "list", "torch.dropout.size", "torch.dropout.data.new", "torch.dropout.data.new"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.convert_padding_direction"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "if", "LanguagePairDataset", ".", "LEFT_PAD_SOURCE", ":", "\n", "# convert left-padding to right-padding", "\n", "            ", "src_tokens", "=", "utils", ".", "convert_padding_direction", "(", "\n", "src_tokens", ",", "\n", "src_lengths", ",", "\n", "self", ".", "padding_idx", ",", "\n", "left_to_right", "=", "True", ",", "\n", ")", "\n", "\n", "", "bsz", ",", "seqlen", "=", "src_tokens", ".", "size", "(", ")", "\n", "\n", "# embed tokens", "\n", "x", "=", "self", ".", "embed_tokens", "(", "src_tokens", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout_in", ",", "training", "=", "self", ".", "training", ")", "\n", "embed_dim", "=", "x", ".", "size", "(", "2", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# pack embedded source tokens into a PackedSequence", "\n", "packed_x", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "x", ",", "src_lengths", ".", "data", ".", "tolist", "(", ")", ")", "\n", "\n", "# apply LSTM", "\n", "h0", "=", "Variable", "(", "x", ".", "data", ".", "new", "(", "self", ".", "num_layers", ",", "bsz", ",", "embed_dim", ")", ".", "zero_", "(", ")", ")", "\n", "c0", "=", "Variable", "(", "x", ".", "data", ".", "new", "(", "self", ".", "num_layers", ",", "bsz", ",", "embed_dim", ")", ".", "zero_", "(", ")", ")", "\n", "packed_outs", ",", "(", "final_hiddens", ",", "final_cells", ")", "=", "self", ".", "lstm", "(", "\n", "packed_x", ",", "\n", "(", "h0", ",", "c0", ")", ",", "\n", ")", "\n", "\n", "# unpack outputs and apply dropout", "\n", "x", ",", "_", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "packed_outs", ",", "padding_value", "=", "0.", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout_out", ",", "training", "=", "self", ".", "training", ")", "\n", "assert", "list", "(", "x", ".", "size", "(", ")", ")", "==", "[", "seqlen", ",", "bsz", ",", "embed_dim", "]", "\n", "\n", "return", "x", ",", "final_hiddens", ",", "final_cells", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.LSTMEncoder.max_positions": [[134, 137], ["int"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the encoder.\"\"\"", "\n", "return", "int", "(", "1e5", ")", "# an arbitrary large number", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.AttentionLayer.__init__": [[140, 145], ["torch.Module.__init__", "lstm.Linear", "lstm.Linear"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Linear", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Linear"], ["    ", "def", "__init__", "(", "self", ",", "input_embed_dim", ",", "output_embed_dim", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "input_proj", "=", "Linear", "(", "input_embed_dim", ",", "output_embed_dim", ",", "bias", "=", "False", ")", "\n", "self", ".", "output_proj", "=", "Linear", "(", "2", "*", "output_embed_dim", ",", "output_embed_dim", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.AttentionLayer.forward": [[146, 162], ["lstm.AttentionLayer.input_proj", "torch.softmax().t", "torch.softmax().t", "torch.softmax().t", "torch.tanh", "torch.tanh", "torch.tanh", "lstm.AttentionLayer.output_proj", "torch.softmax", "torch.softmax", "torch.softmax", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tanh.unsqueeze", "torch.softmax().t.t", "torch.softmax().t.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "source_hids", ")", ":", "\n", "# input: bsz x input_embed_dim", "\n", "# source_hids: srclen x bsz x output_embed_dim", "\n", "\n", "# x: bsz x output_embed_dim", "\n", "        ", "x", "=", "self", ".", "input_proj", "(", "input", ")", "\n", "\n", "# compute attention", "\n", "attn_scores", "=", "(", "source_hids", "*", "x", ".", "unsqueeze", "(", "0", ")", ")", ".", "sum", "(", "dim", "=", "2", ")", "\n", "attn_scores", "=", "F", ".", "softmax", "(", "attn_scores", ".", "t", "(", ")", ",", "dim", "=", "1", ")", ".", "t", "(", ")", "# srclen x bsz", "\n", "\n", "# sum weighted sources", "\n", "x", "=", "(", "attn_scores", ".", "unsqueeze", "(", "2", ")", "*", "source_hids", ")", ".", "sum", "(", "dim", "=", "0", ")", "\n", "\n", "x", "=", "F", ".", "tanh", "(", "self", ".", "output_proj", "(", "torch", ".", "cat", "(", "(", "x", ",", "input", ")", ",", "dim", "=", "1", ")", ")", ")", "\n", "return", "x", ",", "attn_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.LSTMDecoder.__init__": [[166, 185], ["FairseqIncrementalDecoder.__init__", "len", "dictionary.pad", "lstm.Embedding", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "lstm.AttentionLayer", "lstm.Linear", "lstm.Linear", "lstm.LSTMCell", "range"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Embedding", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Linear", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Linear", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.LSTMCell"], ["def", "__init__", "(", "self", ",", "dictionary", ",", "encoder_embed_dim", "=", "512", ",", "embed_dim", "=", "512", ",", "\n", "out_embed_dim", "=", "512", ",", "num_layers", "=", "1", ",", "dropout_in", "=", "0.1", ",", "\n", "dropout_out", "=", "0.1", ",", "attention", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "dropout_in", "=", "dropout_in", "\n", "self", ".", "dropout_out", "=", "dropout_out", "\n", "\n", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "self", ".", "embed_tokens", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "\n", "LSTMCell", "(", "encoder_embed_dim", "+", "embed_dim", "if", "layer", "==", "0", "else", "embed_dim", ",", "embed_dim", ")", "\n", "for", "layer", "in", "range", "(", "num_layers", ")", "\n", "]", ")", "\n", "self", ".", "attention", "=", "AttentionLayer", "(", "encoder_embed_dim", ",", "embed_dim", ")", "\n", "if", "embed_dim", "!=", "out_embed_dim", ":", "\n", "            ", "self", ".", "additional_fc", "=", "Linear", "(", "embed_dim", ",", "out_embed_dim", ")", "\n", "", "self", ".", "fc_out", "=", "Linear", "(", "out_embed_dim", ",", "num_embeddings", ",", "dropout", "=", "dropout_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.LSTMDecoder.forward": [[186, 261], ["prev_output_tokens.size", "encoder_outs.size", "lstm.LSTMDecoder.embed_tokens", "torch.dropout", "torch.dropout", "torch.dropout", "torch.dropout.size", "torch.dropout.transpose", "fairseq.utils.get_incremental_state", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "range", "fairseq.utils.set_incremental_state", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.dropout.transpose", "attn_scores.transpose.transpose.transpose", "hasattr", "lstm.LSTMDecoder.fc_out", "len", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.dropout.data.new().zero_", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "enumerate", "lstm.LSTMDecoder.attention", "torch.dropout", "torch.dropout", "torch.dropout", "outs.append", "lstm.LSTMDecoder.additional_fc", "torch.dropout", "torch.dropout", "torch.dropout", "torch.dropout.data.new().zero_", "rnn", "torch.dropout", "torch.dropout", "torch.dropout", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "range", "range", "torch.dropout.data.new", "torch.dropout.data.new"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.get_incremental_state", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.set_incremental_state"], ["", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "prev_output_tokens", "=", "prev_output_tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "", "bsz", ",", "seqlen", "=", "prev_output_tokens", ".", "size", "(", ")", "\n", "\n", "# get outputs from encoder", "\n", "encoder_outs", ",", "_", ",", "_", "=", "encoder_out", "\n", "srclen", "=", "encoder_outs", ".", "size", "(", "0", ")", "\n", "\n", "# embed tokens", "\n", "x", "=", "self", ".", "embed_tokens", "(", "prev_output_tokens", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout_in", ",", "training", "=", "self", ".", "training", ")", "\n", "embed_dim", "=", "x", ".", "size", "(", "2", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# initialize previous states (or get from cache during incremental generation)", "\n", "cached_state", "=", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'cached_state'", ")", "\n", "if", "cached_state", "is", "not", "None", ":", "\n", "            ", "prev_hiddens", ",", "prev_cells", ",", "input_feed", "=", "cached_state", "\n", "", "else", ":", "\n", "            ", "_", ",", "encoder_hiddens", ",", "encoder_cells", "=", "encoder_out", "\n", "num_layers", "=", "len", "(", "self", ".", "layers", ")", "\n", "prev_hiddens", "=", "[", "encoder_hiddens", "[", "i", "]", "for", "i", "in", "range", "(", "num_layers", ")", "]", "\n", "prev_cells", "=", "[", "encoder_cells", "[", "i", "]", "for", "i", "in", "range", "(", "num_layers", ")", "]", "\n", "input_feed", "=", "Variable", "(", "x", ".", "data", ".", "new", "(", "bsz", ",", "embed_dim", ")", ".", "zero_", "(", ")", ")", "\n", "\n", "", "attn_scores", "=", "Variable", "(", "x", ".", "data", ".", "new", "(", "srclen", ",", "seqlen", ",", "bsz", ")", ".", "zero_", "(", ")", ")", "\n", "outs", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "seqlen", ")", ":", "\n", "# input feeding: concatenate context vector from previous time step", "\n", "            ", "input", "=", "torch", ".", "cat", "(", "(", "x", "[", "j", ",", ":", ",", ":", "]", ",", "input_feed", ")", ",", "dim", "=", "1", ")", "\n", "\n", "for", "i", ",", "rnn", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "# recurrent cell", "\n", "                ", "hidden", ",", "cell", "=", "rnn", "(", "input", ",", "(", "prev_hiddens", "[", "i", "]", ",", "prev_cells", "[", "i", "]", ")", ")", "\n", "\n", "# hidden state becomes the input to the next layer", "\n", "input", "=", "F", ".", "dropout", "(", "hidden", ",", "p", "=", "self", ".", "dropout_out", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# save state for next time step", "\n", "prev_hiddens", "[", "i", "]", "=", "hidden", "\n", "prev_cells", "[", "i", "]", "=", "cell", "\n", "\n", "# apply attention using the last layer's hidden state", "\n", "", "out", ",", "attn_scores", "[", ":", ",", "j", ",", ":", "]", "=", "self", ".", "attention", "(", "hidden", ",", "encoder_outs", ")", "\n", "out", "=", "F", ".", "dropout", "(", "out", ",", "p", "=", "self", ".", "dropout_out", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# input feeding", "\n", "input_feed", "=", "out", "\n", "\n", "# save final output", "\n", "outs", ".", "append", "(", "out", ")", "\n", "\n", "# cache previous states (no-op except during incremental generation)", "\n", "", "utils", ".", "set_incremental_state", "(", "\n", "self", ",", "incremental_state", ",", "'cached_state'", ",", "(", "prev_hiddens", ",", "prev_cells", ",", "input_feed", ")", ")", "\n", "\n", "# collect outputs across time steps", "\n", "x", "=", "torch", ".", "cat", "(", "outs", ",", "dim", "=", "0", ")", ".", "view", "(", "seqlen", ",", "bsz", ",", "embed_dim", ")", "\n", "\n", "# T x B x C -> B x T x C", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "0", ")", "\n", "\n", "# srclen x tgtlen x bsz -> bsz x tgtlen x srclen", "\n", "attn_scores", "=", "attn_scores", ".", "transpose", "(", "0", ",", "2", ")", "\n", "\n", "# project back to size of vocabulary", "\n", "if", "hasattr", "(", "self", ",", "'additional_fc'", ")", ":", "\n", "            ", "x", "=", "self", ".", "additional_fc", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout_out", ",", "training", "=", "self", ".", "training", ")", "\n", "", "x", "=", "self", ".", "fc_out", "(", "x", ")", "\n", "\n", "return", "x", ",", "attn_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.LSTMDecoder.reorder_incremental_state": [[262, 276], ["fairseq.utils.get_incremental_state", "tuple", "fairseq.utils.set_incremental_state", "isinstance", "state.index_select", "isinstance", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "map", "lstm.LSTMDecoder.reorder_incremental_state.reorder_state"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.get_incremental_state", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.set_incremental_state"], ["", "def", "reorder_incremental_state", "(", "self", ",", "incremental_state", ",", "new_order", ")", ":", "\n", "        ", "cached_state", "=", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'cached_state'", ")", "\n", "if", "cached_state", "is", "None", ":", "\n", "            ", "return", "\n", "\n", "", "def", "reorder_state", "(", "state", ")", ":", "\n", "            ", "if", "isinstance", "(", "state", ",", "list", ")", ":", "\n", "                ", "return", "[", "reorder_state", "(", "state_i", ")", "for", "state_i", "in", "state", "]", "\n", "", "return", "state", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "new_order", ",", "Variable", ")", ":", "\n", "            ", "new_order", "=", "Variable", "(", "new_order", ")", "\n", "", "new_state", "=", "tuple", "(", "map", "(", "reorder_state", ",", "cached_state", ")", ")", "\n", "utils", ".", "set_incremental_state", "(", "self", ",", "incremental_state", ",", "'cached_state'", ",", "new_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.LSTMDecoder.max_positions": [[277, 280], ["int"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum output length supported by the decoder.\"\"\"", "\n", "return", "int", "(", "1e5", ")", "# an arbitrary large number", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.Embedding": [[282, 286], ["torch.Embedding", "nn.Embedding.weight.data.uniform_"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Embedding"], ["", "", "def", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "padding_idx", ")", "\n", "m", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.LSTM": [[288, 294], ["torch.LSTM", "nn.LSTM.named_parameters", "param.data.uniform_"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.LSTM"], ["", "def", "LSTM", "(", "input_size", ",", "hidden_size", ",", "**", "kwargs", ")", ":", "\n", "    ", "m", "=", "nn", ".", "LSTM", "(", "input_size", ",", "hidden_size", ",", "**", "kwargs", ")", "\n", "for", "name", ",", "param", "in", "m", ".", "named_parameters", "(", ")", ":", "\n", "        ", "if", "'weight'", "in", "name", "or", "'bias'", "in", "name", ":", "\n", "            ", "param", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.LSTMCell": [[296, 302], ["torch.LSTMCell", "nn.LSTMCell.named_parameters", "param.data.uniform_"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.LSTMCell"], ["", "def", "LSTMCell", "(", "input_size", ",", "hidden_size", ",", "**", "kwargs", ")", ":", "\n", "    ", "m", "=", "nn", ".", "LSTMCell", "(", "input_size", ",", "hidden_size", ",", "**", "kwargs", ")", "\n", "for", "name", ",", "param", "in", "m", ".", "named_parameters", "(", ")", ":", "\n", "        ", "if", "'weight'", "in", "name", "or", "'bias'", "in", "name", ":", "\n", "            ", "param", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.Linear": [[304, 311], ["torch.Linear", "nn.Linear.weight.data.uniform_", "nn.Linear.bias.data.uniform_"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Linear"], ["", "def", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "True", ",", "dropout", "=", "0", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Linear layer (input: N x T x C)\"\"\"", "\n", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "bias", ")", "\n", "m", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "if", "bias", ":", "\n", "        ", "m", ".", "bias", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.base_architecture": [[313, 325], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.__init__.register_model_architecture"], ["", "@", "register_model_architecture", "(", "'lstm'", ",", "'lstm'", ")", "\n", "def", "base_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "1", ")", "\n", "args", ".", "encoder_dropout_in", "=", "getattr", "(", "args", ",", "'encoder_dropout_in'", ",", "args", ".", "dropout", ")", "\n", "args", ".", "encoder_dropout_out", "=", "getattr", "(", "args", ",", "'encoder_dropout_out'", ",", "args", ".", "dropout", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "1", ")", "\n", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_attention", "=", "getattr", "(", "args", ",", "'decoder_attention'", ",", "True", ")", "\n", "args", ".", "decoder_dropout_in", "=", "getattr", "(", "args", ",", "'decoder_dropout_in'", ",", "args", ".", "dropout", ")", "\n", "args", ".", "decoder_dropout_out", "=", "getattr", "(", "args", ",", "'decoder_dropout_out'", ",", "args", ".", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.lstm_wiseman_iwslt_de_en": [[327, 339], ["register_model_architecture", "lstm.base_architecture"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.base_architecture"], ["", "@", "register_model_architecture", "(", "'lstm'", ",", "'lstm_wiseman_iwslt_de_en'", ")", "\n", "def", "lstm_wiseman_iwslt_de_en", "(", "args", ")", ":", "\n", "    ", "base_architecture", "(", "args", ")", "\n", "args", ".", "encoder_embed_dim", "=", "256", "\n", "args", ".", "encoder_layers", "=", "1", "\n", "args", ".", "encoder_dropout_in", "=", "0", "\n", "args", ".", "encoder_dropout_out", "=", "0", "\n", "args", ".", "decoder_embed_dim", "=", "256", "\n", "args", ".", "decoder_layers", "=", "1", "\n", "args", ".", "decoder_out_embed_dim", "=", "256", "\n", "args", ".", "decoder_attention", "=", "True", "\n", "args", ".", "decoder_dropout_in", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.lstm.lstm_luong_wmt_en_de": [[341, 352], ["register_model_architecture", "lstm.base_architecture"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.base_architecture"], ["", "@", "register_model_architecture", "(", "'lstm'", ",", "'lstm_luong_wmt_en_de'", ")", "\n", "def", "lstm_luong_wmt_en_de", "(", "args", ")", ":", "\n", "    ", "base_architecture", "(", "args", ")", "\n", "args", ".", "encoder_embed_dim", "=", "1000", "\n", "args", ".", "encoder_layers", "=", "4", "\n", "args", ".", "encoder_dropout_out", "=", "0", "\n", "args", ".", "decoder_embed_dim", "=", "1000", "\n", "args", ".", "decoder_layers", "=", "4", "\n", "args", ".", "decoder_out_embed_dim", "=", "1000", "\n", "args", ".", "decoder_attention", "=", "True", "\n", "args", ".", "decoder_dropout_out", "=", "0", "\n", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.FConvModel.__init__": [[22, 25], ["FairseqModel.__init__", "sum"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["@", "register_model", "(", "'fconv'", ")", "\n", "class", "FConvModel", "(", "FairseqModel", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.FConvModel.add_args": [[26, 45], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["self", ".", "encoder", ".", "num_attention_layers", "=", "sum", "(", "layer", "is", "not", "None", "for", "layer", "in", "decoder", ".", "attention", ")", "\n", "\n", "", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-layers'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'encoder layers [(dim, kernel_size), ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'decoder layers [(dim, kernel_size), ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-out-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder output embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'decoder attention [True, ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-input-output-embed'", ",", "action", "=", "'store_true'", ",", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.FConvModel.build_model": [[48, 69], ["fconv.FConvEncoder", "fconv.FConvDecoder", "fconv.FConvModel", "eval", "eval", "eval"], "methods", ["None"], ["' to be equal)'", ")", "\n", "\n", "", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "src_dict", ",", "dst_dict", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "encoder", "=", "FConvEncoder", "(", "\n", "src_dict", ",", "\n", "embed_dim", "=", "args", ".", "encoder_embed_dim", ",", "\n", "convolutions", "=", "eval", "(", "args", ".", "encoder_layers", ")", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "max_positions", "=", "args", ".", "max_source_positions", ",", "\n", ")", "\n", "decoder", "=", "FConvDecoder", "(", "\n", "dst_dict", ",", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", ",", "\n", "convolutions", "=", "eval", "(", "args", ".", "decoder_layers", ")", ",", "\n", "out_embed_dim", "=", "args", ".", "decoder_out_embed_dim", ",", "\n", "attention", "=", "eval", "(", "args", ".", "decoder_attention", ")", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "max_positions", "=", "args", ".", "max_target_positions", ",", "\n", "share_embed", "=", "args", ".", "share_input_output_embed", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.FConvEncoder.__init__": [[73, 102], ["FairseqEncoder.__init__", "len", "dictionary.pad", "fconv.Embedding", "fconv.PositionalEmbedding", "fconv.Linear", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "fconv.Linear", "fconv.FConvEncoder.projections.append", "fconv.FConvEncoder.convolutions.append", "fconv.ConvTBC", "fconv.Linear"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Embedding", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.PositionalEmbedding", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Linear", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Linear", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.ConvTBC", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Linear"], ["", "", "class", "FConvEncoder", "(", "FairseqEncoder", ")", ":", "\n", "    ", "\"\"\"Convolutional encoder\"\"\"", "\n", "def", "__init__", "(", "self", ",", "dictionary", ",", "embed_dim", "=", "512", ",", "max_positions", "=", "1024", ",", "\n", "convolutions", "=", "(", "(", "512", ",", "3", ")", ",", ")", "*", "20", ",", "dropout", "=", "0.1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "num_attention_layers", "=", "None", "\n", "\n", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "self", ".", "embed_tokens", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "max_positions", ",", "\n", "embed_dim", ",", "\n", "padding_idx", ",", "\n", "left_pad", "=", "LanguagePairDataset", ".", "LEFT_PAD_SOURCE", ",", "\n", ")", "\n", "\n", "in_channels", "=", "convolutions", "[", "0", "]", "[", "0", "]", "\n", "# Shashi", "\n", "self", ".", "fc1", "=", "Linear", "(", "embed_dim", "+", "embed_dim", ",", "in_channels", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "projections", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "convolutions", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "(", "out_channels", ",", "kernel_size", ")", "in", "convolutions", ":", "\n", "            ", "self", ".", "projections", ".", "append", "(", "Linear", "(", "in_channels", ",", "out_channels", ")", "\n", "if", "in_channels", "!=", "out_channels", "else", "None", ")", "\n", "self", ".", "convolutions", ".", "append", "(", "\n", "ConvTBC", "(", "in_channels", ",", "out_channels", "*", "2", ",", "kernel_size", ",", "\n", "dropout", "=", "dropout", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.FConvEncoder.forward": [[103, 139], ["torch.dropout", "torch.dropout", "torch.dropout", "fconv.FConvEncoder.fc1", "torch.glu.transpose", "zip", "torch.glu.transpose", "fconv.FConvEncoder.fc2", "fairseq.modules.GradMultiply.apply", "fconv.FConvEncoder.embed_tokens", "fconv.FConvEncoder.embed_positions", "torch.dropout", "torch.dropout", "torch.dropout", "torch.pad", "torch.pad", "torch.pad", "conv", "torch.glu", "torch.glu", "torch.glu", "math.sqrt", "proj", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.pad"], ["in_channels", "=", "out_channels", "\n", "", "self", ".", "fc2", "=", "Linear", "(", "in_channels", ",", "embed_dim", "+", "embed_dim", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ",", "src_doctopic", ",", "src_wordtopics", ")", ":", "\n", "# embed tokens and positions", "\n", "# print(self.embed_tokens(src_tokens), self.embed_positions(src_tokens), src_doctopic, src_wordtopics)", "\n", "\n", "# ''' 1)", "\n", "# src_doctopic: batchsize x 512", "\n", "# src_wordtopics: batchsize x wordcount x 512", "\n", "        ", "src_doctopic_ext", "=", "src_doctopic", ".", "unsqueeze", "(", "1", ")", "# batchsize x 1 x 512", "\n", "# print(src_doctopic_ext)", "\n", "src_wordtopics_doctopic", "=", "src_wordtopics", "*", "src_doctopic_ext", "# batchsize x wordcount x 512", "\n", "# print(src_wordtopics_doctopic)", "\n", "# ''' ", "\n", "\n", "''' 2)\n        # src_doctopic: batchsize x 512\n        # src_wordtopics: batchsize x wordcount x 512\n        src_doctopic_ext = src_doctopic.unsqueeze(1) # batchsize x 1 x 512\n        # print(src_doctopic_ext)\n        src_wordtopics_doctopic = src_wordtopics * src_doctopic_ext # batchsize x wordcount x 512\n        # print(src_wordtopics_doctopic)\n\t# Normalize src_wordtopics_doctopic (April 29th)\n        src_wordtopics_doctopic = F.normalize(src_wordtopics_doctopic, p=2, dim=2)\n        '''", "\n", "\n", "\n", "x", "=", "self", ".", "embed_tokens", "(", "src_tokens", ")", "+", "self", ".", "embed_positions", "(", "src_tokens", ")", "# batchsize x wordcount x 512", "\n", "# print(x)", "\n", "\n", "# Concat wordtopics*doctopic to (wordembedding+posembedding)", "\n", "x", "=", "torch", ".", "cat", "(", "(", "x", ",", "src_wordtopics_doctopic", ")", ",", "2", ")", "\n", "# print(x)", "\n", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "input_embedding", "=", "x", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.FConvEncoder.max_positions": [[140, 143], ["fconv.FConvEncoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_decoder.FairseqDecoder.max_positions"], ["\n", "# project to size of convolution", "\n", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.AttentionLayer.__init__": [[146, 154], ["torch.Module.__init__", "fconv.Linear", "fconv.Linear"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Linear", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Linear"], ["\n", "# temporal convolutions", "\n", "for", "proj", ",", "conv", "in", "zip", "(", "self", ".", "projections", ",", "self", ".", "convolutions", ")", ":", "\n", "            ", "residual", "=", "x", "if", "proj", "is", "None", "else", "proj", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "padding_l", "=", "(", "conv", ".", "kernel_size", "[", "0", "]", "-", "1", ")", "//", "2", "\n", "padding_r", "=", "conv", ".", "kernel_size", "[", "0", "]", "//", "2", "\n", "x", "=", "F", ".", "pad", "(", "x", ",", "(", "0", ",", "0", ",", "0", ",", "0", ",", "padding_l", ",", "padding_r", ")", ")", "\n", "x", "=", "conv", "(", "x", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.AttentionLayer.forward": [[155, 177], ["fconv.AttentionLayer.bmm", "fconv.AttentionLayer.size", "torch.softmax", "torch.softmax", "torch.softmax", "fconv.AttentionLayer.view", "fconv.AttentionLayer.bmm", "encoder_out[].size", "math.sqrt", "fconv.AttentionLayer.view", "math.sqrt", "fconv.AttentionLayer.in_projection", "math.sqrt", "fconv.AttentionLayer.out_projection"], "methods", ["None"], ["x", "=", "F", ".", "glu", "(", "x", ",", "dim", "=", "2", ")", "\n", "x", "=", "(", "x", "+", "residual", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "\n", "# T x B x C -> B x T x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "1", ",", "0", ")", "\n", "\n", "# project back to size of embedding", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "\n", "# scale gradients (this only affects backward, not forward)", "\n", "x", "=", "GradMultiply", ".", "apply", "(", "x", ",", "1.0", "/", "(", "2.0", "*", "self", ".", "num_attention_layers", ")", ")", "\n", "\n", "# add output to input embedding for attention", "\n", "y", "=", "(", "x", "+", "input_embedding", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "\n", "# print(x,y)", "\n", "return", "x", ",", "y", "\n", "\n", "", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the encoder.\"\"\"", "\n", "return", "self", ".", "embed_positions", ".", "max_positions", "(", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.AttentionLayer.make_generation_fast_": [[178, 183], ["fconv.AttentionLayer.add_module", "fairseq.modules.BeamableMM"], "methods", ["None"], ["", "", "class", "AttentionLayer", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "conv_channels", ",", "embed_dim", ",", "bmm", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# projects from output of convolution to embedding dimension", "\n", "self", ".", "in_projection", "=", "Linear", "(", "conv_channels", ",", "embed_dim", "+", "embed_dim", ")", "\n", "# projects from embedding dimension to convolution size", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.FConvDecoder.__init__": [[187, 235], ["FairseqIncrementalDecoder.__init__", "fconv.FConvDecoder.register_buffer", "isinstance", "len", "dictionary.pad", "fconv.Embedding", "fconv.PositionalEmbedding", "fconv.Linear", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "enumerate", "fconv.Linear", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "ValueError", "fconv.FConvDecoder.projections.append", "fconv.FConvDecoder.convolutions.append", "fconv.FConvDecoder.attention.append", "torch.Linear", "torch.Linear", "torch.Linear", "fconv.Linear", "len", "isinstance", "len", "len", "fconv.LinearizedConv1d", "fconv.Linear", "fconv.AttentionLayer"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Embedding", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.PositionalEmbedding", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Linear", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Linear", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Linear", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Linear", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Linear", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Linear", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.LinearizedConv1d", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Linear"], ["\n", "", "def", "forward", "(", "self", ",", "x", ",", "target_embedding", ",", "encoder_out", ")", ":", "\n", "        ", "residual", "=", "x", "\n", "\n", "# attention", "\n", "x", "=", "(", "self", ".", "in_projection", "(", "x", ")", "+", "target_embedding", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "x", "=", "self", ".", "bmm", "(", "x", ",", "encoder_out", "[", "0", "]", ")", "\n", "\n", "# softmax over last dim", "\n", "sz", "=", "x", ".", "size", "(", ")", "\n", "x", "=", "F", ".", "softmax", "(", "x", ".", "view", "(", "sz", "[", "0", "]", "*", "sz", "[", "1", "]", ",", "sz", "[", "2", "]", ")", ",", "dim", "=", "1", ")", "\n", "x", "=", "x", ".", "view", "(", "sz", ")", "\n", "attn_scores", "=", "x", "\n", "\n", "x", "=", "self", ".", "bmm", "(", "x", ",", "encoder_out", "[", "1", "]", ")", "\n", "\n", "# scale attention output", "\n", "s", "=", "encoder_out", "[", "1", "]", ".", "size", "(", "1", ")", "\n", "x", "=", "x", "*", "(", "s", "*", "math", ".", "sqrt", "(", "1.0", "/", "s", ")", ")", "\n", "\n", "# project back", "\n", "x", "=", "(", "self", ".", "out_projection", "(", "x", ")", "+", "residual", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "return", "x", ",", "attn_scores", "\n", "\n", "", "def", "make_generation_fast_", "(", "self", ",", "beamable_mm_beam_size", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Replace torch.bmm with BeamableMM.\"\"\"", "\n", "if", "beamable_mm_beam_size", "is", "not", "None", ":", "\n", "            ", "del", "self", ".", "bmm", "\n", "self", ".", "add_module", "(", "'bmm'", ",", "BeamableMM", "(", "beamable_mm_beam_size", ")", ")", "\n", "\n", "\n", "", "", "", "class", "FConvDecoder", "(", "FairseqIncrementalDecoder", ")", ":", "\n", "    ", "\"\"\"Convolutional decoder\"\"\"", "\n", "def", "__init__", "(", "self", ",", "dictionary", ",", "embed_dim", "=", "512", ",", "out_embed_dim", "=", "256", ",", "\n", "max_positions", "=", "1024", ",", "convolutions", "=", "(", "(", "512", ",", "3", ")", ",", ")", "*", "20", ",", "\n", "attention", "=", "True", ",", "dropout", "=", "0.1", ",", "share_embed", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "2", "]", ")", ")", "\n", "self", ".", "dropout", "=", "dropout", "\n", "\n", "in_channels", "=", "convolutions", "[", "0", "]", "[", "0", "]", "\n", "if", "isinstance", "(", "attention", ",", "bool", ")", ":", "\n", "# expand True into [True, True, ...] and do the same with False", "\n", "            ", "attention", "=", "[", "attention", "]", "*", "len", "(", "convolutions", ")", "\n", "", "if", "not", "isinstance", "(", "attention", ",", "list", ")", "or", "len", "(", "attention", ")", "!=", "len", "(", "convolutions", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'Attention is expected to be a list of booleans of '", "\n", "'length equal to the number of layers.'", ")", "\n", "\n", "", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.FConvDecoder.forward": [[236, 287], ["fconv.FConvDecoder._split_encoder_out", "fconv.FConvDecoder._embed_tokens", "fconv.FConvDecoder.embed_positions", "torch.dropout", "torch.dropout", "torch.dropout", "fconv.FConvDecoder.fc1", "fconv.FConvDecoder._transpose_if_training", "len", "zip", "fconv.FConvDecoder._transpose_if_training", "fconv.FConvDecoder.fc2", "torch.dropout", "torch.dropout", "torch.dropout", "fconv.FConvDecoder.fc3", "torch.dropout", "torch.dropout", "torch.dropout", "conv", "torch.glu", "torch.glu", "torch.glu", "proj", "fconv.FConvDecoder._transpose_if_training", "attention", "fconv.FConvDecoder._transpose_if_training", "math.sqrt", "avg_attn_scores.add_"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.FConvDecoder._split_encoder_out", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.FConvDecoder._embed_tokens", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.FConvDecoder._transpose_if_training", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.FConvDecoder._transpose_if_training", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.FConvDecoder._transpose_if_training", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.FConvDecoder._transpose_if_training"], ["padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "self", ".", "embed_tokens", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "max_positions", ",", "\n", "embed_dim", ",", "\n", "padding_idx", ",", "\n", "left_pad", "=", "LanguagePairDataset", ".", "LEFT_PAD_TARGET", ",", "\n", ")", "\n", "\n", "self", ".", "fc1", "=", "Linear", "(", "embed_dim", "+", "embed_dim", ",", "in_channels", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "projections", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "convolutions", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "attention", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", ",", "(", "out_channels", ",", "kernel_size", ")", "in", "enumerate", "(", "convolutions", ")", ":", "\n", "            ", "self", ".", "projections", ".", "append", "(", "Linear", "(", "in_channels", ",", "out_channels", ")", "\n", "if", "in_channels", "!=", "out_channels", "else", "None", ")", "\n", "self", ".", "convolutions", ".", "append", "(", "\n", "LinearizedConv1d", "(", "in_channels", ",", "out_channels", "*", "2", ",", "kernel_size", ",", "\n", "padding", "=", "(", "kernel_size", "-", "1", ")", ",", "dropout", "=", "dropout", ")", "\n", ")", "\n", "self", ".", "attention", ".", "append", "(", "AttentionLayer", "(", "out_channels", ",", "embed_dim", ")", "\n", "if", "attention", "[", "i", "]", "else", "None", ")", "\n", "in_channels", "=", "out_channels", "\n", "", "self", ".", "fc2", "=", "Linear", "(", "in_channels", ",", "out_embed_dim", ")", "\n", "if", "share_embed", ":", "\n", "            ", "assert", "out_embed_dim", "==", "embed_dim", ",", "\"Shared embed weights implies same dimensions \"", "\" out_embed_dim={} vs embed_dim={}\"", ".", "format", "(", "out_embed_dim", ",", "embed_dim", ")", "\n", "self", ".", "fc3", "=", "nn", ".", "Linear", "(", "out_embed_dim", ",", "num_embeddings", ")", "\n", "self", ".", "fc3", ".", "weight", "=", "self", ".", "embed_tokens", ".", "weight", "\n", "", "else", ":", "\n", "            ", "self", ".", "fc3", "=", "Linear", "(", "out_embed_dim", ",", "num_embeddings", ",", "dropout", "=", "dropout", ")", "\n", "\n", "", "", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", ",", "src_doctopic", ",", "incremental_state", "=", "None", ")", ":", "\n", "# split and transpose encoder outputs", "\n", "        ", "encoder_a", ",", "encoder_b", "=", "self", ".", "_split_encoder_out", "(", "encoder_out", ",", "incremental_state", ")", "\n", "# print(encoder_a.size(), encoder_b.size())", "\n", "\n", "# embed tokens and combine with positional embeddings", "\n", "x", "=", "self", ".", "_embed_tokens", "(", "prev_output_tokens", ",", "incremental_state", ")", "\n", "x", "+=", "self", ".", "embed_positions", "(", "prev_output_tokens", ",", "incremental_state", ")", "\n", "# print(x.size())", "\n", "\n", "# Add doctopic vector in the decoder", "\n", "# src_doctopic: batchsize x 512", "\n", "src_doctopic_ext", "=", "src_doctopic", ".", "unsqueeze", "(", "1", ")", "# batchsize x 1 x 512", "\n", "# print(src_doctopic_ext.size())", "\n", "src_doctopic_ext", "=", "src_doctopic_ext", ".", "repeat", "(", "1", ",", "x", ".", "size", "(", ")", "[", "1", "]", ",", "1", ")", "\n", "# print(src_doctopic_ext.size())", "\n", "\n", "# Concat doctopic to (wordembedding+posembedding)", "\n", "x", "=", "torch", ".", "cat", "(", "(", "x", ",", "src_doctopic_ext", ")", ",", "2", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.FConvDecoder.max_positions": [[288, 291], ["fconv.FConvDecoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_decoder.FairseqDecoder.max_positions"], ["# print(x.size())", "\n", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "target_embedding", "=", "x", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.FConvDecoder.upgrade_state_dict": [[292, 301], ["enumerate", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "state_dict.get", "torch.utils.remove_weight_norm", "torch.utils.remove_weight_norm", "torch.utils.remove_weight_norm", "torch.utils.weight_norm", "torch.utils.weight_norm", "torch.utils.weight_norm", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["None"], ["# print(\"Before FC1 \", x.size())", "\n", "\n", "# project to size of convolution", "\n", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "# print(\"FC1 \", x.size())", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "self", ".", "_transpose_if_training", "(", "x", ",", "incremental_state", ")", "\n", "\n", "# temporal convolutions", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.FConvDecoder._embed_tokens": [[302, 307], ["fconv.FConvDecoder.embed_tokens"], "methods", ["None"], ["avg_attn_scores", "=", "None", "\n", "num_attn_layers", "=", "len", "(", "self", ".", "attention", ")", "\n", "for", "proj", ",", "conv", ",", "attention", "in", "zip", "(", "self", ".", "projections", ",", "self", ".", "convolutions", ",", "self", ".", "attention", ")", ":", "\n", "            ", "residual", "=", "x", "if", "proj", "is", "None", "else", "proj", "(", "x", ")", "\n", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.FConvDecoder._split_encoder_out": [[308, 325], ["fairseq.utils.get_incremental_state", "encoder_a.transpose().contiguous.transpose().contiguous.transpose().contiguous", "fairseq.utils.set_incremental_state", "encoder_a.transpose().contiguous.transpose().contiguous.transpose"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.get_incremental_state", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.set_incremental_state"], ["x", "=", "conv", "(", "x", ",", "incremental_state", ")", "\n", "x", "=", "F", ".", "glu", "(", "x", ",", "dim", "=", "2", ")", "\n", "\n", "# attention", "\n", "if", "attention", "is", "not", "None", ":", "\n", "                ", "x", "=", "self", ".", "_transpose_if_training", "(", "x", ",", "incremental_state", ")", "\n", "# print(x.size())", "\n", "\n", "x", ",", "attn_scores", "=", "attention", "(", "x", ",", "target_embedding", ",", "(", "encoder_a", ",", "encoder_b", ")", ")", "\n", "attn_scores", "=", "attn_scores", "/", "num_attn_layers", "\n", "if", "avg_attn_scores", "is", "None", ":", "\n", "                    ", "avg_attn_scores", "=", "attn_scores", "\n", "", "else", ":", "\n", "                    ", "avg_attn_scores", ".", "add_", "(", "attn_scores", ")", "\n", "\n", "", "x", "=", "self", ".", "_transpose_if_training", "(", "x", ",", "incremental_state", ")", "\n", "\n", "# residual", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.FConvDecoder._transpose_if_training": [[326, 330], ["x.transpose.transpose.transpose"], "methods", ["None"], ["", "x", "=", "(", "x", "+", "residual", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "\n", "# T x B x C -> B x T x C", "\n", "", "x", "=", "self", ".", "_transpose_if_training", "(", "x", ",", "incremental_state", ")", "\n", "# print(x.size())", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Embedding": [[332, 336], ["torch.Embedding", "nn.Embedding.weight.data.normal_"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Embedding"], ["# project back to size of vocabulary", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "fc3", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.PositionalEmbedding": [[338, 342], ["fairseq.modules.LearnedPositionalEmbedding", "fairseq.modules.LearnedPositionalEmbedding.weight.data.normal_"], "function", ["None"], ["\n", "", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum output length supported by the decoder.\"\"\"", "\n", "return", "self", ".", "embed_positions", ".", "max_positions", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Linear": [[344, 350], ["torch.Linear", "nn.Linear.weight.data.normal_", "nn.Linear.bias.data.zero_", "torch.utils.weight_norm", "math.sqrt"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.Linear"], ["        ", "if", "state_dict", ".", "get", "(", "'decoder.version'", ",", "torch", ".", "Tensor", "(", "[", "1", "]", ")", ")", "[", "0", "]", "<", "2", ":", "\n", "# old models use incorrect weight norm dimension", "\n", "            ", "for", "i", ",", "conv", "in", "enumerate", "(", "self", ".", "convolutions", ")", ":", "\n", "# reconfigure weight norm", "\n", "                ", "nn", ".", "utils", ".", "remove_weight_norm", "(", "conv", ")", "\n", "self", ".", "convolutions", "[", "i", "]", "=", "nn", ".", "utils", ".", "weight_norm", "(", "conv", ",", "dim", "=", "0", ")", "\n", "", "state_dict", "[", "'decoder.version'", "]", "=", "torch", ".", "Tensor", "(", "[", "1", "]", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.LinearizedConv1d": [[352, 359], ["fairseq.modules.LinearizedConvolution", "math.sqrt", "fairseq.modules.LinearizedConvolution.weight.data.normal_", "fairseq.modules.LinearizedConvolution.bias.data.zero_", "torch.utils.weight_norm"], "function", ["None"], ["\n", "", "def", "_embed_tokens", "(", "self", ",", "tokens", ",", "incremental_state", ")", ":", "\n", "        ", "if", "incremental_state", "is", "not", "None", ":", "\n", "# keep only the last token for incremental forward pass", "\n", "            ", "tokens", "=", "tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "", "return", "self", ".", "embed_tokens", "(", "tokens", ")", "\n", "\n", "", "def", "_split_encoder_out", "(", "self", ",", "encoder_out", ",", "incremental_state", ")", ":", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.ConvTBC": [[361, 369], ["fconv.ConvTBC", "math.sqrt", "ConvTBC.weight.data.normal_", "ConvTBC.bias.data.zero_", "torch.utils.weight_norm"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.ConvTBC"], ["\n", "cached_result", "=", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'encoder_out'", ")", "\n", "if", "cached_result", "is", "not", "None", ":", "\n", "            ", "return", "cached_result", "\n", "\n", "# transpose only once to speed up attention layers", "\n", "", "encoder_a", ",", "encoder_b", "=", "encoder_out", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.base_architecture": [[371, 380], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.__init__.register_model_architecture"], ["result", "=", "(", "encoder_a", ",", "encoder_b", ")", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "utils", ".", "set_incremental_state", "(", "self", ",", "incremental_state", ",", "'encoder_out'", ",", "result", ")", "\n", "", "return", "result", "\n", "\n", "", "def", "_transpose_if_training", "(", "self", ",", "x", ",", "incremental_state", ")", ":", "\n", "        ", "if", "incremental_state", "is", "None", ":", "\n", "            ", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "return", "x", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.fconv_newsroom": [[381, 389], ["register_model_architecture", "fconv.base_architecture"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.base_architecture"], ["\n", "\n", "", "", "def", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "padding_idx", ")", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "0.1", ")", "\n", "return", "m", "\n", "\n", "\n", "", "def", "PositionalEmbedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ",", "left_pad", ")", ":", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.fconv_iwslt_de_en": [[391, 399], ["register_model_architecture", "fconv.base_architecture"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.base_architecture"], ["m", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "0.1", ")", "\n", "return", "m", "\n", "\n", "\n", "", "def", "Linear", "(", "in_features", ",", "out_features", ",", "dropout", "=", "0", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Linear layer (input: N x T x C)\"\"\"", "\n", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ")", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0", ",", "std", "=", "math", ".", "sqrt", "(", "(", "1", "-", "dropout", ")", "/", "in_features", ")", ")", "\n", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.fconv_wmt_en_ro": [[401, 409], ["register_model_architecture", "fconv.base_architecture"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.base_architecture"], ["\n", "\n", "", "def", "LinearizedConv1d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "dropout", "=", "0", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Conv1d layer optimized for decoding\"\"\"", "\n", "m", "=", "LinearizedConvolution", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "std", "=", "math", ".", "sqrt", "(", "(", "4", "*", "(", "1.0", "-", "dropout", ")", ")", "/", "(", "m", ".", "kernel_size", "[", "0", "]", "*", "in_channels", ")", ")", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0", ",", "std", "=", "std", ")", "\n", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "return", "nn", ".", "utils", ".", "weight_norm", "(", "m", ",", "dim", "=", "2", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.fconv_wmt_en_de": [[411, 422], ["register_model_architecture", "fconv.base_architecture"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.base_architecture"], ["\n", "", "def", "ConvTBC", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "dropout", "=", "0", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Conv1d layer\"\"\"", "\n", "from", "fairseq", ".", "modules", "import", "ConvTBC", "\n", "m", "=", "ConvTBC", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "std", "=", "math", ".", "sqrt", "(", "(", "4", "*", "(", "1.0", "-", "dropout", ")", ")", "/", "(", "m", ".", "kernel_size", "[", "0", "]", "*", "in_channels", ")", ")", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0", ",", "std", "=", "std", ")", "\n", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "return", "nn", ".", "utils", ".", "weight_norm", "(", "m", ",", "dim", "=", "2", ")", "\n", "\n", "\n", "", "@", "register_model_architecture", "(", "'fconv'", ",", "'fconv'", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.fconv_wmt_en_fr": [[424, 437], ["register_model_architecture", "fconv.base_architecture"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fconv.base_architecture"], ["    ", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "'[(512, 3)] * 20'", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "'[(512, 3)] * 20'", ")", "\n", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_attention", "=", "getattr", "(", "args", ",", "'decoder_attention'", ",", "'True'", ")", "\n", "args", ".", "share_input_output_embed", "=", "getattr", "(", "args", ",", "'share_input_output_embed'", ",", "False", ")", "\n", "\n", "", "@", "register_model_architecture", "(", "'fconv'", ",", "'fconv_newsroom'", ")", "\n", "def", "fconv_newsroom", "(", "args", ")", ":", "\n", "    ", "base_architecture", "(", "args", ")", "\n", "args", ".", "encoder_embed_dim", "=", "256", "\n", "args", ".", "encoder_layers", "=", "'[(256, 3)] * 20'", "\n", "args", ".", "decoder_embed_dim", "=", "256", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.__init__": [[16, 31], ["torch.Module.__init__", "isinstance", "isinstance", "fairseq_model.FairseqModel.src_dict.pad", "fairseq_model.FairseqModel.dst_dict.pad", "fairseq_model.FairseqModel.src_dict.eos", "fairseq_model.FairseqModel.dst_dict.eos", "fairseq_model.FairseqModel.src_dict.unk", "fairseq_model.FairseqModel.dst_dict.unk"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.unk"], ["    ", "\"\"\"Base class for encoder-decoder models.\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "decoder", "\n", "assert", "isinstance", "(", "self", ".", "encoder", ",", "FairseqEncoder", ")", "\n", "assert", "isinstance", "(", "self", ".", "decoder", ",", "FairseqDecoder", ")", "\n", "\n", "self", ".", "src_dict", "=", "encoder", ".", "dictionary", "\n", "self", ".", "dst_dict", "=", "decoder", ".", "dictionary", "\n", "assert", "self", ".", "src_dict", ".", "pad", "(", ")", "==", "self", ".", "dst_dict", ".", "pad", "(", ")", "\n", "assert", "self", ".", "src_dict", ".", "eos", "(", ")", "==", "self", ".", "dst_dict", ".", "eos", "(", ")", "\n", "assert", "self", ".", "src_dict", ".", "unk", "(", ")", "==", "self", ".", "dst_dict", ".", "unk", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.add_args": [[32, 36], ["None"], "methods", ["None"], ["self", ".", "_is_generation_fast", "=", "False", "\n", "\n", "", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.build_model": [[37, 41], ["None"], "methods", ["None"], ["pass", "\n", "\n", "", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "src_dict", ",", "dst_dict", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.forward": [[42, 46], ["fairseq_model.FairseqModel.encoder", "fairseq_model.FairseqModel.decoder"], "methods", ["None"], ["raise", "NotImplementedError", "\n", "\n", "", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ",", "src_doctopic", ",", "src_wordtopics", ",", "prev_output_tokens", ")", ":", "\n", "        ", "encoder_out", "=", "self", ".", "encoder", "(", "src_tokens", ",", "src_lengths", ",", "src_doctopic", ",", "src_wordtopics", ")", "\n", "decoder_out", ",", "_", "=", "self", ".", "decoder", "(", "prev_output_tokens", ",", "encoder_out", ",", "src_doctopic", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.get_normalized_probs": [[47, 50], ["fairseq_model.FairseqModel.decoder.get_normalized_probs"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_decoder.FairseqDecoder.get_normalized_probs"], ["return", "decoder_out", "\n", "\n", "", "def", "get_normalized_probs", "(", "self", ",", "net_output", ",", "log_probs", ")", ":", "\n", "        ", "\"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.max_encoder_positions": [[51, 54], ["fairseq_model.FairseqModel.encoder.max_positions"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_decoder.FairseqDecoder.max_positions"], ["return", "self", ".", "decoder", ".", "get_normalized_probs", "(", "net_output", ",", "log_probs", ")", "\n", "\n", "", "def", "max_encoder_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the encoder.\"\"\"", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.max_decoder_positions": [[55, 58], ["fairseq_model.FairseqModel.decoder.max_positions"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_decoder.FairseqDecoder.max_positions"], ["return", "self", ".", "encoder", ".", "max_positions", "(", ")", "\n", "\n", "", "def", "max_decoder_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum output length supported by the decoder.\"\"\"", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.load_state_dict": [[59, 68], ["fairseq_model.FairseqModel.upgrade_state_dict", "super().load_state_dict"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_decoder.FairseqDecoder.upgrade_state_dict", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.load_state_dict"], ["return", "self", ".", "decoder", ".", "max_positions", "(", ")", "\n", "\n", "", "def", "load_state_dict", "(", "self", ",", "state_dict", ",", "strict", "=", "True", ")", ":", "\n", "        ", "\"\"\"Copies parameters and buffers from state_dict into this module and\n        its descendants.\n\n        Overrides the method in nn.Module; compared with that method this\n        additionally \"upgrades\" state_dicts from old checkpoints.\n        \"\"\"", "\n", "state_dict", "=", "self", ".", "upgrade_state_dict", "(", "state_dict", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.upgrade_state_dict": [[69, 73], ["fairseq_model.FairseqModel.encoder.upgrade_state_dict", "fairseq_model.FairseqModel.decoder.upgrade_state_dict"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_decoder.FairseqDecoder.upgrade_state_dict", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_decoder.FairseqDecoder.upgrade_state_dict"], ["super", "(", ")", ".", "load_state_dict", "(", "state_dict", ",", "strict", ")", "\n", "\n", "", "def", "upgrade_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "state_dict", "=", "self", ".", "encoder", ".", "upgrade_state_dict", "(", "state_dict", ")", "\n", "state_dict", "=", "self", ".", "decoder", ".", "upgrade_state_dict", "(", "state_dict", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.make_generation_fast_": [[74, 100], ["fairseq_model.FairseqModel.apply", "fairseq_model.FairseqModel.apply", "fairseq_model.FairseqModel.eval", "torch.utils.remove_weight_norm", "hasattr", "module.make_generation_fast_", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.make_generation_fast_"], ["return", "state_dict", "\n", "\n", "", "def", "make_generation_fast_", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Optimize model for faster generation.\"\"\"", "\n", "if", "self", ".", "_is_generation_fast", ":", "\n", "            ", "return", "# only apply once", "\n", "", "self", ".", "_is_generation_fast", "=", "True", "\n", "\n", "# remove weight norm from all modules in the network", "\n", "def", "apply_remove_weight_norm", "(", "module", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "nn", ".", "utils", ".", "remove_weight_norm", "(", "module", ")", "\n", "", "except", "ValueError", ":", "# this module didn't have weight norm", "\n", "                ", "return", "\n", "", "", "self", ".", "apply", "(", "apply_remove_weight_norm", ")", "\n", "\n", "def", "apply_make_generation_fast_", "(", "module", ")", ":", "\n", "            ", "if", "module", "!=", "self", "and", "hasattr", "(", "module", ",", "'make_generation_fast_'", ")", ":", "\n", "                ", "module", ".", "make_generation_fast_", "(", "**", "kwargs", ")", "\n", "", "", "self", ".", "apply", "(", "apply_make_generation_fast_", ")", "\n", "\n", "def", "train", "(", "mode", ")", ":", "\n", "            ", "if", "mode", ":", "\n", "                ", "raise", "RuntimeError", "(", "'cannot train after make_generation_fast'", ")", "\n", "\n", "# this model should no longer be used for training", "\n", "", "", "self", ".", "eval", "(", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.__init__.build_model": [[22, 24], ["ARCH_MODEL_REGISTRY[].build_model"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.__init__.build_model"], []], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.__init__.register_model": [[26, 38], ["ValueError", "issubclass", "ValueError"], "function", ["None"], []], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.__init__.register_model_architecture": [[40, 55], ["ValueError", "ValueError", "callable", "ValueError"], "function", ["None"], []], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_decoder.FairseqDecoder.__init__": [[15, 18], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["def", "__init__", "(", "self", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dictionary", "=", "dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_decoder.FairseqDecoder.forward": [[19, 21], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_decoder.FairseqDecoder.get_normalized_probs": [[22, 30], ["net_output.size", "net_output.view", "torch.log_softmax().view_as", "torch.log_softmax().view_as", "torch.softmax().view_as", "torch.softmax().view_as", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax"], "methods", ["None"], ["", "def", "get_normalized_probs", "(", "self", ",", "net_output", ",", "log_probs", ")", ":", "\n", "        ", "\"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"", "\n", "vocab", "=", "net_output", ".", "size", "(", "-", "1", ")", "\n", "net_output1", "=", "net_output", ".", "view", "(", "-", "1", ",", "vocab", ")", "\n", "if", "log_probs", ":", "\n", "            ", "return", "F", ".", "log_softmax", "(", "net_output1", ",", "dim", "=", "1", ")", ".", "view_as", "(", "net_output", ")", "\n", "", "else", ":", "\n", "            ", "return", "F", ".", "softmax", "(", "net_output1", ",", "dim", "=", "1", ")", ".", "view_as", "(", "net_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_decoder.FairseqDecoder.max_positions": [[31, 34], ["None"], "methods", ["None"], ["", "", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the decoder.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_decoder.FairseqDecoder.upgrade_state_dict": [[35, 37], ["None"], "methods", ["None"], ["", "def", "upgrade_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "return", "state_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.fairseq_criterion.FairseqCriterion.__init__": [[13, 17], ["torch.nn.modules.loss._Loss.__init__", "dst_dict.pad"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.pad"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "src_dict", ",", "dst_dict", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "padding_idx", "=", "dst_dict", ".", "pad", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.fairseq_criterion.FairseqCriterion.add_args": [[18, 22], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add criterion-specific arguments to the parser.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.fairseq_criterion.FairseqCriterion.forward": [[23, 32], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "model", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "        ", "\"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss, as a Variable\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.fairseq_criterion.FairseqCriterion.aggregate_logging_outputs": [[33, 37], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "aggregate_logging_outputs", "(", "logging_outputs", ")", ":", "\n", "        ", "\"\"\"Aggregate logging outputs from data parallel training.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.fairseq_criterion.FairseqCriterion.grad_denom": [[38, 42], ["sum"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "grad_denom", "(", "sample_sizes", ")", ":", "\n", "        ", "\"\"\"Compute the gradient denominator for a set of sample sizes.\"\"\"", "\n", "return", "sum", "(", "sample_sizes", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.cross_entropy.CrossEntropyCriterion.__init__": [[17, 19], ["FairseqCriterion.__init__"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "src_dict", ",", "dst_dict", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "src_dict", ",", "dst_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.cross_entropy.CrossEntropyCriterion.forward": [[20, 41], ["model", "model.get_normalized_probs", "lprobs.view.view.view", "sample[].view", "torch.nll_loss", "lprobs.view.view.size", "sample[].size", "fairseq.utils.item"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_decoder.FairseqDecoder.get_normalized_probs", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.item"], ["", "def", "forward", "(", "self", ",", "model", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "        ", "\"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss, as a Variable\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"", "\n", "net_output", "=", "model", "(", "**", "sample", "[", "'net_input'", "]", ")", "\n", "lprobs", "=", "model", ".", "get_normalized_probs", "(", "net_output", ",", "log_probs", "=", "True", ")", "\n", "lprobs", "=", "lprobs", ".", "view", "(", "-", "1", ",", "lprobs", ".", "size", "(", "-", "1", ")", ")", "\n", "target", "=", "sample", "[", "'target'", "]", ".", "view", "(", "-", "1", ")", "\n", "loss", "=", "F", ".", "nll_loss", "(", "lprobs", ",", "target", ",", "size_average", "=", "False", ",", "ignore_index", "=", "self", ".", "padding_idx", ",", "\n", "reduce", "=", "reduce", ")", "\n", "sample_size", "=", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", "if", "self", ".", "args", ".", "sentence_avg", "else", "sample", "[", "'ntokens'", "]", "\n", "logging_output", "=", "{", "\n", "'loss'", ":", "utils", ".", "item", "(", "loss", ".", "data", ")", "if", "reduce", "else", "loss", ".", "data", ",", "\n", "'ntokens'", ":", "sample", "[", "'ntokens'", "]", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.cross_entropy.CrossEntropyCriterion.aggregate_logging_outputs": [[42, 54], ["sum", "sum", "sum", "log.get", "log.get", "log.get", "math.log", "math.log"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.log", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.log"], ["", "@", "staticmethod", "\n", "def", "aggregate_logging_outputs", "(", "logging_outputs", ")", ":", "\n", "        ", "\"\"\"Aggregate logging outputs from data parallel training.\"\"\"", "\n", "loss_sum", "=", "sum", "(", "log", ".", "get", "(", "'loss'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "ntokens", "=", "sum", "(", "log", ".", "get", "(", "'ntokens'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "sample_size", "=", "sum", "(", "log", ".", "get", "(", "'sample_size'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "agg_output", "=", "{", "\n", "'loss'", ":", "loss_sum", "/", "sample_size", "/", "math", ".", "log", "(", "2", ")", ",", "\n", "}", "\n", "if", "sample_size", "!=", "ntokens", ":", "\n", "            ", "agg_output", "[", "'nll_loss'", "]", "=", "loss_sum", "/", "ntokens", "/", "math", ".", "log", "(", "2", ")", "\n", "", "return", "agg_output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.__init__": [[20, 23], ["FairseqCriterion.__init__"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__"], ["class", "LabelSmoothedCrossEntropyCriterion", "(", "FairseqCriterion", ")", ":", "\n", "\n", "    ", "def", "__init__", "(", "self", ",", "args", ",", "src_dict", ",", "dst_dict", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "src_dict", ",", "dst_dict", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.add_args": [[24, 29], ["parser.add_argument"], "methods", ["None"], ["self", ".", "eps", "=", "args", ".", "label_smoothing", "\n", "\n", "", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add criterion-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--label-smoothing'", ",", "default", "=", "0.", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.forward": [[30, 58], ["model", "model.get_normalized_probs", "sample[].unsqueeze", "sample[].unsqueeze.ne", "nll_loss.sum.sum.sum", "smooth_loss.sum.sum.sum", "model.get_normalized_probs.size", "sample[].size", "model.get_normalized_probs.gather", "model.get_normalized_probs.sum", "fairseq.utils.item", "fairseq.utils.item"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_decoder.FairseqDecoder.get_normalized_probs", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.item", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.item"], ["help", "=", "'epsilon for label smoothing, 0 means no label smoothing'", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "model", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "        ", "\"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss, as a Variable\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"", "\n", "net_output", "=", "model", "(", "**", "sample", "[", "'net_input'", "]", ")", "\n", "lprobs", "=", "model", ".", "get_normalized_probs", "(", "net_output", ",", "log_probs", "=", "True", ")", "\n", "target", "=", "sample", "[", "'target'", "]", ".", "unsqueeze", "(", "-", "1", ")", "\n", "non_pad_mask", "=", "target", ".", "ne", "(", "self", ".", "padding_idx", ")", "\n", "nll_loss", "=", "-", "lprobs", ".", "gather", "(", "dim", "=", "-", "1", ",", "index", "=", "target", ")", "[", "non_pad_mask", "]", "\n", "smooth_loss", "=", "-", "lprobs", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "[", "non_pad_mask", "]", "\n", "if", "reduce", ":", "\n", "            ", "nll_loss", "=", "nll_loss", ".", "sum", "(", ")", "\n", "smooth_loss", "=", "smooth_loss", ".", "sum", "(", ")", "\n", "", "eps_i", "=", "self", ".", "eps", "/", "lprobs", ".", "size", "(", "-", "1", ")", "\n", "loss", "=", "(", "1.", "-", "self", ".", "eps", ")", "*", "nll_loss", "+", "eps_i", "*", "smooth_loss", "\n", "\n", "sample_size", "=", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", "if", "self", ".", "args", ".", "sentence_avg", "else", "sample", "[", "'ntokens'", "]", "\n", "logging_output", "=", "{", "\n", "'loss'", ":", "utils", ".", "item", "(", "loss", ".", "data", ")", "if", "reduce", "else", "loss", ".", "data", ",", "\n", "'nll_loss'", ":", "utils", ".", "item", "(", "nll_loss", ".", "data", ")", "if", "reduce", "else", "loss", ".", "data", ",", "\n", "'ntokens'", ":", "sample", "[", "'ntokens'", "]", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.aggregate_logging_outputs": [[59, 67], ["sum", "sum", "log.get", "log.get", "math.log", "math.log", "sum", "sum", "log.get", "log.get"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.log", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.log"], ["return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n", "", "@", "staticmethod", "\n", "def", "aggregate_logging_outputs", "(", "logging_outputs", ")", ":", "\n", "        ", "\"\"\"Aggregate logging outputs from data parallel training.\"\"\"", "\n", "ntokens", "=", "sum", "(", "log", ".", "get", "(", "'ntokens'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "sample_size", "=", "sum", "(", "log", ".", "get", "(", "'sample_size'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "return", "{", "\n", "'loss'", ":", "sum", "(", "log", ".", "get", "(", "'loss'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "/", "sample_size", "/", "math", ".", "log", "(", "2", ")", ",", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.__init__.build_criterion": [[18, 20], ["None"], "function", ["None"], []], "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.__init__.register_criterion": [[22, 39], ["CRITERION_CLASS_NAMES.add", "ValueError", "issubclass", "ValueError", "ValueError"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.add"], []], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.generate.main": [[17, 156], ["print", "print", "fairseq.utils.load_ensemble_for_inference", "print", "print", "print", "fairseq.utils.load_align_dict", "min", "data.load_raw_text_dataset.eval_dataloader", "fairseq.meters.StopwatchMeter", "fairseq.bleu.Scorer", "print", "torch.cuda.is_available", "fairseq.data.load_dataset", "fairseq.data.load_raw_text_dataset", "model.make_generation_fast_", "fairseq.data.sharded_iterator", "fairseq.sequence_scorer.SequenceScorer", "fairseq.sequence_generator.SequenceGenerator", "fairseq.sequence_generator.SequenceGenerator.cuda", "data.load_raw_text_dataset.dst_dict.pad", "data.load_raw_text_dataset.dst_dict.eos", "data.load_raw_text_dataset.dst_dict.unk", "fairseq.progress_bar.build_progress_bar", "fairseq.meters.TimeMeter", "print", "len", "len", "len", "model.max_encoder_positions", "ValueError", "fairseq.sequence_generator.SequenceGenerator.score_batched_itr", "fairseq.sequence_generator.SequenceGenerator.generate_batched_itr", "enumerate", "fairseq.meters.TimeMeter.update", "t.log", "tokenizer.Tokenizer.tokenize.int().cpu", "data.load_raw_text_dataset.splits[].src.get_original_text", "data.load_raw_text_dataset.splits[].dst.get_original_text", "data.load_raw_text_dataset.src_dict.string", "print", "fairseq.utils.post_process_prediction", "src_tokens.size", "bleu.Scorer.result_string", "data.load_raw_text_dataset.dst_dict.string", "print", "print", "print", "print", "bleu.Scorer.add", "round", "tokenizer.Tokenizer.tokenize.int", "min", "hypo[].int().cpu", "hypo[].int().cpu", "fairseq.tokenizer.Tokenizer.tokenize", "len", "hypo[].int", "hypo[].int", "map", "map", "hypo[].tolist", "str", "fairseq.utils.item"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.load_ensemble_for_inference", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.load_align_dict", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.LanguageDatasets.eval_dataloader", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.load_dataset", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.load_raw_text_dataset", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.make_generation_fast_", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator.cuda", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.max_encoder_positions", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_scorer.SequenceScorer.score_batched_itr", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator.generate_batched_itr", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.log", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDatasetDOCTOPICS.get_original_text", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedRawTextDatasetDOCTOPICS.get_original_text", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.string", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.post_process_prediction", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.result_string", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.string", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.add", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.tokenizer.Tokenizer.tokenize", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.item"], ["def", "main", "(", "args", ")", ":", "\n", "    ", "print", "(", "args", ")", "\n", "\n", "use_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "cpu", "\n", "\n", "# Load dataset", "\n", "if", "args", ".", "replace_unk", "is", "None", ":", "\n", "        ", "dataset", "=", "data", ".", "load_dataset", "(", "\n", "args", ".", "data", ",", "\n", "[", "args", ".", "gen_subset", "]", ",", "\n", "args", ".", "source_lang", ",", "\n", "args", ".", "target_lang", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "dataset", "=", "data", ".", "load_raw_text_dataset", "(", "\n", "args", ".", "data", ",", "\n", "[", "args", ".", "gen_subset", "]", ",", "\n", "args", ".", "source_lang", ",", "\n", "args", ".", "target_lang", ",", "\n", "args", ".", "doctopics", ",", "args", ".", "encoder_embed_dim", ",", "\n", ")", "\n", "", "if", "args", ".", "source_lang", "is", "None", "or", "args", ".", "target_lang", "is", "None", ":", "\n", "# record inferred languages in args", "\n", "        ", "args", ".", "source_lang", ",", "args", ".", "target_lang", "=", "dataset", ".", "src", ",", "dataset", ".", "dst", "\n", "\n", "# Load ensemble", "\n", "", "print", "(", "'| loading model(s) from {}'", ".", "format", "(", "', '", ".", "join", "(", "args", ".", "path", ")", ")", ")", "\n", "models", ",", "_", "=", "utils", ".", "load_ensemble_for_inference", "(", "args", ".", "path", ",", "dataset", ".", "src_dict", ",", "dataset", ".", "dst_dict", ")", "\n", "\n", "print", "(", "'| [{}] dictionary: {} types'", ".", "format", "(", "dataset", ".", "src", ",", "len", "(", "dataset", ".", "src_dict", ")", ")", ")", "\n", "print", "(", "'| [{}] dictionary: {} types'", ".", "format", "(", "dataset", ".", "dst", ",", "len", "(", "dataset", ".", "dst_dict", ")", ")", ")", "\n", "print", "(", "'| {} {} {} examples'", ".", "format", "(", "args", ".", "data", ",", "args", ".", "gen_subset", ",", "len", "(", "dataset", ".", "splits", "[", "args", ".", "gen_subset", "]", ")", ")", ")", "\n", "\n", "# Optimize ensemble for generation", "\n", "for", "model", "in", "models", ":", "\n", "        ", "model", ".", "make_generation_fast_", "(", "\n", "beamable_mm_beam_size", "=", "None", "if", "args", ".", "no_beamable_mm", "else", "args", ".", "beam", ",", "\n", ")", "\n", "\n", "# Load alignment dictionary for unknown word replacement", "\n", "# (None if no unknown word replacement, empty if no path to align dictionary)", "\n", "", "align_dict", "=", "utils", ".", "load_align_dict", "(", "args", ".", "replace_unk", ")", "\n", "\n", "# Load dataset (possibly sharded)", "\n", "max_positions", "=", "min", "(", "model", ".", "max_encoder_positions", "(", ")", "for", "model", "in", "models", ")", "\n", "itr", "=", "dataset", ".", "eval_dataloader", "(", "\n", "args", ".", "gen_subset", ",", "\n", "max_sentences", "=", "args", ".", "max_sentences", ",", "\n", "max_positions", "=", "max_positions", ",", "\n", "skip_invalid_size_inputs_valid_test", "=", "args", ".", "skip_invalid_size_inputs_valid_test", ",", "\n", ")", "\n", "if", "args", ".", "num_shards", ">", "1", ":", "\n", "        ", "if", "args", ".", "shard_id", "<", "0", "or", "args", ".", "shard_id", ">=", "args", ".", "num_shards", ":", "\n", "            ", "raise", "ValueError", "(", "'--shard-id must be between 0 and num_shards'", ")", "\n", "", "itr", "=", "data", ".", "sharded_iterator", "(", "itr", ",", "args", ".", "num_shards", ",", "args", ".", "shard_id", ")", "\n", "\n", "# print(\"SHASHI: I AM HERE\")", "\n", "\n", "# Initialize generator", "\n", "", "gen_timer", "=", "StopwatchMeter", "(", ")", "\n", "if", "args", ".", "score_reference", ":", "\n", "        ", "translator", "=", "SequenceScorer", "(", "models", ")", "\n", "", "else", ":", "\n", "        ", "translator", "=", "SequenceGenerator", "(", "\n", "models", ",", "beam_size", "=", "args", ".", "beam", ",", "stop_early", "=", "(", "not", "args", ".", "no_early_stop", ")", ",", "\n", "normalize_scores", "=", "(", "not", "args", ".", "unnormalized", ")", ",", "len_penalty", "=", "args", ".", "lenpen", ",", "\n", "unk_penalty", "=", "args", ".", "unkpen", ")", "\n", "", "if", "use_cuda", ":", "\n", "        ", "translator", ".", "cuda", "(", ")", "\n", "\n", "# Generate and compute BLEU score", "\n", "", "scorer", "=", "bleu", ".", "Scorer", "(", "dataset", ".", "dst_dict", ".", "pad", "(", ")", ",", "dataset", ".", "dst_dict", ".", "eos", "(", ")", ",", "dataset", ".", "dst_dict", ".", "unk", "(", ")", ")", "\n", "num_sentences", "=", "0", "\n", "has_target", "=", "True", "\n", "with", "progress_bar", ".", "build_progress_bar", "(", "args", ",", "itr", ")", "as", "t", ":", "\n", "        ", "if", "args", ".", "score_reference", ":", "\n", "            ", "translations", "=", "translator", ".", "score_batched_itr", "(", "t", ",", "cuda", "=", "use_cuda", ",", "timer", "=", "gen_timer", ")", "\n", "", "else", ":", "\n", "            ", "translations", "=", "translator", ".", "generate_batched_itr", "(", "\n", "t", ",", "maxlen_a", "=", "args", ".", "max_len_a", ",", "maxlen_b", "=", "args", ".", "max_len_b", ",", "\n", "cuda", "=", "use_cuda", ",", "timer", "=", "gen_timer", ",", "prefix_size", "=", "args", ".", "prefix_size", ")", "\n", "", "wps_meter", "=", "TimeMeter", "(", ")", "\n", "for", "sample_id", ",", "src_tokens", ",", "target_tokens", ",", "hypos", "in", "translations", ":", "\n", "# Process input and ground truth", "\n", "            ", "has_target", "=", "target_tokens", "is", "not", "None", "\n", "target_tokens", "=", "target_tokens", ".", "int", "(", ")", ".", "cpu", "(", ")", "if", "has_target", "else", "None", "\n", "# Either retrieve the original sentences or regenerate them from tokens.", "\n", "if", "align_dict", "is", "not", "None", ":", "\n", "                ", "src_str", "=", "dataset", ".", "splits", "[", "args", ".", "gen_subset", "]", ".", "src", ".", "get_original_text", "(", "sample_id", ")", "\n", "target_str", "=", "dataset", ".", "splits", "[", "args", ".", "gen_subset", "]", ".", "dst", ".", "get_original_text", "(", "sample_id", ")", "\n", "", "else", ":", "\n", "                ", "src_str", "=", "dataset", ".", "src_dict", ".", "string", "(", "src_tokens", ",", "args", ".", "remove_bpe", ")", "\n", "target_str", "=", "dataset", ".", "dst_dict", ".", "string", "(", "target_tokens", ",", "\n", "args", ".", "remove_bpe", ",", "\n", "escape_unk", "=", "True", ")", "if", "has_target", "else", "''", "\n", "\n", "", "if", "not", "args", ".", "quiet", ":", "\n", "                ", "print", "(", "'S-{}\\t{}'", ".", "format", "(", "sample_id", ",", "src_str", ")", ")", "\n", "if", "has_target", ":", "\n", "                    ", "print", "(", "'T-{}\\t{}'", ".", "format", "(", "sample_id", ",", "target_str", ")", ")", "\n", "\n", "# Process top predictions", "\n", "", "", "for", "i", ",", "hypo", "in", "enumerate", "(", "hypos", "[", ":", "min", "(", "len", "(", "hypos", ")", ",", "args", ".", "nbest", ")", "]", ")", ":", "\n", "                ", "hypo_tokens", ",", "hypo_str", ",", "alignment", "=", "utils", ".", "post_process_prediction", "(", "\n", "hypo_tokens", "=", "hypo", "[", "'tokens'", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", ",", "\n", "src_str", "=", "src_str", ",", "\n", "alignment", "=", "hypo", "[", "'alignment'", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", ",", "\n", "align_dict", "=", "align_dict", ",", "\n", "dst_dict", "=", "dataset", ".", "dst_dict", ",", "\n", "remove_bpe", "=", "args", ".", "remove_bpe", ",", "\n", ")", "\n", "\n", "if", "not", "args", ".", "quiet", ":", "\n", "                    ", "print", "(", "'H-{}\\t{}\\t{}'", ".", "format", "(", "sample_id", ",", "hypo", "[", "'score'", "]", ",", "hypo_str", ")", ")", "\n", "print", "(", "'P-{}\\t{}'", ".", "format", "(", "\n", "sample_id", ",", "\n", "' '", ".", "join", "(", "map", "(", "\n", "lambda", "x", ":", "'{:.4f}'", ".", "format", "(", "x", ")", ",", "\n", "hypo", "[", "'positional_scores'", "]", ".", "tolist", "(", ")", ",", "\n", ")", ")", "\n", ")", ")", "\n", "print", "(", "'A-{}\\t{}'", ".", "format", "(", "\n", "sample_id", ",", "\n", "' '", ".", "join", "(", "map", "(", "lambda", "x", ":", "str", "(", "utils", ".", "item", "(", "x", ")", ")", ",", "alignment", ")", ")", "\n", ")", ")", "\n", "\n", "# Score only the top hypothesis", "\n", "", "if", "has_target", "and", "i", "==", "0", ":", "\n", "                    ", "if", "align_dict", "is", "not", "None", "or", "args", ".", "remove_bpe", "is", "not", "None", ":", "\n", "# Convert back to tokens for evaluation with unk replacement and/or without BPE", "\n", "                        ", "target_tokens", "=", "tokenizer", ".", "Tokenizer", ".", "tokenize", "(", "\n", "target_str", ",", "dataset", ".", "dst_dict", ",", "add_if_not_exist", "=", "True", ")", "\n", "", "scorer", ".", "add", "(", "target_tokens", ",", "hypo_tokens", ")", "\n", "\n", "", "", "wps_meter", ".", "update", "(", "src_tokens", ".", "size", "(", "0", ")", ")", "\n", "t", ".", "log", "(", "{", "'wps'", ":", "round", "(", "wps_meter", ".", "avg", ")", "}", ")", "\n", "num_sentences", "+=", "1", "\n", "\n", "", "", "print", "(", "'| Translated {} sentences ({} tokens) in {:.1f}s ({:.2f} tokens/s)'", ".", "format", "(", "\n", "num_sentences", ",", "gen_timer", ".", "n", ",", "gen_timer", ".", "sum", ",", "1.", "/", "gen_timer", ".", "avg", ")", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.main": [[20, 97], ["print", "torch.cuda.set_device", "torch.manual_seed", "fairseq.data.has_binary_files", "print", "print", "fairseq.models.build_model", "fairseq.criterions.build_criterion", "print", "print", "fairseq.trainer.Trainer", "print", "print", "os.makedirs", "os.path.join", "fairseq.trainer.Trainer.load_checkpoint", "fairseq.trainer.Trainer.get_lr", "fairseq.meters.StopwatchMeter", "fairseq.meters.StopwatchMeter.start", "fairseq.meters.StopwatchMeter.stop", "print", "torch.cuda.is_available", "NotImplementedError", "fairseq.data.load_dataset", "fairseq.data.load_raw_text_dataset", "print", "print", "singleprocess_train.train", "enumerate", "len", "len", "sum", "fairseq.trainer.Trainer.lr_step", "args.valid_subset.split", "singleprocess_train.validate", "len", "fairseq.trainer.Trainer.lr_step", "p.data.numel", "singleprocess_train.save_checkpoint", "models.build_model.parameters"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.has_binary_files", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.__init__.build_model", "home.repos.pwc.inspect_result.shashiongithub_XSum.criterions.__init__.build_criterion", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.load_checkpoint", "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.get_lr", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.stop", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.load_dataset", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.load_raw_text_dataset", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.train", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.lr_step", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.validate", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.lr_step", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.save_checkpoint"], ["\n", "\n", "def", "main", "(", "args", ")", ":", "\n", "    ", "print", "(", "args", ")", "\n", "\n", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "'Training on CPU is not supported'", ")", "\n", "", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "device_id", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "\n", "# Load dataset", "\n", "splits", "=", "[", "'train'", ",", "'valid'", "]", "\n", "if", "data", ".", "has_binary_files", "(", "args", ".", "data", ",", "splits", ")", ":", "\n", "        ", "dataset", "=", "data", ".", "load_dataset", "(", "\n", "args", ".", "data", ",", "splits", ",", "args", ".", "source_lang", ",", "args", ".", "target_lang", ")", "\n", "", "else", ":", "\n", "        ", "dataset", "=", "data", ".", "load_raw_text_dataset", "(", "\n", "args", ".", "data", ",", "splits", ",", "args", ".", "source_lang", ",", "args", ".", "target_lang", ",", "args", ".", "doctopics", ",", "args", ".", "encoder_embed_dim", ")", "\n", "", "if", "args", ".", "source_lang", "is", "None", "or", "args", ".", "target_lang", "is", "None", ":", "\n", "# record inferred languages in args, so that it's saved in checkpoints", "\n", "        ", "args", ".", "source_lang", ",", "args", ".", "target_lang", "=", "dataset", ".", "src", ",", "dataset", ".", "dst", "\n", "", "print", "(", "'| [{}] dictionary: {} types'", ".", "format", "(", "dataset", ".", "src", ",", "len", "(", "dataset", ".", "src_dict", ")", ")", ")", "\n", "print", "(", "'| [{}] dictionary: {} types'", ".", "format", "(", "dataset", ".", "dst", ",", "len", "(", "dataset", ".", "dst_dict", ")", ")", ")", "\n", "print", "(", "'| [{}-lemma-topic] dictionary: {} types'", ".", "format", "(", "dataset", ".", "src", ",", "len", "(", "dataset", ".", "src_lemma_topic_dict", ")", ")", ")", "\n", "for", "split", "in", "splits", ":", "\n", "        ", "print", "(", "'| {} {} {} examples'", ".", "format", "(", "args", ".", "data", ",", "split", ",", "len", "(", "dataset", ".", "splits", "[", "split", "]", ")", ")", ")", "\n", "\n", "# exit(0)", "\n", "\n", "# Build model and criterion", "\n", "", "model", "=", "models", ".", "build_model", "(", "args", ",", "dataset", ".", "src_dict", ",", "dataset", ".", "dst_dict", ")", "\n", "criterion", "=", "criterions", ".", "build_criterion", "(", "args", ",", "dataset", ".", "src_dict", ",", "dataset", ".", "dst_dict", ")", "\n", "print", "(", "'| model {}, criterion {}'", ".", "format", "(", "args", ".", "arch", ",", "criterion", ".", "__class__", ".", "__name__", ")", ")", "\n", "print", "(", "'| num. model params: {}'", ".", "format", "(", "sum", "(", "p", ".", "data", ".", "numel", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", ")", ")", ")", "\n", "\n", "# Build trainer", "\n", "trainer", "=", "Trainer", "(", "args", ",", "model", ",", "criterion", ")", "\n", "print", "(", "'| training on {} GPUs'", ".", "format", "(", "args", ".", "distributed_world_size", ")", ")", "\n", "print", "(", "'| max tokens per GPU = {} and max sentences per GPU = {}'", ".", "format", "(", "\n", "args", ".", "max_tokens", ",", "\n", "args", ".", "max_sentences", ",", "\n", ")", ")", "\n", "\n", "# exit(0)", "\n", "\n", "# Load the latest checkpoint if one is available", "\n", "os", ".", "makedirs", "(", "args", ".", "save_dir", ",", "exist_ok", "=", "True", ")", "\n", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save_dir", ",", "args", ".", "restore_file", ")", "\n", "extra_state", "=", "trainer", ".", "load_checkpoint", "(", "checkpoint_path", ")", "\n", "if", "extra_state", "is", "not", "None", ":", "\n", "        ", "epoch", "=", "extra_state", "[", "'epoch'", "]", "\n", "batch_offset", "=", "extra_state", "[", "'batch_offset'", "]", "\n", "print", "(", "'| loaded checkpoint {} (epoch {})'", ".", "format", "(", "checkpoint_path", ",", "epoch", ")", ")", "\n", "if", "batch_offset", "==", "0", ":", "\n", "            ", "trainer", ".", "lr_step", "(", "epoch", ")", "\n", "epoch", "+=", "1", "\n", "", "", "else", ":", "\n", "        ", "epoch", ",", "batch_offset", "=", "1", ",", "0", "\n", "\n", "# Train until the learning rate gets too small", "\n", "", "max_epoch", "=", "args", ".", "max_epoch", "or", "math", ".", "inf", "\n", "lr", "=", "trainer", ".", "get_lr", "(", ")", "\n", "train_meter", "=", "StopwatchMeter", "(", ")", "\n", "train_meter", ".", "start", "(", ")", "\n", "while", "lr", ">", "args", ".", "min_lr", "and", "epoch", "<=", "max_epoch", ":", "\n", "# train for one epoch", "\n", "        ", "train", "(", "args", ",", "trainer", ",", "dataset", ",", "epoch", ",", "batch_offset", ")", "\n", "\n", "# evaluate on validate set", "\n", "for", "k", ",", "subset", "in", "enumerate", "(", "args", ".", "valid_subset", ".", "split", "(", "','", ")", ")", ":", "\n", "            ", "val_loss", "=", "validate", "(", "args", ",", "trainer", ",", "dataset", ",", "subset", ",", "epoch", ")", "\n", "if", "k", "==", "0", ":", "\n", "# only use first validation loss to update the learning schedule", "\n", "                ", "lr", "=", "trainer", ".", "lr_step", "(", "epoch", ",", "val_loss", ")", "\n", "\n", "# save checkpoint", "\n", "if", "not", "args", ".", "no_save", ":", "\n", "                    ", "save_checkpoint", "(", "trainer", ",", "args", ",", "epoch", ",", "0", ",", "val_loss", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.train": [[99, 161], ["torch.manual_seed", "dataset.train_dataloader", "fairseq.progress_bar.build_progress_bar", "itertools.islice", "collections.defaultdict", "enumerate", "singleprocess_train.get_training_stats", "collections.defaultdict.items", "progress_bar.build_progress_bar.print", "min", "min", "trainer.get_meter", "trainer.train_step", "singleprocess_train.get_training_stats", "trainer.train_step.items", "progress_bar.build_progress_bar.log", "trainer.get_model().max_encoder_positions", "trainer.get_model().max_decoder_positions", "trainer.get_meter.reset", "fairseq.meters.AverageMeter", "extra_meters[].update", "trainer.get_meter().reset", "singleprocess_train.save_checkpoint", "trainer.get_model", "trainer.get_model", "trainer.get_meter", "trainer.get_num_updates"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.LanguageDatasets.train_dataloader", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.get_training_stats", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.train_step", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.get_training_stats", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.log", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.max_encoder_positions", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.max_decoder_positions", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.reset", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.reset", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.save_checkpoint", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_model", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_model", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_num_updates"], ["", "", "", "epoch", "+=", "1", "\n", "batch_offset", "=", "0", "\n", "", "train_meter", ".", "stop", "(", ")", "\n", "\n", "print", "(", "'| done training in {:.1f} seconds'", ".", "format", "(", "train_meter", ".", "sum", ")", ")", "\n", "\n", "\n", "", "def", "train", "(", "args", ",", "trainer", ",", "dataset", ",", "epoch", ",", "batch_offset", ")", ":", "\n", "    ", "\"\"\"Train the model for one epoch.\"\"\"", "\n", "\n", "# Set seed based on args.seed and the epoch number so that we get", "\n", "# reproducible results when resuming from checkpoints", "\n", "seed", "=", "args", ".", "seed", "+", "epoch", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "\n", "# The max number of positions can be different for train and valid", "\n", "# e.g., RNNs may support more positions at test time than seen in training", "\n", "max_positions_train", "=", "(", "\n", "min", "(", "args", ".", "max_source_positions", ",", "trainer", ".", "get_model", "(", ")", ".", "max_encoder_positions", "(", ")", ")", ",", "\n", "min", "(", "args", ".", "max_target_positions", ",", "trainer", ".", "get_model", "(", ")", ".", "max_decoder_positions", "(", ")", ")", "\n", ")", "\n", "\n", "# Initialize dataloader, starting at batch_offset", "\n", "itr", "=", "dataset", ".", "train_dataloader", "(", "\n", "args", ".", "train_subset", ",", "\n", "max_tokens", "=", "args", ".", "max_tokens", ",", "\n", "max_sentences", "=", "args", ".", "max_sentences", ",", "\n", "max_positions", "=", "max_positions_train", ",", "\n", "seed", "=", "seed", ",", "\n", "epoch", "=", "epoch", ",", "\n", "sample_without_replacement", "=", "args", ".", "sample_without_replacement", ",", "\n", "sort_by_source_size", "=", "(", "epoch", "<=", "args", ".", "curriculum", ")", ",", "\n", "shard_id", "=", "args", ".", "distributed_rank", ",", "\n", "num_shards", "=", "args", ".", "distributed_world_size", ",", "\n", ")", "\n", "progress", "=", "progress_bar", ".", "build_progress_bar", "(", "args", ",", "itr", ",", "epoch", ",", "no_progress_bar", "=", "'simple'", ")", "\n", "itr", "=", "itertools", ".", "islice", "(", "progress", ",", "batch_offset", ",", "None", ")", "\n", "\n", "# reset training meters", "\n", "for", "k", "in", "[", "'train_loss'", ",", "'train_nll_loss'", ",", "'wps'", ",", "'ups'", ",", "'wpb'", ",", "'bsz'", ",", "'clip'", "]", ":", "\n", "        ", "meter", "=", "trainer", ".", "get_meter", "(", "k", ")", "\n", "if", "meter", "is", "not", "None", ":", "\n", "            ", "meter", ".", "reset", "(", ")", "\n", "\n", "", "", "extra_meters", "=", "collections", ".", "defaultdict", "(", "lambda", ":", "AverageMeter", "(", ")", ")", "\n", "for", "i", ",", "sample", "in", "enumerate", "(", "itr", ",", "start", "=", "batch_offset", ")", ":", "\n", "# print(sample)", "\n", "# print(sample['net_input']['src_doctopic'][1])", "\n", "# print(sample['net_input']['src_wordtopics'][1][0], sample['net_input']['src_wordtopics'][1][1], sample['net_input']['src_wordtopics'][1][2],", "\n", "#       sample['net_input']['src_wordtopics'][1][3], sample['net_input']['src_wordtopics'][1][4])", "\n", "\n", "        ", "log_output", "=", "trainer", ".", "train_step", "(", "sample", ")", "\n", "\n", "# print(log_output)", "\n", "# exit(0)", "\n", "\n", "# log mid-epoch stats", "\n", "stats", "=", "get_training_stats", "(", "trainer", ")", "\n", "for", "k", ",", "v", "in", "log_output", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", "in", "[", "'loss'", ",", "'nll_loss'", "]", ":", "\n", "                ", "continue", "# these are already logged above", "\n", "", "extra_meters", "[", "k", "]", ".", "update", "(", "v", ")", "\n", "stats", "[", "k", "]", "=", "extra_meters", "[", "k", "]", ".", "avg", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.get_training_stats": [[163, 182], ["collections.OrderedDict", "singleprocess_train.get_perplexity", "round", "round", "round", "trainer.get_num_updates", "trainer.get_lr", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.get_perplexity", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_num_updates", "home.repos.pwc.inspect_result.shashiongithub_XSum.optim.fairseq_optimizer.FairseqOptimizer.get_lr", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter"], ["\n", "# save mid-epoch checkpoints", "\n", "if", "i", "==", "batch_offset", ":", "\n", "# ignore the first mini-batch in words-per-second calculation", "\n", "            ", "trainer", ".", "get_meter", "(", "'wps'", ")", ".", "reset", "(", ")", "\n", "", "if", "args", ".", "save_interval", ">", "0", "and", "trainer", ".", "get_num_updates", "(", ")", "%", "args", ".", "save_interval", "==", "0", ":", "\n", "            ", "save_checkpoint", "(", "trainer", ",", "args", ",", "epoch", ",", "i", "+", "1", ")", "\n", "\n", "# log end-of-epoch stats", "\n", "", "", "stats", "=", "get_training_stats", "(", "trainer", ")", "\n", "for", "k", ",", "meter", "in", "extra_meters", ".", "items", "(", ")", ":", "\n", "        ", "stats", "[", "k", "]", "=", "meter", ".", "avg", "\n", "", "progress", ".", "print", "(", "stats", ")", "\n", "\n", "\n", "", "def", "get_training_stats", "(", "trainer", ")", ":", "\n", "    ", "stats", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "stats", "[", "'loss'", "]", "=", "'{:.3f}'", ".", "format", "(", "trainer", ".", "get_meter", "(", "'train_loss'", ")", ".", "avg", ")", "\n", "if", "trainer", ".", "get_meter", "(", "'train_nll_loss'", ")", ".", "count", ">", "0", ":", "\n", "        ", "nll_loss", "=", "trainer", ".", "get_meter", "(", "'train_nll_loss'", ")", ".", "avg", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.validate": [[184, 234], ["dataset.eval_dataloader", "fairseq.progress_bar.build_progress_bar", "collections.defaultdict", "singleprocess_train.get_valid_stats", "collections.defaultdict.items", "progress_bar.build_progress_bar.print", "trainer.get_model().max_encoder_positions", "trainer.get_model().max_decoder_positions", "trainer.get_meter", "trainer.valid_step", "singleprocess_train.get_valid_stats", "trainer.valid_step.items", "progress_bar.build_progress_bar.log", "trainer.get_meter.reset", "fairseq.meters.AverageMeter", "extra_meters[].update", "trainer.get_model", "trainer.get_model"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.data.LanguageDatasets.eval_dataloader", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.get_valid_stats", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.max_encoder_positions", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.max_decoder_positions", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.valid_step", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.get_valid_stats", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.log", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.reset", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.TimeMeter.update", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_model", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_model"], ["", "else", ":", "\n", "        ", "nll_loss", "=", "trainer", ".", "get_meter", "(", "'train_loss'", ")", ".", "avg", "\n", "", "stats", "[", "'ppl'", "]", "=", "get_perplexity", "(", "nll_loss", ")", "\n", "stats", "[", "'wps'", "]", "=", "round", "(", "trainer", ".", "get_meter", "(", "'wps'", ")", ".", "avg", ")", "\n", "stats", "[", "'ups'", "]", "=", "'{:.1f}'", ".", "format", "(", "trainer", ".", "get_meter", "(", "'ups'", ")", ".", "avg", ")", "\n", "stats", "[", "'wpb'", "]", "=", "round", "(", "trainer", ".", "get_meter", "(", "'wpb'", ")", ".", "avg", ")", "\n", "stats", "[", "'bsz'", "]", "=", "round", "(", "trainer", ".", "get_meter", "(", "'bsz'", ")", ".", "avg", ")", "\n", "stats", "[", "'num_updates'", "]", "=", "trainer", ".", "get_num_updates", "(", ")", "\n", "stats", "[", "'lr'", "]", "=", "trainer", ".", "get_lr", "(", ")", "\n", "stats", "[", "'gnorm'", "]", "=", "'{:.3f}'", ".", "format", "(", "trainer", ".", "get_meter", "(", "'gnorm'", ")", ".", "avg", ")", "\n", "stats", "[", "'clip'", "]", "=", "'{:.0%}'", ".", "format", "(", "trainer", ".", "get_meter", "(", "'clip'", ")", ".", "avg", ")", "\n", "stats", "[", "'oom'", "]", "=", "trainer", ".", "get_meter", "(", "'oom'", ")", ".", "avg", "\n", "return", "stats", "\n", "\n", "\n", "", "def", "validate", "(", "args", ",", "trainer", ",", "dataset", ",", "subset", ",", "epoch", ")", ":", "\n", "    ", "\"\"\"Evaluate the model on the validation set and return the average loss.\"\"\"", "\n", "\n", "# Initialize dataloader", "\n", "max_positions_valid", "=", "(", "\n", "trainer", ".", "get_model", "(", ")", ".", "max_encoder_positions", "(", ")", ",", "\n", "trainer", ".", "get_model", "(", ")", ".", "max_decoder_positions", "(", ")", ",", "\n", ")", "\n", "itr", "=", "dataset", ".", "eval_dataloader", "(", "\n", "subset", ",", "\n", "max_tokens", "=", "args", ".", "max_tokens", ",", "\n", "max_sentences", "=", "args", ".", "max_sentences_valid", ",", "\n", "max_positions", "=", "max_positions_valid", ",", "\n", "skip_invalid_size_inputs_valid_test", "=", "args", ".", "skip_invalid_size_inputs_valid_test", ",", "\n", "descending", "=", "True", ",", "# largest batch first to warm the caching allocator", "\n", "shard_id", "=", "args", ".", "distributed_rank", ",", "\n", "num_shards", "=", "args", ".", "distributed_world_size", ",", "\n", ")", "\n", "progress", "=", "progress_bar", ".", "build_progress_bar", "(", "\n", "args", ",", "itr", ",", "epoch", ",", "\n", "prefix", "=", "'valid on \\'{}\\' subset'", ".", "format", "(", "subset", ")", ",", "\n", "no_progress_bar", "=", "'simple'", "\n", ")", "\n", "\n", "# reset validation loss meters", "\n", "for", "k", "in", "[", "'valid_loss'", ",", "'valid_nll_loss'", "]", ":", "\n", "        ", "meter", "=", "trainer", ".", "get_meter", "(", "k", ")", "\n", "if", "meter", "is", "not", "None", ":", "\n", "            ", "meter", ".", "reset", "(", ")", "\n", "\n", "", "", "extra_meters", "=", "collections", ".", "defaultdict", "(", "lambda", ":", "AverageMeter", "(", ")", ")", "\n", "for", "sample", "in", "progress", ":", "\n", "        ", "log_output", "=", "trainer", ".", "valid_step", "(", "sample", ")", "\n", "\n", "# log mid-validation stats", "\n", "stats", "=", "get_valid_stats", "(", "trainer", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.get_valid_stats": [[236, 246], ["collections.OrderedDict", "singleprocess_train.get_perplexity", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.get_perplexity", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.trainer.Trainer.get_meter"], ["            ", "if", "k", "in", "[", "'loss'", ",", "'nll_loss'", "]", ":", "\n", "                ", "continue", "\n", "", "extra_meters", "[", "k", "]", ".", "update", "(", "v", ")", "\n", "stats", "[", "k", "]", "=", "extra_meters", "[", "k", "]", ".", "avg", "\n", "", "progress", ".", "log", "(", "stats", ")", "\n", "\n", "# log validation stats", "\n", "", "stats", "=", "get_valid_stats", "(", "trainer", ")", "\n", "for", "k", ",", "meter", "in", "extra_meters", ".", "items", "(", ")", ":", "\n", "        ", "stats", "[", "k", "]", "=", "meter", ".", "avg", "\n", "", "progress", ".", "print", "(", "stats", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.get_perplexity": [[248, 253], ["math.pow", "float"], "function", ["None"], ["return", "stats", "[", "'valid_loss'", "]", "\n", "\n", "\n", "", "def", "get_valid_stats", "(", "trainer", ")", ":", "\n", "    ", "stats", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "stats", "[", "'valid_loss'", "]", "=", "trainer", ".", "get_meter", "(", "'valid_loss'", ")", ".", "avg", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.save_checkpoint": [[255, 279], ["os.path.join", "trainer.save_checkpoint", "os.path.join", "trainer.save_checkpoint", "os.path.join", "trainer.save_checkpoint", "os.path.join", "trainer.save_checkpoint", "hasattr"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.save_checkpoint", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.save_checkpoint", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.save_checkpoint", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.singleprocess_train.save_checkpoint"], ["        ", "nll_loss", "=", "trainer", ".", "get_meter", "(", "'valid_nll_loss'", ")", ".", "avg", "\n", "stats", "[", "'valid_nll_loss'", "]", "=", "nll_loss", "\n", "", "else", ":", "\n", "        ", "nll_loss", "=", "trainer", ".", "get_meter", "(", "'valid_loss'", ")", ".", "avg", "\n", "", "stats", "[", "'valid_ppl'", "]", "=", "get_perplexity", "(", "nll_loss", ")", "\n", "return", "stats", "\n", "\n", "\n", "", "def", "get_perplexity", "(", "loss", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "return", "'{:.2f}'", ".", "format", "(", "math", ".", "pow", "(", "2", ",", "loss", ")", ")", "\n", "", "except", "OverflowError", ":", "\n", "        ", "return", "float", "(", "'inf'", ")", "\n", "\n", "\n", "", "", "def", "save_checkpoint", "(", "trainer", ",", "args", ",", "epoch", ",", "batch_offset", ",", "val_loss", "=", "None", ")", ":", "\n", "    ", "extra_state", "=", "{", "\n", "'epoch'", ":", "epoch", ",", "\n", "'batch_offset'", ":", "batch_offset", ",", "\n", "'val_loss'", ":", "val_loss", ",", "\n", "}", "\n", "\n", "if", "batch_offset", "==", "0", ":", "\n", "        ", "if", "not", "args", ".", "no_epoch_checkpoints", ":", "\n", "            ", "epoch_filename", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save_dir", ",", "'checkpoint{}.pt'", ".", "format", "(", "epoch", ")", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.interactive.main": [[17, 74], ["print", "print", "fairseq.utils.load_ensemble_for_inference", "print", "print", "fairseq.sequence_generator.SequenceGenerator", "fairseq.utils.load_align_dict", "print", "torch.cuda.is_available", "model.make_generation_fast_", "fairseq.sequence_generator.SequenceGenerator.cuda", "src_str.strip.strip", "fairseq.tokenizer.Tokenizer.tokenize().long", "src_tokens.cuda.new", "fairseq.sequence_generator.SequenceGenerator.generate", "print", "len", "len", "src_tokens.cuda.cuda", "torch.autograd.Variable", "torch.autograd.Variable", "fairseq.utils.post_process_prediction", "print", "print", "fairseq.tokenizer.Tokenizer.tokenize", "src_tokens.cuda.numel", "src_tokens.cuda.view", "src_tokens.new.view", "min", "len", "hypo[].int().cpu", "hypo[].int().cpu", "map", "hypo[].int", "hypo[].int"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.load_ensemble_for_inference", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.load_align_dict", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.models.fairseq_model.FairseqModel.make_generation_fast_", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator.cuda", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.sequence_generator.SequenceGenerator.cuda", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.utils.post_process_prediction", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.tokenizer.Tokenizer.tokenize"], ["def", "main", "(", "args", ")", ":", "\n", "    ", "print", "(", "args", ")", "\n", "\n", "use_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "cpu", "\n", "\n", "# Load ensemble", "\n", "print", "(", "'| loading model(s) from {}'", ".", "format", "(", "', '", ".", "join", "(", "args", ".", "path", ")", ")", ")", "\n", "models", ",", "model_args", "=", "utils", ".", "load_ensemble_for_inference", "(", "args", ".", "path", ",", "data_dir", "=", "args", ".", "data", ")", "\n", "src_dict", ",", "dst_dict", "=", "models", "[", "0", "]", ".", "src_dict", ",", "models", "[", "0", "]", ".", "dst_dict", "\n", "\n", "print", "(", "'| [{}] dictionary: {} types'", ".", "format", "(", "model_args", ".", "source_lang", ",", "len", "(", "src_dict", ")", ")", ")", "\n", "print", "(", "'| [{}] dictionary: {} types'", ".", "format", "(", "model_args", ".", "target_lang", ",", "len", "(", "dst_dict", ")", ")", ")", "\n", "\n", "# Optimize ensemble for generation", "\n", "for", "model", "in", "models", ":", "\n", "        ", "model", ".", "make_generation_fast_", "(", "\n", "beamable_mm_beam_size", "=", "None", "if", "args", ".", "no_beamable_mm", "else", "args", ".", "beam", ",", "\n", ")", "\n", "\n", "# Initialize generator", "\n", "", "translator", "=", "SequenceGenerator", "(", "\n", "models", ",", "beam_size", "=", "args", ".", "beam", ",", "stop_early", "=", "(", "not", "args", ".", "no_early_stop", ")", ",", "\n", "normalize_scores", "=", "(", "not", "args", ".", "unnormalized", ")", ",", "len_penalty", "=", "args", ".", "lenpen", ",", "\n", "unk_penalty", "=", "args", ".", "unkpen", ")", "\n", "if", "use_cuda", ":", "\n", "        ", "translator", ".", "cuda", "(", ")", "\n", "\n", "# Load alignment dictionary for unknown word replacement", "\n", "# (None if no unknown word replacement, empty if no path to align dictionary)", "\n", "", "align_dict", "=", "utils", ".", "load_align_dict", "(", "args", ".", "replace_unk", ")", "\n", "\n", "print", "(", "'| Type the input sentence and press return:'", ")", "\n", "for", "src_str", "in", "sys", ".", "stdin", ":", "\n", "        ", "src_str", "=", "src_str", ".", "strip", "(", ")", "\n", "src_tokens", "=", "tokenizer", ".", "Tokenizer", ".", "tokenize", "(", "src_str", ",", "src_dict", ",", "add_if_not_exist", "=", "False", ")", ".", "long", "(", ")", "\n", "if", "use_cuda", ":", "\n", "            ", "src_tokens", "=", "src_tokens", ".", "cuda", "(", ")", "\n", "", "src_lengths", "=", "src_tokens", ".", "new", "(", "[", "src_tokens", ".", "numel", "(", ")", "]", ")", "\n", "translations", "=", "translator", ".", "generate", "(", "\n", "Variable", "(", "src_tokens", ".", "view", "(", "1", ",", "-", "1", ")", ")", ",", "\n", "Variable", "(", "src_lengths", ".", "view", "(", "-", "1", ")", ")", ",", "\n", ")", "\n", "hypos", "=", "translations", "[", "0", "]", "\n", "print", "(", "'O\\t{}'", ".", "format", "(", "src_str", ")", ")", "\n", "\n", "# Process top predictions", "\n", "for", "hypo", "in", "hypos", "[", ":", "min", "(", "len", "(", "hypos", ")", ",", "args", ".", "nbest", ")", "]", ":", "\n", "            ", "hypo_tokens", ",", "hypo_str", ",", "alignment", "=", "utils", ".", "post_process_prediction", "(", "\n", "hypo_tokens", "=", "hypo", "[", "'tokens'", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", ",", "\n", "src_str", "=", "src_str", ",", "\n", "alignment", "=", "hypo", "[", "'alignment'", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", ",", "\n", "align_dict", "=", "align_dict", ",", "\n", "dst_dict", "=", "dst_dict", ",", "\n", "remove_bpe", "=", "args", ".", "remove_bpe", ",", "\n", ")", "\n", "print", "(", "'H\\t{}\\t{}'", ".", "format", "(", "hypo", "[", "'score'", "]", ",", "hypo_str", ")", ")", "\n", "print", "(", "'A\\t{}'", ".", "format", "(", "' '", ".", "join", "(", "map", "(", "str", ",", "alignment", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.train.main": [[16, 24], ["distributed_train.main", "multiprocessing_train.main", "singleprocess_train.main"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.score.main", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.score.main", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.score.main"], ["def", "main", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "distributed_port", ">", "0", "or", "args", ".", "distributed_init_method", "is", "not", "None", ":", "\n", "        ", "distributed_main", "(", "args", ")", "\n", "", "elif", "args", ".", "distributed_world_size", ">", "1", ":", "\n", "        ", "multiprocessing_main", "(", "args", ")", "\n", "", "else", ":", "\n", "        ", "singleprocess_main", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.__init__": [[59, 67], ["threading.Thread", "multiprocessing_train.ErrorHandler.error_thread.start", "signal.signal"], "methods", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.start"], ["def", "__init__", "(", "self", ",", "error_queue", ")", ":", "\n", "        ", "import", "signal", "\n", "import", "threading", "\n", "self", ".", "error_queue", "=", "error_queue", "\n", "self", ".", "children_pids", "=", "[", "]", "\n", "self", ".", "error_thread", "=", "threading", ".", "Thread", "(", "target", "=", "self", ".", "error_listener", ",", "daemon", "=", "True", ")", "\n", "self", ".", "error_thread", ".", "start", "(", ")", "\n", "signal", ".", "signal", "(", "signal", ".", "SIGUSR1", ",", "self", ".", "signal_handler", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.add_child": [[68, 70], ["multiprocessing_train.ErrorHandler.children_pids.append"], "methods", ["None"], ["", "def", "add_child", "(", "self", ",", "pid", ")", ":", "\n", "        ", "self", ".", "children_pids", ".", "append", "(", "pid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.error_listener": [[71, 75], ["multiprocessing_train.ErrorHandler.error_queue.get", "multiprocessing_train.ErrorHandler.error_queue.put", "os.kill", "os.getpid"], "methods", ["None"], ["", "def", "error_listener", "(", "self", ")", ":", "\n", "        ", "(", "rank", ",", "original_trace", ")", "=", "self", ".", "error_queue", ".", "get", "(", ")", "\n", "self", ".", "error_queue", ".", "put", "(", "(", "rank", ",", "original_trace", ")", ")", "\n", "os", ".", "kill", "(", "os", ".", "getpid", "(", ")", ",", "signal", ".", "SIGUSR1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.signal_handler": [[76, 83], ["multiprocessing_train.ErrorHandler.error_queue.get", "Exception", "os.kill"], "methods", ["None"], ["", "def", "signal_handler", "(", "self", ",", "signalnum", ",", "stackframe", ")", ":", "\n", "        ", "for", "pid", "in", "self", ".", "children_pids", ":", "\n", "            ", "os", ".", "kill", "(", "pid", ",", "signal", ".", "SIGINT", ")", "# kill children processes", "\n", "", "(", "rank", ",", "original_trace", ")", "=", "self", ".", "error_queue", ".", "get", "(", ")", "\n", "msg", "=", "\"\\n\\n-- Tracebacks above this line can probably be ignored --\\n\\n\"", "\n", "msg", "+=", "original_trace", "\n", "raise", "Exception", "(", "msg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.main": [[19, 41], ["torch.cuda.device_count", "torch.multiprocessing.get_context", "torch.multiprocessing.get_context.SimpleQueue", "multiprocessing_train.ErrorHandler", "range", "procs.append", "procs[].start", "multiprocessing_train.ErrorHandler.add_child", "p.join", "random.randint", "torch.multiprocessing.get_context.Process"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.ErrorHandler.add_child"], ["def", "main", "(", "args", ")", ":", "\n", "# Set distributed training parameters for a single node.", "\n", "    ", "args", ".", "distributed_world_size", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "args", ".", "distributed_init_method", "=", "'tcp://localhost:{port}'", ".", "format", "(", "\n", "port", "=", "random", ".", "randint", "(", "10000", ",", "20000", ")", ")", "\n", "\n", "mp", "=", "torch", ".", "multiprocessing", ".", "get_context", "(", "'spawn'", ")", "\n", "\n", "# Create a thread to listen for errors in the child processes.", "\n", "error_queue", "=", "mp", ".", "SimpleQueue", "(", ")", "\n", "error_handler", "=", "ErrorHandler", "(", "error_queue", ")", "\n", "\n", "# Train with multiprocessing.", "\n", "procs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "args", ".", "distributed_world_size", ")", ":", "\n", "        ", "args", ".", "distributed_rank", "=", "i", "\n", "args", ".", "device_id", "=", "i", "\n", "procs", ".", "append", "(", "mp", ".", "Process", "(", "target", "=", "run", ",", "args", "=", "(", "args", ",", "error_queue", ",", ")", ",", "daemon", "=", "True", ")", ")", "\n", "procs", "[", "i", "]", ".", "start", "(", ")", "\n", "error_handler", ".", "add_child", "(", "procs", "[", "i", "]", ".", "pid", ")", "\n", "", "for", "p", "in", "procs", ":", "\n", "        ", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.multiprocessing_train.run": [[43, 53], ["fairseq.distributed_utils.distributed_init", "singleprocess_train.main", "error_queue.put", "traceback.format_exc"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.distributed_utils.distributed_init", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.score.main"], ["", "", "def", "run", "(", "args", ",", "error_queue", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "args", ".", "distributed_rank", "=", "distributed_utils", ".", "distributed_init", "(", "args", ")", "\n", "single_process_main", "(", "args", ")", "\n", "", "except", "KeyboardInterrupt", ":", "\n", "        ", "pass", "# killed by parent, do nothing", "\n", "", "except", "Exception", ":", "\n", "# propagate exception to parent process, keeping original traceback", "\n", "        ", "import", "traceback", "\n", "error_queue", ".", "put", "(", "(", "args", ".", "distributed_rank", ",", "traceback", ".", "format_exc", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.preprocess.get_parser": [[19, 42], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["\n", "\n", "def", "get_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Data pre-processing: Create dictionary and store data in binary format'", ")", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--source-lang'", ",", "default", "=", "None", ",", "metavar", "=", "'SRC'", ",", "help", "=", "'source language'", ")", "\n", "parser", ".", "add_argument", "(", "'-t'", ",", "'--target-lang'", ",", "default", "=", "None", ",", "metavar", "=", "'TARGET'", ",", "help", "=", "'target language'", ")", "\n", "parser", ".", "add_argument", "(", "'--trainpref'", ",", "metavar", "=", "'FP'", ",", "default", "=", "None", ",", "help", "=", "'target language'", ")", "\n", "parser", ".", "add_argument", "(", "'--validpref'", ",", "metavar", "=", "'FP'", ",", "default", "=", "None", ",", "help", "=", "'comma separated, valid language prefixes'", ")", "\n", "parser", ".", "add_argument", "(", "'--testpref'", ",", "metavar", "=", "'FP'", ",", "default", "=", "None", ",", "help", "=", "'comma separated, test language prefixes'", ")", "\n", "parser", ".", "add_argument", "(", "'--destdir'", ",", "metavar", "=", "'DIR'", ",", "default", "=", "'data-bin'", ",", "help", "=", "'destination dir'", ")", "\n", "parser", ".", "add_argument", "(", "'--thresholdtgt'", ",", "metavar", "=", "'N'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "'map words appearing less than threshold times to unknown'", ")", "\n", "parser", ".", "add_argument", "(", "'--thresholdsrc'", ",", "metavar", "=", "'N'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "'map words appearing less than threshold times to unknown'", ")", "\n", "parser", ".", "add_argument", "(", "'--tgtdict'", ",", "metavar", "=", "'FP'", ",", "help", "=", "'reuse given target dictionary'", ")", "\n", "parser", ".", "add_argument", "(", "'--srcdict'", ",", "metavar", "=", "'FP'", ",", "help", "=", "'reuse given source dictionary'", ")", "\n", "parser", ".", "add_argument", "(", "'--nwordstgt'", ",", "metavar", "=", "'N'", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "help", "=", "'number of target words to retain'", ")", "\n", "parser", ".", "add_argument", "(", "'--nwordssrc'", ",", "metavar", "=", "'N'", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "help", "=", "'number of source words to retain'", ")", "\n", "parser", ".", "add_argument", "(", "'--alignfile'", ",", "metavar", "=", "'ALIGN'", ",", "default", "=", "None", ",", "help", "=", "'an alignment file (optional)'", ")", "\n", "parser", ".", "add_argument", "(", "'--output-format'", ",", "metavar", "=", "'FORMAT'", ",", "default", "=", "'binary'", ",", "choices", "=", "[", "'binary'", ",", "'raw'", "]", ",", "\n", "help", "=", "'output format (optional)'", ")", "\n", "parser", ".", "add_argument", "(", "'--joined-dictionary'", ",", "action", "=", "'store_true'", ",", "help", "=", "'Generate joined dictionary'", ")", "\n", "parser", ".", "add_argument", "(", "'--only-source'", ",", "action", "=", "'store_true'", ",", "help", "=", "'Only process the source language'", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.preprocess.main": [[44, 165], ["print", "os.makedirs", "Tokenizer.build_dictionary.save", "preprocess.main.make_all"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.dictionary.Dictionary.save"], ["\n", "\n", "", "def", "main", "(", "args", ")", ":", "\n", "    ", "print", "(", "args", ")", "\n", "os", ".", "makedirs", "(", "args", ".", "destdir", ",", "exist_ok", "=", "True", ")", "\n", "target", "=", "not", "args", ".", "only_source", "\n", "\n", "if", "args", ".", "joined_dictionary", ":", "\n", "        ", "assert", "not", "args", ".", "srcdict", ",", "'cannot combine --srcdict and --joined-dictionary'", "\n", "assert", "not", "args", ".", "tgtdict", ",", "'cannot combine --tgtdict and --joined-dictionary'", "\n", "src_dict", "=", "dictionary", ".", "Dictionary", "(", ")", "\n", "for", "lang", "in", "[", "args", ".", "source_lang", ",", "args", ".", "target_lang", "]", ":", "\n", "            ", "Tokenizer", ".", "add_file_to_dictionary", "(", "\n", "filename", "=", "'{}.{}'", ".", "format", "(", "args", ".", "trainpref", ",", "lang", ")", ",", "\n", "dict", "=", "src_dict", ",", "\n", "tokenize", "=", "tokenize_line", ",", "\n", ")", "\n", "", "src_dict", ".", "finalize", "(", ")", "\n", "tgt_dict", "=", "src_dict", "\n", "", "else", ":", "\n", "        ", "if", "args", ".", "srcdict", ":", "\n", "            ", "src_dict", "=", "dictionary", ".", "Dictionary", ".", "load", "(", "args", ".", "srcdict", ")", "\n", "", "else", ":", "\n", "            ", "assert", "args", ".", "trainpref", ",", "\"--trainpref must be set if --srcdict is not specified\"", "\n", "src_dict", "=", "Tokenizer", ".", "build_dictionary", "(", "filename", "=", "'{}.{}'", ".", "format", "(", "args", ".", "trainpref", ",", "args", ".", "source_lang", ")", ")", "\n", "", "if", "target", ":", "\n", "            ", "if", "args", ".", "tgtdict", ":", "\n", "                ", "tgt_dict", "=", "dictionary", ".", "Dictionary", ".", "load", "(", "args", ".", "tgtdict", ")", "\n", "", "else", ":", "\n", "                ", "assert", "args", ".", "trainpref", ",", "\"--trainpref must be set if --tgtdict is not specified\"", "\n", "tgt_dict", "=", "Tokenizer", ".", "build_dictionary", "(", "filename", "=", "'{}.{}'", ".", "format", "(", "args", ".", "trainpref", ",", "args", ".", "target_lang", ")", ")", "\n", "\n", "", "", "", "src_dict", ".", "save", "(", "os", ".", "path", ".", "join", "(", "args", ".", "destdir", ",", "'dict.{}.txt'", ".", "format", "(", "args", ".", "source_lang", ")", ")", ",", "\n", "threshold", "=", "args", ".", "thresholdsrc", ",", "nwords", "=", "args", ".", "nwordssrc", ")", "\n", "if", "target", ":", "\n", "        ", "tgt_dict", ".", "save", "(", "os", ".", "path", ".", "join", "(", "args", ".", "destdir", ",", "'dict.{}.txt'", ".", "format", "(", "args", ".", "target_lang", ")", ")", ",", "\n", "threshold", "=", "args", ".", "thresholdtgt", ",", "nwords", "=", "args", ".", "nwordstgt", ")", "\n", "\n", "", "exit", "(", "0", ")", "\n", "\n", "def", "make_binary_dataset", "(", "input_prefix", ",", "output_prefix", ",", "lang", ")", ":", "\n", "        ", "dict", "=", "dictionary", ".", "Dictionary", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "destdir", ",", "'dict.{}.txt'", ".", "format", "(", "lang", ")", ")", ")", "\n", "print", "(", "'| [{}] Dictionary: {} types'", ".", "format", "(", "lang", ",", "len", "(", "dict", ")", "-", "1", ")", ")", "\n", "\n", "ds", "=", "indexed_dataset", ".", "IndexedDatasetBuilder", "(", "\n", "'{}/{}.{}-{}.{}.bin'", ".", "format", "(", "args", ".", "destdir", ",", "output_prefix", ",", "args", ".", "source_lang", ",", "\n", "args", ".", "target_lang", ",", "lang", ")", "\n", ")", "\n", "\n", "def", "consumer", "(", "tensor", ")", ":", "\n", "            ", "ds", ".", "add_item", "(", "tensor", ")", "\n", "\n", "", "input_file", "=", "'{}.{}'", ".", "format", "(", "input_prefix", ",", "lang", ")", "\n", "res", "=", "Tokenizer", ".", "binarize", "(", "input_file", ",", "dict", ",", "consumer", ")", "\n", "print", "(", "'| [{}] {}: {} sents, {} tokens, {:.3}% replaced by {}'", ".", "format", "(", "\n", "lang", ",", "input_file", ",", "res", "[", "'nseq'", "]", ",", "res", "[", "'ntok'", "]", ",", "\n", "100", "*", "res", "[", "'nunk'", "]", "/", "res", "[", "'ntok'", "]", ",", "dict", ".", "unk_word", ")", ")", "\n", "ds", ".", "finalize", "(", "'{}/{}.{}-{}.{}.idx'", ".", "format", "(", "\n", "args", ".", "destdir", ",", "output_prefix", ",", "\n", "args", ".", "source_lang", ",", "args", ".", "target_lang", ",", "lang", ")", ")", "\n", "\n", "", "def", "make_dataset", "(", "input_prefix", ",", "output_prefix", ",", "lang", ",", "output_format", "=", "'binary'", ")", ":", "\n", "        ", "print", "(", "input_prefix", ",", "output_prefix", ",", "lang", ")", "\n", "exit", "(", "0", ")", "\n", "if", "output_format", "==", "'binary'", ":", "\n", "            ", "make_binary_dataset", "(", "input_prefix", ",", "output_prefix", ",", "lang", ")", "\n", "", "elif", "output_format", "==", "'raw'", ":", "\n", "# Copy original text file to destination folder", "\n", "            ", "output_text_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "destdir", ",", "'{}.{}'", ".", "format", "(", "output_prefix", ",", "lang", ")", ")", "\n", "shutil", ".", "copyfile", "(", "'{}.{}'", ".", "format", "(", "input_prefix", ",", "lang", ")", ",", "output_text_file", ")", "\n", "\n", "", "", "def", "make_all", "(", "args", ",", "make_dataset", ",", "lang", ")", ":", "\n", "        ", "if", "args", ".", "trainpref", ":", "\n", "            ", "make_dataset", "(", "args", ".", "trainpref", ",", "'train'", ",", "lang", ",", "args", ".", "output_format", ")", "\n", "", "if", "args", ".", "validpref", ":", "\n", "            ", "for", "k", ",", "validpref", "in", "enumerate", "(", "args", ".", "validpref", ".", "split", "(", "','", ")", ")", ":", "\n", "                ", "outprefix", "=", "'valid{}'", ".", "format", "(", "k", ")", "if", "k", ">", "0", "else", "'valid'", "\n", "make_dataset", "(", "validpref", ",", "outprefix", ",", "lang", ",", "args", ".", "output_format", ")", "\n", "", "", "if", "args", ".", "testpref", ":", "\n", "            ", "for", "k", ",", "testpref", "in", "enumerate", "(", "args", ".", "testpref", ".", "split", "(", "','", ")", ")", ":", "\n", "                ", "outprefix", "=", "'test{}'", ".", "format", "(", "k", ")", "if", "k", ">", "0", "else", "'test'", "\n", "make_dataset", "(", "testpref", ",", "outprefix", ",", "lang", ",", "args", ".", "output_format", ")", "\n", "\n", "", "", "", "make_all", "(", "args", ",", "make_dataset", ",", "args", ".", "source_lang", ")", "\n", "if", "target", ":", "\n", "        ", "make_all", "(", "args", ",", "make_dataset", ",", "args", ".", "target_lang", ")", "\n", "", "print", "(", "'| Wrote preprocessed data to {}'", ".", "format", "(", "args", ".", "destdir", ")", ")", "\n", "\n", "\n", "if", "args", ".", "alignfile", ":", "\n", "        ", "assert", "args", ".", "trainpref", ",", "\"--trainpref must be set if --alignfile is specified\"", "\n", "src_file_name", "=", "'{}.{}'", ".", "format", "(", "args", ".", "trainpref", ",", "args", ".", "source_lang", ")", "\n", "tgt_file_name", "=", "'{}.{}'", ".", "format", "(", "args", ".", "trainpref", ",", "args", ".", "target_lang", ")", "\n", "src_dict", "=", "dictionary", ".", "Dictionary", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "destdir", ",", "'dict.{}.txt'", ".", "format", "(", "args", ".", "source_lang", ")", ")", ")", "\n", "tgt_dict", "=", "dictionary", ".", "Dictionary", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "destdir", ",", "'dict.{}.txt'", ".", "format", "(", "args", ".", "target_lang", ")", ")", ")", "\n", "freq_map", "=", "{", "}", "\n", "with", "open", "(", "args", ".", "alignfile", ",", "'r'", ")", "as", "align_file", ":", "\n", "            ", "with", "open", "(", "src_file_name", ",", "'r'", ")", "as", "src_file", ":", "\n", "                ", "with", "open", "(", "tgt_file_name", ",", "'r'", ")", "as", "tgt_file", ":", "\n", "                    ", "for", "a", ",", "s", ",", "t", "in", "zip_longest", "(", "align_file", ",", "src_file", ",", "tgt_file", ")", ":", "\n", "                        ", "si", "=", "Tokenizer", ".", "tokenize", "(", "s", ",", "src_dict", ",", "add_if_not_exist", "=", "False", ")", "\n", "ti", "=", "Tokenizer", ".", "tokenize", "(", "t", ",", "tgt_dict", ",", "add_if_not_exist", "=", "False", ")", "\n", "ai", "=", "list", "(", "map", "(", "lambda", "x", ":", "tuple", "(", "x", ".", "split", "(", "'-'", ")", ")", ",", "a", ".", "split", "(", ")", ")", ")", "\n", "for", "sai", ",", "tai", "in", "ai", ":", "\n", "                            ", "srcidx", "=", "si", "[", "int", "(", "sai", ")", "]", "\n", "tgtidx", "=", "ti", "[", "int", "(", "tai", ")", "]", "\n", "if", "srcidx", "!=", "src_dict", ".", "unk", "(", ")", "and", "tgtidx", "!=", "tgt_dict", ".", "unk", "(", ")", ":", "\n", "                                ", "assert", "srcidx", "!=", "src_dict", ".", "pad", "(", ")", "\n", "assert", "srcidx", "!=", "src_dict", ".", "eos", "(", ")", "\n", "assert", "tgtidx", "!=", "tgt_dict", ".", "pad", "(", ")", "\n", "assert", "tgtidx", "!=", "tgt_dict", ".", "eos", "(", ")", "\n", "\n", "if", "srcidx", "not", "in", "freq_map", ":", "\n", "                                    ", "freq_map", "[", "srcidx", "]", "=", "{", "}", "\n", "", "if", "tgtidx", "not", "in", "freq_map", "[", "srcidx", "]", ":", "\n", "                                    ", "freq_map", "[", "srcidx", "]", "[", "tgtidx", "]", "=", "1", "\n", "", "else", ":", "\n", "                                    ", "freq_map", "[", "srcidx", "]", "[", "tgtidx", "]", "+=", "1", "\n", "\n", "", "", "", "", "", "", "", "align_dict", "=", "{", "}", "\n", "for", "srcidx", "in", "freq_map", ".", "keys", "(", ")", ":", "\n", "            ", "align_dict", "[", "srcidx", "]", "=", "max", "(", "freq_map", "[", "srcidx", "]", ",", "key", "=", "freq_map", "[", "srcidx", "]", ".", "get", ")", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.distributed_train.main": [[17, 40], ["fairseq.distributed_utils.distributed_init", "print", "singleprocess_train.main", "os.environ.get", "ValueError", "socket.gethostname", "subprocess.check_output", "int", "int", "os.environ.get", "os.environ.get", "[].decode", "subprocess.check_output.split"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.distributed_utils.distributed_init", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.score.main"], ["def", "main", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "distributed_init_method", "is", "None", "and", "args", ".", "distributed_port", ">", "0", ":", "\n", "# We can determine the init method automatically for Slurm.", "\n", "        ", "node_list", "=", "os", ".", "environ", ".", "get", "(", "'SLURM_JOB_NODELIST'", ")", "\n", "if", "node_list", "is", "not", "None", ":", "\n", "            ", "try", ":", "\n", "                ", "hostnames", "=", "subprocess", ".", "check_output", "(", "[", "'scontrol'", ",", "'show'", ",", "'hostnames'", ",", "node_list", "]", ")", "\n", "args", ".", "distributed_init_method", "=", "'tcp://{host}:{port}'", ".", "format", "(", "\n", "host", "=", "hostnames", ".", "split", "(", ")", "[", "0", "]", ".", "decode", "(", "'utf-8'", ")", ",", "\n", "port", "=", "args", ".", "distributed_port", ")", "\n", "args", ".", "distributed_rank", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'SLURM_PROCID'", ")", ")", "\n", "args", ".", "device_id", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'SLURM_LOCALID'", ")", ")", "\n", "", "except", "subprocess", ".", "CalledProcessError", "as", "e", ":", "# scontrol failed", "\n", "                ", "raise", "e", "\n", "", "except", "FileNotFoundError", "as", "e", ":", "# Slurm is not installed", "\n", "                ", "pass", "\n", "", "", "", "if", "args", ".", "distributed_init_method", "is", "None", ":", "\n", "        ", "raise", "ValueError", "(", "'--distributed-init-method or --distributed-port '", "\n", "'must be specified for distributed training'", ")", "\n", "\n", "", "args", ".", "distributed_rank", "=", "distributed_utils", ".", "distributed_init", "(", "args", ")", "\n", "print", "(", "'| initialized host {} as rank {}'", ".", "format", "(", "socket", ".", "gethostname", "(", ")", ",", "args", ".", "distributed_rank", ")", ")", "\n", "single_process_main", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.XSum-ConvS2S.score.main": [[17, 56], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "os.path.exists", "fairseq.dictionary.Dictionary", "os.path.exists", "fd.readlines", "score.main.score"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.progress_bar.tqdm_progress_bar.print", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.exists", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.indexed_dataset.IndexedDataset.exists", "home.repos.pwc.inspect_result.shashiongithub_XSum.fairseq.bleu.Scorer.score"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Command-line script for BLEU scoring.'", ")", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--sys'", ",", "default", "=", "'-'", ",", "help", "=", "'system output'", ")", "\n", "parser", ".", "add_argument", "(", "'-r'", ",", "'--ref'", ",", "required", "=", "True", ",", "help", "=", "'references'", ")", "\n", "parser", ".", "add_argument", "(", "'-o'", ",", "'--order'", ",", "default", "=", "4", ",", "metavar", "=", "'N'", ",", "\n", "type", "=", "int", ",", "help", "=", "'consider ngrams up to this order'", ")", "\n", "parser", ".", "add_argument", "(", "'--ignore-case'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'case-insensitive scoring'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "print", "(", "args", ")", "\n", "\n", "assert", "args", ".", "sys", "==", "'-'", "or", "os", ".", "path", ".", "exists", "(", "args", ".", "sys", ")", ",", "\"System output file {} does not exist\"", ".", "format", "(", "args", ".", "sys", ")", "\n", "assert", "os", ".", "path", ".", "exists", "(", "args", ".", "ref", ")", ",", "\"Reference file {} does not exist\"", ".", "format", "(", "args", ".", "ref", ")", "\n", "\n", "dict", "=", "dictionary", ".", "Dictionary", "(", ")", "\n", "\n", "def", "readlines", "(", "fd", ")", ":", "\n", "        ", "for", "line", "in", "fd", ".", "readlines", "(", ")", ":", "\n", "            ", "if", "args", ".", "ignore_case", ":", "\n", "                ", "yield", "line", ".", "lower", "(", ")", "\n", "", "yield", "line", "\n", "\n", "", "", "def", "score", "(", "fdsys", ")", ":", "\n", "        ", "with", "open", "(", "args", ".", "ref", ")", "as", "fdref", ":", "\n", "            ", "scorer", "=", "bleu", ".", "Scorer", "(", "dict", ".", "pad", "(", ")", ",", "dict", ".", "eos", "(", ")", ",", "dict", ".", "unk", "(", ")", ")", "\n", "for", "sys_tok", ",", "ref_tok", "in", "zip", "(", "readlines", "(", "fdsys", ")", ",", "readlines", "(", "fdref", ")", ")", ":", "\n", "                ", "sys_tok", "=", "tokenizer", ".", "Tokenizer", ".", "tokenize", "(", "sys_tok", ",", "dict", ")", "\n", "ref_tok", "=", "tokenizer", ".", "Tokenizer", ".", "tokenize", "(", "ref_tok", ",", "dict", ")", "\n", "scorer", ".", "add", "(", "ref_tok", ",", "sys_tok", ")", "\n", "", "print", "(", "scorer", ".", "result_string", "(", "args", ".", "order", ")", ")", "\n", "\n", "", "", "if", "args", ".", "sys", "==", "'-'", ":", "\n", "        ", "score", "(", "sys", ".", "stdin", ")", "\n", "", "else", ":", "\n", "        ", "with", "open", "(", "args", ".", "sys", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "score", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shashiongithub_XSum.scripts.process-corenlp-xml-data.indent": [[9, 23], ["len", "process-corenlp-xml-data.indent", "elem.text.strip", "elem.tail.strip", "elem.tail.strip", "elem.tail.strip"], "function", ["home.repos.pwc.inspect_result.shashiongithub_XSum.scripts.process-corenlp-xml-data.indent"], ["def", "indent", "(", "elem", ",", "level", "=", "0", ")", ":", "\n", "  ", "i", "=", "\"\\n\"", "+", "level", "*", "\"  \"", "\n", "if", "len", "(", "elem", ")", ":", "\n", "    ", "if", "not", "elem", ".", "text", "or", "not", "elem", ".", "text", ".", "strip", "(", ")", ":", "\n", "      ", "elem", ".", "text", "=", "i", "+", "\"  \"", "\n", "", "if", "not", "elem", ".", "tail", "or", "not", "elem", ".", "tail", ".", "strip", "(", ")", ":", "\n", "      ", "elem", ".", "tail", "=", "i", "\n", "", "for", "elem", "in", "elem", ":", "\n", "      ", "indent", "(", "elem", ",", "level", "+", "1", ")", "\n", "", "if", "not", "elem", ".", "tail", "or", "not", "elem", ".", "tail", ".", "strip", "(", ")", ":", "\n", "      ", "elem", ".", "tail", "=", "i", "\n", "", "", "else", ":", "\n", "    ", "if", "level", "and", "(", "not", "elem", ".", "tail", "or", "not", "elem", ".", "tail", ".", "strip", "(", ")", ")", ":", "\n", "      ", "elem", ".", "tail", "=", "i", "\n", "\n"]]}