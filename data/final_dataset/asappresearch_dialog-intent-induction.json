{"home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train_qt.transform": [[38, 49], ["model.eval", "range", "numpy.concatenate", "np.concatenate.append", "torch.autograd.no_grad", "model", "model.cpu().data.numpy", "len", "model.cpu"], "function", ["None"], ["def", "transform", "(", "data", ",", "model", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "latent_zs", "=", "[", "]", "\n", "n_batch", "=", "(", "len", "(", "data", ")", "+", "BATCH_SIZE", "-", "1", ")", "//", "BATCH_SIZE", "\n", "for", "i", "in", "range", "(", "n_batch", ")", ":", "\n", "        ", "data_batch", "=", "data", "[", "i", "*", "BATCH_SIZE", ":", "(", "i", "+", "1", ")", "*", "BATCH_SIZE", "]", "\n", "with", "autograd", ".", "no_grad", "(", ")", ":", "\n", "            ", "latent_z", "=", "model", "(", "data_batch", ",", "encoder", "=", "'v1'", ")", "\n", "", "latent_zs", ".", "append", "(", "latent_z", ".", "cpu", "(", ")", ".", "data", ".", "numpy", "(", ")", ")", "\n", "", "latent_zs", "=", "np", ".", "concatenate", "(", "latent_zs", ")", "\n", "return", "latent_zs", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train_qt.calc_prec_rec_f1_acc": [[51, 63], ["zip", "metrics.cluster_metrics.calc_prec_rec_f1", "metrics.cluster_metrics.calc_ACC", "list", "torch.LongTensor().to", "torch.LongTensor().to", "lgolds.append", "lpreds.append", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_prec_rec_f1", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_ACC"], ["", "def", "calc_prec_rec_f1_acc", "(", "preds", ",", "golds", ")", ":", "\n", "    ", "lgolds", ",", "lpreds", "=", "[", "]", ",", "[", "]", "\n", "for", "g", ",", "p", "in", "zip", "(", "golds", ",", "list", "(", "preds", ")", ")", ":", "\n", "        ", "if", "g", ">", "0", ":", "\n", "            ", "lgolds", ".", "append", "(", "g", ")", "\n", "lpreds", ".", "append", "(", "p", ")", "\n", "", "", "prec", ",", "rec", ",", "f1", "=", "cluster_metrics", ".", "calc_prec_rec_f1", "(", "\n", "gnd_assignments", "=", "torch", ".", "LongTensor", "(", "lgolds", ")", ".", "to", "(", "device", ")", ",", "\n", "pred_assignments", "=", "torch", ".", "LongTensor", "(", "lpreds", ")", ".", "to", "(", "device", ")", ")", "\n", "acc", "=", "cluster_metrics", ".", "calc_ACC", "(", "\n", "torch", ".", "LongTensor", "(", "lpreds", ")", ".", "to", "(", "device", ")", ",", "torch", ".", "LongTensor", "(", "lgolds", ")", ".", "to", "(", "device", ")", ")", "\n", "return", "prec", ",", "rec", ",", "f1", ",", "acc", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train_qt.main": [[65, 237], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "proc_data.Dataset", "print", "len", "print", "print", "open", "numpy.sqrt", "print", "model.multiview_encoders.MultiviewEncoders.from_embeddings", "MultiviewEncoders.from_embeddings.to", "torch.optim.Adam", "range", "sklearn.cluster.KMeans", "parser.parse_args.scenarios.split", "len", "line.strip().split", "len", "MultiviewEncoders.from_embeddings.parameters", "MultiviewEncoders.from_embeddings.train", "numpy.random.permutation", "pretrain_method", "MultiviewEncoders.from_embeddings.eval", "pretrain_method", "print", "MultiviewEncoders.from_embeddings.load_state_dict", "pretrain.after_pretrain_qt", "train_qt.calc_prec_rec_f1_acc", "print", "float", "word.lower", "pretrained_list.append", "numpy.random.uniform", "pretrained_list.append", "torch.FloatTensor", "copy.deepcopy", "train_qt.transform", "sklearn.cluster.KMeans.fit_predict", "line.strip", "len", "numpy.array", "MultiviewEncoders.from_embeddings.state_dict", "torch.stack", "encoded.numpy.numpy", "sklearn.cluster.KMeans.fit_predict", "datetime.datetime.now", "train_qt.transform", "torch.from_numpy", "encoded_conv.mean.mean", "encoded.numpy.append", "torch.from_numpy", "torch.stack", "torch.cat", "print", "torch.cat.numpy", "sklearn.cluster.KMeans.fit_predict", "datetime.datetime.now", "train_qt.transform", "train_qt.transform", "torch.from_numpy", "encoded_conv.mean.mean", "torch.stack.append", "torch.cat.size", "torch.stack", "print", "encoded.numpy.numpy", "sklearn.cluster.KMeans.fit_predict", "word.lower", "train_qt.transform", "torch.from_numpy", "encoded_conv.mean.mean", "encoded.numpy.append", "encoded.numpy.size", "print", "torch.from_numpy", "torch.stack", "multiview.mvsc.MVSC", "print", "time.time", "multiview.mvsc.MVSC.fit_transform", "print", "print", "Exception", "train_qt.transform", "train_qt.transform", "torch.from_numpy", "encoded_conv.mean.mean", "torch.stack.append", "time.time", "print"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.from_embeddings", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.pretrain.after_pretrain_qt", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train_qt.calc_prec_rec_f1_acc", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.transform", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.transform", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.transform", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.transform", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.transform", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.transform", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.transform"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--data-path'", ",", "type", "=", "str", ",", "default", "=", "'./data/airlines_processed.csv'", ")", "\n", "parser", ".", "add_argument", "(", "'--glove-path'", ",", "type", "=", "str", ",", "default", "=", "'./data/glove.840B.300d.txt'", ")", "\n", "parser", ".", "add_argument", "(", "'--pre-epoch'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "'--pt-batch'", ",", "type", "=", "int", ",", "default", "=", "100", ")", "\n", "parser", ".", "add_argument", "(", "'--scenarios'", ",", "type", "=", "str", ",", "default", "=", "'view1,view2,concatviews,wholeconv'", ",", "\n", "help", "=", "'comma-separated, from [view1|view2|concatviews|wholeconv|mvsc]'", ")", "\n", "parser", ".", "add_argument", "(", "'--mvsc-no-unk'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'only feed non-unk data to MVSC (to avoid oom)'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--view1-col'", ",", "type", "=", "str", ",", "default", "=", "'view1'", ")", "\n", "parser", ".", "add_argument", "(", "'--view2-col'", ",", "type", "=", "str", ",", "default", "=", "'view2'", ")", "\n", "parser", ".", "add_argument", "(", "'--label-col'", ",", "type", "=", "str", ",", "default", "=", "'tag'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "print", "(", "'loading dataset'", ")", "\n", "dataset", "=", "Dataset", "(", "args", ".", "data_path", ",", "view1_col", "=", "args", ".", "view1_col", ",", "view2_col", "=", "args", ".", "view2_col", ",", "\n", "label_col", "=", "args", ".", "label_col", ")", "\n", "n_cluster", "=", "len", "(", "dataset", ".", "id_to_label", ")", "-", "1", "\n", "print", "(", "\"num of class = %d\"", "%", "n_cluster", ")", "\n", "\n", "id_to_token", ",", "token_to_id", "=", "dataset", ".", "id_to_token", ",", "dataset", ".", "token_to_id", "\n", "vocab_size", "=", "len", "(", "dataset", ".", "token_to_id", ")", "\n", "print", "(", "'vocab_size'", ",", "vocab_size", ")", "\n", "\n", "# Load pre-trained GloVe vectors", "\n", "pretrained", "=", "{", "}", "\n", "word_emb_size", "=", "0", "\n", "print", "(", "'loading glove'", ")", "\n", "for", "line", "in", "open", "(", "args", ".", "glove_path", ")", ":", "\n", "        ", "parts", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "if", "len", "(", "parts", ")", "%", "100", "!=", "1", ":", "\n", "            ", "continue", "\n", "", "word", "=", "parts", "[", "0", "]", "\n", "if", "word", "not", "in", "token_to_id", ":", "\n", "            ", "continue", "\n", "", "vector", "=", "[", "float", "(", "v", ")", "for", "v", "in", "parts", "[", "1", ":", "]", "]", "\n", "pretrained", "[", "word", "]", "=", "vector", "\n", "word_emb_size", "=", "len", "(", "vector", ")", "\n", "", "pretrained_list", "=", "[", "]", "\n", "scale", "=", "np", ".", "sqrt", "(", "3.0", "/", "word_emb_size", ")", "\n", "print", "(", "'loading oov'", ")", "\n", "for", "word", "in", "id_to_token", ":", "\n", "# apply lower() because all GloVe vectors are for lowercase words", "\n", "        ", "if", "word", ".", "lower", "(", ")", "in", "pretrained", ":", "\n", "            ", "pretrained_list", ".", "append", "(", "np", ".", "array", "(", "pretrained", "[", "word", ".", "lower", "(", ")", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "random_vector", "=", "np", ".", "random", ".", "uniform", "(", "-", "scale", ",", "scale", ",", "[", "word_emb_size", "]", ")", "\n", "pretrained_list", ".", "append", "(", "random_vector", ")", "\n", "\n", "", "", "model", "=", "MultiviewEncoders", ".", "from_embeddings", "(", "\n", "embeddings", "=", "torch", ".", "FloatTensor", "(", "pretrained_list", ")", ",", "\n", "num_layers", "=", "LSTM_LAYER", ",", "\n", "embedding_size", "=", "word_emb_size", ",", "\n", "lstm_hidden_size", "=", "LSTM_HIDDEN", ",", "\n", "word_dropout", "=", "WORD_DROPOUT_RATE", ",", "\n", "dropout", "=", "DROPOUT_RATE", ",", "\n", "vocab_size", "=", "vocab_size", "\n", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "LEARNING_RATE", ")", "\n", "\n", "expressions", "=", "(", "model", ",", "optimizer", ")", "\n", "pre_acc", ",", "pre_state", "=", "0.", ",", "None", "\n", "pretrain_method", "=", "pretrain", ".", "pretrain_qt", "\n", "for", "epoch", "in", "range", "(", "1", ",", "args", ".", "pre_epoch", "+", "1", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "perm_idx", "=", "np", ".", "random", ".", "permutation", "(", "dataset", ".", "trn_idx", ")", "\n", "trn_loss", ",", "_", "=", "pretrain_method", "(", "dataset", ",", "perm_idx", ",", "expressions", ",", "train", "=", "True", ")", "\n", "model", ".", "eval", "(", ")", "\n", "_", ",", "tst_acc", "=", "pretrain_method", "(", "dataset", ",", "dataset", ".", "tst_idx", ",", "expressions", ",", "train", "=", "False", ")", "\n", "if", "tst_acc", ">", "pre_acc", ":", "\n", "            ", "pre_state", "=", "copy", ".", "deepcopy", "(", "model", ".", "state_dict", "(", ")", ")", "\n", "pre_acc", "=", "tst_acc", "\n", "", "print", "(", "f'{datetime.datetime.now()} epoch {epoch}, train_loss={trn_loss:.4f} '", "\n", "f'test_acc={tst_acc:.4f}'", ")", "\n", "\n", "", "if", "args", ".", "pre_epoch", ">", "0", ":", "\n", "# load best state", "\n", "        ", "model", ".", "load_state_dict", "(", "pre_state", ")", "\n", "\n", "# deepcopy pretrained views into v1 and/or view2", "\n", "pretrain", ".", "after_pretrain_qt", "(", "model", ")", "\n", "\n", "", "kmeans", "=", "sklearn", ".", "cluster", ".", "KMeans", "(", "n_clusters", "=", "n_cluster", ",", "max_iter", "=", "300", ",", "verbose", "=", "0", ",", "random_state", "=", "0", ")", "\n", "\n", "golds", "=", "[", "dataset", "[", "idx", "]", "[", "1", "]", "for", "idx", "in", "dataset", ".", "trn_idx", "]", "\n", "for", "rep", "in", "args", ".", "scenarios", ".", "split", "(", "','", ")", ":", "\n", "        ", "if", "rep", "==", "'view1'", ":", "\n", "            ", "data", "=", "[", "dataset", "[", "idx", "]", "[", "0", "]", "[", "0", "]", "for", "idx", "in", "dataset", ".", "trn_idx", "]", "\n", "encoded", "=", "transform", "(", "data", "=", "data", ",", "model", "=", "model", ")", "\n", "preds", "=", "kmeans", ".", "fit_predict", "(", "encoded", ")", "\n", "", "elif", "rep", "==", "'view2'", ":", "\n", "            ", "data", "=", "[", "dataset", "[", "idx", "]", "[", "0", "]", "[", "1", "]", "for", "idx", "in", "dataset", ".", "trn_idx", "]", "\n", "encoded", "=", "[", "]", "\n", "for", "conv", "in", "data", ":", "\n", "                ", "encoded_conv", "=", "transform", "(", "data", "=", "conv", ",", "model", "=", "model", ")", "\n", "encoded_conv", "=", "torch", ".", "from_numpy", "(", "encoded_conv", ")", "\n", "encoded_conv", "=", "encoded_conv", ".", "mean", "(", "dim", "=", "0", ")", "\n", "encoded", ".", "append", "(", "encoded_conv", ")", "\n", "", "encoded", "=", "torch", ".", "stack", "(", "encoded", ",", "dim", "=", "0", ")", "\n", "# print('encoded.size()', encoded.size())", "\n", "encoded", "=", "encoded", ".", "numpy", "(", ")", "\n", "preds", "=", "kmeans", ".", "fit_predict", "(", "encoded", ")", "\n", "", "elif", "rep", "==", "'concatviews'", ":", "\n", "            ", "v1_data", "=", "[", "dataset", "[", "idx", "]", "[", "0", "]", "[", "0", "]", "for", "idx", "in", "dataset", ".", "trn_idx", "]", "\n", "v1_encoded", "=", "torch", ".", "from_numpy", "(", "transform", "(", "data", "=", "v1_data", ",", "model", "=", "model", ")", ")", "\n", "\n", "v2_data", "=", "[", "dataset", "[", "idx", "]", "[", "0", "]", "[", "1", "]", "for", "idx", "in", "dataset", ".", "trn_idx", "]", "\n", "v2_encoded", "=", "[", "]", "\n", "for", "conv", "in", "v2_data", ":", "\n", "                ", "encoded_conv", "=", "transform", "(", "data", "=", "conv", ",", "model", "=", "model", ")", "\n", "encoded_conv", "=", "torch", ".", "from_numpy", "(", "encoded_conv", ")", "\n", "encoded_conv", "=", "encoded_conv", ".", "mean", "(", "dim", "=", "0", ")", "\n", "v2_encoded", ".", "append", "(", "encoded_conv", ")", "\n", "", "v2_encoded", "=", "torch", ".", "stack", "(", "v2_encoded", ",", "dim", "=", "0", ")", "\n", "concatview", "=", "torch", ".", "cat", "(", "[", "v1_encoded", ",", "v2_encoded", "]", ",", "dim", "=", "-", "1", ")", "\n", "print", "(", "'concatview.size()'", ",", "concatview", ".", "size", "(", ")", ")", "\n", "encoded", "=", "concatview", ".", "numpy", "(", ")", "\n", "preds", "=", "kmeans", ".", "fit_predict", "(", "encoded", ")", "\n", "", "elif", "rep", "==", "'wholeconv'", ":", "\n", "            ", "encoded", "=", "[", "]", "\n", "for", "idx", "in", "dataset", ".", "trn_idx", ":", "\n", "                ", "v1", "=", "dataset", "[", "idx", "]", "[", "0", "]", "[", "0", "]", "\n", "v2", "=", "dataset", "[", "idx", "]", "[", "0", "]", "[", "1", "]", "\n", "conv", "=", "[", "v1", "]", "+", "v2", "\n", "encoded_conv", "=", "transform", "(", "data", "=", "conv", ",", "model", "=", "model", ")", "\n", "encoded_conv", "=", "torch", ".", "from_numpy", "(", "encoded_conv", ")", "\n", "encoded_conv", "=", "encoded_conv", ".", "mean", "(", "dim", "=", "0", ")", "\n", "encoded", ".", "append", "(", "encoded_conv", ")", "\n", "", "encoded", "=", "torch", ".", "stack", "(", "encoded", ",", "dim", "=", "0", ")", "\n", "print", "(", "'encoded.size()'", ",", "encoded", ".", "size", "(", ")", ")", "\n", "encoded", "=", "encoded", ".", "numpy", "(", ")", "\n", "preds", "=", "kmeans", ".", "fit_predict", "(", "encoded", ")", "\n", "", "elif", "rep", "==", "'mvsc'", ":", "\n", "            ", "try", ":", "\n", "                ", "import", "multiview", "\n", "", "except", "Exception", ":", "\n", "                ", "print", "(", "'please install https://github.com/mariceli3/multiview'", ")", "\n", "return", "\n", "", "print", "(", "'imported multiview ok'", ")", "\n", "\n", "idx", "=", "dataset", ".", "trn_idx_no_unk", "if", "args", ".", "mvsc_no_unk", "else", "dataset", ".", "trn_idx", "\n", "v1_data", "=", "[", "dataset", "[", "idx", "]", "[", "0", "]", "[", "0", "]", "for", "idx", "in", "idx", "]", "\n", "v1_encoded", "=", "torch", ".", "from_numpy", "(", "transform", "(", "data", "=", "v1_data", ",", "model", "=", "model", ")", ")", "\n", "\n", "v2_data", "=", "[", "dataset", "[", "idx", "]", "[", "0", "]", "[", "1", "]", "for", "idx", "in", "idx", "]", "\n", "v2_encoded", "=", "[", "]", "\n", "for", "conv", "in", "v2_data", ":", "\n", "                ", "encoded_conv", "=", "transform", "(", "data", "=", "conv", ",", "model", "=", "model", ")", "\n", "encoded_conv", "=", "torch", ".", "from_numpy", "(", "encoded_conv", ")", "\n", "encoded_conv", "=", "encoded_conv", ".", "mean", "(", "dim", "=", "0", ")", "\n", "v2_encoded", ".", "append", "(", "encoded_conv", ")", "\n", "", "v2_encoded", "=", "torch", ".", "stack", "(", "v2_encoded", ",", "dim", "=", "0", ")", "\n", "\n", "mvsc", "=", "multiview", ".", "mvsc", ".", "MVSC", "(", "\n", "k", "=", "n_cluster", "\n", ")", "\n", "print", "(", "'running mvsc'", ",", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "preds", ",", "eivalues", ",", "eivectors", ",", "sigmas", "=", "mvsc", ".", "fit_transform", "(", "\n", "[", "v1_encoded", ",", "v2_encoded", "]", ",", "[", "False", "]", "*", "2", "\n", ")", "\n", "print", "(", "'...done'", ")", "\n", "mvsc_time", "=", "time", ".", "time", "(", ")", "-", "start", "\n", "print", "(", "'time taken %.3f'", "%", "mvsc_time", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "'unimplemented rep'", ",", "rep", ")", "\n", "\n", "", "prec", ",", "rec", ",", "f1", ",", "acc", "=", "calc_prec_rec_f1_acc", "(", "preds", ",", "golds", ")", "\n", "print", "(", "f'{datetime.datetime.now()} {rep}: eval prec={prec:.4f} rec={rec:.4f} f1={f1:.4f} '", "\n", "f'acc={acc:.4f}'", ")", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.samplers.CategoriesSampler.__init__": [[7, 40], ["list", "numpy.array", "isinstance", "labels.tolist.tolist.tolist", "set", "numpy.argwhere().reshape", "numpy.argwhere"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "labels", ",", "n_batch", ",", "n_cls", ",", "n_ins", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            labels: size=(dataset_size), label indices of instances in a data set\n            n_batch: int, number of batchs for episode training\n            n_cls: int, number of sampled classes\n            n_ins: int, number of instances considered for a class\n\n        conceptually, this is for prototypical sampling:\n        - for each training step, we sample 'n_ways' classes (in the paper), which is 'n_cls' here\n        - we draw 'n_shot' examples of each class, ie n_ins here\n            - these will be encoded, and averaged, to get the prototypes\n        - and we draw 'n_query' query examples, of each class, which is also 'n_ins' here\n            - these will be encoded, and then used to generate the prototype loss, relative to the\n              prototypes\n\n        __iter__ returns a generator, which will yield n_batch sets of training data, one set of\n                 training\n        data per yield command\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "labels", ",", "list", ")", ":", "\n", "            ", "labels", "=", "labels", ".", "tolist", "(", ")", "\n", "\n", "", "self", ".", "n_batch", "=", "n_batch", "\n", "self", ".", "n_cls", "=", "n_cls", "\n", "self", ".", "n_ins", "=", "n_ins", "\n", "\n", "self", ".", "classes", "=", "list", "(", "set", "(", "labels", ")", ")", "\n", "labels", "=", "np", ".", "array", "(", "labels", ")", "\n", "self", ".", "cls_indices", "=", "{", "}", "\n", "for", "c", "in", "self", ".", "classes", ":", "\n", "            ", "indices", "=", "np", ".", "argwhere", "(", "labels", "==", "c", ")", ".", "reshape", "(", "-", "1", ")", "\n", "self", ".", "cls_indices", "[", "c", "]", "=", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.samplers.CategoriesSampler.__len__": [[41, 43], ["None"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "n_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.samplers.CategoriesSampler.__iter__": [[44, 55], ["range", "numpy.stack().flatten", "numpy.random.permutation", "numpy.stack().flatten.append", "len", "numpy.concatenate", "numpy.stack", "numpy.random.permutation"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "_", "in", "range", "(", "self", ".", "n_batch", ")", ":", "\n", "            ", "batch", "=", "[", "]", "\n", "classes", "=", "np", ".", "random", ".", "permutation", "(", "self", ".", "classes", ")", "[", ":", "self", ".", "n_cls", "]", "\n", "for", "c", "in", "classes", ":", "\n", "                ", "indices", "=", "self", ".", "cls_indices", "[", "c", "]", "\n", "while", "len", "(", "indices", ")", "<", "self", ".", "n_ins", ":", "\n", "                    ", "indices", "=", "np", ".", "concatenate", "(", "(", "indices", ",", "indices", ")", ")", "\n", "", "batch", ".", "append", "(", "np", ".", "random", ".", "permutation", "(", "indices", ")", "[", ":", "self", ".", "n_ins", "]", ")", "\n", "", "batch", "=", "np", ".", "stack", "(", "batch", ")", ".", "flatten", "(", "'F'", ")", "\n", "yield", "batch", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.run_mvsc.transform": [[34, 56], ["model.eval", "range", "numpy.concatenate", "list", "np.concatenate.append", "zip", "model", "torch.stack.cpu().data.numpy", "len", "torch.stack", "model().mean", "torch.stack.cpu", "model"], "function", ["None"], ["def", "transform", "(", "dataset", ",", "perm_idx", ",", "model", ",", "view", ")", ":", "\n", "    ", "\"\"\"\n    for view1 utterance, simply encode using view1 encoder\n    for view 2 utterances:\n    - encode each utterance, using view 1 encoder, to get utterance embeddings\n    - take average of utterance embeddings to form view 2 embedding\n    \"\"\"", "\n", "model", ".", "eval", "(", ")", "\n", "latent_zs", ",", "golds", "=", "[", "]", ",", "[", "]", "\n", "n_batch", "=", "(", "len", "(", "perm_idx", ")", "+", "BATCH_SIZE", "-", "1", ")", "//", "BATCH_SIZE", "\n", "for", "i", "in", "range", "(", "n_batch", ")", ":", "\n", "        ", "indices", "=", "perm_idx", "[", "i", "*", "BATCH_SIZE", ":", "(", "i", "+", "1", ")", "*", "BATCH_SIZE", "]", "\n", "v1_batch", ",", "v2_batch", "=", "list", "(", "zip", "(", "*", "[", "dataset", "[", "idx", "]", "[", "0", "]", "for", "idx", "in", "indices", "]", ")", ")", "\n", "golds", "+=", "[", "dataset", "[", "idx", "]", "[", "1", "]", "for", "idx", "in", "indices", "]", "\n", "if", "view", "==", "'v1'", ":", "\n", "            ", "latent_z", "=", "model", "(", "v1_batch", ",", "encoder", "=", "'v1'", ")", "\n", "", "elif", "view", "==", "'v2'", ":", "\n", "            ", "latent_z_l", "=", "[", "model", "(", "conv", ",", "encoder", "=", "'v1'", ")", ".", "mean", "(", "dim", "=", "0", ")", "for", "conv", "in", "v2_batch", "]", "\n", "latent_z", "=", "torch", ".", "stack", "(", "latent_z_l", ")", "\n", "", "latent_zs", ".", "append", "(", "latent_z", ".", "cpu", "(", ")", ".", "data", ".", "numpy", "(", ")", ")", "\n", "", "latent_zs", "=", "np", ".", "concatenate", "(", "latent_zs", ")", "\n", "return", "latent_zs", ",", "golds", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.run_mvsc.run": [[58, 112], ["torch.manual_seed", "numpy.random.seed", "random.seed", "model.multiview_encoders.load_model", "print", "print", "proc_data.Dataset", "print", "print", "multiview.mvsc.MVSC", "run_mvsc.transform", "run_mvsc.transform", "print", "time.time", "multiview.mvsc.MVSC.fit_transform", "print", "print", "zip", "metrics.cluster_metrics.calc_prec_rec_f1", "metrics.cluster_metrics.calc_ACC", "sklearn.metrics.silhouette_score", "sklearn.metrics.davies_bouldin_score", "print", "len", "x.item", "time.time", "list", "torch.LongTensor().to", "torch.LongTensor().to", "numpy.random.permutation", "lgolds.append", "lpreds.append", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor", "torch.LongTensor", "datetime.datetime.now", "torch.LongTensor", "torch.LongTensor"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.load_model", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.transform", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.transform", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_prec_rec_f1", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_ACC"], ["", "def", "run", "(", "\n", "ref", ",", "model_path", ",", "num_clusters", ",", "num_cluster_samples", ",", "seed", ",", "\n", "out_cluster_samples_file_hier", ",", "\n", "max_examples", ",", "out_cluster_samples_file", ",", "\n", "data_path", ",", "view1_col", ",", "view2_col", ",", "label_col", ",", "\n", "sampling_strategy", ",", "mvsc_no_unk", ")", ":", "\n", "    ", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "id_to_token", ",", "token_to_id", ",", "vocab_size", ",", "word_emb_size", ",", "mvc_encoder", "=", "multiview_encoders", ".", "load_model", "(", "model_path", ")", "\n", "print", "(", "'loaded model'", ")", "\n", "\n", "print", "(", "'loading dataset'", ")", "\n", "dataset", "=", "Dataset", "(", "data_path", ",", "view1_col", "=", "view1_col", ",", "view2_col", "=", "view2_col", ",", "label_col", "=", "label_col", ")", "\n", "n_cluster", "=", "len", "(", "dataset", ".", "id_to_label", ")", "-", "1", "\n", "print", "(", "\"loaded dataset, num of class = %d\"", "%", "n_cluster", ")", "\n", "\n", "idxes", "=", "dataset", ".", "trn_idx_no_unk", "if", "mvsc_no_unk", "else", "dataset", ".", "trn_idx", "\n", "trn_idx", "=", "[", "x", ".", "item", "(", ")", "for", "x", "in", "np", ".", "random", ".", "permutation", "(", "idxes", ")", "]", "\n", "if", "max_examples", "is", "not", "None", ":", "\n", "        ", "trn_idx", "=", "trn_idx", "[", ":", "max_examples", "]", "\n", "\n", "", "num_clusters", "=", "n_cluster", "if", "num_clusters", "is", "None", "else", "num_clusters", "\n", "print", "(", "'clustering over num clusters'", ",", "num_clusters", ")", "\n", "\n", "mvsc", "=", "multiview", ".", "mvsc", ".", "MVSC", "(", "\n", "k", "=", "n_cluster", "\n", ")", "\n", "latent_z1s", ",", "golds", "=", "transform", "(", "dataset", ",", "trn_idx", ",", "mvc_encoder", ",", "view", "=", "'v1'", ")", "\n", "latent_z2s", ",", "_", "=", "transform", "(", "dataset", ",", "trn_idx", ",", "mvc_encoder", ",", "view", "=", "'v2'", ")", "\n", "print", "(", "'running mvsc'", ",", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "preds", ",", "eivalues", ",", "eivectors", ",", "sigmas", "=", "mvsc", ".", "fit_transform", "(", "\n", "[", "latent_z1s", ",", "latent_z2s", "]", ",", "[", "False", "]", "*", "2", "\n", ")", "\n", "print", "(", "'...done'", ")", "\n", "mvsc_time", "=", "time", ".", "time", "(", ")", "-", "start", "\n", "print", "(", "'time taken %.3f'", "%", "mvsc_time", ")", "\n", "\n", "lgolds", ",", "lpreds", "=", "[", "]", ",", "[", "]", "\n", "for", "g", ",", "p", "in", "zip", "(", "golds", ",", "list", "(", "preds", ")", ")", ":", "\n", "        ", "if", "g", ">", "0", ":", "\n", "            ", "lgolds", ".", "append", "(", "g", ")", "\n", "lpreds", ".", "append", "(", "p", ")", "\n", "", "", "prec", ",", "rec", ",", "f1", "=", "cluster_metrics", ".", "calc_prec_rec_f1", "(", "\n", "gnd_assignments", "=", "torch", ".", "LongTensor", "(", "lgolds", ")", ".", "to", "(", "device", ")", ",", "\n", "pred_assignments", "=", "torch", ".", "LongTensor", "(", "lpreds", ")", ".", "to", "(", "device", ")", ")", "\n", "acc", "=", "cluster_metrics", ".", "calc_ACC", "(", "\n", "torch", ".", "LongTensor", "(", "lpreds", ")", ".", "to", "(", "device", ")", ",", "torch", ".", "LongTensor", "(", "lgolds", ")", ".", "to", "(", "device", ")", ")", "\n", "silhouette", "=", "sklearn", ".", "metrics", ".", "silhouette_score", "(", "latent_z1s", ",", "preds", ",", "metric", "=", "'euclidean'", ")", "\n", "davies_bouldin", "=", "sklearn", ".", "metrics", ".", "davies_bouldin_score", "(", "latent_z1s", ",", "preds", ")", "\n", "print", "(", "f'{datetime.datetime.now()} pretrain: eval prec={prec:.4f} rec={rec:.4f} f1={f1:.4f} '", "\n", "f'acc={acc:.4f} sil={silhouette:.4f}, db={davies_bouldin:.4f}'", ")", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.do_pass": [[34, 56], ["model.train", "enumerate", "F.cross_entropy.item", "model", "proto.reshape().mean.reshape().mean", "torch.arange().repeat", "torch.arange().repeat", "label.type().to.type().to", "model.utils.euclidean_metric", "torch.cross_entropy", "optimizer.zero_grad", "F.cross_entropy.backward", "optimizer.step", "model", "proto.reshape().mean.reshape", "torch.arange", "torch.arange", "label.type().to.type"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.utils.euclidean_metric"], ["def", "do_pass", "(", "batches", ",", "shot", ",", "way", ",", "query", ",", "expressions", ",", "encoder", ")", ":", "\n", "    ", "model", ",", "optimizer", "=", "expressions", "\n", "model", ".", "train", "(", ")", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "batches", ",", "1", ")", ":", "\n", "        ", "data", "=", "[", "x", "[", "{", "'v1'", ":", "0", ",", "'v2'", ":", "1", "}", "[", "encoder", "]", "]", "for", "x", ",", "_", "in", "batch", "]", "\n", "p", "=", "shot", "*", "way", "\n", "data_shot", ",", "data_query", "=", "data", "[", ":", "p", "]", ",", "data", "[", "p", ":", "]", "\n", "\n", "proto", "=", "model", "(", "data_shot", ",", "encoder", "=", "encoder", ")", "\n", "proto", "=", "proto", ".", "reshape", "(", "shot", ",", "way", ",", "-", "1", ")", ".", "mean", "(", "dim", "=", "0", ")", "\n", "\n", "# ignore original labels, reassign labels from 0", "\n", "label", "=", "torch", ".", "arange", "(", "way", ")", ".", "repeat", "(", "query", ")", "\n", "label", "=", "label", ".", "type", "(", "torch", ".", "LongTensor", ")", ".", "to", "(", "device", ")", "\n", "\n", "logits", "=", "utils", ".", "euclidean_metric", "(", "model", "(", "data_query", ",", "encoder", "=", "encoder", ")", ",", "proto", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "logits", ",", "label", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "", "return", "loss", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.transform": [[58, 70], ["model.eval", "range", "numpy.concatenate", "list", "model", "np.concatenate.append", "zip", "model.cpu().data.numpy", "len", "model.cpu"], "function", ["None"], ["", "def", "transform", "(", "dataset", ",", "perm_idx", ",", "model", ",", "encoder", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "latent_zs", ",", "golds", "=", "[", "]", ",", "[", "]", "\n", "n_batch", "=", "(", "len", "(", "perm_idx", ")", "+", "BATCH_SIZE", "-", "1", ")", "//", "BATCH_SIZE", "\n", "for", "i", "in", "range", "(", "n_batch", ")", ":", "\n", "        ", "indices", "=", "perm_idx", "[", "i", "*", "BATCH_SIZE", ":", "(", "i", "+", "1", ")", "*", "BATCH_SIZE", "]", "\n", "v1_batch", ",", "v2_batch", "=", "list", "(", "zip", "(", "*", "[", "dataset", "[", "idx", "]", "[", "0", "]", "for", "idx", "in", "indices", "]", ")", ")", "\n", "golds", "+=", "[", "dataset", "[", "idx", "]", "[", "1", "]", "for", "idx", "in", "indices", "]", "\n", "latent_z", "=", "model", "(", "{", "'v1'", ":", "v1_batch", ",", "'v2'", ":", "v2_batch", "}", "[", "encoder", "]", ",", "encoder", "=", "encoder", ")", "\n", "latent_zs", ".", "append", "(", "latent_z", ".", "cpu", "(", ")", ".", "data", ".", "numpy", "(", ")", ")", "\n", "", "latent_zs", "=", "np", ".", "concatenate", "(", "latent_zs", ")", "\n", "return", "latent_zs", ",", "golds", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.calc_centroids": [[72, 79], ["range", "numpy.stack", "numpy.mean", "centroids.append", "numpy.where"], "function", ["None"], ["", "def", "calc_centroids", "(", "latent_zs", ",", "assignments", ",", "n_cluster", ")", ":", "\n", "    ", "centroids", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "n_cluster", ")", ":", "\n", "        ", "idx", "=", "np", ".", "where", "(", "assignments", "==", "i", ")", "[", "0", "]", "\n", "mean", "=", "np", ".", "mean", "(", "latent_zs", "[", "idx", "]", ",", "0", ")", "\n", "centroids", ".", "append", "(", "mean", ")", "\n", "", "return", "np", ".", "stack", "(", "centroids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.run_one_side": [[81, 102], ["samplers.CategoriesSampler", "train.do_pass", "train.transform", "train.calc_centroids", "sklearn.cluster.KMeans", "sklearn.cluster.KMeans", "sklearn.cluster.KMeans.fit_predict", "train.transform", "sklearn.cluster.KMeans.predict"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.do_pass", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.transform", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.calc_centroids", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.transform"], ["", "def", "run_one_side", "(", "model", ",", "optimizer", ",", "preds_left", ",", "pt_batch", ",", "way", ",", "shot", ",", "query", ",", "n_cluster", ",", "dataset", ",", "\n", "right_encoder_side", ")", ":", "\n", "    ", "\"\"\"\n    encoder_side should be 'v1' or 'v2'. It should match whichever view is 'right' here.\n    \"\"\"", "\n", "loss", "=", "0", "\n", "\n", "sampler", "=", "CategoriesSampler", "(", "preds_left", ",", "pt_batch", ",", "way", ",", "shot", "+", "query", ")", "\n", "train_batches", "=", "[", "[", "dataset", "[", "dataset", ".", "trn_idx", "[", "idx", "]", "]", "for", "idx", "in", "indices", "]", "for", "indices", "in", "sampler", "]", "\n", "loss", "+=", "do_pass", "(", "train_batches", ",", "shot", ",", "way", ",", "query", ",", "(", "model", ",", "optimizer", ")", ",", "encoder", "=", "right_encoder_side", ")", "\n", "\n", "z_right", ",", "_", "=", "transform", "(", "dataset", ",", "dataset", ".", "trn_idx", ",", "model", ",", "encoder", "=", "right_encoder_side", ")", "\n", "centroids", "=", "calc_centroids", "(", "z_right", ",", "preds_left", ",", "n_cluster", ")", "\n", "kmeans", "=", "sklearn", ".", "cluster", ".", "KMeans", "(", "\n", "n_clusters", "=", "n_cluster", ",", "init", "=", "centroids", ",", "max_iter", "=", "10", ",", "verbose", "=", "0", ")", "\n", "preds_right", "=", "kmeans", ".", "fit_predict", "(", "z_right", ")", "\n", "\n", "tst_z_right", ",", "_", "=", "transform", "(", "dataset", ",", "dataset", ".", "tst_idx", ",", "model", ",", "encoder", "=", "right_encoder_side", ")", "\n", "tst_preds_right", "=", "kmeans", ".", "predict", "(", "tst_z_right", ")", "\n", "\n", "return", "loss", ",", "preds_right", ",", "tst_preds_right", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.main": [[104, 276], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "numpy.random.seed", "print", "proc_data.Dataset", "print", "print", "torch.optim.Adam", "torch.optim.Adam", "range", "sklearn.cluster.KMeans", "sklearn.cluster.KMeans", "train.transform", "sklearn.cluster.KMeans.fit_predict", "zip", "metrics.cluster_metrics.calc_prec_rec_f1", "metrics.cluster_metrics.calc_ACC", "print", "range", "print", "model.load_state_dict", "zip", "metrics.cluster_metrics.calc_prec_rec_f1", "metrics.cluster_metrics.calc_ACC", "print", "len", "model.multiview_encoders.load_model", "print", "model.multiview_encoders.from_embeddings", "print", "model.parameters", "model.train", "numpy.random.permutation", "pretrain_method", "model.eval", "pretrain_method", "print", "model.load_state_dict", "print", "torch.optim.Adam", "torch.optim.Adam", "print", "list", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "train.run_one_side", "train.run_one_side", "metrics.cluster_metrics.calc_f1", "metrics.cluster_metrics.calc_ACC", "print", "zip", "metrics.cluster_metrics.calc_prec_rec_f1", "metrics.cluster_metrics.calc_ACC", "print", "list", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.from_numpy", "torch.from_numpy", "print", "copy.deepcopy", "model.parameters", "lgolds.append", "lpreds.append", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "print", "copy.deepcopy", "torch.from_numpy.copy", "torch.from_numpy.copy", "list", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "lgolds.append", "lpreds.append", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.from_numpy", "torch.from_numpy", "model.state_dict", "open", "torch.save", "torch.save", "model.state_dict", "datetime.datetime.now", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "datetime.datetime.now", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "model.state_dict", "lgolds.append", "lpreds.append", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "datetime.datetime.now", "os.path.expanduser", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "datetime.datetime.now", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.transform", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_prec_rec_f1", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_ACC", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_prec_rec_f1", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_ACC", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.load_model", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.from_embeddings", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.run_one_side", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train.run_one_side", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_f1", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_ACC", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_prec_rec_f1", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_ACC"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--data-path'", ",", "type", "=", "str", ",", "default", "=", "'./data/airlines_processed.csv'", ")", "\n", "parser", ".", "add_argument", "(", "'--glove-path'", ",", "type", "=", "str", ",", "default", "=", "'./data/glove.840B.300d.txt'", ")", "\n", "parser", ".", "add_argument", "(", "'--pre-model'", ",", "type", "=", "str", ",", "choices", "=", "[", "'ae'", ",", "'qt'", "]", ",", "default", "=", "'qt'", ")", "\n", "parser", ".", "add_argument", "(", "'--pre-epoch'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--pt-batch'", ",", "type", "=", "int", ",", "default", "=", "100", ")", "\n", "parser", ".", "add_argument", "(", "'--model-path'", ",", "type", "=", "str", ",", "help", "=", "'path of pretrained model to load'", ")", "\n", "parser", ".", "add_argument", "(", "'--way'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "'--num-epochs'", ",", "type", "=", "int", ",", "default", "=", "100", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--save-model-path'", ",", "type", "=", "str", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--view1-col'", ",", "type", "=", "str", ",", "default", "=", "'view1'", ")", "\n", "parser", ".", "add_argument", "(", "'--view2-col'", ",", "type", "=", "str", ",", "default", "=", "'view2'", ")", "\n", "parser", ".", "add_argument", "(", "'--label-col'", ",", "type", "=", "str", ",", "default", "=", "'label'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "\n", "print", "(", "'loading dataset'", ")", "\n", "dataset", "=", "Dataset", "(", "\n", "args", ".", "data_path", ",", "view1_col", "=", "args", ".", "view1_col", ",", "view2_col", "=", "args", ".", "view2_col", ",", "\n", "label_col", "=", "args", ".", "label_col", ")", "\n", "n_cluster", "=", "len", "(", "dataset", ".", "id_to_label", ")", "-", "1", "\n", "print", "(", "\"num of class = %d\"", "%", "n_cluster", ")", "\n", "\n", "if", "args", ".", "model_path", "is", "not", "None", ":", "\n", "        ", "id_to_token", ",", "token_to_id", ",", "vocab_size", ",", "word_emb_size", ",", "model", "=", "multiview_encoders", ".", "load_model", "(", "\n", "args", ".", "model_path", ")", "\n", "print", "(", "'loaded model'", ")", "\n", "", "else", ":", "\n", "        ", "id_to_token", ",", "token_to_id", ",", "vocab_size", ",", "word_emb_size", ",", "model", "=", "multiview_encoders", ".", "from_embeddings", "(", "\n", "args", ".", "glove_path", ",", "dataset", ".", "id_to_token", ",", "dataset", ".", "token_to_id", ")", "\n", "print", "(", "'created randomly initialized model'", ")", "\n", "", "print", "(", "'vocab_size'", ",", "vocab_size", ")", "\n", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "LEARNING_RATE", ")", "\n", "expressions", "=", "(", "model", ",", "optimizer", ")", "\n", "\n", "pre_acc", ",", "pre_state", ",", "pre_state_epoch", "=", "0.", ",", "None", ",", "None", "\n", "pretrain_method", "=", "{", "\n", "'ae'", ":", "pretrain", ".", "pretrain_ae", ",", "\n", "'qt'", ":", "pretrain", ".", "pretrain_qt", ",", "\n", "}", "[", "args", ".", "pre_model", "]", "\n", "for", "epoch", "in", "range", "(", "1", ",", "args", ".", "pre_epoch", "+", "1", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "perm_idx", "=", "np", ".", "random", ".", "permutation", "(", "dataset", ".", "trn_idx", ")", "\n", "trn_loss", ",", "_", "=", "pretrain_method", "(", "dataset", ",", "perm_idx", ",", "expressions", ",", "train", "=", "True", ")", "\n", "model", ".", "eval", "(", ")", "\n", "_", ",", "tst_acc", "=", "pretrain_method", "(", "dataset", ",", "dataset", ".", "tst_idx", ",", "expressions", ",", "train", "=", "False", ")", "\n", "if", "tst_acc", ">", "pre_acc", ":", "\n", "            ", "pre_state", "=", "copy", ".", "deepcopy", "(", "model", ".", "state_dict", "(", ")", ")", "\n", "pre_acc", "=", "tst_acc", "\n", "pre_state_epoch", "=", "epoch", "\n", "", "print", "(", "'{} epoch {}, train_loss={:.4f} test_acc={:.4f}'", ".", "format", "(", "\n", "datetime", ".", "datetime", ".", "now", "(", ")", ",", "epoch", ",", "trn_loss", ",", "tst_acc", ")", ")", "\n", "\n", "", "if", "args", ".", "pre_epoch", ">", "0", ":", "\n", "# load best state", "\n", "        ", "model", ".", "load_state_dict", "(", "pre_state", ")", "\n", "print", "(", "f'loaded best state from epoch {pre_state_epoch}'", ")", "\n", "\n", "# deepcopy pretrained views into v1 and/or view2", "\n", "{", "\n", "'ae'", ":", "pretrain", ".", "after_pretrain_ae", ",", "\n", "'qt'", ":", "pretrain", ".", "after_pretrain_qt", ",", "\n", "}", "[", "args", ".", "pre_model", "]", "(", "model", ")", "\n", "\n", "# reinitialiate optimizer", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "LEARNING_RATE", ")", "\n", "expressions", "=", "(", "model", ",", "optimizer", ")", "\n", "print", "(", "'applied post-pretraining'", ")", "\n", "\n", "", "kmeans", "=", "sklearn", ".", "cluster", ".", "KMeans", "(", "n_clusters", "=", "n_cluster", ",", "max_iter", "=", "300", ",", "verbose", "=", "0", ",", "random_state", "=", "0", ")", "\n", "z_v1", ",", "golds", "=", "transform", "(", "dataset", ",", "dataset", ".", "trn_idx", ",", "model", ",", "encoder", "=", "'v1'", ")", "\n", "preds_v1", "=", "kmeans", ".", "fit_predict", "(", "z_v1", ")", "\n", "\n", "lgolds", ",", "lpreds", "=", "[", "]", ",", "[", "]", "\n", "for", "g", ",", "p", "in", "zip", "(", "golds", ",", "list", "(", "preds_v1", ")", ")", ":", "\n", "        ", "if", "g", ">", "0", ":", "\n", "            ", "lgolds", ".", "append", "(", "g", ")", "\n", "lpreds", ".", "append", "(", "p", ")", "\n", "", "", "prec", ",", "rec", ",", "f1", "=", "cluster_metrics", ".", "calc_prec_rec_f1", "(", "\n", "gnd_assignments", "=", "torch", ".", "LongTensor", "(", "lgolds", ")", ".", "to", "(", "device", ")", ",", "\n", "pred_assignments", "=", "torch", ".", "LongTensor", "(", "lpreds", ")", ".", "to", "(", "device", ")", ")", "\n", "acc", "=", "cluster_metrics", ".", "calc_ACC", "(", "\n", "torch", ".", "LongTensor", "(", "lpreds", ")", ".", "to", "(", "device", ")", ",", "torch", ".", "LongTensor", "(", "lgolds", ")", ".", "to", "(", "device", ")", ")", "\n", "\n", "print", "(", "f'{datetime.datetime.now()} pretrain: test prec={prec:.4f} rec={rec:.4f} '", "\n", "f'f1={f1:.4f} acc={acc:.4f}'", ")", "\n", "\n", "shot", ",", "way", ",", "query", "=", "5", ",", "args", ".", "way", ",", "15", "\n", "\n", "preds_v2", "=", "None", "\n", "best_epoch", ",", "best_model", ",", "best_dev_f1", "=", "None", ",", "None", ",", "None", "\n", "for", "epoch", "in", "range", "(", "1", ",", "args", ".", "num_epochs", "+", "1", ")", ":", "\n", "        ", "trn_loss", "=", "0.", "\n", "\n", "_loss", ",", "preds_v2", ",", "tst_preds_v2", "=", "run_one_side", "(", "\n", "model", "=", "model", ",", "optimizer", "=", "optimizer", ",", "preds_left", "=", "preds_v1", ",", "\n", "pt_batch", "=", "args", ".", "pt_batch", ",", "way", "=", "way", ",", "shot", "=", "shot", ",", "query", "=", "query", ",", "n_cluster", "=", "n_cluster", ",", "\n", "dataset", "=", "dataset", ",", "right_encoder_side", "=", "'v2'", ")", "\n", "trn_loss", "+=", "_loss", "\n", "\n", "_loss", ",", "preds_v1", ",", "tst_preds_v1", "=", "run_one_side", "(", "\n", "model", "=", "model", ",", "optimizer", "=", "optimizer", ",", "preds_left", "=", "preds_v2", ",", "\n", "pt_batch", "=", "args", ".", "pt_batch", ",", "way", "=", "way", ",", "shot", "=", "shot", ",", "query", "=", "query", ",", "n_cluster", "=", "n_cluster", ",", "\n", "dataset", "=", "dataset", ",", "right_encoder_side", "=", "'v1'", ")", "\n", "trn_loss", "+=", "_loss", "\n", "\n", "dev_f1", "=", "cluster_metrics", ".", "calc_f1", "(", "gnd_assignments", "=", "torch", ".", "LongTensor", "(", "tst_preds_v1", ")", ".", "to", "(", "device", ")", ",", "\n", "pred_assignments", "=", "torch", ".", "LongTensor", "(", "tst_preds_v2", ")", ".", "to", "(", "device", ")", ")", "\n", "dev_acc", "=", "cluster_metrics", ".", "calc_ACC", "(", "\n", "torch", ".", "LongTensor", "(", "tst_preds_v2", ")", ".", "to", "(", "device", ")", ",", "torch", ".", "LongTensor", "(", "tst_preds_v1", ")", ".", "to", "(", "device", ")", ")", "\n", "\n", "print", "(", "'dev view 1 vs view 2: f1={:.4f} acc={:.4f}'", ".", "format", "(", "dev_f1", ",", "dev_acc", ")", ")", "\n", "\n", "if", "best_dev_f1", "is", "None", "or", "dev_f1", ">", "best_dev_f1", ":", "\n", "            ", "print", "(", "'new best epoch'", ",", "epoch", ")", "\n", "best_epoch", "=", "epoch", "\n", "best_dev_f1", "=", "dev_f1", "\n", "best_model", "=", "copy", ".", "deepcopy", "(", "model", ".", "state_dict", "(", ")", ")", "\n", "best_preds_v1", "=", "preds_v1", ".", "copy", "(", ")", "\n", "best_preds_v2", "=", "preds_v2", ".", "copy", "(", ")", "\n", "\n", "", "lgolds", ",", "lpreds", "=", "[", "]", ",", "[", "]", "\n", "for", "g", ",", "p", "in", "zip", "(", "golds", ",", "list", "(", "preds_v1", ")", ")", ":", "\n", "            ", "if", "g", ">", "0", ":", "\n", "                ", "lgolds", ".", "append", "(", "g", ")", "\n", "lpreds", ".", "append", "(", "p", ")", "\n", "", "", "prec", ",", "rec", ",", "f1", "=", "cluster_metrics", ".", "calc_prec_rec_f1", "(", "\n", "gnd_assignments", "=", "torch", ".", "LongTensor", "(", "lgolds", ")", ".", "to", "(", "device", ")", ",", "\n", "pred_assignments", "=", "torch", ".", "LongTensor", "(", "lpreds", ")", ".", "to", "(", "device", ")", ")", "\n", "acc", "=", "cluster_metrics", ".", "calc_ACC", "(", "\n", "torch", ".", "LongTensor", "(", "lpreds", ")", ".", "to", "(", "device", ")", ",", "torch", ".", "LongTensor", "(", "lgolds", ")", ".", "to", "(", "device", ")", ")", "\n", "\n", "print", "(", "f'{datetime.datetime.now()} epoch {epoch}, test prec={prec:.4f} rec={rec:.4f} '", "\n", "f'f1={f1:.4f} acc={acc:.4f}'", ")", "\n", "\n", "", "print", "(", "'restoring model for best dev epoch'", ",", "best_epoch", ")", "\n", "model", ".", "load_state_dict", "(", "best_model", ")", "\n", "preds_v1", ",", "preds_v2", "=", "best_preds_v1", ",", "best_preds_v2", "\n", "\n", "lgolds", ",", "lpreds", "=", "[", "]", ",", "[", "]", "\n", "for", "g", ",", "p", "in", "zip", "(", "golds", ",", "list", "(", "preds_v1", ")", ")", ":", "\n", "        ", "if", "g", ">", "0", ":", "\n", "            ", "lgolds", ".", "append", "(", "g", ")", "\n", "lpreds", ".", "append", "(", "p", ")", "\n", "", "", "prec", ",", "rec", ",", "f1", "=", "cluster_metrics", ".", "calc_prec_rec_f1", "(", "\n", "gnd_assignments", "=", "torch", ".", "LongTensor", "(", "lgolds", ")", ".", "to", "(", "device", ")", ",", "\n", "pred_assignments", "=", "torch", ".", "LongTensor", "(", "lpreds", ")", ".", "to", "(", "device", ")", ")", "\n", "acc", "=", "cluster_metrics", ".", "calc_ACC", "(", "\n", "torch", ".", "LongTensor", "(", "lpreds", ")", ".", "to", "(", "device", ")", ",", "torch", ".", "LongTensor", "(", "lgolds", ")", ".", "to", "(", "device", ")", ")", "\n", "print", "(", "f'{datetime.datetime.now()} test prec={prec:.4f} rec={rec:.4f} f1={f1:.4f} acc={acc:.4f}'", ")", "\n", "\n", "if", "args", ".", "save_model_path", "is", "not", "None", ":", "\n", "        ", "preds_v1", "=", "torch", ".", "from_numpy", "(", "preds_v1", ")", "\n", "if", "preds_v2", "is", "not", "None", ":", "\n", "            ", "preds_v2", "=", "torch", ".", "from_numpy", "(", "preds_v2", ")", "\n", "", "state", "=", "{", "\n", "'model_state'", ":", "model", ".", "state_dict", "(", ")", ",", "\n", "'id_to_token'", ":", "dataset", ".", "id_to_token", ",", "\n", "'word_emb_size'", ":", "word_emb_size", ",", "\n", "'v1_assignments'", ":", "preds_v1", ",", "\n", "'v2_assignments'", ":", "preds_v2", "\n", "}", "\n", "with", "open", "(", "expand", "(", "args", ".", "save_model_path", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "torch", ".", "save", "(", "state", ",", "f", ")", "\n", "", "print", "(", "'saved model to '", ",", "args", ".", "save_model_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.pretrain.pretrain_qt": [[7, 50], ["numpy.random.permutation", "range", "enumerate", "list", "model", "model", "model.qt_loss", "loss.item", "utts.append", "zip", "len", "optimizer.zero_grad", "loss.backward", "optimizer.step", "len", "len", "np.random.permutation.append", "len", "ex[].item", "ex[].item"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.MultiviewEncoders.qt_loss"], ["def", "pretrain_qt", "(", "dataset", ",", "perm_idx", ",", "expressions", ",", "train", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    for each pair of utterances:\n    - encodes first utterance using 'v1' encoder\n    - encodes second utterance using 'qt_context' encoder\n    uses negative sampling loss between these two embeddings, relative to the\n    other second utterances in the batch\n    \"\"\"", "\n", "model", ",", "optimizer", "=", "expressions", "\n", "\n", "utts", "=", "[", "]", "\n", "qt_ex", "=", "[", "]", "\n", "for", "idx", "in", "perm_idx", ":", "\n", "        ", "v1", ",", "v2", "=", "dataset", "[", "idx", "]", "[", "0", "]", "\n", "conversation", "=", "[", "v1", "]", "+", "v2", "\n", "for", "n", ",", "utt", "in", "enumerate", "(", "conversation", ")", ":", "\n", "            ", "utts", ".", "append", "(", "utt", ")", "\n", "if", "n", ">", "0", ":", "\n", "                ", "num_utt", "=", "len", "(", "utts", ")", "\n", "ex", "=", "(", "num_utt", "-", "2", ",", "num_utt", "-", "1", ")", "\n", "qt_ex", ".", "append", "(", "ex", ")", "\n", "", "", "", "qt_ex", "=", "np", ".", "random", ".", "permutation", "(", "qt_ex", ")", "\n", "\n", "total_loss", ",", "total_acc", "=", "0.", ",", "0.", "\n", "n_batch", "=", "(", "len", "(", "qt_ex", ")", "+", "train_mod", ".", "BATCH_SIZE", "-", "1", ")", "//", "train_mod", ".", "BATCH_SIZE", "\n", "for", "i", "in", "range", "(", "n_batch", ")", ":", "\n", "        ", "qt_ex_batch", "=", "qt_ex", "[", "i", "*", "train_mod", ".", "BATCH_SIZE", ":", "(", "i", "+", "1", ")", "*", "train_mod", ".", "BATCH_SIZE", "]", "\n", "\n", "v1_idxes", ",", "v2_idxes", "=", "list", "(", "zip", "(", "*", "[", "(", "ex", "[", "0", "]", ".", "item", "(", ")", ",", "ex", "[", "1", "]", ".", "item", "(", ")", ")", "for", "ex", "in", "qt_ex_batch", "]", ")", ")", "\n", "v1_utts", "=", "[", "utts", "[", "idx", "]", "for", "idx", "in", "v1_idxes", "]", "\n", "v2_utts", "=", "[", "utts", "[", "idx", "]", "for", "idx", "in", "v2_idxes", "]", "\n", "\n", "v1_state", "=", "model", "(", "v1_utts", ",", "encoder", "=", "'v1'", ")", "\n", "v2_state", "=", "model", "(", "v2_utts", ",", "encoder", "=", "'qt'", ")", "\n", "\n", "loss", ",", "acc", "=", "model", ".", "qt_loss", "(", "v2_state", ",", "v1_state", ")", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "total_acc", "+=", "acc", "*", "len", "(", "qt_ex_batch", ")", "\n", "if", "train", ":", "\n", "            ", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "", "", "return", "total_loss", ",", "total_acc", "/", "len", "(", "qt_ex", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.pretrain.after_pretrain_qt": [[52, 54], ["copy.deepcopy"], "function", ["None"], ["", "def", "after_pretrain_qt", "(", "model", ")", ":", "\n", "    ", "model", ".", "view2_word_rnn", "=", "copy", ".", "deepcopy", "(", "model", ".", "view1_word_rnn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.pretrain.pretrain_ae": [[56, 88], ["numpy.random.permutation", "range", "model", "model.decode", "model.reconst_loss", "loss.item", "len", "len", "optimizer.zero_grad", "loss.backward", "optimizer.step", "len"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.MultiviewEncoders.decode", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.MultiviewEncoders.reconst_loss"], ["", "def", "pretrain_ae", "(", "dataset", ",", "perm_idx", ",", "expressions", ",", "train", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    uses v1 encoder to encode all utterances in both view1 and view2\n    to utterance-level embeddings\n    uses 'ae_decoder' rnn from model to decode these embeddings\n    (works at utterance level)\n    \"\"\"", "\n", "model", ",", "optimizer", "=", "expressions", "\n", "\n", "utterances", "=", "[", "]", "\n", "for", "idx", "in", "perm_idx", ":", "\n", "        ", "v1", ",", "v2", "=", "dataset", "[", "idx", "]", "[", "0", "]", "\n", "conversation", "=", "[", "v1", "]", "+", "v2", "\n", "utterances", "+=", "conversation", "\n", "", "utterances", "=", "np", ".", "random", ".", "permutation", "(", "utterances", ")", "\n", "\n", "total_loss", ",", "total_acc", "=", "0.", ",", "0.", "\n", "n_batch", "=", "(", "len", "(", "utterances", ")", "+", "train_mod", ".", "AE_BATCH_SIZE", "-", "1", ")", "//", "train_mod", ".", "AE_BATCH_SIZE", "\n", "for", "i", "in", "range", "(", "n_batch", ")", ":", "\n", "        ", "utt_batch", "=", "utterances", "[", "i", "*", "train_mod", ".", "AE_BATCH_SIZE", ":", "(", "i", "+", "1", ")", "*", "train_mod", ".", "AE_BATCH_SIZE", "]", "\n", "enc_state", "=", "model", "(", "utt_batch", ",", "encoder", "=", "'v1'", ")", "\n", "reconst", "=", "model", ".", "decode", "(", "decoder_input", "=", "utt_batch", ",", "latent_z", "=", "enc_state", ")", "\n", "loss", ",", "acc", "=", "model", ".", "reconst_loss", "(", "utt_batch", ",", "reconst", ")", "\n", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "total_acc", "+=", "acc", "*", "len", "(", "utt_batch", ")", "\n", "if", "train", ":", "\n", "            ", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "", "", "total_acc", "=", "total_acc", "/", "len", "(", "utterances", ")", "\n", "return", "total_loss", ",", "total_acc", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.pretrain.after_pretrain_ae": [[90, 93], ["copy.deepcopy"], "function", ["None"], ["", "def", "after_pretrain_ae", "(", "model", ")", ":", "\n", "# we'll use the view1 encoder for both view 1 and view 2", "\n", "    ", "model", ".", "view2_word_rnn", "=", "copy", ".", "deepcopy", "(", "model", ".", "view1_word_rnn", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.proc_data.Dataset.__init__": [[14, 89], ["open", "csv.DictReader", "token_idices.append", "v1_utts.append", "proc_data.Dataset.__init__.tokens_to_idices"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "fname", ",", "view1_col", "=", "'view1_col'", ",", "view2_col", "=", "'view2_col'", ",", "label_col", "=", "'cluster_id'", ",", "\n", "tokenized", "=", "True", ",", "max_sent", "=", "10", ",", "train_ratio", "=", ".9", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            fname: str, training data file\n            view1_col: str, the column corresponding to view 1 input\n            view2_col: str, the column corresponding to view 2 input\n            label_col: str, the column corresponding to label\n        \"\"\"", "\n", "\n", "def", "tokens_to_idices", "(", "tokens", ")", ":", "\n", "            ", "token_idices", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "                ", "if", "token", "not", "in", "token_to_id", ":", "\n", "                    ", "token_to_id", "[", "token", "]", "=", "len", "(", "token_to_id", ")", "\n", "id_to_token", ".", "append", "(", "token", ")", "\n", "", "token_idices", ".", "append", "(", "token_to_id", "[", "token", "]", ")", "\n", "", "return", "token_idices", "\n", "\n", "", "id_to_token", "=", "[", "PAD", ",", "UNK", ",", "START", ",", "END", "]", "\n", "token_to_id", "=", "{", "PAD", ":", "0", ",", "UNK", ":", "1", ",", "START", ":", "2", ",", "END", ":", "3", "}", "\n", "id_to_label", "=", "[", "UNK", "]", "\n", "label_to_id", "=", "{", "UNK", ":", "0", "}", "\n", "data", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "v1_utts", "=", "[", "]", "# needed for displaying cluster samples", "\n", "self", ".", "trn_idx", ",", "self", ".", "tst_idx", "=", "[", "]", ",", "[", "]", "\n", "self", ".", "trn_idx_no_unk", "=", "[", "]", "\n", "with", "open", "(", "fname", ",", "'r'", ")", "as", "csvfile", ":", "\n", "            ", "reader", "=", "csv", ".", "DictReader", "(", "csvfile", ")", "\n", "for", "row", "in", "reader", ":", "\n", "                ", "view1_text", ",", "view2_text", "=", "row", "[", "view1_col", "]", ",", "row", "[", "view2_col", "]", "\n", "label", "=", "row", "[", "label_col", "]", "\n", "if", "'UNK'", "==", "label", ":", "\n", "                    ", "label", "=", "UNK", "\n", "", "if", "'<cust_'", "not", "in", "view1_text", ":", "\n", "                    ", "view2_sents", "=", "sent_tokenize", "(", "view2_text", ".", "lower", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "view2_sents", "=", "view2_text", ".", "split", "(", "\"> <\"", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "view2_sents", ")", "-", "1", ")", ":", "\n", "                        ", "view2_sents", "[", "i", "]", "=", "view2_sents", "[", "i", "]", "+", "'>'", "\n", "view2_sents", "[", "i", "+", "1", "]", "=", "'<'", "+", "view2_sents", "[", "i", "+", "1", "]", "\n", "", "", "v1_utts", ".", "append", "(", "view1_text", ")", "\n", "if", "not", "tokenized", ":", "\n", "                    ", "v1_tokens", "=", "word_tokenize", "(", "view1_text", ".", "lower", "(", ")", ")", "\n", "v2_tokens", "=", "[", "word_tokenize", "(", "sent", ".", "lower", "(", ")", ")", "for", "sent", "in", "view2_sents", "]", "\n", "", "else", ":", "\n", "                    ", "v1_tokens", "=", "view1_text", ".", "lower", "(", ")", ".", "split", "(", ")", "\n", "v2_tokens", "=", "[", "sent", ".", "lower", "(", ")", ".", "split", "(", ")", "for", "sent", "in", "view2_sents", "]", "\n", "", "v2_tokens", "=", "v2_tokens", "[", ":", "max_sent", "]", "\n", "\n", "v1_token_idices", "=", "tokens_to_idices", "(", "v1_tokens", ")", "\n", "v2_token_idices", "=", "[", "tokens_to_idices", "(", "tokens", ")", "for", "tokens", "in", "v2_tokens", "]", "\n", "v2_token_idices", "=", "[", "idices", "for", "idices", "in", "v2_token_idices", "if", "len", "(", "idices", ")", ">", "0", "]", "\n", "if", "len", "(", "v1_token_idices", ")", "==", "0", "or", "len", "(", "v2_token_idices", ")", "==", "0", ":", "\n", "                    ", "continue", "\n", "", "if", "label", "not", "in", "label_to_id", ":", "\n", "                    ", "label_to_id", "[", "label", "]", "=", "len", "(", "label_to_id", ")", "\n", "id_to_label", ".", "append", "(", "label", ")", "\n", "", "data", ".", "append", "(", "(", "v1_token_idices", ",", "v2_token_idices", ")", ")", "\n", "labels", ".", "append", "(", "label_to_id", "[", "label", "]", ")", "\n", "if", "label", "==", "UNK", "and", "np", ".", "random", ".", "random_sample", "(", ")", "<", ".1", ":", "\n", "                    ", "self", ".", "tst_idx", ".", "append", "(", "len", "(", "data", ")", "-", "1", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "trn_idx", ".", "append", "(", "len", "(", "data", ")", "-", "1", ")", "\n", "if", "label", "!=", "UNK", ":", "\n", "                        ", "self", ".", "trn_idx_no_unk", ".", "append", "(", "len", "(", "data", ")", "-", "1", ")", "\n", "\n", "", "", "", "", "self", ".", "v1_utts", "=", "v1_utts", "\n", "self", ".", "id_to_token", "=", "id_to_token", "\n", "self", ".", "token_to_id", "=", "token_to_id", "\n", "self", ".", "id_to_label", "=", "id_to_label", "\n", "self", ".", "label_to_id", "=", "label_to_id", "\n", "self", ".", "data", "=", "data", "\n", "self", ".", "labels", "=", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.proc_data.Dataset.__len__": [[90, 92], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.proc_data.Dataset.__getitem__": [[93, 95], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "self", ".", "data", "[", "i", "]", ",", "self", ".", "labels", "[", "i", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.labeled_unlabeled_merger.get_col_by_role": [[10, 17], ["role_string.split", "print", "s.split"], "function", ["None"], ["def", "get_col_by_role", "(", "role_string", ")", ":", "\n", "    ", "col_by_role", "=", "{", "}", "\n", "for", "s", "in", "role_string", ".", "split", "(", "','", ")", ":", "\n", "        ", "split_s", "=", "s", ".", "split", "(", "'='", ")", "\n", "col_by_role", "[", "split_s", "[", "0", "]", "]", "=", "split_s", "[", "1", "]", "\n", "", "print", "(", "'col_by_role'", ",", "col_by_role", ")", "\n", "return", "col_by_role", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.labeled_unlabeled_merger.run": [[19, 71], ["add_columns.split.split", "open", "csv.DictWriter", "csv.DictWriter.writeheader", "labeled_unlabeled_merger.get_col_by_role", "labeled_unlabeled_merger.get_col_by_role", "collections.OrderedDict", "print", "collections.OrderedDict.values", "open.close", "print", "print", "csv.DictWriter.writerow", "column_order.split", "open", "csv.DictReader", "open", "csv.DictReader", "print", "view1.startswith"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.labeled_unlabeled_merger.get_col_by_role", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.labeled_unlabeled_merger.get_col_by_role"], ["", "def", "run", "(", "labeled_files", ",", "unlabeled_files", ",", "out_file", ",", "unlabeled_columns", ",", "labeled_columns", ",", "no_add_cust_tokens", ",", "add_columns", ",", "column_order", ")", ":", "\n", "    ", "add_columns", "=", "add_columns", ".", "split", "(", "','", ")", "\n", "out_f", "=", "open", "(", "out_file", ",", "'w'", ")", "\n", "dict_writer", "=", "csv", ".", "DictWriter", "(", "out_f", ",", "fieldnames", "=", "column_order", ".", "split", "(", "','", ")", ")", "\n", "dict_writer", ".", "writeheader", "(", ")", "\n", "\n", "labeled_col_by_role", "=", "get_col_by_role", "(", "labeled_columns", ")", "\n", "unlabeled_col_by_role", "=", "get_col_by_role", "(", "unlabeled_columns", ")", "\n", "\n", "labeled_view1", "=", "labeled_col_by_role", "[", "'view1'", "]", "\n", "labeled_label", "=", "labeled_col_by_role", "[", "'label'", "]", "\n", "\n", "unlabeled_view1", "=", "unlabeled_col_by_role", "[", "'view1'", "]", "\n", "unlabeled_view2", "=", "unlabeled_col_by_role", "[", "'view2'", "]", "\n", "\n", "unlabeled_by_first_utterance", "=", "OrderedDict", "(", ")", "\n", "for", "filename", "in", "unlabeled_files", ":", "\n", "        ", "print", "(", "filename", ")", "\n", "with", "open", "(", "filename", ",", "'r'", ")", "as", "in_f", ":", "\n", "            ", "dict_reader", "=", "csv", ".", "DictReader", "(", "in_f", ")", "\n", "for", "row", "in", "dict_reader", ":", "\n", "                ", "view1", "=", "row", "[", "unlabeled_view1", "]", "\n", "if", "not", "no_add_cust_tokens", "and", "not", "view1", ".", "startswith", "(", "'<cust__'", ")", ":", "\n", "                    ", "view1", "=", "'<cust__ '", "+", "view1", "+", "' __cust>'", "\n", "", "out_row", "=", "{", "\n", "'view1'", ":", "view1", ",", "\n", "'view2'", ":", "row", "[", "unlabeled_view2", "]", ",", "\n", "'label'", ":", "'UNK'", "\n", "}", "\n", "for", "k", "in", "add_columns", ":", "\n", "                    ", "out_row", "[", "k", "]", "=", "row", "[", "k", "]", "\n", "", "unlabeled_by_first_utterance", "[", "view1", "]", "=", "out_row", "\n", "", "", "", "print", "(", "'loaded unlabeled'", ")", "\n", "\n", "for", "filename", "in", "labeled_files", ":", "\n", "        ", "print", "(", "filename", ")", "\n", "with", "open", "(", "filename", ",", "'r'", ")", "as", "in_f", ":", "\n", "            ", "dict_reader", "=", "csv", ".", "DictReader", "(", "in_f", ")", "\n", "for", "row", "in", "dict_reader", ":", "\n", "                ", "view1", "=", "row", "[", "labeled_view1", "]", "\n", "if", "view1", "not", "in", "unlabeled_by_first_utterance", ":", "\n", "                    ", "print", "(", "'warning: not found in unlabelled'", ",", "view1", ")", "\n", "continue", "\n", "", "out_row", "=", "unlabeled_by_first_utterance", "[", "view1", "]", "\n", "out_row", "[", "'label'", "]", "=", "row", "[", "labeled_label", "]", "\n", "for", "k", "in", "add_columns", ":", "\n", "                    ", "assert", "out_row", "[", "k", "]", "==", "row", "[", "k", "]", "\n", "\n", "", "", "", "", "for", "row", "in", "unlabeled_by_first_utterance", ".", "values", "(", ")", ":", "\n", "        ", "dict_writer", ".", "writerow", "(", "row", ")", "\n", "\n", "", "out_f", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train_pca.main": [[30, 46], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "train_pca.run"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.labeled_unlabeled_merger.run"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--data-path'", ",", "type", "=", "str", ",", "default", "=", "'./data/airlines_processed.csv'", ")", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "type", "=", "str", ",", "choices", "=", "[", "\n", "'view1pca'", ",", "'view2pca'", ",", "'wholeconvpca'", ",", "'mvsc'", "]", ",", "default", "=", "'view1pca'", ")", "\n", "parser", ".", "add_argument", "(", "'--pca-dims'", ",", "type", "=", "int", ",", "default", "=", "600", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--no-idf'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--mvsc-no-unk'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'only feed non-unk data to MVSC (to avoid oom)'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--view1-col'", ",", "type", "=", "str", ",", "default", "=", "'view1'", ")", "\n", "parser", ".", "add_argument", "(", "'--view2-col'", ",", "type", "=", "str", ",", "default", "=", "'view2'", ")", "\n", "parser", ".", "add_argument", "(", "'--label-col'", ",", "type", "=", "str", ",", "default", "=", "'tag'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "run", "(", "**", "args", ".", "__dict__", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.None.train_pca.run": [[48, 140], ["print", "proc_data.Dataset", "print", "len", "print", "zip", "metrics.cluster_metrics.calc_prec_rec_f1", "metrics.cluster_metrics.calc_ACC", "print", "len", "print", "print", "sklearn.feature_extraction.text.TfidfVectorizer", "sklearn.feature_extraction.text.TfidfVectorizer.fit_transform", "print", "print", "print", "sklearn.decomposition.TruncatedSVD", "sklearn.decomposition.TruncatedSVD.fit_transform", "print", "train_pca.run.run_pca"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_prec_rec_f1", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_ACC"], ["", "def", "run", "(", "data_path", ",", "model", ",", "pca_dims", ",", "view1_col", ",", "view2_col", ",", "label_col", ",", "no_idf", ",", "mvsc_no_unk", ")", ":", "\n", "    ", "print", "(", "'loading dataset'", ")", "\n", "dataset", "=", "Dataset", "(", "data_path", ",", "view1_col", "=", "view1_col", ",", "view2_col", "=", "view2_col", ",", "label_col", "=", "label_col", ")", "\n", "n_cluster", "=", "len", "(", "dataset", ".", "id_to_label", ")", "-", "1", "\n", "print", "(", "\"num of class = %d\"", "%", "n_cluster", ")", "\n", "\n", "vocab_size", "=", "len", "(", "dataset", ".", "token_to_id", ")", "\n", "print", "(", "'vocab_size'", ",", "vocab_size", ")", "\n", "\n", "if", "model", "==", "'mvsc'", ":", "\n", "        ", "try", ":", "\n", "            ", "import", "multiview", "\n", "", "except", "Exception", ":", "\n", "            ", "print", "(", "'please install https://github.com/mariceli3/multiview'", ")", "\n", "return", "\n", "", "print", "(", "'imported multiview ok'", ")", "\n", "\n", "", "def", "run_pca", "(", "features", ")", ":", "\n", "        ", "print", "(", "'fitting tfidf vectorizer'", ",", "flush", "=", "True", ",", "end", "=", "''", ")", "\n", "vectorizer", "=", "TfidfVectorizer", "(", "token_pattern", "=", "'\\\\d+'", ",", "ngram_range", "=", "(", "1", ",", "1", ")", ",", "analyzer", "=", "'word'", ",", "\n", "min_df", "=", "0.0", ",", "max_df", "=", "1.0", ",", "use_idf", "=", "not", "no_idf", ")", "\n", "X", "=", "vectorizer", ".", "fit_transform", "(", "features", ")", "\n", "print", "(", "' ... done'", ")", "\n", "print", "(", "'X.shape'", ",", "X", ".", "shape", ")", "\n", "\n", "print", "(", "'running pca'", ",", "flush", "=", "True", ",", "end", "=", "''", ")", "\n", "pca", "=", "TruncatedSVD", "(", "n_components", "=", "pca_dims", ")", "\n", "X2", "=", "pca", ".", "fit_transform", "(", "X", ")", "\n", "print", "(", "' ... done'", ")", "\n", "return", "X2", "\n", "\n", "", "golds", "=", "[", "dataset", "[", "idx", "]", "[", "1", "]", "for", "idx", "in", "dataset", ".", "trn_idx", "]", "\n", "\n", "if", "model", "in", "[", "'view1pca'", ",", "'view2pca'", ",", "'wholeconvpca'", "]", ":", "\n", "        ", "if", "model", "==", "'view1pca'", ":", "\n", "            ", "utts", "=", "[", "dataset", "[", "idx", "]", "[", "0", "]", "[", "0", "]", "for", "idx", "in", "dataset", ".", "trn_idx", "]", "\n", "utts", "=", "[", "' '", ".", "join", "(", "[", "str", "(", "idx", ")", "for", "idx", "in", "utt", "]", ")", "for", "utt", "in", "utts", "]", "\n", "", "elif", "model", "==", "'view2pca'", ":", "\n", "            ", "convs", "=", "[", "dataset", "[", "idx", "]", "[", "0", "]", "[", "1", "]", "for", "idx", "in", "dataset", ".", "trn_idx", "]", "\n", "utts", "=", "[", "[", "tok", "for", "utt", "in", "conv", "for", "tok", "in", "utt", "]", "for", "conv", "in", "convs", "]", "\n", "utts", "=", "[", "' '", ".", "join", "(", "[", "str", "(", "idx", ")", "for", "idx", "in", "utt", "]", ")", "for", "utt", "in", "utts", "]", "\n", "", "elif", "model", "==", "'wholeconvpca'", ":", "\n", "            ", "v1", "=", "[", "dataset", "[", "idx", "]", "[", "0", "]", "[", "0", "]", "for", "idx", "in", "dataset", ".", "trn_idx", "]", "\n", "convs", "=", "[", "dataset", "[", "idx", "]", "[", "0", "]", "[", "1", "]", "for", "idx", "in", "dataset", ".", "trn_idx", "]", "\n", "v2", "=", "[", "[", "tok", "for", "utt", "in", "conv", "for", "tok", "in", "utt", "]", "for", "conv", "in", "convs", "]", "\n", "utts", "=", "[", "]", "\n", "for", "n", "in", "range", "(", "len", "(", "v1", ")", ")", ":", "\n", "                ", "utts", ".", "append", "(", "v1", "[", "n", "]", "+", "v2", "[", "n", "]", ")", "\n", "", "utts", "=", "[", "' '", ".", "join", "(", "[", "str", "(", "idx", ")", "for", "idx", "in", "utt", "]", ")", "for", "utt", "in", "utts", "]", "\n", "\n", "", "X2", "=", "run_pca", "(", "utts", ")", "\n", "\n", "print", "(", "'running kmeans'", ",", "flush", "=", "True", ",", "end", "=", "''", ")", "\n", "kmeans", "=", "sklearn", ".", "cluster", ".", "KMeans", "(", "\n", "n_clusters", "=", "n_cluster", ",", "max_iter", "=", "300", ",", "verbose", "=", "0", ",", "random_state", "=", "0", ")", "\n", "preds", "=", "kmeans", ".", "fit_predict", "(", "X2", ")", "\n", "print", "(", "' ... done'", ")", "\n", "", "elif", "model", "==", "'mvsc'", ":", "\n", "        ", "mvsc", "=", "multiview", ".", "mvsc", ".", "MVSC", "(", "\n", "k", "=", "n_cluster", "\n", ")", "\n", "idxes", "=", "dataset", ".", "trn_idx_no_unk", "if", "mvsc_no_unk", "else", "dataset", ".", "trn_idx", "\n", "v1", "=", "[", "dataset", "[", "idx", "]", "[", "0", "]", "[", "0", "]", "for", "idx", "in", "idxes", "]", "\n", "convs", "=", "[", "dataset", "[", "idx", "]", "[", "0", "]", "[", "1", "]", "for", "idx", "in", "idxes", "]", "\n", "v2", "=", "[", "[", "tok", "for", "utt", "in", "conv", "for", "tok", "in", "utt", "]", "for", "conv", "in", "convs", "]", "\n", "v1", "=", "[", "' '", ".", "join", "(", "[", "str", "(", "idx", ")", "for", "idx", "in", "utt", "]", ")", "for", "utt", "in", "v1", "]", "\n", "v2", "=", "[", "' '", ".", "join", "(", "[", "str", "(", "idx", ")", "for", "idx", "in", "utt", "]", ")", "for", "utt", "in", "v2", "]", "\n", "v1_pca", "=", "run_pca", "(", "v1", ")", "\n", "v2_pca", "=", "run_pca", "(", "v2", ")", "\n", "print", "(", "'running mvsc'", ",", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "preds", ",", "eivalues", ",", "eivectors", ",", "sigmas", "=", "mvsc", ".", "fit_transform", "(", "\n", "[", "v1_pca", ",", "v2_pca", "]", ",", "[", "False", "]", "*", "2", "\n", ")", "\n", "print", "(", "'...done'", ")", "\n", "mvsc_time", "=", "time", ".", "time", "(", ")", "-", "start", "\n", "print", "(", "'time taken %.3f'", "%", "mvsc_time", ")", "\n", "\n", "", "lgolds", ",", "lpreds", "=", "[", "]", ",", "[", "]", "\n", "for", "g", ",", "p", "in", "zip", "(", "golds", ",", "list", "(", "preds", ")", ")", ":", "\n", "        ", "if", "g", ">", "0", ":", "\n", "            ", "lgolds", ".", "append", "(", "g", ")", "\n", "lpreds", ".", "append", "(", "p", ")", "\n", "", "", "prec", ",", "rec", ",", "f1", "=", "cluster_metrics", ".", "calc_prec_rec_f1", "(", "\n", "gnd_assignments", "=", "torch", ".", "LongTensor", "(", "lgolds", ")", ".", "to", "(", "device", ")", ",", "\n", "pred_assignments", "=", "torch", ".", "LongTensor", "(", "lpreds", ")", ".", "to", "(", "device", ")", ")", "\n", "acc", "=", "cluster_metrics", ".", "calc_ACC", "(", "\n", "torch", ".", "LongTensor", "(", "lpreds", ")", ".", "to", "(", "device", ")", ",", "torch", ".", "LongTensor", "(", "lgolds", ")", ".", "to", "(", "device", ")", ")", "\n", "\n", "print", "(", "f'{datetime.datetime.now()} eval f1={f1:.4f} prec={prec:.4f} rec={rec:.4f} acc={acc:.4f}'", ")", "\n", "\n", "return", "prec", ",", "rec", ",", "f1", ",", "acc", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.preprocessing.twitter_airlines.Preprocessor.__init__": [[7, 16], ["re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile", "re.compile"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "twitter_at_re", "=", "re", ".", "compile", "(", "'@[a-zA-Z1-90]+'", ")", "\n", "self", ".", "initials_re", "=", "re", ".", "compile", "(", "'\\^[A-Z][A-Z]([^A-Za-z]|$)'", ")", "\n", "self", ".", "star_initials_re", "=", "re", ".", "compile", "(", "'\\*[A-Z]+$'", ")", "\n", "self", ".", "money_re", "=", "re", ".", "compile", "(", "'\\$[1-90]+([-.][1-90]+)?'", ")", "\n", "self", ".", "number_re", "=", "re", ".", "compile", "(", "'[1-90]+([-.: ()][1-90]+)?'", ")", "\n", "self", ".", "airport_code_re", "=", "re", ".", "compile", "(", "'([^a-zA-Z])[A-Z][A-Z][A-Z]([^a-zA-Z])'", ")", "\n", "self", ".", "tag_re", "=", "re", ".", "compile", "(", "'([^a-zA-Z])#[a-zA-Z]+'", ")", "\n", "self", ".", "url_re", "=", "re", ".", "compile", "(", "'http[s]?:(//)?(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'", ")", "# from mleng data_preprocess.py", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.preprocessing.twitter_airlines.Preprocessor.__call__": [[17, 41], ["twitter_airlines.Preprocessor.replace().encode().decode", "twitter_airlines.Preprocessor.replace().replace", "twitter_airlines.Preprocessor.replace().replace().replace", "twitter_airlines.Preprocessor.twitter_at_re.sub", "twitter_airlines.Preprocessor.initials_re.sub", "twitter_airlines.Preprocessor.star_initials_re.sub", "twitter_airlines.Preprocessor.money_re.sub", "twitter_airlines.Preprocessor.number_re.sub", "twitter_airlines.Preprocessor.airport_code_re.sub", "twitter_airlines.Preprocessor.tag_re.sub", "twitter_airlines.Preprocessor.casefold", "twitter_airlines.Preprocessor.url_re.sub", "nltk.word_tokenize", "twitter_airlines.Preprocessor.replace().encode", "twitter_airlines.Preprocessor.replace", "twitter_airlines.Preprocessor.replace().replace", "twitter_airlines.Preprocessor.replace", "twitter_airlines.Preprocessor.replace"], "methods", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.MultiviewEncoders.decode"], ["", "def", "__call__", "(", "self", ",", "text", ",", "info_dict", "=", "None", ")", ":", "\n", "        ", "is_valid", "=", "True", "\n", "if", "'DM'", "in", "text", ":", "\n", "            ", "is_valid", "=", "False", "\n", "", "if", "'https://t.co'", "in", "text", ":", "\n", "            ", "is_valid", "=", "False", "\n", "\n", "", "target_company", "=", "info_dict", "[", "'target_company'", "]", "\n", "cust_twitter_id", "=", "info_dict", "[", "'cust_twitter_id'", "]", "\n", "text", "=", "text", ".", "replace", "(", "'\\n'", ",", "' '", ")", ".", "encode", "(", "'ascii'", ",", "'ignore'", ")", ".", "decode", "(", "'ascii'", ")", "\n", "text", "=", "text", ".", "replace", "(", "'@'", "+", "target_company", ",", "' __company__ '", ")", ".", "replace", "(", "'@'", "+", "cust_twitter_id", ",", "' __cust__ '", ")", "\n", "text", "=", "text", ".", "replace", "(", "'&lt;'", ",", "'<'", ")", ".", "replace", "(", "'&gt;'", ",", "'>'", ")", ".", "replace", "(", "'&amp;'", ",", "'&'", ")", "\n", "text", "=", "self", ".", "twitter_at_re", ".", "sub", "(", "'__twitter_at__'", ",", "text", ")", "\n", "text", "=", "self", ".", "initials_re", ".", "sub", "(", "'__initials__'", ",", "text", ")", "\n", "text", "=", "self", ".", "star_initials_re", ".", "sub", "(", "'__initials__'", ",", "text", ")", "\n", "text", "=", "self", ".", "money_re", ".", "sub", "(", "' __money__ '", ",", "text", ")", "\n", "text", "=", "self", ".", "number_re", ".", "sub", "(", "' __num__ '", ",", "text", ")", "\n", "text", "=", "self", ".", "airport_code_re", ".", "sub", "(", "'\\g<1> __airport_code__ \\g<2>'", ",", "text", ")", "\n", "text", "=", "self", ".", "tag_re", ".", "sub", "(", "'\\g<1> __twitter_tag__ '", ",", "text", ")", "\n", "text", "=", "text", ".", "casefold", "(", ")", "\n", "text", "=", "self", ".", "url_re", ".", "sub", "(", "' __url__ '", ",", "text", ")", "\n", "text", "=", "' '", ".", "join", "(", "nltk", ".", "word_tokenize", "(", "text", ")", ")", "\n", "\n", "return", "is_valid", ",", "text", "\n", "", "", ""]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.preprocessing.askubuntu.Preprocessor.__init__": [[62, 64], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "max_len", ")", ":", "\n", "        ", "self", ".", "max_len", "=", "max_len", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.preprocessing.askubuntu.Preprocessor.__call__": [[65, 67], ["askubuntu.preprocess_from_tao"], "methods", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.preprocessing.askubuntu.preprocess_from_tao"], ["", "def", "__call__", "(", "self", ",", "text", ")", ":", "\n", "        ", "return", "True", ",", "preprocess_from_tao", "(", "text", ",", "max_len", "=", "self", ".", "max_len", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.preprocessing.askubuntu.inner_preprocess_text_from_tao": [[20, 36], ["text.casefold.encode().decode", "URLREG.sub", "text.casefold.casefold", "nltk.tokenize.word_tokenize", "text.casefold.encode"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.MultiviewEncoders.decode"], ["def", "inner_preprocess_text_from_tao", "(", "text", ",", "max_len", ")", ":", "\n", "# remove non-ascii chars", "\n", "    ", "text", "=", "text", ".", "encode", "(", "\"utf8\"", ")", ".", "decode", "(", "\"ascii\"", ",", "\"ignore\"", ")", "\n", "\n", "# remove URLs", "\n", "text", "=", "URLREG", ".", "sub", "(", "'*=URL=*'", ",", "text", ")", "\n", "\n", "text", "=", "text", ".", "casefold", "(", ")", "\n", "\n", "# tokenize, filter and truncate", "\n", "words", "=", "word_tokenize", "(", "text", ")", "\n", "if", "REMOVE_STOP", ":", "\n", "        ", "words", "=", "[", "x", "for", "x", "in", "words", "if", "x", "not", "in", "STOPWORDS", "]", "\n", "", "if", "max_len", ">", "0", ":", "\n", "        ", "words", "=", "words", "[", ":", "max_len", "]", "\n", "", "return", "u' '", ".", "join", "(", "words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.preprocessing.askubuntu.preprocess_from_tao": [[38, 59], ["bs4.BeautifulSoup", "askubuntu.inner_preprocess_text_from_tao", "x.extract", "x.extract", "blk.text.strip().startswith", "blk.decompose", "bs4.BeautifulSoup.findAll", "bs4.BeautifulSoup.findAll", "blk.text.strip"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.preprocessing.askubuntu.inner_preprocess_text_from_tao"], ["", "def", "preprocess_from_tao", "(", "text", ",", "max_len", ")", ":", "\n", "    ", "\"\"\"\n    adapted from Tao's code for http://aclweb.org/anthology/D18-1131\n    \"\"\"", "\n", "body_soup", "=", "BeautifulSoup", "(", "text", ",", "\"lxml\"", ")", "\n", "\n", "# remove code", "\n", "[", "x", ".", "extract", "(", ")", "for", "x", "in", "body_soup", ".", "findAll", "(", "'code'", ")", "]", "\n", "\n", "# also remove pre", "\n", "[", "x", ".", "extract", "(", ")", "for", "x", "in", "body_soup", ".", "findAll", "(", "'pre'", ")", "]", "\n", "\n", "# remove \"Possible Duplicate\" section", "\n", "blk", "=", "body_soup", ".", "blockquote", "\n", "if", "blk", "and", "blk", ".", "text", ".", "strip", "(", ")", ".", "startswith", "(", "\"Possible Duplicate:\"", ")", ":", "\n", "        ", "blk", ".", "decompose", "(", ")", "\n", "", "body_cleaned", "=", "inner_preprocess_text_from_tao", "(", "body_soup", ".", "text", ",", "max_len", "=", "max_len", ")", "\n", "assert", "\"Possible Duplicate:\"", "not", "in", "body_cleaned", "\n", "\n", "assert", "\"\\n\"", "not", "in", "body_cleaned", "\n", "return", "body_cleaned", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.twitter_airlines_raw_merger2.run": [[9, 30], ["open", "open", "csv.DictReader", "csv.DictWriter", "csv.DictWriter.writeheader", "enumerate", "csv.DictWriter.writerow", "open.close", "open", "csv.DictReader", "list", "int", "os.path.expanduser", "os.path.expanduser", "int", "csv.DictWriter.writerow", "os.path.expanduser", "enumerate"], "function", ["None"], ["def", "run", "(", "args", ")", ":", "\n", "    ", "with", "open", "(", "expand", "(", "args", ".", "airlines_raw_file", ")", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "dict_reader", "=", "csv", ".", "DictReader", "(", "f", ")", "\n", "raw_rows", "=", "list", "(", "dict_reader", ")", "\n", "", "raw_row_by_tweet_id", "=", "{", "int", "(", "row", "[", "'first_tweet_id'", "]", ")", ":", "row", "for", "i", ",", "row", "in", "enumerate", "(", "raw_rows", ")", "}", "\n", "\n", "f_in", "=", "open", "(", "expand", "(", "args", ".", "airlines_merged_file", ")", ",", "'r'", ")", "\n", "f_out", "=", "open", "(", "expand", "(", "args", ".", "out_file", ")", ",", "'w'", ")", "\n", "dict_reader", "=", "csv", ".", "DictReader", "(", "f_in", ")", "\n", "dict_writer", "=", "csv", ".", "DictWriter", "(", "f_out", ",", "fieldnames", "=", "[", "'first_tweet_id'", ",", "'tag'", ",", "'first_utterance'", ",", "'context'", "]", ")", "\n", "dict_writer", ".", "writeheader", "(", ")", "\n", "for", "i", ",", "old_merged_row", "in", "enumerate", "(", "dict_reader", ")", ":", "\n", "        ", "if", "old_merged_row", "[", "'first_tweet_id'", "]", "==", "''", ":", "\n", "            ", "continue", "\n", "", "tweet_id", "=", "int", "(", "old_merged_row", "[", "'first_tweet_id'", "]", ")", "\n", "raw_row", "=", "raw_row_by_tweet_id", "[", "tweet_id", "]", "\n", "raw_row", "[", "'tag'", "]", "=", "old_merged_row", "[", "'tag'", "]", "\n", "dict_writer", ".", "writerow", "(", "raw_row", ")", "\n", "# so, we accidentally had a blank line at the end of the non-raw dataset. add that in here...", "\n", "", "dict_writer", ".", "writerow", "(", "{", "'first_tweet_id'", ":", "''", ",", "'tag'", ":", "'UNK'", ",", "'first_utterance'", ":", "''", ",", "'context'", ":", "''", "}", ")", "\n", "f_out", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.askubuntu_preprocess.NullPreprocessor.__call__": [[26, 29], ["text.replace().replace.replace().replace.replace().replace", "text.replace().replace.replace().replace.replace"], "methods", ["None"], ["    ", "def", "__call__", "(", "self", ",", "text", ")", ":", "\n", "        ", "text", "=", "text", ".", "replace", "(", "'\\n'", ",", "' '", ")", ".", "replace", "(", "'\\r'", ",", "' '", ")", "\n", "return", "True", ",", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.askubuntu_preprocess.index_posts": [[31, 68], ["open", "open", "csv.DictWriter", "csv.DictWriter.writeheader", "time.time", "enumerate", "os.path.expanduser", "os.path.expanduser", "row.strip.strip", "xml.dom.minidom.parseString", "row.strip.startswith", "csv.DictWriter.writerow", "print", "print", "time.time", "time.time"], "function", ["None"], ["", "", "def", "index_posts", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n    got through Posts.xml, and find for each post id, the id of the answer. store both in a csv file\n\n    example row:\n        <row Id=\"5\" PostTypeId=\"1\" CreationDate=\"2010-07-28T19:23:40.273\" Score=\"22\" ViewCount=\"581\"\n        Body=\"&lt;p&gt;What are some alternatives to upgrading without using the standard upgrade system?\n        Suppose for example that I wanted to upgrade an Ubuntu installation on a machine with a poor Internet connection. \n        What would my options be? Could I just use a standard Ubuntu disk to upgrade this machine? If I already have a standard Ubuntu\n         disk and want to use that, could I do a clean install without wiping data?&lt;/p&gt;&#xA;\"\n         OwnerUserId=\"10\" LastEditorUserId=\"10581\" LastEditDate=\"2014-02-18T13:34:25.793\" LastActivityDate=\"2014-02-18T13:34:25.793\"\n         Title=\"What are some alternatives to upgrading without using the standard upgrade system?\"\n         Tags=\"&lt;upgrade&gt;&lt;live-cd&gt;&lt;system-installation&gt;\"\n         AnswerCount=\"2\" CommentCount=\"1\" FavoriteCount=\"1\" />\n    \"\"\"", "\n", "in_f", "=", "open", "(", "expand", "(", "args", ".", "in_posts", ")", ",", "'r'", ")", "\n", "out_f", "=", "open", "(", "expand", "(", "args", ".", "out_posts_index", ")", ",", "'w'", ")", "\n", "dict_writer", "=", "csv", ".", "DictWriter", "(", "out_f", ",", "fieldnames", "=", "[", "'question_id'", ",", "'answer_id'", "]", ")", "\n", "dict_writer", ".", "writeheader", "(", ")", "\n", "last_print", "=", "time", ".", "time", "(", ")", "\n", "for", "n", ",", "row", "in", "enumerate", "(", "in_f", ")", ":", "\n", "        ", "row", "=", "row", ".", "strip", "(", ")", "\n", "if", "not", "row", ".", "startswith", "(", "'<row'", ")", ":", "\n", "            ", "continue", "\n", "", "dom", "=", "minidom", ".", "parseString", "(", "row", ")", "\n", "node", "=", "dom", ".", "firstChild", "\n", "att", "=", "node", ".", "attributes", "\n", "if", "'AcceptedAnswerId'", "in", "att", ":", "\n", "            ", "id", "=", "att", "[", "'Id'", "]", ".", "value", "\n", "accepted_answer_id", "=", "att", "[", "'AcceptedAnswerId'", "]", ".", "value", "\n", "dict_writer", ".", "writerow", "(", "{", "'question_id'", ":", "id", ",", "'answer_id'", ":", "accepted_answer_id", "}", ")", "\n", "", "if", "args", ".", "in_max_posts", "is", "not", "None", "and", "n", ">=", "args", ".", "in_max_posts", ":", "\n", "            ", "print", "(", "'reached max rows => breaking'", ")", "\n", "break", "\n", "", "if", "time", ".", "time", "(", ")", "-", "last_print", ">=", "3.0", ":", "\n", "            ", "print", "(", "n", ")", "\n", "last_print", "=", "time", ".", "time", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.askubuntu_preprocess.get_clusters": [[70, 122], ["open", "csv.reader", "networkx.Graph", "enumerate", "print", "print", "collections.defaultdict", "collections.defaultdict", "enumerate", "print", "sorted", "print", "enumerate", "os.path.expanduser", "int", "int", "nx.Graph.add_edge", "len", "len", "networkx.connected_components", "len", "clusters_by_size[].append", "collections.defaultdict.items", "len", "print", "list", "clusters.append", "networkx.connected_components", "len", "top_clusters.append"], "function", ["None"], ["", "", "", "def", "get_clusters", "(", "in_dupes_file", ",", "in_max_dupes", ",", "max_clusters", ")", ":", "\n", "    ", "\"\"\"\n    we're reading in the list of pairs of dupes. These are a in format of two post ids per line, like:\n        615465 8653\n        833376 377050\n        30585 120621\n        178532 152184\n        69455 68850\n\n    When we read these in, we have no idea whether these posts have answers etc. We just read in all\n    the pairs of post ids. We are then going to add these post ids to a graph (each post id forms a node),\n    and the pairs of post ids become connectsion in the graph. We then form all connected components.\n\n    We sort the connected components by size (reverse order), and take the top num_test_clusters components\n    We just ignore the other components\n    \"\"\"", "\n", "f_in", "=", "open", "(", "expand", "(", "in_dupes_file", ")", ",", "'r'", ")", "\n", "csv_reader", "=", "csv", ".", "reader", "(", "f_in", ",", "delimiter", "=", "' '", ")", "\n", "G", "=", "nx", ".", "Graph", "(", ")", "\n", "for", "n", ",", "row", "in", "enumerate", "(", "csv_reader", ")", ":", "\n", "        ", "left", "=", "int", "(", "row", "[", "0", "]", ")", "\n", "right", "=", "int", "(", "row", "[", "1", "]", ")", "\n", "G", ".", "add_edge", "(", "left", ",", "right", ")", "\n", "if", "in_max_dupes", "is", "not", "None", "and", "n", ">=", "in_max_dupes", ":", "\n", "            ", "print", "(", "'reached max tao rows => break'", ")", "\n", "break", "\n", "", "", "print", "(", "'num nodes'", ",", "len", "(", "G", ")", ")", "\n", "print", "(", "'num clusters'", ",", "len", "(", "list", "(", "nx", ".", "connected_components", "(", "G", ")", ")", ")", ")", "\n", "count_by_size", "=", "defaultdict", "(", "int", ")", "\n", "clusters_by_size", "=", "defaultdict", "(", "list", ")", "\n", "for", "i", ",", "cluster", "in", "enumerate", "(", "nx", ".", "connected_components", "(", "G", ")", ")", ":", "\n", "        ", "size", "=", "len", "(", "cluster", ")", "\n", "count_by_size", "[", "size", "]", "+=", "1", "\n", "clusters_by_size", "[", "size", "]", ".", "append", "(", "cluster", ")", "\n", "", "print", "(", "'count by size:'", ")", "\n", "clusters", "=", "[", "]", "\n", "top_clusters", "=", "[", "]", "\n", "for", "size", ",", "count", "in", "sorted", "(", "count_by_size", ".", "items", "(", ")", ",", "reverse", "=", "True", ")", ":", "\n", "        ", "for", "cluster", "in", "clusters_by_size", "[", "size", "]", ":", "\n", "            ", "clusters", ".", "append", "(", "cluster", ")", "\n", "if", "len", "(", "top_clusters", ")", "<", "max_clusters", ":", "\n", "                ", "top_clusters", ".", "append", "(", "cluster", ")", "\n", "\n", "", "", "", "print", "(", "'len(clusters)'", ",", "len", "(", "clusters", ")", ")", "\n", "top_cluster_post_ids", "=", "[", "id", "for", "cluster", "in", "top_clusters", "for", "id", "in", "cluster", "]", "\n", "\n", "post2cluster", "=", "{", "}", "\n", "for", "cluster_id", ",", "cluster", "in", "enumerate", "(", "clusters", ")", ":", "\n", "        ", "for", "post", "in", "cluster", ":", "\n", "            ", "post2cluster", "[", "post", "]", "=", "cluster_id", "\n", "\n", "", "", "return", "top_clusters", ",", "top_cluster_post_ids", ",", "post2cluster", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.askubuntu_preprocess.read_posts_index": [[124, 133], ["print", "open", "csv.DictReader", "list", "os.path.expanduser", "int", "int"], "function", ["None"], ["", "def", "read_posts_index", "(", "in_posts_index", ")", ":", "\n", "    ", "with", "open", "(", "expand", "(", "args", ".", "in_posts_index", ")", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "dict_reader", "=", "csv", ".", "DictReader", "(", "f", ")", "\n", "index_rows", "=", "list", "(", "dict_reader", ")", "\n", "", "index_rows", "=", "[", "{", "'question_id'", ":", "int", "(", "row", "[", "'question_id'", "]", ")", ",", "'answer_id'", ":", "int", "(", "row", "[", "'answer_id'", "]", ")", "}", "for", "row", "in", "index_rows", "]", "\n", "post2answer", "=", "{", "row", "[", "'question_id'", "]", ":", "row", "[", "'answer_id'", "]", "for", "row", "in", "index_rows", "}", "\n", "answer2post", "=", "{", "row", "[", "'answer_id'", "]", ":", "row", "[", "'question_id'", "]", "for", "row", "in", "index_rows", "}", "\n", "print", "(", "'loaded index'", ")", "\n", "return", "post2answer", ",", "answer2post", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.askubuntu_preprocess.load_posts": [[135, 183], ["collections.defaultdict", "open", "time.time", "enumerate", "print", "os.path.expanduser", "row.strip.strip", "int", "row.strip.startswith", "[].partition", "print", "xml.dom.minidom.parseString", "print", "time.time", "print", "xml.dom.minidom.parseString", "time.time", "len", "row.strip.partition"], "function", ["None"], ["", "def", "load_posts", "(", "answer2post", ",", "post2answer", ",", "in_posts", ")", ":", "\n", "# load in all the posts from Posts.xml", "\n", "    ", "post_by_id", "=", "defaultdict", "(", "dict", ")", "\n", "posts_f", "=", "open", "(", "expand", "(", "in_posts", ")", ",", "'r'", ")", "\n", "last_print", "=", "time", ".", "time", "(", ")", "\n", "for", "n", ",", "row", "in", "enumerate", "(", "posts_f", ")", ":", "\n", "        ", "row", "=", "row", ".", "strip", "(", ")", "\n", "if", "not", "row", ".", "startswith", "(", "'<row'", ")", ":", "\n", "            ", "continue", "\n", "", "row_id", "=", "row", ".", "partition", "(", "' Id=\"'", ")", "[", "2", "]", ".", "partition", "(", "'\"'", ")", "[", "0", "]", "\n", "if", "row_id", "==", "''", ":", "\n", "            ", "print", "(", "row", ")", "\n", "assert", "row_id", "!=", "''", "\n", "", "row_id", "=", "int", "(", "row_id", ")", "\n", "if", "row_id", "in", "post2answer", ":", "\n", "            ", "\"\"\"\n            example row:\n            <row Id=\"5\" PostTypeId=\"1\" CreationDate=\"2010-07-28T19:23:40.273\" Score=\"22\" ViewCount=\"581\"\n            Body=\"&lt;p&gt;What are some alternatives to upgrading without using the standard upgrade system?\n            Suppose for example that I wanted to upgrade an Ubuntu installation on a machine with a poor Internet connection. \n            What would my options be? Could I just use a standard Ubuntu disk to upgrade this machine? If I already have a standard Ubuntu\n             disk and want to use that, could I do a clean install without wiping data?&lt;/p&gt;&#xA;\"\n             OwnerUserId=\"10\" LastEditorUserId=\"10581\" LastEditDate=\"2014-02-18T13:34:25.793\" LastActivityDate=\"2014-02-18T13:34:25.793\"\n             Title=\"What are some alternatives to upgrading without using the standard upgrade system?\"\n             Tags=\"&lt;upgrade&gt;&lt;live-cd&gt;&lt;system-installation&gt;\"\n             AnswerCount=\"2\" CommentCount=\"1\" FavoriteCount=\"1\" />\n            \"\"\"", "\n", "dom", "=", "minidom", ".", "parseString", "(", "row", ")", "\n", "node", "=", "dom", ".", "firstChild", "\n", "att", "=", "node", ".", "attributes", "\n", "assert", "att", "[", "'PostTypeId'", "]", ".", "value", "==", "'1'", "\n", "post_by_id", "[", "row_id", "]", "[", "'question_title'", "]", "=", "att", "[", "'Title'", "]", ".", "value", "\n", "post_by_id", "[", "row_id", "]", "[", "'question_body'", "]", "=", "att", "[", "'Body'", "]", ".", "value", "\n", "", "elif", "row_id", "in", "answer2post", ":", "\n", "            ", "dom", "=", "minidom", ".", "parseString", "(", "row", ")", "\n", "node", "=", "dom", ".", "firstChild", "\n", "att", "=", "node", ".", "attributes", "\n", "assert", "att", "[", "'PostTypeId'", "]", ".", "value", "==", "'2'", "\n", "post_id", "=", "answer2post", "[", "row_id", "]", "\n", "post_by_id", "[", "post_id", "]", "[", "'answer_body'", "]", "=", "att", "[", "'Body'", "]", ".", "value", "\n", "", "if", "time", ".", "time", "(", ")", "-", "last_print", ">=", "3.0", ":", "\n", "            ", "print", "(", "len", "(", "post_by_id", ")", ")", "\n", "last_print", "=", "time", ".", "time", "(", ")", "\n", "", "if", "args", ".", "in_max_posts", "is", "not", "None", "and", "n", ">", "args", ".", "in_max_posts", ":", "\n", "            ", "print", "(", "'reached in_max_posts => terminating'", ")", "\n", "break", "\n", "", "", "print", "(", "'loaded info from Posts.xml'", ")", "\n", "return", "post_by_id", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.askubuntu_preprocess.create_labeled": [[185, 243], ["torch.manual_seed", "numpy.random.seed", "random.seed", "open", "askubuntu_preprocess.get_clusters", "print", "print", "print", "print", "askubuntu_preprocess.read_posts_index", "print", "print", "set", "print", "print", "askubuntu_preprocess.load_posts", "collections.defaultdict", "csv.DictWriter", "csv.DictWriter.writeheader", "print", "print", "os.path.expanduser", "len", "len", "len", "new_clusters.append", "len", "len", "preprocessing.askubuntu.Preprocessor", "askubuntu_preprocess.NullPreprocessor", "csv.DictWriter.writerow", "len", "len", "preprocessor", "preprocessor", "preprocessor"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.askubuntu_preprocess.get_clusters", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.askubuntu_preprocess.read_posts_index", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.askubuntu_preprocess.load_posts"], ["", "def", "create_labeled", "(", "args", ")", ":", "\n", "    ", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "\n", "out_f", "=", "open", "(", "expand", "(", "args", ".", "out_labeled", ")", ",", "'w'", ")", "# open now to check we can", "\n", "\n", "clusters", ",", "post_ids", ",", "post2cluster", "=", "get_clusters", "(", "\n", "in_dupes_file", "=", "args", ".", "in_dupes", ",", "in_max_dupes", "=", "args", ".", "in_max_dupes", ",", "max_clusters", "=", "args", ".", "max_clusters", ")", "\n", "print", "(", "'cluster sizes:'", ",", "[", "len", "(", "cluster", ")", "for", "cluster", "in", "clusters", "]", ")", "\n", "print", "(", "'len(clusters)'", ",", "len", "(", "clusters", ")", ")", "\n", "print", "(", "'len(post_ids) from dupes graph'", ",", "len", "(", "post_ids", ")", ")", "\n", "\n", "print", "(", "'removing post ids which dont have answers...'", ")", "\n", "post2answer", ",", "answer2post", "=", "read_posts_index", "(", "in_posts_index", "=", "args", ".", "in_posts_index", ")", "\n", "post_ids", "=", "[", "id", "for", "id", "in", "post_ids", "if", "id", "in", "post2answer", "]", "\n", "print", "(", "'len(post_ids) after removing no answer'", ",", "len", "(", "post_ids", ")", ")", "\n", "new_clusters", "=", "[", "]", "\n", "for", "cluster", "in", "clusters", ":", "\n", "        ", "cluster", "=", "[", "id", "for", "id", "in", "cluster", "if", "id", "in", "post2answer", "]", "\n", "new_clusters", ".", "append", "(", "cluster", ")", "\n", "", "clusters", "=", "new_clusters", "\n", "print", "(", "'len clusters after removing no answer'", ",", "[", "len", "(", "cluster", ")", "for", "cluster", "in", "clusters", "]", ")", "\n", "\n", "post_ids_set", "=", "set", "(", "post_ids", ")", "\n", "print", "(", "'len(post_ids_set)'", ",", "len", "(", "post_ids_set", ")", ")", "\n", "answer_ids", "=", "[", "post2answer", "[", "id", "]", "for", "id", "in", "post_ids", "]", "\n", "print", "(", "'len(answer_ids)'", ",", "len", "(", "answer_ids", ")", ")", "\n", "\n", "preprocessor", "=", "Preprocessor", "(", "max_len", "=", "args", ".", "max_len", ")", "if", "not", "args", ".", "no_preprocess", "else", "NullPreprocessor", "(", ")", "\n", "post_by_id", "=", "load_posts", "(", "answer2post", "=", "answer2post", ",", "post2answer", "=", "post2answer", ",", "in_posts", "=", "args", ".", "in_posts", ")", "\n", "\n", "count_by_state", "=", "defaultdict", "(", "int", ")", "\n", "n", "=", "0", "\n", "dict_writer", "=", "csv", ".", "DictWriter", "(", "out_f", ",", "fieldnames", "=", "[", "\n", "'id'", ",", "'cluster_id'", ",", "'question_title'", ",", "'question_body'", ",", "'answer_body'", "]", ")", "\n", "dict_writer", ".", "writeheader", "(", ")", "\n", "for", "post_id", "in", "post_ids", ":", "\n", "        ", "if", "post_id", "not", "in", "post_by_id", ":", "\n", "            ", "count_by_state", "[", "'not in post_by_id'", "]", "+=", "1", "\n", "continue", "\n", "", "post", "=", "post_by_id", "[", "post_id", "]", "\n", "if", "'answer_body'", "not", "in", "post", "or", "'question_body'", "not", "in", "post", ":", "\n", "            ", "count_by_state", "[", "'no body, or no answer'", "]", "+=", "1", "\n", "continue", "\n", "", "count_by_state", "[", "'ok'", "]", "+=", "1", "\n", "cluster_id", "=", "post2cluster", "[", "post_id", "]", "\n", "row", "=", "{", "\n", "'id'", ":", "post_id", ",", "\n", "'cluster_id'", ":", "cluster_id", ",", "\n", "'question_title'", ":", "preprocessor", "(", "post", "[", "'question_title'", "]", ")", "[", "1", "]", ",", "\n", "'question_body'", ":", "preprocessor", "(", "post", "[", "'question_body'", "]", ")", "[", "1", "]", ",", "\n", "'answer_body'", ":", "preprocessor", "(", "post", "[", "'answer_body'", "]", ")", "[", "1", "]", "\n", "}", "\n", "dict_writer", ".", "writerow", "(", "row", ")", "\n", "n", "+=", "1", "\n", "", "print", "(", "count_by_state", ")", "\n", "print", "(", "'rows written'", ",", "n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.askubuntu_preprocess.create_unlabeled": [[245, 288], ["torch.manual_seed", "numpy.random.seed", "random.seed", "open", "askubuntu_preprocess.read_posts_index", "print", "print", "askubuntu_preprocess.load_posts", "print", "collections.defaultdict", "csv.DictWriter", "csv.DictWriter.writeheader", "time.time", "load_posts.items", "print", "os.path.expanduser", "len", "preprocessing.askubuntu.Preprocessor", "askubuntu_preprocess.NullPreprocessor", "len", "csv.DictWriter.writerow", "print", "time.time", "time.time", "preprocessor", "preprocessor", "preprocessor"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.askubuntu_preprocess.read_posts_index", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.askubuntu_preprocess.load_posts"], ["", "def", "create_unlabeled", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n    this is going to do:\n    - take a question (specific type in Posts.xml)\n    - match it with the accepted answer (using the index)\n    - write these out\n    \"\"\"", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "\n", "out_f", "=", "open", "(", "expand", "(", "args", ".", "out_unlabeled", ")", ",", "'w'", ")", "# open now, to check we can...", "\n", "\n", "post2answer", ",", "answer2post", "=", "read_posts_index", "(", "in_posts_index", "=", "args", ".", "in_posts_index", ")", "\n", "print", "(", "'loaded index'", ")", "\n", "print", "(", "'posts in index'", ",", "len", "(", "post2answer", ")", ")", "\n", "\n", "preprocessor", "=", "Preprocessor", "(", "max_len", "=", "args", ".", "max_len", ")", "if", "not", "args", ".", "no_preprocess", "else", "NullPreprocessor", "(", ")", "\n", "post_by_id", "=", "load_posts", "(", "post2answer", "=", "post2answer", ",", "answer2post", "=", "answer2post", ",", "in_posts", "=", "args", ".", "in_posts", ")", "\n", "print", "(", "'loaded all posts, len(post_by_id)'", ",", "len", "(", "post_by_id", ")", ")", "\n", "\n", "count_by_state", "=", "defaultdict", "(", "int", ")", "\n", "n", "=", "0", "\n", "dict_writer", "=", "csv", ".", "DictWriter", "(", "out_f", ",", "fieldnames", "=", "[", "'id'", ",", "'question_title'", ",", "'question_body'", ",", "'answer_body'", "]", ")", "\n", "dict_writer", ".", "writeheader", "(", ")", "\n", "last_print", "=", "time", ".", "time", "(", ")", "\n", "for", "post_id", ",", "info", "in", "post_by_id", ".", "items", "(", ")", ":", "\n", "        ", "if", "'answer_body'", "not", "in", "info", "or", "'question_body'", "not", "in", "info", ":", "\n", "            ", "count_by_state", "[", "'no body, or no answer'", "]", "+=", "1", "\n", "continue", "\n", "", "count_by_state", "[", "'ok'", "]", "+=", "1", "\n", "dict_writer", ".", "writerow", "(", "{", "\n", "'id'", ":", "post_id", ",", "\n", "'question_title'", ":", "preprocessor", "(", "info", "[", "'question_title'", "]", ")", "[", "1", "]", ",", "\n", "'question_body'", ":", "preprocessor", "(", "info", "[", "'question_body'", "]", ")", "[", "1", "]", ",", "\n", "'answer_body'", ":", "preprocessor", "(", "info", "[", "'answer_body'", "]", ")", "[", "1", "]", "\n", "}", ")", "\n", "if", "time", ".", "time", "(", ")", "-", "last_print", ">=", "10", ":", "\n", "            ", "print", "(", "'written'", ",", "n", ")", "\n", "last_print", "=", "time", ".", "time", "(", ")", "\n", "", "n", "+=", "1", "\n", "\n", "", "print", "(", "count_by_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.labeled_unlabeled_merger.get_col_by_role": [[10, 17], ["role_string.split", "print", "s.split"], "function", ["None"], ["def", "get_col_by_role", "(", "role_string", ")", ":", "\n", "    ", "col_by_role", "=", "{", "}", "\n", "for", "s", "in", "role_string", ".", "split", "(", "','", ")", ":", "\n", "        ", "split_s", "=", "s", ".", "split", "(", "'='", ")", "\n", "col_by_role", "[", "split_s", "[", "0", "]", "]", "=", "split_s", "[", "1", "]", "\n", "", "print", "(", "'col_by_role'", ",", "col_by_role", ")", "\n", "return", "col_by_role", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.labeled_unlabeled_merger.run": [[19, 71], ["add_columns.split.split", "open", "csv.DictWriter", "csv.DictWriter.writeheader", "labeled_unlabeled_merger.get_col_by_role", "labeled_unlabeled_merger.get_col_by_role", "collections.OrderedDict", "print", "collections.OrderedDict.values", "open.close", "print", "print", "csv.DictWriter.writerow", "column_order.split", "open", "csv.DictReader", "open", "csv.DictReader", "print", "view1.startswith"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.labeled_unlabeled_merger.get_col_by_role", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.labeled_unlabeled_merger.get_col_by_role"], ["", "def", "run", "(", "labeled_files", ",", "unlabeled_files", ",", "out_file", ",", "unlabeled_columns", ",", "labeled_columns", ",", "no_add_cust_tokens", ",", "add_columns", ",", "column_order", ")", ":", "\n", "    ", "add_columns", "=", "add_columns", ".", "split", "(", "','", ")", "\n", "out_f", "=", "open", "(", "out_file", ",", "'w'", ")", "\n", "dict_writer", "=", "csv", ".", "DictWriter", "(", "out_f", ",", "fieldnames", "=", "column_order", ".", "split", "(", "','", ")", ")", "\n", "dict_writer", ".", "writeheader", "(", ")", "\n", "\n", "labeled_col_by_role", "=", "get_col_by_role", "(", "labeled_columns", ")", "\n", "unlabeled_col_by_role", "=", "get_col_by_role", "(", "unlabeled_columns", ")", "\n", "\n", "labeled_view1", "=", "labeled_col_by_role", "[", "'view1'", "]", "\n", "labeled_label", "=", "labeled_col_by_role", "[", "'label'", "]", "\n", "\n", "unlabeled_view1", "=", "unlabeled_col_by_role", "[", "'view1'", "]", "\n", "unlabeled_view2", "=", "unlabeled_col_by_role", "[", "'view2'", "]", "\n", "\n", "unlabeled_by_first_utterance", "=", "OrderedDict", "(", ")", "\n", "for", "filename", "in", "unlabeled_files", ":", "\n", "        ", "print", "(", "filename", ")", "\n", "with", "open", "(", "filename", ",", "'r'", ")", "as", "in_f", ":", "\n", "            ", "dict_reader", "=", "csv", ".", "DictReader", "(", "in_f", ")", "\n", "for", "row", "in", "dict_reader", ":", "\n", "                ", "view1", "=", "row", "[", "unlabeled_view1", "]", "\n", "if", "not", "no_add_cust_tokens", "and", "not", "view1", ".", "startswith", "(", "'<cust__'", ")", ":", "\n", "                    ", "view1", "=", "'<cust__ '", "+", "view1", "+", "' __cust>'", "\n", "", "out_row", "=", "{", "\n", "'view1'", ":", "view1", ",", "\n", "'view2'", ":", "row", "[", "unlabeled_view2", "]", ",", "\n", "'label'", ":", "'UNK'", "\n", "}", "\n", "for", "k", "in", "add_columns", ":", "\n", "                    ", "out_row", "[", "k", "]", "=", "row", "[", "k", "]", "\n", "", "unlabeled_by_first_utterance", "[", "view1", "]", "=", "out_row", "\n", "", "", "", "print", "(", "'loaded unlabeled'", ")", "\n", "\n", "for", "filename", "in", "labeled_files", ":", "\n", "        ", "print", "(", "filename", ")", "\n", "with", "open", "(", "filename", ",", "'r'", ")", "as", "in_f", ":", "\n", "            ", "dict_reader", "=", "csv", ".", "DictReader", "(", "in_f", ")", "\n", "for", "row", "in", "dict_reader", ":", "\n", "                ", "view1", "=", "row", "[", "labeled_view1", "]", "\n", "if", "view1", "not", "in", "unlabeled_by_first_utterance", ":", "\n", "                    ", "print", "(", "'warning: not found in unlabelled'", ",", "view1", ")", "\n", "continue", "\n", "", "out_row", "=", "unlabeled_by_first_utterance", "[", "view1", "]", "\n", "out_row", "[", "'label'", "]", "=", "row", "[", "labeled_label", "]", "\n", "for", "k", "in", "add_columns", ":", "\n", "                    ", "assert", "out_row", "[", "k", "]", "==", "row", "[", "k", "]", "\n", "\n", "", "", "", "", "for", "row", "in", "unlabeled_by_first_utterance", ".", "values", "(", ")", ":", "\n", "        ", "dict_writer", ".", "writerow", "(", "row", ")", "\n", "\n", "", "out_f", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.twitter_dataset_preprocess.NullPreprocessor.__call__": [[30, 44], ["text.replace().replace.replace().replace.replace().replace", "text.replace().replace.replace().replace.replace().replace", "text.replace().replace.replace().replace.replace", "text.replace().replace.replace().replace.replace"], "methods", ["None"], ["    ", "def", "__call__", "(", "self", ",", "text", ",", "info_dict", "=", "None", ")", ":", "\n", "        ", "is_valid", "=", "True", "\n", "if", "'DM'", "in", "text", ":", "\n", "            ", "is_valid", "=", "False", "\n", "", "if", "'https://t.co'", "in", "text", ":", "\n", "            ", "is_valid", "=", "False", "\n", "\n", "", "text", "=", "text", ".", "replace", "(", "'\\n'", ",", "' '", ")", ".", "replace", "(", "'\\r'", ",", "' '", ")", "\n", "\n", "target_company", "=", "info_dict", "[", "'target_company'", "]", "\n", "cust_twitter_id", "=", "info_dict", "[", "'cust_twitter_id'", "]", "\n", "text", "=", "text", ".", "replace", "(", "'@'", "+", "target_company", ",", "' __company__ '", ")", ".", "replace", "(", "'@'", "+", "cust_twitter_id", ",", "' __cust__ '", ")", "\n", "\n", "return", "is_valid", ",", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.twitter_dataset_preprocess.run_for_company": [[45, 113], ["open", "csv.DictReader", "print", "print", "collections.defaultdict", "collections.defaultdict", "enumerate", "print", "os.path.expanduser", "len", "len", "preprocessing.twitter_airlines.Preprocessor", "twitter_dataset_preprocess.NullPreprocessor", "start_node_ids.append", "print", "node[].replace", "preprocessor", "next_by_prev.get", "len", "conversation_texts.append", "print", "examples_writer.writerow", "len"], "function", ["None"], ["", "", "def", "run_for_company", "(", "in_csv_file", ",", "in_max_rows", ",", "examples_writer", ",", "target_company", ",", "no_preprocess", ")", ":", "\n", "    ", "f_in", "=", "open", "(", "expand", "(", "in_csv_file", ")", ",", "'r'", ")", "\n", "node_by_id", "=", "{", "}", "\n", "start_node_ids", "=", "[", "]", "\n", "dict_reader", "=", "csv", ".", "DictReader", "(", "f_in", ")", "\n", "next_by_prev", "=", "{", "}", "\n", "for", "row", "in", "dict_reader", ":", "\n", "        ", "id", "=", "row", "[", "'tweet_id'", "]", "\n", "prev", "=", "row", "[", "'in_response_to_tweet_id'", "]", "\n", "next_by_prev", "[", "prev", "]", "=", "id", "\n", "if", "prev", "==", "''", "and", "(", "'@'", "+", "target_company", ")", "in", "row", "[", "'text'", "]", ":", "\n", "            ", "start_node_ids", ".", "append", "(", "id", ")", "\n", "", "node_by_id", "[", "id", "]", "=", "row", "\n", "if", "in_max_rows", "is", "not", "None", "and", "len", "(", "node_by_id", ")", ">=", "in_max_rows", ":", "\n", "            ", "print", "(", "'reached max rows'", ",", "in_max_rows", ",", "'=> breaking'", ")", "\n", "break", "\n", "", "", "print", "(", "'len(node_by_id)'", ",", "len", "(", "node_by_id", ")", ")", "\n", "print", "(", "'len(start_node_ids)'", ",", "len", "(", "start_node_ids", ")", ")", "\n", "count_by_status", "=", "defaultdict", "(", "int", ")", "\n", "count_by_count", "=", "defaultdict", "(", "int", ")", "\n", "\n", "preprocessor", "=", "twitter_airlines", ".", "Preprocessor", "(", ")", "if", "not", "no_preprocess", "else", "NullPreprocessor", "(", ")", "\n", "\n", "for", "i", ",", "start_node_id", "in", "enumerate", "(", "start_node_ids", ")", ":", "\n", "        ", "conversation_texts", "=", "[", "]", "\n", "first_utterance", "=", "None", "\n", "is_valid", "=", "True", "\n", "node", "=", "node_by_id", "[", "start_node_id", "]", "\n", "cust_twitter_id", "=", "node", "[", "'author_id'", "]", "\n", "while", "True", ":", "\n", "            ", "text", "=", "node", "[", "'text'", "]", ".", "replace", "(", "'\\n'", ",", "' '", ")", "\n", "if", "node", "[", "'inbound'", "]", "==", "'True'", ":", "\n", "                ", "start_tok", "=", "'<cust__'", "\n", "end_tok", "=", "'__cust>'", "\n", "", "else", ":", "\n", "                ", "start_tok", "=", "'<rep__'", "\n", "end_tok", "=", "'__rep>'", "\n", "\n", "", "_valid", ",", "text", "=", "preprocessor", "(", "text", ",", "info_dict", "=", "{", "\n", "'target_company'", ":", "target_company", ",", "\n", "'cust_twitter_id'", ":", "cust_twitter_id", "\n", "}", ")", "\n", "if", "not", "_valid", ":", "\n", "                ", "is_valid", "=", "False", "\n", "", "text", "=", "start_tok", "+", "' '", "+", "text", "+", "' '", "+", "end_tok", "\n", "if", "first_utterance", "is", "None", ":", "\n", "                ", "first_utterance", "=", "text", "\n", "", "else", ":", "\n", "                ", "conversation_texts", ".", "append", "(", "text", ")", "\n", "", "response_id", "=", "next_by_prev", ".", "get", "(", "node", "[", "'tweet_id'", "]", ",", "None", ")", "\n", "if", "response_id", "is", "None", ":", "\n", "                ", "count_by_count", "[", "len", "(", "conversation_texts", ")", "+", "1", "]", "+=", "1", "\n", "if", "is_valid", ":", "\n", "                    ", "examples_writer", ".", "writerow", "(", "{", "\n", "'first_tweet_id'", ":", "start_node_id", ",", "\n", "'first_utterance'", ":", "first_utterance", ",", "\n", "'context'", ":", "' '", ".", "join", "(", "conversation_texts", ")", "\n", "}", ")", "\n", "count_by_status", "[", "'accept'", "]", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "count_by_status", "[", "'not_valid'", "]", "+=", "1", "\n", "", "break", "\n", "", "if", "response_id", "not", "in", "node_by_id", ":", "\n", "                ", "count_by_status", "[", "'response id not found'", "]", "+=", "1", "\n", "print", "(", "'warning: response_id'", ",", "response_id", ",", "'not found => skipping conversation'", ")", "\n", "break", "\n", "", "node", "=", "node_by_id", "[", "response_id", "]", "\n", "", "", "print", "(", "count_by_status", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.twitter_dataset_preprocess.run_aggregated": [[114, 121], ["open", "csv.DictWriter", "csv.DictWriter.writeheader", "os.path.expanduser", "print", "twitter_dataset_preprocess.run_for_company"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.twitter_dataset_preprocess.run_for_company"], ["", "def", "run_aggregated", "(", "in_csv_file", ",", "in_max_rows", ",", "out_examples", ",", "target_companies", ",", "no_preprocess", ")", ":", "\n", "    ", "with", "open", "(", "expand", "(", "out_examples", ")", ",", "'w'", ")", "as", "f_examples", ":", "\n", "        ", "examples_writer", "=", "csv", ".", "DictWriter", "(", "f_examples", ",", "fieldnames", "=", "[", "'first_tweet_id'", ",", "'first_utterance'", ",", "'context'", "]", ")", "\n", "examples_writer", ".", "writeheader", "(", ")", "\n", "for", "company", "in", "target_companies", ":", "\n", "            ", "print", "(", "company", ")", "\n", "run_for_company", "(", "in_csv_file", ",", "in_max_rows", ",", "examples_writer", ",", "company", ",", "no_preprocess", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.twitter_dataset_preprocess.run_by_company": [[122, 130], ["print", "out_examples_templ.format", "open", "csv.DictWriter", "csv.DictWriter.writeheader", "twitter_dataset_preprocess.run_for_company", "os.path.expanduser"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.datasets.twitter_dataset_preprocess.run_for_company"], ["", "", "", "def", "run_by_company", "(", "in_csv_file", ",", "in_max_rows", ",", "out_examples_templ", ",", "target_companies", ",", "no_preprocess", ")", ":", "\n", "    ", "for", "company", "in", "target_companies", ":", "\n", "        ", "print", "(", "company", ")", "\n", "out_examples", "=", "out_examples_templ", ".", "format", "(", "company", "=", "company", ")", "\n", "with", "open", "(", "expand", "(", "out_examples", ")", ",", "'w'", ")", "as", "f_examples", ":", "\n", "            ", "examples_writer", "=", "csv", ".", "DictWriter", "(", "f_examples", ",", "fieldnames", "=", "[", "'first_tweet_id'", ",", "'first_utterance'", ",", "'context'", "]", ")", "\n", "examples_writer", ".", "writeheader", "(", ")", "\n", "run_for_company", "(", "in_csv_file", ",", "in_max_rows", ",", "examples_writer", ",", "company", ",", "no_preprocess", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.encoder.Encoder.__init__": [[6, 18], ["torch.Module.__init__", "torch.LSTM", "torch.LSTM"], "methods", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.decoder.Decoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "word_emb_size", ",", "encoder_rnn_size", ",", "encoder_num_layers", ",", "dropout", "=", "0.5", ")", ":", "\n", "        ", "super", "(", "Encoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "encoder_rnn_size", "=", "encoder_rnn_size", "\n", "self", ".", "encoder_num_layers", "=", "encoder_num_layers", "\n", "# Create input dropout parameter", "\n", "self", ".", "rnn", "=", "nn", ".", "LSTM", "(", "input_size", "=", "word_emb_size", ",", "\n", "hidden_size", "=", "encoder_rnn_size", ",", "\n", "num_layers", "=", "encoder_num_layers", ",", "\n", "dropout", "=", "dropout", ",", "\n", "batch_first", "=", "True", ",", "\n", "bidirectional", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.encoder.Encoder.forward": [[19, 38], ["lengths.sort", "encoder_input.size", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "encoder.Encoder.rnn", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "perm_idx.sort", "torch.cat.view", "torch.cat.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "encoder_input", ",", "lengths", ")", ":", "\n", "        ", "\"\"\"\n        :param encoder_input: [batch_size, seq_len, emb_size] tensor\n        :return: context of input sentenses with shape of [batch_size, encoder_rnn_size]\n        \"\"\"", "\n", "lengths", ",", "perm_idx", "=", "lengths", ".", "sort", "(", "0", ",", "descending", "=", "True", ")", "\n", "encoder_input", "=", "encoder_input", "[", "perm_idx", "]", "\n", "[", "batch_size", ",", "seq_len", ",", "_", "]", "=", "encoder_input", ".", "size", "(", ")", "\n", "packed_words", "=", "torch", ".", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "\n", "encoder_input", ",", "lengths", ",", "True", ")", "\n", "# Unfold rnn with zero initial state and get its final state from the last layer", "\n", "rnn_out", ",", "(", "_", ",", "final_state", ")", "=", "self", ".", "rnn", "(", "packed_words", ",", "None", ")", "\n", "final_state", "=", "final_state", ".", "view", "(", "\n", "self", ".", "encoder_num_layers", ",", "2", ",", "batch_size", ",", "self", ".", "encoder_rnn_size", ")", "[", "-", "1", "]", "\n", "h_1", ",", "h_2", "=", "final_state", "[", "0", "]", ",", "final_state", "[", "1", "]", "\n", "final_state", "=", "torch", ".", "cat", "(", "[", "h_1", ",", "h_2", "]", ",", "1", ")", "\n", "_", ",", "unperm_idx", "=", "perm_idx", ".", "sort", "(", "0", ")", "\n", "final_state", "=", "final_state", "[", "unperm_idx", "]", "\n", "return", "final_state", "\n", "", "", ""]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.MultiviewEncoders.__init__": [[14, 49], ["torch.nn.Module.__init__", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.Embedding", "torch.nn.Embedding", "multiview_encoders.MultiviewEncoders.__init__.create_rnn"], "methods", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.decoder.Decoder.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "vocab_size", ",", "num_layers", ",", "embedding_size", ",", "lstm_hidden_size", ",", "word_dropout", ",", "dropout", ",", "\n", "start_idx", "=", "2", ",", "end_idx", "=", "3", ",", "pad_idx", "=", "0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "pad_idx", "=", "pad_idx", "\n", "self", ".", "start_idx", "=", "start_idx", "# for RNN autoencoder training", "\n", "self", ".", "end_idx", "=", "end_idx", "# for RNN autoencoder training", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "embedding_size", "=", "embedding_size", "\n", "self", ".", "lstm_hidden_size", "=", "lstm_hidden_size", "\n", "self", ".", "word_dropout", "=", "nn", ".", "Dropout", "(", "word_dropout", ")", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "crit", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "\n", "self", ".", "embedder", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "embedding_size", ")", "\n", "\n", "def", "create_rnn", "(", "embedding_size", ",", "bidirectional", "=", "True", ")", ":", "\n", "            ", "return", "nn", ".", "LSTM", "(", "\n", "embedding_size", ",", "\n", "lstm_hidden_size", ",", "\n", "dropout", "=", "dropout", ",", "\n", "num_layers", "=", "num_layers", ",", "\n", "bidirectional", "=", "bidirectional", ",", "\n", "batch_first", "=", "True", "\n", ")", "\n", "\n", "", "self", ".", "view1_word_rnn", "=", "create_rnn", "(", "embedding_size", ")", "\n", "self", ".", "view2_word_rnn", "=", "create_rnn", "(", "embedding_size", ")", "\n", "self", ".", "view2_sent_rnn", "=", "create_rnn", "(", "2", "*", "lstm_hidden_size", ")", "\n", "\n", "self", ".", "ae_decoder", "=", "create_rnn", "(", "embedding_size", "+", "2", "*", "lstm_hidden_size", ",", "bidirectional", "=", "False", ")", "\n", "self", ".", "qt_context", "=", "create_rnn", "(", "embedding_size", ")", "\n", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "lstm_hidden_size", ",", "vocab_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.MultiviewEncoders.get_encoder": [[50, 58], ["None"], "methods", ["None"], ["", "def", "get_encoder", "(", "self", ",", "encoder", ")", ":", "\n", "        ", "return", "{", "\n", "'v1'", ":", "self", ".", "view1_word_rnn", ",", "\n", "'v2'", ":", "self", ".", "view2_word_rnn", ",", "\n", "'v2sent'", ":", "self", ".", "view2_sent_rnn", ",", "\n", "'ae_decoder'", ":", "self", ".", "ae_decoder", ",", "\n", "'qt'", ":", "self", ".", "qt_context", "\n", "}", "[", "encoder", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.MultiviewEncoders.construct_from_embeddings": [[59, 76], ["cls", "torch.nn.Embedding.from_pretrained", "torch.nn.Embedding.from_pretrained"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "construct_from_embeddings", "(", "\n", "cls", ",", "embeddings", ",", "num_layers", ",", "embedding_size", ",", "lstm_hidden_size", ",", "word_dropout", ",", "dropout", ",", "\n", "vocab_size", ",", "start_idx", "=", "2", ",", "end_idx", "=", "3", ",", "pad_idx", "=", "0", ")", ":", "\n", "        ", "model", "=", "cls", "(", "\n", "num_layers", "=", "num_layers", ",", "\n", "embedding_size", "=", "embedding_size", ",", "\n", "lstm_hidden_size", "=", "lstm_hidden_size", ",", "\n", "word_dropout", "=", "word_dropout", ",", "\n", "dropout", "=", "dropout", ",", "\n", "start_idx", "=", "start_idx", ",", "\n", "end_idx", "=", "end_idx", ",", "\n", "pad_idx", "=", "pad_idx", ",", "\n", "vocab_size", "=", "vocab_size", "\n", ")", "\n", "model", ".", "embedder", "=", "nn", ".", "Embedding", ".", "from_pretrained", "(", "embeddings", ",", "freeze", "=", "False", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.MultiviewEncoders.decode": [[77, 105], ["model.utils.pad_sentences", "multiview_encoders.MultiviewEncoders.embedder", "multiview_encoders.MultiviewEncoders.word_dropout", "torch.cat.size", "torch.cat().view", "torch.cat().view", "torch.cat", "torch.cat", "rnn", "rnn_out.contiguous().view.contiguous().view.contiguous().view", "multiview_encoders.MultiviewEncoders.fc", "result.view.view.view", "torch.cat", "torch.cat", "rnn_out.contiguous().view.contiguous().view.contiguous"], "methods", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.utils.pad_sentences"], ["", "def", "decode", "(", "self", ",", "decoder_input", ",", "latent_z", ")", ":", "\n", "        ", "\"\"\"\n        decode state into word indices\n\n        :param decoder_input: list of lists of indices\n        :param latent_z: sequence context with shape of [batch_size, latent_z_size]\n\n        :return: unnormalized logits of sentense words distribution probabilities\n                     with shape of [batch_size, seq_len, word_vocab_size]\n\n        \"\"\"", "\n", "# this pad_sentences call will a start_idx token at the start of each sequence", "\n", "# so, we are feeding in the previous token each time, in order to generate the", "\n", "# next token", "\n", "padded", ",", "lengths", "=", "pad_sentences", "(", "decoder_input", ",", "pad_idx", "=", "self", ".", "pad_idx", ",", "lpad", "=", "self", ".", "start_idx", ")", "\n", "embeddings", "=", "self", ".", "embedder", "(", "padded", ")", "\n", "embeddings", "=", "self", ".", "word_dropout", "(", "embeddings", ")", "\n", "[", "batch_size", ",", "seq_len", ",", "_", "]", "=", "embeddings", ".", "size", "(", ")", "\n", "# decoder rnn is conditioned on context via additional bias = W_cond * z", "\n", "# to every input token", "\n", "latent_z", "=", "t", ".", "cat", "(", "[", "latent_z", "]", "*", "seq_len", ",", "1", ")", ".", "view", "(", "batch_size", ",", "seq_len", ",", "-", "1", ")", "\n", "embeddings", "=", "t", ".", "cat", "(", "[", "embeddings", ",", "latent_z", "]", ",", "2", ")", "\n", "rnn", "=", "self", ".", "ae_decoder", "\n", "rnn_out", ",", "_", "=", "rnn", "(", "embeddings", ")", "\n", "rnn_out", "=", "rnn_out", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", "*", "seq_len", ",", "self", ".", "lstm_hidden_size", ")", "\n", "result", "=", "self", ".", "fc", "(", "rnn_out", ")", "\n", "result", "=", "result", ".", "view", "(", "batch_size", ",", "seq_len", ",", "self", ".", "vocab_size", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.MultiviewEncoders.forward": [[106, 131], ["len", "model.utils.pad_sentences", "multiview_encoders.MultiviewEncoders.embedder", "multiview_encoders.MultiviewEncoders.word_dropout", "lengths.sort", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "multiview_encoders.MultiviewEncoders.get_encoder", "multiview_encoders.MultiviewEncoders.", "perm_idx.sort", "[].transpose().contiguous().view", "multiview_encoders.MultiviewEncoders.hierarchical_forward", "[].transpose().contiguous", "[].transpose", "[].transpose().contiguous().view.view"], "methods", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.utils.pad_sentences", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.MultiviewEncoders.get_encoder", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.MultiviewEncoders.hierarchical_forward"], ["", "def", "forward", "(", "self", ",", "input", ",", "encoder", ")", ":", "\n", "        ", "\"\"\"\n        Encode an input into a vector representation\n        params:\n            input : word indices\n            encoder: [pt1|pt2|v1|v2]\n        \"\"\"", "\n", "if", "encoder", "==", "'v2'", ":", "\n", "            ", "return", "self", ".", "hierarchical_forward", "(", "input", ")", "\n", "\n", "", "batch_size", "=", "len", "(", "input", ")", "\n", "padded", ",", "lengths", "=", "pad_sentences", "(", "input", ",", "pad_idx", "=", "self", ".", "pad_idx", ")", "\n", "embeddings", "=", "self", ".", "embedder", "(", "padded", ")", "\n", "embeddings", "=", "self", ".", "word_dropout", "(", "embeddings", ")", "\n", "lengths", ",", "perm_idx", "=", "lengths", ".", "sort", "(", "0", ",", "descending", "=", "True", ")", "\n", "embeddings", "=", "embeddings", "[", "perm_idx", "]", "\n", "packed", "=", "torch", ".", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "embeddings", ",", "lengths", ",", "batch_first", "=", "True", ")", "\n", "rnn", "=", "self", ".", "get_encoder", "(", "encoder", ")", "\n", "_", ",", "(", "_", ",", "final_state", ")", "=", "rnn", "(", "packed", ",", "None", ")", "\n", "_", ",", "unperm_idx", "=", "perm_idx", ".", "sort", "(", "0", ")", "\n", "final_state", "=", "final_state", "[", ":", ",", "unperm_idx", "]", "\n", "final_state", "=", "final_state", ".", "view", "(", "self", ".", "num_layers", ",", "2", ",", "batch_size", ",", "self", ".", "lstm_hidden_size", ")", "[", "-", "1", "]", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "2", "*", "self", ".", "lstm_hidden_size", ")", "\n", "return", "final_state", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.MultiviewEncoders.hierarchical_forward": [[132, 160], ["len", "model.utils.pad_paragraphs", "multiview_encoders.MultiviewEncoders.embedder", "multiview_encoders.MultiviewEncoders.word_dropout", "word_lens.sort", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "multiview_encoders.MultiviewEncoders.view2_word_rnn", "perm_idx.sort", "[].transpose().contiguous().view", "sent_lens.sort", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "multiview_encoders.MultiviewEncoders.view2_sent_rnn", "sent_perm_idx.sort", "[].transpose().contiguous().view", "[].transpose().contiguous", "[].transpose().contiguous", "[].transpose", "[].transpose", "[].transpose().contiguous().view.view", "[].transpose().contiguous().view.view"], "methods", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.utils.pad_paragraphs"], ["", "def", "hierarchical_forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "input", ")", "\n", "padded", ",", "word_lens", ",", "sent_lens", ",", "max_sent_len", "=", "pad_paragraphs", "(", "input", ",", "pad_idx", "=", "self", ".", "pad_idx", ")", "\n", "embeddings", "=", "self", ".", "embedder", "(", "padded", ")", "\n", "embeddings", "=", "self", ".", "word_dropout", "(", "embeddings", ")", "\n", "word_lens", ",", "perm_idx", "=", "word_lens", ".", "sort", "(", "0", ",", "descending", "=", "True", ")", "\n", "embeddings", "=", "embeddings", "[", "perm_idx", "]", "\n", "packed", "=", "torch", ".", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "\n", "embeddings", ",", "word_lens", ",", "batch_first", "=", "True", ")", "\n", "_", ",", "(", "_", ",", "final_word_state", ")", "=", "self", ".", "view2_word_rnn", "(", "packed", ",", "None", ")", "\n", "_", ",", "unperm_idx", "=", "perm_idx", ".", "sort", "(", "0", ")", "\n", "final_word_state", "=", "final_word_state", "[", ":", ",", "unperm_idx", "]", "\n", "final_word_state", "=", "final_word_state", ".", "view", "(", "\n", "self", ".", "num_layers", ",", "2", ",", "batch_size", "*", "max_sent_len", ",", "self", ".", "lstm_hidden_size", ")", "[", "-", "1", "]", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "max_sent_len", ",", "2", "*", "self", ".", "lstm_hidden_size", ")", "\n", "\n", "sent_lens", ",", "sent_perm_idx", "=", "sent_lens", ".", "sort", "(", "0", ",", "descending", "=", "True", ")", "\n", "sent_embeddings", "=", "final_word_state", "[", "sent_perm_idx", "]", "\n", "sent_packed", "=", "torch", ".", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "sent_embeddings", ",", "sent_lens", ",", "batch_first", "=", "True", ")", "\n", "_", ",", "(", "_", ",", "final_sent_state", ")", "=", "self", ".", "view2_sent_rnn", "(", "sent_packed", ",", "None", ")", "\n", "_", ",", "sent_unperm_idx", "=", "sent_perm_idx", ".", "sort", "(", "0", ")", "\n", "final_sent_state", "=", "final_sent_state", "[", ":", ",", "sent_unperm_idx", "]", "\n", "final_sent_state", "=", "final_sent_state", ".", "view", "(", "\n", "self", ".", "num_layers", ",", "2", ",", "batch_size", ",", "self", ".", "lstm_hidden_size", ")", "[", "-", "1", "]", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "2", "*", "self", ".", "lstm_hidden_size", ")", "\n", "return", "final_sent_state", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.MultiviewEncoders.qt_loss": [[161, 174], ["scores.size", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "targets.to.to.to", "multiview_encoders.MultiviewEncoders.crit", "scores.max", "examples_correct.float().mean().item", "target_view_state.transpose", "numpy.arange", "examples_correct.float().mean", "examples_correct.float"], "methods", ["None"], ["", "def", "qt_loss", "(", "self", ",", "target_view_state", ",", "input_view_state", ")", ":", "\n", "        ", "\"\"\"\n        pick out the correct example in the target_view, based on the corresponding input_view\n        \"\"\"", "\n", "scores", "=", "input_view_state", "@", "target_view_state", ".", "transpose", "(", "0", ",", "1", ")", "\n", "batch_size", "=", "scores", ".", "size", "(", "0", ")", "\n", "targets", "=", "torch", ".", "from_numpy", "(", "np", ".", "arange", "(", "batch_size", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "targets", "=", "targets", ".", "to", "(", "scores", ".", "device", ")", "\n", "loss", "=", "self", ".", "crit", "(", "scores", ",", "targets", ")", "\n", "_", ",", "argmax", "=", "scores", ".", "max", "(", "dim", "=", "-", "1", ")", "\n", "examples_correct", "=", "(", "argmax", "==", "targets", ")", "\n", "acc", "=", "examples_correct", ".", "float", "(", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "return", "loss", ",", "acc", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.MultiviewEncoders.reconst_loss": [[175, 197], ["reconst.size", "model.utils.pad_sentences", "len", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "reconst.view", "padded.view", "torch.nn.CrossEntropyLoss.", "reconst.max", "correct.float().mean().item", "correct.float().mean", "correct.float"], "methods", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.utils.pad_sentences"], ["", "def", "reconst_loss", "(", "self", ",", "gnd_utts", ",", "reconst", ")", ":", "\n", "        ", "\"\"\"\n        gnd_utts is a list of lists of indices (the outer list should be a minibatch)\n        reconst is a tensor with the logits from a decoder [batchsize][seqlen][vocabsize]\n        (should not have passed through softmax)\n\n        reconst should be one token longer than the inputs in gnd_utts. the additional\n        token to be predicted is the end_idx token\n        \"\"\"", "\n", "batch_size", ",", "seq_len", ",", "vocab_size", "=", "reconst", ".", "size", "(", ")", "\n", "loss", "=", "0", "\n", "# this pad_sentences call will add token self.end_idx at the end of each sequence", "\n", "padded", ",", "lengths", "=", "pad_sentences", "(", "gnd_utts", ",", "pad_idx", "=", "self", ".", "pad_idx", ",", "rpad", "=", "self", ".", "end_idx", ")", "\n", "batch_size", "=", "len", "(", "lengths", ")", "\n", "crit", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "reconst_flat", "=", "reconst", ".", "view", "(", "batch_size", "*", "seq_len", ",", "vocab_size", ")", "\n", "padded_flat", "=", "padded", ".", "view", "(", "batch_size", "*", "seq_len", ")", "\n", "loss", "+=", "crit", "(", "reconst_flat", ",", "padded_flat", ")", "\n", "_", ",", "argmax", "=", "reconst", ".", "max", "(", "dim", "=", "-", "1", ")", "\n", "correct", "=", "(", "argmax", "==", "padded", ")", "\n", "acc", "=", "correct", ".", "float", "(", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "return", "loss", ",", "acc", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.from_embeddings": [[199, 239], ["len", "print", "open", "numpy.sqrt", "print", "print", "multiview_encoders.MultiviewEncoders.construct_from_embeddings", "MultiviewEncoders.construct_from_embeddings.to", "line.strip().split", "len", "float", "word.lower", "pretrained_list.append", "numpy.random.uniform", "pretrained_list.append", "torch.FloatTensor", "torch.FloatTensor", "line.strip", "len", "numpy.array", "word.lower"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.MultiviewEncoders.construct_from_embeddings"], ["", "", "def", "from_embeddings", "(", "glove_path", ",", "id_to_token", ",", "token_to_id", ")", ":", "\n", "    ", "vocab_size", "=", "len", "(", "token_to_id", ")", "\n", "\n", "# Load pre-trained GloVe vectors", "\n", "pretrained", "=", "{", "}", "\n", "word_emb_size", "=", "0", "\n", "print", "(", "'loading glove'", ")", "\n", "for", "line", "in", "open", "(", "glove_path", ")", ":", "\n", "        ", "parts", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "if", "len", "(", "parts", ")", "%", "100", "!=", "1", ":", "\n", "            ", "continue", "\n", "", "word", "=", "parts", "[", "0", "]", "\n", "if", "word", "not", "in", "token_to_id", ":", "\n", "            ", "continue", "\n", "", "vector", "=", "[", "float", "(", "v", ")", "for", "v", "in", "parts", "[", "1", ":", "]", "]", "\n", "pretrained", "[", "word", "]", "=", "vector", "\n", "word_emb_size", "=", "len", "(", "vector", ")", "\n", "", "pretrained_list", "=", "[", "]", "\n", "scale", "=", "np", ".", "sqrt", "(", "3.0", "/", "word_emb_size", ")", "\n", "print", "(", "'loading oov'", ")", "\n", "for", "word", "in", "token_to_id", ":", "\n", "# apply lower() because all GloVe vectors are for lowercase words", "\n", "        ", "if", "word", ".", "lower", "(", ")", "in", "pretrained", ":", "\n", "            ", "pretrained_list", ".", "append", "(", "np", ".", "array", "(", "pretrained", "[", "word", ".", "lower", "(", ")", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "random_vector", "=", "np", ".", "random", ".", "uniform", "(", "-", "scale", ",", "scale", ",", "[", "word_emb_size", "]", ")", "\n", "pretrained_list", ".", "append", "(", "random_vector", ")", "\n", "\n", "", "", "print", "(", "'instantiating model'", ")", "\n", "model", "=", "MultiviewEncoders", ".", "construct_from_embeddings", "(", "\n", "embeddings", "=", "torch", ".", "FloatTensor", "(", "pretrained_list", ")", ",", "\n", "num_layers", "=", "train", ".", "LSTM_LAYER", ",", "\n", "embedding_size", "=", "word_emb_size", ",", "\n", "lstm_hidden_size", "=", "train", ".", "LSTM_HIDDEN", ",", "\n", "word_dropout", "=", "train", ".", "WORD_DROPOUT_RATE", ",", "\n", "dropout", "=", "train", ".", "DROPOUT_RATE", ",", "\n", "vocab_size", "=", "vocab_size", "\n", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "return", "id_to_token", ",", "token_to_id", ",", "vocab_size", ",", "word_emb_size", ",", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.load_model": [[241, 262], ["len", "multiview_encoders.MultiviewEncoders", "MultiviewEncoders.to", "MultiviewEncoders.load_state_dict", "open", "torch.load", "torch.load", "enumerate"], "function", ["None"], ["", "def", "load_model", "(", "model_path", ")", ":", "\n", "    ", "with", "open", "(", "model_path", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "state", "=", "torch", ".", "load", "(", "f", ")", "\n", "\n", "", "id_to_token", "=", "state", "[", "'id_to_token'", "]", "\n", "word_emb_size", "=", "state", "[", "'word_emb_size'", "]", "\n", "\n", "token_to_id", "=", "{", "token", ":", "id", "for", "id", ",", "token", "in", "enumerate", "(", "id_to_token", ")", "}", "\n", "vocab_size", "=", "len", "(", "id_to_token", ")", "\n", "\n", "mvc_encoder", "=", "MultiviewEncoders", "(", "\n", "num_layers", "=", "train", ".", "LSTM_LAYER", ",", "\n", "embedding_size", "=", "word_emb_size", ",", "\n", "lstm_hidden_size", "=", "train", ".", "LSTM_HIDDEN", ",", "\n", "word_dropout", "=", "train", ".", "WORD_DROPOUT_RATE", ",", "\n", "dropout", "=", "train", ".", "DROPOUT_RATE", ",", "\n", "vocab_size", "=", "vocab_size", "\n", ")", "\n", "mvc_encoder", ".", "to", "(", "device", ")", "\n", "mvc_encoder", ".", "load_state_dict", "(", "state", "[", "'model_state'", "]", ")", "\n", "return", "id_to_token", ",", "token_to_id", ",", "vocab_size", ",", "word_emb_size", ",", "mvc_encoder", "\n", "", ""]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.decoder.Decoder.__init__": [[6, 19], ["torch.Module.__init__", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.decoder.Decoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "latent_z_size", ",", "word_emb_size", ",", "word_vocab_size", ",", "decoder_rnn_size", ",", "\n", "decoder_num_layers", ",", "dropout", "=", "0.5", ")", ":", "\n", "        ", "super", "(", "Decoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "latent_z_size", "=", "latent_z_size", "\n", "self", ".", "word_vocab_size", "=", "word_vocab_size", "\n", "self", ".", "decoder_rnn_size", "=", "decoder_rnn_size", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "rnn", "=", "nn", ".", "LSTM", "(", "input_size", "=", "latent_z_size", "+", "word_emb_size", ",", "\n", "hidden_size", "=", "decoder_rnn_size", ",", "\n", "num_layers", "=", "decoder_num_layers", ",", "\n", "dropout", "=", "dropout", ",", "\n", "batch_first", "=", "True", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "decoder_rnn_size", ",", "word_vocab_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.decoder.Decoder.forward": [[20, 40], ["torch.cat.size", "torch.cat().view", "torch.cat().view", "torch.cat", "torch.cat", "decoder.Decoder.rnn", "rnn_out.contiguous().view.contiguous().view.contiguous().view", "decoder.Decoder.fc", "result.view.view.view", "torch.cat", "torch.cat", "rnn_out.contiguous().view.contiguous().view.contiguous"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "decoder_input", ",", "latent_z", ")", ":", "\n", "        ", "\"\"\"\n        :param decoder_input: tensor with shape of [batch_size, seq_len, emb_size]\n        :param latent_z: sequence context with shape of [batch_size, latent_z_size]\n        :return: unnormalized logits of sentense words distribution probabilities\n                     with shape of [batch_size, seq_len, word_vocab_size]\n\n        TODO: add padding support\n        \"\"\"", "\n", "\n", "[", "batch_size", ",", "seq_len", ",", "_", "]", "=", "decoder_input", ".", "size", "(", ")", "\n", "# decoder rnn is conditioned on context via additional bias = W_cond * z to every input", "\n", "# token", "\n", "latent_z", "=", "t", ".", "cat", "(", "[", "latent_z", "]", "*", "seq_len", ",", "1", ")", ".", "view", "(", "batch_size", ",", "seq_len", ",", "self", ".", "latent_z_size", ")", "\n", "decoder_input", "=", "t", ".", "cat", "(", "[", "decoder_input", ",", "latent_z", "]", ",", "2", ")", "\n", "rnn_out", ",", "_", "=", "self", ".", "rnn", "(", "decoder_input", ")", "\n", "rnn_out", "=", "rnn_out", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "decoder_rnn_size", ")", "\n", "result", "=", "self", ".", "fc", "(", "rnn_out", ")", "\n", "result", "=", "result", ".", "view", "(", "batch_size", ",", "seq_len", ",", "self", ".", "word_vocab_size", ")", "\n", "return", "result", "\n", "", "", ""]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.utils.pad_sentences": [[7, 32], ["range", "range", "len", "max", "len", "lengths.append", "torch.LongTensor().to", "torch.LongTensor().to", "len", "sentences.append", "sentences.append", "len", "len", "torch.LongTensor", "torch.LongTensor", "len"], "function", ["None"], ["def", "pad_sentences", "(", "\n", "sents", ",", "lpad", "=", "None", ",", "rpad", "=", "None", ",", "reverse", "=", "False", ",", "pad_idx", "=", "0", ",", "\n", "max_sent_len", "=", "100", ")", ":", "\n", "    ", "sentences", "=", "[", "]", "\n", "max_len", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "sents", ")", ")", ":", "\n", "        ", "if", "len", "(", "sents", "[", "i", "]", ")", ">", "max_sent_len", ":", "\n", "            ", "sentences", ".", "append", "(", "sents", "[", "i", "]", "[", ":", "max_sent_len", "]", ")", "\n", "", "else", ":", "\n", "            ", "sentences", ".", "append", "(", "sents", "[", "i", "]", ")", "\n", "", "max_len", "=", "max", "(", "max_len", ",", "len", "(", "sentences", "[", "i", "]", ")", ")", "\n", "", "if", "reverse", ":", "\n", "        ", "sentences", "=", "[", "sentence", "[", ":", ":", "-", "1", "]", "for", "sentence", "in", "sentences", "]", "\n", "", "if", "lpad", "is", "not", "None", ":", "\n", "        ", "sentences", "=", "[", "[", "lpad", "]", "+", "sentence", "for", "sentence", "in", "sentences", "]", "\n", "max_len", "+=", "1", "\n", "", "if", "rpad", "is", "not", "None", ":", "\n", "        ", "sentences", "=", "[", "sentence", "+", "[", "rpad", "]", "for", "sentence", "in", "sentences", "]", "\n", "max_len", "+=", "1", "\n", "", "lengths", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "sentences", ")", ")", ":", "\n", "        ", "lengths", ".", "append", "(", "len", "(", "sentences", "[", "i", "]", ")", ")", "\n", "sentences", "[", "i", "]", "=", "sentences", "[", "i", "]", "+", "[", "pad_idx", "]", "*", "(", "max_len", "-", "len", "(", "sentences", "[", "i", "]", ")", ")", "\n", "", "return", "(", "torch", ".", "LongTensor", "(", "sentences", ")", ".", "to", "(", "device", ")", ",", "\n", "torch", ".", "LongTensor", "(", "lengths", ")", ".", "to", "(", "device", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.utils.pad_paragraphs": [[34, 47], ["utils.pad_sentences", "max", "lengths.append", "range", "torch.LongTensor().to", "len", "sentences.append", "len", "sentences.append", "len", "torch.LongTensor"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.utils.pad_sentences"], ["", "def", "pad_paragraphs", "(", "paras", ",", "pad_idx", "=", "0", ")", ":", "\n", "    ", "sentences", ",", "lengths", "=", "[", "]", ",", "[", "]", "\n", "max_len", "=", "0", "\n", "for", "para", "in", "paras", ":", "\n", "        ", "max_len", "=", "max", "(", "max_len", ",", "len", "(", "para", ")", ")", "\n", "", "for", "para", "in", "paras", ":", "\n", "        ", "for", "sent", "in", "para", ":", "\n", "            ", "sentences", ".", "append", "(", "sent", "[", ":", "]", ")", "\n", "", "lengths", ".", "append", "(", "len", "(", "para", ")", ")", "\n", "for", "i", "in", "range", "(", "max_len", "-", "len", "(", "para", ")", ")", ":", "\n", "            ", "sentences", ".", "append", "(", "[", "pad_idx", "]", ")", "\n", "", "", "ret_sents", ",", "sent_lens", "=", "pad_sentences", "(", "sentences", ",", "pad_idx", "=", "pad_idx", ")", "\n", "return", "ret_sents", ",", "sent_lens", ",", "torch", ".", "LongTensor", "(", "lengths", ")", ".", "to", "(", "device", ")", ",", "max_len", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.utils.euclidean_metric": [[49, 56], ["a.unsqueeze().expand.unsqueeze().expand", "b.unsqueeze().expand.unsqueeze().expand", "a.unsqueeze().expand.unsqueeze", "b.unsqueeze().expand.unsqueeze"], "function", ["None"], ["", "def", "euclidean_metric", "(", "a", ",", "b", ")", ":", "\n", "    ", "n", "=", "a", ".", "shape", "[", "0", "]", "\n", "m", "=", "b", ".", "shape", "[", "0", "]", "\n", "a", "=", "a", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "n", ",", "m", ",", "-", "1", ")", "\n", "b", "=", "b", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "n", ",", "m", ",", "-", "1", ")", "\n", "logits", "=", "-", "(", "(", "a", "-", "b", ")", "**", "2", ")", ".", "sum", "(", "dim", "=", "2", ")", "\n", "return", "logits", "\n", "", ""]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.tests.test_samplers.test_categories_sampler": [[7, 31], ["numpy.random.seed", "numpy.random.choice", "print", "samplers.CategoriesSampler", "enumerate", "print", "collections.defaultdict", "print", "print", "collections.defaultdict.values", "classes.append", "len"], "function", ["None"], ["def", "test_categories_sampler", "(", ")", ":", "\n", "    ", "N", "=", "50", "\n", "n_batch", "=", "5", "\n", "n_cls", "=", "3", "\n", "n_ins", "=", "4", "\n", "\n", "np", ".", "random", ".", "seed", "(", "123", ")", "\n", "labels", "=", "np", ".", "random", ".", "choice", "(", "5", ",", "N", ",", "replace", "=", "True", ")", "\n", "print", "(", "'labels'", ",", "labels", ")", "\n", "\n", "sampler", "=", "samplers", ".", "CategoriesSampler", "(", "labels", ",", "n_batch", ",", "n_cls", ",", "n_ins", ")", "\n", "for", "b", ",", "batch", "in", "enumerate", "(", "sampler", ")", ":", "\n", "        ", "print", "(", "'batch'", ",", "batch", ")", "\n", "classes", "=", "[", "]", "\n", "count_by_class", "=", "defaultdict", "(", "int", ")", "\n", "for", "i", "in", "batch", ":", "\n", "            ", "classes", ".", "append", "(", "labels", "[", "i", "]", ")", "\n", "count_by_class", "[", "labels", "[", "i", "]", "]", "+=", "1", "\n", "", "print", "(", "'classes'", ",", "classes", ")", "\n", "print", "(", "'count_by_class'", ",", "count_by_class", ")", "\n", "assert", "len", "(", "count_by_class", ")", "==", "n_cls", "\n", "for", "v", "in", "count_by_class", ".", "values", "(", ")", ":", "\n", "            ", "assert", "v", "==", "n_ins", "\n", "", "", "assert", "b", "==", "n_batch", "-", "1", "\n", "", ""]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.tests.test_multiview_encoders.test_ae_decoder": [[7, 47], ["print", "print", "print", "print", "print", "print", "range", "torch.rand", "print", "print", "model.multiview_encoders.MultiviewEncoders", "print", "range", "torch.from_numpy", "_input_idxes.size", "input_idxes.append", "torch.rand.size", "torch.no_grad", "model.multiview_encoders.MultiviewEncoders.decode", "encoders.decode.size", "numpy.random.choice", "[].item", "torch.no_grad", "model.multiview_encoders.MultiviewEncoders.decode", "range"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.MultiviewEncoders.decode", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.MultiviewEncoders.decode"], ["def", "test_ae_decoder", "(", ")", ":", "\n", "    ", "dropout", "=", "word_dropout", "=", "0", "\n", "embedding_size", "=", "15", "\n", "num_layers", "=", "1", "\n", "vocab_size", "=", "13", "\n", "lstm_hidden_size", "=", "11", "\n", "\n", "batch_size", "=", "5", "\n", "seq_len", "=", "6", "\n", "\n", "print", "(", "'vocab_size'", ",", "vocab_size", ")", "\n", "print", "(", "'batch_size'", ",", "batch_size", ")", "\n", "print", "(", "'seq_len'", ",", "seq_len", ")", "\n", "print", "(", "'lstm_hidden_size'", ",", "lstm_hidden_size", ")", "\n", "\n", "_input_idxes", "=", "torch", ".", "from_numpy", "(", "np", ".", "random", ".", "choice", "(", "\n", "vocab_size", "-", "1", ",", "(", "batch_size", ",", "seq_len", ")", ",", "replace", "=", "True", ")", ")", "+", "1", "\n", "print", "(", "'_input_idxes'", ",", "_input_idxes", ")", "\n", "print", "(", "'_input_idxes.size()'", ",", "_input_idxes", ".", "size", "(", ")", ")", "\n", "input_idxes", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "idxes", "=", "[", "_input_idxes", "[", "b", "]", "[", "i", "]", ".", "item", "(", ")", "for", "i", "in", "range", "(", "seq_len", ")", "]", "\n", "input_idxes", ".", "append", "(", "idxes", ")", "\n", "", "latent_z", "=", "torch", ".", "rand", "(", "(", "batch_size", ",", "lstm_hidden_size", "*", "2", ")", ")", "\n", "print", "(", "'latent_z.size()'", ",", "latent_z", ".", "size", "(", ")", ")", "\n", "print", "(", "'input_idxes'", ",", "input_idxes", ")", "\n", "\n", "encoders", "=", "MultiviewEncoders", "(", "vocab_size", ",", "num_layers", ",", "embedding_size", ",", "lstm_hidden_size", ",", "\n", "word_dropout", ",", "dropout", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "logits", "=", "encoders", ".", "decode", "(", "decoder_input", "=", "input_idxes", ",", "latent_z", "=", "latent_z", ")", "\n", "", "print", "(", "'logits.size()'", ",", "logits", ".", "size", "(", ")", ")", "\n", "\n", "for", "n", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "_input_idxes", "=", "input_idxes", "[", "n", ":", "n", "+", "1", "]", "\n", "_latent_z", "=", "latent_z", "[", "n", ":", "n", "+", "1", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "_logits", "=", "encoders", ".", "decode", "(", "decoder_input", "=", "_input_idxes", ",", "latent_z", "=", "_latent_z", ")", "\n", "", "diff", "=", "(", "logits", "[", "n", "]", "-", "_logits", ")", ".", "abs", "(", ")", ".", "max", "(", ")", ".", "item", "(", ")", "\n", "assert", "diff", "<", "1e-7", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.tests.test_multiview_encoders.test_reconst_loss": [[49, 94], ["print", "print", "print", "print", "print", "print", "range", "model.multiview_encoders.MultiviewEncoders", "torch.zeros", "range", "torch.zeros.log", "print", "print", "print", "probs.log.max", "print", "model.multiview_encoders.MultiviewEncoders.reconst_loss", "loss.item.item", "print", "torch.from_numpy", "_input_idxes.size", "input_idxes.append", "enumerate", "probs.log.sum", "numpy.random.choice", "[].item", "probs.log.min", "probs.log.max", "range"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.model.multiview_encoders.MultiviewEncoders.reconst_loss"], ["", "", "def", "test_reconst_loss", "(", ")", ":", "\n", "    ", "dropout", "=", "word_dropout", "=", "0", "\n", "embedding_size", "=", "15", "\n", "num_layers", "=", "1", "\n", "lstm_hidden_size", "=", "11", "\n", "\n", "vocab_size", "=", "13", "\n", "batch_size", "=", "5", "\n", "seq_len", "=", "6", "\n", "\n", "print", "(", "'vocab_size'", ",", "vocab_size", ")", "\n", "print", "(", "'batch_size'", ",", "batch_size", ")", "\n", "print", "(", "'seq_len'", ",", "seq_len", ")", "\n", "print", "(", "'lstm_hidden_size'", ",", "lstm_hidden_size", ")", "\n", "\n", "_input_idxes", "=", "torch", ".", "from_numpy", "(", "np", ".", "random", ".", "choice", "(", "\n", "vocab_size", "-", "1", ",", "(", "batch_size", ",", "seq_len", ")", ",", "replace", "=", "True", ")", ")", "+", "1", "\n", "print", "(", "'_input_idxes'", ",", "_input_idxes", ")", "\n", "print", "(", "'_input_idxes.size()'", ",", "_input_idxes", ".", "size", "(", ")", ")", "\n", "input_idxes", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "idxes", "=", "[", "_input_idxes", "[", "b", "]", "[", "i", "]", ".", "item", "(", ")", "for", "i", "in", "range", "(", "seq_len", ")", "]", "\n", "input_idxes", ".", "append", "(", "idxes", ")", "\n", "\n", "", "encoders", "=", "MultiviewEncoders", "(", "vocab_size", ",", "num_layers", ",", "embedding_size", ",", "lstm_hidden_size", ",", "\n", "word_dropout", ",", "dropout", ")", "\n", "\n", "probs", "=", "torch", ".", "zeros", "(", "batch_size", ",", "seq_len", "+", "1", ",", "vocab_size", ")", "\n", "probs", "[", ":", ",", "seq_len", ",", "encoders", ".", "end_idx", "]", "=", "1", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "for", "i", ",", "idx", "in", "enumerate", "(", "input_idxes", "[", "b", "]", ")", ":", "\n", "            ", "probs", "[", "b", ",", "i", ",", "idx", "]", "=", "1", "\n", "", "", "logits", "=", "probs", ".", "log", "(", ")", "\n", "print", "(", "'logits.sum(dim=-1)'", ",", "logits", ".", "sum", "(", "dim", "=", "-", "1", ")", ")", "\n", "print", "(", "'logits.min(dim=-1)'", ",", "logits", ".", "min", "(", "dim", "=", "-", "1", ")", "[", "0", "]", ")", "\n", "print", "(", "'logits.max(dim=-1)'", ",", "logits", ".", "max", "(", "dim", "=", "-", "1", ")", "[", "0", "]", ")", "\n", "_", ",", "logits_max", "=", "logits", ".", "max", "(", "dim", "=", "-", "1", ")", "\n", "print", "(", "'logits_max'", ",", "logits_max", ")", "\n", "assert", "(", "logits_max", "[", ":", ",", ":", "seq_len", "]", "==", "_input_idxes", ")", ".", "all", "(", ")", "\n", "\n", "loss", ",", "acc", "=", "encoders", ".", "reconst_loss", "(", "input_idxes", ",", "logits", ")", "\n", "loss", "=", "loss", ".", "item", "(", ")", "\n", "print", "(", "'loss'", ",", "loss", ",", "'acc'", ",", "acc", ")", "\n", "assert", "acc", "==", "1.0", "\n", "assert", "loss", "==", "0.0", "\n", "", ""]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_precision": [[10, 34], ["gnd_assignments.size", "range", "len", "len", "pred_assignments.size", "gnd_assignments.size", "gnd_assignments.max().item", "pred_assignments.max().item", "range", "gnd_assignments.size", "pred_assignments.size", "max", "gnd_assignments.max", "pred_assignments.max", "mask.nonzero().long().view", "mask.nonzero().long", "mask.nonzero"], "function", ["None"], ["def", "calc_precision", "(", "gnd_assignments", ",", "pred_assignments", ")", ":", "\n", "    ", "\"\"\"\n    gnd_clusters should be a torch tensor of longs, containing\n    the assignment to each cluster\n\n    assumes that cluster assignments are 0-based, and no 'holes'\n    \"\"\"", "\n", "precision_sum", "=", "0", "\n", "assert", "len", "(", "gnd_assignments", ".", "size", "(", ")", ")", "==", "1", "\n", "assert", "len", "(", "pred_assignments", ".", "size", "(", ")", ")", "==", "1", "\n", "assert", "pred_assignments", ".", "size", "(", "0", ")", "==", "gnd_assignments", ".", "size", "(", "0", ")", "\n", "N", "=", "gnd_assignments", ".", "size", "(", "0", ")", "\n", "K_gnd", "=", "gnd_assignments", ".", "max", "(", ")", ".", "item", "(", ")", "+", "1", "\n", "K_pred", "=", "pred_assignments", ".", "max", "(", ")", ".", "item", "(", ")", "+", "1", "\n", "for", "k_pred", "in", "range", "(", "K_pred", ")", ":", "\n", "        ", "mask", "=", "pred_assignments", "==", "k_pred", "\n", "gnd", "=", "gnd_assignments", "[", "mask", ".", "nonzero", "(", ")", ".", "long", "(", ")", ".", "view", "(", "-", "1", ")", "]", "\n", "max_intersect", "=", "0", "\n", "for", "k_gnd", "in", "range", "(", "K_gnd", ")", ":", "\n", "            ", "intersect", "=", "(", "gnd", "==", "k_gnd", ")", ".", "long", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "max_intersect", "=", "max", "(", "max_intersect", ",", "intersect", ")", "\n", "", "precision_sum", "+=", "max_intersect", "\n", "", "precision", "=", "precision_sum", "/", "N", "\n", "return", "precision", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_recall": [[36, 43], ["cluster_metrics.calc_precision"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_precision"], ["", "def", "calc_recall", "(", "gnd_assignments", ",", "pred_assignments", ")", ":", "\n", "    ", "\"\"\"\n    basically the reverse of calc_precision\n\n    so, we can just call calc_precision in reverse :P\n    \"\"\"", "\n", "return", "calc_precision", "(", "gnd_assignments", "=", "pred_assignments", ",", "pred_assignments", "=", "gnd_assignments", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_f1": [[45, 50], ["cluster_metrics.calc_precision", "cluster_metrics.calc_recall"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_precision", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_recall"], ["", "def", "calc_f1", "(", "gnd_assignments", ",", "pred_assignments", ")", ":", "\n", "    ", "prec", "=", "calc_precision", "(", "gnd_assignments", "=", "gnd_assignments", ",", "pred_assignments", "=", "pred_assignments", ")", "\n", "recall", "=", "calc_recall", "(", "gnd_assignments", "=", "gnd_assignments", ",", "pred_assignments", "=", "pred_assignments", ")", "\n", "f1", "=", "2", "*", "(", "prec", "*", "recall", ")", "/", "(", "prec", "+", "recall", ")", "\n", "return", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_prec_rec_f1": [[52, 57], ["cluster_metrics.calc_precision", "cluster_metrics.calc_recall"], "function", ["home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_precision", "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_recall"], ["", "def", "calc_prec_rec_f1", "(", "gnd_assignments", ",", "pred_assignments", ")", ":", "\n", "    ", "prec", "=", "calc_precision", "(", "gnd_assignments", "=", "gnd_assignments", ",", "pred_assignments", "=", "pred_assignments", ")", "\n", "recall", "=", "calc_recall", "(", "gnd_assignments", "=", "gnd_assignments", ",", "pred_assignments", "=", "pred_assignments", ")", "\n", "f1", "=", "2", "*", "(", "prec", "*", "recall", ")", "/", "(", "prec", "+", "recall", ")", "\n", "return", "prec", ",", "recall", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.asappresearch_dialog-intent-induction.metrics.cluster_metrics.calc_ACC": [[59, 76], ["pred.size", "torch.zeros", "collections.Counter", "torch.LongTensor", "torch.LongTensor", "scipy.sparse.csr_matrix", "M.todense.todense", "scipy.optimize.linear_sum_assignment", "M[].sum().item", "len", "len", "gnd.size", "list", "list", "list", "pred.size", "gnd.size", "zip", "collections.Counter.keys", "collections.Counter.values", "torch.LongTensor.numpy", "M[].sum", "gnd.tolist", "pred.tolist", "keys[].numpy", "keys[].numpy"], "function", ["None"], ["", "def", "calc_ACC", "(", "pred", ",", "gnd", ")", ":", "\n", "    ", "assert", "len", "(", "pred", ".", "size", "(", ")", ")", "==", "1", "\n", "assert", "len", "(", "gnd", ".", "size", "(", ")", ")", "==", "1", "\n", "N", "=", "pred", ".", "size", "(", "0", ")", "\n", "assert", "N", "==", "gnd", ".", "size", "(", "0", ")", "\n", "M", "=", "torch", ".", "zeros", "(", "N", ",", "N", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "counts", "=", "Counter", "(", "list", "(", "zip", "(", "gnd", ".", "tolist", "(", ")", ",", "pred", ".", "tolist", "(", ")", ")", ")", ")", "\n", "keys", "=", "torch", ".", "LongTensor", "(", "list", "(", "counts", ".", "keys", "(", ")", ")", ")", "\n", "values", "=", "torch", ".", "LongTensor", "(", "list", "(", "counts", ".", "values", "(", ")", ")", ")", "\n", "M", "=", "scipy", ".", "sparse", ".", "csr_matrix", "(", "(", "values", ".", "numpy", "(", ")", ",", "(", "keys", "[", ":", ",", "0", "]", ".", "numpy", "(", ")", ",", "keys", "[", ":", ",", "1", "]", ".", "numpy", "(", ")", ")", ")", ")", "\n", "M", "=", "M", ".", "todense", "(", ")", "\n", "\n", "row_ind", ",", "col_ind", "=", "scipy", ".", "optimize", ".", "linear_sum_assignment", "(", "-", "M", ")", "\n", "cost", "=", "M", "[", "row_ind", ",", "col_ind", "]", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "ACC", "=", "cost", "/", "N", "\n", "\n", "return", "ACC", "\n", "", ""]]}