{"home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.BasicBlock.__init__": [[24, 29], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_planes", ",", "out_planes", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ":", "\n", "        ", "super", "(", "BasicBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "in_planes", ",", "out_planes", ",", "kernel_size", "=", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm2d", "(", "out_planes", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.BasicBlock.forward": [[30, 35], ["model_G.BasicBlock.conv1", "model_G.BasicBlock.bn1", "model_G.BasicBlock.relu"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.ResnetBlock.__init__": [[38, 44], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ")", ":", "\n", "        ", "super", "(", "ResnetBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "conv_block", "=", "[", "]", "\n", "conv_block", "+=", "[", "nn", ".", "ReflectionPad2d", "(", "1", ")", ",", "nn", ".", "Conv2d", "(", "dim", ",", "dim", ",", "kernel_size", "=", "3", ")", ",", "nn", ".", "InstanceNorm2d", "(", "dim", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "]", "\n", "conv_block", "+=", "[", "nn", ".", "ReflectionPad2d", "(", "1", ")", ",", "nn", ".", "Conv2d", "(", "dim", ",", "dim", ",", "kernel_size", "=", "3", ")", ",", "nn", ".", "InstanceNorm2d", "(", "dim", ")", "]", "\n", "self", ".", "conv_blocks", "=", "nn", ".", "Sequential", "(", "*", "conv_block", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.ResnetBlock.forward": [[45, 48], ["model_G.ResnetBlock.conv_blocks"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "out", "=", "x", "+", "self", ".", "conv_blocks", "(", "x", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.AudioEncoder.__init__": [[51, 64], ["torch.Module.__init__", "model_G.BasicBlock", "model_G.BasicBlock", "model_G.BasicBlock", "model_G.BasicBlock", "model_G.BasicBlock", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_output_length", ",", "if_tanh", "=", "False", ")", ":", "\n", "        ", "super", "(", "AudioEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "if_tanh", "=", "if_tanh", "\n", "# the input map is 1 x 12 x 35", "\n", "self", ".", "block1", "=", "BasicBlock", "(", "1", ",", "16", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", "# 16 x 12 x 35", "\n", "self", ".", "block2", "=", "BasicBlock", "(", "16", ",", "32", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "# 32 x 6 x 18", "\n", "self", ".", "block3", "=", "BasicBlock", "(", "32", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", "# 64 x 6 x 18", "\n", "self", ".", "block4", "=", "BasicBlock", "(", "64", ",", "128", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", "# 128 x 6 x 18", "\n", "self", ".", "block5", "=", "BasicBlock", "(", "128", ",", "256", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "# 256 x 3 x 9", "\n", "# self.fc1 = nn.Linear(6912, 512)", "\n", "# self.batch_norm = nn.BatchNorm2d(512)", "\n", "self", ".", "fc1", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "6912", ",", "512", ")", ",", "nn", ".", "BatchNorm1d", "(", "512", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "512", ",", "num_output_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.AudioEncoder.forward": [[65, 78], ["model_G.AudioEncoder.block1", "model_G.AudioEncoder.block2", "model_G.AudioEncoder.block3", "model_G.AudioEncoder.block4", "model_G.AudioEncoder.block5", "torch.tanh.contiguous().view", "model_G.AudioEncoder.fc1", "model_G.AudioEncoder.fc2", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh.contiguous"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "out", "=", "self", ".", "block1", "(", "inputs", ")", "\n", "out", "=", "self", ".", "block2", "(", "out", ")", "\n", "out", "=", "self", ".", "block3", "(", "out", ")", "\n", "out", "=", "self", ".", "block4", "(", "out", ")", "\n", "out", "=", "self", ".", "block5", "(", "out", ")", "\n", "out", "=", "out", ".", "contiguous", "(", ")", ".", "view", "(", "out", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "# out = F.relu(self.batch_norm(self.fc1(out)))", "\n", "out", "=", "self", ".", "fc1", "(", "out", ")", "\n", "out", "=", "self", ".", "fc2", "(", "out", ")", "\n", "if", "self", ".", "if_tanh", ":", "\n", "          ", "out", "=", "F", ".", "tanh", "(", "out", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.AudioEncoder_hk_1.__init__": [[81, 98], ["torch.Module.__init__", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "norm_layer", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "norm_layer", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.Tanh", "torch.Tanh", "torch.Tanh"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ")", ":", "\n", "        ", "super", "(", "AudioEncoder_hk_1", ",", "self", ")", ".", "__init__", "(", ")", "\n", "use_bias", "=", "norm_layer", "==", "nn", ".", "InstanceNorm2d", "\n", "self", ".", "relu", "=", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "1", ",", "64", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "\n", "stride", "=", "(", "3", ",", "2", ")", ",", "padding", "=", "(", "1", ",", "2", ")", ",", "bias", "=", "use_bias", ")", "\n", "self", ".", "pool1", "=", "nn", ".", "AvgPool2d", "(", "(", "2", ",", "2", ")", ",", "2", ")", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "64", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "64", ",", "128", ",", "(", "3", ",", "3", ")", ",", "2", ",", "1", ",", "bias", "=", "use_bias", ")", "\n", "self", ".", "pool2", "=", "nn", ".", "AvgPool2d", "(", "2", ",", "2", ")", "\n", "self", ".", "bn2", "=", "norm_layer", "(", "128", ")", "\n", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "128", ",", "256", ",", "(", "3", ",", "3", ")", ",", "1", ",", "0", ",", "bias", "=", "use_bias", ")", "\n", "self", ".", "bn3", "=", "norm_layer", "(", "256", ")", "\n", "self", ".", "conv4", "=", "nn", ".", "Conv2d", "(", "256", ",", "512", ",", "(", "4", ",", "2", ")", ",", "1", ",", "bias", "=", "use_bias", ")", "\n", "\n", "self", ".", "bn5", "=", "norm_layer", "(", "512", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.AudioEncoder_hk_1.forward": [[99, 112], ["model_G.AudioEncoder_hk_1.conv1", "model_G.AudioEncoder_hk_1.bn1", "model_G.AudioEncoder_hk_1.relu", "model_G.AudioEncoder_hk_1.conv2", "model_G.AudioEncoder_hk_1.bn2", "model_G.AudioEncoder_hk_1.relu", "model_G.AudioEncoder_hk_1.conv3", "model_G.AudioEncoder_hk_1.bn3", "model_G.AudioEncoder_hk_1.relu", "model_G.AudioEncoder_hk_1.conv4"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "net1", "=", "self", ".", "conv1", "(", "x", ")", "\n", "net1", "=", "self", ".", "bn1", "(", "net1", ")", "\n", "net1", "=", "self", ".", "relu", "(", "net1", ")", "\n", "\n", "net", "=", "self", ".", "conv2", "(", "net1", ")", "\n", "net", "=", "self", ".", "bn2", "(", "net", ")", "\n", "net", "=", "self", ".", "relu", "(", "net", ")", "\n", "net", "=", "self", ".", "conv3", "(", "net", ")", "\n", "net", "=", "self", ".", "bn3", "(", "net", ")", "\n", "net", "=", "self", ".", "relu", "(", "net", ")", "\n", "net", "=", "self", ".", "conv4", "(", "net", ")", "\n", "return", "net", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.AudioEncoder_hk_2.__init__": [[115, 125], ["torch.Module.__init__", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "AudioEncoder_hk_2", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "relu", "=", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "1", ",", "64", ",", "kernel_size", "=", "(", "3", ",", "12", ")", ",", "stride", "=", "(", "1", ",", "1", ")", ",", "padding", "=", "0", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm2d", "(", "64", ")", "\n", "self", ".", "pool1", "=", "nn", ".", "MaxPool2d", "(", "1", ",", "3", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "64", ",", "256", ",", "(", "3", ",", "1", ")", ",", "1", ",", "(", "1", ",", "0", ")", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "BatchNorm2d", "(", "256", ")", "\n", "self", ".", "pool2", "=", "nn", ".", "MaxPool2d", "(", "1", ",", "2", ")", "\n", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "256", ",", "512", ",", "(", "6", ",", "1", ")", ",", "1", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.AudioEncoder_hk_2.forward": [[126, 135], ["model_G.AudioEncoder_hk_2.conv1", "model_G.AudioEncoder_hk_2.relu", "model_G.AudioEncoder_hk_2.pool1", "model_G.AudioEncoder_hk_2.conv2", "model_G.AudioEncoder_hk_2.relu", "model_G.AudioEncoder_hk_2.pool2", "model_G.AudioEncoder_hk_2.conv3", "model_G.AudioEncoder_hk_2.bn1", "model_G.AudioEncoder_hk_2.bn2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "net", "=", "self", ".", "conv1", "(", "x", ")", "\n", "net", "=", "self", ".", "relu", "(", "self", ".", "bn1", "(", "net", ")", ")", "\n", "net", "=", "self", ".", "pool1", "(", "net", ")", "\n", "net", "=", "self", ".", "conv2", "(", "net", ")", "\n", "net", "=", "self", ".", "relu", "(", "self", ".", "bn2", "(", "net", ")", ")", "\n", "net", "=", "self", ".", "pool2", "(", "net", ")", "\n", "net", "=", "self", ".", "conv3", "(", "net", ")", "\n", "return", "net", "\n", "", "", "class", "AudioEncoder_hk", "(", "nn", ".", "Module", ")", ":", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.AudioEncoder_hk.__init__": [[136, 143], ["torch.Module.__init__", "model_G.AudioEncoder_hk_1", "model_G.AudioEncoder_hk_2", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "mfcc_length", "=", "35", ",", "mfcc_width", "=", "12", ")", ":", "\n", "        ", "super", "(", "AudioEncoder_hk", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "model1", "=", "AudioEncoder_hk_1", "(", ")", "\n", "self", ".", "model2", "=", "AudioEncoder_hk_2", "(", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "1024", ",", "512", ")", "\n", "self", ".", "mfcc_length", "=", "mfcc_length", "\n", "self", ".", "mfcc_width", "=", "mfcc_width", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.AudioEncoder_hk._forward": [[144, 151], ["model_G.AudioEncoder_hk.model1.forward", "model_G.AudioEncoder_hk.model2.forward", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_G.AudioEncoder_hk.view", "model_G.AudioEncoder_hk.fc"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.forward", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.forward"], ["", "def", "_forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "net1", "=", "self", ".", "model1", ".", "forward", "(", "x", ")", "\n", "net2", "=", "self", ".", "model2", ".", "forward", "(", "x", ")", "\n", "net", "=", "torch", ".", "cat", "(", "(", "net1", ",", "net2", ")", ",", "1", ")", "\n", "net", "=", "net", ".", "view", "(", "-", "1", ",", "1024", ")", "\n", "net", "=", "self", ".", "fc", "(", "net", ")", "\n", "return", "net", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.AudioEncoder_hk.forward": [[152, 156], ["x.view", "model_G.AudioEncoder_hk._forward"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.AudioEncoder_hk._forward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x0", "=", "x", ".", "view", "(", "-", "1", ",", "1", ",", "self", ".", "mfcc_length", ",", "self", ".", "mfcc_width", ")", "\n", "net", "=", "self", ".", "_forward", "(", "x0", ")", "\n", "return", "net", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.AudioEncoderBMVC.__init__": [[159, 169], ["torch.Module.__init__", "model_G.BasicBlock", "model_G.BasicBlock", "model_G.BasicBlock", "model_G.BasicBlock", "model_G.BasicBlock", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_output_length", ",", "if_tanh", "=", "False", ")", ":", "\n", "        ", "super", "(", "AudioEncoderBMVC", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "if_tanh", "=", "if_tanh", "\n", "self", ".", "block1", "=", "BasicBlock", "(", "1", ",", "32", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", "# 32 x 12 x 35", "\n", "self", ".", "block2", "=", "BasicBlock", "(", "32", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "[", "1", ",", "2", "]", ")", "# 64 x 12 x 18", "\n", "self", ".", "block3", "=", "BasicBlock", "(", "64", ",", "128", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", "# 128 x 12 x 18", "\n", "self", ".", "block4", "=", "BasicBlock", "(", "128", ",", "128", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", "# 128 x 12 x 18", "\n", "self", ".", "block5", "=", "BasicBlock", "(", "128", ",", "256", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "# 256 x 6 x 9", "\n", "self", ".", "fc1", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "13824", ",", "1024", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "1024", ",", "num_output_length", ")", "\n", "", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.AudioEncoderBMVC.forward": [[169, 182], ["model_G.AudioEncoderBMVC.block1", "model_G.AudioEncoderBMVC.block2", "model_G.AudioEncoderBMVC.block3", "model_G.AudioEncoderBMVC.block4", "model_G.AudioEncoderBMVC.block5", "torch.tanh.contiguous().view", "model_G.AudioEncoderBMVC.fc1", "model_G.AudioEncoderBMVC.fc2", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh.contiguous"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "out", "=", "self", ".", "block1", "(", "inputs", ")", "\n", "out", "=", "self", ".", "block2", "(", "out", ")", "\n", "out", "=", "self", ".", "block3", "(", "out", ")", "\n", "out", "=", "self", ".", "block4", "(", "out", ")", "\n", "out", "=", "self", ".", "block5", "(", "out", ")", "\n", "out", "=", "out", ".", "contiguous", "(", ")", ".", "view", "(", "out", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "# out = F.relu(self.batch_norm(self.fc1(out)))", "\n", "out", "=", "self", ".", "fc1", "(", "out", ")", "\n", "out", "=", "self", ".", "fc2", "(", "out", ")", "\n", "if", "self", ".", "if_tanh", ":", "\n", "          ", "out", "=", "F", ".", "tanh", "(", "out", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.ImageEncoder.__init__": [[185, 194], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "model_G.ImageEncoder.get_size", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.ImageDecoder.get_size"], ["    ", "def", "__init__", "(", "self", ",", "size_image", ",", "num_output_length", ",", "if_tanh", "=", "False", ")", ":", "\n", "        ", "super", "(", "ImageEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "if_tanh", "=", "if_tanh", "\n", "self", ".", "conv1", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "3", ",", "16", ",", "5", ",", "stride", "=", "2", ",", "padding", "=", "2", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "16", ",", "32", ",", "5", ",", "stride", "=", "2", ",", "padding", "=", "2", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "conv3", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "5", ",", "stride", "=", "2", ",", "padding", "=", "2", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "conv4", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "64", ",", "128", ",", "5", ",", "stride", "=", "2", ",", "padding", "=", "2", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "size_mini_map", "=", "self", ".", "get_size", "(", "size_image", ",", "4", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "size_mini_map", "*", "size_mini_map", "*", "128", ",", "num_output_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.ImageEncoder.get_size": [[195, 197], ["int"], "methods", ["None"], ["", "def", "get_size", "(", "self", ",", "size_image", ",", "num_layers", ")", ":", "\n", "        ", "return", "int", "(", "size_image", "/", "2", "**", "num_layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.ImageEncoder.forward": [[198, 208], ["model_G.ImageEncoder.conv1", "model_G.ImageEncoder.conv2", "model_G.ImageEncoder.conv3", "model_G.ImageEncoder.conv4", "model_G.ImageEncoder.contiguous().view", "model_G.ImageEncoder.fc", "torch.tanh", "torch.tanh", "torch.tanh", "model_G.ImageEncoder.contiguous"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "img_e_conv1", "=", "self", ".", "conv1", "(", "inputs", ")", "\n", "img_e_conv2", "=", "self", ".", "conv2", "(", "img_e_conv1", ")", "\n", "img_e_conv3", "=", "self", ".", "conv3", "(", "img_e_conv2", ")", "\n", "img_e_conv4", "=", "self", ".", "conv4", "(", "img_e_conv3", ")", "\n", "img_e_fc_5", "=", "img_e_conv4", ".", "contiguous", "(", ")", ".", "view", "(", "img_e_conv4", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "img_e_fc_5", "=", "self", ".", "fc", "(", "img_e_fc_5", ")", "\n", "if", "self", ".", "if_tanh", ":", "\n", "            ", "img_e_fc_5", "=", "F", ".", "tanh", "(", "img_e_fc_5", ")", "\n", "", "return", "img_e_fc_5", ",", "img_e_conv1", ",", "img_e_conv2", ",", "img_e_conv3", ",", "img_e_conv4", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.ImageEncoderFCN.__init__": [[212, 219], ["torch.Module.__init__", "model_G.BasicBlock", "model_G.BasicBlock", "model_G.BasicBlock", "model_G.BasicBlock"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "size_image", ",", "num_output_length", ",", "if_tanh", "=", "False", ")", ":", "\n", "        ", "super", "(", "ImageEncoderFCN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "if_tanh", "=", "if_tanh", "\n", "self", ".", "conv1", "=", "BasicBlock", "(", "3", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "# nn.Sequential(nn.Conv2d(3, 64, 3, stride=2, padding=1), nn.ReLU(inplace=True))", "\n", "self", ".", "conv2", "=", "BasicBlock", "(", "64", ",", "128", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "# nn.Sequential(nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU(inplace=True))", "\n", "self", ".", "conv3", "=", "BasicBlock", "(", "128", ",", "256", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "# nn.Sequential(nn.Conv2d(128, 256, 3, stride=2, padding=1), nn.ReLU(inplace=True))", "\n", "self", ".", "conv4", "=", "BasicBlock", "(", "256", ",", "512", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "# nn.Sequential(nn.Conv2d(256, 512, 3, stride=2, padding=1), nn.ReLU(inplace=True))", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.ImageEncoderFCN.forward": [[220, 228], ["model_G.ImageEncoderFCN.conv1", "model_G.ImageEncoderFCN.conv2", "model_G.ImageEncoderFCN.conv3", "model_G.ImageEncoderFCN.conv4", "torch.tanh", "torch.tanh", "torch.tanh"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "img_e_conv1", "=", "self", ".", "conv1", "(", "inputs", ")", "# /2", "\n", "img_e_conv2", "=", "self", ".", "conv2", "(", "img_e_conv1", ")", "# /4", "\n", "img_e_conv3", "=", "self", ".", "conv3", "(", "img_e_conv2", ")", "# /8", "\n", "img_e_conv4", "=", "self", ".", "conv4", "(", "img_e_conv3", ")", "# /16", "\n", "if", "self", ".", "if_tanh", ":", "\n", "            ", "img_e_conv4", "=", "F", ".", "tanh", "(", "img_e_conv4", ")", "\n", "", "return", "img_e_conv4", ",", "img_e_conv1", ",", "img_e_conv2", ",", "img_e_conv3", ",", "img_e_conv4", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.ImageDecoder.__init__": [[232, 242], ["torch.Module.__init__", "model_G.ImageDecoder.get_size", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.ImageDecoder.get_size"], ["    ", "def", "__init__", "(", "self", ",", "size_image", ",", "input_dim", ")", ":", "\n", "        ", "super", "(", "ImageDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "size_mini_map", "=", "self", ".", "get_size", "(", "size_image", ",", "4", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "input_dim", ",", "self", ".", "size_mini_map", "*", "self", ".", "size_mini_map", "*", "256", ")", "\n", "self", ".", "dconv1", "=", "nn", ".", "Sequential", "(", "nn", ".", "ConvTranspose2d", "(", "384", ",", "196", ",", "5", ",", "stride", "=", "2", ",", "padding", "=", "2", ",", "output_padding", "=", "1", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "dconv2", "=", "nn", ".", "Sequential", "(", "nn", ".", "ConvTranspose2d", "(", "260", ",", "128", ",", "5", ",", "stride", "=", "2", ",", "padding", "=", "2", ",", "output_padding", "=", "1", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "dconv3", "=", "nn", ".", "Sequential", "(", "nn", ".", "ConvTranspose2d", "(", "160", ",", "80", ",", "5", ",", "stride", "=", "2", ",", "padding", "=", "2", ",", "output_padding", "=", "1", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "dconv4", "=", "nn", ".", "Sequential", "(", "nn", ".", "ConvTranspose2d", "(", "96", ",", "48", ",", "5", ",", "stride", "=", "2", ",", "padding", "=", "2", ",", "output_padding", "=", "1", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "dconv5", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "48", ",", "16", ",", "5", ",", "stride", "=", "1", ",", "padding", "=", "2", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "dconv6", "=", "nn", ".", "Conv2d", "(", "16", ",", "3", ",", "5", ",", "stride", "=", "1", ",", "padding", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.ImageDecoder.get_size": [[243, 245], ["int"], "methods", ["None"], ["", "def", "get_size", "(", "self", ",", "size_image", ",", "num_layers", ")", ":", "\n", "        ", "return", "int", "(", "size_image", "/", "2", "**", "num_layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.ImageDecoder.forward": [[246, 267], ["model_G.ImageDecoder.fc", "model_G.ImageDecoder.contiguous().view", "torch.relu", "torch.relu", "torch.relu", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_G.ImageDecoder.dconv1", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_G.ImageDecoder.dconv2", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_G.ImageDecoder.dconv3", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_G.ImageDecoder.dconv4", "model_G.ImageDecoder.dconv5", "model_G.ImageDecoder.dconv6", "torch.tanh", "torch.tanh", "torch.tanh", "model_G.ImageDecoder.contiguous"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "concat_z", ",", "img_e_conv1", ",", "img_e_conv2", ",", "img_e_conv3", ",", "img_e_conv4", ")", ":", "\n", "# out = torch.cat([img_z, audio_z], dim=1) # (batch_size, input_dim)", "\n", "        ", "out", "=", "self", ".", "fc", "(", "concat_z", ")", "\n", "# reshape 256 x 7 x 7", "\n", "out", "=", "out", ".", "contiguous", "(", ")", ".", "view", "(", "out", ".", "shape", "[", "0", "]", ",", "256", ",", "self", ".", "size_mini_map", ",", "self", ".", "size_mini_map", ")", "\n", "out", "=", "F", ".", "relu", "(", "out", ",", "inplace", "=", "True", ")", "\n", "# concate (256+128) x 7x7", "\n", "out", "=", "torch", ".", "cat", "(", "[", "out", ",", "img_e_conv4", "]", ",", "dim", "=", "1", ")", "\n", "out", "=", "self", ".", "dconv1", "(", "out", ")", "\n", "# concate (196+64) x 14x14", "\n", "out", "=", "torch", ".", "cat", "(", "[", "out", ",", "img_e_conv3", "]", ",", "dim", "=", "1", ")", "\n", "out", "=", "self", ".", "dconv2", "(", "out", ")", "\n", "# concate (128+32) x 28x28", "\n", "out", "=", "torch", ".", "cat", "(", "[", "out", ",", "img_e_conv2", "]", ",", "dim", "=", "1", ")", "\n", "out", "=", "self", ".", "dconv3", "(", "out", ")", "\n", "# concate (80+16) x 56x56", "\n", "out", "=", "torch", ".", "cat", "(", "[", "out", ",", "img_e_conv1", "]", ",", "dim", "=", "1", ")", "\n", "out", "=", "self", ".", "dconv4", "(", "out", ")", "\n", "out", "=", "self", ".", "dconv5", "(", "out", ")", "\n", "out", "=", "self", ".", "dconv6", "(", "out", ")", "\n", "return", "F", ".", "tanh", "(", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.ImageDecoderResidual.__init__": [[270, 282], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model_G.ResnetBlock", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "size_image", ",", "input_dim", ")", ":", "\n", "        ", "super", "(", "ImageDecoderResidual", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fuse", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "input_dim", ",", "512", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ",", "nn", ".", "ReLU", "(", "True", ")", ")", "\n", "self", ".", "resblocks", "=", "nn", ".", "Sequential", "(", "ResnetBlock", "(", "512", ")", ")", "#, ResnetBlock(512), ResnetBlock(512))", "\n", "self", ".", "dconv1", "=", "nn", ".", "Sequential", "(", "nn", ".", "ConvTranspose2d", "(", "512", ",", "256", ",", "4", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "nn", ".", "BatchNorm2d", "(", "256", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "512", ",", "256", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "self", ".", "dconv2", "=", "nn", ".", "Sequential", "(", "nn", ".", "ConvTranspose2d", "(", "256", ",", "128", ",", "4", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "nn", ".", "BatchNorm2d", "(", "128", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "256", ",", "128", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "self", ".", "dconv3", "=", "nn", ".", "Sequential", "(", "nn", ".", "ConvTranspose2d", "(", "128", ",", "64", ",", "4", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "nn", ".", "BatchNorm2d", "(", "64", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "128", ",", "64", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "self", ".", "dconv4", "=", "nn", ".", "Sequential", "(", "nn", ".", "ConvTranspose2d", "(", "64", ",", "32", ",", "4", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "nn", ".", "BatchNorm2d", "(", "32", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "conv4", "=", "nn", ".", "Conv2d", "(", "32", ",", "3", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.ImageDecoderResidual.forward": [[283, 301], ["model_G.ImageDecoderResidual.fuse", "model_G.ImageDecoderResidual.resblocks", "model_G.ImageDecoderResidual.dconv1", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_G.ImageDecoderResidual.conv1", "model_G.ImageDecoderResidual.dconv2", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_G.ImageDecoderResidual.conv2", "model_G.ImageDecoderResidual.dconv3", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_G.ImageDecoderResidual.conv3", "model_G.ImageDecoderResidual.dconv4", "model_G.ImageDecoderResidual.conv4", "torch.tanh", "torch.tanh", "torch.tanh"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "concat_z", ",", "img_e_conv1", ",", "img_e_conv2", ",", "img_e_conv3", ",", "img_e_conv4", ")", ":", "\n", "# audio_z = audio_z.unsqueeze(-1).unsqueeze(-1)", "\n", "# audio_z = audio_z.repeat(1,1, img_z.shape[2], img_z.shape[3])", "\n", "# z = torch.cat([img_z, audio_z], dim=1)", "\n", "        ", "z", "=", "self", ".", "fuse", "(", "concat_z", ")", "\n", "z", "=", "self", ".", "resblocks", "(", "z", ")", "\n", "out", "=", "self", ".", "dconv1", "(", "z", ")", "\n", "out", "=", "torch", ".", "cat", "(", "[", "out", ",", "img_e_conv3", "]", ",", "dim", "=", "1", ")", "\n", "out", "=", "self", ".", "conv1", "(", "out", ")", "\n", "out", "=", "self", ".", "dconv2", "(", "out", ")", "\n", "out", "=", "torch", ".", "cat", "(", "[", "out", ",", "img_e_conv2", "]", ",", "dim", "=", "1", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "dconv3", "(", "out", ")", "\n", "out", "=", "torch", ".", "cat", "(", "[", "out", ",", "img_e_conv1", "]", ",", "dim", "=", "1", ")", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "out", "=", "self", ".", "dconv4", "(", "out", ")", "\n", "out", "=", "self", ".", "conv4", "(", "out", ")", "\n", "return", "F", ".", "tanh", "(", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.LipGeneratorCNN.__init__": [[304, 329], ["torch.Module.__init__", "model_G.LipGeneratorCNN.audio_encoder.apply", "model_G.LipGeneratorCNN.image_encoder.apply", "model_G.LipGeneratorCNN.image_decoder.apply", "model_G.AudioEncoder", "model_G.ImageEncoder", "model_G.ImageDecoder", "model_G.AudioEncoderBMVC", "model_G.ImageEncoderFCN", "model_G.ImageDecoderResidual", "model_G.AudioEncoder_hk"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "audio_encoder_type", ",", "img_encoder_type", ",", "img_decoder_type", ",", "size_image", ",", "num_output_length", ",", "if_tanh", ")", ":", "\n", "        ", "super", "(", "LipGeneratorCNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "audio_encoder_type", "==", "'reduce'", ":", "\n", "            ", "self", ".", "audio_encoder", "=", "AudioEncoder", "(", "num_output_length", ",", "if_tanh", ")", "\n", "", "elif", "audio_encoder_type", "==", "'bmvc'", ":", "\n", "            ", "self", ".", "audio_encoder", "=", "AudioEncoderBMVC", "(", "num_output_length", ",", "if_tanh", ")", "\n", "", "elif", "audio_encoder_type", "==", "'hk'", ":", "\n", "            ", "self", ".", "audio_encoder", "=", "AudioEncoder_hk", "(", "35", ",", "12", ")", "\n", "", "if", "img_encoder_type", "==", "'reduce'", ":", "\n", "            ", "self", ".", "image_encoder", "=", "ImageEncoder", "(", "size_image", ",", "num_output_length", ",", "if_tanh", ")", "\n", "", "elif", "img_encoder_type", "==", "'FCN'", ":", "\n", "            ", "self", ".", "image_encoder", "=", "ImageEncoderFCN", "(", "size_image", ",", "num_output_length", ",", "if_tanh", ")", "\n", "", "if", "img_decoder_type", "==", "'reduce'", ":", "\n", "            ", "self", ".", "image_decoder", "=", "ImageDecoder", "(", "size_image", ",", "2", "*", "num_output_length", ")", "\n", "", "elif", "img_decoder_type", "==", "'residual'", ":", "\n", "            ", "self", ".", "image_decoder", "=", "ImageDecoderResidual", "(", "size_image", ",", "2", "*", "num_output_length", ")", "\n", "\n", "", "self", ".", "audio_encoder_type", "=", "audio_encoder_type", "\n", "self", ".", "img_encoder_type", "=", "img_encoder_type", "\n", "self", ".", "img_decoder_type", "=", "img_decoder_type", "\n", "\n", "# initialize weights", "\n", "self", ".", "audio_encoder", ".", "apply", "(", "weights_init", ")", "\n", "self", ".", "image_encoder", ".", "apply", "(", "weights_init", ")", "\n", "self", ".", "image_decoder", ".", "apply", "(", "weights_init", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.LipGeneratorCNN.forward": [[330, 341], ["model_G.LipGeneratorCNN.audio_encoder", "model_G.LipGeneratorCNN.image_encoder", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_G.LipGeneratorCNN.image_decoder", "audio_z.repeat.repeat.unsqueeze().unsqueeze", "audio_z.repeat.repeat.repeat", "audio_z.repeat.repeat.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "image_inputs", ",", "audio_inputs", ")", ":", "\n", "        ", "audio_z", "=", "self", ".", "audio_encoder", "(", "audio_inputs", ")", "\n", "image_z", ",", "img_e_conv1", ",", "img_e_conv2", ",", "img_e_conv3", ",", "img_e_conv4", "=", "self", ".", "image_encoder", "(", "image_inputs", ")", "\n", "\n", "if", "self", ".", "img_encoder_type", "==", "'FCN'", ":", "\n", "            ", "audio_z", "=", "audio_z", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "audio_z", "=", "audio_z", ".", "repeat", "(", "1", ",", "1", ",", "image_z", ".", "shape", "[", "2", "]", ",", "image_z", ".", "shape", "[", "3", "]", ")", "\n", "", "concat_z", "=", "torch", ".", "cat", "(", "[", "image_z", ",", "audio_z", "]", ",", "dim", "=", "1", ")", "\n", "\n", "G", "=", "self", ".", "image_decoder", "(", "concat_z", ",", "img_e_conv1", ",", "img_e_conv2", ",", "img_e_conv3", ",", "img_e_conv4", ")", "\n", "return", "G", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.LipGeneratorCNN.model_type": [[342, 344], ["None"], "methods", ["None"], ["", "def", "model_type", "(", "self", ")", ":", "\n", "        ", "return", "'CNN'", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.RNNModel.__init__": [[349, 356], ["torch.Module.__init__", "torch.GRU", "torch.GRU", "torch.GRU"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_size", ",", "hidden_size", ",", "rnn_type", ",", "num_layers", "=", "1", ")", ":", "\n", "        ", "super", "(", "RNNModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "rnn_type", "=", "rnn_type", "\n", "self", ".", "nhid", "=", "hidden_size", "\n", "self", ".", "nlayers", "=", "num_layers", "\n", "if", "rnn_type", "==", "'GRU'", ":", "\n", "            ", "self", ".", "rnn", "=", "nn", ".", "GRU", "(", "input_size", ",", "hidden_size", ",", "num_layers", "=", "1", ",", "batch_first", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.RNNModel.forward": [[357, 361], ["model_G.RNNModel.rnn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "inputs", ",", "hidden", ")", ":", "\n", "\n", "        ", "output", ",", "hidden", "=", "self", ".", "rnn", "(", "inputs", ",", "hidden", ")", "\n", "return", "output", ",", "hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.RNNModel.init_hidden": [[363, 370], ["next", "model_G.RNNModel.parameters", "next.new_zeros", "next.new_zeros", "next.new_zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "weight", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", "\n", "if", "self", ".", "rnn_type", "==", "'LSTM'", ":", "\n", "            ", "return", "(", "weight", ".", "new_zeros", "(", "self", ".", "nlayers", ",", "batch_size", ",", "self", ".", "nhid", ")", ",", "\n", "weight", ".", "new_zeros", "(", "self", ".", "nlayers", ",", "batch_size", ",", "self", ".", "nhid", ")", ")", "\n", "", "else", ":", "\n", "            ", "return", "weight", ".", "new_zeros", "(", "self", ".", "nlayers", ",", "batch_size", ",", "self", ".", "nhid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.LipGeneratorRNN.__init__": [[373, 401], ["torch.Module.__init__", "model_G.LipGeneratorRNN.audio_encoder.apply", "model_G.LipGeneratorRNN.image_encoder.apply", "model_G.LipGeneratorRNN.image_decoder.apply", "model_G.LipGeneratorRNN.rnn.apply", "model_G.AudioEncoder", "model_G.ImageEncoder", "model_G.ImageDecoder", "model_G.RNNModel", "model_G.AudioEncoderBMVC", "model_G.ImageEncoderFCN", "model_G.ImageDecoderResidual", "model_G.AudioEncoder_hk"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "audio_encoder_type", ",", "img_encoder_type", ",", "img_decoder_type", ",", "rnn_type", ",", "size_image", ",", "num_output_length", ",", "hidden_size", "=", "1024", ",", "if_tanh", "=", "False", ")", ":", "\n", "        ", "super", "(", "LipGeneratorRNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "audio_encoder_type", "==", "'reduce'", ":", "\n", "            ", "self", ".", "audio_encoder", "=", "AudioEncoder", "(", "num_output_length", ",", "if_tanh", ")", "\n", "", "elif", "audio_encoder_type", "==", "'bmvc'", ":", "\n", "            ", "self", ".", "audio_encoder", "=", "AudioEncoderBMVC", "(", "num_output_length", ",", "if_tanh", ")", "\n", "", "elif", "audio_encoder_type", "==", "'hk'", ":", "\n", "            ", "self", ".", "audio_encoder", "=", "AudioEncoder_hk", "(", "35", ",", "12", ")", "\n", "", "if", "img_encoder_type", "==", "'reduce'", ":", "\n", "            ", "self", ".", "image_encoder", "=", "ImageEncoder", "(", "size_image", ",", "num_output_length", ",", "if_tanh", ")", "\n", "", "elif", "img_encoder_type", "==", "'FCN'", ":", "\n", "            ", "self", ".", "image_encoder", "=", "ImageEncoderFCN", "(", "size_image", ",", "num_output_length", ",", "if_tanh", ")", "\n", "", "if", "img_decoder_type", "==", "'reduce'", ":", "\n", "            ", "self", ".", "image_decoder", "=", "ImageDecoder", "(", "size_image", ",", "hidden_size", ")", "\n", "", "elif", "img_decoder_type", "==", "'residual'", ":", "\n", "            ", "self", ".", "image_decoder", "=", "ImageDecoderResidual", "(", "size_image", ",", "hidden_size", ")", "\n", "", "if", "rnn_type", "==", "'GRU'", ":", "\n", "            ", "self", ".", "rnn", "=", "RNNModel", "(", "2", "*", "num_output_length", ",", "hidden_size", ",", "rnn_type", ")", "\n", "\n", "", "self", ".", "audio_encoder_type", "=", "audio_encoder_type", "\n", "self", ".", "img_encoder_type", "=", "img_encoder_type", "\n", "self", ".", "img_decoder_type", "=", "img_decoder_type", "\n", "\n", "# initialize weights", "\n", "self", ".", "audio_encoder", ".", "apply", "(", "weights_init", ")", "\n", "self", ".", "image_encoder", ".", "apply", "(", "weights_init", ")", "\n", "self", ".", "image_decoder", ".", "apply", "(", "weights_init", ")", "\n", "self", ".", "rnn", ".", "apply", "(", "weights_init", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.LipGeneratorRNN.forward": [[403, 442], ["image_inputs.contiguous().view.contiguous().view.contiguous().view", "audio_inputs.contiguous().view.contiguous().view.contiguous().view", "model_G.LipGeneratorRNN.audio_encoder", "model_G.LipGeneratorRNN.image_encoder", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "model_G.LipGeneratorRNN.rnn.init_hidden", "model_G.LipGeneratorRNN.rnn", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "model_G.LipGeneratorRNN.image_decoder", "G.contiguous().view.contiguous().view.contiguous().view", "audio_z.repeat.repeat.unsqueeze().unsqueeze", "audio_z.repeat.repeat.repeat", "concat_z.contiguous().view.contiguous().view.contiguous().view", "concat_z.contiguous().view.contiguous().view.contiguous().view", "rnn_output.contiguous().view.contiguous().view.contiguous().view", "rnn_output.contiguous().view.contiguous().view.contiguous().view", "image_inputs.contiguous().view.contiguous().view.contiguous", "audio_inputs.contiguous().view.contiguous().view.contiguous", "G.contiguous().view.contiguous().view.contiguous", "audio_z.repeat.repeat.unsqueeze", "concat_z.contiguous().view.contiguous().view.contiguous", "concat_z.contiguous().view.contiguous().view.contiguous", "rnn_output.contiguous().view.contiguous().view.contiguous", "rnn_output.contiguous().view.contiguous().view.contiguous"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.RNNModel.init_hidden"], ["", "def", "forward", "(", "self", ",", "image_inputs", ",", "audio_inputs", ",", "valid_len", ",", "teacher_forcing_ratio", "=", "0.5", ")", ":", "\n", "# reshape inputs to (seq_len*batch_size, ...)", "\n", "        ", "batch_size", "=", "image_inputs", ".", "shape", "[", "0", "]", "\n", "seq_len", "=", "image_inputs", ".", "shape", "[", "1", "]", "\n", "\n", "image_inputs", "=", "image_inputs", ".", "contiguous", "(", ")", ".", "view", "(", "seq_len", "*", "batch_size", ",", "image_inputs", ".", "shape", "[", "2", "]", ",", "image_inputs", ".", "shape", "[", "3", "]", ",", "image_inputs", ".", "shape", "[", "4", "]", ")", "\n", "audio_inputs", "=", "audio_inputs", ".", "contiguous", "(", ")", ".", "view", "(", "seq_len", "*", "batch_size", ",", "audio_inputs", ".", "shape", "[", "2", "]", ",", "audio_inputs", ".", "shape", "[", "3", "]", ",", "audio_inputs", ".", "shape", "[", "4", "]", ")", "\n", "\n", "audio_z", "=", "self", ".", "audio_encoder", "(", "audio_inputs", ")", "\n", "image_z", ",", "img_e_conv1", ",", "img_e_conv2", ",", "img_e_conv3", ",", "img_e_conv4", "=", "self", ".", "image_encoder", "(", "image_inputs", ")", "\n", "\n", "if", "self", ".", "img_encoder_type", "==", "'FCN'", ":", "\n", "            ", "audio_z", "=", "audio_z", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "audio_z", "=", "audio_z", ".", "repeat", "(", "1", ",", "1", ",", "image_z", ".", "shape", "[", "2", "]", ",", "image_z", ".", "shape", "[", "3", "]", ")", "\n", "", "concat_z", "=", "torch", ".", "cat", "(", "[", "image_z", ",", "audio_z", "]", ",", "dim", "=", "1", ")", "\n", "\n", "# reshape z to (batch_size, seq_len, ...)", "\n", "if", "self", ".", "img_encoder_type", "==", "'FCN'", ":", "\n", "            ", "concat_z", "=", "concat_z", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "seq_len", ",", "concat_z", ".", "shape", "[", "1", "]", ",", "concat_z", ".", "shape", "[", "2", "]", ",", "concat_z", ".", "shape", "[", "3", "]", ")", "\n", "", "else", ":", "\n", "            ", "concat_z", "=", "concat_z", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "seq_len", ",", "concat_z", ".", "shape", "[", "1", "]", ")", "\n", "\n", "# fed concat_z to RNN, output size: (batch_size, seq_len, hidden_size)", "\n", "", "concat_z", "=", "pack_padded_sequence", "(", "concat_z", ",", "valid_len", ",", "batch_first", "=", "True", ")", "\n", "hidden", "=", "self", ".", "rnn", ".", "init_hidden", "(", "batch_size", ")", "\n", "rnn_output", ",", "_", "=", "self", ".", "rnn", "(", "concat_z", ",", "hidden", ")", "\n", "rnn_output", ",", "_", "=", "pad_packed_sequence", "(", "rnn_output", ",", "batch_first", "=", "True", ",", "total_length", "=", "seq_len", ")", "\n", "\n", "\n", "# reshap rnn output to (seq_len*batch_size, hidden_size)", "\n", "if", "self", ".", "img_encoder_type", "==", "'FCN'", ":", "\n", "            ", "rnn_output", "=", "rnn_output", ".", "contiguous", "(", ")", ".", "view", "(", "seq_len", "*", "batch_size", ",", "rnn_output", ".", "shape", "[", "2", "]", ",", "rnn_output", ".", "shape", "[", "3", "]", ",", "rnn_output", ".", "shape", "[", "4", "]", ")", "\n", "", "else", ":", "\n", "            ", "rnn_output", "=", "rnn_output", ".", "contiguous", "(", ")", ".", "view", "(", "seq_len", "*", "batch_size", ",", "rnn_output", ".", "shape", "[", "2", "]", ")", "\n", "\n", "# decoder", "\n", "", "G", "=", "self", ".", "image_decoder", "(", "rnn_output", ",", "img_e_conv1", ",", "img_e_conv2", ",", "img_e_conv3", ",", "img_e_conv4", ")", "\n", "G", "=", "G", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "seq_len", ",", "G", ".", "shape", "[", "1", "]", ",", "G", ".", "shape", "[", "2", "]", ",", "G", ".", "shape", "[", "3", "]", ")", "\n", "return", "G", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.LipGeneratorRNN.model_type": [[443, 445], ["None"], "methods", ["None"], ["", "def", "model_type", "(", "self", ")", ":", "\n", "        ", "return", "'RNN'", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G.weights_init": [[6, 21], ["hasattr", "m.weight.data.normal_", "classname.find", "classname.find", "m.weight.data.normal_", "m.bias.data.fill_", "type", "torch.nn.init.xavier_uniform", "torch.nn.init.xavier_uniform", "torch.nn.init.xavier_uniform", "m.bias.data.fill_", "print"], "function", ["None"], ["def", "weights_init", "(", "m", ")", ":", "\n", "    ", "classname", "=", "m", ".", "__class__", ".", "__name__", "\n", "if", "classname", ".", "find", "(", "'Conv'", ")", "!=", "-", "1", "and", "hasattr", "(", "m", ",", "'weight'", ")", ":", "\n", "        ", "m", ".", "weight", ".", "data", ".", "normal_", "(", "0.0", ",", "0.02", ")", "\n", "", "elif", "classname", ".", "find", "(", "'BatchNorm'", ")", "!=", "-", "1", ":", "\n", "        ", "m", ".", "weight", ".", "data", ".", "normal_", "(", "1.0", ",", "0.02", ")", "\n", "m", ".", "bias", ".", "data", ".", "fill_", "(", "0", ")", "\n", "", "elif", "type", "(", "m", ")", "==", "nn", ".", "Linear", ":", "\n", "        ", "torch", ".", "nn", ".", "init", ".", "xavier_uniform", "(", "m", ".", "weight", ")", "\n", "m", ".", "bias", ".", "data", ".", "fill_", "(", "0.01", ")", "\n", "# elif classname.find('GRU') != -1 or classname.find('LSTM') != -1:", "\n", "#     m.weight.data.normal_(0.0, 0.02)", "\n", "#     m.bias.data.fill_(0.01)", "\n", "", "else", ":", "\n", "        ", "print", "(", "classname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.logger.Logger.__init__": [[13, 16], ["tensorflow.summary.FileWriter"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "log_dir", ")", ":", "\n", "        ", "\"\"\"Create a summary writer logging to log_dir.\"\"\"", "\n", "self", ".", "writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "log_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.logger.Logger.scalar_summary": [[17, 21], ["tensorflow.Summary", "logger.Logger.writer.add_summary", "tensorflow.Summary.Value"], "methods", ["None"], ["", "def", "scalar_summary", "(", "self", ",", "tag", ",", "value", ",", "step", ")", ":", "\n", "        ", "\"\"\"Log a scalar variable.\"\"\"", "\n", "summary", "=", "tf", ".", "Summary", "(", "value", "=", "[", "tf", ".", "Summary", ".", "Value", "(", "tag", "=", "tag", ",", "simple_value", "=", "value", ")", "]", ")", "\n", "self", ".", "writer", ".", "add_summary", "(", "summary", ",", "step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.logger.Logger.image_summary": [[22, 44], ["enumerate", "tensorflow.Summary", "logger.Logger.writer.add_summary", "scipy.misc.toimage().save", "tensorflow.Summary.Image", "img_summaries.append", "StringIO", "tensorflow.Summary.Value", "BytesIO", "scipy.misc.toimage", "BytesIO.getvalue"], "methods", ["None"], ["", "def", "image_summary", "(", "self", ",", "tag", ",", "images", ",", "step", ")", ":", "\n", "        ", "\"\"\"Log a list of images.\"\"\"", "\n", "\n", "img_summaries", "=", "[", "]", "\n", "for", "i", ",", "img", "in", "enumerate", "(", "images", ")", ":", "\n", "# Write the image to a string", "\n", "            ", "try", ":", "\n", "                ", "s", "=", "StringIO", "(", ")", "\n", "", "except", ":", "\n", "                ", "s", "=", "BytesIO", "(", ")", "\n", "", "scipy", ".", "misc", ".", "toimage", "(", "img", ")", ".", "save", "(", "s", ",", "format", "=", "\"png\"", ")", "\n", "\n", "# Create an Image object", "\n", "img_sum", "=", "tf", ".", "Summary", ".", "Image", "(", "encoded_image_string", "=", "s", ".", "getvalue", "(", ")", ",", "\n", "height", "=", "img", ".", "shape", "[", "0", "]", ",", "\n", "width", "=", "img", ".", "shape", "[", "1", "]", ")", "\n", "# Create a Summary value", "\n", "img_summaries", ".", "append", "(", "tf", ".", "Summary", ".", "Value", "(", "tag", "=", "'%s/%d'", "%", "(", "tag", ",", "i", ")", ",", "image", "=", "img_sum", ")", ")", "\n", "\n", "# Create and write Summary", "\n", "", "summary", "=", "tf", ".", "Summary", "(", "value", "=", "img_summaries", ")", "\n", "self", ".", "writer", ".", "add_summary", "(", "summary", ",", "step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.logger.Logger.histo_summary": [[45, 72], ["numpy.histogram", "tensorflow.HistogramProto", "float", "float", "int", "float", "float", "tensorflow.Summary", "logger.Logger.writer.add_summary", "logger.Logger.writer.flush", "numpy.min", "numpy.max", "numpy.prod", "numpy.sum", "numpy.sum", "tensorflow.HistogramProto.bucket_limit.append", "tensorflow.HistogramProto.bucket.append", "tensorflow.Summary.Value"], "methods", ["None"], ["", "def", "histo_summary", "(", "self", ",", "tag", ",", "values", ",", "step", ",", "bins", "=", "1000", ")", ":", "\n", "        ", "\"\"\"Log a histogram of the tensor of values.\"\"\"", "\n", "\n", "# Create a histogram using numpy", "\n", "counts", ",", "bin_edges", "=", "np", ".", "histogram", "(", "values", ",", "bins", "=", "bins", ")", "\n", "\n", "# Fill the fields of the histogram proto", "\n", "hist", "=", "tf", ".", "HistogramProto", "(", ")", "\n", "hist", ".", "min", "=", "float", "(", "np", ".", "min", "(", "values", ")", ")", "\n", "hist", ".", "max", "=", "float", "(", "np", ".", "max", "(", "values", ")", ")", "\n", "hist", ".", "num", "=", "int", "(", "np", ".", "prod", "(", "values", ".", "shape", ")", ")", "\n", "hist", ".", "sum", "=", "float", "(", "np", ".", "sum", "(", "values", ")", ")", "\n", "hist", ".", "sum_squares", "=", "float", "(", "np", ".", "sum", "(", "values", "**", "2", ")", ")", "\n", "\n", "# Drop the start of the first bin", "\n", "bin_edges", "=", "bin_edges", "[", "1", ":", "]", "\n", "\n", "# Add bin edges and counts", "\n", "for", "edge", "in", "bin_edges", ":", "\n", "            ", "hist", ".", "bucket_limit", ".", "append", "(", "edge", ")", "\n", "", "for", "c", "in", "counts", ":", "\n", "            ", "hist", ".", "bucket", ".", "append", "(", "c", ")", "\n", "\n", "# Create and write Summary", "\n", "", "summary", "=", "tf", ".", "Summary", "(", "value", "=", "[", "tf", ".", "Summary", ".", "Value", "(", "tag", "=", "tag", ",", "histo", "=", "hist", ")", "]", ")", "\n", "self", ".", "writer", ".", "add_summary", "(", "summary", ",", "step", ")", "\n", "self", ".", "writer", ".", "flush", "(", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.train.get_parser": [[21, 65], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "parser.parse_args.gpu.split", "print"], "function", ["None"], ["def", "get_parser", "(", "args", "=", "None", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Simple training script for training a RetinaNet network.'", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "help", "=", "'Number of epochs'", ",", "type", "=", "int", ",", "default", "=", "50", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "help", "=", "'batch_size'", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "'--num_input_imgs'", ",", "help", "=", "'num of input images'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--num_gt_imgs'", ",", "help", "=", "'num of ground truth images'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--size_image'", ",", "help", "=", "'input image size'", ",", "type", "=", "int", ",", "default", "=", "128", ")", "\n", "parser", ".", "add_argument", "(", "'--audio_encoder'", ",", "help", "=", "'audio encoder network'", ",", "type", "=", "str", ",", "default", "=", "'reduce'", ")", "\n", "parser", ".", "add_argument", "(", "'--img_encoder'", ",", "help", "=", "'image encoder network'", ",", "type", "=", "str", ",", "default", "=", "'reduce'", ")", "\n", "parser", ".", "add_argument", "(", "'--img_decoder'", ",", "help", "=", "'image decoder network'", ",", "type", "=", "str", ",", "default", "=", "'reduce'", ")", "\n", "parser", ".", "add_argument", "(", "'--rnn_type'", ",", "help", "=", "'type of RNN: GRU'", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--discriminator'", ",", "help", "=", "'type of discriminator'", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--discriminator_lip'", ",", "help", "=", "'type of discriminator'", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--discriminator_v'", ",", "help", "=", "'type of discriminator'", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--D_v_weight'", ",", "help", "=", "'video discriminator weight'", ",", "type", "=", "float", ",", "default", "=", "0.01", ")", "\n", "parser", ".", "add_argument", "(", "'--D_lip_weight'", ",", "help", "=", "'lip read discriminator weight'", ",", "type", "=", "float", ",", "default", "=", "0.001", ")", "\n", "parser", ".", "add_argument", "(", "'--ckpt'", ",", "help", "=", "'pretrained model'", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--ckpt_lipmodel'", ",", "help", "=", "'pretrained lip read model'", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--save_dir'", ",", "help", "=", "'saving folder path'", ",", "type", "=", "str", ",", "default", "=", "'save'", ")", "\n", "parser", ".", "add_argument", "(", "'--test_dir'", ",", "help", "=", "'testing sample path'", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--filename_list'", ",", "help", "=", "'the training filename list'", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--teacher_force_ratio'", ",", "help", "=", "'ratio for using target image as input for rnn seq'", ",", "type", "=", "float", ",", "default", "=", "0.5", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "help", "=", "'learn rate'", ",", "type", "=", "float", ",", "default", "=", "0.0002", ")", "\n", "parser", ".", "add_argument", "(", "'--num_output_length'", ",", "help", "=", "'the length of feature'", ",", "type", "=", "int", ",", "default", "=", "512", ")", "\n", "parser", ".", "add_argument", "(", "'--num_seq_length'", ",", "help", "=", "'maximum length of input sequence'", ",", "type", "=", "int", ",", "default", "=", "100", ")", "\n", "parser", ".", "add_argument", "(", "'--num_frames_D'", ",", "help", "=", "'num of frames for video discriminator'", ",", "type", "=", "int", ",", "default", "=", "20", ")", "\n", "parser", ".", "add_argument", "(", "'--num_frames_lipNet'", ",", "help", "=", "'num of input frames for lip read'", ",", "type", "=", "int", ",", "default", "=", "11", ")", "\n", "parser", ".", "add_argument", "(", "'--if_vgg'", ",", "help", "=", "'if use vgg feature'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--use_npy'", ",", "help", "=", "'if use npy input'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--use_seq'", ",", "help", "=", "'if use seq input'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--use_word_label'", ",", "help", "=", "'if use word label'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--use_lip'", ",", "help", "=", "'if use lip coordinate'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--is_train'", ",", "help", "=", "'if train'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--is_region'", ",", "help", "=", "'if train with only mouth region'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--if_tanh'", ",", "help", "=", "'if use tanh'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--optimizer'", ",", "help", "=", "'the optimizer'", ",", "type", "=", "str", ",", "default", "=", "'ADAM'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu'", ",", "help", "=", "'which gpu to use'", ",", "type", "=", "str", ",", "default", "=", "'0'", ")", "\n", "\n", "config", "=", "parser", ".", "parse_args", "(", "args", ")", "\n", "config", ".", "gpu", "=", "config", ".", "gpu", ".", "split", "(", "','", ")", "\n", "print", "(", "\"Using gpu: \"", ",", "config", ".", "gpu", ")", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.train.train_cnn": [[66, 245], ["len", "model_G.LipGeneratorCNN", "model_D.DiscriminatorLip", "model_D.Discriminator", "model_D.DiscriminatorVideo", "loss.GAN_LR_Loss", "loss.GANLoss", "loss.ReconLoss", "torch.Adam", "adversarial_loss.cuda.cuda", "adversarial_loss_lip.cuda.cuda", "recon_loss.cuda.cuda", "tensorboardX.SummaryWriter", "range", "isinstance", "dataloader.NpySeqDataset", "torch.utils.data.DataLoader", "train.load_ckpt", "train.load_ckpt", "G_model.cuda.parameters", "torch.Adam", "torch.Adam", "torch.Adam", "torch.nn.DataParallel().cuda", "torch.nn.DataParallel().cuda", "torch.nn.DataParallel().cuda", "torch.nn.DataParallel().cuda", "torch.nn.DataParallel().cuda", "torch.nn.DataParallel().cuda", "torch.nn.DataParallel().cuda", "torch.nn.DataParallel().cuda", "G_model.cuda.cuda", "D_model.cuda.cuda", "D_v_model.cuda.cuda", "D_model_lip.cuda.cuda", "G_model.cuda.train", "D_model.cuda.train", "D_v_model.cuda.train", "D_model_lip.cuda.train", "enumerate", "train.sample", "train.test", "isinstance", "torch.save", "torch.save", "torch.save", "torch.save", "dataloader.SeqDataset", "torch.utils.data.DataLoader", "dataloader.CSVDataset", "torch.utils.data.DataLoader", "D_model.cuda.parameters", "D_model_lip.cuda.parameters", "D_v_model.cuda.parameters", "torch.save", "torch.save", "torch.save", "torch.save", "G_model.cuda.module.state_dict", "os.path.join", "G_model.cuda.state_dict", "os.path.join", "torchvision.transforms.Compose", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "data[].cuda", "data[].cuda", "data[].cuda", "G_model.cuda.", "optim.Adam.zero_grad", "G_loss.backward", "optim.Adam.step", "G_model.cuda.module.state_dict", "os.path.join", "torch.save", "torch.save", "G_model.cuda.state_dict", "os.path.join", "torch.save", "torch.save", "torchvision.transforms.Compose", "torchvision.transforms.Compose", "len", "data[].cuda", "data[].cuda", "data[].cuda", "recon_loss.cuda.", "recon_loss.cuda.", "adversarial_loss.cuda.", "adversarial_loss.cuda.", "adversarial_loss.cuda.", "G_images.contiguous().view.contiguous().view", "gts.contiguous().view.contiguous().view", "train.get_clip_range", "adversarial_loss.cuda.", "adversarial_loss.cuda.", "adversarial_loss.cuda.", "data[].contiguous().view", "train.get_clip_range", "adversarial_loss_lip.cuda.", "adversarial_loss_lip.cuda.", "adversarial_loss_lip.cuda.", "optim.Adam.zero_grad", "D_loss.backward", "optim.Adam.step", "optim.Adam.zero_grad", "D_loss_lip.backward", "optim.Adam.step", "train.test", "print", "tensorboardX.SummaryWriter.add_scalar", "print", "traceback.print_exc", "D_model_lip.cuda.module.state_dict", "os.path.join", "D_model_lip.cuda.state_dict", "os.path.join", "dataloader.Resizer", "dataloader.Normalizer", "dataloader.ToTensor", "list", "list", "list", "list", "G_images.contiguous().view.contiguous().view", "gts.contiguous().view.contiguous().view", "D_model.cuda.", "D_model.cuda.", "D_model.cuda.", "D_v_model.cuda.", "D_v_model.cuda.", "D_v_model.cuda.", "len", "G_images.contiguous().view.contiguous().view", "gts.contiguous().view.contiguous().view", "D_model_lip.cuda.", "D_model_lip.cuda.", "D_model_lip.cuda.", "print", "tensorboardX.SummaryWriter.add_scalar", "print", "tensorboardX.SummaryWriter.add_scalar", "print", "tensorboardX.SummaryWriter.add_scalar", "dataloader.Resizer", "dataloader.Normalizer", "dataloader.ToTensor", "dataloader.Resizer", "dataloader.Normalizer", "dataloader.ToTensor", "range", "range", "range", "range", "data[].cuda", "G_images.contiguous().view.detach", "G_images.contiguous().view.contiguous", "gts.contiguous().view.contiguous", "get_clip_range.cuda", "get_clip_range.cuda", "G_images.contiguous().view.detach", "get_clip_range.cuda", "data[].contiguous", "get_clip_range.cuda", "data[].contiguous().view.cuda", "data[].cuda", "get_clip_range.cuda", "data[].contiguous().view.cuda", "data[].cuda", "G_images.contiguous().view.detach", "get_clip_range.cuda", "data[].contiguous().view.cuda", "data[].cuda", "float", "G_images.contiguous().view.contiguous", "gts.contiguous().view.contiguous", "G_images.contiguous().view.contiguous", "gts.contiguous().view.contiguous", "float", "float", "float", "float", "float", "float"], "function", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.test.load_ckpt", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.test.load_ckpt", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.train.sample", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.test.test", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.train.get_clip_range", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.train.get_clip_range", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.test.test"], ["", "def", "train_cnn", "(", "config", ")", ":", "\n", "    ", "num_gpu", "=", "len", "(", "config", ".", "gpu", ")", "\n", "if", "config", ".", "use_npy", ":", "\n", "        ", "dataset_train", "=", "NpySeqDataset", "(", "train_file", "=", "config", ".", "filename_list", ",", "config", "=", "config", ",", "transform", "=", "transforms", ".", "Compose", "(", "[", "Resizer", "(", "config", ".", "size_image", ")", ",", "Normalizer", "(", ")", ",", "ToTensor", "(", ")", "]", ")", ")", "\n", "dataloader", "=", "DataLoader", "(", "dataset_train", ",", "num_workers", "=", "8", ",", "pin_memory", "=", "False", ",", "collate_fn", "=", "convert_seq_to_batch", ",", "batch_size", "=", "config", ".", "batch_size", ",", "shuffle", "=", "True", ")", "\n", "", "elif", "config", ".", "use_seq", ":", "\n", "        ", "dataset_train", "=", "SeqDataset", "(", "train_file", "=", "config", ".", "filename_list", ",", "\n", "use_mask", "=", "None", ",", "\n", "config", "=", "config", ",", "\n", "transform", "=", "transforms", ".", "Compose", "(", "[", "Resizer", "(", "config", ".", "size_image", ")", ",", "Normalizer", "(", ")", ",", "ToTensor", "(", ")", "]", ")", ")", "\n", "dataloader", "=", "DataLoader", "(", "dataset_train", ",", "num_workers", "=", "8", ",", "collate_fn", "=", "convert_seq_to_batch", ",", "batch_size", "=", "config", ".", "batch_size", ",", "shuffle", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "dataset_train", "=", "CSVDataset", "(", "train_file", "=", "config", ".", "filename_list", ",", "\n", "use_mask", "=", "None", ",", "\n", "num_input_imgs", "=", "config", ".", "num_input_imgs", ",", "\n", "num_gt_imgs", "=", "config", ".", "num_gt_imgs", ",", "\n", "transform", "=", "transforms", ".", "Compose", "(", "[", "Resizer", "(", "config", ".", "size_image", ")", ",", "Normalizer", "(", ")", ",", "ToTensor", "(", ")", "]", ")", ")", "\n", "\n", "dataloader", "=", "DataLoader", "(", "dataset_train", ",", "num_workers", "=", "8", ",", "pin_memory", "=", "False", ",", "batch_size", "=", "config", ".", "batch_size", ",", "shuffle", "=", "True", ")", "\n", "\n", "", "G_model", "=", "model_G", ".", "LipGeneratorCNN", "(", "config", ".", "audio_encoder", ",", "config", ".", "img_encoder", ",", "config", ".", "img_decoder", ",", "config", ".", "size_image", ",", "config", ".", "num_output_length", ",", "config", ".", "if_tanh", ")", "\n", "D_model_lip", "=", "model_D", ".", "DiscriminatorLip", "(", "config", ")", "\n", "D_model", "=", "model_D", ".", "Discriminator", "(", "config", ")", "\n", "D_v_model", "=", "model_D", ".", "DiscriminatorVideo", "(", "config", ")", "\n", "\n", "\n", "adversarial_loss_lip", "=", "loss", ".", "GAN_LR_Loss", "(", ")", "\n", "adversarial_loss", "=", "loss", ".", "GANLoss", "(", ")", "\n", "recon_loss", "=", "loss", ".", "ReconLoss", "(", ")", "\n", "\n", "\n", "if", "config", ".", "ckpt", "!=", "None", ":", "\n", "        ", "load_ckpt", "(", "G_model", ",", "config", ".", "ckpt", ")", "\n", "", "if", "config", ".", "discriminator_lip", "==", "'lip_read'", ":", "\n", "        ", "load_ckpt", "(", "D_model_lip", ",", "config", ".", "ckpt_lipmodel", ",", "prefix", "=", "'discriminator.'", ")", "\n", "\n", "# optimizer = optim.Adam(filter(lambda p: p.requires_grad, G_model.parameters()), lr=0.002, betas=(0.5, 0.999))", "\n", "", "optimizer_G", "=", "optim", ".", "Adam", "(", "G_model", ".", "parameters", "(", ")", ",", "lr", "=", "config", ".", "lr", ",", "betas", "=", "(", "0.5", ",", "0.999", ")", ")", "\n", "if", "config", ".", "discriminator", "is", "not", "None", ":", "\n", "        ", "optimizer_D", "=", "optim", ".", "Adam", "(", "D_model", ".", "parameters", "(", ")", ",", "lr", "=", "0.001", ",", "betas", "=", "(", "0.5", ",", "0.999", ")", ")", "\n", "", "if", "config", ".", "discriminator_lip", "is", "not", "None", ":", "\n", "        ", "optimizer_D_lip", "=", "optim", ".", "Adam", "(", "D_model_lip", ".", "parameters", "(", ")", ",", "lr", "=", "0.001", ",", "betas", "=", "(", "0.5", ",", "0.999", ")", ")", "\n", "", "if", "config", ".", "discriminator_v", "is", "not", "None", ":", "\n", "        ", "optimizer_D_v", "=", "optim", ".", "Adam", "(", "D_v_model", ".", "parameters", "(", ")", ",", "lr", "=", "0.001", ",", "betas", "=", "(", "0.5", ",", "0.999", ")", ")", "\n", "\n", "\n", "", "if", "num_gpu", ">", "1", ":", "\n", "        ", "G_model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "G_model", ",", "device_ids", "=", "list", "(", "range", "(", "num_gpu", ")", ")", ")", ".", "cuda", "(", ")", "\n", "D_model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "D_model", ",", "device_ids", "=", "list", "(", "range", "(", "num_gpu", ")", ")", ")", ".", "cuda", "(", ")", "\n", "D_v_model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "D_v_model", ",", "device_ids", "=", "list", "(", "range", "(", "num_gpu", ")", ")", ")", ".", "cuda", "(", ")", "\n", "D_model_lip", "=", "torch", ".", "nn", ".", "DataParallel", "(", "D_model_lip", ",", "device_ids", "=", "list", "(", "range", "(", "num_gpu", ")", ")", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "        ", "G_model", "=", "G_model", ".", "cuda", "(", ")", "\n", "D_model", "=", "D_model", ".", "cuda", "(", ")", "\n", "D_v_model", "=", "D_v_model", ".", "cuda", "(", ")", "\n", "D_model_lip", "=", "D_model_lip", ".", "cuda", "(", ")", "\n", "\n", "", "adversarial_loss", "=", "adversarial_loss", ".", "cuda", "(", ")", "\n", "adversarial_loss_lip", "=", "adversarial_loss_lip", ".", "cuda", "(", ")", "\n", "recon_loss", "=", "recon_loss", ".", "cuda", "(", ")", "\n", "\n", "\n", "writer", "=", "SummaryWriter", "(", "log_dir", "=", "config", ".", "save_dir", ")", "\n", "\n", "sample_inputs", "=", "None", "\n", "for", "epoch_num", "in", "range", "(", "config", ".", "epochs", ")", ":", "\n", "        ", "G_model", ".", "train", "(", ")", "\n", "D_model", ".", "train", "(", ")", "\n", "D_v_model", ".", "train", "(", ")", "\n", "D_model_lip", ".", "train", "(", ")", "\n", "for", "iter_num", ",", "data", "in", "enumerate", "(", "dataloader", ")", ":", "\n", "            ", "n_iter", "=", "len", "(", "dataloader", ")", "*", "epoch_num", "+", "iter_num", "\n", "if", "sample_inputs", "==", "None", ":", "\n", "                ", "sample_inputs", "=", "(", "data", "[", "'img'", "]", ".", "cuda", "(", ")", ",", "data", "[", "'audio'", "]", ".", "cuda", "(", ")", ",", "data", "[", "'gt'", "]", ".", "cuda", "(", ")", ")", "\n", "", "try", ":", "\n", "                ", "input_images", "=", "data", "[", "'img'", "]", ".", "cuda", "(", ")", "\n", "input_audios", "=", "data", "[", "'audio'", "]", ".", "cuda", "(", ")", "\n", "gts", "=", "data", "[", "'gt'", "]", ".", "cuda", "(", ")", "\n", "\n", "G_images", "=", "G_model", "(", "input_images", ",", "input_audios", ")", "\n", "\n", "if", "config", ".", "use_seq", "or", "config", ".", "use_npy", ":", "\n", "# G_images = G_images.contiguous().view(config.batch_size, -1, G_images.shape[1], G_images.shape[2], G_images.shape[3])", "\n", "                    ", "loss_EG", "=", "recon_loss", "(", "G_images", ".", "contiguous", "(", ")", ".", "view", "(", "config", ".", "batch_size", ",", "-", "1", ",", "G_images", ".", "shape", "[", "1", "]", ",", "G_images", ".", "shape", "[", "2", "]", ",", "G_images", ".", "shape", "[", "3", "]", ")", ",", "\n", "gts", ".", "contiguous", "(", ")", ".", "view", "(", "config", ".", "batch_size", ",", "-", "1", ",", "gts", ".", "shape", "[", "1", "]", ",", "gts", ".", "shape", "[", "2", "]", ",", "gts", ".", "shape", "[", "3", "]", ")", ",", "\n", "valid_len", "=", "data", "[", "'len'", "]", ".", "cuda", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "loss_EG", "=", "recon_loss", "(", "G_images", ",", "gts", ")", "\n", "", "G_loss", "=", "loss_EG", "\n", "\n", "if", "config", ".", "discriminator", "is", "not", "None", ":", "\n", "                    ", "loss_G_GAN", "=", "adversarial_loss", "(", "D_model", "(", "G_images", ")", ",", "is_real", "=", "True", ")", "\n", "loss_D_real", "=", "adversarial_loss", "(", "D_model", "(", "gts", ")", ",", "is_real", "=", "True", ")", "\n", "loss_D_fake", "=", "adversarial_loss", "(", "D_model", "(", "G_images", ".", "detach", "(", ")", ")", ",", "is_real", "=", "False", ")", "\n", "G_loss", "=", "G_loss", "+", "0.002", "*", "loss_G_GAN", "\n", "D_loss", "=", "loss_D_real", "+", "loss_D_fake", "\n", "\n", "", "if", "config", ".", "discriminator_v", "is", "not", "None", ":", "\n", "# reshape G_images", "\n", "                    ", "G_images", "=", "G_images", ".", "contiguous", "(", ")", ".", "view", "(", "config", ".", "batch_size", ",", "-", "1", ",", "G_images", ".", "shape", "[", "1", "]", ",", "G_images", ".", "shape", "[", "2", "]", ",", "G_images", ".", "shape", "[", "3", "]", ")", "\n", "gts", "=", "gts", ".", "contiguous", "(", ")", ".", "view", "(", "config", ".", "batch_size", ",", "-", "1", ",", "gts", ".", "shape", "[", "1", "]", ",", "gts", ".", "shape", "[", "2", "]", ",", "gts", ".", "shape", "[", "3", "]", ")", "\n", "\n", "clip_range", "=", "get_clip_range", "(", "data", "[", "'len'", "]", ",", "config", ".", "num_frames_D", ")", "\n", "loss_G_GAN_v", "=", "adversarial_loss", "(", "D_v_model", "(", "G_images", ",", "config", ".", "num_frames_D", ",", "clip_range", ".", "cuda", "(", ")", ")", ",", "is_real", "=", "True", ")", "\n", "loss_D_real_v", "=", "adversarial_loss", "(", "D_v_model", "(", "gts", ",", "config", ".", "num_frames_D", ",", "clip_range", ".", "cuda", "(", ")", ")", ",", "is_real", "=", "True", ")", "\n", "loss_D_fake_v", "=", "adversarial_loss", "(", "D_v_model", "(", "G_images", ".", "detach", "(", ")", ",", "config", ".", "num_frames_D", ",", "clip_range", ".", "cuda", "(", ")", ")", ",", "is_real", "=", "False", ")", "\n", "G_loss", "=", "G_loss", "+", "config", ".", "D_v_weight", "*", "loss_G_GAN_v", "\n", "D_loss_v", "=", "loss_D_real_v", "+", "loss_D_fake_v", "\n", "\n", "", "if", "config", ".", "discriminator_lip", "is", "not", "None", ":", "\n", "# reshape G_images", "\n", "                    ", "if", "len", "(", "G_images", ".", "shape", ")", "!=", "5", ":", "\n", "                        ", "G_images", "=", "G_images", ".", "contiguous", "(", ")", ".", "view", "(", "config", ".", "batch_size", ",", "-", "1", ",", "G_images", ".", "shape", "[", "1", "]", ",", "G_images", ".", "shape", "[", "2", "]", ",", "G_images", ".", "shape", "[", "3", "]", ")", "\n", "gts", "=", "gts", ".", "contiguous", "(", ")", ".", "view", "(", "config", ".", "batch_size", ",", "-", "1", ",", "gts", ".", "shape", "[", "1", "]", ",", "gts", ".", "shape", "[", "2", "]", ",", "gts", ".", "shape", "[", "3", "]", ")", "\n", "", "lip_coord", "=", "data", "[", "'lip'", "]", ".", "contiguous", "(", ")", ".", "view", "(", "config", ".", "batch_size", ",", "-", "1", ",", "4", ")", "\n", "\n", "clip_range", "=", "get_clip_range", "(", "data", "[", "'len'", "]", ",", "config", ".", "num_frames_lipNet", ")", "\n", "\n", "loss_G_GAN_lip", "=", "adversarial_loss_lip", "(", "D_model_lip", "(", "G_images", ",", "clip_range", ".", "cuda", "(", ")", ",", "lip_coord", ".", "cuda", "(", ")", ")", ",", "is_real", "=", "True", ",", "targets", "=", "data", "[", "'label'", "]", ".", "cuda", "(", ")", ")", "\n", "loss_D_real_lip", "=", "adversarial_loss_lip", "(", "D_model_lip", "(", "gts", ",", "clip_range", ".", "cuda", "(", ")", ",", "lip_coord", ".", "cuda", "(", ")", ")", ",", "is_real", "=", "True", ",", "targets", "=", "data", "[", "'label'", "]", ".", "cuda", "(", ")", ")", "\n", "loss_D_fake_lip", "=", "adversarial_loss_lip", "(", "D_model_lip", "(", "G_images", ".", "detach", "(", ")", ",", "clip_range", ".", "cuda", "(", ")", ",", "lip_coord", ".", "cuda", "(", ")", ")", ",", "is_real", "=", "False", ",", "targets", "=", "data", "[", "'label'", "]", ".", "cuda", "(", ")", ")", "\n", "\n", "G_loss", "=", "G_loss", "+", "config", ".", "D_lip_weight", "*", "loss_G_GAN_lip", "\n", "D_loss_lip", "=", "loss_D_real_lip", "+", "loss_D_fake_lip", "\n", "\n", "\n", "# for generator", "\n", "", "optimizer_G", ".", "zero_grad", "(", ")", "\n", "G_loss", ".", "backward", "(", ")", "\n", "optimizer_G", ".", "step", "(", ")", "\n", "\n", "# for discriminator", "\n", "if", "config", ".", "discriminator", "is", "not", "None", ":", "\n", "                    ", "optimizer_D", ".", "zero_grad", "(", ")", "\n", "D_loss", ".", "backward", "(", ")", "\n", "optimizer_D", ".", "step", "(", ")", "\n", "\n", "", "if", "config", ".", "discriminator_lip", "is", "not", "None", ":", "\n", "                    ", "optimizer_D_lip", ".", "zero_grad", "(", ")", "\n", "D_loss_lip", ".", "backward", "(", ")", "\n", "optimizer_D_lip", ".", "step", "(", ")", "\n", "\n", "", "if", "iter_num", "%", "200", "==", "0", ":", "\n", "                    ", "test", "(", "G_model", ",", "config", ".", "test_dir", ",", "config", ".", "save_dir", ",", "config", ".", "size_image", ")", "\n", "\n", "", "if", "iter_num", "%", "20", "==", "0", ":", "\n", "                    ", "print", "(", "'Epoch: {} | Iteration: {} | EG loss: {:1.5f}: '", ".", "format", "(", "epoch_num", ",", "iter_num", ",", "float", "(", "loss_EG", ")", ")", ")", "\n", "if", "config", ".", "discriminator", "is", "not", "None", ":", "\n", "                        ", "print", "(", "'D loss {:1.5f} | G_GAN loss {:1.5f} : '", ".", "format", "(", "float", "(", "D_loss", ")", ",", "float", "(", "loss_G_GAN", ")", ")", ")", "\n", "writer", ".", "add_scalar", "(", "'D_loss'", ",", "D_loss", ",", "n_iter", ")", "\n", "", "if", "config", ".", "discriminator_v", "is", "not", "None", ":", "\n", "                        ", "print", "(", "'D_v loss: {:1.5f} | G_GAN_v loss {:1.5f} : '", ".", "format", "(", "float", "(", "D_loss_v", ")", ",", "float", "(", "loss_G_GAN_v", ")", ")", ")", "\n", "writer", ".", "add_scalar", "(", "'D_loss_v'", ",", "D_loss_v", ",", "n_iter", ")", "\n", "", "if", "config", ".", "discriminator_lip", "is", "not", "None", ":", "\n", "                        ", "print", "(", "'D_lip loss {:1.5f} | G_GAN_lip loss {:1.5f} : '", ".", "format", "(", "float", "(", "D_loss_lip", ")", ",", "float", "(", "loss_G_GAN_lip", ")", ")", ")", "\n", "writer", ".", "add_scalar", "(", "'D_loss_lip'", ",", "D_loss_lip", ",", "n_iter", ")", "\n", "", "writer", ".", "add_scalar", "(", "'EG_loss'", ",", "loss_EG", ",", "n_iter", ")", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "                ", "print", "(", "e", ")", "\n", "traceback", ".", "print_exc", "(", ")", "\n", "\n", "# visualize some results", "\n", "", "", "sample", "(", "sample_inputs", ",", "G_model", ",", "epoch_num", ",", "config", ".", "save_dir", ")", "\n", "test", "(", "G_model", ",", "config", ".", "test_dir", ",", "config", ".", "save_dir", ",", "config", ".", "size_image", ")", "\n", "\n", "if", "isinstance", "(", "G_model", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "            ", "torch", ".", "save", "(", "G_model", ".", "module", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "config", ".", "save_dir", ",", "'model_G{}.pt'", ".", "format", "(", "epoch_num", ")", ")", ")", "\n", "if", "config", ".", "discriminator_lip", "is", "not", "None", ":", "\n", "                ", "torch", ".", "save", "(", "D_model_lip", ".", "module", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "config", ".", "save_dir", ",", "'model_D{}.pt'", ".", "format", "(", "epoch_num", ")", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "torch", ".", "save", "(", "G_model", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "config", ".", "save_dir", ",", "'model_G{}.pt'", ".", "format", "(", "epoch_num", ")", ")", ")", "\n", "if", "config", ".", "discriminator_lip", "is", "not", "None", ":", "\n", "                ", "torch", ".", "save", "(", "D_model_lip", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "config", ".", "save_dir", ",", "'model_D{}.pt'", ".", "format", "(", "epoch_num", ")", ")", ")", "\n", "\n", "\n", "", "", "", "if", "isinstance", "(", "G_model", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "        ", "torch", ".", "save", "(", "G_model", ".", "module", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "config", ".", "save_dir", ",", "'model_G_final.pt'", ".", "format", "(", "epoch_num", ")", ")", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "save", "(", "G_model", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "config", ".", "save_dir", ",", "'model_G_final.pt'", ".", "format", "(", "epoch_num", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.train.train_rnn": [[249, 414], ["len", "torch.utils.data.DataLoader", "model_G.LipGeneratorRNN", "model_D.Discriminator", "model_D.DiscriminatorVideo", "model_D.DiscriminatorLip", "loss.GAN_LR_Loss", "loss.GANLoss", "loss.ReconLoss", "torch.Adam", "adversarial_lip_loss.cuda.cuda", "adversarial_loss.cuda.cuda", "recon_loss.cuda.cuda", "tensorboardX.SummaryWriter", "range", "isinstance", "dataloader.NpySeqDataset", "train.load_ckpt", "train.load_ckpt", "G_model.cuda.parameters", "torch.Adam", "torch.Adam", "torch.Adam", "torch.nn.DataParallel().cuda", "torch.nn.DataParallel().cuda", "torch.nn.DataParallel().cuda", "torch.nn.DataParallel().cuda", "torch.nn.DataParallel().cuda", "torch.nn.DataParallel().cuda", "torch.nn.DataParallel().cuda", "torch.nn.DataParallel().cuda", "G_model.cuda.cuda", "D_model.cuda.cuda", "D_model_lip.cuda.cuda", "D_v_model.cuda.cuda", "G_model.cuda.train", "D_model.cuda.train", "D_v_model.cuda.train", "D_model_lip.cuda.train", "enumerate", "train.sample", "train.test", "isinstance", "torch.save", "torch.save", "torch.save", "torch.save", "dataloader.SeqDataset", "D_model.cuda.parameters", "D_v_model.cuda.parameters", "D_model_lip.cuda.parameters", "torch.save", "torch.save", "torch.save", "torch.save", "G_model.cuda.module.state_dict", "os.path.join", "G_model.cuda.state_dict", "os.path.join", "torchvision.transforms.Compose", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "data[].cuda", "data[].cuda", "data[].cuda", "G_model.cuda.", "recon_loss.cuda.", "optim.Adam.zero_grad", "G_loss.backward", "optim.Adam.step", "G_model.cuda.module.state_dict", "os.path.join", "torch.save", "torch.save", "G_model.cuda.state_dict", "os.path.join", "torch.save", "torch.save", "torchvision.transforms.Compose", "len", "data[].cuda", "data[].cuda", "data[].cuda", "data[].cuda", "adversarial_loss.cuda.", "adversarial_loss.cuda.", "adversarial_loss.cuda.", "train.get_clip_range", "adversarial_loss.cuda.", "adversarial_loss.cuda.", "adversarial_loss.cuda.", "train.get_clip_range", "adversarial_lip_loss.cuda.", "adversarial_lip_loss.cuda.", "adversarial_lip_loss.cuda.", "optim.Adam.zero_grad", "D_loss.backward", "optim.Adam.step", "optim.Adam.zero_grad", "D_loss_lip.backward", "optim.Adam.step", "optim.Adam.zero_grad", "D_loss_v.backward", "optim.Adam.step", "print", "tensorboardX.SummaryWriter.add_scalar", "print", "traceback.print_exc", "D_model_lip.cuda.module.state_dict", "os.path.join", "D_model_lip.cuda.state_dict", "os.path.join", "dataloader.Resizer", "dataloader.Normalizer", "dataloader.ToTensor", "list", "list", "list", "list", "data[].cuda", "data[].cuda", "D_model.cuda.", "D_model.cuda.", "D_model.cuda.", "D_v_model.cuda.", "D_v_model.cuda.", "D_v_model.cuda.", "D_model_lip.cuda.", "D_model_lip.cuda.", "D_model_lip.cuda.", "print", "tensorboardX.SummaryWriter.add_scalar", "print", "tensorboardX.SummaryWriter.add_scalar", "print", "tensorboardX.SummaryWriter.add_scalar", "dataloader.Resizer", "dataloader.Normalizer", "dataloader.ToTensor", "range", "range", "range", "range", "G_model.detach", "get_clip_range.cuda", "get_clip_range.cuda", "G_model.detach", "get_clip_range.cuda", "get_clip_range.cuda", "data[].cuda", "data[].cuda", "get_clip_range.cuda", "data[].cuda", "data[].cuda", "G_model.detach", "get_clip_range.cuda", "data[].cuda", "data[].cuda", "float", "float", "float", "float", "float", "float", "float"], "function", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.test.load_ckpt", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.test.load_ckpt", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.train.sample", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.test.test", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.train.get_clip_range", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.train.get_clip_range"], ["", "", "def", "train_rnn", "(", "config", ")", ":", "\n", "    ", "num_gpu", "=", "len", "(", "config", ".", "gpu", ")", "\n", "if", "config", ".", "use_npy", ":", "\n", "        ", "dataset_train", "=", "NpySeqDataset", "(", "train_file", "=", "config", ".", "filename_list", ",", "config", "=", "config", ",", "transform", "=", "transforms", ".", "Compose", "(", "[", "Resizer", "(", "config", ".", "size_image", ")", ",", "Normalizer", "(", ")", ",", "ToTensor", "(", ")", "]", ")", ")", "\n", "", "elif", "config", ".", "use_seq", ":", "\n", "        ", "dataset_train", "=", "SeqDataset", "(", "train_file", "=", "config", ".", "filename_list", ",", "\n", "use_mask", "=", "None", ",", "\n", "config", "=", "config", ",", "\n", "transform", "=", "transforms", ".", "Compose", "(", "[", "Resizer", "(", "config", ".", "size_image", ")", ",", "Normalizer", "(", ")", ",", "ToTensor", "(", ")", "]", ")", ")", "\n", "", "dataloader", "=", "DataLoader", "(", "dataset_train", ",", "num_workers", "=", "8", ",", "collate_fn", "=", "collater", ",", "batch_size", "=", "config", ".", "batch_size", ",", "shuffle", "=", "True", ")", "\n", "\n", "G_model", "=", "model_G", ".", "LipGeneratorRNN", "(", "config", ".", "audio_encoder", ",", "config", ".", "img_encoder", ",", "config", ".", "img_decoder", ",", "config", ".", "rnn_type", ",", "\n", "config", ".", "size_image", ",", "config", ".", "num_output_length", ",", "if_tanh", "=", "config", ".", "if_tanh", ")", "\n", "D_model", "=", "model_D", ".", "Discriminator", "(", "config", ")", "\n", "D_v_model", "=", "model_D", ".", "DiscriminatorVideo", "(", "config", ")", "\n", "D_model_lip", "=", "model_D", ".", "DiscriminatorLip", "(", "config", ")", "\n", "\n", "\n", "\n", "adversarial_lip_loss", "=", "loss", ".", "GAN_LR_Loss", "(", ")", "\n", "adversarial_loss", "=", "loss", ".", "GANLoss", "(", ")", "\n", "recon_loss", "=", "loss", ".", "ReconLoss", "(", ")", "\n", "\n", "\n", "if", "config", ".", "ckpt", "is", "not", "None", ":", "\n", "        ", "load_ckpt", "(", "G_model", ",", "config", ".", "ckpt", ")", "\n", "", "if", "config", ".", "discriminator_lip", "==", "'lip_read'", ":", "\n", "        ", "load_ckpt", "(", "D_model_lip", ",", "config", ".", "ckpt_lipmodel", ",", "prefix", "=", "'discriminator.'", ")", "\n", "\n", "# optimizer = optim.Adam(filter(lambda p: p.requires_grad, G_model.parameters()), lr=0.002, betas=(0.5, 0.999))", "\n", "", "optimizer_G", "=", "optim", ".", "Adam", "(", "G_model", ".", "parameters", "(", ")", ",", "lr", "=", "config", ".", "lr", ",", "betas", "=", "(", "0.5", ",", "0.999", ")", ")", "\n", "if", "config", ".", "discriminator", "is", "not", "None", ":", "\n", "        ", "optimizer_D", "=", "optim", ".", "Adam", "(", "D_model", ".", "parameters", "(", ")", ",", "lr", "=", "0.001", ",", "betas", "=", "(", "0.5", ",", "0.999", ")", ")", "\n", "", "if", "config", ".", "discriminator_v", "is", "not", "None", ":", "\n", "        ", "optimizer_D_v", "=", "optim", ".", "Adam", "(", "D_v_model", ".", "parameters", "(", ")", ",", "lr", "=", "0.001", ",", "betas", "=", "(", "0.5", ",", "0.999", ")", ")", "\n", "", "if", "config", ".", "discriminator_lip", "is", "not", "None", ":", "\n", "        ", "optimizer_D_lip", "=", "optim", ".", "Adam", "(", "D_model_lip", ".", "parameters", "(", ")", ",", "lr", "=", "0.001", ",", "betas", "=", "(", "0.5", ",", "0.999", ")", ")", "\n", "\n", "\n", "", "if", "num_gpu", ">", "1", ":", "\n", "        ", "G_model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "G_model", ",", "device_ids", "=", "list", "(", "range", "(", "num_gpu", ")", ")", ")", ".", "cuda", "(", ")", "\n", "D_model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "D_model", ",", "device_ids", "=", "list", "(", "range", "(", "num_gpu", ")", ")", ")", ".", "cuda", "(", ")", "\n", "D_v_model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "D_v_model", ",", "device_ids", "=", "list", "(", "range", "(", "num_gpu", ")", ")", ")", ".", "cuda", "(", ")", "\n", "D_model_lip", "=", "torch", ".", "nn", ".", "DataParallel", "(", "D_model_lip", ",", "device_ids", "=", "list", "(", "range", "(", "num_gpu", ")", ")", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "        ", "G_model", "=", "G_model", ".", "cuda", "(", ")", "\n", "D_model", "=", "D_model", ".", "cuda", "(", ")", "\n", "D_model_lip", "=", "D_model_lip", ".", "cuda", "(", ")", "\n", "D_v_model", "=", "D_v_model", ".", "cuda", "(", ")", "\n", "\n", "", "adversarial_lip_loss", "=", "adversarial_lip_loss", ".", "cuda", "(", ")", "\n", "adversarial_loss", "=", "adversarial_loss", ".", "cuda", "(", ")", "\n", "recon_loss", "=", "recon_loss", ".", "cuda", "(", ")", "\n", "\n", "\n", "\n", "writer", "=", "SummaryWriter", "(", "log_dir", "=", "config", ".", "save_dir", ")", "\n", "\n", "sample_inputs", "=", "None", "\n", "for", "epoch_num", "in", "range", "(", "config", ".", "epochs", ")", ":", "\n", "        ", "G_model", ".", "train", "(", ")", "\n", "D_model", ".", "train", "(", ")", "\n", "D_v_model", ".", "train", "(", ")", "\n", "D_model_lip", ".", "train", "(", ")", "\n", "for", "iter_num", ",", "data", "in", "enumerate", "(", "dataloader", ")", ":", "\n", "            ", "n_iter", "=", "len", "(", "dataloader", ")", "*", "epoch_num", "+", "iter_num", "\n", "if", "sample_inputs", "==", "None", ":", "\n", "                ", "sample_inputs", "=", "(", "data", "[", "'img'", "]", ".", "cuda", "(", ")", ",", "data", "[", "'audio'", "]", ".", "cuda", "(", ")", ",", "data", "[", "'gt'", "]", ".", "cuda", "(", ")", ",", "data", "[", "'len'", "]", ".", "cuda", "(", ")", ")", "\n", "", "try", ":", "\n", "                ", "input_images", "=", "data", "[", "'img'", "]", ".", "cuda", "(", ")", "\n", "input_audios", "=", "data", "[", "'audio'", "]", ".", "cuda", "(", ")", "\n", "gts_orignal", "=", "data", "[", "'gt'", "]", ".", "cuda", "(", ")", "\n", "\n", "G_images_orignal", "=", "G_model", "(", "input_images", ",", "input_audios", ",", "\n", "valid_len", "=", "data", "[", "'len'", "]", ".", "cuda", "(", ")", ",", "\n", "teacher_forcing_ratio", "=", "config", ".", "teacher_force_ratio", ")", "\n", "loss_EG", "=", "recon_loss", "(", "G_images_orignal", ",", "gts_orignal", ",", "valid_len", "=", "data", "[", "'len'", "]", ".", "cuda", "(", ")", ")", "\n", "G_loss", "=", "loss_EG", "\n", "\n", "if", "config", ".", "discriminator", "is", "not", "None", ":", "\n", "                    ", "loss_G_GAN", "=", "adversarial_loss", "(", "D_model", "(", "G_images_orignal", ")", ",", "is_real", "=", "True", ")", "\n", "loss_D_real", "=", "adversarial_loss", "(", "D_model", "(", "gts_orignal", ")", ",", "is_real", "=", "True", ")", "\n", "loss_D_fake", "=", "adversarial_loss", "(", "D_model", "(", "G_images_orignal", ".", "detach", "(", ")", ")", ",", "is_real", "=", "False", ")", "\n", "G_loss", "=", "G_loss", "+", "0.002", "*", "loss_G_GAN", "\n", "D_loss", "=", "loss_D_real", "+", "loss_D_fake", "\n", "\n", "\n", "", "if", "config", ".", "discriminator_v", "is", "not", "None", ":", "\n", "                    ", "clip_range", "=", "get_clip_range", "(", "data", "[", "'len'", "]", ",", "config", ".", "num_frames_D", ")", "\n", "loss_G_GAN_v", "=", "adversarial_loss", "(", "D_v_model", "(", "G_images_orignal", ",", "config", ".", "num_frames_D", ",", "clip_range", ".", "cuda", "(", ")", ")", ",", "is_real", "=", "True", ")", "\n", "loss_D_real_v", "=", "adversarial_loss", "(", "D_v_model", "(", "gts_orignal", ",", "config", ".", "num_frames_D", ",", "clip_range", ".", "cuda", "(", ")", ")", ",", "is_real", "=", "True", ")", "\n", "loss_D_fake_v", "=", "adversarial_loss", "(", "D_v_model", "(", "G_images_orignal", ".", "detach", "(", ")", ",", "config", ".", "num_frames_D", ",", "clip_range", ".", "cuda", "(", ")", ")", ",", "is_real", "=", "False", ")", "\n", "G_loss", "=", "G_loss", "+", "config", ".", "D_v_weight", "*", "loss_G_GAN_v", "\n", "D_loss_v", "=", "loss_D_real_v", "+", "loss_D_fake_v", "\n", "\n", "\n", "", "if", "config", ".", "discriminator_lip", "is", "not", "None", ":", "\n", "                    ", "clip_range", "=", "get_clip_range", "(", "data", "[", "'len'", "]", ",", "config", ".", "num_frames_lipNet", ")", "\n", "\n", "loss_G_GAN_lip", "=", "adversarial_lip_loss", "(", "D_model_lip", "(", "G_images_orignal", ",", "clip_range", ".", "cuda", "(", ")", ",", "data", "[", "'lip'", "]", ".", "cuda", "(", ")", ")", ",", "is_real", "=", "True", ",", "targets", "=", "data", "[", "'label'", "]", ".", "cuda", "(", ")", ")", "\n", "loss_D_real_lip", "=", "adversarial_lip_loss", "(", "D_model_lip", "(", "gts_orignal", ",", "clip_range", ".", "cuda", "(", ")", ",", "data", "[", "'lip'", "]", ".", "cuda", "(", ")", ")", ",", "is_real", "=", "True", ",", "targets", "=", "data", "[", "'label'", "]", ".", "cuda", "(", ")", ")", "\n", "loss_D_fake_lip", "=", "adversarial_lip_loss", "(", "D_model_lip", "(", "G_images_orignal", ".", "detach", "(", ")", ",", "clip_range", ".", "cuda", "(", ")", ",", "data", "[", "'lip'", "]", ".", "cuda", "(", ")", ")", ",", "is_real", "=", "False", ",", "targets", "=", "data", "[", "'label'", "]", ".", "cuda", "(", ")", ")", "\n", "G_loss", "=", "G_loss", "+", "config", ".", "D_lip_weight", "*", "loss_G_GAN_lip", "\n", "D_loss_lip", "=", "loss_D_real_lip", "+", "loss_D_fake_lip", "\n", "\n", "\n", "\n", "# for generator", "\n", "", "optimizer_G", ".", "zero_grad", "(", ")", "\n", "G_loss", ".", "backward", "(", ")", "\n", "optimizer_G", ".", "step", "(", ")", "\n", "\n", "# for discriminator", "\n", "if", "config", ".", "discriminator", "is", "not", "None", ":", "\n", "                    ", "optimizer_D", ".", "zero_grad", "(", ")", "\n", "D_loss", ".", "backward", "(", ")", "\n", "optimizer_D", ".", "step", "(", ")", "\n", "\n", "", "if", "config", ".", "discriminator_lip", "is", "not", "None", ":", "\n", "                    ", "optimizer_D_lip", ".", "zero_grad", "(", ")", "\n", "D_loss_lip", ".", "backward", "(", ")", "\n", "optimizer_D_lip", ".", "step", "(", ")", "\n", "\n", "", "if", "config", ".", "discriminator_v", "is", "not", "None", ":", "\n", "                    ", "optimizer_D_v", ".", "zero_grad", "(", ")", "\n", "D_loss_v", ".", "backward", "(", ")", "\n", "optimizer_D_v", ".", "step", "(", ")", "\n", "\n", "\n", "", "if", "iter_num", "%", "20", "==", "0", ":", "\n", "                    ", "print", "(", "'Epoch: {} | Iteration: {} | EG loss: {:1.5f}: '", ".", "format", "(", "epoch_num", ",", "iter_num", ",", "float", "(", "loss_EG", ")", ")", ")", "\n", "if", "config", ".", "discriminator", "is", "not", "None", ":", "\n", "                        ", "print", "(", "'D loss {:1.5f} | G_GAN loss {:1.5f} : '", ".", "format", "(", "float", "(", "D_loss", ")", ",", "float", "(", "loss_G_GAN", ")", ")", ")", "\n", "writer", ".", "add_scalar", "(", "'D_loss'", ",", "D_loss", ",", "n_iter", ")", "\n", "", "if", "config", ".", "discriminator_v", "is", "not", "None", ":", "\n", "                        ", "print", "(", "'D_v loss: {:1.5f} | G_GAN_v loss {:1.5f} : '", ".", "format", "(", "float", "(", "D_loss_v", ")", ",", "float", "(", "loss_G_GAN_v", ")", ")", ")", "\n", "writer", ".", "add_scalar", "(", "'D_loss_v'", ",", "D_loss_v", ",", "n_iter", ")", "\n", "", "if", "config", ".", "discriminator_lip", "is", "not", "None", ":", "\n", "                        ", "print", "(", "'D_lip loss {:1.5f} | G_GAN_lip loss {:1.5f} : '", ".", "format", "(", "float", "(", "D_loss_lip", ")", ",", "float", "(", "loss_G_GAN_lip", ")", ")", ")", "\n", "writer", ".", "add_scalar", "(", "'D_loss_lip'", ",", "D_loss_lip", ",", "n_iter", ")", "\n", "\n", "", "writer", ".", "add_scalar", "(", "'loss_EG'", ",", "loss_EG", ",", "n_iter", ")", "\n", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "                ", "print", "(", "e", ")", "\n", "traceback", ".", "print_exc", "(", ")", "\n", "\n", "# visualize some results", "\n", "", "", "sample", "(", "sample_inputs", ",", "G_model", ",", "epoch_num", ",", "config", ".", "save_dir", ",", "config", ".", "teacher_force_ratio", ")", "\n", "test", "(", "G_model", ",", "config", ".", "test_dir", ",", "config", ".", "save_dir", ",", "config", ".", "size_image", ")", "\n", "\n", "\n", "if", "isinstance", "(", "G_model", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "            ", "torch", ".", "save", "(", "G_model", ".", "module", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "config", ".", "save_dir", ",", "'model_G{}.pt'", ".", "format", "(", "epoch_num", ")", ")", ")", "\n", "if", "config", ".", "discriminator_lip", "is", "not", "None", ":", "\n", "                ", "torch", ".", "save", "(", "D_model_lip", ".", "module", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "config", ".", "save_dir", ",", "'model_D{}.pt'", ".", "format", "(", "epoch_num", ")", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "torch", ".", "save", "(", "G_model", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "config", ".", "save_dir", ",", "'model_G{}.pt'", ".", "format", "(", "epoch_num", ")", ")", ")", "\n", "if", "config", ".", "discriminator_lip", "is", "not", "None", ":", "\n", "                ", "torch", ".", "save", "(", "D_model_lip", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "config", ".", "save_dir", ",", "'model_D{}.pt'", ".", "format", "(", "epoch_num", ")", ")", ")", "\n", "\n", "", "", "", "if", "isinstance", "(", "G_model", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "        ", "torch", ".", "save", "(", "G_model", ".", "module", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "config", ".", "save_dir", ",", "'model_G_final.pt'", ")", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "save", "(", "G_model", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "config", ".", "save_dir", ",", "'model_G_final.pt'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.train.get_clip_range": [[416, 427], ["valid_len.numpy", "torch.LongTensor", "torch.LongTensor", "clip_range.append", "random.randint"], "function", ["None"], ["", "", "def", "get_clip_range", "(", "valid_len", ",", "required_len", ")", ":", "\n", "    ", "clip_range", "=", "[", "]", "\n", "for", "seq_len", "in", "valid_len", ".", "numpy", "(", ")", ":", "\n", "        ", "if", "seq_len", ">", "required_len", ":", "\n", "            ", "start_i", "=", "random", ".", "randint", "(", "0", ",", "seq_len", "-", "required_len", ")", "\n", "end_i", "=", "start_i", "+", "required_len", "\n", "", "else", ":", "\n", "            ", "start_i", "=", "0", "\n", "end_i", "=", "seq_len", "\n", "", "clip_range", ".", "append", "(", "[", "start_i", ",", "end_i", "]", ")", "\n", "", "return", "torch", ".", "LongTensor", "(", "clip_range", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.train.sample": [[433, 461], ["model.eval", "os.path.join", "utils.utils.create_dir", "utils.utils.save_sample_images", "utils.utils.save_sample_images", "utils.utils.save_sample_images", "model.train", "isinstance", "model.module.model_type", "model.model_type", "torch.no_grad", "torch.no_grad", "input_images.contiguous().view.cpu().detach().numpy", "os.path.join", "gt_images.contiguous().view.cpu().detach().numpy", "os.path.join", "model.cpu().detach().numpy", "os.path.join", "model", "model.contiguous().view", "input_images.contiguous().view.contiguous().view", "gt_images.contiguous().view.contiguous().view", "model", "input_images.contiguous().view.cpu().detach", "gt_images.contiguous().view.cpu().detach", "model.cpu().detach", "model.contiguous", "input_images.contiguous().view.contiguous", "gt_images.contiguous().view.contiguous", "input_images.contiguous().view.cpu", "gt_images.contiguous().view.cpu", "model.cpu"], "function", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.utils.create_dir", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.utils.save_sample_images", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.utils.save_sample_images", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.utils.save_sample_images", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.model_type", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.model_type"], ["", "def", "sample", "(", "sample_inputs", ",", "model", ",", "epoch", ",", "save_dir", ",", "teacher_forcing_ratio", "=", "0", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "sample_dir", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'sample'", ")", "\n", "utils", ".", "create_dir", "(", "sample_dir", ")", "\n", "\n", "model_type", "=", "model", ".", "module", ".", "model_type", "(", ")", "if", "isinstance", "(", "model", ",", "torch", ".", "nn", ".", "DataParallel", ")", "else", "model", ".", "model_type", "(", ")", "\n", "\n", "input_images", ",", "input_audios", ",", "gt_images", "=", "sample_inputs", "[", "0", "]", ",", "sample_inputs", "[", "1", "]", ",", "sample_inputs", "[", "2", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "if", "model_type", "==", "'RNN'", ":", "\n", "            ", "G_images", "=", "model", "(", "input_images", ",", "input_audios", ",", "\n", "valid_len", "=", "sample_inputs", "[", "3", "]", ",", "\n", "teacher_forcing_ratio", "=", "teacher_forcing_ratio", ")", "\n", "G_images", "=", "G_images", ".", "contiguous", "(", ")", ".", "view", "(", "G_images", ".", "shape", "[", "0", "]", "*", "G_images", ".", "shape", "[", "1", "]", ",", "G_images", ".", "shape", "[", "2", "]", ",", "G_images", ".", "shape", "[", "3", "]", ",", "G_images", ".", "shape", "[", "4", "]", ")", "\n", "input_images", "=", "input_images", ".", "contiguous", "(", ")", ".", "view", "(", "input_images", ".", "shape", "[", "0", "]", "*", "input_images", ".", "shape", "[", "1", "]", ",", "input_images", ".", "shape", "[", "2", "]", ",", "input_images", ".", "shape", "[", "3", "]", ",", "input_images", ".", "shape", "[", "4", "]", ")", "\n", "gt_images", "=", "gt_images", ".", "contiguous", "(", ")", ".", "view", "(", "gt_images", ".", "shape", "[", "0", "]", "*", "gt_images", ".", "shape", "[", "1", "]", ",", "gt_images", ".", "shape", "[", "2", "]", ",", "gt_images", ".", "shape", "[", "3", "]", ",", "gt_images", ".", "shape", "[", "4", "]", ")", "\n", "\n", "", "else", ":", "\n", "            ", "G_images", "=", "model", "(", "input_images", ",", "input_audios", ")", "\n", "\n", "# save input images", "\n", "", "", "utils", ".", "save_sample_images", "(", "input_images", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ",", "os", ".", "path", ".", "join", "(", "sample_dir", ",", "'input.png'", ")", ")", "\n", "# save ground truth images", "\n", "utils", ".", "save_sample_images", "(", "gt_images", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ",", "os", ".", "path", ".", "join", "(", "sample_dir", ",", "'ground_truth.png'", ")", ")", "\n", "# save generated images", "\n", "g_name", "=", "'{:02d}.png'", ".", "format", "(", "epoch", "+", "1", ")", "\n", "utils", ".", "save_sample_images", "(", "G_images", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ",", "os", ".", "path", ".", "join", "(", "sample_dir", ",", "g_name", ")", ")", "\n", "model", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.train.test": [[462, 496], ["model.eval", "utils.utils.listdir_nohidden", "os.path.join", "glob.glob", "utils.utils.sort_filename", "os.path.join", "os.path.join", "utils.utils.get_wav_duration", "utils.data.load_image", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "utils.utils.save_video", "model.train", "os.path.basename", "os.path.join", "utils.data.load_audio", "len", "isinstance", "model.module.model_type", "model.model_type", "torch.no_grad", "torch.no_grad", "model.cpu().detach().numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "input_images.unsqueeze.unsqueeze", "input_audios.unsqueeze.unsqueeze", "model", "model.squeeze", "model", "numpy.array().transpose", "numpy.array().transpose", "model.cpu().detach", "torch.tensor().cuda", "torch.tensor().cuda", "numpy.array", "numpy.array", "model.cpu", "torch.tensor", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.trainList_LRW.listdir_nohidden", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.utils.sort_filename", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.utils.get_wav_duration", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.data.load_image", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.utils.save_video", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.data.load_audio", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.model_type", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.model_type"], ["", "def", "test", "(", "model", ",", "test_dir", ",", "save_dir", ",", "image_size", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "test_dirs", "=", "utils", ".", "listdir_nohidden", "(", "test_dir", ")", "\n", "for", "sub_folder", "in", "test_dirs", ":", "\n", "        ", "save_test_dir", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "os", ".", "path", ".", "basename", "(", "sub_folder", ")", ")", "\n", "\n", "audio_feature_files", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "sub_folder", ",", "'audio_sample/*.mat'", ")", ")", "\n", "audio_feature_files", "=", "utils", ".", "sort_filename", "(", "audio_feature_files", ")", "\n", "image_test_file", "=", "os", ".", "path", ".", "join", "(", "sub_folder", ",", "'image_sample.jpg'", ")", "\n", "audio_test_file", "=", "os", ".", "path", ".", "join", "(", "sub_folder", ",", "'audio_sample.wav'", ")", "\n", "audio_duration", "=", "utils", ".", "get_wav_duration", "(", "audio_test_file", ")", "\n", "\n", "input_image", "=", "data", ".", "load_image", "(", "image_test_file", ",", "image_size", ")", "\n", "input_audios", "=", "[", "data", ".", "load_audio", "(", "audio_feature_file", ")", "for", "audio_feature_file", "in", "audio_feature_files", "]", "\n", "input_images", "=", "[", "input_image", "]", "*", "len", "(", "input_audios", ")", "\n", "\n", "# convert to tensor", "\n", "input_images", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "input_images", ")", ".", "transpose", "(", "(", "0", ",", "3", ",", "1", ",", "2", ")", ")", ")", ".", "cuda", "(", ")", "# (seq_len, c, h, w)", "\n", "input_audios", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "input_audios", ")", ".", "transpose", "(", "(", "0", ",", "3", ",", "1", ",", "2", ")", ")", ")", ".", "cuda", "(", ")", "\n", "\n", "model_type", "=", "model", ".", "module", ".", "model_type", "(", ")", "if", "isinstance", "(", "model", ",", "torch", ".", "nn", ".", "DataParallel", ")", "else", "model", ".", "model_type", "(", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "model_type", "==", "'RNN'", ":", "\n", "                ", "input_images", "=", "input_images", ".", "unsqueeze", "(", "0", ")", "# (1, seq_len, c, h, w)", "\n", "input_audios", "=", "input_audios", ".", "unsqueeze", "(", "0", ")", "\n", "G_images", "=", "model", "(", "input_images", ",", "input_audios", ",", "\n", "valid_len", "=", "torch", ".", "tensor", "(", "[", "input_audios", ".", "shape", "[", "1", "]", "]", ",", "dtype", "=", "torch", ".", "int32", ")", ".", "cuda", "(", ")", ",", "\n", "teacher_forcing_ratio", "=", "0", ")", "\n", "G_images", "=", "G_images", ".", "squeeze", "(", "0", ")", "\n", "", "else", ":", "\n", "                ", "G_images", "=", "model", "(", "input_images", ",", "input_audios", ")", "\n", "", "", "utils", ".", "save_video", "(", "audio_duration", ",", "audio_test_file", ",", "G_images", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ",", "save_test_dir", ")", "\n", "model", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.train.load_ckpt": [[498, 511], ["torch.load", "torch.load", "model.state_dict", "param.replace", "print", "[].data.copy_", "print", "cur_state_dict[].size", "old_state_dict[].size", "model.state_dict"], "function", ["None"], ["", "", "def", "load_ckpt", "(", "model", ",", "ckpt_path", ",", "prefix", "=", "None", ")", ":", "\n", "    ", "old_state_dict", "=", "torch", ".", "load", "(", "ckpt_path", ")", "\n", "cur_state_dict", "=", "model", ".", "state_dict", "(", ")", "\n", "for", "param", "in", "cur_state_dict", ":", "\n", "        ", "if", "prefix", "is", "not", "None", ":", "\n", "            ", "old_param", "=", "param", ".", "replace", "(", "prefix", ",", "''", ")", "\n", "", "else", ":", "\n", "            ", "old_param", "=", "param", "\n", "", "if", "old_param", "in", "old_state_dict", "and", "cur_state_dict", "[", "param", "]", ".", "size", "(", ")", "==", "old_state_dict", "[", "old_param", "]", ".", "size", "(", ")", ":", "\n", "            ", "print", "(", "\"loading param: \"", ",", "param", ")", "\n", "model", ".", "state_dict", "(", ")", "[", "param", "]", ".", "data", ".", "copy_", "(", "old_state_dict", "[", "old_param", "]", ".", "data", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"warning cannot load param: \"", ",", "param", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.train.freeze_model": [[513, 521], ["model.parameters", "isinstance", "model.module.freeze_bn", "model.freeze_bn"], "function", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_LipNet.LipReadModel.freeze_bn", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_LipNet.LipReadModel.freeze_bn"], ["", "", "", "def", "freeze_model", "(", "model", ")", ":", "\n", "# model.eval()", "\n", "    ", "for", "params", "in", "model", ".", "parameters", "(", ")", ":", "\n", "        ", "params", ".", "requires_grad", "=", "False", "\n", "", "if", "isinstance", "(", "model", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "        ", "model", ".", "module", ".", "freeze_bn", "(", ")", "\n", "", "else", ":", "\n", "        ", "model", ".", "freeze_bn", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.loss.SoftCrossEntropy.__init__": [[7, 10], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "SoftCrossEntropy", ",", "self", ")", ".", "__init__", "(", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.loss.SoftCrossEntropy.forward": [[11, 34], ["torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "input.log", "input.log"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "target", ",", "size_average", "=", "True", ")", ":", "\n", "        ", "\"\"\" Cross entropy that accepts soft targets\n        Args:\n             pred: predictions for neural network\n             targets: targets, can be soft\n             size_average: if false, sum is returned instead of mean\n\n        Examples::\n\n            input = torch.FloatTensor([[1.1, 2.8, 1.3], [1.1, 2.1, 4.8]])\n            input = torch.autograd.Variable(out, requires_grad=True)\n\n            target = torch.FloatTensor([[0.05, 0.9, 0.05], [0.05, 0.05, 0.9]])\n            target = torch.autograd.Variable(y1)\n            loss = cross_entropy(input, target)\n            loss.backward()\n        \"\"\"", "\n", "# logsoftmax = nn.LogSoftmax()", "\n", "\n", "if", "size_average", ":", "\n", "            ", "return", "torch", ".", "mean", "(", "torch", ".", "sum", "(", "-", "target", "*", "input", ".", "log", "(", ")", ",", "dim", "=", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "return", "torch", ".", "sum", "(", "torch", ".", "sum", "(", "-", "target", "*", "input", ".", "log", "(", ")", ",", "dim", "=", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.loss.GANLoss.__init__": [[37, 40], ["torch.Module.__init__", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "GANLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "criterionGAN", "=", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.loss.GANLoss.forward": [[41, 48], ["inputs.get_device", "loss.GANLoss.criterionGAN", "torch.FloatTensor().cuda().fill_", "torch.FloatTensor().cuda().fill_", "torch.FloatTensor().cuda().fill_", "torch.FloatTensor().cuda().fill_", "torch.FloatTensor().cuda().fill_", "torch.FloatTensor().cuda().fill_", "torch.FloatTensor().cuda().fill_", "torch.FloatTensor().cuda().fill_", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "inputs.size", "inputs.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "is_real", ")", ":", "\n", "        ", "gpu_id", "=", "inputs", ".", "get_device", "(", ")", "\n", "if", "is_real", ":", "\n", "            ", "target", "=", "torch", ".", "FloatTensor", "(", "inputs", ".", "size", "(", ")", ")", ".", "cuda", "(", "gpu_id", ")", ".", "fill_", "(", "1.0", ")", "\n", "", "else", ":", "\n", "            ", "target", "=", "torch", ".", "FloatTensor", "(", "inputs", ".", "size", "(", ")", ")", ".", "cuda", "(", "gpu_id", ")", ".", "fill_", "(", "0.0", ")", "\n", "", "return", "self", ".", "criterionGAN", "(", "inputs", ",", "target", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.loss.GAN_LR_Loss.__init__": [[51, 56], ["print", "torch.Module.__init__", "torch.MSELoss", "torch.MSELoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"************ GAN LR LOSS **********\"", ")", "\n", "super", "(", "GAN_LR_Loss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "criterionL2", "=", "nn", ".", "MSELoss", "(", "reduce", "=", "False", ")", "\n", "self", ".", "criterionCE", "=", "nn", ".", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.loss.GAN_LR_Loss.forward": [[57, 70], ["inputs.get_device", "loss.GAN_LR_Loss.GAN_LR_Loss.criterionCE", "torch.ge", "torch.ge", "torch.ge", "torch.ge", "mask.unsqueeze().repeat.unsqueeze().repeat.unsqueeze().repeat", "torch.FloatTensor().cuda().fill_", "torch.FloatTensor().cuda().fill_", "torch.FloatTensor().cuda().fill_", "torch.FloatTensor().cuda().fill_", "loss.GAN_LR_Loss.GAN_LR_Loss.criterionL2", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "mask.unsqueeze().repeat.unsqueeze().repeat.unsqueeze", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "mask.unsqueeze().repeat.unsqueeze().repeat.float", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "inputs.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "is_real", ",", "targets", "=", "None", ")", ":", "\n", "        ", "gpu_id", "=", "inputs", ".", "get_device", "(", ")", "\n", "if", "is_real", ":", "\n", "            ", "return", "self", ".", "criterionCE", "(", "inputs", ",", "targets", ")", "\n", "", "else", ":", "\n", "            ", "mask", "=", "torch", ".", "ge", "(", "targets", ",", "0", ")", "# (batch_size,)", "\n", "mask", "=", "mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "repeat", "(", "1", ",", "inputs", ".", "shape", "[", "1", "]", ")", "\n", "targets", "=", "torch", ".", "FloatTensor", "(", "inputs", ".", "size", "(", ")", ")", ".", "cuda", "(", "gpu_id", ")", ".", "fill_", "(", "0.002", ")", "\n", "loss", "=", "self", ".", "criterionL2", "(", "inputs", ",", "targets", ")", "\n", "\n", "loss", "=", "torch", ".", "sum", "(", "loss", "*", "mask", ".", "float", "(", ")", ")", "/", "(", "inputs", ".", "shape", "[", "0", "]", "*", "inputs", ".", "shape", "[", "1", "]", ")", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.loss.ReconLoss.__init__": [[74, 77], ["torch.Module.__init__", "torch.L1Loss", "torch.L1Loss"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "ReconLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "criterion", "=", "nn", ".", "L1Loss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.loss.ReconLoss.forward": [[78, 94], ["len", "range", "inputs.new_zeros", "gt.new_zeros", "range", "loss.ReconLoss.criterion", "loss.ReconLoss.criterion", "valid_len[].item"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "gt", ",", "valid_len", "=", "None", ")", ":", "\n", "        ", "batch_size", "=", "inputs", ".", "shape", "[", "0", "]", "\n", "if", "len", "(", "inputs", ".", "shape", ")", "==", "5", ":", "# batch_size, seq_len, c, h, w", "\n", "            ", "total_len", "=", "0", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "total_len", "+=", "valid_len", "[", "i", "]", ".", "item", "(", ")", "\n", "", "valid_inputs", "=", "inputs", ".", "new_zeros", "(", "total_len", ",", "inputs", ".", "shape", "[", "2", "]", ",", "inputs", ".", "shape", "[", "3", "]", ",", "inputs", ".", "shape", "[", "4", "]", ")", "\n", "valid_gt", "=", "gt", ".", "new_zeros", "(", "total_len", ",", "gt", ".", "shape", "[", "2", "]", ",", "gt", ".", "shape", "[", "3", "]", ",", "gt", ".", "shape", "[", "4", "]", ")", "\n", "idx", "=", "0", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "valid_inputs", "[", "idx", ":", "idx", "+", "valid_len", "[", "i", "]", "]", "=", "inputs", "[", "i", ",", "0", ":", "valid_len", "[", "i", "]", "]", "\n", "valid_gt", "[", "idx", ":", "idx", "+", "valid_len", "[", "i", "]", "]", "=", "gt", "[", "i", ",", "0", ":", "valid_len", "[", "i", "]", "]", "\n", "idx", "=", "idx", "+", "valid_len", "[", "i", "]", "\n", "", "return", "self", ".", "criterion", "(", "valid_inputs", ",", "valid_gt", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "criterion", "(", "inputs", ",", "gt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.loss.LipReadLoss.__init__": [[99, 112], ["torch.Module.__init__", "torch.L1Loss", "torch.L1Loss", "torch.MSELoss", "torch.MSELoss", "torch.KLDivLoss", "torch.KLDivLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "loss.SoftCrossEntropy"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "criterion", "=", "'l1'", ")", ":", "\n", "        ", "super", "(", "LipReadLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "criterion", "=", "criterion", "\n", "if", "criterion", "==", "'l1'", ":", "\n", "            ", "self", ".", "criterionL1", "=", "nn", ".", "L1Loss", "(", ")", "\n", "", "elif", "criterion", "==", "'l2'", ":", "\n", "            ", "self", ".", "criterionL2", "=", "nn", ".", "MSELoss", "(", ")", "\n", "", "elif", "criterion", "==", "'KL'", ":", "\n", "            ", "self", ".", "criterionKL", "=", "nn", ".", "KLDivLoss", "(", ")", "\n", "", "elif", "criterion", "==", "'CE'", ":", "\n", "            ", "self", ".", "criterionCE", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "", "elif", "criterion", "==", "'soft_CE'", ":", "\n", "            ", "self", ".", "criterionCE_soft", "=", "SoftCrossEntropy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.loss.LipReadLoss.forward": [[114, 130], ["loss.LipReadLoss.criterionL1", "loss.LipReadLoss.criterionL2", "loss.LipReadLoss.criterionKL", "fake.log", "loss.LipReadLoss.criterionCE", "loss.LipReadLoss.criterionCE_soft"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "fake", ",", "real", ")", ":", "\n", "\n", "# score, pred = real.topk(5, 1, True, True)", "\n", "# fake_score =  torch.gather(fake, 1, pred)", "\n", "# print(score, fake_score)", "\n", "\n", "        ", "if", "self", ".", "criterion", "==", "'l1'", ":", "\n", "            ", "return", "self", ".", "criterionL1", "(", "fake", ",", "real", ")", "\n", "", "elif", "self", ".", "criterion", "==", "'l2'", ":", "\n", "            ", "return", "self", ".", "criterionL2", "(", "fake", ",", "real", ")", "\n", "", "elif", "self", ".", "criterion", "==", "'KL'", ":", "\n", "            ", "return", "self", ".", "criterionKL", "(", "fake", ".", "log", "(", ")", ",", "real", ")", "\n", "", "elif", "self", ".", "criterion", "==", "'CE'", ":", "\n", "            ", "return", "self", ".", "criterionCE", "(", "fake", ",", "real", ")", "\n", "", "elif", "self", ".", "criterion", "==", "'soft_CE'", ":", "\n", "            ", "return", "self", ".", "criterionCE_soft", "(", "fake", ",", "real", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_D.BasicBlock.__init__": [[25, 30], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_planes", ",", "out_planes", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ":", "\n", "        ", "super", "(", "BasicBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "in_planes", ",", "out_planes", ",", "kernel_size", "=", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "1", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm2d", "(", "out_planes", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_D.BasicBlock.forward": [[31, 36], ["model_D.BasicBlock.conv1", "model_D.BasicBlock.bn1", "model_D.BasicBlock.relu"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_D.ResnetBlock.__init__": [[39, 45], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ")", ":", "\n", "        ", "super", "(", "ResnetBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "conv_block", "=", "[", "]", "\n", "conv_block", "+=", "[", "nn", ".", "ReflectionPad2d", "(", "1", ")", ",", "nn", ".", "Conv2d", "(", "dim", ",", "dim", ",", "kernel_size", "=", "3", ")", ",", "nn", ".", "InstanceNorm2d", "(", "dim", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "]", "\n", "conv_block", "+=", "[", "nn", ".", "ReflectionPad2d", "(", "1", ")", ",", "nn", ".", "Conv2d", "(", "dim", ",", "dim", ",", "kernel_size", "=", "3", ")", ",", "nn", ".", "InstanceNorm2d", "(", "dim", ")", "]", "\n", "self", ".", "conv_blocks", "=", "nn", ".", "Sequential", "(", "*", "conv_block", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_D.ResnetBlock.forward": [[46, 49], ["model_D.ResnetBlock.conv_blocks"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "out", "=", "x", "+", "self", ".", "conv_blocks", "(", "x", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_D.DiscriminatorFrame.__init__": [[54, 61], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "DiscriminatorFrame", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv1", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "3", ",", "16", ",", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "nn", ".", "InstanceNorm2d", "(", "16", ")", ",", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "16", ",", "32", ",", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "nn", ".", "InstanceNorm2d", "(", "32", ")", ",", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", ")", "\n", "self", ".", "conv3", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "nn", ".", "InstanceNorm2d", "(", "64", ")", ",", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", ")", "\n", "self", ".", "conv4", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "64", ",", "128", ",", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "nn", ".", "InstanceNorm2d", "(", "128", ")", ",", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", ")", "\n", "self", ".", "conv5", "=", "nn", ".", "Conv2d", "(", "128", ",", "1", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_D.DiscriminatorFrame.forward": [[62, 69], ["model_D.DiscriminatorFrame.conv1", "model_D.DiscriminatorFrame.conv2", "model_D.DiscriminatorFrame.conv3", "model_D.DiscriminatorFrame.conv4", "model_D.DiscriminatorFrame.conv5"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "out", "=", "self", ".", "conv1", "(", "inputs", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "out", "=", "self", ".", "conv4", "(", "out", ")", "\n", "out", "=", "self", ".", "conv5", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_D.DiscriminatorVideoConv3D.__init__": [[72, 78], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.BatchNorm3d", "torch.BatchNorm3d", "torch.BatchNorm3d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.BatchNorm3d", "torch.BatchNorm3d", "torch.BatchNorm3d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.BatchNorm3d", "torch.BatchNorm3d", "torch.BatchNorm3d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "ndf", "=", "64", ")", ":", "\n", "        ", "super", "(", "DiscriminatorVideoConv3D", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv1", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv3d", "(", "3", ",", "ndf", ",", "3", ",", "stride", "=", "(", "1", ",", "2", ",", "2", ")", ",", "padding", "=", "(", "0", ",", "1", ",", "1", ")", ")", ",", "nn", ".", "BatchNorm3d", "(", "ndf", ")", ",", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv3d", "(", "ndf", ",", "ndf", "*", "2", ",", "3", ",", "stride", "=", "(", "1", ",", "2", ",", "2", ")", ",", "padding", "=", "(", "0", ",", "1", ",", "1", ")", ")", ",", "nn", ".", "BatchNorm3d", "(", "ndf", "*", "2", ")", ",", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", ")", "\n", "self", ".", "conv3", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv3d", "(", "ndf", "*", "2", ",", "ndf", "*", "4", ",", "3", ",", "stride", "=", "(", "1", ",", "2", ",", "2", ")", ",", "padding", "=", "(", "0", ",", "1", ",", "1", ")", ")", ",", "nn", ".", "BatchNorm3d", "(", "ndf", "*", "4", ")", ",", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", ")", "\n", "self", ".", "conv4", "=", "nn", ".", "Conv3d", "(", "ndf", "*", "4", ",", "1", ",", "3", ",", "stride", "=", "(", "1", ",", "2", ",", "2", ")", ",", "padding", "=", "(", "0", ",", "1", ",", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_D.DiscriminatorVideoConv3D.forward": [[79, 89], ["model_D.DiscriminatorVideoConv3D.conv1", "model_D.DiscriminatorVideoConv3D.conv2", "model_D.DiscriminatorVideoConv3D.conv3", "model_D.DiscriminatorVideoConv3D.conv4"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "out", "=", "self", ".", "conv1", "(", "inputs", ")", "\n", "# print(out.size())", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "# print(out.size())", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "# print(out.size())", "\n", "out", "=", "self", ".", "conv4", "(", "out", ")", "\n", "# print(out.size())", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_D.DiscriminatorVideoConv2D.__init__": [[91, 98], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_of_frame", ",", "ndf", "=", "64", ")", ":", "\n", "        ", "super", "(", "DiscriminatorVideoConv2D", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv1", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "3", "*", "num_of_frame", ",", "ndf", ",", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "nn", ".", "InstanceNorm2d", "(", "64", ")", ",", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "ndf", ",", "ndf", "*", "2", ",", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "nn", ".", "InstanceNorm2d", "(", "128", ")", ",", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", ")", "\n", "self", ".", "conv3", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "ndf", "*", "2", ",", "ndf", "*", "4", ",", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "nn", ".", "InstanceNorm2d", "(", "256", ")", ",", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", ")", "\n", "self", ".", "conv4", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "ndf", "*", "4", ",", "ndf", "*", "8", ",", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "nn", ".", "InstanceNorm2d", "(", "512", ")", ",", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", ")", "\n", "self", ".", "conv5", "=", "nn", ".", "Conv2d", "(", "ndf", "*", "8", ",", "1", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_D.DiscriminatorVideoConv2D.forward": [[99, 106], ["model_D.DiscriminatorVideoConv2D.conv1", "model_D.DiscriminatorVideoConv2D.conv2", "model_D.DiscriminatorVideoConv2D.conv3", "model_D.DiscriminatorVideoConv2D.conv4", "model_D.DiscriminatorVideoConv2D.conv5"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "out", "=", "self", ".", "conv1", "(", "inputs", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "out", "=", "self", ".", "conv4", "(", "out", ")", "\n", "out", "=", "self", ".", "conv5", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_D.DiscriminatorVideo.__init__": [[109, 115], ["torch.Module.__init__", "model_D.DiscriminatorVideoConv3D", "model_D.DiscriminatorVideoConv2D"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "DiscriminatorVideo", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "discriminator_v", "==", "'video_3D'", ":", "\n", "            ", "self", ".", "discriminator", "=", "DiscriminatorVideoConv3D", "(", "ndf", "=", "64", ")", "\n", "", "elif", "config", ".", "discriminator_v", "==", "'video_2D'", ":", "\n", "            ", "self", ".", "discriminator", "=", "DiscriminatorVideoConv2D", "(", "config", ".", "num_frames_D", ",", "ndf", "=", "64", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_D.DiscriminatorVideo.forward": [[116, 135], ["inputs.new_zeros", "range", "clip_inputs.permute.permute.permute", "model_D.DiscriminatorVideo.discriminator", "print", "inputs[].unsqueeze().repeat", "inputs[].unsqueeze"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "inputs", ",", "num_frames_D", ",", "clip_range", "=", "None", ")", ":", "# (batch_size, seq_len, c, h, w)", "\n", "\n", "        ", "batch_size", "=", "inputs", ".", "shape", "[", "0", "]", "\n", "clip_inputs", "=", "inputs", ".", "new_zeros", "(", "batch_size", ",", "num_frames_D", ",", "inputs", ".", "shape", "[", "2", "]", ",", "inputs", ".", "shape", "[", "3", "]", ",", "inputs", ".", "shape", "[", "4", "]", ")", "\n", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "start_idx", "=", "clip_range", "[", "i", "]", "[", "0", "]", "\n", "end_dix", "=", "clip_range", "[", "i", "]", "[", "1", "]", "\n", "if", "(", "end_dix", "-", "start_idx", ")", "==", "num_frames_D", ":", "\n", "                ", "clip_inputs", "[", "i", ",", ":", ",", ":", ",", ":", ",", ":", "]", "=", "inputs", "[", "i", ",", "start_idx", ":", "end_dix", ",", ":", ",", ":", ",", ":", "]", "\n", "", "else", ":", "\n", "                ", "print", "(", "\"sequence length less than num_frames_D\"", ")", "\n", "diff", "=", "num_frames_D", "-", "(", "end_dix", "-", "start_idx", ")", "\n", "padding", "=", "inputs", "[", "i", ",", "-", "1", ",", ":", ",", ":", ",", ":", "]", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "1", ",", "diff", ",", "1", ",", "1", ",", "1", ")", "\n", "clip_inputs", "[", "i", ",", ":", "(", "end_dix", "-", "start_idx", ")", ",", ":", ",", ":", ",", ":", "]", "=", "inputs", "[", "i", ",", "start_idx", ":", "end_dix", ",", ":", ",", ":", ",", ":", "]", "\n", "clip_inputs", "[", "i", ",", "(", "end_dix", "-", "start_idx", ")", ":", ",", ":", ",", ":", ",", ":", "]", "=", "padding", "\n", "\n", "", "", "clip_inputs", "=", "clip_inputs", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ",", "4", ")", "\n", "return", "self", ".", "discriminator", "(", "clip_inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_D.Discriminator.__init__": [[138, 141], ["torch.Module.__init__", "model_D.DiscriminatorFrame"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "Discriminator", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "discriminator", "=", "DiscriminatorFrame", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_D.Discriminator.forward": [[142, 146], ["model_D.Discriminator.discriminator", "len", "inputs.contiguous().view.contiguous().view.contiguous().view", "inputs.contiguous().view.contiguous().view.contiguous"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "if", "len", "(", "inputs", ".", "shape", ")", "==", "5", ":", "\n", "            ", "inputs", "=", "inputs", ".", "contiguous", "(", ")", ".", "view", "(", "inputs", ".", "shape", "[", "0", "]", "*", "inputs", ".", "shape", "[", "1", "]", ",", "inputs", ".", "shape", "[", "2", "]", ",", "inputs", ".", "shape", "[", "3", "]", ",", "inputs", ".", "shape", "[", "4", "]", ")", "\n", "", "return", "self", ".", "discriminator", "(", "inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_D.DiscriminatorLip.__init__": [[150, 153], ["torch.Module.__init__", "model_LipNet.LipReadModel"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "DiscriminatorLip", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "discriminator", "=", "LipNet", ".", "LipReadModel", "(", "512", ",", "512", ",", "img_channel", "=", "3", ",", "num_layers", "=", "2", ",", "num_classes", "=", "500", ",", "encoder_type", "=", "'fc'", ",", "rnn_type", "=", "'LSTM'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_D.DiscriminatorLip.forward": [[154, 156], ["model_D.DiscriminatorLip.discriminator"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "clip_range", "=", "None", ",", "lip_coords", "=", "None", ")", ":", "\n", "        ", "return", "self", ".", "discriminator", "(", "inputs", ",", "clip_range", ",", "lip_coords", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_D.weights_init": [[7, 22], ["hasattr", "m.weight.data.normal_", "classname.find", "classname.find", "m.weight.data.normal_", "m.bias.data.fill_", "type", "torch.nn.init.xavier_uniform", "torch.nn.init.xavier_uniform", "torch.nn.init.xavier_uniform", "m.bias.data.fill_", "print"], "function", ["None"], ["def", "weights_init", "(", "m", ")", ":", "\n", "    ", "classname", "=", "m", ".", "__class__", ".", "__name__", "\n", "if", "classname", ".", "find", "(", "'Conv'", ")", "!=", "-", "1", "and", "hasattr", "(", "m", ",", "'weight'", ")", ":", "\n", "        ", "m", ".", "weight", ".", "data", ".", "normal_", "(", "0.0", ",", "0.02", ")", "\n", "", "elif", "classname", ".", "find", "(", "'BatchNorm'", ")", "!=", "-", "1", ":", "\n", "        ", "m", ".", "weight", ".", "data", ".", "normal_", "(", "1.0", ",", "0.02", ")", "\n", "m", ".", "bias", ".", "data", ".", "fill_", "(", "0", ")", "\n", "", "elif", "type", "(", "m", ")", "==", "nn", ".", "Linear", ":", "\n", "        ", "torch", ".", "nn", ".", "init", ".", "xavier_uniform", "(", "m", ".", "weight", ")", "\n", "m", ".", "bias", ".", "data", ".", "fill_", "(", "0.01", ")", "\n", "# elif classname.find('GRU') != -1 or classname.find('LSTM') != -1:", "\n", "#     m.weight.data.normal_(0.0, 0.02)", "\n", "#     m.bias.data.fill_(0.01)", "\n", "", "else", ":", "\n", "        ", "print", "(", "classname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_LipNet.LipEncoder.__init__": [[26, 42], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.Linear", "torch.Linear", "torch.Linear", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "embeding_size", ",", "img_channel", ")", ":", "\n", "        ", "super", "(", "LipEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv1", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "img_channel", ",", "16", ",", "kernel_size", "=", "5", ",", "stride", "=", "1", ",", "padding", "=", "2", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "16", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "2", ",", "stride", "=", "2", ")", ")", "# (batch_size, 16, 20, 20)", "\n", "self", ".", "conv2", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "16", ",", "32", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "32", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "2", ",", "stride", "=", "2", ")", ")", "# (batch_size, 32, 10, 10)", "\n", "self", ".", "conv3", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "64", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "2", ",", "stride", "=", "2", ")", ")", "# (batch_size, 64, 5, 5)", "\n", "# reshape to (batch_size, 64x5x5)", "\n", "self", ".", "fc1", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "1600", ",", "embeding_size", ")", ",", "nn", ".", "BatchNorm1d", "(", "embeding_size", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_LipNet.LipEncoder.forward": [[43, 52], ["model_LipNet.LipEncoder.conv1", "model_LipNet.LipEncoder.conv2", "model_LipNet.LipEncoder.conv3", "model_LipNet.LipEncoder.contiguous().view", "model_LipNet.LipEncoder.fc1", "model_LipNet.LipEncoder.contiguous"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "# input shape  (batch_size, 1, 40, 40)", "\n", "        ", "out", "=", "self", ".", "conv1", "(", "inputs", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "# (batch_size, 64, 5, 5)", "\n", "# reshape to (batch_size, 64x5x5)", "\n", "out", "=", "out", ".", "contiguous", "(", ")", ".", "view", "(", "out", ".", "shape", "[", "0", "]", ",", "out", ".", "shape", "[", "1", "]", "*", "out", ".", "shape", "[", "2", "]", "*", "out", ".", "shape", "[", "3", "]", ")", "\n", "\n", "out", "=", "self", ".", "fc1", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_LipNet.RNNModel.__init__": [[55, 64], ["torch.Module.__init__", "torch.GRU", "torch.GRU", "torch.GRU", "torch.LSTM", "torch.LSTM", "torch.LSTM"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_size", ",", "hidden_size", ",", "rnn_type", ",", "num_layers", ")", ":", "\n", "        ", "super", "(", "RNNModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "rnn_type", "=", "rnn_type", "\n", "self", ".", "nhid", "=", "hidden_size", "\n", "self", ".", "nlayers", "=", "num_layers", "\n", "if", "rnn_type", "==", "'GRU'", ":", "\n", "            ", "self", ".", "rnn", "=", "nn", ".", "GRU", "(", "input_size", ",", "hidden_size", ",", "num_layers", "=", "num_layers", ",", "batch_first", "=", "True", ")", "\n", "", "elif", "rnn_type", "==", "'LSTM'", ":", "\n", "            ", "self", ".", "rnn", "=", "nn", ".", "LSTM", "(", "input_size", ",", "hidden_size", ",", "num_layers", "=", "num_layers", ",", "batch_first", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_LipNet.RNNModel.forward": [[65, 71], ["torch.backends.cudnn.flags", "torch.backends.cudnn.flags", "torch.backends.cudnn.flags", "torch.backends.cudnn.flags", "torch.backends.cudnn.flags", "torch.backends.cudnn.flags", "torch.backends.cudnn.flags", "torch.backends.cudnn.flags", "torch.backends.cudnn.flags", "model_LipNet.RNNModel.rnn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "inputs", ",", "hidden", ")", ":", "\n", "\n", "        ", "with", "torch", ".", "backends", ".", "cudnn", ".", "flags", "(", "enabled", "=", "False", ")", ":", "\n", "            ", "output", ",", "hidden", "=", "self", ".", "rnn", "(", "inputs", ",", "hidden", ")", "\n", "\n", "", "return", "output", ",", "hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_LipNet.RNNModel.init_hidden": [[72, 79], ["next", "model_LipNet.RNNModel.parameters", "next.new_zeros", "next.new_zeros", "next.new_zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "weight", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", "\n", "if", "self", ".", "rnn_type", "==", "'LSTM'", ":", "\n", "            ", "return", "(", "weight", ".", "new_zeros", "(", "self", ".", "nlayers", ",", "batch_size", ",", "self", ".", "nhid", ")", ",", "\n", "weight", ".", "new_zeros", "(", "self", ".", "nlayers", ",", "batch_size", ",", "self", ".", "nhid", ")", ")", "\n", "", "else", ":", "\n", "            ", "return", "weight", ".", "new_zeros", "(", "self", ".", "nlayers", ",", "batch_size", ",", "self", ".", "nhid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_LipNet.RNNModel.init_weights": [[80, 88], ["model_LipNet.RNNModel.named_parameters", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.orthogonal_", "torch.nn.init.orthogonal_", "torch.nn.init.orthogonal_", "torch.nn.init.orthogonal_", "torch.nn.init.orthogonal_", "torch.nn.init.orthogonal_", "torch.nn.init.orthogonal_", "torch.nn.init.orthogonal_", "torch.nn.init.orthogonal_", "param.data.fill_"], "methods", ["None"], ["", "", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "for", "name", ",", "param", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "'weight_ih'", "in", "name", ":", "\n", "                ", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "param", ".", "data", ")", "\n", "", "elif", "'weight_hh'", "in", "name", ":", "\n", "                ", "torch", ".", "nn", ".", "init", ".", "orthogonal_", "(", "param", ".", "data", ")", "\n", "", "elif", "'bias'", "in", "name", ":", "\n", "                ", "param", ".", "data", ".", "fill_", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_LipNet.LipReadModel.__init__": [[91, 104], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "model_LipNet.LipEncoder", "model_LipNet.RNNModel", "model_LipNet.RNNModel"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "embeding_size", ",", "hidden_size", ",", "img_channel", ",", "num_layers", ",", "num_classes", ",", "encoder_type", ",", "rnn_type", ")", ":", "\n", "        ", "super", "(", "LipReadModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "encoder_type", "==", "'fc'", ":", "\n", "            ", "self", ".", "encoder", "=", "LipEncoder", "(", "embeding_size", ",", "img_channel", ")", "\n", "", "if", "rnn_type", "==", "'GRU'", ":", "\n", "            ", "self", ".", "rnn", "=", "RNNModel", "(", "embeding_size", ",", "hidden_size", ",", "'GRU'", ",", "num_layers", ")", "\n", "", "elif", "rnn_type", "==", "'LSTM'", ":", "\n", "            ", "self", ".", "rnn", "=", "RNNModel", "(", "embeding_size", ",", "hidden_size", ",", "'LSTM'", ",", "num_layers", ")", "\n", "\n", "", "self", ".", "img_channel", "=", "img_channel", "\n", "self", ".", "encoder_type", "=", "encoder_type", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "num_classes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_LipNet.LipReadModel.freeze_bn": [[106, 111], ["model_LipNet.LipReadModel.modules", "isinstance", "isinstance", "layer.eval"], "methods", ["None"], ["", "def", "freeze_bn", "(", "self", ")", ":", "\n", "        ", "'''Freeze BatchNorm layers.'''", "\n", "for", "layer", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "layer", ",", "nn", ".", "BatchNorm2d", ")", "or", "isinstance", "(", "layer", ",", "nn", ".", "BatchNorm1d", ")", ":", "\n", "                ", "layer", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_LipNet.LipReadModel.forward": [[113, 162], ["int", "int", "inputs.new_zeros", "range", "lip_inputs.contiguous().view.contiguous().view.contiguous().view", "model_LipNet.LipReadModel.encoder", "model_LipNet.LipReadModel.rnn.init_hidden", "model_LipNet.LipReadModel.rnn", "model_LipNet.LipReadModel.fc", "print", "range", "embedding.contiguous().view.contiguous().view.contiguous().view", "embedding.contiguous().view.contiguous().view.contiguous().view", "lip_inputs.contiguous().view.contiguous().view.contiguous", "print", "print", "embedding.contiguous().view.contiguous().view.contiguous", "embedding.contiguous().view.contiguous().view.contiguous"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.RNNModel.init_hidden"], ["", "", "", "def", "forward", "(", "self", ",", "inputs", ",", "clip_range", ",", "lip_coords", ")", ":", "# (batch_size, seq_len, 3, H, W)", "\n", "# clip inputs to mouth region", "\n", "        ", "batch_size", "=", "inputs", ".", "shape", "[", "0", "]", "\n", "\n", "clip_len", "=", "int", "(", "clip_range", "[", "0", "]", "[", "1", "]", "-", "clip_range", "[", "0", "]", "[", "0", "]", ")", "\n", "lip_size", "=", "int", "(", "lip_coords", "[", "0", "]", "[", "0", "]", "[", "2", "]", "-", "lip_coords", "[", "0", "]", "[", "0", "]", "[", "0", "]", ")", "\n", "if", "lip_size", "!=", "40", ":", "\n", "            ", "print", "(", "'********* here ***********'", ")", "\n", "", "lip_inputs", "=", "inputs", ".", "new_zeros", "(", "batch_size", ",", "clip_len", ",", "self", ".", "img_channel", ",", "lip_size", ",", "lip_size", ")", "\n", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "start_frame_id", "=", "clip_range", "[", "i", "]", "[", "0", "]", "\n", "end_frame_id", "=", "clip_range", "[", "i", "]", "[", "1", "]", "\n", "for", "j", "in", "range", "(", "clip_len", ")", ":", "\n", "                ", "x1", ",", "y1", ",", "x2", ",", "y2", "=", "lip_coords", "[", "i", "]", "[", "j", "]", "[", "0", "]", ",", "lip_coords", "[", "i", "]", "[", "j", "]", "[", "1", "]", ",", "lip_coords", "[", "i", "]", "[", "j", "]", "[", "2", "]", ",", "lip_coords", "[", "i", "]", "[", "j", "]", "[", "3", "]", "\n", "if", "x2", "-", "x1", ">", "lip_size", ":", "\n", "                    ", "x2", "-=", "(", "x2", "-", "x1", ")", "-", "lip_size", "\n", "", "if", "x2", "-", "x1", "<", "lip_size", ":", "\n", "                    ", "x2", "+=", "lip_size", "-", "(", "x2", "-", "x1", ")", "\n", "", "if", "y2", "-", "y1", ">", "lip_size", ":", "\n", "                    ", "y2", "-=", "(", "y2", "-", "y1", ")", "-", "lip_size", "\n", "", "if", "y2", "-", "y1", "<", "lip_size", ":", "\n", "                    ", "y2", "+=", "lip_size", "-", "(", "y2", "-", "y1", ")", "\n", "\n", "\n", "", "if", "lip_inputs", "[", "i", ",", "j", ",", ":", ",", ":", ",", ":", "]", ".", "shape", "!=", "inputs", "[", "i", ",", "start_frame_id", "+", "j", ",", ":", ",", "y1", ":", "y2", ",", "x1", ":", "x2", "]", ".", "shape", ":", "\n", "                    ", "print", "(", "lip_inputs", ".", "shape", ",", "inputs", "[", "i", ",", "start_frame_id", "+", "j", ",", ":", ",", "y1", ":", "y2", ",", "x1", ":", "x2", "]", ".", "shape", ")", "\n", "print", "(", "x1", ",", "y1", ",", "x2", ",", "y2", ")", "\n", "\n", "", "lip_inputs", "[", "i", ",", "j", ",", ":", ",", ":", ",", ":", "]", "=", "inputs", "[", "i", ",", "start_frame_id", "+", "j", ",", ":", ",", "y1", ":", "y2", ",", "x1", ":", "x2", "]", "\n", "\n", "# reshape inputs to (seq_len*batch_size, ...)", "\n", "", "", "lip_inputs", "=", "lip_inputs", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", "*", "clip_len", ",", "lip_inputs", ".", "shape", "[", "2", "]", ",", "lip_inputs", ".", "shape", "[", "3", "]", ",", "lip_inputs", ".", "shape", "[", "4", "]", ")", "\n", "\n", "embedding", "=", "self", ".", "encoder", "(", "lip_inputs", ")", "# (seq_len*batch_size, ...)", "\n", "\n", "# reshape to (batch_size, seq_len, ...)", "\n", "if", "self", ".", "encoder_type", "==", "'FCN'", ":", "\n", "            ", "embedding", "=", "embedding", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "clip_len", ",", "embedding", ".", "shape", "[", "1", "]", ",", "embedding", ".", "shape", "[", "2", "]", ",", "embedding", ".", "shape", "[", "3", "]", ")", "\n", "", "else", ":", "\n", "            ", "embedding", "=", "embedding", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "clip_len", ",", "embedding", ".", "shape", "[", "1", "]", ")", "\n", "\n", "# fed to rnn", "\n", "", "hidden", "=", "self", ".", "rnn", ".", "init_hidden", "(", "batch_size", ")", "\n", "rnn_output", ",", "_", "=", "self", ".", "rnn", "(", "embedding", ",", "hidden", ")", "# (batch_size, seq_len, hidden_size)", "\n", "output", "=", "self", ".", "fc", "(", "rnn_output", "[", ":", ",", "-", "1", ",", ":", "]", ")", "\n", "#        output = F.softmax(output, dim=1)", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_LipNet.Loss.__init__": [[167, 170], ["torch.Module.__init__", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "Loss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_LipNet.Loss.forward": [[171, 173], ["model_LipNet.Loss.criterion"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "gt", ")", ":", "\n", "        ", "return", "self", ".", "criterion", "(", "inputs", ",", "gt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_LipNet.weights_init": [[8, 23], ["hasattr", "m.weight.data.normal_", "classname.find", "classname.find", "m.weight.data.normal_", "m.bias.data.fill_", "type", "torch.nn.init.xavier_uniform", "torch.nn.init.xavier_uniform", "torch.nn.init.xavier_uniform", "m.bias.data.fill_", "print"], "function", ["None"], ["def", "weights_init", "(", "m", ")", ":", "\n", "    ", "classname", "=", "m", ".", "__class__", ".", "__name__", "\n", "if", "classname", ".", "find", "(", "'Conv'", ")", "!=", "-", "1", "and", "hasattr", "(", "m", ",", "'weight'", ")", ":", "\n", "        ", "m", ".", "weight", ".", "data", ".", "normal_", "(", "0.0", ",", "0.02", ")", "\n", "", "elif", "classname", ".", "find", "(", "'BatchNorm'", ")", "!=", "-", "1", ":", "\n", "        ", "m", ".", "weight", ".", "data", ".", "normal_", "(", "1.0", ",", "0.02", ")", "\n", "m", ".", "bias", ".", "data", ".", "fill_", "(", "0", ")", "\n", "", "elif", "type", "(", "m", ")", "==", "nn", ".", "Linear", ":", "\n", "        ", "torch", ".", "nn", ".", "init", ".", "xavier_uniform", "(", "m", ".", "weight", ")", "\n", "m", ".", "bias", ".", "data", ".", "fill_", "(", "0.01", ")", "\n", "# elif classname.find('GRU') != -1 or classname.find('LSTM') != -1:", "\n", "#     m.weight.data.normal_(0.0, 0.02)", "\n", "#     m.bias.data.fill_(0.01)", "\n", "", "else", ":", "\n", "        ", "print", "(", "classname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.test.get_parser": [[18, 42], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "parser.parse_args.gpu.split", "print"], "function", ["None"], ["def", "get_parser", "(", "args", "=", "None", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Simple training script for training a RetinaNet network.'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_input_imgs'", ",", "help", "=", "'num of input images'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--size_image'", ",", "help", "=", "'input image size'", ",", "type", "=", "int", ",", "default", "=", "112", ")", "\n", "parser", ".", "add_argument", "(", "'--audio_encoder'", ",", "help", "=", "'audio encoder network'", ",", "type", "=", "str", ",", "default", "=", "'reduce'", ")", "\n", "parser", ".", "add_argument", "(", "'--img_encoder'", ",", "help", "=", "'image encoder network'", ",", "type", "=", "str", ",", "default", "=", "'reduce'", ")", "\n", "parser", ".", "add_argument", "(", "'--img_decoder'", ",", "help", "=", "'image decoder network'", ",", "type", "=", "str", ",", "default", "=", "'reduce'", ")", "\n", "parser", ".", "add_argument", "(", "'--rnn_type'", ",", "help", "=", "'type or RNN'", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--discriminator'", ",", "help", "=", "'multi patch or cond'", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--ckpt'", ",", "help", "=", "'pretrained model folder'", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--save_dir'", ",", "help", "=", "'saving folder path'", ",", "type", "=", "str", ",", "default", "=", "'save'", ")", "\n", "parser", ".", "add_argument", "(", "'--test_dir'", ",", "help", "=", "'testing sample path'", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--num_output_length'", ",", "help", "=", "'the length of feature'", ",", "type", "=", "int", ",", "default", "=", "50", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--is_train'", ",", "help", "=", "'if train'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--is_region'", ",", "help", "=", "'if train with only mouth region'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--if_tanh'", ",", "help", "=", "'if use tanh'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--gpu'", ",", "help", "=", "'which gpu to use'", ",", "type", "=", "str", ",", "default", "=", "'0'", ")", "\n", "\n", "config", "=", "parser", ".", "parse_args", "(", "args", ")", "\n", "config", ".", "gpu", "=", "config", ".", "gpu", ".", "split", "(", "','", ")", "\n", "print", "(", "\"Using gpu: \"", ",", "config", ".", "gpu", ")", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.test.test_cnn": [[43, 57], ["len", "model_G.LipGeneratorCNN", "test.load_ckpt", "test.test", "torch.nn.DataParallel().cuda", "torch.nn.DataParallel().cuda", "G_model.cuda.cuda", "torch.nn.DataParallel", "torch.nn.DataParallel", "list", "range"], "function", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.test.load_ckpt", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.test.test"], ["", "def", "test_cnn", "(", "config", ")", ":", "\n", "    ", "num_gpu", "=", "len", "(", "config", ".", "gpu", ")", "\n", "\n", "G_model", "=", "model_G", ".", "LipGeneratorCNN", "(", "config", ".", "audio_encoder", ",", "config", ".", "img_encoder", ",", "config", ".", "img_decoder", ",", "config", ".", "size_image", ",", "config", ".", "num_output_length", ",", "config", ".", "if_tanh", ")", "\n", "# G_model.load_state_dict(torch.load(config.ckpt))", "\n", "load_ckpt", "(", "G_model", ",", "config", ".", "ckpt", ")", "\n", "\n", "\n", "if", "num_gpu", ">", "1", ":", "\n", "        ", "G_model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "G_model", ",", "device_ids", "=", "list", "(", "range", "(", "num_gpu", ")", ")", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "        ", "G_model", "=", "G_model", ".", "cuda", "(", ")", "\n", "\n", "", "test", "(", "G_model", ",", "config", ".", "test_dir", ",", "config", ".", "save_dir", ",", "config", ".", "size_image", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.test.test_rnn": [[59, 72], ["len", "model_G.LipGeneratorRNN", "test.load_ckpt", "test.test", "torch.nn.DataParallel().cuda", "torch.nn.DataParallel().cuda", "G_model.cuda.cuda", "torch.nn.DataParallel", "torch.nn.DataParallel", "list", "range"], "function", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.test.load_ckpt", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.test.test"], ["", "def", "test_rnn", "(", "config", ")", ":", "\n", "    ", "num_gpu", "=", "len", "(", "config", ".", "gpu", ")", "\n", "\n", "G_model", "=", "model_G", ".", "LipGeneratorRNN", "(", "config", ".", "audio_encoder", ",", "config", ".", "img_encoder", ",", "config", ".", "img_decoder", ",", "config", ".", "rnn_type", ",", "\n", "config", ".", "size_image", ",", "config", ".", "num_output_length", ",", "if_tanh", "=", "config", ".", "if_tanh", ")", "\n", "load_ckpt", "(", "G_model", ",", "config", ".", "ckpt", ")", "\n", "\n", "if", "num_gpu", ">", "1", ":", "\n", "        ", "G_model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "G_model", ",", "device_ids", "=", "list", "(", "range", "(", "num_gpu", ")", ")", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "        ", "G_model", "=", "G_model", ".", "cuda", "(", ")", "\n", "\n", "", "test", "(", "G_model", ",", "config", ".", "test_dir", ",", "config", ".", "save_dir", ",", "config", ".", "size_image", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.test.test": [[76, 111], ["model.eval", "utils.utils.listdir_nohidden", "os.path.join", "glob.glob", "utils.utils.sort_filename", "os.path.join", "os.path.join", "utils.utils.get_wav_duration", "utils.data.load_image", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "print", "print", "model.cpu().detach().numpy", "utils.utils.save_video", "os.path.basename", "os.path.join", "utils.data.load_audio", "len", "input_images.unsqueeze.size", "input_audios.unsqueeze.size", "isinstance", "model.module.model_type", "model.model_type", "input_images.unsqueeze.unsqueeze", "input_audios.unsqueeze.unsqueeze", "model", "model.squeeze", "model", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.tensor().cuda", "torch.tensor().cuda", "model.cpu().detach", "numpy.array().transpose", "numpy.array().transpose", "torch.tensor", "torch.tensor", "model.cpu", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.trainList_LRW.listdir_nohidden", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.utils.sort_filename", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.utils.get_wav_duration", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.data.load_image", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.utils.save_video", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.data.load_audio", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.model_type", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.model_type"], ["", "def", "test", "(", "model", ",", "test_dir", ",", "save_dir", ",", "image_size", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "test_dirs", "=", "utils", ".", "listdir_nohidden", "(", "test_dir", ")", "\n", "for", "sub_folder", "in", "test_dirs", ":", "\n", "        ", "save_test_dir", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "os", ".", "path", ".", "basename", "(", "sub_folder", ")", ")", "\n", "\n", "audio_feature_files", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "sub_folder", ",", "'audio_sample/*.mat'", ")", ")", "\n", "audio_feature_files", "=", "utils", ".", "sort_filename", "(", "audio_feature_files", ")", "\n", "image_test_file", "=", "os", ".", "path", ".", "join", "(", "sub_folder", ",", "'image_sample.jpg'", ")", "\n", "audio_test_file", "=", "os", ".", "path", ".", "join", "(", "sub_folder", ",", "'audio_sample.wav'", ")", "\n", "audio_duration", "=", "utils", ".", "get_wav_duration", "(", "audio_test_file", ")", "\n", "\n", "input_image", "=", "data", ".", "load_image", "(", "image_test_file", ",", "image_size", ")", "\n", "input_audios", "=", "[", "data", ".", "load_audio", "(", "audio_feature_file", ")", "for", "audio_feature_file", "in", "audio_feature_files", "]", "\n", "input_images", "=", "[", "input_image", "]", "*", "len", "(", "input_audios", ")", "\n", "\n", "# convert to tensor", "\n", "input_images", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "input_images", ")", ".", "transpose", "(", "(", "0", ",", "3", ",", "1", ",", "2", ")", ")", ")", ".", "cuda", "(", ")", "\n", "input_audios", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "input_audios", ")", ".", "transpose", "(", "(", "0", ",", "3", ",", "1", ",", "2", ")", ")", ")", ".", "cuda", "(", ")", "\n", "\n", "print", "(", "\"input image shape: \"", ",", "input_images", ".", "size", "(", ")", ")", "\n", "print", "(", "\"input audio shape: \"", ",", "input_audios", ".", "size", "(", ")", ")", "\n", "\n", "model_type", "=", "model", ".", "module", ".", "model_type", "(", ")", "if", "isinstance", "(", "model", ",", "torch", ".", "nn", ".", "DataParallel", ")", "else", "model", ".", "model_type", "(", ")", "\n", "\n", "if", "model_type", "==", "'RNN'", ":", "\n", "            ", "input_images", "=", "input_images", ".", "unsqueeze", "(", "0", ")", "\n", "input_audios", "=", "input_audios", ".", "unsqueeze", "(", "0", ")", "\n", "G_images", "=", "model", "(", "input_images", ",", "input_audios", ",", "torch", ".", "tensor", "(", "[", "input_audios", ".", "shape", "[", "1", "]", "]", ",", "dtype", "=", "torch", ".", "int32", ")", ".", "cuda", "(", ")", ")", "\n", "G_images", "=", "G_images", ".", "squeeze", "(", "0", ")", "\n", "", "else", ":", "\n", "            ", "G_images", "=", "model", "(", "input_images", ",", "input_audios", ")", "\n", "", "G_images", "=", "G_images", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "utils", ".", "save_video", "(", "audio_duration", ",", "audio_test_file", ",", "G_images", ",", "save_test_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.test.load_ckpt": [[114, 121], ["torch.load", "torch.load", "model.state_dict", "print", "[].data.copy_", "cur_state_dict[].size", "old_state_dict[].size", "model.state_dict"], "function", ["None"], ["", "", "def", "load_ckpt", "(", "model", ",", "ckpt_path", ")", ":", "\n", "    ", "old_state_dict", "=", "torch", ".", "load", "(", "ckpt_path", ")", "\n", "cur_state_dict", "=", "model", ".", "state_dict", "(", ")", "\n", "for", "param", "in", "cur_state_dict", ":", "\n", "        ", "if", "param", "in", "old_state_dict", "and", "cur_state_dict", "[", "param", "]", ".", "size", "(", ")", "==", "old_state_dict", "[", "param", "]", ".", "size", "(", ")", ":", "\n", "            ", "print", "(", "\"loading param: \"", ",", "param", ")", "\n", "model", ".", "state_dict", "(", ")", "[", "param", "]", ".", "data", ".", "copy_", "(", "old_state_dict", "[", "param", "]", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.SeqDataset.__init__": [[68, 86], ["dataloader.read_csv_file", "dataloader.SeqDataset.load_seq_input", "print", "len"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.read_csv_file", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.trainList_VOX_dlib.load_seq_input"], ["    ", "def", "__init__", "(", "self", ",", "train_file", ",", "use_mask", ",", "config", ",", "transform", "=", "None", ")", ":", "\n", "        ", "self", ".", "train_file", "=", "train_file", "\n", "self", ".", "use_mask", "=", "use_mask", "\n", "self", ".", "num_gt_imgs", "=", "config", ".", "num_gt_imgs", "\n", "self", ".", "num_input_imgs", "=", "config", ".", "num_input_imgs", "\n", "self", ".", "num_seq_length", "=", "config", ".", "num_seq_length", "\n", "self", ".", "use_lip", "=", "config", ".", "use_lip", "\n", "self", ".", "use_word_label", "=", "config", ".", "use_word_label", "\n", "self", ".", "transform", "=", "transform", "\n", "\n", "self", ".", "input_img_seq", "=", "[", "]", "\n", "self", ".", "gt_seq", "=", "[", "]", "\n", "self", ".", "input_audio_seq", "=", "[", "]", "\n", "self", ".", "lip_coord_seq", "=", "[", "]", "\n", "self", ".", "word_label_seq", "=", "[", "]", "\n", "image_list", ",", "gt_list", ",", "audio_list", ",", "coord_list", ",", "word_label_list", "=", "read_csv_file", "(", "train_file", ",", "use_mask", ",", "self", ".", "num_input_imgs", ",", "self", ".", "num_gt_imgs", ",", "self", ".", "use_lip", ",", "self", ".", "use_word_label", ")", "\n", "self", ".", "load_seq_input", "(", "image_list", ",", "gt_list", ",", "audio_list", ",", "coord_list", ",", "word_label_list", ")", "\n", "print", "(", "\"total number of sequence: \"", ",", "len", "(", "self", ".", "input_img_seq", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.SeqDataset.load_seq_input": [[88, 118], ["collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "range", "collections.defaultdict.items", "print", "len", "os.path.split", "os.path.basename", "dataloader.SeqDataset.input_img_seq.append", "dataloader.SeqDataset.gt_seq.append", "dataloader.SeqDataset.input_audio_seq.append", "len", "seq_dic[].append", "dataloader.SeqDataset.lip_coord_seq.append", "dataloader.SeqDataset.word_label_seq.append", "filename.split", "seq_dic[].append", "seq_dic[].append", "seq_dic[].append"], "methods", ["None"], ["", "def", "load_seq_input", "(", "self", ",", "image_list", ",", "gt_list", ",", "audio_list", ",", "coord_list", ",", "word_label_list", ")", ":", "\n", "        ", "seq_dic", "=", "defaultdict", "(", "list", ")", "\n", "coord_dic", "=", "defaultdict", "(", "list", ")", "\n", "word_dic", "=", "defaultdict", "(", "list", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "image_list", ")", ")", ":", "\n", "            ", "basepath", ",", "filename", "=", "os", ".", "path", ".", "split", "(", "image_list", "[", "i", "]", "[", "0", "]", ")", "\n", "celebname", "=", "os", ".", "path", ".", "basename", "(", "basepath", ")", "\n", "audioname", "=", "'_'", ".", "join", "(", "filename", ".", "split", "(", "'_'", ")", "[", ":", "-", "1", "]", ")", "\n", "key", "=", "celebname", "+", "'/'", "+", "audioname", "\n", "\n", "if", "self", ".", "use_lip", "and", "self", ".", "use_word_label", ":", "\n", "                ", "seq_dic", "[", "key", "]", ".", "append", "(", "[", "image_list", "[", "i", "]", ",", "gt_list", "[", "i", "]", ",", "audio_list", "[", "i", "]", ",", "coord_list", "[", "i", "]", ",", "word_label_list", "[", "i", "]", "]", ")", "\n", "", "elif", "self", ".", "use_lip", ":", "\n", "                ", "seq_dic", "[", "key", "]", ".", "append", "(", "[", "image_list", "[", "i", "]", ",", "gt_list", "[", "i", "]", ",", "audio_list", "[", "i", "]", ",", "coord_list", "[", "i", "]", "]", ")", "\n", "", "elif", "self", ".", "use_word_label", ":", "\n", "                ", "seq_dic", "[", "key", "]", ".", "append", "(", "[", "image_list", "[", "i", "]", ",", "gt_list", "[", "i", "]", ",", "audio_list", "[", "i", "]", ",", "word_label_list", "[", "i", "]", "]", ")", "\n", "", "else", ":", "\n", "                ", "seq_dic", "[", "key", "]", ".", "append", "(", "[", "image_list", "[", "i", "]", ",", "gt_list", "[", "i", "]", ",", "audio_list", "[", "i", "]", "]", ")", "\n", "\n", "", "", "for", "key", ",", "items", "in", "seq_dic", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "input_img_seq", ".", "append", "(", "[", "elem", "[", "0", "]", "for", "elem", "in", "items", "]", ")", "\n", "self", ".", "gt_seq", ".", "append", "(", "[", "elem", "[", "1", "]", "for", "elem", "in", "items", "]", ")", "\n", "self", ".", "input_audio_seq", ".", "append", "(", "[", "elem", "[", "2", "]", "for", "elem", "in", "items", "]", ")", "\n", "if", "self", ".", "use_lip", ":", "\n", "                ", "self", ".", "lip_coord_seq", ".", "append", "(", "[", "elem", "[", "3", "]", "for", "elem", "in", "items", "]", ")", "\n", "", "if", "self", ".", "use_word_label", ":", "\n", "                ", "self", ".", "word_label_seq", ".", "append", "(", "items", "[", "0", "]", "[", "-", "1", "]", ")", "\n", "\n", "", "", "print", "(", "\"total sequence: \"", ",", "len", "(", "self", ".", "input_img_seq", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.SeqDataset.__len__": [[120, 122], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "input_img_seq", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.SeqDataset.__getitem__": [[123, 155], ["range", "len", "random.randint", "min", "images.append", "gts.append", "audios.append", "dataloader.SeqDataset.transform", "len", "len", "lips.append", "len", "dataloader.load_image", "dataloader.load_image", "dataloader.load_audio"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.data.load_image", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.data.load_image", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.data.load_audio"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "images", ",", "gts", ",", "audios", ",", "lips", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "\n", "if", "len", "(", "self", ".", "input_img_seq", "[", "idx", "]", ")", ">", "self", ".", "num_seq_length", ":", "\n", "            ", "start_i", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "self", ".", "input_img_seq", "[", "idx", "]", ")", "-", "self", ".", "num_seq_length", ")", "\n", "end_i", "=", "min", "(", "len", "(", "self", ".", "input_img_seq", "[", "idx", "]", ")", ",", "start_i", "+", "self", ".", "num_seq_length", ")", "\n", "", "else", ":", "\n", "            ", "start_i", ",", "end_i", "=", "0", ",", "len", "(", "self", ".", "input_img_seq", "[", "idx", "]", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "start_i", ",", "end_i", ")", ":", "\n", "            ", "images", ".", "append", "(", "[", "load_image", "(", "img_path", ")", "for", "img_path", "in", "self", ".", "input_img_seq", "[", "idx", "]", "[", "i", "]", "]", ")", "#(seq_len, num_input_imgs, h,w,c)", "\n", "gts", ".", "append", "(", "[", "load_image", "(", "img_path", ")", "for", "img_path", "in", "self", ".", "gt_seq", "[", "idx", "]", "[", "i", "]", "]", ")", "\n", "audios", ".", "append", "(", "[", "load_audio", "(", "audio_path", ")", "for", "audio_path", "in", "self", ".", "input_audio_seq", "[", "idx", "]", "[", "i", "]", "]", ")", "\n", "\n", "if", "self", ".", "use_lip", ":", "\n", "                ", "lips", ".", "append", "(", "self", ".", "lip_coord_seq", "[", "idx", "]", "[", "i", "]", ")", "#(seq_len, 4)", "\n", "", "if", "self", ".", "use_word_label", ":", "\n", "                ", "label", "=", "self", ".", "word_label_seq", "[", "idx", "]", "# int", "\n", "\n", "", "", "if", "self", ".", "use_word_label", "and", "self", ".", "use_lip", ":", "\n", "            ", "sample", "=", "{", "'img'", ":", "images", ",", "'audio'", ":", "audios", ",", "'gt'", ":", "gts", ",", "'lip'", ":", "lips", ",", "'label'", ":", "label", "}", "\n", "", "elif", "self", ".", "use_lip", ":", "\n", "            ", "sample", "=", "{", "'img'", ":", "images", ",", "'audio'", ":", "audios", ",", "'gt'", ":", "gts", ",", "'lip'", ":", "lips", "}", "\n", "", "elif", "self", ".", "use_word_label", ":", "\n", "            ", "sample", "=", "{", "'img'", ":", "images", ",", "'audio'", ":", "audios", ",", "'gt'", ":", "gts", ",", "'label'", ":", "label", "}", "\n", "", "else", ":", "\n", "            ", "sample", "=", "{", "'img'", ":", "images", ",", "'audio'", ":", "audios", ",", "'gt'", ":", "gts", "}", "\n", "", "if", "self", ".", "transform", ":", "\n", "            ", "sample", "=", "self", ".", "transform", "(", "sample", ")", "\n", "\n", "", "return", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.CSVDataset.__init__": [[160, 174], ["dataloader.read_csv_file"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.read_csv_file"], ["def", "__init__", "(", "self", ",", "train_file", ",", "use_mask", ",", "num_input_imgs", ",", "num_gt_imgs", ",", "transform", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            train_file (string): CSV file with training annotations\n            annotations (string): CSV file with class list\n            test_file (string, optional): CSV file with testing annotations\n        \"\"\"", "\n", "self", ".", "train_file", "=", "train_file", "\n", "self", ".", "use_mask", "=", "use_mask", "\n", "self", ".", "num_gt_imgs", "=", "num_gt_imgs", "\n", "self", ".", "num_input_imgs", "=", "num_input_imgs", "\n", "self", ".", "transform", "=", "transform", "\n", "\n", "self", ".", "input_imgs", ",", "self", ".", "gt_imgs", ",", "self", ".", "input_audios", ",", "_", ",", "_", "=", "read_csv_file", "(", "train_file", ",", "use_mask", ",", "num_input_imgs", ",", "num_gt_imgs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.CSVDataset.__len__": [[176, 178], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "input_imgs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.CSVDataset.__getitem__": [[179, 190], ["dataloader.load_image", "dataloader.load_image", "dataloader.load_audio", "dataloader.CSVDataset.transform"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.data.load_image", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.data.load_image", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.data.load_audio"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "images", "=", "[", "load_image", "(", "img_path", ")", "for", "img_path", "in", "self", ".", "input_imgs", "[", "idx", "]", "]", "\n", "gts", "=", "[", "load_image", "(", "img_path", ")", "for", "img_path", "in", "self", ".", "gt_imgs", "[", "idx", "]", "]", "\n", "audios", "=", "[", "load_audio", "(", "audio_path", ")", "for", "audio_path", "in", "self", ".", "input_audios", "[", "idx", "]", "]", "\n", "# audios = load_audio(self.input_audios[idx][0])", "\n", "\n", "\n", "sample", "=", "{", "'img'", ":", "images", ",", "'audio'", ":", "audios", ",", "'gt'", ":", "gts", "}", "\n", "if", "self", ".", "transform", ":", "\n", "            ", "sample", "=", "self", ".", "transform", "(", "sample", ")", "\n", "", "return", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.NpySeqDataset.__init__": [[258, 269], ["dataloader.NpySeqDataset.read_npy_file", "print", "len"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.NpySeqDataset.read_npy_file"], ["    ", "def", "__init__", "(", "self", ",", "train_file", ",", "config", ",", "transform", "=", "None", ")", ":", "\n", "        ", "self", ".", "transform", "=", "transform", "\n", "self", ".", "img_seq", "=", "[", "]", "\n", "self", ".", "gt_seq", "=", "[", "]", "\n", "self", ".", "audio_seq", "=", "[", "]", "\n", "self", ".", "label_seq", "=", "[", "]", "\n", "self", ".", "lip_coord_seq", "=", "[", "]", "\n", "self", ".", "use_word_label", "=", "config", ".", "use_word_label", "\n", "self", ".", "use_lip", "=", "config", ".", "use_lip", "\n", "self", ".", "read_npy_file", "(", "train_file", ")", "\n", "print", "(", "'length of training list: '", ",", "len", "(", "self", ".", "img_seq", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.NpySeqDataset.read_npy_file": [[270, 298], ["open", "f.readlines", "line.rstrip().split", "dataloader.NpySeqDataset.img_seq.append", "dataloader.NpySeqDataset.gt_seq.append", "dataloader.NpySeqDataset.audio_seq.append", "range", "dataloader.NpySeqDataset.lip_coord_seq.append", "dataloader.NpySeqDataset.label_seq.append", "line.rstrip", "list", "coords.append", "int", "map", "print", "print", "print", "print", "elems[].split"], "methods", ["None"], ["", "def", "read_npy_file", "(", "self", ",", "train_file", ")", ":", "\n", "        ", "with", "open", "(", "train_file", ")", "as", "f", ":", "\n", "            ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "", "for", "line", "in", "lines", ":", "\n", "            ", "elems", "=", "line", ".", "rstrip", "(", "'\\n'", ")", ".", "split", "(", "' '", ")", "\n", "self", ".", "img_seq", ".", "append", "(", "elems", "[", "0", "]", ")", "\n", "self", ".", "gt_seq", ".", "append", "(", "elems", "[", "1", "]", ")", "\n", "self", ".", "audio_seq", ".", "append", "(", "elems", "[", "2", "]", ")", "\n", "if", "self", ".", "use_lip", ":", "\n", "                ", "coords", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "11", ")", ":", "\n", "                    ", "x1", ",", "y1", ",", "x2", ",", "y2", "=", "list", "(", "map", "(", "int", ",", "elems", "[", "3", "+", "i", "]", ".", "split", "(", "','", ")", ")", ")", "\n", "if", "x2", "-", "x1", ">", "40", ":", "\n", "                        ", "print", "(", "'mouth size > 40'", ")", "\n", "x2", "-=", "(", "x2", "-", "x1", ")", "-", "40", "\n", "", "if", "y2", "-", "y1", ">", "40", ":", "\n", "                        ", "print", "(", "'mouth size > 40'", ")", "\n", "y2", "-=", "(", "y2", "-", "y1", ")", "-", "40", "\n", "", "if", "x2", "-", "x1", "<", "40", ":", "\n", "                        ", "print", "(", "'mouth size < 40'", ")", "\n", "x2", "+=", "40", "-", "(", "x2", "-", "x1", ")", "\n", "", "if", "y2", "-", "y1", "<", "40", ":", "\n", "                        ", "print", "(", "'mouth size < 40'", ")", "\n", "y2", "+=", "40", "-", "(", "y2", "-", "y1", ")", "\n", "", "coords", ".", "append", "(", "[", "x1", ",", "y1", ",", "x2", ",", "y2", "]", ")", "# [[x1, y1, x2, y2],...[x1, y1, x2, y2]]", "\n", "", "self", ".", "lip_coord_seq", ".", "append", "(", "coords", ")", "# [coords, ..., coords]", "\n", "", "if", "self", ".", "use_word_label", ":", "\n", "                ", "self", ".", "label_seq", ".", "append", "(", "int", "(", "elems", "[", "-", "1", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.NpySeqDataset.__len__": [[299, 301], ["len"], "methods", ["None"], ["", "", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "img_seq", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.NpySeqDataset.__getitem__": [[302, 334], ["numpy.load", "numpy.load", "numpy.load", "range", "print", "sys.exit", "input_images.append", "gt_images.append", "input_audios.append", "dataloader.NpySeqDataset.transform"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "images", "=", "np", ".", "load", "(", "self", ".", "img_seq", "[", "idx", "]", ")", "# np array: (seq_len, h, w, 3)", "\n", "gts", "=", "np", ".", "load", "(", "self", ".", "gt_seq", "[", "idx", "]", ")", "# np array: (seq_len, h, w, 3)", "\n", "audios", "=", "np", ".", "load", "(", "self", ".", "audio_seq", "[", "idx", "]", ")", "# np array: (seq_len, h, w, 1)", "\n", "if", "self", ".", "use_word_label", ":", "\n", "            ", "label", "=", "self", ".", "label_seq", "[", "idx", "]", "# int", "\n", "\n", "", "if", "audios", ".", "shape", "[", "0", "]", "!=", "11", "and", "audios", ".", "shape", "[", "1", "]", "!=", "12", "and", "audios", ".", "shape", "[", "2", "]", "!=", "35", "and", "audios", ".", "shape", "[", "3", "]", "!=", "1", ":", "\n", "            ", "print", "(", "\"********** audio shape does not match ************\"", ",", "self", ".", "audio_seq", "[", "idx", "]", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "input_images", ",", "gt_images", ",", "input_audios", ",", "lips", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "images", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "input_images", ".", "append", "(", "[", "images", "[", "i", "]", "]", ")", "\n", "gt_images", ".", "append", "(", "[", "gts", "[", "i", "]", "]", ")", "\n", "input_audios", ".", "append", "(", "[", "audios", "[", "i", "]", "]", ")", "\n", "", "if", "self", ".", "use_lip", ":", "\n", "            ", "lips", "=", "self", ".", "lip_coord_seq", "[", "idx", "]", "# [[x1, y1, x2, y2],...[x1, y1, x2, y2]]", "\n", "\n", "", "if", "self", ".", "use_word_label", "and", "self", ".", "use_lip", ":", "\n", "            ", "sample", "=", "{", "'img'", ":", "input_images", ",", "'audio'", ":", "input_audios", ",", "'gt'", ":", "gt_images", ",", "'lip'", ":", "lips", ",", "'label'", ":", "label", "}", "\n", "", "elif", "self", ".", "use_word_label", ":", "\n", "            ", "sample", "=", "{", "'img'", ":", "input_images", ",", "'audio'", ":", "input_audios", ",", "'gt'", ":", "gt_images", ",", "'label'", ":", "label", "}", "\n", "", "elif", "self", ".", "use_lip", ":", "\n", "            ", "sample", "=", "{", "'img'", ":", "input_images", ",", "'audio'", ":", "input_audios", ",", "'gt'", ":", "gt_images", ",", "'lip'", ":", "lips", "}", "\n", "", "else", ":", "\n", "            ", "sample", "=", "{", "'img'", ":", "input_images", ",", "'audio'", ":", "input_audios", ",", "'gt'", ":", "gt_images", "}", "\n", "\n", "", "if", "self", ".", "transform", ":", "\n", "            ", "sample", "=", "self", ".", "transform", "(", "sample", ")", "\n", "", "return", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.Resizer.__init__": [[440, 442], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "image_size", "=", "112", ")", ":", "\n", "        ", "self", ".", "image_size", "=", "image_size", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.Resizer.__call__": [[443, 459], ["range", "len", "isinstance", "re_images.append", "re_gts.append", "re_images.append", "re_gts.append", "scipy.misc.imresize", "scipy.misc.imresize", "scipy.misc.imresize", "scipy.misc.imresize"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "sample", ")", ":", "\n", "        ", "images", ",", "gts", "=", "sample", "[", "'img'", "]", ",", "sample", "[", "'gt'", "]", "# (seq_len, num_input_imgs, h,w,c) or (num_input_imgs, h,w,c)", "\n", "\n", "re_images", ",", "re_gts", ",", "re_lips", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "images", ")", ")", ":", "\n", "            ", "if", "isinstance", "(", "images", "[", "i", "]", ",", "list", ")", ":", "\n", "                ", "re_images", ".", "append", "(", "[", "imresize", "(", "img", ",", "[", "self", ".", "image_size", ",", "self", ".", "image_size", "]", ")", "for", "img", "in", "images", "[", "i", "]", "]", ")", "\n", "re_gts", ".", "append", "(", "[", "imresize", "(", "img", ",", "[", "self", ".", "image_size", ",", "self", ".", "image_size", "]", ")", "for", "img", "in", "gts", "[", "i", "]", "]", ")", "\n", "", "else", ":", "#(h, w, c)", "\n", "                ", "re_images", ".", "append", "(", "imresize", "(", "images", "[", "i", "]", ",", "[", "self", ".", "image_size", ",", "self", ".", "image_size", "]", ")", ")", "\n", "re_gts", ".", "append", "(", "imresize", "(", "gts", "[", "i", "]", ",", "[", "self", ".", "image_size", ",", "self", ".", "image_size", "]", ")", ")", "\n", "\n", "", "", "sample", "[", "'img'", "]", "=", "re_images", "\n", "sample", "[", "'gt'", "]", "=", "re_gts", "\n", "\n", "return", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.Normalizer.__call__": [[462, 475], ["range", "len", "isinstance", "norm_images.append", "norm_gts.append", "norm_images.append", "norm_gts.append"], "methods", ["None"], ["    ", "def", "__call__", "(", "self", ",", "sample", ")", ":", "\n", "        ", "images", ",", "gts", "=", "sample", "[", "'img'", "]", ",", "sample", "[", "'gt'", "]", "\n", "norm_images", ",", "norm_gts", "=", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "images", ")", ")", ":", "\n", "            ", "if", "isinstance", "(", "images", "[", "i", "]", ",", "list", ")", ":", "# sequence input", "\n", "                ", "norm_images", ".", "append", "(", "[", "(", "img", "*", "2.0", "/", "255.0", "-", "1.0", ")", ".", "astype", "(", "np", ".", "float32", ")", "for", "img", "in", "images", "[", "i", "]", "]", ")", "\n", "norm_gts", ".", "append", "(", "[", "(", "gt", "*", "2.0", "/", "255.0", "-", "1.0", ")", ".", "astype", "(", "np", ".", "float32", ")", "for", "gt", "in", "gts", "[", "i", "]", "]", ")", "\n", "", "else", ":", "\n", "                ", "norm_images", ".", "append", "(", "(", "images", "[", "i", "]", "*", "2.0", "/", "255.0", "-", "1.0", ")", ".", "astype", "(", "np", ".", "float32", ")", ")", "\n", "norm_gts", ".", "append", "(", "(", "gts", "[", "i", "]", "*", "2.0", "/", "255.0", "-", "1.0", ")", ".", "astype", "(", "np", ".", "float32", ")", ")", "\n", "", "", "sample", "[", "'img'", "]", "=", "norm_images", "\n", "sample", "[", "'gt'", "]", "=", "norm_gts", "\n", "return", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.ToTensor.__call__": [[478, 500], ["isinstance", "numpy.array", "numpy.array", "numpy.array", "numpy.concatenate().transpose", "numpy.concatenate().transpose", "numpy.concatenate().transpose", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.LongTensor", "torch.tensor", "numpy.concatenate().transpose", "numpy.concatenate().transpose", "numpy.concatenate().transpose", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.LongTensor", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.LongTensor", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate"], "methods", ["None"], ["    ", "def", "__call__", "(", "self", ",", "sample", ")", ":", "\n", "        ", "images", ",", "audios", ",", "gts", "=", "sample", "[", "'img'", "]", ",", "sample", "[", "'audio'", "]", ",", "sample", "[", "'gt'", "]", "\n", "lips", "=", "sample", "[", "'lip'", "]", "if", "'lip'", "in", "sample", "else", "None", "\n", "label", "=", "sample", "[", "'label'", "]", "if", "'label'", "in", "sample", "else", "None", "\n", "\n", "if", "isinstance", "(", "images", "[", "0", "]", ",", "list", ")", ":", "# sequence input", "\n", "            ", "images", "=", "np", ".", "array", "(", "[", "np", ".", "concatenate", "(", "imgs_", ",", "axis", "=", "-", "1", ")", ".", "transpose", "(", "(", "2", ",", "0", ",", "1", ")", ")", "for", "imgs_", "in", "images", "]", ")", "\n", "audios", "=", "np", ".", "array", "(", "[", "np", ".", "concatenate", "(", "audios_", ",", "axis", "=", "-", "1", ")", ".", "transpose", "(", "(", "2", ",", "0", ",", "1", ")", ")", "for", "audios_", "in", "audios", "]", ")", "\n", "gts", "=", "np", ".", "array", "(", "[", "np", ".", "concatenate", "(", "gts_", ",", "axis", "=", "-", "1", ")", ".", "transpose", "(", "(", "2", ",", "0", ",", "1", ")", ")", "for", "gts_", "in", "gts", "]", ")", "\n", "", "else", ":", "\n", "            ", "images", "=", "np", ".", "concatenate", "(", "images", ",", "axis", "=", "-", "1", ")", ".", "transpose", "(", "(", "2", ",", "0", ",", "1", ")", ")", "\n", "audios", "=", "np", ".", "concatenate", "(", "audios", ",", "axis", "=", "-", "1", ")", ".", "transpose", "(", "(", "2", ",", "0", ",", "1", ")", ")", "\n", "gts", "=", "np", ".", "concatenate", "(", "gts", ",", "axis", "=", "-", "1", ")", ".", "transpose", "(", "(", "2", ",", "0", ",", "1", ")", ")", "\n", "\n", "", "if", "lips", "is", "not", "None", "and", "label", "is", "not", "None", ":", "\n", "            ", "return", "{", "'img'", ":", "torch", ".", "from_numpy", "(", "images", ")", ",", "'audio'", ":", "torch", ".", "from_numpy", "(", "audios", ")", ",", "'gt'", ":", "torch", ".", "from_numpy", "(", "gts", ")", ",", "'lip'", ":", "torch", ".", "LongTensor", "(", "lips", ")", ",", "'label'", ":", "torch", ".", "tensor", "(", "label", ",", "dtype", "=", "torch", ".", "long", ")", "}", "\n", "", "elif", "lips", "is", "not", "None", ":", "\n", "            ", "return", "{", "'img'", ":", "torch", ".", "from_numpy", "(", "images", ")", ",", "'audio'", ":", "torch", ".", "from_numpy", "(", "audios", ")", ",", "'gt'", ":", "torch", ".", "from_numpy", "(", "gts", ")", ",", "'lip'", ":", "torch", ".", "LongTensor", "(", "lips", ")", "}", "\n", "", "elif", "label", "is", "not", "None", ":", "\n", "            ", "return", "{", "'img'", ":", "torch", ".", "from_numpy", "(", "images", ")", ",", "'audio'", ":", "torch", ".", "from_numpy", "(", "audios", ")", ",", "'gt'", ":", "torch", ".", "from_numpy", "(", "gts", ")", ",", "'label'", ":", "torch", ".", "LongTensor", "(", "label", ")", "}", "\n", "", "else", ":", "\n", "            ", "return", "{", "'img'", ":", "torch", ".", "from_numpy", "(", "images", ")", ",", "'audio'", ":", "torch", ".", "from_numpy", "(", "audios", ")", ",", "'gt'", ":", "torch", ".", "from_numpy", "(", "gts", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.AspectRatioBasedSampler.__init__": [[505, 510], ["dataloader.AspectRatioBasedSampler.group_images"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.AspectRatioBasedSampler.group_images"], ["    ", "def", "__init__", "(", "self", ",", "data_source", ",", "batch_size", ",", "drop_last", ")", ":", "\n", "        ", "self", ".", "data_source", "=", "data_source", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "drop_last", "=", "drop_last", "\n", "self", ".", "groups", "=", "self", ".", "group_images", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.AspectRatioBasedSampler.__iter__": [[511, 515], ["random.shuffle"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "random", ".", "shuffle", "(", "self", ".", "groups", ")", "\n", "for", "group", "in", "self", ".", "groups", ":", "\n", "            ", "yield", "group", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.AspectRatioBasedSampler.__len__": [[516, 521], ["len", "len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "drop_last", ":", "\n", "            ", "return", "len", "(", "self", ".", "sampler", ")", "//", "self", ".", "batch_size", "\n", "", "else", ":", "\n", "            ", "return", "(", "len", "(", "self", ".", "sampler", ")", "+", "self", ".", "batch_size", "-", "1", ")", "//", "self", ".", "batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.AspectRatioBasedSampler.group_images": [[522, 529], ["list", "list.sort", "range", "len", "range", "dataloader.AspectRatioBasedSampler.data_source.image_aspect_ratio", "range", "len", "len"], "methods", ["None"], ["", "", "def", "group_images", "(", "self", ")", ":", "\n", "# determine the order of the images", "\n", "        ", "order", "=", "list", "(", "range", "(", "len", "(", "self", ".", "data_source", ")", ")", ")", "\n", "order", ".", "sort", "(", "key", "=", "lambda", "x", ":", "self", ".", "data_source", ".", "image_aspect_ratio", "(", "x", ")", ")", "\n", "\n", "# divide into groups, one group = one batch", "\n", "return", "[", "[", "order", "[", "x", "%", "len", "(", "order", ")", "]", "for", "x", "in", "range", "(", "i", ",", "i", "+", "self", ".", "batch_size", ")", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "order", ")", ",", "self", ".", "batch_size", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.load_image": [[22, 29], ["scipy.misc.imread", "np.stack.astype", "len", "numpy.stack", "len", "print"], "function", ["None"], ["def", "load_image", "(", "img_path", ")", ":", "\n", "    ", "img", "=", "imread", "(", "img_path", ")", "\n", "if", "len", "(", "img", ".", "shape", ")", "==", "2", ":", "\n", "        ", "img", "=", "np", ".", "stack", "(", "(", "img", ",", ")", "*", "3", ",", "axis", "=", "0", ")", "\n", "", "if", "len", "(", "img", ".", "shape", ")", "!=", "3", ":", "\n", "        ", "print", "(", "\"***** not rgb image ******\"", ",", "img_path", ",", "img", ".", "shape", ")", "\n", "", "return", "img", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.load_audio": [[31, 37], ["scipy.io.loadmat", "numpy.expand_dims().astype", "numpy.transpose", "numpy.expand_dims"], "function", ["None"], ["", "def", "load_audio", "(", "audio_path", ")", ":", "\n", "# load the mat file", "\n", "    ", "file", "=", "loadmat", "(", "audio_path", ")", "\n", "audio", "=", "np", ".", "transpose", "(", "file", "[", "'mfcc'", "]", ",", "(", "1", ",", "0", ")", ")", "[", "1", ":", ",", ":", "]", "\n", "audio", "=", "np", ".", "expand_dims", "(", "audio", ",", "axis", "=", "-", "1", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "return", "audio", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.load_mask": [[39, 42], ["img.astype", "scipy.misc.imread"], "function", ["None"], ["", "def", "load_mask", "(", "mask_path", ")", ":", "\n", "    ", "img", "=", "imread", "(", "mask_path", ",", "flatten", "=", "True", ")", "/", "255.0", "\n", "return", "img", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.read_csv_file": [[44, 65], ["os.path.abspath", "open", "f.readlines", "line.rstrip().split", "input_imgs.append", "gt_imgs.append", "input_audios.append", "elems[].split", "lip_coordinates.append", "word_labels.append", "line.rstrip", "list", "int", "map"], "function", ["None"], ["", "def", "read_csv_file", "(", "path", ",", "use_mask", ",", "num_input_imgs", ",", "num_gt_imgs", ",", "use_lip", "=", "False", ",", "use_word_label", "=", "False", ")", ":", "\n", "    ", "filename", "=", "os", ".", "path", ".", "abspath", "(", "path", ")", "\n", "input_imgs", ",", "gt_imgs", ",", "input_audios", ",", "lip_coordinates", ",", "word_labels", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "with", "open", "(", "filename", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "", "for", "line", "in", "lines", ":", "\n", "        ", "elems", "=", "line", ".", "rstrip", "(", "'\\n'", ")", ".", "split", "(", "' '", ")", "\n", "# if len(elems) < num_input_imgs + num_gt_imgs + 1:", "\n", "#     raise ValueError(\"input file list has less elements as needed\")", "\n", "input_imgs", ".", "append", "(", "elems", "[", ":", "num_input_imgs", "]", ")", "\n", "gt_imgs", ".", "append", "(", "elems", "[", "num_input_imgs", ":", "num_input_imgs", "+", "num_gt_imgs", "]", ")", "\n", "input_audios", ".", "append", "(", "elems", "[", "num_input_imgs", "+", "num_gt_imgs", ":", "num_input_imgs", "+", "num_gt_imgs", "+", "1", "]", ")", "\n", "\n", "if", "use_lip", "==", "True", ":", "\n", "            ", "x1", ",", "y1", ",", "x2", ",", "y2", "=", "elems", "[", "num_input_imgs", "+", "num_gt_imgs", "+", "1", "]", ".", "split", "(", "','", ")", "\n", "lip_coordinates", ".", "append", "(", "list", "(", "map", "(", "int", ",", "[", "x1", ",", "y1", ",", "x2", ",", "y2", "]", ")", ")", ")", "\n", "", "if", "use_word_label", ":", "\n", "            ", "word_labels", ".", "append", "(", "int", "(", "elems", "[", "-", "1", "]", ")", ")", "\n", "\n", "\n", "", "", "return", "input_imgs", ",", "gt_imgs", ",", "input_audios", ",", "lip_coordinates", ",", "word_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.convert_seq_to_batch": [[337, 385], ["numpy.array", "int", "len", "torch.zeros", "torch.zeros", "torch.zeros", "range", "padded_img_batch.contiguous().view.contiguous().view", "padded_audio_batch.contiguous().view.contiguous().view", "padded_gt_batch.contiguous().view.contiguous().view", "max", "torch.zeros", "torch.stack", "padded_lip_batch.contiguous().view.contiguous().view", "padded_img_batch.contiguous().view.contiguous", "padded_audio_batch.contiguous().view.contiguous", "padded_gt_batch.contiguous().view.contiguous", "torch.from_numpy", "padded_lip_batch.contiguous().view.contiguous", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "function", ["None"], ["", "", "def", "convert_seq_to_batch", "(", "data", ")", ":", "\n", "    ", "img_batch", "=", "[", "s", "[", "'img'", "]", "for", "s", "in", "data", "]", "# [(seq_len, h, w, c), ..., (seq_len, h, w, c)]", "\n", "audio_batch", "=", "[", "s", "[", "'audio'", "]", "for", "s", "in", "data", "]", "\n", "gt_batch", "=", "[", "s", "[", "'gt'", "]", "for", "s", "in", "data", "]", "\n", "lip_batch", ",", "label_batch", "=", "None", ",", "None", "\n", "if", "'lip'", "in", "data", "[", "0", "]", ":", "\n", "        ", "lip_batch", "=", "[", "s", "[", "'lip'", "]", "for", "s", "in", "data", "]", "# [(seq_len, 4), ..., (seq_len, 4)]", "\n", "", "if", "'label'", "in", "data", "[", "0", "]", ":", "\n", "        ", "label_batch", "=", "[", "s", "[", "'label'", "]", "for", "s", "in", "data", "]", "# [label, ..., label]", "\n", "\n", "", "seq_len_batch", "=", "np", ".", "array", "(", "[", "s", ".", "shape", "[", "0", "]", "for", "s", "in", "img_batch", "]", ",", "dtype", "=", "np", ".", "int32", ")", "#[seq_len1, seq_len2, ...]", "\n", "\n", "\n", "# padding to the same length", "\n", "max_len", "=", "int", "(", "max", "(", "seq_len_batch", ")", ")", "\n", "batch_size", "=", "len", "(", "img_batch", ")", "\n", "padded_img_batch", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max_len", ",", "img_batch", "[", "0", "]", ".", "shape", "[", "1", "]", ",", "img_batch", "[", "0", "]", ".", "shape", "[", "2", "]", ",", "img_batch", "[", "0", "]", ".", "shape", "[", "3", "]", ")", "\n", "padded_gt_batch", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max_len", ",", "gt_batch", "[", "0", "]", ".", "shape", "[", "1", "]", ",", "gt_batch", "[", "0", "]", ".", "shape", "[", "2", "]", ",", "gt_batch", "[", "0", "]", ".", "shape", "[", "3", "]", ")", "\n", "padded_audio_batch", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max_len", ",", "audio_batch", "[", "0", "]", ".", "shape", "[", "1", "]", ",", "audio_batch", "[", "0", "]", ".", "shape", "[", "2", "]", ",", "audio_batch", "[", "0", "]", ".", "shape", "[", "3", "]", ")", "\n", "if", "lip_batch", "is", "not", "None", ":", "\n", "        ", "padded_lip_batch", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "max_len", ",", "lip_batch", "[", "0", "]", ".", "shape", "[", "1", "]", ")", ",", "dtype", "=", "torch", ".", "int32", ")", "# (batch_size, max_len, 4)", "\n", "", "if", "label_batch", "is", "not", "None", ":", "\n", "        ", "label_batch", "=", "torch", ".", "stack", "(", "label_batch", ")", "# (batch_size,)", "\n", "\n", "", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "padded_img_batch", "[", "i", ",", ":", "img_batch", "[", "i", "]", ".", "shape", "[", "0", "]", ",", ":", ",", ":", ",", ":", "]", "=", "img_batch", "[", "i", "]", "\n", "padded_gt_batch", "[", "i", ",", ":", "gt_batch", "[", "i", "]", ".", "shape", "[", "0", "]", ",", ":", ",", ":", ",", ":", "]", "=", "gt_batch", "[", "i", "]", "\n", "padded_audio_batch", "[", "i", ",", ":", "audio_batch", "[", "i", "]", ".", "shape", "[", "0", "]", ",", ":", ",", ":", ",", ":", "]", "=", "audio_batch", "[", "i", "]", "\n", "if", "lip_batch", "is", "not", "None", ":", "\n", "            ", "padded_lip_batch", "[", "i", ",", ":", "lip_batch", "[", "i", "]", ".", "shape", "[", "0", "]", ",", ":", "]", "=", "lip_batch", "[", "i", "]", "\n", "\n", "\n", "# reshape", "\n", "", "", "padded_img_batch", "=", "padded_img_batch", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", "*", "max_len", ",", "padded_img_batch", ".", "shape", "[", "2", "]", ",", "padded_img_batch", ".", "shape", "[", "3", "]", ",", "padded_img_batch", ".", "shape", "[", "4", "]", ")", "\n", "padded_audio_batch", "=", "padded_audio_batch", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", "*", "max_len", ",", "padded_audio_batch", ".", "shape", "[", "2", "]", ",", "padded_audio_batch", ".", "shape", "[", "3", "]", ",", "padded_audio_batch", ".", "shape", "[", "4", "]", ")", "\n", "padded_gt_batch", "=", "padded_gt_batch", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", "*", "max_len", ",", "padded_gt_batch", ".", "shape", "[", "2", "]", ",", "padded_gt_batch", ".", "shape", "[", "3", "]", ",", "padded_gt_batch", ".", "shape", "[", "4", "]", ")", "\n", "if", "lip_batch", "is", "not", "None", ":", "\n", "        ", "padded_lip_batch", "=", "padded_lip_batch", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", "*", "max_len", ",", "padded_lip_batch", ".", "shape", "[", "2", "]", ")", "\n", "\n", "\n", "", "if", "lip_batch", "is", "not", "None", "and", "label_batch", "is", "not", "None", ":", "\n", "        ", "return", "{", "'img'", ":", "padded_img_batch", ",", "'audio'", ":", "padded_audio_batch", ",", "'gt'", ":", "padded_gt_batch", ",", "'len'", ":", "torch", ".", "from_numpy", "(", "seq_len_batch", ")", ",", "'label'", ":", "label_batch", ",", "'lip'", ":", "padded_lip_batch", "}", "\n", "", "elif", "lip_batch", "is", "not", "None", ":", "\n", "        ", "return", "{", "'img'", ":", "padded_img_batch", ",", "'audio'", ":", "padded_audio_batch", ",", "'gt'", ":", "padded_gt_batch", ",", "'len'", ":", "torch", ".", "from_numpy", "(", "seq_len_batch", ")", ",", "'lip'", ":", "padded_lip_batch", "}", "\n", "", "elif", "label_batch", "is", "not", "None", ":", "\n", "        ", "return", "{", "'img'", ":", "padded_img_batch", ",", "'audio'", ":", "padded_audio_batch", ",", "'gt'", ":", "padded_gt_batch", ",", "'len'", ":", "torch", ".", "from_numpy", "(", "seq_len_batch", ")", ",", "'label'", ":", "label_batch", "}", "\n", "", "else", ":", "\n", "        ", "return", "{", "'img'", ":", "padded_img_batch", ",", "'audio'", ":", "padded_audio_batch", ",", "'gt'", ":", "padded_gt_batch", ",", "'len'", ":", "torch", ".", "from_numpy", "(", "seq_len_batch", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.dataloader.collater": [[389, 435], ["sorted", "sorted", "sorted", "numpy.array", "len", "torch.zeros", "torch.zeros", "torch.zeros", "range", "sorted", "torch.zeros", "torch.stack", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "function", ["None"], ["", "", "def", "collater", "(", "data", ")", ":", "\n", "    ", "img_batch", "=", "[", "s", "[", "'img'", "]", "for", "s", "in", "data", "]", "\n", "audio_batch", "=", "[", "s", "[", "'audio'", "]", "for", "s", "in", "data", "]", "\n", "gt_batch", "=", "[", "s", "[", "'gt'", "]", "for", "s", "in", "data", "]", "\n", "lip_batch", "=", "None", "\n", "label_batch", "=", "None", "\n", "if", "'lip'", "in", "data", "[", "0", "]", ":", "\n", "        ", "lip_batch", "=", "[", "s", "[", "'lip'", "]", "for", "s", "in", "data", "]", "# (batch_size, seq_len, 4)", "\n", "", "if", "'label'", "in", "data", "[", "0", "]", ":", "\n", "        ", "label_batch", "=", "[", "s", "[", "'label'", "]", "for", "s", "in", "data", "]", "# [label, ..., label]", "\n", "\n", "", "img_batch", "=", "sorted", "(", "img_batch", ",", "key", "=", "lambda", "x", ":", "x", ".", "shape", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "audio_batch", "=", "sorted", "(", "audio_batch", ",", "key", "=", "lambda", "x", ":", "x", ".", "shape", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "gt_batch", "=", "sorted", "(", "gt_batch", ",", "key", "=", "lambda", "x", ":", "x", ".", "shape", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "if", "lip_batch", "is", "not", "None", ":", "\n", "        ", "lip_batch", "=", "sorted", "(", "lip_batch", ",", "key", "=", "lambda", "x", ":", "x", ".", "shape", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "", "seq_len", "=", "np", ".", "array", "(", "[", "s", ".", "shape", "[", "0", "]", "for", "s", "in", "img_batch", "]", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "\n", "# padding to max_len", "\n", "max_len", "=", "img_batch", "[", "0", "]", ".", "shape", "[", "0", "]", "\n", "batch_size", "=", "len", "(", "img_batch", ")", "\n", "\n", "padded_img_batch", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max_len", ",", "img_batch", "[", "0", "]", ".", "shape", "[", "1", "]", ",", "img_batch", "[", "0", "]", ".", "shape", "[", "2", "]", ",", "img_batch", "[", "0", "]", ".", "shape", "[", "3", "]", ")", "\n", "padded_gt_batch", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max_len", ",", "gt_batch", "[", "0", "]", ".", "shape", "[", "1", "]", ",", "gt_batch", "[", "0", "]", ".", "shape", "[", "2", "]", ",", "gt_batch", "[", "0", "]", ".", "shape", "[", "3", "]", ")", "\n", "padded_audio_batch", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max_len", ",", "audio_batch", "[", "0", "]", ".", "shape", "[", "1", "]", ",", "audio_batch", "[", "0", "]", ".", "shape", "[", "2", "]", ",", "audio_batch", "[", "0", "]", ".", "shape", "[", "3", "]", ")", "\n", "if", "lip_batch", "is", "not", "None", ":", "\n", "        ", "padded_lip_batch", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "max_len", ",", "lip_batch", "[", "0", "]", ".", "shape", "[", "1", "]", ")", ",", "dtype", "=", "torch", ".", "int32", ")", "# (batch_size, max_len, 4)", "\n", "", "if", "label_batch", "is", "not", "None", ":", "\n", "        ", "label_batch", "=", "torch", ".", "stack", "(", "label_batch", ")", "# (batch_size,)", "\n", "\n", "", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "padded_img_batch", "[", "i", ",", ":", "img_batch", "[", "i", "]", ".", "shape", "[", "0", "]", ",", ":", ",", ":", ",", ":", "]", "=", "img_batch", "[", "i", "]", "\n", "padded_gt_batch", "[", "i", ",", ":", "gt_batch", "[", "i", "]", ".", "shape", "[", "0", "]", ",", ":", ",", ":", ",", ":", "]", "=", "gt_batch", "[", "i", "]", "\n", "padded_audio_batch", "[", "i", ",", ":", "audio_batch", "[", "i", "]", ".", "shape", "[", "0", "]", ",", ":", ",", ":", ",", ":", "]", "=", "audio_batch", "[", "i", "]", "\n", "if", "lip_batch", "is", "not", "None", ":", "\n", "            ", "padded_lip_batch", "[", "i", ",", ":", "lip_batch", "[", "i", "]", ".", "shape", "[", "0", "]", ",", ":", "]", "=", "lip_batch", "[", "i", "]", "\n", "\n", "", "", "if", "lip_batch", "is", "not", "None", "and", "label_batch", "is", "not", "None", ":", "\n", "        ", "return", "{", "'img'", ":", "padded_img_batch", ",", "'audio'", ":", "padded_audio_batch", ",", "'gt'", ":", "padded_gt_batch", ",", "'len'", ":", "torch", ".", "from_numpy", "(", "seq_len", ")", ",", "'label'", ":", "label_batch", ",", "'lip'", ":", "padded_lip_batch", "}", "\n", "", "elif", "lip_batch", "is", "not", "None", ":", "\n", "        ", "return", "{", "'img'", ":", "padded_img_batch", ",", "'audio'", ":", "padded_audio_batch", ",", "'gt'", ":", "padded_gt_batch", ",", "'len'", ":", "torch", ".", "from_numpy", "(", "seq_len", ")", ",", "'lip'", ":", "padded_lip_batch", "}", "\n", "", "elif", "label_batch", "is", "not", "None", ":", "\n", "        ", "return", "{", "'img'", ":", "padded_img_batch", ",", "'audio'", ":", "padded_audio_batch", ",", "'gt'", ":", "padded_gt_batch", ",", "'len'", ":", "torch", ".", "from_numpy", "(", "seq_len", ")", ",", "'label'", ":", "label_batch", "}", "\n", "", "else", ":", "\n", "        ", "return", "{", "'img'", ":", "padded_img_batch", ",", "'audio'", ":", "padded_audio_batch", ",", "'gt'", ":", "padded_gt_batch", ",", "'len'", ":", "torch", ".", "from_numpy", "(", "seq_len", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.BasicBlock.__init__": [[25, 30], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_planes", ",", "out_planes", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ":", "\n", "        ", "super", "(", "BasicBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "in_planes", ",", "out_planes", ",", "kernel_size", "=", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm2d", "(", "out_planes", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.BasicBlock.forward": [[31, 36], ["model_G_seq.BasicBlock.conv1", "model_G_seq.BasicBlock.bn1", "model_G_seq.BasicBlock.relu"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.ResnetBlock.__init__": [[39, 45], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d", "torch.InstanceNorm2d"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ")", ":", "\n", "        ", "super", "(", "ResnetBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "conv_block", "=", "[", "]", "\n", "conv_block", "+=", "[", "nn", ".", "ReflectionPad2d", "(", "1", ")", ",", "nn", ".", "Conv2d", "(", "dim", ",", "dim", ",", "kernel_size", "=", "3", ")", ",", "nn", ".", "InstanceNorm2d", "(", "dim", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "]", "\n", "conv_block", "+=", "[", "nn", ".", "ReflectionPad2d", "(", "1", ")", ",", "nn", ".", "Conv2d", "(", "dim", ",", "dim", ",", "kernel_size", "=", "3", ")", ",", "nn", ".", "InstanceNorm2d", "(", "dim", ")", "]", "\n", "self", ".", "conv_blocks", "=", "nn", ".", "Sequential", "(", "*", "conv_block", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.ResnetBlock.forward": [[46, 49], ["model_G_seq.ResnetBlock.conv_blocks"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "out", "=", "x", "+", "self", ".", "conv_blocks", "(", "x", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.AudioEncoder.__init__": [[52, 65], ["torch.Module.__init__", "model_G_seq.BasicBlock", "model_G_seq.BasicBlock", "model_G_seq.BasicBlock", "model_G_seq.BasicBlock", "model_G_seq.BasicBlock", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_output_length", ",", "if_tanh", "=", "False", ")", ":", "\n", "        ", "super", "(", "AudioEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "if_tanh", "=", "if_tanh", "\n", "# the input map is 1 x 12 x 35", "\n", "self", ".", "block1", "=", "BasicBlock", "(", "1", ",", "16", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", "# 16 x 12 x 35", "\n", "self", ".", "block2", "=", "BasicBlock", "(", "16", ",", "32", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "# 32 x 6 x 18", "\n", "self", ".", "block3", "=", "BasicBlock", "(", "32", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", "# 64 x 6 x 18", "\n", "self", ".", "block4", "=", "BasicBlock", "(", "64", ",", "128", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", "# 128 x 6 x 18", "\n", "self", ".", "block5", "=", "BasicBlock", "(", "128", ",", "256", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "# 256 x 3 x 9", "\n", "# self.fc1 = nn.Linear(6912, 512)", "\n", "# self.batch_norm = nn.BatchNorm2d(512)", "\n", "self", ".", "fc1", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "6912", ",", "512", ")", ",", "nn", ".", "BatchNorm1d", "(", "512", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "512", ",", "num_output_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.AudioEncoder.forward": [[66, 79], ["model_G_seq.AudioEncoder.block1", "model_G_seq.AudioEncoder.block2", "model_G_seq.AudioEncoder.block3", "model_G_seq.AudioEncoder.block4", "model_G_seq.AudioEncoder.block5", "torch.tanh.contiguous().view", "model_G_seq.AudioEncoder.fc1", "model_G_seq.AudioEncoder.fc2", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh.contiguous"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "out", "=", "self", ".", "block1", "(", "inputs", ")", "\n", "out", "=", "self", ".", "block2", "(", "out", ")", "\n", "out", "=", "self", ".", "block3", "(", "out", ")", "\n", "out", "=", "self", ".", "block4", "(", "out", ")", "\n", "out", "=", "self", ".", "block5", "(", "out", ")", "\n", "out", "=", "out", ".", "contiguous", "(", ")", ".", "view", "(", "out", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "# out = F.relu(self.batch_norm(self.fc1(out)))", "\n", "out", "=", "self", ".", "fc1", "(", "out", ")", "\n", "out", "=", "self", ".", "fc2", "(", "out", ")", "\n", "if", "self", ".", "if_tanh", ":", "\n", "          ", "out", "=", "F", ".", "tanh", "(", "out", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.AudioEncoderBMVC.__init__": [[81, 91], ["torch.Module.__init__", "model_G_seq.BasicBlock", "model_G_seq.BasicBlock", "model_G_seq.BasicBlock", "model_G_seq.BasicBlock", "model_G_seq.BasicBlock", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_output_length", ",", "if_tanh", "=", "False", ")", ":", "\n", "        ", "super", "(", "AudioEncoderBMVC", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "if_tanh", "=", "if_tanh", "\n", "self", ".", "block1", "=", "BasicBlock", "(", "1", ",", "32", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", "# 32 x 12 x 35", "\n", "self", ".", "block2", "=", "BasicBlock", "(", "32", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "[", "1", ",", "2", "]", ")", "# 64 x 12 x 18", "\n", "self", ".", "block3", "=", "BasicBlock", "(", "64", ",", "128", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", "# 128 x 12 x 18", "\n", "self", ".", "block4", "=", "BasicBlock", "(", "128", ",", "128", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", "# 128 x 12 x 18", "\n", "self", ".", "block5", "=", "BasicBlock", "(", "128", ",", "256", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "# 256 x 6 x 9", "\n", "self", ".", "fc1", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "13824", ",", "1024", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "1024", ",", "num_output_length", ")", "\n", "", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.AudioEncoderBMVC.forward": [[91, 104], ["model_G_seq.AudioEncoderBMVC.block1", "model_G_seq.AudioEncoderBMVC.block2", "model_G_seq.AudioEncoderBMVC.block3", "model_G_seq.AudioEncoderBMVC.block4", "model_G_seq.AudioEncoderBMVC.block5", "torch.tanh.contiguous().view", "model_G_seq.AudioEncoderBMVC.fc1", "model_G_seq.AudioEncoderBMVC.fc2", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh.contiguous"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "out", "=", "self", ".", "block1", "(", "inputs", ")", "\n", "out", "=", "self", ".", "block2", "(", "out", ")", "\n", "out", "=", "self", ".", "block3", "(", "out", ")", "\n", "out", "=", "self", ".", "block4", "(", "out", ")", "\n", "out", "=", "self", ".", "block5", "(", "out", ")", "\n", "out", "=", "out", ".", "contiguous", "(", ")", ".", "view", "(", "out", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "# out = F.relu(self.batch_norm(self.fc1(out)))", "\n", "out", "=", "self", ".", "fc1", "(", "out", ")", "\n", "out", "=", "self", ".", "fc2", "(", "out", ")", "\n", "if", "self", ".", "if_tanh", ":", "\n", "          ", "out", "=", "F", ".", "tanh", "(", "out", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.ImageEncoder.__init__": [[107, 116], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "model_G_seq.ImageEncoder.get_size", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.ImageDecoder.get_size"], ["    ", "def", "__init__", "(", "self", ",", "size_image", ",", "num_output_length", ",", "if_tanh", "=", "False", ")", ":", "\n", "        ", "super", "(", "ImageEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "if_tanh", "=", "if_tanh", "\n", "self", ".", "conv1", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "3", ",", "16", ",", "5", ",", "stride", "=", "2", ",", "padding", "=", "2", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "16", ",", "32", ",", "5", ",", "stride", "=", "2", ",", "padding", "=", "2", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "conv3", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "5", ",", "stride", "=", "2", ",", "padding", "=", "2", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "conv4", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "64", ",", "128", ",", "5", ",", "stride", "=", "2", ",", "padding", "=", "2", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "size_mini_map", "=", "self", ".", "get_size", "(", "size_image", ",", "4", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "size_mini_map", "*", "size_mini_map", "*", "128", ",", "num_output_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.ImageEncoder.get_size": [[117, 119], ["int"], "methods", ["None"], ["", "def", "get_size", "(", "self", ",", "size_image", ",", "num_layers", ")", ":", "\n", "        ", "return", "int", "(", "size_image", "/", "2", "**", "num_layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.ImageEncoder.forward": [[120, 130], ["model_G_seq.ImageEncoder.conv1", "model_G_seq.ImageEncoder.conv2", "model_G_seq.ImageEncoder.conv3", "model_G_seq.ImageEncoder.conv4", "model_G_seq.ImageEncoder.contiguous().view", "model_G_seq.ImageEncoder.fc", "torch.tanh", "torch.tanh", "torch.tanh", "model_G_seq.ImageEncoder.contiguous"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "img_e_conv1", "=", "self", ".", "conv1", "(", "inputs", ")", "\n", "img_e_conv2", "=", "self", ".", "conv2", "(", "img_e_conv1", ")", "\n", "img_e_conv3", "=", "self", ".", "conv3", "(", "img_e_conv2", ")", "\n", "img_e_conv4", "=", "self", ".", "conv4", "(", "img_e_conv3", ")", "\n", "img_e_fc_5", "=", "img_e_conv4", ".", "contiguous", "(", ")", ".", "view", "(", "img_e_conv4", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "img_e_fc_5", "=", "self", ".", "fc", "(", "img_e_fc_5", ")", "\n", "if", "self", ".", "if_tanh", ":", "\n", "            ", "img_e_fc_5", "=", "F", ".", "tanh", "(", "img_e_fc_5", ")", "\n", "", "return", "img_e_fc_5", ",", "img_e_conv1", ",", "img_e_conv2", ",", "img_e_conv3", ",", "img_e_conv4", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.ImageEncoderFCN.__init__": [[134, 141], ["torch.Module.__init__", "model_G_seq.BasicBlock", "model_G_seq.BasicBlock", "model_G_seq.BasicBlock", "model_G_seq.BasicBlock"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "size_image", ",", "num_output_length", ",", "if_tanh", "=", "False", ")", ":", "\n", "        ", "super", "(", "ImageEncoderFCN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "if_tanh", "=", "if_tanh", "\n", "self", ".", "conv1", "=", "BasicBlock", "(", "3", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "# nn.Sequential(nn.Conv2d(3, 64, 3, stride=2, padding=1), nn.ReLU(inplace=True))", "\n", "self", ".", "conv2", "=", "BasicBlock", "(", "64", ",", "128", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "# nn.Sequential(nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU(inplace=True))", "\n", "self", ".", "conv3", "=", "BasicBlock", "(", "128", ",", "256", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "# nn.Sequential(nn.Conv2d(128, 256, 3, stride=2, padding=1), nn.ReLU(inplace=True))", "\n", "self", ".", "conv4", "=", "BasicBlock", "(", "256", ",", "512", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "# nn.Sequential(nn.Conv2d(256, 512, 3, stride=2, padding=1), nn.ReLU(inplace=True))", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.ImageEncoderFCN.forward": [[142, 150], ["model_G_seq.ImageEncoderFCN.conv1", "model_G_seq.ImageEncoderFCN.conv2", "model_G_seq.ImageEncoderFCN.conv3", "model_G_seq.ImageEncoderFCN.conv4", "torch.tanh", "torch.tanh", "torch.tanh"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "img_e_conv1", "=", "self", ".", "conv1", "(", "inputs", ")", "# /2", "\n", "img_e_conv2", "=", "self", ".", "conv2", "(", "img_e_conv1", ")", "# /4", "\n", "img_e_conv3", "=", "self", ".", "conv3", "(", "img_e_conv2", ")", "# /8", "\n", "img_e_conv4", "=", "self", ".", "conv4", "(", "img_e_conv3", ")", "# /16", "\n", "if", "self", ".", "if_tanh", ":", "\n", "            ", "img_e_conv4", "=", "F", ".", "tanh", "(", "img_e_conv4", ")", "\n", "", "return", "img_e_conv4", ",", "img_e_conv1", ",", "img_e_conv2", ",", "img_e_conv3", ",", "img_e_conv4", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.ImageDecoder.__init__": [[154, 164], ["torch.Module.__init__", "model_G_seq.ImageDecoder.get_size", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.ImageDecoder.get_size"], ["    ", "def", "__init__", "(", "self", ",", "size_image", ",", "input_dim", ")", ":", "\n", "        ", "super", "(", "ImageDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "size_mini_map", "=", "self", ".", "get_size", "(", "size_image", ",", "4", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "input_dim", ",", "self", ".", "size_mini_map", "*", "self", ".", "size_mini_map", "*", "256", ")", "\n", "self", ".", "dconv1", "=", "nn", ".", "Sequential", "(", "nn", ".", "ConvTranspose2d", "(", "384", ",", "196", ",", "5", ",", "stride", "=", "2", ",", "padding", "=", "2", ",", "output_padding", "=", "1", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "dconv2", "=", "nn", ".", "Sequential", "(", "nn", ".", "ConvTranspose2d", "(", "260", ",", "128", ",", "5", ",", "stride", "=", "2", ",", "padding", "=", "2", ",", "output_padding", "=", "1", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "dconv3", "=", "nn", ".", "Sequential", "(", "nn", ".", "ConvTranspose2d", "(", "160", ",", "80", ",", "5", ",", "stride", "=", "2", ",", "padding", "=", "2", ",", "output_padding", "=", "1", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "dconv4", "=", "nn", ".", "Sequential", "(", "nn", ".", "ConvTranspose2d", "(", "96", ",", "48", ",", "5", ",", "stride", "=", "2", ",", "padding", "=", "2", ",", "output_padding", "=", "1", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "dconv5", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "48", ",", "16", ",", "5", ",", "stride", "=", "1", ",", "padding", "=", "2", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "dconv6", "=", "nn", ".", "Conv2d", "(", "16", ",", "3", ",", "5", ",", "stride", "=", "1", ",", "padding", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.ImageDecoder.get_size": [[165, 167], ["int"], "methods", ["None"], ["", "def", "get_size", "(", "self", ",", "size_image", ",", "num_layers", ")", ":", "\n", "        ", "return", "int", "(", "size_image", "/", "2", "**", "num_layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.ImageDecoder.forward": [[168, 189], ["model_G_seq.ImageDecoder.fc", "model_G_seq.ImageDecoder.contiguous().view", "torch.relu", "torch.relu", "torch.relu", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_G_seq.ImageDecoder.dconv1", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_G_seq.ImageDecoder.dconv2", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_G_seq.ImageDecoder.dconv3", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_G_seq.ImageDecoder.dconv4", "model_G_seq.ImageDecoder.dconv5", "model_G_seq.ImageDecoder.dconv6", "torch.tanh", "torch.tanh", "torch.tanh", "model_G_seq.ImageDecoder.contiguous"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "concat_z", ",", "img_e_conv1", ",", "img_e_conv2", ",", "img_e_conv3", ",", "img_e_conv4", ")", ":", "\n", "# out = torch.cat([img_z, audio_z], dim=1) # (batch_size, input_dim)", "\n", "        ", "out", "=", "self", ".", "fc", "(", "concat_z", ")", "\n", "# reshape 256 x 7 x 7", "\n", "out", "=", "out", ".", "contiguous", "(", ")", ".", "view", "(", "out", ".", "shape", "[", "0", "]", ",", "256", ",", "self", ".", "size_mini_map", ",", "self", ".", "size_mini_map", ")", "\n", "out", "=", "F", ".", "relu", "(", "out", ",", "inplace", "=", "True", ")", "\n", "# concate (256+128) x 7x7", "\n", "out", "=", "torch", ".", "cat", "(", "[", "out", ",", "img_e_conv4", "]", ",", "dim", "=", "1", ")", "\n", "out", "=", "self", ".", "dconv1", "(", "out", ")", "\n", "# concate (196+64) x 14x14", "\n", "out", "=", "torch", ".", "cat", "(", "[", "out", ",", "img_e_conv3", "]", ",", "dim", "=", "1", ")", "\n", "out", "=", "self", ".", "dconv2", "(", "out", ")", "\n", "# concate (128+32) x 28x28", "\n", "out", "=", "torch", ".", "cat", "(", "[", "out", ",", "img_e_conv2", "]", ",", "dim", "=", "1", ")", "\n", "out", "=", "self", ".", "dconv3", "(", "out", ")", "\n", "# concate (80+16) x 56x56", "\n", "out", "=", "torch", ".", "cat", "(", "[", "out", ",", "img_e_conv1", "]", ",", "dim", "=", "1", ")", "\n", "out", "=", "self", ".", "dconv4", "(", "out", ")", "\n", "out", "=", "self", ".", "dconv5", "(", "out", ")", "\n", "out", "=", "self", ".", "dconv6", "(", "out", ")", "\n", "return", "F", ".", "tanh", "(", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.ImageDecoderResidual.__init__": [[192, 204], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model_G_seq.ResnetBlock", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "size_image", ",", "input_dim", ")", ":", "\n", "        ", "super", "(", "ImageDecoderResidual", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fuse", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "input_dim", ",", "512", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ",", "nn", ".", "ReLU", "(", "True", ")", ")", "\n", "self", ".", "resblocks", "=", "nn", ".", "Sequential", "(", "ResnetBlock", "(", "512", ")", ")", "#, ResnetBlock(512), ResnetBlock(512))", "\n", "self", ".", "dconv1", "=", "nn", ".", "Sequential", "(", "nn", ".", "ConvTranspose2d", "(", "512", ",", "256", ",", "4", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "nn", ".", "BatchNorm2d", "(", "256", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "512", ",", "256", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "self", ".", "dconv2", "=", "nn", ".", "Sequential", "(", "nn", ".", "ConvTranspose2d", "(", "256", ",", "128", ",", "4", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "nn", ".", "BatchNorm2d", "(", "128", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "256", ",", "128", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "self", ".", "dconv3", "=", "nn", ".", "Sequential", "(", "nn", ".", "ConvTranspose2d", "(", "128", ",", "64", ",", "4", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "nn", ".", "BatchNorm2d", "(", "64", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "128", ",", "64", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "self", ".", "dconv4", "=", "nn", ".", "Sequential", "(", "nn", ".", "ConvTranspose2d", "(", "64", ",", "32", ",", "4", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "nn", ".", "BatchNorm2d", "(", "32", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "conv4", "=", "nn", ".", "Conv2d", "(", "32", ",", "3", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.ImageDecoderResidual.forward": [[205, 223], ["model_G_seq.ImageDecoderResidual.fuse", "model_G_seq.ImageDecoderResidual.resblocks", "model_G_seq.ImageDecoderResidual.dconv1", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_G_seq.ImageDecoderResidual.conv1", "model_G_seq.ImageDecoderResidual.dconv2", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_G_seq.ImageDecoderResidual.conv2", "model_G_seq.ImageDecoderResidual.dconv3", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_G_seq.ImageDecoderResidual.conv3", "model_G_seq.ImageDecoderResidual.dconv4", "model_G_seq.ImageDecoderResidual.conv4", "torch.tanh", "torch.tanh", "torch.tanh"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "concat_z", ",", "img_e_conv1", ",", "img_e_conv2", ",", "img_e_conv3", ",", "img_e_conv4", ")", ":", "\n", "# audio_z = audio_z.unsqueeze(-1).unsqueeze(-1)", "\n", "# audio_z = audio_z.repeat(1,1, img_z.shape[2], img_z.shape[3])", "\n", "# z = torch.cat([img_z, audio_z], dim=1)", "\n", "        ", "z", "=", "self", ".", "fuse", "(", "concat_z", ")", "\n", "z", "=", "self", ".", "resblocks", "(", "z", ")", "\n", "out", "=", "self", ".", "dconv1", "(", "z", ")", "\n", "out", "=", "torch", ".", "cat", "(", "[", "out", ",", "img_e_conv3", "]", ",", "dim", "=", "1", ")", "\n", "out", "=", "self", ".", "conv1", "(", "out", ")", "\n", "out", "=", "self", ".", "dconv2", "(", "out", ")", "\n", "out", "=", "torch", ".", "cat", "(", "[", "out", ",", "img_e_conv2", "]", ",", "dim", "=", "1", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "dconv3", "(", "out", ")", "\n", "out", "=", "torch", ".", "cat", "(", "[", "out", ",", "img_e_conv1", "]", ",", "dim", "=", "1", ")", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "out", "=", "self", ".", "dconv4", "(", "out", ")", "\n", "out", "=", "self", ".", "conv4", "(", "out", ")", "\n", "return", "F", ".", "tanh", "(", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorCNN.__init__": [[226, 249], ["torch.Module.__init__", "model_G_seq.LipGeneratorCNN.audio_encoder.apply", "model_G_seq.LipGeneratorCNN.image_encoder.apply", "model_G_seq.LipGeneratorCNN.image_decoder.apply", "model_G_seq.AudioEncoder", "model_G_seq.ImageEncoder", "model_G_seq.ImageDecoder", "model_G_seq.AudioEncoderBMVC", "model_G_seq.ImageEncoderFCN", "model_G_seq.ImageDecoderResidual"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "audio_encoder_type", ",", "img_encoder_type", ",", "img_decoder_type", ",", "size_image", ",", "num_output_length", ",", "if_tanh", ")", ":", "\n", "        ", "super", "(", "LipGeneratorCNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "audio_encoder_type", "==", "'reduce'", ":", "\n", "            ", "self", ".", "audio_encoder", "=", "AudioEncoder", "(", "num_output_length", ",", "if_tanh", ")", "\n", "", "elif", "audio_encoder_type", "==", "'bmvc'", ":", "\n", "            ", "self", ".", "audio_encoder", "=", "AudioEncoderBMVC", "(", "num_output_length", ",", "if_tanh", ")", "\n", "", "if", "img_encoder_type", "==", "'reduce'", ":", "\n", "            ", "self", ".", "image_encoder", "=", "ImageEncoder", "(", "size_image", ",", "num_output_length", ",", "if_tanh", ")", "\n", "", "elif", "img_encoder_type", "==", "'FCN'", ":", "\n", "            ", "self", ".", "image_encoder", "=", "ImageEncoderFCN", "(", "size_image", ",", "num_output_length", ",", "if_tanh", ")", "\n", "", "if", "img_decoder_type", "==", "'reduce'", ":", "\n", "            ", "self", ".", "image_decoder", "=", "ImageDecoder", "(", "size_image", ",", "2", "*", "num_output_length", ")", "\n", "", "elif", "img_decoder_type", "==", "'residual'", ":", "\n", "            ", "self", ".", "image_decoder", "=", "ImageDecoderResidual", "(", "size_image", ",", "2", "*", "num_output_length", ")", "\n", "\n", "", "self", ".", "audio_encoder_type", "=", "audio_encoder_type", "\n", "self", ".", "img_encoder_type", "=", "img_encoder_type", "\n", "self", ".", "img_decoder_type", "=", "img_decoder_type", "\n", "\n", "# initialize weights", "\n", "self", ".", "audio_encoder", ".", "apply", "(", "weights_init", ")", "\n", "self", ".", "image_encoder", ".", "apply", "(", "weights_init", ")", "\n", "self", ".", "image_decoder", ".", "apply", "(", "weights_init", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorCNN.forward": [[250, 261], ["model_G_seq.LipGeneratorCNN.audio_encoder", "model_G_seq.LipGeneratorCNN.image_encoder", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_G_seq.LipGeneratorCNN.image_decoder", "audio_z.repeat.repeat.unsqueeze().unsqueeze", "audio_z.repeat.repeat.repeat", "audio_z.repeat.repeat.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "image_inputs", ",", "audio_inputs", ")", ":", "\n", "        ", "audio_z", "=", "self", ".", "audio_encoder", "(", "audio_inputs", ")", "\n", "image_z", ",", "img_e_conv1", ",", "img_e_conv2", ",", "img_e_conv3", ",", "img_e_conv4", "=", "self", ".", "image_encoder", "(", "image_inputs", ")", "\n", "\n", "if", "self", ".", "img_encoder_type", "==", "'FCN'", ":", "\n", "            ", "audio_z", "=", "audio_z", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "audio_z", "=", "audio_z", ".", "repeat", "(", "1", ",", "1", ",", "image_z", ".", "shape", "[", "2", "]", ",", "image_z", ".", "shape", "[", "3", "]", ")", "\n", "", "concat_z", "=", "torch", ".", "cat", "(", "[", "image_z", ",", "audio_z", "]", ",", "dim", "=", "1", ")", "\n", "\n", "G", "=", "self", ".", "image_decoder", "(", "concat_z", ",", "img_e_conv1", ",", "img_e_conv2", ",", "img_e_conv3", ",", "img_e_conv4", ")", "\n", "return", "G", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorCNN.model_type": [[262, 264], ["None"], "methods", ["None"], ["", "def", "model_type", "(", "self", ")", ":", "\n", "        ", "return", "'CNN'", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.RNNModel.__init__": [[269, 276], ["torch.Module.__init__", "torch.GRU", "torch.GRU", "torch.GRU"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_size", ",", "hidden_size", ",", "rnn_type", ",", "num_layers", "=", "1", ")", ":", "\n", "        ", "super", "(", "RNNModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "rnn_type", "=", "rnn_type", "\n", "self", ".", "nhid", "=", "hidden_size", "\n", "self", ".", "nlayers", "=", "num_layers", "\n", "if", "rnn_type", "==", "'GRU'", ":", "\n", "            ", "self", ".", "rnn", "=", "nn", ".", "GRU", "(", "input_size", ",", "hidden_size", ",", "num_layers", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.RNNModel.forward": [[277, 281], ["model_G_seq.RNNModel.rnn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "inputs", ",", "hidden", ")", ":", "\n", "\n", "        ", "output", ",", "hidden", "=", "self", ".", "rnn", "(", "inputs", ",", "hidden", ")", "\n", "return", "output", ",", "hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.RNNModel.init_hidden": [[282, 289], ["next", "model_G_seq.RNNModel.parameters", "next.new_zeros", "next.new_zeros", "next.new_zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "weight", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", "\n", "if", "self", ".", "rnn_type", "==", "'LSTM'", ":", "\n", "            ", "return", "(", "weight", ".", "new_zeros", "(", "self", ".", "nlayers", ",", "batch_size", ",", "self", ".", "nhid", ")", ",", "\n", "weight", ".", "new_zeros", "(", "self", ".", "nlayers", ",", "batch_size", ",", "self", ".", "nhid", ")", ")", "\n", "", "else", ":", "\n", "            ", "return", "weight", ".", "new_zeros", "(", "self", ".", "nlayers", ",", "batch_size", ",", "self", ".", "nhid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__": [[292, 319], ["torch.Module.__init__", "model_G_seq.LipGeneratorRNN.audio_encoder.apply", "model_G_seq.LipGeneratorRNN.image_encoder.apply", "model_G_seq.LipGeneratorRNN.image_decoder.apply", "model_G_seq.LipGeneratorRNN.rnn.apply", "model_G_seq.AudioEncoder", "model_G_seq.ImageEncoder", "model_G_seq.ImageDecoder", "model_G_seq.RNNModel", "model_G_seq.AudioEncoderBMVC", "model_G_seq.ImageEncoderFCN", "model_G_seq.ImageDecoderResidual"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "audio_encoder_type", ",", "img_encoder_type", ",", "img_decoder_type", ",", "rnn_type", ",", "size_image", ",", "num_output_length", ",", "hidden_size", "=", "1024", ",", "if_tanh", "=", "False", ")", ":", "\n", "        ", "super", "(", "LipGeneratorRNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "audio_encoder_type", "==", "'reduce'", ":", "\n", "            ", "self", ".", "audio_encoder", "=", "AudioEncoder", "(", "num_output_length", ",", "if_tanh", ")", "\n", "", "elif", "audio_encoder_type", "==", "'bmvc'", ":", "\n", "            ", "self", ".", "audio_encoder", "=", "AudioEncoderBMVC", "(", "num_output_length", ",", "if_tanh", ")", "\n", "", "if", "img_encoder_type", "==", "'reduce'", ":", "\n", "            ", "self", ".", "image_encoder", "=", "ImageEncoder", "(", "size_image", ",", "num_output_length", ",", "if_tanh", ")", "\n", "", "elif", "img_encoder_type", "==", "'FCN'", ":", "\n", "            ", "self", ".", "image_encoder", "=", "ImageEncoderFCN", "(", "size_image", ",", "num_output_length", ",", "if_tanh", ")", "\n", "", "if", "img_decoder_type", "==", "'reduce'", ":", "\n", "            ", "self", ".", "image_decoder", "=", "ImageDecoder", "(", "size_image", ",", "hidden_size", ")", "\n", "", "elif", "img_decoder_type", "==", "'residual'", ":", "\n", "            ", "self", ".", "image_decoder", "=", "ImageDecoderResidual", "(", "size_image", ",", "hidden_size", ")", "\n", "", "if", "rnn_type", "==", "'GRU'", ":", "\n", "            ", "self", ".", "rnn", "=", "RNNModel", "(", "2", "*", "num_output_length", ",", "hidden_size", ",", "rnn_type", ")", "\n", "\n", "\n", "", "self", ".", "audio_encoder_type", "=", "audio_encoder_type", "\n", "self", ".", "img_encoder_type", "=", "img_encoder_type", "\n", "self", ".", "img_decoder_type", "=", "img_decoder_type", "\n", "\n", "# initialize weights", "\n", "self", ".", "audio_encoder", ".", "apply", "(", "weights_init", ")", "\n", "self", ".", "image_encoder", ".", "apply", "(", "weights_init", ")", "\n", "self", ".", "image_decoder", ".", "apply", "(", "weights_init", ")", "\n", "self", ".", "rnn", ".", "apply", "(", "weights_init", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.forward": [[321, 417], ["valid_len.min", "model_G_seq.LipGeneratorRNN.rnn.init_hidden", "range", "model_G_seq.LipGeneratorRNN.audio_encoder", "model_G_seq.LipGeneratorRNN.image_encoder", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "concat_z.unsqueeze.unsqueeze.unsqueeze", "model_G_seq.LipGeneratorRNN.rnn", "rnn_output.squeeze.squeeze.squeeze", "model_G_seq.LipGeneratorRNN.image_decoder", "model_G_seq.LipGeneratorRNN.detach", "audio_z.repeat.repeat.unsqueeze().unsqueeze", "audio_z.repeat.repeat.repeat", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "random.random", "audio_z.repeat.repeat.unsqueeze", "model_G_seq.LipGeneratorRNN.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.RNNModel.init_hidden"], ["", "def", "forward", "(", "self", ",", "image_inputs", ",", "audio_inputs", ",", "valid_len", ",", "teacher_forcing_ratio", "=", "0.5", ")", ":", "\n", "# reshape inputs to (seq_len*batch_size, ...)", "\n", "        ", "seq_len", "=", "image_inputs", ".", "shape", "[", "1", "]", "\n", "batch_size", "=", "image_inputs", ".", "shape", "[", "0", "]", "\n", "\n", "# G = image_inputs.new_zeros(batch_size, seq_len, image_inputs.shape[2], image_inputs.shape[3], image_inputs.shape[4])", "\n", "\n", "valid_l", "=", "valid_len", ".", "min", "(", ")", "\n", "hidden", "=", "self", ".", "rnn", ".", "init_hidden", "(", "batch_size", ")", "\n", "for", "i", "in", "range", "(", "valid_l", ")", ":", "\n", "            ", "image_batch", "=", "image_inputs", "[", ":", ",", "i", ",", ":", ",", ":", ",", ":", "]", "# (batch_size, c, h, w)", "\n", "audio_batch", "=", "audio_inputs", "[", ":", ",", "i", ",", ":", ",", ":", ",", ":", "]", "\n", "\n", "use_teacher_forcing", "=", "True", "if", "random", ".", "random", "(", ")", "<", "teacher_forcing_ratio", "else", "False", "\n", "if", "i", ">", "0", "and", "use_teacher_forcing", "==", "False", ":", "\n", "                ", "image_batch", "=", "img_output", ".", "detach", "(", ")", "\n", "# image_batch = G[:,i-1,:,:,:].detach()", "\n", "\n", "# encoder", "\n", "", "audio_z", "=", "self", ".", "audio_encoder", "(", "audio_batch", ")", "\n", "image_z", ",", "img_e_conv1", ",", "img_e_conv2", ",", "img_e_conv3", ",", "img_e_conv4", "=", "self", ".", "image_encoder", "(", "image_batch", ")", "\n", "# concatenate", "\n", "if", "self", ".", "img_encoder_type", "==", "'FCN'", ":", "\n", "                ", "audio_z", "=", "audio_z", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "audio_z", "=", "audio_z", ".", "repeat", "(", "1", ",", "1", ",", "image_z", ".", "shape", "[", "2", "]", ",", "image_z", ".", "shape", "[", "3", "]", ")", "\n", "", "concat_z", "=", "torch", ".", "cat", "(", "[", "image_z", ",", "audio_z", "]", ",", "dim", "=", "1", ")", "#(batch_size, c, h, w)", "\n", "\n", "# rnn", "\n", "concat_z", "=", "concat_z", ".", "unsqueeze", "(", "0", ")", "#(1, batch_size, c, h, w)", "\n", "rnn_output", ",", "hidden", "=", "self", ".", "rnn", "(", "concat_z", ",", "hidden", ")", "\n", "\n", "# decoder", "\n", "rnn_output", "=", "rnn_output", ".", "squeeze", "(", "0", ")", "#(batch_size, c, h, w)", "\n", "img_output", "=", "self", ".", "image_decoder", "(", "rnn_output", ",", "img_e_conv1", ",", "img_e_conv2", ",", "img_e_conv3", ",", "img_e_conv4", ")", "\n", "\n", "if", "i", "==", "0", ":", "\n", "                ", "G", "=", "img_output", "\n", "G", "=", "G", ".", "unsqueeze", "(", "1", ")", "\n", "", "else", ":", "\n", "\n", "                ", "G", "=", "torch", ".", "cat", "(", "[", "G", ",", "img_output", ".", "unsqueeze", "(", "1", ")", "]", ",", "dim", "=", "1", ")", "\n", "# G[:,i,:,:,:] = img_output", "\n", "\n", "\n", "\n", "# for batch_id in range(batch_size):", "\n", "#     valid_l = valid_len[batch_id]", "\n", "#     image_seq = image_inputs[batch_id] # (seq_len, c, h, w)", "\n", "#     audio_seq = audio_inputs[batch_id] # (seq_len, ...)", "\n", "#     # encoder z", "\n", "#     audio_seq_z = self.audio_encoder(audio_seq)", "\n", "\n", "#     # fed concat_z to RNN", "\n", "#     hidden = self.rnn.init_hidden(batch_size=1)", "\n", "#     use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False", "\n", "\n", "#     if use_teacher_forcing: # Teacher forcing: Feed the target as the next input", "\n", "#         image_z,  img_e_conv1, img_e_conv2, img_e_conv3, img_e_conv4 = self.image_encoder(image_seq)", "\n", "#         audio_z = audio_seq_z", "\n", "#         if self.img_encoder_type=='FCN':", "\n", "#             audio_z = audio_z.unsqueeze(-1).unsqueeze(-1)", "\n", "#             audio_z = audio_z.repeat(1,1, image_z.shape[2], image_z.shape[3])", "\n", "#         concat_z = torch.cat([image_z, audio_z], dim=1)", "\n", "\n", "#         # rnn", "\n", "#         concat_z = concat_z.unsqueeze(1)", "\n", "#         rnn_output, _ = self.rnn(concat_z, hidden)", "\n", "\n", "#         # decoder", "\n", "#         rnn_output = rnn_output.squeeze(1)", "\n", "#         img_output = self.image_decoder(rnn_output, img_e_conv1, img_e_conv2, img_e_conv3, img_e_conv4)", "\n", "#         G[batch_id] = img_output", "\n", "\n", "#     else: # Without teacher forcing: use its own predictions as the next input", "\n", "#         image_input = image_seq[0].unsqueeze(0) # (1, c, h, w)", "\n", "#         for i in range(valid_l):", "\n", "#             image_z, img_e_conv1, img_e_conv2, img_e_conv3, img_e_conv4 = self.image_encoder(image_input)", "\n", "#             audio_z = audio_seq_z[i].unsqueeze(0) # (1, ...)", "\n", "\n", "#             if self.img_encoder_type=='FCN':", "\n", "#                 audio_z = audio_z.unsqueeze(-1).unsqueeze(-1)", "\n", "#                 audio_z = audio_z.repeat(1,1, image_z.shape[2], image_z.shape[3])", "\n", "#             concat_z = torch.cat([image_z, audio_z], dim=1)", "\n", "\n", "#             # rnn", "\n", "#             concat_z = concat_z.unsqueeze(1)", "\n", "#             rnn_output, hidden = self.rnn(concat_z, hidden)", "\n", "\n", "#             # decoder", "\n", "#             rnn_output = rnn_output.squeeze(1)", "\n", "#             img_output = self.image_decoder(rnn_output, img_e_conv1, img_e_conv2, img_e_conv3, img_e_conv4)", "\n", "\n", "#             image_input = img_output.detach()", "\n", "#             G[batch_id, i] = img_output", "\n", "\n", "", "", "return", "G", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.LipGeneratorRNN.model_type": [[418, 420], ["None"], "methods", ["None"], ["", "def", "model_type", "(", "self", ")", ":", "\n", "        ", "return", "'RNN'", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.None.model_G_seq.weights_init": [[7, 22], ["hasattr", "m.weight.data.normal_", "classname.find", "classname.find", "m.weight.data.normal_", "m.bias.data.fill_", "type", "torch.nn.init.xavier_uniform", "torch.nn.init.xavier_uniform", "torch.nn.init.xavier_uniform", "m.bias.data.fill_", "print"], "function", ["None"], ["def", "weights_init", "(", "m", ")", ":", "\n", "    ", "classname", "=", "m", ".", "__class__", ".", "__name__", "\n", "if", "classname", ".", "find", "(", "'Conv'", ")", "!=", "-", "1", "and", "hasattr", "(", "m", ",", "'weight'", ")", ":", "\n", "        ", "m", ".", "weight", ".", "data", ".", "normal_", "(", "0.0", ",", "0.02", ")", "\n", "", "elif", "classname", ".", "find", "(", "'BatchNorm'", ")", "!=", "-", "1", ":", "\n", "        ", "m", ".", "weight", ".", "data", ".", "normal_", "(", "1.0", ",", "0.02", ")", "\n", "m", ".", "bias", ".", "data", ".", "fill_", "(", "0", ")", "\n", "", "elif", "type", "(", "m", ")", "==", "nn", ".", "Linear", ":", "\n", "        ", "torch", ".", "nn", ".", "init", ".", "xavier_uniform", "(", "m", ".", "weight", ")", "\n", "m", ".", "bias", ".", "data", ".", "fill_", "(", "0.01", ")", "\n", "# elif classname.find('GRU') != -1 or classname.find('LSTM') != -1:", "\n", "#     m.weight.data.normal_(0.0, 0.02)", "\n", "#     m.bias.data.fill_(0.01)", "\n", "", "else", ":", "\n", "        ", "print", "(", "classname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.data.load_image": [[8, 15], ["scipy.misc.imread().astype", "scipy.misc.imresize", "scipy.misc.imread"], "function", ["None"], ["def", "load_image", "(", "filename", ",", "image_size", "=", "112", ")", ":", "\n", "    ", "image", "=", "imread", "(", "filename", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "# resize", "\n", "image", "=", "imresize", "(", "image", ",", "[", "image_size", ",", "image_size", "]", ")", "\n", "# normalize", "\n", "image", "=", "(", "image", "*", "2.0", "/", "255.0", "-", "1.0", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.data.load_audio": [[17, 23], ["scipy.io.loadmat", "numpy.expand_dims", "numpy.transpose"], "function", ["None"], ["", "def", "load_audio", "(", "audio_path", ")", ":", "\n", "# load the mat file", "\n", "    ", "file", "=", "loadmat", "(", "audio_path", ")", "\n", "audio", "=", "np", ".", "transpose", "(", "file", "[", "'mfcc'", "]", ",", "(", "1", ",", "0", ")", ")", "[", "1", ":", ",", ":", "]", "\n", "audio", "=", "np", ".", "expand_dims", "(", "audio", ",", "axis", "=", "-", "1", ")", "\n", "return", "audio", "", "", ""]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.utils.create_dir": [[8, 11], ["os.path.exists", "os.makedirs"], "function", ["None"], ["def", "create_dir", "(", "dir", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.utils.listdir_nohidden": [[12, 14], ["glob.glob", "os.path.join"], "function", ["None"], ["", "", "def", "listdir_nohidden", "(", "path", ")", ":", "\n", "    ", "return", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'*'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.utils.save_sample_images": [[16, 29], ["int", "numpy.zeros", "enumerate", "scipy.misc.imsave", "numpy.sqrt", "numpy.transpose", "len"], "function", ["None"], ["", "def", "save_sample_images", "(", "images", ",", "save_path", ")", ":", "\n", "    ", "size_frame", "=", "int", "(", "np", ".", "sqrt", "(", "len", "(", "images", ")", ")", ")", "\n", "img_h", ",", "img_w", "=", "images", ".", "shape", "[", "2", "]", ",", "images", ".", "shape", "[", "3", "]", "\n", "frame", "=", "np", ".", "zeros", "(", "[", "img_h", "*", "size_frame", ",", "img_w", "*", "size_frame", ",", "3", "]", ")", "\n", "\n", "for", "ind", ",", "image", "in", "enumerate", "(", "images", ")", ":", "\n", "        ", "if", "ind", ">=", "size_frame", "*", "size_frame", ":", "\n", "            ", "break", "\n", "", "ind_col", "=", "ind", "%", "size_frame", "\n", "ind_row", "=", "ind", "//", "size_frame", "\n", "frame", "[", "(", "ind_row", "*", "img_h", ")", ":", "(", "ind_row", "*", "img_h", "+", "img_h", ")", ",", "(", "ind_col", "*", "img_w", ")", ":", "(", "ind_col", "*", "img_w", "+", "img_w", ")", ",", ":", "]", "=", "np", ".", "transpose", "(", "image", ",", "(", "1", ",", "2", ",", "0", ")", ")", "\n", "\n", "", "imsave", "(", "save_path", ",", "frame", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.utils.get_wav_duration": [[31, 36], ["os.path.abspath", "scipy.read", "len", "float"], "function", ["None"], ["", "def", "get_wav_duration", "(", "audioname", ")", ":", "\n", "#get the wav duration in second", "\n", "    ", "fname", "=", "os", ".", "path", ".", "abspath", "(", "audioname", ")", "\n", "fs", ",", "input_signal", "=", "wav", ".", "read", "(", "audioname", ")", "\n", "return", "len", "(", "input_signal", ")", "/", "float", "(", "fs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.utils.sort_filename": [[38, 44], ["sorted", "int", "[].split", "[].split", "[].split", "[].split", "x.split", "x.split"], "function", ["None"], ["", "def", "sort_filename", "(", "files", ")", ":", "\n", "    ", "files_sort", "=", "sorted", "(", "files", ",", "key", "=", "lambda", "x", ":", "(", "\n", "'%s_%04d'", "%", "(", "'_'", ".", "join", "(", "x", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", ".", "split", "(", "'_'", ")", "[", ":", "-", "1", "]", ")", ",", "\n", "int", "(", "x", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", ".", "split", "(", "'_'", ")", "[", "-", "1", "]", ")", ")", "\n", ")", ")", "\n", "return", "files_sort", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.utils.save_video": [[45, 53], ["utils.create_dir", "enumerate", "subprocess.call", "numpy.transpose", "scipy.misc.imsave", "len", "os.path.join", "str", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.utils.utils.create_dir"], ["", "def", "save_video", "(", "audio_duration", ",", "audio_test_file", ",", "images", ",", "save_dir", ")", ":", "\n", "    ", "create_dir", "(", "save_dir", ")", "\n", "for", "idx", ",", "image", "in", "enumerate", "(", "images", ")", ":", "\n", "        ", "image", "=", "np", ".", "transpose", "(", "image", ",", "(", "1", ",", "2", ",", "0", ")", ")", "\n", "imsave", "(", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'%03d.png'", "%", "(", "idx", ")", ")", ",", "image", ")", "\n", "\n", "", "time_per_img", "=", "audio_duration", "/", "len", "(", "images", ")", "\n", "call", "(", "[", "\"ffmpeg\"", ",", "\"-y\"", ",", "\"-framerate\"", ",", "str", "(", "1.0", "/", "time_per_img", ")", ",", "\"-start_number\"", ",", "\"0\"", ",", "\"-i\"", ",", "os", ".", "path", ".", "join", "(", "save_dir", ",", "\"%03d.png\"", ")", ",", "\"-i\"", ",", "audio_test_file", ",", "\"-c:v\"", ",", "\"libx264\"", ",", "\"-r\"", ",", "\"25\"", ",", "\"-pix_fmt\"", ",", "\"yuv420p\"", ",", "\"-c:a\"", ",", "\"aac\"", ",", "\"-strict\"", ",", "\"experimental\"", ",", "\"-shortest\"", ",", "os", ".", "path", ".", "join", "(", "save_dir", ",", "\"./output.mp4\"", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.get_LRW_landmarks.tryint": [[19, 24], ["int"], "function", ["None"], ["def", "tryint", "(", "s", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "return", "int", "(", "s", ")", "\n", "", "except", ":", "\n", "        ", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.get_LRW_landmarks.alphanum_key": [[25, 30], ["get_LRW_landmarks.tryint", "re.split"], "function", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.trainList_LRW.tryint"], ["", "", "def", "alphanum_key", "(", "s", ")", ":", "\n", "    ", "\"\"\" Turn a string into a list of string and number chunks.\n        \"z23a\" -> [\"z\", 23, \"a\"]\n    \"\"\"", "\n", "return", "[", "tryint", "(", "c", ")", "for", "c", "in", "re", ".", "split", "(", "'([0-9]+)'", ",", "s", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.get_LRW_landmarks.listdir_nohidden": [[31, 37], ["os.listdir", "f.startswith", "file_list.append"], "function", ["None"], ["", "def", "listdir_nohidden", "(", "path", ")", ":", "\n", "  ", "file_list", "=", "[", "]", "\n", "for", "f", "in", "os", ".", "listdir", "(", "path", ")", ":", "\n", "    ", "if", "not", "f", ".", "startswith", "(", "'.'", ")", ":", "\n", "      ", "file_list", ".", "append", "(", "f", ")", "\n", "", "", "return", "file_list", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.get_LRW_landmarks.get_landmarks": [[39, 46], ["detector", "len", "predictor().parts", "predictor"], "function", ["None"], ["", "def", "get_landmarks", "(", "im", ")", ":", "\n", "  ", "rects", "=", "detector", "(", "im", ",", "0", ")", "\n", "if", "len", "(", "rects", ")", "==", "0", ":", "\n", "    ", "return", "None", "\n", "", "max_rect", "=", "rects", "[", "0", "]", "\n", "s", "=", "[", "[", "p", ".", "x", ",", "p", ".", "y", "]", "for", "p", "in", "predictor", "(", "im", ",", "max_rect", ")", ".", "parts", "(", ")", "]", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.get_LRW_landmarks.run_each_folder": [[48, 69], ["glob.glob", "cv2.imread", "get_LRW_landmarks.get_landmarks", "img_path.replace().replace", "print", "os.path.join", "os.path.exists", "open", "img_path.replace", "print", "os.remove", "F.write", "str", "str"], "function", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.trainList_VOX_dlib.get_landmarks"], ["", "def", "run_each_folder", "(", "sub_folder", ")", ":", "\n", "  ", "image_list", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "gt_image_dir", ",", "sub_folder", ")", "+", "'/*.jpg'", ")", "\n", "\n", "for", "img_path", "in", "image_list", ":", "\n", "    ", "img", "=", "cv2", ".", "imread", "(", "img_path", ")", "\n", "\n", "landmarks", "=", "get_landmarks", "(", "img", ")", "\n", "\n", "landmark_path", "=", "img_path", ".", "replace", "(", "'gt_image'", ",", "'landmarks'", ")", ".", "replace", "(", "'.jpg'", ",", "'.txt'", ")", "\n", "\n", "if", "landmarks", "is", "None", ":", "\n", "      ", "if", "os", ".", "path", ".", "exists", "(", "landmark_path", ")", ":", "\n", "        ", "print", "(", "'remove existing file: '", ",", "landmark_path", ")", "\n", "os", ".", "remove", "(", "landmark_path", ")", "\n", "continue", "\n", "\n", "", "", "print", "(", "img_path", ",", "landmark_path", ")", "\n", "\n", "with", "open", "(", "landmark_path", ",", "'w'", ")", "as", "F", ":", "\n", "      ", "for", "landmark", "in", "landmarks", ":", "\n", "        ", "F", ".", "write", "(", "str", "(", "landmark", "[", "0", "]", ")", "+", "','", "+", "str", "(", "landmark", "[", "1", "]", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.trainList_TCD.tryint": [[15, 20], ["int"], "function", ["None"], ["def", "tryint", "(", "s", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "return", "int", "(", "s", ")", "\n", "", "except", ":", "\n", "        ", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.trainList_TCD.alphanum_key": [[21, 26], ["trainList_TCD.tryint", "re.split"], "function", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.trainList_LRW.tryint"], ["", "", "def", "alphanum_key", "(", "s", ")", ":", "\n", "    ", "\"\"\" Turn a string into a list of string and number chunks.\n        \"z23a\" -> [\"z\", 23, \"a\"]\n    \"\"\"", "\n", "return", "[", "tryint", "(", "c", ")", "for", "c", "in", "re", ".", "split", "(", "'([0-9]+)'", ",", "s", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.trainList_TCD.listdir_nohidden": [[27, 33], ["os.listdir", "f.startswith", "file_list.append"], "function", ["None"], ["", "def", "listdir_nohidden", "(", "path", ")", ":", "\n", "  ", "file_list", "=", "[", "]", "\n", "for", "f", "in", "os", ".", "listdir", "(", "path", ")", ":", "\n", "    ", "if", "not", "f", ".", "startswith", "(", "'.'", ")", ":", "\n", "      ", "file_list", ".", "append", "(", "f", ")", "\n", "", "", "return", "file_list", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.trainList_TCD.load_seq_input": [[35, 53], ["collections.defaultdict", "collections.defaultdict.items", "line.rstrip().split", "os.path.split", "os.path.basename", "seq_dic[].append", "input_seq_list.append", "gt_seq_list.append", "audio_seq_list.append", "line.rstrip", "filename.split"], "function", ["None"], ["", "def", "load_seq_input", "(", "training_list", ")", ":", "\n", "  ", "seq_dic", "=", "defaultdict", "(", "list", ")", "\n", "for", "line", "in", "training_list", ":", "\n", "    ", "image_path", ",", "gt_path", ",", "audio_path", "=", "line", ".", "rstrip", "(", "'\\n'", ")", ".", "split", "(", "' '", ")", "\n", "basepath", ",", "filename", "=", "os", ".", "path", ".", "split", "(", "gt_path", ")", "\n", "celebname", "=", "os", ".", "path", ".", "basename", "(", "basepath", ")", "\n", "audioname", "=", "'_'", ".", "join", "(", "filename", ".", "split", "(", "spliter", ")", "[", ":", "-", "1", "]", ")", "\n", "key", "=", "celebname", "+", "'/'", "+", "audioname", "\n", "seq_dic", "[", "key", "]", ".", "append", "(", "[", "image_path", ",", "gt_path", ",", "audio_path", "]", ")", "\n", "\n", "", "input_seq_list", "=", "[", "]", "\n", "gt_seq_list", "=", "[", "]", "\n", "audio_seq_list", "=", "[", "]", "\n", "for", "key", ",", "items", "in", "seq_dic", ".", "items", "(", ")", ":", "\n", "    ", "input_seq_list", ".", "append", "(", "[", "elem", "[", "0", "]", "for", "elem", "in", "items", "]", ")", "\n", "gt_seq_list", ".", "append", "(", "[", "elem", "[", "1", "]", "for", "elem", "in", "items", "]", ")", "\n", "audio_seq_list", ".", "append", "(", "[", "elem", "[", "2", "]", "for", "elem", "in", "items", "]", ")", "\n", "", "return", "input_seq_list", ",", "gt_seq_list", ",", "audio_seq_list", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.if_one_face": [[44, 51], ["detector", "len", "len"], "function", ["None"], ["", "def", "if_one_face", "(", "im", ")", ":", "\n", "    ", "rects", "=", "detector", "(", "im", ",", "1", ")", "\n", "if", "len", "(", "rects", ")", ">", "1", ":", "\n", "        ", "return", "2", "\n", "", "if", "len", "(", "rects", ")", "==", "0", ":", "\n", "        ", "return", "0", "\n", "", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.get_landmarks": [[53, 80], ["detector", "range", "numpy.matrix", "len", "numpy.array", "numpy.linalg.norm", "len", "rects[].left", "rects[].top", "rects[].right", "rects[].bottom", "predictor().parts", "predictor"], "function", ["None"], ["", "def", "get_landmarks", "(", "im", ",", "bbox_gt", ")", ":", "\n", "    ", "rects", "=", "detector", "(", "im", ",", "0", ")", "\n", "\n", "# max_area = 0", "\n", "min_dist", "=", "10000", "\n", "for", "i", "in", "range", "(", "len", "(", "rects", ")", ")", ":", "\n", "# if dlib.rectangle.area(rects[i]) > max_area:", "\n", "#     max_area = dlib.rectangle.area(rects[i])", "\n", "#     max_rect = rects[i]", "\n", "\n", "        ", "bbox_dlib", "=", "np", ".", "array", "(", "[", "rects", "[", "i", "]", ".", "left", "(", ")", ",", "rects", "[", "i", "]", ".", "top", "(", ")", ",", "rects", "[", "i", "]", ".", "right", "(", ")", ",", "rects", "[", "i", "]", ".", "bottom", "(", ")", "]", ")", "\n", "dist", "=", "np", ".", "linalg", ".", "norm", "(", "bbox_gt", "-", "bbox_dlib", ")", "\n", "\n", "if", "dist", "<", "min_dist", ":", "\n", "            ", "min_dist", "=", "dist", "\n", "max_rect", "=", "rects", "[", "i", "]", "\n", "\n", "", "", "if", "min_dist", ">", "200", ":", "\n", "        ", "return", "None", "\n", "\n", "# if len(rects) > 1:", "\n", "#     raise TooManyFaces", "\n", "", "if", "len", "(", "rects", ")", "==", "0", ":", "\n", "        ", "return", "None", "\n", "", "s", "=", "np", ".", "matrix", "(", "[", "[", "p", ".", "x", ",", "p", ".", "y", "]", "for", "p", "in", "predictor", "(", "im", ",", "max_rect", ")", ".", "parts", "(", ")", "]", ")", "\n", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.get_landmarks_ref": [[81, 100], ["detector", "range", "numpy.matrix", "len", "len", "dlib.rectangle.area", "dlib.rectangle.area", "predictor().parts", "predictor"], "function", ["None"], ["", "def", "get_landmarks_ref", "(", "im", ")", ":", "\n", "    ", "rects", "=", "detector", "(", "im", ",", "0", ")", "\n", "\n", "max_area", "=", "0", "\n", "min_dist", "=", "10000", "\n", "for", "i", "in", "range", "(", "len", "(", "rects", ")", ")", ":", "\n", "        ", "if", "dlib", ".", "rectangle", ".", "area", "(", "rects", "[", "i", "]", ")", ">", "max_area", ":", "\n", "            ", "max_area", "=", "dlib", ".", "rectangle", ".", "area", "(", "rects", "[", "i", "]", ")", "\n", "max_rect", "=", "rects", "[", "i", "]", "\n", "\n", "# print(max_rect.left(), max_rect.top(), max_rect.right(), max_rect.bottom())", "\n", "\n", "# if len(rects) > 1:", "\n", "#     raise TooManyFaces", "\n", "", "", "if", "len", "(", "rects", ")", "==", "0", ":", "\n", "        ", "return", "None", "\n", "", "s", "=", "np", ".", "matrix", "(", "[", "[", "p", ".", "x", ",", "p", ".", "y", "]", "for", "p", "in", "predictor", "(", "im", ",", "max_rect", ")", ".", "parts", "(", ")", "]", ")", "\n", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.transformation_from_points": [[102, 138], ["points1.astype.astype", "points2.astype.astype", "numpy.mean", "numpy.mean", "numpy.std", "numpy.std", "numpy.linalg.svd", "numpy.vstack", "numpy.hstack", "numpy.matrix"], "function", ["None"], ["", "def", "transformation_from_points", "(", "points1", ",", "points2", ")", ":", "\n", "    ", "\"\"\"\n    Return an affine transformation [s * R | T] such that:\n        sum ||s*R*p1,i + T - p2,i||^2\n    is minimized.\n    \"\"\"", "\n", "# Solve the procrustes problem by subtracting centroids, scaling by the", "\n", "# standard deviation, and then using the SVD to calculate the rotation. See", "\n", "# the following for more details:", "\n", "#   https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem", "\n", "\n", "points1", "=", "points1", ".", "astype", "(", "np", ".", "float64", ")", "\n", "points2", "=", "points2", ".", "astype", "(", "np", ".", "float64", ")", "\n", "\n", "c1", "=", "np", ".", "mean", "(", "points1", ",", "axis", "=", "0", ")", "\n", "c2", "=", "np", ".", "mean", "(", "points2", ",", "axis", "=", "0", ")", "\n", "points1", "-=", "c1", "\n", "points2", "-=", "c2", "\n", "\n", "s1", "=", "np", ".", "std", "(", "points1", ")", "\n", "s2", "=", "np", ".", "std", "(", "points2", ")", "\n", "points1", "/=", "s1", "\n", "points2", "/=", "s2", "\n", "\n", "U", ",", "S", ",", "Vt", "=", "np", ".", "linalg", ".", "svd", "(", "points1", ".", "T", "*", "points2", ")", "\n", "\n", "# The R we seek is in fact the transpose of the one given by U * Vt. This", "\n", "# is because the above formulation assumes the matrix goes on the right", "\n", "# (with row vectors) where as our solution requires the matrix to be on the", "\n", "# left (with column vectors).", "\n", "R", "=", "(", "U", "*", "Vt", ")", ".", "T", "\n", "M", "=", "np", ".", "vstack", "(", "[", "np", ".", "hstack", "(", "(", "(", "s2", "/", "s1", ")", "*", "R", ",", "\n", "c2", ".", "T", "-", "(", "s2", "/", "s1", ")", "*", "R", "*", "c1", ".", "T", ")", ")", ",", "\n", "np", ".", "matrix", "(", "[", "0.", ",", "0.", ",", "1.", "]", ")", "]", ")", "\n", "\n", "return", "M", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.warp_im": [[141, 150], ["numpy.zeros", "cv2.warpAffine"], "function", ["None"], ["", "def", "warp_im", "(", "im", ",", "M", ",", "dshape", ")", ":", "\n", "    ", "output_im", "=", "np", ".", "zeros", "(", "dshape", ",", "dtype", "=", "im", ".", "dtype", ")", "\n", "cv2", ".", "warpAffine", "(", "im", ",", "\n", "M", "[", ":", "2", "]", ",", "\n", "(", "dshape", "[", "1", "]", ",", "dshape", "[", "0", "]", ")", ",", "\n", "dst", "=", "output_im", ",", "\n", "borderMode", "=", "cv2", ".", "BORDER_TRANSPARENT", ",", "\n", "flags", "=", "cv2", ".", "WARP_INVERSE_MAP", ")", "\n", "return", "output_im", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.warp_lip_landmark": [[151, 158], ["numpy.ones", "numpy.column_stack", "numpy.around().astype", "np.around().astype.T.tolist", "numpy.around", "numpy.matmul"], "function", ["None"], ["", "def", "warp_lip_landmark", "(", "M", ",", "lip_landmark", ")", ":", "\n", "    ", "allone", "=", "np", ".", "ones", "(", "(", "lip_landmark", ".", "shape", "[", "0", "]", ",", "1", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "lip_landmark_3d", "=", "np", ".", "column_stack", "(", "(", "lip_landmark", ",", "allone", ")", ")", "\n", "lip_region", "=", "np", ".", "around", "(", "np", ".", "matmul", "(", "M", ",", "lip_landmark_3d", ".", "T", ")", "[", ":", "2", "]", ")", ".", "astype", "(", "int", ")", "\n", "list_lip_region", "=", "lip_region", ".", "T", ".", "tolist", "(", ")", "\n", "return", "list_lip_region", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.align_im_to_ref": [[160, 209], ["faceAlignment.transformation_from_points", "faceAlignment.transformation_from_points", "faceAlignment.warp_im", "faceAlignment.warp_lip_landmark", "faceAlignment.warp_lip_landmark", "numpy.linalg.norm", "numpy.linalg.norm", "float", "print", "print"], "function", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.transformation_from_points", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.transformation_from_points", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.warp_im", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.warp_lip_landmark", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.warp_lip_landmark"], ["", "def", "align_im_to_ref", "(", "img", ",", "img_ref", ",", "landmark", ",", "landmark_ref", ",", "is_only_front", "=", "True", ",", "is_only_lip", "=", "True", ")", ":", "\n", "    ", "M", "=", "transformation_from_points", "(", "landmark_ref", "[", "ALIGN_POINTS", "]", ",", "\n", "landmark", "[", "ALIGN_POINTS", "]", ")", "\n", "\n", "M_2", "=", "transformation_from_points", "(", "landmark", "[", "ALIGN_POINTS", "]", ",", "landmark_ref", "[", "ALIGN_POINTS", "]", ")", "\n", "\n", "warped_im", "=", "warp_im", "(", "img", ",", "M", ",", "img_ref", ".", "shape", ")", "\n", "c1", "=", "255", "\n", "r1", "=", "209", "\n", "# roi = warped_im[c1:c1+109,r1:r1+109]", "\n", "\n", "lip_landmark", "=", "warp_lip_landmark", "(", "M_2", ",", "landmark", "[", "MOUTH_POINTS", "]", ")", "\n", "warp_landmark", "=", "warp_lip_landmark", "(", "M_2", ",", "landmark", ")", "\n", "\n", "# # draw landmark on face", "\n", "# for i in range(len(warp_landmark)):", "\n", "#    pos = tuple(warp_landmark[i])", "\n", "#    cv2.circle(warped_im, pos, 2, (255,255,255), -1)", "\n", "\n", "warped_face", "=", "warped_im", "[", "100", ":", "400", ",", "115", ":", "415", "]", "\n", "mouth_roi", "=", "warped_im", "[", "255", ":", "364", ",", "209", ":", "318", "]", "\n", "# warped_face = warped_im[131:449, 332:670]", "\n", "\n", "# for i in range(len(landmark)):", "\n", "#    cv2.circle(img, (landmark[i][0,0],landmark[i][0,1]) , 2, (255,255,255), -1)", "\n", "# warped_face = img", "\n", "\n", "if", "is_only_lip", ":", "\n", "        ", "return_landmark", "=", "lip_landmark", "\n", "", "else", ":", "\n", "        ", "return_landmark", "=", "warp_landmark", "\n", "\n", "", "if", "is_only_front", ":", "\n", "#calculate the ratio between dist(40-28)/dist(43-28)", "\n", "        ", "dist_p40_p28", "=", "np", ".", "linalg", ".", "norm", "(", "landmark", "[", "39", "]", "-", "landmark", "[", "27", "]", ")", "\n", "dist_p43_p28", "=", "np", ".", "linalg", ".", "norm", "(", "landmark", "[", "42", "]", "-", "landmark", "[", "27", "]", ")", "\n", "eye_ratio", "=", "float", "(", "dist_p40_p28", "/", "dist_p43_p28", ")", "\n", "# dist_p65_p66 = np.linalg.norm(landmark[64]-landmark[65])", "\n", "# dist_p61_p68 = np.linalg.norm(landmark[60]-landmark[67])", "\n", "# lip_ratio = float(dist_p61_p68/dist_p65_p66)", "\n", "\n", "print", "(", "'the distance eyeration is'", ",", "eye_ratio", ")", "\n", "if", "0.8", "<", "eye_ratio", "and", "eye_ratio", "<", "1.25", ":", "\n", "            ", "return", "warped_face", ",", "return_landmark", "\n", "", "else", ":", "\n", "            ", "return", "None", ",", "None", "\n", "", "", "else", ":", "\n", "        ", "print", "(", "'the wrap face and whole face landmark'", ")", "\n", "return", "warped_face", ",", "mouth_roi", ",", "return_landmark", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.align_ref_to_im": [[213, 223], ["faceAlignment.transformation_from_points", "faceAlignment.transformation_from_points", "numpy.matrix", "faceAlignment.warp_lip_landmark"], "function", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.transformation_from_points", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.transformation_from_points", "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.warp_lip_landmark"], ["", "", "def", "align_ref_to_im", "(", "img", ",", "img_ref", ",", "landmark", ",", "landmark_ref", ")", ":", "\n", "    ", "M", "=", "transformation_from_points", "(", "landmark_ref", "[", "ALIGN_POINTS", "]", ",", "\n", "landmark", "[", "ALIGN_POINTS", "]", ")", "\n", "\n", "M_2", "=", "transformation_from_points", "(", "landmark", "[", "ALIGN_POINTS", "]", ",", "landmark_ref", "[", "ALIGN_POINTS", "]", ")", "\n", "\n", "ref_mouth_coordinates", "=", "np", ".", "matrix", "(", "[", "[", "209", ",", "255", "]", ",", "[", "318", ",", "364", "]", "]", ")", "\n", "src_mouth_coordinates", "=", "warp_lip_landmark", "(", "M", ",", "ref_mouth_coordinates", ")", "\n", "\n", "return", "src_mouth_coordinates", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.get_mouth_region": [[225, 238], ["numpy.array", "print"], "function", ["None"], ["", "def", "get_mouth_region", "(", "img", ",", "landmark", ")", ":", "\n", "    ", "landmark", "=", "np", ".", "array", "(", "landmark", ")", "\n", "left_x", ",", "left_y", "=", "landmark", "[", "48", "]", "[", "0", "]", ",", "landmark", "[", "48", "]", "[", "1", "]", "\n", "right_x", ",", "right_y", "=", "landmark", "[", "54", "]", "[", "0", "]", ",", "landmark", "[", "54", "]", "[", "1", "]", "\n", "\n", "height", "=", "width", "=", "1.5", "*", "(", "right_x", "-", "left_x", ")", "\n", "print", "(", "height", ")", "\n", "mid_x", "=", "(", "left_x", "+", "right_x", ")", "/", "2.0", "\n", "mid_y", "=", "(", "left_y", "+", "right_y", ")", "/", "2.0", "\n", "\n", "roi_x1", ",", "roi_x2", ",", "roi_y1", ",", "roi_y2", "=", "mid_x", "-", "0.5", "*", "width", ",", "mid_x", "+", "0.5", "*", "width", ",", "mid_y", "-", "0.62", "*", "height", ",", "mid_y", "+", "0.38", "*", "height", "\n", "# return img[roi_y1:roi_y2, roi_x1:roi_x2]", "\n", "return", "roi_x1", ",", "roi_x2", ",", "roi_y1", ",", "roi_y2", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.save_landmark_img": [[240, 246], ["numpy.zeros", "faceAlignment.draw_landmark"], "function", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.draw_landmark"], ["", "def", "save_landmark_img", "(", "lip_landmark", ",", "img_shape", ",", "offset", ",", "filename", ",", "radius", "=", "2", ")", ":", "\n", "    ", "imagename", "=", "filename", "\n", "img", "=", "np", ".", "zeros", "(", "img_shape", ")", "\n", "if", "offset", "is", "None", ":", "\n", "        ", "offset", "=", "[", "115", ",", "100", "]", "\n", "", "draw_landmark", "(", "img", ",", "lip_landmark", ",", "offset", ",", "imagename", ",", "radius", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.draw_landmark": [[248, 256], ["range", "cv2.imwrite", "len", "tuple", "cv2.circle"], "function", ["None"], ["", "def", "draw_landmark", "(", "img", ",", "landmarks", ",", "offset", ",", "imagename", ",", "radius", "=", "2", ")", ":", "\n", "    ", "for", "i", "in", "range", "(", "len", "(", "landmarks", ")", ")", ":", "\n", "# ref_landmark = lip_landmark[i]", "\n", "        ", "ref_landmark", "=", "(", "landmarks", "[", "i", "]", "[", "0", "]", "-", "offset", "[", "0", "]", ",", "landmarks", "[", "i", "]", "[", "1", "]", "-", "offset", "[", "1", "]", ")", "\n", "pos", "=", "tuple", "(", "ref_landmark", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "pos", ",", "radius", ",", "(", "255", ",", "255", ",", "255", ")", ",", "-", "1", ")", "\n", "# print('saving image to', imagename)", "\n", "", "cv2", ".", "imwrite", "(", "imagename", ",", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.save_landmark_pos": [[259, 270], ["open", "range", "f.close", "len", "f.writelines", "str", "str", "str", "str"], "function", ["None"], ["", "def", "save_landmark_pos", "(", "landmark", ",", "offset", ",", "filename", ")", ":", "\n", "# print('saving mouth region landmark into filename', filename)", "\n", "    ", "with", "open", "(", "filename", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "landmark", ")", ")", ":", "\n", "# it is writen as the image coordination space", "\n", "            ", "if", "offset", "is", "not", "None", ":", "\n", "                ", "ref_landmark", "=", "str", "(", "landmark", "[", "i", "]", "[", "0", "]", "-", "offset", "[", "0", "]", ")", "+", "','", "+", "str", "(", "landmark", "[", "i", "]", "[", "1", "]", "-", "offset", "[", "1", "]", ")", "+", "'\\n'", "\n", "", "else", ":", "\n", "                ", "ref_landmark", "=", "str", "(", "landmark", "[", "i", "]", "[", "0", "]", "-", "115", ")", "+", "','", "+", "str", "(", "landmark", "[", "i", "]", "[", "1", "]", "-", "100", ")", "+", "'\\n'", "\n", "", "f", ".", "writelines", "(", "ref_landmark", ")", "\n", "", "f", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.faceAlignment.drawlanmark": [[272, 279], ["numpy.around().astype().tolist", "range", "cv2.imwrite", "len", "tuple", "print", "cv2.circle", "numpy.around().astype", "numpy.around"], "function", ["None"], ["", "", "def", "drawlanmark", "(", "img", ",", "landmark", ")", ":", "\n", "    ", "landmark_list", "=", "np", ".", "around", "(", "landmark", ")", ".", "astype", "(", "int", ")", ".", "tolist", "(", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "landmark_list", ")", ")", ":", "\n", "        ", "pos", "=", "tuple", "(", "landmark_list", "[", "i", "]", ")", "\n", "print", "(", "pos", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "pos", ",", "1", ",", "(", "0", ",", "255", ",", "0", ")", ",", "-", "1", ")", "\n", "", "cv2", ".", "imwrite", "(", "\"square_circle_opencv.jpg\"", ",", "img", ")", "", "", ""]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.trainList_VOX_dlib.tryint": [[23, 28], ["int"], "function", ["None"], ["def", "tryint", "(", "s", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "return", "int", "(", "s", ")", "\n", "", "except", ":", "\n", "        ", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.trainList_VOX_dlib.alphanum_key": [[29, 34], ["trainList_VOX_dlib.tryint", "re.split"], "function", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.trainList_LRW.tryint"], ["", "", "def", "alphanum_key", "(", "s", ")", ":", "\n", "    ", "\"\"\" Turn a string into a list of string and number chunks.\n        \"z23a\" -> [\"z\", 23, \"a\"]\n    \"\"\"", "\n", "return", "[", "tryint", "(", "c", ")", "for", "c", "in", "re", ".", "split", "(", "'([0-9]+)'", ",", "s", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.trainList_VOX_dlib.listdir_nohidden": [[35, 41], ["os.listdir", "f.startswith", "file_list.append"], "function", ["None"], ["", "def", "listdir_nohidden", "(", "path", ")", ":", "\n", "  ", "file_list", "=", "[", "]", "\n", "for", "f", "in", "os", ".", "listdir", "(", "path", ")", ":", "\n", "    ", "if", "not", "f", ".", "startswith", "(", "'.'", ")", ":", "\n", "      ", "file_list", ".", "append", "(", "f", ")", "\n", "", "", "return", "file_list", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.trainList_VOX_dlib.load_seq_input": [[43, 61], ["collections.defaultdict", "collections.defaultdict.items", "line.rstrip().split", "os.path.split", "os.path.basename", "seq_dic[].append", "input_seq_list.append", "gt_seq_list.append", "audio_seq_list.append", "line.rstrip", "filename.split"], "function", ["None"], ["", "def", "load_seq_input", "(", "training_list", ")", ":", "\n", "  ", "seq_dic", "=", "defaultdict", "(", "list", ")", "\n", "for", "line", "in", "training_list", ":", "\n", "    ", "image_path", ",", "gt_path", ",", "audio_path", "=", "line", ".", "rstrip", "(", "'\\n'", ")", ".", "split", "(", "' '", ")", "\n", "basepath", ",", "filename", "=", "os", ".", "path", ".", "split", "(", "gt_path", ")", "\n", "celebname", "=", "os", ".", "path", ".", "basename", "(", "basepath", ")", "\n", "audioname", "=", "'_'", ".", "join", "(", "filename", ".", "split", "(", "spliter", ")", "[", ":", "-", "1", "]", ")", "\n", "key", "=", "celebname", "+", "'/'", "+", "audioname", "\n", "seq_dic", "[", "key", "]", ".", "append", "(", "[", "image_path", ",", "gt_path", ",", "audio_path", "]", ")", "\n", "\n", "", "input_seq_list", "=", "[", "]", "\n", "gt_seq_list", "=", "[", "]", "\n", "audio_seq_list", "=", "[", "]", "\n", "for", "key", ",", "items", "in", "seq_dic", ".", "items", "(", ")", ":", "\n", "    ", "input_seq_list", ".", "append", "(", "[", "elem", "[", "0", "]", "for", "elem", "in", "items", "]", ")", "\n", "gt_seq_list", ".", "append", "(", "[", "elem", "[", "1", "]", "for", "elem", "in", "items", "]", ")", "\n", "audio_seq_list", ".", "append", "(", "[", "elem", "[", "2", "]", "for", "elem", "in", "items", "]", ")", "\n", "", "return", "input_seq_list", ",", "gt_seq_list", ",", "audio_seq_list", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.trainList_VOX_dlib.get_landmarks": [[63, 71], ["detector", "len", "predictor().parts", "predictor"], "function", ["None"], ["", "def", "get_landmarks", "(", "im", ")", ":", "\n", "  ", "rects", "=", "detector", "(", "im", ",", "0", ")", "\n", "if", "len", "(", "rects", ")", "==", "0", ":", "\n", "    ", "return", "None", "\n", "", "max_rect", "=", "rects", "[", "0", "]", "\n", "s", "=", "[", "[", "p", ".", "x", ",", "p", ".", "y", "]", "for", "p", "in", "predictor", "(", "im", ",", "max_rect", ")", ".", "parts", "(", ")", "]", "\n", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.trainList_LRW.tryint": [[16, 21], ["int"], "function", ["None"], ["def", "tryint", "(", "s", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "return", "int", "(", "s", ")", "\n", "", "except", ":", "\n", "        ", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.trainList_LRW.alphanum_key": [[22, 27], ["trainList_LRW.tryint", "re.split"], "function", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.trainList_LRW.tryint"], ["", "", "def", "alphanum_key", "(", "s", ")", ":", "\n", "    ", "\"\"\" Turn a string into a list of string and number chunks.\n        \"z23a\" -> [\"z\", 23, \"a\"]\n    \"\"\"", "\n", "return", "[", "tryint", "(", "c", ")", "for", "c", "in", "re", ".", "split", "(", "'([0-9]+)'", ",", "s", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.lip_read_training_list.trainList_LRW.listdir_nohidden": [[28, 34], ["os.listdir", "f.startswith", "file_list.append"], "function", ["None"], ["", "def", "listdir_nohidden", "(", "path", ")", ":", "\n", "  ", "file_list", "=", "[", "]", "\n", "for", "f", "in", "os", ".", "listdir", "(", "path", ")", ":", "\n", "    ", "if", "not", "f", ".", "startswith", "(", "'.'", ")", ":", "\n", "      ", "file_list", ".", "append", "(", "f", ")", "\n", "", "", "return", "file_list", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.trainList_LRW_continous_nonalign.tryint": [[27, 32], ["int"], "function", ["None"], ["def", "tryint", "(", "s", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "return", "int", "(", "s", ")", "\n", "", "except", ":", "\n", "        ", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.trainList_LRW_continous_nonalign.alphanum_key": [[33, 38], ["trainList_LRW_continous_nonalign.tryint", "re.split"], "function", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.trainList_LRW.tryint"], ["", "", "def", "alphanum_key", "(", "s", ")", ":", "\n", "    ", "\"\"\" Turn a string into a list of string and number chunks.\n        \"z23a\" -> [\"z\", 23, \"a\"]\n    \"\"\"", "\n", "return", "[", "tryint", "(", "c", ")", "for", "c", "in", "re", ".", "split", "(", "'([0-9]+)'", ",", "s", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.trainList_LRW_continous_nonalign.listdir_nohidden": [[39, 45], ["os.listdir", "f.startswith", "file_list.append"], "function", ["None"], ["", "def", "listdir_nohidden", "(", "path", ")", ":", "\n", "  ", "file_list", "=", "[", "]", "\n", "for", "f", "in", "os", ".", "listdir", "(", "path", ")", ":", "\n", "    ", "if", "not", "f", ".", "startswith", "(", "'.'", ")", ":", "\n", "      ", "file_list", ".", "append", "(", "f", ")", "\n", "", "", "return", "file_list", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.trainList_LRW_continous_nonalign.get_training_list": [[46, 52], ["list", "random.shuffle", "int", "set", "len", "[].split", "filename.split"], "function", ["None"], ["", "def", "get_training_list", "(", "file_list", ")", ":", "\n", "  ", "sentence_list", "=", "[", "'_'", ".", "join", "(", "filename", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "split", "(", "'_'", ")", "[", ":", "-", "1", "]", ")", "for", "filename", "in", "file_list", "]", "\n", "sentence_list", "=", "list", "(", "set", "(", "sentence_list", ")", ")", "\n", "random", ".", "shuffle", "(", "sentence_list", ")", "\n", "num_train", "=", "int", "(", "len", "(", "sentence_list", ")", "*", "ratio", ")", "\n", "return", "sentence_list", "[", ":", "num_train", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.create_LRW_train_list_front_face.tryint": [[22, 27], ["int"], "function", ["None"], ["def", "tryint", "(", "s", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "return", "int", "(", "s", ")", "\n", "", "except", ":", "\n", "        ", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.create_LRW_train_list_front_face.alphanum_key": [[28, 33], ["create_LRW_train_list_front_face.tryint", "re.split"], "function", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.trainList_LRW.tryint"], ["", "", "def", "alphanum_key", "(", "s", ")", ":", "\n", "    ", "\"\"\" Turn a string into a list of string and number chunks.\n        \"z23a\" -> [\"z\", 23, \"a\"]\n    \"\"\"", "\n", "return", "[", "tryint", "(", "c", ")", "for", "c", "in", "re", ".", "split", "(", "'([0-9]+)'", ",", "s", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.create_LRW_train_list_front_face.listdir_nohidden": [[34, 40], ["os.listdir", "f.startswith", "folder_list.append"], "function", ["None"], ["", "def", "listdir_nohidden", "(", "path", ")", ":", "\n", "  ", "folder_list", "=", "[", "]", "\n", "for", "f", "in", "os", ".", "listdir", "(", "path", ")", ":", "\n", "      ", "if", "not", "f", ".", "startswith", "(", "'.'", ")", ":", "\n", "          ", "folder_list", ".", "append", "(", "f", ")", "\n", "", "", "return", "folder_list", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.create_LRW_train_list_front_face.run_each_folder": [[41, 113], ["os.path.join", "glob.glob", "glob.glob.sort", "collections.defaultdict", "collections.defaultdict.iteritems", "os.path.split", "os.path.basename", "dic[].append", "item.sort", "numpy.array().astype", "numpy.zeros", "range", "numpy.zeros", "range", "range", "numpy.zeros", "range", "print", "os.path.basename().replace.split", "len", "len", "open", "f.readlines", "line.rstrip", "line.split", "numpy.array", "np.array().astype.append", "abs", "os.path.basename().replace", "os.path.join", "os.path.join", "os.path.join", "FILE_IN.write", "numpy.array", "os.path.basename().replace.replace", "float", "float", "os.path.basename"], "function", ["None"], ["", "def", "run_each_folder", "(", "sub_folder", ")", ":", "\n", "\n", "  ", "sub_path", "=", "os", ".", "path", ".", "join", "(", "landmark_dir", ",", "sub_folder", ")", "\n", "file_list", "=", "glob", ".", "glob", "(", "sub_path", "+", "'/*.txt'", ")", "\n", "file_list", ".", "sort", "(", "key", "=", "alphanum_key", ")", "\n", "\n", "dic", "=", "defaultdict", "(", "list", ")", "\n", "for", "landmark_path", "in", "file_list", ":", "\n", "    ", "filedir", ",", "filename", "=", "os", ".", "path", ".", "split", "(", "landmark_path", ")", "\n", "word", "=", "os", ".", "path", ".", "basename", "(", "filedir", ")", "\n", "videoname", "=", "'_'", ".", "join", "(", "filename", ".", "split", "(", "spliter", ")", "[", ":", "-", "1", "]", ")", "\n", "key", "=", "word", "+", "'/'", "+", "videoname", "\n", "dic", "[", "key", "]", ".", "append", "(", "landmark_path", ")", "\n", "\n", "\n", "", "for", "key", ",", "item", "in", "dic", ".", "iteritems", "(", ")", ":", "\n", "    ", "if", "len", "(", "item", ")", "<", "len_range", "[", "0", "]", "or", "len", "(", "item", ")", ">", "len_range", "[", "1", "]", ":", "\n", "      ", "continue", "\n", "\n", "", "item", ".", "sort", "(", "key", "=", "alphanum_key", ")", "\n", "count", "=", "0", "\n", "for", "landmark_file", "in", "item", ":", "\n", "      ", "with", "open", "(", "landmark_file", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "content", "=", "f", ".", "readlines", "(", ")", "\n", "", "landmarks", "=", "[", "]", "\n", "for", "line", "in", "content", ":", "\n", "        ", "line", ".", "rstrip", "(", "'\\n'", ")", "\n", "[", "x", ",", "y", "]", "=", "line", ".", "split", "(", "','", ")", "\n", "pos", "=", "np", ".", "array", "(", "[", "float", "(", "x", ")", ",", "float", "(", "y", ")", "]", ")", "\n", "landmarks", ".", "append", "(", "pos", ")", "\n", "", "landmarks", "=", "np", ".", "array", "(", "landmarks", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "\n", "# left eye avg position", "\n", "left_eye_pos", "=", "np", ".", "zeros", "(", "2", ")", "\n", "for", "ii", "in", "range", "(", "36", ",", "42", ")", ":", "\n", "        ", "left_eye_pos", "+=", "landmarks", "[", "ii", "]", "\n", "", "left_eye_pos", "=", "left_eye_pos", "/", "6", "\n", "\n", "# righ eye avg position", "\n", "right_eye_pos", "=", "np", ".", "zeros", "(", "2", ")", "\n", "for", "ii", "in", "range", "(", "42", ",", "48", ")", ":", "\n", "        ", "right_eye_pos", "+=", "landmarks", "[", "ii", "]", "\n", "", "right_eye_pos", "=", "right_eye_pos", "/", "6", "\n", "\n", "# nose avg position", "\n", "nose_pos_x", "=", "0", "\n", "for", "ii", "in", "range", "(", "30", ",", "31", ")", ":", "\n", "        ", "nose_pos_x", "+=", "landmarks", "[", "ii", "]", "[", "0", "]", "\n", "", "nose_pos_x", "=", "nose_pos_x", "/", "1", "\n", "\n", "# mouth avg position", "\n", "mouth_pos", "=", "np", ".", "zeros", "(", "2", ")", "\n", "for", "ii", "in", "range", "(", "48", ",", "61", ")", ":", "\n", "        ", "mouth_pos", "+=", "landmarks", "[", "ii", "]", "\n", "", "mouth_pos", "=", "mouth_pos", "/", "13", "\n", "\n", "# print(abs(left_eye_pos[0]+right_eye_pos[0]-2*nose_pos_x))", "\n", "if", "abs", "(", "left_eye_pos", "[", "0", "]", "+", "right_eye_pos", "[", "0", "]", "-", "2", "*", "nose_pos_x", ")", "<", "33", ":", "\n", "        ", "count", "+=", "1", "\n", "# filename = os.path.basename(landmark_file).replace('.txt', '.jpg')", "\n", "# src_path = os.path.join(gt_img_dir, sub_folder, filename)", "\n", "# dst_path = os.path.join('temp', filename)", "\n", "# shutil.copyfile(src_path, dst_path)", "\n", "", "", "if", "count", ">=", "len_range", "[", "0", "]", "and", "count", "<=", "len_range", "[", "1", "]", ":", "\n", "      ", "print", "(", "item", "[", "0", "]", ")", "\n", "for", "landmark_file", "in", "item", ":", "\n", "        ", "filename", "=", "os", ".", "path", ".", "basename", "(", "landmark_file", ")", ".", "replace", "(", "'.txt'", ",", "'.jpg'", ")", "\n", "input_img", "=", "os", ".", "path", ".", "join", "(", "input_img_dir", ",", "sub_folder", ",", "filename", ")", "\n", "gt_img", "=", "os", ".", "path", ".", "join", "(", "gt_img_dir", ",", "sub_folder", ",", "filename", ")", "\n", "input_audio", "=", "os", ".", "path", ".", "join", "(", "audio_dir", ",", "sub_folder", ",", "filename", ".", "replace", "(", "'.jpg'", ",", "'.mat'", ")", ")", "\n", "FILE_IN", ".", "write", "(", "input_img", "+", "' '", "+", "gt_img", "+", "' '", "+", "input_audio", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.trainList_LRW.tryint": [[23, 28], ["int"], "function", ["None"], ["    ", "\"\"\" Turn a string into a list of string and number chunks.\n        \"z23a\" -> [\"z\", 23, \"a\"]\n    \"\"\"", "\n", "return", "[", "tryint", "(", "c", ")", "for", "c", "in", "re", ".", "split", "(", "'([0-9]+)'", ",", "s", ")", "]", "\n", "\n", "", "def", "listdir_nohidden", "(", "path", ")", ":", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.trainList_LRW.alphanum_key": [[29, 34], ["trainList_LRW.tryint", "re.split"], "function", ["home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.trainList_LRW.tryint"], ["  ", "file_list", "=", "[", "]", "\n", "for", "f", "in", "os", ".", "listdir", "(", "path", ")", ":", "\n", "    ", "if", "not", "f", ".", "startswith", "(", "'.'", ")", ":", "\n", "      ", "file_list", ".", "append", "(", "f", ")", "\n", "", "", "return", "file_list", "\n", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.trainList_LRW.listdir_nohidden": [[35, 41], ["os.listdir", "f.startswith", "file_list.append"], "function", ["None"], ["\n", "\n", "", "if", "__name__", "==", "'__main__'", ":", "\n", "  ", "r_file", "=", "open", "(", "orig_training_file", ",", "'r'", ")", "\n", "w_file", "=", "open", "(", "new_training_file", ",", "'w'", ")", "\n", "\n", "training_list", "=", "r_file", ".", "readlines", "(", ")", "# image_path audio_path", "\n"]], "home.repos.pwc.inspect_result.susanqq_Talking_Face_Generation.preprocess_LRW.trainList_LRW.get_training_list": [[42, 48], ["list", "random.shuffle", "int", "set", "len", "[].split", "filename.split"], "function", ["None"], ["\n", "word_dict", "=", "{", "}", "\n", "\n", "for", "line", "in", "training_list", ":", "\n", "\n", "    ", "line", "=", "line", ".", "rstrip", "(", "'\\n'", ")", "\n", "\n"]]}