{"home.repos.pwc.inspect_result.shaanchandra_SAFER.code.lr_train.LR_model.__init__": [[40, 53], ["super().__init__", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "LR_model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "text_dim", "=", "1024", "if", "config", "[", "'text_encoder'", "]", "==", "'roberta'", "else", "384", "\n", "self", ".", "graph_dim", "=", "config", "[", "'graph_dim'", "]", "\n", "\n", "if", "config", "[", "'mode'", "]", "==", "'gnn'", ":", "\n", "            ", "self", ".", "in_dim", "=", "self", ".", "graph_dim", "*", "3", "if", "config", "[", "'model_name'", "]", "in", "[", "'gat'", ",", "'rgat'", "]", "else", "self", ".", "graph_dim", "\n", "", "elif", "config", "[", "'mode'", "]", "==", "'text'", ":", "\n", "            ", "self", ".", "in_dim", "=", "self", ".", "text_dim", "\n", "", "else", ":", "\n", "            ", "self", ".", "in_dim", "=", "self", ".", "graph_dim", "*", "3", "if", "config", "[", "'model_name'", "]", "==", "'gat'", "else", "self", ".", "graph_dim", "\n", "self", ".", "in_dim", "+=", "self", ".", "text_dim", "\n", "", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "self", ".", "in_dim", ",", "config", "[", "'n_classes'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.code.lr_train.LR_model.forward": [[55, 58], ["lr_train.LR_model.classifier"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "out", "=", "self", ".", "classifier", "(", "x", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.code.lr_train.LR_Learner.__init__": [[61, 83], ["time.time", "lr_train.LR_model", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "torch.optim.AdamW", "torch.optim.AdamW", "torch.optim.AdamW", "torch.optim.AdamW", "torch.optim.AdamW", "torch.optim.AdamW", "torch.optim.AdamW", "torch.optim.AdamW", "torch.optim.AdamW", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.BCELoss", "torch.BCELoss", "torch.BCELoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "lr_train.LR_Learner.model.parameters", "lr_train.LR_Learner.model.parameters"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "self", ".", "best_val_acc", ",", "self", ".", "best_val_f1", ",", "self", ".", "best_val_recall", ",", "self", ".", "best_val_precision", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "self", ".", "actual_best_f1", "=", "0", "\n", "self", ".", "preds_list", ",", "self", ".", "labels_list", "=", "[", "]", ",", "[", "]", "\n", "self", ".", "train_f1", ",", "self", ".", "train_precision", ",", "self", ".", "train_recall", ",", "self", ".", "train_accuracy", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "self", ".", "train_loss", "=", "[", "]", "\n", "self", ".", "threshold", "=", "0", "\n", "self", ".", "prev_val_loss", ",", "self", ".", "not_improved", "=", "0", ",", "0", "\n", "self", ".", "best_val_loss", "=", "1e4", "\n", "self", ".", "total_iters", "=", "0", "\n", "self", ".", "terminate_training", "=", "False", "\n", "self", ".", "start_epoch", ",", "self", ".", "iters", "=", "1", ",", "0", "\n", "self", ".", "preds", ",", "self", ".", "loss", "=", "0", ",", "0", "\n", "self", ".", "start", "=", "time", ".", "time", "(", ")", "\n", "\n", "self", ".", "model", "=", "LR_model", "(", "config", ")", "\n", "if", "config", "[", "'optimizer'", "]", "==", "'Adam'", ":", "\n", "            ", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "AdamW", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "lr", "=", "config", "[", "'lr'", "]", ",", "weight_decay", "=", "config", "[", "'weight_decay'", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "lr", "=", "config", "[", "'lr'", "]", ",", "momentum", "=", "config", "[", "'momentum'", "]", ",", "weight_decay", "=", "config", "[", "'weight_decay'", "]", ")", "\n", "", "self", ".", "scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "StepLR", "(", "self", ".", "optimizer", ",", "step_size", "=", "config", "[", "'lr_decay_step'", "]", ",", "gamma", "=", "config", "[", "'lr_decay_factor'", "]", ")", "\n", "self", ".", "criterion", "=", "nn", ".", "BCELoss", "(", ")", "if", "config", "[", "'loss_func'", "]", "==", "'bce'", "else", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.code.lr_train.LR_Learner.eval_lr": [[86, 132], ["lr_train.LR_Learner.model.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "lr_train.LR_Learner.model", "torch.autograd.Variable.float", "torch.autograd.Variable.float", "torch.autograd.Variable.float", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "lr_train.LR_Learner.criterion", "eval_loss.append", "preds_list.append", "labels_list.append", "docs_list.append", "sum", "len", "print", "utils.utils.evaluation_measures", "utils.utils.evaluation_measures", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.argmax.to", "torch.argmax.to", "torch.argmax.to", "torch.autograd.Variable.float.to", "lr_train.LR_Learner.detach().item", "torch.softmax", "torch.softmax", "torch.softmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax.cpu().detach().numpy", "torch.argmax.cpu().detach().numpy", "torch.argmax.cpu().detach().numpy", "torch.autograd.Variable.float.cpu().detach().numpy", "sklearn.metrics.classification_report", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "lr_train.LR_Learner.detach", "torch.argmax.cpu().detach", "torch.argmax.cpu().detach", "torch.argmax.cpu().detach", "torch.autograd.Variable.float.cpu().detach", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.argmax.cpu", "torch.argmax.cpu", "torch.argmax.cpu", "torch.autograd.Variable.float.cpu"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.utils.evaluation_measures", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.utils.evaluation_measures"], ["", "def", "eval_lr", "(", "self", ",", "test", "=", "False", ")", ":", "\n", "        ", "self", ".", "model", ".", "eval", "(", ")", "\n", "preds_list", ",", "labels_list", "=", "[", "]", ",", "[", "]", "\n", "docs_list", "=", "[", "]", "\n", "eval_loss", "=", "[", "]", "\n", "\n", "loader", "=", "test_loader", "if", "test", "else", "val_loader", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "iters", ",", "(", "batch_x", ",", "batch_y", ",", "doc", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "                ", "batch_x", "=", "Variable", "(", "batch_x", ")", "\n", "batch_y", "=", "Variable", "(", "batch_y", ")", "\n", "preds", "=", "self", ".", "model", "(", "batch_x", ")", "\n", "if", "config", "[", "'loss_func'", "]", "==", "'bce'", ":", "\n", "                    ", "preds", "=", "F", ".", "sigmoid", "(", "preds", ")", "\n", "", "labels", "=", "batch_y", ".", "float", "(", ")", "\n", "# labels = batch_y.float()", "\n", "preds", "=", "torch", ".", "where", "(", "torch", ".", "isnan", "(", "preds", ")", ",", "torch", ".", "zeros_like", "(", "preds", ")", ",", "preds", ")", "\n", "loss", "=", "self", ".", "criterion", "(", "preds", ".", "to", "(", "device", ")", ",", "labels", ".", "to", "(", "device", ")", ")", "\n", "eval_loss", ".", "append", "(", "loss", ".", "detach", "(", ")", ".", "item", "(", ")", ")", "\n", "if", "config", "[", "'loss_func'", "]", "==", "'ce'", ":", "\n", "                    ", "preds", "=", "F", ".", "softmax", "(", "preds", ",", "dim", "=", "1", ")", "\n", "preds", "=", "torch", ".", "argmax", "(", "preds", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "                    ", "preds", "=", "(", "F", ".", "sigmoid", "(", "preds", ")", ">", "0.5", ")", ".", "type", "(", "torch", ".", "FloatTensor", ")", "if", "config", "[", "'loss_func'", "]", "==", "'bce_logits'", "else", "(", "preds", ">", "0.5", ")", ".", "type", "(", "torch", ".", "FloatTensor", ")", "\n", "", "preds_list", ".", "append", "(", "preds", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "labels_list", ".", "append", "(", "labels", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "docs_list", ".", "append", "(", "doc", ")", "\n", "\n", "\n", "", "preds_list", "=", "[", "pred", "for", "batch_pred", "in", "preds_list", "for", "pred", "in", "batch_pred", "]", "\n", "labels_list", "=", "[", "label", "for", "batch_labels", "in", "labels_list", "for", "label", "in", "batch_labels", "]", "\n", "docs_list", "=", "[", "doc", "for", "docs", "in", "docs_list", "for", "doc", "in", "docs", "]", "\n", "# if test:", "\n", "#     self.save_correct_preds(docs_list, preds_list, labels_list)", "\n", "eval_loss", "=", "sum", "(", "eval_loss", ")", "/", "len", "(", "eval_loss", ")", "\n", "\n", "if", "test", ":", "\n", "                ", "print", "(", "classification_report", "(", "np", ".", "array", "(", "labels_list", ")", ",", "np", ".", "array", "(", "preds_list", ")", ")", ")", "\n", "\n", "", "if", "not", "test", ":", "\n", "                ", "eval_f1", ",", "eval_macro_f1", ",", "eval_recall", ",", "eval_precision", ",", "eval_accuracy", "=", "evaluation_measures", "(", "config", ",", "np", ".", "array", "(", "preds_list", ")", ",", "np", ".", "array", "(", "labels_list", ")", ")", "\n", "return", "eval_f1", ",", "eval_macro_f1", ",", "eval_precision", ",", "eval_recall", ",", "eval_accuracy", ",", "eval_loss", "\n", "\n", "", "else", ":", "\n", "                ", "eval_f1", ",", "eval_macro_f1", ",", "eval_recall", ",", "eval_precision", ",", "eval_accuracy", "=", "evaluation_measures", "(", "config", ",", "np", ".", "array", "(", "preds_list", ")", ",", "np", ".", "array", "(", "labels_list", ")", ")", "\n", "return", "eval_f1", ",", "eval_macro_f1", ",", "eval_precision", ",", "eval_recall", ",", "eval_accuracy", ",", "eval_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.code.lr_train.LR_Learner.save_correct_preds": [[138, 151], ["print", "print", "os.path.join", "print", "len", "len", "len", "len", "list", "open", "json.dump", "range", "range", "len", "len"], "methods", ["None"], ["", "", "", "def", "save_correct_preds", "(", "self", ",", "docs", ",", "preds", ",", "labels", ")", ":", "\n", "        ", "print", "(", "len", "(", "preds", ")", ",", "len", "(", "labels", ")", ",", "len", "(", "docs", ")", ")", "\n", "idx_correct_preds", "=", "[", "idx", "for", "idx", "in", "range", "(", "len", "(", "preds", ")", ")", "if", "preds", "[", "idx", "]", "==", "labels", "[", "idx", "]", "]", "\n", "correct_pred_docs", "=", "[", "docs", "[", "idx", "]", "for", "idx", "in", "range", "(", "len", "(", "docs", ")", ")", "if", "idx", "in", "idx_correct_preds", "]", "\n", "print", "(", "len", "(", "correct_pred_docs", ")", ")", "\n", "temp_dict", "=", "{", "'correct_preds'", ":", "list", "(", "correct_pred_docs", ")", "}", "\n", "model", "=", "'roberta'", "if", "config", "[", "'mode'", "]", "==", "'text'", "else", "config", "[", "'model_name'", "]", "\n", "dataname", "=", "config", "[", "'data_name'", "]", "\n", "correct_doc_file", "=", "os", ".", "path", ".", "join", "(", "'data'", ",", "'complete_data'", ",", "dataname", ",", "'cached_embeds'", ",", "'correct_docs_{}.json'", ".", "format", "(", "model", ")", ")", "\n", "\n", "print", "(", "\"Saving the list of correct test preds in : \"", ",", "correct_doc_file", ")", "\n", "with", "open", "(", "correct_doc_file", ",", "'w+'", ")", "as", "j", ":", "\n", "            ", "json", ".", "dump", "(", "temp_dict", ",", "j", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.code.lr_train.LR_Learner.train_epoch_step": [[153, 230], ["lr_train.LR_Learner.model.train", "utils.utils.evaluation_measures", "lr_train.LR_Learner.eval_lr", "utils.utils.print_stats", "lr_train.LR_Learner.scheduler.step", "lr_train.LR_Learner.scheduler.get_lr", "numpy.array", "numpy.array", "print", "lr_train.LR_Learner.model.state_dict", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "print", "lr_train.LR_Learner.model.state_dict", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "print", "os.path.join", "os.path.join", "lr_train.LR_Learner.optimizer.state_dict", "lr_train.LR_Learner.optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.utils.evaluation_measures", "home.repos.pwc.inspect_result.shaanchandra_SAFER.code.lr_train.LR_Learner.eval_lr", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.utils.print_stats", "home.repos.pwc.inspect_result.shaanchandra_SAFER.optimizers.radam.RiemannianAdam.step"], ["", "", "def", "train_epoch_step", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", ".", "train", "(", ")", "\n", "lr", "=", "self", ".", "scheduler", ".", "get_lr", "(", ")", "[", "0", "]", "\n", "self", ".", "total_iters", "+=", "self", ".", "iters", "\n", "self", ".", "preds_list", "=", "[", "pred", "for", "batch_pred", "in", "self", ".", "preds_list", "for", "pred", "in", "batch_pred", "]", "\n", "self", ".", "labels_list", "=", "[", "label", "for", "batch_labels", "in", "self", ".", "labels_list", "for", "label", "in", "batch_labels", "]", "\n", "\n", "self", ".", "train_f1", ",", "self", ".", "train_macro_f1", ",", "self", ".", "train_recall", ",", "self", ".", "train_precision", ",", "self", ".", "train_accuracy", "=", "evaluation_measures", "(", "config", ",", "np", ".", "array", "(", "self", ".", "preds_list", ")", ",", "np", ".", "array", "(", "self", ".", "labels_list", ")", ")", "\n", "\n", "# Evaluate on dev set", "\n", "self", ".", "eval_f1", ",", "self", ".", "eval_macro_f1", ",", "self", ".", "eval_precision", ",", "self", ".", "eval_recall", ",", "self", ".", "eval_accuracy", ",", "self", ".", "eval_loss", "=", "self", ".", "eval_lr", "(", ")", "\n", "\n", "# print stats", "\n", "print_stats", "(", "config", ",", "self", ".", "epoch", ",", "self", ".", "train_loss", ",", "self", ".", "train_accuracy", ",", "self", ".", "train_f1", ",", "self", ".", "train_precision", ",", "self", ".", "train_recall", ",", "\n", "self", ".", "eval_loss", ",", "self", ".", "eval_accuracy", ",", "self", ".", "eval_f1", ",", "self", ".", "eval_precision", ",", "self", ".", "eval_recall", ",", "self", ".", "start", ",", "lr", ")", "\n", "\n", "\n", "if", "self", ".", "eval_f1", ">", "self", ".", "best_val_f1", ":", "\n", "            ", "print", "(", "\"New High Score! Saving model...\"", ")", "\n", "self", ".", "best_val_f1", "=", "self", ".", "eval_f1", "\n", "self", ".", "best_val_acc", "=", "self", ".", "eval_accuracy", "\n", "self", ".", "best_val_recall", "=", "self", ".", "eval_recall", "\n", "self", ".", "best_val_precision", "=", "self", ".", "eval_precision", "\n", "best_model", "=", "self", ".", "model", ".", "state_dict", "(", ")", "\n", "# Save the state and the vocabulary", "\n", "torch", ".", "save", "(", "{", "\n", "'epoch'", ":", "self", ".", "epoch", ",", "\n", "'best_val_f1'", ":", "self", ".", "best_val_f1", ",", "\n", "'model_state_dict'", ":", "best_model", ",", "\n", "# 'model_classif_state_dict': model.classifier.state_dict(),", "\n", "'optimizer_state_dict'", ":", "self", ".", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "}", ",", "os", ".", "path", ".", "join", "(", "model_path", ",", "config", "[", "'model_save_name'", "]", ")", ")", "\n", "\n", "\n", "", "if", "self", ".", "epoch", "==", "1", ":", "\n", "            ", "print", "(", "\"Saving model !\"", ")", "\n", "best_model", "=", "self", ".", "model", ".", "state_dict", "(", ")", "\n", "# Save the state and the vocabulary", "\n", "torch", ".", "save", "(", "{", "\n", "'epoch'", ":", "self", ".", "epoch", ",", "\n", "'best_val_f1'", ":", "self", ".", "best_val_f1", ",", "\n", "'model_state_dict'", ":", "best_model", ",", "\n", "# 'model_classif_state_dict': model.classifier.state_dict(),", "\n", "'optimizer_state_dict'", ":", "self", ".", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "}", ",", "os", ".", "path", ".", "join", "(", "model_path", ",", "config", "[", "'model_save_name'", "]", ")", ")", "\n", "\n", "\n", "", "self", ".", "scheduler", ".", "step", "(", ")", "\n", "\n", "\n", "if", "self", ".", "eval_f1", "-", "self", ".", "best_val_f1", "!=", "0", "and", "self", ".", "eval_f1", "-", "self", ".", "best_val_f1", "<", "1e-3", ":", "\n", "            ", "self", ".", "not_improved", "+=", "1", "\n", "print", "(", "self", ".", "not_improved", ")", "\n", "if", "self", ".", "not_improved", ">=", "config", "[", "'patience'", "]", ":", "\n", "                ", "self", ".", "terminate_training", "=", "True", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "not_improved", "=", "0", "\n", "\n", "", "if", "self", ".", "eval_f1", ">", "self", ".", "best_val_f1", "and", "self", ".", "eval_f1", "-", "self", ".", "best_val_f1", ">", "1e-3", ":", "\n", "            ", "self", ".", "best_val_f1", "=", "self", ".", "eval_f1", "\n", "self", ".", "not_improved", "=", "0", "\n", "\n", "# if self.best_val_loss - self.eval_loss < 1e-3:", "\n", "#     self.not_improved+=1", "\n", "#     print(self.not_improved)", "\n", "#     if self.not_improved >= config['patience']:", "\n", "#         self.terminate_training= True", "\n", "# else:", "\n", "#     self.not_improved = 0", "\n", "\n", "# if self.eval_loss < self.best_val_loss and self.best_val_loss - self.eval_loss > 1e-3:", "\n", "#     self.best_val_loss = self.eval_loss", "\n", "#     self.not_improved = 0", "\n", "\n", "\n", "", "self", ".", "preds_list", "=", "[", "]", "\n", "self", ".", "labels_list", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.code.lr_train.LR_Learner.train_iters_step": [[234, 259], ["lr_train.LR_Learner.batch_y.float", "lr_train.LR_Learner.criterion", "lr_train.LR_Learner.optimizer.zero_grad", "lr_train.LR_Learner.loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "lr_train.LR_Learner.optimizer.step", "lr_train.LR_Learner.preds_list.append", "lr_train.LR_Learner.labels_list.append", "lr_train.LR_Learner.train_loss.append", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "lr_train.LR_Learner.preds.to", "lr_train.LR_Learner.batch_labels.to", "lr_train.LR_Learner.model.parameters", "torch.softmax", "torch.softmax", "torch.softmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "lr_train.LR_Learner.preds.cpu().detach().numpy", "lr_train.LR_Learner.batch_labels.cpu().detach().numpy", "lr_train.LR_Learner.loss.detach().item", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "lr_train.LR_Learner.preds.cpu().detach", "lr_train.LR_Learner.batch_labels.cpu().detach", "lr_train.LR_Learner.loss.detach", "lr_train.LR_Learner.preds.cpu", "lr_train.LR_Learner.batch_labels.cpu"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.att_layers.SpecialSpmmFunction.backward", "home.repos.pwc.inspect_result.shaanchandra_SAFER.optimizers.radam.RiemannianAdam.step"], ["", "def", "train_iters_step", "(", "self", ")", ":", "\n", "        ", "if", "config", "[", "'loss_func'", "]", "==", "'bce'", ":", "\n", "            ", "self", ".", "preds", "=", "F", ".", "sigmoid", "(", "self", ".", "preds", ")", "\n", "\n", "", "self", ".", "batch_labels", "=", "self", ".", "batch_y", ".", "float", "(", ")", "\n", "self", ".", "loss", "=", "self", ".", "criterion", "(", "self", ".", "preds", ".", "to", "(", "device", ")", ",", "self", ".", "batch_labels", ".", "to", "(", "device", ")", ")", "\n", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "self", ".", "loss", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "5", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "config", "[", "'loss_func'", "]", "==", "'ce'", ":", "\n", "            ", "self", ".", "preds", "=", "F", ".", "softmax", "(", "self", ".", "preds", ",", "dim", "=", "1", ")", "\n", "self", ".", "preds", "=", "torch", ".", "argmax", "(", "self", ".", "preds", ",", "dim", "=", "1", ")", "\n", "", "elif", "config", "[", "'loss_func'", "]", "==", "'bce'", ":", "\n", "            ", "self", ".", "preds", "=", "(", "self", ".", "preds", ">", "0.5", ")", ".", "type", "(", "torch", ".", "FloatTensor", ")", "\n", "", "elif", "config", "[", "'loss_func'", "]", "==", "'bce_logits'", ":", "\n", "            ", "self", ".", "preds", "=", "F", ".", "sigmoid", "(", "self", ".", "preds", ")", "\n", "self", ".", "preds", "=", "(", "self", ".", "preds", ">", "self", ".", "threshold", ")", ".", "type", "(", "torch", ".", "FloatTensor", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "", "self", ".", "preds_list", ".", "append", "(", "self", ".", "preds", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "self", ".", "labels_list", ".", "append", "(", "self", ".", "batch_labels", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "self", ".", "train_loss", ".", "append", "(", "self", ".", "loss", ".", "detach", "(", ")", ".", "item", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.code.lr_train.LR_Learner.train_main": [[262, 307], ["print", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "numpy.random.seed", "random.seed", "print", "range", "print", "os.path.isfile", "lr_train.LR_Learner.eval_lr", "utils.utils.print_test_stats", "enumerate", "lr_train.LR_Learner.train_epoch_step", "print", "print", "os.path.join", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "lr_train.LR_Learner.model.load_state_dict", "lr_train.LR_Learner.optimizer.load_state_dict", "ValueError", "datetime.datetime.now", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "lr_train.LR_Learner.model", "lr_train.LR_Learner.train_iters_step", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.code.lr_train.LR_Learner.eval_lr", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.utils.print_test_stats", "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.train_epoch_step", "home.repos.pwc.inspect_result.shaanchandra_SAFER.code.lr_train.LR_Learner.train_iters_step"], ["", "def", "train_main", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"\\n\\n\"", "+", "\"=\"", "*", "100", "+", "\"\\n\\t\\t\\t\\t\\t Training Network\\n\"", "+", "\"=\"", "*", "100", ")", "\n", "\n", "# Seeds for reproduceable runs", "\n", "torch", ".", "manual_seed", "(", "config", "[", "'seed'", "]", ")", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "config", "[", "'seed'", "]", ")", "\n", "np", ".", "random", ".", "seed", "(", "config", "[", "'seed'", "]", ")", "\n", "random", ".", "seed", "(", "config", "[", "'seed'", "]", ")", "\n", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "\n", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "False", "\n", "\n", "print", "(", "\"\\nBeginning training at:  {} \\n\"", ".", "format", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", ")", "\n", "\n", "for", "self", ".", "epoch", "in", "range", "(", "1", ",", "config", "[", "'max_epoch'", "]", "+", "1", ")", ":", "\n", "            ", "for", "self", ".", "iters", ",", "(", "batch_x", ",", "batch_y", ",", "doc", ")", "in", "enumerate", "(", "train_loader", ")", ":", "\n", "                ", "self", ".", "batch_x", "=", "Variable", "(", "batch_x", ")", "\n", "self", ".", "batch_y", "=", "Variable", "(", "batch_y", ")", "\n", "self", ".", "preds", "=", "self", ".", "model", "(", "self", ".", "batch_x", ")", "\n", "# print(self.preds)", "\n", "self", ".", "train_iters_step", "(", ")", "\n", "", "self", ".", "train_epoch_step", "(", ")", "\n", "if", "self", ".", "terminate_training", ":", "\n", "                    ", "break", "\n", "\n", "# Termination message", "\n", "", "", "if", "self", ".", "terminate_training", ":", "\n", "            ", "print", "(", "\"\\n\"", "+", "\"-\"", "*", "100", "+", "\"\\nTraining terminated early because the Validation loss did not improve for   {}   epochs\"", ".", "format", "(", "config", "[", "'patience'", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"\\n\"", "+", "\"-\"", "*", "100", "+", "\"\\nMaximum epochs reached. Finished training !!\"", ")", "\n", "\n", "", "print", "(", "\"\\n\"", "+", "\"-\"", "*", "50", "+", "\"\\n\\t\\tEvaluating on test set\\n\"", "+", "\"-\"", "*", "50", ")", "\n", "\n", "if", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "model_path", ",", "config", "[", "'model_save_name'", "]", ")", ")", ":", "\n", "            ", "checkpoint", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "model_path", ",", "config", "[", "'model_save_name'", "]", ")", ")", "\n", "self", ".", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'model_state_dict'", "]", ")", "\n", "self", ".", "optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'optimizer_state_dict'", "]", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"No Saved model state_dict found for the chosen model...!!! \\nAborting evaluation on test set...\"", ".", "format", "(", "config", "[", "'model_name'", "]", ")", ")", "\n", "\n", "\n", "# Evaluate on dev set", "\n", "", "test_f1", ",", "test_macro_f1", ",", "test_precision", ",", "test_recall", ",", "test_accuracy", ",", "test_loss", "=", "self", ".", "eval_lr", "(", "test", "=", "True", ")", "\n", "\n", "print_test_stats", "(", "test_accuracy", ",", "test_precision", ",", "test_recall", ",", "test_f1", ",", "test_macro_f1", ",", "self", ".", "best_val_acc", ",", "self", ".", "best_val_precision", ",", "self", ".", "best_val_recall", ",", "self", ".", "best_val_f1", ")", "\n", "return", "test_f1", ",", "test_macro_f1", ",", "test_accuracy", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_gnn.Prepare_GNN_Dataset.__init__": [[24, 28], ["super().__init__", "data_utils_gnn.Prepare_GNN_Dataset.read_files"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_gnn.Prepare_GNN_Dataset.read_files"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "Prepare_GNN_Dataset", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "read_files", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_gnn.Prepare_GNN_Dataset.read_files": [[30, 115], ["time.time", "numpy.load", "torch.from_numpy().long", "scipy.sparse.load_npz", "torch.from_numpy", "json.load", "torch.LongTensor", "json.load", "json.load", "print", "print", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "data_utils_gnn.Prepare_GNN_Dataset.x_data.toarray", "open", "open", "numpy.load", "torch.from_numpy().long", "open", "torch_geometric.utils.contains_isolated_nodes", "data_utils_gnn.Prepare_GNN_Dataset.", "print", "print", "print", "print", "print", "print", "print", "print", "time.time", "calc_elapsed_time", "print", "os.path.join", "os.path.join", "torch.from_numpy", "print", "print", "data_utils_gnn.Prepare_GNN_Dataset.data.train_mask.sum().item", "data_utils_gnn.Prepare_GNN_Dataset.data.val_mask.sum().item", "torch.from_numpy", "data_utils_gnn.Prepare_GNN_Dataset.data.val_mask.sum().item", "data_utils_gnn.Prepare_GNN_Dataset.data.train_mask.sum", "data_utils_gnn.Prepare_GNN_Dataset.data.val_mask.sum", "data_utils_gnn.Prepare_GNN_Dataset.data.train_mask.sum().item", "data_utils_gnn.Prepare_GNN_Dataset.data.val_mask.sum", "data_utils_gnn.Prepare_GNN_Dataset.data.train_mask.sum"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.calc_elapsed_time"], ["", "def", "read_files", "(", "self", ",", "verbose", "=", "True", ")", ":", "\n", "        ", "start", "=", "time", ".", "time", "(", ")", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "\"=\"", "*", "100", "+", "\"\\n\\t\\t\\t\\t Preparing Data for {}\\n\"", ".", "format", "(", "self", ".", "config", "[", "'data_name'", "]", ")", "+", "\"=\"", "*", "100", ")", "\n", "print", "(", "\"\\n\\n==>> Loading feature matrix and adj matrix....\"", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "'data_name'", "]", "in", "[", "'gossipcop'", ",", "'politifact'", "]", ":", "\n", "            ", "x_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "config", "[", "'data_path'", "]", ",", "self", ".", "config", "[", "'data_name'", "]", ",", "'feat_matrix_lr_train_30_5.npz'", ".", "format", "(", "self", ".", "config", "[", "'data_name'", "]", ")", ")", "\n", "y_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "config", "[", "'data_path'", "]", ",", "self", ".", "config", "[", "'data_name'", "]", ",", "'all_labels_lr_train_30_5.json'", ".", "format", "(", "self", ".", "config", "[", "'data_name'", "]", ")", ")", "\n", "# adj_name = 'adj_matrix_lr_train_30_5_edge.npy'.format(self.config['data_name']) if self.config['model_name'] != 'HGCN' else 'adj_matrix_lr_train_30_5.npz'.format(self.config['data_name'])", "\n", "adj_name", "=", "'adj_matrix_lr_train_30_5_edge.npy'", ".", "format", "(", "self", ".", "config", "[", "'data_name'", "]", ")", "\n", "edge_index_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "config", "[", "'data_path'", "]", ",", "self", ".", "config", "[", "'data_name'", "]", ",", "adj_name", ")", "\n", "node2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "config", "[", "'data_path'", "]", ",", "self", ".", "config", "[", "'data_name'", "]", ",", "'node2id_lr_train_30_5.json'", ".", "format", "(", "self", ".", "config", "[", "'data_name'", "]", ")", ")", "\n", "node_type_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "config", "[", "'data_path'", "]", ",", "self", ".", "config", "[", "'data_name'", "]", ",", "'node_type_lr_train_30_5.npy'", ".", "format", "(", "self", ".", "config", "[", "'data_name'", "]", ")", ")", "\n", "split_mask_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "config", "[", "'data_path'", "]", ",", "self", ".", "config", "[", "'data_name'", "]", ",", "'split_mask_lr_30_5.json'", ")", "\n", "if", "self", ".", "config", "[", "'model_name'", "]", "in", "[", "'rgcn'", ",", "'rgat'", ",", "'rsage'", "]", ":", "\n", "                ", "edge_type_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "config", "[", "'data_path'", "]", ",", "self", ".", "config", "[", "'data_name'", "]", ",", "'edge_type_lr_train_30_5_edge.npy'", ".", "format", "(", "self", ".", "config", "[", "'data_name'", "]", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "x_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "config", "[", "'data_path'", "]", ",", "self", ".", "config", "[", "'data_name'", "]", ",", "'feat_matrix_lr_top10_train.npz'", ")", "\n", "y_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "config", "[", "'data_path'", "]", ",", "self", ".", "config", "[", "'data_name'", "]", ",", "'all_labels_lr_top10_train.json'", ")", "\n", "# adj_name = 'adj_matrix_lr_top10_train_edge.npy' if self.config['model_name'] != 'HGCN' else 'adj_matrix_lr_top10_train.npz'", "\n", "adj_name", "=", "'adj_matrix_lr_top10_train_edge.npy'", "\n", "edge_index_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "config", "[", "'data_path'", "]", ",", "self", ".", "config", "[", "'data_name'", "]", ",", "adj_name", ")", "\n", "node2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "config", "[", "'data_path'", "]", ",", "self", ".", "config", "[", "'data_name'", "]", ",", "'node2id_lr_top10_train.json'", ")", "\n", "node_type_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "config", "[", "'data_path'", "]", ",", "self", ".", "config", "[", "'data_name'", "]", ",", "'node_type_lr_top10_train.npy'", ")", "\n", "split_mask_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "config", "[", "'data_path'", "]", ",", "self", ".", "config", "[", "'data_name'", "]", ",", "'split_mask_top10.json'", ")", "\n", "if", "self", ".", "config", "[", "'model_name'", "]", "in", "[", "'rgcn'", ",", "'rgat'", ",", "'rsage'", "]", ":", "\n", "                ", "edge_type_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "config", "[", "'data_path'", "]", ",", "self", ".", "config", "[", "'data_name'", "]", ",", "'edge_type_lr_top10_edge.npy'", ")", "\n", "\n", "\n", "# if self.config['model_name'] != 'HGCN':", "\n", "#     edge_index_data = np.load(edge_index_file)", "\n", "#     edge_index_data = torch.from_numpy(edge_index_data).long()", "\n", "\n", "# elif self.config['model_name'] == 'HGCN':", "\n", "#     edge_index_data = load_npz(edge_index_file)", "\n", "\n", "#     # edge_index_data = torch.from_numpy(edge_index_data.toarray())", "\n", "\n", "#     edge_index_data = edge_index_data.tocoo()", "\n", "#     indices = torch.from_numpy(np.vstack((edge_index_data.row, edge_index_data.col)).astype(np.int64))", "\n", "#     values = torch.Tensor(edge_index_data.data)", "\n", "#     shape = torch.Size(edge_index_data.shape)", "\n", "#     edge_index_data = torch.sparse.FloatTensor(indices, values, shape)", "\n", "\n", "", "", "self", ".", "edge_index_data", "=", "np", ".", "load", "(", "edge_index_file", ")", "\n", "self", ".", "edge_index_data", "=", "torch", ".", "from_numpy", "(", "edge_index_data", ")", ".", "long", "(", ")", "\n", "\n", "self", ".", "x_data", "=", "load_npz", "(", "x_file", ")", "\n", "self", ".", "x_data", "=", "torch", ".", "from_numpy", "(", "self", ".", "x_data", ".", "toarray", "(", ")", ")", "\n", "self", ".", "y_data", "=", "json", ".", "load", "(", "open", "(", "y_file", ",", "'r'", ")", ")", "\n", "self", ".", "y_data", "=", "torch", ".", "LongTensor", "(", "self", ".", "y_data", "[", "'all_labels'", "]", ")", "\n", "self", ".", "node2id", "=", "json", ".", "load", "(", "open", "(", "node2id_file", ",", "'r'", ")", ")", "\n", "# node_type = np.load(node_type_file)", "\n", "# node_type = torch.from_numpy(node_type).float()", "\n", "if", "self", ".", "config", "[", "'model_name'", "]", "in", "[", "'rgcn'", ",", "'rgat'", ",", "'rsage'", "]", ":", "\n", "            ", "self", ".", "edge_type_data", "=", "np", ".", "load", "(", "edge_type_file", ")", "\n", "self", ".", "edge_type_data", "=", "torch", ".", "from_numpy", "(", "self", ".", "edge_type_data", ")", ".", "long", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "edge_type_data", "=", "None", "\n", "\n", "", "self", ".", "split_masks", "=", "json", ".", "load", "(", "open", "(", "split_mask_file", ",", "'r'", ")", ")", "\n", "\n", "num_nodes", ",", "self", ".", "vocab_size", "=", "self", ".", "x_data", ".", "shape", "\n", "if", "self", ".", "config", "[", "'model_name'", "]", "!=", "'HGCN'", ":", "\n", "            ", "isolated_nodes", "=", "contains_isolated_nodes", "(", "edge_index", "=", "self", ".", "edge_index_data", ")", "\n", "self_loops", "=", "contains_self_loops", "(", "edge_index", "=", "self", ".", "edge_index_data", ")", "\n", "\n", "", "if", "verbose", ":", "\n", "            ", "print", "(", "\"\\n\\n\"", "+", "\"-\"", "*", "50", "+", "\"\\nDATA STATISTICS:\\n\"", "+", "\"-\"", "*", "50", ")", "\n", "if", "self", ".", "config", "[", "'model_name'", "]", "!=", "'HGCN'", ":", "\n", "                ", "print", "(", "\"Contains isolated nodes = \"", ",", "isolated_nodes", ")", "\n", "print", "(", "\"Contains self loops = \"", ",", "self_loops", ")", "\n", "", "print", "(", "\"Vocabulary size = \"", ",", "self", ".", "vocab_size", ")", "\n", "print", "(", "'No. of nodes in graph = '", ",", "num_nodes", ")", "\n", "print", "(", "'No. of nodes after removing isolated nodes = '", ",", "new_num_nodes", ")", "\n", "print", "(", "\"No. of edges in graph = \"", ",", "self", ".", "data", ".", "num_edges", ")", "\n", "\n", "print", "(", "\"\\nNo.of train instances = \"", ",", "self", ".", "data", ".", "train_mask", ".", "sum", "(", ")", ".", "item", "(", ")", ")", "\n", "print", "(", "\"No.of val instances = \"", ",", "self", ".", "data", ".", "val_mask", ".", "sum", "(", ")", ".", "item", "(", ")", ")", "\n", "print", "(", "\"No.of test instances = \"", ",", "num_nodes", "-", "self", ".", "data", ".", "train_mask", ".", "sum", "(", ")", ".", "item", "(", ")", "-", "self", ".", "data", ".", "val_mask", ".", "sum", "(", ")", ".", "item", "(", ")", ")", "\n", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "hours", ",", "minutes", ",", "seconds", "=", "calc_elapsed_time", "(", "start", ",", "end", ")", "\n", "print", "(", "\"\\n\"", "+", "\"-\"", "*", "50", "+", "\"\\nTook  {:0>2} hours: {:0>2} mins: {:05.2f} secs  to Prepare Data\\n\"", ".", "format", "(", "hours", ",", "minutes", ",", "seconds", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_gnn.Prepare_GNN_Dataset.prepare_gnn_training": [[120, 149], ["torch_geometric.data.Data", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.tensor", "print", "list", "data_utils_gnn.Prepare_GNN_Dataset.x_data.float", "data_utils_gnn.Prepare_GNN_Dataset.edge_index_data.long", "data_utils_gnn.Prepare_GNN_Dataset.node2id.values", "torch_geometric.data.cluster.ClusterData", "torch_geometric.data.cluster.ClusterLoader", "torch_geometric.data.GraphSAINTRandomWalkSampler", "torch_geometric.data.GraphSAINTNodeSampler", "torch_geometric.data.GraphSAINTEdgeSampler"], "methods", ["None"], ["", "", "def", "prepare_gnn_training", "(", "self", ")", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "print", "(", "\"\\n\\n==>> Clustering the graph and preparing dataloader....\"", ")", "\n", "\n", "", "self", ".", "data", "=", "Data", "(", "x", "=", "self", ".", "x_data", ".", "float", "(", ")", ",", "edge_index", "=", "self", ".", "edge_index_data", ".", "long", "(", ")", ",", "edge_attr", "=", "self", ".", "edge_type_data", ",", "y", "=", "self", ".", "y_data", ")", "\n", "new_num_nodes", ",", "_", "=", "self", ".", "data", ".", "x", ".", "shape", "\n", "\n", "self", ".", "data", ".", "train_mask", "=", "torch", ".", "FloatTensor", "(", "self", ".", "split_masks", "[", "'train_mask'", "]", ")", "\n", "self", ".", "data", ".", "val_mask", "=", "torch", ".", "FloatTensor", "(", "self", ".", "split_masks", "[", "'val_mask'", "]", ")", "\n", "self", ".", "data", ".", "representation_mask", "=", "torch", ".", "FloatTensor", "(", "self", ".", "split_masks", "[", "'repr_mask'", "]", ")", "\n", "self", ".", "data", ".", "node2id", "=", "torch", ".", "tensor", "(", "list", "(", "self", ".", "node2id", ".", "values", "(", ")", ")", ")", "\n", "# self.data.node_type = self.node_type", "\n", "\n", "\n", "if", "not", "self", ".", "config", "[", "'full_graph'", "]", ":", "\n", "            ", "if", "self", ".", "config", "[", "'cluster'", "]", ":", "\n", "                ", "cluster_data", "=", "ClusterData", "(", "self", ".", "data", ",", "num_parts", "=", "self", ".", "config", "[", "'clusters'", "]", ",", "recursive", "=", "False", ")", "\n", "self", ".", "loader", "=", "ClusterLoader", "(", "cluster_data", ",", "batch_size", "=", "self", ".", "config", "[", "'batch_size'", "]", ",", "shuffle", "=", "self", ".", "config", "[", "'shuffle'", "]", ",", "num_workers", "=", "0", ")", "\n", "", "elif", "self", ".", "config", "[", "'saint'", "]", "==", "'random_walk'", ":", "\n", "                ", "self", ".", "loader", "=", "GraphSAINTRandomWalkSampler", "(", "self", ".", "data", ",", "batch_size", "=", "6000", ",", "walk_length", "=", "2", ",", "num_steps", "=", "5", ",", "sample_coverage", "=", "100", ",", "num_workers", "=", "0", ")", "\n", "", "elif", "self", ".", "config", "[", "'saint'", "]", "==", "'node'", ":", "\n", "                ", "self", ".", "loader", "=", "GraphSAINTNodeSampler", "(", "self", ".", "data", ",", "batch_size", "=", "6000", ",", "num_steps", "=", "5", ",", "sample_coverage", "=", "100", ",", "num_workers", "=", "0", ")", "\n", "", "elif", "self", ".", "config", "[", "'saint'", "]", "==", "'edge'", ":", "\n", "                ", "self", ".", "loader", "=", "GraphSAINTEdgeSampler", "(", "self", ".", "data", ",", "batch_size", "=", "6000", ",", "num_steps", "=", "5", ",", "sample_coverage", "=", "100", ",", "num_workers", "=", "0", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "loader", "=", "None", "\n", "\n", "\n", "", "return", "self", ".", "loader", ",", "self", ".", "vocab_size", ",", "self", ".", "data", "", "", "", ""]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.Prepare_Dataset.__init__": [[65, 70], ["super().__init__", "data_utils_txt.Prepare_Dataset.read_dataset_splits"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.Prepare_Dataset.read_dataset_splits"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "Prepare_Dataset", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "read_dataset_splits", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.Prepare_Dataset.read_dataset_splits": [[72, 113], ["print", "time.time", "os.path.join", "os.path.join", "os.path.join", "range", "len", "eval", "print", "print", "print", "print", "print", "time.time", "data_utils_txt.calc_elapsed_time", "print", "open", "csv.reader", "len", "len", "len", "data_utils_txt.Prepare_Dataset.train_text.append", "data_utils_txt.Prepare_Dataset.train_labels.append", "data_utils_txt.Prepare_Dataset.train_ids.append", "data_utils_txt.Prepare_Dataset.val_text.append", "data_utils_txt.Prepare_Dataset.val_labels.append", "data_utils_txt.Prepare_Dataset.test_ids.append", "data_utils_txt.Prepare_Dataset.test_text.append", "data_utils_txt.Prepare_Dataset.test_labels.append", "data_utils_txt.Prepare_Dataset.test_ids.append"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.calc_elapsed_time"], ["", "def", "read_dataset_splits", "(", "self", ",", "verbose", "=", "True", ")", ":", "\n", "        ", "print", "(", "\"=\"", "*", "100", "+", "\"\\n\\t\\t\\t\\t\\t Preparing Data\\n\"", "+", "\"=\"", "*", "100", ")", "\n", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "train_data_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "config", "[", "'data_path'", "]", ",", "'train.tsv'", ")", "\n", "val_data_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "config", "[", "'data_path'", "]", ",", "'val.tsv'", ")", "\n", "test_data_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "config", "[", "'data_path'", "]", ",", "'test.tsv'", ")", "\n", "\n", "directories", "=", "[", "'train_data_dir'", ",", "'val_data_dir'", ",", "'test_data_dir'", "]", "\n", "self", ".", "train_text", ",", "self", ".", "val_text", ",", "self", ".", "test_text", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "self", ".", "train_labels", ",", "self", ".", "val_labels", ",", "self", ".", "test_labels", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "self", ".", "train_ids", ",", "self", ".", "val_ids", ",", "self", ".", "test_ids", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "directories", ")", ")", ":", "\n", "            ", "data_dir", "=", "eval", "(", "directories", "[", "i", "]", ")", "\n", "with", "open", "(", "data_dir", ",", "encoding", "=", "'utf8'", ")", "as", "read_file", ":", "\n", "                ", "rows", "=", "csv", ".", "reader", "(", "read_file", ",", "delimiter", "=", "\"\\t\"", ",", "quotechar", "=", "'\"'", ")", "\n", "for", "row", "in", "rows", ":", "\n", "                    ", "if", "i", "==", "0", ":", "\n", "                        ", "self", ".", "train_text", ".", "append", "(", "row", "[", "0", "]", ")", "\n", "self", ".", "train_labels", ".", "append", "(", "row", "[", "1", "]", ")", "\n", "self", ".", "train_ids", ".", "append", "(", "row", "[", "2", "]", ")", "\n", "", "elif", "i", "==", "1", ":", "\n", "                        ", "self", ".", "val_text", ".", "append", "(", "row", "[", "0", "]", ")", "\n", "self", ".", "val_labels", ".", "append", "(", "row", "[", "1", "]", ")", "\n", "self", ".", "test_ids", ".", "append", "(", "row", "[", "2", "]", ")", "\n", "", "else", ":", "\n", "                        ", "self", ".", "test_text", ".", "append", "(", "row", "[", "0", "]", ")", "\n", "self", ".", "test_labels", ".", "append", "(", "row", "[", "1", "]", ")", "\n", "self", ".", "test_ids", ".", "append", "(", "row", "[", "2", "]", ")", "\n", "\n", "", "", "", "", "if", "verbose", ":", "\n", "            ", "print", "(", "\"\\n\\n\"", "+", "\"-\"", "*", "50", "+", "\"\\nDATA STATISTICS:\\n\"", "+", "\"-\"", "*", "50", ")", "\n", "print", "(", "'No. of target classes = '", ",", "self", ".", "config", "[", "\"n_classes\"", "]", ")", "\n", "print", "(", "'No. of train instances = '", ",", "len", "(", "self", ".", "train_labels", ")", ")", "\n", "print", "(", "'No. of dev instances = '", ",", "len", "(", "self", ".", "val_labels", ")", ")", "\n", "print", "(", "'No. of test instances = '", ",", "len", "(", "self", ".", "test_labels", ")", ")", "\n", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "hours", ",", "minutes", ",", "seconds", "=", "calc_elapsed_time", "(", "start", ",", "end", ")", "\n", "print", "(", "\"\\n\"", "+", "\"-\"", "*", "50", "+", "\"\\nTook  {:0>2} hours: {:0>2} mins: {:05.2f} secs  to Prepare Data\\n\"", ".", "format", "(", "hours", ",", "minutes", ",", "seconds", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.Prepare_Dataset.prepare_glove_training": [[116, 134], ["FakeNews_dataset.splits", "torchtext.vocab.Vectors", "FakeNews_dataset.TEXT.build_vocab", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torchtext.data.BucketIterator.splits", "os.path.join", "os.path.join", "os.path.join", "int"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.build_vocab"], ["", "", "def", "prepare_glove_training", "(", "self", ",", "verbose", "=", "True", ")", ":", "\n", "        ", "data_dir", "=", "self", ".", "config", "[", "'data_path'", "]", "\n", "train", ",", "val", ",", "test", "=", "FakeNews_dataset", ".", "splits", "(", "data_dir", ",", "train", "=", "os", ".", "path", ".", "join", "(", "'train.tsv'", ")", ",", "\n", "validation", "=", "os", ".", "path", ".", "join", "(", "'val.tsv'", ")", ",", "\n", "test", "=", "os", ".", "path", ".", "join", "(", "'test.tsv'", ")", ",", "\n", "format", "=", "'tsv'", ",", "fields", "=", "[", "(", "'text'", ",", "FakeNews_dataset", ".", "TEXT", ")", ",", "(", "'label'", ",", "FakeNews_dataset", ".", "LABEL", ")", ",", "(", "'id'", ",", "FakeNews_dataset", ".", "ID", ")", "]", ")", "\n", "\n", "# Build Vocabulary and obtain embeddings for each word in Vocabulary", "\n", "glove_embeds", "=", "torchtext", ".", "vocab", ".", "Vectors", "(", "name", "=", "self", ".", "config", "[", "'glove_path'", "]", ",", "max_vectors", "=", "int", "(", "5e4", ")", ")", "\n", "FakeNews_dataset", ".", "TEXT", ".", "build_vocab", "(", "train", ",", "val", ",", "test", ",", "vectors", "=", "glove_embeds", ")", "\n", "\n", "# Setting 'unk' token as the average of all other embeddings", "\n", "FakeNews_dataset", ".", "TEXT", ".", "vocab", ".", "vectors", "[", "FakeNews_dataset", ".", "TEXT", ".", "vocab", ".", "stoi", "[", "'<unk>'", "]", "]", "=", "torch", ".", "mean", "(", "FakeNews_dataset", ".", "TEXT", ".", "vocab", ".", "vectors", ",", "dim", "=", "0", ")", "\n", "\n", "train_loader", ",", "dev_loader", ",", "test_loader", "=", "BucketIterator", ".", "splits", "(", "(", "train", ",", "val", ",", "test", ")", ",", "batch_size", "=", "self", ".", "config", "[", "'batch_size'", "]", ",", "repeat", "=", "False", ",", "shuffle", "=", "True", ",", "\n", "sort_within_batch", "=", "False", ",", "device", "=", "device", ")", "\n", "\n", "return", "train_loader", ",", "dev_loader", ",", "test_loader", ",", "FakeNews_dataset", ".", "TEXT", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.Prepare_Dataset.prepare_transformer_training": [[137, 156], ["data_utils_txt.Prepare_Dataset.tokenizer", "data_utils_txt.Prepare_Dataset.tokenizer", "data_utils_txt.Prepare_Dataset.tokenizer", "data_utils_txt.FakeNews_dataset_transformer", "data_utils_txt.FakeNews_dataset_transformer", "data_utils_txt.FakeNews_dataset_transformer", "DataLoader", "DataLoader", "DataLoader", "transformers.DistilBertTokenizerFast.from_pretrained", "transformers.RobertaTokenizerFast.from_pretrained"], "methods", ["None"], ["", "def", "prepare_transformer_training", "(", "self", ",", "verbose", "=", "True", ")", ":", "\n", "        ", "if", "self", ".", "config", "[", "'embed_name'", "]", "==", "'dbert'", ":", "\n", "            ", "self", ".", "tokenizer", "=", "DistilBertTokenizerFast", ".", "from_pretrained", "(", "self", ".", "config", "[", "'model_name'", "]", ")", "\n", "", "elif", "self", ".", "config", "[", "'embed_name'", "]", "==", "'roberta'", ":", "\n", "            ", "self", ".", "tokenizer", "=", "RobertaTokenizerFast", ".", "from_pretrained", "(", "self", ".", "config", "[", "'model_name'", "]", ")", "\n", "\n", "", "self", ".", "train_encodings", "=", "self", ".", "tokenizer", "(", "self", ".", "train_text", ",", "truncation", "=", "True", ",", "padding", "=", "True", ")", "\n", "self", ".", "val_encodings", "=", "self", ".", "tokenizer", "(", "self", ".", "val_text", ",", "truncation", "=", "True", ",", "padding", "=", "True", ")", "\n", "self", ".", "test_encodings", "=", "self", ".", "tokenizer", "(", "self", ".", "test_text", ",", "truncation", "=", "True", ",", "padding", "=", "True", ")", "\n", "\n", "train_dataset", "=", "FakeNews_dataset_transformer", "(", "self", ".", "config", ",", "self", ".", "train_encodings", ",", "self", ".", "train_labels", ",", "self", ".", "train_ids", ")", "\n", "val_dataset", "=", "FakeNews_dataset_transformer", "(", "self", ".", "config", ",", "self", ".", "val_encodings", ",", "self", ".", "val_labels", ",", "self", ".", "val_ids", ")", "\n", "test_dataset", "=", "FakeNews_dataset_transformer", "(", "self", ".", "config", ",", "self", ".", "test_encodings", ",", "self", ".", "test_labels", ",", "self", ".", "test_ids", ")", "\n", "\n", "train_loader", "=", "DataLoader", "(", "train_dataset", ",", "batch_size", "=", "self", ".", "config", "[", "'batch_size'", "]", ",", "shuffle", "=", "True", ")", "\n", "val_loader", "=", "DataLoader", "(", "val_dataset", ",", "batch_size", "=", "self", ".", "config", "[", "'batch_size'", "]", ",", "shuffle", "=", "True", ")", "\n", "test_loader", "=", "DataLoader", "(", "test_dataset", ",", "batch_size", "=", "self", ".", "config", "[", "'batch_size'", "]", ",", "shuffle", "=", "True", ")", "\n", "\n", "return", "train_loader", ",", "val_loader", ",", "test_loader", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.Prepare_Dataset.prepare_elmo_training": [[159, 170], ["range", "nltk.word_tokenize", "nltk.word_tokenize", "nltk.word_tokenize"], "methods", ["None"], ["", "def", "prepare_elmo_training", "(", "self", ",", "verbose", "=", "True", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "# Each ele is a document, that we tokenize", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "train_data", "=", "[", "nltk", ".", "word_tokenize", "(", "ele", ")", "for", "ele", "in", "self", ".", "train_text", "]", "\n", "", "elif", "i", "==", "1", ":", "\n", "                ", "val_data", "=", "[", "nltk", ".", "word_tokenize", "(", "ele", ")", "for", "ele", "in", "self", ".", "val_text", "]", "\n", "", "else", ":", "\n", "                ", "test_data", "=", "[", "nltk", ".", "word_tokenize", "(", "ele", ")", "for", "ele", "in", "self", ".", "test_text", "]", "\n", "\n", "", "", "return", "train_data", ",", "self", ".", "train_labels", ",", "val_data", ",", "self", ".", "val_labels", ",", "test_data", ",", "self", ".", "test_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.Prepare_Dataset.prepare_HAN_elmo_training": [[173, 184], ["data_utils_txt.Dataset_Helper_HAN.get_dataset", "data_utils_txt.DataLoader_Helper_HAN.create_dataloaders"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.Dataset_Helper_HAN.get_dataset", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.DataLoader_Helper_HAN.create_dataloaders"], ["", "def", "prepare_HAN_elmo_training", "(", "config", ")", ":", "\n", "        ", "train_data", ",", "val_data", ",", "test_data", "=", "Dataset_Helper_HAN", ".", "get_dataset", "(", "config", ",", "lowercase_sentences", "=", "config", "[", "'lowercase'", "]", ")", "\n", "\n", "# Getting iterators for each set", "\n", "train_loader", ",", "val_loader", ",", "test_loader", "=", "DataLoader_Helper_HAN", ".", "create_dataloaders", "(", "\n", "train_dataset", "=", "train_data", ",", "\n", "validation_dataset", "=", "val_data", ",", "\n", "test_dataset", "=", "test_data", ",", "\n", "batch_size", "=", "config", "[", "'batch_size'", "]", ",", "\n", "shuffle", "=", "True", ")", "\n", "return", "train_loader", ",", "val_loader", ",", "test_loader", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.Prepare_Dataset.prepare_lr_training": [[187, 198], ["data_utils_txt.LR_Dataset", "data_utils_txt.LR_Dataset", "data_utils_txt.LR_Dataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader"], "methods", ["None"], ["", "def", "prepare_lr_training", "(", "config", ",", "seed", ",", "fold", "=", "None", ")", ":", "\n", "\n", "        ", "train_dataset", "=", "LR_Dataset", "(", "config", ",", "split", "=", "'train'", ",", "seed", "=", "seed", ")", "\n", "val_dataset", "=", "LR_Dataset", "(", "config", ",", "split", "=", "'val'", ",", "seed", "=", "seed", ")", "\n", "test_dataset", "=", "LR_Dataset", "(", "config", ",", "split", "=", "'test'", ",", "seed", "=", "seed", ")", "\n", "\n", "train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset", "=", "train_dataset", ",", "batch_size", "=", "config", "[", "'batch_size'", "]", ",", "collate_fn", "=", "collate_for_lr", ",", "shuffle", "=", "True", ")", "\n", "val_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset", "=", "val_dataset", ",", "batch_size", "=", "config", "[", "'batch_size'", "]", ",", "collate_fn", "=", "collate_for_lr", ",", "shuffle", "=", "True", ")", "\n", "test_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset", "=", "test_dataset", ",", "batch_size", "=", "config", "[", "'batch_size'", "]", ",", "collate_fn", "=", "collate_for_lr", ",", "shuffle", "=", "True", ")", "\n", "\n", "return", "train_loader", ",", "val_loader", ",", "test_loader", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.FakeNews_dataset.sort_key": [[223, 225], ["len"], "methods", ["None"], ["def", "sort_key", "(", "ex", ")", ":", "\n", "        ", "return", "len", "(", "ex", ".", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.FakeNews_dataset_transformer.__init__": [[249, 255], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "encodings", ",", "labels", ",", "ids", ")", ":", "\n", "        ", "super", "(", "FakeNews_dataset_transformer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "encodings", "=", "encodings", "\n", "self", ".", "labels", "=", "labels", "\n", "self", ".", "ids", "=", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.FakeNews_dataset_transformer.__getitem__": [[256, 261], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "data_utils_txt.FakeNews_dataset_transformer.encodings.items"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "item", "=", "{", "key", ":", "torch", ".", "tensor", "(", "val", "[", "idx", "]", ")", "for", "key", ",", "val", "in", "self", ".", "encodings", ".", "items", "(", ")", "}", "\n", "item", "[", "'labels'", "]", "=", "torch", ".", "tensor", "(", "self", ".", "labels", "[", "idx", "]", ")", "\n", "item", "[", "'ids'", "]", "=", "torch", ".", "tensor", "(", "self", ".", "ids", "[", "idx", "]", ")", "\n", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.FakeNews_dataset_transformer.__len__": [[262, 264], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Dataset.__init__": [[312, 319], ["data_utils_txt.HAN_Dataset._parse_csv_file", "len"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Dataset._parse_csv_file"], ["def", "__init__", "(", "self", ",", "filename", ",", "lowercase_sentences", "=", "True", ",", "max_article_length", "=", "4000", ")", ":", "\n", "\n", "        ", "self", ".", "lowercase_sentences", "=", "lowercase_sentences", "\n", "self", ".", "sent_boundaries", "=", "{", "}", "\n", "# Here body_tokens are ALL the docs in a data split which are sentence tokenized", "\n", "self", ".", "_labels", ",", "self", ".", "_all_docs", "=", "self", ".", "_parse_csv_file", "(", "filename", ",", "max_article_length", ")", "\n", "self", ".", "_data_size", "=", "len", "(", "self", ".", "_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Dataset.__getitem__": [[321, 367], ["enumerate", "allennlp.modules.elmo.batch_to_ids().squeeze", "range", "len", "len", "len", "len", "token.lower", "allennlp.modules.elmo.batch_to_ids", "int", "range", "result_embeddings.append", "result_embeddings.append", "numpy.ceil", "result_embeddings.append"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "# Taking just 1 document out of all the docs in the split based on the 'idx'", "\n", "        ", "body_tokens", "=", "self", ".", "_all_docs", "[", "idx", "]", "\n", "\n", "# Get sentence boundaries", "\n", "article_indexes", "=", "{", "}", "\n", "start_index", "=", "0", "\n", "for", "index", ",", "current_sent", "in", "enumerate", "(", "body_tokens", ")", ":", "\n", "            ", "end_index", "=", "start_index", "+", "len", "(", "current_sent", ")", "\n", "article_indexes", "[", "index", "]", "=", "(", "start_index", ",", "end_index", ")", "\n", "start_index", "=", "end_index", "\n", "\n", "", "if", "self", ".", "lowercase_sentences", ":", "\n", "            ", "body_tokens", "=", "[", "token", ".", "lower", "(", ")", "for", "sentence_tokens", "in", "body_tokens", "for", "token", "in", "sentence_tokens", "]", "\n", "\n", "", "result_embeddings", "=", "[", "]", "\n", "doc_elmo_ids", "=", "batch_to_ids", "(", "body_tokens", ")", ".", "squeeze", "(", "0", ")", "# bach_to_ids returns a tensor that is (B x num_words x 50) = (1 x num_words x 50) here", "\n", "\n", "# Get the sentence tokenized structure of documents back using the sentence boundaries", "\n", "total_sents", "=", "0", "\n", "for", "i", "in", "range", "(", "index", "+", "1", ")", ":", "\n", "            ", "total_sents", "+=", "1", "\n", "start", ",", "end", "=", "article_indexes", "[", "i", "]", "\n", "len_sent", "=", "end", "-", "start", "\n", "# Limiting the length of a single sentence for memory efficient batching later (splitting longer sentences of a doc into smaller ones)", "\n", "if", "len_sent", ">", "500", ":", "\n", "                ", "num_sub_sents", "=", "int", "(", "np", ".", "ceil", "(", "len_sent", "/", "500", ")", ")", "\n", "for", "j", "in", "range", "(", "num_sub_sents", "-", "1", ")", ":", "\n", "                    ", "total_sents", "+=", "1", "\n", "sub_start", "=", "start", "+", "500", "*", "j", "\n", "sub_end", "=", "sub_start", "+", "500", "\n", "result_embeddings", ".", "append", "(", "doc_elmo_ids", "[", "sub_start", ":", "sub_end", ",", ":", "]", ")", "\n", "", "result_embeddings", ".", "append", "(", "doc_elmo_ids", "[", "sub_end", ":", "end", ",", ":", "]", ")", "\n", "", "else", ":", "\n", "                ", "result_embeddings", ".", "append", "(", "doc_elmo_ids", "[", "start", ":", "end", ",", ":", "]", ")", "\n", "\n", "", "", "num_tokens_per_sent", "=", "[", "len", "(", "sentence_embeddings", ")", "for", "sentence_embeddings", "in", "result_embeddings", "]", "\n", "num_of_sents_in_doc", "=", "len", "(", "result_embeddings", ")", "\n", "\n", "assert", "len", "(", "num_tokens_per_sent", ")", "==", "num_of_sents_in_doc", "\n", "if", "num_of_sents_in_doc", ">", "15", ":", "\n", "            ", "num_of_sents_in_doc", "=", "15", "\n", "result_embeddings", "=", "result_embeddings", "[", ":", "15", "]", "\n", "num_tokens_per_sent", "=", "num_tokens_per_sent", "[", ":", "15", "]", "\n", "\n", "", "return", "result_embeddings", ",", "self", ".", "_labels", "[", "idx", "]", ",", "num_of_sents_in_doc", ",", "num_tokens_per_sent", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Dataset.__len__": [[368, 370], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_data_size", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Dataset._parse_csv_file": [[371, 380], ["pandas.read_csv", "nltk.sent_tokenize", "list", "list"], "methods", ["None"], ["", "def", "_parse_csv_file", "(", "self", ",", "filename", ",", "max_article_length", ")", ":", "\n", "        ", "'''\n        Parses the metaphor CSV file and creates the necessary objects for the dataset\n        '''", "\n", "df", "=", "pd", ".", "read_csv", "(", "filename", ",", "sep", "=", "'\\t'", ",", "encoding", "=", "'utf8'", ",", "header", "=", "None", ")", "\n", "# Each ele is a document, that we tokenize", "\n", "data", "=", "[", "nltk", ".", "sent_tokenize", "(", "ele", "[", ":", "4000", "]", ")", "for", "ele", "in", "list", "(", "df", "[", "0", "]", ")", "]", "\n", "labels", "=", "[", "ele", "for", "ele", "in", "list", "(", "df", "[", "1", "]", ")", "]", "\n", "return", "labels", ",", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.Dataset_Helper_HAN.get_dataset": [[385, 400], ["os.path.join", "os.path.join", "os.path.join", "data_utils_txt.HAN_Dataset", "data_utils_txt.HAN_Dataset", "data_utils_txt.HAN_Dataset"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "get_dataset", "(", "config", ",", "lowercase_sentences", "=", "False", ")", "->", "Tuple", "[", "HAN_Dataset", ",", "HAN_Dataset", "]", ":", "\n", "        ", "'''\n        Parses the data files and creates HAN_Dataset objects\n        '''", "\n", "train_data_dir", "=", "os", ".", "path", ".", "join", "(", "config", "[", "'data_path'", "]", ",", "'train.tsv'", ")", "\n", "val_data_dir", "=", "os", ".", "path", ".", "join", "(", "config", "[", "'data_path'", "]", ",", "'val.tsv'", ")", "\n", "test_data_dir", "=", "os", ".", "path", ".", "join", "(", "config", "[", "'data_path'", "]", ",", "'test.tsv'", ")", "\n", "# Train", "\n", "train_dataset", "=", "HAN_Dataset", "(", "filename", "=", "train_data_dir", ",", "lowercase_sentences", "=", "lowercase_sentences", ")", "\n", "# Validation", "\n", "val_dataset", "=", "HAN_Dataset", "(", "filename", "=", "val_data_dir", ",", "lowercase_sentences", "=", "lowercase_sentences", ")", "\n", "# Test    ", "\n", "test_dataset", "=", "HAN_Dataset", "(", "filename", "=", "test_data_dir", ",", "lowercase_sentences", "=", "lowercase_sentences", ")", "\n", "return", "train_dataset", ",", "val_dataset", ",", "test_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.DataLoader_Helper_HAN.create_dataloaders": [[404, 434], ["torch.DataLoader", "torch.DataLoader", "torch.DataLoader", "torch.DataLoader", "torch.DataLoader", "torch.DataLoader"], "methods", ["None"], ["    ", "@", "classmethod", "\n", "def", "create_dataloaders", "(", "cls", ",", "train_dataset", ":", "data", ".", "Dataset", "=", "None", ",", "validation_dataset", ":", "data", ".", "Dataset", "=", "None", ",", "test_dataset", ":", "data", ".", "Dataset", "=", "None", ",", "batch_size", ":", "int", "=", "32", ",", "shuffle", ":", "bool", "=", "True", ")", ":", "\n", "        ", "'''\n        Creates DataLoader objects for the given datasets while including padding and sorting\n        '''", "\n", "# NOTE: We work with batch_szie = 1 since we deploy dynamic batching and thus add docs to batch dynamically based on available memory", "\n", "train_loader", "=", "None", "\n", "if", "train_dataset", ":", "\n", "            ", "train_loader", "=", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "train_dataset", ",", "\n", "batch_size", "=", "1", ",", "\n", "num_workers", "=", "1", ",", "\n", "shuffle", "=", "shuffle", ")", "\n", "\n", "", "validation_loader", "=", "None", "\n", "if", "validation_dataset", ":", "\n", "            ", "validation_loader", "=", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "validation_dataset", ",", "\n", "batch_size", "=", "1", ",", "\n", "num_workers", "=", "1", ",", "\n", "shuffle", "=", "False", ")", "\n", "\n", "", "test_loader", "=", "None", "\n", "if", "test_dataset", ":", "\n", "            ", "test_loader", "=", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "test_dataset", ",", "\n", "batch_size", "=", "1", ",", "\n", "num_workers", "=", "1", ",", "\n", "shuffle", "=", "False", ")", "\n", "", "return", "train_loader", ",", "validation_loader", ",", "test_loader", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch.__init__": [[442, 448], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "max_length", ":", "int", "=", "15000", ")", ":", "\n", "        ", "self", ".", "_list_of_sequences", "=", "[", "]", "\n", "self", ".", "_targets", "=", "[", "]", "\n", "self", ".", "_list_of_lengths", "=", "[", "]", "\n", "self", ".", "_num_of_sent", "=", "[", "]", "\n", "self", ".", "_max_length", "=", "max_length", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch.list_of_sequences": [[449, 452], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "list_of_sequences", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_list_of_sequences", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch.targets": [[453, 456], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "targets", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_targets", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch.list_of_lengths": [[457, 460], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "list_of_lengths", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_list_of_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch.num_of_sent": [[461, 464], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_of_sent", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_num_of_sent", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch.is_full": [[465, 468], ["sum", "sum"], "methods", ["None"], ["", "def", "is_full", "(", "self", ")", ":", "\n", "        ", "total_sum", "=", "sum", "(", "[", "sum", "(", "lengths", ")", "for", "lengths", "in", "self", ".", "_list_of_lengths", "]", ")", "\n", "return", "total_sum", ">", "self", ".", "_max_length", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch.add_data": [[469, 474], ["data_utils_txt.HAN_Batch._list_of_sequences.append", "data_utils_txt.HAN_Batch._targets.append", "data_utils_txt.HAN_Batch._list_of_lengths.append", "data_utils_txt.HAN_Batch._num_of_sent.append"], "methods", ["None"], ["", "def", "add_data", "(", "self", ",", "list_of_sequences", ",", "target", ",", "num_of_sent", ",", "list_of_lengths", ")", ":", "\n", "        ", "self", ".", "_list_of_sequences", ".", "append", "(", "list_of_sequences", ")", "\n", "self", ".", "_targets", ".", "append", "(", "target", ")", "\n", "self", ".", "_list_of_lengths", ".", "append", "(", "list_of_lengths", ")", "\n", "self", ".", "_num_of_sent", ".", "append", "(", "num_of_sent", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch.just_pad_batch": [[476, 494], ["len", "numpy.array", "numpy.amax", "max", "numpy.zeros", "numpy.ones", "range", "range", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "concat_sequences[].squeeze"], "methods", ["None"], ["", "def", "just_pad_batch", "(", "self", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "self", ".", "_num_of_sent", ")", "\n", "concat_lengths", "=", "np", ".", "array", "(", "[", "length", "for", "lengths", "in", "self", ".", "_list_of_lengths", "for", "length", "in", "lengths", "]", ")", "\n", "concat_sequences", "=", "[", "seq", "[", "0", "]", "for", "sequences", "in", "self", ".", "_list_of_sequences", "for", "seq", "in", "sequences", "]", "\n", "\n", "max_word_len", "=", "np", ".", "amax", "(", "concat_lengths", ")", "\n", "max_sen_len", "=", "max", "(", "self", ".", "_num_of_sent", ")", "\n", "embedding_dimension", "=", "concat_sequences", "[", "0", "]", ".", "shape", "[", "2", "]", "\n", "\n", "padded_sequences", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", "max_sen_len", ",", "max_word_len", ",", "embedding_dimension", ")", ")", "\n", "padded_lengths", "=", "np", ".", "ones", "(", "(", "batch_size", ",", "max_sen_len", ")", ")", "\n", "i", "=", "0", "\n", "for", "batch", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "for", "num_sent", "in", "range", "(", "self", ".", "_num_of_sent", "[", "batch", "]", ")", ":", "\n", "                ", "padded_sequences", "[", "batch", ",", "num_sent", ",", ":", "concat_lengths", "[", "i", "]", ",", ":", "]", "=", "concat_sequences", "[", "i", "]", ".", "squeeze", "(", "1", ")", "\n", "padded_lengths", "[", "batch", ",", "num_sent", "]", "=", "concat_lengths", "[", "i", "]", "\n", "i", "+=", "1", "\n", "", "", "return", "torch", ".", "Tensor", "(", "padded_sequences", ")", ",", "torch", ".", "Tensor", "(", "self", ".", "_targets", ")", ",", "torch", ".", "LongTensor", "(", "self", ".", "_num_of_sent", ")", ",", "torch", ".", "LongTensor", "(", "padded_lengths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch.pad_batch": [[496, 516], ["numpy.array", "numpy.amax", "numpy.zeros", "enumerate", "concat_sequences[].squeeze", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "sum"], "methods", ["None"], ["", "def", "pad_batch", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        DataLoaderBatch for the HAN should be a list of (sequence, target, length) tuples...\n        Returns a padded tensor of sequences sorted from longest to shortest, \n        \"\"\"", "\n", "# concat_lengths - concatted lengths of each sentence - [ batch_size * n_sentences ]", "\n", "concat_lengths", "=", "np", ".", "array", "(", "[", "length", "for", "lengths", "in", "self", ".", "_list_of_lengths", "for", "length", "in", "lengths", "]", ")", "\n", "# concat_sequences - the embeddings for each batch for each sentence for each word - [ (batch_size * n_sentences) x n_words x embedding_dim ]", "\n", "concat_sequences", "=", "[", "seq", "[", "0", "]", "for", "sequences", "in", "self", ".", "_list_of_sequences", "for", "seq", "in", "sequences", "]", "\n", "max_length", "=", "np", ".", "amax", "(", "concat_lengths", ")", "\n", "\n", "# # These should be the same list of numbers - checked", "\n", "# print(\"concat lengths =  \", concat_lengths)", "\n", "# print([len(seq) for seq in concat_sequences])", "\n", "\n", "embedding_dimension", "=", "concat_sequences", "[", "0", "]", ".", "shape", "[", "2", "]", "\n", "padded_sequences", "=", "np", ".", "zeros", "(", "(", "sum", "(", "self", ".", "_num_of_sent", ")", ",", "max_length", ",", "embedding_dimension", ")", ")", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "concat_lengths", ")", ":", "\n", "            ", "padded_sequences", "[", "i", ",", ":", "l", ",", ":", "]", "=", "concat_sequences", "[", "i", "]", ".", "squeeze", "(", "1", ")", "\n", "", "return", "torch", ".", "Tensor", "(", "padded_sequences", ")", ",", "torch", ".", "Tensor", "(", "self", ".", "_targets", ")", ",", "torch", ".", "LongTensor", "(", "self", ".", "_num_of_sent", ")", ",", "torch", ".", "LongTensor", "(", "concat_lengths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch.pad_and_sort_batch": [[517, 537], ["numpy.array", "numpy.amax", "numpy.zeros", "enumerate", "data_utils_txt.HAN_Batch._sort_batch", "concat_sequences[].squeeze", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "sum"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch._sort_batch"], ["", "def", "pad_and_sort_batch", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        DataLoaderBatch for the HAN should be a list of (sequence, target, length) tuples...\n        Returns a padded tensor of sequences sorted from longest to shortest, \n        \"\"\"", "\n", "# concat_lengths - concatted lengths of each sentence - [ batch_size * n_sentences ]", "\n", "concat_lengths", "=", "np", ".", "array", "(", "[", "length", "for", "lengths", "in", "self", ".", "_list_of_lengths", "for", "length", "in", "lengths", "]", ")", "\n", "# concat_sequences - the embeddings for each batch for each sentence for each word - [ (batch_size * n_sentences) x n_words x embedding_dim ]", "\n", "concat_sequences", "=", "[", "seq", "[", "0", "]", "for", "sequences", "in", "self", ".", "_list_of_sequences", "for", "seq", "in", "sequences", "]", "\n", "max_length", "=", "np", ".", "amax", "(", "concat_lengths", ")", "\n", "\n", "# # These should be the same list of numbers - checked", "\n", "# print(\"concat lengths =  \", concat_lengths)", "\n", "# print([len(seq) for seq in concat_sequences])", "\n", "\n", "embedding_dimension", "=", "concat_sequences", "[", "0", "]", ".", "shape", "[", "2", "]", "\n", "padded_sequences", "=", "np", ".", "zeros", "(", "(", "sum", "(", "self", ".", "_num_of_sent", ")", ",", "max_length", ",", "embedding_dimension", ")", ")", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "concat_lengths", ")", ":", "\n", "            ", "padded_sequences", "[", "i", ",", ":", "l", ",", ":", "]", "=", "concat_sequences", "[", "i", "]", ".", "squeeze", "(", "1", ")", "\n", "", "return", "self", ".", "_sort_batch", "(", "torch", ".", "Tensor", "(", "padded_sequences", ")", ",", "torch", ".", "Tensor", "(", "self", ".", "_targets", ")", ",", "torch", ".", "LongTensor", "(", "self", ".", "_num_of_sent", ")", ",", "torch", ".", "LongTensor", "(", "concat_lengths", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch._sort_batch": [[539, 551], ["lengths.sort", "perm_idx.sort"], "methods", ["None"], ["", "def", "_sort_batch", "(", "self", ",", "batch", ",", "targets", ",", "num_sentences", ",", "lengths", ")", ":", "\n", "        ", "\"\"\"\n        Sort a minibatch by the length of the sequences with the longest sequences first\n        return the sorted batch targes and sequence lengths.\n        This way the output can be used by pack_padded_sequences(...)\n        \"\"\"", "\n", "seq_lengths", ",", "perm_idx", "=", "lengths", ".", "sort", "(", "0", ",", "descending", "=", "True", ")", "\n", "seq_tensor", "=", "batch", "[", "perm_idx", "]", "\n", "\n", "_", ",", "recover_idx", "=", "perm_idx", ".", "sort", "(", "0", ")", "\n", "# print(seq_tensor.shape, targets.shape, recover_idx.shape, num_sentences.shape, seq_lengths.shape)", "\n", "return", "seq_tensor", ",", "targets", ",", "num_sentences", ",", "seq_lengths", ",", "recover_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.LR_Dataset.__init__": [[569, 657], ["os.path.join", "os.path.join", "os.path.join", "json.load", "json.load", "torch.load", "torch.load", "torch.load", "torch.load", "json.load", "torch.load", "torch.load", "torch.load", "torch.load", "len", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "open", "open", "open", "json.load", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "json.load", "os.path.join", "torch.device", "torch.device", "torch.device", "torch.device", "os.path.isfile", "print", "print", "print", "print", "json.load", "os.path.isfile", "print", "print", "print", "print", "json.load", "open", "open", "json.load", "os.path.join", "os.path.join", "os.path.isfile", "len", "open", "json.dump", "open", "len", "open", "json.dump", "open", "open", "os.path.isfile", "str", "str", "str", "str", "str"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "split", ",", "seed", ")", ":", "\n", "\n", "        ", "self", ".", "base_dir", "=", "os", ".", "path", ".", "join", "(", "'data'", ",", "'complete_data'", ",", "config", "[", "'data_name'", "]", ")", "\n", "\n", "if", "config", "[", "'data_name'", "]", "in", "[", "'gossipcop'", ",", "'politifact'", "]", ":", "\n", "            ", "self", ".", "node2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_dir", ",", "'node2id_lr_30_30_gossipcop.json'", ")", "\n", "self", ".", "doc_embeds_file_gnn", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_dir", ",", "'cached_embeds'", ",", "'doc_embeds_graph_lr_wt3_30_30_{}_{}_{}.pt'", ".", "format", "(", "split", ",", "seed", ",", "config", "[", "'model_name'", "]", ")", ")", "\n", "# self.doc_embeds_file_text = os.path.join(self.base_dir, 'cached_embeds', 'doc_embeds_roberta_lr_{}.pt'.format(split))", "\n", "self", ".", "doc_embeds_file_text", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_dir", ",", "'cached_embeds'", ",", "'doc_embeds_cnn_21_{}.pt'", ".", "format", "(", "split", ")", ")", "\n", "# self.doc_embeds_file_text = os.path.join(self.base_dir, 'cached_embeds', 'doc_embeds_gloveavg_lr_{}.pt'.format(split))", "\n", "self", ".", "split_docs_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_dir", ",", "'doc_splits_lr.json'", ")", "\n", "self", ".", "_split_docs", "=", "json", ".", "load", "(", "open", "(", "self", ".", "split_docs_file", ",", "'r'", ")", ")", "[", "'{}_docs'", ".", "format", "(", "split", ")", "]", "\n", "\n", "", "elif", "config", "[", "'data_name'", "]", "in", "[", "'HealthRelease'", ",", "'HealthStory'", "]", ":", "\n", "            ", "self", ".", "node2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_dir", ",", "'node2id_lr_top10.json'", ")", "\n", "# self.doc_embeds_file_gnn = os.path.join(self.base_dir, 'cached_embeds', 'doc_embeds_graph_top10_lr_test_21_hgt.pt')", "\n", "self", ".", "doc_embeds_file_gnn", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_dir", ",", "'cached_embeds'", ",", "'doc_embeds_graph_poinc_wt3_attn_lr_{}_{}_{}.pt'", ".", "format", "(", "split", ",", "seed", ",", "config", "[", "'model_name'", "]", ")", ")", "\n", "# self.doc_embeds_file_text = os.path.join(self.base_dir, 'cached_embeds', 'doc_embeds_roberta_21_{}.pt'.format(split))", "\n", "# self.doc_embeds_file_text = os.path.join(self.base_dir, 'cached_embeds', 'doc_embeds_roberta_{}_{}.pt'.format(seed, split))", "\n", "self", ".", "doc_embeds_file_text", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_dir", ",", "'cached_embeds'", ",", "'doc_embeds_cnn_42_{}.pt'", ".", "format", "(", "split", ")", ")", "\n", "doc2labels_file", "=", "os", ".", "path", ".", "join", "(", "'FakeHealth'", ",", "'doc2labels_{}.json'", ".", "format", "(", "config", "[", "'data_name'", "]", ")", ")", "\n", "doc2labels", "=", "json", ".", "load", "(", "open", "(", "doc2labels_file", ",", "'r'", ")", ")", "\n", "self", ".", "split_docs_file", "=", "os", ".", "path", ".", "join", "(", "'FakeHealth'", ",", "'doc_splits_{}.json'", ".", "format", "(", "config", "[", "'data_name'", "]", ")", ")", "\n", "self", ".", "_split_docs", "=", "json", ".", "load", "(", "open", "(", "self", ".", "split_docs_file", ",", "'r'", ")", ")", "[", "'{}_docs'", ".", "format", "(", "split", ")", "]", "\n", "\n", "# self.test2id_file = os.path.join(self.base_dir, 'doc2id_encoder.json')", "\n", "", "self", ".", "test2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_dir", ",", "'doc2id_cnn_encoder.json'", ")", "\n", "# self.test2id_file = os.path.join(self.base_dir, 'doc2id_encoder_gloveavg.json')", "\n", "self", ".", "user_type_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_dir", ",", "'user_types.json'", ")", "\n", "\n", "if", "config", "[", "'data_name'", "]", "in", "[", "'gossipcop'", ",", "'politifact'", "]", ":", "\n", "            ", "self", ".", "split_docs_labels_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_dir", ",", "'cached_embeds'", ",", "'docs_labels_lr_30_30_{}.json'", ".", "format", "(", "split", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "split_docs_labels_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_dir", ",", "'cached_embeds'", ",", "'docs_labels_lr_{}.json'", ".", "format", "(", "split", ")", ")", "\n", "\n", "", "self", ".", "node2id", "=", "json", ".", "load", "(", "open", "(", "self", ".", "node2id_file", ",", "'r'", ")", ")", "\n", "self", ".", "test2id", "=", "json", ".", "load", "(", "open", "(", "self", ".", "test2id_file", ",", "'r'", ")", ")", "\n", "self", ".", "_doc_embeds_gnn", "=", "torch", ".", "load", "(", "self", ".", "doc_embeds_file_gnn", ")", "\n", "self", ".", "user_types", "=", "json", ".", "load", "(", "open", "(", "self", ".", "user_type_file", ",", "'r'", ")", ")", "\n", "self", ".", "_doc_embeds_text", "=", "torch", ".", "load", "(", "self", ".", "doc_embeds_file_text", ",", "map_location", "=", "torch", ".", "device", "(", "'cpu'", ")", ")", "\n", "self", ".", "data_size", "=", "len", "(", "self", ".", "_split_docs", ")", "\n", "self", ".", "mode", "=", "config", "[", "'mode'", "]", "\n", "\n", "if", "config", "[", "'data_name'", "]", "in", "[", "'gossipcop'", ",", "'politifact'", "]", ":", "\n", "            ", "if", "not", "os", ".", "path", ".", "isfile", "(", "self", ".", "split_docs_labels_file", ")", ":", "\n", "                ", "self", ".", "labels", "=", "{", "}", "\n", "print", "(", "\"\\nCreating labels dict of {} docs...\"", ".", "format", "(", "split", ")", ")", "\n", "not_in_either", "=", "0", "\n", "for", "split_doc", "in", "self", ".", "_split_docs", ":", "\n", "                    ", "real_file", "=", "os", ".", "path", ".", "join", "(", "'data'", ",", "'base_data'", ",", "config", "[", "'data_name'", "]", ",", "'real'", ",", "str", "(", "split_doc", ")", "+", "'.json'", ")", "\n", "fake_file", "=", "os", ".", "path", ".", "join", "(", "'data'", ",", "'base_data'", ",", "config", "[", "'data_name'", "]", ",", "'fake'", ",", "str", "(", "split_doc", ")", "+", "'.json'", ")", "\n", "# print(real_file + \"\\n\" + fake_file)", "\n", "if", "os", ".", "path", ".", "isfile", "(", "fake_file", ")", ":", "\n", "                        ", "label", "=", "1", "\n", "", "elif", "os", ".", "path", ".", "isfile", "(", "real_file", ")", ":", "\n", "                        ", "label", "=", "0", "\n", "", "else", ":", "\n", "# print(\"[!] WARNING: Did not find {} in either real or fake..\".format(split_doc))", "\n", "# sys.exit()", "\n", "                        ", "not_in_either", "+=", "1", "\n", "", "self", ".", "labels", "[", "str", "(", "split_doc", ")", "]", "=", "label", "\n", "\n", "", "print", "(", "\"not_in_either = \"", ",", "not_in_either", ")", "\n", "print", "(", "\"Len split_docs_labels = \"", ",", "len", "(", "self", ".", "labels", ")", ")", "\n", "print", "(", "\"\\nWriting test_doc_labels in: \"", ",", "self", ".", "split_docs_labels_file", ")", "\n", "with", "open", "(", "self", ".", "split_docs_labels_file", ",", "'w+'", ")", "as", "json_file", ":", "\n", "                    ", "json", ".", "dump", "(", "self", ".", "labels", ",", "json_file", ")", "\n", "\n", "", "", "else", ":", "\n", "                ", "self", ".", "labels", "=", "json", ".", "load", "(", "open", "(", "self", ".", "split_docs_labels_file", ",", "'r'", ")", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "if", "not", "os", ".", "path", ".", "isfile", "(", "self", ".", "split_docs_labels_file", ")", ":", "\n", "                ", "self", ".", "labels", "=", "{", "}", "\n", "print", "(", "\"\\nCreating labels dict of {} docs...\"", ".", "format", "(", "split", ")", ")", "\n", "not_in_either", "=", "0", "\n", "for", "split_doc", "in", "self", ".", "_split_docs", ":", "\n", "                    ", "label", "=", "doc2labels", "[", "str", "(", "split_doc", ")", "]", "\n", "self", ".", "labels", "[", "str", "(", "split_doc", ")", "]", "=", "label", "\n", "\n", "", "print", "(", "\"not_in_either = \"", ",", "not_in_either", ")", "\n", "print", "(", "\"Len split_docs_labels = \"", ",", "len", "(", "self", ".", "labels", ")", ")", "\n", "print", "(", "\"\\nWriting test_doc_labels in: \"", ",", "self", ".", "split_docs_labels_file", ")", "\n", "with", "open", "(", "self", ".", "split_docs_labels_file", ",", "'w+'", ")", "as", "json_file", ":", "\n", "                    ", "json", ".", "dump", "(", "self", ".", "labels", ",", "json_file", ")", "\n", "\n", "", "", "else", ":", "\n", "                ", "self", ".", "labels", "=", "json", ".", "load", "(", "open", "(", "self", ".", "split_docs_labels_file", ",", "'r'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.LR_Dataset.__getitem__": [[661, 692], ["str", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.squeeze", "torch.cat.squeeze", "str", "str", "doc_embed_text.unsqueeze", "doc_embed_gnn.unsqueeze", "str", "str", "str", "str"], "methods", ["None"], ["", "", "", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "\n", "# Taking just 1 document out of all the docs in the split based on the 'idx'", "\n", "        ", "if", "self", ".", "mode", "==", "'gnn'", ":", "\n", "            ", "doc", "=", "self", ".", "_split_docs", "[", "idx", "]", "\n", "doc_embed", "=", "self", ".", "_doc_embeds_gnn", "[", "self", ".", "node2id", "[", "str", "(", "doc", ")", "]", ",", ":", "]", "\n", "label", "=", "self", ".", "labels", "[", "str", "(", "doc", ")", "]", "\n", "return", "doc_embed", ",", "label", ",", "doc", "\n", "", "elif", "self", ".", "mode", "==", "'text'", ":", "\n", "            ", "try", ":", "\n", "                ", "doc", "=", "self", ".", "_split_docs", "[", "idx", "]", "\n", "# doc_txt = str(doc).split('gossipcop-')[-1]", "\n", "# doc_embed = self._doc_embeds_text[self.test2id[str(doc_txt)], :]", "\n", "doc_embed", "=", "self", ".", "_doc_embeds_text", "[", "self", ".", "test2id", "[", "str", "(", "doc", ")", "]", ",", ":", "]", "\n", "label", "=", "self", ".", "labels", "[", "str", "(", "doc", ")", "]", "\n", "", "except", ":", "\n", "                ", "return", "None", "\n", "", "return", "doc_embed", ",", "label", ",", "doc", "\n", "", "elif", "self", ".", "mode", "==", "'gnn+text'", ":", "\n", "            ", "try", ":", "\n", "                ", "doc", "=", "self", ".", "_split_docs", "[", "idx", "]", "\n", "# doc_txt = str(doc).split('gossipcop-')[-1]", "\n", "# doc_embed_text = self._doc_embeds_text[self.test2id[str(doc_txt)], :]", "\n", "doc_embed_text", "=", "self", ".", "_doc_embeds_text", "[", "self", ".", "test2id", "[", "str", "(", "doc", ")", "]", ",", ":", "]", "\n", "doc_embed_gnn", "=", "self", ".", "_doc_embeds_gnn", "[", "self", ".", "node2id", "[", "str", "(", "doc", ")", "]", ",", ":", "]", "\n", "doc_embed", "=", "torch", ".", "cat", "(", "(", "doc_embed_text", ".", "unsqueeze", "(", "1", ")", ",", "doc_embed_gnn", ".", "unsqueeze", "(", "1", ")", ")", ")", "\n", "label", "=", "self", ".", "labels", "[", "str", "(", "doc", ")", "]", "\n", "", "except", ":", "\n", "                ", "return", "None", "\n", "\n", "", "return", "doc_embed", ".", "squeeze", "(", "1", ")", ",", "label", ",", "doc", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.LR_Dataset.__len__": [[694, 696], ["None"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "data_size", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.calc_elapsed_time": [[25, 31], ["divmod", "divmod", "divmod", "divmod", "int", "int"], "function", ["None"], ["def", "calc_elapsed_time", "(", "start", ",", "end", ")", ":", "\n", "    ", "hours", ",", "rem", "=", "divmod", "(", "end", "-", "start", ",", "3600", ")", "\n", "time_hours", ",", "time_rem", "=", "divmod", "(", "end", ",", "3600", ")", "\n", "minutes", ",", "seconds", "=", "divmod", "(", "rem", ",", "60", ")", "\n", "time_mins", ",", "_", "=", "divmod", "(", "time_rem", ",", "60", ")", "\n", "return", "int", "(", "hours", ")", ",", "int", "(", "minutes", ")", ",", "seconds", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.clean_string": [[32, 37], ["re.sub", "re.sub.lower().strip().split", "re.sub.lower().strip", "re.sub.lower"], "function", ["None"], ["", "def", "clean_string", "(", "string", ")", ":", "\n", "# string = re.sub(r\"[^A-Za-z0-9(),!?\\'`]\", \" \", string)", "\n", "    ", "string", "=", "re", ".", "sub", "(", "r\"\\s{2,}\"", ",", "\" \"", ",", "string", ")", "\n", "tokens", "=", "string", ".", "lower", "(", ")", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.process_labels": [[38, 40], ["float"], "function", ["None"], ["", "def", "process_labels", "(", "string", ")", ":", "\n", "    ", "return", "[", "float", "(", "x", ")", "for", "x", "in", "string", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.process_ids": [[41, 44], ["str"], "function", ["None"], ["", "def", "process_ids", "(", "string", ")", ":", "\n", "# return [str(x) for x in string]", "\n", "    ", "return", "str", "(", "string", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.split_sents": [[45, 48], ["re.sub", "re.sub.strip().split", "re.sub.strip"], "function", ["None"], ["", "def", "split_sents", "(", "string", ")", ":", "\n", "    ", "string", "=", "re", ".", "sub", "(", "r\"[!?]\"", ",", "\" \"", ",", "string", ")", "\n", "return", "string", ".", "strip", "(", ")", ".", "split", "(", "'.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.clean_string_stop_words_remove": [[49, 56], ["re.sub", "nltk.corpus.stopwords.words", "re.sub.lower().strip().split", "re.sub.lower().strip", "re.sub.lower"], "function", ["None"], ["", "def", "clean_string_stop_words_remove", "(", "string", ")", ":", "\n", "# string = re.sub(r\"[^A-Za-z0-9(),!?\\'`]\", \" \", string)", "\n", "    ", "string", "=", "re", ".", "sub", "(", "r\"\\s{2,}\"", ",", "\" \"", ",", "string", ")", "\n", "nltk_stopwords", "=", "nltk", ".", "corpus", ".", "stopwords", ".", "words", "(", "'english'", ")", "\n", "tokens", "=", "string", ".", "lower", "(", ")", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "tokens", "=", "[", "t", "for", "t", "in", "tokens", "if", "not", "t", "in", "nltk_stopwords", "]", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.get_elmo_batches": [[276, 298], ["allennlp.modules.elmo.batch_to_ids().to", "numpy.array", "numpy.array", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "allennlp.modules.elmo.batch_to_ids", "numpy.array", "len", "len"], "function", ["None"], ["", "", "def", "get_elmo_batches", "(", "config", ",", "n_iter", ",", "iters", ",", "data", ",", "label", ")", ":", "\n", "    ", "\"\"\"\n    Elmo requires input as a list of tokens and so can not use the TorchText BucketIterator.\n    This function creates batches for Elmo in the required format. \n    \"\"\"", "\n", "\n", "# To make use of all the training examples while training", "\n", "if", "iters", "!=", "n_iter", ":", "\n", "        ", "start", "=", "iters", "*", "config", "[", "'batch_size'", "]", "\n", "end", "=", "start", "+", "config", "[", "'batch_size'", "]", "\n", "batch_text", "=", "data", "[", "start", ":", "end", "]", "\n", "sen_lens", "=", "np", ".", "array", "(", "[", "len", "(", "x", ")", "for", "x", "in", "batch_text", "]", ")", "\n", "batch_label", "=", "label", "[", "start", ":", "end", "]", "\n", "", "else", ":", "\n", "# For the final batch, the batch size will be smaller than the batch_size", "\n", "        ", "start", "=", "iters", "*", "config", "[", "'batch_size'", "]", "\n", "batch_text", "=", "data", "[", "start", ":", "]", "\n", "sen_lens", "=", "np", ".", "array", "(", "[", "len", "(", "x", ")", "for", "x", "in", "batch_text", "]", ")", "\n", "batch_label", "=", "label", "[", "start", ":", "]", "\n", "\n", "", "batch_ids", "=", "batch_to_ids", "(", "batch_text", ")", ".", "to", "(", "device", ")", "\n", "return", "batch_ids", ",", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "batch_label", ")", ")", ",", "torch", ".", "from_numpy", "(", "sen_lens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.collate_for_lr": [[563, 566], ["list", "torch.utils.data.dataloader.default_collate", "torch.utils.data.dataloader.default_collate", "filter"], "function", ["None"], ["", "", "def", "collate_for_lr", "(", "batch", ")", ":", "\n", "    ", "batch", "=", "list", "(", "filter", "(", "lambda", "x", ":", "x", "is", "not", "None", ",", "batch", ")", ")", "\n", "return", "torch", ".", "utils", ".", "data", ".", "dataloader", ".", "default_collate", "(", "batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.hyperbolicity.hyperbolicity_sample": [[13, 36], ["time.time", "tqdm.tqdm", "print", "max", "range", "time.time", "numpy.random.choice", "G.nodes", "networkx.shortest_path_length", "networkx.shortest_path_length", "networkx.shortest_path_length", "networkx.shortest_path_length", "networkx.shortest_path_length", "networkx.shortest_path_length", "s.append", "s.append", "s.append", "s.sort", "hyps.append", "time.time"], "function", ["None"], ["def", "hyperbolicity_sample", "(", "G", ",", "num_samples", "=", "50000", ")", ":", "\n", "    ", "curr_time", "=", "time", ".", "time", "(", ")", "\n", "hyps", "=", "[", "]", "\n", "for", "i", "in", "tqdm", "(", "range", "(", "num_samples", ")", ")", ":", "\n", "        ", "curr_time", "=", "time", ".", "time", "(", ")", "\n", "node_tuple", "=", "np", ".", "random", ".", "choice", "(", "G", ".", "nodes", "(", ")", ",", "4", ",", "replace", "=", "False", ")", "\n", "s", "=", "[", "]", "\n", "try", ":", "\n", "            ", "d01", "=", "nx", ".", "shortest_path_length", "(", "G", ",", "source", "=", "node_tuple", "[", "0", "]", ",", "target", "=", "node_tuple", "[", "1", "]", ",", "weight", "=", "None", ")", "\n", "d23", "=", "nx", ".", "shortest_path_length", "(", "G", ",", "source", "=", "node_tuple", "[", "2", "]", ",", "target", "=", "node_tuple", "[", "3", "]", ",", "weight", "=", "None", ")", "\n", "d02", "=", "nx", ".", "shortest_path_length", "(", "G", ",", "source", "=", "node_tuple", "[", "0", "]", ",", "target", "=", "node_tuple", "[", "2", "]", ",", "weight", "=", "None", ")", "\n", "d13", "=", "nx", ".", "shortest_path_length", "(", "G", ",", "source", "=", "node_tuple", "[", "1", "]", ",", "target", "=", "node_tuple", "[", "3", "]", ",", "weight", "=", "None", ")", "\n", "d03", "=", "nx", ".", "shortest_path_length", "(", "G", ",", "source", "=", "node_tuple", "[", "0", "]", ",", "target", "=", "node_tuple", "[", "3", "]", ",", "weight", "=", "None", ")", "\n", "d12", "=", "nx", ".", "shortest_path_length", "(", "G", ",", "source", "=", "node_tuple", "[", "1", "]", ",", "target", "=", "node_tuple", "[", "2", "]", ",", "weight", "=", "None", ")", "\n", "s", ".", "append", "(", "d01", "+", "d23", ")", "\n", "s", ".", "append", "(", "d02", "+", "d13", ")", "\n", "s", ".", "append", "(", "d03", "+", "d12", ")", "\n", "s", ".", "sort", "(", ")", "\n", "hyps", ".", "append", "(", "(", "s", "[", "-", "1", "]", "-", "s", "[", "-", "2", "]", ")", "/", "2", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "continue", "\n", "", "", "print", "(", "'Time for hyp: '", ",", "time", ".", "time", "(", ")", "-", "curr_time", ")", "\n", "return", "max", "(", "hyps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.load_data": [[12, 32], ["data_utils_hygnn.process", "data_utils_hygnn.load_data_nc", "data_utils_hygnn.load_data_lp", "data_utils_hygnn.augment", "data_utils_hygnn.mask_edges"], "function", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.process", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.load_data_nc", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.load_data_lp", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.augment", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.mask_edges"], ["def", "load_data", "(", "args", ",", "datapath", ")", ":", "\n", "    ", "if", "args", ".", "task", "==", "'nc'", ":", "\n", "        ", "data", "=", "load_data_nc", "(", "args", ".", "dataset", ",", "args", ".", "use_feats", ",", "datapath", ",", "args", ".", "split_seed", ")", "\n", "", "else", ":", "\n", "        ", "data", "=", "load_data_lp", "(", "args", ".", "dataset", ",", "args", ".", "use_feats", ",", "datapath", ")", "\n", "adj", "=", "data", "[", "'adj_train'", "]", "\n", "if", "args", ".", "task", "==", "'lp'", ":", "\n", "            ", "adj_train", ",", "train_edges", ",", "train_edges_false", ",", "val_edges", ",", "val_edges_false", ",", "test_edges", ",", "test_edges_false", "=", "mask_edges", "(", "\n", "adj", ",", "args", ".", "val_prop", ",", "args", ".", "test_prop", ",", "args", ".", "split_seed", "\n", ")", "\n", "data", "[", "'adj_train'", "]", "=", "adj_train", "\n", "data", "[", "'train_edges'", "]", ",", "data", "[", "'train_edges_false'", "]", "=", "train_edges", ",", "train_edges_false", "\n", "data", "[", "'val_edges'", "]", ",", "data", "[", "'val_edges_false'", "]", "=", "val_edges", ",", "val_edges_false", "\n", "data", "[", "'test_edges'", "]", ",", "data", "[", "'test_edges_false'", "]", "=", "test_edges", ",", "test_edges_false", "\n", "", "", "data", "[", "'adj_train_norm'", "]", ",", "data", "[", "'features'", "]", "=", "process", "(", "\n", "data", "[", "'adj_train'", "]", ",", "data", "[", "'features'", "]", ",", "args", ".", "normalize_adj", ",", "args", ".", "normalize_feats", "\n", ")", "\n", "if", "args", ".", "dataset", "==", "'airport'", ":", "\n", "        ", "data", "[", "'features'", "]", "=", "augment", "(", "data", "[", "'adj_train'", "]", ",", "data", "[", "'features'", "]", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.process": [[37, 47], ["scipy.isspmatrix", "torch.Tensor", "data_utils_hygnn.sparse_mx_to_torch_sparse_tensor", "numpy.array", "data_utils_hygnn.normalize", "data_utils_hygnn.normalize", "normalize.todense", "scipy.eye"], "function", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.sparse_mx_to_torch_sparse_tensor", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.euclidean.Euclidean.normalize", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.euclidean.Euclidean.normalize"], ["", "def", "process", "(", "adj", ",", "features", ",", "normalize_adj", ",", "normalize_feats", ")", ":", "\n", "    ", "if", "sp", ".", "isspmatrix", "(", "features", ")", ":", "\n", "        ", "features", "=", "np", ".", "array", "(", "features", ".", "todense", "(", ")", ")", "\n", "", "if", "normalize_feats", ":", "\n", "        ", "features", "=", "normalize", "(", "features", ")", "\n", "", "features", "=", "torch", ".", "Tensor", "(", "features", ")", "\n", "if", "normalize_adj", ":", "\n", "        ", "adj", "=", "normalize", "(", "adj", "+", "sp", ".", "eye", "(", "adj", ".", "shape", "[", "0", "]", ")", ")", "\n", "", "adj", "=", "sparse_mx_to_torch_sparse_tensor", "(", "adj", ")", "\n", "return", "adj", ",", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.normalize": [[49, 57], ["numpy.array", "numpy.power().flatten", "scipy.diags", "sp.diags.dot", "r_mat_inv.dot.sum", "numpy.power", "numpy.isinf"], "function", ["None"], ["", "def", "normalize", "(", "mx", ")", ":", "\n", "    ", "\"\"\"Row-normalize sparse matrix.\"\"\"", "\n", "rowsum", "=", "np", ".", "array", "(", "mx", ".", "sum", "(", "1", ")", ")", "\n", "r_inv", "=", "np", ".", "power", "(", "rowsum", ",", "-", "1", ")", ".", "flatten", "(", ")", "\n", "r_inv", "[", "np", ".", "isinf", "(", "r_inv", ")", "]", "=", "0.", "\n", "r_mat_inv", "=", "sp", ".", "diags", "(", "r_inv", ")", "\n", "mx", "=", "r_mat_inv", ".", "dot", "(", "mx", ")", "\n", "return", "mx", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.sparse_mx_to_torch_sparse_tensor": [[59, 68], ["sparse_mx.tocoo.tocoo", "torch.from_numpy", "torch.Tensor", "torch.Size", "torch.sparse.FloatTensor", "numpy.vstack().astype", "numpy.vstack"], "function", ["None"], ["", "def", "sparse_mx_to_torch_sparse_tensor", "(", "sparse_mx", ")", ":", "\n", "    ", "\"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"", "\n", "sparse_mx", "=", "sparse_mx", ".", "tocoo", "(", ")", "\n", "indices", "=", "torch", ".", "from_numpy", "(", "\n", "np", ".", "vstack", "(", "(", "sparse_mx", ".", "row", ",", "sparse_mx", ".", "col", ")", ")", ".", "astype", "(", "np", ".", "int64", ")", "\n", ")", "\n", "values", "=", "torch", ".", "Tensor", "(", "sparse_mx", ".", "data", ")", "\n", "shape", "=", "torch", ".", "Size", "(", "sparse_mx", ".", "shape", ")", "\n", "return", "torch", ".", "sparse", ".", "FloatTensor", "(", "indices", ",", "values", ",", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.augment": [[70, 77], ["numpy.squeeze", "torch.tensor().squeeze", "torch.ones", "torch.cat", "numpy.sum().astype", "torch.cat.size", "torch.tensor", "numpy.sum", "numpy.eye"], "function", ["None"], ["", "def", "augment", "(", "adj", ",", "features", ",", "normalize_feats", "=", "True", ")", ":", "\n", "    ", "deg", "=", "np", ".", "squeeze", "(", "np", ".", "sum", "(", "adj", ",", "axis", "=", "0", ")", ".", "astype", "(", "int", ")", ")", "\n", "deg", "[", "deg", ">", "5", "]", "=", "5", "\n", "deg_onehot", "=", "torch", ".", "tensor", "(", "np", ".", "eye", "(", "6", ")", "[", "deg", "]", ",", "dtype", "=", "torch", ".", "float", ")", ".", "squeeze", "(", ")", "\n", "const_f", "=", "torch", ".", "ones", "(", "features", ".", "size", "(", "0", ")", ",", "1", ")", "\n", "features", "=", "torch", ".", "cat", "(", "(", "features", ",", "deg_onehot", ",", "const_f", ")", ",", "dim", "=", "1", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.mask_edges": [[82, 103], ["numpy.random.seed", "scipy.triu().nonzero", "numpy.array", "numpy.random.shuffle", "scipy.triu().nonzero", "numpy.array", "numpy.random.shuffle", "len", "int", "int", "numpy.concatenate", "scipy.csr_matrix", "list", "list", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "scipy.triu", "zip", "scipy.triu", "zip", "numpy.ones", "scipy.csr_matrix", "adj.toarray"], "function", ["None"], ["", "def", "mask_edges", "(", "adj", ",", "val_prop", ",", "test_prop", ",", "seed", ")", ":", "\n", "    ", "np", ".", "random", ".", "seed", "(", "seed", ")", "# get tp edges", "\n", "x", ",", "y", "=", "sp", ".", "triu", "(", "adj", ")", ".", "nonzero", "(", ")", "\n", "pos_edges", "=", "np", ".", "array", "(", "list", "(", "zip", "(", "x", ",", "y", ")", ")", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "pos_edges", ")", "\n", "# get tn edges", "\n", "x", ",", "y", "=", "sp", ".", "triu", "(", "sp", ".", "csr_matrix", "(", "1.", "-", "adj", ".", "toarray", "(", ")", ")", ")", ".", "nonzero", "(", ")", "\n", "neg_edges", "=", "np", ".", "array", "(", "list", "(", "zip", "(", "x", ",", "y", ")", ")", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "neg_edges", ")", "\n", "\n", "m_pos", "=", "len", "(", "pos_edges", ")", "\n", "n_val", "=", "int", "(", "m_pos", "*", "val_prop", ")", "\n", "n_test", "=", "int", "(", "m_pos", "*", "test_prop", ")", "\n", "val_edges", ",", "test_edges", ",", "train_edges", "=", "pos_edges", "[", ":", "n_val", "]", ",", "pos_edges", "[", "n_val", ":", "n_test", "+", "n_val", "]", ",", "pos_edges", "[", "n_test", "+", "n_val", ":", "]", "\n", "val_edges_false", ",", "test_edges_false", "=", "neg_edges", "[", ":", "n_val", "]", ",", "neg_edges", "[", "n_val", ":", "n_test", "+", "n_val", "]", "\n", "train_edges_false", "=", "np", ".", "concatenate", "(", "[", "neg_edges", ",", "val_edges", ",", "test_edges", "]", ",", "axis", "=", "0", ")", "\n", "adj_train", "=", "sp", ".", "csr_matrix", "(", "(", "np", ".", "ones", "(", "train_edges", ".", "shape", "[", "0", "]", ")", ",", "(", "train_edges", "[", ":", ",", "0", "]", ",", "train_edges", "[", ":", ",", "1", "]", ")", ")", ",", "shape", "=", "adj", ".", "shape", ")", "\n", "adj_train", "=", "adj_train", "+", "adj_train", ".", "T", "\n", "return", "adj_train", ",", "torch", ".", "LongTensor", "(", "train_edges", ")", ",", "torch", ".", "LongTensor", "(", "train_edges_false", ")", ",", "torch", ".", "LongTensor", "(", "val_edges", ")", ",", "torch", ".", "LongTensor", "(", "val_edges_false", ")", ",", "torch", ".", "LongTensor", "(", "test_edges", ")", ",", "torch", ".", "LongTensor", "(", "\n", "test_edges_false", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.split_data": [[105, 123], ["numpy.random.seed", "numpy.arange", "numpy.random.shuffle", "numpy.random.shuffle", "pos_idx.tolist.tolist", "neg_idx.tolist.tolist", "min", "round", "round", "labels.nonzero", "len", "len"], "function", ["None"], ["", "def", "split_data", "(", "labels", ",", "val_prop", ",", "test_prop", ",", "seed", ")", ":", "\n", "    ", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "nb_nodes", "=", "labels", ".", "shape", "[", "0", "]", "\n", "all_idx", "=", "np", ".", "arange", "(", "nb_nodes", ")", "\n", "pos_idx", "=", "labels", ".", "nonzero", "(", ")", "[", "0", "]", "\n", "neg_idx", "=", "(", "1.", "-", "labels", ")", ".", "nonzero", "(", ")", "[", "0", "]", "\n", "np", ".", "random", ".", "shuffle", "(", "pos_idx", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "neg_idx", ")", "\n", "pos_idx", "=", "pos_idx", ".", "tolist", "(", ")", "\n", "neg_idx", "=", "neg_idx", ".", "tolist", "(", ")", "\n", "nb_pos_neg", "=", "min", "(", "len", "(", "pos_idx", ")", ",", "len", "(", "neg_idx", ")", ")", "\n", "nb_val", "=", "round", "(", "val_prop", "*", "nb_pos_neg", ")", "\n", "nb_test", "=", "round", "(", "test_prop", "*", "nb_pos_neg", ")", "\n", "idx_val_pos", ",", "idx_test_pos", ",", "idx_train_pos", "=", "pos_idx", "[", ":", "nb_val", "]", ",", "pos_idx", "[", "nb_val", ":", "nb_val", "+", "nb_test", "]", ",", "pos_idx", "[", "\n", "nb_val", "+", "nb_test", ":", "]", "\n", "idx_val_neg", ",", "idx_test_neg", ",", "idx_train_neg", "=", "neg_idx", "[", ":", "nb_val", "]", ",", "neg_idx", "[", "nb_val", ":", "nb_val", "+", "nb_test", "]", ",", "neg_idx", "[", "\n", "nb_val", "+", "nb_test", ":", "]", "\n", "return", "idx_val_pos", "+", "idx_val_neg", ",", "idx_test_pos", "+", "idx_test_neg", ",", "idx_train_pos", "+", "idx_train_neg", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.bin_feat": [[125, 128], ["numpy.digitize", "np.digitize.min"], "function", ["None"], ["", "def", "bin_feat", "(", "feat", ",", "bins", ")", ":", "\n", "    ", "digitized", "=", "np", ".", "digitize", "(", "feat", ",", "bins", ")", "\n", "return", "digitized", "-", "digitized", ".", "min", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.load_data_lp": [[133, 144], ["data_utils_hygnn.load_citation_data", "data_utils_hygnn.load_synthetic_data", "data_utils_hygnn.load_data_airport", "FileNotFoundError"], "function", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.load_citation_data", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.load_synthetic_data", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.load_data_airport"], ["", "def", "load_data_lp", "(", "dataset", ",", "use_feats", ",", "data_path", ")", ":", "\n", "    ", "if", "dataset", "in", "[", "'cora'", ",", "'pubmed'", "]", ":", "\n", "        ", "adj", ",", "features", "=", "load_citation_data", "(", "dataset", ",", "use_feats", ",", "data_path", ")", "[", ":", "2", "]", "\n", "", "elif", "dataset", "==", "'disease_lp'", ":", "\n", "        ", "adj", ",", "features", "=", "load_synthetic_data", "(", "dataset", ",", "use_feats", ",", "data_path", ")", "[", ":", "2", "]", "\n", "", "elif", "dataset", "==", "'airport'", ":", "\n", "        ", "adj", ",", "features", "=", "load_data_airport", "(", "dataset", ",", "data_path", ",", "return_label", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "'Dataset {} is not supported.'", ".", "format", "(", "dataset", ")", ")", "\n", "", "data", "=", "{", "'adj_train'", ":", "adj", ",", "'features'", ":", "features", "}", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.load_data_nc": [[149, 168], ["torch.LongTensor", "data_utils_hygnn.load_citation_data", "data_utils_hygnn.split_data", "data_utils_hygnn.load_synthetic_data", "data_utils_hygnn.load_data_airport", "FileNotFoundError"], "function", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.load_citation_data", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.split_data", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.load_synthetic_data", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.load_data_airport"], ["", "def", "load_data_nc", "(", "dataset", ",", "use_feats", ",", "data_path", ",", "split_seed", ")", ":", "\n", "    ", "if", "dataset", "in", "[", "'cora'", ",", "'pubmed'", "]", ":", "\n", "        ", "adj", ",", "features", ",", "labels", ",", "idx_train", ",", "idx_val", ",", "idx_test", "=", "load_citation_data", "(", "\n", "dataset", ",", "use_feats", ",", "data_path", ",", "split_seed", "\n", ")", "\n", "", "else", ":", "\n", "        ", "if", "dataset", "==", "'disease_nc'", ":", "\n", "            ", "adj", ",", "features", ",", "labels", "=", "load_synthetic_data", "(", "dataset", ",", "use_feats", ",", "data_path", ")", "\n", "val_prop", ",", "test_prop", "=", "0.10", ",", "0.60", "\n", "", "elif", "dataset", "==", "'airport'", ":", "\n", "            ", "adj", ",", "features", ",", "labels", "=", "load_data_airport", "(", "dataset", ",", "data_path", ",", "return_label", "=", "True", ")", "\n", "val_prop", ",", "test_prop", "=", "0.15", ",", "0.15", "\n", "", "else", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "'Dataset {} is not supported.'", ".", "format", "(", "dataset", ")", ")", "\n", "", "idx_val", ",", "idx_test", ",", "idx_train", "=", "split_data", "(", "labels", ",", "val_prop", ",", "test_prop", ",", "seed", "=", "split_seed", ")", "\n", "\n", "", "labels", "=", "torch", ".", "LongTensor", "(", "labels", ")", "\n", "data", "=", "{", "'adj_train'", ":", "adj", ",", "'features'", ":", "features", ",", "'labels'", ":", "labels", ",", "'idx_train'", ":", "idx_train", ",", "'idx_val'", ":", "idx_val", ",", "'idx_test'", ":", "idx_test", "}", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.load_citation_data": [[173, 202], ["range", "tuple", "data_utils_hygnn.parse_index_file", "numpy.sort", "scipy.vstack().tolil", "numpy.vstack", "numpy.argmax", "np.sort.tolist", "list", "range", "networkx.adjacency_matrix", "len", "os.path.join", "range", "len", "networkx.from_dict_of_lists", "scipy.eye", "open", "scipy.vstack", "len", "len", "os.path.join", "objects.append", "objects.append", "pickle.load", "pickle.load"], "function", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.parse_index_file"], ["", "def", "load_citation_data", "(", "dataset_str", ",", "use_feats", ",", "data_path", ",", "split_seed", "=", "None", ")", ":", "\n", "    ", "names", "=", "[", "'x'", ",", "'y'", ",", "'tx'", ",", "'ty'", ",", "'allx'", ",", "'ally'", ",", "'graph'", "]", "\n", "objects", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "names", ")", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "\"ind.{}.{}\"", ".", "format", "(", "dataset_str", ",", "names", "[", "i", "]", ")", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "if", "sys", ".", "version_info", ">", "(", "3", ",", "0", ")", ":", "\n", "                ", "objects", ".", "append", "(", "pkl", ".", "load", "(", "f", ",", "encoding", "=", "'latin1'", ")", ")", "\n", "", "else", ":", "\n", "                ", "objects", ".", "append", "(", "pkl", ".", "load", "(", "f", ")", ")", "\n", "\n", "", "", "", "x", ",", "y", ",", "tx", ",", "ty", ",", "allx", ",", "ally", ",", "graph", "=", "tuple", "(", "objects", ")", "\n", "test_idx_reorder", "=", "parse_index_file", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "\"ind.{}.test.index\"", ".", "format", "(", "dataset_str", ")", ")", ")", "\n", "test_idx_range", "=", "np", ".", "sort", "(", "test_idx_reorder", ")", "\n", "\n", "features", "=", "sp", ".", "vstack", "(", "(", "allx", ",", "tx", ")", ")", ".", "tolil", "(", ")", "\n", "features", "[", "test_idx_reorder", ",", ":", "]", "=", "features", "[", "test_idx_range", ",", ":", "]", "\n", "\n", "labels", "=", "np", ".", "vstack", "(", "(", "ally", ",", "ty", ")", ")", "\n", "labels", "[", "test_idx_reorder", ",", ":", "]", "=", "labels", "[", "test_idx_range", ",", ":", "]", "\n", "labels", "=", "np", ".", "argmax", "(", "labels", ",", "1", ")", "\n", "\n", "idx_test", "=", "test_idx_range", ".", "tolist", "(", ")", "\n", "idx_train", "=", "list", "(", "range", "(", "len", "(", "y", ")", ")", ")", "\n", "idx_val", "=", "range", "(", "len", "(", "y", ")", ",", "len", "(", "y", ")", "+", "500", ")", "\n", "\n", "adj", "=", "nx", ".", "adjacency_matrix", "(", "nx", ".", "from_dict_of_lists", "(", "graph", ")", ")", "\n", "if", "not", "use_feats", ":", "\n", "        ", "features", "=", "sp", ".", "eye", "(", "adj", ".", "shape", "[", "0", "]", ")", "\n", "", "return", "adj", ",", "features", ",", "labels", ",", "idx_train", ",", "idx_val", ",", "idx_test", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.parse_index_file": [[204, 209], ["open", "index.append", "int", "line.strip"], "function", ["None"], ["", "def", "parse_index_file", "(", "filename", ")", ":", "\n", "    ", "index", "=", "[", "]", "\n", "for", "line", "in", "open", "(", "filename", ")", ":", "\n", "        ", "index", ".", "append", "(", "int", "(", "line", ".", "strip", "(", ")", ")", ")", "\n", "", "return", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.load_synthetic_data": [[211, 242], ["numpy.zeros", "numpy.load", "open", "f.readlines", "line.rstrip().split", "edges.append", "scipy.load_npz", "scipy.eye", "os.path.join", "scipy.csr_matrix", "os.path.join", "len", "len", "os.path.join", "line.rstrip"], "function", ["None"], ["", "def", "load_synthetic_data", "(", "dataset_str", ",", "use_feats", ",", "data_path", ")", ":", "\n", "    ", "object_to_idx", "=", "{", "}", "\n", "idx_counter", "=", "0", "\n", "edges", "=", "[", "]", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "\"{}.edges.csv\"", ".", "format", "(", "dataset_str", ")", ")", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "all_edges", "=", "f", ".", "readlines", "(", ")", "\n", "", "for", "line", "in", "all_edges", ":", "\n", "        ", "n1", ",", "n2", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", "','", ")", "\n", "if", "n1", "in", "object_to_idx", ":", "\n", "            ", "i", "=", "object_to_idx", "[", "n1", "]", "\n", "", "else", ":", "\n", "            ", "i", "=", "idx_counter", "\n", "object_to_idx", "[", "n1", "]", "=", "i", "\n", "idx_counter", "+=", "1", "\n", "", "if", "n2", "in", "object_to_idx", ":", "\n", "            ", "j", "=", "object_to_idx", "[", "n2", "]", "\n", "", "else", ":", "\n", "            ", "j", "=", "idx_counter", "\n", "object_to_idx", "[", "n2", "]", "=", "j", "\n", "idx_counter", "+=", "1", "\n", "", "edges", ".", "append", "(", "(", "i", ",", "j", ")", ")", "\n", "", "adj", "=", "np", ".", "zeros", "(", "(", "len", "(", "object_to_idx", ")", ",", "len", "(", "object_to_idx", ")", ")", ")", "\n", "for", "i", ",", "j", "in", "edges", ":", "\n", "        ", "adj", "[", "i", ",", "j", "]", "=", "1.", "# comment this line for directed adjacency matrix", "\n", "adj", "[", "j", ",", "i", "]", "=", "1.", "\n", "", "if", "use_feats", ":", "\n", "        ", "features", "=", "sp", ".", "load_npz", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "\"{}.feats.npz\"", ".", "format", "(", "dataset_str", ")", ")", ")", "\n", "", "else", ":", "\n", "        ", "features", "=", "sp", ".", "eye", "(", "adj", ".", "shape", "[", "0", "]", ")", "\n", "", "labels", "=", "np", ".", "load", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "\"{}.labels.npy\"", ".", "format", "(", "dataset_str", ")", ")", ")", "\n", "return", "sp", ".", "csr_matrix", "(", "adj", ")", ",", "features", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.load_data_airport": [[244, 256], ["pickle.load", "networkx.adjacency_matrix", "numpy.array", "open", "data_utils_hygnn.bin_feat", "os.path.join", "scipy.csr_matrix", "scipy.csr_matrix", "pkl.load.nodes"], "function", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_hygnn.bin_feat"], ["", "def", "load_data_airport", "(", "dataset_str", ",", "data_path", ",", "return_label", "=", "False", ")", ":", "\n", "    ", "graph", "=", "pkl", ".", "load", "(", "open", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "dataset_str", "+", "'.p'", ")", ",", "'rb'", ")", ")", "\n", "adj", "=", "nx", ".", "adjacency_matrix", "(", "graph", ")", "\n", "features", "=", "np", ".", "array", "(", "[", "graph", ".", "node", "[", "u", "]", "[", "'feat'", "]", "for", "u", "in", "graph", ".", "nodes", "(", ")", "]", ")", "\n", "if", "return_label", ":", "\n", "        ", "label_idx", "=", "4", "\n", "labels", "=", "features", "[", ":", ",", "label_idx", "]", "\n", "features", "=", "features", "[", ":", ",", ":", "label_idx", "]", "\n", "labels", "=", "bin_feat", "(", "labels", ",", "bins", "=", "[", "7.0", "/", "7", ",", "8.0", "/", "7", ",", "9.0", "/", "7", "]", ")", "\n", "return", "sp", ".", "csr_matrix", "(", "adj", ")", ",", "features", ",", "labels", "\n", "", "else", ":", "\n", "        ", "return", "sp", ".", "csr_matrix", "(", "adj", ")", ",", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.Artanh.forward": [[31, 37], ["x.clamp.clamp.clamp", "ctx.save_for_backward", "x.clamp.clamp.double", "torch.log_().sub_().mul_().to", "torch.log_().sub_().mul_", "torch.log_().sub_", "torch.log_", "torch.log_"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", ".", "clamp", "(", "-", "1", "+", "1e-15", ",", "1", "-", "1e-15", ")", "\n", "ctx", ".", "save_for_backward", "(", "x", ")", "\n", "z", "=", "x", ".", "double", "(", ")", "\n", "return", "(", "torch", ".", "log_", "(", "1", "+", "z", ")", ".", "sub_", "(", "torch", ".", "log_", "(", "1", "-", "z", ")", ")", ")", ".", "mul_", "(", "0.5", ")", ".", "to", "(", "x", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.Artanh.backward": [[38, 42], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "input", ",", "=", "ctx", ".", "saved_tensors", "\n", "return", "grad_output", "/", "(", "1", "-", "input", "**", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.Arsinh.forward": [[45, 50], ["ctx.save_for_backward", "x.double", "torch.sqrt_", "x.double.pow"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "x", ")", "\n", "z", "=", "x", ".", "double", "(", ")", "\n", "return", "(", "z", "+", "torch", ".", "sqrt_", "(", "1", "+", "z", ".", "pow", "(", "2", ")", ")", ")", ".", "clamp_min_", "(", "1e-15", ")", ".", "log_", "(", ")", ".", "to", "(", "x", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.Arsinh.backward": [[51, 55], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "input", ",", "=", "ctx", ".", "saved_tensors", "\n", "return", "grad_output", "/", "(", "1", "+", "input", "**", "2", ")", "**", "0.5", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.Arcosh.forward": [[58, 64], ["x.clamp.clamp.clamp", "ctx.save_for_backward", "x.clamp.clamp.double", "torch.sqrt_", "x.clamp.double.pow"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", ".", "clamp", "(", "min", "=", "1.0", "+", "1e-15", ")", "\n", "ctx", ".", "save_for_backward", "(", "x", ")", "\n", "z", "=", "x", ".", "double", "(", ")", "\n", "return", "(", "z", "+", "torch", ".", "sqrt_", "(", "z", ".", "pow", "(", "2", ")", "-", "1", ")", ")", ".", "clamp_min_", "(", "1e-15", ")", ".", "log_", "(", ")", ".", "to", "(", "x", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.Arcosh.backward": [[65, 69], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "input", ",", "=", "ctx", ".", "saved_tensors", "\n", "return", "grad_output", "/", "(", "input", "**", "2", "-", "1", ")", "**", "0.5", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.cosh": [[6, 8], ["x.clamp().cosh", "x.clamp"], "function", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.cosh"], ["def", "cosh", "(", "x", ",", "clamp", "=", "15", ")", ":", "\n", "    ", "return", "x", ".", "clamp", "(", "-", "clamp", ",", "clamp", ")", ".", "cosh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.sinh": [[10, 12], ["x.clamp().sinh", "x.clamp"], "function", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.sinh"], ["", "def", "sinh", "(", "x", ",", "clamp", "=", "15", ")", ":", "\n", "    ", "return", "x", ".", "clamp", "(", "-", "clamp", ",", "clamp", ")", ".", "sinh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.tanh": [[14, 16], ["x.clamp().tanh", "x.clamp"], "function", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.tanh"], ["", "def", "tanh", "(", "x", ",", "clamp", "=", "15", ")", ":", "\n", "    ", "return", "x", ".", "clamp", "(", "-", "clamp", ",", "clamp", ")", ".", "tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.arcosh": [[18, 20], ["Arcosh.apply"], "function", ["None"], ["", "def", "arcosh", "(", "x", ")", ":", "\n", "    ", "return", "Arcosh", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.arsinh": [[22, 24], ["Arsinh.apply"], "function", ["None"], ["", "def", "arsinh", "(", "x", ")", ":", "\n", "    ", "return", "Arsinh", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.artanh": [[26, 28], ["Artanh.apply"], "function", ["None"], ["", "def", "artanh", "(", "x", ")", ":", "\n", "    ", "return", "Artanh", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.train_utils.format_metrics": [[9, 13], ["metrics.items"], "function", ["None"], ["def", "format_metrics", "(", "metrics", ",", "split", ")", ":", "\n", "    ", "\"\"\"Format metric in metric dict for logging.\"\"\"", "\n", "return", "\" \"", ".", "join", "(", "\n", "[", "\"{}_{}: {:.4f}\"", ".", "format", "(", "split", ",", "metric_name", ",", "metric_val", ")", "for", "metric_name", ",", "metric_val", "in", "metrics", ".", "items", "(", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.train_utils.get_dir_name": [[15, 46], ["os.path.exists", "os.path.join", "os.makedirs", "numpy.array().astype", "os.path.join", "os.makedirs", "len", "str", "numpy.array", "np.array().astype.max", "os.listdir", "os.path.isdir", "os.path.join"], "function", ["None"], ["", "def", "get_dir_name", "(", "models_dir", ")", ":", "\n", "    ", "\"\"\"Gets a directory to save the model.\n\n    If the directory already exists, then append a new integer to the end of\n    it. This method is useful so that we don't overwrite existing models\n    when launching new jobs.\n\n    Args:\n        models_dir: The directory where all the models are.\n\n    Returns:\n        The name of a new directory to save the training logs and model weights.\n    \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "models_dir", ")", ":", "\n", "        ", "save_dir", "=", "os", ".", "path", ".", "join", "(", "models_dir", ",", "'0'", ")", "\n", "os", ".", "makedirs", "(", "save_dir", ")", "\n", "", "else", ":", "\n", "        ", "existing_dirs", "=", "np", ".", "array", "(", "\n", "[", "\n", "d", "\n", "for", "d", "in", "os", ".", "listdir", "(", "models_dir", ")", "\n", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "models_dir", ",", "d", ")", ")", "\n", "]", "\n", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "if", "len", "(", "existing_dirs", ")", ">", "0", ":", "\n", "            ", "dir_id", "=", "str", "(", "existing_dirs", ".", "max", "(", ")", "+", "1", ")", "\n", "", "else", ":", "\n", "            ", "dir_id", "=", "\"1\"", "\n", "", "save_dir", "=", "os", ".", "path", ".", "join", "(", "models_dir", ",", "dir_id", ")", "\n", "os", ".", "makedirs", "(", "save_dir", ")", "\n", "", "return", "save_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.train_utils.add_flags_from_config": [[48, 93], ["isinstance", "x.lower", "train_utils.add_flags_from_config", "isinstance", "print", "str", "add_flags_from_config.add_argument", "type", "len", "add_flags_from_config.add_argument", "add_flags_from_config.add_argument", "train_utils.add_flags_from_config.OrNone"], "function", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.train_utils.add_flags_from_config"], ["", "def", "add_flags_from_config", "(", "parser", ",", "config_dict", ")", ":", "\n", "    ", "\"\"\"\n    Adds a flag (and default value) to an ArgumentParser for each parameter in a config\n    \"\"\"", "\n", "\n", "def", "OrNone", "(", "default", ")", ":", "\n", "        ", "def", "func", "(", "x", ")", ":", "\n", "# Convert \"none\" to proper None object", "\n", "            ", "if", "x", ".", "lower", "(", ")", "==", "\"none\"", ":", "\n", "                ", "return", "None", "\n", "# If default is None (and x is not None), return x without conversion as str", "\n", "", "elif", "default", "is", "None", ":", "\n", "                ", "return", "str", "(", "x", ")", "\n", "# Otherwise, default has non-None type; convert x to that type", "\n", "", "else", ":", "\n", "                ", "return", "type", "(", "default", ")", "(", "x", ")", "\n", "\n", "", "", "return", "func", "\n", "\n", "", "for", "param", "in", "config_dict", ":", "\n", "        ", "default", ",", "description", "=", "config_dict", "[", "param", "]", "\n", "try", ":", "\n", "            ", "if", "isinstance", "(", "default", ",", "dict", ")", ":", "\n", "                ", "parser", "=", "add_flags_from_config", "(", "parser", ",", "default", ")", "\n", "", "elif", "isinstance", "(", "default", ",", "list", ")", ":", "\n", "                ", "if", "len", "(", "default", ")", ">", "0", ":", "\n", "# pass a list as argument", "\n", "                    ", "parser", ".", "add_argument", "(", "\n", "f\"--{param}\"", ",", "\n", "action", "=", "\"append\"", ",", "\n", "type", "=", "type", "(", "default", "[", "0", "]", ")", ",", "\n", "default", "=", "default", ",", "\n", "help", "=", "description", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "pass", "\n", "parser", ".", "add_argument", "(", "f\"--{param}\"", ",", "action", "=", "\"append\"", ",", "default", "=", "default", ",", "help", "=", "description", ")", "\n", "", "", "else", ":", "\n", "                ", "pass", "\n", "parser", ".", "add_argument", "(", "f\"--{param}\"", ",", "type", "=", "OrNone", "(", "default", ")", ",", "default", "=", "default", ",", "help", "=", "description", ")", "\n", "", "", "except", "argparse", ".", "ArgumentError", ":", "\n", "            ", "print", "(", "\n", "f\"Could not add flag for param {param} because it was already present.\"", "\n", ")", "\n", "", "", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.eval_utils.acc_f1": [[3, 11], ["[].type_as", "sklearn.metrics.accuracy_score", "sklearn.metrics.f1_score", "preds.cpu.cpu", "labels.cpu.cpu", "output.max"], "function", ["None"], ["def", "acc_f1", "(", "output", ",", "labels", ",", "average", "=", "'binary'", ")", ":", "\n", "    ", "preds", "=", "output", ".", "max", "(", "1", ")", "[", "1", "]", ".", "type_as", "(", "labels", ")", "\n", "if", "preds", ".", "is_cuda", ":", "\n", "        ", "preds", "=", "preds", ".", "cpu", "(", ")", "\n", "labels", "=", "labels", ".", "cpu", "(", ")", "\n", "", "accuracy", "=", "accuracy_score", "(", "preds", ",", "labels", ")", "\n", "f1", "=", "f1_score", "(", "preds", ",", "labels", ",", "average", "=", "average", ")", "\n", "return", "accuracy", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.gnn_explainer.GNNExplainer.__init__": [[39, 45], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "model", ",", "epochs", "=", "100", ",", "lr", "=", "0.01", ",", "log", "=", "True", ")", ":", "\n", "        ", "super", "(", "GNNExplainer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "epochs", "=", "epochs", "\n", "self", ".", "lr", "=", "lr", "\n", "self", ".", "log", "=", "log", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.gnn_explainer.GNNExplainer.__set_masks__": [[46, 59], ["torch.nn.Parameter", "torch.nn.Parameter", "gnn_explainer.GNNExplainer.model.modules", "x.size", "edge_index.size", "torch.nn.init.calculate_gain", "math.sqrt", "isinstance", "torch.randn", "torch.randn"], "methods", ["None"], ["", "def", "__set_masks__", "(", "self", ",", "x", ",", "edge_index", ",", "init", "=", "\"normal\"", ")", ":", "\n", "        ", "(", "N", ",", "F", ")", ",", "E", "=", "x", ".", "size", "(", ")", ",", "edge_index", ".", "size", "(", "1", ")", "\n", "\n", "std", "=", "0.1", "\n", "self", ".", "node_feat_mask", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "F", ")", "*", "0.1", ")", "\n", "\n", "std", "=", "torch", ".", "nn", ".", "init", ".", "calculate_gain", "(", "'relu'", ")", "*", "sqrt", "(", "2.0", "/", "(", "2", "*", "N", ")", ")", "\n", "self", ".", "edge_mask", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "E", ")", "*", "std", ")", "\n", "\n", "for", "module", "in", "self", ".", "model", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "module", ",", "MessagePassing", ")", ":", "\n", "                ", "module", ".", "__explain__", "=", "True", "\n", "module", ".", "__edge_mask__", "=", "self", ".", "edge_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.gnn_explainer.GNNExplainer.__clear_masks__": [[60, 67], ["gnn_explainer.GNNExplainer.model.modules", "isinstance"], "methods", ["None"], ["", "", "", "def", "__clear_masks__", "(", "self", ")", ":", "\n", "        ", "for", "module", "in", "self", ".", "model", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "module", ",", "MessagePassing", ")", ":", "\n", "                ", "module", ".", "__explain__", "=", "False", "\n", "module", ".", "__edge_mask__", "=", "None", "\n", "", "", "self", ".", "node_feat_masks", "=", "None", "\n", "self", ".", "edge_mask", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.gnn_explainer.GNNExplainer.__num_hops__": [[68, 74], ["gnn_explainer.GNNExplainer.model.modules", "isinstance"], "methods", ["None"], ["", "def", "__num_hops__", "(", "self", ")", ":", "\n", "        ", "num_hops", "=", "0", "\n", "for", "module", "in", "self", ".", "model", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "module", ",", "MessagePassing", ")", ":", "\n", "                ", "num_hops", "+=", "1", "\n", "", "", "return", "num_hops", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.gnn_explainer.GNNExplainer.__flow__": [[75, 80], ["gnn_explainer.GNNExplainer.model.modules", "isinstance"], "methods", ["None"], ["", "def", "__flow__", "(", "self", ")", ":", "\n", "        ", "for", "module", "in", "self", ".", "model", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "module", ",", "MessagePassing", ")", ":", "\n", "                ", "return", "module", ".", "flow", "\n", "", "", "return", "'source_to_target'", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.gnn_explainer.GNNExplainer.__subgraph__": [[81, 97], ["torch_geometric.utils.k_hop_subgraph", "kwargs.items", "x.size", "edge_index.size", "gnn_explainer.GNNExplainer.__num_hops__", "gnn_explainer.GNNExplainer.__flow__", "torch.is_tensor", "item.size", "torch.is_tensor", "item.size"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.gnn_explainer.GNNExplainer.__num_hops__", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.gnn_explainer.GNNExplainer.__flow__"], ["", "def", "__subgraph__", "(", "self", ",", "node_idx", ",", "x", ",", "edge_index", ",", "**", "kwargs", ")", ":", "\n", "        ", "num_nodes", ",", "num_edges", "=", "x", ".", "size", "(", "0", ")", ",", "edge_index", ".", "size", "(", "1", ")", "\n", "\n", "subset", ",", "edge_index", ",", "mapping", ",", "edge_mask", "=", "k_hop_subgraph", "(", "\n", "node_idx", ",", "self", ".", "__num_hops__", "(", ")", ",", "edge_index", ",", "relabel_nodes", "=", "True", ",", "\n", "num_nodes", "=", "num_nodes", ",", "flow", "=", "self", ".", "__flow__", "(", ")", ")", "\n", "\n", "x", "=", "x", "[", "subset", "]", "\n", "for", "key", ",", "item", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "if", "torch", ".", "is_tensor", "(", "item", ")", "and", "item", ".", "size", "(", "0", ")", "==", "num_nodes", ":", "\n", "                ", "item", "=", "item", "[", "subset", "]", "\n", "", "elif", "torch", ".", "is_tensor", "(", "item", ")", "and", "item", ".", "size", "(", "0", ")", "==", "num_edges", ":", "\n", "                ", "item", "=", "item", "[", "edge_mask", "]", "\n", "", "kwargs", "[", "key", "]", "=", "item", "\n", "\n", "", "return", "x", ",", "edge_index", ",", "mapping", ",", "edge_mask", ",", "kwargs", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.gnn_explainer.GNNExplainer.__loss__": [[98, 112], ["gnn_explainer.GNNExplainer.edge_mask.sigmoid", "gnn_explainer.GNNExplainer.node_feat_mask.sigmoid", "gnn_explainer.GNNExplainer.sum", "torch.log", "torch.log", "ent.mean", "gnn_explainer.GNNExplainer.sum", "torch.log", "torch.log", "ent.mean"], "methods", ["None"], ["", "def", "__loss__", "(", "self", ",", "node_idx", ",", "log_logits", ",", "pred_label", ")", ":", "\n", "        ", "loss", "=", "-", "log_logits", "[", "node_idx", ",", "pred_label", "[", "node_idx", "]", "]", "\n", "\n", "m", "=", "self", ".", "edge_mask", ".", "sigmoid", "(", ")", "\n", "loss", "=", "loss", "+", "self", ".", "coeffs", "[", "'edge_size'", "]", "*", "m", ".", "sum", "(", ")", "\n", "ent", "=", "-", "m", "*", "torch", ".", "log", "(", "m", "+", "EPS", ")", "-", "(", "1", "-", "m", ")", "*", "torch", ".", "log", "(", "1", "-", "m", "+", "EPS", ")", "\n", "loss", "=", "loss", "+", "self", ".", "coeffs", "[", "'edge_ent'", "]", "*", "ent", ".", "mean", "(", ")", "\n", "\n", "m", "=", "self", ".", "node_feat_mask", ".", "sigmoid", "(", ")", "\n", "loss", "=", "loss", "+", "self", ".", "coeffs", "[", "'node_feat_size'", "]", "*", "m", ".", "sum", "(", ")", "\n", "ent", "=", "-", "m", "*", "torch", ".", "log", "(", "m", "+", "EPS", ")", "-", "(", "1", "-", "m", ")", "*", "torch", ".", "log", "(", "1", "-", "m", "+", "EPS", ")", "\n", "loss", "=", "loss", "+", "self", ".", "coeffs", "[", "'node_feat_ent'", "]", "*", "ent", ".", "mean", "(", ")", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.gnn_explainer.GNNExplainer.explain_node": [[113, 172], ["gnn_explainer.GNNExplainer.model.eval", "gnn_explainer.GNNExplainer.__clear_masks__", "edge_index.size", "gnn_explainer.GNNExplainer.__subgraph__", "gnn_explainer.GNNExplainer.__set_masks__", "gnn_explainer.GNNExplainer.to", "torch.optim.Adam", "range", "gnn_explainer.GNNExplainer.node_feat_mask.detach().sigmoid", "gnn_explainer.GNNExplainer.edge_mask.new_zeros", "gnn_explainer.GNNExplainer.edge_mask.detach().sigmoid", "gnn_explainer.GNNExplainer.__clear_masks__", "torch.no_grad", "log_logits.argmax", "tqdm.tqdm.tqdm", "tqdm.tqdm.tqdm.set_description", "torch.optim.Adam.zero_grad", "gnn_explainer.GNNExplainer.__loss__", "gnn_explainer.GNNExplainer.backward", "torch.optim.Adam.step", "tqdm.tqdm.tqdm.close", "gnn_explainer.GNNExplainer.model", "gnn_explainer.GNNExplainer.node_feat_mask.view().sigmoid", "gnn_explainer.GNNExplainer.model", "tqdm.tqdm.tqdm.update", "gnn_explainer.GNNExplainer.node_feat_mask.detach", "gnn_explainer.GNNExplainer.edge_mask.detach", "gnn_explainer.GNNExplainer.node_feat_mask.view"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.gnn_explainer.GNNExplainer.__clear_masks__", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.gnn_explainer.GNNExplainer.__subgraph__", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.gnn_explainer.GNNExplainer.__set_masks__", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.gnn_explainer.GNNExplainer.__clear_masks__", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.gnn_explainer.GNNExplainer.__loss__", "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.att_layers.SpecialSpmmFunction.backward", "home.repos.pwc.inspect_result.shaanchandra_SAFER.optimizers.radam.RiemannianAdam.step"], ["", "def", "explain_node", "(", "self", ",", "node_idx", ",", "x", ",", "edge_index", ",", "**", "kwargs", ")", ":", "\n", "        ", "r\"\"\"Learns and returns a node feature mask and an edge mask that play a\n        crucial role to explain the prediction made by the GNN for node\n        :attr:`node_idx`.\n\n        Args:\n            node_idx (int): The node to explain.\n            x (Tensor): The node feature matrix.\n            edge_index (LongTensor): The edge indices.\n            **kwargs (optional): Additional arguments passed to the GNN module.\n\n        :rtype: (:class:`Tensor`, :class:`Tensor`)\n        \"\"\"", "\n", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "self", ".", "__clear_masks__", "(", ")", "\n", "\n", "num_edges", "=", "edge_index", ".", "size", "(", "1", ")", "\n", "\n", "# Only operate on a k-hop subgraph around `node_idx`.", "\n", "x", ",", "edge_index", ",", "mapping", ",", "hard_edge_mask", ",", "kwargs", "=", "self", ".", "__subgraph__", "(", "\n", "node_idx", ",", "x", ",", "edge_index", ",", "**", "kwargs", ")", "\n", "\n", "# Get the initial prediction.", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "log_logits", "=", "self", ".", "model", "(", "x", "=", "x", ",", "edge_index", "=", "edge_index", ",", "**", "kwargs", ")", "[", "0", "]", "\n", "pred_label", "=", "log_logits", ".", "argmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "", "self", ".", "__set_masks__", "(", "x", ",", "edge_index", ")", "\n", "self", ".", "to", "(", "x", ".", "device", ")", "\n", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "[", "self", ".", "node_feat_mask", ",", "self", ".", "edge_mask", "]", ",", "\n", "lr", "=", "self", ".", "lr", ")", "\n", "\n", "if", "self", ".", "log", ":", "# pragma: no cover", "\n", "            ", "pbar", "=", "tqdm", "(", "total", "=", "self", ".", "epochs", ")", "\n", "pbar", ".", "set_description", "(", "f'Explain node {node_idx}'", ")", "\n", "\n", "", "for", "epoch", "in", "range", "(", "1", ",", "self", ".", "epochs", "+", "1", ")", ":", "\n", "            ", "optimizer", ".", "zero_grad", "(", ")", "\n", "h", "=", "x", "*", "self", ".", "node_feat_mask", ".", "view", "(", "1", ",", "-", "1", ")", ".", "sigmoid", "(", ")", "\n", "log_logits", "=", "self", ".", "model", "(", "x", "=", "h", ",", "edge_index", "=", "edge_index", ",", "**", "kwargs", ")", "[", "0", "]", "\n", "loss", "=", "self", ".", "__loss__", "(", "mapping", ",", "log_logits", ",", "pred_label", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "self", ".", "log", ":", "# pragma: no cover", "\n", "                ", "pbar", ".", "update", "(", "1", ")", "\n", "\n", "", "", "if", "self", ".", "log", ":", "# pragma: no cover", "\n", "            ", "pbar", ".", "close", "(", ")", "\n", "\n", "", "node_feat_mask", "=", "self", ".", "node_feat_mask", ".", "detach", "(", ")", ".", "sigmoid", "(", ")", "\n", "edge_mask", "=", "self", ".", "edge_mask", ".", "new_zeros", "(", "num_edges", ")", "\n", "edge_mask", "[", "hard_edge_mask", "]", "=", "self", ".", "edge_mask", ".", "detach", "(", ")", ".", "sigmoid", "(", ")", "\n", "\n", "self", ".", "__clear_masks__", "(", ")", "\n", "\n", "return", "node_feat_mask", ",", "edge_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.gnn_explainer.GNNExplainer.visualize_subgraph": [[173, 240], ["torch_geometric.utils.k_hop_subgraph", "torch_geometric.data.Data().to", "torch_geometric.utils.to_networkx", "networkx.relabel_nodes", "networkx.spring_layout", "matplotlib.gca", "networkx.relabel_nodes.edges", "networkx.draw_networkx_nodes", "networkx.draw_networkx_labels", "edge_mask.size", "edge_index.size", "gnn_explainer.GNNExplainer.__num_hops__", "torch.zeros", "kwargs.get", "kwargs.get", "kwargs.get", "matplotlib.gca.annotate", "gnn_explainer.GNNExplainer.__flow__", "y[].to", "torch.zeros.max().item", "torch_geometric.data.Data", "enumerate", "torch.zeros.tolist", "edge_index.max().item", "subset.tolist", "dict", "torch.zeros.max", "torch.zeros.size", "edge_index.max", "max", "math.sqrt", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.gnn_explainer.GNNExplainer.__num_hops__", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.gnn_explainer.GNNExplainer.__flow__"], ["", "def", "visualize_subgraph", "(", "self", ",", "node_idx", ",", "edge_index", ",", "edge_mask", ",", "y", "=", "None", ",", "\n", "threshold", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "r\"\"\"Visualizes the subgraph around :attr:`node_idx` given an edge mask\n        :attr:`edge_mask`.\n\n        Args:\n            node_idx (int): The node id to explain.\n            edge_index (LongTensor): The edge indices.\n            edge_mask (Tensor): The edge mask.\n            y (Tensor, optional): The ground-truth node-prediction labels used\n                as node colorings. (default: :obj:`None`)\n            threshold (float, optional): Sets a threshold for visualizing\n                important edges. If set to :obj:`None`, will visualize all\n                edges with transparancy indicating the importance of edges.\n                (default: :obj:`None`)\n            **kwargs (optional): Additional arguments passed to\n                :func:`nx.draw`.\n\n        :rtype: :class:`matplotlib.axes.Axes`, :class:`networkx.DiGraph`\n        \"\"\"", "\n", "\n", "assert", "edge_mask", ".", "size", "(", "0", ")", "==", "edge_index", ".", "size", "(", "1", ")", "\n", "\n", "# Only operate on a k-hop subgraph around `node_idx`.", "\n", "subset", ",", "edge_index", ",", "_", ",", "hard_edge_mask", "=", "k_hop_subgraph", "(", "\n", "node_idx", ",", "self", ".", "__num_hops__", "(", ")", ",", "edge_index", ",", "relabel_nodes", "=", "True", ",", "\n", "num_nodes", "=", "None", ",", "flow", "=", "self", ".", "__flow__", "(", ")", ")", "\n", "\n", "edge_mask", "=", "edge_mask", "[", "hard_edge_mask", "]", "\n", "\n", "if", "threshold", "is", "not", "None", ":", "\n", "            ", "edge_mask", "=", "(", "edge_mask", ">=", "threshold", ")", ".", "to", "(", "torch", ".", "float", ")", "\n", "\n", "", "if", "y", "is", "None", ":", "\n", "            ", "y", "=", "torch", ".", "zeros", "(", "edge_index", ".", "max", "(", ")", ".", "item", "(", ")", "+", "1", ",", "\n", "device", "=", "edge_index", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "y", "=", "y", "[", "subset", "]", ".", "to", "(", "torch", ".", "float", ")", "/", "y", ".", "max", "(", ")", ".", "item", "(", ")", "\n", "\n", "", "data", "=", "Data", "(", "edge_index", "=", "edge_index", ",", "att", "=", "edge_mask", ",", "y", "=", "y", ",", "\n", "num_nodes", "=", "y", ".", "size", "(", "0", ")", ")", ".", "to", "(", "'cpu'", ")", "\n", "G", "=", "to_networkx", "(", "data", ",", "node_attrs", "=", "[", "'y'", "]", ",", "edge_attrs", "=", "[", "'att'", "]", ")", "\n", "mapping", "=", "{", "k", ":", "i", "for", "k", ",", "i", "in", "enumerate", "(", "subset", ".", "tolist", "(", ")", ")", "}", "\n", "G", "=", "nx", ".", "relabel_nodes", "(", "G", ",", "mapping", ")", "\n", "\n", "# kwargs['with_labels'] = kwargs.get('with_labels') or True", "\n", "kwargs", "[", "'with_labels'", "]", "=", "False", "\n", "kwargs", "[", "'font_size'", "]", "=", "kwargs", ".", "get", "(", "'font_size'", ")", "or", "5", "\n", "kwargs", "[", "'node_size'", "]", "=", "kwargs", ".", "get", "(", "'node_size'", ")", "or", "800", "\n", "kwargs", "[", "'cmap'", "]", "=", "kwargs", ".", "get", "(", "'cmap'", ")", "or", "'RdYlGn'", "\n", "\n", "pos", "=", "nx", ".", "spring_layout", "(", "G", ")", "\n", "ax", "=", "plt", ".", "gca", "(", ")", "\n", "for", "source", ",", "target", ",", "data", "in", "G", ".", "edges", "(", "data", "=", "True", ")", ":", "\n", "            ", "ax", ".", "annotate", "(", "\n", "''", ",", "xy", "=", "pos", "[", "target", "]", ",", "xycoords", "=", "'data'", ",", "xytext", "=", "pos", "[", "source", "]", ",", "\n", "textcoords", "=", "'data'", ",", "arrowprops", "=", "dict", "(", "\n", "arrowstyle", "=", "\"->\"", ",", "\n", "alpha", "=", "max", "(", "data", "[", "'att'", "]", ",", "0.1", ")", ",", "\n", "shrinkA", "=", "sqrt", "(", "kwargs", "[", "'node_size'", "]", ")", "/", "2.0", ",", "\n", "shrinkB", "=", "sqrt", "(", "kwargs", "[", "'node_size'", "]", ")", "/", "2.0", ",", "\n", "connectionstyle", "=", "\"arc3,rad=0.1\"", ",", "\n", ")", ")", "\n", "", "nx", ".", "draw_networkx_nodes", "(", "G", ",", "pos", ",", "node_color", "=", "y", ".", "tolist", "(", ")", ",", "alpha", "=", "0.5", ",", "**", "kwargs", ")", "\n", "nx", ".", "draw_networkx_labels", "(", "G", ",", "pos", ",", "**", "kwargs", ")", "\n", "\n", "return", "ax", ",", "G", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.gnn_explainer.GNNExplainer.__repr__": [[241, 243], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "f'{self.__class__.__name__}()'", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.utils.calc_elapsed_time": [[39, 45], ["divmod", "divmod", "divmod", "divmod", "int", "int"], "function", ["None"], ["def", "calc_elapsed_time", "(", "start", ",", "end", ")", ":", "\n", "    ", "hours", ",", "rem", "=", "divmod", "(", "end", "-", "start", ",", "3600", ")", "\n", "time_hours", ",", "time_rem", "=", "divmod", "(", "end", ",", "3600", ")", "\n", "minutes", ",", "seconds", "=", "divmod", "(", "rem", ",", "60", ")", "\n", "time_mins", ",", "_", "=", "divmod", "(", "time_rem", ",", "60", ")", "\n", "return", "int", "(", "hours", ")", ",", "int", "(", "minutes", ")", ",", "seconds", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.utils.evaluation_measures": [[48, 56], ["sklearn.metrics.f1_score", "sklearn.metrics.f1_score", "sklearn.metrics.recall_score", "sklearn.metrics.precision_score", "sklearn.metrics.accuracy_score"], "function", ["None"], ["", "def", "evaluation_measures", "(", "config", ",", "preds", ",", "labels", ")", ":", "\n", "    ", "f1", "=", "f1_score", "(", "labels", ",", "preds", ",", "average", "=", "'binary'", ",", "pos_label", "=", "1", ")", "\n", "f1_macro", "=", "f1_score", "(", "labels", ",", "preds", ",", "average", "=", "'macro'", ")", "\n", "recall", "=", "recall_score", "(", "labels", ",", "preds", ",", "average", "=", "'binary'", ",", "pos_label", "=", "1", ")", "\n", "precision", "=", "precision_score", "(", "labels", ",", "preds", ",", "average", "=", "'binary'", ",", "pos_label", "=", "1", ")", "\n", "accuracy", "=", "accuracy_score", "(", "labels", ",", "preds", ")", "\n", "# print(metrics.classification_report(labels, preds))", "\n", "return", "f1", ",", "f1_macro", ",", "recall", ",", "precision", ",", "accuracy", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.utils.log_tensorboard": [[59, 137], ["writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "sum", "len", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar"], "function", ["None"], ["", "def", "log_tensorboard", "(", "config", ",", "writer", ",", "model", ",", "epoch", ",", "iters", ",", "total_iters", ",", "loss", ",", "f1", ",", "prec", ",", "recall", ",", "acc", ",", "lr", "=", "0", ",", "thresh", "=", "0", ",", "loss_only", "=", "True", ",", "val", "=", "False", ")", ":", "\n", "    ", "if", "config", "[", "'parallel_computing'", "]", "==", "True", ":", "\n", "        ", "model_log", "=", "model", ".", "module", "\n", "", "else", ":", "\n", "        ", "model_log", "=", "model", "\n", "\n", "", "if", "loss_only", ":", "\n", "        ", "writer", ".", "add_scalar", "(", "'Train/Loss'", ",", "sum", "(", "loss", ")", "/", "len", "(", "loss", ")", ",", "(", "(", "iters", "+", "1", ")", "+", "total_iters", ")", ")", "\n", "# if iters%500 == 0:", "\n", "#     for name, param in model_log.encoder.named_parameters():", "\n", "#         print(\"\\nparam {} grad = {}\".format(name, param.grad.data.view(-1)))", "\n", "#         sys.exit()", "\n", "#         if not param.requires_grad or param.grad is None:", "\n", "#             continue", "\n", "#         writer.add_histogram('Iters/'+name, param.data.view(-1), global_step= ((iters+1)+total_iters))", "\n", "#         writer.add_histogram('Grads/'+ name, param.grad.data.view(-1), global_step = ((iters+1)+ total_iters))", "\n", "", "else", ":", "\n", "        ", "if", "not", "val", "and", "config", "[", "'data_name'", "]", "!=", "'pheme'", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "'Train/F1'", ",", "f1", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'Train/Precision'", ",", "prec", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'Train/Recall'", ",", "recall", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'Train/Accuracy'", ",", "acc", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "\"Train/learning_rate\"", ",", "lr", ",", "epoch", ")", "\n", "\n", "# for name, param in model_log.encoder.named_parameters():", "\n", "#     if not param.requires_grad:", "\n", "#         continue", "\n", "#     writer.add_histogram('Epochs/' + name, param.data.view(-1), global_step= epoch)", "\n", "\n", "", "elif", "not", "val", "and", "config", "[", "'data_name'", "]", "==", "'pheme'", ":", "\n", "            ", "f1_micro", ",", "f1_macro", ",", "f1_weighted", "=", "f1", "\n", "recall_micro", ",", "recall_macro", ",", "recall_weighted", "=", "recall", "\n", "precision_micro", ",", "precision_macro", ",", "precision_weighted", "=", "prec", "\n", "\n", "writer", ".", "add_scalar", "(", "'Train_F1/macro'", ",", "f1_macro", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'Train_F1/micro'", ",", "f1_micro", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'Train_F1/weighted'", ",", "f1_weighted", ",", "epoch", ")", "\n", "\n", "writer", ".", "add_scalar", "(", "'Train_Precision/macro'", ",", "precision_macro", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'Train_Precision/micro'", ",", "precision_micro", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'Train_Precision/weighted'", ",", "precision_weighted", ",", "epoch", ")", "\n", "\n", "writer", ".", "add_scalar", "(", "'Train_Recall/macro'", ",", "recall_macro", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'Train_Recall/micro'", ",", "recall_micro", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'Train_Recall/weighted'", ",", "recall_weighted", ",", "epoch", ")", "\n", "\n", "writer", ".", "add_scalar", "(", "\"Train/learning_rate\"", ",", "lr", ",", "epoch", ")", "\n", "\n", "# for name, param in model_log.encoder.named_parameters():", "\n", "#     if not param.requires_grad:", "\n", "#         continue", "\n", "#     writer.add_histogram('Epochs/' + name, param.data.view(-1), global_step= epoch)", "\n", "\n", "", "elif", "val", "and", "config", "[", "'data_name'", "]", "!=", "'pheme'", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "'Validation/Loss'", ",", "loss", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'Validation/F1'", ",", "f1", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'Validation/Recall'", ",", "recall", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'Validation/Precision'", ",", "prec", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'Validation/Accuracy'", ",", "acc", ",", "epoch", ")", "\n", "\n", "", "elif", "val", "and", "config", "[", "'data_name'", "]", "==", "'pheme'", ":", "\n", "            ", "f1_micro", ",", "f1_macro", ",", "f1_weighted", "=", "f1", "\n", "recall_micro", ",", "recall_macro", ",", "recall_weighted", "=", "recall", "\n", "precision_micro", ",", "precision_macro", ",", "precision_weighted", "=", "prec", "\n", "\n", "writer", ".", "add_scalar", "(", "'Validation/Loss'", ",", "loss", ",", "epoch", ")", "\n", "\n", "writer", ".", "add_scalar", "(", "'Validation_F1/macro'", ",", "f1_macro", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'Validation_F1/micro'", ",", "f1_micro", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'Validation_F1/weighted'", ",", "f1_weighted", ",", "epoch", ")", "\n", "\n", "writer", ".", "add_scalar", "(", "'Validation_Precision/macro'", ",", "precision_macro", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'Validation_Precision/micro'", ",", "precision_micro", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'Validation_Precision/weighted'", ",", "precision_weighted", ",", "epoch", ")", "\n", "\n", "writer", ".", "add_scalar", "(", "'Validation_Recall/macro'", ",", "recall_macro", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'Validation_Recall/micro'", ",", "recall_micro", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'Validation_Recall/weighted'", ",", "recall_weighted", ",", "epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.utils.print_stats": [[140, 151], ["time.time", "utils.calc_elapsed_time", "print", "sum", "len"], "function", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.calc_elapsed_time"], ["", "", "", "def", "print_stats", "(", "config", ",", "epoch", ",", "train_loss", ",", "train_acc", ",", "train_f1", ",", "train_f1_macro", ",", "train_prec", ",", "train_recall", ",", "val_loss", ",", "val_acc", ",", "val_f1", ",", "val_f1_macro", ",", "val_precision", ",", "val_recall", ",", "start", ",", "lr", ")", ":", "\n", "    ", "end", "=", "time", ".", "time", "(", ")", "\n", "hours", ",", "minutes", ",", "seconds", "=", "calc_elapsed_time", "(", "start", ",", "end", ")", "\n", "\n", "train_loss", "=", "sum", "(", "train_loss", ")", "/", "len", "(", "train_loss", ")", "\n", "print", "(", "\"\\nEpoch: {}/{},  \\\n          \\ntrain_loss = {:.4f},    train_acc = {:.4f},    train_prec = {:.4f},    train_recall = {:.4f},    train_f1 = {:.4f},    train_macro_f1 = {:.4f}  \\\n          \\neval_loss = {:.4f},     eval_acc = {:.4f},     eval_prec = {:.4f},     eval_recall = {:.4f},     eval_f1 = {:.4f},    val_f1_macro = {:.4f}  \\\n              \\nlr  =  {:.8f}\\nElapsed Time:  {:0>2}:{:0>2}:{:05.2f}\"", "\n", ".", "format", "(", "epoch", ",", "config", "[", "'max_epoch'", "]", ",", "train_loss", ",", "train_acc", ",", "train_prec", ",", "train_recall", ",", "train_f1", ",", "train_f1_macro", ",", "val_loss", ",", "val_acc", ",", "\n", "val_precision", ",", "val_recall", ",", "val_f1", ",", "val_f1_macro", ",", "lr", ",", "hours", ",", "minutes", ",", "seconds", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.utils.print_test_stats": [[155, 166], ["print", "print", "print", "print", "print", "print", "print", "print", "print", "print"], "function", ["None"], ["", "def", "print_test_stats", "(", "test_accuracy", ",", "test_precision", ",", "test_recall", ",", "test_f1", ",", "test_f1_macro", ",", "best_val_acc", ",", "best_val_precision", ",", "best_val_recall", ",", "best_val_f1", ")", ":", "\n", "    ", "print", "(", "\"\\nTest accuracy of best model = {:.2f}\"", ".", "format", "(", "test_accuracy", "*", "100", ")", ")", "\n", "print", "(", "\"Test precision of best model = {:.2f}\"", ".", "format", "(", "test_precision", "*", "100", ")", ")", "\n", "print", "(", "\"Test recall of best model = {:.2f}\"", ".", "format", "(", "test_recall", "*", "100", ")", ")", "\n", "print", "(", "\"Test f1 of best model = {:.2f}\"", ".", "format", "(", "test_f1", "*", "100", ")", ")", "\n", "print", "(", "\"Test macro-F1 of best model = {:.2f}\"", ".", "format", "(", "test_f1_macro", "*", "100", ")", ")", "\n", "print", "(", "\"\\n\"", "+", "\"-\"", "*", "50", "+", "\"\\nBest Validation scores:\\n\"", "+", "\"-\"", "*", "50", ")", "\n", "print", "(", "\"\\nVal accuracy of best model = {:.2f}\"", ".", "format", "(", "best_val_acc", "*", "100", ")", ")", "\n", "print", "(", "\"Val precision of best model = {:.2f}\"", ".", "format", "(", "best_val_precision", "*", "100", ")", ")", "\n", "print", "(", "\"Val recall of best model = {:.2f}\"", ".", "format", "(", "best_val_recall", "*", "100", ")", ")", "\n", "print", "(", "\"Val f1 of best model = {:.2f}\"", ".", "format", "(", "best_val_f1", "*", "100", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.utils.calculate_transformer_stats": [[169, 178], ["None"], "function", ["None"], ["", "def", "calculate_transformer_stats", "(", "train_result", ")", ":", "\n", "    ", "train_prec_pos", "=", "train_result", "[", "'tp'", "]", "/", "(", "train_result", "[", "'tp'", "]", "+", "train_result", "[", "'fp'", "]", ")", "\n", "train_recall_pos", "=", "train_result", "[", "'tp'", "]", "/", "(", "train_result", "[", "'tp'", "]", "+", "train_result", "[", "'fn'", "]", ")", "\n", "train_f1_pos", "=", "(", "2", "*", "train_prec_pos", "*", "train_recall_pos", ")", "/", "(", "train_prec_pos", "+", "train_recall_pos", ")", "\n", "train_prec_neg", "=", "train_result", "[", "'tn'", "]", "/", "(", "train_result", "[", "'tn'", "]", "+", "train_result", "[", "'fn'", "]", ")", "\n", "train_recall_neg", "=", "train_result", "[", "'tn'", "]", "/", "(", "train_result", "[", "'tn'", "]", "+", "train_result", "[", "'fp'", "]", ")", "\n", "train_f1_neg", "=", "(", "2", "*", "train_prec_neg", "*", "train_recall_neg", ")", "/", "(", "train_prec_neg", "+", "train_recall_neg", ")", "\n", "macro_f1", "=", "(", "train_f1_pos", "+", "train_f1_neg", ")", "/", "2", "\n", "return", "train_prec_pos", ",", "train_recall_pos", ",", "train_f1_pos", ",", "macro_f1", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.utils.print_transformer_results": [[180, 206], ["print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print"], "function", ["None"], ["", "def", "print_transformer_results", "(", "config", ",", "val_stats", ",", "test_stats", ",", "val_result", ",", "test_result", ")", ":", "\n", "\n", "    ", "val_f1_pos", ",", "val_f1_neg", ",", "val_macro_f1", ",", "val_micro_f1", ",", "val_recall", ",", "val_prec", ",", "val_acc", "=", "val_stats", "\n", "test_f1_pos", ",", "test_f1_neg", ",", "test_macro_f1", ",", "test_micro_f1", ",", "test_recall", ",", "test_prec", ",", "test_acc", "=", "test_stats", "\n", "val_mcc", "=", "val_result", "[", "'mcc'", "]", "\n", "test_mcc", "=", "test_result", "[", "'mcc'", "]", "\n", "\n", "print", "(", "\"\\nVal evaluation stats: \\n\"", "+", "\"-\"", "*", "50", ")", "\n", "print", "(", "\"Val precision of best model = {:.2f}\"", ".", "format", "(", "val_prec", "*", "100", ")", ")", "\n", "print", "(", "\"Val recall of best model = {:.2f}\"", ".", "format", "(", "val_recall", "*", "100", ")", ")", "\n", "print", "(", "\"Val f1 (fake) of best model = {:.2f}\"", ".", "format", "(", "val_f1_pos", "*", "100", ")", ")", "\n", "print", "(", "\"Val f1 (real) of best model = {:.2f}\"", ".", "format", "(", "val_f1_neg", "*", "100", ")", ")", "\n", "print", "(", "\"Val macro-f1 of best model = {:.2f}\"", ".", "format", "(", "val_macro_f1", "*", "100", ")", ")", "\n", "print", "(", "\"Val micro-f1 of best model = {:.2f}\"", ".", "format", "(", "val_micro_f1", "*", "100", ")", ")", "\n", "print", "(", "\"Val accuracy of best model = {:.2f}\"", ".", "format", "(", "val_acc", "*", "100", ")", ")", "\n", "print", "(", "\"Val MCC of best model = {:.2f}\"", ".", "format", "(", "val_mcc", "*", "100", ")", ")", "\n", "\n", "print", "(", "\"\\nTest evaluation stats: \\n\"", "+", "\"-\"", "*", "50", ")", "\n", "print", "(", "\"Test precision of best model = {:.2f}\"", ".", "format", "(", "test_prec", "*", "100", ")", ")", "\n", "print", "(", "\"Test recall of best model = {:.2f}\"", ".", "format", "(", "test_recall", "*", "100", ")", ")", "\n", "print", "(", "\"Test f1 (fake) of best model = {:.2f}\"", ".", "format", "(", "test_f1_pos", "*", "100", ")", ")", "\n", "print", "(", "\"Test f1 (real) of best model = {:.2f}\"", ".", "format", "(", "test_f1_neg", "*", "100", ")", ")", "\n", "print", "(", "\"Test macro-f1 of best model = {:.2f}\"", ".", "format", "(", "test_macro_f1", "*", "100", ")", ")", "\n", "print", "(", "\"Test micro-f1 of best model = {:.2f}\"", ".", "format", "(", "test_micro_f1", "*", "100", ")", ")", "\n", "print", "(", "\"Test accuracy of best model = {:.2f}\"", ".", "format", "(", "test_acc", "*", "100", ")", ")", "\n", "print", "(", "\"Test MCC of best model = {:.2f}\"", ".", "format", "(", "test_mcc", "*", "100", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_gnn.Cache_GNN_Embeds.__init__": [[16, 28], ["os.path.join", "os.path.join", "cache_gnn.Cache_GNN_Embeds.predict_and_cache_fakenews", "cache_gnn.Cache_GNN_Embeds.predict_and_cache_fakehealth"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_text.Cache_Text_Embeds.predict_and_cache_fakenews", "home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_text.Cache_Text_Embeds.predict_and_cache_fakehealth"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "model", ")", ":", "\n", "        ", "self", ".", "config", "=", "config", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "model_file", "=", "config", "[", "'model_file'", "]", "\n", "self", ".", "comp_dir", "=", "os", ".", "path", ".", "join", "(", "'data'", ",", "'complete_data'", ",", "config", "[", "'data_name'", "]", ")", "\n", "self", ".", "data_dir", "=", "os", ".", "path", ".", "join", "(", "'FakeHealth'", ")", "\n", "\n", "\n", "if", "config", "[", "'data_name'", "]", "in", "[", "'gossipcop'", ",", "'politifcat'", "]", ":", "\n", "            ", "self", ".", "predict_and_cache_fakenews", "(", ")", "\n", "", "elif", "config", "[", "'data_name'", "]", "in", "[", "'HealthStory'", ",", "'HealthRelease'", "]", ":", "\n", "            ", "self", ".", "predict_and_cache_fakehealth", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_gnn.Cache_GNN_Embeds.predict_and_cache_fakenews": [[31, 47], ["print", "torch.load", "cache_gnn.Cache_GNN_Embeds.model.load_state_dict", "os.path.join", "os.path.join", "json.load", "json.load", "cache_gnn.Cache_GNN_Embeds.prepare_id2node", "cache_gnn.Cache_GNN_Embeds.obtain_user_representations", "cache_gnn.Cache_GNN_Embeds.obtain_doc_representations", "open", "open"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_gnn.Cache_GNN_Embeds.prepare_id2node", "home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_gnn.Cache_GNN_Embeds.obtain_user_representations", "home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_text.Cache_Text_Embeds.obtain_doc_representations"], ["", "", "def", "predict_and_cache_fakenews", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"\\n\\nCaching FakeNews dataset : \"", ",", "self", ".", "config", "[", "'data_name'", "]", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "self", ".", "model_file", ")", "\n", "self", ".", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'model_state_dict'", "]", ")", "\n", "\n", "node2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "'node2id_lr_30_30.json'", ")", "\n", "docs_splits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "'doc_splits_lr.json'", ")", "\n", "self", ".", "node2id", "=", "json", ".", "load", "(", "open", "(", "node2id_file", ",", "'r'", ")", ")", "\n", "self", ".", "doc_splits", "=", "json", ".", "load", "(", "open", "(", "docs_splits_file", ",", "'r'", ")", ")", "\n", "self", ".", "train_docs", "=", "self", ".", "doc_splits", "[", "'train_docs'", "]", "\n", "self", ".", "val_docs", "=", "self", ".", "doc_splits", "[", "'val_docs'", "]", "\n", "self", ".", "test_docs", "=", "self", ".", "doc_splits", "[", "'test_docs'", "]", "\n", "\n", "self", ".", "prepare_id2node", "(", ")", "\n", "self", ".", "obtain_user_representations", "(", ")", "\n", "self", ".", "obtain_doc_representations", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_gnn.Cache_GNN_Embeds.predict_and_cache_fakehealth": [[51, 68], ["print", "torch.load", "cache_gnn.Cache_GNN_Embeds.model.load_state_dict", "os.path.join", "json.load", "os.path.join", "json.load", "cache_gnn.Cache_GNN_Embeds.prepare_id2node", "cache_gnn.Cache_GNN_Embeds.obtain_user_representations", "cache_gnn.Cache_GNN_Embeds.obtain_doc_representations", "open", "open"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_gnn.Cache_GNN_Embeds.prepare_id2node", "home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_gnn.Cache_GNN_Embeds.obtain_user_representations", "home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_text.Cache_Text_Embeds.obtain_doc_representations"], ["", "def", "predict_and_cache_fakehealth", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"\\n\\nCaching FakeHealth dataset : \"", ",", "self", ".", "config", "[", "'data_name'", "]", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "self", ".", "model_file", ")", "\n", "self", ".", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'model_state_dict'", "]", ")", "\n", "\n", "node2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "'node2id_lr_top10.json'", ")", "\n", "self", ".", "node2id", "=", "json", ".", "load", "(", "open", "(", "node2id_file", ",", "'r'", ")", ")", "\n", "docs_splits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'doc_splits_{}.json'", ".", "format", "(", "self", ".", "config", "[", "'data_name'", "]", ")", ")", "\n", "self", ".", "doc_splits", "=", "json", ".", "load", "(", "open", "(", "docs_splits_file", ",", "'r'", ")", ")", "\n", "\n", "self", ".", "train_docs", "=", "self", ".", "doc_splits", "[", "'train_docs'", "]", "\n", "self", ".", "val_docs", "=", "self", ".", "doc_splits", "[", "'val_docs'", "]", "\n", "self", ".", "test_docs", "=", "self", ".", "doc_splits", "[", "'test_docs'", "]", "\n", "\n", "self", ".", "prepare_id2node", "(", ")", "\n", "self", ".", "obtain_user_representations", "(", ")", "\n", "self", ".", "obtain_doc_representations", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_gnn.Cache_GNN_Embeds.prepare_id2node": [[71, 76], ["print", "cache_gnn.Cache_GNN_Embeds.node2id.items", "int"], "methods", ["None"], ["", "def", "prepare_id2node", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"\\n\\nPreparing id2node dict..\"", ")", "\n", "self", ".", "id2node", "=", "{", "}", "\n", "for", "node", ",", "idx", "in", "self", ".", "node2id", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "id2node", "[", "int", "(", "idx", ")", "]", "=", "node", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_gnn.Cache_GNN_Embeds.obtain_user_representations": [[79, 112], ["print", "os.path.join", "print", "torch.save", "torch.zeros", "torch.zeros", "torch.no_grad", "cache_gnn.Cache_GNN_Embeds.model.eval", "len", "len", "cache_gnn.Cache_GNN_Embeds.model", "enumerate", "enumerate", "cache_gnn.Cache_GNN_Embeds.config[].x.to", "cache_gnn.Cache_GNN_Embeds.config[].edge_index.to", "cache_gnn.Cache_GNN_Embeds.node2id.values", "enumerate", "str().startswith", "cache_gnn.Cache_GNN_Embeds.model", "int", "batch_data.x.to", "batch_data.edge_index.to", "batch_data.edge_attr.to", "torch_geometric.utils.to_dense_adj().squeeze", "cache_gnn.Cache_GNN_Embeds.model.encode", "cache_gnn.Cache_GNN_Embeds.model.decode", "cache_gnn.Cache_GNN_Embeds.model", "str().startswith", "str", "batch_data.x.to", "batch_data.edge_index.to", "cache_gnn.Cache_GNN_Embeds.to", "batch_data.edge_index.to", "batch_data.x.to", "batch_data.edge_index.to", "int", "torch_geometric.utils.to_dense_adj", "str"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.models.encoders.Shallow.encode", "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.decoders.LinearDecoder.decode"], ["", "", "def", "obtain_user_representations", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"\\nObtaining user representations...\"", ")", "\n", "self", ".", "user_cache", "=", "torch", ".", "zeros", "(", "len", "(", "self", ".", "node2id", ")", ",", "self", ".", "config", "[", "'embed_dim'", "]", ")", "if", "self", ".", "config", "[", "'model_name'", "]", "not", "in", "[", "'gat'", ",", "'rgat'", "]", "else", "torch", ".", "zeros", "(", "len", "(", "self", ".", "node2id", ")", ",", "3", "*", "self", ".", "config", "[", "'embed_dim'", "]", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "self", ".", "model", ".", "eval", "(", ")", "\n", "if", "self", ".", "config", "[", "'full_graph'", "]", ":", "\n", "                ", "_", ",", "_", ",", "node_embeds", "=", "self", ".", "model", "(", "self", ".", "config", "[", "'data'", "]", ".", "x", ".", "to", "(", "device", ")", ",", "self", ".", "config", "[", "'data'", "]", ".", "edge_index", ".", "to", "(", "device", ")", ")", "\n", "for", "idx", ",", "ids", "in", "enumerate", "(", "self", ".", "node2id", ".", "values", "(", ")", ")", ":", "\n", "                    ", "node", "=", "self", ".", "id2node", "[", "int", "(", "ids", ")", "]", "\n", "if", "not", "str", "(", "node", ")", ".", "startswith", "(", "'g'", ")", ":", "\n", "                        ", "self", ".", "user_cache", "[", "ids", ",", ":", "]", "=", "node_embeds", "[", "ids", ",", ":", "]", "\n", "", "", "", "else", ":", "\n", "                ", "for", "iters", ",", "batch_data", "in", "enumerate", "(", "self", ".", "config", "[", "'loader'", "]", ")", ":", "\n", "                    ", "if", "self", ".", "config", "[", "'model_name'", "]", "in", "[", "'rgcn'", ",", "'rgat'", ",", "'rsage'", "]", ":", "\n", "                        ", "_", ",", "_", ",", "node_embeds", "=", "self", ".", "model", "(", "batch_data", ".", "x", ".", "to", "(", "device", ")", ",", "batch_data", ".", "edge_index", ".", "to", "(", "device", ")", ",", "batch_data", ".", "edge_attr", ".", "to", "(", "device", ")", ")", "\n", "", "elif", "self", ".", "config", "[", "'model_name'", "]", "==", "'HGCN'", ":", "\n", "                        ", "batch_data", ".", "edge_index", "=", "to_dense_adj", "(", "batch_data", ".", "edge_index", ")", ".", "squeeze", "(", "0", ")", "\n", "preds", "=", "self", ".", "model", ".", "encode", "(", "batch_data", ".", "x", ".", "to", "(", "device", ")", ",", "batch_data", ".", "edge_index", ".", "to", "(", "device", ")", ")", "\n", "_", ",", "node_embeds", "=", "self", ".", "model", ".", "decode", "(", "preds", ".", "to", "(", "device", ")", ",", "batch_data", ".", "edge_index", ".", "to", "(", "device", ")", ")", "\n", "", "else", ":", "\n", "                        ", "_", ",", "_", ",", "node_embeds", "=", "self", ".", "model", "(", "batch_data", ".", "x", ".", "to", "(", "device", ")", ",", "batch_data", ".", "edge_index", ".", "to", "(", "device", ")", ")", "\n", "\n", "", "for", "idx", ",", "ids", "in", "enumerate", "(", "batch_data", ".", "node2id", ")", ":", "\n", "                        ", "node", "=", "self", ".", "id2node", "[", "int", "(", "ids", ")", "]", "\n", "if", "not", "str", "(", "node", ")", ".", "startswith", "(", "'g'", ")", ":", "\n", "                            ", "self", ".", "user_cache", "[", "ids", ",", ":", "]", "=", "node_embeds", "[", "idx", ",", ":", "]", "\n", "\n", "\n", "", "", "", "", "", "name", "=", "'user_embeds_graph_lr'", "if", "self", ".", "config", "[", "'data_name'", "]", "in", "[", "'HealthRelease'", ",", "'HealthStory'", "]", "else", "'user_embeds_graph_lr_30_30'", "\n", "user_embed_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "'cached_embeds'", ",", "'{}_{}_{}.pt'", ".", "format", "(", "name", ",", "self", ".", "config", "[", "'seed'", "]", ",", "self", ".", "config", "[", "'model_name'", "]", ")", ")", "\n", "print", "(", "\"\\nSaving user embeddings in : \"", ",", "user_embed_file", ")", "\n", "torch", ".", "save", "(", "self", ".", "user_cache", ",", "user_embed_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_gnn.Cache_GNN_Embeds.obtain_doc_representations": [[116, 154], ["print", "print", "enumerate", "os.path.join", "print", "torch.save", "torch.zeros", "torch.zeros", "len", "len", "os.path.join", "os.path.join", "os.path.isfile", "print", "json.load", "str", "str", "open", "str", "json.load", "open", "str", "str", "str"], "methods", ["None"], ["", "def", "obtain_doc_representations", "(", "self", ")", ":", "\n", "        ", "splits", "=", "[", "'train'", ",", "'val'", ",", "'test'", "]", "\n", "for", "split", "in", "splits", ":", "\n", "            ", "print", "(", "\"\\nObtaining {} doc representations...\"", ".", "format", "(", "split", ")", ")", "\n", "if", "split", "==", "'train'", ":", "\n", "                ", "split_docs", "=", "self", ".", "train_docs", "\n", "", "elif", "split", "==", "'val'", ":", "\n", "                ", "split_docs", "=", "self", ".", "val_docs", "\n", "", "else", ":", "\n", "                ", "split_docs", "=", "self", ".", "test_docs", "\n", "\n", "", "split_doc_cache", "=", "torch", ".", "zeros", "(", "len", "(", "self", ".", "node2id", ")", ",", "self", ".", "config", "[", "'embed_dim'", "]", ")", "if", "self", ".", "config", "[", "'model_name'", "]", "!=", "'gat'", "else", "torch", ".", "zeros", "(", "len", "(", "self", ".", "node2id", ")", ",", "3", "*", "self", ".", "config", "[", "'embed_dim'", "]", ")", "\n", "print", "(", "\"split_doc_cache shape = \"", ",", "split_doc_cache", ".", "shape", ")", "\n", "for", "count", ",", "doc", "in", "enumerate", "(", "split_docs", ")", ":", "\n", "                ", "if", "self", ".", "config", "[", "'data_name'", "]", "in", "[", "'gossipcop'", ",", "'politifact'", "]", ":", "\n", "                    ", "doc_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "'complete'", ",", "str", "(", "doc", ")", "+", "'.json'", ")", "\n", "users", "=", "json", ".", "load", "(", "open", "(", "doc_file", ",", "'r'", ")", ")", "[", "'users'", "]", "\n", "", "else", ":", "\n", "                    ", "doc_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'engagements'", ",", "'complete'", ",", "self", ".", "config", "[", "'data_name'", "]", ",", "str", "(", "doc", ")", "+", "'.json'", ")", "\n", "if", "os", ".", "path", ".", "isfile", "(", "doc_file", ")", ":", "\n", "                        ", "users", "=", "json", ".", "load", "(", "open", "(", "doc_file", ",", "'r'", ")", ")", "[", "'users'", "]", "\n", "\n", "", "", "c", "=", "0", "\n", "for", "user", "in", "users", ":", "\n", "                    ", "if", "str", "(", "user", ")", "in", "self", ".", "node2id", ":", "\n", "                        ", "c", "+=", "1", "\n", "split_doc_cache", "[", "self", ".", "node2id", "[", "str", "(", "doc", ")", "]", ",", ":", "]", "+=", "self", ".", "user_cache", "[", "self", ".", "node2id", "[", "str", "(", "user", ")", "]", "]", "\n", "\n", "", "", "if", "c", ">", "0", ":", "\n", "                    ", "split_doc_cache", "[", "self", ".", "node2id", "[", "str", "(", "doc", ")", "]", ",", ":", "]", "/=", "c", "# normalize the sum by no. of users ", "\n", "", "if", "count", "%", "500", "==", "0", ":", "\n", "                    ", "print", "(", "\"{} done...\"", ".", "format", "(", "count", ")", ")", "\n", "\n", "\n", "", "", "name", "=", "'doc_embeds_graph_poinc_wt3_lr'", "if", "self", ".", "config", "[", "'data_name'", "]", "in", "[", "'HealthRelease'", ",", "'HealthStory'", "]", "else", "'doc_embeds_graph_lr_30_30_poinc'", "\n", "doc_embed_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "'cached_embeds'", ",", "'{}_{}_{}_{}.pt'", ".", "format", "(", "name", ",", "split", ",", "self", ".", "config", "[", "'seed'", "]", ",", "self", ".", "config", "[", "'model_name'", "]", ")", ")", "\n", "print", "(", "\"\\nSaving doc embeddings in : \"", ",", "doc_embed_file", ")", "\n", "torch", ".", "save", "(", "split_doc_cache", ",", "doc_embed_file", ")", "\n", "# loaded_embeds = torch.load(doc_embed_file)", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_text.Cache_Text_Embeds.__init__": [[21, 41], ["os.path.join", "os.path.join", "Prepare_Dataset", "Prepare_Dataset.prepare_glove_training", "cache_text.Cache_Text_Embeds.predict_and_cache_fakenews", "Prepare_Dataset.prepare_elmo_training", "cache_text.Cache_Text_Embeds.predict_and_cache_fakehealth", "Prepare_Dataset.prepare_transformer_training"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.Prepare_Dataset.prepare_glove_training", "home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_text.Cache_Text_Embeds.predict_and_cache_fakenews", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.Prepare_Dataset.prepare_elmo_training", "home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_text.Cache_Text_Embeds.predict_and_cache_fakehealth", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.Prepare_Dataset.prepare_transformer_training"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "model", ")", ":", "\n", "        ", "self", ".", "config", "=", "config", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "comp_dir", "=", "os", ".", "path", ".", "join", "(", "'data'", ",", "'complete_data'", ",", "config", "[", "'data_name'", "]", ")", "\n", "self", ".", "data_dir", "=", "os", ".", "path", ".", "join", "(", "'FakeHealth'", ")", "\n", "self", ".", "config", "[", "'batch_size'", "]", "=", "1", "\n", "\n", "prep_data", "=", "Prepare_Dataset", "(", "config", ")", "\n", "if", "self", ".", "config", "[", "'embed_name'", "]", "==", "'glove'", ":", "\n", "            ", "self", ".", "config", "[", "'train_loader'", "]", ",", "self", ".", "config", "[", "'val_loader'", "]", ",", "self", ".", "config", "[", "'test_loader'", "]", ",", "self", ".", "config", "[", "'TEXT'", "]", "=", "prep_data", ".", "prepare_glove_training", "(", ")", "\n", "", "elif", "self", ".", "config", "[", "'embed_name'", "]", "==", "'elmo'", ":", "\n", "            ", "self", ".", "config", "[", "'train_data'", "]", ",", "self", ".", "config", "[", "'train_labels'", "]", ",", "self", ".", "config", "[", "'val_data'", "]", ",", "self", ".", "config", "[", "'val_labels'", "]", ",", "self", ".", "config", "[", "'test_data'", "]", ",", "self", ".", "config", "[", "'test_labels'", "]", "=", "prep_data", ".", "prepare_elmo_training", "(", ")", "\n", "", "elif", "config", "[", "'embed_name'", "]", "in", "[", "'dbert'", ",", "'xlnet'", ",", "'roberta'", "]", ":", "\n", "            ", "config", "[", "'train_loader'", "]", ",", "config", "[", "'val_loader'", "]", ",", "config", "[", "'test_loader'", "]", "=", "prep_data", ".", "prepare_transformer_training", "(", ")", "\n", "\n", "\n", "", "if", "config", "[", "'data_name'", "]", "in", "[", "'gossipcop'", ",", "'politifcat'", "]", ":", "\n", "            ", "self", ".", "predict_and_cache_fakenews", "(", ")", "\n", "", "elif", "config", "[", "'data_name'", "]", "in", "[", "'HealthStory'", ",", "'HealthRelease'", "]", ":", "\n", "            ", "self", ".", "predict_and_cache_fakehealth", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_text.Cache_Text_Embeds.predict_and_cache_fakenews": [[45, 55], ["print", "os.path.join", "json.load", "cache_text.Cache_Text_Embeds.prepare_doc2id", "cache_text.Cache_Text_Embeds.obtain_doc_representations", "open"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_text.Cache_Text_Embeds.prepare_doc2id", "home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_text.Cache_Text_Embeds.obtain_doc_representations"], ["", "", "def", "predict_and_cache_fakenews", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"\\n\\nCaching FakeNews dataset : \"", ",", "self", ".", "config", "[", "'data_name'", "]", ")", "\n", "docs_splits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "'doc_splits_lr.json'", ")", "\n", "self", ".", "doc_splits", "=", "json", ".", "load", "(", "open", "(", "docs_splits_file", ",", "'r'", ")", ")", "\n", "self", ".", "test_docs", "=", "self", ".", "doc_splits", "[", "'test_docs'", "]", "\n", "self", ".", "train_docs", "=", "self", ".", "doc_splits", "[", "'train_docs'", "]", "\n", "self", ".", "val_docs", "=", "self", ".", "doc_splits", "[", "'val_docs'", "]", "\n", "\n", "self", ".", "prepare_doc2id", "(", ")", "\n", "self", ".", "obtain_doc_representations", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_text.Cache_Text_Embeds.predict_and_cache_fakehealth": [[58, 68], ["print", "os.path.join", "json.load", "cache_text.Cache_Text_Embeds.prepare_doc2id", "cache_text.Cache_Text_Embeds.obtain_doc_representations", "open"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_text.Cache_Text_Embeds.prepare_doc2id", "home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_text.Cache_Text_Embeds.obtain_doc_representations"], ["", "def", "predict_and_cache_fakehealth", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"\\n\\nCaching FakeHealth dataset : \"", ",", "self", ".", "config", "[", "'data_name'", "]", ")", "\n", "docs_splits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'doc_splits_{}.json'", ".", "format", "(", "self", ".", "config", "[", "'data_name'", "]", ")", ")", "\n", "self", ".", "doc_splits", "=", "json", ".", "load", "(", "open", "(", "docs_splits_file", ",", "'r'", ")", ")", "\n", "self", ".", "test_docs", "=", "self", ".", "doc_splits", "[", "'test_docs'", "]", "\n", "self", ".", "train_docs", "=", "self", ".", "doc_splits", "[", "'train_docs'", "]", "\n", "self", ".", "val_docs", "=", "self", ".", "doc_splits", "[", "'val_docs'", "]", "\n", "\n", "self", ".", "prepare_doc2id", "(", ")", "\n", "self", ".", "obtain_doc_representations", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_text.Cache_Text_Embeds.prepare_doc2id": [[71, 99], ["print", "os.path.join", "print", "print", "os.path.join", "os.walk", "len", "open", "json.dump", "os.path.join", "os.walk", "enumerate", "enumerate", "len", "len", "file.split", "file.split", "str", "str"], "methods", ["None"], ["", "def", "prepare_doc2id", "(", "self", ")", ":", "\n", "# Creating doc2id dictionary", "\n", "        ", "print", "(", "\"\\nCreating doc2id dictionary..\"", ")", "\n", "self", ".", "doc2id", "=", "{", "}", "\n", "if", "self", ".", "config", "[", "'data_name'", "]", "in", "[", "'gossipcop'", ",", "'politifact'", "]", ":", "\n", "            ", "splits", "=", "[", "'fake'", ",", "'real'", "]", "\n", "for", "split", "in", "splits", ":", "\n", "                ", "src_dir", "=", "os", ".", "path", ".", "join", "(", "'data'", ",", "'base_data'", ",", "self", ".", "config", "[", "'data_name'", "]", ",", "split", ")", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "src_dir", ")", ":", "\n", "                    ", "for", "count", ",", "file", "in", "enumerate", "(", "files", ")", ":", "\n", "                        ", "doc", "=", "file", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "# if str(doc) in test_docs:", "\n", "self", ".", "doc2id", "[", "str", "(", "doc", ")", "]", "=", "len", "(", "self", ".", "doc2id", ")", "\n", "\n", "", "", "", "", "else", ":", "\n", "            ", "src_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'content'", ",", "self", ".", "config", "[", "'data_name'", "]", ")", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "src_dir", ")", ":", "\n", "                ", "for", "count", ",", "file", "in", "enumerate", "(", "files", ")", ":", "\n", "                    ", "doc", "=", "file", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "# if str(doc) in test_docs:", "\n", "self", ".", "doc2id", "[", "str", "(", "doc", ")", "]", "=", "len", "(", "self", ".", "doc2id", ")", "\n", "\n", "", "", "", "name", "=", "'doc2id_{}_encoder.json'", ".", "format", "(", "self", ".", "config", "[", "'embed_name'", "]", ")", "\n", "doc2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "name", ")", "\n", "print", "(", "\"doc2id = \"", ",", "len", "(", "self", ".", "doc2id", ")", ")", "\n", "print", "(", "\"Saving in : \"", ",", "doc2id_file", ")", "\n", "with", "open", "(", "doc2id_file", ",", "'w+'", ")", "as", "json_file", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "doc2id", ",", "json_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.caching_funcs.cache_text.Cache_Text_Embeds.obtain_doc_representations": [[103, 143], ["torch.no_grad", "print", "torch.zeros().to", "print", "enumerate", "print", "print", "os.path.join", "print", "torch.save", "torch.zeros", "cache_text.Cache_Text_Embeds.model", "print", "len", "batch.text[].to", "batch.text[].to", "cache_text.Cache_Text_Embeds.model", "str", "batch.id.item", "str", "batch[].item"], "methods", ["None"], ["", "", "def", "obtain_doc_representations", "(", "self", ")", ":", "\n", "# iterating over test_docs and saving their representations", "\n", "        ", "splits", "=", "[", "'train'", ",", "'val'", ",", "'test'", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "self", ".", "split", "in", "splits", ":", "\n", "                ", "print", "(", "\"\\nObtaining {} doc representations...\"", ".", "format", "(", "self", ".", "split", ")", ")", "\n", "if", "self", ".", "split", "==", "'train'", ":", "\n", "                    ", "loader", "=", "self", ".", "config", "[", "'train_loader'", "]", "\n", "", "elif", "self", ".", "split", "==", "'val'", ":", "\n", "                    ", "loader", "=", "self", ".", "config", "[", "'val_loader'", "]", "\n", "", "else", ":", "\n", "                    ", "loader", "=", "self", ".", "config", "[", "'test_loader'", "]", "\n", "\n", "", "embed_dim", "=", "1024", "if", "self", ".", "config", "[", "'embed_name'", "]", "==", "'roberta'", "else", "384", "\n", "self", ".", "split_doc_cache", "=", "torch", ".", "zeros", "(", "len", "(", "self", ".", "doc2id", ")", ",", "embed_dim", ")", ".", "to", "(", "device", ")", "\n", "print", "(", "\"split_doc_cache shape = \"", ",", "self", ".", "split_doc_cache", ".", "shape", ")", "\n", "self", ".", "not_found", "=", "0", "\n", "\n", "for", "count", ",", "batch", "in", "enumerate", "(", "loader", ")", ":", "\n", "                    ", "if", "self", ".", "config", "[", "'embed_name'", "]", "==", "'cnn'", ":", "\n", "                        ", "embeds", "=", "self", ".", "model", "(", "batch", ".", "text", "[", "0", "]", ".", "to", "(", "device", ")", ",", "batch", ".", "text", "[", "1", "]", ".", "to", "(", "device", ")", ",", "cache", "=", "True", ")", "\n", "self", ".", "split_doc_cache", "[", "self", ".", "doc2id", "[", "prefix", "+", "str", "(", "batch", ".", "id", ".", "item", "(", ")", ")", "]", ",", ":", "]", "=", "embeds", "[", ":", "]", "\n", "# ids = list(batch.id)", "\n", "# for i in range(len(ids)):", "\n", "#     split_doc_cache[doc2id['gossipcop-'+str(ids[i].item())], :] = embeds[i, :]", "\n", "\n", "", "elif", "config", "[", "'embed_name'", "]", "in", "[", "'dbert'", ",", "'xlnet'", ",", "'roberta'", "]", ":", "\n", "                        ", "embeds", "=", "self", ".", "model", "(", "inp", "=", "batch", "[", "'input_ids'", "]", ",", "attn_mask", "=", "batch", "[", "'attention_mask'", "]", ")", "\n", "self", ".", "split_doc_cache", "[", "self", ".", "doc2id", "[", "prefix", "+", "str", "(", "batch", "[", "'ids'", "]", ".", "item", "(", ")", ")", "]", ",", ":", "]", "=", "embeds", "[", ":", "]", "\n", "\n", "", "if", "self", ".", "count", "%", "500", "==", "0", ":", "\n", "                        ", "print", "(", "\"{} done..\"", ".", "format", "(", "count", ")", ")", "\n", "\n", "\n", "", "", "print", "(", "\"count = \"", ",", "self", ".", "count", ")", "\n", "print", "(", "\"Not found = \"", ",", "self", ".", "not_found", ")", "\n", "name", "=", "'doc_embeds_{}'", ".", "format", "(", "self", ".", "config", "[", "'embed_name'", "]", ")", "\n", "doc_embed_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "'cached_embeds'", ",", "'{}_{}_{}.pt'", ".", "format", "(", "name", ",", "self", ".", "config", "[", "'seed'", "]", ",", "self", ".", "split", ")", ")", "\n", "print", "(", "\"\\nSaving docs embeddings in : \"", ",", "doc_embed_file", ")", "\n", "torch", ".", "save", "(", "self", ".", "split_doc_cache", ",", "doc_embed_file", ")", "\n", "# loaded_embeds = torch.load(doc_embed_file)", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.__init__": [[31, 50], ["os.path.join", "time.time", "text_train_main.Doc_Encoder_Main.init_training_params"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.init_training_params"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "self", ".", "best_val_acc", ",", "self", ".", "best_val_f1", ",", "self", ".", "best_val_recall", ",", "self", ".", "best_val_precision", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "self", ".", "preds_list", ",", "self", ".", "labels_list", "=", "[", "]", ",", "[", "]", "\n", "self", ".", "train_f1", ",", "self", ".", "train_precision", ",", "self", ".", "train_recall", ",", "self", ".", "train_accuracy", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "self", ".", "loss_list", "=", "[", "]", "\n", "self", ".", "prev_val_loss", ",", "self", ".", "not_improved", "=", "0", ",", "0", "\n", "self", ".", "best_val_loss", "=", "1000", "\n", "self", ".", "total_iters", ",", "self", ".", "threshold", "=", "0", ",", "0", "\n", "self", ".", "terminate_training", "=", "False", "\n", "self", ".", "model_file", "=", "os", ".", "path", ".", "join", "(", "config", "[", "'model_path'", "]", ",", "config", "[", "'model_save_name'", "]", ")", "\n", "self", ".", "start_epoch", ",", "self", ".", "iters", "=", "1", ",", "0", "\n", "self", ".", "model_name", "=", "config", "[", "'model_name'", "]", "\n", "self", ".", "embed_name", "=", "config", "[", "'embed_name'", "]", "\n", "self", ".", "preds", ",", "self", ".", "loss", "=", "0", ",", "0", "\n", "self", ".", "start", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n", "# Initialize the model, optimizer and loss function", "\n", "self", ".", "init_training_params", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.init_training_params": [[52, 80], ["models.model.Document_Classifier().to", "models.model.Document_Classifier().to", "torch.DataParallel", "torch.DataParallel", "torch.DataParallel", "torch.optim.AdamW", "torch.optim.AdamW", "torch.optim.AdamW", "torch.optim.AdamW", "torch.optim.AdamW", "torch.optim.AdamW", "torch.optim.AdamW", "torch.optim.AdamW", "torch.optim.AdamW", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss", "torch.BCELoss", "torch.BCELoss", "torch.BCELoss", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "text_train_main.Doc_Encoder_Main.model.parameters", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.lr_scheduler.MultiStepLR", "torch.optim.lr_scheduler.MultiStepLR", "torch.optim.lr_scheduler.MultiStepLR", "torch.optim.lr_scheduler.MultiStepLR", "torch.optim.lr_scheduler.MultiStepLR", "torch.optim.lr_scheduler.MultiStepLR", "torch.optim.lr_scheduler.MultiStepLR", "torch.optim.lr_scheduler.MultiStepLR", "torch.optim.lr_scheduler.MultiStepLR", "models.model.Document_Classifier", "models.model.Document_Classifier", "text_train_main.Doc_Encoder_Main.model.parameters", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "transformers.get_linear_schedule_with_warmup", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "init_training_params", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "embed_name", "==", "'glove'", ":", "\n", "            ", "self", ".", "model", "=", "Document_Classifier", "(", "self", ".", "config", ",", "pre_trained_embeds", "=", "self", ".", "config", "[", "'TEXT'", "]", ".", "vocab", ".", "vectors", ")", ".", "to", "(", "device", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "model", "=", "Document_Classifier", "(", "self", ".", "config", ")", ".", "to", "(", "device", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "'parallel_computing'", "]", ":", "\n", "            ", "self", ".", "model", "=", "nn", ".", "DataParallel", "(", "self", ".", "model", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "'optimizer'", "]", "==", "'Adam'", ":", "\n", "            ", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "AdamW", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "config", "[", "'lr'", "]", ",", "weight_decay", "=", "self", ".", "config", "[", "'weight_decay'", "]", ")", "\n", "", "elif", "self", ".", "config", "[", "'optimizer'", "]", "==", "'SGD'", ":", "\n", "            ", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "config", "[", "'lr'", "]", ",", "momentum", "=", "self", ".", "config", "[", "'momentum'", "]", ",", "weight_decay", "=", "self", ".", "config", "[", "'weight_decay'", "]", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "'loss_func'", "]", "==", "'bce_logits'", ":", "\n", "            ", "self", ".", "criterion", "=", "nn", ".", "BCEWithLogitsLoss", "(", "pos_weight", "=", "torch", ".", "tensor", "(", "[", "self", ".", "config", "[", "'pos_wt'", "]", "]", ")", ".", "to", "(", "device", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "criterion", "=", "nn", ".", "BCELoss", "(", ")", "\n", "\n", "\n", "", "if", "self", ".", "config", "[", "'scheduler'", "]", "==", "'step'", ":", "\n", "            ", "self", ".", "scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "StepLR", "(", "self", ".", "optimizer", ",", "step_size", "=", "self", ".", "config", "[", "'lr_decay_step'", "]", ",", "gamma", "=", "self", ".", "config", "[", "'lr_decay_factor'", "]", ")", "\n", "", "elif", "self", ".", "config", "[", "'scheduler'", "]", "==", "'multi_step'", ":", "\n", "            ", "self", ".", "scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "MultiStepLR", "(", "self", ".", "optimizer", ",", "milestones", "=", "[", "2", ",", "5", ",", "10", ",", "15", ",", "20", ",", "25", ",", "35", ",", "45", "]", ",", "gamma", "=", "self", ".", "config", "[", "'lr_decay_factor'", "]", ")", "\n", "", "elif", "self", ".", "config", "[", "'scheduler'", "]", "==", "'warmup'", ":", "\n", "            ", "self", ".", "scheduler", "=", "get_linear_schedule_with_warmup", "(", "self", ".", "optimizer", ",", "self", ".", "config", "[", "'warmup_steps'", "]", ",", "self", ".", "config", "[", "'train_steps'", "]", ")", "\n", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.eval_han_elmo": [[83, 161], ["text_train_main.Doc_Encoder_Main.model.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "HAN_Batch", "HAN_Batch.add_data", "han_data[].long().to", "han_data[].to", "han_data[].to", "han_data[].to", "len", "text_train_main.Doc_Encoder_Main.criterion", "eval_loss.append", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "preds_list.append", "labels_list.append", "numpy.arange", "max", "eval_f1_list.index", "evaluation_measures", "sum", "len", "next", "han_data[].item", "han_data[].item", "HAN_Batch.is_full", "HAN_Batch.add_data", "HAN_Batch.pad_and_sort_batch", "HAN_Batch.just_pad_batch", "text_train_main.Doc_Encoder_Main.model", "text_train_main.Doc_Encoder_Main.model", "text_train_main.Doc_Encoder_Main.to", "han_data[].to.float().unsqueeze().to", "text_train_main.Doc_Encoder_Main.detach().item", "text_train_main.Doc_Encoder_Main.cpu().squeeze().detach().numpy", "han_data[].to.cpu().detach().numpy", "evaluation_measures", "eval_f1_list.append", "eval_recall_list.append", "eval_precision_list.append", "eval_accuracy_list.append", "max", "numpy.asarray", "numpy.asarray", "next", "han_data[].item", "han_data[].item", "han_data[].long", "float", "numpy.asarray", "numpy.asarray", "han_data[].to.float().unsqueeze", "text_train_main.Doc_Encoder_Main.detach", "text_train_main.Doc_Encoder_Main.cpu().squeeze().detach", "han_data[].to.cpu().detach", "han_data[].to.float", "text_train_main.Doc_Encoder_Main.cpu().squeeze", "han_data[].to.cpu", "text_train_main.Doc_Encoder_Main.cpu"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch.add_data", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.utils.evaluation_measures", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch.is_full", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch.add_data", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch.pad_and_sort_batch", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch.just_pad_batch", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.utils.evaluation_measures"], ["", "def", "eval_han_elmo", "(", "self", ",", "test", "=", "False", ")", ":", "\n", "        ", "self", ".", "model", ".", "eval", "(", ")", "\n", "preds_list", ",", "labels_list", ",", "eval_loss", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "eval_f1_list", ",", "eval_recall_list", ",", "eval_precision_list", ",", "eval_accuracy_list", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "batch_loader", "=", "self", ".", "config", "[", "'val_loader'", "]", "if", "not", "test", "else", "self", ".", "config", "[", "'test_loader'", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "han_iterator", "=", "enumerate", "(", "batch_loader", ")", "\n", "batch_counter", "=", "0", "\n", "while", "True", ":", "\n", "                ", "try", ":", "\n", "                    ", "step", ",", "han_data", "=", "next", "(", "han_iterator", ")", "\n", "", "except", "StopIteration", ":", "\n", "                    ", "break", "\n", "", "han_batch", "=", "HAN_Batch", "(", "self", ".", "config", "[", "'han_max_batch_size'", "]", ")", "\n", "han_batch", ".", "add_data", "(", "han_data", "[", "0", "]", ",", "han_data", "[", "1", "]", ".", "item", "(", ")", ",", "han_data", "[", "2", "]", ".", "item", "(", ")", ",", "han_data", "[", "3", "]", ")", "\n", "if", "batch_counter", ">", "5", ":", "\n", "                    ", "break", "\n", "", "while", "not", "han_batch", ".", "is_full", "(", ")", ":", "\n", "                    ", "try", ":", "\n", "                        ", "step", ",", "han_data", "=", "next", "(", "han_iterator", ")", "\n", "", "except", "StopIteration", ":", "\n", "                        ", "break", "\n", "", "han_batch", ".", "add_data", "(", "han_data", "[", "0", "]", ",", "han_data", "[", "1", "]", ".", "item", "(", ")", ",", "han_data", "[", "2", "]", ".", "item", "(", ")", ",", "han_data", "[", "3", "]", ")", "\n", "\n", "", "if", "not", "self", ".", "config", "[", "'parallel_computing'", "]", ":", "\n", "                    ", "han_data", "=", "han_batch", ".", "pad_and_sort_batch", "(", ")", "\n", "", "else", ":", "\n", "                    ", "han_data", "=", "han_batch", ".", "just_pad_batch", "(", ")", "\n", "\n", "", "batch_ids", "=", "han_data", "[", "0", "]", ".", "long", "(", ")", ".", "to", "(", "device", ")", "\n", "batch_label", "=", "han_data", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "num_of_sents_in_doc", "=", "han_data", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "num_tokens_per_sent", "=", "han_data", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "batch_counter", "+=", "len", "(", "num_of_sents_in_doc", ")", "\n", "max_num_sent", "=", "batch_ids", ".", "shape", "[", "1", "]", "\n", "if", "not", "self", ".", "config", "[", "'parallel_computing'", "]", ":", "\n", "# recover_idxs = han_data[4].to(device)", "\n", "                    ", "preds", "=", "self", ".", "model", "(", "inp", "=", "batch_ids", ",", "sent_lens", "=", "num_tokens_per_sent", ",", "doc_lens", "=", "num_of_sents_in_doc", ",", "arg", "=", "max_num_sent", ")", "\n", "", "else", ":", "\n", "                    ", "preds", "=", "self", ".", "model", "(", "inp", "=", "batch_ids", ",", "sent_lens", "=", "num_tokens_per_sent", ",", "doc_lens", "=", "num_of_sents_in_doc", ",", "arg", "=", "max_num_sent", ")", "\n", "\n", "", "loss", "=", "self", ".", "criterion", "(", "preds", ".", "to", "(", "device", ")", ",", "batch_label", ".", "float", "(", ")", ".", "unsqueeze", "(", "1", ")", ".", "to", "(", "device", ")", ")", "\n", "eval_loss", ".", "append", "(", "loss", ".", "detach", "(", ")", ".", "item", "(", ")", ")", "\n", "preds", "=", "F", ".", "sigmoid", "(", "preds", ")", "\n", "preds_list", ".", "append", "(", "preds", ".", "cpu", "(", ")", ".", "squeeze", "(", "1", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "labels_list", ".", "append", "(", "batch_label", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "", "preds_list", "=", "[", "pred", "for", "batch_pred", "in", "preds_list", "for", "pred", "in", "batch_pred", "]", "\n", "labels_list", "=", "[", "label", "for", "batch_labels", "in", "labels_list", "for", "label", "in", "batch_labels", "]", "\n", "\n", "# HAN has reweighted loss. So need to optimize the threshold for binary classification. Check thresholds from 0.2-0.8 with intervals of 0.1", "\n", "if", "not", "test", ":", "\n", "                ", "thresh_list", "=", "np", ".", "arange", "(", "0.2", ",", "0.8", ",", "0.1", ")", "\n", "for", "thresh", "in", "thresh_list", ":", "\n", "                    ", "temp_preds_list", "=", "(", "preds_list", ">", "thresh", ")", "\n", "temp_preds_list", "=", "[", "float", "(", "i", ")", "for", "i", "in", "temp_preds_list", "]", "\n", "eval_f1", ",", "eval_recall", ",", "eval_precision", ",", "eval_accuracy", "=", "evaluation_measures", "(", "self", ".", "config", ",", "np", ".", "asarray", "(", "temp_preds_list", ")", ",", "np", ".", "asarray", "(", "labels_list", ")", ")", "\n", "eval_f1_list", ".", "append", "(", "eval_f1", ")", "\n", "eval_recall_list", ".", "append", "(", "eval_recall", ")", "\n", "eval_precision_list", ".", "append", "(", "eval_precision", ")", "\n", "eval_accuracy_list", ".", "append", "(", "eval_accuracy", ")", "\n", "\n", "# We log the validation stats for the threshold that gives best F1", "\n", "", "best_f1", "=", "max", "(", "eval_f1_list", ")", "\n", "best_thresh_idx", "=", "eval_f1_list", ".", "index", "(", "max", "(", "eval_f1_list", ")", ")", "\n", "if", "best_f1", ">", "self", ".", "best_val_f1", ":", "\n", "                    ", "self", ".", "threshold", "=", "thresh_list", "[", "best_thresh_idx", "]", "\n", "", "eval_precision", "=", "eval_precision_list", "[", "best_thresh_idx", "]", "\n", "eval_recall", "=", "eval_recall_list", "[", "best_thresh_idx", "]", "\n", "eval_accuracy", "=", "eval_accuracy_list", "[", "best_thresh_idx", "]", "\n", "", "else", ":", "\n", "# During test, we use the best threshold", "\n", "                ", "temp_preds_list", "=", "(", "preds_list", ">", "self", ".", "threshold", ")", ".", "type", "(", "torch", ".", "FloatTensor", ")", ".", "squeeze", "(", "1", ")", "\n", "best_f1", ",", "eval_recall", ",", "eval_precision", ",", "eval_accuracy", "=", "evaluation_measures", "(", "self", ".", "config", ",", "np", ".", "asarray", "(", "temp_preds_list", ")", ",", "np", ".", "asarray", "(", "labels_list", ")", ")", "\n", "\n", "", "eval_loss", "=", "sum", "(", "eval_loss", ")", "/", "len", "(", "eval_loss", ")", "\n", "", "return", "best_f1", ",", "eval_precision", ",", "eval_recall", ",", "eval_accuracy", ",", "eval_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.calculate_loss": [[164, 188], ["text_train_main.Doc_Encoder_Main.criterion", "text_train_main.Doc_Encoder_Main.preds_list.append", "text_train_main.Doc_Encoder_Main.labels_list.append", "text_train_main.Doc_Encoder_Main.loss_list.append", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid.squeeze().to", "torch.sigmoid.to", "batch_label.float().squeeze().to", "text_train_main.Doc_Encoder_Main.optimizer.zero_grad", "text_train_main.Doc_Encoder_Main.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "text_train_main.Doc_Encoder_Main.optimizer.step", "torch.sigmoid.cpu().detach().numpy", "batch_label.cpu().detach().numpy", "text_train_main.Doc_Encoder_Main.detach().item", "text_train_main.Doc_Encoder_Main.model.parameters", "torch.softmax", "torch.softmax", "torch.softmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.sigmoid.squeeze", "batch_label.float().squeeze", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid.cpu().detach", "batch_label.cpu().detach", "text_train_main.Doc_Encoder_Main.detach", "batch_label.float", "torch.sigmoid.cpu", "batch_label.cpu"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.att_layers.SpecialSpmmFunction.backward", "home.repos.pwc.inspect_result.shaanchandra_SAFER.optimizers.radam.RiemannianAdam.step"], ["", "def", "calculate_loss", "(", "self", ",", "preds", ",", "batch_label", ",", "grad_step", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "config", "[", "'loss_func'", "]", "==", "'bce'", ":", "\n", "            ", "preds", "=", "F", ".", "sigmoid", "(", "preds", ")", "\n", "", "preds", "=", "preds", ".", "squeeze", "(", "1", ")", ".", "to", "(", "device", ")", "if", "self", ".", "config", "[", "'loss_func'", "]", "==", "'bce_logits'", "else", "preds", ".", "to", "(", "device", ")", "\n", "loss", "=", "self", ".", "criterion", "(", "preds", ",", "batch_label", ".", "float", "(", ")", ".", "squeeze", "(", "1", ")", ".", "to", "(", "device", ")", ")", "\n", "\n", "if", "grad_step", ":", "\n", "            ", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "5", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "'loss_func'", "]", "==", "'bce'", ":", "\n", "            ", "preds", "=", "(", "preds", ">", "0.5", ")", ".", "type", "(", "torch", ".", "FloatTensor", ")", "\n", "", "elif", "self", ".", "config", "[", "'loss_func'", "]", "==", "'ce'", ":", "\n", "            ", "preds", "=", "F", ".", "softmax", "(", "preds", ",", "dim", "=", "1", ")", "\n", "preds", "=", "torch", ".", "argmax", "(", "preds", ",", "dim", "=", "1", ")", "\n", "", "elif", "self", ".", "config", "[", "'loss_func'", "]", "==", "'bce_logits'", ":", "\n", "            ", "preds", "=", "F", ".", "sigmoid", "(", "preds", ")", "\n", "preds", "=", "(", "preds", ">", "0.5", ")", ".", "type", "(", "torch", ".", "FloatTensor", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "", "self", ".", "preds_list", ".", "append", "(", "preds", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "self", ".", "labels_list", ".", "append", "(", "batch_label", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "self", ".", "loss_list", ".", "append", "(", "loss", ".", "detach", "(", ")", ".", "item", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.eval_model": [[191, 226], ["text_train_main.Doc_Encoder_Main.model.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "evaluation_measures", "numpy.random.permutation", "int", "range", "enumerate", "sum", "len", "numpy.array", "numpy.array", "len", "numpy.ceil", "get_elmo_batches", "text_train_main.Doc_Encoder_Main.model", "text_train_main.Doc_Encoder_Main.calculate_loss", "text_train_main.Doc_Encoder_Main.calculate_loss", "batch_ids.to", "text_train_main.Doc_Encoder_Main.model", "len", "text_train_main.Doc_Encoder_Main.model", "batch[].to", "batch.text[].to", "batch.text[].to"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.utils.evaluation_measures", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.get_elmo_batches", "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.calculate_loss", "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.calculate_loss"], ["", "def", "eval_model", "(", "self", ",", "test", "=", "False", ")", ":", "\n", "        ", "self", ".", "model", ".", "eval", "(", ")", "\n", "self", ".", "preds_list", ",", "self", ".", "labels_list", "=", "[", "]", ",", "[", "]", "\n", "self", ".", "loss_list", "=", "[", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "self", ".", "config", "[", "'embed_name'", "]", "==", "'elmo'", ":", "\n", "                ", "data", "=", "self", ".", "config", "[", "'val_data'", "]", "if", "not", "test", "else", "self", ".", "config", "[", "'test_data'", "]", "\n", "labels", "=", "self", ".", "config", "[", "'val_labels'", "]", "if", "not", "test", "else", "self", ".", "config", "[", "'test_labels'", "]", "\n", "rand_idxs", "=", "np", ".", "random", ".", "permutation", "(", "len", "(", "data", ")", ")", "\n", "data_shuffle", "=", "[", "data", "[", "i", "]", "for", "i", "in", "rand_idxs", "]", "\n", "label_shuffle", "=", "[", "labels", "[", "i", "]", "for", "i", "in", "rand_idxs", "]", "\n", "total_iters", "=", "int", "(", "np", ".", "ceil", "(", "len", "(", "labels", ")", "/", "self", ".", "config", "[", "'batch_size'", "]", ")", ")", "\n", "for", "iters", "in", "range", "(", "total_iters", ")", ":", "\n", "                    ", "batch_ids", ",", "batch_label", ",", "sen_lens", "=", "get_elmo_batches", "(", "self", ".", "config", ",", "total_iters", ",", "iters", ",", "data_shuffle", ",", "label_shuffle", ")", "\n", "preds", "=", "self", ".", "model", "(", "batch_ids", ".", "to", "(", "device", ")", ",", "sen_lens", ")", "\n", "self", ".", "calculate_loss", "(", "preds", ",", "batch_label", ")", "\n", "", "", "else", ":", "\n", "                ", "batch_loader", "=", "self", ".", "config", "[", "'val_loader'", "]", "if", "not", "test", "else", "self", ".", "config", "[", "'test_loader'", "]", "\n", "for", "iters", ",", "batch", "in", "enumerate", "(", "batch_loader", ")", ":", "\n", "                    ", "if", "self", ".", "config", "[", "'embed_name'", "]", "==", "'glove'", ":", "\n", "                        ", "preds", "=", "self", ".", "model", "(", "inp", "=", "batch", ".", "text", "[", "0", "]", ".", "to", "(", "device", ")", ",", "sent_lens", "=", "batch", ".", "text", "[", "1", "]", ".", "to", "(", "device", ")", ")", "\n", "batch_label", "=", "batch", ".", "label", "\n", "", "elif", "self", ".", "config", "[", "'embed_name'", "]", "in", "[", "'dbert'", ",", "'roberta'", "]", ":", "\n", "                        ", "preds", "=", "self", ".", "model", "(", "inp", "=", "batch", "[", "'input_ids'", "]", ",", "attn_mask", "=", "batch", "[", "'attention_mask'", "]", ")", "\n", "batch_label", "=", "batch", "[", "'labels'", "]", ".", "to", "(", "device", ")", "\n", "", "self", ".", "calculate_loss", "(", "preds", ",", "batch_label", ")", "\n", "\n", "", "", "self", ".", "preds_list", "=", "[", "pred", "for", "batch_pred", "in", "self", ".", "preds_list", "for", "pred", "in", "batch_pred", "]", "\n", "self", ".", "labels_list", "=", "[", "label", "for", "batch_labels", "in", "self", ".", "labels_list", "for", "label", "in", "batch_labels", "]", "\n", "\n", "eval_loss", "=", "sum", "(", "self", ".", "loss_list", ")", "/", "len", "(", "self", ".", "loss_list", ")", "\n", "eval_f1", ",", "eval_macro_f1", ",", "eval_recall", ",", "eval_precision", ",", "eval_accuracy", "=", "evaluation_measures", "(", "self", ".", "config", ",", "np", ".", "array", "(", "self", ".", "preds_list", ")", ",", "np", ".", "array", "(", "self", ".", "labels_list", ")", ")", "\n", "\n", "", "return", "eval_f1", ",", "eval_macro_f1", ",", "eval_precision", ",", "eval_recall", ",", "eval_accuracy", ",", "eval_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.save_model": [[228, 236], ["torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "os.path.join", "text_train_main.Doc_Encoder_Main.model.state_dict", "text_train_main.Doc_Encoder_Main.optimizer.state_dict"], "methods", ["None"], ["", "def", "save_model", "(", "self", ")", ":", "\n", "        ", "torch", ".", "save", "(", "{", "\n", "'epoch'", ":", "self", ".", "epoch", ",", "\n", "'best_val_f1'", ":", "self", ".", "best_val_f1", ",", "\n", "'model_state_dict'", ":", "self", ".", "model", ".", "state_dict", "(", ")", ",", "\n", "# 'model_classif_state_dict': model.classifier.state_dict(),", "\n", "'optimizer_state_dict'", ":", "self", ".", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "}", ",", "os", ".", "path", ".", "join", "(", "self", ".", "model_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.load_model": [[237, 241], ["torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "text_train_main.Doc_Encoder_Main.model.load_state_dict", "text_train_main.Doc_Encoder_Main.optimizer.load_state_dict"], "methods", ["None"], ["", "def", "load_model", "(", "self", ")", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "self", ".", "model_file", ")", "\n", "self", ".", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'model_state_dict'", "]", ")", "\n", "self", ".", "optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'optimizer_state_dict'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.generate_summary": [[243, 247], ["print", "sklearn.metrics.classification_report"], "methods", ["None"], ["", "def", "generate_summary", "(", "self", ",", "preds", ",", "labels", ")", ":", "\n", "        ", "target_names", "=", "[", "'False'", ",", "'True'", ",", "'Unverified'", "]", "\n", "print", "(", "classification_report", "(", "labels", ",", "preds", ",", "target_names", "=", "target_names", ")", ")", "\n", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.check_early_stopping": [[250, 275], ["text_train_main.Doc_Encoder_Main.scheduler.step", "print", "print", "text_train_main.Doc_Encoder_Main.save_model"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.optimizers.radam.RiemannianAdam.step", "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.save_model"], ["", "def", "check_early_stopping", "(", "self", ")", ":", "\n", "        ", "self", ".", "this_metric", "=", "self", ".", "eval_f1", "if", "self", ".", "config", "[", "'optimze_for'", "]", "==", "'f1'", "else", "self", ".", "eval_loss", "\n", "self", ".", "current_best", "=", "self", ".", "best_val_f1", "if", "self", ".", "config", "[", "'optimze_for'", "]", "==", "'f1'", "else", "self", ".", "best_val_loss", "\n", "\n", "new_best", "=", "self", ".", "this_metric", ">", "self", ".", "current_best", "if", "self", ".", "config", "[", "'optimze_for'", "]", "==", "'f1'", "else", "self", ".", "this_metric", "<", "self", ".", "current_best", "\n", "if", "new_best", ":", "\n", "            ", "print", "(", "\"New High Score! Saving model...\"", ")", "\n", "self", ".", "best_val_f1", "=", "self", ".", "eval_f1", "\n", "self", ".", "best_val_loss", "=", "self", ".", "eval_loss", "\n", "self", ".", "best_val_acc", "=", "self", ".", "eval_accuracy", "\n", "self", ".", "best_val_recall", "=", "self", ".", "eval_recall", "\n", "self", ".", "best_val_precision", "=", "self", ".", "eval_precision", "\n", "self", ".", "save_model", "(", ")", "\n", "\n", "", "self", ".", "scheduler", ".", "step", "(", ")", "\n", "\n", "### Stopping Criteria based on patience ###        ", "\n", "diff", "=", "self", ".", "this_metric", "-", "self", ".", "current_best", "if", "self", ".", "config", "[", "'optimze_for'", "]", "==", "'f1'", "else", "self", ".", "current_best", "-", "self", ".", "this_metric", "\n", "if", "diff", "<", "1e-3", ":", "\n", "            ", "self", ".", "not_improved", "+=", "1", "\n", "if", "self", ".", "not_improved", ">=", "self", ".", "config", "[", "'patience'", "]", ":", "\n", "                ", "self", ".", "terminate_training", "=", "True", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "not_improved", "=", "0", "\n", "", "print", "(", "\"current patience: \"", ",", "self", ".", "not_improved", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.train_epoch_step": [[279, 306], ["text_train_main.Doc_Encoder_Main.model.train", "text_train_main.Doc_Encoder_Main.scheduler.get_lr", "evaluation_measures", "log_tensorboard", "text_train_main.Doc_Encoder_Main.eval_model", "print_stats", "log_tensorboard", "text_train_main.Doc_Encoder_Main.check_early_stopping", "numpy.array", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.utils.evaluation_measures", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.utils.log_tensorboard", "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.eval_model", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.utils.print_stats", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.utils.log_tensorboard", "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.check_early_stopping"], ["", "def", "train_epoch_step", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", ".", "train", "(", ")", "\n", "lr", "=", "self", ".", "scheduler", ".", "get_lr", "(", ")", "\n", "self", ".", "total_iters", "+=", "self", ".", "iters", "\n", "self", ".", "preds_list", "=", "[", "pred", "for", "batch_pred", "in", "self", ".", "preds_list", "for", "pred", "in", "batch_pred", "]", "\n", "self", ".", "labels_list", "=", "[", "label", "for", "batch_labels", "in", "self", ".", "labels_list", "for", "label", "in", "batch_labels", "]", "\n", "\n", "# Evaluate on train set", "\n", "self", ".", "train_f1", ",", "self", ".", "train_f1_macro", ",", "self", ".", "train_recall", ",", "self", ".", "train_precision", ",", "self", ".", "train_accuracy", "=", "evaluation_measures", "(", "self", ".", "config", ",", "np", ".", "array", "(", "self", ".", "preds_list", ")", ",", "np", ".", "array", "(", "self", ".", "labels_list", ")", ")", "\n", "log_tensorboard", "(", "self", ".", "config", ",", "self", ".", "config", "[", "'writer'", "]", ",", "self", ".", "model", ",", "self", ".", "epoch", ",", "self", ".", "iters", ",", "self", ".", "total_iters", ",", "self", ".", "train_loss", ",", "self", ".", "train_f1", ",", "self", ".", "train_precision", ",", "self", ".", "train_recall", ",", "self", ".", "train_accuracy", ",", "lr", "[", "0", "]", ",", "self", ".", "threshold", ",", "loss_only", "=", "False", ",", "val", "=", "False", ")", "\n", "\n", "# Evaluate on dev set", "\n", "self", ".", "eval_f1", ",", "self", ".", "eval_f1_macro", ",", "self", ".", "eval_precision", ",", "self", ".", "eval_recall", ",", "self", ".", "eval_accuracy", ",", "self", ".", "eval_loss", "=", "self", ".", "eval_model", "(", ")", "\n", "\n", "# print stats", "\n", "print_stats", "(", "self", ".", "config", ",", "self", ".", "epoch", ",", "self", ".", "loss_list", ",", "self", ".", "train_accuracy", ",", "self", ".", "train_f1", ",", "self", ".", "train_f1_macro", ",", "self", ".", "train_precision", ",", "self", ".", "train_recall", ",", "\n", "self", ".", "eval_loss", ",", "self", ".", "eval_accuracy", ",", "self", ".", "eval_f1", ",", "self", ".", "eval_f1_macro", ",", "self", ".", "eval_precision", ",", "self", ".", "eval_recall", ",", "self", ".", "start", ",", "lr", "[", "0", "]", ")", "\n", "\n", "# log validation stats in tensorboard", "\n", "log_tensorboard", "(", "self", ".", "config", ",", "self", ".", "config", "[", "'writer'", "]", ",", "self", ".", "model", ",", "self", ".", "epoch", ",", "self", ".", "iters", ",", "self", ".", "total_iters", ",", "self", ".", "eval_loss", ",", "self", ".", "eval_f1", ",", "self", ".", "eval_precision", ",", "self", ".", "eval_recall", ",", "self", ".", "eval_accuracy", ",", "lr", "[", "0", "]", ",", "self", ".", "threshold", ",", "loss_only", "=", "False", ",", "val", "=", "True", ")", "\n", "\n", "# Check for early stopping criteria", "\n", "self", ".", "check_early_stopping", "(", ")", "\n", "\n", "self", ".", "preds_list", "=", "[", "]", "\n", "self", ".", "labels_list", "=", "[", "]", "\n", "self", ".", "loss_list", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.end_training": [[310, 327], ["print", "os.path.isfile", "text_train_main.Doc_Encoder_Main.eval_model", "print_test_stats", "text_train_main.Doc_Encoder_Main.config[].close", "print", "print", "text_train_main.Doc_Encoder_Main.load_model", "ValueError"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.eval_model", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.utils.print_test_stats", "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.load_model"], ["", "def", "end_training", "(", "self", ")", ":", "\n", "# Termination message", "\n", "        ", "if", "self", ".", "terminate_training", ":", "\n", "            ", "print", "(", "\"\\n\"", "+", "\"-\"", "*", "100", "+", "\"\\nTraining terminated early because the Validation loss did not improve for   {}   epochs\"", ".", "format", "(", "self", ".", "config", "[", "'patience'", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"\\n\"", "+", "\"-\"", "*", "100", "+", "\"\\nMaximum epochs of {} reached. Finished training !!\"", ".", "format", "(", "self", ".", "config", "[", "'max_epoch'", "]", ")", ")", "\n", "\n", "", "print", "(", "\"\\n\"", "+", "\"-\"", "*", "50", "+", "\"\\n\\t\\tEvaluating on test set\\n\"", "+", "\"-\"", "*", "50", ")", "\n", "# self.model_file = os.path.join(self.config['model_checkpoint_path'], self.config['data_name'], self.config['model_name'], self.config['model_save_name'])", "\n", "if", "os", ".", "path", ".", "isfile", "(", "self", ".", "model_file", ")", ":", "\n", "            ", "self", ".", "load_model", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"No Saved model state_dict found for the chosen model...!!! \\nAborting evaluation on test set...\"", ".", "format", "(", "self", ".", "config", "[", "'model_name'", "]", ")", ")", "\n", "\n", "", "self", ".", "test_f1", ",", "self", ".", "test_f1_macro", ",", "self", ".", "test_precision", ",", "self", ".", "test_recall", ",", "self", ".", "test_accuracy", ",", "_", "=", "self", ".", "eval_model", "(", "test", "=", "True", ")", "\n", "print_test_stats", "(", "self", ".", "test_accuracy", ",", "self", ".", "test_precision", ",", "self", ".", "test_recall", ",", "self", ".", "test_f1", ",", "self", ".", "test_f1_macro", ",", "self", ".", "best_val_acc", ",", "self", ".", "best_val_precision", ",", "self", ".", "best_val_recall", ",", "self", ".", "best_val_f1", ")", "\n", "self", ".", "config", "[", "'writer'", "]", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.train_main": [[331, 444], ["print", "time.time", "print", "text_train_main.Doc_Encoder_Main.end_training", "range", "Cache_Text_Embeds", "datetime.datetime.now", "enumerate", "text_train_main.Doc_Encoder_Main.train_epoch_step", "range", "text_train_main.Doc_Encoder_Main.model.train", "text_train_main.Doc_Encoder_Main.model", "numpy.random.permutation", "int", "range", "text_train_main.Doc_Encoder_Main.train_epoch_step", "range", "len", "numpy.ceil", "text_train_main.Doc_Encoder_Main.model.train", "get_elmo_batches", "text_train_main.Doc_Encoder_Main.model", "enumerate", "text_train_main.Doc_Encoder_Main.train_epoch_step", "range", "text_train_main.Doc_Encoder_Main.batch.text[].to", "text_train_main.Doc_Encoder_Main.batch.text[].to", "text_train_main.Doc_Encoder_Main.batch_ids.to", "text_train_main.Doc_Encoder_Main.model.train", "text_train_main.Doc_Encoder_Main.model", "text_train_main.Doc_Encoder_Main.batch[].to", "text_train_main.Doc_Encoder_Main.model.train", "len", "enumerate", "text_train_main.Doc_Encoder_Main.train_epoch_step", "len", "HAN_Batch", "HAN_Batch.add_data", "han_data[].long().to", "han_data[].to", "han_data[].to", "han_data[].to", "len", "next", "han_data[].item", "han_data[].item", "HAN_Batch.is_full", "HAN_Batch.add_data", "HAN_Batch.pad_and_sort_batch", "HAN_Batch.just_pad_batch", "text_train_main.Doc_Encoder_Main.model", "text_train_main.Doc_Encoder_Main.model", "next", "han_data[].item", "han_data[].item", "han_data[].long"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.end_training", "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.train_epoch_step", "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.train_epoch_step", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.get_elmo_batches", "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.train_epoch_step", "home.repos.pwc.inspect_result.shaanchandra_SAFER.text_train.text_train_main.Doc_Encoder_Main.train_epoch_step", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch.add_data", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch.is_full", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch.add_data", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch.pad_and_sort_batch", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.data_utils_txt.HAN_Batch.just_pad_batch"], ["", "def", "train_main", "(", "self", ",", "cache", "=", "False", ")", ":", "\n", "        ", "print", "(", "\"\\n\\n\"", "+", "\"=\"", "*", "100", "+", "\"\\n\\t\\t\\t\\t\\t Training Network\\n\"", "+", "\"=\"", "*", "100", ")", "\n", "\n", "# Load the checkpoint to resume training if found        ", "\n", "# if os.path.isfile(self.model_file):", "\n", "#     checkpoint = torch.load(self.model_file)", "\n", "#     self.best_val_f1 = checkpoint['best_val']", "\n", "#     self.model.load_state_dict(checkpoint['model_state_dict'])", "\n", "#     self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])", "\n", "#     self.start_epoch = checkpoint['epoch'] + 1", "\n", "#     self.threshold = checkpoint['threshold']", "\n", "#     print(\"\\nResuming training from epoch {} with loaded model and optimizer...\\n\".format(self.start_epoch))", "\n", "#     print(\"Using the model defined below: \\n\\n\")", "\n", "#     print(self.model)", "\n", "# else:", "\n", "#     print(\"\\nNo Checkpoints found for the chosen model to reusme training... \\nTraining the  ''{}''  model from scratch...\\n\".format(self.config['model_name']))", "\n", "#     print(\"Using the model defined below: \\n\\n\")", "\n", "#     print(self.model)", "\n", "# print(self.model)", "\n", "\n", "self", ".", "start", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "\"\\nBeginning training at:  {} \\n\"", ".", "format", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", ")", "\n", "\n", "if", "self", ".", "model_name", "!=", "'han'", "and", "self", ".", "embed_name", "==", "'glove'", ":", "\n", "# for self.epoch in range(1):", "\n", "            ", "for", "self", ".", "epoch", "in", "range", "(", "self", ".", "start_epoch", ",", "self", ".", "config", "[", "'max_epoch'", "]", "+", "1", ")", ":", "\n", "                ", "for", "self", ".", "iters", ",", "self", ".", "batch", "in", "enumerate", "(", "self", ".", "config", "[", "'train_loader'", "]", ")", ":", "\n", "                    ", "self", ".", "model", ".", "train", "(", ")", "\n", "self", ".", "preds", "=", "self", ".", "model", "(", "inp", "=", "self", ".", "batch", ".", "text", "[", "0", "]", ".", "to", "(", "device", ")", ",", "sent_lens", "=", "self", ".", "batch", ".", "text", "[", "1", "]", ".", "to", "(", "device", ")", ")", "\n", "", "self", ".", "train_epoch_step", "(", ")", "\n", "\n", "if", "self", ".", "terminate_training", ":", "\n", "                    ", "break", "\n", "\n", "", "", "", "elif", "self", ".", "model_name", "!=", "'han'", "and", "self", ".", "embed_name", "==", "'elmo'", ":", "\n", "# for self.epoch in range(1):", "\n", "            ", "for", "self", ".", "epoch", "in", "range", "(", "self", ".", "start_epoch", ",", "self", ".", "config", "[", "'max_epoch'", "]", "+", "1", ")", ":", "\n", "                ", "rand_idxs", "=", "np", ".", "random", ".", "permutation", "(", "len", "(", "self", ".", "config", "[", "'train_data'", "]", ")", ")", "\n", "train_data_shuffle", "=", "[", "self", ".", "config", "[", "'train_data'", "]", "[", "i", "]", "for", "i", "in", "rand_idxs", "]", "\n", "train_label_shuffle", "=", "[", "self", ".", "config", "[", "'train_labels'", "]", "[", "i", "]", "for", "i", "in", "rand_idxs", "]", "\n", "\n", "self", ".", "max_iters", "=", "int", "(", "np", ".", "ceil", "(", "len", "(", "train_label_shuffle", ")", "/", "self", ".", "config", "[", "'batch_size'", "]", ")", ")", "\n", "for", "self", ".", "iters", "in", "range", "(", "self", ".", "max_iters", ")", ":", "\n", "                    ", "self", ".", "model", ".", "train", "(", ")", "\n", "self", ".", "batch_ids", ",", "self", ".", "batch_label", ",", "self", ".", "sen_lens", "=", "get_elmo_batches", "(", "self", ".", "config", ",", "self", ".", "max_iters", ",", "self", ".", "iters", ",", "train_data_shuffle", ",", "train_label_shuffle", ")", "\n", "self", ".", "preds", "=", "self", ".", "model", "(", "self", ".", "batch_ids", ".", "to", "(", "device", ")", ",", "self", ".", "sen_lens", ")", "\n", "", "self", ".", "train_epoch_step", "(", ")", "\n", "if", "self", ".", "terminate_training", ":", "\n", "                    ", "break", "\n", "\n", "", "", "", "elif", "self", ".", "embed_name", "in", "[", "'dbert'", ",", "'xlnet'", ",", "'roberta'", "]", ":", "\n", "            ", "for", "self", ".", "epoch", "in", "range", "(", "self", ".", "start_epoch", ",", "self", ".", "config", "[", "'max_epoch'", "]", "+", "1", ")", ":", "\n", "                ", "for", "self", ".", "iters", ",", "self", ".", "batch", "in", "enumerate", "(", "self", ".", "config", "[", "'train_loader'", "]", ")", ":", "\n", "                    ", "self", ".", "model", ".", "train", "(", ")", "\n", "self", ".", "preds", "=", "self", ".", "model", "(", "inp", "=", "self", ".", "batch", "[", "'input_ids'", "]", ",", "attn_mask", "=", "self", ".", "batch", "[", "'attention_mask'", "]", ")", "\n", "self", ".", "batch_label", "=", "self", ".", "batch", "[", "'labels'", "]", ".", "to", "(", "device", ")", "\n", "", "self", ".", "train_epoch_step", "(", ")", "\n", "if", "self", ".", "terminate_training", ":", "\n", "                    ", "break", "\n", "\n", "\n", "", "", "", "elif", "self", ".", "model_name", "==", "'han'", "and", "self", ".", "embed_name", "==", "'elmo'", ":", "\n", "# for self.epoch in range(1):", "\n", "            ", "for", "self", ".", "epoch", "in", "range", "(", "self", ".", "start_epoch", ",", "self", ".", "config", "[", "'max_epoch'", "]", "+", "1", ")", ":", "\n", "# with torch.autograd.set_detect_anomaly(True):", "\n", "                ", "self", ".", "model", ".", "train", "(", ")", "\n", "self", ".", "total_len_train", "=", "len", "(", "self", ".", "config", "[", "'train_loader'", "]", ")", "\n", "han_iterator", "=", "enumerate", "(", "self", ".", "config", "[", "'train_loader'", "]", ")", "\n", "self", ".", "batch_counter", "=", "0", "\n", "self", ".", "iters", "=", "0", "\n", "while", "True", ":", "\n", "                    ", "self", ".", "iters", "+=", "1", "\n", "try", ":", "\n", "                        ", "step", ",", "han_data", "=", "next", "(", "han_iterator", ")", "\n", "", "except", "StopIteration", ":", "\n", "                        ", "break", "\n", "", "han_batch", "=", "HAN_Batch", "(", "self", ".", "config", "[", "'han_max_batch_size'", "]", ")", "\n", "han_batch", ".", "add_data", "(", "han_data", "[", "0", "]", ",", "han_data", "[", "1", "]", ".", "item", "(", ")", ",", "han_data", "[", "2", "]", ".", "item", "(", ")", ",", "han_data", "[", "3", "]", ")", "\n", "if", "self", ".", "batch_counter", ">", "10", ":", "\n", "                        ", "break", "\n", "", "while", "not", "han_batch", ".", "is_full", "(", ")", ":", "\n", "                        ", "try", ":", "\n", "                            ", "step", ",", "han_data", "=", "next", "(", "han_iterator", ")", "\n", "", "except", "StopIteration", ":", "\n", "                            ", "break", "\n", "", "han_batch", ".", "add_data", "(", "han_data", "[", "0", "]", ",", "han_data", "[", "1", "]", ".", "item", "(", ")", ",", "han_data", "[", "2", "]", ".", "item", "(", ")", ",", "han_data", "[", "3", "]", ")", "\n", "\n", "", "if", "not", "self", ".", "config", "[", "'parallel_computing'", "]", ":", "\n", "                        ", "han_data", "=", "han_batch", ".", "pad_and_sort_batch", "(", ")", "\n", "", "else", ":", "\n", "                        ", "han_data", "=", "han_batch", ".", "just_pad_batch", "(", ")", "\n", "\n", "", "self", ".", "batch_ids", "=", "han_data", "[", "0", "]", ".", "long", "(", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "batch_label", "=", "han_data", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "self", ".", "num_of_sents_in_doc", "=", "han_data", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "self", ".", "num_tokens_per_sent", "=", "han_data", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "self", ".", "batch_counter", "+=", "len", "(", "self", ".", "num_of_sents_in_doc", ")", "\n", "max_num_sent", "=", "self", ".", "batch_ids", ".", "shape", "[", "1", "]", "\n", "\n", "if", "not", "self", ".", "config", "[", "'parallel_computing'", "]", ":", "\n", "# self.recover_idxs = han_data[4].to(device)", "\n", "                        ", "self", ".", "preds", "=", "self", ".", "model", "(", "inp", "=", "self", ".", "batch_ids", ",", "sent_lens", "=", "self", ".", "num_tokens_per_sent", ",", "doc_lens", "=", "self", ".", "num_of_sents_in_doc", ",", "arg", "=", "max_num_sent", ")", "\n", "", "else", ":", "\n", "                        ", "self", ".", "preds", "=", "self", ".", "model", "(", "inp", "=", "self", ".", "batch_ids", ",", "sent_lens", "=", "self", ".", "num_tokens_per_sent", ",", "doc_lens", "=", "self", ".", "num_of_sents_in_doc", ",", "arg", "=", "max_num_sent", ")", "\n", "", "", "self", ".", "train_epoch_step", "(", ")", "\n", "if", "self", ".", "terminate_training", ":", "\n", "                    ", "break", "\n", "\n", "\n", "", "", "", "self", ".", "end_training", "(", ")", "\n", "if", "cache", ":", "\n", "            ", "Cache_Text_Embeds", "(", "self", ".", "config", ",", "self", ".", "model", ")", "\n", "", "return", "self", ".", "best_val_f1", ",", "self", ".", "best_val_acc", ",", "self", ".", "best_val_recall", ",", "self", ".", "best_val_precision", ",", "self", ".", "test_f1", ",", "self", ".", "test_f1_macro", ",", "self", ".", "test_accuracy", ",", "self", ".", "test_recall", ",", "self", ".", "test_precision", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.att_layers.DenseAtt.__init__": [[9, 14], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_features", ",", "dropout", ")", ":", "\n", "        ", "super", "(", "DenseAtt", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "2", "*", "in_features", ",", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "in_features", "=", "in_features", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.att_layers.DenseAtt.forward": [[15, 29], ["x.size", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "x_left.expand.expand.expand", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "x_right.expand.expand.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "att_layers.DenseAtt.linear().squeeze", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "att_layers.DenseAtt.linear"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "adj", ")", ":", "\n", "        ", "n", "=", "x", ".", "size", "(", "0", ")", "\n", "# n x 1 x d", "\n", "x_left", "=", "torch", ".", "unsqueeze", "(", "x", ",", "1", ")", "\n", "x_left", "=", "x_left", ".", "expand", "(", "-", "1", ",", "n", ",", "-", "1", ")", "\n", "# 1 x n x d", "\n", "x_right", "=", "torch", ".", "unsqueeze", "(", "x", ",", "0", ")", "\n", "x_right", "=", "x_right", ".", "expand", "(", "n", ",", "-", "1", ",", "-", "1", ")", "\n", "\n", "x_cat", "=", "torch", ".", "cat", "(", "(", "x_left", ",", "x_right", ")", ",", "dim", "=", "2", ")", "\n", "att_adj", "=", "self", ".", "linear", "(", "x_cat", ")", ".", "squeeze", "(", ")", "\n", "att_adj", "=", "F", ".", "sigmoid", "(", "att_adj", ")", "\n", "att_adj", "=", "torch", ".", "mul", "(", "adj", ",", "att_adj", ")", "\n", "return", "att_adj", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.att_layers.SpecialSpmmFunction.forward": [[34, 41], ["torch.sparse_coo_tensor", "torch.sparse_coo_tensor", "torch.sparse_coo_tensor", "torch.sparse_coo_tensor", "torch.sparse_coo_tensor", "torch.sparse_coo_tensor", "torch.sparse_coo_tensor", "torch.sparse_coo_tensor", "torch.sparse_coo_tensor", "ctx.save_for_backward", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "indices", ",", "values", ",", "shape", ",", "b", ")", ":", "\n", "        ", "assert", "indices", ".", "requires_grad", "==", "False", "\n", "a", "=", "torch", ".", "sparse_coo_tensor", "(", "indices", ",", "values", ",", "shape", ")", "\n", "ctx", ".", "save_for_backward", "(", "a", ",", "b", ")", "\n", "ctx", ".", "N", "=", "shape", "[", "0", "]", "\n", "return", "torch", ".", "matmul", "(", "a", ",", "b", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.att_layers.SpecialSpmmFunction.backward": [[42, 53], ["grad_output.matmul", "a.t().matmul", "b.t", "grad_output.matmul.view", "a._indices", "a.t", "a._indices"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "a", ",", "b", "=", "ctx", ".", "saved_tensors", "\n", "grad_values", "=", "grad_b", "=", "None", "\n", "if", "ctx", ".", "needs_input_grad", "[", "1", "]", ":", "\n", "            ", "grad_a_dense", "=", "grad_output", ".", "matmul", "(", "b", ".", "t", "(", ")", ")", "\n", "edge_idx", "=", "a", ".", "_indices", "(", ")", "[", "0", ",", ":", "]", "*", "ctx", ".", "N", "+", "a", ".", "_indices", "(", ")", "[", "1", ",", ":", "]", "\n", "grad_values", "=", "grad_a_dense", ".", "view", "(", "-", "1", ")", "[", "edge_idx", "]", "\n", "", "if", "ctx", ".", "needs_input_grad", "[", "3", "]", ":", "\n", "            ", "grad_b", "=", "a", ".", "t", "(", ")", ".", "matmul", "(", "grad_output", ")", "\n", "", "return", "None", ",", "grad_values", ",", "None", ",", "grad_b", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.att_layers.SpecialSpmm.forward": [[56, 58], ["SpecialSpmmFunction.apply"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "indices", ",", "values", ",", "shape", ",", "b", ")", ":", "\n", "        ", "return", "SpecialSpmmFunction", ".", "apply", "(", "indices", ",", "values", ",", "shape", ",", "b", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.att_layers.SpGraphAttentionLayer.__init__": [[65, 81], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.init.xavier_normal_", "torch.init.xavier_normal_", "torch.init.xavier_normal_", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.init.xavier_normal_", "torch.init.xavier_normal_", "torch.init.xavier_normal_", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU", "att_layers.SpecialSpmm", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "in_features", ",", "out_features", ",", "dropout", ",", "alpha", ",", "activation", ")", ":", "\n", "        ", "super", "(", "SpGraphAttentionLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "in_features", "=", "in_features", "\n", "self", ".", "out_features", "=", "out_features", "\n", "self", ".", "alpha", "=", "alpha", "\n", "\n", "self", ".", "W", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "size", "=", "(", "in_features", ",", "out_features", ")", ")", ")", "\n", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "W", ".", "data", ",", "gain", "=", "1.414", ")", "\n", "\n", "self", ".", "a", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "size", "=", "(", "1", ",", "2", "*", "out_features", ")", ")", ")", "\n", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "a", ".", "data", ",", "gain", "=", "1.414", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "leakyrelu", "=", "nn", ".", "LeakyReLU", "(", "self", ".", "alpha", ")", "\n", "self", ".", "special_spmm", "=", "SpecialSpmm", "(", ")", "\n", "self", ".", "act", "=", "activation", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.att_layers.SpGraphAttentionLayer.forward": [[82, 115], ["adj._indices", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.cat().t", "torch.cat().t", "torch.cat().t", "torch.cat().t", "torch.cat().t", "torch.cat().t", "torch.cat().t", "torch.cat().t", "torch.cat().t", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "att_layers.SpGraphAttentionLayer.special_spmm", "att_layers.SpGraphAttentionLayer.dropout", "att_layers.SpGraphAttentionLayer.special_spmm", "h_prime.div.div.div", "att_layers.SpGraphAttentionLayer.act", "input.size", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "ones.cuda.cuda.cuda", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "att_layers.SpGraphAttentionLayer.leakyrelu", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "att_layers.SpGraphAttentionLayer.a.mm().squeeze", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "att_layers.SpGraphAttentionLayer.a.mm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "adj", ")", ":", "\n", "        ", "N", "=", "input", ".", "size", "(", ")", "[", "0", "]", "\n", "edge", "=", "adj", ".", "_indices", "(", ")", "\n", "\n", "h", "=", "torch", ".", "mm", "(", "input", ",", "self", ".", "W", ")", "\n", "# h: N x out", "\n", "assert", "not", "torch", ".", "isnan", "(", "h", ")", ".", "any", "(", ")", "\n", "\n", "# Self-attention on the nodes - Shared attention mechanism", "\n", "edge_h", "=", "torch", ".", "cat", "(", "(", "h", "[", "edge", "[", "0", ",", ":", "]", ",", ":", "]", ",", "h", "[", "edge", "[", "1", ",", ":", "]", ",", ":", "]", ")", ",", "dim", "=", "1", ")", ".", "t", "(", ")", "\n", "# edge: 2*D x E", "\n", "\n", "edge_e", "=", "torch", ".", "exp", "(", "-", "self", ".", "leakyrelu", "(", "self", ".", "a", ".", "mm", "(", "edge_h", ")", ".", "squeeze", "(", ")", ")", ")", "\n", "assert", "not", "torch", ".", "isnan", "(", "edge_e", ")", ".", "any", "(", ")", "\n", "# edge_e: E", "\n", "\n", "ones", "=", "torch", ".", "ones", "(", "size", "=", "(", "N", ",", "1", ")", ")", "\n", "if", "h", ".", "is_cuda", ":", "\n", "            ", "ones", "=", "ones", ".", "cuda", "(", ")", "\n", "", "e_rowsum", "=", "self", ".", "special_spmm", "(", "edge", ",", "edge_e", ",", "torch", ".", "Size", "(", "[", "N", ",", "N", "]", ")", ",", "ones", ")", "\n", "# e_rowsum: N x 1", "\n", "\n", "edge_e", "=", "self", ".", "dropout", "(", "edge_e", ")", "\n", "# edge_e: E", "\n", "\n", "h_prime", "=", "self", ".", "special_spmm", "(", "edge", ",", "edge_e", ",", "torch", ".", "Size", "(", "[", "N", ",", "N", "]", ")", ",", "h", ")", "\n", "assert", "not", "torch", ".", "isnan", "(", "h_prime", ")", ".", "any", "(", ")", "\n", "# h_prime: N x out", "\n", "\n", "h_prime", "=", "h_prime", ".", "div", "(", "e_rowsum", ")", "\n", "# h_prime: N x out", "\n", "assert", "not", "torch", ".", "isnan", "(", "h_prime", ")", ".", "any", "(", ")", "\n", "return", "self", ".", "act", "(", "h_prime", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.att_layers.SpGraphAttentionLayer.__repr__": [[116, 118], ["str", "str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "' ('", "+", "str", "(", "self", ".", "in_features", ")", "+", "' -> '", "+", "str", "(", "self", ".", "out_features", ")", "+", "')'", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.att_layers.GraphAttentionLayer.__init__": [[121, 134], ["torch.Module.__init__", "enumerate", "att_layers.SpGraphAttentionLayer", "att_layers.GraphAttentionLayer.add_module", "range"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "output_dim", ",", "dropout", ",", "activation", ",", "alpha", ",", "nheads", ",", "concat", ")", ":", "\n", "        ", "\"\"\"Sparse version of GAT.\"\"\"", "\n", "super", "(", "GraphAttentionLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "output_dim", "=", "output_dim", "\n", "self", ".", "attentions", "=", "[", "SpGraphAttentionLayer", "(", "input_dim", ",", "\n", "output_dim", ",", "\n", "dropout", "=", "dropout", ",", "\n", "alpha", "=", "alpha", ",", "\n", "activation", "=", "activation", ")", "for", "_", "in", "range", "(", "nheads", ")", "]", "\n", "self", ".", "concat", "=", "concat", "\n", "for", "i", ",", "attention", "in", "enumerate", "(", "self", ".", "attentions", ")", ":", "\n", "            ", "self", ".", "add_module", "(", "'attention_{}'", ".", "format", "(", "i", ")", ",", "attention", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.att_layers.GraphAttentionLayer.forward": [[135, 145], ["torch.dropout", "torch.dropout", "torch.dropout", "torch.dropout", "torch.dropout", "torch.dropout", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "att", "att().view", "att"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "x", ",", "adj", "=", "input", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "if", "self", ".", "concat", ":", "\n", "            ", "h", "=", "torch", ".", "cat", "(", "[", "att", "(", "x", ",", "adj", ")", "for", "att", "in", "self", ".", "attentions", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "h_cat", "=", "torch", ".", "cat", "(", "[", "att", "(", "x", ",", "adj", ")", ".", "view", "(", "(", "-", "1", ",", "self", ".", "output_dim", ",", "1", ")", ")", "for", "att", "in", "self", ".", "attentions", "]", ",", "dim", "=", "2", ")", "\n", "h", "=", "torch", ".", "mean", "(", "h_cat", ",", "dim", "=", "2", ")", "\n", "", "h", "=", "F", ".", "dropout", "(", "h", ",", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "return", "(", "h", ",", "adj", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.layers.GraphConvolution.__init__": [[34, 41], ["torch.nn.modules.module.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "in_features", ",", "out_features", ",", "dropout", ",", "act", ",", "use_bias", ")", ":", "\n", "        ", "super", "(", "GraphConvolution", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "use_bias", ")", "\n", "self", ".", "act", "=", "act", "\n", "self", ".", "in_features", "=", "in_features", "\n", "self", ".", "out_features", "=", "out_features", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.layers.GraphConvolution.forward": [[42, 52], ["layers.GraphConvolution.linear.forward", "torch.dropout", "torch.dropout", "torch.dropout", "torch.spmm", "torch.spmm", "torch.spmm", "torch.spmm", "torch.spmm", "torch.spmm", "torch.spmm", "torch.spmm", "torch.spmm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "layers.GraphConvolution.act"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.models.gnn_model.Relational_GNN.forward"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "x", ",", "adj", "=", "input", "\n", "hidden", "=", "self", ".", "linear", ".", "forward", "(", "x", ")", "\n", "hidden", "=", "F", ".", "dropout", "(", "hidden", ",", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "if", "adj", ".", "is_sparse", ":", "\n", "            ", "support", "=", "torch", ".", "spmm", "(", "adj", ",", "hidden", ")", "\n", "", "else", ":", "\n", "            ", "support", "=", "torch", ".", "mm", "(", "adj", ",", "hidden", ")", "\n", "", "output", "=", "self", ".", "act", "(", "support", ")", ",", "adj", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.layers.GraphConvolution.extra_repr": [[53, 56], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "'input_dim={}, output_dim={}'", ".", "format", "(", "\n", "self", ".", "in_features", ",", "self", ".", "out_features", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.layers.Linear.__init__": [[64, 69], ["torch.nn.modules.module.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "in_features", ",", "out_features", ",", "dropout", ",", "act", ",", "use_bias", ")", ":", "\n", "        ", "super", "(", "Linear", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "use_bias", ")", "\n", "self", ".", "act", "=", "act", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.layers.Linear.forward": [[70, 75], ["layers.Linear.linear.forward", "torch.dropout", "torch.dropout", "torch.dropout", "layers.Linear.act", "x.to"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.models.gnn_model.Relational_GNN.forward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "hidden", "=", "self", ".", "linear", ".", "forward", "(", "x", ".", "to", "(", "device", ")", ")", "\n", "hidden", "=", "F", ".", "dropout", "(", "hidden", ",", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "out", "=", "self", ".", "act", "(", "hidden", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.layers.FermiDiracDecoder.__init__": [[80, 84], ["torch.nn.modules.module.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "r", ",", "t", ")", ":", "\n", "        ", "super", "(", "FermiDiracDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "r", "=", "r", "\n", "self", ".", "t", "=", "t", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.layers.FermiDiracDecoder.forward": [[85, 88], ["torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "dist", ")", ":", "\n", "        ", "probs", "=", "1.", "/", "(", "torch", ".", "exp", "(", "(", "dist", "-", "self", ".", "r", ")", "/", "self", ".", "t", ")", "+", "1.0", ")", "\n", "return", "probs", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.layers.get_dim_act": [[11, 27], ["getattr"], "function", ["None"], ["def", "get_dim_act", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n    Helper function to get dimension and activation at every layer.\n    :param args:\n    :return:\n    \"\"\"", "\n", "if", "not", "args", ".", "act", ":", "\n", "        ", "act", "=", "lambda", "x", ":", "x", "\n", "", "else", ":", "\n", "        ", "act", "=", "getattr", "(", "F", ",", "args", ".", "act", ")", "\n", "", "acts", "=", "[", "act", "]", "*", "(", "args", ".", "num_layers", "-", "1", ")", "\n", "dims", "=", "[", "args", ".", "feat_dim", "]", "+", "(", "[", "args", ".", "dim", "]", "*", "(", "args", ".", "num_layers", "-", "1", ")", ")", "\n", "if", "args", ".", "task", "in", "[", "'lp'", ",", "'rec'", "]", ":", "\n", "        ", "dims", "+=", "[", "args", ".", "dim", "]", "\n", "acts", "+=", "[", "act", "]", "\n", "", "return", "dims", ",", "acts", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.hyp_layers.HNNLayer.__init__": [[47, 51], ["torch.Module.__init__", "hyp_layers.HypLinear", "hyp_layers.HypAct"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "manifold", ",", "in_features", ",", "out_features", ",", "c", ",", "dropout", ",", "act", ",", "use_bias", ")", ":", "\n", "        ", "super", "(", "HNNLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear", "=", "HypLinear", "(", "manifold", ",", "in_features", ",", "out_features", ",", "c", ",", "dropout", ",", "use_bias", ")", "\n", "self", ".", "hyp_act", "=", "HypAct", "(", "manifold", ",", "c", ",", "c", ",", "act", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.hyp_layers.HNNLayer.forward": [[52, 56], ["hyp_layers.HNNLayer.linear.forward", "hyp_layers.HNNLayer.hyp_act.forward"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.models.gnn_model.Relational_GNN.forward", "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.gnn_model.Relational_GNN.forward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "h", "=", "self", ".", "linear", ".", "forward", "(", "x", ")", "\n", "h", "=", "self", ".", "hyp_act", ".", "forward", "(", "h", ")", "\n", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.hyp_layers.HyperbolicGraphConvolution.__init__": [[63, 68], ["torch.Module.__init__", "hyp_layers.HypLinear", "hyp_layers.HypAgg", "hyp_layers.HypAct"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "manifold", ",", "in_features", ",", "out_features", ",", "c_in", ",", "c_out", ",", "dropout", ",", "act", ",", "use_bias", ",", "use_att", ")", ":", "\n", "        ", "super", "(", "HyperbolicGraphConvolution", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear", "=", "HypLinear", "(", "manifold", ",", "in_features", ",", "out_features", ",", "c_in", ",", "dropout", ",", "use_bias", ")", "\n", "self", ".", "agg", "=", "HypAgg", "(", "manifold", ",", "c_in", ",", "out_features", ",", "dropout", ",", "use_att", ")", "\n", "self", ".", "hyp_act", "=", "HypAct", "(", "manifold", ",", "c_in", ",", "c_out", ",", "act", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.hyp_layers.HyperbolicGraphConvolution.forward": [[69, 76], ["hyp_layers.HyperbolicGraphConvolution.linear.forward", "hyp_layers.HyperbolicGraphConvolution.agg.forward", "hyp_layers.HyperbolicGraphConvolution.hyp_act.forward"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.models.gnn_model.Relational_GNN.forward", "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.gnn_model.Relational_GNN.forward", "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.gnn_model.Relational_GNN.forward"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "x", ",", "adj", "=", "input", "\n", "h", "=", "self", ".", "linear", ".", "forward", "(", "x", ")", "\n", "h", "=", "self", ".", "agg", ".", "forward", "(", "h", ",", "adj", ")", "\n", "h", "=", "self", ".", "hyp_act", ".", "forward", "(", "h", ")", "\n", "output", "=", "h", ",", "adj", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.hyp_layers.HypLinear.__init__": [[83, 94], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "hyp_layers.HypLinear.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__", "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.hyp_layers.HypLinear.reset_parameters"], ["def", "__init__", "(", "self", ",", "manifold", ",", "in_features", ",", "out_features", ",", "c", ",", "dropout", ",", "use_bias", ")", ":", "\n", "        ", "super", "(", "HypLinear", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "manifold", "=", "manifold", "\n", "self", ".", "in_features", "=", "in_features", "\n", "self", ".", "out_features", "=", "out_features", "\n", "self", ".", "c", "=", "c", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "use_bias", "=", "use_bias", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "out_features", ")", ")", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "out_features", ",", "in_features", ")", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.hyp_layers.HypLinear.reset_parameters": [[95, 98], ["torch.xavier_uniform_", "torch.xavier_uniform_", "torch.xavier_uniform_", "torch.xavier_uniform_", "torch.constant_", "torch.constant_", "torch.constant_", "torch.constant_", "math.sqrt"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "init", ".", "xavier_uniform_", "(", "self", ".", "weight", ",", "gain", "=", "math", ".", "sqrt", "(", "2", ")", ")", "\n", "init", ".", "constant_", "(", "self", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.hyp_layers.HypLinear.forward": [[99, 110], ["torch.dropout", "torch.dropout", "torch.dropout", "torch.dropout", "hyp_layers.HypLinear.manifold.mobius_matvec", "hyp_layers.HypLinear.manifold.proj", "hyp_layers.HypLinear.manifold.proj_tan0", "hyp_layers.HypLinear.manifold.expmap0", "hyp_layers.HypLinear.manifold.proj", "hyp_layers.HypLinear.manifold.mobius_add", "hyp_layers.HypLinear.manifold.proj", "hyp_layers.HypLinear.bias.view"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.mobius_matvec", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj_tan0", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.expmap0", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.mobius_add", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "drop_weight", "=", "F", ".", "dropout", "(", "self", ".", "weight", ",", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "mv", "=", "self", ".", "manifold", ".", "mobius_matvec", "(", "drop_weight", ",", "x", ",", "self", ".", "c", ")", "\n", "res", "=", "self", ".", "manifold", ".", "proj", "(", "mv", ",", "self", ".", "c", ")", "\n", "if", "self", ".", "use_bias", ":", "\n", "            ", "bias", "=", "self", ".", "manifold", ".", "proj_tan0", "(", "self", ".", "bias", ".", "view", "(", "1", ",", "-", "1", ")", ",", "self", ".", "c", ")", "\n", "hyp_bias", "=", "self", ".", "manifold", ".", "expmap0", "(", "bias", ",", "self", ".", "c", ")", "\n", "hyp_bias", "=", "self", ".", "manifold", ".", "proj", "(", "hyp_bias", ",", "self", ".", "c", ")", "\n", "res", "=", "self", ".", "manifold", ".", "mobius_add", "(", "res", ",", "hyp_bias", ",", "c", "=", "self", ".", "c", ")", "\n", "res", "=", "self", ".", "manifold", ".", "proj", "(", "res", ",", "self", ".", "c", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.hyp_layers.HypLinear.extra_repr": [[111, 114], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "'in_features={}, out_features={}, c={}'", ".", "format", "(", "\n", "self", ".", "in_features", ",", "self", ".", "out_features", ",", "self", ".", "c", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.hyp_layers.HypAgg.__init__": [[122, 132], ["torch.nn.modules.module.Module.__init__", "layers.att_layers.DenseAtt"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "manifold", ",", "c", ",", "in_features", ",", "dropout", ",", "use_att", ")", ":", "\n", "        ", "super", "(", "HypAgg", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "manifold", "=", "manifold", "\n", "self", ".", "c", "=", "c", "\n", "\n", "self", ".", "in_features", "=", "in_features", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "use_att", "=", "use_att", "\n", "if", "self", ".", "use_att", ":", "\n", "            ", "self", ".", "att", "=", "DenseAtt", "(", "in_features", ",", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.hyp_layers.HypAgg.forward": [[133, 142], ["hyp_layers.HypAgg.manifold.logmap0", "hyp_layers.HypAgg.manifold.proj", "hyp_layers.HypAgg.att", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.spmm", "torch.spmm", "torch.spmm", "torch.spmm", "torch.spmm", "torch.spmm", "torch.spmm", "torch.spmm", "torch.spmm", "torch.spmm", "torch.spmm", "torch.spmm", "torch.spmm", "torch.spmm", "torch.spmm", "torch.spmm", "hyp_layers.HypAgg.manifold.expmap0", "adj.float"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.logmap0", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.expmap0"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "adj", ")", ":", "\n", "        ", "x_tangent", "=", "self", ".", "manifold", ".", "logmap0", "(", "x", ",", "c", "=", "self", ".", "c", ")", "\n", "if", "self", ".", "use_att", ":", "\n", "            ", "adj_att", "=", "self", ".", "att", "(", "x_tangent", ",", "adj", ")", "\n", "support_t", "=", "torch", ".", "matmul", "(", "adj_att", ",", "x_tangent", ")", "\n", "", "else", ":", "\n", "            ", "support_t", "=", "torch", ".", "spmm", "(", "adj", ".", "float", "(", ")", ",", "x_tangent", ")", "\n", "", "output", "=", "self", ".", "manifold", ".", "proj", "(", "self", ".", "manifold", ".", "expmap0", "(", "support_t", ",", "c", "=", "self", ".", "c", ")", ",", "c", "=", "self", ".", "c", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.hyp_layers.HypAgg.extra_repr": [[143, 145], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "'c={}'", ".", "format", "(", "self", ".", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.hyp_layers.HypAct.__init__": [[152, 158], ["torch.nn.modules.module.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "manifold", ",", "c_in", ",", "c_out", ",", "act", ")", ":", "\n", "        ", "super", "(", "HypAct", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "manifold", "=", "manifold", "\n", "self", ".", "c_in", "=", "c_in", "\n", "self", ".", "c_out", "=", "c_out", "\n", "self", ".", "act", "=", "act", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.hyp_layers.HypAct.forward": [[159, 163], ["hyp_layers.HypAct.act", "hyp_layers.HypAct.manifold.proj_tan0", "hyp_layers.HypAct.manifold.proj", "hyp_layers.HypAct.manifold.logmap0", "hyp_layers.HypAct.manifold.expmap0"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj_tan0", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.logmap0", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.expmap0"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "xt", "=", "self", ".", "act", "(", "self", ".", "manifold", ".", "logmap0", "(", "x", ",", "c", "=", "self", ".", "c_in", ")", ")", "\n", "xt", "=", "self", ".", "manifold", ".", "proj_tan0", "(", "xt", ",", "c", "=", "self", ".", "c_out", ")", "\n", "return", "self", ".", "manifold", ".", "proj", "(", "self", ".", "manifold", ".", "expmap0", "(", "xt", ",", "c", "=", "self", ".", "c_out", ")", ",", "c", "=", "self", ".", "c_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.hyp_layers.HypAct.extra_repr": [[164, 167], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "'c_in={}, c_out={}'", ".", "format", "(", "\n", "self", ".", "c_in", ",", "self", ".", "c_out", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.hyp_layers.get_dim_act_curv": [[13, 40], ["getattr", "torch.Parameter", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "range", "range", "curv.to"], "function", ["None"], ["def", "get_dim_act_curv", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n    Helper function to get dimension and activation at every layer.\n    :param args:\n    :return:\n    \"\"\"", "\n", "if", "not", "args", ".", "act", ":", "\n", "        ", "act", "=", "lambda", "x", ":", "x", "\n", "", "else", ":", "\n", "        ", "act", "=", "getattr", "(", "F", ",", "args", ".", "act", ")", "\n", "", "acts", "=", "[", "act", "]", "*", "(", "args", ".", "num_layers", "-", "1", ")", "\n", "dims", "=", "[", "args", ".", "feat_dim", "]", "+", "(", "[", "args", ".", "dim", "]", "*", "(", "args", ".", "num_layers", "-", "1", ")", ")", "\n", "if", "args", ".", "task", "in", "[", "'lp'", ",", "'rec'", "]", ":", "\n", "        ", "dims", "+=", "[", "args", ".", "dim", "]", "\n", "acts", "+=", "[", "act", "]", "\n", "n_curvatures", "=", "args", ".", "num_layers", "\n", "", "else", ":", "\n", "        ", "n_curvatures", "=", "args", ".", "num_layers", "-", "1", "\n", "", "if", "args", ".", "c", "is", "None", ":", "\n", "# create list of trainable curvature parameters", "\n", "        ", "curvatures", "=", "[", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "[", "1.", "]", ")", ")", "for", "_", "in", "range", "(", "n_curvatures", ")", "]", "\n", "", "else", ":", "\n", "# fixed curvature", "\n", "        ", "curvatures", "=", "[", "torch", ".", "tensor", "(", "[", "args", ".", "c", "]", ")", "for", "_", "in", "range", "(", "n_curvatures", ")", "]", "\n", "if", "not", "args", ".", "cuda", "==", "-", "1", ":", "\n", "            ", "curvatures", "=", "[", "curv", ".", "to", "(", "args", ".", "device", ")", "for", "curv", "in", "curvatures", "]", "\n", "", "", "return", "dims", ",", "acts", ",", "curvatures", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.optimizers.radam.OptimMixin.__init__": [[10, 13], ["object.__init__"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "stabilize", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "_stabilize", "=", "stabilize", "\n", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.optimizers.radam.OptimMixin.stabilize_group": [[14, 16], ["None"], "methods", ["None"], ["", "def", "stabilize_group", "(", "self", ",", "group", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.optimizers.radam.OptimMixin.stabilize": [[17, 22], ["radam.OptimMixin.stabilize_group"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.optimizers.radam.RiemannianAdam.stabilize_group"], ["", "def", "stabilize", "(", "self", ")", ":", "\n", "        ", "\"\"\"Stabilize parameters if they are off-manifold due to numerical reasons\n        \"\"\"", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "self", ".", "stabilize_group", "(", "group", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.optimizers.radam.RiemannianAdam.step": [[75, 161], ["closure", "torch.no_grad", "isinstance", "manifold.egrad2rgrad.add_", "manifold.egrad2rgrad", "exp_avg.mul_().add_", "exp_avg_sq.mul_().add_", "manifold.proj", "manifold.ptransp", "radam.copy_or_set_", "exp_avg.set_", "radam.RiemannianAdam.stabilize_group", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "manifold.inner", "torch.max", "max_exp_avg_sq.sqrt().add_", "exp_avg_sq.sqrt().add_", "manifold.expmap", "torch.zeros_like", "exp_avg.mul_", "exp_avg_sq.mul_", "max_exp_avg_sq.sqrt", "exp_avg_sq.sqrt"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.egrad2rgrad", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.ptransp", "home.repos.pwc.inspect_result.shaanchandra_SAFER.optimizers.radam.copy_or_set_", "home.repos.pwc.inspect_result.shaanchandra_SAFER.optimizers.radam.RiemannianAdam.stabilize_group", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.inner", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.expmap"], ["def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n        Arguments\n        ---------\n        closure : callable (optional)\n            A closure that reevaluates the model\n            and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "                ", "if", "\"step\"", "not", "in", "group", ":", "\n", "                    ", "group", "[", "\"step\"", "]", "=", "0", "\n", "", "betas", "=", "group", "[", "\"betas\"", "]", "\n", "weight_decay", "=", "group", "[", "\"weight_decay\"", "]", "\n", "eps", "=", "group", "[", "\"eps\"", "]", "\n", "learning_rate", "=", "group", "[", "\"lr\"", "]", "\n", "amsgrad", "=", "group", "[", "\"amsgrad\"", "]", "\n", "for", "point", "in", "group", "[", "\"params\"", "]", ":", "\n", "                    ", "grad", "=", "point", ".", "grad", "\n", "if", "grad", "is", "None", ":", "\n", "                        ", "continue", "\n", "", "if", "isinstance", "(", "point", ",", "(", "ManifoldParameter", ")", ")", ":", "\n", "                        ", "manifold", "=", "point", ".", "manifold", "\n", "c", "=", "point", ".", "c", "\n", "", "else", ":", "\n", "                        ", "manifold", "=", "_default_manifold", "\n", "c", "=", "None", "\n", "", "if", "grad", ".", "is_sparse", ":", "\n", "                        ", "raise", "RuntimeError", "(", "\n", "\"Riemannian Adam does not support sparse gradients yet (PR is welcome)\"", "\n", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "point", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                        ", "state", "[", "\"step\"", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "\"exp_avg\"", "]", "=", "torch", ".", "zeros_like", "(", "point", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "\"exp_avg_sq\"", "]", "=", "torch", ".", "zeros_like", "(", "point", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                            ", "state", "[", "\"max_exp_avg_sq\"", "]", "=", "torch", ".", "zeros_like", "(", "point", ")", "\n", "# make local variables for easy access", "\n", "", "", "exp_avg", "=", "state", "[", "\"exp_avg\"", "]", "\n", "exp_avg_sq", "=", "state", "[", "\"exp_avg_sq\"", "]", "\n", "# actual step", "\n", "grad", ".", "add_", "(", "weight_decay", ",", "point", ")", "\n", "grad", "=", "manifold", ".", "egrad2rgrad", "(", "point", ",", "grad", ",", "c", ")", "\n", "exp_avg", ".", "mul_", "(", "betas", "[", "0", "]", ")", ".", "add_", "(", "1", "-", "betas", "[", "0", "]", ",", "grad", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "betas", "[", "1", "]", ")", ".", "add_", "(", "\n", "1", "-", "betas", "[", "1", "]", ",", "manifold", ".", "inner", "(", "point", ",", "c", ",", "grad", ",", "keepdim", "=", "True", ")", "\n", ")", "\n", "if", "amsgrad", ":", "\n", "                        ", "max_exp_avg_sq", "=", "state", "[", "\"max_exp_avg_sq\"", "]", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "torch", ".", "max", "(", "max_exp_avg_sq", ",", "exp_avg_sq", ",", "out", "=", "max_exp_avg_sq", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", ")", "\n", "", "else", ":", "\n", "                        ", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", ")", "\n", "", "group", "[", "\"step\"", "]", "+=", "1", "\n", "bias_correction1", "=", "1", "-", "betas", "[", "0", "]", "**", "group", "[", "\"step\"", "]", "\n", "bias_correction2", "=", "1", "-", "betas", "[", "1", "]", "**", "group", "[", "\"step\"", "]", "\n", "step_size", "=", "(", "\n", "learning_rate", "*", "bias_correction2", "**", "0.5", "/", "bias_correction1", "\n", ")", "\n", "\n", "# copy the state, we need it for retraction", "\n", "# get the direction for ascend", "\n", "direction", "=", "exp_avg", "/", "denom", "\n", "# transport the exponential averaging to the new point", "\n", "new_point", "=", "manifold", ".", "proj", "(", "manifold", ".", "expmap", "(", "-", "step_size", "*", "direction", ",", "point", ",", "c", ")", ",", "c", ")", "\n", "exp_avg_new", "=", "manifold", ".", "ptransp", "(", "point", ",", "new_point", ",", "exp_avg", ",", "c", ")", "\n", "# use copy only for user facing point", "\n", "copy_or_set_", "(", "point", ",", "new_point", ")", "\n", "exp_avg", ".", "set_", "(", "exp_avg_new", ")", "\n", "\n", "group", "[", "\"step\"", "]", "+=", "1", "\n", "", "if", "self", ".", "_stabilize", "is", "not", "None", "and", "group", "[", "\"step\"", "]", "%", "self", ".", "_stabilize", "==", "0", ":", "\n", "                    ", "self", ".", "stabilize_group", "(", "group", ")", "\n", "", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.optimizers.radam.RiemannianAdam.stabilize_group": [[162, 175], ["torch.no_grad", "radam.copy_or_set_", "exp_avg.set_", "isinstance", "manifold.proj", "manifold.proj_tan"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.optimizers.radam.copy_or_set_", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj_tan"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "stabilize_group", "(", "self", ",", "group", ")", ":", "\n", "        ", "for", "p", "in", "group", "[", "\"params\"", "]", ":", "\n", "            ", "if", "not", "isinstance", "(", "p", ",", "ManifoldParameter", ")", ":", "\n", "                ", "continue", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "if", "not", "state", ":", "# due to None grads", "\n", "                ", "continue", "\n", "", "manifold", "=", "p", ".", "manifold", "\n", "c", "=", "p", ".", "c", "\n", "exp_avg", "=", "state", "[", "\"exp_avg\"", "]", "\n", "copy_or_set_", "(", "p", ",", "manifold", ".", "proj", "(", "p", ",", "c", ")", ")", "\n", "exp_avg", ".", "set_", "(", "manifold", ".", "proj_tan", "(", "exp_avg", ",", "u", ",", "c", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.optimizers.radam.copy_or_set_": [[24, 43], ["dest.stride", "source.stride", "dest.copy_", "dest.set_"], "function", ["None"], ["", "", "", "def", "copy_or_set_", "(", "dest", ",", "source", ")", ":", "\n", "    ", "\"\"\"\n    A workaround to respect strides of :code:`dest` when copying :code:`source`\n    (https://github.com/geoopt/geoopt/issues/70)\n    Parameters\n    ----------\n    dest : torch.Tensor\n        Destination tensor where to store new data\n    source : torch.Tensor\n        Source data to put in the new tensor\n    Returns\n    -------\n    dest\n        torch.Tensor, modified inplace\n    \"\"\"", "\n", "if", "dest", ".", "stride", "(", ")", "!=", "source", ".", "stride", "(", ")", ":", "\n", "        ", "return", "dest", ".", "copy_", "(", "source", ")", "\n", "", "else", ":", "\n", "        ", "return", "dest", ".", "set_", "(", "source", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.base_models.BaseModel.__init__": [[22, 36], ["torch.Module.__init__", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.Parameter", "torch.Parameter", "torch.Parameter", "getattr", "getattr", "base_models.BaseModel.c.to", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "BaseModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "manifold_name", "=", "args", ".", "manifold", "\n", "if", "args", ".", "c", "is", "not", "None", ":", "\n", "            ", "self", ".", "c", "=", "torch", ".", "tensor", "(", "[", "args", ".", "c", "]", ")", "\n", "if", "not", "args", ".", "cuda", "==", "-", "1", ":", "\n", "                ", "self", ".", "c", "=", "self", ".", "c", ".", "to", "(", "args", ".", "device", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "c", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "[", "1.", "]", ")", ")", "\n", "", "self", ".", "manifold", "=", "getattr", "(", "manifolds", ",", "self", ".", "manifold_name", ")", "(", ")", "\n", "if", "self", ".", "manifold", ".", "name", "==", "'Hyperboloid'", ":", "\n", "            ", "args", ".", "feat_dim", "=", "args", ".", "feat_dim", "+", "1", "\n", "", "self", ".", "nnodes", "=", "args", ".", "n_nodes", "\n", "self", ".", "encoder", "=", "getattr", "(", "encoders", ",", "args", ".", "model", ")", "(", "self", ".", "c", ",", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.base_models.BaseModel.encode": [[37, 43], ["base_models.BaseModel.encoder.encode", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.models.encoders.Shallow.encode"], ["", "def", "encode", "(", "self", ",", "x", ",", "adj", ")", ":", "\n", "        ", "if", "self", ".", "manifold", ".", "name", "==", "'Hyperboloid'", ":", "\n", "            ", "o", "=", "torch", ".", "zeros_like", "(", "x", ")", "\n", "x", "=", "torch", ".", "cat", "(", "[", "o", "[", ":", ",", "0", ":", "1", "]", ",", "x", "]", ",", "dim", "=", "1", ")", "\n", "", "h", "=", "self", ".", "encoder", ".", "encode", "(", "x", ",", "adj", ")", "\n", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.base_models.BaseModel.compute_metrics": [[44, 46], ["None"], "methods", ["None"], ["", "def", "compute_metrics", "(", "self", ",", "embeddings", ",", "data", ",", "split", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.base_models.BaseModel.init_metric_dict": [[47, 49], ["None"], "methods", ["None"], ["", "def", "init_metric_dict", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.base_models.BaseModel.has_improved": [[50, 52], ["None"], "methods", ["None"], ["", "def", "has_improved", "(", "self", ",", "m1", ",", "m2", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.base_models.NCModel.__init__": [[59, 72], ["base_models.BaseModel.__init__", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "base_models.NCModel.weights.to", "[].mean"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "NCModel", ",", "self", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "decoder", "=", "model2decoder", "[", "args", ".", "model", "]", "(", "self", ".", "c", ",", "args", ")", "\n", "if", "args", ".", "n_classes", ">", "2", ":", "\n", "            ", "self", ".", "f1_average", "=", "'micro'", "\n", "", "else", ":", "\n", "            ", "self", ".", "f1_average", "=", "'binary'", "\n", "", "if", "args", ".", "pos_weight", ":", "\n", "            ", "self", ".", "weights", "=", "torch", ".", "Tensor", "(", "[", "1.", ",", "1.", "/", "data", "[", "'labels'", "]", "[", "idx_train", "]", ".", "mean", "(", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "weights", "=", "torch", ".", "Tensor", "(", "[", "1.", "]", "*", "args", ".", "n_classes", ")", "\n", "", "if", "not", "args", ".", "cuda", "==", "-", "1", ":", "\n", "            ", "self", ".", "weights", "=", "self", ".", "weights", ".", "to", "(", "args", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.base_models.NCModel.decode": [[73, 77], ["base_models.NCModel.decoder.decode"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.models.decoders.LinearDecoder.decode"], ["", "", "def", "decode", "(", "self", ",", "h", ",", "adj", ",", "idx", "=", "None", ")", ":", "\n", "        ", "output", "=", "self", ".", "decoder", ".", "decode", "(", "h", ",", "adj", ")", "\n", "# return F.log_softmax(output[idx], dim=1)", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.base_models.NCModel.compute_metrics": [[78, 85], ["base_models.NCModel.decode", "torch.nll_loss", "torch.nll_loss", "torch.nll_loss", "utils.eval_utils.acc_f1"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.models.decoders.LinearDecoder.decode", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.eval_utils.acc_f1"], ["", "def", "compute_metrics", "(", "self", ",", "embeddings", ",", "data", ",", "split", ")", ":", "\n", "        ", "idx", "=", "data", "[", "f'idx_{split}'", "]", "\n", "output", "=", "self", ".", "decode", "(", "embeddings", ",", "data", "[", "'adj_train_norm'", "]", ",", "idx", ")", "\n", "loss", "=", "F", ".", "nll_loss", "(", "output", ",", "data", "[", "'labels'", "]", "[", "idx", "]", ",", "self", ".", "weights", ")", "\n", "acc", ",", "f1", "=", "acc_f1", "(", "output", ",", "data", "[", "'labels'", "]", "[", "idx", "]", ",", "average", "=", "self", ".", "f1_average", ")", "\n", "metrics", "=", "{", "'loss'", ":", "loss", ",", "'acc'", ":", "acc", ",", "'f1'", ":", "f1", "}", "\n", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.base_models.NCModel.init_metric_dict": [[86, 88], ["None"], "methods", ["None"], ["", "def", "init_metric_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "'acc'", ":", "-", "1", ",", "'f1'", ":", "-", "1", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.base_models.NCModel.has_improved": [[89, 91], ["None"], "methods", ["None"], ["", "def", "has_improved", "(", "self", ",", "m1", ",", "m2", ")", ":", "\n", "        ", "return", "m1", "[", "\"f1\"", "]", "<", "m2", "[", "\"f1\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.base_models.LPModel.__init__": [[98, 103], ["base_models.BaseModel.__init__", "layers.layers.FermiDiracDecoder"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "LPModel", ",", "self", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "dc", "=", "FermiDiracDecoder", "(", "r", "=", "args", ".", "r", ",", "t", "=", "args", ".", "t", ")", "\n", "self", ".", "nb_false_edges", "=", "args", ".", "nb_false_edges", "\n", "self", ".", "nb_edges", "=", "args", ".", "nb_edges", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.base_models.LPModel.decode": [[104, 112], ["base_models.LPModel.manifold.sqdist", "base_models.LPModel.dc.forward", "base_models.LPModel.manifold.normalize"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.sqdist", "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.gnn_model.Relational_GNN.forward", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.euclidean.Euclidean.normalize"], ["", "def", "decode", "(", "self", ",", "h", ",", "idx", ")", ":", "\n", "        ", "if", "self", ".", "manifold_name", "==", "'Euclidean'", ":", "\n", "            ", "h", "=", "self", ".", "manifold", ".", "normalize", "(", "h", ")", "\n", "", "emb_in", "=", "h", "[", "idx", "[", ":", ",", "0", "]", ",", ":", "]", "\n", "emb_out", "=", "h", "[", "idx", "[", ":", ",", "1", "]", ",", ":", "]", "\n", "sqdist", "=", "self", ".", "manifold", ".", "sqdist", "(", "emb_in", ",", "emb_out", ",", "self", ".", "c", ")", "\n", "probs", "=", "self", ".", "dc", ".", "forward", "(", "sqdist", ")", "\n", "return", "probs", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.base_models.LPModel.compute_metrics": [[113, 131], ["base_models.LPModel.decode", "base_models.LPModel.decode", "torch.binary_cross_entropy", "torch.binary_cross_entropy", "torch.binary_cross_entropy", "torch.binary_cross_entropy", "torch.binary_cross_entropy", "torch.binary_cross_entropy", "sklearn.metrics.roc_auc_score", "sklearn.metrics.average_precision_score", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "pos_scores.cpu.cpu.cpu", "neg_scores.cpu.cpu.cpu", "list", "list", "pos_scores.cpu.cpu.data.numpy", "neg_scores.cpu.cpu.data.numpy", "numpy.random.randint"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.models.decoders.LinearDecoder.decode", "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.decoders.LinearDecoder.decode"], ["", "def", "compute_metrics", "(", "self", ",", "embeddings", ",", "data", ",", "split", ")", ":", "\n", "        ", "if", "split", "==", "'train'", ":", "\n", "            ", "edges_false", "=", "data", "[", "f'{split}_edges_false'", "]", "[", "np", ".", "random", ".", "randint", "(", "0", ",", "self", ".", "nb_false_edges", ",", "self", ".", "nb_edges", ")", "]", "\n", "", "else", ":", "\n", "            ", "edges_false", "=", "data", "[", "f'{split}_edges_false'", "]", "\n", "", "pos_scores", "=", "self", ".", "decode", "(", "embeddings", ",", "data", "[", "f'{split}_edges'", "]", ")", "\n", "neg_scores", "=", "self", ".", "decode", "(", "embeddings", ",", "edges_false", ")", "\n", "loss", "=", "F", ".", "binary_cross_entropy", "(", "pos_scores", ",", "torch", ".", "ones_like", "(", "pos_scores", ")", ")", "\n", "loss", "+=", "F", ".", "binary_cross_entropy", "(", "neg_scores", ",", "torch", ".", "zeros_like", "(", "neg_scores", ")", ")", "\n", "if", "pos_scores", ".", "is_cuda", ":", "\n", "            ", "pos_scores", "=", "pos_scores", ".", "cpu", "(", ")", "\n", "neg_scores", "=", "neg_scores", ".", "cpu", "(", ")", "\n", "", "labels", "=", "[", "1", "]", "*", "pos_scores", ".", "shape", "[", "0", "]", "+", "[", "0", "]", "*", "neg_scores", ".", "shape", "[", "0", "]", "\n", "preds", "=", "list", "(", "pos_scores", ".", "data", ".", "numpy", "(", ")", ")", "+", "list", "(", "neg_scores", ".", "data", ".", "numpy", "(", ")", ")", "\n", "roc", "=", "roc_auc_score", "(", "labels", ",", "preds", ")", "\n", "ap", "=", "average_precision_score", "(", "labels", ",", "preds", ")", "\n", "metrics", "=", "{", "'loss'", ":", "loss", ",", "'roc'", ":", "roc", ",", "'ap'", ":", "ap", "}", "\n", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.base_models.LPModel.init_metric_dict": [[132, 134], ["None"], "methods", ["None"], ["", "def", "init_metric_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "'roc'", ":", "-", "1", ",", "'ap'", ":", "-", "1", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.base_models.LPModel.has_improved": [[135, 137], ["None"], "methods", ["None"], ["", "def", "has_improved", "(", "self", ",", "m1", ",", "m2", ")", ":", "\n", "        ", "return", "0.5", "*", "(", "m1", "[", "'roc'", "]", "+", "m1", "[", "'ap'", "]", ")", "<", "0.5", "*", "(", "m2", "[", "'roc'", "]", "+", "m2", "[", "'ap'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.encoders.Encoder.__init__": [[20, 23], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "c", ")", ":", "\n", "        ", "super", "(", "Encoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "c", "=", "c", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.encoders.Encoder.encode": [[24, 31], ["encoders.Encoder.layers.forward", "encoders.Encoder.layers.forward"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.models.gnn_model.Relational_GNN.forward", "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.gnn_model.Relational_GNN.forward"], ["", "def", "encode", "(", "self", ",", "x", ",", "adj", ")", ":", "\n", "        ", "if", "self", ".", "encode_graph", ":", "\n", "            ", "input", "=", "(", "x", ",", "adj", ")", "\n", "output", ",", "_", "=", "self", ".", "layers", ".", "forward", "(", "input", ")", "\n", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "layers", ".", "forward", "(", "x", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.encoders.MLP.__init__": [[37, 48], ["encoders.Encoder.__init__", "layers.layers.get_dim_act", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "layers.append", "len", "layers.layers.Linear"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__", "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.layers.get_dim_act"], ["def", "__init__", "(", "self", ",", "c", ",", "args", ")", ":", "\n", "        ", "super", "(", "MLP", ",", "self", ")", ".", "__init__", "(", "c", ")", "\n", "assert", "args", ".", "num_layers", ">", "0", "\n", "dims", ",", "acts", "=", "get_dim_act", "(", "args", ")", "\n", "layers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "dims", ")", "-", "1", ")", ":", "\n", "            ", "in_dim", ",", "out_dim", "=", "dims", "[", "i", "]", ",", "dims", "[", "i", "+", "1", "]", "\n", "act", "=", "acts", "[", "i", "]", "\n", "layers", ".", "append", "(", "Linear", "(", "in_dim", ",", "out_dim", ",", "args", ".", "dropout", ",", "act", ",", "args", ".", "bias", ")", ")", "\n", "", "self", ".", "layers", "=", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "self", ".", "encode_graph", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.encoders.HNN.__init__": [[55, 70], ["encoders.Encoder.__init__", "layers.get_dim_act_curv", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "getattr", "hnn_layers.append", "len", "layers.HNNLayer"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__", "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.hyp_layers.get_dim_act_curv"], ["def", "__init__", "(", "self", ",", "c", ",", "args", ")", ":", "\n", "        ", "super", "(", "HNN", ",", "self", ")", ".", "__init__", "(", "c", ")", "\n", "self", ".", "manifold", "=", "getattr", "(", "manifolds", ",", "args", ".", "manifold", ")", "(", ")", "\n", "assert", "args", ".", "num_layers", ">", "1", "\n", "dims", ",", "acts", ",", "_", "=", "hyp_layers", ".", "get_dim_act_curv", "(", "args", ")", "\n", "hnn_layers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "dims", ")", "-", "1", ")", ":", "\n", "            ", "in_dim", ",", "out_dim", "=", "dims", "[", "i", "]", ",", "dims", "[", "i", "+", "1", "]", "\n", "act", "=", "acts", "[", "i", "]", "\n", "hnn_layers", ".", "append", "(", "\n", "hyp_layers", ".", "HNNLayer", "(", "\n", "self", ".", "manifold", ",", "in_dim", ",", "out_dim", ",", "self", ".", "c", ",", "args", ".", "dropout", ",", "act", ",", "args", ".", "bias", ")", "\n", ")", "\n", "", "self", ".", "layers", "=", "nn", ".", "Sequential", "(", "*", "hnn_layers", ")", "\n", "self", ".", "encode_graph", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.encoders.HNN.encode": [[71, 74], ["encoders.HNN.manifold.proj", "encoders.Encoder.encode", "encoders.HNN.manifold.expmap0", "encoders.HNN.manifold.proj_tan0"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj", "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.encoders.Shallow.encode", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.expmap0", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj_tan0"], ["", "def", "encode", "(", "self", ",", "x", ",", "adj", ")", ":", "\n", "        ", "x_hyp", "=", "self", ".", "manifold", ".", "proj", "(", "self", ".", "manifold", ".", "expmap0", "(", "self", ".", "manifold", ".", "proj_tan0", "(", "x", ",", "self", ".", "c", ")", ",", "c", "=", "self", ".", "c", ")", ",", "c", "=", "self", ".", "c", ")", "\n", "return", "super", "(", "HNN", ",", "self", ")", ".", "encode", "(", "x_hyp", ",", "adj", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.encoders.GCN.__init__": [[80, 91], ["encoders.Encoder.__init__", "layers.layers.get_dim_act", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "gc_layers.append", "len", "layers.layers.GraphConvolution"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__", "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.layers.get_dim_act"], ["def", "__init__", "(", "self", ",", "c", ",", "args", ")", ":", "\n", "        ", "super", "(", "GCN", ",", "self", ")", ".", "__init__", "(", "c", ")", "\n", "assert", "args", ".", "num_layers", ">", "0", "\n", "dims", ",", "acts", "=", "get_dim_act", "(", "args", ")", "\n", "gc_layers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "dims", ")", "-", "1", ")", ":", "\n", "            ", "in_dim", ",", "out_dim", "=", "dims", "[", "i", "]", ",", "dims", "[", "i", "+", "1", "]", "\n", "act", "=", "acts", "[", "i", "]", "\n", "gc_layers", ".", "append", "(", "GraphConvolution", "(", "in_dim", ",", "out_dim", ",", "args", ".", "dropout", ",", "act", ",", "args", ".", "bias", ")", ")", "\n", "", "self", ".", "layers", "=", "nn", ".", "Sequential", "(", "*", "gc_layers", ")", "\n", "self", ".", "encode_graph", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.encoders.HGCN.__init__": [[98, 117], ["encoders.Encoder.__init__", "layers.get_dim_act_curv", "encoders.HGCN.curvatures.append", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "getattr", "hgc_layers.append", "len", "layers.HyperbolicGraphConvolution"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__", "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.hyp_layers.get_dim_act_curv"], ["def", "__init__", "(", "self", ",", "c", ",", "args", ")", ":", "\n", "        ", "super", "(", "HGCN", ",", "self", ")", ".", "__init__", "(", "c", ")", "\n", "self", ".", "manifold", "=", "getattr", "(", "manifolds", ",", "args", ".", "manifold", ")", "(", ")", "\n", "self", ".", "node_drop", "=", "args", ".", "node_drop", "\n", "assert", "args", ".", "num_layers", ">", "1", "\n", "dims", ",", "acts", ",", "self", ".", "curvatures", "=", "hyp_layers", ".", "get_dim_act_curv", "(", "args", ")", "\n", "self", ".", "curvatures", ".", "append", "(", "self", ".", "c", ")", "\n", "hgc_layers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "dims", ")", "-", "1", ")", ":", "\n", "            ", "c_in", ",", "c_out", "=", "self", ".", "curvatures", "[", "i", "]", ",", "self", ".", "curvatures", "[", "i", "+", "1", "]", "\n", "in_dim", ",", "out_dim", "=", "dims", "[", "i", "]", ",", "dims", "[", "i", "+", "1", "]", "\n", "act", "=", "acts", "[", "i", "]", "\n", "hgc_layers", ".", "append", "(", "\n", "hyp_layers", ".", "HyperbolicGraphConvolution", "(", "\n", "self", ".", "manifold", ",", "in_dim", ",", "out_dim", ",", "c_in", ",", "c_out", ",", "args", ".", "dropout", ",", "act", ",", "args", ".", "bias", ",", "args", ".", "use_att", "\n", ")", "\n", ")", "\n", "", "self", ".", "layers", "=", "nn", ".", "Sequential", "(", "*", "hgc_layers", ")", "\n", "self", ".", "encode_graph", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.encoders.HGCN.encode": [[118, 125], ["encoders.HGCN.manifold.proj_tan0", "encoders.HGCN.manifold.expmap0", "encoders.HGCN.manifold.proj", "encoders.Encoder.encode", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "node_mask.to", "x.to", "encoders.HGCN.to", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj_tan0", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.expmap0", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj", "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.encoders.Shallow.encode"], ["", "def", "encode", "(", "self", ",", "x", ",", "adj", ")", ":", "\n", "        ", "node_mask", "=", "torch", ".", "FloatTensor", "(", "x", ".", "shape", "[", "0", "]", ",", "1", ")", ".", "uniform_", "(", ")", ">", "self", ".", "node_drop", "\n", "x", "=", "node_mask", ".", "to", "(", "device", ")", "*", "x", "\n", "x_tan", "=", "self", ".", "manifold", ".", "proj_tan0", "(", "x", ".", "to", "(", "device", ")", ",", "self", ".", "curvatures", "[", "0", "]", ")", "\n", "x_hyp", "=", "self", ".", "manifold", ".", "expmap0", "(", "x_tan", ".", "to", "(", "device", ")", ",", "c", "=", "self", ".", "curvatures", "[", "0", "]", ")", "\n", "x_hyp", "=", "self", ".", "manifold", ".", "proj", "(", "x_hyp", ",", "c", "=", "self", ".", "curvatures", "[", "0", "]", ")", "\n", "return", "super", "(", "HGCN", ",", "self", ")", ".", "encode", "(", "x_hyp", ",", "adj", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.encoders.GAT.__init__": [[132, 147], ["encoders.Encoder.__init__", "layers.layers.get_dim_act", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "gat_layers.append", "len", "layers.att_layers.GraphAttentionLayer"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__", "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.layers.get_dim_act"], ["def", "__init__", "(", "self", ",", "c", ",", "args", ")", ":", "\n", "        ", "super", "(", "GAT", ",", "self", ")", ".", "__init__", "(", "c", ")", "\n", "assert", "args", ".", "num_layers", ">", "0", "\n", "dims", ",", "acts", "=", "get_dim_act", "(", "args", ")", "\n", "gat_layers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "dims", ")", "-", "1", ")", ":", "\n", "            ", "in_dim", ",", "out_dim", "=", "dims", "[", "i", "]", ",", "dims", "[", "i", "+", "1", "]", "\n", "act", "=", "acts", "[", "i", "]", "\n", "assert", "dims", "[", "i", "+", "1", "]", "%", "args", ".", "n_heads", "==", "0", "\n", "out_dim", "=", "dims", "[", "i", "+", "1", "]", "//", "args", ".", "n_heads", "\n", "concat", "=", "True", "\n", "gat_layers", ".", "append", "(", "\n", "GraphAttentionLayer", "(", "in_dim", ",", "out_dim", ",", "args", ".", "dropout", ",", "act", ",", "args", ".", "alpha", ",", "args", ".", "n_heads", ",", "concat", ")", ")", "\n", "", "self", ".", "layers", "=", "nn", ".", "Sequential", "(", "*", "gat_layers", ")", "\n", "self", ".", "encode_graph", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.encoders.Shallow.__init__": [[155, 183], ["encoders.Encoder.__init__", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "manifolds.ManifoldParameter", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.Sequential", "torch.Sequential", "torch.Sequential", "getattr", "encoders.Shallow.manifold.init_weights", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "list", "layers.layers.get_dim_act", "range", "numpy.load", "range", "layers.append", "len", "layers.layers.Linear"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.init_weights", "home.repos.pwc.inspect_result.shaanchandra_SAFER.layers.layers.get_dim_act"], ["def", "__init__", "(", "self", ",", "c", ",", "args", ")", ":", "\n", "        ", "super", "(", "Shallow", ",", "self", ")", ".", "__init__", "(", "c", ")", "\n", "self", ".", "manifold", "=", "getattr", "(", "manifolds", ",", "args", ".", "manifold", ")", "(", ")", "\n", "self", ".", "use_feats", "=", "args", ".", "use_feats", "\n", "weights", "=", "torch", ".", "Tensor", "(", "args", ".", "n_nodes", ",", "args", ".", "dim", ")", "\n", "if", "not", "args", ".", "pretrained_embeddings", ":", "\n", "            ", "weights", "=", "self", ".", "manifold", ".", "init_weights", "(", "weights", ",", "self", ".", "c", ")", "\n", "trainable", "=", "True", "\n", "", "else", ":", "\n", "            ", "weights", "=", "torch", ".", "Tensor", "(", "np", ".", "load", "(", "args", ".", "pretrained_embeddings", ")", ")", "\n", "assert", "weights", ".", "shape", "[", "0", "]", "==", "args", ".", "n_nodes", ",", "\"The embeddings you passed seem to be for another dataset.\"", "\n", "trainable", "=", "False", "\n", "", "self", ".", "lt", "=", "manifolds", ".", "ManifoldParameter", "(", "weights", ",", "trainable", ",", "self", ".", "manifold", ",", "self", ".", "c", ")", "\n", "self", ".", "all_nodes", "=", "torch", ".", "LongTensor", "(", "list", "(", "range", "(", "args", ".", "n_nodes", ")", ")", ")", "\n", "layers", "=", "[", "]", "\n", "if", "args", ".", "pretrained_embeddings", "is", "not", "None", "and", "args", ".", "num_layers", ">", "0", ":", "\n", "# MLP layers after pre-trained embeddings", "\n", "            ", "dims", ",", "acts", "=", "get_dim_act", "(", "args", ")", "\n", "if", "self", ".", "use_feats", ":", "\n", "                ", "dims", "[", "0", "]", "=", "args", ".", "feat_dim", "+", "weights", ".", "shape", "[", "1", "]", "\n", "", "else", ":", "\n", "                ", "dims", "[", "0", "]", "=", "weights", ".", "shape", "[", "1", "]", "\n", "", "for", "i", "in", "range", "(", "len", "(", "dims", ")", "-", "1", ")", ":", "\n", "                ", "in_dim", ",", "out_dim", "=", "dims", "[", "i", "]", ",", "dims", "[", "i", "+", "1", "]", "\n", "act", "=", "acts", "[", "i", "]", "\n", "layers", ".", "append", "(", "Linear", "(", "in_dim", ",", "out_dim", ",", "args", ".", "dropout", ",", "act", ",", "args", ".", "bias", ")", ")", "\n", "", "", "self", ".", "layers", "=", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "self", ".", "encode_graph", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.encoders.Shallow.encode": [[184, 189], ["encoders.Encoder.encode", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.models.encoders.Shallow.encode"], ["", "def", "encode", "(", "self", ",", "x", ",", "adj", ")", ":", "\n", "        ", "h", "=", "self", ".", "lt", "[", "self", ".", "all_nodes", ",", ":", "]", "\n", "if", "self", ".", "use_feats", ":", "\n", "            ", "h", "=", "torch", ".", "cat", "(", "(", "h", ",", "x", ")", ",", "1", ")", "\n", "", "return", "super", "(", "Shallow", ",", "self", ")", ".", "encode", "(", "h", ",", "adj", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.decoders.Decoder.__init__": [[15, 18], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "c", ")", ":", "\n", "        ", "super", "(", "Decoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "c", "=", "c", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.decoders.Decoder.decode": [[19, 26], ["decoders.Decoder.cls.forward", "decoders.Decoder.cls.forward"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.models.gnn_model.Relational_GNN.forward", "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.gnn_model.Relational_GNN.forward"], ["", "def", "decode", "(", "self", ",", "x", ",", "adj", ")", ":", "\n", "        ", "if", "self", ".", "decode_adj", ":", "\n", "            ", "input", "=", "(", "x", ",", "adj", ")", "\n", "probs", ",", "_", "=", "self", ".", "cls", ".", "forward", "(", "input", ")", "\n", "", "else", ":", "\n", "            ", "probs", "=", "self", ".", "cls", ".", "forward", "(", "x", ")", "\n", "", "return", "probs", ",", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.decoders.GCNDecoder.__init__": [[33, 38], ["decoders.Decoder.__init__", "layers.layers.GraphConvolution"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "c", ",", "args", ")", ":", "\n", "        ", "super", "(", "GCNDecoder", ",", "self", ")", ".", "__init__", "(", "c", ")", "\n", "act", "=", "lambda", "x", ":", "x", "\n", "self", ".", "cls", "=", "GraphConvolution", "(", "args", ".", "dim", ",", "args", ".", "n_classes", ",", "args", ".", "dropout", ",", "act", ",", "args", ".", "bias", ")", "\n", "self", ".", "decode_adj", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.decoders.GATDecoder.__init__": [[45, 49], ["decoders.Decoder.__init__", "layers.att_layers.GraphAttentionLayer"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "c", ",", "args", ")", ":", "\n", "        ", "super", "(", "GATDecoder", ",", "self", ")", ".", "__init__", "(", "c", ")", "\n", "self", ".", "cls", "=", "GraphAttentionLayer", "(", "args", ".", "dim", ",", "args", ".", "n_classes", ",", "args", ".", "dropout", ",", "F", ".", "elu", ",", "args", ".", "alpha", ",", "1", ",", "True", ")", "\n", "self", ".", "decode_adj", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.decoders.LinearDecoder.__init__": [[56, 64], ["decoders.Decoder.__init__", "layers.layers.Linear", "getattr"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "c", ",", "args", ")", ":", "\n", "        ", "super", "(", "LinearDecoder", ",", "self", ")", ".", "__init__", "(", "c", ")", "\n", "self", ".", "manifold", "=", "getattr", "(", "manifolds", ",", "args", ".", "manifold", ")", "(", ")", "\n", "self", ".", "input_dim", "=", "args", ".", "dim", "\n", "self", ".", "output_dim", "=", "args", ".", "n_classes", "\n", "self", ".", "bias", "=", "args", ".", "bias", "\n", "self", ".", "cls", "=", "Linear", "(", "self", ".", "input_dim", ",", "self", ".", "output_dim", ",", "args", ".", "dropout", ",", "lambda", "x", ":", "x", ",", "self", ".", "bias", ")", "\n", "self", ".", "decode_adj", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.decoders.LinearDecoder.decode": [[65, 68], ["decoders.LinearDecoder.manifold.proj_tan0", "decoders.Decoder.decode", "decoders.LinearDecoder.manifold.logmap0"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj_tan0", "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.decoders.LinearDecoder.decode", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.logmap0"], ["", "def", "decode", "(", "self", ",", "x", ",", "adj", ")", ":", "\n", "        ", "h", "=", "self", ".", "manifold", ".", "proj_tan0", "(", "self", ".", "manifold", ".", "logmap0", "(", "x", ",", "c", "=", "self", ".", "c", ")", ",", "c", "=", "self", ".", "c", ")", "\n", "return", "super", "(", "LinearDecoder", ",", "self", ")", ".", "decode", "(", "h", ",", "adj", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.decoders.LinearDecoder.extra_repr": [[69, 72], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "'in_features={}, out_features={}, bias={}, c={}'", ".", "format", "(", "\n", "self", ".", "input_dim", ",", "self", ".", "output_dim", ",", "self", ".", "bias", ",", "self", ".", "c", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.WeightDrop_manual.__init__": [[65, 72], ["super().__init__", "model.WeightDrop_manual._setup"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__", "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.WeightDrop_manual._setup"], ["    ", "def", "__init__", "(", "self", ",", "module", ",", "weights", ",", "dropout", "=", "0", ",", "variational", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "module", "=", "module", "\n", "self", ".", "weights", "=", "weights", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "variational", "=", "variational", "\n", "self", ".", "_setup", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.WeightDrop_manual.null_function": [[73, 76], ["None"], "methods", ["None"], ["", "def", "null_function", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "# We need to replace flatten_parameters with a nothing function", "\n", "        ", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.WeightDrop_manual._setup": [[77, 87], ["issubclass", "type", "print", "getattr", "model.WeightDrop_manual.module.register_parameter", "torch.nn.Parameter", "torch.nn.Parameter"], "methods", ["None"], ["", "def", "_setup", "(", "self", ")", ":", "\n", "# Terrible temporary solution to an issue regarding compacting weights re: CUDNN RNN", "\n", "        ", "if", "issubclass", "(", "type", "(", "self", ".", "module", ")", ",", "torch", ".", "nn", ".", "RNNBase", ")", ":", "\n", "            ", "self", ".", "module", ".", "flatten_parameters", "=", "self", ".", "null_function", "\n", "\n", "", "for", "name_w", "in", "self", ".", "weights", ":", "\n", "            ", "print", "(", "'Applying weight drop of {} to {}'", ".", "format", "(", "self", ".", "dropout", ",", "name_w", ")", ")", "\n", "w", "=", "getattr", "(", "self", ".", "module", ",", "name_w", ")", "\n", "del", "self", ".", "module", ".", "_parameters", "[", "name_w", "]", "\n", "self", ".", "module", ".", "register_parameter", "(", "name_w", "+", "'_raw'", ",", "Parameter", "(", "w", ".", "data", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.WeightDrop_manual._setweights": [[102, 115], ["getattr", "setattr", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.nn.functional.dropout().to", "torch.nn.functional.dropout().to", "torch.nn.functional.dropout().to", "torch.nn.functional.dropout().to", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.nn.functional.dropout.cuda", "torch.nn.functional.dropout.cuda", "torch.nn.functional.dropout", "torch.nn.functional.dropout", "torch.nn.functional.dropout", "torch.nn.functional.dropout", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "getattr.size", "torch.nn.functional.dropout", "torch.nn.functional.dropout", "torch.nn.functional.dropout", "torch.nn.functional.dropout", "torch.nn.functional.dropout.expand_as", "torch.nn.functional.dropout.expand_as"], "methods", ["None"], ["", "", "def", "_setweights", "(", "self", ")", ":", "\n", "        ", "for", "name_w", "in", "self", ".", "weights", ":", "\n", "            ", "raw_w", "=", "getattr", "(", "self", ".", "module", ",", "name_w", "+", "'_raw'", ")", "\n", "w", "=", "None", "\n", "if", "self", ".", "variational", ":", "\n", "                ", "mask", "=", "torch", ".", "autograd", ".", "Variable", "(", "torch", ".", "ones", "(", "raw_w", ".", "size", "(", "0", ")", ",", "1", ")", ")", "\n", "if", "raw_w", ".", "is_cuda", ":", "\n", "                    ", "mask", "=", "mask", ".", "cuda", "(", ")", "\n", "mask", "=", "torch", ".", "nn", ".", "functional", ".", "dropout", "(", "mask", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "True", ")", "\n", "w", "=", "torch", ".", "nn", ".", "Parameter", "(", "mask", ".", "expand_as", "(", "raw_w", ")", "*", "raw_w", ")", "\n", "", "", "else", ":", "\n", "                ", "w", "=", "torch", ".", "nn", ".", "functional", ".", "dropout", "(", "raw_w", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", ".", "to", "(", "device", ")", "\n", "", "setattr", "(", "self", ".", "module", ",", "name_w", ",", "w", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.WeightDrop_manual.forward": [[116, 119], ["model.WeightDrop_manual._setweights", "model.WeightDrop_manual.module.forward"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.WeightDrop_manual._setweights", "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.gnn_model.Relational_GNN.forward"], ["", "", "def", "forward", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "self", ".", "_setweights", "(", ")", "\n", "return", "self", ".", "module", ".", "forward", "(", "*", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.Document_Classifier.__init__": [[133, 199], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Sequential", "int", "torch.nn.Embedding", "torch.nn.Embedding", "model.Document_Classifier.embedding.weight.data.copy_", "model.BiLSTM", "model.Transformer_model", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "config[].split", "allennlp.modules.Elmo", "model.BiLSTM", "allennlp.modules.Elmo", "model.BiLSTM_reg", "allennlp.modules.Elmo", "model.HAN", "model.Kim_CNN", "len"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "pre_trained_embeds", "=", "None", ")", ":", "\n", "        ", "super", "(", "Document_Classifier", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "lstm_dim", "=", "config", "[", "'lstm_dim'", "]", "\n", "self", ".", "model_name", "=", "config", "[", "'model_name'", "]", "\n", "self", ".", "embed_name", "=", "config", "[", "'embed_dim'", "]", "\n", "self", ".", "fc_dim", "=", "config", "[", "'fc_dim'", "]", "\n", "self", ".", "num_classes", "=", "config", "[", "'n_classes'", "]", "\n", "self", ".", "embed_dim", "=", "config", "[", "'embed_dim'", "]", "if", "config", "[", "'embed_dim'", "]", "==", "300", "else", "2", "*", "config", "[", "'embed_dim'", "]", "\n", "self", ".", "batch_size", "=", "config", "[", "'batch_size'", "]", "\n", "self", ".", "num_kernels", "=", "config", "[", "\"kernel_num\"", "]", "\n", "self", ".", "kernel_sizes", "=", "[", "int", "(", "k", ")", "for", "k", "in", "config", "[", "\"kernel_sizes\"", "]", ".", "split", "(", "','", ")", "]", "\n", "self", ".", "mode", "=", "'single'", "if", "not", "config", "[", "'parallel_computing'", "]", "else", "'multi'", "\n", "\n", "# Choose the right embedding method based on embed_dim given", "\n", "if", "config", "[", "'embed_dim'", "]", "==", "300", ":", "\n", "            ", "self", ".", "vocab_size", "=", "config", "[", "'vocab_size'", "]", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "self", ".", "vocab_size", ",", "self", ".", "embed_dim", ")", "\n", "self", ".", "embedding", ".", "weight", ".", "data", ".", "copy_", "(", "pre_trained_embeds", ")", "\n", "self", ".", "embedding", ".", "requires_grad", "=", "False", "\n", "", "elif", "config", "[", "'embed_dim'", "]", "==", "128", ":", "\n", "# Small", "\n", "            ", "self", ".", "options_file", "=", "\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json\"", "\n", "self", ".", "weight_file", "=", "\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5\"", "\n", "self", ".", "elmo", "=", "Elmo", "(", "options_file", "=", "self", ".", "options_file", ",", "weight_file", "=", "self", ".", "weight_file", ",", "num_output_representations", "=", "1", ",", "requires_grad", "=", "False", ")", "\n", "", "elif", "config", "[", "'embed_dim'", "]", "==", "256", ":", "\n", "# Medium", "\n", "            ", "self", ".", "options_file", "=", "\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x2048_256_2048cnn_1xhighway/elmo_2x2048_256_2048cnn_1xhighway_options.json\"", "\n", "self", ".", "weight_file", "=", "\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x2048_256_2048cnn_1xhighway/elmo_2x2048_256_2048cnn_1xhighway_weights.hdf5\"", "\n", "self", ".", "elmo", "=", "Elmo", "(", "options_file", "=", "self", ".", "options_file", ",", "weight_file", "=", "self", ".", "weight_file", ",", "num_output_representations", "=", "1", ",", "requires_grad", "=", "False", ")", "\n", "", "elif", "config", "[", "'embed_dim'", "]", "==", "512", ":", "\n", "# Highest", "\n", "            ", "self", ".", "options_file", "=", "\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\"", "\n", "self", ".", "weight_file", "=", "\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\"", "\n", "self", ".", "elmo", "=", "Elmo", "(", "options_file", "=", "self", ".", "options_file", ",", "weight_file", "=", "self", ".", "weight_file", ",", "num_output_representations", "=", "1", ",", "requires_grad", "=", "False", ")", "\n", "\n", "\n", "", "if", "self", ".", "model_name", "==", "'bilstm'", ":", "\n", "            ", "self", ".", "encoder", "=", "BiLSTM", "(", "config", ")", "\n", "self", ".", "fc_inp_dim", "=", "2", "*", "config", "[", "'lstm_dim'", "]", "\n", "", "elif", "self", ".", "model_name", "==", "'bilstm_pool'", ":", "\n", "            ", "self", ".", "encoder", "=", "BiLSTM", "(", "config", ",", "max_pool", "=", "True", ")", "\n", "self", ".", "fc_inp_dim", "=", "2", "*", "config", "[", "'lstm_dim'", "]", "\n", "", "elif", "self", ".", "model_name", "==", "'bilstm_reg'", ":", "\n", "            ", "self", ".", "encoder", "=", "BiLSTM_reg", "(", "config", ")", "\n", "self", ".", "fc_inp_dim", "=", "config", "[", "'lstm_dim'", "]", "\n", "", "elif", "self", ".", "model_name", "==", "'han'", ":", "\n", "            ", "self", ".", "encoder", "=", "HAN", "(", "config", ")", "\n", "self", ".", "fc_inp_dim", "=", "2", "*", "config", "[", "'sent_lstm_dim'", "]", "\n", "", "elif", "self", ".", "model_name", "==", "'cnn'", ":", "\n", "            ", "self", ".", "encoder", "=", "Kim_CNN", "(", "config", ")", "\n", "self", ".", "fc_inp_dim", "=", "self", ".", "num_kernels", "*", "len", "(", "self", ".", "kernel_sizes", ")", "\n", "\n", "", "if", "config", "[", "'embed_name'", "]", "in", "[", "'dbert'", ",", "'roberta'", "]", ":", "\n", "            ", "MODEL_CLASSES", "=", "{", "\n", "\"dbert\"", ":", "(", "DistilBertConfig", ",", "DistilBertPreTrainedModel", ",", "DistilBertTokenizerFast", ")", ",", "\n", "\"roberta\"", ":", "(", "RobertaConfig", ",", "RobertaModel", ",", "RobertaTokenizerFast", ")", ",", "\n", "}", "\n", "config_class", ",", "_", ",", "_", "=", "MODEL_CLASSES", "[", "config", "[", "'embed_name'", "]", "]", "\n", "self", ".", "encoder", "=", "Transformer_model", "(", "config", ",", "config_class", ")", "\n", "self", ".", "fc_inp_dim", "=", "768", "if", "'base'", "in", "config", "[", "'model_name'", "]", "else", "1024", "\n", "\n", "", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "nn", ".", "Dropout", "(", "config", "[", "\"dropout\"", "]", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_inp_dim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "self", ".", "num_classes", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.Document_Classifier.forward": [[201, 231], ["model.Document_Classifier.encoder", "model.Document_Classifier.classifier", "model.Document_Classifier.embedding", "inp.reshape.reshape.contiguous", "model.Document_Classifier.encoder", "model.Document_Classifier.elmo", "model.Document_Classifier.embedding", "model.Document_Classifier.encoder", "model.Document_Classifier.encoder", "inp.reshape.reshape.contiguous", "inp.reshape.reshape.reshape", "sent_lens.reshape.reshape.reshape", "model.Document_Classifier.elmo", "inp.reshape.reshape.contiguous"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inp", ",", "sent_lens", "=", "0", ",", "doc_lens", "=", "0", ",", "arg", "=", "0", ",", "cache", "=", "False", ",", "attn_mask", "=", "None", ")", ":", "\n", "\n", "        ", "if", "self", ".", "model_name", "in", "[", "'bilstm'", ",", "'bilstm_pool'", ",", "'cnn'", "]", ":", "\n", "            ", "if", "self", ".", "embed_name", "==", "'glove'", ":", "\n", "                ", "inp", "=", "self", ".", "embedding", "(", "inp", ")", "\n", "", "else", ":", "\n", "                ", "inp", "=", "self", ".", "elmo", "(", "inp", ".", "contiguous", "(", ")", ")", "[", "'elmo_representations'", "]", "[", "0", "]", "\n", "\n", "", "out", "=", "self", ".", "encoder", "(", "inp", ".", "contiguous", "(", ")", ",", "lengths", "=", "sent_lens", ")", "\n", "\n", "", "elif", "self", ".", "embed_name", "in", "[", "'dbert'", ",", "'roberta'", "]", ":", "\n", "            ", "out", "=", "self", ".", "encoder", "(", "inp", ",", "attn_mask", ")", "\n", "\n", "# for HAN the embeddings are taken care of in its model class", "\n", "", "elif", "self", ".", "model_name", "==", "'han'", ":", "\n", "            ", "if", "self", ".", "embed_dim", "==", "300", ":", "\n", "                ", "inp", "=", "self", ".", "embedding", "(", "inp", ")", "\n", "# out = self.encoder(inp, embedding = self.embedding, sent_lengths = sent_lens, doc_lengths = doc_lens)", "\n", "out", "=", "self", ".", "encoder", "(", "inp", ",", "sent_lengths", "=", "sent_lens", ",", "num_sent_per_document", "=", "doc_lens", ",", "arg", "=", "arg", ")", "\n", "", "else", ":", "\n", "                ", "if", "self", ".", "mode", "==", "'multi'", ":", "\n", "                    ", "inp", "=", "inp", ".", "reshape", "(", "(", "inp", ".", "shape", "[", "0", "]", "*", "inp", ".", "shape", "[", "1", "]", ",", "inp", ".", "shape", "[", "2", "]", ",", "inp", ".", "shape", "[", "3", "]", ")", ")", "\n", "sent_lens", "=", "sent_lens", ".", "reshape", "(", "(", "sent_lens", ".", "shape", "[", "0", "]", "*", "sent_lens", ".", "shape", "[", "1", "]", ")", ")", "\n", "", "inp", "=", "self", ".", "elmo", "(", "inp", ".", "contiguous", "(", ")", ")", "[", "'elmo_representations'", "]", "[", "0", "]", "\n", "out", "=", "self", ".", "encoder", "(", "inp", ",", "sent_lengths", "=", "sent_lens", ",", "num_sent_per_document", "=", "doc_lens", ",", "arg", "=", "arg", ")", "\n", "\n", "", "", "if", "not", "cache", ":", "\n", "            ", "out", "=", "self", ".", "classifier", "(", "out", ")", "\n", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.BiLSTM.__init__": [[241, 246], ["torch.nn.Module.__init__", "torch.nn.GRU", "torch.nn.GRU"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "max_pool", "=", "False", ")", ":", "\n", "        ", "super", "(", "BiLSTM", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "pool", "=", "max_pool", "\n", "self", ".", "embed_dim", "=", "config", "[", "'embed_dim'", "]", "if", "config", "[", "'embed_dim'", "]", "==", "300", "else", "2", "*", "config", "[", "'embed_dim'", "]", "\n", "self", ".", "lstm", "=", "nn", ".", "GRU", "(", "self", ".", "embed_dim", ",", "config", "[", "\"lstm_dim\"", "]", ",", "bidirectional", "=", "True", ",", "dropout", "=", "config", "[", "'dropout'", "]", ",", "batch_first", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.BiLSTM.forward": [[247, 269], ["torch.nn.utils.rnn.pack_padded_sequence().to", "torch.nn.utils.rnn.pack_padded_sequence().to", "model.BiLSTM.lstm", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.where.to", "torch.where.to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.where", "torch.where", "torch.where", "torch.where", "torch.max", "torch.max", "torch.max", "torch.max", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "all_states.to", "all_states.to"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "embed", ",", "lengths", ")", ":", "\n", "# sorted_len, sorted_idxs = torch.sort(lengths, descending =True)", "\n", "# embed = embed[ sorted_idxs ,: , :].to(device)", "\n", "        ", "sorted_len", "=", "lengths", "\n", "#print(\"lengths in each forward = \", sorted_len)", "\n", "packed_embed", "=", "pack_padded_sequence", "(", "embed", ",", "sorted_len", ",", "batch_first", "=", "True", ",", "enforce_sorted", "=", "False", ")", ".", "to", "(", "device", ")", "\n", "all_states", ",", "hidden_states", "=", "self", ".", "lstm", "(", "packed_embed", ")", "\n", "all_states", ",", "_", "=", "pad_packed_sequence", "(", "all_states", ",", "batch_first", "=", "True", ")", "\n", "\n", "# If not max-pool biLSTM, we extract the h0_l and h0_r from the tuple of tuples 'hn', and concat them to get the final embedding", "\n", "if", "not", "self", ".", "pool", ":", "\n", "            ", "out", "=", "torch", ".", "cat", "(", "(", "hidden_states", "[", "0", "]", "[", "0", "]", ",", "hidden_states", "[", "0", "]", "[", "1", "]", ")", ",", "dim", "=", "1", ")", "\n", "\n", "# If it is max-pooling biLSTM, set the PADS to very low numbers so that they never get selected in max-pooling", "\n", "# Then, max-pool over each dimension(which is now 2D, as 'X' = ALL) to get the final embedding", "\n", "", "elif", "self", ".", "pool", ":", "\n", "# replace PADs with very low numbers so that they never get picked", "\n", "            ", "out", "=", "torch", ".", "where", "(", "all_states", ".", "to", "(", "'cpu'", ")", "==", "0", ",", "torch", ".", "tensor", "(", "-", "1e8", ")", ",", "all_states", ".", "to", "(", "'cpu'", ")", ")", "\n", "out", ",", "_", "=", "torch", ".", "max", "(", "out", ",", "1", ")", "\n", "# _, unsorted_idxs = torch.sort(sorted_idxs)", "\n", "# out = out[unsorted_idxs, :].to(device)", "\n", "", "return", "out", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.BiLSTM_reg.__init__": [[277, 301], ["torch.nn.Module.__init__", "torch.nn.LSTM().to", "torch.nn.LSTM().to", "torch.nn.Sequential", "torch.nn.Sequential", "torchnlp.nn.WeightDrop().to", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "copy.deepcopy", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.nn.LSTM", "torch.nn.LSTM", "torchnlp.nn.WeightDrop", "list", "a.cuda", "model.BiLSTM_reg.parameters"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BiLSTM_reg", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "tar", "=", "0.0", "\n", "self", ".", "ar", "=", "0.0", "\n", "self", ".", "beta_ema", "=", "config", "[", "\"beta_ema\"", "]", "# Temporal averaging", "\n", "self", ".", "wdrop", "=", "config", "[", "\"wdrop\"", "]", "# Weight dropping", "\n", "self", ".", "embed_droprate", "=", "config", "[", "\"embed_drop\"", "]", "# Embedding dropouts", "\n", "self", ".", "dropout", "=", "config", "[", "'dropout'", "]", "\n", "self", ".", "lstm_dim", "=", "config", "[", "'lstm_dim'", "]", "\n", "self", ".", "embed_dim", "=", "config", "[", "'embed_dim'", "]", "\n", "self", ".", "num_classes", "=", "config", "[", "'n_classes'", "]", "\n", "\n", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "self", ".", "embed_dim", ",", "self", ".", "lstm_dim", ",", "bidirectional", "=", "True", ",", "dropout", "=", "config", "[", "\"dropout\"", "]", ",", "num_layers", "=", "1", ",", "batch_first", "=", "False", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "nn", ".", "Dropout", "(", "self", ".", "dropout", ")", ",", "nn", ".", "Linear", "(", "2", "*", "self", ".", "lstm_dim", ",", "2", "*", "self", ".", "lstm_dim", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Linear", "(", "2", "*", "self", ".", "lstm_dim", ",", "self", ".", "num_classes", "-", "1", ")", ")", "\n", "\n", "# Applyying Weight dropout to hh_l0 layer of the LSTM", "\n", "weights", "=", "[", "'weight_hh_l0'", "]", "\n", "self", ".", "lstm", "=", "WeightDrop", "(", "self", ".", "lstm", ",", "weights", ",", "self", ".", "wdrop", ")", ".", "to", "(", "device", ")", "\n", "\n", "if", "self", ".", "beta_ema", ">", "0", ":", "\n", "            ", "self", ".", "avg_param", "=", "deepcopy", "(", "list", "(", "p", ".", "data", "for", "p", "in", "self", ".", "parameters", "(", ")", ")", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                ", "self", ".", "avg_param", "=", "[", "a", ".", "cuda", "(", ")", "for", "a", "in", "self", ".", "avg_param", "]", "\n", "", "self", ".", "steps_ema", "=", "0.", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.BiLSTM_reg.forward": [[302, 324], ["torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.nn.utils.rnn.pack_padded_sequence.permute", "torch.nn.utils.rnn.pack_padded_sequence.permute", "inp[].to", "model.BiLSTM_reg.lstm", "torch.where", "torch.where", "torch.where", "torch.where", "torch.max", "torch.max", "torch.max", "torch.max", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "out[].to", "model.BiLSTM_reg.classifier", "model.embedded_dropout", "embedding", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "all_states.to", "all_states.to"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.embedded_dropout"], ["", "", "def", "forward", "(", "self", ",", "inp", ",", "embedding", ",", "lengths", "=", "None", ")", ":", "\n", "        ", "sorted_len", ",", "sorted_idxs", "=", "torch", ".", "sort", "(", "lengths", ",", "descending", "=", "True", ")", "\n", "inp", "=", "inp", ".", "permute", "(", "1", ",", "0", ")", "\n", "inp", "=", "inp", "[", ":", ",", "sorted_idxs", "]", ".", "to", "(", "device", ")", "\n", "\n", "inp", "=", "embedded_dropout", "(", "embedding", ",", "inp", ",", "dropout", "=", "self", ".", "embed_droprate", "if", "self", ".", "training", "else", "0", ")", "if", "self", ".", "embed_droprate", "else", "embedding", "(", "inp", ")", "\n", "# print(\"Input embedding shape = \", inp.shape)", "\n", "\n", "if", "lengths", "is", "not", "None", ":", "\n", "            ", "inp", "=", "torch", ".", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "inp", ",", "sorted_len", ",", "batch_first", "=", "False", ")", "\n", "", "all_states", ",", "_", "=", "self", ".", "lstm", "(", "inp", ")", "\n", "\n", "if", "lengths", "is", "not", "None", ":", "\n", "            ", "all_states", ",", "_", "=", "torch", ".", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "all_states", ",", "batch_first", "=", "False", ")", "\n", "# print(\"rnn_outs(after lstm) shape = \", rnn_outs.shape)", "\n", "\n", "", "out", "=", "torch", ".", "where", "(", "all_states", ".", "to", "(", "'cpu'", ")", "==", "0", ",", "torch", ".", "tensor", "(", "-", "1e8", ")", ",", "all_states", ".", "to", "(", "'cpu'", ")", ")", "\n", "out", ",", "_", "=", "torch", ".", "max", "(", "out", ",", "0", ")", "\n", "_", ",", "unsorted_idxs", "=", "torch", ".", "sort", "(", "sorted_idxs", ")", "\n", "out", "=", "out", "[", "unsorted_idxs", ",", ":", "]", ".", "to", "(", "device", ")", "\n", "out", "=", "self", ".", "classifier", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.BiLSTM_reg.update_ema": [[326, 330], ["zip", "model.BiLSTM_reg.parameters", "avg_p.mul_().add_", "avg_p.mul_"], "methods", ["None"], ["", "def", "update_ema", "(", "self", ")", ":", "\n", "        ", "self", ".", "steps_ema", "+=", "1", "\n", "for", "p", ",", "avg_p", "in", "zip", "(", "self", ".", "parameters", "(", ")", ",", "self", ".", "avg_param", ")", ":", "\n", "            ", "avg_p", ".", "mul_", "(", "self", ".", "beta_ema", ")", ".", "add_", "(", "(", "1", "-", "self", ".", "beta_ema", ")", "*", "p", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.BiLSTM_reg.load_ema_params": [[331, 334], ["zip", "model.BiLSTM_reg.parameters", "p.data.copy_"], "methods", ["None"], ["", "", "def", "load_ema_params", "(", "self", ")", ":", "\n", "        ", "for", "p", ",", "avg_p", "in", "zip", "(", "self", ".", "parameters", "(", ")", ",", "self", ".", "avg_param", ")", ":", "\n", "            ", "p", ".", "data", ".", "copy_", "(", "avg_p", "/", "(", "1", "-", "self", ".", "beta_ema", "**", "self", ".", "steps_ema", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.BiLSTM_reg.load_params": [[335, 338], ["zip", "model.BiLSTM_reg.parameters", "p.data.copy_"], "methods", ["None"], ["", "", "def", "load_params", "(", "self", ",", "params", ")", ":", "\n", "        ", "for", "p", ",", "avg_p", "in", "zip", "(", "self", ".", "parameters", "(", ")", ",", "params", ")", ":", "\n", "            ", "p", ".", "data", ".", "copy_", "(", "avg_p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.BiLSTM_reg.get_params": [[339, 342], ["copy.deepcopy", "list", "model.BiLSTM_reg.parameters"], "methods", ["None"], ["", "", "def", "get_params", "(", "self", ")", ":", "\n", "        ", "params", "=", "deepcopy", "(", "list", "(", "p", ".", "data", "for", "p", "in", "self", ".", "parameters", "(", ")", ")", ")", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.Transformer_model.__init__": [[353, 363], ["torch.nn.Module.__init__", "model_class.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "transf_config", ")", ":", "\n", "        ", "super", "(", "Transformer_model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "MODEL_CLASSES", "=", "{", "\n", "\"dbert\"", ":", "(", "DistilBertConfig", ",", "DistilBertPreTrainedModel", ",", "DistilBertTokenizerFast", ")", ",", "\n", "\"roberta\"", ":", "(", "RobertaConfig", ",", "RobertaModel", ",", "RobertaTokenizerFast", ")", ",", "\n", "}", "\n", "_", ",", "model_class", ",", "_", "=", "MODEL_CLASSES", "[", "config", "[", "'embed_name'", "]", "]", "\n", "\n", "self", ".", "model", "=", "model_class", ".", "from_pretrained", "(", "transf_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.Transformer_model.forward": [[364, 367], ["model.Transformer_model.model"], "methods", ["None"], ["", "def", "forward", "(", "inp_ids", ",", "attn_mask", ")", ":", "\n", "        ", "out", "=", "self", ".", "model", "(", "inp_ids", ",", "attention_mask", "=", "attn_mask", ")", "[", "0", "]", "\n", "return", "out", "[", ":", ",", "0", ",", ":", "]", "# first tuple element is BxLxD where we need the first token ([CLS])", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.Kim_CNN.__init__": [[377, 388], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "int", "len", "config[].split", "torch.nn.Conv2d", "torch.nn.Conv2d"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "Kim_CNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "config", "[", "'embed_dim'", "]", "if", "config", "[", "'embed_dim'", "]", "==", "300", "else", "2", "*", "config", "[", "'embed_dim'", "]", "\n", "self", ".", "num_classes", "=", "config", "[", "\"n_classes\"", "]", "\n", "self", ".", "input_channels", "=", "1", "\n", "self", ".", "num_kernels", "=", "config", "[", "\"kernel_num\"", "]", "\n", "self", ".", "kernel_sizes", "=", "[", "int", "(", "k", ")", "for", "k", "in", "config", "[", "\"kernel_sizes\"", "]", ".", "split", "(", "','", ")", "]", "\n", "self", ".", "fc_inp_dim", "=", "self", ".", "num_kernels", "*", "len", "(", "self", ".", "kernel_sizes", ")", "\n", "self", ".", "fc_dim", "=", "config", "[", "'fc_dim'", "]", "\n", "\n", "self", ".", "cnn", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Conv2d", "(", "self", ".", "input_channels", ",", "self", ".", "num_kernels", ",", "(", "k_size", ",", "self", ".", "embed_dim", ")", ")", "for", "k_size", "in", "self", ".", "kernel_sizes", "]", ")", "\n", "# self.classifier = nn.Sequential(nn.Dropout(config[\"dropout\"]), nn.Linear(self.fc_inp_dim, self.fc_dim), nn.ReLU(), nn.Linear(self.fc_dim, self.num_classes-1))", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.Kim_CNN.forward": [[391, 399], ["inp.unsqueeze.unsqueeze.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "F.relu().squeeze", "F.max_pool1d().squeeze", "F.relu", "F.max_pool1d", "conv", "i.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inp", ",", "embedding", "=", "None", ",", "lengths", "=", "None", ")", ":", "\n", "# x is (B, L, D)", "\n", "        ", "inp", "=", "inp", ".", "unsqueeze", "(", "1", ")", "# (B, Ci, L, D)", "\n", "inp", "=", "[", "F", ".", "relu", "(", "conv", "(", "inp", ")", ")", ".", "squeeze", "(", "3", ")", "for", "conv", "in", "self", ".", "cnn", "]", "# [(B, Co, L), ...]*len(Ks)", "\n", "inp", "=", "[", "F", ".", "max_pool1d", "(", "i", ",", "i", ".", "size", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "for", "i", "in", "inp", "]", "# [(B, Co), ...]*len(Ks)", "\n", "out", "=", "torch", ".", "cat", "(", "inp", ",", "1", ")", "# (B, len(Ks)*Co)", "\n", "# out = self.classifier(out)", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.HAN.__init__": [[483, 515], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "model.HAN.word_context_vector.data.normal_", "torch.nn.GRU", "torch.nn.GRU", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Parameter", "torch.nn.Parameter", "model.HAN.sent_context_vector.data.normal_", "torch.nn.GRU", "torch.nn.GRU", "torch.nn.Sequential", "torch.nn.Sequential", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "math.sqrt", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "math.sqrt", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh", "model.HAN.word_context_vector.size", "model.HAN.sent_context_vector.size"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "HAN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_hidden_dim", "=", "config", "[", "'word_lstm_dim'", "]", "\n", "self", ".", "sent_hidden_dim", "=", "config", "[", "'sent_lstm_dim'", "]", "\n", "self", ".", "embed_dim", "=", "config", "[", "'embed_dim'", "]", "\n", "self", ".", "num_classes", "=", "config", "[", "'n_classes'", "]", "\n", "self", ".", "dropout", "=", "config", "[", "\"dropout\"", "]", "\n", "self", ".", "fc_dim", "=", "config", "[", "'fc_dim'", "]", "\n", "self", ".", "mode", "=", "'single'", "if", "not", "config", "[", "'parallel_computing'", "]", "else", "'multi'", "\n", "\n", "### Word attention ###   ", "\n", "######################            ", "\n", "# Initializing the word_context_vector", "\n", "self", ".", "word_context_vector", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "(", "2", "*", "self", ".", "word_hidden_dim", ",", "1", ")", ")", ")", "\n", "stdv", "=", "1.", "/", "math", ".", "sqrt", "(", "self", ".", "word_context_vector", ".", "size", "(", "0", ")", ")", "\n", "self", ".", "word_context_vector", ".", "data", ".", "normal_", "(", "mean", "=", "0", ",", "std", "=", "stdv", ")", "\n", "# Initializing the word LSTM", "\n", "self", ".", "word_attn_lstm", "=", "nn", ".", "GRU", "(", "2", "*", "self", ".", "embed_dim", ",", "self", ".", "word_hidden_dim", ",", "bidirectional", "=", "True", ",", "batch_first", "=", "True", ")", "\n", "# Word level Linear projection layer and attention", "\n", "self", ".", "word_lin_projection", "=", "nn", ".", "Sequential", "(", "nn", ".", "Dropout", "(", "config", "[", "\"dropout\"", "]", ")", ",", "nn", ".", "Linear", "(", "2", "*", "self", ".", "word_hidden_dim", ",", "2", "*", "self", ".", "word_hidden_dim", ")", ",", "nn", ".", "Tanh", "(", ")", ")", "\n", "\n", "\n", "### Sentence attention ###", "\n", "##########################         ", "\n", "# Initializing the sent_context_vector", "\n", "self", ".", "sent_context_vector", "=", "nn", ".", "Parameter", "(", "torch", ".", "rand", "(", "2", "*", "self", ".", "sent_hidden_dim", ",", "1", ")", ")", "\n", "stdv", "=", "1.", "/", "math", ".", "sqrt", "(", "self", ".", "sent_context_vector", ".", "size", "(", "0", ")", ")", "\n", "self", ".", "sent_context_vector", ".", "data", ".", "normal_", "(", "mean", "=", "0", ",", "std", "=", "stdv", ")", "\n", "# Initializing the sentence LSTM", "\n", "self", ".", "sentence_attn_lstm", "=", "nn", ".", "GRU", "(", "2", "*", "self", ".", "word_hidden_dim", ",", "self", ".", "sent_hidden_dim", ",", "bidirectional", "=", "True", ",", "batch_first", "=", "True", ")", "\n", "# Sentence level Linear projection layer and attention", "\n", "self", ".", "sent_lin_projection", "=", "nn", ".", "Sequential", "(", "nn", ".", "Dropout", "(", "config", "[", "\"dropout\"", "]", ")", ",", "nn", ".", "Linear", "(", "2", "*", "self", ".", "sent_hidden_dim", ",", "2", "*", "self", ".", "sent_hidden_dim", ")", ",", "nn", ".", "Tanh", "(", ")", ")", "\n", "# self.classifier = nn.Sequential(nn.Dropout(self.dropout), nn.Linear(2*self.sent_hidden_dim, self.fc_dim), nn.ReLU(), nn.Linear(self.fc_dim, self.num_classes-1))", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.HAN.forward": [[517, 524], ["model.HAN._forward_single", "model.HAN._forward_multi"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.HAN._forward_single", "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.HAN._forward_multi"], ["", "def", "forward", "(", "self", ",", "inp", ",", "sent_lengths", "=", "None", ",", "num_sent_per_document", "=", "None", ",", "arg", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "mode", "==", "'single'", ":", "\n", "            ", "doc_embedding", "=", "self", ".", "_forward_single", "(", "inp", ",", "sent_lengths", "=", "sent_lengths", ",", "num_sent_per_document", "=", "num_sent_per_document", ",", "recover_idxs", "=", "arg", ")", "\n", "", "else", ":", "\n", "            ", "doc_embedding", "=", "self", ".", "_forward_multi", "(", "inp", ",", "sent_lengths", "=", "sent_lengths", ",", "num_sent_per_document", "=", "num_sent_per_document", ",", "max_num_sent", "=", "arg", ")", "\n", "\n", "", "return", "doc_embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.HAN._forward_single": [[527, 573], ["len", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "model.HAN.word_attn_lstm", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "model.HAN.word_lin_projection", "mask.unsqueeze.unsqueeze.unsqueeze", "float", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.max", "torch.max", "torch.max", "torch.max", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "enumerate", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "model.HAN.sentence_attn_lstm", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "model.HAN.sent_lin_projection", "mask.unsqueeze.unsqueeze.unsqueeze", "float", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "num_sent.item", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.softmax.expand_as", "torch.softmax.expand_as", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.softmax.expand_as", "torch.softmax.expand_as", "num_sent.item", "num_sent.item"], "methods", ["None"], ["", "def", "_forward_single", "(", "self", ",", "inp", ",", "sent_lengths", "=", "None", ",", "num_sent_per_document", "=", "None", ",", "recover_idxs", "=", "None", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "num_sent_per_document", ")", "\n", "## Word Level attention block ##", "\n", "################################", "\n", "packed_seq", "=", "pack_padded_sequence", "(", "inp", ",", "sent_lengths", ",", "batch_first", "=", "True", ",", "enforce_sorted", "=", "False", ")", "\n", "out", ",", "_", "=", "self", ".", "word_attn_lstm", "(", "packed_seq", ")", "\n", "pad_packed_states_word", ",", "_", "=", "pad_packed_sequence", "(", "out", ",", "batch_first", "=", "True", ")", "\n", "\n", "word_pre_attn", "=", "self", ".", "word_lin_projection", "(", "pad_packed_states_word", ")", "\n", "\n", "# Masked attention", "\n", "max_len", "=", "pad_packed_states_word", ".", "shape", "[", "1", "]", "\n", "mask", "=", "torch", ".", "arange", "(", "max_len", ",", "device", "=", "inp", ".", "device", ")", "[", "None", ",", ":", "]", "<", "sent_lengths", "[", ":", ",", "None", "]", "\n", "mask", "=", "mask", ".", "unsqueeze", "(", "2", ")", "\n", "dot_product", "=", "word_pre_attn", "@", "self", ".", "word_context_vector", "\n", "dot_product", "[", "~", "mask", "]", "=", "float", "(", "'-inf'", ")", "\n", "word_attn", "=", "torch", ".", "softmax", "(", "dot_product", ",", "dim", "=", "1", ")", "\n", "sent_embeddings", "=", "torch", ".", "sum", "(", "word_attn", ".", "expand_as", "(", "pad_packed_states_word", ")", "*", "pad_packed_states_word", ",", "dim", "=", "1", ")", "\n", "\n", "# create new 3d tensor (already padded across dim=1)", "\n", "max_num_sent", "=", "torch", ".", "max", "(", "num_sent_per_document", ")", "\n", "sent_embeddings_3d", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max_num_sent", ",", "sent_embeddings", ".", "shape", "[", "1", "]", ")", ".", "to", "(", "inp", ".", "device", ")", "\n", "\n", "# fill the 3d tensor", "\n", "processed_sent", "=", "0", "\n", "for", "i", ",", "num_sent", "in", "enumerate", "(", "num_sent_per_document", ")", ":", "\n", "            ", "sent_embeddings_3d", "[", "i", ",", ":", "num_sent", ".", "item", "(", ")", ",", ":", "]", "=", "sent_embeddings", "[", "processed_sent", ":", "processed_sent", "+", "num_sent", ".", "item", "(", ")", ",", ":", "]", "\n", "processed_sent", "+=", "num_sent", ".", "item", "(", ")", "\n", "\n", "## Sentence Level attention block ##", "\n", "####################################", "\n", "", "packed_seq", "=", "pack_padded_sequence", "(", "sent_embeddings_3d", ",", "num_sent_per_document", ",", "batch_first", "=", "True", ",", "enforce_sorted", "=", "False", ")", "\n", "out", ",", "_", "=", "self", ".", "sentence_attn_lstm", "(", "packed_seq", ")", "\n", "pad_packed_states_sent", ",", "_", "=", "pad_packed_sequence", "(", "out", ",", "batch_first", "=", "True", ")", "\n", "sent_pre_attn", "=", "self", ".", "sent_lin_projection", "(", "pad_packed_states_sent", ")", "\n", "\n", "# Masked attention", "\n", "max_len", "=", "pad_packed_states_sent", ".", "shape", "[", "1", "]", "\n", "mask", "=", "torch", ".", "arange", "(", "max_len", ",", "device", "=", "inp", ".", "device", ")", "[", "None", ",", ":", "]", "<", "num_sent_per_document", "[", ":", ",", "None", "]", "\n", "mask", "=", "mask", ".", "unsqueeze", "(", "2", ")", "\n", "dot_product", "=", "sent_pre_attn", "@", "self", ".", "sent_context_vector", "\n", "dot_product", "[", "~", "mask", "]", "=", "float", "(", "'-inf'", ")", "\n", "sent_attn", "=", "torch", ".", "softmax", "(", "dot_product", ",", "dim", "=", "1", ")", "\n", "doc_embedding", "=", "torch", ".", "sum", "(", "sent_attn", ".", "expand_as", "(", "pad_packed_states_sent", ")", "*", "pad_packed_states_sent", ",", "dim", "=", "1", ")", "\n", "# sent_attn = torch.index_select(sorted_sent_attn, dim=0, index=recover_idx_sent).squeeze(2)", "\n", "return", "doc_embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.HAN._forward_multi": [[575, 626], ["len", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "model.HAN.word_attn_lstm", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "enumerate", "model.HAN.word_lin_projection", "mask.unsqueeze.unsqueeze.unsqueeze", "float", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "enumerate", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "model.HAN.sentence_attn_lstm", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "model.HAN.sent_lin_projection", "mask.unsqueeze.unsqueeze.unsqueeze", "float", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "num_sent.item", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.softmax.expand_as", "torch.softmax.expand_as", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.softmax.expand_as", "torch.softmax.expand_as", "num_sent.item", "num_sent.item"], "methods", ["None"], ["", "def", "_forward_multi", "(", "self", ",", "inp", ",", "sent_lengths", "=", "None", ",", "num_sent_per_document", "=", "None", ",", "max_num_sent", "=", "None", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "num_sent_per_document", ")", "\n", "## Word Level attention block ##", "\n", "################################", "\n", "packed_seq", "=", "pack_padded_sequence", "(", "inp", ",", "sent_lengths", ",", "batch_first", "=", "True", ",", "enforce_sorted", "=", "False", ")", "\n", "out", ",", "_", "=", "self", ".", "word_attn_lstm", "(", "packed_seq", ")", "\n", "pad_packed_states_word", ",", "_", "=", "pad_packed_sequence", "(", "out", ",", "batch_first", "=", "True", ")", "\n", "\n", "mask", "=", "torch", ".", "ones", "(", "(", "pad_packed_states_word", ".", "shape", "[", "0", "]", ",", "pad_packed_states_word", ".", "shape", "[", "1", "]", ",", "2", "*", "self", ".", "word_hidden_dim", ")", ")", ".", "to", "(", "inp", ".", "device", ")", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "sent_lengths", ")", ":", "\n", "            ", "if", "l", "==", "1", ":", "\n", "                ", "mask", "[", "i", ",", ":", ",", ":", "]", "=", "0", "\n", "", "", "pad_packed_states_word", "=", "mask", "*", "pad_packed_states_word", "\n", "\n", "word_pre_attn", "=", "self", ".", "word_lin_projection", "(", "pad_packed_states_word", ")", "\n", "\n", "# Masked attention", "\n", "max_len", "=", "pad_packed_states_word", ".", "shape", "[", "1", "]", "\n", "mask", "=", "torch", ".", "arange", "(", "max_len", ",", "device", "=", "inp", ".", "device", ")", "[", "None", ",", ":", "]", "<", "sent_lengths", "[", ":", ",", "None", "]", "\n", "mask", "=", "mask", ".", "unsqueeze", "(", "2", ")", "\n", "dot_product", "=", "word_pre_attn", "@", "self", ".", "word_context_vector", "\n", "dot_product", "[", "~", "mask", "]", "=", "float", "(", "'-inf'", ")", "\n", "word_attn", "=", "torch", ".", "softmax", "(", "dot_product", ",", "dim", "=", "1", ")", "\n", "sent_embeddings", "=", "torch", ".", "sum", "(", "word_attn", ".", "expand_as", "(", "pad_packed_states_word", ")", "*", "pad_packed_states_word", ",", "dim", "=", "1", ")", "\n", "\n", "# create new 3d tensor (already padded across dim=1)", "\n", "sent_embeddings_3d", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max_num_sent", ",", "sent_embeddings", ".", "shape", "[", "1", "]", ")", ".", "to", "(", "inp", ".", "device", ")", "\n", "\n", "# fill the 3d tensor", "\n", "processed_sent", "=", "0", "\n", "for", "i", ",", "num_sent", "in", "enumerate", "(", "num_sent_per_document", ")", ":", "\n", "            ", "sent_embeddings_3d", "[", "i", ",", ":", "num_sent", ".", "item", "(", ")", ",", ":", "]", "=", "sent_embeddings", "[", "processed_sent", ":", "processed_sent", "+", "num_sent", ".", "item", "(", ")", ",", ":", "]", "\n", "processed_sent", "+=", "num_sent", ".", "item", "(", ")", "\n", "\n", "## Sentence Level attention block ##", "\n", "####################################", "\n", "", "packed_seq", "=", "pack_padded_sequence", "(", "sent_embeddings_3d", ",", "num_sent_per_document", ",", "batch_first", "=", "True", ",", "enforce_sorted", "=", "False", ")", "\n", "out", ",", "_", "=", "self", ".", "sentence_attn_lstm", "(", "packed_seq", ")", "\n", "pad_packed_states_sent", ",", "_", "=", "pad_packed_sequence", "(", "out", ",", "batch_first", "=", "True", ")", "\n", "sent_pre_attn", "=", "self", ".", "sent_lin_projection", "(", "pad_packed_states_sent", ")", "\n", "\n", "# Masked attention", "\n", "max_len", "=", "pad_packed_states_sent", ".", "shape", "[", "1", "]", "\n", "mask", "=", "torch", ".", "arange", "(", "max_len", ",", "device", "=", "inp", ".", "device", ")", "[", "None", ",", ":", "]", "<", "num_sent_per_document", "[", ":", ",", "None", "]", "\n", "mask", "=", "mask", ".", "unsqueeze", "(", "2", ")", "\n", "dot_product", "=", "sent_pre_attn", "@", "self", ".", "sent_context_vector", "\n", "dot_product", "[", "~", "mask", "]", "=", "float", "(", "'-inf'", ")", "\n", "sent_attn", "=", "torch", ".", "softmax", "(", "dot_product", ",", "dim", "=", "1", ")", "\n", "doc_embedding", "=", "torch", ".", "sum", "(", "sent_attn", ".", "expand_as", "(", "pad_packed_states_sent", ")", "*", "pad_packed_states_sent", ",", "dim", "=", "1", ")", "\n", "# sent_attn = torch.index_select(sorted_sent_attn, dim=0, index=recover_idx_sent).squeeze(2)", "\n", "return", "doc_embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.model.embedded_dropout": [[47, 62], ["torch.nn.functional.embedding", "torch.nn.functional.embedding", "embed.weight.data.new().resize_().bernoulli_().expand_as", "scale.expand_as", "embed.weight.data.new().resize_().bernoulli_", "embed.weight.data.new().resize_", "embed.weight.data.new", "embed.weight.size"], "function", ["None"], ["def", "embedded_dropout", "(", "embed", ",", "words", ",", "dropout", "=", "0.1", ",", "scale", "=", "None", ")", ":", "\n", "    ", "if", "dropout", ":", "\n", "      ", "mask", "=", "embed", ".", "weight", ".", "data", ".", "new", "(", ")", ".", "resize_", "(", "(", "embed", ".", "weight", ".", "size", "(", "0", ")", ",", "1", ")", ")", ".", "bernoulli_", "(", "1", "-", "dropout", ")", ".", "expand_as", "(", "embed", ".", "weight", ")", "/", "(", "1", "-", "dropout", ")", "\n", "masked_embed_weight", "=", "mask", "*", "embed", ".", "weight", "\n", "", "else", ":", "\n", "      ", "masked_embed_weight", "=", "embed", ".", "weight", "\n", "", "if", "scale", ":", "\n", "      ", "masked_embed_weight", "=", "scale", ".", "expand_as", "(", "masked_embed_weight", ")", "*", "masked_embed_weight", "\n", "\n", "", "padding_idx", "=", "embed", ".", "padding_idx", "\n", "if", "padding_idx", "is", "None", ":", "\n", "        ", "padding_idx", "=", "-", "1", "\n", "\n", "", "X", "=", "torch", ".", "nn", ".", "functional", ".", "embedding", "(", "words", ",", "masked_embed_weight", ",", "padding_idx", ",", "embed", ".", "max_norm", ",", "embed", ".", "norm_type", ",", "embed", ".", "scale_grad_by_freq", ",", "embed", ".", "sparse", ")", "\n", "return", "X", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.gnn_model.Graph_Net.__init__": [[26, 65], ["super().__init__", "torch_geometric.nn.SAGEConv", "torch_geometric.nn.SAGEConv", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch_geometric.nn.GCNConv", "torch_geometric.nn.GCNConv", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch_geometric.nn.GraphConv", "torch_geometric.nn.GraphConv", "torch_geometric.nn.GATConv", "torch_geometric.nn.GATConv"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "Graph_Net", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "in_dim", "=", "config", "[", "'vocab_size'", "]", "\n", "self", ".", "embed_dim", "=", "config", "[", "'embed_dim'", "]", "\n", "self", ".", "dropout", "=", "config", "[", "'dropout'", "]", "\n", "self", ".", "fc_dim", "=", "config", "[", "'fc_dim'", "]", "\n", "self", ".", "node_drop", "=", "config", "[", "'node_drop'", "]", "\n", "self", ".", "model", "=", "config", "[", "'model_name'", "]", "\n", "\n", "if", "config", "[", "'model_name'", "]", "==", "'graph_sage'", ":", "\n", "            ", "self", ".", "conv1", "=", "SAGEConv", "(", "self", ".", "in_dim", ",", "self", ".", "embed_dim", ",", "normalize", "=", "True", ")", "\n", "self", ".", "conv2", "=", "SAGEConv", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ",", "normalize", "=", "True", ")", "\n", "# self.conv3 = SAGEConv(self.embed_dim, config['n_classes'], normalize=False)", "\n", "\n", "", "elif", "config", "[", "'model_name'", "]", "==", "'gcn'", ":", "\n", "            ", "self", ".", "conv1", "=", "GCNConv", "(", "self", ".", "in_dim", ",", "self", ".", "embed_dim", ",", "improved", "=", "True", ")", "\n", "self", ".", "conv2", "=", "GCNConv", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ",", "improved", "=", "True", ")", "\n", "# self.conv2 = GCNConv(self.embed_dim, config['n_classes'], improved=True)", "\n", "\n", "", "elif", "config", "[", "'model_name'", "]", "==", "'graph_conv'", ":", "\n", "            ", "self", ".", "conv1", "=", "GraphConv", "(", "self", ".", "in_dim", ",", "self", ".", "embed_dim", ",", "aggr", "=", "'add'", ")", "\n", "self", ".", "conv2", "=", "GraphConv", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ",", "aggr", "=", "'add'", ")", "\n", "# self.conv2 = GraphConv(self.embed_dim, config['n_classes'], aggr='add')", "\n", "\n", "", "elif", "config", "[", "'model_name'", "]", "==", "'gat'", ":", "\n", "            ", "self", ".", "conv1", "=", "GATConv", "(", "self", ".", "in_dim", ",", "self", ".", "embed_dim", ",", "heads", "=", "3", ",", "concat", "=", "True", ",", "dropout", "=", "0.1", ")", "\n", "self", ".", "conv2", "=", "GATConv", "(", "3", "*", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ",", "heads", "=", "3", ",", "concat", "=", "True", ",", "dropout", "=", "0.1", ")", "\n", "# self.conv2 = GATConv(3*self.embed_dim, config['n_classes'], heads=3, concat=False, dropout=0.1)", "\n", "\n", "", "if", "config", "[", "'model_name'", "]", "==", "'gat'", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "nn", ".", "Dropout", "(", "config", "[", "\"dropout\"", "]", ")", ",", "\n", "nn", ".", "Linear", "(", "3", "*", "self", ".", "embed_dim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "config", "[", "'n_classes'", "]", ")", ")", "\n", "", "else", ":", "\n", "             ", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "nn", ".", "Dropout", "(", "config", "[", "\"dropout\"", "]", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "config", "[", "'n_classes'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.gnn_model.Graph_Net.forward": [[67, 77], ["torch.relu", "torch.relu", "torch.dropout", "torch.dropout", "gnn_model.Graph_Net.conv2", "gnn_model.Graph_Net.classifier", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "gnn_model.Graph_Net.conv1", "gnn_model.Graph_Net.float", "node_mask.to", "gnn_model.Graph_Net.float", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "edge_index", ",", "edge_type", "=", "None", ")", ":", "\n", "\n", "        ", "node_mask", "=", "torch", ".", "FloatTensor", "(", "x", ".", "shape", "[", "0", "]", ",", "1", ")", ".", "uniform_", "(", ")", ">", "self", ".", "node_drop", "\n", "if", "self", ".", "training", ":", "\n", "            ", "x", "=", "node_mask", ".", "to", "(", "device", ")", "*", "x", "#/ (1 - self.node_drop)", "\n", "", "x", "=", "F", ".", "relu", "(", "self", ".", "conv1", "(", "x", ".", "float", "(", ")", ",", "edge_index", ")", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "conv2", "(", "x", ".", "float", "(", ")", ",", "edge_index", ")", "\n", "out", "=", "self", ".", "classifier", "(", "x", ")", "\n", "return", "out", ",", "node_mask", ",", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.gnn_model.Relational_GNN.__init__": [[93, 125], ["super().__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch_geometric.nn.SAGEConv", "torch_geometric.nn.SAGEConv", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "range", "range", "torch_geometric.nn.GCNConv", "torch_geometric.nn.GCNConv", "range", "range", "torch_geometric.nn.GATConv", "torch_geometric.nn.GATConv", "range", "range"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "Relational_GNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_rels", "=", "config", "[", "'num_rels'", "]", "\n", "self", ".", "in_dim", "=", "config", "[", "'vocab_size'", "]", "\n", "self", ".", "embed_dim", "=", "config", "[", "'embed_dim'", "]", "\n", "self", ".", "dropout", "=", "config", "[", "'dropout'", "]", "\n", "self", ".", "fc_dim", "=", "config", "[", "'fc_dim'", "]", "\n", "self", ".", "node_drop", "=", "config", "[", "'node_drop'", "]", "\n", "\n", "if", "config", "[", "'model_name'", "]", "==", "'rsage'", ":", "\n", "            ", "self", ".", "conv1", "=", "torch", ".", "nn", ".", "ModuleList", "(", "[", "SAGEConv", "(", "self", ".", "in_dim", ",", "self", ".", "embed_dim", ",", "normalize", "=", "True", ",", "concat", "=", "True", ")", "for", "_", "in", "range", "(", "self", ".", "num_rels", ")", "]", ")", "\n", "self", ".", "conv2", "=", "torch", ".", "nn", ".", "ModuleList", "(", "[", "SAGEConv", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ",", "normalize", "=", "True", ",", "concat", "=", "True", ")", "for", "_", "in", "range", "(", "self", ".", "num_rels", ")", "]", ")", "\n", "\n", "", "elif", "config", "[", "'model_name'", "]", "==", "'rgcn'", ":", "\n", "            ", "self", ".", "conv1", "=", "torch", ".", "nn", ".", "ModuleList", "(", "[", "GCNConv", "(", "self", ".", "in_dim", ",", "self", ".", "embed_dim", ",", "improved", "=", "True", ")", "for", "_", "in", "range", "(", "self", ".", "num_rels", ")", "]", ")", "\n", "self", ".", "conv2", "=", "torch", ".", "nn", ".", "ModuleList", "(", "[", "GCNConv", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ",", "improved", "=", "True", ")", "for", "_", "in", "range", "(", "self", ".", "num_rels", ")", "]", ")", "\n", "\n", "", "elif", "config", "[", "'model_name'", "]", "==", "'rgat'", ":", "\n", "            ", "self", ".", "conv1", "=", "torch", ".", "nn", ".", "ModuleList", "(", "[", "GATConv", "(", "self", ".", "in_dim", ",", "self", ".", "embed_dim", ",", "heads", "=", "3", ",", "concat", "=", "True", ",", "dropout", "=", "0.1", ")", "for", "_", "in", "range", "(", "self", ".", "num_rels", ")", "]", ")", "\n", "self", ".", "conv1", "=", "torch", ".", "nn", ".", "ModuleList", "(", "[", "GATConv", "(", "3", "*", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ",", "heads", "=", "3", ",", "concat", "=", "True", ",", "dropout", "=", "0.1", ")", "for", "_", "in", "range", "(", "self", ".", "num_rels", ")", "]", ")", "\n", "\n", "\n", "", "if", "config", "[", "'model_name'", "]", "==", "'rgat'", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "nn", ".", "Dropout", "(", "config", "[", "\"dropout\"", "]", ")", ",", "\n", "nn", ".", "Linear", "(", "3", "*", "self", ".", "embed_dim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "config", "[", "'n_classes'", "]", ")", ")", "\n", "", "else", ":", "\n", "              ", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "nn", ".", "Dropout", "(", "config", "[", "\"dropout\"", "]", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "config", "[", "'n_classes'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.models.gnn_model.Relational_GNN.forward": [[126, 145], ["range", "torch.relu", "torch.relu", "torch.dropout", "torch.dropout", "range", "gnn_model.Relational_GNN.classifier", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "node_mask.to", "x.float", "torch.dropout.float", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "edge_index", ",", "edge_type", "=", "None", ")", ":", "\n", "        ", "node_mask", "=", "torch", ".", "FloatTensor", "(", "x", ".", "shape", "[", "0", "]", ",", "1", ")", ".", "uniform_", "(", ")", ">", "self", ".", "node_drop", "\n", "if", "self", ".", "training", ":", "\n", "            ", "x", "=", "node_mask", ".", "to", "(", "device", ")", "*", "x", "#/ (1 - self.node_drop)", "\n", "\n", "", "out1", "=", "0", "\n", "for", "rels", "in", "range", "(", "self", ".", "num_rels", ")", ":", "\n", "            ", "rel_mask", "=", "edge_type", "==", "rels", "+", "1", "\n", "out1", "+=", "self", ".", "conv1", "[", "rels", "]", "(", "x", ".", "float", "(", ")", ",", "edge_index", "[", ":", ",", "rel_mask", "]", ")", "\n", "\n", "", "out2", "=", "0", "\n", "out1", "=", "F", ".", "relu", "(", "out1", ")", "\n", "out1", "=", "F", ".", "dropout", "(", "out1", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "for", "rels", "in", "range", "(", "self", ".", "num_rels", ")", ":", "\n", "            ", "rel_mask", "=", "edge_type", "==", "rels", "+", "1", "\n", "out2", "+=", "self", ".", "conv2", "[", "rels", "]", "(", "out1", ".", "float", "(", ")", ",", "edge_index", "[", ":", ",", "rel_mask", "]", ")", "\n", "\n", "", "out", "=", "self", ".", "classifier", "(", "out2", ")", "\n", "return", "out", ",", "node_mask", ",", "out2", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.euclidean.Euclidean.__init__": [[11, 14], ["manifolds.base.Manifold.__init__"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "Euclidean", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "name", "=", "'Euclidean'", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.euclidean.Euclidean.normalize": [[15, 19], ["p.size", "p.view().renorm_", "p.view"], "methods", ["None"], ["", "def", "normalize", "(", "self", ",", "p", ")", ":", "\n", "        ", "dim", "=", "p", ".", "size", "(", "-", "1", ")", "\n", "p", ".", "view", "(", "-", "1", ",", "dim", ")", ".", "renorm_", "(", "2", ",", "0", ",", "1.", ")", "\n", "return", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.euclidean.Euclidean.sqdist": [[20, 22], ["None"], "methods", ["None"], ["", "def", "sqdist", "(", "self", ",", "p1", ",", "p2", ",", "c", ")", ":", "\n", "        ", "return", "(", "p1", "-", "p2", ")", ".", "pow", "(", "2", ")", ".", "sum", "(", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.euclidean.Euclidean.egrad2rgrad": [[23, 25], ["None"], "methods", ["None"], ["", "def", "egrad2rgrad", "(", "self", ",", "p", ",", "dp", ",", "c", ")", ":", "\n", "        ", "return", "dp", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.euclidean.Euclidean.proj": [[26, 28], ["None"], "methods", ["None"], ["", "def", "proj", "(", "self", ",", "p", ",", "c", ")", ":", "\n", "        ", "return", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.euclidean.Euclidean.proj_tan": [[29, 31], ["None"], "methods", ["None"], ["", "def", "proj_tan", "(", "self", ",", "u", ",", "p", ",", "c", ")", ":", "\n", "        ", "return", "u", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.euclidean.Euclidean.proj_tan0": [[32, 34], ["None"], "methods", ["None"], ["", "def", "proj_tan0", "(", "self", ",", "u", ",", "c", ")", ":", "\n", "        ", "return", "u", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.euclidean.Euclidean.expmap": [[35, 37], ["None"], "methods", ["None"], ["", "def", "expmap", "(", "self", ",", "u", ",", "p", ",", "c", ")", ":", "\n", "        ", "return", "p", "+", "u", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.euclidean.Euclidean.logmap": [[38, 40], ["None"], "methods", ["None"], ["", "def", "logmap", "(", "self", ",", "p1", ",", "p2", ",", "c", ")", ":", "\n", "        ", "return", "p2", "-", "p1", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.euclidean.Euclidean.expmap0": [[41, 43], ["None"], "methods", ["None"], ["", "def", "expmap0", "(", "self", ",", "u", ",", "c", ")", ":", "\n", "        ", "return", "u", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.euclidean.Euclidean.logmap0": [[44, 46], ["None"], "methods", ["None"], ["", "def", "logmap0", "(", "self", ",", "p", ",", "c", ")", ":", "\n", "        ", "return", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.euclidean.Euclidean.mobius_add": [[47, 49], ["None"], "methods", ["None"], ["", "def", "mobius_add", "(", "self", ",", "x", ",", "y", ",", "c", ",", "dim", "=", "-", "1", ")", ":", "\n", "        ", "return", "x", "+", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.euclidean.Euclidean.mobius_matvec": [[50, 53], ["m.transpose"], "methods", ["None"], ["", "def", "mobius_matvec", "(", "self", ",", "m", ",", "x", ",", "c", ")", ":", "\n", "        ", "mx", "=", "x", "@", "m", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "return", "mx", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.euclidean.Euclidean.init_weights": [[54, 57], ["w.data.uniform_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ",", "w", ",", "c", ",", "irange", "=", "1e-5", ")", ":", "\n", "        ", "w", ".", "data", ".", "uniform_", "(", "-", "irange", ",", "irange", ")", "\n", "return", "w", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.euclidean.Euclidean.inner": [[58, 62], ["None"], "methods", ["None"], ["", "def", "inner", "(", "self", ",", "p", ",", "c", ",", "u", ",", "v", "=", "None", ",", "keepdim", "=", "False", ")", ":", "\n", "        ", "if", "v", "is", "None", ":", "\n", "            ", "v", "=", "u", "\n", "", "return", "(", "u", "*", "v", ")", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "keepdim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.euclidean.Euclidean.ptransp": [[63, 65], ["None"], "methods", ["None"], ["", "def", "ptransp", "(", "self", ",", "x", ",", "y", ",", "v", ",", "c", ")", ":", "\n", "        ", "return", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.euclidean.Euclidean.ptransp0": [[66, 68], ["None"], "methods", ["None"], ["", "def", "ptransp0", "(", "self", ",", "x", ",", "v", ",", "c", ")", ":", "\n", "        ", "return", "x", "+", "v", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.Manifold.__init__": [[11, 14], ["object.__init__"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "eps", "=", "10e-8", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.Manifold.sqdist": [[15, 18], ["None"], "methods", ["None"], ["", "def", "sqdist", "(", "self", ",", "p1", ",", "p2", ",", "c", ")", ":", "\n", "        ", "\"\"\"Squared distance between pairs of points.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.Manifold.egrad2rgrad": [[19, 22], ["None"], "methods", ["None"], ["", "def", "egrad2rgrad", "(", "self", ",", "p", ",", "dp", ",", "c", ")", ":", "\n", "        ", "\"\"\"Converts Euclidean Gradient to Riemannian Gradients.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.Manifold.proj": [[23, 26], ["None"], "methods", ["None"], ["", "def", "proj", "(", "self", ",", "p", ",", "c", ")", ":", "\n", "        ", "\"\"\"Projects point p on the manifold.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.Manifold.proj_tan": [[27, 30], ["None"], "methods", ["None"], ["", "def", "proj_tan", "(", "self", ",", "u", ",", "p", ",", "c", ")", ":", "\n", "        ", "\"\"\"Projects u on the tangent space of p.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.Manifold.proj_tan0": [[31, 34], ["None"], "methods", ["None"], ["", "def", "proj_tan0", "(", "self", ",", "u", ",", "c", ")", ":", "\n", "        ", "\"\"\"Projects u on the tangent space of the origin.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.Manifold.expmap": [[35, 38], ["None"], "methods", ["None"], ["", "def", "expmap", "(", "self", ",", "u", ",", "p", ",", "c", ")", ":", "\n", "        ", "\"\"\"Exponential map of u at point p.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.Manifold.logmap": [[39, 42], ["None"], "methods", ["None"], ["", "def", "logmap", "(", "self", ",", "p1", ",", "p2", ",", "c", ")", ":", "\n", "        ", "\"\"\"Logarithmic map of point p1 at point p2.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.Manifold.expmap0": [[43, 46], ["None"], "methods", ["None"], ["", "def", "expmap0", "(", "self", ",", "u", ",", "c", ")", ":", "\n", "        ", "\"\"\"Exponential map of u at the origin.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.Manifold.logmap0": [[47, 50], ["None"], "methods", ["None"], ["", "def", "logmap0", "(", "self", ",", "p", ",", "c", ")", ":", "\n", "        ", "\"\"\"Logarithmic map of point p at the origin.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.Manifold.mobius_add": [[51, 54], ["None"], "methods", ["None"], ["", "def", "mobius_add", "(", "self", ",", "x", ",", "y", ",", "c", ",", "dim", "=", "-", "1", ")", ":", "\n", "        ", "\"\"\"Adds points x and y.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.Manifold.mobius_matvec": [[55, 58], ["None"], "methods", ["None"], ["", "def", "mobius_matvec", "(", "self", ",", "m", ",", "x", ",", "c", ")", ":", "\n", "        ", "\"\"\"Performs hyperboic martrix-vector multiplication.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.Manifold.init_weights": [[59, 62], ["None"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ",", "w", ",", "c", ",", "irange", "=", "1e-5", ")", ":", "\n", "        ", "\"\"\"Initializes random weigths on the manifold.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.Manifold.inner": [[63, 66], ["None"], "methods", ["None"], ["", "def", "inner", "(", "self", ",", "p", ",", "c", ",", "u", ",", "v", "=", "None", ",", "keepdim", "=", "False", ")", ":", "\n", "        ", "\"\"\"Inner product for tangent vectors at point x.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.Manifold.ptransp": [[67, 70], ["None"], "methods", ["None"], ["", "def", "ptransp", "(", "self", ",", "x", ",", "y", ",", "u", ",", "c", ")", ":", "\n", "        ", "\"\"\"Parallel transport of u from x to y.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.Manifold.ptransp0": [[71, 74], ["None"], "methods", ["None"], ["", "def", "ptransp0", "(", "self", ",", "x", ",", "u", ",", "c", ")", ":", "\n", "        ", "\"\"\"Parallel transport of u from the origin to y.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.ManifoldParameter.__new__": [[80, 82], ["torch.nn.Parameter.__new__"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.ManifoldParameter.__new__"], ["def", "__new__", "(", "cls", ",", "data", ",", "requires_grad", ",", "manifold", ",", "c", ")", ":", "\n", "        ", "return", "Parameter", ".", "__new__", "(", "cls", ",", "data", ",", "requires_grad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.ManifoldParameter.__init__": [[83, 86], ["None"], "methods", ["None"], ["", "def", "__init__", "(", "self", ",", "data", ",", "requires_grad", ",", "manifold", ",", "c", ")", ":", "\n", "        ", "self", ".", "c", "=", "c", "\n", "self", ".", "manifold", "=", "manifold", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.ManifoldParameter.__repr__": [[87, 89], ["torch.nn.Parameter.__repr__"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.base.ManifoldParameter.__repr__"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "'{} Parameter containing:\\n'", ".", "format", "(", "self", ".", "manifold", ".", "name", ")", "+", "super", "(", "Parameter", ",", "self", ")", ".", "__repr__", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.__init__": [[18, 24], ["manifolds.base.Manifold.__init__"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "Hyperboloid", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "name", "=", "'Hyperboloid'", "\n", "self", ".", "eps", "=", "{", "torch", ".", "float32", ":", "1e-7", ",", "torch", ".", "float64", ":", "1e-15", "}", "\n", "self", ".", "min_norm", "=", "1e-15", "\n", "self", ".", "max_norm", "=", "1e6", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.minkowski_dot": [[25, 30], ["torch.sum", "res.view.view.view"], "methods", ["None"], ["", "def", "minkowski_dot", "(", "self", ",", "x", ",", "y", ",", "keepdim", "=", "True", ")", ":", "\n", "        ", "res", "=", "torch", ".", "sum", "(", "x", "*", "y", ",", "dim", "=", "-", "1", ")", "-", "2", "*", "x", "[", "...", ",", "0", "]", "*", "y", "[", "...", ",", "0", "]", "\n", "if", "keepdim", ":", "\n", "            ", "res", "=", "res", ".", "view", "(", "res", ".", "shape", "+", "(", "1", ",", ")", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.minkowski_norm": [[31, 34], ["hyperboloid.Hyperboloid.minkowski_dot", "torch.sqrt", "torch.clamp"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.minkowski_dot"], ["", "def", "minkowski_norm", "(", "self", ",", "u", ",", "keepdim", "=", "True", ")", ":", "\n", "        ", "dot", "=", "self", ".", "minkowski_dot", "(", "u", ",", "u", ",", "keepdim", "=", "keepdim", ")", "\n", "return", "torch", ".", "sqrt", "(", "torch", ".", "clamp", "(", "dot", ",", "min", "=", "self", ".", "eps", "[", "u", ".", "dtype", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.sqdist": [[35, 43], ["hyperboloid.Hyperboloid.minkowski_dot", "torch.clamp", "torch.clamp", "utils.math_utils.arcosh"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.minkowski_dot", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.arcosh"], ["", "def", "sqdist", "(", "self", ",", "x", ",", "y", ",", "c", ")", ":", "\n", "# c = torch.tensor(c).to(device)", "\n", "        ", "K", "=", "1.", "/", "c", "\n", "prod", "=", "self", ".", "minkowski_dot", "(", "x", ",", "y", ")", "\n", "theta", "=", "torch", ".", "clamp", "(", "-", "prod", "/", "K", ",", "min", "=", "1.0", "+", "self", ".", "eps", "[", "x", ".", "dtype", "]", ")", "\n", "sqdist", "=", "K", "*", "arcosh", "(", "theta", ")", "**", "2", "\n", "# clamp distance to avoid nans in Fermi-Dirac decoder", "\n", "return", "torch", ".", "clamp", "(", "sqdist", ",", "max", "=", "50.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.proj": [[44, 55], ["x.narrow", "torch.ones_like", "torch.zeros_like", "torch.sqrt", "x.size", "torch.norm", "torch.clamp"], "methods", ["None"], ["", "def", "proj", "(", "self", ",", "x", ",", "c", ")", ":", "\n", "# c = torch.tensor(c).to(device)", "\n", "        ", "K", "=", "1.", "/", "c", "\n", "d", "=", "x", ".", "size", "(", "-", "1", ")", "-", "1", "\n", "y", "=", "x", ".", "narrow", "(", "-", "1", ",", "1", ",", "d", ")", "\n", "y_sqnorm", "=", "torch", ".", "norm", "(", "y", ",", "p", "=", "2", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "**", "2", "\n", "mask", "=", "torch", ".", "ones_like", "(", "x", ")", "\n", "mask", "[", ":", ",", "0", "]", "=", "0", "\n", "vals", "=", "torch", ".", "zeros_like", "(", "x", ")", "\n", "vals", "[", ":", ",", "0", ":", "1", "]", "=", "torch", ".", "sqrt", "(", "torch", ".", "clamp", "(", "K", "+", "y_sqnorm", ",", "min", "=", "self", ".", "eps", "[", "x", ".", "dtype", "]", ")", ")", "\n", "return", "vals", "+", "mask", "*", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.proj_tan": [[56, 66], ["torch.sum", "torch.ones_like", "torch.zeros_like", "x.size", "torch.clamp", "x.narrow", "u.narrow"], "methods", ["None"], ["", "def", "proj_tan", "(", "self", ",", "u", ",", "x", ",", "c", ")", ":", "\n", "# c = torch.tensor(c).to(device)", "\n", "        ", "K", "=", "1.", "/", "c", "\n", "d", "=", "x", ".", "size", "(", "1", ")", "-", "1", "\n", "ux", "=", "torch", ".", "sum", "(", "x", ".", "narrow", "(", "-", "1", ",", "1", ",", "d", ")", "*", "u", ".", "narrow", "(", "-", "1", ",", "1", ",", "d", ")", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "mask", "=", "torch", ".", "ones_like", "(", "u", ")", "\n", "mask", "[", ":", ",", "0", "]", "=", "0", "\n", "vals", "=", "torch", ".", "zeros_like", "(", "u", ")", "\n", "vals", "[", ":", ",", "0", ":", "1", "]", "=", "ux", "/", "torch", ".", "clamp", "(", "x", "[", ":", ",", "0", ":", "1", "]", ",", "min", "=", "self", ".", "eps", "[", "x", ".", "dtype", "]", ")", "\n", "return", "vals", "+", "mask", "*", "u", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.proj_tan0": [[67, 73], ["u.narrow", "torch.zeros_like"], "methods", ["None"], ["", "def", "proj_tan0", "(", "self", ",", "u", ",", "c", ")", ":", "\n", "# c = torch.tensor(c).to(device)", "\n", "        ", "narrowed", "=", "u", ".", "narrow", "(", "-", "1", ",", "0", ",", "1", ")", "\n", "vals", "=", "torch", ".", "zeros_like", "(", "u", ")", "\n", "vals", "[", ":", ",", "0", ":", "1", "]", "=", "narrowed", "\n", "return", "u", "-", "vals", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.expmap": [[74, 84], ["hyperboloid.Hyperboloid.minkowski_norm", "torch.clamp", "torch.clamp", "hyperboloid.Hyperboloid.proj", "utils.math_utils.cosh", "utils.math_utils.sinh"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.minkowski_norm", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.cosh", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.sinh"], ["", "def", "expmap", "(", "self", ",", "u", ",", "x", ",", "c", ")", ":", "\n", "# c = torch.tensor(c).to(device)", "\n", "        ", "K", "=", "1.", "/", "c", "\n", "sqrtK", "=", "K", "**", "0.5", "\n", "normu", "=", "self", ".", "minkowski_norm", "(", "u", ")", "\n", "normu", "=", "torch", ".", "clamp", "(", "normu", ",", "max", "=", "self", ".", "max_norm", ")", "\n", "theta", "=", "normu", "/", "sqrtK", "\n", "theta", "=", "torch", ".", "clamp", "(", "theta", ",", "min", "=", "self", ".", "min_norm", ")", "\n", "result", "=", "cosh", "(", "theta", ")", "*", "x", "+", "sinh", "(", "theta", ")", "*", "u", "/", "theta", "\n", "return", "self", ".", "proj", "(", "result", ",", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.logmap": [[85, 95], ["hyperboloid.Hyperboloid.minkowski_norm", "torch.clamp", "hyperboloid.Hyperboloid.proj_tan", "torch.clamp", "hyperboloid.Hyperboloid.sqdist", "hyperboloid.Hyperboloid.minkowski_dot"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.minkowski_norm", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj_tan", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.sqdist", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.minkowski_dot"], ["", "def", "logmap", "(", "self", ",", "x", ",", "y", ",", "c", ")", ":", "\n", "# c = torch.tensor(c).to(device)", "\n", "        ", "K", "=", "1.", "/", "c", "\n", "xy", "=", "torch", ".", "clamp", "(", "self", ".", "minkowski_dot", "(", "x", ",", "y", ")", "+", "K", ",", "max", "=", "-", "self", ".", "eps", "[", "x", ".", "dtype", "]", ")", "-", "K", "\n", "u", "=", "y", "+", "xy", "*", "x", "*", "c", "\n", "normu", "=", "self", ".", "minkowski_norm", "(", "u", ")", "\n", "normu", "=", "torch", ".", "clamp", "(", "normu", ",", "min", "=", "self", ".", "min_norm", ")", "\n", "dist", "=", "self", ".", "sqdist", "(", "x", ",", "y", ",", "c", ")", "**", "0.5", "\n", "result", "=", "dist", "*", "u", "/", "normu", "\n", "return", "self", ".", "proj_tan", "(", "result", ",", "x", ",", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.expmap0": [[96, 109], ["u.narrow().view", "torch.norm", "torch.clamp().to", "torch.ones_like", "hyperboloid.Hyperboloid.proj", "u.size", "utils.math_utils.cosh", "u.narrow", "torch.clamp", "utils.math_utils.sinh"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.cosh", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.sinh"], ["", "def", "expmap0", "(", "self", ",", "u", ",", "c", ")", ":", "\n", "# c = torch.tensor(c).to(device)", "\n", "        ", "K", "=", "1.", "/", "c", "\n", "sqrtK", "=", "K", "**", "0.5", "\n", "d", "=", "u", ".", "size", "(", "-", "1", ")", "-", "1", "\n", "x", "=", "u", ".", "narrow", "(", "-", "1", ",", "1", ",", "d", ")", ".", "view", "(", "-", "1", ",", "d", ")", "\n", "x_norm", "=", "torch", ".", "norm", "(", "x", ",", "p", "=", "2", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "x_norm", "=", "torch", ".", "clamp", "(", "x_norm", ",", "min", "=", "self", ".", "min_norm", ")", ".", "to", "(", "device", ")", "\n", "theta", "=", "x_norm", "/", "sqrtK", "\n", "res", "=", "torch", ".", "ones_like", "(", "u", ")", "\n", "res", "[", ":", ",", "0", ":", "1", "]", "=", "sqrtK", "*", "cosh", "(", "theta", ")", "\n", "res", "[", ":", ",", "1", ":", "]", "=", "sqrtK", "*", "sinh", "(", "theta", ")", "*", "x", "/", "x_norm", "\n", "return", "self", ".", "proj", "(", "res", ",", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.logmap0": [[110, 122], ["x.narrow().view", "torch.norm", "torch.clamp", "torch.zeros_like", "torch.clamp", "x.size", "x.narrow", "utils.math_utils.arcosh"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.arcosh"], ["", "def", "logmap0", "(", "self", ",", "x", ",", "c", ")", ":", "\n", "# c = torch.tensor(c).to(device)", "\n", "        ", "K", "=", "1.", "/", "c", "\n", "sqrtK", "=", "K", "**", "0.5", "\n", "d", "=", "x", ".", "size", "(", "-", "1", ")", "-", "1", "\n", "y", "=", "x", ".", "narrow", "(", "-", "1", ",", "1", ",", "d", ")", ".", "view", "(", "-", "1", ",", "d", ")", "\n", "y_norm", "=", "torch", ".", "norm", "(", "y", ",", "p", "=", "2", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "y_norm", "=", "torch", ".", "clamp", "(", "y_norm", ",", "min", "=", "self", ".", "min_norm", ")", "\n", "res", "=", "torch", ".", "zeros_like", "(", "x", ")", "\n", "theta", "=", "torch", ".", "clamp", "(", "x", "[", ":", ",", "0", ":", "1", "]", "/", "sqrtK", ",", "min", "=", "1.0", "+", "self", ".", "eps", "[", "x", ".", "dtype", "]", ")", "\n", "res", "[", ":", ",", "1", ":", "]", "=", "sqrtK", "*", "arcosh", "(", "theta", ")", "*", "y", "/", "y_norm", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.mobius_add": [[123, 128], ["hyperboloid.Hyperboloid.logmap0", "hyperboloid.Hyperboloid.ptransp0", "hyperboloid.Hyperboloid.expmap"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.logmap0", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.ptransp0", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.expmap"], ["", "def", "mobius_add", "(", "self", ",", "x", ",", "y", ",", "c", ")", ":", "\n", "# c = torch.tensor(c).to(device)", "\n", "        ", "u", "=", "self", ".", "logmap0", "(", "y", ",", "c", ")", "\n", "v", "=", "self", ".", "ptransp0", "(", "x", ",", "u", ",", "c", ")", "\n", "return", "self", ".", "expmap", "(", "v", ",", "x", ",", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.mobius_matvec": [[129, 134], ["hyperboloid.Hyperboloid.logmap0", "hyperboloid.Hyperboloid.expmap0", "m.to().transpose", "m.to"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.logmap0", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.expmap0"], ["", "def", "mobius_matvec", "(", "self", ",", "m", ",", "x", ",", "c", ")", ":", "\n", "# c = torch.tensor(c).to(device)", "\n", "        ", "u", "=", "self", ".", "logmap0", "(", "x", ",", "c", ")", "\n", "mu", "=", "u", "@", "m", ".", "to", "(", "device", ")", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "return", "self", ".", "expmap0", "(", "mu", ",", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.ptransp": [[135, 143], ["hyperboloid.Hyperboloid.logmap", "hyperboloid.Hyperboloid.logmap", "torch.clamp", "hyperboloid.Hyperboloid.proj_tan", "hyperboloid.Hyperboloid.sqdist", "hyperboloid.Hyperboloid.minkowski_dot"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.logmap", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.logmap", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj_tan", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.sqdist", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.minkowski_dot"], ["", "def", "ptransp", "(", "self", ",", "x", ",", "y", ",", "u", ",", "c", ")", ":", "\n", "# c = torch.tensor(c).to(device)", "\n", "        ", "logxy", "=", "self", ".", "logmap", "(", "x", ",", "y", ",", "c", ")", "\n", "logyx", "=", "self", ".", "logmap", "(", "y", ",", "x", ",", "c", ")", "\n", "sqdist", "=", "torch", ".", "clamp", "(", "self", ".", "sqdist", "(", "x", ",", "y", ",", "c", ")", ",", "min", "=", "self", ".", "min_norm", ")", "\n", "alpha", "=", "self", ".", "minkowski_dot", "(", "logxy", ",", "u", ")", "/", "sqdist", "\n", "res", "=", "u", "-", "alpha", "*", "(", "logxy", "+", "logyx", ")", "\n", "return", "self", ".", "proj_tan", "(", "res", ",", "y", ",", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.ptransp0": [[144, 159], ["x.narrow", "x.narrow", "torch.clamp", "torch.ones_like", "hyperboloid.Hyperboloid.proj_tan", "x.size", "torch.norm", "torch.sum"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj_tan"], ["", "def", "ptransp0", "(", "self", ",", "x", ",", "u", ",", "c", ")", ":", "\n", "# c = torch.tensor(c).to(device)", "\n", "        ", "K", "=", "1.", "/", "c", "\n", "sqrtK", "=", "K", "**", "0.5", "\n", "x0", "=", "x", ".", "narrow", "(", "-", "1", ",", "0", ",", "1", ")", "\n", "d", "=", "x", ".", "size", "(", "-", "1", ")", "-", "1", "\n", "y", "=", "x", ".", "narrow", "(", "-", "1", ",", "1", ",", "d", ")", "\n", "y_norm", "=", "torch", ".", "clamp", "(", "torch", ".", "norm", "(", "y", ",", "p", "=", "2", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", ",", "min", "=", "self", ".", "min_norm", ")", "\n", "y_normalized", "=", "y", "/", "y_norm", "\n", "v", "=", "torch", ".", "ones_like", "(", "x", ")", "\n", "v", "[", ":", ",", "0", ":", "1", "]", "=", "-", "y_norm", "\n", "v", "[", ":", ",", "1", ":", "]", "=", "(", "sqrtK", "-", "x0", ")", "*", "y_normalized", "\n", "alpha", "=", "torch", ".", "sum", "(", "y_normalized", "*", "u", "[", ":", ",", "1", ":", "]", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "/", "sqrtK", "\n", "res", "=", "u", "-", "alpha", "*", "v", "\n", "return", "self", ".", "proj_tan", "(", "res", ",", "x", ",", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.hyperboloid.Hyperboloid.to_poincare": [[160, 166], ["x.size", "x.narrow"], "methods", ["None"], ["", "def", "to_poincare", "(", "self", ",", "x", ",", "c", ")", ":", "\n", "# c = torch.tensor(c).to(device)", "\n", "        ", "K", "=", "1.", "/", "c", "\n", "sqrtK", "=", "K", "**", "0.5", "\n", "d", "=", "x", ".", "size", "(", "-", "1", ")", "-", "1", "\n", "return", "sqrtK", "*", "x", ".", "narrow", "(", "-", "1", ",", "1", ",", "d", ")", "/", "(", "x", "[", ":", ",", "0", ":", "1", "]", "+", "sqrtK", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.__init__": [[19, 24], ["manifolds.base.Manifold.__init__"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__"], ["def", "__init__", "(", "self", ",", ")", ":", "\n", "        ", "super", "(", "PoincareBall", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "name", "=", "'PoincareBall'", "\n", "self", ".", "min_norm", "=", "1e-15", "\n", "self", ".", "eps", "=", "{", "torch", ".", "float32", ":", "4e-3", ",", "torch", ".", "float64", ":", "1e-5", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.sqdist": [[25, 32], ["utils.math_utils.artanh", "poincare.PoincareBall.mobius_add().norm", "poincare.PoincareBall.mobius_add"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.artanh", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.mobius_add"], ["", "def", "sqdist", "(", "self", ",", "p1", ",", "p2", ",", "c", ")", ":", "\n", "        ", "sqrt_c", "=", "c", "**", "0.5", "\n", "dist_c", "=", "artanh", "(", "\n", "sqrt_c", "*", "self", ".", "mobius_add", "(", "-", "p1", ",", "p2", ",", "c", ",", "dim", "=", "-", "1", ")", ".", "norm", "(", "dim", "=", "-", "1", ",", "p", "=", "2", ",", "keepdim", "=", "False", ")", "\n", ")", "\n", "dist", "=", "dist_c", "*", "2", "/", "sqrt_c", "\n", "return", "dist", "**", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall._lambda_x": [[33, 36], ["torch.sum", "x.data.pow"], "methods", ["None"], ["", "def", "_lambda_x", "(", "self", ",", "x", ",", "c", ")", ":", "\n", "        ", "x_sqnorm", "=", "torch", ".", "sum", "(", "x", ".", "data", ".", "pow", "(", "2", ")", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "return", "2", "/", "(", "1.", "-", "c", "*", "x_sqnorm", ")", ".", "clamp_min", "(", "self", ".", "min_norm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.egrad2rgrad": [[37, 41], ["poincare.PoincareBall._lambda_x", "poincare.PoincareBall.pow"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall._lambda_x"], ["", "def", "egrad2rgrad", "(", "self", ",", "p", ",", "dp", ",", "c", ")", ":", "\n", "        ", "lambda_p", "=", "self", ".", "_lambda_x", "(", "p", ",", "c", ")", "\n", "dp", "/=", "lambda_p", ".", "pow", "(", "2", ")", "\n", "return", "dp", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj": [[42, 48], ["torch.clamp_min", "torch.where", "x.norm"], "methods", ["None"], ["", "def", "proj", "(", "self", ",", "x", ",", "c", ")", ":", "\n", "        ", "norm", "=", "torch", ".", "clamp_min", "(", "x", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ",", "p", "=", "2", ")", ",", "self", ".", "min_norm", ")", "\n", "maxnorm", "=", "(", "1", "-", "self", ".", "eps", "[", "x", ".", "dtype", "]", ")", "/", "(", "c", "**", "0.5", ")", "\n", "cond", "=", "norm", ">", "maxnorm", "\n", "projected", "=", "x", "/", "norm", "*", "maxnorm", "\n", "return", "torch", ".", "where", "(", "cond", ",", "projected", ",", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj_tan": [[49, 51], ["None"], "methods", ["None"], ["", "def", "proj_tan", "(", "self", ",", "u", ",", "p", ",", "c", ")", ":", "\n", "        ", "return", "u", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.proj_tan0": [[52, 54], ["None"], "methods", ["None"], ["", "def", "proj_tan0", "(", "self", ",", "u", ",", "c", ")", ":", "\n", "        ", "return", "u", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.expmap": [[55, 65], ["u.norm().clamp_min", "poincare.PoincareBall.mobius_add", "u.norm", "utils.math_utils.tanh", "poincare.PoincareBall._lambda_x"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.mobius_add", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.tanh", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall._lambda_x"], ["", "def", "expmap", "(", "self", ",", "u", ",", "p", ",", "c", ")", ":", "\n", "        ", "sqrt_c", "=", "c", "**", "0.5", "\n", "u_norm", "=", "u", ".", "norm", "(", "dim", "=", "-", "1", ",", "p", "=", "2", ",", "keepdim", "=", "True", ")", ".", "clamp_min", "(", "self", ".", "min_norm", ")", "\n", "second_term", "=", "(", "\n", "tanh", "(", "sqrt_c", "/", "2", "*", "self", ".", "_lambda_x", "(", "p", ",", "c", ")", "*", "u_norm", ")", "\n", "*", "u", "\n", "/", "(", "sqrt_c", "*", "u_norm", ")", "\n", ")", "\n", "gamma_1", "=", "self", ".", "mobius_add", "(", "p", ",", "second_term", ",", "c", ")", "\n", "return", "gamma_1", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.logmap": [[66, 72], ["poincare.PoincareBall.mobius_add", "poincare.PoincareBall.norm().clamp_min", "poincare.PoincareBall._lambda_x", "poincare.PoincareBall.norm", "utils.math_utils.artanh"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.mobius_add", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall._lambda_x", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.artanh"], ["", "def", "logmap", "(", "self", ",", "p1", ",", "p2", ",", "c", ")", ":", "\n", "        ", "sub", "=", "self", ".", "mobius_add", "(", "-", "p1", ",", "p2", ",", "c", ")", "\n", "sub_norm", "=", "sub", ".", "norm", "(", "dim", "=", "-", "1", ",", "p", "=", "2", ",", "keepdim", "=", "True", ")", ".", "clamp_min", "(", "self", ".", "min_norm", ")", "\n", "lam", "=", "self", ".", "_lambda_x", "(", "p1", ",", "c", ")", "\n", "sqrt_c", "=", "c", "**", "0.5", "\n", "return", "2", "/", "sqrt_c", "/", "lam", "*", "artanh", "(", "sqrt_c", "*", "sub_norm", ")", "*", "sub", "/", "sub_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.expmap0": [[73, 78], ["torch.clamp_min", "u.norm", "utils.math_utils.tanh"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.tanh"], ["", "def", "expmap0", "(", "self", ",", "u", ",", "c", ")", ":", "\n", "        ", "sqrt_c", "=", "c", "**", "0.5", "\n", "u_norm", "=", "torch", ".", "clamp_min", "(", "u", ".", "norm", "(", "dim", "=", "-", "1", ",", "p", "=", "2", ",", "keepdim", "=", "True", ")", ",", "self", ".", "min_norm", ")", "\n", "gamma_1", "=", "tanh", "(", "sqrt_c", "*", "u_norm", ")", "*", "u", "/", "(", "sqrt_c", "*", "u_norm", ")", "\n", "return", "gamma_1", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.logmap0": [[79, 84], ["p.norm().clamp_min", "p.norm", "utils.math_utils.artanh"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.artanh"], ["", "def", "logmap0", "(", "self", ",", "p", ",", "c", ")", ":", "\n", "        ", "sqrt_c", "=", "c", "**", "0.5", "\n", "p_norm", "=", "p", ".", "norm", "(", "dim", "=", "-", "1", ",", "p", "=", "2", ",", "keepdim", "=", "True", ")", ".", "clamp_min", "(", "self", ".", "min_norm", ")", "\n", "scale", "=", "1.", "/", "sqrt_c", "*", "artanh", "(", "sqrt_c", "*", "p_norm", ")", "/", "p_norm", "\n", "return", "scale", "*", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.mobius_add": [[85, 92], ["x.pow().sum", "y.pow().sum", "denom.clamp_min", "x.pow", "y.pow"], "methods", ["None"], ["", "def", "mobius_add", "(", "self", ",", "x", ",", "y", ",", "c", ",", "dim", "=", "-", "1", ")", ":", "\n", "        ", "x2", "=", "x", ".", "pow", "(", "2", ")", ".", "sum", "(", "dim", "=", "dim", ",", "keepdim", "=", "True", ")", "\n", "y2", "=", "y", ".", "pow", "(", "2", ")", ".", "sum", "(", "dim", "=", "dim", ",", "keepdim", "=", "True", ")", "\n", "xy", "=", "(", "x", "*", "y", ")", ".", "sum", "(", "dim", "=", "dim", ",", "keepdim", "=", "True", ")", "\n", "num", "=", "(", "1", "+", "2", "*", "c", "*", "xy", "+", "c", "*", "y2", ")", "*", "x", "+", "(", "1", "-", "c", "*", "x2", ")", "*", "y", "\n", "denom", "=", "1", "+", "2", "*", "c", "*", "xy", "+", "c", "**", "2", "*", "x2", "*", "y2", "\n", "return", "num", "/", "denom", ".", "clamp_min", "(", "self", ".", "min_norm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.mobius_matvec": [[93, 103], ["x.norm().clamp_min", "mx.norm().clamp_min", "torch.zeros", "torch.where", "m.transpose", "x.norm", "mx.norm", "utils.math_utils.tanh", "utils.math_utils.artanh"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.tanh", "home.repos.pwc.inspect_result.shaanchandra_SAFER.utils.math_utils.artanh"], ["", "def", "mobius_matvec", "(", "self", ",", "m", ",", "x", ",", "c", ")", ":", "\n", "        ", "sqrt_c", "=", "c", "**", "0.5", "\n", "x_norm", "=", "x", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ",", "p", "=", "2", ")", ".", "clamp_min", "(", "self", ".", "min_norm", ")", "\n", "mx", "=", "x", "@", "m", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "mx_norm", "=", "mx", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ",", "p", "=", "2", ")", ".", "clamp_min", "(", "self", ".", "min_norm", ")", "\n", "res_c", "=", "tanh", "(", "mx_norm", "/", "x_norm", "*", "artanh", "(", "sqrt_c", "*", "x_norm", ")", ")", "*", "mx", "/", "(", "mx_norm", "*", "sqrt_c", ")", "\n", "cond", "=", "(", "mx", "==", "0", ")", ".", "prod", "(", "-", "1", ",", "keepdim", "=", "True", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "res_0", "=", "torch", ".", "zeros", "(", "1", ",", "dtype", "=", "res_c", ".", "dtype", ",", "device", "=", "res_c", ".", "device", ")", "\n", "res", "=", "torch", ".", "where", "(", "cond", ",", "res_0", ",", "res_c", ")", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.init_weights": [[104, 107], ["w.data.uniform_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ",", "w", ",", "c", ",", "irange", "=", "1e-5", ")", ":", "\n", "        ", "w", ".", "data", ".", "uniform_", "(", "-", "irange", ",", "irange", ")", "\n", "return", "w", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall._gyration": [[108, 119], ["u.pow().sum", "v.pow().sum", "u.pow", "v.pow", "d.clamp_min"], "methods", ["None"], ["", "def", "_gyration", "(", "self", ",", "u", ",", "v", ",", "w", ",", "c", ",", "dim", ":", "int", "=", "-", "1", ")", ":", "\n", "        ", "u2", "=", "u", ".", "pow", "(", "2", ")", ".", "sum", "(", "dim", "=", "dim", ",", "keepdim", "=", "True", ")", "\n", "v2", "=", "v", ".", "pow", "(", "2", ")", ".", "sum", "(", "dim", "=", "dim", ",", "keepdim", "=", "True", ")", "\n", "uv", "=", "(", "u", "*", "v", ")", ".", "sum", "(", "dim", "=", "dim", ",", "keepdim", "=", "True", ")", "\n", "uw", "=", "(", "u", "*", "w", ")", ".", "sum", "(", "dim", "=", "dim", ",", "keepdim", "=", "True", ")", "\n", "vw", "=", "(", "v", "*", "w", ")", ".", "sum", "(", "dim", "=", "dim", ",", "keepdim", "=", "True", ")", "\n", "c2", "=", "c", "**", "2", "\n", "a", "=", "-", "c2", "*", "uw", "*", "v2", "+", "c", "*", "vw", "+", "2", "*", "c2", "*", "uv", "*", "vw", "\n", "b", "=", "-", "c2", "*", "vw", "*", "u2", "-", "c", "*", "uw", "\n", "d", "=", "1", "+", "2", "*", "c", "*", "uv", "+", "c2", "*", "u2", "*", "v2", "\n", "return", "w", "+", "2", "*", "(", "a", "*", "u", "+", "b", "*", "v", ")", "/", "d", ".", "clamp_min", "(", "self", ".", "min_norm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.inner": [[120, 125], ["poincare.PoincareBall._lambda_x"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall._lambda_x"], ["", "def", "inner", "(", "self", ",", "x", ",", "c", ",", "u", ",", "v", "=", "None", ",", "keepdim", "=", "False", ")", ":", "\n", "        ", "if", "v", "is", "None", ":", "\n", "            ", "v", "=", "u", "\n", "", "lambda_x", "=", "self", ".", "_lambda_x", "(", "x", ",", "c", ")", "\n", "return", "lambda_x", "**", "2", "*", "(", "u", "*", "v", ")", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "keepdim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.ptransp": [[126, 130], ["poincare.PoincareBall._lambda_x", "poincare.PoincareBall._lambda_x", "poincare.PoincareBall._gyration"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall._lambda_x", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall._lambda_x", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall._gyration"], ["", "def", "ptransp", "(", "self", ",", "x", ",", "y", ",", "u", ",", "c", ")", ":", "\n", "        ", "lambda_x", "=", "self", ".", "_lambda_x", "(", "x", ",", "c", ")", "\n", "lambda_y", "=", "self", ".", "_lambda_x", "(", "y", ",", "c", ")", "\n", "return", "self", ".", "_gyration", "(", "y", ",", "-", "x", ",", "u", ",", "c", ")", "*", "lambda_x", "/", "lambda_y", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.ptransp_": [[131, 135], ["poincare.PoincareBall._lambda_x", "poincare.PoincareBall._lambda_x", "poincare.PoincareBall._gyration"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall._lambda_x", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall._lambda_x", "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall._gyration"], ["", "def", "ptransp_", "(", "self", ",", "x", ",", "y", ",", "u", ",", "c", ")", ":", "\n", "        ", "lambda_x", "=", "self", ".", "_lambda_x", "(", "x", ",", "c", ")", "\n", "lambda_y", "=", "self", ".", "_lambda_x", "(", "y", ",", "c", ")", "\n", "return", "self", ".", "_gyration", "(", "y", ",", "-", "x", ",", "u", ",", "c", ")", "*", "lambda_x", "/", "lambda_y", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.ptransp0": [[136, 139], ["poincare.PoincareBall._lambda_x", "poincare.PoincareBall.clamp_min"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall._lambda_x"], ["", "def", "ptransp0", "(", "self", ",", "x", ",", "u", ",", "c", ")", ":", "\n", "        ", "lambda_x", "=", "self", ".", "_lambda_x", "(", "x", ",", "c", ")", "\n", "return", "2", "*", "u", "/", "lambda_x", ".", "clamp_min", "(", "self", ".", "min_norm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.manifolds.poincare.PoincareBall.to_hyperboloid": [[140, 145], ["torch.norm", "torch.cat"], "methods", ["None"], ["", "def", "to_hyperboloid", "(", "self", ",", "x", ",", "c", ")", ":", "\n", "        ", "K", "=", "1.", "/", "c", "\n", "sqrtK", "=", "K", "**", "0.5", "\n", "sqnorm", "=", "torch", ".", "norm", "(", "x", ",", "p", "=", "2", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "**", "2", "\n", "return", "sqrtK", "*", "torch", ".", "cat", "(", "[", "K", "+", "sqnorm", ",", "2", "*", "sqrtK", "*", "x", "]", ",", "dim", "=", "1", ")", "/", "(", "K", "-", "sqnorm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.data_prep_fakehealth.prepare_data_corpus": [[13, 56], ["print", "print", "os.path.join", "json.load", "enumerate", "print", "os.path.join", "print", "json.dump", "print", "os.path.join", "json.load", "os.path.join", "os.path.join", "glob.glob", "print", "open", "open", "open", "open", "csv.writer", "csv.writer.writerow", "str", "open", "json.load", "csv.writer.writerow", "file.split", "ID.split", "file_content[].replace", "str"], "function", ["None"], ["def", "prepare_data_corpus", "(", "base_dir", "=", "'../FakeHealth'", ")", ":", "\n", "    ", "\"\"\"\n    This method reads all the individual JSON files of both the datasets and creates separate .tsv files for each.\n    The .tsv file contains the fields: ID, article title, article content and the label\n    \"\"\"", "\n", "\n", "print", "(", "\"\\n\\n\"", "+", "\"=\"", "*", "50", "+", "\"\\n\\t\\tPreparing Data Corpus\\n\"", "+", "\"=\"", "*", "50", ")", "\n", "\n", "datasets", "=", "[", "'HealthRelease'", ",", "'HealthStory'", "]", "\n", "for", "dataset", "in", "datasets", ":", "\n", "        ", "doc2labels", "=", "{", "}", "\n", "print", "(", "\"\\nCreating doc2labels for:  \"", ",", "dataset", ")", "\n", "src_dir", "=", "os", ".", "path", ".", "join", "(", "base_dir", ",", "'reviews'", ",", "dataset", "+", "'.json'", ")", "\n", "doc_labels", "=", "json", ".", "load", "(", "open", "(", "src_dir", ",", "'r'", ")", ")", "\n", "for", "count", ",", "doc", "in", "enumerate", "(", "doc_labels", ")", ":", "\n", "            ", "label", "=", "1", "if", "doc", "[", "'rating'", "]", "<", "3", "else", "0", "# rating less than 3 is fake", "\n", "doc2labels", "[", "str", "(", "doc", "[", "'news_id'", "]", ")", "]", "=", "label", "\n", "\n", "", "print", "(", "\"Total docs : \"", ",", "count", ")", "\n", "doc2labels_file", "=", "os", ".", "path", ".", "join", "(", "base_dir", ",", "'doc2labels_{}.json'", ".", "format", "(", "dataset", ")", ")", "\n", "print", "(", "\"\\nWriting doc2labels file in :  \"", ",", "doc2labels_file", ")", "\n", "json", ".", "dump", "(", "doc2labels", ",", "open", "(", "doc2labels_file", ",", "'w+'", ")", ")", "\n", "\n", "", "for", "dataset", "in", "datasets", ":", "\n", "        ", "print", "(", "\"\\nCreating the data corpus file for :  \"", ",", "dataset", ")", "\n", "doc2labels_file", "=", "os", ".", "path", ".", "join", "(", "base_dir", ",", "'doc2labels_{}.json'", ".", "format", "(", "dataset", ")", ")", "\n", "doc2labels", "=", "json", ".", "load", "(", "open", "(", "doc2labels_file", ",", "'r'", ")", ")", "\n", "src_dir", "=", "os", ".", "path", ".", "join", "(", "base_dir", ",", "'content'", ",", "dataset", "+", "\"/*.json\"", ")", "\n", "final_data_file", "=", "os", ".", "path", ".", "join", "(", "'data'", ",", "dataset", "+", "'.tsv'", ")", "\n", "all_files", "=", "glob", ".", "glob", "(", "src_dir", ")", "\n", "with", "open", "(", "final_data_file", ",", "'a'", ",", "encoding", "=", "'utf-8'", ",", "newline", "=", "''", ")", "as", "csv_file", ":", "\n", "            ", "csv_writer", "=", "csv", ".", "writer", "(", "csv_file", ",", "delimiter", "=", "'\\t'", ")", "\n", "csv_writer", ".", "writerow", "(", "[", "'id'", ",", "'title'", ",", "'text'", ",", "'label'", "]", ")", "\n", "for", "file", "in", "all_files", ":", "\n", "                ", "with", "open", "(", "file", ",", "'r'", ")", "as", "f", ":", "\n", "                    ", "file_content", "=", "json", ".", "load", "(", "f", ")", "\n", "ID", "=", "file", ".", "split", "(", "'\\\\'", ")", "[", "-", "1", "]", "\n", "ID", "=", "ID", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "csv_writer", ".", "writerow", "(", "[", "ID", ",", "file_content", "[", "'title'", "]", ",", "file_content", "[", "'text'", "]", ".", "replace", "(", "'\\n'", ",", "\" \"", ")", ",", "doc2labels", "[", "str", "(", "ID", ")", "]", "]", ")", "\n", "\n", "", "", "", "print", "(", "\"Final file written in :  \"", ",", "final_data_file", ")", "\n", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.data_prep_fakehealth.create_data_splits_standard": [[58, 170], ["os.path.join", "print", "print", "os.path.join", "print", "print", "print", "print", "sklearn.model_selection.StratifiedShuffleSplit", "sklearn.model_selection.StratifiedShuffleSplit.split", "sklearn.model_selection.StratifiedShuffleSplit", "enumerate", "data_prep_fakehealth.get_label_distribution", "print", "print", "data_prep_fakehealth.get_label_distribution", "print", "print", "data_prep_fakehealth.get_label_distribution", "print", "print", "print", "os.path.join", "print", "json.dump", "open", "csv.DictReader", "max", "min", "sklearn.model_selection.StratifiedShuffleSplit.split", "os.path.join", "os.path.join", "print", "open", "sum", "len", "x_rest.append", "y_rest.append", "doc_rest.append", "x_test.append", "y_test.append", "doc_id_test.append", "x_train.append", "y_train.append", "doc_id_train.append", "x_val.append", "y_val.append", "doc_id_val.append", "os.path.exists", "os.makedirs", "open", "csv.writer", "range", "isinstance", "row[].replace", "re.sub.replace", "re.sub", "re.sub", "x_data.append", "lens.append", "y_data.append", "doc_data.append", "len", "csv.writer.writerow", "len", "str", "len", "int", "str"], "function", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.data_prep_fakenews.get_label_distribution", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.data_prep_fakenews.get_label_distribution", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.data_prep_fakenews.get_label_distribution"], ["", "def", "create_data_splits_standard", "(", "base_dir", "=", "os", ".", "path", ".", "join", "(", "'../FakeHealth'", ")", ")", ":", "\n", "    ", "\"\"\"\n    This method creates train-val-test via random splitting of the dataset in a stratified fashion to ensure similar data distribution\n    \"\"\"", "\n", "print", "(", "\"\\n\\n\"", "+", "\"=\"", "*", "50", "+", "\"\\n\\t\\tCreating Data Splits\\n\"", "+", "\"=\"", "*", "50", ")", "\n", "\n", "datasets", "=", "[", "'HealthRelease'", ",", "'HealthStory'", "]", "\n", "for", "dataset", "in", "datasets", ":", "\n", "        ", "print", "(", "\"\\nPreparing {} ...\"", ".", "format", "(", "dataset", ")", ")", "\n", "src_dir", "=", "os", ".", "path", ".", "join", "(", "'data'", ",", "dataset", "+", "'.tsv'", ")", "\n", "x_data", ",", "y_data", ",", "doc_data", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "# Reading the dataset into workable lists", "\n", "removed", "=", "0", "\n", "lens", "=", "[", "]", "\n", "with", "open", "(", "src_dir", ",", "encoding", "=", "'utf-8'", ")", "as", "data", ":", "\n", "            ", "reader", "=", "csv", ".", "DictReader", "(", "data", ",", "delimiter", "=", "'\\t'", ")", "\n", "for", "row", "in", "reader", ":", "\n", "                ", "if", "isinstance", "(", "row", "[", "'text'", "]", ",", "str", ")", "and", "len", "(", "row", "[", "'text'", "]", ")", ">", "5", ":", "\n", "                    ", "text", "=", "row", "[", "'text'", "]", ".", "replace", "(", "'\\n'", ",", "' '", ")", "\n", "text", "=", "text", ".", "replace", "(", "'\\t'", ",", "' '", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'#[\\w-]+'", ",", "'hashtag'", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'https?://\\S+'", ",", "'url'", ",", "text", ")", "\n", "\n", "x_data", ".", "append", "(", "str", "(", "text", "[", ":", "5000", "]", ")", ")", "\n", "lens", ".", "append", "(", "len", "(", "text", "[", ":", "5000", "]", ")", ")", "\n", "y_data", ".", "append", "(", "int", "(", "row", "[", "'label'", "]", ")", ")", "\n", "doc_data", ".", "append", "(", "str", "(", "row", "[", "'id'", "]", ")", ")", "\n", "", "else", ":", "\n", "                    ", "removed", "+=", "1", "\n", "", "", "", "print", "(", "\"avg lens = \"", ",", "sum", "(", "lens", ")", "/", "len", "(", "lens", ")", ")", "\n", "print", "(", "\"max lens = \"", ",", "max", "(", "lens", ")", ")", "\n", "print", "(", "\"minimum lens = \"", ",", "min", "(", "lens", ")", ")", "\n", "print", "(", "\"Total data points removed = \"", ",", "removed", ")", "\n", "\n", "# Creating train-val-test split with same/similar label distribution in each split", "\n", "sss", "=", "StratifiedShuffleSplit", "(", "n_splits", "=", "1", ",", "test_size", "=", "0.20", ",", "random_state", "=", "21", ")", "\n", "x_rest", ",", "x_test", ",", "y_rest", ",", "y_test", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "doc_rest", ",", "doc_id_test", "=", "[", "]", ",", "[", "]", "\n", "for", "train_index", ",", "test_index", "in", "sss", ".", "split", "(", "x_data", ",", "y_data", ")", ":", "\n", "            ", "for", "idx", "in", "train_index", ":", "\n", "                ", "x_rest", ".", "append", "(", "x_data", "[", "idx", "]", ")", "\n", "y_rest", ".", "append", "(", "y_data", "[", "idx", "]", ")", "\n", "doc_rest", ".", "append", "(", "doc_data", "[", "idx", "]", ")", "\n", "\n", "", "for", "idx", "in", "test_index", ":", "\n", "                ", "x_test", ".", "append", "(", "x_data", "[", "idx", "]", ")", "\n", "y_test", ".", "append", "(", "y_data", "[", "idx", "]", ")", "\n", "doc_id_test", ".", "append", "(", "doc_data", "[", "idx", "]", ")", "\n", "\n", "\n", "", "", "sss", "=", "StratifiedShuffleSplit", "(", "n_splits", "=", "1", ",", "test_size", "=", "0.10", ",", "random_state", "=", "21", ")", "\n", "for", "fold", ",", "(", "train_index", ",", "val_index", ")", "in", "enumerate", "(", "sss", ".", "split", "(", "x_rest", ",", "y_rest", ")", ")", ":", "\n", "            ", "x_train", ",", "x_val", ",", "y_train", ",", "y_val", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "doc_id_train", ",", "doc_id_val", "=", "[", "]", ",", "[", "]", "\n", "for", "idx", "in", "train_index", ":", "\n", "                ", "x_train", ".", "append", "(", "x_rest", "[", "idx", "]", ")", "\n", "y_train", ".", "append", "(", "y_rest", "[", "idx", "]", ")", "\n", "doc_id_train", ".", "append", "(", "doc_rest", "[", "idx", "]", ")", "\n", "", "for", "idx", "in", "val_index", ":", "\n", "                ", "x_val", ".", "append", "(", "x_rest", "[", "idx", "]", ")", "\n", "y_val", ".", "append", "(", "y_rest", "[", "idx", "]", ")", "\n", "doc_id_val", ".", "append", "(", "doc_rest", "[", "idx", "]", ")", "\n", "\n", "\n", "", "", "fake", ",", "real", "=", "get_label_distribution", "(", "y_train", ")", "\n", "print", "(", "\"\\nFake labels in train split  = {:.2f} %\"", ".", "format", "(", "fake", "*", "100", ")", ")", "\n", "print", "(", "\"Real labels in train split  = {:.2f} %\"", ".", "format", "(", "real", "*", "100", ")", ")", "\n", "\n", "fake", ",", "real", "=", "get_label_distribution", "(", "y_val", ")", "\n", "print", "(", "\"\\nFake labels in val split  = {:.2f} %\"", ".", "format", "(", "fake", "*", "100", ")", ")", "\n", "print", "(", "\"Real labels in val split  = {:.2f} %\"", ".", "format", "(", "real", "*", "100", ")", ")", "\n", "\n", "fake", ",", "real", "=", "get_label_distribution", "(", "y_test", ")", "\n", "print", "(", "\"\\nFake labels in test split = {:.2f} %\"", ".", "format", "(", "fake", "*", "100", ")", ")", "\n", "print", "(", "\"Real labels in test split  = {:.2f} %\"", ".", "format", "(", "real", "*", "100", ")", ")", "\n", "\n", "print", "(", "\"\\nWriting train-val-test files..\"", ")", "\n", "splits", "=", "[", "'train'", ",", "'val'", ",", "'test'", "]", "\n", "for", "split", "in", "splits", ":", "\n", "            ", "if", "split", "==", "'train'", ":", "\n", "                ", "x", "=", "x_train", "\n", "y", "=", "y_train", "\n", "id_list", "=", "doc_id_train", "\n", "", "elif", "split", "==", "'val'", ":", "\n", "                ", "x", "=", "x_val", "\n", "y", "=", "y_val", "\n", "id_list", "=", "doc_id_val", "\n", "", "else", ":", "\n", "                ", "x", "=", "x_test", "\n", "y", "=", "y_test", "\n", "id_list", "=", "doc_id_test", "\n", "\n", "", "write_dir", "=", "os", ".", "path", ".", "join", "(", "'data'", ",", "dataset", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "write_dir", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "write_dir", ")", "\n", "", "write_dir", "=", "os", ".", "path", ".", "join", "(", "write_dir", ",", "split", "+", "'.tsv'", ")", "\n", "print", "(", "\"{} file in : {}\"", ".", "format", "(", "split", ",", "write_dir", ")", ")", "\n", "with", "open", "(", "write_dir", ",", "'a'", ",", "encoding", "=", "'utf-8'", ",", "newline", "=", "''", ")", "as", "csv_file", ":", "\n", "                ", "csv_writer", "=", "csv", ".", "writer", "(", "csv_file", ",", "delimiter", "=", "'\\t'", ")", "\n", "# csv_writer.writerow(['text', 'label'])", "\n", "for", "i", "in", "range", "(", "len", "(", "x", ")", ")", ":", "\n", "                    ", "csv_writer", ".", "writerow", "(", "[", "x", "[", "i", "]", ",", "y", "[", "i", "]", ",", "id_list", "[", "i", "]", "]", ")", "\n", "\n", "\n", "", "", "", "temp_dict", "=", "{", "}", "\n", "temp_dict", "[", "'test_docs'", "]", "=", "doc_id_test", "\n", "temp_dict", "[", "'train_docs'", "]", "=", "doc_id_train", "\n", "temp_dict", "[", "'val_docs'", "]", "=", "doc_id_val", "\n", "doc_splits_file", "=", "os", ".", "path", ".", "join", "(", "base_dir", ",", "'doc_splits_{}.json'", ".", "format", "(", "dataset", ")", ")", "\n", "print", "(", "\"Writing doc_splits in : \"", ",", "doc_splits_file", ")", "\n", "json", ".", "dump", "(", "temp_dict", ",", "open", "(", "doc_splits_file", ",", "'w+'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.data_prep_fakehealth.create_data_splits_cv": [[176, 287], ["print", "print", "os.path.join", "json.load", "os.path.join", "print", "print", "print", "print", "sklearn.model_selection.StratifiedShuffleSplit", "sklearn.model_selection.StratifiedShuffleSplit.split", "sklearn.model_selection.StratifiedShuffleSplit", "enumerate", "open", "open", "csv.DictReader", "max", "min", "sklearn.model_selection.StratifiedShuffleSplit.split", "data_prep_fakehealth.get_label_distribution", "print", "print", "data_prep_fakehealth.get_label_distribution", "print", "print", "data_prep_fakehealth.get_label_distribution", "print", "print", "print", "os.path.join", "json.dump", "sum", "len", "x_rest.append", "y_rest.append", "doc_rest.append", "x_test.append", "y_test.append", "doc_id_test.append", "x_train.append", "y_train.append", "doc_id_train.append", "x_val.append", "y_val.append", "doc_id_val.append", "os.path.join", "os.path.join", "print", "open", "isinstance", "row[].replace", "re.sub.replace", "re.sub", "re.sub", "x_data.append", "lens.append", "y_data.append", "doc_data.append", "os.path.exists", "os.makedirs", "open", "csv.writer", "range", "len", "re.sub.lower", "str", "len", "int", "str", "len", "csv.writer.writerow"], "function", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.data_prep_fakenews.get_label_distribution", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.data_prep_fakenews.get_label_distribution", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.data_prep_fakenews.get_label_distribution"], ["", "", "def", "create_data_splits_cv", "(", "base_dir", "=", "'./FakeHealth'", ")", ":", "\n", "    ", "\"\"\"\n    This method creates cross validaiton splits of the dataset in a stratified fashion to ensure similar data distribution across folds\n    \"\"\"", "\n", "\n", "print", "(", "\"\\n\\n\"", "+", "\"=\"", "*", "50", "+", "\"\\n\\t\\tCreating Data Splits\\n\"", "+", "\"=\"", "*", "50", ")", "\n", "\n", "datasets", "=", "[", "'HealthRelease'", ",", "'HealthStory'", "]", "\n", "for", "dataset", "in", "datasets", ":", "\n", "        ", "print", "(", "\"\\nPreparing {} ...\"", ".", "format", "(", "dataset", ")", ")", "\n", "doc2labels_file", "=", "os", ".", "path", ".", "join", "(", "base_dir", ",", "'doc2labels_{}.json'", ".", "format", "(", "dataset", ")", ")", "\n", "doc2labels", "=", "json", ".", "load", "(", "open", "(", "doc2labels_file", ",", "'r'", ")", ")", "\n", "src_dir", "=", "os", ".", "path", ".", "join", "(", "'data'", ",", "dataset", "+", "'.tsv'", ")", "\n", "x_data", ",", "y_data", ",", "doc_data", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "# Reading the dataset into workable lists", "\n", "removed", "=", "0", "\n", "lens", "=", "[", "]", "\n", "with", "open", "(", "src_dir", ",", "encoding", "=", "'utf-8'", ")", "as", "data", ":", "\n", "            ", "reader", "=", "csv", ".", "DictReader", "(", "data", ",", "delimiter", "=", "'\\t'", ")", "\n", "for", "row", "in", "reader", ":", "\n", "                ", "if", "isinstance", "(", "row", "[", "'text'", "]", ",", "str", ")", "and", "len", "(", "row", "[", "'text'", "]", ")", ">", "5", ":", "\n", "                    ", "text", "=", "row", "[", "'text'", "]", ".", "replace", "(", "'\\n'", ",", "' '", ")", "\n", "text", "=", "text", ".", "replace", "(", "'\\t'", ",", "' '", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'#[\\w-]+'", ",", "'hashtag'", ",", "text", ".", "lower", "(", ")", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'https?://\\S+'", ",", "'url'", ",", "text", ")", "\n", "\n", "x_data", ".", "append", "(", "str", "(", "text", "[", ":", "5000", "]", ")", ")", "\n", "lens", ".", "append", "(", "len", "(", "text", "[", ":", "5000", "]", ")", ")", "\n", "y_data", ".", "append", "(", "int", "(", "row", "[", "'label'", "]", ")", ")", "\n", "doc_data", ".", "append", "(", "str", "(", "row", "[", "'id'", "]", ")", ")", "\n", "", "else", ":", "\n", "                    ", "removed", "+=", "1", "\n", "", "", "", "print", "(", "\"avg lens = \"", ",", "sum", "(", "lens", ")", "/", "len", "(", "lens", ")", ")", "\n", "print", "(", "\"max lens = \"", ",", "max", "(", "lens", ")", ")", "\n", "print", "(", "\"minimum lens = \"", ",", "min", "(", "lens", ")", ")", "\n", "print", "(", "\"Total data points removed = \"", ",", "removed", ")", "\n", "\n", "# Creating train-val-test split with same/similar label distribution in each split", "\n", "sss", "=", "StratifiedShuffleSplit", "(", "n_splits", "=", "1", ",", "test_size", "=", "0.20", ",", "random_state", "=", "21", ")", "\n", "x_rest", ",", "x_test", ",", "y_rest", ",", "y_test", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "doc_rest", ",", "doc_id_test", "=", "[", "]", ",", "[", "]", "\n", "for", "train_index", ",", "test_index", "in", "sss", ".", "split", "(", "x_data", ",", "y_data", ")", ":", "\n", "            ", "for", "idx", "in", "train_index", ":", "\n", "                ", "x_rest", ".", "append", "(", "x_data", "[", "idx", "]", ")", "\n", "y_rest", ".", "append", "(", "y_data", "[", "idx", "]", ")", "\n", "doc_rest", ".", "append", "(", "doc_data", "[", "idx", "]", ")", "\n", "\n", "", "for", "idx", "in", "test_index", ":", "\n", "                ", "x_test", ".", "append", "(", "x_data", "[", "idx", "]", ")", "\n", "y_test", ".", "append", "(", "y_data", "[", "idx", "]", ")", "\n", "doc_id_test", ".", "append", "(", "doc_data", "[", "idx", "]", ")", "\n", "\n", "\n", "", "", "sss", "=", "StratifiedShuffleSplit", "(", "n_splits", "=", "5", ",", "test_size", "=", "0.10", ",", "random_state", "=", "21", ")", "\n", "for", "fold", ",", "(", "train_index", ",", "val_index", ")", "in", "enumerate", "(", "sss", ".", "split", "(", "x_rest", ",", "y_rest", ")", ")", ":", "\n", "            ", "x_train", ",", "x_val", ",", "y_train", ",", "y_val", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "doc_id_train", ",", "doc_id_val", "=", "[", "]", ",", "[", "]", "\n", "for", "idx", "in", "train_index", ":", "\n", "                ", "x_train", ".", "append", "(", "x_rest", "[", "idx", "]", ")", "\n", "y_train", ".", "append", "(", "y_rest", "[", "idx", "]", ")", "\n", "doc_id_train", ".", "append", "(", "doc_rest", "[", "idx", "]", ")", "\n", "", "for", "idx", "in", "val_index", ":", "\n", "                ", "x_val", ".", "append", "(", "x_rest", "[", "idx", "]", ")", "\n", "y_val", ".", "append", "(", "y_rest", "[", "idx", "]", ")", "\n", "doc_id_val", ".", "append", "(", "doc_rest", "[", "idx", "]", ")", "\n", "\n", "\n", "", "fake", ",", "real", "=", "get_label_distribution", "(", "y_train", ")", "\n", "print", "(", "\"\\nFake labels in train split of fold {} = {:.2f} %\"", ".", "format", "(", "fold", ",", "fake", "*", "100", ")", ")", "\n", "print", "(", "\"Real labels in train split of fold {} = {:.2f} %\"", ".", "format", "(", "fold", ",", "real", "*", "100", ")", ")", "\n", "\n", "fake", ",", "real", "=", "get_label_distribution", "(", "y_val", ")", "\n", "print", "(", "\"\\nFake labels in val split of fold {} = {:.2f} %\"", ".", "format", "(", "fold", ",", "fake", "*", "100", ")", ")", "\n", "print", "(", "\"Real labels in val split of fold {} = {:.2f} %\"", ".", "format", "(", "fold", ",", "real", "*", "100", ")", ")", "\n", "\n", "fake", ",", "real", "=", "get_label_distribution", "(", "y_test", ")", "\n", "print", "(", "\"\\nFake labels in test split of fold {} = {:.2f} %\"", ".", "format", "(", "fold", ",", "fake", "*", "100", ")", ")", "\n", "print", "(", "\"Real labels in test split of fold {} = {:.2f} %\"", ".", "format", "(", "fold", ",", "real", "*", "100", ")", ")", "\n", "\n", "print", "(", "\"\\nWriting train-val-test files for fold {}..\"", ".", "format", "(", "fold", ")", ")", "\n", "splits", "=", "[", "'train'", ",", "'val'", ",", "'test'", "]", "\n", "for", "split", "in", "splits", ":", "\n", "                ", "if", "split", "==", "'train'", ":", "\n", "                    ", "x", "=", "x_train", "\n", "y", "=", "y_train", "\n", "", "elif", "split", "==", "'val'", ":", "\n", "                    ", "x", "=", "x_val", "\n", "y", "=", "y_val", "\n", "", "else", ":", "\n", "                    ", "x", "=", "x_test", "\n", "y", "=", "y_test", "\n", "\n", "", "write_dir", "=", "os", ".", "path", ".", "join", "(", "'data'", ",", "dataset", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "write_dir", ")", ":", "\n", "                    ", "os", ".", "makedirs", "(", "write_dir", ")", "\n", "", "write_dir", "=", "os", ".", "path", ".", "join", "(", "write_dir", ",", "split", "+", "'_{}.tsv'", ".", "format", "(", "fold", "+", "1", ")", ")", "\n", "print", "(", "\"{} fold, {} file in : {}\"", ".", "format", "(", "fold", ",", "split", ",", "write_dir", ")", ")", "\n", "with", "open", "(", "write_dir", ",", "'a'", ",", "encoding", "=", "'utf-8'", ",", "newline", "=", "''", ")", "as", "csv_file", ":", "\n", "                    ", "csv_writer", "=", "csv", ".", "writer", "(", "csv_file", ",", "delimiter", "=", "'\\t'", ")", "\n", "# csv_writer.writerow(['text', 'label'])", "\n", "for", "i", "in", "range", "(", "len", "(", "x", ")", ")", ":", "\n", "                        ", "csv_writer", ".", "writerow", "(", "[", "x", "[", "i", "]", ",", "y", "[", "i", "]", "]", ")", "\n", "\n", "\n", "", "", "", "temp_dict", "=", "{", "}", "\n", "temp_dict", "[", "'test_docs'", "]", "=", "doc_id_test", "\n", "temp_dict", "[", "'train_docs'", "]", "=", "doc_id_train", "\n", "temp_dict", "[", "'val_docs'", "]", "=", "doc_id_val", "\n", "doc_splits_file", "=", "os", ".", "path", ".", "join", "(", "base_dir", ",", "'doc_splits_{}_{}.json'", ".", "format", "(", "fold", "+", "1", ",", "dataset", ")", ")", "\n", "json", ".", "dump", "(", "temp_dict", ",", "open", "(", "doc_splits_file", ",", "'w+'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.data_prep_fakehealth.get_label_distribution": [[290, 295], ["labels.count", "labels.count"], "function", ["None"], ["", "", "", "def", "get_label_distribution", "(", "labels", ")", ":", "\n", "    ", "fake", "=", "labels", ".", "count", "(", "1", ")", "\n", "real", "=", "labels", ".", "count", "(", "0", ")", "\n", "denom", "=", "fake", "+", "real", "\n", "return", "fake", "/", "denom", ",", "real", "/", "denom", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakenews.GNN_PreProcess_news.__init__": [[20, 41], ["gnn_prep_main.GNN_PreProcess.__init__", "gnn_preprocess_lr_fakenews.GNN_PreProcess_news.datasets.append", "gnn_preprocess_lr_fakenews.GNN_PreProcess_news.datasets.append", "gnn_preprocess_lr_fakenews.GNN_PreProcess_news.create_aggregate_folder", "gnn_preprocess_lr_fakenews.GNN_PreProcess_news.create_dicts", "gnn_preprocess_lr_fakenews.GNN_PreProcess_news.create_adj_matrix", "gnn_preprocess_lr_fakenews.GNN_PreProcess_news.create_feat_matrix", "gnn_preprocess_lr_fakenews.GNN_PreProcess_news.create_labels", "gnn_preprocess_lr_fakenews.GNN_PreProcess_news.create_split_masks"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_aggregate_folder", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_dicts", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_adj_matrix", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_feat_matrix", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_labels", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_split_masks"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "gossipcop", "=", "False", ",", "politifact", "=", "False", ")", ":", "\n", "        ", "super", "(", "GNN_PreProcess_news", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "\n", "if", "politifact", ":", "\n", "            ", "self", ".", "datasets", ".", "append", "(", "'politifact'", ")", "\n", "", "if", "gossipcop", ":", "\n", "            ", "self", ".", "datasets", ".", "append", "(", "'gossipcop'", ")", "\n", "\n", "", "if", "config", "[", "'create_aggregate_folder'", "]", ":", "\n", "            ", "self", ".", "create_aggregate_folder", "(", ")", "\n", "", "if", "config", "[", "'create_dicts'", "]", ":", "\n", "            ", "self", ".", "create_dicts", "(", ")", "\n", "", "if", "config", "[", "'create_adj_matrix'", "]", ":", "\n", "            ", "self", ".", "create_adj_matrix", "(", ")", "\n", "", "if", "config", "[", "'create_feat_matrix'", "]", ":", "\n", "            ", "self", ".", "create_feat_matrix", "(", ")", "\n", "", "if", "config", "[", "'create_labels'", "]", ":", "\n", "            ", "self", ".", "create_labels", "(", ")", "\n", "", "if", "config", "[", "'create_split_masks'", "]", ":", "\n", "            ", "self", ".", "create_split_masks", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakenews.GNN_PreProcess_news.create_aggregate_folder": [[48, 104], ["print", "print", "collections.defaultdict", "print", "collections.defaultdict.items", "print", "os.path.join", "os.path.join", "os.path.join", "os.path.exists", "print", "os.makedirs", "os.walk", "open", "list", "json.dump", "enumerate", "os.walk", "str", "os.path.join", "pandas.read_csv", "list", "docs_done[].update", "os.path.exists", "print", "os.makedirs", "enumerate", "json.loads.split", "set", "print", "print", "os.path.join", "isinstance", "json.loads.split", "open", "csv_file.readlines", "print", "print", "json.loads", "docs_done[].update"], "methods", ["None"], ["", "", "def", "create_aggregate_folder", "(", "self", ")", ":", "\n", "        ", "\"\"\"Create the 'complete' folder that has files as doc_IDs and contains alll the users that interacted with it \"\"\"", "\n", "\n", "print", "(", "\"\\nCreating aggregate files for all docs and their users......\"", ")", "\n", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "            ", "print", "(", "\"\\n\"", "+", "\"-\"", "*", "60", "+", "\"\\n \\t\\t Analyzing {} dataset\\n\"", ".", "format", "(", "dataset", ")", "+", "'-'", "*", "60", ")", "\n", "\n", "user_contexts", "=", "[", "'tweets'", ",", "'retweets'", "]", "\n", "docs_done", "=", "defaultdict", "(", "set", ")", "\n", "for", "user_context", "in", "user_contexts", ":", "\n", "                ", "print", "(", "\"\\nIterating over : \"", ",", "user_context", ")", "\n", "src_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'complete_data'", ",", "dataset", ",", "user_context", ")", "\n", "dest_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'complete_data'", ",", "dataset", ",", "'complete'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "dest_dir", ")", ":", "\n", "                    ", "print", "(", "\"Creating dir:  {}\\n\"", ".", "format", "(", "dest_dir", ")", ")", "\n", "os", ".", "makedirs", "(", "dest_dir", ")", "\n", "", "if", "user_context", "==", "'tweets'", ":", "\n", "                    ", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "src_dir", ")", ":", "\n", "                        ", "for", "count", ",", "file", "in", "enumerate", "(", "files", ")", ":", "\n", "                            ", "doc", "=", "file", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "src_file_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "file", ")", "\n", "src_file", "=", "pd", ".", "read_csv", "(", "src_file_path", ")", "\n", "user_ids", "=", "src_file", "[", "'user_id'", "]", "\n", "user_ids", "=", "[", "s", "for", "s", "in", "user_ids", "if", "isinstance", "(", "s", ",", "int", ")", "]", "\n", "user_ids", "=", "list", "(", "set", "(", "user_ids", ")", ")", "\n", "docs_done", "[", "doc", "]", ".", "update", "(", "user_ids", ")", "\n", "if", "count", "==", "1", ":", "\n", "                                ", "print", "(", "doc", ",", "docs_done", "[", "doc", "]", ")", "\n", "", "if", "count", "%", "2000", "==", "0", ":", "\n", "                                ", "print", "(", "\"{} done\"", ".", "format", "(", "count", ")", ")", "\n", "\n", "", "", "", "", "elif", "user_context", "==", "'retweets'", ":", "\n", "                    ", "if", "not", "os", ".", "path", ".", "exists", "(", "dest_dir", ")", ":", "\n", "                        ", "print", "(", "\"Creating dir   {}\"", ",", "dest_dir", ")", "\n", "os", ".", "makedirs", "(", "dest_dir", ")", "\n", "", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "src_dir", ")", ":", "\n", "                        ", "for", "count", ",", "file", "in", "enumerate", "(", "files", ")", ":", "\n", "                            ", "doc", "=", "file", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "src_file_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "file", ")", "\n", "with", "open", "(", "src_file_path", ",", "encoding", "=", "'utf-8'", ",", "newline", "=", "''", ")", "as", "csv_file", ":", "\n", "                                ", "lines", "=", "csv_file", ".", "readlines", "(", ")", "\n", "for", "line", "in", "lines", ":", "\n", "                                    ", "file", "=", "json", ".", "loads", "(", "line", ")", "\n", "docs_done", "[", "doc", "]", ".", "update", "(", "[", "file", "[", "'user'", "]", "[", "\"id\"", "]", "]", ")", "\n", "", "", "if", "count", "==", "1", ":", "\n", "                                ", "print", "(", "doc", ",", "docs_done", "[", "doc", "]", ")", "\n", "", "if", "count", "%", "2000", "==", "0", ":", "\n", "                                ", "print", "(", "\"{} done\"", ".", "format", "(", "count", ")", ")", "\n", "", "", "", "", "", "print", "(", "\"\\nWriting into files at:   \"", ",", "dest_dir", ")", "\n", "for", "doc", ",", "user_list", "in", "docs_done", ".", "items", "(", ")", ":", "\n", "                ", "dest_file_path", "=", "os", ".", "path", ".", "join", "(", "dest_dir", ",", "str", "(", "doc", ")", "+", "'.json'", ")", "\n", "with", "open", "(", "dest_file_path", ",", "'w+'", ")", "as", "j", ":", "\n", "                    ", "temp_dict", "=", "{", "}", "\n", "temp_dict", "[", "'users'", "]", "=", "list", "(", "user_list", ")", "\n", "json", ".", "dump", "(", "temp_dict", ",", "j", ")", "\n", "", "", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakenews.GNN_PreProcess_news.create_doc_user_splits": [[108, 148], ["print", "os.path.join", "json.load", "print", "os.path.join", "os.walk", "list", "list", "list", "os.path.join", "print", "open", "set", "set", "set", "enumerate", "open", "json.dump", "os.path.join", "json.load", "file.split", "open", "int", "str", "train_users.update", "str", "val_users.update", "str", "test_users.update", "isinstance"], "methods", ["None"], ["", "def", "create_doc_user_splits", "(", "self", ")", ":", "\n", "        ", "\"\"\"Create and save docs and users present in the data splits\"\"\"", "\n", "\n", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "            ", "print", "(", "\"\\n\\n\"", "+", "\"-\"", "*", "100", "+", "\"\\n \\t\\t   Creating doc and user splits for {}\\n\"", ".", "format", "(", "dataset", ")", "+", "'-'", "*", "100", ")", "\n", "\n", "docsplits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "dataset", ",", "'doc_splits.json'", ")", "\n", "\n", "docsplits", "=", "json", ".", "load", "(", "open", "(", "docsplits_file", ",", "'r'", ")", ")", "\n", "train_docs", "=", "docsplits", "[", "'train_docs'", "]", "\n", "test_docs", "=", "docsplits", "[", "'test_docs'", "]", "\n", "val_docs", "=", "docsplits", "[", "'val_docs'", "]", "\n", "\n", "print", "(", "\"\\nCreating users in splits file..\"", ")", "\n", "src_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'complete_data'", ",", "dataset", ",", "'complete'", ")", "\n", "train_users", ",", "val_users", ",", "test_users", "=", "set", "(", ")", ",", "set", "(", ")", ",", "set", "(", ")", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "src_dir", ")", ":", "\n", "                ", "for", "count", ",", "file", "in", "enumerate", "(", "files", ")", ":", "\n", "                    ", "doc_key", "=", "file", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "src_file_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "file", ")", "\n", "src_file", "=", "json", ".", "load", "(", "open", "(", "src_file_path", ",", "'r'", ")", ")", "\n", "users", "=", "src_file", "[", "'users'", "]", "\n", "users", "=", "[", "int", "(", "s", ")", "for", "s", "in", "users", "if", "isinstance", "(", "s", ",", "int", ")", "]", "\n", "if", "str", "(", "doc_key", ")", "in", "train_docs", ":", "\n", "                        ", "train_users", ".", "update", "(", "users", ")", "\n", "", "if", "str", "(", "doc_key", ")", "in", "val_docs", ":", "\n", "                        ", "val_users", ".", "update", "(", "users", ")", "\n", "", "if", "str", "(", "doc_key", ")", "in", "test_docs", ":", "\n", "                        ", "test_users", ".", "update", "(", "users", ")", "\n", "\n", "", "", "", "temp_dict", "=", "{", "}", "\n", "temp_dict", "[", "'train_users'", "]", "=", "list", "(", "train_users", ")", "\n", "temp_dict", "[", "'val_users'", "]", "=", "list", "(", "val_users", ")", "\n", "temp_dict", "[", "'test_users'", "]", "=", "list", "(", "test_users", ")", "\n", "usersplits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'complete_data'", ",", "dataset", ",", "'user_splits_lr.json'", ")", "\n", "print", "(", "\"user_splits written in : \"", ",", "usersplits_file", ")", "\n", "with", "open", "(", "usersplits_file", ",", "'w+'", ")", "as", "j", ":", "\n", "                ", "json", ".", "dump", "(", "temp_dict", ",", "j", ")", "\n", "\n", "", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakenews.GNN_PreProcess_news.create_dicts": [[153, 274], ["print", "print", "os.path.join", "os.path.join", "json.load", "json.load", "list", "print", "print", "print", "print", "json.load", "json.load", "enumerate", "print", "print", "enumerate", "print", "print", "print", "os.path.join", "print", "set", "set", "print", "list", "print", "print", "print", "print", "enumerate", "os.path.join", "print", "print", "doc2id_train.copy", "doc2id_train.copy.update", "print", "print", "os.path.join", "print", "os.path.join", "numpy.array", "print", "print", "numpy.save", "print", "len", "enumerate", "print", "print", "doc2id_train.copy", "doc2id_train.copy.update", "print", "os.path.join", "print", "print", "open", "open", "set", "len", "len", "len", "len", "open", "open", "numpy.array.append", "len", "len", "numpy.array.append", "len", "len", "len", "open", "json.dump", "len", "set", "len", "len", "len", "len", "numpy.array.append", "len", "open", "json.dump", "len", "len", "open", "json.dump", "len", "len", "open", "json.dump", "len", "open", "json.dump", "os.path.join", "os.path.join", "len", "set.intersection", "len", "os.path.join", "str", "str", "str", "str", "len", "str", "str", "str"], "methods", ["None"], ["def", "create_dicts", "(", "self", ")", ":", "\n", "\n", "        ", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "            ", "print", "(", "\"\\n\\n\"", "+", "\"-\"", "*", "100", "+", "\"\\n \\t\\t\\t   Creating dicts for  {} dataset \\n\"", ".", "format", "(", "dataset", ")", "+", "'-'", "*", "100", ")", "\n", "\n", "print", "(", "\"\\nCreating doc2id and user2id dicts....\\n\"", ")", "\n", "usersplits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'complete_data'", ",", "dataset", ",", "'user_splits_lr.json'", ")", "\n", "docsplits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'doc_splits.json'", ")", "\n", "\n", "user_splits", "=", "json", ".", "load", "(", "open", "(", "usersplits_file", ",", "'r'", ")", ")", "\n", "doc_splits", "=", "json", ".", "load", "(", "open", "(", "docsplits_file", ",", "'r'", ")", ")", "\n", "\n", "train_users", "=", "user_splits", "[", "'train_users'", "]", "\n", "val_users", "=", "user_splits", "[", "'val_users'", "]", "\n", "test_users", "=", "user_splits", "[", "'test_users'", "]", "\n", "\n", "all_users", "=", "list", "(", "set", "(", "train_users", "+", "val_users", "+", "test_users", ")", ")", "\n", "\n", "print", "(", "'\\nTrain users = '", ",", "len", "(", "train_users", ")", ")", "\n", "print", "(", "\"Test users = \"", ",", "len", "(", "test_users", ")", ")", "\n", "print", "(", "\"Val users = \"", ",", "len", "(", "val_users", ")", ")", "\n", "print", "(", "\"All users = \"", ",", "len", "(", "all_users", ")", ")", "\n", "\n", "train_docs", "=", "doc_splits", "[", "'train_docs'", "]", "\n", "val_docs", "=", "doc_splits", "[", "'val_docs'", "]", "\n", "test_docs", "=", "doc_splits", "[", "'test_docs'", "]", "\n", "restricted_users", "=", "json", ".", "load", "(", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'restricted_users_gossipcop_5.json'", ")", ",", "'r'", ")", ")", "\n", "done_users", "=", "json", ".", "load", "(", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'done_users_30.json'", ")", ",", "'r'", ")", ")", "\n", "\n", "\n", "doc2id_train", "=", "{", "}", "\n", "node_type", "=", "[", "]", "\n", "for", "train_count", ",", "doc", "in", "enumerate", "(", "train_docs", ")", ":", "\n", "                ", "doc2id_train", "[", "str", "(", "doc", ")", "]", "=", "train_count", "\n", "node_type", ".", "append", "(", "1", ")", "\n", "", "print", "(", "\"Train docs = \"", ",", "len", "(", "train_docs", ")", ")", "\n", "print", "(", "\"Node_type = \"", ",", "len", "(", "node_type", ")", ")", "\n", "\n", "\n", "for", "val_count", ",", "doc", "in", "enumerate", "(", "val_docs", ")", ":", "\n", "                ", "doc2id_train", "[", "str", "(", "doc", ")", "]", "=", "val_count", "+", "len", "(", "train_docs", ")", "\n", "node_type", ".", "append", "(", "1", ")", "\n", "", "print", "(", "\"Val docs = \"", ",", "len", "(", "val_docs", ")", ")", "\n", "print", "(", "\"doc2id_train = \"", ",", "len", "(", "doc2id_train", ")", ")", "\n", "print", "(", "\"Node_type = \"", ",", "len", "(", "node_type", ")", ")", "\n", "doc2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'doc2id_lr_train_30_5.json'", ")", "\n", "print", "(", "\"Saving doc2id dict in :\"", ",", "doc2id_file", ")", "\n", "with", "open", "(", "doc2id_file", ",", "'w+'", ")", "as", "j", ":", "\n", "                ", "json", ".", "dump", "(", "doc2id_train", ",", "j", ")", "\n", "\n", "\n", "# If DO NOT want to include the most frequent users                ", "\n", "", "train_users", "=", "[", "u", "for", "u", "in", "train_users", "if", "u", "in", "done_users", "[", "'done_users'", "]", "and", "str", "(", "u", ")", "not", "in", "restricted_users", "[", "'restricted_users'", "]", "]", "\n", "val_users", "=", "[", "u", "for", "u", "in", "val_users", "if", "u", "in", "done_users", "[", "'done_users'", "]", "and", "str", "(", "u", ")", "not", "in", "restricted_users", "[", "'restricted_users'", "]", "]", "\n", "test_users", "=", "[", "u", "for", "u", "in", "test_users", "if", "u", "in", "done_users", "[", "'done_users'", "]", "and", "str", "(", "u", ")", "not", "in", "restricted_users", "[", "'restricted_users'", "]", "]", "\n", "\n", "\n", "# # Include ALL users", "\n", "# train_users = [u for u in train_users if u in done_users['done_users']]", "\n", "# val_users = [u for u in val_users if u in done_users['done_users']]", "\n", "# test_users = [u for u in test_users if u in done_users['done_users']]", "\n", "\n", "# train_users = list(set(train_users) - set(done_users['done_users']) - set(restricted_users['restricted_users']))", "\n", "\n", "a", "=", "set", "(", "train_users", "+", "val_users", ")", "\n", "b", "=", "set", "(", "test_users", ")", "\n", "print", "(", "\"\\nUsers common between train/val and test = \"", ",", "len", "(", "a", ".", "intersection", "(", "b", ")", ")", ")", "\n", "\n", "all_users", "=", "list", "(", "set", "(", "train_users", "+", "val_users", "+", "test_users", ")", ")", "\n", "\n", "print", "(", "'\\nTrain users = '", ",", "len", "(", "train_users", ")", ")", "\n", "print", "(", "\"Test users = \"", ",", "len", "(", "test_users", ")", ")", "\n", "print", "(", "\"Val users = \"", ",", "len", "(", "val_users", ")", ")", "\n", "print", "(", "\"All users = \"", ",", "len", "(", "all_users", ")", ")", "\n", "\n", "user2id_train", "=", "{", "}", "\n", "for", "count", ",", "user", "in", "enumerate", "(", "all_users", ")", ":", "\n", "                ", "user2id_train", "[", "str", "(", "user", ")", "]", "=", "count", "+", "len", "(", "doc2id_train", ")", "\n", "node_type", ".", "append", "(", "2", ")", "\n", "", "user2id_train_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'user2id_lr_train_30_5.json'", ")", "\n", "print", "(", "\"user2_id = \"", ",", "len", "(", "user2id_train", ")", ")", "\n", "print", "(", "\"Saving user2id_train in : \"", ",", "user2id_train_file", ")", "\n", "with", "open", "(", "user2id_train_file", ",", "'w+'", ")", "as", "j", ":", "\n", "                ", "json", ".", "dump", "(", "user2id_train", ",", "j", ")", "\n", "\n", "\n", "", "node2id", "=", "doc2id_train", ".", "copy", "(", ")", "\n", "node2id", ".", "update", "(", "user2id_train", ")", "\n", "print", "(", "\"node2id size = \"", ",", "len", "(", "node2id", ")", ")", "\n", "print", "(", "\"Node_type = \"", ",", "len", "(", "node_type", ")", ")", "\n", "node2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'node2id_lr_train_30_5.json'", ")", "\n", "print", "(", "\"Saving node2id_lr_train in : \"", ",", "node2id_file", ")", "\n", "with", "open", "(", "node2id_file", ",", "'w+'", ")", "as", "json_file", ":", "\n", "                ", "json", ".", "dump", "(", "node2id", ",", "json_file", ")", "\n", "\n", "", "node_type_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'node_type_lr_train_30_5.npy'", ")", "\n", "node_type", "=", "np", ".", "array", "(", "node_type", ")", "\n", "print", "(", "node_type", ".", "shape", ")", "\n", "print", "(", "\"Saving node_type in :\"", ",", "node_type_file", ")", "\n", "np", ".", "save", "(", "node_type_file", ",", "node_type", ",", "allow_pickle", "=", "True", ")", "\n", "\n", "print", "(", "\"\\nAdding test docs..\"", ")", "\n", "orig_doc2id_len", "=", "len", "(", "doc2id_train", ")", "\n", "for", "test_count", ",", "doc", "in", "enumerate", "(", "test_docs", ")", ":", "\n", "                ", "doc2id_train", "[", "str", "(", "doc", ")", "]", "=", "test_count", "+", "len", "(", "user2id_train", ")", "+", "orig_doc2id_len", "\n", "", "print", "(", "\"Test docs = \"", ",", "len", "(", "test_docs", ")", ")", "\n", "print", "(", "\"doc2id_train = \"", ",", "len", "(", "doc2id_train", ")", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'doc2id_lr_train_30_5.json'", ")", ",", "'w+'", ")", "as", "j", ":", "\n", "                ", "json", ".", "dump", "(", "doc2id_train", ",", "j", ")", "\n", "\n", "", "node2id", "=", "doc2id_train", ".", "copy", "(", ")", "\n", "node2id", ".", "update", "(", "user2id_train", ")", "\n", "print", "(", "\"node2id size = \"", ",", "len", "(", "node2id", ")", ")", "\n", "node2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'node2id_lr_30_5.json'", ")", "\n", "print", "(", "\"Saving node2id_lr in : \"", ",", "node2id_file", ")", "\n", "with", "open", "(", "node2id_file", ",", "'w+'", ")", "as", "json_file", ":", "\n", "                ", "json", ".", "dump", "(", "node2id", ",", "json_file", ")", "\n", "\n", "", "print", "(", "\"Done ! All files written..\"", ")", "\n", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakenews.GNN_PreProcess_news.create_filtered_follower_following": [[277, 324], ["print", "print", "int", "print", "open", "json.load", "json.load", "len", "print", "os.path.join", "os.path.join", "os.walk", "os.path.join", "open", "len", "enumerate", "os.path.join", "os.path.join", "json.load", "int", "os.path.join", "open", "os.path.isfile", "print", "str", "int", "set", "list", "list", "map", "open", "json.dump", "int", "set.update"], "methods", ["None"], ["", "def", "create_filtered_follower_following", "(", "self", ")", ":", "\n", "\n", "        ", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "            ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'user2id_lr_train_30_5.json'", ")", ",", "'r'", ")", "as", "j", ":", "\n", "               ", "all_users", "=", "json", ".", "load", "(", "j", ")", "\n", "\n", "", "done_users", "=", "json", ".", "load", "(", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'done_users_30.json'", ")", ",", "'r'", ")", ")", "[", "'done_users'", "]", "\n", "print", "(", "\"Total done users = \"", ",", "len", "(", "done_users", ")", ")", "\n", "\n", "print", "(", "\"\\n\\n\"", "+", "\"-\"", "*", "100", "+", "\"\\n \\t\\t   Creating filtered follower-following\\n\"", "+", "'-'", "*", "100", ")", "\n", "user_contexts", "=", "[", "'user_followers'", ",", "'user_following'", "]", "\n", "print_iter", "=", "int", "(", "len", "(", "all_users", ")", "/", "10", ")", "\n", "not_found", "=", "0", "\n", "\n", "for", "user_context", "in", "user_contexts", ":", "\n", "                ", "print", "(", "\"    - from {}  folder...\"", ".", "format", "(", "user_context", ")", ")", "\n", "src_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "user_context", ")", "\n", "dest_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "user_context", "+", "'_filtered'", ")", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "src_dir", ")", ":", "\n", "                    ", "for", "count", ",", "file", "in", "enumerate", "(", "files", ")", ":", "\n", "                        ", "src_file_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "file", ")", "\n", "# user_id = src_file_path.split(\".\")[0]", "\n", "src_file", "=", "json", ".", "load", "(", "open", "(", "src_file_path", ",", "'r'", ")", ")", "\n", "user_id", "=", "int", "(", "src_file", "[", "'user_id'", "]", ")", "\n", "dest_file_path", "=", "os", ".", "path", ".", "join", "(", "dest_dir", ",", "str", "(", "user_id", ")", "+", "'.json'", ")", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "dest_file_path", ")", ":", "\n", "                            ", "if", "int", "(", "user_id", ")", "in", "done_users", ":", "\n", "                                ", "temp", "=", "set", "(", ")", "\n", "followers", "=", "src_file", "[", "'followers'", "]", "if", "user_context", "==", "'user_followers'", "else", "src_file", "[", "'following'", "]", "\n", "followers", "=", "list", "(", "map", "(", "int", ",", "followers", ")", ")", "\n", "for", "follower", "in", "followers", ":", "\n", "                                    ", "if", "int", "(", "follower", ")", "in", "done_users", ":", "\n", "                                        ", "temp", ".", "update", "(", "[", "follower", "]", ")", "\n", "", "", "temp_dict", "=", "{", "}", "\n", "temp_dict", "[", "'user_id'", "]", "=", "user_id", "\n", "name", "=", "'followers'", "if", "user_context", "==", "'user_followers'", "else", "'following'", "\n", "temp_dict", "[", "name", "]", "=", "list", "(", "temp", ")", "\n", "with", "open", "(", "dest_file_path", ",", "'w+'", ")", "as", "v", ":", "\n", "                                    ", "json", ".", "dump", "(", "temp_dict", ",", "v", ")", "\n", "", "", "else", ":", "\n", "                                ", "not_found", "+=", "1", "\n", "# print(\"{}  not found..\".format(user_id))", "\n", "", "", "if", "count", "%", "2000", "==", "0", ":", "\n", "# print(\"{}/{} done..  Non-zeros =  {}\".format(count+1, num_users, adj_matrix.getnnz()))", "\n", "                            ", "print", "(", "\"{} done..\"", ".", "format", "(", "count", "+", "1", ")", ")", "\n", "", "", "", "", "print", "(", "\"\\nNot found users = \"", ",", "not_found", ")", "\n", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakenews.GNN_PreProcess_news.create_adj_matrix": [[327, 433], ["print", "json.load", "json.load", "os.path.join", "json.load", "print", "print", "scipy.sparse.lil_matrix", "scipy.sparse.lil_matrix", "range", "int", "print", "time.time", "print", "os.path.join", "os.walk", "time.time", "gnn_preprocess_lr_fakenews.GNN_PreProcess_news.calc_elapsed_time", "print", "print", "print", "print", "time.time", "print", "int", "print", "gnn_preprocess_lr_fakenews.GNN_PreProcess_news.calc_elapsed_time", "print", "print", "print", "print", "gnn_preprocess_lr_fakenews.GNN_PreProcess_news.save_adj_matrix", "open", "open", "open", "len", "enumerate", "scipy.sparse.lil_matrix.getnnz", "scipy.sparse.lil_matrix.getnnz", "print", "os.path.join", "os.walk", "time.time", "scipy.sparse.lil_matrix.getnnz", "scipy.sparse.lil_matrix.getnnz", "os.path.join", "os.path.join", "len", "len", "os.path.join", "json.load", "enumerate", "file.split", "open", "os.path.join", "json.load", "int", "open", "str", "list", "str", "str", "str", "map", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.calc_elapsed_time", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.calc_elapsed_time", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.save_adj_matrix"], ["", "", "def", "create_adj_matrix", "(", "self", ")", ":", "\n", "        ", "\"\"\"create and save adjacency matrix of the community graph\"\"\"", "\n", "\n", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "            ", "print", "(", "\"\\n\\n\"", "+", "\"-\"", "*", "100", "+", "\"\\n \\t\\t\\tAnalyzing  {} dataset for adj_matrix\\n\"", ".", "format", "(", "dataset", ")", "+", "'-'", "*", "100", ")", "\n", "\n", "user2id", "=", "json", ".", "load", "(", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'user2id_lr_train_30_5.json'", ")", ",", "'r'", ")", ")", "\n", "doc2id", "=", "json", ".", "load", "(", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'doc2id_lr_train_30_5.json'", ")", ",", "'r'", ")", ")", "\n", "doc_splits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'doc_splits.json'", ")", "\n", "doc_splits", "=", "json", ".", "load", "(", "open", "(", "doc_splits_file", ",", "'r'", ")", ")", "\n", "test_docs", "=", "doc_splits", "[", "'test_docs'", "]", "\n", "\n", "\n", "num_users", ",", "num_docs", "=", "len", "(", "user2id", ")", ",", "len", "(", "doc2id", ")", "-", "len", "(", "test_docs", ")", "\n", "print", "(", "\"\\nNo.of unique users = \"", ",", "num_users", ")", "\n", "print", "(", "\"No.of docs = \"", ",", "num_docs", ")", "\n", "\n", "# Creating the adjacency matrix (doc-user edges)", "\n", "adj_matrix", "=", "lil_matrix", "(", "(", "num_docs", "+", "num_users", ",", "num_users", "+", "num_docs", ")", ")", "\n", "edge_type", "=", "lil_matrix", "(", "(", "num_docs", "+", "num_users", ",", "num_users", "+", "num_docs", ")", ")", "\n", "# adj_matrix = np.zeros((num_docs+num_users, num_users+num_docs))", "\n", "# adj_matrix_file = './data/complete_data/adj_matrix_pheme.npz'", "\n", "# adj_matrix = load_npz(adj_matrix_file)", "\n", "# adj_matrix = lil_matrix(adj_matrix)", "\n", "# Creating self-loops for each node (diagonals are 1's)", "\n", "for", "i", "in", "range", "(", "adj_matrix", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "adj_matrix", "[", "i", ",", "i", "]", "=", "1", "\n", "edge_type", "[", "i", ",", "i", "]", "=", "1", "\n", "", "print_iter", "=", "int", "(", "num_docs", "/", "10", ")", "\n", "print", "(", "\"\\nSize of adjacency matrix = {} \\nPrinting every  {} docs\"", ".", "format", "(", "adj_matrix", ".", "shape", ",", "print_iter", ")", ")", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "\n", "print", "(", "\"\\nPreparing entries for doc-user pairs...\"", ")", "\n", "src_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'complete_data'", ",", "dataset", ",", "'complete'", ")", "\n", "not_found", "=", "0", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "src_dir", ")", ":", "\n", "                ", "for", "count", ",", "file", "in", "enumerate", "(", "files", ")", ":", "\n", "                    ", "src_file_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "file", ")", "\n", "doc_key", "=", "file", ".", "split", "(", "\".\"", ")", "[", "0", "]", "\n", "src_file", "=", "json", ".", "load", "(", "open", "(", "src_file_path", ",", "'r'", ")", ")", "\n", "users", "=", "src_file", "[", "'users'", "]", "\n", "for", "user", "in", "users", ":", "\n", "                        ", "if", "str", "(", "doc_key", ")", "in", "doc2id", "and", "str", "(", "user", ")", "in", "user2id", "and", "str", "(", "doc_key", ")", "not", "in", "test_docs", ":", "\n", "                            ", "adj_matrix", "[", "doc2id", "[", "str", "(", "doc_key", ")", "]", ",", "user2id", "[", "str", "(", "user", ")", "]", "]", "=", "1", "\n", "adj_matrix", "[", "user2id", "[", "str", "(", "user", ")", "]", ",", "doc2id", "[", "str", "(", "doc_key", ")", "]", "]", "=", "1", "\n", "edge_type", "[", "doc2id", "[", "str", "(", "doc_key", ")", "]", ",", "user2id", "[", "str", "(", "user", ")", "]", "]", "=", "2", "\n", "edge_type", "[", "user2id", "[", "str", "(", "user", ")", "]", ",", "doc2id", "[", "str", "(", "doc_key", ")", "]", "]", "=", "2", "\n", "", "else", ":", "\n", "                            ", "not_found", "+=", "1", "\n", "\n", "\n", "", "", "", "", "end", "=", "time", ".", "time", "(", ")", "\n", "hrs", ",", "mins", ",", "secs", "=", "self", ".", "calc_elapsed_time", "(", "start", ",", "end", ")", "\n", "print", "(", "\"Done. Took {}hrs and {}mins and {}secs\\n\"", ".", "format", "(", "hrs", ",", "mins", ",", "secs", ")", ")", "\n", "print", "(", "\"Not Found users = \"", ",", "not_found", ")", "\n", "print", "(", "\"Non-zero entries = \"", ",", "adj_matrix", ".", "getnnz", "(", ")", ")", "\n", "print", "(", "\"Non-zero entries edge_type = \"", ",", "edge_type", ".", "getnnz", "(", ")", ")", "\n", "# print(\"Non-zero entries = \", len(np.nonzero(adj_matrix)[0]))", "\n", "\n", "# Creating the adjacency matrix (user-user edges)", "\n", "user_contexts", "=", "[", "'user_followers_filtered'", ",", "'user_following_filtered'", "]", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "key_errors", ",", "not_found", ",", "overlaps", "=", "0", ",", "0", ",", "0", "\n", "print", "(", "\"\\nPreparing entries for user-user pairs...\"", ")", "\n", "print_iter", "=", "int", "(", "num_users", "/", "10", ")", "\n", "print", "(", "\"Printing every {}  users done\"", ".", "format", "(", "print_iter", ")", ")", "\n", "\n", "for", "user_context", "in", "user_contexts", ":", "\n", "                ", "print", "(", "\"    - from {}  folder...\"", ".", "format", "(", "user_context", ")", ")", "\n", "src_dir2", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'complete_data'", ",", "dataset", ",", "user_context", ")", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "src_dir2", ")", ":", "\n", "                    ", "for", "count", ",", "file", "in", "enumerate", "(", "files", ")", ":", "\n", "                        ", "src_file_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "file", ")", "\n", "# user_id = src_file_path.split(\".\")[0]", "\n", "src_file", "=", "json", ".", "load", "(", "open", "(", "src_file_path", ",", "'r'", ")", ")", "\n", "user_id", "=", "int", "(", "src_file", "[", "'user_id'", "]", ")", "\n", "if", "str", "(", "user_id", ")", "in", "user2id", ":", "\n", "                            ", "followers", "=", "src_file", "[", "'followers'", "]", "if", "user_context", "==", "'user_followers_filtered'", "else", "src_file", "[", "'following'", "]", "\n", "followers", "=", "list", "(", "map", "(", "int", ",", "followers", ")", ")", "\n", "for", "follower", "in", "followers", ":", "\n", "                                ", "if", "str", "(", "follower", ")", "in", "user2id", ":", "\n", "                                    ", "adj_matrix", "[", "user2id", "[", "str", "(", "user_id", ")", "]", ",", "user2id", "[", "str", "(", "follower", ")", "]", "]", "=", "1", "\n", "adj_matrix", "[", "user2id", "[", "str", "(", "follower", ")", "]", ",", "user2id", "[", "str", "(", "user_id", ")", "]", "]", "=", "1", "\n", "edge_type", "[", "user2id", "[", "str", "(", "user_id", ")", "]", ",", "user2id", "[", "str", "(", "follower", ")", "]", "]", "=", "3", "\n", "edge_type", "[", "user2id", "[", "str", "(", "follower", ")", "]", ",", "user2id", "[", "str", "(", "user_id", ")", "]", "]", "=", "3", "\n", "\n", "", "", "", "else", ":", "\n", "                            ", "not_found", "+=", "1", "\n", "# if count%print_iter==0:", "\n", "#     # print(\"{}/{} done..  Non-zeros =  {}\".format(count+1, num_users, adj_matrix.getnnz()))", "\n", "#     print(\"{}/{} done..  Non-zeros =  {}\".format(count+1, num_users, len(np.nonzero(adj_matrix)[0])))", "\n", "\n", "", "", "", "", "hrs", ",", "mins", ",", "secs", "=", "self", ".", "calc_elapsed_time", "(", "start", ",", "time", ".", "time", "(", ")", ")", "\n", "print", "(", "\"Done. Took {}hrs and {}mins and {}secs\\n\"", ".", "format", "(", "hrs", ",", "mins", ",", "secs", ")", ")", "\n", "print", "(", "\"Not found user_ids = \"", ",", "not_found", ")", "\n", "print", "(", "\"Total Non-zero entries = \"", ",", "adj_matrix", ".", "getnnz", "(", ")", ")", "\n", "print", "(", "\"Non-zero entries edge_type = \"", ",", "edge_type", ".", "getnnz", "(", ")", ")", "\n", "# print(\"Total Non-zero entries = \", len(np.nonzero(adj_matrix)[0]))", "\n", "\n", "self", ".", "adj_file", "=", "'adj_matrix_lr_train_30_5'", "\n", "# filename = self.data_dir+ '/complete_data' + '/adj_matrix_{}.npy'.format(dataset)", "\n", "\n", "self", ".", "edge_type_file", "=", "'edge_type_lr_train_30_5'", "\n", "self", ".", "save_adj_matrix", "(", "self", ",", "dataset", ",", "adj_matrix", ",", "edge_type", ")", "\n", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakenews.GNN_PreProcess_news.create_feat_matrix": [[438, 582], ["print", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "json.load", "json.load", "json.load", "json.load", "os.path.join", "gnn_preprocess_lr_fakenews.GNN_PreProcess_news.build_vocab", "len", "set", "scipy.sparse.lil_matrix", "print", "print", "time.time", "gnn_preprocess_lr_fakenews.GNN_PreProcess_news.calc_elapsed_time", "print", "numpy.array().squeeze", "print", "numpy.where", "print", "print", "time.time", "os.path.join", "os.walk", "gnn_preprocess_lr_fakenews.GNN_PreProcess_news.calc_elapsed_time", "print", "print", "feat_matrix.astype.astype.astype", "numpy.array().squeeze", "print", "numpy.where", "print", "os.path.join", "print", "scipy.sparse.save_npz", "open", "open", "open", "open", "len", "nltk.corpus.stopwords.words", "os.path.join", "os.walk", "time.time", "len", "enumerate", "time.time", "len", "feat_matrix.astype.astype.tocsr", "len", "len", "enumerate", "numpy.array", "int", "os.path.join", "json.load", "numpy.array", "int", "feat_matrix.astype.astype.sum", "open", "file.split", "print", "print", "feat_matrix.astype.astype.sum", "file.split", "str", "print", "len", "str", "str", "str", "os.path.join", "os.path.join", "os.path.isfile", "datetime.datetime.now", "len", "os.path.join", "str", "os.path.isfile", "len", "str", "str", "open", "json.load", "re.sub", "re.sub", "nltk.word_tokenize.replace", "nltk.word_tokenize.replace", "nltk.word_tokenize", "numpy.zeros", "len", "str", "str", "open", "json.load", "re.sub", "re.sub", "nltk.word_tokenize.replace", "nltk.word_tokenize.replace", "nltk.word_tokenize", "file_content[].lower", "len", "file_content[].lower", "gnn_preprocess_lr_fakenews.GNN_PreProcess_news.keys", "numpy.zeros", "int", "int", "str", "len", "str", "str", "gnn_preprocess_lr_fakenews.GNN_PreProcess_news.keys", "str", "str"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.build_vocab", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.calc_elapsed_time", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.calc_elapsed_time"], ["", "def", "create_feat_matrix", "(", "self", ",", "binary", "=", "True", ")", ":", "\n", "        ", "\"\"\"create and save the initial node representations of each node of the graph\"\"\"", "\n", "\n", "labels", "=", "[", "'fake'", ",", "'real'", "]", "\n", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "            ", "print", "(", "\"\\n\\n\"", "+", "\"-\"", "*", "100", "+", "\"\\n \\t\\tAnalyzing  {} dataset  for feature_matrix\\n\"", ".", "format", "(", "dataset", ")", "+", "'-'", "*", "100", ")", "\n", "\n", "doc2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'doc2id_lr_train_30_5.json'", ")", "\n", "doc_splits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'doc_splits.json'", ")", "\n", "user_splits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'complete_data'", ",", "dataset", ",", "'user_splits_lr.json'", ")", "\n", "user2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'user2id_lr_train_30_5.json'", ")", "\n", "\n", "user_splits", "=", "json", ".", "load", "(", "open", "(", "user_splits_file", ",", "'r'", ")", ")", "\n", "train_users", "=", "user_splits", "[", "'train_users'", "]", "\n", "val_users", "=", "user_splits", "[", "'val_users'", "]", "\n", "test_users", "=", "user_splits", "[", "'test_users'", "]", "\n", "\n", "doc2id", "=", "json", ".", "load", "(", "open", "(", "doc2id_file", ",", "'r'", ")", ")", "\n", "doc_splits", "=", "json", ".", "load", "(", "open", "(", "doc_splits_file", ",", "'r'", ")", ")", "\n", "train_docs", "=", "doc_splits", "[", "'train_docs'", "]", "\n", "val_docs", "=", "doc_splits", "[", "'val_docs'", "]", "\n", "test_docs", "=", "doc_splits", "[", "'test_docs'", "]", "\n", "\n", "user2id", "=", "json", ".", "load", "(", "open", "(", "user2id_file", ",", "'r'", ")", ")", "\n", "N", "=", "len", "(", "train_docs", ")", "+", "len", "(", "val_docs", ")", "+", "len", "(", "user2id", ")", "\n", "\n", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'vocab_lr_30_5.json'", ")", "\n", "vocab", "=", "self", ".", "build_vocab", "(", "vocab_file", ",", "dataset", ",", "train_docs", ",", "doc2id", ")", "\n", "vocab_size", "=", "len", "(", "vocab", ")", "\n", "stop_words", "=", "set", "(", "stopwords", ".", "words", "(", "'english'", ")", ")", "\n", "\n", "\n", "feat_matrix", "=", "lil_matrix", "(", "(", "N", ",", "vocab_size", ")", ")", "\n", "print", "(", "\"\\nSize of feature matrix = \"", ",", "feat_matrix", ".", "shape", ")", "\n", "print", "(", "\"\\nCreating feat_matrix entries for docs nodes...\"", ")", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "split_docs", "=", "train_docs", "+", "val_docs", "\n", "split_users", "=", "train_users", "+", "val_users", "\n", "\n", "for", "label", "in", "labels", ":", "\n", "                ", "src_doc_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'base_data'", ",", "dataset", ",", "label", ")", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "src_doc_dir", ")", ":", "\n", "                    ", "for", "count", ",", "file", "in", "enumerate", "(", "files", ")", ":", "\n", "                        ", "print_iter", "=", "int", "(", "len", "(", "files", ")", "/", "5", ")", "\n", "doc_name", "=", "file", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "if", "str", "(", "doc_name", ")", "in", "split_docs", ":", "\n", "                            ", "if", "str", "(", "doc_name", ")", "in", "doc2id", "and", "str", "(", "doc_name", ")", "not", "in", "test_docs", ":", "\n", "# feat_matrix[doc2id[str(doc_name)], :] = np.random.random(len(vocab)) > 0.99", "\n", "\n", "                                ", "doc_file", "=", "os", ".", "path", ".", "join", "(", "root", ",", "file", ")", "\n", "with", "open", "(", "doc_file", ",", "'r'", ")", "as", "f", ":", "\n", "                                    ", "file_content", "=", "json", ".", "load", "(", "f", ")", "\n", "text", "=", "file_content", "[", "'text'", "]", ".", "lower", "(", ")", "[", ":", "500", "]", "\n", "text", "=", "re", ".", "sub", "(", "r'#[\\w-]+'", ",", "'hashtag'", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'https?://\\S+'", ",", "'url'", ",", "text", ")", "\n", "text", "=", "text", ".", "replace", "(", "'\\t'", ",", "' '", ")", "\n", "text", "=", "text", ".", "replace", "(", "'\\n'", ",", "' '", ")", "\n", "# text = re.sub(r\"[^A-Za-z(),!?\\'`]\", \" \", text)", "\n", "text", "=", "nltk", ".", "word_tokenize", "(", "text", ")", "\n", "text", "=", "[", "w", "for", "w", "in", "text", "if", "not", "w", "in", "stop_words", "]", "\n", "vector", "=", "np", ".", "zeros", "(", "len", "(", "vocab", ")", ")", "\n", "for", "token", "in", "text", ":", "\n", "                                        ", "if", "token", "in", "vocab", ".", "keys", "(", ")", ":", "\n", "                                            ", "vector", "[", "vocab", "[", "token", "]", "]", "=", "1", "\n", "", "", "feat_matrix", "[", "doc2id", "[", "str", "(", "doc_name", ")", "]", ",", ":", "]", "=", "vector", "\n", "", "", "", "if", "count", "%", "print_iter", "==", "0", ":", "\n", "                            ", "print", "(", "\"{} / {} done..\"", ".", "format", "(", "count", "+", "1", ",", "len", "(", "files", ")", ")", ")", "\n", "\n", "", "", "", "", "hrs", ",", "mins", ",", "secs", "=", "self", ".", "calc_elapsed_time", "(", "start", ",", "time", ".", "time", "(", ")", ")", "\n", "print", "(", "\"Done. Took {}hrs and {}mins and {}secs\\n\"", ".", "format", "(", "hrs", ",", "mins", ",", "secs", ")", ")", "\n", "\n", "sum_1", "=", "np", ".", "array", "(", "feat_matrix", ".", "sum", "(", "axis", "=", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "print", "(", "sum_1", ".", "shape", ")", "\n", "idx", "=", "np", ".", "where", "(", "sum_1", "==", "0", ")", "\n", "print", "(", "len", "(", "idx", "[", "0", "]", ")", ")", "\n", "\n", "\n", "print", "(", "\"\\nCreating feat_matrix entries for users nodes...\"", ")", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "not_found", ",", "use", "=", "0", ",", "0", "\n", "# user_splits = json.load(open('./data/complete_data/{}/user_splits.json'.format(dataset), 'r'))", "\n", "# train_users = user_splits['train_users']", "\n", "src_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'complete'", ")", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "src_dir", ")", ":", "\n", "                ", "for", "count", ",", "file", "in", "enumerate", "(", "files", ")", ":", "\n", "                    ", "print_iter", "=", "int", "(", "len", "(", "files", ")", "/", "10", ")", "\n", "src_file_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "file", ")", "\n", "src_file", "=", "json", ".", "load", "(", "open", "(", "src_file_path", ",", "'r'", ")", ")", "\n", "users", "=", "src_file", "[", "'users'", "]", "\n", "doc_key", "=", "file", ".", "split", "(", "\".\"", ")", "[", "0", "]", "\n", "# if str(doc_key) in train_docs:", "\n", "# Each user of this doc has its features as the features of the doc", "\n", "if", "(", "str", "(", "doc_key", ")", "in", "split_docs", ")", "and", "str", "(", "doc_key", ")", "in", "doc2id", ":", "\n", "                        ", "for", "user", "in", "users", ":", "\n", "                            ", "if", "str", "(", "user", ")", "in", "user2id", ":", "\n", "                                ", "feat_matrix", "[", "user2id", "[", "str", "(", "user", ")", "]", ",", ":", "]", "+=", "feat_matrix", "[", "doc2id", "[", "str", "(", "doc_key", ")", "]", ",", ":", "]", "\n", "\n", "", "", "", "else", ":", "\n", "                        ", "if", "str", "(", "doc_key", ")", "in", "doc2id", ":", "\n", "                            ", "real_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'base_data'", ",", "dataset", ",", "'real'", ",", "str", "(", "doc_key", ")", "+", "'.json'", ")", "\n", "fake_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'base_data'", ",", "dataset", ",", "'fake'", ",", "str", "(", "doc_key", ")", "+", "'.json'", ")", "\n", "this_file", "=", "real_file", "if", "os", ".", "path", ".", "isfile", "(", "real_file", ")", "else", "fake_file", "\n", "if", "os", ".", "path", ".", "isfile", "(", "this_file", ")", ":", "\n", "# for user in users:", "\n", "#     if int(user) in test_users and int(user) not in split_users and str(user) in user2id:", "\n", "#         feat_matrix[user2id[str(user)], :] = np.random.random(len(vocab)) > 0.99", "\n", "\n", "                                ", "with", "open", "(", "this_file", ",", "'r'", ")", "as", "f", ":", "\n", "                                    ", "file_content", "=", "json", ".", "load", "(", "f", ")", "\n", "text", "=", "file_content", "[", "'text'", "]", ".", "lower", "(", ")", "[", ":", "500", "]", "\n", "text", "=", "re", ".", "sub", "(", "r'#[\\w-]+'", ",", "'hashtag'", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'https?://\\S+'", ",", "'url'", ",", "text", ")", "\n", "text", "=", "text", ".", "replace", "(", "'\\t'", ",", "' '", ")", "\n", "text", "=", "text", ".", "replace", "(", "'\\n'", ",", "' '", ")", "\n", "# text = re.sub(r\"[^A-Za-z(),!?\\'`]\", \" \", text)", "\n", "text", "=", "nltk", ".", "word_tokenize", "(", "text", ")", "\n", "for", "user", "in", "users", ":", "\n", "                                        ", "if", "int", "(", "user", ")", "in", "test_users", "and", "int", "(", "user", ")", "not", "in", "split_users", "and", "str", "(", "user", ")", "in", "user2id", ":", "\n", "                                            ", "vector", "=", "np", ".", "zeros", "(", "len", "(", "vocab", ")", ")", "\n", "for", "token", "in", "text", ":", "\n", "                                                ", "if", "token", "in", "vocab", ".", "keys", "(", ")", ":", "\n", "                                                    ", "vector", "[", "vocab", "[", "token", "]", "]", "=", "1", "\n", "", "", "feat_matrix", "[", "user2id", "[", "str", "(", "user", ")", "]", ",", ":", "]", "=", "vector", "\n", "\n", "", "", "", "", "", "", "if", "count", "%", "print_iter", "==", "0", ":", "\n", "                        ", "print", "(", "\" {} / {} done..\"", ".", "format", "(", "count", "+", "1", ",", "len", "(", "files", ")", ")", ")", "\n", "print", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", "\n", "\n", "", "", "", "hrs", ",", "mins", ",", "secs", "=", "self", ".", "calc_elapsed_time", "(", "start", ",", "time", ".", "time", "(", ")", ")", "\n", "print", "(", "not_found", ",", "use", ")", "\n", "print", "(", "\"Done. Took {}hrs and {}mins and {}secs\\n\"", ".", "format", "(", "hrs", ",", "mins", ",", "secs", ")", ")", "\n", "\n", "feat_matrix", "=", "feat_matrix", ">=", "1", "\n", "feat_matrix", "=", "feat_matrix", ".", "astype", "(", "int", ")", "\n", "\n", "# Sanity Checks", "\n", "sum_1", "=", "np", ".", "array", "(", "feat_matrix", ".", "sum", "(", "axis", "=", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "print", "(", "sum_1", ".", "shape", ")", "\n", "idx", "=", "np", ".", "where", "(", "sum_1", "==", "0", ")", "\n", "print", "(", "len", "(", "idx", "[", "0", "]", ")", ")", "\n", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'feat_matrix_lr_train_30_5.npz'", ")", "\n", "print", "(", "\"Matrix construction done! Saving in :   {}\"", ".", "format", "(", "filename", ")", ")", "\n", "save_npz", "(", "filename", ",", "feat_matrix", ".", "tocsr", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakenews.GNN_PreProcess_news.create_labels": [[587, 633], ["print", "os.path.join", "os.path.join", "json.load", "scipy.sparse.load_npz", "os.path.join", "json.load", "print", "print", "gnn_preprocess_lr_fakenews.GNN_PreProcess_news.save_labels", "open", "open", "os.path.join", "os.walk", "len", "enumerate", "gnn_preprocess_lr_fakenews.GNN_PreProcess_news.doc2labels.keys", "len", "len", "len", "gnn_preprocess_lr_fakenews.GNN_PreProcess_news.doc2id.keys", "root.split", "str", "str"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.save_labels"], ["", "", "def", "create_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Create labels for each node of the graph\n        \"\"\"", "\n", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "            ", "print", "(", "\"\\n\\n\"", "+", "\"-\"", "*", "100", "+", "\"\\n \\t\\t   Analyzing  {} dataset  for Creating Labels\\n\"", ".", "format", "(", "dataset", ")", "+", "'-'", "*", "100", ")", "\n", "\n", "\n", "doc2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'doc2id_lr_train_30_5.json'", ")", "\n", "adj_matrix_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'adj_matrix_lr_train_30_5.npz'", ")", "\n", "\n", "self", ".", "doc2id", "=", "json", ".", "load", "(", "open", "(", "doc2id_file", ",", "'r'", ")", ")", "\n", "adj_matrix", "=", "load_npz", "(", "adj_matrix_file", ")", "\n", "self", ".", "N", ",", "_", "=", "adj_matrix", ".", "shape", "\n", "del", "adj_matrix", "\n", "\n", "doc_splits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'doc_splits.json'", ")", "\n", "doc_splits", "=", "json", ".", "load", "(", "open", "(", "doc_splits_file", ",", "'r'", ")", ")", "\n", "train_docs", "=", "doc_splits", "[", "'train_docs'", "]", "\n", "val_docs", "=", "doc_splits", "[", "'val_docs'", "]", "\n", "\n", "split_docs", "=", "train_docs", "+", "val_docs", "\n", "\n", "print", "(", "\"\\nCreating doc2labels dictionary...\"", ")", "\n", "self", ".", "doc2labels", "=", "{", "}", "\n", "user_contexts", "=", "[", "'fake'", ",", "'real'", "]", "\n", "for", "user_context", "in", "user_contexts", ":", "\n", "                ", "data_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'complete_data'", ",", "dataset", ",", "user_context", ")", "\n", "label", "=", "1", "if", "user_context", "==", "'fake'", "else", "0", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "data_dir", ")", ":", "\n", "                    ", "for", "count", ",", "file", "in", "enumerate", "(", "files", ")", ":", "\n", "                        ", "doc", "=", "root", ".", "split", "(", "'\\\\'", ")", "[", "-", "1", "]", "\n", "if", "str", "(", "doc", ")", "in", "split_docs", ":", "\n", "                            ", "self", ".", "doc2labels", "[", "str", "(", "doc", ")", "]", "=", "label", "\n", "\n", "# print(len(doc2labels.keys()))", "\n", "# print(len(doc2id.keys()) - len(doc_splits['test_docs']))", "\n", "", "", "", "", "assert", "len", "(", "self", ".", "doc2labels", ".", "keys", "(", ")", ")", "==", "len", "(", "self", ".", "doc2id", ".", "keys", "(", ")", ")", "-", "len", "(", "doc_splits", "[", "'test_docs'", "]", ")", "\n", "print", "(", "\"Len of doc2labels  = {}\\n\"", ".", "format", "(", "len", "(", "self", ".", "doc2labels", ")", ")", ")", "\n", "self", ".", "doc2labels_file", "=", "'doc2labels_lr_train_30_5.json'", "\n", "self", ".", "labels_list_file", "=", "'labels_list_lr_train_30_5.json'", "\n", "self", ".", "all_labels_file", "=", "'all_labels_lr_train_30_5.json'", "\n", "\n", "self", ".", "save_labels", "(", "dataset", ")", "\n", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakenews.GNN_PreProcess_news.create_split_masks": [[636, 650], ["print", "os.path.join", "os.path.join", "os.path.join", "gnn_preprocess_lr_fakenews.GNN_PreProcess_news.create_split_masks_main"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.create_split_masks_main"], ["", "def", "create_split_masks", "(", "self", ")", ":", "\n", "        ", "\"\"\"create and save node masks for the train and val article nodes\"\"\"", "\n", "\n", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "            ", "print", "(", "\"\\n\\n\"", "+", "\"-\"", "*", "100", "+", "\"\\n \\t\\t   Creating split masks for {}\\n\"", ".", "format", "(", "dataset", ")", "+", "'-'", "*", "100", ")", "\n", "\n", "self", ".", "doc2id_train_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'doc2id_lr_train_30_5.json'", ")", "\n", "self", ".", "doc_splits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'doc_splits.json'", ")", "\n", "self", ".", "train_adj_matrix_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'adj_matrix_lr_train_30_5.npz'", ")", "\n", "\n", "self", ".", "split_mask_file", "=", "'split_mask_lr_30_5.json'", "\n", "self", ".", "create_split_masks_main", "(", "dataset", ")", "\n", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.__init__": [[16, 21], ["os.path.join"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "self", ".", "config", "=", "config", "\n", "self", ".", "data_dir", "=", "config", "[", "'data_dir'", "]", "\n", "self", ".", "comp_dir", "=", "os", ".", "path", ".", "join", "(", "'..'", ",", "'data'", ",", "'complete_data'", ")", "\n", "self", ".", "datasets", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.get_label_distribution": [[23, 28], ["labels.count", "labels.count"], "methods", ["None"], ["", "def", "get_label_distribution", "(", "self", ",", "labels", ")", ":", "\n", "        ", "fake", "=", "labels", ".", "count", "(", "1", ")", "\n", "real", "=", "labels", ".", "count", "(", "0", ")", "\n", "denom", "=", "fake", "+", "real", "\n", "return", "fake", "/", "denom", ",", "real", "/", "denom", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.calc_elapsed_time": [[31, 37], ["divmod", "divmod", "divmod", "divmod", "int", "int", "int"], "methods", ["None"], ["", "def", "calc_elapsed_time", "(", "self", ",", "start", ",", "end", ")", ":", "\n", "        ", "hours", ",", "rem", "=", "divmod", "(", "end", "-", "start", ",", "3600", ")", "\n", "time_hours", ",", "time_rem", "=", "divmod", "(", "end", ",", "3600", ")", "\n", "minutes", ",", "seconds", "=", "divmod", "(", "rem", ",", "60", ")", "\n", "time_mins", ",", "_", "=", "divmod", "(", "time_rem", ",", "60", ")", "\n", "return", "int", "(", "hours", ")", ",", "int", "(", "minutes", ")", ",", "int", "(", "seconds", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.save_adj_matrix": [[41, 73], ["os.path.join", "print", "scipy.sparse.save_npz", "os.path.join", "print", "scipy.sparse.save_npz", "print", "adj_matrix.nonzero", "numpy.vstack", "print", "os.path.join", "print", "numpy.save", "edge_index.squeeze.squeeze.toarray", "edge_index.squeeze.squeeze.squeeze", "print", "os.path.join", "print", "numpy.save", "adj_matrix.tocsr", "edge_type.tocsr", "numpy.array", "numpy.array", "edge_type.nonzero"], "methods", ["None"], ["", "def", "save_adj_matrix", "(", "self", ",", "dataset", ",", "adj_matrix", ",", "edge_type", ")", ":", "\n", "\n", "        ", "adj_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "self", ".", "adj_file", "+", "'.npz'", ")", "\n", "print", "(", "\"\\nMatrix construction done! Saving in  {}\"", ".", "format", "(", "adj_file", ")", ")", "\n", "save_npz", "(", "adj_file", ",", "adj_matrix", ".", "tocsr", "(", ")", ")", "\n", "# np.save(filename, adj_matrix)", "\n", "\n", "edge_type_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "self", ".", "edge_type_file", "+", "'.npz'", ")", "\n", "print", "(", "\"\\nedge_type construction done! Saving in  {}\"", ".", "format", "(", "edge_type_file", ")", ")", "\n", "save_npz", "(", "edge_type_file", ",", "edge_type", ".", "tocsr", "(", ")", ")", "\n", "\n", "# Creating an edge_list matrix of the adj_matrix as required by some GCN frameworks", "\n", "print", "(", "\"\\nCreating edge_index format of adj_matrix...\"", ")", "\n", "# G = nx.DiGraph(adj_matrix.tocsr())", "\n", "# temp_matrix = adj_matrix.toarray()", "\n", "# rows, cols = np.nonzero(temp_matrix)", "\n", "rows", ",", "cols", "=", "adj_matrix", ".", "nonzero", "(", ")", "\n", "\n", "edge_index", "=", "np", ".", "vstack", "(", "(", "np", ".", "array", "(", "rows", ")", ",", "np", ".", "array", "(", "cols", ")", ")", ")", "\n", "print", "(", "\"Edge index shape = \"", ",", "edge_index", ".", "shape", ")", "\n", "\n", "edge_matrix_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "self", ".", "adj_file", "+", "'_edge.npy'", ")", "\n", "print", "(", "\"saving edge_list format in :  \"", ",", "edge_matrix_file", ")", "\n", "np", ".", "save", "(", "edge_matrix_file", ",", "edge_index", ",", "allow_pickle", "=", "True", ")", "\n", "\n", "edge_index", "=", "edge_type", "[", "edge_type", ".", "nonzero", "(", ")", "]", "\n", "edge_index", "=", "edge_index", ".", "toarray", "(", ")", "\n", "edge_index", "=", "edge_index", ".", "squeeze", "(", "0", ")", "\n", "print", "(", "\"edge_type shape = \"", ",", "edge_index", ".", "shape", ")", "\n", "edge_matrix_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "self", ".", "edge_type_file", "+", "'_edge.npy'", ")", "\n", "print", "(", "\"saving edge_type edge list format in :  \"", ",", "edge_matrix_file", ")", "\n", "np", ".", "save", "(", "edge_matrix_file", ",", "edge_index", ",", "allow_pickle", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.build_vocab": [[78, 141], ["set", "time.time", "nltk.corpus.stopwords.words", "os.path.isfile", "print", "print", "json.load", "open", "os.path.join", "os.walk", "gnn_preprocess_main.GNN_PreProcess.calc_elapsed_time", "print", "print", "print", "os.path.join", "glob.glob", "print", "time.time", "len", "open", "json.dump", "open", "json.dump", "open", "json.load", "nltk.word_tokenize.replace", "nltk.word_tokenize.lower", "re.sub", "re.sub", "nltk.word_tokenize", "file.split", "os.path.join", "file_content[].replace", "str", "str", "open", "json.load", "re.sub", "re.sub", "nltk.word_tokenize", "json.load.keys", "len", "file_content[].lower", "json.load.keys", "len"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.calc_elapsed_time"], ["", "def", "build_vocab", "(", "self", ",", "vocab_file", ",", "dataset", ",", "train_docs", ",", "doc2id", ")", ":", "\n", "        ", "vocab", "=", "{", "}", "\n", "stop_words", "=", "set", "(", "stopwords", ".", "words", "(", "'english'", ")", ")", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "vocab_file", ")", ":", "\n", "            ", "print", "(", "\"\\nBuilding vocabulary...\"", ")", "\n", "if", "self", ".", "dataset", "in", "[", "'gossipcop'", ",", "'politifact'", "]", ":", "\n", "                ", "labels", "=", "[", "'fake'", ",", "'real'", "]", "\n", "for", "label", "in", "labels", ":", "\n", "                    ", "src_doc_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'base_data'", ",", "dataset", ",", "label", ")", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "src_doc_dir", ")", ":", "\n", "                        ", "for", "file", "in", "files", ":", "\n", "                            ", "doc", "=", "file", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "if", "str", "(", "doc", ")", "in", "train_docs", "and", "str", "(", "doc", ")", "in", "doc2id", ":", "\n", "                                ", "src_file_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "file", ")", "\n", "with", "open", "(", "src_file_path", ",", "'r'", ")", "as", "f", ":", "\n", "                                    ", "file_content", "=", "json", ".", "load", "(", "f", ")", "\n", "text", "=", "file_content", "[", "'text'", "]", ".", "lower", "(", ")", "[", ":", "500", "]", "\n", "text", "=", "re", ".", "sub", "(", "r'#[\\w-]+'", ",", "'hashtag'", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'https?://\\S+'", ",", "'url'", ",", "text", ")", "\n", "# text = re.sub(r\"[^A-Za-z(),!?\\'`]\", \" \", text)", "\n", "text", "=", "nltk", ".", "word_tokenize", "(", "text", ")", "\n", "text", "=", "[", "w", "for", "w", "in", "text", "if", "not", "w", "in", "stop_words", "]", "\n", "for", "token", "in", "text", ":", "\n", "                                        ", "if", "token", "not", "in", "vocab", ".", "keys", "(", ")", ":", "\n", "                                            ", "vocab", "[", "token", "]", "=", "len", "(", "vocab", ")", "\n", "\n", "\n", "", "", "", "", "", "", "hrs", ",", "mins", ",", "secs", "=", "self", ".", "calc_elapsed_time", "(", "start", ",", "time", ".", "time", "(", ")", ")", "\n", "print", "(", "\"Done. Took {}hrs and {}mins and {}secs\\n\"", ".", "format", "(", "hrs", ",", "mins", ",", "secs", ")", ")", "\n", "print", "(", "\"Size of vocab =  \"", ",", "len", "(", "vocab", ")", ")", "\n", "print", "(", "\"Saving vocab for  {}  at:  {}\"", ".", "format", "(", "dataset", ",", "vocab_file", ")", ")", "\n", "with", "open", "(", "vocab_file", ",", "'w+'", ")", "as", "v", ":", "\n", "                        ", "json", ".", "dump", "(", "vocab", ",", "v", ")", "\n", "\n", "", "", "", "elif", "self", ".", "dataset", "in", "[", "'HealthStory'", ",", "'HelathRelease'", "]", ":", "\n", "                ", "src_doc_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "config", "[", "'data_dir'", "]", ",", "'content'", ",", "dataset", "+", "\"/*.json\"", ")", "\n", "all_files", "=", "glob", ".", "glob", "(", "src_doc_dir", ")", "\n", "for", "file", "in", "all_files", ":", "\n", "                    ", "with", "open", "(", "file", ",", "'r'", ")", "as", "f", ":", "\n", "                        ", "file_content", "=", "json", ".", "load", "(", "f", ")", "\n", "text", "=", "file_content", "[", "'text'", "]", ".", "replace", "(", "'\\n'", ",", "' '", ")", "[", ":", "1500", "]", "\n", "text", "=", "text", ".", "replace", "(", "'\\t'", ",", "' '", ")", "\n", "text", "=", "text", ".", "lower", "(", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'#[\\w-]+'", ",", "'hashtag'", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'https?://\\S+'", ",", "'url'", ",", "text", ")", "\n", "# text = re.sub(r\"[^A-Za-z(),!?\\'`]\", \" \", text)", "\n", "text", "=", "nltk", ".", "word_tokenize", "(", "text", ")", "\n", "text", "=", "[", "w", "for", "w", "in", "text", "if", "not", "w", "in", "stop_words", "]", "\n", "for", "token", "in", "text", ":", "\n", "                            ", "if", "token", "not", "in", "vocab", ".", "keys", "(", ")", ":", "\n", "                                ", "vocab", "[", "token", "]", "=", "len", "(", "vocab", ")", "\n", "\n", "", "", "", "", "print", "(", "\"Saving vocab for  {}  at:  {}\"", ".", "format", "(", "dataset", ",", "vocab_file", ")", ")", "\n", "with", "open", "(", "vocab_file", ",", "'w+'", ")", "as", "v", ":", "\n", "                    ", "json", ".", "dump", "(", "vocab", ",", "v", ")", "\n", "\n", "", "", "", "else", ":", "\n", "            ", "print", "(", "\"\\nReading vocabulary from:  \"", ",", "vocab_file", ")", "\n", "vocab", "=", "json", ".", "load", "(", "open", "(", "vocab_file", ",", "'r'", ")", ")", "\n", "\n", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.save_labels": [[147, 184], ["os.path.join", "print", "numpy.zeros", "gnn_preprocess_main.GNN_PreProcess.doc2labels.items", "os.path.join", "list", "print", "numpy.zeros", "os.path.join", "gnn_preprocess_main.GNN_PreProcess.doc2labels.keys", "list", "print", "print", "print", "open", "json.dump", "open", "json.dump", "sum", "len", "open", "json.dump", "str", "str", "str"], "methods", ["None"], ["", "def", "save_labels", "(", "self", ",", "dataset", ")", ":", "\n", "        ", "self", ".", "doc2labels_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "self", ".", "doc2labels_file", ")", "\n", "print", "(", "\"Saving doc2labels for  {} at:  {}\"", ".", "format", "(", "dataset", ",", "self", ".", "doc2labels_file", ")", ")", "\n", "with", "open", "(", "self", ".", "doc2labels_file", ",", "'w+'", ")", "as", "v", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "doc2labels", ",", "v", ")", "\n", "\n", "", "labels_list", "=", "np", ".", "zeros", "(", "self", ".", "N", ")", "\n", "for", "key", ",", "value", "in", "self", ".", "doc2labels", ".", "items", "(", ")", ":", "\n", "            ", "labels_list", "[", "self", ".", "doc2id", "[", "str", "(", "key", ")", "]", "]", "=", "value", "\n", "\n", "# Sanity Checks", "\n", "# print(sum(labels_list))", "\n", "# print(len(labels_list))", "\n", "# print(sum(labels_list[2402:]))", "\n", "# print(sum(labels_list[:2402]))", "\n", "\n", "", "self", ".", "labels_list_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "self", ".", "labels_list_file", ")", "\n", "temp_dict", "=", "{", "}", "\n", "temp_dict", "[", "'labels_list'", "]", "=", "list", "(", "labels_list", ")", "\n", "print", "(", "\"Labels list construction done! Saving in :   {}\"", ".", "format", "(", "self", ".", "labels_list_file", ")", ")", "\n", "with", "open", "(", "self", ".", "labels_list_file", ",", "'w+'", ")", "as", "v", ":", "\n", "            ", "json", ".", "dump", "(", "temp_dict", ",", "v", ")", "\n", "\n", "\n", "# Create the all_labels file", "\n", "", "all_labels", "=", "np", ".", "zeros", "(", "self", ".", "N", ")", "\n", "self", ".", "all_labels_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "self", ".", "all_labels_file", ")", "\n", "for", "doc", "in", "self", ".", "doc2labels", ".", "keys", "(", ")", ":", "\n", "            ", "all_labels", "[", "self", ".", "doc2id", "[", "str", "(", "doc", ")", "]", "]", "=", "self", ".", "doc2labels", "[", "str", "(", "doc", ")", "]", "\n", "\n", "", "temp_dict", "=", "{", "}", "\n", "temp_dict", "[", "'all_labels'", "]", "=", "list", "(", "all_labels", ")", "\n", "print", "(", "\"Sum of labels this test set = \"", ",", "sum", "(", "all_labels", ")", ")", "\n", "print", "(", "\"Len of labels = \"", ",", "len", "(", "all_labels", ")", ")", "\n", "print", "(", "\"all_labels list construction done! Saving in :   {}\"", ".", "format", "(", "self", ".", "all_labels_file", ")", ")", "\n", "with", "open", "(", "self", ".", "all_labels_file", ",", "'w+'", ")", "as", "j", ":", "\n", "            ", "json", ".", "dump", "(", "temp_dict", ",", "j", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.create_split_masks_main": [[189, 225], ["json.load", "json.load", "scipy.sparse.load_npz", "numpy.ones", "json.load.items", "print", "print", "print", "list", "list", "list", "os.path.join", "print", "open", "open", "numpy.zeros", "numpy.zeros", "sum", "sum", "open", "json.dump", "str", "str"], "methods", ["None"], ["", "", "def", "create_split_masks_main", "(", "self", ",", "dataset", ")", ":", "\n", "        ", "doc2id", "=", "json", ".", "load", "(", "open", "(", "self", ".", "doc2id_file", ",", "'r'", ")", ")", "\n", "doc_splits", "=", "json", ".", "load", "(", "open", "(", "self", ".", "doc_splits_file", ",", "'r'", ")", ")", "\n", "train_adj", "=", "load_npz", "(", "self", ".", "train_adj_matrix_file", ")", "\n", "\n", "train_docs", "=", "doc_splits", "[", "'train_docs'", "]", "\n", "val_docs", "=", "doc_splits", "[", "'val_docs'", "]", "\n", "\n", "train_n", ",", "_", "=", "train_adj", ".", "shape", "\n", "del", "train_adj", "\n", "\n", "train_mask", ",", "val_mask", "=", "np", ".", "zeros", "(", "train_n", ")", ",", "np", ".", "zeros", "(", "train_n", ")", "# np.zeros(test_n)", "\n", "representation_mask", "=", "np", ".", "ones", "(", "train_n", ")", "\n", "\n", "not_in_either", "=", "0", "\n", "for", "doc", ",", "id", "in", "doc2id", ".", "items", "(", ")", ":", "\n", "            ", "if", "str", "(", "doc", ")", "in", "train_docs", ":", "\n", "                ", "train_mask", "[", "id", "]", "=", "1", "\n", "", "elif", "str", "(", "doc", ")", "in", "val_docs", ":", "\n", "                ", "val_mask", "[", "id", "]", "=", "1", "\n", "representation_mask", "[", "id", "]", "=", "0", "\n", "", "else", ":", "\n", "                ", "not_in_either", "+=", "1", "\n", "\n", "", "", "print", "(", "\"\\nNot_in_either = \"", ",", "not_in_either", ")", "\n", "print", "(", "\"train_mask sum = \"", ",", "sum", "(", "train_mask", ")", ")", "\n", "print", "(", "\"val_mask sum = \"", ",", "sum", "(", "val_mask", ")", ")", "\n", "\n", "temp_dict", "=", "{", "}", "\n", "temp_dict", "[", "'train_mask'", "]", "=", "list", "(", "train_mask", ")", "\n", "temp_dict", "[", "'val_mask'", "]", "=", "list", "(", "val_mask", ")", "\n", "temp_dict", "[", "'repr_mask'", "]", "=", "list", "(", "representation_mask", ")", "\n", "self", ".", "split_mask_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "self", ".", "split_mask_file", ")", "\n", "print", "(", "\"Writing split mask file in : \"", ",", "self", ".", "split_mask_file", ")", "\n", "with", "open", "(", "self", ".", "split_mask_file", ",", "'w+'", ")", "as", "j", ":", "\n", "            ", "json", ".", "dump", "(", "temp_dict", ",", "j", ")", "", "", "", "", ""]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.data_prep_fakenews.create_base_directory": [[12, 57], ["print", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.walk", "os.walk", "print", "shutil.move", "os.path.join", "os.path.join", "os.path.join.split", "os.rename", "os.path.join.split", "file_list.append"], "function", ["None"], ["def", "create_base_directory", "(", ")", ":", "\n", "    ", "\"\"\"\n    The crawled folder has files divided by labels and dataset and all individual content in their respective folders.\n    This script takes them from their respective folders and moves them to a common folder for easier processing later on.\n    \"\"\"", "\n", "labels", "=", "[", "'real'", ",", "'fake'", "]", "\n", "dataset", "=", "[", "'gossipcop'", ",", "'politifact'", "]", "\n", "for", "data", "in", "dataset", ":", "\n", "        ", "for", "label", "in", "labels", ":", "\n", "            ", "print", "(", "\"\\n --> Processing :  {} dataset ({} labels)\"", ".", "format", "(", "data", ",", "label", ")", ")", "\n", "\n", "src_path", "=", "os", ".", "path", ".", "join", "(", "'../new/FakeNewsNet/code/fakenewsnet_dataset'", ",", "data", ")", "\n", "src_path", "=", "os", ".", "path", ".", "join", "(", "src_path", ",", "label", ")", "\n", "\n", "dest_path", "=", "os", ".", "path", ".", "join", "(", "'base_data'", ",", "data", ")", "\n", "dest_path", "=", "os", ".", "path", ".", "join", "(", "dest_path", ",", "label", ")", "\n", "\n", "\n", "# Renaming the JSON files as unique IDs", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "src_path", ")", ":", "\n", "                ", "for", "file", "in", "files", ":", "\n", "                    ", "orig_file_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "file", ")", "\n", "folder_name", "=", "orig_file_path", ".", "split", "(", "\"\\\\\"", ")", "[", "-", "2", "]", "\n", "if", "folder_name", "==", "'tweets'", "or", "folder_name", "==", "'retweets'", ":", "\n", "                        ", "continue", "\n", "", "else", ":", "\n", "# print(\"Original file path  =    \", orig_file_path)", "\n", "# print(\"Folder name  =    \", folder_name)", "\n", "                        ", "new_file_name", "=", "\"/\"", "+", "folder_name", "+", "\".json\"", "\n", "os", ".", "rename", "(", "orig_file_path", ",", "root", "+", "new_file_name", ")", "\n", "\n", "# Moving files from individual folders to one central folder", "\n", "", "", "", "file_list", "=", "[", "]", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "src_path", ")", ":", "\n", "                ", "for", "file", "in", "files", ":", "\n", "                    ", "orig_file_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "file", ")", "\n", "folder_name", "=", "orig_file_path", ".", "split", "(", "\"\\\\\"", ")", "[", "-", "2", "]", "\n", "if", "folder_name", "==", "'tweets'", "or", "folder_name", "==", "'retweets'", ":", "\n", "                        ", "continue", "\n", "", "else", ":", "\n", "                        ", "file_list", ".", "append", "(", "orig_file_path", ")", "\n", "", "", "", "for", "f", "in", "file_list", ":", "\n", "                ", "shutil", ".", "move", "(", "f", ",", "dest_path", ")", "\n", "", "print", "(", "\"DONE\"", ")", "\n", "", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.data_prep_fakenews.prepare_data_corpus": [[61, 102], ["os.path.join", "os.path.join", "print", "print", "os.path.join", "glob.glob", "print", "open", "csv.writer", "csv.writer.writerow", "open", "json.load", "csv.writer.writerow", "[].replace", "[].replace", "str", "file.split", "file.split"], "function", ["None"], ["", "def", "prepare_data_corpus", "(", "new_dir", "=", "os", ".", "path", ".", "join", "(", "'data'", ",", "'base_data'", ")", ")", ":", "\n", "    ", "\"\"\"\n    This method reads all the individual JSON files of both the datasets and creates separate .tsv files for each.\n    The .tsv file contains the fields: ID, article title, article content and the label\n    \"\"\"", "\n", "labels", "=", "[", "'real'", ",", "'fake'", "]", "\n", "dataset", "=", "[", "'gossipcop'", ",", "'politifact'", "]", "\n", "for", "data", "in", "dataset", ":", "\n", "        ", "c", "=", "0", "\n", "doc2id", "=", "{", "}", "\n", "final_data_file", "=", "os", ".", "path", ".", "join", "(", "'data'", ",", "data", "+", "'.tsv'", ")", "\n", "\n", "for", "label", "in", "labels", ":", "\n", "            ", "print", "(", "\"\\n --> Processing :  {} dataset ({} labels)\"", ".", "format", "(", "data", ",", "label", ")", ")", "\n", "data_label", "=", "1", "if", "label", "==", "'fake'", "else", "0", "\n", "print", "(", "data_label", ")", "\n", "DATA_DIR", "=", "os", ".", "path", ".", "join", "(", "new_dir", ",", "data", ",", "label", ")", "\n", "DATA_DIR", "=", "DATA_DIR", "+", "\"/*.json\"", "\n", "\n", "all_files", "=", "glob", ".", "glob", "(", "DATA_DIR", ")", "\n", "with", "open", "(", "final_data_file", ",", "'a'", ",", "encoding", "=", "'utf-8'", ")", "as", "csv_file", ":", "\n", "                ", "csv_writer", "=", "csv", ".", "writer", "(", "csv_file", ",", "delimiter", "=", "'\\t'", ")", "\n", "csv_writer", ".", "writerow", "(", "[", "'id'", ",", "'title'", ",", "'text'", ",", "'label'", "]", ")", "\n", "for", "file", "in", "all_files", ":", "\n", "                    ", "with", "open", "(", "file", ",", "'r'", ")", "as", "f", ":", "\n", "                        ", "file_content", "=", "json", ".", "load", "(", "f", ")", "\n", "if", "data", "==", "'politifact'", ":", "\n", "                            ", "ID", "=", "file", ".", "split", "(", "'politifact'", ")", "[", "2", "]", ".", "replace", "(", "'.json'", ",", "''", ")", "\n", "", "else", ":", "\n", "# ID = file.split('\\\\')[-1]", "\n", "# ID = ID.split('.')[0]", "\n", "                            ", "ID", "=", "file", ".", "split", "(", "'gossipcop-'", ")", "[", "1", "]", ".", "replace", "(", "'.json'", ",", "''", ")", "\n", "doc2id", "[", "str", "(", "ID", ")", "]", "=", "c", "\n", "c", "+=", "1", "\n", "", "csv_writer", ".", "writerow", "(", "[", "ID", ",", "file_content", "[", "'title'", "]", ",", "file_content", "[", "'text'", "]", ",", "data_label", "]", ")", "\n", "", "", "", "print", "(", "\"Done: \"", ",", "label", ")", "\n", "# doc2id_file = os.path.join('complete_data', data, 'doc2id_encoder.json')", "\n", "# print(\"Saving doc2id_glove in :\", doc2id_file)", "\n", "# with open(doc2id_file, 'w+') as v:", "\n", "#     json.dump(doc2id, v)", "\n", "", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.data_prep_fakenews.create_data_splits": [[106, 218], ["print", "print", "os.path.join", "print", "print", "print", "print", "sklearn.model_selection.StratifiedShuffleSplit", "sklearn.model_selection.StratifiedShuffleSplit.split", "sklearn.model_selection.StratifiedShuffleSplit", "enumerate", "data_prep_fakenews.get_label_distribution", "print", "print", "data_prep_fakenews.get_label_distribution", "print", "print", "data_prep_fakenews.get_label_distribution", "print", "print", "print", "os.path.join", "print", "json.dump", "open", "csv.DictReader", "max", "min", "sklearn.model_selection.StratifiedShuffleSplit.split", "os.path.join", "os.path.join", "print", "open", "sum", "len", "x_rest.append", "y_rest.append", "doc_id_rest.append", "x_test.append", "y_test.append", "doc_id_test.append", "x_train.append", "y_train.append", "doc_id_train.append", "x_val.append", "y_val.append", "doc_id_val.append", "os.path.exists", "os.makedirs", "open", "csv.writer", "range", "isinstance", "row[].replace", "re.sub.replace", "re.sub", "re.sub", "x_data.append", "lens.append", "y_data.append", "doc_id.append", "len", "csv.writer.writerow", "len", "str", "len", "int", "str"], "function", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.data_prep_fakenews.get_label_distribution", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.data_prep_fakenews.get_label_distribution", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.data_prep_fakenews.get_label_distribution"], ["", "def", "create_data_splits", "(", "max_len", "=", "5000", ")", ":", "\n", "    ", "\"\"\"\n    This method creates train-val-test via random splitting of the dataset in a stratified fashion to ensure similar data distribution\n    \"\"\"", "\n", "print", "(", "\"\\n\\n\"", "+", "\"=\"", "*", "50", "+", "\"\\n\\t\\tCreating Data Splits\\n\"", "+", "\"=\"", "*", "50", ")", "\n", "\n", "datasets", "=", "[", "'gossipcop'", ",", "'politifact'", "]", "\n", "for", "dataset", "in", "datasets", ":", "\n", "        ", "print", "(", "\"\\nPreparing {} ...\"", ".", "format", "(", "dataset", ")", ")", "\n", "src_dir", "=", "os", ".", "path", ".", "join", "(", "'data'", ",", "dataset", "+", "'.tsv'", ")", "\n", "x_data", ",", "y_data", ",", "doc_id", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "# Reading the dataset into workable lists", "\n", "removed", "=", "0", "\n", "lens", "=", "[", "]", "\n", "with", "open", "(", "src_dir", ",", "encoding", "=", "'utf-8'", ")", "as", "data", ":", "\n", "            ", "reader", "=", "csv", ".", "DictReader", "(", "data", ",", "delimiter", "=", "'\\t'", ")", "\n", "for", "row", "in", "reader", ":", "\n", "                ", "if", "isinstance", "(", "row", "[", "'text'", "]", ",", "str", ")", "and", "len", "(", "row", "[", "'text'", "]", ")", ">", "25", ":", "\n", "                    ", "text", "=", "row", "[", "'text'", "]", ".", "replace", "(", "'\\n'", ",", "' '", ")", "\n", "text", "=", "text", ".", "replace", "(", "'\\t'", ",", "' '", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'#[\\w-]+'", ",", "'hashtag'", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'https?://\\S+'", ",", "'url'", ",", "text", ")", "\n", "\n", "x_data", ".", "append", "(", "str", "(", "text", "[", ":", "max_len", "]", ")", ")", "\n", "lens", ".", "append", "(", "len", "(", "text", "[", ":", "max_len", "]", ")", ")", "\n", "y_data", ".", "append", "(", "int", "(", "row", "[", "'label'", "]", ")", ")", "\n", "doc_id", ".", "append", "(", "str", "(", "row", "[", "'id'", "]", ")", ")", "\n", "", "else", ":", "\n", "                    ", "removed", "+=", "1", "\n", "", "", "", "print", "(", "\"avg lens = \"", ",", "sum", "(", "lens", ")", "/", "len", "(", "lens", ")", ")", "\n", "print", "(", "\"max lens = \"", ",", "max", "(", "lens", ")", ")", "\n", "print", "(", "\"minimum lens = \"", ",", "min", "(", "lens", ")", ")", "\n", "print", "(", "\"Total data points removed = \"", ",", "removed", ")", "\n", "\n", "# Creating train-val-test split with same/similar label distribution in each split", "\n", "sss", "=", "StratifiedShuffleSplit", "(", "n_splits", "=", "1", ",", "test_size", "=", "0.20", ",", "random_state", "=", "21", ")", "\n", "x_rest", ",", "x_test", ",", "y_rest", ",", "y_test", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "doc_id_rest", ",", "doc_id_test", "=", "[", "]", ",", "[", "]", "\n", "for", "train_index", ",", "test_index", "in", "sss", ".", "split", "(", "x_data", ",", "y_data", ")", ":", "\n", "            ", "for", "idx", "in", "train_index", ":", "\n", "                ", "x_rest", ".", "append", "(", "x_data", "[", "idx", "]", ")", "\n", "y_rest", ".", "append", "(", "y_data", "[", "idx", "]", ")", "\n", "doc_id_rest", ".", "append", "(", "doc_id", "[", "idx", "]", ")", "\n", "\n", "", "for", "idx", "in", "test_index", ":", "\n", "                ", "x_test", ".", "append", "(", "x_data", "[", "idx", "]", ")", "\n", "y_test", ".", "append", "(", "y_data", "[", "idx", "]", ")", "\n", "doc_id_test", ".", "append", "(", "doc_id", "[", "idx", "]", ")", "\n", "\n", "\n", "", "", "sss", "=", "StratifiedShuffleSplit", "(", "n_splits", "=", "1", ",", "test_size", "=", "0.10", ",", "random_state", "=", "21", ")", "\n", "for", "fold", ",", "(", "train_index", ",", "val_index", ")", "in", "enumerate", "(", "sss", ".", "split", "(", "x_rest", ",", "y_rest", ")", ")", ":", "\n", "            ", "x_train", ",", "x_val", ",", "y_train", ",", "y_val", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "doc_id_train", ",", "doc_id_val", "=", "[", "]", ",", "[", "]", "\n", "for", "idx", "in", "train_index", ":", "\n", "                ", "x_train", ".", "append", "(", "x_rest", "[", "idx", "]", ")", "\n", "y_train", ".", "append", "(", "y_rest", "[", "idx", "]", ")", "\n", "doc_id_train", ".", "append", "(", "doc_id_rest", "[", "idx", "]", ")", "\n", "", "for", "idx", "in", "val_index", ":", "\n", "                ", "x_val", ".", "append", "(", "x_rest", "[", "idx", "]", ")", "\n", "y_val", ".", "append", "(", "y_rest", "[", "idx", "]", ")", "\n", "doc_id_val", ".", "append", "(", "doc_id_rest", "[", "idx", "]", ")", "\n", "\n", "\n", "", "", "fake", ",", "real", "=", "get_label_distribution", "(", "y_train", ")", "\n", "print", "(", "\"\\nFake labels in train split  = {:.2f} %\"", ".", "format", "(", "fake", "*", "100", ")", ")", "\n", "print", "(", "\"Real labels in train split  = {:.2f} %\"", ".", "format", "(", "real", "*", "100", ")", ")", "\n", "\n", "fake", ",", "real", "=", "get_label_distribution", "(", "y_val", ")", "\n", "print", "(", "\"\\nFake labels in val split  = {:.2f} %\"", ".", "format", "(", "fake", "*", "100", ")", ")", "\n", "print", "(", "\"Real labels in val split  = {:.2f} %\"", ".", "format", "(", "real", "*", "100", ")", ")", "\n", "\n", "fake", ",", "real", "=", "get_label_distribution", "(", "y_test", ")", "\n", "print", "(", "\"\\nFake labels in test split = {:.2f} %\"", ".", "format", "(", "fake", "*", "100", ")", ")", "\n", "print", "(", "\"Real labels in test split  = {:.2f} %\"", ".", "format", "(", "real", "*", "100", ")", ")", "\n", "\n", "print", "(", "\"\\nWriting train-val-test files..\"", ")", "\n", "splits", "=", "[", "'train'", ",", "'val'", ",", "'test'", "]", "\n", "for", "split", "in", "splits", ":", "\n", "            ", "if", "split", "==", "'train'", ":", "\n", "                ", "x", "=", "x_train", "\n", "y", "=", "y_train", "\n", "id_list", "=", "doc_id_train", "\n", "", "elif", "split", "==", "'val'", ":", "\n", "                ", "x", "=", "x_val", "\n", "y", "=", "y_val", "\n", "id_list", "=", "doc_id_val", "\n", "", "else", ":", "\n", "                ", "x", "=", "x_test", "\n", "y", "=", "y_test", "\n", "id_list", "=", "doc_id_test", "\n", "\n", "", "write_dir", "=", "os", ".", "path", ".", "join", "(", "'data'", ",", "dataset", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "write_dir", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "write_dir", ")", "\n", "", "write_dir", "=", "os", ".", "path", ".", "join", "(", "write_dir", ",", "split", "+", "'.tsv'", ")", "\n", "print", "(", "\"{} file in : {}\"", ".", "format", "(", "split", ",", "write_dir", ")", ")", "\n", "with", "open", "(", "write_dir", ",", "'a'", ",", "encoding", "=", "'utf-8'", ",", "newline", "=", "''", ")", "as", "csv_file", ":", "\n", "                ", "csv_writer", "=", "csv", ".", "writer", "(", "csv_file", ",", "delimiter", "=", "'\\t'", ")", "\n", "# csv_writer.writerow(['text', 'label'])", "\n", "for", "i", "in", "range", "(", "len", "(", "x", ")", ")", ":", "\n", "                    ", "csv_writer", ".", "writerow", "(", "[", "x", "[", "i", "]", ",", "y", "[", "i", "]", ",", "id_list", "[", "i", "]", "]", ")", "\n", "\n", "\n", "", "", "", "temp_dict", "=", "{", "}", "\n", "temp_dict", "[", "'test_docs'", "]", "=", "doc_id_test", "\n", "temp_dict", "[", "'train_docs'", "]", "=", "doc_id_train", "\n", "temp_dict", "[", "'val_docs'", "]", "=", "doc_id_val", "\n", "doc_splits_file", "=", "os", ".", "path", ".", "join", "(", "'data'", ",", "dataset", ",", "'doc_splits.json'", ")", "\n", "print", "(", "\"Writing doc_splits in : \"", ",", "doc_splits_file", ")", "\n", "json", ".", "dump", "(", "temp_dict", ",", "open", "(", "doc_splits_file", ",", "'w+'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.data_prep_fakenews.get_label_distribution": [[222, 227], ["labels.count", "labels.count"], "function", ["None"], ["", "", "def", "get_label_distribution", "(", "labels", ")", ":", "\n", "    ", "fake", "=", "labels", ".", "count", "(", "1", ")", "\n", "real", "=", "labels", ".", "count", "(", "0", ")", "\n", "denom", "=", "fake", "+", "real", "\n", "return", "fake", "/", "denom", ",", "real", "/", "denom", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.data_prep_fakenews.get_data_size": [[230, 259], ["os.path.join", "os.walk", "print", "print", "print", "print", "print", "max", "os.path.join", "sum", "len", "open", "json.load", "file_content[].lower", "lengths.append", "len", "len", "small.append", "root.split", "file.split"], "function", ["None"], ["", "def", "get_data_size", "(", ")", ":", "\n", "\n", "    ", "src_doc_dir", "=", "os", ".", "path", ".", "join", "(", "'data'", ",", "'base_data'", ",", "'gossipcop'", ")", "\n", "count", ",", "total", ",", "fake", ",", "real", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "small", "=", "[", "]", "\n", "lengths", "=", "[", "]", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "src_doc_dir", ")", ":", "\n", "        ", "for", "file", "in", "files", ":", "\n", "            ", "src_file_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "file", ")", "\n", "with", "open", "(", "src_file_path", ",", "'r'", ")", "as", "f", ":", "\n", "                ", "total", "+=", "1", "\n", "file_content", "=", "json", ".", "load", "(", "f", ")", "\n", "text", "=", "file_content", "[", "'text'", "]", ".", "lower", "(", ")", "\n", "lengths", ".", "append", "(", "len", "(", "text", ")", ")", "\n", "if", "len", "(", "text", ")", ">", "25", ":", "\n", "                    ", "split", "=", "root", ".", "split", "(", "'\\\\'", ")", "[", "-", "1", "]", "\n", "if", "split", "==", "'fake'", ":", "\n", "                        ", "fake", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "real", "+=", "1", "\n", "", "doc", "=", "file", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "small", ".", "append", "(", "doc", ")", "\n", "count", "+=", "1", "\n", "\n", "", "", "", "", "print", "(", "count", ",", "total", ")", "\n", "print", "(", "fake", ",", "real", ")", "\n", "print", "(", "max", "(", "lengths", ")", ")", "\n", "print", "(", "sum", "(", "lengths", ")", "/", "len", "(", "lengths", ")", ")", "\n", "print", "(", "small", "[", ":", "10", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.__init__": [[19, 42], ["gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.datasets.append", "gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.datasets.append", "gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_aggregate_folder", "gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_doc_user_splits", "gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_dicts", "gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_filtered_follower_following", "gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_adj_matrix", "gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_feat_matrix", "gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_labels", "gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_split_masks"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_aggregate_folder", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_doc_user_splits", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_dicts", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_filtered_follower_following", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_adj_matrix", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_feat_matrix", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_labels", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_split_masks"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "release", "=", "False", ",", "story", "=", "False", ")", ":", "\n", "\n", "        ", "if", "release", ":", "\n", "            ", "self", ".", "datasets", ".", "append", "(", "'HealthRelease'", ")", "\n", "", "if", "story", ":", "\n", "            ", "self", ".", "datasets", ".", "append", "(", "'HealthStory'", ")", "\n", "\n", "", "if", "config", "[", "'create_aggregate_folder'", "]", ":", "\n", "            ", "self", ".", "create_aggregate_folder", "(", ")", "\n", "", "if", "config", "[", "'create_doc_user_splits'", "]", ":", "\n", "            ", "self", ".", "create_doc_user_splits", "(", ")", "\n", "", "if", "config", "[", "'create_dicts'", "]", ":", "\n", "            ", "self", ".", "create_dicts", "(", ")", "\n", "", "if", "config", "[", "'create_filtered_follower_following'", "]", ":", "\n", "            ", "self", ".", "create_filtered_follower_following", "(", ")", "\n", "", "if", "config", "[", "'create_adj_matrix'", "]", ":", "\n", "            ", "self", ".", "create_adj_matrix", "(", ")", "\n", "", "if", "config", "[", "'create_feat_matrix'", "]", ":", "\n", "            ", "self", ".", "create_feat_matrix", "(", ")", "\n", "", "if", "config", "[", "'create_labels'", "]", ":", "\n", "            ", "self", ".", "create_labels", "(", ")", "\n", "", "if", "config", "[", "'create_split_masks'", "]", ":", "\n", "            ", "self", ".", "create_split_masks", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_aggregate_folder": [[48, 87], ["print", "os.path.join", "os.path.join", "collections.defaultdict", "os.walk", "print", "print", "collections.defaultdict.items", "print", "os.path.exists", "print", "os.makedirs", "root.endswith", "enumerate", "os.path.join", "file.startswith", "os.path.join", "json.load", "docs_done[].append", "open", "list", "json.dump", "root.split", "open", "print", "str", "set"], "methods", ["None"], ["", "", "def", "create_aggregate_folder", "(", "self", ")", ":", "\n", "        ", "\"\"\"Create the 'complete' folder that has files as doc_IDs and contains alll the users that interacted with it \"\"\"", "\n", "\n", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "            ", "print", "(", "\"\\n\"", "+", "\"-\"", "*", "100", "+", "\"\\n \\t\\t Processing {} dataset for aggregating engagement info\\n\"", ".", "format", "(", "dataset", ")", "+", "'-'", "*", "100", ")", "\n", "src_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'engagements'", ",", "dataset", ")", "\n", "dest_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'engagements'", ",", "'complete'", ",", "dataset", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "dest_dir", ")", ":", "\n", "                ", "print", "(", "\"Creating dir:  {}\\n\"", ".", "format", "(", "dest_dir", ")", ")", "\n", "os", ".", "makedirs", "(", "dest_dir", ")", "\n", "", "c", "=", "0", "\n", "docs_done", "=", "defaultdict", "(", "list", ")", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "src_dir", ")", ":", "\n", "                ", "if", "root", ".", "endswith", "(", "'replies'", ")", ":", "\n", "                    ", "continue", "\n", "", "for", "count", ",", "file", "in", "enumerate", "(", "files", ")", ":", "\n", "                    ", "if", "file", ".", "startswith", "(", "'.'", ")", ":", "\n", "                        ", "continue", "\n", "", "c", "+=", "1", "\n", "doc", "=", "root", ".", "split", "(", "'\\\\'", ")", "[", "-", "2", "]", "\n", "src_file_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "file", ")", "\n", "src_file", "=", "json", ".", "load", "(", "open", "(", "src_file_path", ",", "'r'", ")", ")", "\n", "user", "=", "src_file", "[", "'user'", "]", "[", "'id'", "]", "\n", "docs_done", "[", "doc", "]", ".", "append", "(", "user", ")", "\n", "if", "c", "%", "10000", "==", "0", ":", "\n", "                        ", "print", "(", "\"{} done\"", ".", "format", "(", "c", ")", ")", "\n", "\n", "\n", "", "", "", "print", "(", "\"\\nTotal tweets/re-tweets in the data set = \"", ",", "c", ")", "\n", "print", "(", "\"\\nWriting all the info in the dir: \"", ",", "dest_dir", ")", "\n", "for", "doc", ",", "user_list", "in", "docs_done", ".", "items", "(", ")", ":", "\n", "                ", "write_file", "=", "os", ".", "path", ".", "join", "(", "dest_dir", ",", "str", "(", "doc", ")", "+", "'.json'", ")", "\n", "with", "open", "(", "write_file", ",", "'w+'", ")", "as", "j", ":", "\n", "                    ", "temp_dict", "=", "{", "}", "\n", "temp_dict", "[", "'users'", "]", "=", "list", "(", "set", "(", "user_list", ")", ")", "\n", "json", ".", "dump", "(", "temp_dict", ",", "j", ")", "\n", "", "", "print", "(", "\"\\nDONE..!!\"", ")", "\n", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_doc_user_splits": [[91, 131], ["print", "os.path.join", "json.load", "print", "os.path.join", "os.walk", "list", "list", "list", "os.path.join", "print", "open", "set", "set", "set", "enumerate", "open", "json.dump", "os.path.join", "json.load", "file.split", "open", "int", "str", "train_users.update", "str", "val_users.update", "str", "test_users.update", "isinstance"], "methods", ["None"], ["", "def", "create_doc_user_splits", "(", "self", ")", ":", "\n", "        ", "\"\"\"Create and save docs and users present in the data splits\"\"\"", "\n", "\n", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "            ", "print", "(", "\"\\n\\n\"", "+", "\"-\"", "*", "100", "+", "\"\\n \\t\\t   Creating doc and user splits for {}\\n\"", ".", "format", "(", "dataset", ")", "+", "'-'", "*", "100", ")", "\n", "\n", "docsplits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'doc_splits_{}.json'", ".", "format", "(", "dataset", ")", ")", "\n", "\n", "docsplits", "=", "json", ".", "load", "(", "open", "(", "docsplits_file", ",", "'r'", ")", ")", "\n", "train_docs", "=", "docsplits", "[", "'train_docs'", "]", "\n", "test_docs", "=", "docsplits", "[", "'test_docs'", "]", "\n", "val_docs", "=", "docsplits", "[", "'val_docs'", "]", "\n", "\n", "print", "(", "\"\\nCreating users in splits file..\"", ")", "\n", "src_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'engagements'", ",", "'complete'", ",", "dataset", ")", "\n", "train_users", ",", "val_users", ",", "test_users", "=", "set", "(", ")", ",", "set", "(", ")", ",", "set", "(", ")", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "src_dir", ")", ":", "\n", "                ", "for", "count", ",", "file", "in", "enumerate", "(", "files", ")", ":", "\n", "                    ", "doc_key", "=", "file", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "src_file_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "file", ")", "\n", "src_file", "=", "json", ".", "load", "(", "open", "(", "src_file_path", ",", "'r'", ")", ")", "\n", "users", "=", "src_file", "[", "'users'", "]", "\n", "users", "=", "[", "int", "(", "s", ")", "for", "s", "in", "users", "if", "isinstance", "(", "s", ",", "int", ")", "]", "\n", "if", "str", "(", "doc_key", ")", "in", "train_docs", ":", "\n", "                        ", "train_users", ".", "update", "(", "users", ")", "\n", "", "if", "str", "(", "doc_key", ")", "in", "val_docs", ":", "\n", "                        ", "val_users", ".", "update", "(", "users", ")", "\n", "", "if", "str", "(", "doc_key", ")", "in", "test_docs", ":", "\n", "                        ", "test_users", ".", "update", "(", "users", ")", "\n", "\n", "", "", "", "temp_dict", "=", "{", "}", "\n", "temp_dict", "[", "'train_users'", "]", "=", "list", "(", "train_users", ")", "\n", "temp_dict", "[", "'val_users'", "]", "=", "list", "(", "val_users", ")", "\n", "temp_dict", "[", "'test_users'", "]", "=", "list", "(", "test_users", ")", "\n", "usersplits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'user_splits_{}.json'", ".", "format", "(", "dataset", ")", ")", "\n", "print", "(", "\"user_splits written in : \"", ",", "usersplits_file", ")", "\n", "with", "open", "(", "usersplits_file", ",", "'w+'", ")", "as", "j", ":", "\n", "                ", "json", ".", "dump", "(", "temp_dict", ",", "j", ")", "\n", "\n", "", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_dicts": [[135, 264], ["print", "print", "os.path.join", "os.path.join", "json.load", "json.load", "enumerate", "print", "print", "enumerate", "print", "print", "print", "os.path.join", "print", "print", "print", "print", "os.path.join", "os.path.join", "set", "set", "print", "list", "print", "print", "print", "print", "enumerate", "os.path.join", "print", "print", "doc2id_train.copy", "doc2id_train.copy.update", "print", "print", "os.path.join", "print", "os.path.join", "numpy.array", "print", "print", "numpy.save", "print", "len", "enumerate", "print", "print", "doc2id_train.copy", "doc2id_train.copy.update", "print", "os.path.join", "print", "print", "open", "open", "numpy.array.append", "len", "len", "numpy.array.append", "len", "len", "len", "open", "json.dump", "len", "len", "len", "json.load", "json.load", "len", "set", "len", "len", "len", "len", "numpy.array.append", "len", "open", "json.dump", "len", "len", "open", "json.dump", "len", "len", "open", "json.dump", "len", "open", "json.dump", "len", "open", "open", "set.intersection", "len", "os.path.join", "str", "str", "str", "str", "len", "str", "str", "str", "str", "str", "str"], "methods", ["None"], ["", "def", "create_dicts", "(", "self", ")", ":", "\n", "        ", "\"\"\"Create and save doc2id and node2id dicts\"\"\"", "\n", "\n", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "            ", "print", "(", "\"\\n\\n\"", "+", "\"-\"", "*", "100", "+", "\"\\n \\t\\t\\t   Creating dicts for  {} dataset \\n\"", ".", "format", "(", "dataset", ")", "+", "'-'", "*", "100", ")", "\n", "\n", "print", "(", "\"\\nCreating doc2id and user2id dicts....\\n\"", ")", "\n", "user_splits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'user_splits_{}.json'", ".", "format", "(", "dataset", ")", ")", "\n", "doc_splits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'doc_splits_{}.json'", ".", "format", "(", "dataset", ")", ")", "\n", "\n", "user_splits", "=", "json", ".", "load", "(", "open", "(", "user_splits_file", ",", "'r'", ")", ")", "\n", "doc_splits", "=", "json", ".", "load", "(", "open", "(", "doc_splits_file", ",", "'r'", ")", ")", "\n", "\n", "train_users", "=", "user_splits", "[", "'train_users'", "]", "\n", "val_users", "=", "user_splits", "[", "'val_users'", "]", "\n", "test_users", "=", "user_splits", "[", "'test_users'", "]", "\n", "\n", "train_docs", "=", "doc_splits", "[", "'train_docs'", "]", "\n", "val_docs", "=", "doc_splits", "[", "'val_docs'", "]", "\n", "test_docs", "=", "doc_splits", "[", "'test_docs'", "]", "\n", "\n", "doc2id_train", "=", "{", "}", "\n", "node_type", "=", "[", "]", "\n", "for", "train_count", ",", "doc", "in", "enumerate", "(", "train_docs", ")", ":", "\n", "                ", "doc2id_train", "[", "str", "(", "doc", ")", "]", "=", "train_count", "\n", "node_type", ".", "append", "(", "1", ")", "\n", "", "print", "(", "\"Train docs = \"", ",", "len", "(", "train_docs", ")", ")", "\n", "print", "(", "\"Node_type = \"", ",", "len", "(", "node_type", ")", ")", "\n", "\n", "\n", "for", "val_count", ",", "doc", "in", "enumerate", "(", "val_docs", ")", ":", "\n", "                ", "doc2id_train", "[", "str", "(", "doc", ")", "]", "=", "val_count", "+", "len", "(", "train_docs", ")", "\n", "node_type", ".", "append", "(", "1", ")", "\n", "", "print", "(", "\"Val docs = \"", ",", "len", "(", "val_docs", ")", ")", "\n", "print", "(", "\"doc2id_train = \"", ",", "len", "(", "doc2id_train", ")", ")", "\n", "print", "(", "\"Node_type = \"", ",", "len", "(", "node_type", ")", ")", "\n", "doc2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'doc2id_lr_top50_train.json'", ")", "\n", "print", "(", "\"Saving doc2id dict in :\"", ",", "doc2id_file", ")", "\n", "with", "open", "(", "doc2id_file", ",", "'w+'", ")", "as", "j", ":", "\n", "                ", "json", ".", "dump", "(", "doc2id_train", ",", "j", ")", "\n", "\n", "\n", "", "print", "(", "'\\nTrain users = '", ",", "len", "(", "train_users", ")", ")", "\n", "print", "(", "\"Test users = \"", ",", "len", "(", "test_users", ")", ")", "\n", "print", "(", "\"Val users = \"", ",", "len", "(", "val_users", ")", ")", "\n", "# print(\"All users = \", len(all_users))", "\n", "\n", "# If do not want to include the most frequent users", "\n", "# if dataset == 'HealthStory':", "\n", "restricted_users_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'restricted_users_5_{}.json'", ".", "format", "(", "dataset", ")", ")", "\n", "valid_users_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'valid_users_top50_{}.json'", ".", "format", "(", "dataset", ")", ")", "\n", "restricted_users", "=", "json", ".", "load", "(", "open", "(", "restricted_users_file", ",", "'r'", ")", ")", "[", "'restricted_users'", "]", "\n", "valid_users", "=", "json", ".", "load", "(", "open", "(", "valid_users_file", ",", "'r'", ")", ")", "[", "'valid_users'", "]", "\n", "# train_users = [u for u in train_users if str(u) not in restricted_users]", "\n", "# val_users = [u for u in val_users if str(u) not in restricted_users]", "\n", "# test_users = [u for u in test_users if str(u) not in restricted_users]", "\n", "\n", "train_users", "=", "[", "u", "for", "u", "in", "train_users", "if", "str", "(", "u", ")", "not", "in", "restricted_users", "and", "str", "(", "u", ")", "in", "valid_users", "]", "\n", "val_users", "=", "[", "u", "for", "u", "in", "val_users", "if", "str", "(", "u", ")", "not", "in", "restricted_users", "and", "str", "(", "u", ")", "in", "valid_users", "]", "\n", "test_users", "=", "[", "u", "for", "u", "in", "test_users", "if", "str", "(", "u", ")", "not", "in", "restricted_users", "and", "str", "(", "u", ")", "in", "valid_users", "]", "\n", "\n", "# train_users = [u for u in train_users if str(u) in valid_users]", "\n", "# val_users = [u for u in val_users if str(u) in valid_users]", "\n", "# test_users = [u for u in test_users if str(u) in valid_users]", "\n", "\n", "\n", "# train_users = [u for u in train_users if u in done_users['done_users']]", "\n", "# val_users = [u for u in val_users if u in done_users['done_users']]", "\n", "# test_users = [u for u in test_users if u in done_users['done_users']]", "\n", "\n", "# train_users = list(set(train_users) - set(done_users['done_users']) - set(restricted_users['restricted_users']))", "\n", "\n", "a", "=", "set", "(", "train_users", "+", "val_users", ")", "\n", "b", "=", "set", "(", "test_users", ")", "\n", "print", "(", "\"\\nUsers common between train/val and test = \"", ",", "len", "(", "a", ".", "intersection", "(", "b", ")", ")", ")", "\n", "\n", "all_users", "=", "list", "(", "set", "(", "train_users", "+", "val_users", "+", "test_users", ")", ")", "\n", "\n", "print", "(", "'\\nTrain users = '", ",", "len", "(", "train_users", ")", ")", "\n", "print", "(", "\"Test users = \"", ",", "len", "(", "test_users", ")", ")", "\n", "print", "(", "\"Val users = \"", ",", "len", "(", "val_users", ")", ")", "\n", "print", "(", "\"All users = \"", ",", "len", "(", "all_users", ")", ")", "\n", "\n", "user2id_train", "=", "{", "}", "\n", "for", "count", ",", "user", "in", "enumerate", "(", "all_users", ")", ":", "\n", "                ", "user2id_train", "[", "str", "(", "user", ")", "]", "=", "count", "+", "len", "(", "doc2id_train", ")", "\n", "node_type", ".", "append", "(", "2", ")", "\n", "", "user2id_train_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'user2id_lr_top50_train.json'", ")", "\n", "print", "(", "\"user2_id = \"", ",", "len", "(", "user2id_train", ")", ")", "\n", "print", "(", "\"Saving user2id_train in : \"", ",", "user2id_train_file", ")", "\n", "with", "open", "(", "user2id_train_file", ",", "'w+'", ")", "as", "j", ":", "\n", "                ", "json", ".", "dump", "(", "user2id_train", ",", "j", ")", "\n", "\n", "\n", "", "node2id", "=", "doc2id_train", ".", "copy", "(", ")", "\n", "node2id", ".", "update", "(", "user2id_train", ")", "\n", "print", "(", "\"node2id size = \"", ",", "len", "(", "node2id", ")", ")", "\n", "print", "(", "\"Node_type = \"", ",", "len", "(", "node_type", ")", ")", "\n", "node2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'node2id_lr_top50_train.json'", ")", "\n", "print", "(", "\"Saving node2id_lr_train in : \"", ",", "node2id_file", ")", "\n", "with", "open", "(", "node2id_file", ",", "'w+'", ")", "as", "json_file", ":", "\n", "                ", "json", ".", "dump", "(", "node2id", ",", "json_file", ")", "\n", "\n", "", "node_type_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'node_type_lr_top50_train.npy'", ")", "\n", "node_type", "=", "np", ".", "array", "(", "node_type", ")", "\n", "print", "(", "node_type", ".", "shape", ")", "\n", "print", "(", "\"Saving node_type in :\"", ",", "node_type_file", ")", "\n", "np", ".", "save", "(", "node_type_file", ",", "node_type", ",", "allow_pickle", "=", "True", ")", "\n", "\n", "print", "(", "\"\\nAdding test docs..\"", ")", "\n", "orig_doc2id_len", "=", "len", "(", "doc2id_train", ")", "\n", "for", "test_count", ",", "doc", "in", "enumerate", "(", "test_docs", ")", ":", "\n", "                ", "doc2id_train", "[", "str", "(", "doc", ")", "]", "=", "test_count", "+", "len", "(", "user2id_train", ")", "+", "orig_doc2id_len", "\n", "", "print", "(", "\"Test docs = \"", ",", "len", "(", "test_docs", ")", ")", "\n", "print", "(", "\"doc2id_train = \"", ",", "len", "(", "doc2id_train", ")", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'doc2id_lr_top50_train.json'", ")", ",", "'w+'", ")", "as", "j", ":", "\n", "                ", "json", ".", "dump", "(", "doc2id_train", ",", "j", ")", "\n", "\n", "", "node2id", "=", "doc2id_train", ".", "copy", "(", ")", "\n", "node2id", ".", "update", "(", "user2id_train", ")", "\n", "print", "(", "\"node2id size = \"", ",", "len", "(", "node2id", ")", ")", "\n", "node2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'node2id_lr_top50.json'", ")", "\n", "print", "(", "\"Saving node2id_lr in : \"", ",", "node2id_file", ")", "\n", "with", "open", "(", "node2id_file", ",", "'w+'", ")", "as", "json_file", ":", "\n", "                ", "json", ".", "dump", "(", "node2id", ",", "json_file", ")", "\n", "\n", "", "print", "(", "\"Done ! All files written..\"", ")", "\n", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_filtered_follower_following": [[268, 318], ["print", "int", "print", "open", "json.load", "len", "print", "os.path.join", "os.path.join", "os.walk", "len", "os.path.exists", "os.makedirs", "enumerate", "int", "os.path.join", "os.path.isfile", "print", "print", "print", "file.split", "os.path.join", "str", "json.load", "os.path.join", "set", "list", "list", "print", "len", "len", "open", "map", "open", "json.dump", "str", "str", "str", "set.update"], "methods", ["None"], ["", "def", "create_filtered_follower_following", "(", "self", ")", ":", "\n", "        ", "\"\"\"Creates a filtered lists of followers and following list of each user comprising of only those users that are in the dataset \"\"\"", "\n", "\n", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "            ", "with", "open", "(", "'./data/complete_data/{}/user2id_lr_top50_train.json'", ".", "format", "(", "dataset", ")", ",", "'r'", ")", "as", "j", ":", "\n", "               ", "all_users", "=", "json", ".", "load", "(", "j", ")", "\n", "\n", "", "print", "(", "\"\\n\\n\"", "+", "\"-\"", "*", "100", "+", "\"\\n \\t\\t   Creating filtered follower-following for {}\\n\"", ".", "format", "(", "dataset", ")", "+", "'-'", "*", "100", ")", "\n", "user_contexts", "=", "[", "'user_followers'", ",", "'user_following'", "]", "\n", "print_iter", "=", "int", "(", "len", "(", "all_users", ")", "/", "10", ")", "\n", "print", "(", "\"Total users in this dataset = \"", ",", "len", "(", "all_users", ")", ")", "\n", "\n", "for", "user_context", "in", "user_contexts", ":", "\n", "                ", "print", "(", "\"    - from {}  folder...\"", ".", "format", "(", "user_context", ")", ")", "\n", "src_dir2", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "user_context", ")", "\n", "dest_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "user_context", "+", "'_filtered'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "dest_dir", ")", ":", "\n", "                    ", "os", ".", "makedirs", "(", "dest_dir", ")", "\n", "", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "src_dir2", ")", ":", "\n", "                    ", "for", "count", ",", "file", "in", "enumerate", "(", "files", ")", ":", "\n", "                        ", "print_iter", "=", "int", "(", "len", "(", "files", ")", "/", "10", ")", "\n", "if", "count", "==", "0", ":", "\n", "                            ", "print", "(", "\"Total user files = \"", ",", "len", "(", "files", ")", ")", "\n", "print", "(", "\"Writing filtered lists in : \"", ",", "dest_dir", ")", "\n", "print", "(", "\"Printing every: \"", ",", "print_iter", ")", "\n", "", "src_file_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "file", ")", "\n", "user_id", "=", "file", ".", "split", "(", "\".\"", ")", "[", "0", "]", "\n", "if", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "dest_dir", ",", "str", "(", "user_id", ")", "+", "'.json'", ")", ")", ":", "\n", "                            ", "continue", "\n", "", "if", "str", "(", "user_id", ")", "in", "all_users", ":", "\n", "                            ", "src_file", "=", "json", ".", "load", "(", "open", "(", "src_file_path", ",", "'r'", ")", ")", "\n", "dest_file_path", "=", "os", ".", "path", ".", "join", "(", "dest_dir", ",", "str", "(", "user_id", ")", "+", "'.json'", ")", "\n", "temp", "=", "set", "(", ")", "\n", "followers", "=", "src_file", "[", "'ids'", "]", "\n", "followers", "=", "list", "(", "map", "(", "int", ",", "followers", ")", ")", "\n", "for", "follower", "in", "followers", ":", "\n", "                                ", "if", "str", "(", "follower", ")", "in", "all_users", ":", "\n", "                                    ", "temp", ".", "update", "(", "[", "follower", "]", ")", "\n", "", "", "temp_dict", "=", "{", "}", "\n", "temp_dict", "[", "'user_id'", "]", "=", "user_id", "\n", "name", "=", "'followers'", "if", "user_context", "==", "'user_followers'", "else", "'following'", "\n", "temp_dict", "[", "name", "]", "=", "list", "(", "temp", ")", "\n", "with", "open", "(", "dest_file_path", ",", "'w+'", ")", "as", "v", ":", "\n", "                                ", "json", ".", "dump", "(", "temp_dict", ",", "v", ")", "\n", "\n", "", "", "if", "count", "%", "print_iter", "==", "0", ":", "\n", "# print(\"{}/{} done..  Non-zeros =  {}\".format(count+1, num_users, adj_matrix.getnnz()))", "\n", "                            ", "print", "(", "\"{} done..\"", ".", "format", "(", "count", "+", "1", ")", ")", "\n", "\n", "", "", "", "", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_adj_matrix": [[324, 431], ["print", "os.path.join", "os.path.join", "os.path.join", "json.load", "json.load", "json.load", "print", "print", "scipy.sparse.lil_matrix", "scipy.sparse.lil_matrix", "range", "int", "print", "time.time", "print", "os.path.join", "os.walk", "time.time", "gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.calc_elapsed_time", "print", "print", "print", "print", "time.time", "print", "int", "print", "gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.calc_elapsed_time", "print", "print", "print", "print", "gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.save_adj_matrix", "open", "open", "open", "len", "enumerate", "scipy.sparse.lil_matrix.getnnz", "scipy.sparse.lil_matrix.getnnz", "print", "os.path.join", "os.walk", "time.time", "scipy.sparse.lil_matrix.getnnz", "scipy.sparse.lil_matrix.getnnz", "len", "len", "os.path.join", "json.load", "enumerate", "file.split", "open", "os.path.join", "json.load", "int", "open", "str", "list", "str", "str", "str", "map", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.calc_elapsed_time", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.calc_elapsed_time", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.save_adj_matrix"], ["", "def", "create_adj_matrix", "(", "self", ")", ":", "\n", "        ", "\"\"\"create and save adjacency matrix of the community graph\"\"\"", "\n", "\n", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "            ", "print", "(", "\"\\n\\n\"", "+", "\"-\"", "*", "100", "+", "\"\\n \\t\\t\\tProcessing  {} dataset for adj_matrix\\n\"", ".", "format", "(", "dataset", ")", "+", "'-'", "*", "100", ")", "\n", "\n", "user2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'user2id_lr_top50_train.json'", ")", "\n", "doc2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'doc2id_lr_top50_train.json'", ")", "\n", "doc_splits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'doc_splits_{}.json'", ".", "format", "(", "dataset", ")", ")", "\n", "user2id", "=", "json", ".", "load", "(", "open", "(", "user2id_file", ",", "'r'", ")", ")", "\n", "doc2id", "=", "json", ".", "load", "(", "open", "(", "doc2id_file", ",", "'r'", ")", ")", "\n", "doc_splits", "=", "json", ".", "load", "(", "open", "(", "doc_splits_file", ",", "'r'", ")", ")", "\n", "test_docs", "=", "doc_splits", "[", "'test_docs'", "]", "\n", "\n", "num_users", ",", "num_docs", "=", "len", "(", "user2id", ")", ",", "len", "(", "doc2id", ")", "-", "len", "(", "test_docs", ")", "\n", "print", "(", "\"\\nNo.of unique users = \"", ",", "num_users", ")", "\n", "print", "(", "\"No.of docs = \"", ",", "num_docs", ")", "\n", "\n", "# Creating the adjacency matrix (doc-user edges)", "\n", "adj_matrix", "=", "lil_matrix", "(", "(", "num_docs", "+", "num_users", ",", "num_users", "+", "num_docs", ")", ")", "\n", "edge_type", "=", "lil_matrix", "(", "(", "num_docs", "+", "num_users", ",", "num_users", "+", "num_docs", ")", ")", "\n", "# adj_matrix = np.zeros((num_docs+num_users, num_users+num_docs))", "\n", "# adj_matrix_file = './data/complete_data/adj_matrix_pheme.npz'", "\n", "# adj_matrix = load_npz(adj_matrix_file)", "\n", "# adj_matrix = lil_matrix(adj_matrix)", "\n", "# Creating self-loops for each node (diagonals are 1's)", "\n", "for", "i", "in", "range", "(", "adj_matrix", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "adj_matrix", "[", "i", ",", "i", "]", "=", "1", "\n", "edge_type", "[", "i", ",", "i", "]", "=", "1", "\n", "", "print_iter", "=", "int", "(", "num_docs", "/", "10", ")", "\n", "print", "(", "\"\\nSize of adjacency matrix = {} \\nPrinting every  {} docs\"", ".", "format", "(", "adj_matrix", ".", "shape", ",", "print_iter", ")", ")", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "\n", "print", "(", "\"\\nPreparing entries for doc-user pairs...\"", ")", "\n", "src_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'engagements'", ",", "'complete'", ",", "dataset", ")", "\n", "not_found", "=", "0", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "src_dir", ")", ":", "\n", "                ", "for", "count", ",", "file", "in", "enumerate", "(", "files", ")", ":", "\n", "                    ", "src_file_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "file", ")", "\n", "doc_key", "=", "file", ".", "split", "(", "\".\"", ")", "[", "0", "]", "\n", "src_file", "=", "json", ".", "load", "(", "open", "(", "src_file_path", ",", "'r'", ")", ")", "\n", "users", "=", "src_file", "[", "'users'", "]", "\n", "for", "user", "in", "users", ":", "\n", "                        ", "if", "str", "(", "doc_key", ")", "in", "doc2id", "and", "str", "(", "user", ")", "in", "user2id", "and", "str", "(", "doc_key", ")", "not", "in", "test_docs", ":", "\n", "                            ", "adj_matrix", "[", "doc2id", "[", "str", "(", "doc_key", ")", "]", ",", "user2id", "[", "str", "(", "user", ")", "]", "]", "=", "1", "\n", "adj_matrix", "[", "user2id", "[", "str", "(", "user", ")", "]", ",", "doc2id", "[", "str", "(", "doc_key", ")", "]", "]", "=", "1", "\n", "edge_type", "[", "doc2id", "[", "str", "(", "doc_key", ")", "]", ",", "user2id", "[", "str", "(", "user", ")", "]", "]", "=", "2", "\n", "edge_type", "[", "user2id", "[", "str", "(", "user", ")", "]", ",", "doc2id", "[", "str", "(", "doc_key", ")", "]", "]", "=", "2", "\n", "", "else", ":", "\n", "                            ", "not_found", "+=", "1", "\n", "\n", "\n", "", "", "", "", "end", "=", "time", ".", "time", "(", ")", "\n", "hrs", ",", "mins", ",", "secs", "=", "self", ".", "calc_elapsed_time", "(", "start", ",", "end", ")", "\n", "print", "(", "\"Done. Took {}hrs and {}mins and {}secs\\n\"", ".", "format", "(", "hrs", ",", "mins", ",", "secs", ")", ")", "\n", "print", "(", "\"Not Found users = \"", ",", "not_found", ")", "\n", "print", "(", "\"Non-zero entries = \"", ",", "adj_matrix", ".", "getnnz", "(", ")", ")", "\n", "print", "(", "\"Non-zero entries edge_type = \"", ",", "edge_type", ".", "getnnz", "(", ")", ")", "\n", "# print(\"Non-zero entries = \", len(np.nonzero(adj_matrix)[0]))", "\n", "\n", "# Creating the adjacency matrix (user-user edges)", "\n", "user_contexts", "=", "[", "'user_followers_filtered'", ",", "'user_following_filtered'", "]", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "key_errors", ",", "not_found", ",", "overlaps", "=", "0", ",", "0", ",", "0", "\n", "print", "(", "\"\\nPreparing entries for user-user pairs...\"", ")", "\n", "print_iter", "=", "int", "(", "num_users", "/", "10", ")", "\n", "print", "(", "\"Printing every {}  users done\"", ".", "format", "(", "print_iter", ")", ")", "\n", "\n", "for", "user_context", "in", "user_contexts", ":", "\n", "                ", "print", "(", "\"    - from {}  folder...\"", ".", "format", "(", "user_context", ")", ")", "\n", "src_dir2", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "user_context", ")", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "src_dir2", ")", ":", "\n", "                    ", "for", "count", ",", "file", "in", "enumerate", "(", "files", ")", ":", "\n", "                        ", "src_file_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "file", ")", "\n", "# user_id = src_file_path.split(\".\")[0]", "\n", "src_file", "=", "json", ".", "load", "(", "open", "(", "src_file_path", ",", "'r'", ")", ")", "\n", "user_id", "=", "int", "(", "src_file", "[", "'user_id'", "]", ")", "\n", "if", "str", "(", "user_id", ")", "in", "user2id", ":", "\n", "                            ", "followers", "=", "src_file", "[", "'followers'", "]", "if", "user_context", "==", "'user_followers_filtered'", "else", "src_file", "[", "'following'", "]", "\n", "followers", "=", "list", "(", "map", "(", "int", ",", "followers", ")", ")", "\n", "for", "follower", "in", "followers", ":", "\n", "                                ", "if", "str", "(", "follower", ")", "in", "user2id", ":", "\n", "                                    ", "adj_matrix", "[", "user2id", "[", "str", "(", "user_id", ")", "]", ",", "user2id", "[", "str", "(", "follower", ")", "]", "]", "=", "1", "\n", "adj_matrix", "[", "user2id", "[", "str", "(", "follower", ")", "]", ",", "user2id", "[", "str", "(", "user_id", ")", "]", "]", "=", "1", "\n", "edge_type", "[", "user2id", "[", "str", "(", "user_id", ")", "]", ",", "user2id", "[", "str", "(", "follower", ")", "]", "]", "=", "3", "\n", "edge_type", "[", "user2id", "[", "str", "(", "follower", ")", "]", ",", "user2id", "[", "str", "(", "user_id", ")", "]", "]", "=", "3", "\n", "\n", "", "", "", "else", ":", "\n", "                            ", "not_found", "+=", "1", "\n", "# if count%print_iter==0:", "\n", "#     # print(\"{}/{} done..  Non-zeros =  {}\".format(count+1, num_users, adj_matrix.getnnz()))", "\n", "#     print(\"{}/{} done..  Non-zeros =  {}\".format(count+1, num_users, len(np.nonzero(adj_matrix)[0])))", "\n", "\n", "", "", "", "", "hrs", ",", "mins", ",", "secs", "=", "self", ".", "calc_elapsed_time", "(", "start", ",", "time", ".", "time", "(", ")", ")", "\n", "print", "(", "\"Done. Took {}hrs and {}mins and {}secs\\n\"", ".", "format", "(", "hrs", ",", "mins", ",", "secs", ")", ")", "\n", "print", "(", "\"Not found user_ids = \"", ",", "not_found", ")", "\n", "print", "(", "\"Total Non-zero entries = \"", ",", "adj_matrix", ".", "getnnz", "(", ")", ")", "\n", "print", "(", "\"Total Non-zero entries edge_type = \"", ",", "edge_type", ".", "getnnz", "(", ")", ")", "\n", "# print(\"Total Non-zero entries = \", len(np.nonzero(adj_matrix)[0]))", "\n", "\n", "self", ".", "adj_file", "=", "'adj_matrix_lr_top50_train'", "\n", "# filename = self.data_dir+ '/complete_data' + '/adj_matrix_{}.npy'.format(dataset)", "\n", "\n", "self", ".", "edge_type_file", "=", "'edge_type_lr_top50'", "\n", "self", ".", "save_adj_matrix", "(", "self", ",", "dataset", ",", "adj_matrix", ",", "edge_type", ")", "\n", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_feat_matrix": [[434, 544], ["print", "os.path.join", "os.path.join", "os.path.join", "json.load", "json.load", "json.load", "os.path.join", "gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.build_vocab", "len", "set", "scipy.sparse.lil_matrix", "print", "print", "time.time", "os.path.join", "glob.glob", "enumerate", "gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.calc_elapsed_time", "print", "numpy.array().squeeze", "print", "numpy.where", "print", "print", "time.time", "os.path.join", "os.walk", "gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.calc_elapsed_time", "print", "print", "feat_matrix.astype.astype.astype", "numpy.array().squeeze", "print", "numpy.where", "print", "os.path.join", "print", "scipy.sparse.save_npz", "open", "open", "open", "len", "nltk.corpus.stopwords.words", "int", "time.time", "len", "enumerate", "time.time", "len", "feat_matrix.astype.astype.tocsr", "len", "len", "[].split", "str", "print", "numpy.array", "int", "os.path.join", "json.load", "numpy.array", "len", "feat_matrix.astype.astype.sum", "open", "file.split", "print", "print", "feat_matrix.astype.astype.sum", "str", "str", "open", "numpy.zeros", "json.load", "nltk.word_tokenize.replace", "nltk.word_tokenize.lower", "re.sub", "re.sub", "nltk.word_tokenize", "len", "len", "str", "str", "datetime.datetime.now", "file.split", "len", "file_content[].replace", "str", "len", "gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.keys", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.build_vocab", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.calc_elapsed_time", "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.calc_elapsed_time"], ["", "def", "create_feat_matrix", "(", "self", ",", "binary", "=", "True", ")", ":", "\n", "        ", "\"\"\" Create and save the initial node representations of each node of the graph \"\"\"", "\n", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "            ", "print", "(", "\"\\n\\n\"", "+", "\"-\"", "*", "100", "+", "\"\\n \\t\\tProcessing  {} dataset  for feature_matrix\\n\"", ".", "format", "(", "dataset", ")", "+", "'-'", "*", "100", ")", "\n", "user2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'user2id_lr_top50_train.json'", ")", "\n", "doc2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'doc2id_lr_top50_train.json'", ")", "\n", "doc_splits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'doc_splits_{}.json'", ".", "format", "(", "dataset", ")", ")", "\n", "\n", "user2id", "=", "json", ".", "load", "(", "open", "(", "user2id_file", ",", "'r'", ")", ")", "\n", "doc2id", "=", "json", ".", "load", "(", "open", "(", "doc2id_file", ",", "'r'", ")", ")", "\n", "doc_splits", "=", "json", ".", "load", "(", "open", "(", "doc_splits_file", ",", "'r'", ")", ")", "\n", "\n", "\n", "train_docs", "=", "doc_splits", "[", "'train_docs'", "]", "\n", "val_docs", "=", "doc_splits", "[", "'val_docs'", "]", "\n", "test_docs", "=", "doc_splits", "[", "'test_docs'", "]", "\n", "\n", "N", "=", "len", "(", "train_docs", ")", "+", "len", "(", "val_docs", ")", "+", "len", "(", "user2id", ")", "\n", "\n", "\n", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'vocab.json'", ")", "\n", "vocab", "=", "self", ".", "build_vocab", "(", "vocab_file", ",", "dataset", ",", "train_docs", ",", "doc2id", ")", "\n", "vocab_size", "=", "len", "(", "vocab", ")", "\n", "stop_words", "=", "set", "(", "stopwords", ".", "words", "(", "'english'", ")", ")", "\n", "\n", "feat_matrix", "=", "lil_matrix", "(", "(", "N", ",", "vocab_size", ")", ")", "\n", "print", "(", "\"\\nSize of feature matrix = \"", ",", "feat_matrix", ".", "shape", ")", "\n", "print", "(", "\"\\nCreating feat_matrix entries for docs nodes...\"", ")", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "split_docs", "=", "train_docs", "+", "val_docs", "\n", "\n", "src_doc_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'content'", ",", "dataset", "+", "\"/*.json\"", ")", "\n", "all_files", "=", "glob", ".", "glob", "(", "src_doc_dir", ")", "\n", "for", "count", ",", "file", "in", "enumerate", "(", "all_files", ")", ":", "\n", "                ", "print_iter", "=", "int", "(", "len", "(", "all_files", ")", "/", "5", ")", "\n", "doc_name", "=", "file", ".", "split", "(", "'\\\\'", ")", "[", "-", "1", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "if", "str", "(", "doc_name", ")", "in", "split_docs", ":", "\n", "                    ", "if", "str", "(", "doc_name", ")", "in", "doc2id", "and", "str", "(", "doc_name", ")", "not", "in", "test_docs", ":", "\n", "# feat_matrix[doc2id[str(doc_name)], :] = np.random.random(len(vocab)) > 0.99", "\n", "\n", "                        ", "with", "open", "(", "file", ",", "'r'", ")", "as", "f", ":", "\n", "                            ", "vector", "=", "np", ".", "zeros", "(", "len", "(", "vocab", ")", ")", "\n", "file_content", "=", "json", ".", "load", "(", "f", ")", "\n", "text", "=", "file_content", "[", "'text'", "]", ".", "replace", "(", "'\\n'", ",", "' '", ")", "[", ":", "1500", "]", "\n", "text", "=", "text", ".", "replace", "(", "'\\t'", ",", "' '", ")", "\n", "text", "=", "text", ".", "lower", "(", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'#[\\w-]+'", ",", "'hashtag'", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'https?://\\S+'", ",", "'url'", ",", "text", ")", "\n", "# text = re.sub(r\"[^A-Za-z(),!?\\'`]\", \" \", text)", "\n", "text", "=", "nltk", ".", "word_tokenize", "(", "text", ")", "\n", "text_filtered", "=", "[", "w", "for", "w", "in", "text", "if", "not", "w", "in", "stop_words", "]", "\n", "for", "token", "in", "text_filtered", ":", "\n", "                                ", "if", "token", "in", "vocab", ".", "keys", "(", ")", ":", "\n", "                                    ", "vector", "[", "vocab", "[", "token", "]", "]", "=", "1", "\n", "", "", "feat_matrix", "[", "doc2id", "[", "str", "(", "doc_name", ")", "]", ",", ":", "]", "=", "vector", "\n", "", "", "", "if", "count", "%", "print_iter", "==", "0", ":", "\n", "                    ", "print", "(", "\"{} / {} done..\"", ".", "format", "(", "count", "+", "1", ",", "len", "(", "all_files", ")", ")", ")", "\n", "\n", "", "", "hrs", ",", "mins", ",", "secs", "=", "self", ".", "calc_elapsed_time", "(", "start", ",", "time", ".", "time", "(", ")", ")", "\n", "print", "(", "\"Done. Took {}hrs and {}mins and {}secs\\n\"", ".", "format", "(", "hrs", ",", "mins", ",", "secs", ")", ")", "\n", "\n", "sum_1", "=", "np", ".", "array", "(", "feat_matrix", ".", "sum", "(", "axis", "=", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "print", "(", "sum_1", ".", "shape", ")", "\n", "idx", "=", "np", ".", "where", "(", "sum_1", "==", "0", ")", "\n", "print", "(", "len", "(", "idx", "[", "0", "]", ")", ")", "\n", "\n", "\n", "print", "(", "\"\\nCreating feat_matrix entries for users nodes...\"", ")", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "not_found", ",", "use", "=", "0", ",", "0", "\n", "# user_splits = json.load(open('./data/complete_data/{}/user_splits.json'.format(dataset), 'r'))", "\n", "# train_users = user_splits['train_users']", "\n", "src_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'engagements'", ",", "'complete'", ",", "dataset", ")", "\n", "user_contexts", "=", "[", "'user_followers_filtered'", ",", "'user_following_filtered'", "]", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "src_dir", ")", ":", "\n", "                ", "for", "count", ",", "file", "in", "enumerate", "(", "files", ")", ":", "\n", "                    ", "print_iter", "=", "int", "(", "len", "(", "files", ")", "/", "10", ")", "\n", "src_file_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "file", ")", "\n", "src_file", "=", "json", ".", "load", "(", "open", "(", "src_file_path", ",", "'r'", ")", ")", "\n", "users", "=", "src_file", "[", "'users'", "]", "\n", "doc_key", "=", "file", ".", "split", "(", "\".\"", ")", "[", "0", "]", "\n", "# if str(doc_key) in train_docs:", "\n", "# Each user of this doc has its features as the features of the doc", "\n", "if", "(", "str", "(", "doc_key", ")", "in", "split_docs", ")", "and", "str", "(", "doc_key", ")", "in", "doc2id", ":", "\n", "                        ", "for", "user", "in", "users", ":", "\n", "                            ", "if", "str", "(", "user", ")", "in", "user2id", ":", "\n", "                                ", "feat_matrix", "[", "user2id", "[", "str", "(", "user", ")", "]", ",", ":", "]", "+=", "feat_matrix", "[", "doc2id", "[", "str", "(", "doc_key", ")", "]", ",", ":", "]", "\n", "\n", "\n", "", "", "", "if", "count", "%", "print_iter", "==", "0", ":", "\n", "                        ", "print", "(", "\" {} / {} done..\"", ".", "format", "(", "count", "+", "1", ",", "len", "(", "files", ")", ")", ")", "\n", "print", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", "\n", "\n", "", "", "", "hrs", ",", "mins", ",", "secs", "=", "self", ".", "calc_elapsed_time", "(", "start", ",", "time", ".", "time", "(", ")", ")", "\n", "print", "(", "not_found", ",", "use", ")", "\n", "print", "(", "\"Done. Took {}hrs and {}mins and {}secs\\n\"", ".", "format", "(", "hrs", ",", "mins", ",", "secs", ")", ")", "\n", "\n", "feat_matrix", "=", "feat_matrix", ">=", "1", "\n", "feat_matrix", "=", "feat_matrix", ".", "astype", "(", "int", ")", "\n", "\n", "# Sanity Checks", "\n", "sum_1", "=", "np", ".", "array", "(", "feat_matrix", ".", "sum", "(", "axis", "=", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "print", "(", "sum_1", ".", "shape", ")", "\n", "idx", "=", "np", ".", "where", "(", "sum_1", "==", "0", ")", "\n", "print", "(", "len", "(", "idx", "[", "0", "]", ")", ")", "\n", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'feat_matrix_lr_top50_train.npz'", ")", "\n", "print", "(", "\"Matrix construction done! Saving in :   {}\"", ".", "format", "(", "filename", ")", ")", "\n", "save_npz", "(", "filename", ",", "feat_matrix", ".", "tocsr", "(", ")", ")", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_labels": [[548, 589], ["print", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "json.load", "json.load", "scipy.sparse.load_npz", "json.load", "print", "enumerate", "print", "print", "print", "gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.save_labels", "open", "open", "open", "len", "len", "str", "doc2labels.keys", "len", "len", "doc2labels.keys", "len", "len", "len", "json.load.keys", "json.load.keys", "str"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.save_labels"], ["", "def", "create_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Create labels for each node of the graph\n        \"\"\"", "\n", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "            ", "print", "(", "\"\\n\\n\"", "+", "\"-\"", "*", "100", "+", "\"\\n \\t\\t   Processing  {} dataset  for Creating Labels\\n\"", ".", "format", "(", "dataset", ")", "+", "'-'", "*", "100", ")", "\n", "doc2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'doc2id_lr_top50_train.json'", ")", "\n", "adj_matrix_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'adj_matrix_lr_top50_train.npz'", ")", "\n", "doc_splits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'doc_splits_{}.json'", ".", "format", "(", "dataset", ")", ")", "\n", "src_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'reviews'", ",", "dataset", "+", "'.json'", ")", "\n", "doc_labels", "=", "json", ".", "load", "(", "open", "(", "src_dir", ",", "'r'", ")", ")", "\n", "doc2id", "=", "json", ".", "load", "(", "open", "(", "doc2id_file", ",", "'r'", ")", ")", "\n", "adj_matrix", "=", "load_npz", "(", "adj_matrix_file", ")", "\n", "N", ",", "_", "=", "adj_matrix", ".", "shape", "\n", "del", "adj_matrix", "\n", "\n", "doc_splits", "=", "json", ".", "load", "(", "open", "(", "doc_splits_file", ",", "'r'", ")", ")", "\n", "train_docs", "=", "doc_splits", "[", "'train_docs'", "]", "\n", "val_docs", "=", "doc_splits", "[", "'val_docs'", "]", "\n", "\n", "split_docs", "=", "train_docs", "+", "val_docs", "\n", "\n", "print", "(", "\"\\nCreating doc2labels dictionary...\"", ")", "\n", "doc2labels", "=", "{", "}", "\n", "\n", "for", "count", ",", "doc", "in", "enumerate", "(", "doc_labels", ")", ":", "\n", "                ", "if", "str", "(", "doc", "[", "'news_id'", "]", ")", "in", "split_docs", ":", "\n", "                    ", "label", "=", "1", "if", "doc", "[", "'rating'", "]", "<", "3", "else", "0", "# rating less than 3 is fake", "\n", "doc2labels", "[", "str", "(", "doc", "[", "'news_id'", "]", ")", "]", "=", "label", "\n", "\n", "", "", "print", "(", "len", "(", "doc2labels", ".", "keys", "(", ")", ")", ")", "\n", "print", "(", "len", "(", "doc2id", ".", "keys", "(", ")", ")", "-", "len", "(", "doc_splits", "[", "'test_docs'", "]", ")", ")", "\n", "assert", "len", "(", "doc2labels", ".", "keys", "(", ")", ")", "==", "len", "(", "doc2id", ".", "keys", "(", ")", ")", "-", "len", "(", "doc_splits", "[", "'test_docs'", "]", ")", "\n", "print", "(", "\"Len of doc2labels  = {}\\n\"", ".", "format", "(", "len", "(", "doc2labels", ")", ")", ")", "\n", "self", ".", "doc2labels_file", "=", "'doc2labels_lr_top50_train.json'", "\n", "self", ".", "labels_list_file", "=", "'labels_list_lr_top50_train.json'", "\n", "self", ".", "all_labels_file", "=", "'all_labels_lr_top50_train.json'", "\n", "\n", "self", ".", "save_labels", "(", "dataset", ")", "\n", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_split_masks": [[591, 604], ["print", "os.path.join", "os.path.join", "os.path.join", "gnn_preprocess_lr_fakehealth.GCN_PreProcess_health.create_split_masks_main"], "methods", ["home.repos.pwc.inspect_result.shaanchandra_SAFER.data_prep.gnn_preprocess_main.GNN_PreProcess.create_split_masks_main"], ["", "def", "create_split_masks", "(", "self", ")", ":", "\n", "        ", "\"\"\"create and save node masks for the train and val article nodes\"\"\"", "\n", "\n", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "            ", "print", "(", "\"\\n\\n\"", "+", "\"-\"", "*", "100", "+", "\"\\n \\t\\t   Creating split masks for {}\\n\"", ".", "format", "(", "dataset", ")", "+", "'-'", "*", "100", ")", "\n", "\n", "self", ".", "doc2id_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'doc2id_lr_top50_train.json'", ")", "\n", "self", ".", "doc_splits_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'doc_splits_{}.json'", ".", "format", "(", "dataset", ")", ")", "\n", "self", ".", "train_adj_matrix_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "comp_dir", ",", "dataset", ",", "'adj_matrix_lr_top50_train.npz'", ")", "\n", "self", ".", "split_mask_file", "=", "'split_mask_top50.json'", "\n", "self", ".", "create_split_masks_main", "(", "dataset", ")", "\n", "\n", "", "return", "None", "\n", "\n"]]}