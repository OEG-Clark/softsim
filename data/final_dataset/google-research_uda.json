{"home.repos.pwc.inspect_result.google-research_uda.back_translate.sent_to_paragraph.main": [[36, 52], ["print", "print", "tensorflow.gfile.Open", "inf.readlines", "tensorflow.gfile.Open", "json.load", "tensorflow.gfile.Open", "enumerate", "range", "print", "ouf.write", "sentences[].strip", "para.strip"], "function", ["None"], ["def", "main", "(", "argv", ")", ":", "\n", "  ", "with", "tf", ".", "gfile", ".", "Open", "(", "FLAGS", ".", "input_file", ")", "as", "inf", ":", "\n", "    ", "sentences", "=", "inf", ".", "readlines", "(", ")", "\n", "", "with", "tf", ".", "gfile", ".", "Open", "(", "FLAGS", ".", "doc_len_file", ")", "as", "inf", ":", "\n", "    ", "doc_len_list", "=", "json", ".", "load", "(", "inf", ")", "\n", "", "cnt", "=", "0", "\n", "print", "(", "\"\\n\"", "*", "2", ")", "\n", "print", "(", "\"*** printing paraphrases ***\"", ")", "\n", "with", "tf", ".", "gfile", ".", "Open", "(", "FLAGS", ".", "output_file", ",", "\"w\"", ")", "as", "ouf", ":", "\n", "    ", "for", "i", ",", "sent_num", "in", "enumerate", "(", "doc_len_list", ")", ":", "\n", "      ", "para", "=", "\"\"", "\n", "for", "_", "in", "range", "(", "sent_num", ")", ":", "\n", "        ", "para", "+=", "sentences", "[", "cnt", "]", ".", "strip", "(", ")", "+", "\" \"", "\n", "cnt", "+=", "1", "\n", "", "print", "(", "\"paraphrase {}: {}\"", ".", "format", "(", "i", ",", "para", ")", ")", "\n", "ouf", ".", "write", "(", "para", ".", "strip", "(", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.back_translate.split_paragraphs.split_sent_by_punc": [[52, 68], ["len", "sent.find"], "function", ["None"], ["def", "split_sent_by_punc", "(", "sent", ",", "punc", ",", "offset", ")", ":", "\n", "  ", "\"\"\"Further split sentences when nltk's sent_tokenizer fail.\"\"\"", "\n", "sent_list", "=", "[", "]", "\n", "start", "=", "0", "\n", "while", "start", "<", "len", "(", "sent", ")", ":", "\n", "    ", "if", "punc", ":", "\n", "      ", "pos", "=", "sent", ".", "find", "(", "punc", ",", "start", "+", "offset", ")", "\n", "", "else", ":", "\n", "      ", "pos", "=", "start", "+", "offset", "\n", "", "if", "pos", "!=", "-", "1", ":", "\n", "      ", "sent_list", "+=", "[", "sent", "[", "start", ":", "pos", "+", "1", "]", "]", "\n", "start", "=", "pos", "+", "1", "\n", "", "else", ":", "\n", "      ", "sent_list", "+=", "[", "sent", "[", "start", ":", "]", "]", "\n", "break", "\n", "", "", "return", "sent_list", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.back_translate.split_paragraphs.divide_data_for_worker": [[70, 85], ["tensorflow.logging.info", "len", "len", "len"], "function", ["None"], ["", "def", "divide_data_for_worker", "(", "contents", ")", ":", "\n", "  ", "data_per_worker", "=", "len", "(", "contents", ")", "//", "FLAGS", ".", "replicas", "\n", "remainder", "=", "len", "(", "contents", ")", "-", "FLAGS", ".", "replicas", "*", "data_per_worker", "\n", "worker_id", "=", "FLAGS", ".", "worker_id", "\n", "if", "worker_id", "<", "remainder", ":", "\n", "    ", "start", "=", "(", "data_per_worker", "+", "1", ")", "*", "worker_id", "\n", "end", "=", "(", "data_per_worker", "+", "1", ")", "*", "(", "worker_id", "+", "1", ")", "\n", "", "else", ":", "\n", "    ", "start", "=", "data_per_worker", "*", "worker_id", "+", "remainder", "\n", "end", "=", "data_per_worker", "*", "(", "worker_id", "+", "1", ")", "+", "remainder", "\n", "", "if", "worker_id", "==", "FLAGS", ".", "replicas", "-", "1", ":", "\n", "    ", "assert", "end", "==", "len", "(", "contents", ")", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"processing data from {:d} to {:d}\"", ".", "format", "(", "start", ",", "end", ")", ")", "\n", "contents", "=", "contents", "[", "start", ":", "end", "]", "\n", "return", "contents", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.back_translate.split_paragraphs.main": [[87, 144], ["tensorflow.logging.info", "tensorflow.logging.info", "split_paragraphs.divide_data_for_worker", "tensorflow.logging.info", "range", "tensorflow.logging.info", "tensorflow.gfile.Open", "inf.readlines", "len", "len", "contents[].strip", "isinstance", "sent_tokenizer", "tensorflow.gfile.Open", "tensorflow.gfile.Open", "json.dump", "contents[].decode", "tensorflow.logging.info", "len", "ouf.write", "len", "split_paragraphs.split_sent_by_punc"], "function", ["home.repos.pwc.inspect_result.google-research_uda.back_translate.split_paragraphs.divide_data_for_worker", "home.repos.pwc.inspect_result.google-research_uda.back_translate.split_paragraphs.split_sent_by_punc"], ["", "def", "main", "(", "_", ")", ":", "\n", "  ", "sent_tokenizer", "=", "nltk", ".", "tokenize", ".", "sent_tokenize", "\n", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"loading input data\"", ")", "\n", "with", "tf", ".", "gfile", ".", "Open", "(", "FLAGS", ".", "input_file", ")", "as", "inf", ":", "\n", "    ", "contents", "=", "inf", ".", "readlines", "(", ")", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"finished loading input data\"", ")", "\n", "assert", "len", "(", "contents", ")", ">=", "FLAGS", ".", "replicas", "\n", "\n", "contents", "=", "divide_data_for_worker", "(", "contents", ")", "\n", "\n", "new_contents", "=", "[", "]", "\n", "doc_len", "=", "[", "]", "\n", "# Split paragraphs into sentences since the model is trained on sentence-level", "\n", "# translations.", "\n", "tf", ".", "logging", ".", "info", "(", "\"splitting sentence\"", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "contents", ")", ")", ":", "\n", "    ", "contents", "[", "i", "]", "=", "contents", "[", "i", "]", ".", "strip", "(", ")", "\n", "if", "isinstance", "(", "contents", "[", "i", "]", ",", "bytes", ")", ":", "\n", "      ", "contents", "[", "i", "]", "=", "contents", "[", "i", "]", ".", "decode", "(", "\"utf-8\"", ")", "\n", "", "sent_list", "=", "sent_tokenizer", "(", "contents", "[", "i", "]", ")", "\n", "has_long", "=", "False", "\n", "if", "i", "%", "100", "==", "0", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"splitting sentence {:d}\"", ".", "format", "(", "i", ")", ")", "\n", "", "for", "split_punc", "in", "[", "\".\"", ",", "\";\"", ",", "\",\"", ",", "\" \"", ",", "\"\"", "]", ":", "\n", "      ", "if", "split_punc", "==", "\" \"", "or", "not", "split_punc", ":", "\n", "        ", "offset", "=", "100", "\n", "", "else", ":", "\n", "        ", "offset", "=", "5", "\n", "", "has_long", "=", "False", "\n", "new_sent_list", "=", "[", "]", "\n", "for", "sent", "in", "sent_list", ":", "\n", "        ", "if", "len", "(", "sent", ")", "<", "300", ":", "\n", "          ", "new_sent_list", "+=", "[", "sent", "]", "\n", "", "else", ":", "\n", "          ", "has_long", "=", "True", "\n", "sent_split", "=", "split_sent_by_punc", "(", "sent", ",", "split_punc", ",", "offset", ")", "\n", "new_sent_list", "+=", "sent_split", "\n", "", "", "sent_list", "=", "new_sent_list", "\n", "if", "not", "has_long", ":", "\n", "        ", "break", "\n", "\n", "# free up memory", "\n", "", "", "contents", "[", "i", "]", "=", "None", "\n", "doc_len", "+=", "[", "len", "(", "sent_list", ")", "]", "\n", "#  nltk.sent_tokenize in python2 will omit some unicode characters", "\n", "for", "st", "in", "sent_list", ":", "\n", "      ", "new_contents", "+=", "[", "st", "]", "\n", "\n", "", "", "tf", ".", "logging", ".", "info", "(", "\"finished spliting paragraphs\"", ")", "\n", "\n", "with", "tf", ".", "gfile", ".", "Open", "(", "FLAGS", ".", "output_file", ",", "\"w\"", ")", "as", "ouf", ":", "\n", "    ", "for", "st", "in", "new_contents", ":", "\n", "      ", "ouf", ".", "write", "(", "st", "+", "\"\\n\"", ")", "\n", "", "", "with", "tf", ".", "gfile", ".", "Open", "(", "FLAGS", ".", "doc_len_file", ",", "\"w\"", ")", "as", "ouf", ":", "\n", "    ", "json", ".", "dump", "(", "doc_len", ",", "ouf", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.uda.kl_for_log_probs": [[33, 39], ["tensorflow.exp", "tensorflow.reduce_sum", "tensorflow.reduce_sum"], "function", ["None"], ["def", "kl_for_log_probs", "(", "log_p", ",", "log_q", ")", ":", "\n", "  ", "p", "=", "tf", ".", "exp", "(", "log_p", ")", "\n", "neg_ent", "=", "tf", ".", "reduce_sum", "(", "p", "*", "log_p", ",", "axis", "=", "-", "1", ")", "\n", "neg_cross_ent", "=", "tf", ".", "reduce_sum", "(", "p", "*", "log_q", ",", "axis", "=", "-", "1", ")", "\n", "kl", "=", "neg_ent", "-", "neg_cross_ent", "\n", "return", "kl", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.uda.hidden_to_logits": [[41, 63], ["tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.bias_add", "tensorflow.nn.dropout", "tensorflow.einsum", "tensorflow.einsum", "tensorflow.truncated_normal_initializer", "tensorflow.zeros_initializer"], "function", ["home.repos.pwc.inspect_result.google-research_uda.bert.modeling.dropout"], ["", "def", "hidden_to_logits", "(", "hidden", ",", "is_training", ",", "num_classes", ",", "scope", ")", ":", "\n", "  ", "hidden_size", "=", "hidden", ".", "shape", "[", "-", "1", "]", ".", "value", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "scope", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "    ", "output_weights", "=", "tf", ".", "get_variable", "(", "\n", "\"output_weights\"", ",", "[", "num_classes", ",", "hidden_size", "]", ",", "\n", "initializer", "=", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "0.02", ")", ")", "\n", "\n", "output_bias", "=", "tf", ".", "get_variable", "(", "\n", "\"output_bias\"", ",", "[", "num_classes", "]", ",", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "\n", "if", "is_training", ":", "\n", "# I.e., 0.1 dropout", "\n", "      ", "hidden", "=", "tf", ".", "nn", ".", "dropout", "(", "hidden", ",", "keep_prob", "=", "0.9", ")", "\n", "\n", "", "if", "hidden", ".", "shape", ".", "ndims", "==", "3", ":", "\n", "      ", "logits", "=", "tf", ".", "einsum", "(", "\"bid,nd->bin\"", ",", "hidden", ",", "output_weights", ")", "\n", "", "else", ":", "\n", "      ", "logits", "=", "tf", ".", "einsum", "(", "\"bd,nd->bn\"", ",", "hidden", ",", "output_weights", ")", "\n", "", "logits", "=", "tf", ".", "nn", ".", "bias_add", "(", "logits", ",", "output_bias", ")", "\n", "\n", "", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.uda.get_tsa_threshold": [[65, 78], ["tensorflow.to_float", "tensorflow.to_float", "tensorflow.exp", "tensorflow.exp"], "function", ["None"], ["", "def", "get_tsa_threshold", "(", "schedule", ",", "global_step", ",", "num_train_steps", ",", "start", ",", "end", ")", ":", "\n", "  ", "training_progress", "=", "tf", ".", "to_float", "(", "global_step", ")", "/", "tf", ".", "to_float", "(", "num_train_steps", ")", "\n", "if", "schedule", "==", "\"linear_schedule\"", ":", "\n", "    ", "threshold", "=", "training_progress", "\n", "", "elif", "schedule", "==", "\"exp_schedule\"", ":", "\n", "    ", "scale", "=", "5", "\n", "threshold", "=", "tf", ".", "exp", "(", "(", "training_progress", "-", "1", ")", "*", "scale", ")", "\n", "# [exp(-5), exp(0)] = [1e-2, 1]", "\n", "", "elif", "schedule", "==", "\"log_schedule\"", ":", "\n", "    ", "scale", "=", "5", "\n", "# [1 - exp(0), 1 - exp(-5)] = [0, 0.99]", "\n", "threshold", "=", "1", "-", "tf", ".", "exp", "(", "(", "-", "training_progress", ")", "*", "scale", ")", "\n", "", "return", "threshold", "*", "(", "end", "-", "start", ")", "+", "start", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.uda.create_model": [[80, 183], ["bert.modeling.bert_model", "uda.hidden_to_logits", "tensorflow.nn.log_softmax", "tensorflow.variable_scope", "tensorflow.one_hot", "tensorflow.ones_like", "tensorflow.reduce_sum", "tensorflow.stop_gradient", "tensorflow.reduce_sum", "uda.get_tsa_threshold", "tensorflow.greater", "tensorflow.reduce_sum", "tensorflow.maximum", "tensorflow.variable_scope", "tensorflow.reduce_mean", "tensorflow.exp", "tensorflow.reduce_sum", "tensorflow.nn.log_softmax", "tensorflow.stop_gradient", "tensorflow.stop_gradient", "tensorflow.reduce_max", "tensorflow.cast", "tensorflow.stop_gradient", "uda.kl_for_log_probs", "tensorflow.cast", "tensorflow.exp", "tensorflow.greater"], "function", ["home.repos.pwc.inspect_result.google-research_uda.bert.modeling.bert_model", "home.repos.pwc.inspect_result.google-research_uda.text.uda.hidden_to_logits", "home.repos.pwc.inspect_result.google-research_uda.image.main.get_tsa_threshold", "home.repos.pwc.inspect_result.google-research_uda.text.uda.kl_for_log_probs"], ["", "def", "create_model", "(", "\n", "bert_config", ",", "\n", "is_training", ",", "\n", "input_ids", ",", "\n", "input_mask", ",", "\n", "input_type_ids", ",", "\n", "labels", ",", "\n", "num_labels", ",", "\n", "use_one_hot_embeddings", ",", "\n", "tsa", ",", "\n", "unsup_ratio", ",", "\n", "global_step", ",", "\n", "num_train_steps", ",", "\n", ")", ":", "\n", "\n", "  ", "num_sample", "=", "input_ids", ".", "shape", "[", "0", "]", ".", "value", "\n", "if", "is_training", ":", "\n", "    ", "assert", "num_sample", "%", "(", "1", "+", "2", "*", "unsup_ratio", ")", "==", "0", "\n", "sup_batch_size", "=", "num_sample", "//", "(", "1", "+", "2", "*", "unsup_ratio", ")", "\n", "unsup_batch_size", "=", "sup_batch_size", "*", "unsup_ratio", "\n", "", "else", ":", "\n", "    ", "sup_batch_size", "=", "num_sample", "\n", "unsup_batch_size", "=", "0", "\n", "\n", "", "pooled", "=", "modeling", ".", "bert_model", "(", "\n", "config", "=", "bert_config", ",", "\n", "is_training", "=", "is_training", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "token_type_ids", "=", "input_type_ids", ",", "\n", "use_one_hot_embeddings", "=", "use_one_hot_embeddings", ")", "\n", "\n", "clas_logits", "=", "hidden_to_logits", "(", "\n", "hidden", "=", "pooled", ",", "\n", "is_training", "=", "is_training", ",", "\n", "num_classes", "=", "num_labels", ",", "\n", "scope", "=", "\"classifier\"", ")", "\n", "\n", "log_probs", "=", "tf", ".", "nn", ".", "log_softmax", "(", "clas_logits", ",", "axis", "=", "-", "1", ")", "\n", "correct_label_probs", "=", "None", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"sup_loss\"", ")", ":", "\n", "    ", "sup_log_probs", "=", "log_probs", "[", ":", "sup_batch_size", "]", "\n", "one_hot_labels", "=", "tf", ".", "one_hot", "(", "labels", ",", "depth", "=", "num_labels", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "tgt_label_prob", "=", "one_hot_labels", "\n", "\n", "per_example_loss", "=", "-", "tf", ".", "reduce_sum", "(", "tgt_label_prob", "*", "sup_log_probs", ",", "axis", "=", "-", "1", ")", "\n", "loss_mask", "=", "tf", ".", "ones_like", "(", "per_example_loss", ",", "dtype", "=", "per_example_loss", ".", "dtype", ")", "\n", "correct_label_probs", "=", "tf", ".", "reduce_sum", "(", "\n", "one_hot_labels", "*", "tf", ".", "exp", "(", "sup_log_probs", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n", "if", "tsa", ":", "\n", "      ", "tsa_start", "=", "1.", "/", "num_labels", "\n", "tsa_threshold", "=", "get_tsa_threshold", "(", "\n", "tsa", ",", "global_step", ",", "num_train_steps", ",", "\n", "tsa_start", ",", "end", "=", "1", ")", "\n", "\n", "larger_than_threshold", "=", "tf", ".", "greater", "(", "\n", "correct_label_probs", ",", "tsa_threshold", ")", "\n", "loss_mask", "=", "loss_mask", "*", "(", "1", "-", "tf", ".", "cast", "(", "larger_than_threshold", ",", "tf", ".", "float32", ")", ")", "\n", "", "else", ":", "\n", "      ", "tsa_threshold", "=", "1", "\n", "\n", "", "loss_mask", "=", "tf", ".", "stop_gradient", "(", "loss_mask", ")", "\n", "per_example_loss", "=", "per_example_loss", "*", "loss_mask", "\n", "sup_loss", "=", "(", "tf", ".", "reduce_sum", "(", "per_example_loss", ")", "/", "\n", "tf", ".", "maximum", "(", "tf", ".", "reduce_sum", "(", "loss_mask", ")", ",", "1", ")", ")", "\n", "\n", "", "unsup_loss_mask", "=", "None", "\n", "if", "is_training", "and", "unsup_ratio", ">", "0", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "\"unsup_loss\"", ")", ":", "\n", "      ", "ori_start", "=", "sup_batch_size", "\n", "ori_end", "=", "ori_start", "+", "unsup_batch_size", "\n", "aug_start", "=", "sup_batch_size", "+", "unsup_batch_size", "\n", "aug_end", "=", "aug_start", "+", "unsup_batch_size", "\n", "\n", "ori_log_probs", "=", "log_probs", "[", "ori_start", ":", "ori_end", "]", "\n", "aug_log_probs", "=", "log_probs", "[", "aug_start", ":", "aug_end", "]", "\n", "unsup_loss_mask", "=", "1", "\n", "if", "FLAGS", ".", "uda_softmax_temp", "!=", "-", "1", ":", "\n", "        ", "tgt_ori_log_probs", "=", "tf", ".", "nn", ".", "log_softmax", "(", "\n", "clas_logits", "[", "ori_start", ":", "ori_end", "]", "/", "FLAGS", ".", "uda_softmax_temp", ",", "\n", "axis", "=", "-", "1", ")", "\n", "tgt_ori_log_probs", "=", "tf", ".", "stop_gradient", "(", "tgt_ori_log_probs", ")", "\n", "", "else", ":", "\n", "        ", "tgt_ori_log_probs", "=", "tf", ".", "stop_gradient", "(", "ori_log_probs", ")", "\n", "\n", "", "if", "FLAGS", ".", "uda_confidence_thresh", "!=", "-", "1", ":", "\n", "        ", "largest_prob", "=", "tf", ".", "reduce_max", "(", "tf", ".", "exp", "(", "ori_log_probs", ")", ",", "axis", "=", "-", "1", ")", "\n", "unsup_loss_mask", "=", "tf", ".", "cast", "(", "tf", ".", "greater", "(", "\n", "largest_prob", ",", "FLAGS", ".", "uda_confidence_thresh", ")", ",", "tf", ".", "float32", ")", "\n", "unsup_loss_mask", "=", "tf", ".", "stop_gradient", "(", "unsup_loss_mask", ")", "\n", "\n", "", "per_example_kl_loss", "=", "kl_for_log_probs", "(", "\n", "tgt_ori_log_probs", ",", "aug_log_probs", ")", "*", "unsup_loss_mask", "\n", "unsup_loss", "=", "tf", ".", "reduce_mean", "(", "per_example_kl_loss", ")", "\n", "\n", "", "", "else", ":", "\n", "    ", "unsup_loss", "=", "0.", "\n", "\n", "", "return", "(", "sup_loss", ",", "unsup_loss", ",", "clas_logits", "[", ":", "sup_batch_size", "]", ",", "\n", "per_example_loss", ",", "loss_mask", ",", "\n", "tsa_threshold", ",", "unsup_loss_mask", ",", "correct_label_probs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.uda.get_assignment_map_from_checkpoint": [[185, 210], ["collections.OrderedDict", "tensorflow.train.list_variables", "collections.OrderedDict", "re.match", "re.match.group"], "function", ["None"], ["", "def", "get_assignment_map_from_checkpoint", "(", "tvars", ",", "init_checkpoint", ")", ":", "\n", "  ", "\"\"\"Compute the union of the current variables and checkpoint variables.\"\"\"", "\n", "assignment_map", "=", "{", "}", "\n", "initialized_variable_names", "=", "{", "}", "\n", "\n", "name_to_variable", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "var", "in", "tvars", ":", "\n", "    ", "name", "=", "var", ".", "name", "\n", "m", "=", "re", ".", "match", "(", "\"^(.*):\\\\d+$\"", ",", "name", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "      ", "name", "=", "m", ".", "group", "(", "1", ")", "\n", "", "name_to_variable", "[", "name", "]", "=", "var", "\n", "\n", "", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "init_checkpoint", ")", "\n", "\n", "assignment_map", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "x", "in", "init_vars", ":", "\n", "    ", "(", "name", ",", "var", ")", "=", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", "\n", "if", "name", "not", "in", "name_to_variable", ":", "\n", "      ", "continue", "\n", "", "assignment_map", "[", "name", "]", "=", "name_to_variable", "[", "name", "]", "\n", "initialized_variable_names", "[", "name", "]", "=", "1", "\n", "initialized_variable_names", "[", "name", "+", "\":0\"", "]", "=", "1", "\n", "\n", "", "return", "(", "assignment_map", ",", "initialized_variable_names", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.uda.model_fn_builder": [[212, 383], ["tensorflow.train.get_or_create_global_step", "tensorflow.reshape", "uda.create_model", "tensorflow.argmax", "tensorflow.to_float", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.trainable_variables", "tensorflow.logging.info", "sorted", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.equal", "tensorflow.reduce_mean", "uda.get_assignment_map_from_checkpoint", "tensorflow.logging.info", "bert.optimization.create_optimizer", "utils.tpu_utils.construct_scalar_host_call", "tensorflow.contrib.tpu.TPUEstimatorSpec", "features.keys", "tensorflow.logging.info", "tensorflow.train.init_from_checkpoint", "tensorflow.logging.info", "tensorflow.contrib.tpu.TPUEstimatorSpec", "ValueError", "tensorflow.train.init_from_checkpoint", "tensorflow.train.Scaffold", "tensorflow.metrics.mean", "tensorflow.argmax", "tensorflow.metrics.accuracy"], "function", ["home.repos.pwc.inspect_result.google-research_uda.text.uda.create_model", "home.repos.pwc.inspect_result.google-research_uda.text.uda.get_assignment_map_from_checkpoint", "home.repos.pwc.inspect_result.google-research_uda.bert.optimization.create_optimizer", "home.repos.pwc.inspect_result.google-research_uda.image.utils.construct_scalar_host_call"], ["", "def", "model_fn_builder", "(", "\n", "bert_config", ",", "\n", "init_checkpoint", ",", "\n", "learning_rate", ",", "\n", "clip_norm", ",", "\n", "num_train_steps", ",", "\n", "num_warmup_steps", ",", "\n", "use_tpu", ",", "\n", "use_one_hot_embeddings", ",", "\n", "num_labels", ",", "\n", "unsup_ratio", ",", "\n", "uda_coeff", ",", "\n", "tsa", ",", "\n", "print_feature", "=", "True", ",", "\n", "print_structure", "=", "True", ")", ":", "\n", "  ", "\"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"", "\n", "\n", "def", "model_fn", "(", "features", ",", "labels", ",", "mode", ",", "params", ")", ":", "# pylint: disable=unused-argument", "\n", "    ", "\"\"\"The `model_fn` for TPUEstimator.\"\"\"", "\n", "if", "print_feature", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"*** Features ***\"", ")", "\n", "for", "name", "in", "sorted", "(", "features", ".", "keys", "(", ")", ")", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\n", "\"  name = %s, shape = %s\"", "%", "(", "name", ",", "features", "[", "name", "]", ".", "shape", ")", ")", "\n", "\n", "", "", "is_training", "=", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ")", "\n", "\n", "global_step", "=", "tf", ".", "train", ".", "get_or_create_global_step", "(", ")", "\n", "##### Classification objective", "\n", "label_ids", "=", "features", "[", "\"label_ids\"", "]", "\n", "label_ids", "=", "tf", ".", "reshape", "(", "label_ids", ",", "[", "-", "1", "]", ")", "\n", "\n", "if", "unsup_ratio", ">", "0", "and", "\"ori_input_ids\"", "in", "features", ":", "\n", "      ", "input_ids", "=", "tf", ".", "concat", "(", "[", "\n", "features", "[", "\"input_ids\"", "]", ",", "\n", "features", "[", "\"ori_input_ids\"", "]", ",", "\n", "features", "[", "\"aug_input_ids\"", "]", "]", ",", "0", ")", "\n", "input_mask", "=", "tf", ".", "concat", "(", "[", "\n", "features", "[", "\"input_mask\"", "]", ",", "\n", "features", "[", "\"ori_input_mask\"", "]", ",", "\n", "features", "[", "\"aug_input_mask\"", "]", "]", ",", "0", ")", "\n", "input_type_ids", "=", "tf", ".", "concat", "(", "[", "\n", "features", "[", "\"input_type_ids\"", "]", ",", "\n", "features", "[", "\"ori_input_type_ids\"", "]", ",", "\n", "features", "[", "\"aug_input_type_ids\"", "]", "]", ",", "0", ")", "\n", "", "else", ":", "\n", "      ", "input_ids", "=", "features", "[", "\"input_ids\"", "]", "\n", "input_mask", "=", "features", "[", "\"input_mask\"", "]", "\n", "input_type_ids", "=", "features", "[", "\"input_type_ids\"", "]", "\n", "\n", "", "(", "sup_loss", ",", "unsup_loss", ",", "logits", ",", "\n", "per_example_loss", ",", "loss_mask", ",", "\n", "tsa_threshold", ",", "\n", "unsup_loss_mask", ",", "correct_label_probs", ")", "=", "create_model", "(", "\n", "bert_config", "=", "bert_config", ",", "\n", "is_training", "=", "is_training", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "input_type_ids", "=", "input_type_ids", ",", "\n", "labels", "=", "label_ids", ",", "\n", "num_labels", "=", "num_labels", ",", "\n", "use_one_hot_embeddings", "=", "use_one_hot_embeddings", ",", "\n", "tsa", "=", "tsa", ",", "\n", "unsup_ratio", "=", "unsup_ratio", ",", "\n", "global_step", "=", "global_step", ",", "\n", "num_train_steps", "=", "num_train_steps", ",", "\n", ")", "\n", "\n", "##### Aggregate losses into total_loss", "\n", "metric_dict", "=", "{", "}", "\n", "\n", "# number of correct predictions", "\n", "predictions", "=", "tf", ".", "argmax", "(", "logits", ",", "axis", "=", "-", "1", ",", "output_type", "=", "label_ids", ".", "dtype", ")", "\n", "is_correct", "=", "tf", ".", "to_float", "(", "tf", ".", "equal", "(", "predictions", ",", "label_ids", ")", ")", "\n", "acc", "=", "tf", ".", "reduce_mean", "(", "is_correct", ")", "\n", "# add sup. metrics to dict", "\n", "metric_dict", "[", "\"sup/loss\"", "]", "=", "sup_loss", "\n", "metric_dict", "[", "\"sup/accu\"", "]", "=", "acc", "\n", "metric_dict", "[", "\"sup/correct_cat_probs\"", "]", "=", "correct_label_probs", "\n", "metric_dict", "[", "\"sup/tsa_threshold\"", "]", "=", "tsa_threshold", "\n", "\n", "metric_dict", "[", "\"sup/sup_trained_ratio\"", "]", "=", "tf", ".", "reduce_mean", "(", "loss_mask", ")", "\n", "total_loss", "=", "sup_loss", "\n", "\n", "if", "unsup_ratio", ">", "0", "and", "uda_coeff", ">", "0", "and", "\"input_ids\"", "in", "features", ":", "\n", "      ", "total_loss", "+=", "uda_coeff", "*", "unsup_loss", "\n", "metric_dict", "[", "\"unsup/loss\"", "]", "=", "unsup_loss", "\n", "\n", "", "if", "unsup_loss_mask", "is", "not", "None", ":", "\n", "      ", "metric_dict", "[", "\"unsup/high_prob_ratio\"", "]", "=", "tf", ".", "reduce_mean", "(", "unsup_loss_mask", ")", "\n", "\n", "##### Initialize variables with pre-trained models", "\n", "", "tvars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "\n", "scaffold_fn", "=", "None", "\n", "if", "init_checkpoint", ":", "\n", "      ", "(", "assignment_map", ",", "\n", "initialized_variable_names", ")", "=", "get_assignment_map_from_checkpoint", "(", "\n", "tvars", ",", "init_checkpoint", ")", "\n", "if", "use_tpu", ":", "\n", "        ", "def", "tpu_scaffold", "(", ")", ":", "\n", "          ", "tf", ".", "train", ".", "init_from_checkpoint", "(", "init_checkpoint", ",", "assignment_map", ")", "\n", "return", "tf", ".", "train", ".", "Scaffold", "(", ")", "\n", "\n", "", "scaffold_fn", "=", "tpu_scaffold", "\n", "", "else", ":", "\n", "        ", "tf", ".", "train", ".", "init_from_checkpoint", "(", "init_checkpoint", ",", "assignment_map", ")", "\n", "", "", "else", ":", "\n", "      ", "initialized_variable_names", "=", "{", "}", "\n", "\n", "", "if", "print_structure", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"**** Trainable Variables ****\"", ")", "\n", "for", "var", "in", "tvars", ":", "\n", "        ", "init_string", "=", "\"\"", "\n", "if", "var", ".", "name", "in", "initialized_variable_names", ":", "\n", "          ", "init_string", "=", "\", *INIT_FROM_CKPT*\"", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"  name = %s, shape = %s%s\"", ",", "var", ".", "name", ",", "var", ".", "shape", ",", "\n", "init_string", ")", "\n", "\n", "##### Construct TPU Estimator Spec based on the specific mode", "\n", "", "", "output_spec", "=", "None", "\n", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "## Create optimizer for training", "\n", "      ", "train_op", ",", "curr_lr", "=", "optimization", ".", "create_optimizer", "(", "\n", "total_loss", ",", "learning_rate", ",", "num_train_steps", ",", "num_warmup_steps", ",", "\n", "use_tpu", ",", "clip_norm", ",", "global_step", ")", "\n", "metric_dict", "[", "\"learning_rate\"", "]", "=", "curr_lr", "\n", "\n", "## Create host_call for training", "\n", "host_call", "=", "tpu_utils", ".", "construct_scalar_host_call", "(", "\n", "metric_dict", "=", "metric_dict", ",", "\n", "model_dir", "=", "params", "[", "\"model_dir\"", "]", ",", "\n", "prefix", "=", "\"training/\"", ",", "\n", "reduce_fn", "=", "tf", ".", "reduce_mean", ")", "\n", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "\n", "loss", "=", "total_loss", ",", "\n", "train_op", "=", "train_op", ",", "\n", "host_call", "=", "host_call", ",", "\n", "scaffold_fn", "=", "scaffold_fn", ")", "\n", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "\n", "      ", "def", "clas_metric_fn", "(", "per_example_loss", ",", "label_ids", ",", "logits", ")", ":", "\n", "## classification loss & accuracy", "\n", "        ", "loss", "=", "tf", ".", "metrics", ".", "mean", "(", "per_example_loss", ")", "\n", "\n", "predictions", "=", "tf", ".", "argmax", "(", "logits", ",", "axis", "=", "-", "1", ",", "output_type", "=", "tf", ".", "int32", ")", "\n", "accuracy", "=", "tf", ".", "metrics", ".", "accuracy", "(", "label_ids", ",", "predictions", ")", "\n", "\n", "ret_dict", "=", "{", "\n", "\"eval_classify_loss\"", ":", "loss", ",", "\n", "\"eval_classify_accuracy\"", ":", "accuracy", "\n", "}", "\n", "\n", "return", "ret_dict", "\n", "\n", "", "eval_metrics", "=", "(", "clas_metric_fn", ",", "[", "per_example_loss", ",", "label_ids", ",", "logits", "]", ")", "\n", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "\n", "loss", "=", "total_loss", ",", "\n", "eval_metrics", "=", "eval_metrics", ",", "\n", "scaffold_fn", "=", "scaffold_fn", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Only TRAIN and EVAL modes are supported: %s\"", "%", "(", "mode", ")", ")", "\n", "\n", "", "return", "output_spec", "\n", "\n", "", "return", "model_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.main.main": [[175, 329], ["tensorflow.logging.set_verbosity", "utils.raw_data_utils.get_processor", "raw_data_utils.get_processor.get_labels", "bert.modeling.BertConfig.from_json_file", "tensorflow.gfile.MakeDirs", "tensorflow.app.flags.FLAGS.flag_values_dict", "tensorflow.logging.info", "tensorflow.logging.info", "min", "tensorflow.contrib.tpu.RunConfig", "uda.model_fn_builder", "tensorflow.contrib.tpu.TPUEstimator", "tensorflow.gfile.Open", "json.dump", "tensorflow.contrib.cluster_resolver.TPUClusterResolver", "tensorflow.logging.info", "utils.proc_data_utils.training_input_fn_builder", "tensorflow.logging.info", "utils.proc_data_utils.evaluation_input_fn_builder", "raw_data_utils.get_processor.get_dev_size", "int", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "range", "tensorflow.logging.info", "tensorflow.logging.info", "os.path.join", "tensorflow.contrib.tpu.TPUConfig", "len", "tensorflow.logging.info", "tensorflow.logging.info", "tf.contrib.tpu.TPUEstimator.train", "tensorflow.logging.info", "tf.contrib.tpu.TPUEstimator.evaluate", "tensorflow.logging.info", "estimator.evaluate.keys", "max", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tf.contrib.tpu.TPUEstimator.train", "tensorflow.logging.info", "dev_result[].item", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.train.get_checkpoint_state", "tensorflow.logging.info", "tensorflow.logging.info", "str", "tensorflow.logging.info", "tf.contrib.tpu.TPUEstimator.evaluate", "tensorflow.logging.info", "estimator.evaluate.keys", "max", "tensorflow.gfile.Exists", "tensorflow.logging.info", "tensorflow.logging.info", "dev_result[].item", "str"], "function", ["home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.get_processor", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DBPediaProcessor.get_labels", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.BertConfig.from_json_file", "home.repos.pwc.inspect_result.google-research_uda.text.uda.model_fn_builder", "home.repos.pwc.inspect_result.google-research_uda.utils.proc_data_utils.training_input_fn_builder", "home.repos.pwc.inspect_result.google-research_uda.utils.proc_data_utils.evaluation_input_fn_builder", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DBPediaProcessor.get_dev_size", "home.repos.pwc.inspect_result.google-research_uda.image.main.train", "home.repos.pwc.inspect_result.google-research_uda.image.main.train"], ["def", "main", "(", "_", ")", ":", "\n", "\n", "  ", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "\n", "processor", "=", "raw_data_utils", ".", "get_processor", "(", "FLAGS", ".", "task_name", ")", "\n", "label_list", "=", "processor", ".", "get_labels", "(", ")", "\n", "\n", "bert_config", "=", "modeling", ".", "BertConfig", ".", "from_json_file", "(", "\n", "FLAGS", ".", "bert_config_file", ",", "\n", "FLAGS", ".", "model_dropout", ")", "\n", "\n", "\n", "tf", ".", "gfile", ".", "MakeDirs", "(", "FLAGS", ".", "model_dir", ")", "\n", "\n", "flags_dict", "=", "tf", ".", "app", ".", "flags", ".", "FLAGS", ".", "flag_values_dict", "(", ")", "\n", "with", "tf", ".", "gfile", ".", "Open", "(", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "model_dir", ",", "\"FLAGS.json\"", ")", ",", "\"w\"", ")", "as", "ouf", ":", "\n", "    ", "json", ".", "dump", "(", "flags_dict", ",", "ouf", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"warmup steps {}/{}\"", ".", "format", "(", "\n", "FLAGS", ".", "num_warmup_steps", ",", "FLAGS", ".", "num_train_steps", ")", ")", "\n", "\n", "save_checkpoints_steps", "=", "FLAGS", ".", "num_train_steps", "//", "FLAGS", ".", "save_checkpoints_num", "\n", "tf", ".", "logging", ".", "info", "(", "\"setting save checkpoints steps to {:d}\"", ".", "format", "(", "\n", "save_checkpoints_steps", ")", ")", "\n", "\n", "FLAGS", ".", "iterations_per_loop", "=", "min", "(", "save_checkpoints_steps", ",", "\n", "FLAGS", ".", "iterations_per_loop", ")", "\n", "if", "FLAGS", ".", "use_tpu", "and", "FLAGS", ".", "tpu_name", ":", "\n", "    ", "tpu_cluster_resolver", "=", "tf", ".", "contrib", ".", "cluster_resolver", ".", "TPUClusterResolver", "(", "\n", "FLAGS", ".", "tpu_name", ",", "zone", "=", "FLAGS", ".", "tpu_zone", ",", "project", "=", "FLAGS", ".", "gcp_project", ")", "\n", "", "else", ":", "\n", "    ", "tpu_cluster_resolver", "=", "None", "\n", "# if not FLAGS.use_tpu and FLAGS.num_gpu > 1:", "\n", "#   train_distribute = tf.contrib.distribute.MirroredStrategy(", "\n", "#       num_gpus=FLAGS.num_gpu)", "\n", "# else:", "\n", "#   train_distribute = None", "\n", "\n", "", "is_per_host", "=", "tf", ".", "contrib", ".", "tpu", ".", "InputPipelineConfig", ".", "PER_HOST_V2", "\n", "run_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "RunConfig", "(", "\n", "cluster", "=", "tpu_cluster_resolver", ",", "\n", "master", "=", "FLAGS", ".", "master", ",", "\n", "model_dir", "=", "FLAGS", ".", "model_dir", ",", "\n", "save_checkpoints_steps", "=", "save_checkpoints_steps", ",", "\n", "keep_checkpoint_max", "=", "1000", ",", "\n", "# train_distribute=train_distribute,", "\n", "tpu_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUConfig", "(", "\n", "iterations_per_loop", "=", "FLAGS", ".", "iterations_per_loop", ",", "\n", "per_host_input_for_training", "=", "is_per_host", ")", ")", "\n", "\n", "model_fn", "=", "uda", ".", "model_fn_builder", "(", "\n", "bert_config", "=", "bert_config", ",", "\n", "init_checkpoint", "=", "FLAGS", ".", "init_checkpoint", ",", "\n", "learning_rate", "=", "FLAGS", ".", "learning_rate", ",", "\n", "clip_norm", "=", "FLAGS", ".", "clip_norm", ",", "\n", "num_train_steps", "=", "FLAGS", ".", "num_train_steps", ",", "\n", "num_warmup_steps", "=", "FLAGS", ".", "num_warmup_steps", ",", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "use_one_hot_embeddings", "=", "FLAGS", ".", "use_one_hot_embeddings", ",", "\n", "num_labels", "=", "len", "(", "label_list", ")", ",", "\n", "unsup_ratio", "=", "FLAGS", ".", "unsup_ratio", ",", "\n", "uda_coeff", "=", "FLAGS", ".", "uda_coeff", ",", "\n", "tsa", "=", "FLAGS", ".", "tsa", ",", "\n", "print_feature", "=", "False", ",", "\n", "print_structure", "=", "False", ",", "\n", ")", "\n", "\n", "# If TPU is not available, this will fall back to normal Estimator on CPU", "\n", "# or GPU.", "\n", "estimator", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimator", "(", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "model_fn", "=", "model_fn", ",", "\n", "config", "=", "run_config", ",", "\n", "params", "=", "{", "\"model_dir\"", ":", "FLAGS", ".", "model_dir", "}", ",", "\n", "train_batch_size", "=", "FLAGS", ".", "train_batch_size", ",", "\n", "eval_batch_size", "=", "FLAGS", ".", "eval_batch_size", ")", "\n", "\n", "if", "FLAGS", ".", "do_train", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"  >>> sup data dir : {}\"", ".", "format", "(", "FLAGS", ".", "sup_train_data_dir", ")", ")", "\n", "if", "FLAGS", ".", "unsup_ratio", ">", "0", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"  >>> unsup data dir : {}\"", ".", "format", "(", "\n", "FLAGS", ".", "unsup_data_dir", ")", ")", "\n", "\n", "", "train_input_fn", "=", "proc_data_utils", ".", "training_input_fn_builder", "(", "\n", "FLAGS", ".", "sup_train_data_dir", ",", "\n", "FLAGS", ".", "unsup_data_dir", ",", "\n", "FLAGS", ".", "aug_ops", ",", "\n", "FLAGS", ".", "aug_copy", ",", "\n", "FLAGS", ".", "unsup_ratio", ")", "\n", "\n", "", "if", "FLAGS", ".", "do_eval", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"  >>> dev data dir : {}\"", ".", "format", "(", "FLAGS", ".", "eval_data_dir", ")", ")", "\n", "eval_input_fn", "=", "proc_data_utils", ".", "evaluation_input_fn_builder", "(", "\n", "FLAGS", ".", "eval_data_dir", ",", "\n", "\"clas\"", ")", "\n", "\n", "eval_size", "=", "processor", ".", "get_dev_size", "(", ")", "\n", "eval_steps", "=", "int", "(", "eval_size", "/", "FLAGS", ".", "eval_batch_size", ")", "\n", "\n", "", "if", "FLAGS", ".", "do_train", "and", "FLAGS", ".", "do_eval", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"***** Running training & evaluation *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Supervised batch size = %d\"", ",", "FLAGS", ".", "train_batch_size", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Unsupervised batch size = %d\"", ",", "\n", "FLAGS", ".", "train_batch_size", "*", "FLAGS", ".", "unsup_ratio", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num steps = %d\"", ",", "FLAGS", ".", "num_train_steps", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Base evaluation batch size = %d\"", ",", "FLAGS", ".", "eval_batch_size", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num steps = %d\"", ",", "eval_steps", ")", "\n", "best_acc", "=", "0", "\n", "for", "_", "in", "range", "(", "0", ",", "FLAGS", ".", "num_train_steps", ",", "save_checkpoints_steps", ")", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"*** Running training ***\"", ")", "\n", "estimator", ".", "train", "(", "\n", "input_fn", "=", "train_input_fn", ",", "\n", "steps", "=", "save_checkpoints_steps", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"*** Running evaluation ***\"", ")", "\n", "dev_result", "=", "estimator", ".", "evaluate", "(", "input_fn", "=", "eval_input_fn", ",", "steps", "=", "eval_steps", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\">> Results:\"", ")", "\n", "for", "key", "in", "dev_result", ".", "keys", "(", ")", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "dev_result", "[", "key", "]", ")", ")", "\n", "dev_result", "[", "key", "]", "=", "dev_result", "[", "key", "]", ".", "item", "(", ")", "\n", "", "best_acc", "=", "max", "(", "best_acc", ",", "dev_result", "[", "\"eval_classify_accuracy\"", "]", ")", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"***** Final evaluation result *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Best acc: {:.3f}\\n\\n\"", ".", "format", "(", "best_acc", ")", ")", "\n", "", "elif", "FLAGS", ".", "do_train", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Supervised batch size = %d\"", ",", "FLAGS", ".", "train_batch_size", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Unsupervised batch size = %d\"", ",", "\n", "FLAGS", ".", "train_batch_size", "*", "FLAGS", ".", "unsup_ratio", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num steps = %d\"", ",", "FLAGS", ".", "num_train_steps", ")", "\n", "estimator", ".", "train", "(", "input_fn", "=", "train_input_fn", ",", "max_steps", "=", "FLAGS", ".", "num_train_steps", ")", "\n", "", "elif", "FLAGS", ".", "do_eval", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"***** Running evaluation *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Base evaluation batch size = %d\"", ",", "FLAGS", ".", "eval_batch_size", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num steps = %d\"", ",", "eval_steps", ")", "\n", "checkpoint_state", "=", "tf", ".", "train", ".", "get_checkpoint_state", "(", "FLAGS", ".", "model_dir", ")", "\n", "\n", "best_acc", "=", "0", "\n", "for", "ckpt_path", "in", "checkpoint_state", ".", "all_model_checkpoint_paths", ":", "\n", "      ", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "ckpt_path", "+", "\".data-00000-of-00001\"", ")", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\n", "\"Warning: checkpoint {:s} does not exist\"", ".", "format", "(", "ckpt_path", ")", ")", "\n", "continue", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"Evaluating {:s}\"", ".", "format", "(", "ckpt_path", ")", ")", "\n", "dev_result", "=", "estimator", ".", "evaluate", "(", "\n", "input_fn", "=", "eval_input_fn", ",", "\n", "steps", "=", "eval_steps", ",", "\n", "checkpoint_path", "=", "ckpt_path", ",", "\n", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\">> Results:\"", ")", "\n", "for", "key", "in", "dev_result", ".", "keys", "(", ")", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "dev_result", "[", "key", "]", ")", ")", "\n", "dev_result", "[", "key", "]", "=", "dev_result", "[", "key", "]", ".", "item", "(", ")", "\n", "", "best_acc", "=", "max", "(", "best_acc", ",", "dev_result", "[", "\"eval_classify_accuracy\"", "]", ")", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"***** Final evaluation result *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Best acc: {:.3f}\\n\\n\"", ".", "format", "(", "best_acc", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.InputFeatures.__init__": [[297, 302], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "input_type_ids", ",", "label_id", ")", ":", "\n", "    ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "input_type_ids", "=", "input_type_ids", "\n", "self", ".", "label_id", "=", "label_id", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.InputFeatures.get_dict_features": [[303, 309], ["preprocess._create_int_feature", "preprocess._create_int_feature", "preprocess._create_int_feature", "preprocess._create_int_feature"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.text.preprocess._create_int_feature", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess._create_int_feature", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess._create_int_feature", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess._create_int_feature"], ["", "def", "get_dict_features", "(", "self", ")", ":", "\n", "    ", "return", "{", "\n", "\"input_ids\"", ":", "_create_int_feature", "(", "self", ".", "input_ids", ")", ",", "\n", "\"input_mask\"", ":", "_create_int_feature", "(", "self", ".", "input_mask", ")", ",", "\n", "\"input_type_ids\"", ":", "_create_int_feature", "(", "self", ".", "input_type_ids", ")", ",", "\n", "\"label_ids\"", ":", "_create_int_feature", "(", "[", "self", ".", "label_id", "]", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.PairedUnsupInputFeatures.__init__": [[315, 323], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "ori_input_ids", ",", "ori_input_mask", ",", "ori_input_type_ids", ",", "\n", "aug_input_ids", ",", "aug_input_mask", ",", "aug_input_type_ids", ")", ":", "\n", "    ", "self", ".", "ori_input_ids", "=", "ori_input_ids", "\n", "self", ".", "ori_input_mask", "=", "ori_input_mask", "\n", "self", ".", "ori_input_type_ids", "=", "ori_input_type_ids", "\n", "self", ".", "aug_input_ids", "=", "aug_input_ids", "\n", "self", ".", "aug_input_mask", "=", "aug_input_mask", "\n", "self", ".", "aug_input_type_ids", "=", "aug_input_type_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.PairedUnsupInputFeatures.get_dict_features": [[324, 332], ["preprocess._create_int_feature", "preprocess._create_int_feature", "preprocess._create_int_feature", "preprocess._create_int_feature", "preprocess._create_int_feature", "preprocess._create_int_feature"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.text.preprocess._create_int_feature", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess._create_int_feature", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess._create_int_feature", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess._create_int_feature", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess._create_int_feature", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess._create_int_feature"], ["", "def", "get_dict_features", "(", "self", ")", ":", "\n", "    ", "return", "{", "\n", "\"ori_input_ids\"", ":", "_create_int_feature", "(", "self", ".", "ori_input_ids", ")", ",", "\n", "\"ori_input_mask\"", ":", "_create_int_feature", "(", "self", ".", "ori_input_mask", ")", ",", "\n", "\"ori_input_type_ids\"", ":", "_create_int_feature", "(", "self", ".", "ori_input_type_ids", ")", ",", "\n", "\"aug_input_ids\"", ":", "_create_int_feature", "(", "self", ".", "aug_input_ids", ")", ",", "\n", "\"aug_input_mask\"", ":", "_create_int_feature", "(", "self", ".", "aug_input_mask", ")", ",", "\n", "\"aug_input_type_ids\"", ":", "_create_int_feature", "(", "self", ".", "aug_input_type_ids", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.get_data_for_worker": [[99, 113], ["tensorflow.logging.info", "len", "len", "len"], "function", ["None"], ["def", "get_data_for_worker", "(", "examples", ",", "replicas", ",", "worker_id", ")", ":", "\n", "  ", "data_per_worker", "=", "len", "(", "examples", ")", "//", "replicas", "\n", "remainder", "=", "len", "(", "examples", ")", "-", "replicas", "*", "data_per_worker", "\n", "if", "worker_id", "<", "remainder", ":", "\n", "    ", "start", "=", "(", "data_per_worker", "+", "1", ")", "*", "worker_id", "\n", "end", "=", "(", "data_per_worker", "+", "1", ")", "*", "(", "worker_id", "+", "1", ")", "\n", "", "else", ":", "\n", "    ", "start", "=", "data_per_worker", "*", "worker_id", "+", "remainder", "\n", "end", "=", "data_per_worker", "*", "(", "worker_id", "+", "1", ")", "+", "remainder", "\n", "", "if", "worker_id", "==", "replicas", "-", "1", ":", "\n", "    ", "assert", "end", "==", "len", "(", "examples", ")", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"processing data from {:d} to {:d}\"", ".", "format", "(", "start", ",", "end", ")", ")", "\n", "examples", "=", "examples", "[", "start", ":", "end", "]", "\n", "return", "examples", ",", "start", ",", "end", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.build_vocab": [[115, 126], ["range", "len", "preprocess.build_vocab.add_to_vocab"], "function", ["None"], ["", "def", "build_vocab", "(", "examples", ")", ":", "\n", "  ", "vocab", "=", "{", "}", "\n", "def", "add_to_vocab", "(", "word_list", ")", ":", "\n", "    ", "for", "word", "in", "word_list", ":", "\n", "      ", "if", "word", "not", "in", "vocab", ":", "\n", "        ", "vocab", "[", "word", "]", "=", "len", "(", "vocab", ")", "\n", "", "", "", "for", "i", "in", "range", "(", "len", "(", "examples", ")", ")", ":", "\n", "    ", "add_to_vocab", "(", "examples", "[", "i", "]", ".", "word_list_a", ")", "\n", "if", "examples", "[", "i", "]", ".", "text_b", ":", "\n", "      ", "add_to_vocab", "(", "examples", "[", "i", "]", ".", "word_list_b", ")", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.get_data_stats": [[128, 154], ["tensorflow.logging.info", "augmentation.word_level_augment.get_data_stats", "tensorflow.gfile.MakeDirs", "tensorflow.logging.info", "tensorflow.gfile.Exists", "tensorflow.logging.info", "tensorflow.gfile.Open", "json.load", "tensorflow.gfile.Open", "json.dump"], "function", ["home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.get_data_stats"], ["", "def", "get_data_stats", "(", "data_stats_dir", ",", "sub_set", ",", "sup_size", ",", "replicas", ",", "examples", ")", ":", "\n", "  ", "data_stats_dir", "=", "\"{}/{}\"", ".", "format", "(", "data_stats_dir", ",", "sub_set", ")", "\n", "keys", "=", "[", "\"tf_idf\"", ",", "\"idf\"", "]", "\n", "all_exist", "=", "True", "\n", "for", "key", "in", "keys", ":", "\n", "    ", "data_stats_path", "=", "\"{}/{}.json\"", ".", "format", "(", "data_stats_dir", ",", "key", ")", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "data_stats_path", ")", ":", "\n", "      ", "all_exist", "=", "False", "\n", "tf", ".", "logging", ".", "info", "(", "\"Not exist: {}\"", ".", "format", "(", "data_stats_path", ")", ")", "\n", "", "", "if", "all_exist", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"loading data stats from {:s}\"", ".", "format", "(", "data_stats_dir", ")", ")", "\n", "data_stats", "=", "{", "}", "\n", "for", "key", "in", "keys", ":", "\n", "      ", "with", "tf", ".", "gfile", ".", "Open", "(", "\n", "\"{}/{}.json\"", ".", "format", "(", "data_stats_dir", ",", "key", ")", ")", "as", "inf", ":", "\n", "        ", "data_stats", "[", "key", "]", "=", "json", ".", "load", "(", "inf", ")", "\n", "", "", "", "else", ":", "\n", "    ", "assert", "sup_size", "==", "-", "1", ",", "\"should use the complete set to get tf_idf\"", "\n", "assert", "replicas", "==", "1", ",", "\"should use the complete set to get tf_idf\"", "\n", "data_stats", "=", "word_level_augment", ".", "get_data_stats", "(", "examples", ")", "\n", "tf", ".", "gfile", ".", "MakeDirs", "(", "data_stats_dir", ")", "\n", "for", "key", "in", "keys", ":", "\n", "      ", "with", "tf", ".", "gfile", ".", "Open", "(", "\"{}/{}.json\"", ".", "format", "(", "data_stats_dir", ",", "key", ")", ",", "\"w\"", ")", "as", "ouf", ":", "\n", "        ", "json", ".", "dump", "(", "data_stats", "[", "key", "]", ",", "ouf", ")", "\n", "", "", "tf", ".", "logging", ".", "info", "(", "\"dumped data stats to {:s}\"", ".", "format", "(", "data_stats_dir", ")", ")", "\n", "", "return", "data_stats", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.tokenize_examples": [[156, 165], ["tensorflow.logging.info", "range", "len", "tokenizer.tokenize_to_word", "tokenizer.tokenize_to_word", "tensorflow.logging.info"], "function", ["home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.FullTokenizer.tokenize_to_word", "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.FullTokenizer.tokenize_to_word"], ["", "def", "tokenize_examples", "(", "examples", ",", "tokenizer", ")", ":", "\n", "  ", "tf", ".", "logging", ".", "info", "(", "\"tokenizing examples\"", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "examples", ")", ")", ":", "\n", "    ", "examples", "[", "i", "]", ".", "word_list_a", "=", "tokenizer", ".", "tokenize_to_word", "(", "examples", "[", "i", "]", ".", "text_a", ")", "\n", "if", "examples", "[", "i", "]", ".", "text_b", ":", "\n", "      ", "examples", "[", "i", "]", ".", "word_list_b", "=", "tokenizer", ".", "tokenize_to_word", "(", "examples", "[", "i", "]", ".", "text_b", ")", "\n", "", "if", "i", "%", "10000", "==", "0", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"finished tokenizing example {:d}\"", ".", "format", "(", "i", ")", ")", "\n", "", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.convert_examples_to_features": [[167, 287], ["enumerate", "tensorflow.logging.info", "enumerate", "tensorflow.logging.info", "preprocess.build_vocab", "augmentation.word_level_augment.word_level_augment", "tokenizer.tokenize_to_wordpiece", "tokens.append", "input_type_ids.append", "tokens.append", "input_type_ids.append", "tokenizer.convert_tokens_to_ids", "features.append", "len", "tensorflow.logging.info", "tokenizer.tokenize_to_wordpiece", "tokens.append", "input_type_ids.append", "tokens.append", "input_type_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "input_type_ids.append", "len", "len", "len", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "preprocess.InputFeatures", "preprocess._truncate_seq_pair_keep_right", "preprocess._truncate_seq_pair", "len", "tokens.append", "input_type_ids.append", "isinstance", "x.encode", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.google-research_uda.text.preprocess.build_vocab", "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.word_level_augment", "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.FullTokenizer.tokenize_to_wordpiece", "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.FullTokenizer.tokenize_to_wordpiece", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess._truncate_seq_pair_keep_right", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess._truncate_seq_pair"], ["", "def", "convert_examples_to_features", "(", "\n", "examples", ",", "label_list", ",", "seq_length", ",", "tokenizer", ",", "trunc_keep_right", ",", "\n", "data_stats", "=", "None", ",", "aug_ops", "=", "None", ")", ":", "\n", "  ", "\"\"\"convert examples to features.\"\"\"", "\n", "\n", "label_map", "=", "{", "}", "\n", "for", "(", "i", ",", "label", ")", "in", "enumerate", "(", "label_list", ")", ":", "\n", "    ", "label_map", "[", "label", "]", "=", "i", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"number of examples to process: {}\"", ".", "format", "(", "len", "(", "examples", ")", ")", ")", "\n", "\n", "features", "=", "[", "]", "\n", "\n", "if", "aug_ops", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"building vocab\"", ")", "\n", "word_vocab", "=", "build_vocab", "(", "examples", ")", "\n", "examples", "=", "word_level_augment", ".", "word_level_augment", "(", "\n", "examples", ",", "aug_ops", ",", "word_vocab", ",", "data_stats", "\n", ")", "\n", "\n", "", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "    ", "if", "ex_index", "%", "10000", "==", "0", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"processing {:d}\"", ".", "format", "(", "ex_index", ")", ")", "\n", "", "tokens_a", "=", "tokenizer", ".", "tokenize_to_wordpiece", "(", "example", ".", "word_list_a", ")", "\n", "tokens_b", "=", "None", "\n", "if", "example", ".", "text_b", ":", "\n", "      ", "tokens_b", "=", "tokenizer", ".", "tokenize_to_wordpiece", "(", "example", ".", "word_list_b", ")", "\n", "\n", "", "if", "tokens_b", ":", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total", "\n", "# length is less than the specified length.", "\n", "# Account for [CLS], [SEP], [SEP] with \"- 3\"", "\n", "      ", "if", "trunc_keep_right", ":", "\n", "        ", "_truncate_seq_pair_keep_right", "(", "tokens_a", ",", "tokens_b", ",", "seq_length", "-", "3", ")", "\n", "", "else", ":", "\n", "        ", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "seq_length", "-", "3", ")", "\n", "", "", "else", ":", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "      ", "if", "len", "(", "tokens_a", ")", ">", "seq_length", "-", "2", ":", "\n", "        ", "if", "trunc_keep_right", ":", "\n", "          ", "tokens_a", "=", "tokens_a", "[", "-", "(", "seq_length", "-", "2", ")", ":", "]", "\n", "", "else", ":", "\n", "          ", "tokens_a", "=", "tokens_a", "[", "0", ":", "(", "seq_length", "-", "2", ")", "]", "\n", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] the dog is hairy . [SEP]", "\n", "#  type_ids: 0     0   0   0  0     0 0", "\n", "#", "\n", "# Where \"type_ids\" are used to indicate whether this is the first", "\n", "# sequence or the second sequence. The embedding vectors for `type=0` and", "\n", "# `type=1` were learned during pre-training and are added to the wordpiece", "\n", "# embedding vector (and position vector). This is not *strictly* necessary", "\n", "# since the [SEP] token unambigiously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "#", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "", "", "", "tokens", "=", "[", "]", "\n", "input_type_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "tokens_a", ":", "\n", "      ", "tokens", ".", "append", "(", "token", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "if", "tokens_b", ":", "\n", "      ", "for", "token", "in", "tokens_b", ":", "\n", "        ", "tokens", ".", "append", "(", "token", ")", "\n", "input_type_ids", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_type_ids", ".", "append", "(", "1", ")", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "seq_length", ":", "\n", "      ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_type_ids", ")", "==", "seq_length", "\n", "\n", "label_id", "=", "label_map", "[", "example", ".", "label", "]", "\n", "if", "ex_index", "<", "1", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"guid: %s\"", "%", "(", "example", ".", "guid", ")", ")", "\n", "# st = \" \".join([str(x) for x in tokens])", "\n", "st", "=", "\"\"", "\n", "for", "x", "in", "tokens", ":", "\n", "        ", "if", "isinstance", "(", "x", ",", "unicode", ")", ":", "\n", "          ", "st", "+=", "x", ".", "encode", "(", "\"ascii\"", ",", "\"replace\"", ")", "+", "\" \"", "\n", "", "else", ":", "\n", "          ", "st", "+=", "str", "(", "x", ")", "+", "\" \"", "\n", "", "", "tf", ".", "logging", ".", "info", "(", "\"tokens: %s\"", "%", "st", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "\"input_type_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_type_ids", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"label: %s (id = %d)\"", "%", "(", "example", ".", "label", ",", "label_id", ")", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "input_type_ids", "=", "input_type_ids", ",", "\n", "label_id", "=", "label_id", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.preprocess._create_int_feature": [[289, 292], ["tensorflow.train.Feature", "tensorflow.train.Int64List", "list"], "function", ["None"], ["", "def", "_create_int_feature", "(", "values", ")", ":", "\n", "  ", "feature", "=", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "list", "(", "values", ")", ")", ")", "\n", "return", "feature", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.obtain_tfrecord_writer": [[335, 341], ["tensorflow.python_io.TFRecordWriter", "os.path.join"], "function", ["None"], ["", "", "def", "obtain_tfrecord_writer", "(", "data_path", ",", "worker_id", ",", "shard_cnt", ")", ":", "\n", "  ", "tfrecord_writer", "=", "tf", ".", "python_io", ".", "TFRecordWriter", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "data_path", ",", "\n", "\"tf_examples.tfrecord.{:d}.{:d}\"", ".", "format", "(", "worker_id", ",", "shard_cnt", ")", ")", ")", "\n", "return", "tfrecord_writer", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.dump_tfrecord": [[343, 363], ["tensorflow.logging.info", "numpy.random.shuffle", "preprocess.obtain_tfrecord_writer", "obtain_tfrecord_writer.close", "tensorflow.gfile.Exists", "tensorflow.gfile.MakeDirs", "tensorflow.train.Example", "obtain_tfrecord_writer.write", "obtain_tfrecord_writer.close", "preprocess.obtain_tfrecord_writer", "tf.train.Example.SerializeToString", "tensorflow.train.Features", "feature.get_dict_features"], "function", ["home.repos.pwc.inspect_result.google-research_uda.image.preprocess.obtain_tfrecord_writer", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.obtain_tfrecord_writer", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.PairedUnsupInputFeatures.get_dict_features"], ["", "def", "dump_tfrecord", "(", "features", ",", "data_path", ",", "worker_id", "=", "None", ",", "max_shard_size", "=", "4096", ")", ":", "\n", "  ", "\"\"\"Dump tf record.\"\"\"", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "data_path", ")", ":", "\n", "    ", "tf", ".", "gfile", ".", "MakeDirs", "(", "data_path", ")", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"dumping TFRecords\"", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "features", ")", "\n", "shard_cnt", "=", "0", "\n", "shard_size", "=", "0", "\n", "tfrecord_writer", "=", "obtain_tfrecord_writer", "(", "data_path", ",", "worker_id", ",", "shard_cnt", ")", "\n", "for", "feature", "in", "features", ":", "\n", "    ", "tf_example", "=", "tf", ".", "train", ".", "Example", "(", "\n", "features", "=", "tf", ".", "train", ".", "Features", "(", "feature", "=", "feature", ".", "get_dict_features", "(", ")", ")", ")", "\n", "if", "shard_size", ">=", "max_shard_size", ":", "\n", "      ", "tfrecord_writer", ".", "close", "(", ")", "\n", "shard_cnt", "+=", "1", "\n", "tfrecord_writer", "=", "obtain_tfrecord_writer", "(", "data_path", ",", "worker_id", ",", "shard_cnt", ")", "\n", "shard_size", "=", "0", "\n", "", "shard_size", "+=", "1", "\n", "tfrecord_writer", ".", "write", "(", "tf_example", ".", "SerializeToString", "(", ")", ")", "\n", "", "tfrecord_writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.get_data_by_size_lim": [[365, 389], ["range", "processor.get_labels", "range", "len", "len", "processor.get_labels", "len", "processor.get_labels", "len", "len", "processor.get_labels"], "function", ["home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DBPediaProcessor.get_labels", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DBPediaProcessor.get_labels", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DBPediaProcessor.get_labels", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DBPediaProcessor.get_labels"], ["", "def", "get_data_by_size_lim", "(", "train_examples", ",", "processor", ",", "sup_size", ")", ":", "\n", "  ", "\"\"\"Deterministicly get a dataset with only sup_size examples.\"\"\"", "\n", "# Assuming sup_size < number of labeled data and", "\n", "# that there are same number of examples for each category", "\n", "assert", "sup_size", "%", "len", "(", "processor", ".", "get_labels", "(", ")", ")", "==", "0", "\n", "per_label_size", "=", "sup_size", "//", "len", "(", "processor", ".", "get_labels", "(", ")", ")", "\n", "per_label_examples", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "len", "(", "train_examples", ")", ")", ":", "\n", "    ", "label", "=", "train_examples", "[", "i", "]", ".", "label", "\n", "if", "label", "not", "in", "per_label_examples", ":", "\n", "      ", "per_label_examples", "[", "label", "]", "=", "[", "]", "\n", "", "per_label_examples", "[", "label", "]", "+=", "[", "train_examples", "[", "i", "]", "]", "\n", "\n", "", "for", "label", "in", "processor", ".", "get_labels", "(", ")", ":", "\n", "    ", "assert", "len", "(", "per_label_examples", "[", "label", "]", ")", ">=", "per_label_size", ",", "(", "\n", "\"label {} only has {} examples while the limit\"", "\n", "\"is {}\"", ".", "format", "(", "label", ",", "len", "(", "per_label_examples", "[", "label", "]", ")", ",", "per_label_size", ")", ")", "\n", "\n", "", "new_train_examples", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "per_label_size", ")", ":", "\n", "    ", "for", "label", "in", "processor", ".", "get_labels", "(", ")", ":", "\n", "      ", "new_train_examples", "+=", "[", "per_label_examples", "[", "label", "]", "[", "i", "]", "]", "\n", "", "", "train_examples", "=", "new_train_examples", "\n", "return", "train_examples", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.preprocess._truncate_seq_pair_keep_right": [[391, 406], ["len", "len", "len", "len", "tokens_a.pop", "tokens_b.pop"], "function", ["None"], ["", "def", "_truncate_seq_pair_keep_right", "(", "tokens_a", ",", "tokens_b", ",", "max_length", ")", ":", "\n", "  ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "    ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "      ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "      ", "tokens_a", ".", "pop", "(", "0", ")", "\n", "", "else", ":", "\n", "      ", "tokens_b", ".", "pop", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.preprocess._truncate_seq_pair": [[408, 423], ["len", "len", "len", "len", "tokens_a.pop", "tokens_b.pop"], "function", ["None"], ["", "", "", "def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_length", ")", ":", "\n", "  ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "    ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "      ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "      ", "tokens_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "      ", "tokens_b", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.proc_and_save_sup_data": [[425, 457], ["tensorflow.logging.info", "tensorflow.logging.info", "preprocess.tokenize_examples", "preprocess.convert_examples_to_features", "preprocess.dump_tfrecord", "processor.get_train_examples", "tensorflow.logging.info", "preprocess.get_data_by_size_lim", "preprocess.get_data_for_worker", "processor.get_labels", "processor.get_dev_examples", "len", "len"], "function", ["home.repos.pwc.inspect_result.google-research_uda.text.preprocess.tokenize_examples", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.convert_examples_to_features", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.dump_tfrecord", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.TextClassProcessor.get_train_examples", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.get_data_by_size_lim", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.get_data_for_worker", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DBPediaProcessor.get_labels", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.TextClassProcessor.get_dev_examples"], ["", "", "", "def", "proc_and_save_sup_data", "(", "\n", "processor", ",", "sub_set", ",", "raw_data_dir", ",", "sup_out_dir", ",", "\n", "tokenizer", ",", "max_seq_length", ",", "trunc_keep_right", ",", "\n", "worker_id", ",", "replicas", ",", "sup_size", ")", ":", "\n", "  ", "tf", ".", "logging", ".", "info", "(", "\"getting examples\"", ")", "\n", "if", "sub_set", "==", "\"train\"", ":", "\n", "    ", "examples", "=", "processor", ".", "get_train_examples", "(", "raw_data_dir", ")", "\n", "", "elif", "sub_set", "==", "\"dev\"", ":", "\n", "    ", "examples", "=", "processor", ".", "get_dev_examples", "(", "raw_data_dir", ")", "\n", "assert", "replicas", "==", "1", ",", "\"dev set can be processsed with just one worker\"", "\n", "assert", "sup_size", "==", "-", "1", ",", "\"should use the full dev set\"", "\n", "\n", "", "if", "sup_size", "!=", "-", "1", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"setting number of examples to {:d}\"", ".", "format", "(", "\n", "sup_size", ")", ")", "\n", "examples", "=", "get_data_by_size_lim", "(", "\n", "examples", ",", "processor", ",", "sup_size", ")", "\n", "", "if", "replicas", "!=", "1", ":", "\n", "    ", "if", "len", "(", "examples", ")", "<", "replicas", ":", "\n", "      ", "replicas", "=", "len", "(", "examples", ")", "\n", "if", "worker_id", ">=", "replicas", ":", "\n", "        ", "return", "\n", "", "", "examples", "=", "get_data_for_worker", "(", "\n", "examples", ",", "replicas", ",", "worker_id", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"processing data\"", ")", "\n", "examples", "=", "tokenize_examples", "(", "examples", ",", "tokenizer", ")", "\n", "\n", "features", "=", "convert_examples_to_features", "(", "\n", "examples", ",", "processor", ".", "get_labels", "(", ")", ",", "max_seq_length", ",", "tokenizer", ",", "\n", "trunc_keep_right", ",", "None", ",", "None", ")", "\n", "dump_tfrecord", "(", "features", ",", "sup_out_dir", ",", "worker_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.proc_and_save_unsup_data": [[459, 526], ["numpy.random.randint", "tensorflow.logging.info", "numpy.random.seed", "tensorflow.logging.info", "len", "tensorflow.logging.info", "copy.deepcopy", "augmentation.sent_level_augment.run_augment", "tensorflow.logging.info", "preprocess.tokenize_examples", "preprocess.convert_examples_to_features", "tensorflow.logging.info", "preprocess.tokenize_examples", "preprocess.convert_examples_to_features", "zip", "preprocess.dump_tfrecord", "processor.get_train_examples", "sub_set.startswith", "preprocess.get_data_for_worker", "len", "processor.get_labels", "preprocess.get_data_stats", "unsup_features.append", "processor.get_unsup_examples", "preprocess.PairedUnsupInputFeatures"], "function", ["home.repos.pwc.inspect_result.google-research_uda.augmentation.sent_level_augment.run_augment", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.tokenize_examples", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.convert_examples_to_features", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.tokenize_examples", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.convert_examples_to_features", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.dump_tfrecord", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.TextClassProcessor.get_train_examples", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.get_data_for_worker", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DBPediaProcessor.get_labels", "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.get_data_stats", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.AMAZON5Processor.get_unsup_examples"], ["", "def", "proc_and_save_unsup_data", "(", "\n", "processor", ",", "sub_set", ",", "\n", "raw_data_dir", ",", "data_stats_dir", ",", "unsup_out_dir", ",", "\n", "tokenizer", ",", "\n", "max_seq_length", ",", "trunc_keep_right", ",", "\n", "aug_ops", ",", "aug_copy_num", ",", "\n", "worker_id", ",", "replicas", ")", ":", "\n", "# print random seed just to double check that we use different random seeds", "\n", "# for different runs so that we generate different augmented examples for the same original example.", "\n", "  ", "random_seed", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "100000", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"random seed: {:d}\"", ".", "format", "(", "random_seed", ")", ")", "\n", "np", ".", "random", ".", "seed", "(", "random_seed", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"getting examples\"", ")", "\n", "\n", "if", "sub_set", "==", "\"train\"", ":", "\n", "    ", "ori_examples", "=", "processor", ".", "get_train_examples", "(", "raw_data_dir", ")", "\n", "", "elif", "sub_set", ".", "startswith", "(", "\"unsup\"", ")", ":", "\n", "    ", "ori_examples", "=", "processor", ".", "get_unsup_examples", "(", "raw_data_dir", ",", "sub_set", ")", "\n", "", "else", ":", "\n", "    ", "assert", "False", "\n", "# this is the size before spliting data for each worker", "\n", "", "data_total_size", "=", "len", "(", "ori_examples", ")", "\n", "if", "replicas", "!=", "-", "1", ":", "\n", "    ", "ori_examples", ",", "start", ",", "end", "=", "get_data_for_worker", "(", "\n", "ori_examples", ",", "replicas", ",", "worker_id", ")", "\n", "", "else", ":", "\n", "    ", "start", "=", "0", "\n", "end", "=", "len", "(", "ori_examples", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"getting augmented examples\"", ")", "\n", "aug_examples", "=", "copy", ".", "deepcopy", "(", "ori_examples", ")", "\n", "aug_examples", "=", "sent_level_augment", ".", "run_augment", "(", "\n", "aug_examples", ",", "aug_ops", ",", "sub_set", ",", "\n", "aug_copy_num", ",", "\n", "start", ",", "end", ",", "data_total_size", ")", "\n", "\n", "labels", "=", "processor", ".", "get_labels", "(", ")", "+", "[", "\"unsup\"", "]", "\n", "tf", ".", "logging", ".", "info", "(", "\"processing ori examples\"", ")", "\n", "ori_examples", "=", "tokenize_examples", "(", "ori_examples", ",", "tokenizer", ")", "\n", "ori_features", "=", "convert_examples_to_features", "(", "\n", "ori_examples", ",", "labels", ",", "max_seq_length", ",", "tokenizer", ",", "\n", "trunc_keep_right", ",", "None", ",", "None", ")", "\n", "\n", "if", "\"idf\"", "in", "aug_ops", ":", "\n", "    ", "data_stats", "=", "get_data_stats", "(", "\n", "data_stats_dir", ",", "sub_set", ",", "\n", "-", "1", ",", "replicas", ",", "ori_examples", ")", "\n", "", "else", ":", "\n", "    ", "data_stats", "=", "None", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"processing aug examples\"", ")", "\n", "aug_examples", "=", "tokenize_examples", "(", "aug_examples", ",", "tokenizer", ")", "\n", "aug_features", "=", "convert_examples_to_features", "(", "\n", "aug_examples", ",", "labels", ",", "max_seq_length", ",", "tokenizer", ",", "\n", "trunc_keep_right", ",", "data_stats", ",", "aug_ops", ")", "\n", "\n", "unsup_features", "=", "[", "]", "\n", "for", "ori_feat", ",", "aug_feat", "in", "zip", "(", "ori_features", ",", "aug_features", ")", ":", "\n", "    ", "unsup_features", ".", "append", "(", "PairedUnsupInputFeatures", "(", "\n", "ori_feat", ".", "input_ids", ",", "\n", "ori_feat", ".", "input_mask", ",", "\n", "ori_feat", ".", "input_type_ids", ",", "\n", "aug_feat", ".", "input_ids", ",", "\n", "aug_feat", ".", "input_mask", ",", "\n", "aug_feat", ".", "input_type_ids", ",", "\n", ")", ")", "\n", "", "dump_tfrecord", "(", "unsup_features", ",", "unsup_out_dir", ",", "worker_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.main": [[528, 570], ["utils.raw_data_utils.get_processor", "utils.tokenization.FullTokenizer", "ValueError", "tensorflow.logging.info", "preprocess.proc_and_save_sup_data", "os.path.join", "os.path.join", "tensorflow.logging.info", "preprocess.proc_and_save_unsup_data", "str"], "function", ["home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.get_processor", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.proc_and_save_sup_data", "home.repos.pwc.inspect_result.google-research_uda.text.preprocess.proc_and_save_unsup_data"], ["", "def", "main", "(", "_", ")", ":", "\n", "\n", "\n", "  ", "if", "FLAGS", ".", "max_seq_length", ">", "512", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"Cannot use sequence length {:d} because the BERT model \"", "\n", "\"was only trained up to sequence length {:d}\"", ".", "format", "(", "\n", "FLAGS", ".", "max_seq_length", ",", "512", ")", ")", "\n", "\n", "", "processor", "=", "raw_data_utils", ".", "get_processor", "(", "FLAGS", ".", "task_name", ")", "\n", "# Create tokenizer", "\n", "tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "FLAGS", ".", "vocab_file", ",", "do_lower_case", "=", "FLAGS", ".", "do_lower_case", ")", "\n", "\n", "if", "FLAGS", ".", "data_type", "==", "\"sup\"", ":", "\n", "    ", "sup_out_dir", "=", "FLAGS", ".", "output_base_dir", "\n", "tf", ".", "logging", ".", "info", "(", "\"Create sup. data: subset {} => {}\"", ".", "format", "(", "\n", "FLAGS", ".", "sub_set", ",", "sup_out_dir", ")", ")", "\n", "\n", "proc_and_save_sup_data", "(", "\n", "processor", ",", "FLAGS", ".", "sub_set", ",", "FLAGS", ".", "raw_data_dir", ",", "sup_out_dir", ",", "\n", "tokenizer", ",", "FLAGS", ".", "max_seq_length", ",", "FLAGS", ".", "trunc_keep_right", ",", "\n", "FLAGS", ".", "worker_id", ",", "FLAGS", ".", "replicas", ",", "FLAGS", ".", "sup_size", ",", "\n", ")", "\n", "", "elif", "FLAGS", ".", "data_type", "==", "\"unsup\"", ":", "\n", "    ", "assert", "FLAGS", ".", "aug_ops", "is", "not", "None", ",", "\"aug_ops is required to preprocess unsupervised data.\"", "\n", "unsup_out_dir", "=", "os", ".", "path", ".", "join", "(", "\n", "FLAGS", ".", "output_base_dir", ",", "\n", "FLAGS", ".", "aug_ops", ",", "\n", "str", "(", "FLAGS", ".", "aug_copy_num", ")", ")", "\n", "data_stats_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "raw_data_dir", ",", "\"data_stats\"", ")", "\n", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"Create unsup. data: subset {} => {}\"", ".", "format", "(", "\n", "FLAGS", ".", "sub_set", ",", "unsup_out_dir", ")", ")", "\n", "proc_and_save_unsup_data", "(", "\n", "processor", ",", "FLAGS", ".", "sub_set", ",", "\n", "FLAGS", ".", "raw_data_dir", ",", "data_stats_dir", ",", "unsup_out_dir", ",", "\n", "tokenizer", ",", "FLAGS", ".", "max_seq_length", ",", "FLAGS", ".", "trunc_keep_right", ",", "\n", "FLAGS", ".", "aug_ops", ",", "FLAGS", ".", "aug_copy_num", ",", "\n", "FLAGS", ".", "worker_id", ",", "FLAGS", ".", "replicas", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.extract_raw_text.dump_raw_examples": [[54, 72], ["tensorflow.logging.info", "os.path.join", "os.path.join", "tensorflow.logging.info", "tensorflow.gfile.Open", "tensorflow.gfile.Open", "text_ouf.write", "label_ouf.write", "text_ouf.write", "text_ouf.write"], "function", ["None"], ["def", "dump_raw_examples", "(", "examples", ",", "separate_doc_by_newline", ")", ":", "\n", "  ", "\"\"\"dump raw examples.\"\"\"", "\n", "tf", ".", "logging", ".", "info", "(", "\"dumpping raw examples\"", ")", "\n", "text_path", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_data_dir", ",", "\"text.txt\"", ")", "\n", "label_path", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_data_dir", ",", "\"label.txt\"", ")", "\n", "with", "tf", ".", "gfile", ".", "Open", "(", "text_path", ",", "\"w\"", ")", "as", "text_ouf", ":", "\n", "    ", "with", "tf", ".", "gfile", ".", "Open", "(", "label_path", ",", "\"w\"", ")", "as", "label_ouf", ":", "\n", "      ", "for", "example", "in", "examples", ":", "\n", "        ", "text_a", "=", "example", ".", "text_a", "\n", "text_b", "=", "example", ".", "text_b", "\n", "label", "=", "example", ".", "label", "\n", "text_ouf", ".", "write", "(", "text_a", "+", "\"\\n\"", ")", "\n", "if", "text_b", "is", "not", "None", ":", "\n", "          ", "text_ouf", ".", "write", "(", "text_b", "+", "\"\\n\"", ")", "\n", "", "if", "separate_doc_by_newline", ":", "\n", "          ", "text_ouf", ".", "write", "(", "\"\\n\"", ")", "\n", "", "label_ouf", ".", "write", "(", "label", "+", "\"\\n\"", ")", "\n", "", "", "", "tf", ".", "logging", ".", "info", "(", "\"finished dumpping raw examples\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.text.extract_raw_text.main": [[74, 90], ["utils.raw_data_utils.get_processor", "tensorflow.logging.info", "os.path.join", "tensorflow.logging.info", "tensorflow.logging.info", "extract_raw_text.dump_raw_examples", "tensorflow.gfile.Exists", "tensorflow.gfile.MakeDirs", "raw_data_utils.get_processor.get_train_examples", "FLAGS.sub_set.startswith", "raw_data_utils.get_processor.get_unsup_examples", "len"], "function", ["home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.get_processor", "home.repos.pwc.inspect_result.google-research_uda.text.extract_raw_text.dump_raw_examples", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.TextClassProcessor.get_train_examples", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.AMAZON5Processor.get_unsup_examples"], ["", "def", "main", "(", "argv", ")", ":", "\n", "  ", "processor", "=", "raw_data_utils", ".", "get_processor", "(", "FLAGS", ".", "task_name", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"loading examples\"", ")", "\n", "FLAGS", ".", "output_data_dir", "=", "os", ".", "path", ".", "join", "(", "\n", "FLAGS", ".", "output_data_dir", ",", "FLAGS", ".", "sub_set", ")", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "FLAGS", ".", "output_data_dir", ")", ":", "\n", "    ", "tf", ".", "gfile", ".", "MakeDirs", "(", "FLAGS", ".", "output_data_dir", ")", "\n", "", "if", "FLAGS", ".", "sub_set", "==", "\"train\"", ":", "\n", "    ", "examples", "=", "processor", ".", "get_train_examples", "(", "FLAGS", ".", "raw_data_dir", ")", "\n", "", "elif", "FLAGS", ".", "sub_set", ".", "startswith", "(", "\"unsup\"", ")", ":", "\n", "    ", "examples", "=", "processor", ".", "get_unsup_examples", "(", "FLAGS", ".", "raw_data_dir", ",", "FLAGS", ".", "sub_set", ")", "\n", "", "else", ":", "\n", "    ", "assert", "False", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"finished loading examples\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"examples num: {:d}\"", ".", "format", "(", "len", "(", "examples", ")", ")", ")", "\n", "dump_raw_examples", "(", "examples", ",", "FLAGS", ".", "separate_doc_by_newline", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.proc_data_utils._decode_record": [[36, 49], ["tensorflow.parse_single_example", "list", "tf.parse_single_example.keys", "tensorflow.to_int32"], "function", ["None"], ["def", "_decode_record", "(", "record", ",", "name_to_features", ")", ":", "\n", "  ", "\"\"\"Decodes a record to a TensorFlow example.\"\"\"", "\n", "example", "=", "tf", ".", "parse_single_example", "(", "record", ",", "name_to_features", ")", "\n", "\n", "# tf.Example only supports tf.int64, but the TPU only supports tf.int32.", "\n", "# So cast all int64 to int32.", "\n", "for", "name", "in", "list", "(", "example", ".", "keys", "(", ")", ")", ":", "\n", "    ", "t", "=", "example", "[", "name", "]", "\n", "if", "t", ".", "dtype", "==", "tf", ".", "int64", ":", "\n", "      ", "t", "=", "tf", ".", "to_int32", "(", "t", ")", "\n", "", "example", "[", "name", "]", "=", "t", "\n", "\n", "", "return", "example", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.proc_data_utils.get_sup_feature_specs": [[51, 63], ["collections.OrderedDict", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature"], "function", ["None"], ["", "def", "get_sup_feature_specs", "(", ")", ":", "\n", "  ", "\"\"\"Get supervised feature.\"\"\"", "\n", "feature_specs", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "feature_specs", "[", "\"input_ids\"", "]", "=", "tf", ".", "FixedLenFeature", "(", "\n", "[", "FLAGS", ".", "max_seq_length", "]", ",", "tf", ".", "int64", ")", "\n", "feature_specs", "[", "\"input_mask\"", "]", "=", "tf", ".", "FixedLenFeature", "(", "\n", "[", "FLAGS", ".", "max_seq_length", "]", ",", "tf", ".", "int64", ")", "\n", "feature_specs", "[", "\"input_type_ids\"", "]", "=", "tf", ".", "FixedLenFeature", "(", "\n", "[", "FLAGS", ".", "max_seq_length", "]", ",", "tf", ".", "int64", ")", "\n", "feature_specs", "[", "\"label_ids\"", "]", "=", "tf", ".", "FixedLenFeature", "(", "\n", "[", "1", "]", ",", "tf", ".", "int64", ")", "\n", "return", "feature_specs", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.proc_data_utils.get_unsup_feature_specs": [[65, 81], ["collections.OrderedDict", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature"], "function", ["None"], ["", "def", "get_unsup_feature_specs", "(", ")", ":", "\n", "  ", "\"\"\"Get unsupervised feature.\"\"\"", "\n", "feature_specs", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "feature_specs", "[", "\"ori_input_ids\"", "]", "=", "tf", ".", "FixedLenFeature", "(", "\n", "[", "FLAGS", ".", "max_seq_length", "]", ",", "tf", ".", "int64", ")", "\n", "feature_specs", "[", "\"ori_input_mask\"", "]", "=", "tf", ".", "FixedLenFeature", "(", "\n", "[", "FLAGS", ".", "max_seq_length", "]", ",", "tf", ".", "int64", ")", "\n", "feature_specs", "[", "\"ori_input_type_ids\"", "]", "=", "tf", ".", "FixedLenFeature", "(", "\n", "[", "FLAGS", ".", "max_seq_length", "]", ",", "tf", ".", "int64", ")", "\n", "feature_specs", "[", "\"aug_input_ids\"", "]", "=", "tf", ".", "FixedLenFeature", "(", "\n", "[", "FLAGS", ".", "max_seq_length", "]", ",", "tf", ".", "int64", ")", "\n", "feature_specs", "[", "\"aug_input_mask\"", "]", "=", "tf", ".", "FixedLenFeature", "(", "\n", "[", "FLAGS", ".", "max_seq_length", "]", ",", "tf", ".", "int64", ")", "\n", "feature_specs", "[", "\"aug_input_type_ids\"", "]", "=", "tf", ".", "FixedLenFeature", "(", "\n", "[", "FLAGS", ".", "max_seq_length", "]", ",", "tf", ".", "int64", ")", "\n", "return", "feature_specs", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.proc_data_utils.get_aug_files": [[83, 112], ["aug_ops.split", "numpy.random.shuffle", "tensorflow.gfile.ListDirectory", "os.path.join", "int", "os.path.join", "tensorflow.contrib.slim.parallel_reader.get_data_files", "len", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "len", "copy_dir.strip", "len"], "function", ["None"], ["", "def", "get_aug_files", "(", "data_base_path", ",", "aug_ops", ",", "aug_copy", ")", ":", "\n", "  ", "\"\"\"get aug files.\"\"\"", "\n", "\n", "sub_policy_list", "=", "aug_ops", ".", "split", "(", "\"+\"", ")", "\n", "total_data_files", "=", "[", "]", "\n", "for", "sub_policy", "in", "sub_policy_list", ":", "\n", "    ", "sub_policy_data_files", "=", "[", "]", "\n", "exist_copy_num", "=", "{", "}", "\n", "for", "copy_dir", "in", "tf", ".", "gfile", ".", "ListDirectory", "(", "os", ".", "path", ".", "join", "(", "\n", "data_base_path", ",", "sub_policy", ")", ")", ":", "\n", "      ", "copy_num", "=", "int", "(", "copy_dir", ".", "strip", "(", "\"/\"", ")", ")", "\n", "if", "copy_num", ">=", "aug_copy", ":", "\n", "        ", "continue", "\n", "", "exist_copy_num", "[", "copy_num", "]", "=", "1", "\n", "data_record_path", "=", "os", ".", "path", ".", "join", "(", "\n", "data_base_path", ",", "sub_policy", ",", "copy_dir", ",", "\"tf_examples.tfrecord*\"", ")", "\n", "data_files", "=", "tf", ".", "contrib", ".", "slim", ".", "parallel_reader", ".", "get_data_files", "(", "\n", "data_record_path", ")", "\n", "sub_policy_data_files", "+=", "data_files", "\n", "", "if", "len", "(", "exist_copy_num", ")", "<", "aug_copy", "*", "0.9", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"not enough copies for aug op: {:s}\"", ".", "format", "(", "aug_ops", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"found files: {:s}\"", ".", "format", "(", "\n", "\" \"", ".", "join", "(", "sub_policy_data_files", ")", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"found copy: {:d} / desired copy: {:d}\"", ".", "format", "(", "\n", "len", "(", "exist_copy_num", ")", ",", "aug_copy", ")", ")", "\n", "", "assert", "len", "(", "exist_copy_num", ")", ">", "aug_copy", "*", "0.9", "\n", "total_data_files", "+=", "sub_policy_data_files", "\n", "", "np", ".", "random", ".", "shuffle", "(", "total_data_files", ")", "\n", "return", "total_data_files", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.proc_data_utils.get_training_dataset": [[114, 140], ["tensorflow.data.Dataset.from_tensor_slices", "d.apply.apply", "min", "d.apply.apply", "d.apply.shuffle", "d.apply.apply", "tensorflow.constant", "tensorflow.contrib.data.shuffle_and_repeat", "len", "tensorflow.contrib.data.parallel_interleave", "tensorflow.contrib.data.map_and_batch", "len", "proc_data_utils._decode_record"], "function", ["home.repos.pwc.inspect_result.google-research_uda.utils.proc_data_utils._decode_record"], ["", "def", "get_training_dataset", "(", "total_data_files", ",", "batch_size", ",", "num_threads", ",", "is_training", ",", "\n", "shuffle_buffer_size", ",", "feature_specs", ")", ":", "\n", "  ", "\"\"\"build dataset from files.\"\"\"", "\n", "d", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "tf", ".", "constant", "(", "total_data_files", ")", ")", "\n", "d", "=", "d", ".", "apply", "(", "\n", "tf", ".", "contrib", ".", "data", ".", "shuffle_and_repeat", "(", "\n", "buffer_size", "=", "len", "(", "total_data_files", ")", ")", ")", "\n", "\n", "# `cycle_length` is the number of parallel files that get read.", "\n", "cycle_length", "=", "min", "(", "num_threads", ",", "len", "(", "total_data_files", ")", ")", "\n", "\n", "# `sloppy` mode means that the interleaving is not exact. This adds", "\n", "# even more randomness to the training pipeline.", "\n", "d", "=", "d", ".", "apply", "(", "\n", "tf", ".", "contrib", ".", "data", ".", "parallel_interleave", "(", "\n", "tf", ".", "data", ".", "TFRecordDataset", ",", "\n", "sloppy", "=", "is_training", ",", "\n", "cycle_length", "=", "cycle_length", ")", ")", "\n", "d", "=", "d", ".", "shuffle", "(", "buffer_size", "=", "shuffle_buffer_size", ")", "\n", "d", "=", "d", ".", "apply", "(", "\n", "tf", ".", "contrib", ".", "data", ".", "map_and_batch", "(", "\n", "lambda", "record", ":", "_decode_record", "(", "record", ",", "feature_specs", ")", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "num_parallel_batches", "=", "num_threads", ",", "\n", "drop_remainder", "=", "is_training", ")", ")", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.proc_data_utils.get_evaluation_dataset": [[142, 153], ["tensorflow.data.TFRecordDataset", "d.apply.apply", "tensorflow.contrib.data.map_and_batch", "proc_data_utils._decode_record"], "function", ["home.repos.pwc.inspect_result.google-research_uda.utils.proc_data_utils._decode_record"], ["", "def", "get_evaluation_dataset", "(", "total_data_files", ",", "batch_size", ",", "feature_specs", ")", ":", "\n", "  ", "\"\"\"build non-repeat dataset from files.\"\"\"", "\n", "d", "=", "tf", ".", "data", ".", "TFRecordDataset", "(", "total_data_files", ")", "\n", "d", "=", "d", ".", "apply", "(", "\n", "tf", ".", "contrib", ".", "data", ".", "map_and_batch", "(", "\n", "lambda", "record", ":", "_decode_record", "(", "record", ",", "feature_specs", ")", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "num_parallel_batches", "=", "None", ",", "\n", "drop_remainder", "=", "True", ")", ")", "\n", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.proc_data_utils.evaluation_input_fn_builder": [[155, 178], ["tensorflow.contrib.slim.parallel_reader.get_data_files", "tensorflow.logging.info", "os.path.join", "get_evaluation_dataset.prefetch", "proc_data_utils.get_evaluation_dataset", "proc_data_utils.get_sup_feature_specs"], "function", ["home.repos.pwc.inspect_result.google-research_uda.utils.proc_data_utils.get_evaluation_dataset", "home.repos.pwc.inspect_result.google-research_uda.utils.proc_data_utils.get_sup_feature_specs"], ["", "def", "evaluation_input_fn_builder", "(", "data_base_path", ",", "task", ",", "prefetch_size", "=", "1000", ")", ":", "\n", "\n", "  ", "total_data_files", "=", "tf", ".", "contrib", ".", "slim", ".", "parallel_reader", ".", "get_data_files", "(", "\n", "os", ".", "path", ".", "join", "(", "data_base_path", ",", "\"tf_examples.tfrecord*\"", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"loading eval {} data from these files: {:s}\"", ".", "format", "(", "\n", "task", ",", "\" \"", ".", "join", "(", "total_data_files", ")", ")", ")", "\n", "\n", "def", "input_fn", "(", "params", ")", ":", "\n", "    ", "batch_size", "=", "params", "[", "\"batch_size\"", "]", "\n", "\n", "if", "task", "==", "\"clas\"", ":", "\n", "      ", "dataset", "=", "get_evaluation_dataset", "(", "\n", "total_data_files", ",", "\n", "batch_size", ",", "\n", "get_sup_feature_specs", "(", ")", ")", "\n", "", "else", ":", "\n", "      ", "assert", "False", "\n", "\n", "", "dataset", "=", "dataset", ".", "prefetch", "(", "prefetch_size", ")", "\n", "\n", "return", "dataset", "\n", "\n", "", "return", "input_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.proc_data_utils.training_input_fn_builder": [[180, 266], ["tensorflow.contrib.slim.parallel_reader.get_data_files", "os.path.join", "proc_data_utils.get_aug_files", "tensorflow.logging.info", "tensorflow.logging.info", "d.map.prefetch", "proc_data_utils.get_training_dataset", "tensorflow.logging.info", "dataset_list.append", "len", "tensorflow.data.Dataset.zip", "d.map.map", "proc_data_utils.get_sup_feature_specs", "proc_data_utils.get_training_dataset", "dataset_list.append", "tensorflow.logging.info", "tuple", "proc_data_utils.get_unsup_feature_specs"], "function", ["home.repos.pwc.inspect_result.google-research_uda.utils.proc_data_utils.get_aug_files", "home.repos.pwc.inspect_result.google-research_uda.utils.proc_data_utils.get_training_dataset", "home.repos.pwc.inspect_result.google-research_uda.utils.proc_data_utils.get_sup_feature_specs", "home.repos.pwc.inspect_result.google-research_uda.utils.proc_data_utils.get_training_dataset", "home.repos.pwc.inspect_result.google-research_uda.utils.proc_data_utils.get_unsup_feature_specs"], ["", "def", "training_input_fn_builder", "(", "\n", "sup_data_base_path", "=", "None", ",", "\n", "unsup_data_base_path", "=", "None", ",", "\n", "aug_ops", "=", "None", ",", "\n", "aug_copy", "=", "None", ",", "\n", "unsup_ratio", "=", "None", ",", "\n", "num_threads", "=", "8", ",", "\n", "shuffle_buffer_size", "=", "100000", ",", "\n", "prefetch_size", "=", "1000", ")", ":", "\n", "\n", "  ", "sup_total_data_files", "=", "tf", ".", "contrib", ".", "slim", ".", "parallel_reader", ".", "get_data_files", "(", "\n", "os", ".", "path", ".", "join", "(", "sup_data_base_path", ",", "\"tf_examples.tfrecord*\"", ")", ")", "\n", "\n", "if", "unsup_ratio", "is", "not", "None", "and", "unsup_ratio", ">", "0", ":", "\n", "    ", "assert", "aug_ops", "is", "not", "None", "and", "aug_copy", "is", "not", "None", ",", "\"Require aug_ops, aug_copy to load augmented unsup data.\"", "\n", "assert", "unsup_data_base_path", "is", "not", "None", "and", "unsup_data_base_path", "!=", "\"\"", ",", "\"Require unsup_data_base_path to load unsup data. Get {}.\"", ".", "format", "(", "\n", "unsup_data_base_path", ")", "\n", "\n", "unsup_total_data_files", "=", "get_aug_files", "(", "\n", "unsup_data_base_path", ",", "aug_ops", ",", "aug_copy", ")", "\n", "\n", "", "is_training", "=", "True", "\n", "\n", "def", "input_fn", "(", "params", ")", ":", "\n", "    ", "\"\"\"The `input_fn` for TPUEstimator which generates the feature dataset.\"\"\"", "\n", "sup_batch_size", "=", "params", "[", "\"batch_size\"", "]", "\n", "total_batch_size", "=", "0", "\n", "tf", ".", "logging", ".", "info", "(", "\"sup batch size: %d\"", ",", "(", "sup_batch_size", ")", ")", "\n", "\n", "dataset_list", "=", "[", "]", "\n", "\n", "# For training, we want a lot of parallel reading and shuffling.", "\n", "# For eval, we want no shuffling and parallel reading doesn't matter.", "\n", "if", "sup_data_base_path", "is", "not", "None", ":", "\n", "      ", "sup_dst", "=", "get_training_dataset", "(", "\n", "sup_total_data_files", ",", "\n", "sup_batch_size", ",", "\n", "num_threads", ",", "\n", "is_training", ",", "\n", "shuffle_buffer_size", ",", "\n", "get_sup_feature_specs", "(", ")", ")", "\n", "total_batch_size", "+=", "sup_batch_size", "\n", "tf", ".", "logging", ".", "info", "(", "\"sup batch size: %d\"", ",", "(", "sup_batch_size", ")", ")", "\n", "dataset_list", ".", "append", "(", "sup_dst", ")", "\n", "\n", "## only consider unsupervised data when supervised data is considered", "\n", "if", "unsup_data_base_path", "is", "not", "None", "and", "FLAGS", ".", "unsup_ratio", ">", "0", ":", "\n", "        ", "unsup_dst", "=", "get_training_dataset", "(", "\n", "unsup_total_data_files", ",", "\n", "sup_batch_size", "*", "unsup_ratio", ",", "\n", "num_threads", ",", "\n", "is_training", ",", "\n", "shuffle_buffer_size", ",", "\n", "get_unsup_feature_specs", "(", ")", ")", "\n", "total_batch_size", "+=", "sup_batch_size", "*", "unsup_ratio", "*", "2", "\n", "dataset_list", ".", "append", "(", "unsup_dst", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"unsup batch size: %d\"", ",", "(", "sup_batch_size", "*", "unsup_ratio", ")", ")", "\n", "\n", "", "", "tf", ".", "logging", ".", "info", "(", "\"total sample in a batch: %d\"", ",", "(", "total_batch_size", ")", ")", "\n", "\n", "def", "flatten_input", "(", "*", "features", ")", ":", "\n", "      ", "\"\"\"Merging multiple feature dicts resulted from zipped datasets.\"\"\"", "\n", "result", "=", "{", "}", "\n", "for", "feature", "in", "features", ":", "\n", "        ", "for", "key", "in", "feature", ":", "\n", "          ", "assert", "key", "not", "in", "result", "\n", "result", "[", "key", "]", "=", "feature", "[", "key", "]", "\n", "\n", "", "", "return", "result", "\n", "\n", "", "if", "len", "(", "dataset_list", ")", ">", "1", ":", "\n", "      ", "d", "=", "tf", ".", "data", ".", "Dataset", ".", "zip", "(", "tuple", "(", "dataset_list", ")", ")", "\n", "d", "=", "d", ".", "map", "(", "flatten_input", ")", "\n", "", "else", ":", "\n", "      ", "d", "=", "dataset_list", "[", "0", "]", "\n", "\n", "# Prefetching creates a buffer to make sure there is always data to", "\n", "# read in the event of network latency variance.", "\n", "", "d", "=", "d", ".", "prefetch", "(", "prefetch_size", ")", "\n", "\n", "# TPUEstimator supports returning a dataset instead of just features.", "\n", "# It will call `make_one_shot_iterator()` and such.", "\n", "return", "d", "\n", "", "return", "input_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.imdb_format.dump_raw_data": [[32, 37], ["open", "csv.writer", "csv.writer.writerow"], "function", ["None"], ["def", "dump_raw_data", "(", "contents", ",", "file_path", ")", ":", "\n", "  ", "with", "open", "(", "file_path", ",", "\"w\"", ")", "as", "ouf", ":", "\n", "    ", "writer", "=", "csv", ".", "writer", "(", "ouf", ",", "delimiter", "=", "\"\\t\"", ",", "quotechar", "=", "\"\\\"\"", ")", "\n", "for", "line", "in", "contents", ":", "\n", "      ", "writer", ".", "writerow", "(", "line", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.imdb_format.clean_web_text": [[38, 61], ["st.replace.replace", "st.replace.replace", "st.replace.replace", "st.replace.replace", "st.replace.replace", "st.replace.find", "st.replace.find", "print", "print", "print", "len"], "function", ["None"], ["", "", "", "def", "clean_web_text", "(", "st", ")", ":", "\n", "  ", "\"\"\"clean text.\"\"\"", "\n", "st", "=", "st", ".", "replace", "(", "\"<br />\"", ",", "\" \"", ")", "\n", "st", "=", "st", ".", "replace", "(", "\"&quot;\"", ",", "\"\\\"\"", ")", "\n", "st", "=", "st", ".", "replace", "(", "\"<p>\"", ",", "\" \"", ")", "\n", "if", "\"<a href=\"", "in", "st", ":", "\n", "    ", "while", "\"<a href=\"", "in", "st", ":", "\n", "      ", "start_pos", "=", "st", ".", "find", "(", "\"<a href=\"", ")", "\n", "end_pos", "=", "st", ".", "find", "(", "\">\"", ",", "start_pos", ")", "\n", "if", "end_pos", "!=", "-", "1", ":", "\n", "        ", "st", "=", "st", "[", ":", "start_pos", "]", "+", "st", "[", "end_pos", "+", "1", ":", "]", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"incomplete href\"", ")", "\n", "print", "(", "\"before\"", ",", "st", ")", "\n", "st", "=", "st", "[", ":", "start_pos", "]", "+", "st", "[", "start_pos", "+", "len", "(", "\"<a href=\"", ")", "]", "\n", "print", "(", "\"after\"", ",", "st", ")", "\n", "\n", "", "", "st", "=", "st", ".", "replace", "(", "\"</a>\"", ",", "\"\"", ")", "\n", "", "st", "=", "st", ".", "replace", "(", "\"\\\\n\"", ",", "\" \"", ")", "\n", "# st = st.replace(\"\\\\\", \" \")", "\n", "# while \"  \" in st:", "\n", "#   st = st.replace(\"  \", \" \")", "\n", "return", "st", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.imdb_format.load_data_by_id": [[63, 77], ["open", "inf.readlines", "example_id.strip.strip", "os.path.join", "example_id.strip.split", "open", "inf.readlines", "imdb_format.clean_web_text", "len", "st_list[].strip", "len"], "function", ["home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.clean_web_text"], ["", "def", "load_data_by_id", "(", "sub_set", ",", "id_path", ")", ":", "\n", "  ", "with", "open", "(", "id_path", ")", "as", "inf", ":", "\n", "    ", "id_list", "=", "inf", ".", "readlines", "(", ")", "\n", "", "contents", "=", "[", "]", "\n", "for", "example_id", "in", "id_list", ":", "\n", "    ", "example_id", "=", "example_id", ".", "strip", "(", ")", "\n", "label", "=", "example_id", ".", "split", "(", "\"_\"", ")", "[", "0", "]", "\n", "file_path", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "raw_data_dir", ",", "sub_set", ",", "label", ",", "example_id", "[", "len", "(", "label", ")", "+", "1", ":", "]", ")", "\n", "with", "open", "(", "file_path", ")", "as", "inf", ":", "\n", "      ", "st_list", "=", "inf", ".", "readlines", "(", ")", "\n", "assert", "len", "(", "st_list", ")", "==", "1", "\n", "st", "=", "clean_web_text", "(", "st_list", "[", "0", "]", ".", "strip", "(", ")", ")", "\n", "contents", "+=", "[", "(", "st", ",", "label", ",", "example_id", ")", "]", "\n", "", "", "return", "contents", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.imdb_format.load_all_data": [[79, 94], ["os.path.join", "os.listdir", "os.path.exists", "os.path.join", "open", "inf.readlines", "imdb_format.clean_web_text", "len", "st_list[].strip"], "function", ["home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.clean_web_text"], ["", "def", "load_all_data", "(", "sub_set", ")", ":", "\n", "  ", "contents", "=", "[", "]", "\n", "for", "label", "in", "[", "\"pos\"", ",", "\"neg\"", ",", "\"unsup\"", "]", ":", "\n", "    ", "data_path", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "raw_data_dir", ",", "sub_set", ",", "label", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "data_path", ")", ":", "\n", "      ", "continue", "\n", "", "for", "filename", "in", "os", ".", "listdir", "(", "data_path", ")", ":", "\n", "      ", "file_path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "filename", ")", "\n", "with", "open", "(", "file_path", ")", "as", "inf", ":", "\n", "        ", "st_list", "=", "inf", ".", "readlines", "(", ")", "\n", "assert", "len", "(", "st_list", ")", "==", "1", "\n", "st", "=", "clean_web_text", "(", "st_list", "[", "0", "]", ".", "strip", "(", ")", ")", "\n", "example_id", "=", "\"{}_{}\"", ".", "format", "(", "label", ",", "filename", ")", "\n", "contents", "+=", "[", "(", "st", ",", "label", ",", "example_id", ")", "]", "\n", "", "", "", "return", "contents", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.imdb_format.main": [[96, 110], ["imdb_format.load_data_by_id", "os.mkdir", "imdb_format.dump_raw_data", "imdb_format.load_all_data", "imdb_format.dump_raw_data", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.google-research_uda.utils.imdb_format.load_data_by_id", "home.repos.pwc.inspect_result.google-research_uda.utils.imdb_format.dump_raw_data", "home.repos.pwc.inspect_result.google-research_uda.utils.imdb_format.load_all_data", "home.repos.pwc.inspect_result.google-research_uda.utils.imdb_format.dump_raw_data"], ["", "def", "main", "(", "_", ")", ":", "\n", "# load train", "\n", "  ", "header", "=", "[", "\"content\"", ",", "\"label\"", ",", "\"id\"", "]", "\n", "contents", "=", "load_data_by_id", "(", "\"train\"", ",", "FLAGS", ".", "train_id_path", ")", "\n", "os", ".", "mkdir", "(", "FLAGS", ".", "output_dir", ")", "\n", "dump_raw_data", "(", "\n", "[", "header", "]", "+", "contents", ",", "\n", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"train.csv\"", ")", ",", "\n", ")", "\n", "# load test", "\n", "contents", "=", "load_all_data", "(", "\"test\"", ")", "\n", "dump_raw_data", "(", "\n", "[", "header", "]", "+", "contents", ",", "\n", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"test.csv\"", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tpu_utils.construct_scalar_host_call": [[22, 50], ["list", "tensorflow.reshape", "metric_dict.keys", "tensorflow.train.get_or_create_global_step", "tensorflow.reshape", "tensorflow.contrib.summary.create_file_writer().as_default", "tensorflow.contrib.summary.always_record_summaries", "enumerate", "tensorflow.contrib.summary.all_summary_ops", "tensorflow.contrib.summary.create_file_writer", "reduce_fn", "tensorflow.contrib.summary.record_summaries_every_n_global_steps", "tensorflow.contrib.summary.scalar"], "function", ["None"], ["def", "construct_scalar_host_call", "(", "\n", "metric_dict", ",", "\n", "model_dir", ",", "\n", "prefix", "=", "\"\"", ",", "\n", "reduce_fn", "=", "None", ")", ":", "\n", "\n", "  ", "metric_names", "=", "list", "(", "metric_dict", ".", "keys", "(", ")", ")", "\n", "\n", "def", "host_call_fn", "(", "global_step", ",", "*", "args", ")", ":", "\n", "    ", "step", "=", "global_step", "[", "0", "]", "\n", "with", "tf", ".", "contrib", ".", "summary", ".", "create_file_writer", "(", "\n", "logdir", "=", "model_dir", ",", "filename_suffix", "=", "\".host_call\"", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "contrib", ".", "summary", ".", "always_record_summaries", "(", ")", ":", "\n", "        ", "for", "i", ",", "name", "in", "enumerate", "(", "metric_names", ")", ":", "\n", "          ", "if", "reduce_fn", "is", "None", ":", "\n", "            ", "scalar", "=", "args", "[", "i", "]", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "scalar", "=", "reduce_fn", "(", "args", "[", "i", "]", ")", "\n", "", "with", "tf", ".", "contrib", ".", "summary", ".", "record_summaries_every_n_global_steps", "(", "\n", "1", ",", "step", ")", ":", "\n", "            ", "tf", ".", "contrib", ".", "summary", ".", "scalar", "(", "prefix", "+", "name", ",", "scalar", ",", "step", "=", "step", ")", "\n", "\n", "", "", "return", "tf", ".", "contrib", ".", "summary", ".", "all_summary_ops", "(", ")", "\n", "\n", "", "", "", "global_step_tensor", "=", "tf", ".", "reshape", "(", "tf", ".", "train", ".", "get_or_create_global_step", "(", ")", ",", "[", "1", "]", ")", "\n", "other_tensors", "=", "[", "tf", ".", "reshape", "(", "metric_dict", "[", "key", "]", ",", "[", "-", "1", "]", ")", "for", "key", "in", "metric_names", "]", "\n", "\n", "return", "host_call_fn", ",", "[", "global_step_tensor", "]", "+", "other_tensors", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.InputExample.__init__": [[35, 51], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "guid", ",", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "None", ")", ":", "\n", "    ", "\"\"\"Constructs a InputExample.\n\n    Args:\n      guid: Unique id for the example.\n      text_a: string. The untokenized text of the first sequence. For single\n        sequence tasks, only this sequence must be specified.\n      text_b: (Optional) string. The untokenized text of the second sequence.\n        Only must be specified for sequence pair tasks.\n      label: (Optional) string. The label of the example. This should be\n        specified for train and dev examples, but not for test examples.\n    \"\"\"", "\n", "self", ".", "guid", "=", "guid", "\n", "self", ".", "text_a", "=", "text_a", "\n", "self", ".", "text_b", "=", "text_b", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DataProcessor.get_train_examples": [[56, 59], ["NotImplementedError"], "methods", ["None"], ["def", "get_train_examples", "(", "self", ",", "raw_data_dir", ")", ":", "\n", "    ", "\"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DataProcessor.get_dev_examples": [[60, 63], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_dev_examples", "(", "self", ",", "raw_data_dir", ")", ":", "\n", "    ", "\"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DataProcessor.get_labels": [[64, 67], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "    ", "\"\"\"Gets the list of labels for this data set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DataProcessor.get_train_size": [[68, 70], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_train_size", "(", "self", ")", ":", "\n", "    ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DataProcessor._read_tsv": [[71, 80], ["tensorflow.gfile.Open", "csv.reader", "lines.append"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "_read_tsv", "(", "cls", ",", "input_file", ",", "quotechar", "=", "None", ",", "delimiter", "=", "\"\\t\"", ")", ":", "\n", "    ", "\"\"\"Reads a tab separated value file.\"\"\"", "\n", "with", "tf", ".", "gfile", ".", "Open", "(", "input_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "      ", "reader", "=", "csv", ".", "reader", "(", "f", ",", "delimiter", "=", "delimiter", ",", "quotechar", "=", "quotechar", ")", "\n", "lines", "=", "[", "]", "\n", "for", "line", "in", "reader", ":", "\n", "        ", "lines", ".", "append", "(", "line", ")", "\n", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.IMDbProcessor.get_train_examples": [[113, 118], ["raw_data_utils.IMDbProcessor._create_examples", "raw_data_utils.IMDbProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.TextClassProcessor._create_examples", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DataProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "raw_data_dir", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "raw_data_dir", ",", "\"train.csv\"", ")", ",", "\n", "quotechar", "=", "'\"'", ")", ",", "\"train\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.IMDbProcessor.get_dev_examples": [[119, 124], ["raw_data_utils.IMDbProcessor._create_examples", "raw_data_utils.IMDbProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.TextClassProcessor._create_examples", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DataProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "raw_data_dir", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "raw_data_dir", ",", "\"test.csv\"", ")", ",", "\n", "quotechar", "=", "'\"'", ")", ",", "\"test\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.IMDbProcessor.get_unsup_examples": [[125, 135], ["raw_data_utils.IMDbProcessor._create_examples", "raw_data_utils.IMDbProcessor._read_tsv", "raw_data_utils.IMDbProcessor._create_examples", "os.path.join", "raw_data_utils.IMDbProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.TextClassProcessor._create_examples", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DataProcessor._read_tsv", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.TextClassProcessor._create_examples", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DataProcessor._read_tsv"], ["", "def", "get_unsup_examples", "(", "self", ",", "raw_data_dir", ",", "unsup_set", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "if", "unsup_set", "==", "\"unsup_ext\"", ":", "\n", "      ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "raw_data_dir", ",", "\"unsup_ext.csv\"", ")", ",", "\n", "quotechar", "=", "'\"'", ")", ",", "\"unsup_ext\"", ",", "skip_unsup", "=", "False", ")", "\n", "", "elif", "unsup_set", "==", "\"unsup_in\"", ":", "\n", "      ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "raw_data_dir", ",", "\"train.csv\"", ")", ",", "\n", "quotechar", "=", "'\"'", ")", ",", "\"unsup_in\"", ",", "skip_unsup", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.IMDbProcessor.get_labels": [[136, 139], ["None"], "methods", ["None"], ["", "", "def", "get_labels", "(", "self", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "\"pos\"", ",", "\"neg\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.IMDbProcessor._create_examples": [[140, 158], ["enumerate", "raw_data_utils.clean_web_text", "examples.append", "raw_data_utils.InputExample", "len"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.clean_web_text"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ",", "skip_unsup", "=", "True", ")", ":", "\n", "    ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "      ", "if", "i", "==", "0", ":", "\n", "        ", "continue", "\n", "", "if", "skip_unsup", "and", "line", "[", "1", "]", "==", "\"unsup\"", ":", "\n", "        ", "continue", "\n", "", "if", "line", "[", "1", "]", "==", "\"unsup\"", "and", "len", "(", "line", "[", "0", "]", ")", "<", "500", ":", "\n", "# tf.logging.info(\"skipping short samples:{:s}\".format(line[0]))", "\n", "        ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "line", "[", "2", "]", ")", "\n", "text_a", "=", "line", "[", "0", "]", "\n", "label", "=", "line", "[", "1", "]", "\n", "text_a", "=", "clean_web_text", "(", "text_a", ")", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.IMDbProcessor.get_train_size": [[159, 161], ["None"], "methods", ["None"], ["", "def", "get_train_size", "(", "self", ")", ":", "\n", "    ", "return", "25000", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.IMDbProcessor.get_dev_size": [[162, 164], ["None"], "methods", ["None"], ["", "def", "get_dev_size", "(", "self", ")", ":", "\n", "    ", "return", "25000", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.TextClassProcessor.get_train_examples": [[168, 176], ["raw_data_utils.TextClassProcessor._create_examples", "raw_data_utils.TextClassProcessor._read_tsv", "len", "raw_data_utils.TextClassProcessor.get_train_size", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.TextClassProcessor._create_examples", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DataProcessor._read_tsv", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DBPediaProcessor.get_train_size"], ["  ", "def", "get_train_examples", "(", "self", ",", "raw_data_dir", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "examples", "=", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "raw_data_dir", ",", "\"train.csv\"", ")", ",", "\n", "quotechar", "=", "\"\\\"\"", ",", "\n", "delimiter", "=", "\",\"", ")", ",", "\"train\"", ")", "\n", "assert", "len", "(", "examples", ")", "==", "self", ".", "get_train_size", "(", ")", "\n", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.TextClassProcessor.get_dev_examples": [[177, 183], ["raw_data_utils.TextClassProcessor._create_examples", "raw_data_utils.TextClassProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.TextClassProcessor._create_examples", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DataProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "raw_data_dir", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "raw_data_dir", ",", "\"test.csv\"", ")", ",", "\n", "quotechar", "=", "\"\\\"\"", ",", "\n", "delimiter", "=", "\",\"", ")", ",", "\"test\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.TextClassProcessor.get_unsup_examples": [[184, 200], ["raw_data_utils.TextClassProcessor._create_examples", "raw_data_utils.TextClassProcessor._create_examples", "raw_data_utils.TextClassProcessor._read_tsv", "raw_data_utils.TextClassProcessor._read_tsv", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.TextClassProcessor._create_examples", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.TextClassProcessor._create_examples", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DataProcessor._read_tsv", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DataProcessor._read_tsv"], ["", "def", "get_unsup_examples", "(", "self", ",", "raw_data_dir", ",", "unsup_set", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "if", "unsup_set", "==", "\"unsup_in\"", ":", "\n", "      ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "\n", "os", ".", "path", ".", "join", "(", "raw_data_dir", ",", "\"train.csv\"", ")", ",", "\n", "quotechar", "=", "\"\\\"\"", ",", "\n", "delimiter", "=", "\",\"", ")", ",", "\n", "\"unsup_in\"", ",", "skip_unsup", "=", "False", ")", "\n", "", "else", ":", "\n", "      ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "\n", "os", ".", "path", ".", "join", "(", "raw_data_dir", ",", "\"{:s}.csv\"", ".", "format", "(", "unsup_set", ")", ")", ",", "\n", "quotechar", "=", "\"\\\"\"", ",", "\n", "delimiter", "=", "\",\"", ")", ",", "\n", "unsup_set", ",", "skip_unsup", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.TextClassProcessor._create_examples": [[201, 224], ["enumerate", "raw_data_utils.clean_web_text", "examples.append", "raw_data_utils.clean_web_text", "raw_data_utils.InputExample"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.clean_web_text", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.clean_web_text"], ["", "", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ",", "skip_unsup", "=", "True", ",", "\n", "only_unsup", "=", "False", ")", ":", "\n", "    ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "      ", "if", "skip_unsup", "and", "line", "[", "0", "]", "==", "\"unsup\"", ":", "\n", "        ", "continue", "\n", "", "if", "only_unsup", "and", "line", "[", "0", "]", "!=", "\"unsup\"", ":", "\n", "        ", "continue", "\n", "", "guid", "=", "\"%s-%d\"", "%", "(", "set_type", ",", "i", ")", "\n", "if", "self", ".", "has_title", ":", "\n", "        ", "text_a", "=", "line", "[", "2", "]", "\n", "text_b", "=", "line", "[", "1", "]", "\n", "", "else", ":", "\n", "        ", "text_a", "=", "line", "[", "1", "]", "\n", "text_b", "=", "None", "\n", "", "label", "=", "line", "[", "0", "]", "\n", "text_a", "=", "clean_web_text", "(", "text_a", ")", "\n", "if", "text_b", "is", "not", "None", ":", "\n", "        ", "text_b", "=", "clean_web_text", "(", "text_b", ")", "\n", "", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.YELP2Processor.__init__": [[228, 230], ["None"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ")", ":", "\n", "    ", "self", ".", "has_title", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.YELP2Processor.get_labels": [[231, 234], ["str", "range"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "str", "(", "i", ")", "for", "i", "in", "range", "(", "1", ",", "3", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.YELP2Processor.get_train_size": [[235, 237], ["None"], "methods", ["None"], ["", "def", "get_train_size", "(", "self", ")", ":", "\n", "    ", "return", "560000", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.YELP2Processor.get_dev_size": [[238, 240], ["None"], "methods", ["None"], ["", "def", "get_dev_size", "(", "self", ")", ":", "\n", "    ", "return", "38000", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.YELP5Processor.__init__": [[244, 246], ["None"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ")", ":", "\n", "    ", "self", ".", "has_title", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.YELP5Processor.get_labels": [[247, 250], ["str", "range"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "str", "(", "i", ")", "for", "i", "in", "range", "(", "1", ",", "6", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.YELP5Processor.get_train_size": [[251, 253], ["None"], "methods", ["None"], ["", "def", "get_train_size", "(", "self", ")", ":", "\n", "    ", "return", "650000", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.YELP5Processor.get_dev_size": [[254, 256], ["None"], "methods", ["None"], ["", "def", "get_dev_size", "(", "self", ")", ":", "\n", "    ", "return", "50000", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.AMAZON2Processor.__init__": [[260, 262], ["None"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ")", ":", "\n", "    ", "self", ".", "has_title", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.AMAZON2Processor.get_labels": [[263, 266], ["str", "range"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "str", "(", "i", ")", "for", "i", "in", "range", "(", "1", ",", "3", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.AMAZON2Processor.get_train_size": [[267, 269], ["None"], "methods", ["None"], ["", "def", "get_train_size", "(", "self", ")", ":", "\n", "    ", "return", "3600000", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.AMAZON2Processor.get_dev_size": [[270, 272], ["None"], "methods", ["None"], ["", "def", "get_dev_size", "(", "self", ")", ":", "\n", "    ", "return", "400000", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.AMAZON2Processor.get_unsup_examples": [[273, 291], ["raw_data_utils.AMAZON2Processor._create_examples", "raw_data_utils.AMAZON2Processor._create_examples", "raw_data_utils.AMAZON2Processor._read_tsv", "raw_data_utils.AMAZON2Processor._read_tsv", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.TextClassProcessor._create_examples", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.TextClassProcessor._create_examples", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DataProcessor._read_tsv", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DataProcessor._read_tsv"], ["", "def", "get_unsup_examples", "(", "self", ",", "raw_data_dir", ",", "unsup_set", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "if", "unsup_set", "==", "\"unsup_in\"", ":", "\n", "      ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "\n", "os", ".", "path", ".", "join", "(", "raw_data_dir", ",", "\"train.csv\"", ")", ",", "\n", "quotechar", "=", "\"\\\"\"", ",", "\n", "delimiter", "=", "\",\"", ")", ",", "\n", "\"unsup_in\"", ",", "skip_unsup", "=", "False", ")", "\n", "", "else", ":", "\n", "      ", "dir_cell", "=", "raw_data_dir", "[", "5", ":", "7", "]", "\n", "unsup_dir", "=", "None", "# update this path if you use unsupervised data", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "\n", "os", ".", "path", ".", "join", "(", "unsup_dir", ",", "\"{:s}.csv\"", ".", "format", "(", "unsup_set", ")", ")", ",", "\n", "quotechar", "=", "\"\\\"\"", ",", "\n", "delimiter", "=", "\",\"", ")", ",", "\n", "unsup_set", ",", "skip_unsup", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.AMAZON5Processor.__init__": [[294, 296], ["None"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ")", ":", "\n", "    ", "self", ".", "has_title", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.AMAZON5Processor.get_labels": [[297, 300], ["str", "range"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "str", "(", "i", ")", "for", "i", "in", "range", "(", "1", ",", "6", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.AMAZON5Processor.get_unsup_examples": [[301, 319], ["raw_data_utils.AMAZON5Processor._create_examples", "raw_data_utils.AMAZON5Processor._create_examples", "raw_data_utils.AMAZON5Processor._read_tsv", "raw_data_utils.AMAZON5Processor._read_tsv", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.TextClassProcessor._create_examples", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.TextClassProcessor._create_examples", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DataProcessor._read_tsv", "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DataProcessor._read_tsv"], ["", "def", "get_unsup_examples", "(", "self", ",", "raw_data_dir", ",", "unsup_set", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "if", "unsup_set", "==", "\"unsup_in\"", ":", "\n", "      ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "\n", "os", ".", "path", ".", "join", "(", "raw_data_dir", ",", "\"train.csv\"", ")", ",", "\n", "quotechar", "=", "\"\\\"\"", ",", "\n", "delimiter", "=", "\",\"", ")", ",", "\n", "\"unsup_in\"", ",", "skip_unsup", "=", "False", ")", "\n", "", "else", ":", "\n", "      ", "dir_cell", "=", "raw_data_dir", "[", "5", ":", "7", "]", "\n", "unsup_dir", "=", "None", "# update this path if you use unsupervised data", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "\n", "os", ".", "path", ".", "join", "(", "unsup_dir", ",", "\"{:s}.csv\"", ".", "format", "(", "unsup_set", ")", ")", ",", "\n", "quotechar", "=", "\"\\\"\"", ",", "\n", "delimiter", "=", "\",\"", ")", ",", "\n", "unsup_set", ",", "skip_unsup", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.AMAZON5Processor.get_train_size": [[320, 322], ["None"], "methods", ["None"], ["", "", "def", "get_train_size", "(", "self", ")", ":", "\n", "    ", "return", "3000000", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.AMAZON5Processor.get_dev_size": [[323, 325], ["None"], "methods", ["None"], ["", "def", "get_dev_size", "(", "self", ")", ":", "\n", "    ", "return", "650000", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DBPediaProcessor.__init__": [[329, 331], ["None"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ")", ":", "\n", "    ", "self", ".", "has_title", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DBPediaProcessor.get_labels": [[332, 335], ["str", "range"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "str", "(", "i", ")", "for", "i", "in", "range", "(", "1", ",", "15", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DBPediaProcessor.get_train_size": [[336, 338], ["None"], "methods", ["None"], ["", "def", "get_train_size", "(", "self", ")", ":", "\n", "    ", "return", "560000", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.DBPediaProcessor.get_dev_size": [[339, 341], ["None"], "methods", ["None"], ["", "def", "get_dev_size", "(", "self", ")", ":", "\n", "    ", "return", "70000", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.clean_web_text": [[82, 108], ["st.replace.replace", "st.replace.replace", "st.replace.replace", "st.replace.replace", "st.replace.replace", "st.replace.replace", "st.replace.find", "st.replace.find", "print", "print", "print", "len"], "function", ["None"], ["", "", "", "def", "clean_web_text", "(", "st", ")", ":", "\n", "  ", "\"\"\"clean text.\"\"\"", "\n", "st", "=", "st", ".", "replace", "(", "\"<br />\"", ",", "\" \"", ")", "\n", "st", "=", "st", ".", "replace", "(", "\"&quot;\"", ",", "\"\\\"\"", ")", "\n", "st", "=", "st", ".", "replace", "(", "\"<p>\"", ",", "\" \"", ")", "\n", "if", "\"<a href=\"", "in", "st", ":", "\n", "# print(\"before:\\n\", st)", "\n", "    ", "while", "\"<a href=\"", "in", "st", ":", "\n", "      ", "start_pos", "=", "st", ".", "find", "(", "\"<a href=\"", ")", "\n", "end_pos", "=", "st", ".", "find", "(", "\">\"", ",", "start_pos", ")", "\n", "if", "end_pos", "!=", "-", "1", ":", "\n", "        ", "st", "=", "st", "[", ":", "start_pos", "]", "+", "st", "[", "end_pos", "+", "1", ":", "]", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"incomplete href\"", ")", "\n", "print", "(", "\"before\"", ",", "st", ")", "\n", "st", "=", "st", "[", ":", "start_pos", "]", "+", "st", "[", "start_pos", "+", "len", "(", "\"<a href=\"", ")", "]", "\n", "print", "(", "\"after\"", ",", "st", ")", "\n", "\n", "", "", "st", "=", "st", ".", "replace", "(", "\"</a>\"", ",", "\"\"", ")", "\n", "# print(\"after\\n\", st)", "\n", "# print(\"\")", "\n", "", "st", "=", "st", ".", "replace", "(", "\"\\\\n\"", ",", "\" \"", ")", "\n", "st", "=", "st", ".", "replace", "(", "\"\\\\\"", ",", "\" \"", ")", "\n", "# while \"  \" in st:", "\n", "#   st = st.replace(\"  \", \" \")", "\n", "return", "st", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.raw_data_utils.get_processor": [[343, 356], ["task_name.lower.lower"], "function", ["None"], ["", "", "def", "get_processor", "(", "task_name", ")", ":", "\n", "  ", "\"\"\"get processor.\"\"\"", "\n", "task_name", "=", "task_name", ".", "lower", "(", ")", "\n", "processors", "=", "{", "\n", "\"imdb\"", ":", "IMDbProcessor", ",", "\n", "\"dbpedia\"", ":", "DBPediaProcessor", ",", "\n", "\"yelp-2\"", ":", "YELP2Processor", ",", "\n", "\"yelp-5\"", ":", "YELP5Processor", ",", "\n", "\"amazon-2\"", ":", "AMAZON2Processor", ",", "\n", "\"amazon-5\"", ":", "AMAZON5Processor", ",", "\n", "}", "\n", "processor", "=", "processors", "[", "task_name", "]", "(", ")", "\n", "return", "processor", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.FullTokenizer.__init__": [[68, 72], ["tokenization.load_vocab", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ")", ":", "\n", "    ", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.FullTokenizer.tokenize": [[73, 79], ["tokenization.FullTokenizer.basic_tokenizer.tokenize", "tokenization.FullTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "      ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "        ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.FullTokenizer.tokenize_to_word": [[80, 82], ["tokenization.FullTokenizer.basic_tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenize_to_word", "(", "self", ",", "text", ")", ":", "\n", "    ", "return", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.FullTokenizer.tokenize_to_wordpiece": [[83, 88], ["tokenization.FullTokenizer.wordpiece_tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenize_to_wordpiece", "(", "self", ",", "tokens", ")", ":", "\n", "    ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "      ", "split_tokens", "+=", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", "\n", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.FullTokenizer.convert_tokens_to_ids": [[89, 91], ["tokenization.FullTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.convert_tokens_to_ids"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "    ", "return", "convert_tokens_to_ids", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.BasicTokenizer.__init__": [[96, 103], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ")", ":", "\n", "    ", "\"\"\"Constructs a BasicTokenizer.\n\n    Args:\n      do_lower_case: Whether to lower case the input.\n    \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.BasicTokenizer.tokenize": [[104, 118], ["tokenization._convert_to_unicode_or_throw", "tokenization.BasicTokenizer._clean_text", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.utils.tokenization._convert_to_unicode_or_throw", "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "_convert_to_unicode_or_throw", "(", "text", ")", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "      ", "if", "self", ".", "do_lower_case", ":", "\n", "        ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.BasicTokenizer._run_strip_accents": [[119, 129], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "        ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.BasicTokenizer._run_split_on_punc": [[130, 149], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.utils.tokenization._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "      ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "        ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "        ", "if", "start_new_word", ":", "\n", "          ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.BasicTokenizer._clean_text": [[150, 162], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.utils.tokenization._is_whitespace", "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "        ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "        ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "        ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.WordpieceTokenizer.__init__": [[167, 171], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "    ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.WordpieceTokenizer.tokenize": [[172, 224], ["tokenization._convert_to_unicode_or_throw", "tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.utils.tokenization._convert_to_unicode_or_throw", "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n    This uses a greedy longest-match-first algorithm to perform tokenization\n    using the given vocabulary.\n\n    For example:\n      input = \"unaffable\"\n      output = [\"un\", \"##aff\", \"##able\"]\n\n    Args:\n      text: A single token or whitespace separated tokens. This should have\n        already been passed through `BasicTokenizer.\n\n    Returns:\n      A list of wordpiece tokens.\n    \"\"\"", "\n", "\n", "text", "=", "_convert_to_unicode_or_throw", "(", "text", ")", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "      ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "        ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "        ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "          ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "            ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "            ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "          ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "        ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "        ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.open_reader": [[28, 31], ["codecs.getreader", "tensorflow.gfile.GFile"], "function", ["None"], ["def", "open_reader", "(", "input_file", ",", "encoding", "=", "\"utf-8\"", ")", ":", "\n", "  ", "\"\"\"Opens a text file for reading.\"\"\"", "\n", "return", "codecs", ".", "getreader", "(", "encoding", ")", "(", "tf", ".", "gfile", ".", "GFile", "(", "input_file", ",", "\"r\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.load_vocab": [[33, 46], ["collections.OrderedDict", "tokenization.open_reader", "reader.readline", "token.strip.strip"], "function", ["home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.open_reader"], ["", "def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "  ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "open_reader", "(", "vocab_file", ")", "as", "reader", ":", "\n", "    ", "while", "True", ":", "\n", "      ", "token", "=", "reader", ".", "readline", "(", ")", "\n", "if", "not", "token", ":", "\n", "        ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.convert_tokens_to_ids": [[48, 54], ["ids.append"], "function", ["None"], ["", "def", "convert_tokens_to_ids", "(", "vocab", ",", "tokens", ")", ":", "\n", "  ", "\"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"", "\n", "ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "    ", "ids", ".", "append", "(", "vocab", "[", "token", "]", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.whitespace_tokenize": [[56, 63], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "  ", "\"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "    ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization._is_whitespace": [[226, 236], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "    ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization._is_control": [[238, 248], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "    ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization._is_punctuation": [[250, 264], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "    ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization._convert_to_unicode_or_throw": [[266, 274], ["isinstance", "text.decode.decode", "isinstance", "ValueError", "type"], "function", ["None"], ["", "def", "_convert_to_unicode_or_throw", "(", "text", ")", ":", "\n", "  ", "\"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"", "\n", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "    ", "text", "=", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "if", "not", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "    ", "raise", "ValueError", "(", "\"`text` must be of type `unicode` or `str`, but is \"", "\n", "\"actually of type: %s\"", "%", "(", "type", "(", "text", ")", ".", "__name__", ")", ")", "\n", "", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.utils.tokenization.printable_text": [[276, 297], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "isinstance", "text.encode", "ValueError", "type", "type"], "function", ["None"], ["", "def", "printable_text", "(", "text", ")", ":", "\n", "  ", "\"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"", "\n", "\n", "# These functions want `str` for both Python2 and Python3, but in one case", "\n", "# it's a Unicode string and in the other it's a byte string.", "\n", "if", "six", ".", "PY3", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "      ", "return", "text", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.EfficientRandomGen.reset_random_prob": [[45, 50], ["numpy.random.random"], "methods", ["None"], ["def", "reset_random_prob", "(", "self", ")", ":", "\n", "    ", "\"\"\"Generate many random numbers at the same time and cache them.\"\"\"", "\n", "cache_len", "=", "100000", "\n", "self", ".", "random_prob_cache", "=", "np", ".", "random", ".", "random", "(", "size", "=", "(", "cache_len", ",", ")", ")", "\n", "self", ".", "random_prob_ptr", "=", "cache_len", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.EfficientRandomGen.get_random_prob": [[51, 58], ["word_level_augment.EfficientRandomGen.reset_random_prob"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.EfficientRandomGen.reset_random_prob"], ["", "def", "get_random_prob", "(", "self", ")", ":", "\n", "    ", "\"\"\"Get a random number.\"\"\"", "\n", "value", "=", "self", ".", "random_prob_cache", "[", "self", ".", "random_prob_ptr", "]", "\n", "self", ".", "random_prob_ptr", "-=", "1", "\n", "if", "self", ".", "random_prob_ptr", "==", "-", "1", ":", "\n", "      ", "self", ".", "reset_random_prob", "(", ")", "\n", "", "return", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.EfficientRandomGen.get_random_token": [[59, 66], ["word_level_augment.EfficientRandomGen.reset_token_list"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.TfIdfWordRep.reset_token_list"], ["", "def", "get_random_token", "(", "self", ")", ":", "\n", "    ", "\"\"\"Get a random token.\"\"\"", "\n", "token", "=", "self", ".", "token_list", "[", "self", ".", "token_ptr", "]", "\n", "self", ".", "token_ptr", "-=", "1", "\n", "if", "self", ".", "token_ptr", "==", "-", "1", ":", "\n", "      ", "self", ".", "reset_token_list", "(", ")", "\n", "", "return", "token", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.UnifRep.__init__": [[71, 77], ["len", "word_level_augment.UnifRep.reset_token_list", "word_level_augment.UnifRep.reset_random_prob"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.TfIdfWordRep.reset_token_list", "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.EfficientRandomGen.reset_random_prob"], ["def", "__init__", "(", "self", ",", "token_prob", ",", "vocab", ")", ":", "\n", "    ", "self", ".", "token_prob", "=", "token_prob", "\n", "self", ".", "vocab_size", "=", "len", "(", "vocab", ")", "\n", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "reset_token_list", "(", ")", "\n", "self", ".", "reset_random_prob", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.UnifRep.__call__": [[78, 83], ["word_level_augment.UnifRep.replace_tokens", "word_level_augment.UnifRep.replace_tokens"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.TfIdfWordRep.replace_tokens", "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.TfIdfWordRep.replace_tokens"], ["", "def", "__call__", "(", "self", ",", "example", ")", ":", "\n", "    ", "example", ".", "word_list_a", "=", "self", ".", "replace_tokens", "(", "example", ".", "word_list_a", ")", "\n", "if", "example", ".", "text_b", ":", "\n", "      ", "example", ".", "word_list_b", "=", "self", ".", "replace_tokens", "(", "example", ".", "word_list_b", ")", "\n", "", "return", "example", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.UnifRep.replace_tokens": [[84, 101], ["len", "range", "numpy.random.random", "tensorflow.logging.info", "len", "tensorflow.logging.info", "word_level_augment.UnifRep.get_random_prob", "word_level_augment.UnifRep.get_random_token", "word_level_augment.filter_unicode", "word_level_augment.filter_unicode"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.EfficientRandomGen.get_random_prob", "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.EfficientRandomGen.get_random_token", "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.filter_unicode", "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.filter_unicode"], ["", "def", "replace_tokens", "(", "self", ",", "tokens", ")", ":", "\n", "    ", "\"\"\"Replace tokens randomly.\"\"\"", "\n", "if", "len", "(", "tokens", ")", ">=", "3", ":", "\n", "      ", "if", "np", ".", "random", ".", "random", "(", ")", "<", "0.001", ":", "\n", "        ", "show_example", "=", "True", "\n", "", "else", ":", "\n", "        ", "show_example", "=", "False", "\n", "", "if", "show_example", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"before augment: {:s}\"", ".", "format", "(", "\n", "filter_unicode", "(", "\" \"", ".", "join", "(", "tokens", ")", ")", ")", ")", "\n", "", "for", "i", "in", "range", "(", "len", "(", "tokens", ")", ")", ":", "\n", "        ", "if", "self", ".", "get_random_prob", "(", ")", "<", "self", ".", "token_prob", ":", "\n", "          ", "tokens", "[", "i", "]", "=", "self", ".", "get_random_token", "(", ")", "\n", "", "", "if", "show_example", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"after augment: {:s}\"", ".", "format", "(", "\n", "filter_unicode", "(", "\" \"", ".", "join", "(", "tokens", ")", ")", ")", ")", "\n", "", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.UnifRep.reset_token_list": [[102, 107], ["word_level_augment.UnifRep.vocab.keys", "numpy.random.shuffle", "len"], "methods", ["None"], ["", "def", "reset_token_list", "(", "self", ")", ":", "\n", "    ", "\"\"\"Generate many random tokens at the same time and cache them.\"\"\"", "\n", "self", ".", "token_list", "=", "self", ".", "vocab", ".", "keys", "(", ")", "\n", "self", ".", "token_ptr", "=", "len", "(", "self", ".", "token_list", ")", "-", "1", "\n", "np", ".", "random", ".", "shuffle", "(", "self", ".", "token_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.TfIdfWordRep.__init__": [[145, 166], ["object.__init__", "copy.deepcopy", "data_stats[].items", "sorted", "numpy.array", "word_level_augment.TfIdfWordRep.reset_token_list", "word_level_augment.TfIdfWordRep.reset_random_prob", "word_level_augment.TfIdfWordRep.normalized_tf_idf.max", "word_level_augment.TfIdfWordRep.normalized_tf_idf.sum"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.TransformT.__init__", "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.TfIdfWordRep.reset_token_list", "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.EfficientRandomGen.reset_random_prob"], ["def", "__init__", "(", "self", ",", "token_prob", ",", "data_stats", ")", ":", "\n", "    ", "super", "(", "TfIdfWordRep", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "token_prob", "=", "token_prob", "\n", "self", ".", "data_stats", "=", "data_stats", "\n", "self", ".", "idf", "=", "data_stats", "[", "\"idf\"", "]", "\n", "self", ".", "tf_idf", "=", "data_stats", "[", "\"tf_idf\"", "]", "\n", "data_stats", "=", "copy", ".", "deepcopy", "(", "data_stats", ")", "\n", "tf_idf_items", "=", "data_stats", "[", "\"tf_idf\"", "]", ".", "items", "(", ")", "\n", "tf_idf_items", "=", "sorted", "(", "tf_idf_items", ",", "key", "=", "lambda", "item", ":", "-", "item", "[", "1", "]", ")", "\n", "self", ".", "tf_idf_keys", "=", "[", "]", "\n", "self", ".", "tf_idf_values", "=", "[", "]", "\n", "for", "key", ",", "value", "in", "tf_idf_items", ":", "\n", "      ", "self", ".", "tf_idf_keys", "+=", "[", "key", "]", "\n", "self", ".", "tf_idf_values", "+=", "[", "value", "]", "\n", "", "self", ".", "normalized_tf_idf", "=", "np", ".", "array", "(", "self", ".", "tf_idf_values", ")", "\n", "self", ".", "normalized_tf_idf", "=", "(", "self", ".", "normalized_tf_idf", ".", "max", "(", ")", "\n", "-", "self", ".", "normalized_tf_idf", ")", "\n", "self", ".", "normalized_tf_idf", "=", "(", "self", ".", "normalized_tf_idf", "\n", "/", "self", ".", "normalized_tf_idf", ".", "sum", "(", ")", ")", "\n", "self", ".", "reset_token_list", "(", ")", "\n", "self", ".", "reset_random_prob", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.TfIdfWordRep.get_replace_prob": [[167, 180], ["collections.defaultdict", "numpy.array", "numpy.max", "len", "len", "numpy.array.sum"], "methods", ["None"], ["", "def", "get_replace_prob", "(", "self", ",", "all_words", ")", ":", "\n", "    ", "\"\"\"Compute the probability of replacing tokens in a sentence.\"\"\"", "\n", "cur_tf_idf", "=", "collections", ".", "defaultdict", "(", "int", ")", "\n", "for", "word", "in", "all_words", ":", "\n", "      ", "cur_tf_idf", "[", "word", "]", "+=", "1.", "/", "len", "(", "all_words", ")", "*", "self", ".", "idf", "[", "word", "]", "\n", "", "replace_prob", "=", "[", "]", "\n", "for", "word", "in", "all_words", ":", "\n", "      ", "replace_prob", "+=", "[", "cur_tf_idf", "[", "word", "]", "]", "\n", "", "replace_prob", "=", "np", ".", "array", "(", "replace_prob", ")", "\n", "replace_prob", "=", "np", ".", "max", "(", "replace_prob", ")", "-", "replace_prob", "\n", "replace_prob", "=", "(", "replace_prob", "/", "replace_prob", ".", "sum", "(", ")", "*", "\n", "self", ".", "token_prob", "*", "len", "(", "all_words", ")", ")", "\n", "return", "replace_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.TfIdfWordRep.__call__": [[181, 212], ["copy.deepcopy", "word_level_augment.TfIdfWordRep.get_replace_prob", "word_level_augment.TfIdfWordRep.replace_tokens", "word_level_augment.TfIdfWordRep.get_random_prob", "tensorflow.logging.info", "word_level_augment.TfIdfWordRep.replace_tokens", "copy.deepcopy", "tensorflow.logging.info", "word_level_augment.filter_unicode", "len", "word_level_augment.filter_unicode", "len"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.TfIdfWordRep.get_replace_prob", "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.TfIdfWordRep.replace_tokens", "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.EfficientRandomGen.get_random_prob", "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.TfIdfWordRep.replace_tokens", "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.filter_unicode", "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.filter_unicode"], ["", "def", "__call__", "(", "self", ",", "example", ")", ":", "\n", "    ", "if", "self", ".", "get_random_prob", "(", ")", "<", "0.001", ":", "\n", "      ", "show_example", "=", "True", "\n", "", "else", ":", "\n", "      ", "show_example", "=", "False", "\n", "", "all_words", "=", "copy", ".", "deepcopy", "(", "example", ".", "word_list_a", ")", "\n", "if", "example", ".", "text_b", ":", "\n", "      ", "all_words", "+=", "example", ".", "word_list_b", "\n", "\n", "", "if", "show_example", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"before tf_idf_unif aug: {:s}\"", ".", "format", "(", "\n", "filter_unicode", "(", "\" \"", ".", "join", "(", "all_words", ")", ")", ")", ")", "\n", "\n", "", "replace_prob", "=", "self", ".", "get_replace_prob", "(", "all_words", ")", "\n", "example", ".", "word_list_a", "=", "self", ".", "replace_tokens", "(", "\n", "example", ".", "word_list_a", ",", "\n", "replace_prob", "[", ":", "len", "(", "example", ".", "word_list_a", ")", "]", "\n", ")", "\n", "if", "example", ".", "text_b", ":", "\n", "      ", "example", ".", "word_list_b", "=", "self", ".", "replace_tokens", "(", "\n", "example", ".", "word_list_b", ",", "\n", "replace_prob", "[", "len", "(", "example", ".", "word_list_a", ")", ":", "]", "\n", ")", "\n", "\n", "", "if", "show_example", ":", "\n", "      ", "all_words", "=", "copy", ".", "deepcopy", "(", "example", ".", "word_list_a", ")", "\n", "if", "example", ".", "text_b", ":", "\n", "        ", "all_words", "+=", "example", ".", "word_list_b", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"after tf_idf_unif aug: {:s}\"", ".", "format", "(", "\n", "filter_unicode", "(", "\" \"", ".", "join", "(", "all_words", ")", ")", ")", ")", "\n", "", "return", "example", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.TfIdfWordRep.replace_tokens": [[213, 219], ["range", "len", "word_level_augment.TfIdfWordRep.get_random_prob", "word_level_augment.TfIdfWordRep.get_random_token"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.EfficientRandomGen.get_random_prob", "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.EfficientRandomGen.get_random_token"], ["", "def", "replace_tokens", "(", "self", ",", "word_list", ",", "replace_prob", ")", ":", "\n", "    ", "\"\"\"Replace tokens in a sentence.\"\"\"", "\n", "for", "i", "in", "range", "(", "len", "(", "word_list", ")", ")", ":", "\n", "      ", "if", "self", ".", "get_random_prob", "(", ")", "<", "replace_prob", "[", "i", "]", ":", "\n", "        ", "word_list", "[", "i", "]", "=", "self", ".", "get_random_token", "(", ")", "\n", "", "", "return", "word_list", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.TfIdfWordRep.reset_token_list": [[220, 230], ["len", "numpy.random.choice", "tensorflow.logging.info", "len", "word_level_augment.filter_unicode"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.filter_unicode"], ["", "def", "reset_token_list", "(", "self", ")", ":", "\n", "    ", "cache_len", "=", "len", "(", "self", ".", "tf_idf_keys", ")", "\n", "token_list_idx", "=", "np", ".", "random", ".", "choice", "(", "\n", "cache_len", ",", "(", "cache_len", ",", ")", ",", "p", "=", "self", ".", "normalized_tf_idf", ")", "\n", "self", ".", "token_list", "=", "[", "]", "\n", "for", "idx", "in", "token_list_idx", ":", "\n", "      ", "self", ".", "token_list", "+=", "[", "self", ".", "tf_idf_keys", "[", "idx", "]", "]", "\n", "", "self", ".", "token_ptr", "=", "len", "(", "self", ".", "token_list", ")", "-", "1", "\n", "tf", ".", "logging", ".", "info", "(", "\"sampled token list: {:s}\"", ".", "format", "(", "\n", "filter_unicode", "(", "\" \"", ".", "join", "(", "self", ".", "token_list", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.filter_unicode": [[38, 40], ["None"], "function", ["None"], ["def", "filter_unicode", "(", "st", ")", ":", "\n", "  ", "return", "\"\"", ".", "join", "(", "[", "c", "for", "c", "in", "st", "if", "c", "in", "printable", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.get_data_stats": [[109, 139], ["collections.defaultdict", "range", "range", "len", "copy.deepcopy", "math.log", "len", "copy.deepcopy", "len", "len"], "function", ["None"], ["", "", "def", "get_data_stats", "(", "examples", ")", ":", "\n", "  ", "\"\"\"Compute the IDF score for each word. Then compute the TF-IDF score.\"\"\"", "\n", "word_doc_freq", "=", "collections", ".", "defaultdict", "(", "int", ")", "\n", "# Compute IDF", "\n", "for", "i", "in", "range", "(", "len", "(", "examples", ")", ")", ":", "\n", "    ", "cur_word_dict", "=", "{", "}", "\n", "cur_sent", "=", "copy", ".", "deepcopy", "(", "examples", "[", "i", "]", ".", "word_list_a", ")", "\n", "if", "examples", "[", "i", "]", ".", "text_b", ":", "\n", "      ", "cur_sent", "+=", "examples", "[", "i", "]", ".", "word_list_b", "\n", "", "for", "word", "in", "cur_sent", ":", "\n", "      ", "cur_word_dict", "[", "word", "]", "=", "1", "\n", "", "for", "word", "in", "cur_word_dict", ":", "\n", "      ", "word_doc_freq", "[", "word", "]", "+=", "1", "\n", "", "", "idf", "=", "{", "}", "\n", "for", "word", "in", "word_doc_freq", ":", "\n", "    ", "idf", "[", "word", "]", "=", "math", ".", "log", "(", "len", "(", "examples", ")", "*", "1.", "/", "word_doc_freq", "[", "word", "]", ")", "\n", "# Compute TF-IDF", "\n", "", "tf_idf", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "len", "(", "examples", ")", ")", ":", "\n", "    ", "cur_word_dict", "=", "{", "}", "\n", "cur_sent", "=", "copy", ".", "deepcopy", "(", "examples", "[", "i", "]", ".", "word_list_a", ")", "\n", "if", "examples", "[", "i", "]", ".", "text_b", ":", "\n", "      ", "cur_sent", "+=", "examples", "[", "i", "]", ".", "word_list_b", "\n", "", "for", "word", "in", "cur_sent", ":", "\n", "      ", "if", "word", "not", "in", "tf_idf", ":", "\n", "        ", "tf_idf", "[", "word", "]", "=", "0", "\n", "", "tf_idf", "[", "word", "]", "+=", "1.", "/", "len", "(", "cur_sent", ")", "*", "idf", "[", "word", "]", "\n", "", "", "return", "{", "\n", "\"idf\"", ":", "idf", ",", "\n", "\"tf_idf\"", ":", "tf_idf", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.word_level_augment": [[232, 249], ["aug_ops.startswith", "tensorflow.logging.info", "float", "word_level_augment.UnifRep", "range", "aug_ops.startswith", "len", "TfIdfWordRep.", "tensorflow.logging.info", "float", "word_level_augment.TfIdfWordRep", "range", "aug_ops.split", "len", "TfIdfWordRep.", "aug_ops.split"], "function", ["None"], ["", "", "def", "word_level_augment", "(", "\n", "examples", ",", "aug_ops", ",", "vocab", ",", "data_stats", ")", ":", "\n", "  ", "\"\"\"Word level augmentations. Used before augmentation.\"\"\"", "\n", "if", "aug_ops", ":", "\n", "    ", "if", "aug_ops", ".", "startswith", "(", "\"unif\"", ")", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"\\n>>Using augmentation {}\"", ".", "format", "(", "aug_ops", ")", ")", "\n", "token_prob", "=", "float", "(", "aug_ops", ".", "split", "(", "\"-\"", ")", "[", "1", "]", ")", "\n", "op", "=", "UnifRep", "(", "token_prob", ",", "vocab", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "examples", ")", ")", ":", "\n", "        ", "examples", "[", "i", "]", "=", "op", "(", "examples", "[", "i", "]", ")", "\n", "", "", "elif", "aug_ops", ".", "startswith", "(", "\"tf_idf\"", ")", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"\\n>>Using augmentation {}\"", ".", "format", "(", "aug_ops", ")", ")", "\n", "token_prob", "=", "float", "(", "aug_ops", ".", "split", "(", "\"-\"", ")", "[", "1", "]", ")", "\n", "op", "=", "TfIdfWordRep", "(", "token_prob", ",", "data_stats", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "examples", ")", ")", ":", "\n", "        ", "examples", "[", "i", "]", "=", "op", "(", "examples", "[", "i", "]", ")", "\n", "", "", "", "return", "examples", "\n", "", ""]], "home.repos.pwc.inspect_result.google-research_uda.augmentation.sent_level_augment.replace_with_length_check": [[37, 59], ["len", "math.fabs", "len", "len", "random.random", "tensorflow.logging.info", "random.random", "tensorflow.logging.info", "len", "len", "augmentation.word_level_augment.filter_unicode", "augmentation.word_level_augment.filter_unicode", "augmentation.word_level_augment.filter_unicode", "augmentation.word_level_augment.filter_unicode"], "function", ["home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.filter_unicode", "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.filter_unicode", "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.filter_unicode", "home.repos.pwc.inspect_result.google-research_uda.augmentation.word_level_augment.filter_unicode"], ["def", "replace_with_length_check", "(", "\n", "ori_text", ",", "new_text", ",", "\n", "use_min_length", ",", "\n", "use_max_length_diff_ratio", ")", ":", "\n", "  ", "\"\"\"Use new_text if the text length satisfies several constraints.\"\"\"", "\n", "if", "len", "(", "ori_text", ")", "<", "use_min_length", "or", "len", "(", "new_text", ")", "<", "use_min_length", ":", "\n", "    ", "if", "random", ".", "random", "(", ")", "<", "0.001", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\n", "\"not replacing due to short text: \\n\\tori: {:s}\\n\\tnew: {:s}\\n\"", ".", "format", "(", "\n", "word_level_augment", ".", "filter_unicode", "(", "ori_text", ")", ",", "\n", "word_level_augment", ".", "filter_unicode", "(", "new_text", ")", ")", ")", "\n", "", "return", "ori_text", "\n", "", "length_diff_ratio", "=", "1.0", "*", "(", "len", "(", "new_text", ")", "-", "len", "(", "ori_text", ")", ")", "/", "len", "(", "ori_text", ")", "\n", "if", "math", ".", "fabs", "(", "length_diff_ratio", ")", ">", "use_max_length_diff_ratio", ":", "\n", "    ", "if", "random", ".", "random", "(", ")", "<", "0.001", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\n", "(", "\"not replacing due to too different text length:\\n\"", "\n", "\"\\tori: {:s}\\n\\tnew: {:s}\\n\"", ".", "format", "(", "\n", "word_level_augment", ".", "filter_unicode", "(", "ori_text", ")", ",", "\n", "word_level_augment", ".", "filter_unicode", "(", "new_text", ")", ")", ")", ")", "\n", "", "return", "ori_text", "\n", "", "return", "new_text", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.augmentation.sent_level_augment.back_translation": [[61, 131], ["tensorflow.logging.info", "aug_ops.split", "float", "tensorflow.logging.info", "range", "range", "tensorflow.logging.info", "tensorflow.logging.info", "len", "tensorflow.gfile.Open", "inf.readlines", "len", "paraphrases[].strip", "len", "len", "sent_level_augment.replace_with_length_check", "utils.raw_data_utils.InputExample", "len", "float", "sent_level_augment.replace_with_length_check", "numpy.random.random", "tensorflow.logging.info", "tensorflow.logging.info", "print", "len"], "function", ["home.repos.pwc.inspect_result.google-research_uda.augmentation.sent_level_augment.replace_with_length_check", "home.repos.pwc.inspect_result.google-research_uda.augmentation.sent_level_augment.replace_with_length_check"], ["", "def", "back_translation", "(", "examples", ",", "aug_ops", ",", "sub_set", ",", "aug_copy_num", ",", "\n", "start", ",", "end", ",", "data_total_size", ")", ":", "\n", "  ", "\"\"\"Run back translation.\"\"\"", "\n", "use_min_length", "=", "10", "\n", "use_max_length_diff_ratio", "=", "0.5", "\n", "tf", ".", "logging", ".", "info", "(", "\"running bt augmentation\"", ")", "\n", "bt_args", "=", "aug_ops", ".", "split", "(", "\"-\"", ")", "\n", "temp", "=", "float", "(", "bt_args", "[", "1", "]", ")", "\n", "\n", "if", "len", "(", "bt_args", ")", ">", "2", ":", "\n", "    ", "assert", "len", "(", "bt_args", ")", "==", "3", "\n", "assert", "float", "(", "bt_args", "[", "2", "]", ")", "==", "1.", "\n", "\n", "", "if", "examples", "[", "0", "]", ".", "text_b", "is", "not", "None", ":", "\n", "    ", "text_per_example", "=", "2", "\n", "", "else", ":", "\n", "    ", "text_per_example", "=", "1", "\n", "\n", "", "back_translation_file", "=", "\"{:s}/{:s}/sample_{:.1f}/para/para_{:d}.txt\"", ".", "format", "(", "\n", "FLAGS", ".", "back_translation_dir", ",", "sub_set", ",", "\n", "temp", ",", "aug_copy_num", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Using back translation file: {:s}\"", ".", "format", "(", "\n", "back_translation_file", ")", ")", "\n", "\n", "with", "tf", ".", "gfile", ".", "Open", "(", "back_translation_file", ")", "as", "inf", ":", "\n", "    ", "paraphrases", "=", "inf", ".", "readlines", "(", ")", "\n", "", "for", "i", "in", "range", "(", "len", "(", "paraphrases", ")", ")", ":", "\n", "    ", "paraphrases", "[", "i", "]", "=", "paraphrases", "[", "i", "]", ".", "strip", "(", ")", "\n", "", "assert", "len", "(", "paraphrases", ")", "==", "data_total_size", "\n", "\n", "paraphrases", "=", "paraphrases", "[", "start", "*", "text_per_example", ":", "end", "*", "text_per_example", "]", "\n", "aug_examples", "=", "[", "]", "\n", "aug_cnt", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "examples", ")", ")", ":", "\n", "    ", "ori_example", "=", "examples", "[", "i", "]", "\n", "text_a", "=", "replace_with_length_check", "(", "\n", "ori_example", ".", "text_a", ",", "\n", "paraphrases", "[", "i", "*", "text_per_example", "]", ",", "\n", "use_min_length", ",", "\n", "use_max_length_diff_ratio", ",", "\n", ")", "\n", "if", "text_a", "==", "paraphrases", "[", "i", "*", "text_per_example", "]", ":", "\n", "      ", "aug_cnt", "+=", "1", "\n", "", "if", "ori_example", ".", "text_b", "is", "not", "None", ":", "\n", "      ", "text_b", "=", "replace_with_length_check", "(", "\n", "ori_example", ".", "text_b", ",", "\n", "paraphrases", "[", "i", "*", "text_per_example", "+", "1", "]", ",", "\n", "use_min_length", ",", "\n", "use_max_length_diff_ratio", ",", "\n", ")", "\n", "", "else", ":", "\n", "      ", "text_b", "=", "None", "\n", "\n", "", "example", "=", "raw_data_utils", ".", "InputExample", "(", "\n", "guid", "=", "ori_example", ".", "guid", ",", "\n", "text_a", "=", "text_a", ",", "\n", "text_b", "=", "text_b", ",", "\n", "label", "=", "ori_example", ".", "label", ")", "\n", "aug_examples", "+=", "[", "example", "]", "\n", "if", "np", ".", "random", ".", "random", "(", ")", "<", "0.0001", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"\\tori:\\n\\t\\t{:s}\\n\\t\\t{:s}\\n\\t\\t{:s}\\n\"", ".", "format", "(", "\n", "ori_example", ".", "text_a", ",", "ori_example", ".", "text_b", ",", "ori_example", ".", "label", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"\\tnew:\\n\\t\\t{:s}\\n\\t\\t{:s}\\n\\t\\t{:s}\\n\"", ".", "format", "(", "\n", "example", ".", "text_a", ",", "example", ".", "text_b", ",", "example", ".", "label", ")", ")", "\n", "", "if", "i", "%", "10000", "==", "0", ":", "\n", "      ", "print", "(", "\"processing example # {:d}\"", ".", "format", "(", "i", ")", ")", "\n", "", "", "tf", ".", "logging", ".", "info", "(", "\"applied back translation for {:.1f} percent of data\"", ".", "format", "(", "\n", "aug_cnt", "*", "1.", "/", "len", "(", "examples", ")", "*", "100", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"finishing running back translation augmentation\"", ")", "\n", "return", "aug_examples", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.augmentation.sent_level_augment.run_augment": [[133, 144], ["aug_ops.startswith", "sent_level_augment.back_translation"], "function", ["home.repos.pwc.inspect_result.google-research_uda.augmentation.sent_level_augment.back_translation"], ["", "def", "run_augment", "(", "\n", "examples", ",", "aug_ops", ",", "sub_set", ",", "aug_copy_num", ",", "\n", "start", ",", "end", ",", "dst_tot_size", ")", ":", "\n", "  ", "\"\"\"Sentence level augmentations. Used before augmentation.\"\"\"", "\n", "if", "aug_ops", ":", "\n", "    ", "if", "aug_ops", ".", "startswith", "(", "\"bt\"", ")", ":", "\n", "      ", "examples", "=", "back_translation", "(", "\n", "examples", ",", "aug_ops", ",", "sub_set", ",", "aug_copy_num", ",", "start", ",", "end", ",", "dst_tot_size", ")", "\n", "", "else", ":", "\n", "      ", "pass", "\n", "", "", "return", "examples", "\n", "", ""]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.BertConfig.__init__": [[34, 81], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "16", ",", "\n", "initializer_range", "=", "0.02", ")", ":", "\n", "    ", "\"\"\"Constructs BertConfig.\n\n    Args:\n      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n      hidden_size: Size of the encoder layers and the pooler layer.\n      num_hidden_layers: Number of hidden layers in the Transformer encoder.\n      num_attention_heads: Number of attention heads for each attention layer in\n        the Transformer encoder.\n      intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n        layer in the Transformer encoder.\n      hidden_act: The non-linear activation function (function or string) in the\n        encoder and pooler.\n      hidden_dropout_prob: The dropout probabilitiy for all fully connected\n        layers in the embeddings, encoder, and pooler.\n      attention_probs_dropout_prob: The dropout ratio for the attention\n        probabilities.\n      max_position_embeddings: The maximum sequence length that this model might\n        ever be used with. Typically set this to something large just in case\n        (e.g., 512 or 1024 or 2048).\n      type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n        `BertModel`.\n      initializer_range: The sttdev of the truncated_normal_initializer for\n        initializing all weight matrices.\n    \"\"\"", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.BertConfig.from_dict": [[82, 89], ["modeling.BertConfig", "six.iteritems"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "    ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size", "=", "None", ")", "\n", "for", "(", "key", ",", "value", ")", "in", "six", ".", "iteritems", "(", "json_object", ")", ":", "\n", "      ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.BertConfig.from_json_file": [[90, 100], ["cls.from_dict", "tensorflow.gfile.GFile", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.bert.modeling.BertConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ",", "model_dropout", ")", ":", "\n", "    ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "json_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "      ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "config", "=", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "if", "model_dropout", "!=", "-", "1", ":", "\n", "      ", "config", ".", "hidden_dropout_prob", "=", "model_dropout", "\n", "config", ".", "attention_probs_dropout_prob", "=", "model_dropout", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.BertConfig.to_dict": [[101, 105], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "    ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.BertConfig.to_json_string": [[106, 109], ["json.dumps", "modeling.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.bert.modeling.BertConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "    ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.bert_embedding": [[111, 160], ["copy.deepcopy", "modeling.get_shape_list", "tensorflow.ones", "tensorflow.zeros", "tensorflow.variable_scope", "tensorflow.variable_scope", "modeling.embedding_lookup", "modeling.embedding_postprocessor"], "function", ["home.repos.pwc.inspect_result.google-research_uda.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.embedding_lookup", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.embedding_postprocessor"], ["", "", "def", "bert_embedding", "(", "config", ",", "\n", "is_training", ",", "\n", "input_ids", ",", "\n", "input_mask", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "use_one_hot_embeddings", "=", "True", ",", "\n", "scope", "=", "None", ")", ":", "\n", "\n", "  ", "config", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "if", "not", "is_training", ":", "\n", "    ", "config", ".", "hidden_dropout_prob", "=", "0.0", "\n", "config", ".", "attention_probs_dropout_prob", "=", "0.0", "\n", "\n", "", "input_shape", "=", "get_shape_list", "(", "input_ids", ",", "expected_rank", "=", "2", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "\n", "if", "input_mask", "is", "None", ":", "\n", "    ", "input_mask", "=", "tf", ".", "ones", "(", "shape", "=", "[", "batch_size", ",", "seq_length", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "    ", "token_type_ids", "=", "tf", ".", "zeros", "(", "shape", "=", "[", "batch_size", ",", "seq_length", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"bert\"", ",", "scope", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "\"embeddings\"", ")", ":", "\n", "# Perform embedding lookup on the word ids.", "\n", "      ", "(", "embedding_output", ",", "embedding_table", ")", "=", "embedding_lookup", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "vocab_size", "=", "config", ".", "vocab_size", ",", "\n", "embedding_size", "=", "config", ".", "hidden_size", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "word_embedding_name", "=", "\"word_embeddings\"", ",", "\n", "use_one_hot_embeddings", "=", "use_one_hot_embeddings", ")", "\n", "\n", "# Add positional embeddings and token type embeddings, then layer", "\n", "# normalize and perform dropout.", "\n", "embedding_output", "=", "embedding_postprocessor", "(", "\n", "input_tensor", "=", "embedding_output", ",", "\n", "use_token_type", "=", "True", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "token_type_vocab_size", "=", "config", ".", "type_vocab_size", ",", "\n", "token_type_embedding_name", "=", "\"token_type_embeddings\"", ",", "\n", "use_position_embeddings", "=", "True", ",", "\n", "position_embedding_name", "=", "\"position_embeddings\"", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "max_position_embeddings", "=", "config", ".", "max_position_embeddings", ",", "\n", "dropout_prob", "=", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "", "return", "embedding_output", ",", "embedding_table", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.bert_attention": [[162, 200], ["copy.deepcopy", "tensorflow.variable_scope", "tensorflow.variable_scope", "modeling.create_attention_mask_from_input_mask", "modeling.transformer_model", "modeling.get_activation"], "function", ["home.repos.pwc.inspect_result.google-research_uda.bert.modeling.create_attention_mask_from_input_mask", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.transformer_model", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.get_activation"], ["", "", "def", "bert_attention", "(", "config", ",", "\n", "is_training", ",", "\n", "input_ids", ",", "\n", "input_mask", ",", "\n", "embedding_output", ",", "\n", "scope", "=", "None", ")", ":", "\n", "\n", "  ", "config", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "if", "not", "is_training", ":", "\n", "    ", "config", ".", "hidden_dropout_prob", "=", "0.0", "\n", "config", ".", "attention_probs_dropout_prob", "=", "0.0", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"bert\"", ",", "scope", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "\"encoder\"", ")", ":", "\n", "# This converts a 2D mask of shape [batch_size, seq_length] to a 3D", "\n", "# mask of shape [batch_size, seq_length, seq_length] which is used", "\n", "# for the attention scores.", "\n", "      ", "attention_mask", "=", "create_attention_mask_from_input_mask", "(", "\n", "input_ids", ",", "input_mask", ")", "\n", "\n", "# Run the stacked transformer.", "\n", "# `sequence_output` shape = [batch_size, seq_length, hidden_size].", "\n", "all_encoder_layers", "=", "transformer_model", "(", "\n", "input_tensor", "=", "embedding_output", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "hidden_size", "=", "config", ".", "hidden_size", ",", "\n", "num_hidden_layers", "=", "config", ".", "num_hidden_layers", ",", "\n", "num_attention_heads", "=", "config", ".", "num_attention_heads", ",", "\n", "intermediate_size", "=", "config", ".", "intermediate_size", ",", "\n", "intermediate_act_fn", "=", "get_activation", "(", "config", ".", "hidden_act", ")", ",", "\n", "hidden_dropout_prob", "=", "config", ".", "hidden_dropout_prob", ",", "\n", "attention_probs_dropout_prob", "=", "config", ".", "attention_probs_dropout_prob", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "do_return_all_layers", "=", "True", ")", "\n", "\n", "", "sequence_output", "=", "all_encoder_layers", "[", "-", "1", "]", "\n", "\n", "return", "sequence_output", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.bert_pooler": [[202, 230], ["copy.deepcopy", "tensorflow.variable_scope", "tensorflow.variable_scope", "tensorflow.squeeze", "tensorflow.layers.dense", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.google-research_uda.bert.modeling.create_initializer"], ["", "", "def", "bert_pooler", "(", "config", ",", "\n", "is_training", ",", "\n", "sequence_output", ",", "\n", "scope", "=", "None", ")", ":", "\n", "\n", "  ", "config", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "if", "not", "is_training", ":", "\n", "    ", "config", ".", "hidden_dropout_prob", "=", "0.0", "\n", "config", ".", "attention_probs_dropout_prob", "=", "0.0", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"bert\"", ",", "scope", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "# The \"pooler\" converts the encoded sequence tensor of shape", "\n", "# [batch_size, seq_length, hidden_size] to a tensor of shape", "\n", "# [batch_size, hidden_size]. This is necessary for segment-level", "\n", "# (or segment-pair-level) classification tasks where we need a fixed", "\n", "# dimensional representation of the segment.", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "\"pooler\"", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token. We assume that this has been pre-trained", "\n", "      ", "clas_rep", "=", "tf", ".", "squeeze", "(", "sequence_output", "[", ":", ",", "0", ":", "1", ",", ":", "]", ",", "axis", "=", "1", ")", "\n", "\n", "pooled_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "clas_rep", ",", "\n", "config", ".", "hidden_size", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "config", ".", "initializer_range", ")", ")", "\n", "\n", "", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.bert_model": [[232, 278], ["modeling.bert_attention", "modeling.bert_pooler", "modeling.bert_embedding"], "function", ["home.repos.pwc.inspect_result.google-research_uda.bert.modeling.bert_attention", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.bert_pooler", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.bert_embedding"], ["", "", "def", "bert_model", "(", "config", ",", "\n", "is_training", ",", "\n", "input_ids", ",", "\n", "input_mask", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "input_embedding", "=", "None", ",", "\n", "output_type", "=", "\"pooled\"", ",", "\n", "use_one_hot_embeddings", "=", "True", ",", "\n", "scope", "=", "None", ")", ":", "\n", "  ", "\"\"\"doc.\"\"\"", "\n", "\n", "assert", "output_type", "in", "[", "\"embedding\"", ",", "\"pooled\"", ",", "\"sequence\"", "]", ",", "(", "\n", "\"Unsupported output type {}\"", ".", "format", "(", "output_type", ")", ")", "\n", "\n", "if", "input_embedding", "is", "None", ":", "\n", "    ", "embedding_output", ",", "embedding_table", "=", "bert_embedding", "(", "\n", "config", ",", "\n", "is_training", ",", "\n", "input_ids", ",", "\n", "input_mask", ",", "\n", "token_type_ids", ",", "\n", "use_one_hot_embeddings", ",", "\n", "scope", ")", "\n", "\n", "if", "output_type", "==", "\"embedding\"", ":", "\n", "      ", "return", "embedding_output", ",", "embedding_table", "\n", "\n", "", "", "sequence_output", "=", "bert_attention", "(", "\n", "config", ",", "\n", "is_training", ",", "\n", "input_ids", ",", "\n", "input_mask", ",", "\n", "embedding_output", ",", "\n", "scope", ")", "\n", "\n", "if", "output_type", "==", "\"sequence\"", ":", "\n", "    ", "return", "sequence_output", "\n", "\n", "", "pooled_output", "=", "bert_pooler", "(", "\n", "config", ",", "\n", "is_training", ",", "\n", "sequence_output", ",", "\n", "scope", ")", "\n", "\n", "if", "output_type", "==", "\"pooled\"", ":", "\n", "    ", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.gelu": [[280, 294], ["tensorflow.erf", "tensorflow.sqrt"], "function", ["None"], ["", "", "def", "gelu", "(", "input_tensor", ")", ":", "\n", "  ", "\"\"\"Gaussian Error Linear Unit.\n\n  This is a smoother version of the RELU.\n  Original paper: https://arxiv.org/abs/1606.08415\n\n  Args:\n    input_tensor: float Tensor to perform activation.\n\n  Returns:\n    `input_tensor` with the GELU activation applied.\n  \"\"\"", "\n", "cdf", "=", "0.5", "*", "(", "1.0", "+", "tf", ".", "erf", "(", "input_tensor", "/", "tf", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "return", "input_tensor", "*", "cdf", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.get_activation": [[296, 332], ["activation_string.lower", "isinstance", "ValueError"], "function", ["None"], ["", "def", "get_activation", "(", "activation_string", ")", ":", "\n", "  ", "\"\"\"Maps a string to a Python function, e.g., \"relu\" => `tf.nn.relu`.\n\n  Args:\n    activation_string: String name of the activation function.\n\n  Returns:\n    A Python function corresponding to the activation function. If\n    `activation_string` is None, empty, or \"linear\",\n    this will return None.\n    If `activation_string` is not a string, it will return `activation_string`.\n\n  Raises:\n    ValueError: The `activation_string` does not correspond to a known\n      activation.\n  \"\"\"", "\n", "\n", "# We assume that anything that's not a string is already an activation", "\n", "# function, so we just return it.", "\n", "if", "not", "isinstance", "(", "activation_string", ",", "(", "str", ",", "unicode", ")", ")", ":", "\n", "    ", "return", "activation_string", "\n", "\n", "", "if", "not", "activation_string", ":", "\n", "    ", "return", "None", "\n", "\n", "", "act", "=", "activation_string", ".", "lower", "(", ")", "\n", "if", "act", "==", "\"linear\"", ":", "\n", "    ", "return", "None", "\n", "", "elif", "act", "==", "\"relu\"", ":", "\n", "    ", "return", "tf", ".", "nn", ".", "relu", "\n", "", "elif", "act", "==", "\"gelu\"", ":", "\n", "    ", "return", "gelu", "\n", "", "elif", "act", "==", "\"tanh\"", ":", "\n", "    ", "return", "tf", ".", "tanh", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Unsupported activation: %s\"", "%", "act", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.dropout": [[334, 350], ["tensorflow.nn.dropout"], "function", ["home.repos.pwc.inspect_result.google-research_uda.bert.modeling.dropout"], ["", "", "def", "dropout", "(", "input_tensor", ",", "dropout_prob", ")", ":", "\n", "  ", "\"\"\"Perform dropout.\n\n  Args:\n    input_tensor: float Tensor.\n    dropout_prob: Python float. The probabiltiy of dropping out a value (NOT of\n      *keeping* a dimension as in `tf.nn.dropout`).\n\n  Returns:\n    A version of `input_tensor` with dropout applied.\n  \"\"\"", "\n", "if", "dropout_prob", "is", "None", "or", "dropout_prob", "==", "0.0", ":", "\n", "    ", "return", "input_tensor", "\n", "\n", "", "output", "=", "tf", ".", "nn", ".", "dropout", "(", "input_tensor", ",", "1.0", "-", "dropout_prob", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.layer_norm": [[352, 356], ["tensorflow.contrib.layers.layer_norm"], "function", ["home.repos.pwc.inspect_result.google-research_uda.bert.modeling.layer_norm"], ["", "def", "layer_norm", "(", "input_tensor", ",", "name", "=", "None", ")", ":", "\n", "  ", "\"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"", "\n", "return", "tf", ".", "contrib", ".", "layers", ".", "layer_norm", "(", "\n", "inputs", "=", "input_tensor", ",", "begin_norm_axis", "=", "-", "1", ",", "begin_params_axis", "=", "-", "1", ",", "scope", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.layer_norm_and_dropout": [[358, 363], ["modeling.layer_norm", "modeling.dropout"], "function", ["home.repos.pwc.inspect_result.google-research_uda.bert.modeling.layer_norm", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.dropout"], ["", "def", "layer_norm_and_dropout", "(", "input_tensor", ",", "dropout_prob", ",", "name", "=", "None", ")", ":", "\n", "  ", "\"\"\"Runs layer normalization followed by dropout.\"\"\"", "\n", "output_tensor", "=", "layer_norm", "(", "input_tensor", ",", "name", ")", "\n", "output_tensor", "=", "dropout", "(", "output_tensor", ",", "dropout_prob", ")", "\n", "return", "output_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.create_initializer": [[365, 368], ["tensorflow.truncated_normal_initializer"], "function", ["None"], ["", "def", "create_initializer", "(", "initializer_range", "=", "0.02", ")", ":", "\n", "  ", "\"\"\"Creates a `truncated_normal_initializer` with the given range.\"\"\"", "\n", "return", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "initializer_range", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.embedding_lookup": [[370, 417], ["tensorflow.get_variable", "modeling.get_shape_list", "tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.reshape", "tensorflow.one_hot", "tensorflow.matmul", "tensorflow.nn.embedding_lookup", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.google-research_uda.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.embedding_lookup", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.create_initializer"], ["", "def", "embedding_lookup", "(", "input_ids", ",", "\n", "vocab_size", ",", "\n", "embedding_size", "=", "128", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "word_embedding_name", "=", "\"word_embeddings\"", ",", "\n", "use_one_hot_embeddings", "=", "False", ")", ":", "\n", "  ", "\"\"\"Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.nn.embedding_lookup()`. One hot is better\n      for TPUs.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  \"\"\"", "\n", "# This function assumes that the input is of shape [batch_size, seq_length,", "\n", "# num_inputs].", "\n", "#", "\n", "# If the input is a 2D tensor of shape [batch_size, seq_length], we", "\n", "# reshape to [batch_size, seq_length, 1].", "\n", "if", "input_ids", ".", "shape", ".", "ndims", "==", "2", ":", "\n", "    ", "input_ids", "=", "tf", ".", "expand_dims", "(", "input_ids", ",", "axis", "=", "[", "-", "1", "]", ")", "\n", "\n", "", "embedding_table", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "word_embedding_name", ",", "\n", "shape", "=", "[", "vocab_size", ",", "embedding_size", "]", ",", "\n", "initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "if", "use_one_hot_embeddings", ":", "\n", "    ", "flat_input_ids", "=", "tf", ".", "reshape", "(", "input_ids", ",", "[", "-", "1", "]", ")", "\n", "one_hot_input_ids", "=", "tf", ".", "one_hot", "(", "flat_input_ids", ",", "depth", "=", "vocab_size", ")", "\n", "output", "=", "tf", ".", "matmul", "(", "one_hot_input_ids", ",", "embedding_table", ")", "\n", "", "else", ":", "\n", "    ", "output", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embedding_table", ",", "input_ids", ")", "\n", "\n", "", "input_shape", "=", "get_shape_list", "(", "input_ids", ")", "\n", "\n", "output", "=", "tf", ".", "reshape", "(", "output", ",", "\n", "input_shape", "[", "0", ":", "-", "1", "]", "+", "[", "input_shape", "[", "-", "1", "]", "*", "embedding_size", "]", ")", "\n", "return", "(", "output", ",", "embedding_table", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.embedding_postprocessor": [[419, 516], ["modeling.get_shape_list", "modeling.layer_norm_and_dropout", "ValueError", "tensorflow.get_variable", "tensorflow.reshape", "tensorflow.one_hot", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.get_variable", "tensorflow.slice", "len", "range", "position_broadcast_shape.extend", "tensorflow.reshape", "ValueError", "layer_norm_and_dropout.shape.as_list", "position_broadcast_shape.append", "modeling.create_initializer", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.google-research_uda.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.layer_norm_and_dropout", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.create_initializer", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.create_initializer"], ["", "def", "embedding_postprocessor", "(", "input_tensor", ",", "\n", "use_token_type", "=", "False", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "token_type_vocab_size", "=", "16", ",", "\n", "token_type_embedding_name", "=", "\"token_type_embeddings\"", ",", "\n", "use_position_embeddings", "=", "True", ",", "\n", "position_embedding_name", "=", "\"position_embeddings\"", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "dropout_prob", "=", "0.1", ")", ":", "\n", "  ", "\"\"\"Performs various post-processing on a word embedding tensor.\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length,\n      embedding_size].\n    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      Must be specified if `use_token_type` is True.\n    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n    token_type_embedding_name: string. The name of the embedding table variable\n      for token type ids.\n    use_position_embeddings: bool. Whether to add position embeddings for the\n      position of each token in the sequence.\n    position_embedding_name: string. The name of the embedding table variable\n      for positional embeddings.\n    initializer_range: float. Range of the weight initialization.\n    max_position_embeddings: int. Maximum sequence length that might ever be\n      used with this model. This can be longer than the sequence length of\n      input_tensor, but cannot be shorter.\n    dropout_prob: float. Dropout probability applied to the final output tensor.\n\n  Returns:\n    float tensor with same shape as `input_tensor`.\n\n  Raises:\n    ValueError: One of the tensor shapes or input values is invalid.\n  \"\"\"", "\n", "input_shape", "=", "get_shape_list", "(", "input_tensor", ",", "expected_rank", "=", "3", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "width", "=", "input_shape", "[", "2", "]", "\n", "\n", "if", "seq_length", ">", "max_position_embeddings", ":", "\n", "    ", "raise", "ValueError", "(", "\"The seq length (%d) cannot be greater than \"", "\n", "\"`max_position_embeddings` (%d)\"", "%", "\n", "(", "seq_length", ",", "max_position_embeddings", ")", ")", "\n", "\n", "", "output", "=", "input_tensor", "\n", "\n", "if", "use_token_type", ":", "\n", "    ", "if", "token_type_ids", "is", "None", ":", "\n", "      ", "raise", "ValueError", "(", "\"`token_type_ids` must be specified if\"", "\n", "\"`use_token_type` is True.\"", ")", "\n", "", "token_type_table", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "token_type_embedding_name", ",", "\n", "shape", "=", "[", "token_type_vocab_size", ",", "width", "]", ",", "\n", "initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "# This vocab will be small so we always do one-hot here, since it is always", "\n", "# faster for a small vocabulary.", "\n", "flat_token_type_ids", "=", "tf", ".", "reshape", "(", "token_type_ids", ",", "[", "-", "1", "]", ")", "\n", "one_hot_ids", "=", "tf", ".", "one_hot", "(", "flat_token_type_ids", ",", "depth", "=", "token_type_vocab_size", ")", "\n", "token_type_embeddings", "=", "tf", ".", "matmul", "(", "one_hot_ids", ",", "token_type_table", ")", "\n", "token_type_embeddings", "=", "tf", ".", "reshape", "(", "token_type_embeddings", ",", "\n", "[", "batch_size", ",", "seq_length", ",", "width", "]", ")", "\n", "output", "+=", "token_type_embeddings", "\n", "\n", "", "if", "use_position_embeddings", ":", "\n", "    ", "full_position_embeddings", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "position_embedding_name", ",", "\n", "shape", "=", "[", "max_position_embeddings", ",", "width", "]", ",", "\n", "initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "# Since the position embedding table is a learned variable, we create it", "\n", "# using a (long) sequence length `max_position_embeddings`. The actual", "\n", "# sequence length might be shorter than this, for faster training of", "\n", "# tasks that do not have long sequences.", "\n", "#", "\n", "# So `full_position_embeddings` is effectively an embedding table", "\n", "# for position [0, 1, 2, ..., max_position_embeddings-1], and the current", "\n", "# sequence has positions [0, 1, 2, ... seq_length-1], so we can just", "\n", "# perform a slice.", "\n", "position_embeddings", "=", "tf", ".", "slice", "(", "full_position_embeddings", ",", "[", "0", ",", "0", "]", ",", "\n", "[", "seq_length", ",", "-", "1", "]", ")", "\n", "num_dims", "=", "len", "(", "output", ".", "shape", ".", "as_list", "(", ")", ")", "\n", "\n", "# Only the last two dimensions are relevant (`seq_length` and `width`), so", "\n", "# we broadcast among the first dimensions, which is typically just", "\n", "# the batch size.", "\n", "position_broadcast_shape", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "num_dims", "-", "2", ")", ":", "\n", "      ", "position_broadcast_shape", ".", "append", "(", "1", ")", "\n", "", "position_broadcast_shape", ".", "extend", "(", "[", "seq_length", ",", "width", "]", ")", "\n", "position_embeddings", "=", "tf", ".", "reshape", "(", "position_embeddings", ",", "\n", "position_broadcast_shape", ")", "\n", "output", "+=", "position_embeddings", "\n", "\n", "", "output", "=", "layer_norm_and_dropout", "(", "output", ",", "dropout_prob", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.create_attention_mask_from_input_mask": [[518, 550], ["modeling.get_shape_list", "modeling.get_shape_list", "tensorflow.cast", "tensorflow.ones", "tensorflow.reshape"], "function", ["home.repos.pwc.inspect_result.google-research_uda.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.get_shape_list"], ["", "def", "create_attention_mask_from_input_mask", "(", "from_tensor", ",", "to_mask", ")", ":", "\n", "  ", "\"\"\"Create 3D attention mask from a 2D tensor mask.\n\n  Args:\n    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n  \"\"\"", "\n", "from_shape", "=", "get_shape_list", "(", "from_tensor", ",", "expected_rank", "=", "[", "2", ",", "3", "]", ")", "\n", "batch_size", "=", "from_shape", "[", "0", "]", "\n", "from_seq_length", "=", "from_shape", "[", "1", "]", "\n", "\n", "to_shape", "=", "get_shape_list", "(", "to_mask", ",", "expected_rank", "=", "2", ")", "\n", "to_seq_length", "=", "to_shape", "[", "1", "]", "\n", "\n", "to_mask", "=", "tf", ".", "cast", "(", "\n", "tf", ".", "reshape", "(", "to_mask", ",", "[", "batch_size", ",", "1", ",", "to_seq_length", "]", ")", ",", "tf", ".", "float32", ")", "\n", "\n", "# We don't assume that `from_tensor` is a mask (although it could be). We", "\n", "# don't actually care if we attend *from* padding tokens (only *to* padding)", "\n", "# tokens so we create a tensor of all ones.", "\n", "#", "\n", "# `broadcast_ones` = [batch_size, from_seq_length, 1]", "\n", "broadcast_ones", "=", "tf", ".", "ones", "(", "\n", "shape", "=", "[", "batch_size", ",", "from_seq_length", ",", "1", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "# Here we broadcast along two dimensions to create the mask.", "\n", "mask", "=", "broadcast_ones", "*", "to_mask", "\n", "\n", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.attention_layer": [[552, 745], ["modeling.get_shape_list", "modeling.get_shape_list", "modeling.reshape_to_matrix", "modeling.reshape_to_matrix", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.layers.dense", "modeling.attention_layer.transpose_for_scores"], "function", ["home.repos.pwc.inspect_result.google-research_uda.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.reshape_to_matrix", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.reshape_to_matrix"], ["", "def", "attention_layer", "(", "from_tensor", ",", "\n", "to_tensor", ",", "\n", "attention_mask", "=", "None", ",", "\n", "num_attention_heads", "=", "1", ",", "\n", "size_per_head", "=", "512", ",", "\n", "query_act", "=", "None", ",", "\n", "key_act", "=", "None", ",", "\n", "value_act", "=", "None", ",", "\n", "attention_probs_dropout_prob", "=", "0.0", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "do_return_2d_tensor", "=", "False", ",", "\n", "batch_size", "=", "None", ",", "\n", "from_seq_length", "=", "None", ",", "\n", "to_seq_length", "=", "None", ")", ":", "\n", "  ", "\"\"\"Performs multi-headed attention from `from_tensor` to `to_tensor`.\n\n  This is an implementation of multi-headed attention based on \"Attention\n  is all you Need\". If `from_tensor` and `to_tensor` are the same, then\n  this is self-attention. Each timestep in `from_tensor` attends to the\n  corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n\n  This function first projects `from_tensor` into a \"query\" tensor and\n  `to_tensor` into \"key\" and \"value\" tensors. These are (effectively) a list\n  of tensors of length `num_attention_heads`, where each tensor is of shape\n  [batch_size, seq_length, size_per_head].\n\n  Then, the query and key tensors are dot-producted and scaled. These are\n  softmaxed to obtain attention probabilities. The value tensors are then\n  interpolated by these probabilities, then concatenated back to a single\n  tensor and returned.\n\n  In practice, the multi-headed attention are done with transposes and\n  reshapes rather than actual separate tensors.\n\n  Args:\n    from_tensor: float Tensor of shape [batch_size, from_seq_length,\n      from_width].\n    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n    attention_mask: (optional) int32 Tensor of shape [batch_size,\n      from_seq_length, to_seq_length]. The values should be 1 or 0. The\n      attention scores will effectively be set to -infinity for any positions in\n      the mask that are 0, and will be unchaged for positions that are 1.\n    num_attention_heads: int. Number of attention heads.\n    size_per_head: int. Size of each attention head.\n    query_act: (optional) Activation function for the query transform.\n    key_act: (optional) Activation function for the key transform.\n    value_act: (optional) Activation function for the value transform.\n    attention_probs_dropout_prob:\n    initializer_range: float. Range of the weight initializer.\n    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n      * from_seq_length, num_attention_heads * size_per_head]. If False, the\n      output will be of shape [batch_size, from_seq_length, num_attention_heads\n      * size_per_head].\n    batch_size: (Optional) int. If the input is 2D, this might be the batch size\n      of the 3D version of the `from_tensor` and `to_tensor`.\n    from_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `from_tensor`.\n    to_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `to_tensor`.\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length,\n      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n      true, this will be of shape [batch_size * from_seq_length,\n      num_attention_heads * size_per_head]).\n\n  Raises:\n    ValueError: Any of the arguments or tensor shapes are invalid.\n  \"\"\"", "\n", "\n", "def", "transpose_for_scores", "(", "input_tensor", ",", "batch_size", ",", "num_attention_heads", ",", "\n", "seq_length", ",", "width", ")", ":", "\n", "    ", "output_tensor", "=", "tf", ".", "reshape", "(", "\n", "input_tensor", ",", "[", "batch_size", ",", "seq_length", ",", "num_attention_heads", ",", "width", "]", ")", "\n", "\n", "output_tensor", "=", "tf", ".", "transpose", "(", "output_tensor", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "return", "output_tensor", "\n", "\n", "", "from_shape", "=", "get_shape_list", "(", "from_tensor", ",", "expected_rank", "=", "[", "2", ",", "3", "]", ")", "\n", "to_shape", "=", "get_shape_list", "(", "to_tensor", ",", "expected_rank", "=", "[", "2", ",", "3", "]", ")", "\n", "\n", "if", "len", "(", "from_shape", ")", "!=", "len", "(", "to_shape", ")", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"The rank of `from_tensor` must match the rank of `to_tensor`.\"", ")", "\n", "\n", "", "if", "len", "(", "from_shape", ")", "==", "3", ":", "\n", "    ", "batch_size", "=", "from_shape", "[", "0", "]", "\n", "from_seq_length", "=", "from_shape", "[", "1", "]", "\n", "to_seq_length", "=", "to_shape", "[", "1", "]", "\n", "", "elif", "len", "(", "from_shape", ")", "==", "2", ":", "\n", "    ", "if", "(", "batch_size", "is", "None", "or", "from_seq_length", "is", "None", "or", "to_seq_length", "is", "None", ")", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "\"When passing in rank 2 tensors to attention_layer, the values \"", "\n", "\"for `batch_size`, `from_seq_length`, and `to_seq_length` \"", "\n", "\"must all be specified.\"", ")", "\n", "\n", "# Scalar dimensions referenced here:", "\n", "#   B = batch size (number of sequences)", "\n", "#   F = `from_tensor` sequence length", "\n", "#   T = `to_tensor` sequence length", "\n", "#   N = `num_attention_heads`", "\n", "#   H = `size_per_head`", "\n", "\n", "", "", "from_tensor_2d", "=", "reshape_to_matrix", "(", "from_tensor", ")", "\n", "to_tensor_2d", "=", "reshape_to_matrix", "(", "to_tensor", ")", "\n", "\n", "# `query_layer` = [B*F, N*H]", "\n", "query_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "from_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation", "=", "query_act", ",", "\n", "name", "=", "\"query\"", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# `key_layer` = [B*T, N*H]", "\n", "key_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "to_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation", "=", "key_act", ",", "\n", "name", "=", "\"key\"", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# `value_layer` = [B*T, N*H]", "\n", "value_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "to_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation", "=", "value_act", ",", "\n", "name", "=", "\"value\"", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# `query_layer` = [B, N, F, H]", "\n", "query_layer", "=", "transpose_for_scores", "(", "query_layer", ",", "batch_size", ",", "\n", "num_attention_heads", ",", "from_seq_length", ",", "\n", "size_per_head", ")", "\n", "\n", "# `key_layer` = [B, N, T, H]", "\n", "key_layer", "=", "transpose_for_scores", "(", "key_layer", ",", "batch_size", ",", "num_attention_heads", ",", "\n", "to_seq_length", ",", "size_per_head", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw", "\n", "# attention scores.", "\n", "# `attention_scores` = [B, N, F, T]", "\n", "attention_scores", "=", "tf", ".", "matmul", "(", "query_layer", ",", "key_layer", ",", "transpose_b", "=", "True", ")", "\n", "attention_scores", "=", "tf", ".", "multiply", "(", "attention_scores", ",", "\n", "1.0", "/", "math", ".", "sqrt", "(", "float", "(", "size_per_head", ")", ")", ")", "\n", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "# `attention_mask` = [B, 1, F, T]", "\n", "    ", "attention_mask", "=", "tf", ".", "expand_dims", "(", "attention_mask", ",", "axis", "=", "[", "1", "]", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "adder", "=", "(", "1.0", "-", "tf", ".", "cast", "(", "attention_mask", ",", "tf", ".", "float32", ")", ")", "*", "-", "10000.0", "\n", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "attention_scores", "+=", "adder", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "# `attention_probs` = [B, N, F, T]", "\n", "", "attention_probs", "=", "tf", ".", "nn", ".", "softmax", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "dropout", "(", "attention_probs", ",", "attention_probs_dropout_prob", ")", "\n", "\n", "# `value_layer` = [B, T, N, H]", "\n", "value_layer", "=", "tf", ".", "reshape", "(", "\n", "value_layer", ",", "\n", "[", "batch_size", ",", "to_seq_length", ",", "num_attention_heads", ",", "size_per_head", "]", ")", "\n", "\n", "# `value_layer` = [B, N, T, H]", "\n", "value_layer", "=", "tf", ".", "transpose", "(", "value_layer", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n", "# `context_layer` = [B, N, F, H]", "\n", "context_layer", "=", "tf", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "\n", "# `context_layer` = [B, F, N, H]", "\n", "context_layer", "=", "tf", ".", "transpose", "(", "context_layer", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n", "if", "do_return_2d_tensor", ":", "\n", "# `context_layer` = [B*F, N*V]", "\n", "    ", "context_layer", "=", "tf", ".", "reshape", "(", "\n", "context_layer", ",", "\n", "[", "batch_size", "*", "from_seq_length", ",", "num_attention_heads", "*", "size_per_head", "]", ")", "\n", "", "else", ":", "\n", "# `context_layer` = [B, F, N*V]", "\n", "    ", "context_layer", "=", "tf", ".", "reshape", "(", "\n", "context_layer", ",", "\n", "[", "batch_size", ",", "from_seq_length", ",", "num_attention_heads", "*", "size_per_head", "]", ")", "\n", "\n", "", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.transformer_model": [[747, 886], ["int", "modeling.get_shape_list", "modeling.reshape_to_matrix", "range", "ValueError", "ValueError", "modeling.reshape_from_matrix", "tensorflow.variable_scope", "modeling.reshape_from_matrix", "final_outputs.append", "tensorflow.variable_scope", "tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.variable_scope", "tensorflow.layers.dense", "modeling.dropout", "modeling.layer_norm", "all_layer_outputs.append", "tensorflow.variable_scope", "modeling.attention_layer", "attention_heads.append", "len", "tensorflow.concat", "tensorflow.variable_scope", "tensorflow.layers.dense", "modeling.dropout", "modeling.layer_norm", "modeling.create_initializer", "modeling.create_initializer", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.google-research_uda.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.reshape_to_matrix", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.reshape_from_matrix", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.reshape_from_matrix", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.dropout", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.layer_norm", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.attention_layer", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.dropout", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.layer_norm", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.create_initializer", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.create_initializer", "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.create_initializer"], ["", "def", "transformer_model", "(", "input_tensor", ",", "\n", "attention_mask", "=", "None", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "intermediate_act_fn", "=", "gelu", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "do_return_all_layers", "=", "False", ")", ":", "\n", "  ", "\"\"\"Multi-headed, multi-layer Transformer from \"Attention is All You Need\".\n\n  This is almost an exact implementation of the original Transformer encoder.\n\n  See the original paper:\n  https://arxiv.org/abs/1706.03762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the \"intermediate\" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  \"\"\"", "\n", "if", "hidden_size", "%", "num_attention_heads", "!=", "0", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "hidden_size", ",", "num_attention_heads", ")", ")", "\n", "\n", "", "attention_head_size", "=", "int", "(", "hidden_size", "/", "num_attention_heads", ")", "\n", "input_shape", "=", "get_shape_list", "(", "input_tensor", ",", "expected_rank", "=", "3", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "input_width", "=", "input_shape", "[", "2", "]", "\n", "\n", "# The Transformer performs sum residuals on all layers so the input needs", "\n", "# to be the same as the hidden size.", "\n", "if", "input_width", "!=", "hidden_size", ":", "\n", "    ", "raise", "ValueError", "(", "\"The width of the input tensor (%d) != hidden size (%d)\"", "%", "\n", "(", "input_width", ",", "hidden_size", ")", ")", "\n", "\n", "# We keep the representation as a 2D tensor to avoid re-shaping it back and", "\n", "# forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on", "\n", "# the GPU/CPU but may not be free on the TPU, so we want to minimize them to", "\n", "# help the optimizer.", "\n", "", "prev_output", "=", "reshape_to_matrix", "(", "input_tensor", ")", "\n", "\n", "all_layer_outputs", "=", "[", "]", "\n", "for", "layer_idx", "in", "range", "(", "num_hidden_layers", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "\"layer_%d\"", "%", "layer_idx", ")", ":", "\n", "      ", "layer_input", "=", "prev_output", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"attention\"", ")", ":", "\n", "        ", "attention_heads", "=", "[", "]", "\n", "with", "tf", ".", "variable_scope", "(", "\"self\"", ")", ":", "\n", "          ", "attention_head", "=", "attention_layer", "(", "\n", "from_tensor", "=", "layer_input", ",", "\n", "to_tensor", "=", "layer_input", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "num_attention_heads", "=", "num_attention_heads", ",", "\n", "size_per_head", "=", "attention_head_size", ",", "\n", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", ",", "\n", "initializer_range", "=", "initializer_range", ",", "\n", "do_return_2d_tensor", "=", "True", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "from_seq_length", "=", "seq_length", ",", "\n", "to_seq_length", "=", "seq_length", ")", "\n", "attention_heads", ".", "append", "(", "attention_head", ")", "\n", "\n", "", "attention_output", "=", "None", "\n", "if", "len", "(", "attention_heads", ")", "==", "1", ":", "\n", "          ", "attention_output", "=", "attention_heads", "[", "0", "]", "\n", "", "else", ":", "\n", "# In the case where we have other sequences, we just concatenate", "\n", "# them to the self-attention head before the projection.", "\n", "          ", "attention_output", "=", "tf", ".", "concat", "(", "attention_heads", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# Run a linear projection of `hidden_size` then add a residual", "\n", "# with `layer_input`.", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"output\"", ")", ":", "\n", "          ", "attention_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "attention_output", ",", "\n", "hidden_size", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "attention_output", "=", "dropout", "(", "attention_output", ",", "hidden_dropout_prob", ")", "\n", "attention_output", "=", "layer_norm", "(", "attention_output", "+", "layer_input", ")", "\n", "\n", "# The activation is only applied to the \"intermediate\" hidden layer.", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"intermediate\"", ")", ":", "\n", "        ", "intermediate_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "attention_output", ",", "\n", "intermediate_size", ",", "\n", "activation", "=", "intermediate_act_fn", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# Down-project back to `hidden_size` then add the residual.", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"output\"", ")", ":", "\n", "        ", "layer_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "intermediate_output", ",", "\n", "hidden_size", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "layer_output", "=", "dropout", "(", "layer_output", ",", "hidden_dropout_prob", ")", "\n", "layer_output", "=", "layer_norm", "(", "layer_output", "+", "attention_output", ")", "\n", "prev_output", "=", "layer_output", "\n", "all_layer_outputs", ".", "append", "(", "layer_output", ")", "\n", "\n", "", "", "", "if", "do_return_all_layers", ":", "\n", "    ", "final_outputs", "=", "[", "]", "\n", "for", "layer_output", "in", "all_layer_outputs", ":", "\n", "      ", "final_output", "=", "reshape_from_matrix", "(", "layer_output", ",", "input_shape", ")", "\n", "final_outputs", ".", "append", "(", "final_output", ")", "\n", "", "return", "final_outputs", "\n", "", "else", ":", "\n", "    ", "final_output", "=", "reshape_from_matrix", "(", "prev_output", ",", "input_shape", ")", "\n", "return", "final_output", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.get_shape_list": [[888, 923], ["tensor.shape.as_list", "enumerate", "tensorflow.shape", "modeling.assert_rank", "non_static_indexes.append"], "function", ["home.repos.pwc.inspect_result.google-research_uda.bert.modeling.assert_rank"], ["", "", "def", "get_shape_list", "(", "tensor", ",", "expected_rank", "=", "None", ",", "name", "=", "None", ")", ":", "\n", "  ", "\"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n\n  Args:\n    tensor: A tf.Tensor object to find the shape of.\n    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n      specified and the `tensor` has a different rank, and exception will be\n      thrown.\n    name: Optional name of the tensor for the error message.\n\n  Returns:\n    A list of dimensions of the shape of tensor. All static dimensions will\n    be returned as python integers, and dynamic dimensions will be returned\n    as tf.Tensor scalars.\n  \"\"\"", "\n", "if", "name", "is", "None", ":", "\n", "    ", "name", "=", "tensor", ".", "name", "\n", "\n", "", "if", "expected_rank", "is", "not", "None", ":", "\n", "    ", "assert_rank", "(", "tensor", ",", "expected_rank", ",", "name", ")", "\n", "\n", "", "shape", "=", "tensor", ".", "shape", ".", "as_list", "(", ")", "\n", "\n", "non_static_indexes", "=", "[", "]", "\n", "for", "(", "index", ",", "dim", ")", "in", "enumerate", "(", "shape", ")", ":", "\n", "    ", "if", "dim", "is", "None", ":", "\n", "      ", "non_static_indexes", ".", "append", "(", "index", ")", "\n", "\n", "", "", "if", "not", "non_static_indexes", ":", "\n", "    ", "return", "shape", "\n", "\n", "", "dyn_shape", "=", "tf", ".", "shape", "(", "tensor", ")", "\n", "for", "index", "in", "non_static_indexes", ":", "\n", "    ", "shape", "[", "index", "]", "=", "dyn_shape", "[", "index", "]", "\n", "", "return", "shape", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.reshape_to_matrix": [[925, 937], ["tensorflow.reshape", "ValueError"], "function", ["None"], ["", "def", "reshape_to_matrix", "(", "input_tensor", ")", ":", "\n", "  ", "\"\"\"Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).\"\"\"", "\n", "ndims", "=", "input_tensor", ".", "shape", ".", "ndims", "\n", "if", "ndims", "<", "2", ":", "\n", "    ", "raise", "ValueError", "(", "\"Input tensor must have at least rank 2. Shape = %s\"", "%", "\n", "(", "input_tensor", ".", "shape", ")", ")", "\n", "", "if", "ndims", "==", "2", ":", "\n", "    ", "return", "input_tensor", "\n", "\n", "", "width", "=", "input_tensor", ".", "shape", "[", "-", "1", "]", "\n", "output_tensor", "=", "tf", ".", "reshape", "(", "input_tensor", ",", "[", "-", "1", ",", "width", "]", ")", "\n", "return", "output_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.reshape_from_matrix": [[939, 950], ["modeling.get_shape_list", "tensorflow.reshape", "len"], "function", ["home.repos.pwc.inspect_result.google-research_uda.bert.modeling.get_shape_list"], ["", "def", "reshape_from_matrix", "(", "output_tensor", ",", "orig_shape_list", ")", ":", "\n", "  ", "\"\"\"Reshapes a rank 2 tensor back to its original rank >= 2 tensor.\"\"\"", "\n", "if", "len", "(", "orig_shape_list", ")", "==", "2", ":", "\n", "    ", "return", "output_tensor", "\n", "\n", "", "output_shape", "=", "get_shape_list", "(", "output_tensor", ")", "\n", "\n", "orig_dims", "=", "orig_shape_list", "[", "0", ":", "-", "1", "]", "\n", "width", "=", "output_shape", "[", "-", "1", "]", "\n", "\n", "return", "tf", ".", "reshape", "(", "output_tensor", ",", "orig_dims", "+", "[", "width", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.modeling.assert_rank": [[952, 980], ["isinstance", "ValueError", "tensorflow.get_variable_scope", "str", "str"], "function", ["None"], ["", "def", "assert_rank", "(", "tensor", ",", "expected_rank", ",", "name", "=", "None", ")", ":", "\n", "  ", "\"\"\"Raises an exception if the tensor rank is not of the expected rank.\n\n  Args:\n    tensor: A tf.Tensor to check the rank of.\n    expected_rank: Python integer or list of integers, expected rank.\n    name: Optional name of the tensor for the error message.\n\n  Raises:\n    ValueError: If the expected shape doesn't match the actual shape.\n  \"\"\"", "\n", "if", "name", "is", "None", ":", "\n", "    ", "name", "=", "tensor", ".", "name", "\n", "\n", "", "expected_rank_dict", "=", "{", "}", "\n", "if", "isinstance", "(", "expected_rank", ",", "(", "int", ",", "long", ")", ")", ":", "\n", "    ", "expected_rank_dict", "[", "expected_rank", "]", "=", "True", "\n", "", "else", ":", "\n", "    ", "for", "x", "in", "expected_rank", ":", "\n", "      ", "expected_rank_dict", "[", "x", "]", "=", "True", "\n", "\n", "", "", "actual_rank", "=", "tensor", ".", "shape", ".", "ndims", "\n", "if", "actual_rank", "not", "in", "expected_rank_dict", ":", "\n", "    ", "scope_name", "=", "tf", ".", "get_variable_scope", "(", ")", ".", "name", "\n", "raise", "ValueError", "(", "\n", "\"For the tensor `%s` in scope `%s`, the actual rank \"", "\n", "\"`%d` (shape = %s) is not equal to the expected rank `%s`\"", "%", "\n", "(", "name", ",", "scope_name", ",", "actual_rank", ",", "str", "(", "tensor", ".", "shape", ")", ",", "str", "(", "expected_rank", ")", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.google-research_uda.bert.optimization.AdamWeightDecayOptimizer.__init__": [[31, 48], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.TransformT.__init__"], ["def", "__init__", "(", "self", ",", "\n", "learning_rate", ",", "\n", "weight_decay_rate", "=", "0.0", ",", "\n", "beta_1", "=", "0.9", ",", "\n", "beta_2", "=", "0.999", ",", "\n", "epsilon", "=", "1e-6", ",", "\n", "exclude_from_weight_decay", "=", "None", ",", "\n", "name", "=", "\"AdamWeightDecayOptimizer\"", ")", ":", "\n", "    ", "\"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"", "\n", "super", "(", "AdamWeightDecayOptimizer", ",", "self", ")", ".", "__init__", "(", "False", ",", "name", ")", "\n", "\n", "self", ".", "learning_rate", "=", "learning_rate", "\n", "self", ".", "weight_decay_rate", "=", "weight_decay_rate", "\n", "self", ".", "beta_1", "=", "beta_1", "\n", "self", ".", "beta_2", "=", "beta_2", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "exclude_from_weight_decay", "=", "exclude_from_weight_decay", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.optimization.AdamWeightDecayOptimizer.apply_gradients": [[49, 99], ["tensorflow.group", "optimization.AdamWeightDecayOptimizer._get_variable_name", "tensorflow.get_variable", "tensorflow.get_variable", "optimization.AdamWeightDecayOptimizer._do_use_weight_decay", "assignments.extend", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.multiply", "param.shape.as_list", "tensorflow.zeros_initializer", "param.shape.as_list", "tensorflow.zeros_initializer", "tensorflow.square", "tensorflow.sqrt", "param.assign", "tensorflow.get_variable.assign", "tensorflow.get_variable.assign"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.bert.optimization.AdamWeightDecayOptimizer._get_variable_name", "home.repos.pwc.inspect_result.google-research_uda.bert.multi_gpu_optimizer.AdamWeightDecayOptimizer._do_use_weight_decay"], ["", "def", "apply_gradients", "(", "self", ",", "grads_and_vars", ",", "global_step", "=", "None", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "assignments", "=", "[", "]", "\n", "for", "(", "grad", ",", "param", ")", "in", "grads_and_vars", ":", "\n", "      ", "if", "grad", "is", "None", "or", "param", "is", "None", ":", "\n", "        ", "continue", "\n", "\n", "", "param_name", "=", "self", ".", "_get_variable_name", "(", "param", ".", "name", ")", "\n", "\n", "m", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "param_name", "+", "\"/adam_m\"", ",", "\n", "shape", "=", "param", ".", "shape", ".", "as_list", "(", ")", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "trainable", "=", "False", ",", "\n", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "v", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "param_name", "+", "\"/adam_v\"", ",", "\n", "shape", "=", "param", ".", "shape", ".", "as_list", "(", ")", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "trainable", "=", "False", ",", "\n", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "\n", "# Standard Adam update.", "\n", "next_m", "=", "(", "\n", "tf", ".", "multiply", "(", "self", ".", "beta_1", ",", "m", ")", "+", "tf", ".", "multiply", "(", "1.0", "-", "self", ".", "beta_1", ",", "grad", ")", ")", "\n", "next_v", "=", "(", "\n", "tf", ".", "multiply", "(", "self", ".", "beta_2", ",", "v", ")", "+", "tf", ".", "multiply", "(", "1.0", "-", "self", ".", "beta_2", ",", "\n", "tf", ".", "square", "(", "grad", ")", ")", ")", "\n", "\n", "update", "=", "next_m", "/", "(", "tf", ".", "sqrt", "(", "next_v", ")", "+", "self", ".", "epsilon", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want ot decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "if", "self", ".", "_do_use_weight_decay", "(", "param_name", ")", ":", "\n", "        ", "update", "+=", "self", ".", "weight_decay_rate", "*", "param", "\n", "\n", "", "update_with_lr", "=", "self", ".", "learning_rate", "*", "update", "\n", "\n", "next_param", "=", "param", "-", "update_with_lr", "\n", "\n", "assignments", ".", "extend", "(", "\n", "[", "param", ".", "assign", "(", "next_param", ")", ",", "\n", "m", ".", "assign", "(", "next_m", ")", ",", "\n", "v", ".", "assign", "(", "next_v", ")", "]", ")", "\n", "", "return", "tf", ".", "group", "(", "*", "assignments", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.optimization.AdamWeightDecayOptimizer._do_use_weight_decay": [[100, 109], ["re.search"], "methods", ["None"], ["", "def", "_do_use_weight_decay", "(", "self", ",", "param_name", ")", ":", "\n", "    ", "\"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"", "\n", "if", "not", "self", ".", "weight_decay_rate", ":", "\n", "      ", "return", "False", "\n", "", "if", "self", ".", "exclude_from_weight_decay", ":", "\n", "      ", "for", "r", "in", "self", ".", "exclude_from_weight_decay", ":", "\n", "        ", "if", "re", ".", "search", "(", "r", ",", "param_name", ")", "is", "not", "None", ":", "\n", "          ", "return", "False", "\n", "", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.optimization.AdamWeightDecayOptimizer._get_variable_name": [[110, 116], ["re.match", "re.match.group"], "methods", ["None"], ["", "def", "_get_variable_name", "(", "self", ",", "param_name", ")", ":", "\n", "    ", "\"\"\"Get the variable name from the tensor name.\"\"\"", "\n", "m", "=", "re", ".", "match", "(", "\"^(.*):\\\\d+$\"", ",", "param_name", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "      ", "param_name", "=", "m", ".", "group", "(", "1", ")", "\n", "", "return", "param_name", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.optimization.get_adam_optimizer": [[118, 134], ["optimization.AdamWeightDecayOptimizer", "tensorflow.contrib.tpu.CrossShardOptimizer"], "function", ["None"], ["", "", "def", "get_adam_optimizer", "(", "learning_rate", ",", "use_tpu", ")", ":", "\n", "  ", "\"\"\"get adam optimizer.\"\"\"", "\n", "# It is recommended that you use this optimizer for fine tuning, since this", "\n", "# is how the model was trained (note that the Adam m/v variables are NOT", "\n", "# loaded from init_checkpoint.)", "\n", "optimizer", "=", "AdamWeightDecayOptimizer", "(", "\n", "learning_rate", "=", "learning_rate", ",", "\n", "weight_decay_rate", "=", "0.01", ",", "\n", "beta_1", "=", "0.9", ",", "\n", "beta_2", "=", "0.999", ",", "\n", "epsilon", "=", "1e-6", ",", "\n", "exclude_from_weight_decay", "=", "[", "\"LayerNorm\"", ",", "\"layer_norm\"", ",", "\"bias\"", "]", ")", "\n", "\n", "if", "use_tpu", ":", "\n", "    ", "optimizer", "=", "tf", ".", "contrib", ".", "tpu", ".", "CrossShardOptimizer", "(", "optimizer", ")", "\n", "", "return", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.optimization.create_optimizer": [[136, 179], ["tensorflow.constant", "tensorflow.train.polynomial_decay", "tensorflow.trainable_variables", "tensorflow.gradients", "tensorflow.clip_by_global_norm", "optimization.get_adam_optimizer", "get_adam_optimizer.apply_gradients", "tensorflow.group", "tensorflow.cast", "tensorflow.constant", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "zip", "global_step.assign"], "function", ["home.repos.pwc.inspect_result.google-research_uda.bert.optimization.get_adam_optimizer", "home.repos.pwc.inspect_result.google-research_uda.bert.optimization.AdamWeightDecayOptimizer.apply_gradients"], ["", "def", "create_optimizer", "(", "loss", ",", "init_lr", ",", "num_train_steps", ",", "num_warmup_steps", ",", "\n", "use_tpu", ",", "clip_norm", ",", "global_step", ")", ":", "\n", "  ", "\"\"\"Creates an optimizer training op.\"\"\"", "\n", "\n", "learning_rate", "=", "tf", ".", "constant", "(", "value", "=", "init_lr", ",", "shape", "=", "[", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "# Implements linear decay of the learning rate.", "\n", "learning_rate", "=", "tf", ".", "train", ".", "polynomial_decay", "(", "\n", "learning_rate", ",", "\n", "global_step", ",", "\n", "num_train_steps", ",", "\n", "end_learning_rate", "=", "0.0", ",", "\n", "power", "=", "1.0", ",", "\n", "cycle", "=", "False", ")", "\n", "\n", "# Implements linear warmup. I.e., if global_step < num_warmup_steps, the", "\n", "# learning rate will be `global_step/num_warmup_steps * init_lr`.", "\n", "if", "num_warmup_steps", ":", "\n", "    ", "global_steps_int", "=", "tf", ".", "cast", "(", "global_step", ",", "tf", ".", "int32", ")", "\n", "warmup_steps_int", "=", "tf", ".", "constant", "(", "num_warmup_steps", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "global_steps_float", "=", "tf", ".", "cast", "(", "global_steps_int", ",", "tf", ".", "float32", ")", "\n", "warmup_steps_float", "=", "tf", ".", "cast", "(", "warmup_steps_int", ",", "tf", ".", "float32", ")", "\n", "\n", "warmup_percent_done", "=", "global_steps_float", "/", "warmup_steps_float", "\n", "warmup_learning_rate", "=", "init_lr", "*", "warmup_percent_done", "\n", "\n", "is_warmup", "=", "tf", ".", "cast", "(", "global_steps_int", "<", "warmup_steps_int", ",", "tf", ".", "float32", ")", "\n", "learning_rate", "=", "(", "\n", "(", "1.0", "-", "is_warmup", ")", "*", "learning_rate", "+", "is_warmup", "*", "warmup_learning_rate", ")", "\n", "\n", "", "tvars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "grads", "=", "tf", ".", "gradients", "(", "loss", ",", "tvars", ")", "\n", "# the model was pre-trained with grad clip 1.0.", "\n", "(", "grads", ",", "_", ")", "=", "tf", ".", "clip_by_global_norm", "(", "grads", ",", "clip_norm", "=", "clip_norm", ")", "\n", "\n", "optimizer", "=", "get_adam_optimizer", "(", "learning_rate", ",", "use_tpu", ")", "\n", "train_op", "=", "optimizer", ".", "apply_gradients", "(", "\n", "zip", "(", "grads", ",", "tvars", ")", ",", "global_step", "=", "global_step", ")", "\n", "\n", "new_global_step", "=", "global_step", "+", "1", "\n", "train_op", "=", "tf", ".", "group", "(", "train_op", ",", "[", "global_step", ".", "assign", "(", "new_global_step", ")", "]", ")", "\n", "return", "train_op", ",", "learning_rate", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.multi_gpu_optimizer.AdamWeightDecayOptimizer.__init__": [[37, 55], ["tensorflow.python.training.optimizer.Optimizer.__init__"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.TransformT.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "learning_rate", ",", "\n", "weight_decay_rate", "=", "0.0", ",", "\n", "beta_1", "=", "0.9", ",", "\n", "beta_2", "=", "0.999", ",", "\n", "epsilon", "=", "1e-6", ",", "\n", "exclude_from_weight_decay", "=", "None", ",", "\n", "name", "=", "\"AdamWeightDecayOptimizer\"", ")", ":", "\n", "    ", "\"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"", "\n", "super", "(", "AdamWeightDecayOptimizer", ",", "self", ")", ".", "__init__", "(", "False", ",", "name", ")", "\n", "\n", "self", ".", "learning_rate", "=", "learning_rate", "\n", "self", ".", "weight_decay_rate", "=", "weight_decay_rate", "\n", "self", ".", "beta_1", "=", "beta_1", "\n", "self", ".", "beta_2", "=", "beta_2", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "exclude_from_weight_decay", "=", "exclude_from_weight_decay", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.multi_gpu_optimizer.AdamWeightDecayOptimizer._prepare": [[56, 64], ["tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.python.framework.ops.convert_to_tensor", "tensorflow.python.framework.ops.convert_to_tensor"], "methods", ["None"], ["", "def", "_prepare", "(", "self", ")", ":", "\n", "    ", "self", ".", "learning_rate_t", "=", "ops", ".", "convert_to_tensor", "(", "\n", "self", ".", "learning_rate", ",", "name", "=", "'learning_rate'", ")", "\n", "self", ".", "weight_decay_rate_t", "=", "ops", ".", "convert_to_tensor", "(", "\n", "self", ".", "weight_decay_rate", ",", "name", "=", "'weight_decay_rate'", ")", "\n", "self", ".", "beta_1_t", "=", "ops", ".", "convert_to_tensor", "(", "self", ".", "beta_1", ",", "name", "=", "'beta_1'", ")", "\n", "self", ".", "beta_2_t", "=", "ops", ".", "convert_to_tensor", "(", "self", ".", "beta_2", ",", "name", "=", "'beta_2'", ")", "\n", "self", ".", "epsilon_t", "=", "ops", ".", "convert_to_tensor", "(", "self", ".", "epsilon", ",", "name", "=", "'epsilon'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.multi_gpu_optimizer.AdamWeightDecayOptimizer._create_slots": [[65, 69], ["multi_gpu_optimizer.AdamWeightDecayOptimizer._zeros_slot", "multi_gpu_optimizer.AdamWeightDecayOptimizer._zeros_slot"], "methods", ["None"], ["", "def", "_create_slots", "(", "self", ",", "var_list", ")", ":", "\n", "    ", "for", "v", "in", "var_list", ":", "\n", "      ", "self", ".", "_zeros_slot", "(", "v", ",", "'m'", ",", "self", ".", "_name", ")", "\n", "self", ".", "_zeros_slot", "(", "v", ",", "'v'", ",", "self", ".", "_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.multi_gpu_optimizer.AdamWeightDecayOptimizer._apply_dense": [[70, 102], ["tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "multi_gpu_optimizer.AdamWeightDecayOptimizer.get_slot", "multi_gpu_optimizer.AdamWeightDecayOptimizer.get_slot", "multi_gpu_optimizer.AdamWeightDecayOptimizer._do_use_weight_decay", "tensorflow.python.ops.control_flow_ops.group", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.square", "tensorflow.sqrt", "var.assign", "multi_gpu_optimizer.AdamWeightDecayOptimizer.assign", "multi_gpu_optimizer.AdamWeightDecayOptimizer.assign"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.bert.multi_gpu_optimizer.AdamWeightDecayOptimizer._do_use_weight_decay"], ["", "", "def", "_apply_dense", "(", "self", ",", "grad", ",", "var", ")", ":", "\n", "    ", "learning_rate_t", "=", "math_ops", ".", "cast", "(", "\n", "self", ".", "learning_rate_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "beta_1_t", "=", "math_ops", ".", "cast", "(", "self", ".", "beta_1_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "beta_2_t", "=", "math_ops", ".", "cast", "(", "self", ".", "beta_2_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "epsilon_t", "=", "math_ops", ".", "cast", "(", "self", ".", "epsilon_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "weight_decay_rate_t", "=", "math_ops", ".", "cast", "(", "\n", "self", ".", "weight_decay_rate_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "\n", "m", "=", "self", ".", "get_slot", "(", "var", ",", "'m'", ")", "\n", "v", "=", "self", ".", "get_slot", "(", "var", ",", "'v'", ")", "\n", "\n", "# Standard Adam update.", "\n", "next_m", "=", "(", "\n", "tf", ".", "multiply", "(", "beta_1_t", ",", "m", ")", "+", "\n", "tf", ".", "multiply", "(", "1.0", "-", "beta_1_t", ",", "grad", ")", ")", "\n", "next_v", "=", "(", "\n", "tf", ".", "multiply", "(", "beta_2_t", ",", "v", ")", "+", "tf", ".", "multiply", "(", "1.0", "-", "beta_2_t", ",", "\n", "tf", ".", "square", "(", "grad", ")", ")", ")", "\n", "\n", "update", "=", "next_m", "/", "(", "tf", ".", "sqrt", "(", "next_v", ")", "+", "epsilon_t", ")", "\n", "\n", "if", "self", ".", "_do_use_weight_decay", "(", "var", ".", "name", ")", ":", "\n", "      ", "update", "+=", "weight_decay_rate_t", "*", "var", "\n", "\n", "", "update_with_lr", "=", "learning_rate_t", "*", "update", "\n", "\n", "next_param", "=", "var", "-", "update_with_lr", "\n", "\n", "return", "control_flow_ops", ".", "group", "(", "*", "[", "var", ".", "assign", "(", "next_param", ")", ",", "\n", "m", ".", "assign", "(", "next_m", ")", ",", "\n", "v", ".", "assign", "(", "next_v", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.multi_gpu_optimizer.AdamWeightDecayOptimizer._resource_apply_dense": [[103, 135], ["tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "multi_gpu_optimizer.AdamWeightDecayOptimizer.get_slot", "multi_gpu_optimizer.AdamWeightDecayOptimizer.get_slot", "multi_gpu_optimizer.AdamWeightDecayOptimizer._do_use_weight_decay", "tensorflow.python.ops.control_flow_ops.group", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.square", "tensorflow.sqrt", "var.assign", "multi_gpu_optimizer.AdamWeightDecayOptimizer.assign", "multi_gpu_optimizer.AdamWeightDecayOptimizer.assign"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.bert.multi_gpu_optimizer.AdamWeightDecayOptimizer._do_use_weight_decay"], ["", "def", "_resource_apply_dense", "(", "self", ",", "grad", ",", "var", ")", ":", "\n", "    ", "learning_rate_t", "=", "math_ops", ".", "cast", "(", "\n", "self", ".", "learning_rate_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "beta_1_t", "=", "math_ops", ".", "cast", "(", "self", ".", "beta_1_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "beta_2_t", "=", "math_ops", ".", "cast", "(", "self", ".", "beta_2_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "epsilon_t", "=", "math_ops", ".", "cast", "(", "self", ".", "epsilon_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "weight_decay_rate_t", "=", "math_ops", ".", "cast", "(", "\n", "self", ".", "weight_decay_rate_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "\n", "m", "=", "self", ".", "get_slot", "(", "var", ",", "'m'", ")", "\n", "v", "=", "self", ".", "get_slot", "(", "var", ",", "'v'", ")", "\n", "\n", "# Standard Adam update.", "\n", "next_m", "=", "(", "\n", "tf", ".", "multiply", "(", "beta_1_t", ",", "m", ")", "+", "\n", "tf", ".", "multiply", "(", "1.0", "-", "beta_1_t", ",", "grad", ")", ")", "\n", "next_v", "=", "(", "\n", "tf", ".", "multiply", "(", "beta_2_t", ",", "v", ")", "+", "tf", ".", "multiply", "(", "1.0", "-", "beta_2_t", ",", "\n", "tf", ".", "square", "(", "grad", ")", ")", ")", "\n", "\n", "update", "=", "next_m", "/", "(", "tf", ".", "sqrt", "(", "next_v", ")", "+", "epsilon_t", ")", "\n", "\n", "if", "self", ".", "_do_use_weight_decay", "(", "var", ".", "name", ")", ":", "\n", "      ", "update", "+=", "weight_decay_rate_t", "*", "var", "\n", "\n", "", "update_with_lr", "=", "learning_rate_t", "*", "update", "\n", "\n", "next_param", "=", "var", "-", "update_with_lr", "\n", "\n", "return", "control_flow_ops", ".", "group", "(", "*", "[", "var", ".", "assign", "(", "next_param", ")", ",", "\n", "m", ".", "assign", "(", "next_m", ")", ",", "\n", "v", ".", "assign", "(", "next_v", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.multi_gpu_optimizer.AdamWeightDecayOptimizer._apply_sparse_shared": [[136, 171], ["tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "tensorflow.python.ops.math_ops.cast", "multi_gpu_optimizer.AdamWeightDecayOptimizer.get_slot", "multi_gpu_optimizer.AdamWeightDecayOptimizer.get_slot", "tensorflow.python.ops.state_ops.assign", "tensorflow.python.ops.state_ops.assign", "multi_gpu_optimizer.AdamWeightDecayOptimizer._do_use_weight_decay", "tensorflow.python.ops.state_ops.assign_sub", "tensorflow.python.ops.control_flow_ops.group", "tensorflow.python.framework.ops.control_dependencies", "scatter_add", "tensorflow.python.framework.ops.control_dependencies", "scatter_add", "tensorflow.python.ops.math_ops.sqrt"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.bert.multi_gpu_optimizer.AdamWeightDecayOptimizer._do_use_weight_decay"], ["", "def", "_apply_sparse_shared", "(", "self", ",", "grad", ",", "var", ",", "indices", ",", "scatter_add", ")", ":", "\n", "    ", "learning_rate_t", "=", "math_ops", ".", "cast", "(", "\n", "self", ".", "learning_rate_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "beta_1_t", "=", "math_ops", ".", "cast", "(", "self", ".", "beta_1_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "beta_2_t", "=", "math_ops", ".", "cast", "(", "self", ".", "beta_2_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "epsilon_t", "=", "math_ops", ".", "cast", "(", "self", ".", "epsilon_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "weight_decay_rate_t", "=", "math_ops", ".", "cast", "(", "\n", "self", ".", "weight_decay_rate_t", ",", "var", ".", "dtype", ".", "base_dtype", ")", "\n", "\n", "m", "=", "self", ".", "get_slot", "(", "var", ",", "'m'", ")", "\n", "v", "=", "self", ".", "get_slot", "(", "var", ",", "'v'", ")", "\n", "\n", "m_t", "=", "state_ops", ".", "assign", "(", "m", ",", "m", "*", "beta_1_t", ",", "\n", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "\n", "m_scaled_g_values", "=", "grad", "*", "(", "1", "-", "beta_1_t", ")", "\n", "with", "ops", ".", "control_dependencies", "(", "[", "m_t", "]", ")", ":", "\n", "      ", "m_t", "=", "scatter_add", "(", "m", ",", "indices", ",", "m_scaled_g_values", ")", "\n", "\n", "", "v_scaled_g_values", "=", "(", "grad", "*", "grad", ")", "*", "(", "1", "-", "beta_2_t", ")", "\n", "v_t", "=", "state_ops", ".", "assign", "(", "v", ",", "v", "*", "beta_2_t", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "with", "ops", ".", "control_dependencies", "(", "[", "v_t", "]", ")", ":", "\n", "      ", "v_t", "=", "scatter_add", "(", "v", ",", "indices", ",", "v_scaled_g_values", ")", "\n", "\n", "", "update", "=", "m_t", "/", "(", "math_ops", ".", "sqrt", "(", "v_t", ")", "+", "epsilon_t", ")", "\n", "\n", "if", "self", ".", "_do_use_weight_decay", "(", "var", ".", "name", ")", ":", "\n", "      ", "update", "+=", "weight_decay_rate_t", "*", "var", "\n", "\n", "", "update_with_lr", "=", "learning_rate_t", "*", "update", "\n", "\n", "var_update", "=", "state_ops", ".", "assign_sub", "(", "var", ",", "\n", "update_with_lr", ",", "\n", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "return", "control_flow_ops", ".", "group", "(", "*", "[", "var_update", ",", "m_t", ",", "v_t", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.multi_gpu_optimizer.AdamWeightDecayOptimizer._apply_sparse": [[172, 177], ["multi_gpu_optimizer.AdamWeightDecayOptimizer._apply_sparse_shared", "tensorflow.python.ops.state_ops.scatter_add"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.bert.multi_gpu_optimizer.AdamWeightDecayOptimizer._apply_sparse_shared"], ["", "def", "_apply_sparse", "(", "self", ",", "grad", ",", "var", ")", ":", "\n", "    ", "return", "self", ".", "_apply_sparse_shared", "(", "\n", "grad", ".", "values", ",", "var", ",", "grad", ".", "indices", ",", "\n", "lambda", "x", ",", "i", ",", "v", ":", "state_ops", ".", "scatter_add", "(", "# pylint: disable=g-long-lambda", "\n", "x", ",", "i", ",", "v", ",", "use_locking", "=", "self", ".", "_use_locking", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.multi_gpu_optimizer.AdamWeightDecayOptimizer._resource_scatter_add": [[178, 183], ["tensorflow.python.framework.ops.control_dependencies", "x.value", "tensorflow.python.ops.resource_variable_ops.resource_scatter_add"], "methods", ["None"], ["", "def", "_resource_scatter_add", "(", "self", ",", "x", ",", "i", ",", "v", ")", ":", "\n", "    ", "with", "ops", ".", "control_dependencies", "(", "\n", "[", "resource_variable_ops", ".", "resource_scatter_add", "(", "\n", "x", ".", "handle", ",", "i", ",", "v", ")", "]", ")", ":", "\n", "      ", "return", "x", ".", "value", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.multi_gpu_optimizer.AdamWeightDecayOptimizer._resource_apply_sparse": [[184, 187], ["multi_gpu_optimizer.AdamWeightDecayOptimizer._apply_sparse_shared"], "methods", ["home.repos.pwc.inspect_result.google-research_uda.bert.multi_gpu_optimizer.AdamWeightDecayOptimizer._apply_sparse_shared"], ["", "", "def", "_resource_apply_sparse", "(", "self", ",", "grad", ",", "var", ",", "indices", ")", ":", "\n", "    ", "return", "self", ".", "_apply_sparse_shared", "(", "\n", "grad", ",", "var", ",", "indices", ",", "self", ".", "_resource_scatter_add", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.bert.multi_gpu_optimizer.AdamWeightDecayOptimizer._do_use_weight_decay": [[188, 197], ["re.search"], "methods", ["None"], ["", "def", "_do_use_weight_decay", "(", "self", ",", "param_name", ")", ":", "\n", "    ", "\"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"", "\n", "if", "not", "self", ".", "weight_decay_rate", ":", "\n", "      ", "return", "False", "\n", "", "if", "self", ".", "exclude_from_weight_decay", ":", "\n", "      ", "for", "r", "in", "self", ".", "exclude_from_weight_decay", ":", "\n", "        ", "if", "re", ".", "search", "(", "r", ",", "param_name", ")", "is", "not", "None", ":", "\n", "          ", "return", "False", "\n", "", "", "", "return", "True", "\n", "", "", ""]], "home.repos.pwc.inspect_result.google-research_uda.image.main.get_tsa_threshold": [[178, 191], ["tensorflow.to_float", "tensorflow.to_float", "tensorflow.exp", "tensorflow.exp"], "function", ["None"], ["\n", "processor", "=", "raw_data_utils", ".", "get_processor", "(", "FLAGS", ".", "task_name", ")", "\n", "label_list", "=", "processor", ".", "get_labels", "(", ")", "\n", "\n", "bert_config", "=", "modeling", ".", "BertConfig", ".", "from_json_file", "(", "\n", "FLAGS", ".", "bert_config_file", ",", "\n", "FLAGS", ".", "model_dropout", ")", "\n", "\n", "\n", "tf", ".", "gfile", ".", "MakeDirs", "(", "FLAGS", ".", "model_dir", ")", "\n", "\n", "flags_dict", "=", "tf", ".", "app", ".", "flags", ".", "FLAGS", ".", "flag_values_dict", "(", ")", "\n", "with", "tf", ".", "gfile", ".", "Open", "(", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "model_dir", ",", "\"FLAGS.json\"", ")", ",", "\"w\"", ")", "as", "ouf", ":", "\n", "    ", "json", ".", "dump", "(", "flags_dict", ",", "ouf", ")", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.main.setup_arg_scopes": [[193, 219], ["scopes.append", "arg_scope"], "function", ["None"], ["", "tf", ".", "logging", ".", "info", "(", "\"warmup steps {}/{}\"", ".", "format", "(", "\n", "FLAGS", ".", "num_warmup_steps", ",", "FLAGS", ".", "num_train_steps", ")", ")", "\n", "\n", "save_checkpoints_steps", "=", "FLAGS", ".", "num_train_steps", "//", "FLAGS", ".", "save_checkpoints_num", "\n", "tf", ".", "logging", ".", "info", "(", "\"setting save checkpoints steps to {:d}\"", ".", "format", "(", "\n", "save_checkpoints_steps", ")", ")", "\n", "\n", "FLAGS", ".", "iterations_per_loop", "=", "min", "(", "save_checkpoints_steps", ",", "\n", "FLAGS", ".", "iterations_per_loop", ")", "\n", "if", "FLAGS", ".", "use_tpu", "and", "FLAGS", ".", "tpu_name", ":", "\n", "    ", "tpu_cluster_resolver", "=", "tf", ".", "contrib", ".", "cluster_resolver", ".", "TPUClusterResolver", "(", "\n", "FLAGS", ".", "tpu_name", ",", "zone", "=", "FLAGS", ".", "tpu_zone", ",", "project", "=", "FLAGS", ".", "gcp_project", ")", "\n", "", "else", ":", "\n", "    ", "tpu_cluster_resolver", "=", "None", "\n", "# if not FLAGS.use_tpu and FLAGS.num_gpu > 1:", "\n", "#   train_distribute = tf.contrib.distribute.MirroredStrategy(", "\n", "#       num_gpus=FLAGS.num_gpu)", "\n", "# else:", "\n", "#   train_distribute = None", "\n", "\n", "", "is_per_host", "=", "tf", ".", "contrib", ".", "tpu", ".", "InputPipelineConfig", ".", "PER_HOST_V2", "\n", "run_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "RunConfig", "(", "\n", "cluster", "=", "tpu_cluster_resolver", ",", "\n", "master", "=", "FLAGS", ".", "master", ",", "\n", "model_dir", "=", "FLAGS", ".", "model_dir", ",", "\n", "save_checkpoints_steps", "=", "save_checkpoints_steps", ",", "\n", "keep_checkpoint_max", "=", "1000", ",", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.main.build_model": [[221, 259], ["main.setup_arg_scopes", "main.build_model.nested"], "function", ["home.repos.pwc.inspect_result.google-research_uda.image.main.setup_arg_scopes"], ["tpu_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUConfig", "(", "\n", "iterations_per_loop", "=", "FLAGS", ".", "iterations_per_loop", ",", "\n", "per_host_input_for_training", "=", "is_per_host", ")", ")", "\n", "\n", "model_fn", "=", "uda", ".", "model_fn_builder", "(", "\n", "bert_config", "=", "bert_config", ",", "\n", "init_checkpoint", "=", "FLAGS", ".", "init_checkpoint", ",", "\n", "learning_rate", "=", "FLAGS", ".", "learning_rate", ",", "\n", "clip_norm", "=", "FLAGS", ".", "clip_norm", ",", "\n", "num_train_steps", "=", "FLAGS", ".", "num_train_steps", ",", "\n", "num_warmup_steps", "=", "FLAGS", ".", "num_warmup_steps", ",", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "use_one_hot_embeddings", "=", "FLAGS", ".", "use_one_hot_embeddings", ",", "\n", "num_labels", "=", "len", "(", "label_list", ")", ",", "\n", "unsup_ratio", "=", "FLAGS", ".", "unsup_ratio", ",", "\n", "uda_coeff", "=", "FLAGS", ".", "uda_coeff", ",", "\n", "tsa", "=", "FLAGS", ".", "tsa", ",", "\n", "print_feature", "=", "False", ",", "\n", "print_structure", "=", "False", ",", "\n", ")", "\n", "\n", "# If TPU is not available, this will fall back to normal Estimator on CPU", "\n", "# or GPU.", "\n", "estimator", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimator", "(", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "model_fn", "=", "model_fn", ",", "\n", "config", "=", "run_config", ",", "\n", "params", "=", "{", "\"model_dir\"", ":", "FLAGS", ".", "model_dir", "}", ",", "\n", "train_batch_size", "=", "FLAGS", ".", "train_batch_size", ",", "\n", "eval_batch_size", "=", "FLAGS", ".", "eval_batch_size", ")", "\n", "\n", "if", "FLAGS", ".", "do_train", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"  >>> sup data dir : {}\"", ".", "format", "(", "FLAGS", ".", "sup_train_data_dir", ")", ")", "\n", "if", "FLAGS", ".", "unsup_ratio", ">", "0", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"  >>> unsup data dir : {}\"", ".", "format", "(", "\n", "FLAGS", ".", "unsup_data_dir", ")", ")", "\n", "\n", "", "train_input_fn", "=", "proc_data_utils", ".", "training_input_fn_builder", "(", "\n", "FLAGS", ".", "sup_train_data_dir", ",", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.main._kl_divergence_with_logits": [[261, 268], ["tensorflow.nn.softmax", "tensorflow.nn.log_softmax", "tensorflow.nn.log_softmax", "tensorflow.reduce_sum"], "function", ["None"], ["FLAGS", ".", "aug_ops", ",", "\n", "FLAGS", ".", "aug_copy", ",", "\n", "FLAGS", ".", "unsup_ratio", ")", "\n", "\n", "", "if", "FLAGS", ".", "do_eval", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"  >>> dev data dir : {}\"", ".", "format", "(", "FLAGS", ".", "eval_data_dir", ")", ")", "\n", "eval_input_fn", "=", "proc_data_utils", ".", "evaluation_input_fn_builder", "(", "\n", "FLAGS", ".", "eval_data_dir", ",", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.main.anneal_sup_loss": [[270, 291], ["main.get_tsa_threshold", "tensorflow.one_hot", "tensorflow.nn.softmax", "tensorflow.reduce_sum", "tensorflow.greater", "tensorflow.stop_gradient", "tensorflow.reduce_mean", "tensorflow.cast", "tensorflow.reduce_sum", "tensorflow.maximum", "tensorflow.reduce_sum"], "function", ["home.repos.pwc.inspect_result.google-research_uda.image.main.get_tsa_threshold"], ["\n", "eval_size", "=", "processor", ".", "get_dev_size", "(", ")", "\n", "eval_steps", "=", "int", "(", "eval_size", "/", "FLAGS", ".", "eval_batch_size", ")", "\n", "\n", "", "if", "FLAGS", ".", "do_train", "and", "FLAGS", ".", "do_eval", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"***** Running training & evaluation *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Supervised batch size = %d\"", ",", "FLAGS", ".", "train_batch_size", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Unsupervised batch size = %d\"", ",", "\n", "FLAGS", ".", "train_batch_size", "*", "FLAGS", ".", "unsup_ratio", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num steps = %d\"", ",", "FLAGS", ".", "num_train_steps", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Base evaluation batch size = %d\"", ",", "FLAGS", ".", "eval_batch_size", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num steps = %d\"", ",", "eval_steps", ")", "\n", "best_acc", "=", "0", "\n", "for", "_", "in", "range", "(", "0", ",", "FLAGS", ".", "num_train_steps", ",", "save_checkpoints_steps", ")", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"*** Running training ***\"", ")", "\n", "estimator", ".", "train", "(", "\n", "input_fn", "=", "train_input_fn", ",", "\n", "steps", "=", "save_checkpoints_steps", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"*** Running evaluation ***\"", ")", "\n", "dev_result", "=", "estimator", ".", "evaluate", "(", "input_fn", "=", "eval_input_fn", ",", "steps", "=", "eval_steps", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\">> Results:\"", ")", "\n", "for", "key", "in", "dev_result", ".", "keys", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.main._scaffold_fn": [[293, 296], ["tensorflow.train.Saver", "tensorflow.train.Scaffold"], "function", ["None"], ["dev_result", "[", "key", "]", "=", "dev_result", "[", "key", "]", ".", "item", "(", ")", "\n", "", "best_acc", "=", "max", "(", "best_acc", ",", "dev_result", "[", "\"eval_classify_accuracy\"", "]", ")", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"***** Final evaluation result *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Best acc: {:.3f}\\n\\n\"", ".", "format", "(", "best_acc", ")", ")", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.main.get_ent": [[298, 305], ["tensorflow.nn.log_softmax", "tensorflow.exp", "tensorflow.reduce_sum", "tensorflow.reduce_mean"], "function", ["None"], ["    ", "tf", ".", "logging", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Supervised batch size = %d\"", ",", "FLAGS", ".", "train_batch_size", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Unsupervised batch size = %d\"", ",", "\n", "FLAGS", ".", "train_batch_size", "*", "FLAGS", ".", "unsup_ratio", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num steps = %d\"", ",", "FLAGS", ".", "num_train_steps", ")", "\n", "estimator", ".", "train", "(", "input_fn", "=", "train_input_fn", ",", "max_steps", "=", "FLAGS", ".", "num_train_steps", ")", "\n", "", "elif", "FLAGS", ".", "do_eval", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"***** Running evaluation *****\"", ")", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.main.get_model_fn": [[307, 516], ["tensorflow.reshape", "tensorflow.train.get_global_step", "utils.decay_weights", "sum", "tensorflow.logging.info", "tensorflow.clip_by_value", "tensorflow.where", "tensorflow.train.MomentumOptimizer", "tf.contrib.tpu.CrossShardOptimizer.compute_gradients", "zip", "tensorflow.get_collection", "tensorflow.argmax", "tensorflow.to_float", "tensorflow.reduce_mean", "tensorflow.concat", "tensorflow.variable_scope", "main.build_model", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.nn.softmax", "tensorflow.reduce_mean", "main.anneal_sup_loss", "tensorflow.reduce_mean", "tensorflow.nn.softmax", "tensorflow.nn.softmax", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "main._kl_divergence_with_logits", "tensorflow.reduce_mean", "tensorflow.trainable_variables", "tensorflow.train.ExponentialMovingAverage", "utils.get_all_variable", "tensorflow.contrib.tpu.TPUEstimatorSpec", "tensorflow.cos", "tensorflow.contrib.tpu.CrossShardOptimizer", "tensorflow.control_dependencies", "tf.contrib.tpu.CrossShardOptimizer.apply_gradients", "tensorflow.equal", "tensorflow.train.LoggingTensorHook", "tensorflow.contrib.tpu.TPUEstimatorSpec", "utils.construct_scalar_host_call", "tensorflow.contrib.tpu.TPUEstimatorSpec", "tensorflow.shape", "tensorflow.reduce_max", "tensorflow.shape", "tensorflow.reduce_max", "tensorflow.reduce_max", "tensorflow.nn.softmax", "tensorflow.reduce_max", "tensorflow.cast", "tensorflow.reduce_mean", "tensorflow.stop_gradient", "tensorflow.reduce_mean", "main.get_ent", "tensorflow.reduce_mean", "numpy.prod", "max", "tensorflow.logging.info", "tf.train.ExponentialMovingAverage.variables_to_restore", "tensorflow.metrics.mean", "tensorflow.argmax", "tensorflow.metrics.accuracy", "tensorflow.to_float", "zip", "tensorflow.control_dependencies", "tf.train.ExponentialMovingAverage.apply", "log_info.format", "tensorflow.stop_gradient", "tensorflow.greater", "tensorflow.trainable_variables", "format_str.format", "functools.partial", "tensorflow.to_float", "tensorflow.to_float", "tensorflow.train.get_global_step", "len", "v.get_shape", "tensorflow.trainable_variables"], "function", ["home.repos.pwc.inspect_result.google-research_uda.image.utils.decay_weights", "home.repos.pwc.inspect_result.google-research_uda.image.main.build_model", "home.repos.pwc.inspect_result.google-research_uda.image.main.anneal_sup_loss", "home.repos.pwc.inspect_result.google-research_uda.image.main._kl_divergence_with_logits", "home.repos.pwc.inspect_result.google-research_uda.image.utils.get_all_variable", "home.repos.pwc.inspect_result.google-research_uda.bert.optimization.AdamWeightDecayOptimizer.apply_gradients", "home.repos.pwc.inspect_result.google-research_uda.image.utils.construct_scalar_host_call", "home.repos.pwc.inspect_result.google-research_uda.image.main.get_ent"], ["tf", ".", "logging", ".", "info", "(", "\"  Num steps = %d\"", ",", "eval_steps", ")", "\n", "checkpoint_state", "=", "tf", ".", "train", ".", "get_checkpoint_state", "(", "FLAGS", ".", "model_dir", ")", "\n", "\n", "best_acc", "=", "0", "\n", "for", "ckpt_path", "in", "checkpoint_state", ".", "all_model_checkpoint_paths", ":", "\n", "      ", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "ckpt_path", "+", "\".data-00000-of-00001\"", ")", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\n", "\"Warning: checkpoint {:s} does not exist\"", ".", "format", "(", "ckpt_path", ")", ")", "\n", "continue", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"Evaluating {:s}\"", ".", "format", "(", "ckpt_path", ")", ")", "\n", "dev_result", "=", "estimator", ".", "evaluate", "(", "\n", "input_fn", "=", "eval_input_fn", ",", "\n", "steps", "=", "eval_steps", ",", "\n", "checkpoint_path", "=", "ckpt_path", ",", "\n", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\">> Results:\"", ")", "\n", "for", "key", "in", "dev_result", ".", "keys", "(", ")", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "dev_result", "[", "key", "]", ")", ")", "\n", "dev_result", "[", "key", "]", "=", "dev_result", "[", "key", "]", ".", "item", "(", ")", "\n", "", "best_acc", "=", "max", "(", "best_acc", ",", "dev_result", "[", "\"eval_classify_accuracy\"", "]", ")", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"***** Final evaluation result *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Best acc: {:.3f}\\n\\n\"", ".", "format", "(", "best_acc", ")", ")", "\n", "\n", "\n", "", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "  ", "tf", ".", "app", ".", "run", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.google-research_uda.image.main.train": [[518, 600], ["main.get_model_fn", "utils.get_TPU_estimator", "data.get_input_fn", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "data.get_input_fn", "data.get_input_fn", "tensorflow.logging.info", "min", "utils.get_TPU_estimator.train", "utils.get_TPU_estimator.evaluate", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "utils.get_TPU_estimator.train", "tensorflow.logging.info", "utils.get_TPU_estimator.evaluate", "tensorflow.logging.info", "estimator.evaluate.keys", "tensorflow.logging.info", "results[].item", "tensorflow.gfile.Open", "ouf.write", "str", "str"], "function", ["home.repos.pwc.inspect_result.google-research_uda.image.main.get_model_fn", "home.repos.pwc.inspect_result.google-research_uda.image.utils.get_TPU_estimator", "home.repos.pwc.inspect_result.google-research_uda.image.data.get_input_fn", "home.repos.pwc.inspect_result.google-research_uda.image.data.get_input_fn", "home.repos.pwc.inspect_result.google-research_uda.image.data.get_input_fn", "home.repos.pwc.inspect_result.google-research_uda.image.main.train", "home.repos.pwc.inspect_result.google-research_uda.image.main.train"], []], "home.repos.pwc.inspect_result.google-research_uda.image.main.main": [[602, 629], ["tensorflow.contrib.training.HParams", "main.train", "tensorflow.gfile.MakeDirs", "tensorflow.app.flags.FLAGS.flag_values_dict", "tf.contrib.training.HParams.add_hparam", "tf.contrib.training.HParams.add_hparam", "tensorflow.gfile.Open", "json.dump", "tf.contrib.training.HParams.add_hparam", "tf.contrib.training.HParams.add_hparam", "os.path.join", "tf.contrib.training.HParams.add_hparam", "tf.contrib.training.HParams.add_hparam", "tf.contrib.training.HParams.add_hparam", "tf.contrib.training.HParams.add_hparam", "tf.contrib.training.HParams.add_hparam", "ValueError"], "function", ["home.repos.pwc.inspect_result.google-research_uda.image.main.train"], []], "home.repos.pwc.inspect_result.google-research_uda.image.data.format_sup_filename": [[34, 44], ["None"], "function", ["None"], ["def", "format_sup_filename", "(", "split", ",", "sup_size", "=", "-", "1", ")", ":", "\n", "  ", "if", "split", "==", "\"test\"", ":", "\n", "    ", "return", "\"test.tfrecord\"", "\n", "", "elif", "split", "==", "\"train\"", "or", "split", "==", "\"dev\"", ":", "\n", "    ", "if", "sup_size", "==", "-", "1", ":", "\n", "      ", "return", "\"{}-full.tfrecord\"", ".", "format", "(", "split", ")", "\n", "", "else", ":", "\n", "      ", "return", "\"{}-size_{:d}.tfrecord\"", ".", "format", "(", "split", ",", "sup_size", ")", "\n", "", "", "else", ":", "\n", "    ", "assert", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.data.format_unsup_filename": [[46, 48], ["None"], "function", ["None"], ["", "", "def", "format_unsup_filename", "(", "aug_copy_num", ")", ":", "\n", "  ", "return", "\"unsup-{:d}.tfrecord\"", ".", "format", "(", "aug_copy_num", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.data._postprocess_example": [[50, 59], ["list", "example.keys", "tensorflow.keras.backend.is_sparse", "tensorflow.sparse.to_dense", "tensorflow.to_int32"], "function", ["None"], ["", "def", "_postprocess_example", "(", "example", ")", ":", "\n", "  ", "\"\"\"Convert tensor type for TPU, cast int64 into int32 and cast sparse to dense.\"\"\"", "\n", "for", "key", "in", "list", "(", "example", ".", "keys", "(", ")", ")", ":", "\n", "    ", "val", "=", "example", "[", "key", "]", "\n", "if", "tf", ".", "keras", ".", "backend", ".", "is_sparse", "(", "val", ")", ":", "\n", "      ", "val", "=", "tf", ".", "sparse", ".", "to_dense", "(", "val", ")", "\n", "", "if", "val", ".", "dtype", "==", "tf", ".", "int64", ":", "\n", "      ", "val", "=", "tf", ".", "to_int32", "(", "val", ")", "\n", "", "example", "[", "key", "]", "=", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.data.get_dataset": [[61, 119], ["tensorflow.constant", "tensorflow.data.Dataset.from_tensor_slices", "dataset.shuffle.apply", "dataset.shuffle.map", "dataset.shuffle.batch", "dataset.shuffle.prefetch", "data.crop", "tensorflow.parse_single_example", "tf.parse_single_example.keys", "data._postprocess_example", "tensorflow.contrib.slim.parallel_reader.get_data_files", "dataset.shuffle.shuffle().repeat", "tensorflow.contrib.data.parallel_interleave", "dataset.shuffle.shuffle", "dataset.shuffle.repeat", "dataset.shuffle.shuffle", "data.flip", "tensorflow.reshape", "data.get_dataset.apply_normal_aug"], "function", ["home.repos.pwc.inspect_result.google-research_uda.image.data.crop", "home.repos.pwc.inspect_result.google-research_uda.image.data._postprocess_example", "home.repos.pwc.inspect_result.google-research_uda.image.data.flip"], ["", "", "def", "get_dataset", "(", "file_prefix_list", ",", "record_spec", ",", "task_name", ",", "\n", "split", ",", "per_core_bsz", ")", ":", "\n", "\n", "  ", "is_training", "=", "(", "split", "==", "\"train\"", ")", "\n", "is_training_tensor", "=", "tf", ".", "constant", "(", "is_training", ",", "dtype", "=", "tf", ".", "bool", ")", "\n", "\n", "def", "apply_normal_aug", "(", "image", ")", ":", "\n", "    ", "if", "task_name", "==", "\"cifar10\"", ":", "\n", "      ", "image", "=", "flip", "(", "image", ",", "is_training_tensor", ")", "\n", "", "image", "=", "crop", "(", "image", ",", "is_training_tensor", ")", "\n", "return", "image", "\n", "\n", "", "def", "parser", "(", "record", ")", ":", "\n", "# retrieve serialized example", "\n", "    ", "example", "=", "tf", ".", "parse_single_example", "(", "\n", "serialized", "=", "record", ",", "\n", "features", "=", "record_spec", ")", "\n", "# reshape image back to 3D shape", "\n", "for", "key", "in", "example", ".", "keys", "(", ")", ":", "\n", "      ", "if", "\"image\"", "in", "key", ":", "\n", "        ", "example", "[", "key", "]", "=", "tf", ".", "reshape", "(", "example", "[", "key", "]", ",", "[", "32", ",", "32", ",", "3", "]", ")", "\n", "example", "[", "key", "]", "=", "apply_normal_aug", "(", "example", "[", "key", "]", ")", "\n", "\n", "", "", "_postprocess_example", "(", "example", ")", "\n", "\n", "return", "example", "\n", "\n", "", "all_file_list", "=", "[", "]", "\n", "for", "file_prefix", "in", "file_prefix_list", ":", "\n", "    ", "cur_file_list", "=", "tf", ".", "contrib", ".", "slim", ".", "parallel_reader", ".", "get_data_files", "(", "\n", "file_prefix", "+", "\"*\"", ")", "\n", "all_file_list", "+=", "cur_file_list", "\n", "", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "all_file_list", ")", "\n", "\n", "if", "is_training", ":", "\n", "    ", "dataset", "=", "dataset", ".", "shuffle", "(", "len", "(", "all_file_list", ")", ")", ".", "repeat", "(", ")", "\n", "# read from 4 tfrecord files in parallel", "\n", "", "dataset", "=", "dataset", ".", "apply", "(", "\n", "tf", ".", "contrib", ".", "data", ".", "parallel_interleave", "(", "\n", "tf", ".", "data", ".", "TFRecordDataset", ",", "\n", "sloppy", "=", "is_training", ",", "\n", "cycle_length", "=", "4", ")", ")", "\n", "\n", "if", "is_training", ":", "\n", "# Shuffle and then repeat to maintain the epoch boundary", "\n", "    ", "dataset", "=", "dataset", ".", "shuffle", "(", "100000", ")", "\n", "dataset", "=", "dataset", ".", "repeat", "(", ")", "\n", "\n", "", "dataset", "=", "dataset", ".", "map", "(", "parser", ",", "num_parallel_calls", "=", "32", ")", "\n", "dataset", "=", "dataset", ".", "batch", "(", "per_core_bsz", ",", "drop_remainder", "=", "True", ")", "\n", "\n", "# Safe guard the case that the shuffle buffer size for record is smaller", "\n", "# than the batch size.", "\n", "if", "is_training", ":", "\n", "    ", "dataset", "=", "dataset", ".", "shuffle", "(", "512", ")", "\n", "", "dataset", "=", "dataset", ".", "prefetch", "(", "1", ")", "\n", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.data.flip": [[121, 128], ["tensorflow.cond", "tensorflow.to_float", "tensorflow.image.flip_left_right", "tensorflow.random_uniform", "data.flip.func"], "function", ["None"], ["", "def", "flip", "(", "image", ",", "is_training", ")", ":", "\n", "  ", "def", "func", "(", "inp", ")", ":", "\n", "    ", "flips", "=", "tf", ".", "to_float", "(", "tf", ".", "random_uniform", "(", "[", "1", ",", "1", ",", "1", "]", ",", "0", ",", "2", ",", "tf", ".", "int32", ")", ")", "\n", "flipped_inp", "=", "tf", ".", "image", ".", "flip_left_right", "(", "inp", ")", "\n", "return", "flips", "*", "flipped_inp", "+", "(", "1", "-", "flips", ")", "*", "inp", "\n", "\n", "", "return", "tf", ".", "cond", "(", "is_training", ",", "lambda", ":", "func", "(", "image", ")", ",", "lambda", ":", "image", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.data.crop": [[130, 142], ["tensorflow.cond", "tensorflow.pad", "tensorflow.random_crop", "tensorflow.constant", "tensorflow.shape", "data.flip.func"], "function", ["None"], ["", "def", "crop", "(", "image", ",", "is_training", ")", ":", "\n", "  ", "def", "func", "(", "inp", ")", ":", "\n", "    ", "amount", "=", "4", "\n", "pad_inp", "=", "tf", ".", "pad", "(", "inp", ",", "\n", "tf", ".", "constant", "(", "[", "[", "amount", ",", "amount", "]", ",", "\n", "[", "amount", ",", "amount", "]", ",", "\n", "[", "0", ",", "0", "]", "]", ")", ",", "\n", "\"REFLECT\"", ")", "\n", "cropped_data", "=", "tf", ".", "random_crop", "(", "pad_inp", ",", "tf", ".", "shape", "(", "image", ")", ")", "\n", "return", "cropped_data", "\n", "\n", "", "return", "tf", ".", "cond", "(", "is_training", ",", "lambda", ":", "func", "(", "image", ")", ",", "lambda", ":", "image", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.data.get_input_fn": [[144, 208], ["data.format_sup_filename", "tensorflow.logging.info", "data.get_dataset", "datasets.append", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "os.path.join", "tensorflow.logging.info", "data.get_dataset", "datasets.append", "len", "tensorflow.data.Dataset.zip", "dataset.map.map", "len", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "os.path.join", "tuple", "data.format_unsup_filename", "range", "len"], "function", ["home.repos.pwc.inspect_result.google-research_uda.image.preprocess.format_sup_filename", "home.repos.pwc.inspect_result.google-research_uda.image.data.get_dataset", "home.repos.pwc.inspect_result.google-research_uda.image.data.get_dataset", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.format_unsup_filename"], ["", "def", "get_input_fn", "(", "\n", "data_dir", ",", "split", ",", "task_name", ",", "sup_size", "=", "-", "1", ",", "\n", "unsup_ratio", "=", "0", ",", "aug_copy", "=", "0", ")", ":", "\n", "\n", "  ", "def", "input_fn", "(", "params", ")", ":", "\n", "    ", "per_core_bsz", "=", "params", "[", "\"batch_size\"", "]", "\n", "\n", "datasets", "=", "[", "]", "\n", "# Supervised data", "\n", "filename", "=", "format_sup_filename", "(", "split", ",", "sup_size", "=", "sup_size", ")", "\n", "sup_record_spec", "=", "{", "\n", "\"image\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "32", "*", "32", "*", "3", "]", ",", "tf", ".", "float32", ")", ",", "\n", "\"label\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "1", "]", ",", "tf", ".", "int64", ")", "\n", "}", "\n", "sup_file_list", "=", "[", "os", ".", "path", ".", "join", "(", "data_dir", ",", "filename", ")", "]", "\n", "tf", ".", "logging", ".", "info", "(", "\"getting supervised dataset from {} file prefixes\"", ".", "format", "(", "\n", "len", "(", "sup_file_list", ")", ")", ")", "\n", "sup_dataset", "=", "get_dataset", "(", "\n", "file_prefix_list", "=", "sup_file_list", ",", "\n", "record_spec", "=", "sup_record_spec", ",", "\n", "task_name", "=", "task_name", ",", "\n", "split", "=", "split", ",", "\n", "per_core_bsz", "=", "per_core_bsz", ",", "\n", ")", "\n", "\n", "datasets", ".", "append", "(", "sup_dataset", ")", "\n", "\n", "if", "unsup_ratio", ">", "0", ":", "\n", "      ", "aug_record_spec", "=", "{", "\n", "\"ori_image\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "32", "*", "32", "*", "3", "]", ",", "tf", ".", "float32", ")", ",", "\n", "\"aug_image\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "32", "*", "32", "*", "3", "]", ",", "tf", ".", "float32", ")", ",", "\n", "}", "\n", "aug_file_list", "=", "[", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "format_unsup_filename", "(", "i", ")", ")", "for", "i", "in", "range", "(", "aug_copy", ")", "]", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "\"getting unsupervised dataset from {} file prefixes\"", ".", "format", "(", "\n", "len", "(", "aug_file_list", ")", ")", "\n", ")", "\n", "aug_dataset", "=", "get_dataset", "(", "\n", "file_prefix_list", "=", "aug_file_list", ",", "\n", "record_spec", "=", "aug_record_spec", ",", "\n", "task_name", "=", "task_name", ",", "\n", "split", "=", "split", ",", "\n", "per_core_bsz", "=", "per_core_bsz", "*", "unsup_ratio", "\n", ")", "\n", "datasets", ".", "append", "(", "aug_dataset", ")", "\n", "\n", "", "def", "flatten_input", "(", "*", "features", ")", ":", "\n", "      ", "result", "=", "{", "}", "\n", "for", "feature", "in", "features", ":", "\n", "        ", "for", "key", "in", "feature", ":", "\n", "          ", "assert", "key", "not", "in", "result", "\n", "result", "[", "key", "]", "=", "feature", "[", "key", "]", "\n", "", "", "return", "result", "\n", "\n", "", "if", "len", "(", "datasets", ")", ">", "1", ":", "\n", "      ", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "zip", "(", "tuple", "(", "datasets", ")", ")", "\n", "dataset", "=", "dataset", ".", "map", "(", "flatten_input", ")", "\n", "", "else", ":", "\n", "      ", "dataset", "=", "datasets", "[", "0", "]", "\n", "\n", "", "return", "dataset", "\n", "\n", "", "return", "input_fn", "\n", "", ""]], "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.format_sup_filename": [[53, 61], ["None"], "function", ["None"], ["help", "=", "\"We generate multiple augmented examples for one\"", "\n", "\"unlabeled example, aug_copy_num is the index of the generated augmented\"", "\n", "\"example\"", ")", "\n", "\n", "flags", ".", "DEFINE_integer", "(", "\n", "\"max_seq_length\"", ",", "512", ",", "\n", "help", "=", "\"The maximum total sequence length after WordPiece tokenization. \"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \"", "\n", "\"than this will be padded.\"", ")", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.format_unsup_filename": [[63, 65], ["None"], "function", ["None"], ["flags", ".", "DEFINE_integer", "(", "\n", "\"sup_size\"", ",", "-", "1", ",", "\"size of the labeled set\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.preprocess._int64_feature": [[67, 69], ["tensorflow.train.Feature", "tensorflow.train.Int64List", "list"], "function", ["None"], ["\"trunc_keep_right\"", ",", "True", ",", "\n", "help", "=", "\"Whether to keep the right part when truncate a sentence.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.preprocess._float_feature": [[71, 73], ["tensorflow.train.Feature", "tensorflow.train.FloatList", "list"], "function", ["None"], ["\"data_type\"", ",", "default", "=", "\"sup\"", ",", "\n", "enum_values", "=", "[", "\"sup\"", ",", "\"unsup\"", "]", ",", "\n", "help", "=", "\"Which preprocess task to perform.\"", ")", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.get_raw_data_filenames": [[75, 84], ["six.moves.xrange"], "function", ["None"], ["flags", ".", "DEFINE_string", "(", "\n", "\"sub_set\"", ",", "\"train\"", ",", "\n", "\"Which sub_set to preprocess. The sub_set can be train, dev and unsup_in\"", ")", "\n", "\n", "flags", ".", "DEFINE_string", "(", "\n", "\"vocab_file\"", ",", "\"\"", ",", "\"The path of the vocab file of BERT.\"", ")", "\n", "\n", "flags", ".", "DEFINE_bool", "(", "\n", "\"do_lower_case\"", ",", "True", ",", "\"Whether to use uncased text for BERT.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.read_pickle_from_file": [[86, 93], ["tensorflow.gfile.Open", "six.moves.cPickle.load", "six.moves.cPickle.load"], "function", ["None"], ["\"back_translation_dir\"", ",", "\"\"", ",", "\"Directory for back translated sentence.\"", ")", "\n", "\n", "flags", ".", "DEFINE_integer", "(", "\n", "\"replicas\"", ",", "1", ",", "\n", "\"An argument for parallel preprocessing. For example, when replicas=3,\"", "\n", "\"we divide the data into three parts, and only process one part\"", "\n", "\"according to the worker_id.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.obtain_tfrecord_writer": [[95, 99], ["tensorflow.python_io.TFRecordWriter"], "function", ["None"], ["\"worker_id\"", ",", "0", ",", "\n", "\"An argument for parallel preprocessing. See 'replicas' for more details\"", ")", "\n", "\n", "\n", "def", "get_data_for_worker", "(", "examples", ",", "replicas", ",", "worker_id", ")", ":", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.save_tfrecord": [[101, 115], ["preprocess.obtain_tfrecord_writer", "obtain_tfrecord_writer.close", "tensorflow.logging.info", "obtain_tfrecord_writer.write", "obtain_tfrecord_writer.close", "preprocess.obtain_tfrecord_writer", "example.SerializeToString", "len"], "function", ["home.repos.pwc.inspect_result.google-research_uda.image.preprocess.obtain_tfrecord_writer", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.obtain_tfrecord_writer"], ["remainder", "=", "len", "(", "examples", ")", "-", "replicas", "*", "data_per_worker", "\n", "if", "worker_id", "<", "remainder", ":", "\n", "    ", "start", "=", "(", "data_per_worker", "+", "1", ")", "*", "worker_id", "\n", "end", "=", "(", "data_per_worker", "+", "1", ")", "*", "(", "worker_id", "+", "1", ")", "\n", "", "else", ":", "\n", "    ", "start", "=", "data_per_worker", "*", "worker_id", "+", "remainder", "\n", "end", "=", "data_per_worker", "*", "(", "worker_id", "+", "1", ")", "+", "remainder", "\n", "", "if", "worker_id", "==", "replicas", "-", "1", ":", "\n", "    ", "assert", "end", "==", "len", "(", "examples", ")", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"processing data from {:d} to {:d}\"", ".", "format", "(", "start", ",", "end", ")", ")", "\n", "examples", "=", "examples", "[", "start", ":", "end", "]", "\n", "return", "examples", ",", "start", ",", "end", "\n", "\n", "\n", "", "def", "build_vocab", "(", "examples", ")", ":", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.save_merged_data": [[117, 124], ["tensorflow.gfile.Open", "numpy.save", "tensorflow.gfile.Open", "numpy.save", "os.path.join", "os.path.join"], "function", ["None"], ["def", "add_to_vocab", "(", "word_list", ")", ":", "\n", "    ", "for", "word", "in", "word_list", ":", "\n", "      ", "if", "word", "not", "in", "vocab", ":", "\n", "        ", "vocab", "[", "word", "]", "=", "len", "(", "vocab", ")", "\n", "", "", "", "for", "i", "in", "range", "(", "len", "(", "examples", ")", ")", ":", "\n", "    ", "add_to_vocab", "(", "examples", "[", "i", "]", ".", "word_list_a", ")", "\n", "if", "examples", "[", "i", "]", ".", "text_b", ":", "\n", "      ", "add_to_vocab", "(", "examples", "[", "i", "]", ".", "word_list_b", ")", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.download_and_extract": [[126, 171], ["os.path.join", "os.path.join", "tensorflow.logging.info", "tensorflow.gfile.MakeDirs", "tensorflow.gfile.MakeDirs", "tensorflow.logging.info", "tensorflow.contrib.learn.datasets.base.maybe_download", "tarfile.open().extractall", "preprocess.get_raw_data_filenames", "numpy.concatenate", "numpy.concatenate", "np.transpose.reshape", "np.transpose.transpose", "preprocess.save_merged_data", "tensorflow.gfile.Exists", "tarfile.open", "preprocess.read_pickle_from_file", "tensorflow.contrib.learn.datasets.base.maybe_download", "os.path.join", "scipy.io.loadmat", "numpy.transpose", "data_dict[].reshape", "preprocess.save_merged_data", "os.path.join", "os.path.join", "os.path.join", "SVHN_DOWNLOAD_URL.format", "tensorflow.gfile.Open"], "function", ["home.repos.pwc.inspect_result.google-research_uda.image.preprocess.get_raw_data_filenames", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.save_merged_data", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.read_pickle_from_file", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.save_merged_data"], ["\n", "\n", "", "def", "get_data_stats", "(", "data_stats_dir", ",", "sub_set", ",", "sup_size", ",", "replicas", ",", "examples", ")", ":", "\n", "  ", "data_stats_dir", "=", "\"{}/{}\"", ".", "format", "(", "data_stats_dir", ",", "sub_set", ")", "\n", "keys", "=", "[", "\"tf_idf\"", ",", "\"idf\"", "]", "\n", "all_exist", "=", "True", "\n", "for", "key", "in", "keys", ":", "\n", "    ", "data_stats_path", "=", "\"{}/{}.json\"", ".", "format", "(", "data_stats_dir", ",", "key", ")", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "data_stats_path", ")", ":", "\n", "      ", "all_exist", "=", "False", "\n", "tf", ".", "logging", ".", "info", "(", "\"Not exist: {}\"", ".", "format", "(", "data_stats_path", ")", ")", "\n", "", "", "if", "all_exist", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"loading data stats from {:s}\"", ".", "format", "(", "data_stats_dir", ")", ")", "\n", "data_stats", "=", "{", "}", "\n", "for", "key", "in", "keys", ":", "\n", "      ", "with", "tf", ".", "gfile", ".", "Open", "(", "\n", "\"{}/{}.json\"", ".", "format", "(", "data_stats_dir", ",", "key", ")", ")", "as", "inf", ":", "\n", "        ", "data_stats", "[", "key", "]", "=", "json", ".", "load", "(", "inf", ")", "\n", "", "", "", "else", ":", "\n", "    ", "assert", "sup_size", "==", "-", "1", ",", "\"should use the complete set to get tf_idf\"", "\n", "assert", "replicas", "==", "1", ",", "\"should use the complete set to get tf_idf\"", "\n", "data_stats", "=", "word_level_augment", ".", "get_data_stats", "(", "examples", ")", "\n", "tf", ".", "gfile", ".", "MakeDirs", "(", "data_stats_dir", ")", "\n", "for", "key", "in", "keys", ":", "\n", "      ", "with", "tf", ".", "gfile", ".", "Open", "(", "\"{}/{}.json\"", ".", "format", "(", "data_stats_dir", ",", "key", ")", ",", "\"w\"", ")", "as", "ouf", ":", "\n", "        ", "json", ".", "dump", "(", "data_stats", "[", "key", "]", ",", "ouf", ")", "\n", "", "", "tf", ".", "logging", ".", "info", "(", "\"dumped data stats to {:s}\"", ".", "format", "(", "data_stats_dir", ")", ")", "\n", "", "return", "data_stats", "\n", "\n", "\n", "", "def", "tokenize_examples", "(", "examples", ",", "tokenizer", ")", ":", "\n", "  ", "tf", ".", "logging", ".", "info", "(", "\"tokenizing examples\"", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "examples", ")", ")", ":", "\n", "    ", "examples", "[", "i", "]", ".", "word_list_a", "=", "tokenizer", ".", "tokenize_to_word", "(", "examples", "[", "i", "]", ".", "text_a", ")", "\n", "if", "examples", "[", "i", "]", ".", "text_b", ":", "\n", "      ", "examples", "[", "i", "]", ".", "word_list_b", "=", "tokenizer", ".", "tokenize_to_word", "(", "examples", "[", "i", "]", ".", "text_b", ")", "\n", "", "if", "i", "%", "10000", "==", "0", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"finished tokenizing example {:d}\"", ".", "format", "(", "i", ")", ")", "\n", "", "", "return", "examples", "\n", "\n", "\n", "", "def", "convert_examples_to_features", "(", "\n", "examples", ",", "label_list", ",", "seq_length", ",", "tokenizer", ",", "trunc_keep_right", ",", "\n", "data_stats", "=", "None", ",", "aug_ops", "=", "None", ")", ":", "\n", "  ", "\"\"\"convert examples to features.\"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.load_dataset": [[173, 186], ["preprocess.download_and_extract", "os.path.join", "tensorflow.gfile.Open", "numpy.load", "tensorflow.gfile.Open", "numpy.load", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.google-research_uda.image.preprocess.download_and_extract"], ["for", "(", "i", ",", "label", ")", "in", "enumerate", "(", "label_list", ")", ":", "\n", "    ", "label_map", "[", "label", "]", "=", "i", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"number of examples to process: {}\"", ".", "format", "(", "len", "(", "examples", ")", ")", ")", "\n", "\n", "features", "=", "[", "]", "\n", "\n", "if", "aug_ops", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"building vocab\"", ")", "\n", "word_vocab", "=", "build_vocab", "(", "examples", ")", "\n", "examples", "=", "word_level_augment", ".", "word_level_augment", "(", "\n", "examples", ",", "aug_ops", ",", "word_vocab", ",", "data_stats", "\n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.get_data_by_size_lim": [[188, 229], ["collections.defaultdict", "range", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "len", "numpy.random.seed", "numpy.arange", "numpy.random.shuffle"], "function", ["None"], ["    ", "if", "ex_index", "%", "10000", "==", "0", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"processing {:d}\"", ".", "format", "(", "ex_index", ")", ")", "\n", "", "tokens_a", "=", "tokenizer", ".", "tokenize_to_wordpiece", "(", "example", ".", "word_list_a", ")", "\n", "tokens_b", "=", "None", "\n", "if", "example", ".", "text_b", ":", "\n", "      ", "tokens_b", "=", "tokenizer", ".", "tokenize_to_wordpiece", "(", "example", ".", "word_list_b", ")", "\n", "\n", "", "if", "tokens_b", ":", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total", "\n", "# length is less than the specified length.", "\n", "# Account for [CLS], [SEP], [SEP] with \"- 3\"", "\n", "      ", "if", "trunc_keep_right", ":", "\n", "        ", "_truncate_seq_pair_keep_right", "(", "tokens_a", ",", "tokens_b", ",", "seq_length", "-", "3", ")", "\n", "", "else", ":", "\n", "        ", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "seq_length", "-", "3", ")", "\n", "", "", "else", ":", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "      ", "if", "len", "(", "tokens_a", ")", ">", "seq_length", "-", "2", ":", "\n", "        ", "if", "trunc_keep_right", ":", "\n", "          ", "tokens_a", "=", "tokens_a", "[", "-", "(", "seq_length", "-", "2", ")", ":", "]", "\n", "", "else", ":", "\n", "          ", "tokens_a", "=", "tokens_a", "[", "0", ":", "(", "seq_length", "-", "2", ")", "]", "\n", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] the dog is hairy . [SEP]", "\n", "#  type_ids: 0     0   0   0  0     0 0", "\n", "#", "\n", "# Where \"type_ids\" are used to indicate whether this is the first", "\n", "# sequence or the second sequence. The embedding vectors for `type=0` and", "\n", "# `type=1` were learned during pre-training and are added to the wordpiece", "\n", "# embedding vector (and position vector). This is not *strictly* necessary", "\n", "# since the [SEP] token unambigiously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "#", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "", "", "", "tokens", "=", "[", "]", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.process_and_save_sup_data": [[231, 251], ["randaugment.augmentation_transforms.get_mean_and_std", "zip", "os.path.join", "tensorflow.logging.info", "preprocess.save_tfrecord", "tensorflow.train.Example", "preprocess.format_sup_filename", "len", "tensorflow.train.Features", "preprocess._float_feature", "preprocess._int64_feature", "image.reshape", "label.reshape"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.get_mean_and_std", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.save_tfrecord", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.format_sup_filename", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess._float_feature", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess._int64_feature"], ["tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "tokens_a", ":", "\n", "      ", "tokens", ".", "append", "(", "token", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "if", "tokens_b", ":", "\n", "      ", "for", "token", "in", "tokens_b", ":", "\n", "        ", "tokens", ".", "append", "(", "token", ")", "\n", "input_type_ids", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_type_ids", ".", "append", "(", "1", ")", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.proc_and_dump_sup_data": [[253, 270], ["preprocess.get_data_by_size_lim", "preprocess.get_data_by_size_lim", "preprocess.process_and_save_sup_data", "preprocess.process_and_save_sup_data", "preprocess.process_and_save_sup_data"], "function", ["home.repos.pwc.inspect_result.google-research_uda.image.preprocess.get_data_by_size_lim", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.get_data_by_size_lim", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.process_and_save_sup_data", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.process_and_save_sup_data", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.process_and_save_sup_data"], ["while", "len", "(", "input_ids", ")", "<", "seq_length", ":", "\n", "      ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_type_ids", ")", "==", "seq_length", "\n", "\n", "label_id", "=", "label_map", "[", "example", ".", "label", "]", "\n", "if", "ex_index", "<", "1", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"guid: %s\"", "%", "(", "example", ".", "guid", ")", ")", "\n", "# st = \" \".join([str(x) for x in tokens])", "\n", "st", "=", "\"\"", "\n", "for", "x", "in", "tokens", ":", "\n", "        ", "if", "isinstance", "(", "x", ",", "unicode", ")", ":", "\n", "          ", "st", "+=", "x", ".", "encode", "(", "\"ascii\"", ",", "\"replace\"", ")", "+", "\" \"", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.proc_and_dump_unsup_data": [[272, 312], ["sub_set_data[].copy", "numpy.arange", "numpy.random.shuffle", "randaugment.augmentation_transforms.get_mean_and_std", "os.path.join", "preprocess.save_tfrecord", "len", "randaugment.policies.randaug_policies", "randaugment.augmentation_transforms.apply_policy", "randaugment.augmentation_transforms.cutout_numpy", "tensorflow.train.Example", "preprocess.format_unsup_filename", "randaugment.policies.randaug_policies", "numpy.random.choice", "tensorflow.train.Features", "len", "preprocess._float_feature", "preprocess._float_feature", "image.reshape", "augmentation_transforms.cutout_numpy.reshape"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.get_mean_and_std", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.save_tfrecord", "home.repos.pwc.inspect_result.google-research_uda.randaugment.policies.randaug_policies", "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.apply_policy", "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.cutout_numpy", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.format_unsup_filename", "home.repos.pwc.inspect_result.google-research_uda.randaugment.policies.randaug_policies", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess._float_feature", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess._float_feature"], ["          ", "st", "+=", "str", "(", "x", ")", "+", "\" \"", "\n", "", "", "tf", ".", "logging", ".", "info", "(", "\"tokens: %s\"", "%", "st", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "\"input_type_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_type_ids", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"label: %s (id = %d)\"", "%", "(", "example", ".", "label", ",", "label_id", ")", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "input_type_ids", "=", "input_type_ids", ",", "\n", "label_id", "=", "label_id", ")", ")", "\n", "", "return", "features", "\n", "\n", "\n", "", "def", "_create_int_feature", "(", "values", ")", ":", "\n", "  ", "feature", "=", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "list", "(", "values", ")", ")", ")", "\n", "return", "feature", "\n", "\n", "\n", "", "class", "InputFeatures", "(", "object", ")", ":", "\n", "  ", "\"\"\"A single set of features of data.\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "input_type_ids", ",", "label_id", ")", ":", "\n", "    ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "input_type_ids", "=", "input_type_ids", "\n", "self", ".", "label_id", "=", "label_id", "\n", "\n", "", "def", "get_dict_features", "(", "self", ")", ":", "\n", "    ", "return", "{", "\n", "\"input_ids\"", ":", "_create_int_feature", "(", "self", ".", "input_ids", ")", ",", "\n", "\"input_mask\"", ":", "_create_int_feature", "(", "self", ".", "input_mask", ")", ",", "\n", "\"input_type_ids\"", ":", "_create_int_feature", "(", "self", ".", "input_type_ids", ")", ",", "\n", "\"label_ids\"", ":", "_create_int_feature", "(", "[", "self", ".", "label_id", "]", ")", "\n", "}", "\n", "\n", "\n", "", "", "class", "PairedUnsupInputFeatures", "(", "object", ")", ":", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.main": [[314, 342], ["preprocess.load_dataset", "tensorflow.gfile.Exists", "tensorflow.gfile.MakeDirs", "tensorflow.logging.info", "preprocess.proc_and_dump_sup_data", "preprocess.proc_and_dump_sup_data", "tensorflow.logging.info", "tensorflow.logging.info", "numpy.random.seed", "range", "tensorflow.logging.info", "preprocess.proc_and_dump_unsup_data"], "function", ["home.repos.pwc.inspect_result.google-research_uda.image.preprocess.load_dataset", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.proc_and_dump_sup_data", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.proc_and_dump_sup_data", "home.repos.pwc.inspect_result.google-research_uda.image.preprocess.proc_and_dump_unsup_data"], ["\n", "def", "__init__", "(", "self", ",", "ori_input_ids", ",", "ori_input_mask", ",", "ori_input_type_ids", ",", "\n", "aug_input_ids", ",", "aug_input_mask", ",", "aug_input_type_ids", ")", ":", "\n", "    ", "self", ".", "ori_input_ids", "=", "ori_input_ids", "\n", "self", ".", "ori_input_mask", "=", "ori_input_mask", "\n", "self", ".", "ori_input_type_ids", "=", "ori_input_type_ids", "\n", "self", ".", "aug_input_ids", "=", "aug_input_ids", "\n", "self", ".", "aug_input_mask", "=", "aug_input_mask", "\n", "self", ".", "aug_input_type_ids", "=", "aug_input_type_ids", "\n", "\n", "", "def", "get_dict_features", "(", "self", ")", ":", "\n", "    ", "return", "{", "\n", "\"ori_input_ids\"", ":", "_create_int_feature", "(", "self", ".", "ori_input_ids", ")", ",", "\n", "\"ori_input_mask\"", ":", "_create_int_feature", "(", "self", ".", "ori_input_mask", ")", ",", "\n", "\"ori_input_type_ids\"", ":", "_create_int_feature", "(", "self", ".", "ori_input_type_ids", ")", ",", "\n", "\"aug_input_ids\"", ":", "_create_int_feature", "(", "self", ".", "aug_input_ids", ")", ",", "\n", "\"aug_input_mask\"", ":", "_create_int_feature", "(", "self", ".", "aug_input_mask", ")", ",", "\n", "\"aug_input_type_ids\"", ":", "_create_int_feature", "(", "self", ".", "aug_input_type_ids", ")", ",", "\n", "}", "\n", "\n", "\n", "", "", "def", "obtain_tfrecord_writer", "(", "data_path", ",", "worker_id", ",", "shard_cnt", ")", ":", "\n", "  ", "tfrecord_writer", "=", "tf", ".", "python_io", ".", "TFRecordWriter", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "data_path", ",", "\n", "\"tf_examples.tfrecord.{:d}.{:d}\"", ".", "format", "(", "worker_id", ",", "shard_cnt", ")", ")", ")", "\n", "return", "tfrecord_writer", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.utils.decay_weights": [[25, 32], ["tensorflow.trainable_variables", "tensorflow.multiply", "costs.append", "tensorflow.add_n", "tensorflow.nn.l2_loss"], "function", ["None"], ["def", "decay_weights", "(", "cost", ",", "weight_decay_rate", ")", ":", "\n", "  ", "\"\"\"Calculates the loss for l2 weight decay and adds it to `cost`.\"\"\"", "\n", "costs", "=", "[", "]", "\n", "for", "var", "in", "tf", ".", "trainable_variables", "(", ")", ":", "\n", "    ", "costs", ".", "append", "(", "tf", ".", "nn", ".", "l2_loss", "(", "var", ")", ")", "\n", "", "cost", "+=", "tf", ".", "multiply", "(", "weight_decay_rate", ",", "tf", ".", "add_n", "(", "costs", ")", ")", "\n", "return", "cost", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.utils.get_TPU_estimator": [[34, 73], ["tensorflow.contrib.tpu.RunConfig", "tensorflow.contrib.tpu.TPUEstimator", "tensorflow.ConfigProto", "tensorflow.contrib.cluster_resolver.TPUClusterResolver", "tensorflow.contrib.tpu.TPUConfig"], "function", ["None"], ["", "def", "get_TPU_estimator", "(", "FLAGS", ",", "model_fn", ",", "model_dir", "=", "None", ")", ":", "\n", "##### Create TPUEstimator", "\n", "# TPU Configuration", "\n", "  ", "if", "FLAGS", ".", "use_tpu", ":", "\n", "    ", "if", "FLAGS", ".", "tpu", ":", "\n", "      ", "tpu_cluster", "=", "tf", ".", "contrib", ".", "cluster_resolver", ".", "TPUClusterResolver", "(", "\n", "FLAGS", ".", "tpu", ",", "zone", "=", "FLAGS", ".", "tpu_zone", ",", "project", "=", "FLAGS", ".", "gcp_project", ")", "\n", "", "else", ":", "\n", "      ", "tpu_cluster", "=", "None", "\n", "", "session_config", "=", "tf", ".", "ConfigProto", "(", "\n", "allow_soft_placement", "=", "True", ",", "log_device_placement", "=", "True", ")", "\n", "", "else", ":", "\n", "    ", "tpu_cluster", "=", "None", "\n", "session_config", "=", "None", "\n", "", "per_host_input", "=", "tf", ".", "contrib", ".", "tpu", ".", "InputPipelineConfig", ".", "PER_HOST_V2", "\n", "run_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "RunConfig", "(", "\n", "cluster", "=", "tpu_cluster", ",", "\n", "master", "=", "FLAGS", ".", "master", ",", "\n", "model_dir", "=", "model_dir", "or", "FLAGS", ".", "model_dir", ",", "\n", "session_config", "=", "session_config", ",", "\n", "tpu_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUConfig", "(", "\n", "# if there is name for the job, then add a name here", "\n", "iterations_per_loop", "=", "FLAGS", ".", "iterations", ",", "\n", "# num_shards=FLAGS.num_core_per_host * FLAGS.num_hosts,", "\n", "per_host_input_for_training", "=", "per_host_input", ")", ",", "\n", "keep_checkpoint_max", "=", "FLAGS", ".", "max_save", ",", "\n", "save_checkpoints_secs", "=", "None", ",", "\n", "save_checkpoints_steps", "=", "FLAGS", ".", "save_steps", "\n", ")", "\n", "\n", "# TPU Estimator", "\n", "estimator", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimator", "(", "\n", "model_fn", "=", "model_fn", ",", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "config", "=", "run_config", ",", "\n", "params", "=", "{", "\"model_dir\"", ":", "model_dir", "or", "FLAGS", ".", "model_dir", "}", ",", "\n", "train_batch_size", "=", "FLAGS", ".", "train_batch_size", ",", "\n", "eval_batch_size", "=", "FLAGS", ".", "eval_batch_size", ")", "\n", "return", "estimator", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.utils.construct_scalar_host_call": [[75, 102], ["list", "tensorflow.reshape", "metric_dict.keys", "tensorflow.train.get_or_create_global_step", "tensorflow.reshape", "tensorflow.contrib.summary.create_file_writer().as_default", "tensorflow.contrib.summary.always_record_summaries", "enumerate", "tensorflow.contrib.summary.all_summary_ops", "tensorflow.contrib.summary.create_file_writer", "reduce_fn", "tensorflow.contrib.summary.record_summaries_every_n_global_steps", "tensorflow.contrib.summary.scalar"], "function", ["None"], ["", "def", "construct_scalar_host_call", "(", "\n", "metric_dict", ",", "\n", "model_dir", ",", "\n", "prefix", "=", "\"\"", ",", "\n", "reduce_fn", "=", "None", ")", ":", "\n", "\n", "  ", "metric_names", "=", "list", "(", "metric_dict", ".", "keys", "(", ")", ")", "\n", "\n", "def", "host_call_fn", "(", "global_step", ",", "*", "args", ")", ":", "\n", "    ", "step", "=", "global_step", "[", "0", "]", "\n", "with", "tf", ".", "contrib", ".", "summary", ".", "create_file_writer", "(", "\n", "logdir", "=", "model_dir", ",", "filename_suffix", "=", "\".host_call\"", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "contrib", ".", "summary", ".", "always_record_summaries", "(", ")", ":", "\n", "        ", "for", "i", ",", "name", "in", "enumerate", "(", "metric_names", ")", ":", "\n", "          ", "if", "reduce_fn", "is", "None", ":", "\n", "            ", "scalar", "=", "args", "[", "i", "]", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "scalar", "=", "reduce_fn", "(", "args", "[", "i", "]", ")", "\n", "", "with", "tf", ".", "contrib", ".", "summary", ".", "record_summaries_every_n_global_steps", "(", "1000", ",", "step", ")", ":", "\n", "            ", "tf", ".", "contrib", ".", "summary", ".", "scalar", "(", "prefix", "+", "name", ",", "scalar", ",", "step", "=", "step", ")", "\n", "\n", "", "", "return", "tf", ".", "contrib", ".", "summary", ".", "all_summary_ops", "(", ")", "\n", "\n", "", "", "", "global_step_tensor", "=", "tf", ".", "reshape", "(", "tf", ".", "train", ".", "get_or_create_global_step", "(", ")", ",", "[", "1", "]", ")", "\n", "other_tensors", "=", "[", "tf", ".", "reshape", "(", "metric_dict", "[", "key", "]", ",", "[", "-", "1", "]", ")", "for", "key", "in", "metric_names", "]", "\n", "\n", "return", "host_call_fn", ",", "[", "global_step_tensor", "]", "+", "other_tensors", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.image.utils.get_all_variable": [[104, 113], ["tensorflow.global_variables", "list", "sorted", "tensorflow.trainable_variables", "tensorflow.get_collection", "set", "sorted.append"], "function", ["None"], ["", "def", "get_all_variable", "(", ")", ":", "\n", "  ", "var_list", "=", "tf", ".", "trainable_variables", "(", ")", "+", "tf", ".", "get_collection", "(", "'moving_vars'", ")", "\n", "for", "v", "in", "tf", ".", "global_variables", "(", ")", ":", "\n", "# We maintain ema for batch norm moving mean and variance as well.", "\n", "    ", "if", "'moving_mean'", "in", "v", ".", "name", "or", "'moving_variance'", "in", "v", ".", "name", ":", "\n", "      ", "var_list", ".", "append", "(", "v", ")", "\n", "", "", "var_list", "=", "list", "(", "set", "(", "var_list", ")", ")", "\n", "var_list", "=", "sorted", "(", "var_list", ",", "key", "=", "lambda", "var", ":", "var", ".", "name", ")", "\n", "return", "var_list", "\n", "", ""]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.round_int": [[29, 32], ["int", "math.floor"], "function", ["None"], ["def", "round_int", "(", "x", ")", ":", "\n", "  ", "\"\"\"Rounds `x` and then converts to an int.\"\"\"", "\n", "return", "int", "(", "math", ".", "floor", "(", "x", "+", "0.5", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.shortcut": [[34, 46], ["int", "randaugment.custom_ops.avg_pool", "tensorflow.pad"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.avg_pool"], ["", "def", "shortcut", "(", "x", ",", "output_filters", ",", "stride", ")", ":", "\n", "  ", "\"\"\"Applies strided avg pool or zero padding to make output_filters match x.\"\"\"", "\n", "num_filters", "=", "int", "(", "x", ".", "shape", "[", "3", "]", ")", "\n", "if", "stride", "==", "2", ":", "\n", "    ", "x", "=", "ops", ".", "avg_pool", "(", "x", ",", "2", ",", "stride", "=", "stride", ",", "padding", "=", "'SAME'", ")", "\n", "", "if", "num_filters", "!=", "output_filters", ":", "\n", "    ", "diff", "=", "output_filters", "-", "num_filters", "\n", "assert", "diff", ">", "0", "\n", "# Zero padd diff zeros", "\n", "padding", "=", "[", "[", "0", ",", "0", "]", ",", "[", "0", ",", "0", "]", ",", "[", "0", ",", "0", "]", ",", "[", "0", ",", "diff", "]", "]", "\n", "x", "=", "tf", ".", "pad", "(", "x", ",", "padding", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.calc_prob": [[48, 51], ["float"], "function", ["None"], ["", "def", "calc_prob", "(", "curr_layer", ",", "total_layers", ",", "p_l", ")", ":", "\n", "  ", "\"\"\"Calculates drop prob depending on the current layer.\"\"\"", "\n", "return", "1", "-", "(", "float", "(", "curr_layer", ")", "/", "total_layers", ")", "*", "p_l", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.bottleneck_layer": [[53, 97], ["tensorflow.variable_scope", "randaugment.custom_ops.batch_norm", "randaugment.custom_ops.conv2d", "randaugment.custom_ops.batch_norm", "tensorflow.nn.relu", "randaugment.custom_ops.conv2d", "randaugment.custom_ops.batch_norm", "tensorflow.nn.relu", "randaugment.custom_ops.conv2d", "randaugment.custom_ops.batch_norm", "shake_drop.shortcut", "tensorflow.random_uniform", "tensorflow.floor", "tensorflow.random_uniform", "tensorflow.random_uniform", "tensorflow.shape", "tensorflow.stop_gradient"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.batch_norm", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.conv2d", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.batch_norm", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.conv2d", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.batch_norm", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.conv2d", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.batch_norm", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.shortcut"], ["", "def", "bottleneck_layer", "(", "x", ",", "n", ",", "stride", ",", "prob", ",", "is_training", ",", "alpha", ",", "beta", ")", ":", "\n", "  ", "\"\"\"Bottleneck layer for shake drop model.\"\"\"", "\n", "assert", "alpha", "[", "1", "]", ">", "alpha", "[", "0", "]", "\n", "assert", "beta", "[", "1", "]", ">", "beta", "[", "0", "]", "\n", "with", "tf", ".", "variable_scope", "(", "'bottleneck_{}'", ".", "format", "(", "prob", ")", ")", ":", "\n", "    ", "input_layer", "=", "x", "\n", "x", "=", "ops", ".", "batch_norm", "(", "x", ",", "scope", "=", "'bn_1_pre'", ")", "\n", "x", "=", "ops", ".", "conv2d", "(", "x", ",", "n", ",", "1", ",", "scope", "=", "'1x1_conv_contract'", ")", "\n", "x", "=", "ops", ".", "batch_norm", "(", "x", ",", "scope", "=", "'bn_1_post'", ")", "\n", "x", "=", "tf", ".", "nn", ".", "relu", "(", "x", ")", "\n", "x", "=", "ops", ".", "conv2d", "(", "x", ",", "n", ",", "3", ",", "stride", "=", "stride", ",", "scope", "=", "'3x3'", ")", "\n", "x", "=", "ops", ".", "batch_norm", "(", "x", ",", "scope", "=", "'bn_2'", ")", "\n", "x", "=", "tf", ".", "nn", ".", "relu", "(", "x", ")", "\n", "x", "=", "ops", ".", "conv2d", "(", "x", ",", "n", "*", "4", ",", "1", ",", "scope", "=", "'1x1_conv_expand'", ")", "\n", "x", "=", "ops", ".", "batch_norm", "(", "x", ",", "scope", "=", "'bn_3'", ")", "\n", "\n", "# Apply regularization here", "\n", "# Sample bernoulli with prob", "\n", "if", "is_training", ":", "\n", "      ", "batch_size", "=", "tf", ".", "shape", "(", "x", ")", "[", "0", "]", "\n", "bern_shape", "=", "[", "batch_size", ",", "1", ",", "1", ",", "1", "]", "\n", "random_tensor", "=", "prob", "\n", "random_tensor", "+=", "tf", ".", "random_uniform", "(", "bern_shape", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "binary_tensor", "=", "tf", ".", "floor", "(", "random_tensor", ")", "\n", "\n", "alpha_values", "=", "tf", ".", "random_uniform", "(", "\n", "[", "batch_size", ",", "1", ",", "1", ",", "1", "]", ",", "minval", "=", "alpha", "[", "0", "]", ",", "maxval", "=", "alpha", "[", "1", "]", ",", "\n", "dtype", "=", "tf", ".", "float32", ")", "\n", "beta_values", "=", "tf", ".", "random_uniform", "(", "\n", "[", "batch_size", ",", "1", ",", "1", ",", "1", "]", ",", "minval", "=", "beta", "[", "0", "]", ",", "maxval", "=", "beta", "[", "1", "]", ",", "\n", "dtype", "=", "tf", ".", "float32", ")", "\n", "rand_forward", "=", "(", "\n", "binary_tensor", "+", "alpha_values", "-", "binary_tensor", "*", "alpha_values", ")", "\n", "rand_backward", "=", "(", "\n", "binary_tensor", "+", "beta_values", "-", "binary_tensor", "*", "beta_values", ")", "\n", "x", "=", "x", "*", "rand_backward", "+", "tf", ".", "stop_gradient", "(", "x", "*", "rand_forward", "-", "\n", "x", "*", "rand_backward", ")", "\n", "", "else", ":", "\n", "      ", "expected_alpha", "=", "(", "alpha", "[", "1", "]", "+", "alpha", "[", "0", "]", ")", "/", "2", "\n", "# prob is the expectation of the bernoulli variable", "\n", "x", "=", "(", "prob", "+", "expected_alpha", "-", "prob", "*", "expected_alpha", ")", "*", "x", "\n", "\n", "", "res", "=", "shortcut", "(", "input_layer", ",", "n", "*", "4", ",", "stride", ")", "\n", "return", "x", "+", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.build_shake_drop_model": [[99, 183], ["int", "randaugment.custom_ops.conv2d", "randaugment.custom_ops.batch_norm", "shake_drop.calc_prob", "shake_drop.bottleneck_layer", "range", "shake_drop.calc_prob", "shake_drop.bottleneck_layer", "range", "shake_drop.calc_prob", "shake_drop.bottleneck_layer", "range", "randaugment.custom_ops.batch_norm", "tensorflow.nn.relu", "randaugment.custom_ops.global_avg_pool", "randaugment.custom_ops.fc", "shake_drop.round_int", "shake_drop.calc_prob", "shake_drop.bottleneck_layer", "shake_drop.round_int", "shake_drop.calc_prob", "shake_drop.bottleneck_layer", "shake_drop.round_int", "shake_drop.calc_prob", "shake_drop.bottleneck_layer", "shake_drop.round_int", "shake_drop.round_int", "shake_drop.round_int"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.conv2d", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.batch_norm", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.calc_prob", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.bottleneck_layer", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.calc_prob", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.bottleneck_layer", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.calc_prob", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.bottleneck_layer", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.batch_norm", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.global_avg_pool", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.fc", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.round_int", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.calc_prob", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.bottleneck_layer", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.round_int", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.calc_prob", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.bottleneck_layer", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.round_int", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.calc_prob", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.bottleneck_layer", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.round_int", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.round_int", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_drop.round_int"], ["", "", "def", "build_shake_drop_model", "(", "images", ",", "num_classes", ",", "is_training", ")", ":", "\n", "  ", "\"\"\"Builds the PyramidNet Shake-Drop model.\n\n  Build the PyramidNet Shake-Drop model from https://arxiv.org/abs/1802.02375.\n\n  Args:\n    images: Tensor of images that will be fed into the Wide ResNet Model.\n    num_classes: Number of classed that the model needs to predict.\n    is_training: Is the model training or not.\n\n  Returns:\n    The logits of the PyramidNet Shake-Drop model.\n  \"\"\"", "\n", "\n", "is_training", "=", "is_training", "\n", "# ShakeDrop Hparams", "\n", "p_l", "=", "0.5", "\n", "alpha_shake", "=", "[", "-", "1", ",", "1", "]", "\n", "beta_shake", "=", "[", "0", ",", "1", "]", "\n", "\n", "# PyramidNet Hparams", "\n", "alpha", "=", "200", "\n", "depth", "=", "272", "\n", "# This is for the bottleneck architecture specifically", "\n", "n", "=", "int", "(", "(", "depth", "-", "2", ")", "/", "9", ")", "\n", "start_channel", "=", "16", "\n", "add_channel", "=", "alpha", "/", "(", "3", "*", "n", ")", "\n", "\n", "# Building the models", "\n", "x", "=", "images", "\n", "x", "=", "ops", ".", "conv2d", "(", "x", ",", "16", ",", "3", ",", "scope", "=", "'init_conv'", ")", "\n", "x", "=", "ops", ".", "batch_norm", "(", "x", ",", "scope", "=", "'init_bn'", ")", "\n", "\n", "layer_num", "=", "1", "\n", "total_layers", "=", "n", "*", "3", "\n", "start_channel", "+=", "add_channel", "\n", "prob", "=", "calc_prob", "(", "layer_num", ",", "total_layers", ",", "p_l", ")", "\n", "x", "=", "bottleneck_layer", "(", "\n", "x", ",", "round_int", "(", "start_channel", ")", ",", "1", ",", "prob", ",", "is_training", ",", "alpha_shake", ",", "\n", "beta_shake", ")", "\n", "layer_num", "+=", "1", "\n", "for", "_", "in", "range", "(", "1", ",", "n", ")", ":", "\n", "    ", "start_channel", "+=", "add_channel", "\n", "prob", "=", "calc_prob", "(", "layer_num", ",", "total_layers", ",", "p_l", ")", "\n", "x", "=", "bottleneck_layer", "(", "\n", "x", ",", "round_int", "(", "start_channel", ")", ",", "1", ",", "prob", ",", "is_training", ",", "alpha_shake", ",", "\n", "beta_shake", ")", "\n", "layer_num", "+=", "1", "\n", "\n", "", "start_channel", "+=", "add_channel", "\n", "prob", "=", "calc_prob", "(", "layer_num", ",", "total_layers", ",", "p_l", ")", "\n", "x", "=", "bottleneck_layer", "(", "\n", "x", ",", "round_int", "(", "start_channel", ")", ",", "2", ",", "prob", ",", "is_training", ",", "alpha_shake", ",", "\n", "beta_shake", ")", "\n", "layer_num", "+=", "1", "\n", "for", "_", "in", "range", "(", "1", ",", "n", ")", ":", "\n", "    ", "start_channel", "+=", "add_channel", "\n", "prob", "=", "calc_prob", "(", "layer_num", ",", "total_layers", ",", "p_l", ")", "\n", "x", "=", "bottleneck_layer", "(", "\n", "x", ",", "round_int", "(", "start_channel", ")", ",", "1", ",", "prob", ",", "is_training", ",", "alpha_shake", ",", "\n", "beta_shake", ")", "\n", "layer_num", "+=", "1", "\n", "\n", "", "start_channel", "+=", "add_channel", "\n", "prob", "=", "calc_prob", "(", "layer_num", ",", "total_layers", ",", "p_l", ")", "\n", "x", "=", "bottleneck_layer", "(", "\n", "x", ",", "round_int", "(", "start_channel", ")", ",", "2", ",", "prob", ",", "is_training", ",", "alpha_shake", ",", "\n", "beta_shake", ")", "\n", "layer_num", "+=", "1", "\n", "for", "_", "in", "range", "(", "1", ",", "n", ")", ":", "\n", "    ", "start_channel", "+=", "add_channel", "\n", "prob", "=", "calc_prob", "(", "layer_num", ",", "total_layers", ",", "p_l", ")", "\n", "x", "=", "bottleneck_layer", "(", "\n", "x", ",", "round_int", "(", "start_channel", ")", ",", "1", ",", "prob", ",", "is_training", ",", "alpha_shake", ",", "\n", "beta_shake", ")", "\n", "layer_num", "+=", "1", "\n", "\n", "", "assert", "layer_num", "-", "1", "==", "total_layers", "\n", "x", "=", "ops", ".", "batch_norm", "(", "x", ",", "scope", "=", "'final_bn'", ")", "\n", "x", "=", "tf", ".", "nn", ".", "relu", "(", "x", ")", "\n", "x", "=", "ops", ".", "global_avg_pool", "(", "x", ")", "\n", "# Fully connected", "\n", "logits", "=", "ops", ".", "fc", "(", "x", ",", "num_classes", ")", "\n", "return", "logits", "\n", "", ""]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.policies.imagenet_policies": [[25, 55], ["None"], "function", ["None"], ["def", "imagenet_policies", "(", ")", ":", "\n", "  ", "\"\"\"AutoAugment policies found on ImageNet.\n\n  This policy also transfers to five FGVC datasets with image size similar to\n  ImageNet including Oxford 102 Flowers, Caltech-101, Oxford-IIIT Pets,\n  FGVC Aircraft and Stanford Cars.\n  \"\"\"", "\n", "policies", "=", "[", "\n", "[", "(", "\"Posterize\"", ",", "0.4", ",", "8", ")", ",", "(", "\"Rotate\"", ",", "0.6", ",", "9", ")", "]", ",", "\n", "[", "(", "\"Solarize\"", ",", "0.6", ",", "5", ")", ",", "(", "\"AutoContrast\"", ",", "0.6", ",", "5", ")", "]", ",", "\n", "[", "(", "\"Equalize\"", ",", "0.8", ",", "8", ")", ",", "(", "\"Equalize\"", ",", "0.6", ",", "3", ")", "]", ",", "\n", "[", "(", "\"Posterize\"", ",", "0.6", ",", "7", ")", ",", "(", "\"Posterize\"", ",", "0.6", ",", "6", ")", "]", ",", "\n", "[", "(", "\"Equalize\"", ",", "0.4", ",", "7", ")", ",", "(", "\"Solarize\"", ",", "0.2", ",", "4", ")", "]", ",", "\n", "[", "(", "\"Equalize\"", ",", "0.4", ",", "4", ")", ",", "(", "\"Rotate\"", ",", "0.8", ",", "8", ")", "]", ",", "\n", "[", "(", "\"Solarize\"", ",", "0.6", ",", "3", ")", ",", "(", "\"Equalize\"", ",", "0.6", ",", "7", ")", "]", ",", "\n", "[", "(", "\"Posterize\"", ",", "0.8", ",", "5", ")", ",", "(", "\"Equalize\"", ",", "1.0", ",", "2", ")", "]", ",", "\n", "[", "(", "\"Rotate\"", ",", "0.2", ",", "3", ")", ",", "(", "\"Solarize\"", ",", "0.6", ",", "8", ")", "]", ",", "\n", "[", "(", "\"Equalize\"", ",", "0.6", ",", "8", ")", ",", "(", "\"Posterize\"", ",", "0.4", ",", "6", ")", "]", ",", "\n", "[", "(", "\"Rotate\"", ",", "0.8", ",", "8", ")", ",", "(", "\"Color\"", ",", "0.4", ",", "0", ")", "]", ",", "\n", "[", "(", "\"Rotate\"", ",", "0.4", ",", "9", ")", ",", "(", "\"Equalize\"", ",", "0.6", ",", "2", ")", "]", ",", "\n", "[", "(", "\"Equalize\"", ",", "0.0", ",", "7", ")", ",", "(", "\"Equalize\"", ",", "0.8", ",", "8", ")", "]", ",", "\n", "[", "(", "\"Invert\"", ",", "0.6", ",", "4", ")", ",", "(", "\"Equalize\"", ",", "1.0", ",", "8", ")", "]", ",", "\n", "[", "(", "\"Color\"", ",", "0.6", ",", "4", ")", ",", "(", "\"Contrast\"", ",", "1.0", ",", "8", ")", "]", ",", "\n", "[", "(", "\"Rotate\"", ",", "0.8", ",", "8", ")", ",", "(", "\"Color\"", ",", "1.0", ",", "2", ")", "]", ",", "\n", "[", "(", "\"Color\"", ",", "0.8", ",", "8", ")", ",", "(", "\"Solarize\"", ",", "0.8", ",", "7", ")", "]", ",", "\n", "[", "(", "\"Sharpness\"", ",", "0.4", ",", "7", ")", ",", "(", "\"Invert\"", ",", "0.6", ",", "8", ")", "]", ",", "\n", "[", "(", "\"ShearX\"", ",", "0.6", ",", "5", ")", ",", "(", "\"Equalize\"", ",", "1.0", ",", "9", ")", "]", ",", "\n", "[", "(", "\"Color\"", ",", "0.4", ",", "0", ")", ",", "(", "\"Equalize\"", ",", "0.6", ",", "3", ")", "]", "\n", "]", "\n", "return", "policies", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.policies.get_trans_list": [[57, 63], ["None"], "function", ["None"], ["", "def", "get_trans_list", "(", ")", ":", "\n", "  ", "trans_list", "=", "[", "\n", "'Invert'", ",", "'Cutout'", ",", "'Sharpness'", ",", "'AutoContrast'", ",", "'Posterize'", ",", "\n", "'ShearX'", ",", "'TranslateX'", ",", "'TranslateY'", ",", "'ShearY'", ",", "'Rotate'", ",", "\n", "'Equalize'", ",", "'Contrast'", ",", "'Color'", ",", "'Solarize'", ",", "'Brightness'", "]", "\n", "return", "trans_list", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.policies.randaug_policies": [[65, 77], ["policies.get_trans_list", "tensorflow.logging.info", "str", "range"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.policies.get_trans_list"], ["", "def", "randaug_policies", "(", ")", ":", "\n", "  ", "trans_list", "=", "get_trans_list", "(", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"trans_list: %s\"", ",", "str", "(", "trans_list", ")", ")", "\n", "op_list", "=", "[", "]", "\n", "for", "trans", "in", "trans_list", ":", "\n", "    ", "for", "magnitude", "in", "range", "(", "1", ",", "10", ")", ":", "\n", "      ", "op_list", "+=", "[", "(", "trans", ",", "0.5", ",", "magnitude", ")", "]", "\n", "", "", "policies", "=", "[", "]", "\n", "for", "op_1", "in", "op_list", ":", "\n", "    ", "for", "op_2", "in", "op_list", ":", "\n", "      ", "policies", "+=", "[", "[", "op_1", ",", "op_2", "]", "]", "\n", "", "", "return", "policies", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.TransformFunction.__init__": [[227, 230], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "func", ",", "name", ")", ":", "\n", "    ", "self", ".", "f", "=", "func", "\n", "self", ".", "name", "=", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.TransformFunction.__repr__": [[231, 233], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "    ", "return", "'<'", "+", "self", ".", "name", "+", "'>'", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.TransformFunction.__call__": [[234, 236], ["augmentation_transforms.TransformFunction.f"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "pil_img", ")", ":", "\n", "    ", "return", "self", ".", "f", "(", "pil_img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.TransformT.__init__": [[241, 244], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "name", ",", "xform_fn", ")", ":", "\n", "    ", "self", ".", "name", "=", "name", "\n", "self", ".", "xform", "=", "xform_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.TransformT.pil_transformer": [[245, 254], ["augmentation_transforms.TransformFunction", "random.random", "augmentation_transforms.TransformT.xform"], "methods", ["None"], ["", "def", "pil_transformer", "(", "self", ",", "probability", ",", "level", ",", "img_shape", ")", ":", "\n", "\n", "    ", "def", "return_function", "(", "im", ")", ":", "\n", "      ", "if", "random", ".", "random", "(", ")", "<", "probability", ":", "\n", "        ", "im", "=", "self", ".", "xform", "(", "im", ",", "level", ",", "img_shape", ")", "\n", "", "return", "im", "\n", "\n", "", "name", "=", "self", ".", "name", "+", "'({:.1f},{})'", ".", "format", "(", "probability", ",", "level", ")", "\n", "return", "TransformFunction", "(", "return_function", ",", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.get_mean_and_std": [[40, 50], ["None"], "function", ["None"], ["def", "get_mean_and_std", "(", ")", ":", "\n", "  ", "if", "FLAGS", ".", "task_name", "==", "\"cifar10\"", ":", "\n", "    ", "means", "=", "[", "0.49139968", ",", "0.48215841", ",", "0.44653091", "]", "\n", "stds", "=", "[", "0.24703223", ",", "0.24348513", ",", "0.26158784", "]", "\n", "", "elif", "FLAGS", ".", "task_name", "==", "\"svhn\"", ":", "\n", "    ", "means", "=", "[", "0.4376821", ",", "0.4437697", ",", "0.47280442", "]", "\n", "stds", "=", "[", "0.19803012", ",", "0.20101562", ",", "0.19703614", "]", "\n", "", "else", ":", "\n", "    ", "assert", "False", "\n", "", "return", "means", ",", "stds", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms._width_height_from_img_shape": [[52, 55], ["None"], "function", ["None"], ["", "def", "_width_height_from_img_shape", "(", "img_shape", ")", ":", "\n", "  ", "\"\"\"`img_shape` in autoaugment is (height, width).\"\"\"", "\n", "return", "(", "img_shape", "[", "1", "]", ",", "img_shape", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.random_flip": [[57, 62], ["numpy.fliplr", "numpy.random.rand"], "function", ["None"], ["", "def", "random_flip", "(", "x", ")", ":", "\n", "  ", "\"\"\"Flip the input x horizontally with 50% probability.\"\"\"", "\n", "if", "np", ".", "random", ".", "rand", "(", "1", ")", "[", "0", "]", ">", "0.5", ":", "\n", "    ", "return", "np", ".", "fliplr", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.zero_pad_and_crop": [[64, 83], ["numpy.zeros", "numpy.random.randint", "numpy.random.randint"], "function", ["None"], ["", "def", "zero_pad_and_crop", "(", "img", ",", "amount", "=", "4", ")", ":", "\n", "  ", "\"\"\"Zero pad by `amount` zero pixels on each side then take a random crop.\n\n  Args:\n    img: numpy image that will be zero padded and cropped.\n    amount: amount of zeros to pad `img` with horizontally and verically.\n\n  Returns:\n    The cropped zero padded img. The returned numpy array will be of the same\n    shape as `img`.\n  \"\"\"", "\n", "padded_img", "=", "np", ".", "zeros", "(", "(", "img", ".", "shape", "[", "0", "]", "+", "amount", "*", "2", ",", "img", ".", "shape", "[", "1", "]", "+", "amount", "*", "2", ",", "\n", "img", ".", "shape", "[", "2", "]", ")", ")", "\n", "padded_img", "[", "amount", ":", "img", ".", "shape", "[", "0", "]", "+", "amount", ",", "amount", ":", "\n", "img", ".", "shape", "[", "1", "]", "+", "amount", ",", ":", "]", "=", "img", "\n", "top", "=", "np", ".", "random", ".", "randint", "(", "low", "=", "0", ",", "high", "=", "2", "*", "amount", ")", "\n", "left", "=", "np", ".", "random", ".", "randint", "(", "low", "=", "0", ",", "high", "=", "2", "*", "amount", ")", "\n", "new_img", "=", "padded_img", "[", "top", ":", "top", "+", "img", ".", "shape", "[", "0", "]", ",", "left", ":", "left", "+", "img", ".", "shape", "[", "1", "]", ",", ":", "]", "\n", "return", "new_img", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.create_cutout_mask": [[85, 121], ["numpy.random.randint", "numpy.random.randint", "numpy.ones", "numpy.zeros", "max", "max", "min", "min"], "function", ["None"], ["", "def", "create_cutout_mask", "(", "img_height", ",", "img_width", ",", "num_channels", ",", "size", ")", ":", "\n", "  ", "\"\"\"Creates a zero mask used for cutout of shape `img_height` x `img_width`.\n\n  Args:\n    img_height: Height of image cutout mask will be applied to.\n    img_width: Width of image cutout mask will be applied to.\n    num_channels: Number of channels in the image.\n    size: Size of the zeros mask.\n\n  Returns:\n    A mask of shape `img_height` x `img_width` with all ones except for a\n    square of zeros of shape `size` x `size`. This mask is meant to be\n    elementwise multiplied with the original image. Additionally returns\n    the `upper_coord` and `lower_coord` which specify where the cutout mask\n    will be applied.\n  \"\"\"", "\n", "assert", "img_height", "==", "img_width", "\n", "\n", "# Sample center where cutout mask will be applied", "\n", "height_loc", "=", "np", ".", "random", ".", "randint", "(", "low", "=", "0", ",", "high", "=", "img_height", ")", "\n", "width_loc", "=", "np", ".", "random", ".", "randint", "(", "low", "=", "0", ",", "high", "=", "img_width", ")", "\n", "\n", "# Determine upper right and lower left corners of patch", "\n", "upper_coord", "=", "(", "max", "(", "0", ",", "height_loc", "-", "size", "//", "2", ")", ",", "max", "(", "0", ",", "width_loc", "-", "size", "//", "2", ")", ")", "\n", "lower_coord", "=", "(", "min", "(", "img_height", ",", "height_loc", "+", "size", "//", "2", ")", ",", "\n", "min", "(", "img_width", ",", "width_loc", "+", "size", "//", "2", ")", ")", "\n", "mask_height", "=", "lower_coord", "[", "0", "]", "-", "upper_coord", "[", "0", "]", "\n", "mask_width", "=", "lower_coord", "[", "1", "]", "-", "upper_coord", "[", "1", "]", "\n", "assert", "mask_height", ">", "0", "\n", "assert", "mask_width", ">", "0", "\n", "\n", "mask", "=", "np", ".", "ones", "(", "(", "img_height", ",", "img_width", ",", "num_channels", ")", ")", "\n", "zeros", "=", "np", ".", "zeros", "(", "(", "mask_height", ",", "mask_width", ",", "num_channels", ")", ")", "\n", "mask", "[", "upper_coord", "[", "0", "]", ":", "lower_coord", "[", "0", "]", ",", "upper_coord", "[", "1", "]", ":", "lower_coord", "[", "1", "]", ",", ":", "]", "=", "(", "\n", "zeros", ")", "\n", "return", "mask", ",", "upper_coord", ",", "lower_coord", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.cutout_numpy": [[123, 142], ["augmentation_transforms.create_cutout_mask", "len"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.create_cutout_mask"], ["", "def", "cutout_numpy", "(", "img", ",", "size", "=", "16", ")", ":", "\n", "  ", "\"\"\"Apply cutout with mask of shape `size` x `size` to `img`.\n\n  The cutout operation is from the paper https://arxiv.org/abs/1708.04552.\n  This operation applies a `size`x`size` mask of zeros to a random location\n  within `img`.\n\n  Args:\n    img: Numpy image that cutout will be applied to.\n    size: Height/width of the cutout mask that will be\n\n  Returns:\n    A numpy tensor that is the result of applying the cutout mask to `img`.\n  \"\"\"", "\n", "img_height", ",", "img_width", ",", "num_channels", "=", "(", "img", ".", "shape", "[", "0", "]", ",", "img", ".", "shape", "[", "1", "]", ",", "\n", "img", ".", "shape", "[", "2", "]", ")", "\n", "assert", "len", "(", "img", ".", "shape", ")", "==", "3", "\n", "mask", ",", "_", ",", "_", "=", "create_cutout_mask", "(", "img_height", ",", "img_width", ",", "num_channels", ",", "size", ")", "\n", "return", "img", "*", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.float_parameter": [[144, 156], ["float"], "function", ["None"], ["", "def", "float_parameter", "(", "level", ",", "maxval", ")", ":", "\n", "  ", "\"\"\"Helper function to scale `val` between 0 and maxval .\n\n  Args:\n    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n    maxval: Maximum value that the operation can have. This will be scaled\n      to level/PARAMETER_MAX.\n\n  Returns:\n    A float that results from scaling `maxval` according to `level`.\n  \"\"\"", "\n", "return", "float", "(", "level", ")", "*", "maxval", "/", "PARAMETER_MAX", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.int_parameter": [[158, 170], ["int"], "function", ["None"], ["", "def", "int_parameter", "(", "level", ",", "maxval", ")", ":", "\n", "  ", "\"\"\"Helper function to scale `val` between 0 and maxval .\n\n  Args:\n    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n    maxval: Maximum value that the operation can have. This will be scaled\n      to level/PARAMETER_MAX.\n\n  Returns:\n    An int that results from scaling `maxval` according to `level`.\n  \"\"\"", "\n", "return", "int", "(", "level", "*", "maxval", "/", "PARAMETER_MAX", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.pil_wrap": [[172, 184], ["PIL.Image.fromarray().convert", "augmentation_transforms.get_mean_and_std", "PIL.Image.fromarray", "numpy.uint8"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.get_mean_and_std"], ["", "def", "pil_wrap", "(", "img", ",", "use_mean_std", ")", ":", "\n", "  ", "\"\"\"Convert the `img` numpy tensor to a PIL Image.\"\"\"", "\n", "\n", "if", "use_mean_std", ":", "\n", "    ", "MEANS", ",", "STDS", "=", "get_mean_and_std", "(", ")", "\n", "", "else", ":", "\n", "    ", "MEANS", "=", "[", "0", ",", "0", ",", "0", "]", "\n", "STDS", "=", "[", "1", ",", "1", ",", "1", "]", "\n", "", "img_ori", "=", "(", "img", "*", "STDS", "+", "MEANS", ")", "*", "255", "\n", "\n", "return", "Image", ".", "fromarray", "(", "\n", "np", ".", "uint8", "(", "(", "img", "*", "STDS", "+", "MEANS", ")", "*", "255.0", ")", ")", ".", "convert", "(", "'RGBA'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.pil_unwrap": [[186, 198], ["numpy.where", "augmentation_transforms.get_mean_and_std", "numpy.array().reshape", "numpy.array", "pil_img.getdata"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.get_mean_and_std"], ["", "def", "pil_unwrap", "(", "pil_img", ",", "use_mean_std", ",", "img_shape", ")", ":", "\n", "  ", "\"\"\"Converts the PIL img to a numpy array.\"\"\"", "\n", "if", "use_mean_std", ":", "\n", "    ", "MEANS", ",", "STDS", "=", "get_mean_and_std", "(", ")", "\n", "", "else", ":", "\n", "    ", "MEANS", "=", "[", "0", ",", "0", ",", "0", "]", "\n", "STDS", "=", "[", "1", ",", "1", ",", "1", "]", "\n", "", "pic_array", "=", "np", ".", "array", "(", "pil_img", ".", "getdata", "(", ")", ")", ".", "reshape", "(", "(", "img_shape", "[", "0", "]", ",", "img_shape", "[", "1", "]", ",", "4", ")", ")", "/", "255.0", "\n", "i1", ",", "i2", "=", "np", ".", "where", "(", "pic_array", "[", ":", ",", ":", ",", "3", "]", "==", "0", ")", "\n", "pic_array", "=", "(", "pic_array", "[", ":", ",", ":", ",", ":", "3", "]", "-", "MEANS", ")", "/", "STDS", "\n", "pic_array", "[", "i1", ",", "i2", "]", "=", "[", "0", ",", "0", ",", "0", "]", "\n", "return", "pic_array", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.apply_policy": [[200, 222], ["augmentation_transforms.pil_wrap", "augmentation_transforms.pil_unwrap", "NAME_TO_TRANSFORM[].pil_transformer", "NAME_TO_TRANSFORM[].pil_transformer.", "len"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.pil_wrap", "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.pil_unwrap", "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.TransformT.pil_transformer"], ["", "def", "apply_policy", "(", "policy", ",", "img", ",", "use_mean_std", "=", "True", ")", ":", "\n", "  ", "\"\"\"Apply the `policy` to the numpy `img`.\n\n  Args:\n    policy: A list of tuples with the form (name, probability, level) where\n      `name` is the name of the augmentation operation to apply, `probability`\n      is the probability of applying the operation and `level` is what strength\n      the operation to apply.\n    img: Numpy image that will have `policy` applied to it.\n\n  Returns:\n    The result of applying `policy` to `img`.\n  \"\"\"", "\n", "img_shape", "=", "img", ".", "shape", "\n", "pil_img", "=", "pil_wrap", "(", "img", ",", "use_mean_std", ")", "\n", "for", "xform", "in", "policy", ":", "\n", "    ", "assert", "len", "(", "xform", ")", "==", "3", "\n", "name", ",", "probability", ",", "level", "=", "xform", "\n", "xform_fn", "=", "NAME_TO_TRANSFORM", "[", "name", "]", ".", "pil_transformer", "(", "\n", "probability", ",", "level", ",", "img_shape", ")", "\n", "pil_img", "=", "xform_fn", "(", "pil_img", ")", "\n", "", "return", "pil_unwrap", "(", "pil_img", ",", "use_mean_std", ",", "img_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms._rotate_impl": [[285, 291], ["augmentation_transforms.int_parameter", "pil_img.rotate", "random.random"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.int_parameter"], ["def", "_rotate_impl", "(", "pil_img", ",", "level", ",", "_", ")", ":", "\n", "  ", "\"\"\"Rotates `pil_img` from -30 to 30 degrees depending on `level`.\"\"\"", "\n", "degrees", "=", "int_parameter", "(", "level", ",", "30", ")", "\n", "if", "random", ".", "random", "(", ")", ">", "0.5", ":", "\n", "    ", "degrees", "=", "-", "degrees", "\n", "", "return", "pil_img", ".", "rotate", "(", "degrees", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms._posterize_impl": [[296, 300], ["augmentation_transforms.int_parameter", "PIL.ImageOps.posterize().convert", "PIL.ImageOps.posterize", "pil_img.convert"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.int_parameter"], ["def", "_posterize_impl", "(", "pil_img", ",", "level", ",", "_", ")", ":", "\n", "  ", "\"\"\"Applies PIL Posterize to `pil_img`.\"\"\"", "\n", "level", "=", "int_parameter", "(", "level", ",", "4", ")", "\n", "return", "ImageOps", ".", "posterize", "(", "pil_img", ".", "convert", "(", "'RGB'", ")", ",", "4", "-", "level", ")", ".", "convert", "(", "'RGBA'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms._shear_x_impl": [[305, 326], ["augmentation_transforms.float_parameter", "pil_img.transform", "random.random", "augmentation_transforms._width_height_from_img_shape"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.float_parameter", "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms._width_height_from_img_shape"], ["def", "_shear_x_impl", "(", "pil_img", ",", "level", ",", "img_shape", ")", ":", "\n", "  ", "\"\"\"Applies PIL ShearX to `pil_img`.\n\n  The ShearX operation shears the image along the horizontal axis with `level`\n  magnitude.\n\n  Args:\n    pil_img: Image in PIL object.\n    level: Strength of the operation specified as an Integer from\n      [0, `PARAMETER_MAX`].\n\n  Returns:\n    A PIL Image that has had ShearX applied to it.\n  \"\"\"", "\n", "level", "=", "float_parameter", "(", "level", ",", "0.3", ")", "\n", "if", "random", ".", "random", "(", ")", ">", "0.5", ":", "\n", "    ", "level", "=", "-", "level", "\n", "", "return", "pil_img", ".", "transform", "(", "\n", "_width_height_from_img_shape", "(", "img_shape", ")", ",", "\n", "Image", ".", "AFFINE", ",", "\n", "(", "1", ",", "level", ",", "0", ",", "0", ",", "1", ",", "0", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms._shear_y_impl": [[331, 352], ["augmentation_transforms.float_parameter", "pil_img.transform", "random.random", "augmentation_transforms._width_height_from_img_shape"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.float_parameter", "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms._width_height_from_img_shape"], ["def", "_shear_y_impl", "(", "pil_img", ",", "level", ",", "img_shape", ")", ":", "\n", "  ", "\"\"\"Applies PIL ShearY to `pil_img`.\n\n  The ShearY operation shears the image along the vertical axis with `level`\n  magnitude.\n\n  Args:\n    pil_img: Image in PIL object.\n    level: Strength of the operation specified as an Integer from\n      [0, `PARAMETER_MAX`].\n\n  Returns:\n    A PIL Image that has had ShearX applied to it.\n  \"\"\"", "\n", "level", "=", "float_parameter", "(", "level", ",", "0.3", ")", "\n", "if", "random", ".", "random", "(", ")", ">", "0.5", ":", "\n", "    ", "level", "=", "-", "level", "\n", "", "return", "pil_img", ".", "transform", "(", "\n", "_width_height_from_img_shape", "(", "img_shape", ")", ",", "\n", "Image", ".", "AFFINE", ",", "\n", "(", "1", ",", "0", ",", "0", ",", "level", ",", "1", ",", "0", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms._translate_x_impl": [[357, 378], ["augmentation_transforms.int_parameter", "pil_img.transform", "random.random", "augmentation_transforms._width_height_from_img_shape"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.int_parameter", "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms._width_height_from_img_shape"], ["def", "_translate_x_impl", "(", "pil_img", ",", "level", ",", "img_shape", ")", ":", "\n", "  ", "\"\"\"Applies PIL TranslateX to `pil_img`.\n\n  Translate the image in the horizontal direction by `level`\n  number of pixels.\n\n  Args:\n    pil_img: Image in PIL object.\n    level: Strength of the operation specified as an Integer from\n      [0, `PARAMETER_MAX`].\n\n  Returns:\n    A PIL Image that has had TranslateX applied to it.\n  \"\"\"", "\n", "level", "=", "int_parameter", "(", "level", ",", "10", ")", "\n", "if", "random", ".", "random", "(", ")", ">", "0.5", ":", "\n", "    ", "level", "=", "-", "level", "\n", "", "return", "pil_img", ".", "transform", "(", "\n", "_width_height_from_img_shape", "(", "img_shape", ")", ",", "\n", "Image", ".", "AFFINE", ",", "\n", "(", "1", ",", "0", ",", "level", ",", "0", ",", "1", ",", "0", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms._translate_y_impl": [[383, 404], ["augmentation_transforms.int_parameter", "pil_img.transform", "random.random", "augmentation_transforms._width_height_from_img_shape"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.int_parameter", "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms._width_height_from_img_shape"], ["def", "_translate_y_impl", "(", "pil_img", ",", "level", ",", "img_shape", ")", ":", "\n", "  ", "\"\"\"Applies PIL TranslateY to `pil_img`.\n\n  Translate the image in the vertical direction by `level`\n  number of pixels.\n\n  Args:\n    pil_img: Image in PIL object.\n    level: Strength of the operation specified as an Integer from\n      [0, `PARAMETER_MAX`].\n\n  Returns:\n    A PIL Image that has had TranslateY applied to it.\n  \"\"\"", "\n", "level", "=", "int_parameter", "(", "level", ",", "10", ")", "\n", "if", "random", ".", "random", "(", ")", ">", "0.5", ":", "\n", "    ", "level", "=", "-", "level", "\n", "", "return", "pil_img", ".", "transform", "(", "\n", "_width_height_from_img_shape", "(", "img_shape", ")", ",", "\n", "Image", ".", "AFFINE", ",", "\n", "(", "1", ",", "0", ",", "0", ",", "0", ",", "1", ",", "level", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms._crop_impl": [[409, 414], ["pil_img.crop", "pil_img.crop.resize"], "function", ["home.repos.pwc.inspect_result.google-research_uda.image.data.crop"], ["def", "_crop_impl", "(", "pil_img", ",", "level", ",", "img_shape", ",", "interpolation", "=", "Image", ".", "BILINEAR", ")", ":", "\n", "  ", "\"\"\"Applies a crop to `pil_img` with the size depending on the `level`.\"\"\"", "\n", "cropped", "=", "pil_img", ".", "crop", "(", "(", "level", ",", "level", ",", "img_shape", "[", "0", "]", "-", "level", ",", "img_shape", "[", "1", "]", "-", "level", ")", ")", "\n", "resized", "=", "cropped", ".", "resize", "(", "(", "img_shape", "[", "0", "]", ",", "img_shape", "[", "1", "]", ")", ",", "interpolation", ")", "\n", "return", "resized", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms._solarize_impl": [[419, 435], ["augmentation_transforms.int_parameter", "PIL.ImageOps.solarize().convert", "PIL.ImageOps.solarize", "pil_img.convert"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.int_parameter"], ["def", "_solarize_impl", "(", "pil_img", ",", "level", ",", "_", ")", ":", "\n", "  ", "\"\"\"Applies PIL Solarize to `pil_img`.\n\n  Translate the image in the vertical direction by `level`\n  number of pixels.\n\n  Args:\n    pil_img: Image in PIL object.\n    level: Strength of the operation specified as an Integer from\n      [0, `PARAMETER_MAX`].\n\n  Returns:\n    A PIL Image that has had Solarize applied to it.\n  \"\"\"", "\n", "level", "=", "int_parameter", "(", "level", ",", "256", ")", "\n", "return", "ImageOps", ".", "solarize", "(", "pil_img", ".", "convert", "(", "'RGB'", ")", ",", "256", "-", "level", ")", ".", "convert", "(", "'RGBA'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms._cutout_pil_impl": [[440, 453], ["augmentation_transforms.int_parameter", "augmentation_transforms.create_cutout_mask", "pil_img.load", "range", "range"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.int_parameter", "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.create_cutout_mask"], ["def", "_cutout_pil_impl", "(", "pil_img", ",", "level", ",", "img_shape", ")", ":", "\n", "  ", "\"\"\"Apply cutout to pil_img at the specified level.\"\"\"", "\n", "size", "=", "int_parameter", "(", "level", ",", "20", ")", "\n", "if", "size", "<=", "0", ":", "\n", "    ", "return", "pil_img", "\n", "", "img_height", ",", "img_width", ",", "num_channels", "=", "(", "img_shape", "[", "0", "]", ",", "img_shape", "[", "1", "]", ",", "3", ")", "\n", "_", ",", "upper_coord", ",", "lower_coord", "=", "(", "\n", "create_cutout_mask", "(", "img_height", ",", "img_width", ",", "num_channels", ",", "size", ")", ")", "\n", "pixels", "=", "pil_img", ".", "load", "(", ")", "# create the pixel map", "\n", "for", "i", "in", "range", "(", "upper_coord", "[", "0", "]", ",", "lower_coord", "[", "0", "]", ")", ":", "# for every col:", "\n", "    ", "for", "j", "in", "range", "(", "upper_coord", "[", "1", "]", ",", "lower_coord", "[", "1", "]", ")", ":", "# For every row", "\n", "      ", "pixels", "[", "i", ",", "j", "]", "=", "(", "125", ",", "122", ",", "113", ",", "0", ")", "# set the colour accordingly", "\n", "", "", "return", "pil_img", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms._enhancer_impl": [[457, 463], ["enhancer().enhance", "augmentation_transforms.float_parameter", "enhancer"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.augmentation_transforms.float_parameter"], ["def", "_enhancer_impl", "(", "enhancer", ")", ":", "\n", "  ", "\"\"\"Sets level to be between 0.1 and 1.8 for ImageEnhance transforms of PIL.\"\"\"", "\n", "def", "impl", "(", "pil_img", ",", "level", ",", "_", ")", ":", "\n", "    ", "v", "=", "float_parameter", "(", "level", ",", "1.8", ")", "+", ".1", "# going to 0 just destroys it", "\n", "return", "enhancer", "(", "pil_img", ")", ".", "enhance", "(", "v", ")", "\n", "", "return", "impl", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.variable": [[36, 45], ["tensorflow.get_variable"], "function", ["None"], ["def", "variable", "(", "name", ",", "shape", ",", "dtype", ",", "initializer", ",", "trainable", ")", ":", "\n", "  ", "\"\"\"Returns a TF variable with the passed in specifications.\"\"\"", "\n", "var", "=", "tf", ".", "get_variable", "(", "\n", "name", ",", "\n", "shape", "=", "shape", ",", "\n", "dtype", "=", "dtype", ",", "\n", "initializer", "=", "initializer", ",", "\n", "trainable", "=", "trainable", ")", "\n", "return", "var", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.global_avg_pool": [[47, 60], ["tensorflow.name_scope", "tensorflow.nn.avg_pool", "tensorflow.squeeze", "x.get_shape", "int", "int"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.avg_pool"], ["", "def", "global_avg_pool", "(", "x", ",", "scope", "=", "None", ")", ":", "\n", "  ", "\"\"\"Average pools away spatial height and width dimension of 4D tensor.\"\"\"", "\n", "assert", "x", ".", "get_shape", "(", ")", ".", "ndims", "==", "4", "\n", "with", "tf", ".", "name_scope", "(", "scope", ",", "'global_avg_pool'", ",", "[", "x", "]", ")", ":", "\n", "    ", "kernel_size", "=", "(", "1", ",", "int", "(", "x", ".", "shape", "[", "1", "]", ")", ",", "int", "(", "x", ".", "shape", "[", "2", "]", ")", ",", "1", ")", "\n", "squeeze_dims", "=", "(", "1", ",", "2", ")", "\n", "result", "=", "tf", ".", "nn", ".", "avg_pool", "(", "\n", "x", ",", "\n", "ksize", "=", "kernel_size", ",", "\n", "strides", "=", "(", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "padding", "=", "'VALID'", ",", "\n", "data_format", "=", "'NHWC'", ")", "\n", "return", "tf", ".", "squeeze", "(", "result", ",", "squeeze_dims", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.zero_pad": [[62, 68], ["tensorflow.pad"], "function", ["None"], ["", "", "def", "zero_pad", "(", "inputs", ",", "in_filter", ",", "out_filter", ")", ":", "\n", "  ", "\"\"\"Zero pads `input` tensor to have `out_filter` number of filters.\"\"\"", "\n", "outputs", "=", "tf", ".", "pad", "(", "inputs", ",", "[", "[", "0", ",", "0", "]", ",", "[", "0", ",", "0", "]", ",", "[", "0", ",", "0", "]", ",", "\n", "[", "(", "out_filter", "-", "in_filter", ")", "//", "2", ",", "\n", "(", "out_filter", "-", "in_filter", ")", "//", "2", "]", "]", ")", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.batch_norm": [[70, 94], ["tensorflow.layers.batch_normalization"], "function", ["None"], ["", "@", "tf", ".", "contrib", ".", "framework", ".", "add_arg_scope", "\n", "def", "batch_norm", "(", "inputs", ",", "\n", "update_stats", "=", "True", ",", "\n", "decay", "=", "0.999", ",", "\n", "center", "=", "True", ",", "\n", "scale", "=", "False", ",", "\n", "epsilon", "=", "0.001", ",", "\n", "is_training", "=", "True", ",", "\n", "reuse", "=", "None", ",", "\n", "scope", "=", "None", ",", "\n", ")", ":", "\n", "  ", "\"\"\"Small wrapper around tf.contrib.layers.batch_norm.\"\"\"", "\n", "batch_norm_op", "=", "tf", ".", "layers", ".", "batch_normalization", "(", "\n", "inputs", ",", "\n", "axis", "=", "-", "1", ",", "\n", "momentum", "=", "decay", ",", "\n", "epsilon", "=", "epsilon", ",", "\n", "center", "=", "center", ",", "\n", "scale", "=", "scale", ",", "\n", "training", "=", "is_training", ",", "\n", "fused", "=", "True", ",", "\n", "trainable", "=", "True", ",", "\n", ")", "\n", "return", "batch_norm_op", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.stride_arr": [[96, 98], ["None"], "function", ["None"], ["", "def", "stride_arr", "(", "stride_h", ",", "stride_w", ")", ":", "\n", "  ", "return", "[", "1", ",", "stride_h", ",", "stride_w", ",", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.conv2d": [[100, 141], ["tensorflow.variable_scope", "int", "int", "tensorflow.random_normal_initializer", "custom_ops.variable", "custom_ops.stride_arr", "tensorflow.nn.conv2d", "numpy.sqrt"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.variable", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.stride_arr", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.conv2d"], ["", "@", "tf", ".", "contrib", ".", "framework", ".", "add_arg_scope", "\n", "def", "conv2d", "(", "inputs", ",", "\n", "num_filters_out", ",", "\n", "kernel_size", ",", "\n", "stride", "=", "1", ",", "\n", "scope", "=", "None", ",", "\n", "reuse", "=", "None", ")", ":", "\n", "  ", "\"\"\"Adds a 2D convolution.\n\n  conv2d creates a variable called 'weights', representing the convolutional\n  kernel, that is convolved with the input.\n\n  Args:\n    inputs: a 4D tensor in NHWC format.\n    num_filters_out: the number of output filters.\n    kernel_size: an int specifying the kernel height and width size.\n    stride: an int specifying the height and width stride.\n    scope: Optional scope for variable_scope.\n    reuse: whether or not the layer and its variables should be reused.\n  Returns:\n    a tensor that is the result of a convolution being applied to `inputs`.\n  \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "scope", ",", "'Conv'", ",", "[", "inputs", "]", ",", "reuse", "=", "reuse", ")", ":", "\n", "    ", "num_filters_in", "=", "int", "(", "inputs", ".", "shape", "[", "3", "]", ")", "\n", "weights_shape", "=", "[", "kernel_size", ",", "kernel_size", ",", "num_filters_in", ",", "num_filters_out", "]", "\n", "\n", "# Initialization", "\n", "n", "=", "int", "(", "weights_shape", "[", "0", "]", "*", "weights_shape", "[", "1", "]", "*", "weights_shape", "[", "3", "]", ")", "\n", "weights_initializer", "=", "tf", ".", "random_normal_initializer", "(", "\n", "stddev", "=", "np", ".", "sqrt", "(", "2.0", "/", "n", ")", ")", "\n", "\n", "weights", "=", "variable", "(", "\n", "name", "=", "'weights'", ",", "\n", "shape", "=", "weights_shape", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "initializer", "=", "weights_initializer", ",", "\n", "trainable", "=", "True", ")", "\n", "strides", "=", "stride_arr", "(", "stride", ",", "stride", ")", "\n", "outputs", "=", "tf", ".", "nn", ".", "conv2d", "(", "\n", "inputs", ",", "weights", ",", "strides", ",", "padding", "=", "'SAME'", ",", "data_format", "=", "'NHWC'", ")", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.fc": [[143, 184], ["len", "tensorflow.reshape", "tensorflow.variable_scope", "tensorflow.random_uniform_initializer", "custom_ops.variable", "tensorflow.constant_initializer", "custom_ops.variable", "tensorflow.nn.xw_plus_b", "int"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.variable", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.variable"], ["", "", "@", "tf", ".", "contrib", ".", "framework", ".", "add_arg_scope", "\n", "def", "fc", "(", "inputs", ",", "\n", "num_units_out", ",", "\n", "scope", "=", "None", ",", "\n", "reuse", "=", "None", ")", ":", "\n", "  ", "\"\"\"Creates a fully connected layer applied to `inputs`.\n\n  Args:\n    inputs: a tensor that the fully connected layer will be applied to. It\n      will be reshaped if it is not 2D.\n    num_units_out: the number of output units in the layer.\n    scope: Optional scope for variable_scope.\n    reuse: whether or not the layer and its variables should be reused.\n\n  Returns:\n     a tensor that is the result of applying a linear matrix to `inputs`.\n  \"\"\"", "\n", "if", "len", "(", "inputs", ".", "shape", ")", ">", "2", ":", "\n", "    ", "inputs", "=", "tf", ".", "reshape", "(", "inputs", ",", "[", "int", "(", "inputs", ".", "shape", "[", "0", "]", ")", ",", "-", "1", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "scope", ",", "'FC'", ",", "[", "inputs", "]", ",", "reuse", "=", "reuse", ")", ":", "\n", "    ", "num_units_in", "=", "inputs", ".", "shape", "[", "1", "]", "\n", "weights_shape", "=", "[", "num_units_in", ",", "num_units_out", "]", "\n", "unif_init_range", "=", "1.0", "/", "(", "num_units_out", ")", "**", "(", "0.5", ")", "\n", "weights_initializer", "=", "tf", ".", "random_uniform_initializer", "(", "\n", "-", "unif_init_range", ",", "unif_init_range", ")", "\n", "weights", "=", "variable", "(", "\n", "name", "=", "'weights'", ",", "\n", "shape", "=", "weights_shape", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "initializer", "=", "weights_initializer", ",", "\n", "trainable", "=", "True", ")", "\n", "bias_initializer", "=", "tf", ".", "constant_initializer", "(", "0.0", ")", "\n", "biases", "=", "variable", "(", "\n", "name", "=", "'biases'", ",", "\n", "shape", "=", "[", "num_units_out", ",", "]", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "initializer", "=", "bias_initializer", ",", "\n", "trainable", "=", "True", ")", "\n", "outputs", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "inputs", ",", "weights", ",", "biases", ")", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.avg_pool": [[186, 198], ["tensorflow.name_scope", "custom_ops.stride_arr", "custom_ops.stride_arr", "tensorflow.nn.avg_pool"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.stride_arr", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.stride_arr", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.avg_pool"], ["", "", "@", "tf", ".", "contrib", ".", "framework", ".", "add_arg_scope", "\n", "def", "avg_pool", "(", "inputs", ",", "kernel_size", ",", "stride", "=", "2", ",", "padding", "=", "'VALID'", ",", "scope", "=", "None", ")", ":", "\n", "  ", "\"\"\"Wrapper around tf.nn.avg_pool.\"\"\"", "\n", "with", "tf", ".", "name_scope", "(", "scope", ",", "'AvgPool'", ",", "[", "inputs", "]", ")", ":", "\n", "    ", "kernel", "=", "stride_arr", "(", "kernel_size", ",", "kernel_size", ")", "\n", "strides", "=", "stride_arr", "(", "stride", ",", "stride", ")", "\n", "return", "tf", ".", "nn", ".", "avg_pool", "(", "\n", "inputs", ",", "\n", "ksize", "=", "kernel", ",", "\n", "strides", "=", "strides", ",", "\n", "padding", "=", "padding", ",", "\n", "data_format", "=", "'NHWC'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.wrn.residual_block": [[28, 65], ["tensorflow.variable_scope", "randaugment.custom_ops.batch_norm", "tensorflow.nn.relu", "tensorflow.variable_scope", "randaugment.custom_ops.conv2d", "tensorflow.variable_scope", "randaugment.custom_ops.batch_norm", "tensorflow.nn.relu", "randaugment.custom_ops.conv2d", "randaugment.custom_ops.conv2d"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.batch_norm", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.conv2d", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.batch_norm", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.conv2d", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.conv2d"], ["def", "residual_block", "(", "\n", "x", ",", "in_filter", ",", "out_filter", ",", "stride", ",", "update_bn", "=", "True", ")", ":", "\n", "  ", "\"\"\"Adds residual connection to `x` in addition to applying BN->ReLU->3x3 Conv.\n\n  Args:\n    x: Tensor that is the output of the previous layer in the model.\n    in_filter: Number of filters `x` has.\n    out_filter: Number of filters that the output of this layer will have.\n    stride: Integer that specified what stride should be applied to `x`.\n\n  Returns:\n    A Tensor that is the result of applying two sequences of BN->ReLU->3x3 Conv\n    and then adding that Tensor to `x`.\n  \"\"\"", "\n", "\n", "orig_x", "=", "x", "\n", "block_x", "=", "x", "\n", "with", "tf", ".", "variable_scope", "(", "'residual_only_activation'", ")", ":", "\n", "    ", "block_x", "=", "ops", ".", "batch_norm", "(", "block_x", ",", "update_stats", "=", "update_bn", ",", "\n", "scope", "=", "'init_bn'", ")", "\n", "block_x", "=", "tf", ".", "nn", ".", "relu", "(", "block_x", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'sub1'", ")", ":", "\n", "    ", "block_x", "=", "ops", ".", "conv2d", "(", "\n", "block_x", ",", "out_filter", ",", "3", ",", "stride", "=", "stride", ",", "scope", "=", "'conv1'", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'sub2'", ")", ":", "\n", "    ", "block_x", "=", "ops", ".", "batch_norm", "(", "block_x", ",", "update_stats", "=", "update_bn", ",", "scope", "=", "'bn2'", ")", "\n", "block_x", "=", "tf", ".", "nn", ".", "relu", "(", "block_x", ")", "\n", "block_x", "=", "ops", ".", "conv2d", "(", "\n", "block_x", ",", "out_filter", ",", "3", ",", "stride", "=", "1", ",", "scope", "=", "'conv2'", ")", "\n", "\n", "", "if", "stride", "!=", "1", "or", "out_filter", "!=", "in_filter", ":", "\n", "    ", "orig_x", "=", "ops", ".", "conv2d", "(", "\n", "orig_x", ",", "out_filter", ",", "1", ",", "stride", "=", "stride", ",", "scope", "=", "'conv3'", ")", "\n", "", "x", "=", "orig_x", "+", "block_x", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.wrn.build_wrn_model": [[67, 122], ["range", "min", "tensorflow.variable_scope", "randaugment.custom_ops.conv2d", "range", "tensorflow.variable_scope", "randaugment.custom_ops.batch_norm", "tensorflow.nn.relu", "randaugment.custom_ops.global_avg_pool", "randaugment.custom_ops.fc", "tensorflow.variable_scope", "wrn.residual_block", "tensorflow.variable_scope", "wrn.residual_block"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.conv2d", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.batch_norm", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.global_avg_pool", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.fc", "home.repos.pwc.inspect_result.google-research_uda.randaugment.wrn.residual_block", "home.repos.pwc.inspect_result.google-research_uda.randaugment.wrn.residual_block"], ["", "def", "build_wrn_model", "(", "images", ",", "num_classes", ",", "wrn_size", ",", "update_bn", "=", "True", ")", ":", "\n", "  ", "\"\"\"Builds the WRN model.\n\n  Build the Wide ResNet model from https://arxiv.org/abs/1605.07146.\n\n  Args:\n    images: Tensor of images that will be fed into the Wide ResNet Model.\n    num_classes: Number of classed that the model needs to predict.\n    wrn_size: Parameter that scales the number of filters in the Wide ResNet\n      model.\n\n  Returns:\n    The logits of the Wide ResNet model.\n  \"\"\"", "\n", "# wrn_size = 16 * widening factor k", "\n", "kernel_size", "=", "wrn_size", "\n", "filter_size", "=", "3", "\n", "# depth = num_blocks_per_resnet * 6 + 4 = 28", "\n", "num_blocks_per_resnet", "=", "4", "\n", "filters", "=", "[", "\n", "min", "(", "kernel_size", ",", "16", ")", ",", "kernel_size", ",", "kernel_size", "*", "2", ",", "kernel_size", "*", "4", "\n", "]", "\n", "strides", "=", "[", "1", ",", "2", ",", "2", "]", "# stride for each resblock", "\n", "\n", "# Run the first conv", "\n", "with", "tf", ".", "variable_scope", "(", "'init'", ")", ":", "\n", "    ", "x", "=", "images", "\n", "output_filters", "=", "filters", "[", "0", "]", "\n", "x", "=", "ops", ".", "conv2d", "(", "x", ",", "output_filters", ",", "filter_size", ",", "scope", "=", "'init_conv'", ")", "\n", "\n", "", "first_x", "=", "x", "# Res from the beginning", "\n", "orig_x", "=", "x", "# Res from previous block", "\n", "\n", "for", "block_num", "in", "range", "(", "1", ",", "4", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "'unit_{}_0'", ".", "format", "(", "block_num", ")", ")", ":", "\n", "      ", "x", "=", "residual_block", "(", "\n", "x", ",", "\n", "filters", "[", "block_num", "-", "1", "]", ",", "\n", "filters", "[", "block_num", "]", ",", "\n", "strides", "[", "block_num", "-", "1", "]", ",", "\n", "update_bn", "=", "update_bn", ")", "\n", "", "for", "i", "in", "range", "(", "1", ",", "num_blocks_per_resnet", ")", ":", "\n", "      ", "with", "tf", ".", "variable_scope", "(", "'unit_{}_{}'", ".", "format", "(", "block_num", ",", "i", ")", ")", ":", "\n", "        ", "x", "=", "residual_block", "(", "\n", "x", ",", "\n", "filters", "[", "block_num", "]", ",", "\n", "filters", "[", "block_num", "]", ",", "\n", "1", ",", "\n", "update_bn", "=", "update_bn", ")", "\n", "", "", "", "with", "tf", ".", "variable_scope", "(", "'unit_last'", ")", ":", "\n", "    ", "x", "=", "ops", ".", "batch_norm", "(", "x", ",", "scope", "=", "'final_bn'", ")", "\n", "x", "=", "tf", ".", "nn", ".", "relu", "(", "x", ")", "\n", "x", "=", "ops", ".", "global_avg_pool", "(", "x", ")", "\n", "logits", "=", "ops", ".", "fc", "(", "x", ",", "num_classes", ")", "\n", "", "return", "logits", "\n", "", ""]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_shake._shake_shake_skip_connection": [[28, 53], ["int", "randaugment.custom_ops.stride_arr", "tensorflow.nn.avg_pool", "randaugment.custom_ops.conv2d", "tensorflow.nn.avg_pool", "randaugment.custom_ops.conv2d", "tensorflow.concat", "randaugment.custom_ops.batch_norm", "int", "tensorflow.pad", "int"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.stride_arr", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.avg_pool", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.conv2d", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.avg_pool", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.conv2d", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.batch_norm"], ["def", "_shake_shake_skip_connection", "(", "x", ",", "output_filters", ",", "stride", ")", ":", "\n", "  ", "\"\"\"Adds a residual connection to the filter x for the shake-shake model.\"\"\"", "\n", "curr_filters", "=", "int", "(", "x", ".", "shape", "[", "3", "]", ")", "\n", "if", "curr_filters", "==", "output_filters", ":", "\n", "    ", "return", "x", "\n", "", "stride_spec", "=", "ops", ".", "stride_arr", "(", "stride", ",", "stride", ")", "\n", "# Skip path 1", "\n", "path1", "=", "tf", ".", "nn", ".", "avg_pool", "(", "\n", "x", ",", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "stride_spec", ",", "'VALID'", ",", "data_format", "=", "'NHWC'", ")", "\n", "path1", "=", "ops", ".", "conv2d", "(", "path1", ",", "int", "(", "output_filters", "/", "2", ")", ",", "1", ",", "scope", "=", "'path1_conv'", ")", "\n", "\n", "# Skip path 2", "\n", "# First pad with 0's then crop", "\n", "pad_arr", "=", "[", "[", "0", ",", "0", "]", ",", "[", "0", ",", "1", "]", ",", "[", "0", ",", "1", "]", ",", "[", "0", ",", "0", "]", "]", "\n", "path2", "=", "tf", ".", "pad", "(", "x", ",", "pad_arr", ")", "[", ":", ",", "1", ":", ",", "1", ":", ",", ":", "]", "\n", "concat_axis", "=", "3", "\n", "\n", "path2", "=", "tf", ".", "nn", ".", "avg_pool", "(", "\n", "path2", ",", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "stride_spec", ",", "'VALID'", ",", "data_format", "=", "'NHWC'", ")", "\n", "path2", "=", "ops", ".", "conv2d", "(", "path2", ",", "int", "(", "output_filters", "/", "2", ")", ",", "1", ",", "scope", "=", "'path2_conv'", ")", "\n", "\n", "# Concat and apply BN", "\n", "final_path", "=", "tf", ".", "concat", "(", "values", "=", "[", "path1", ",", "path2", "]", ",", "axis", "=", "concat_axis", ")", "\n", "final_path", "=", "ops", ".", "batch_norm", "(", "final_path", ",", "scope", "=", "'final_path_bn'", ")", "\n", "return", "final_path", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_shake._shake_shake_branch": [[55, 70], ["tensorflow.nn.relu", "randaugment.custom_ops.conv2d", "randaugment.custom_ops.batch_norm", "tensorflow.nn.relu", "randaugment.custom_ops.conv2d", "randaugment.custom_ops.batch_norm", "tensorflow.stop_gradient"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.conv2d", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.batch_norm", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.conv2d", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.batch_norm"], ["", "def", "_shake_shake_branch", "(", "x", ",", "output_filters", ",", "stride", ",", "rand_forward", ",", "rand_backward", ",", "\n", "is_training", ")", ":", "\n", "  ", "\"\"\"Building a 2 branching convnet.\"\"\"", "\n", "x", "=", "tf", ".", "nn", ".", "relu", "(", "x", ")", "\n", "x", "=", "ops", ".", "conv2d", "(", "x", ",", "output_filters", ",", "3", ",", "stride", "=", "stride", ",", "scope", "=", "'conv1'", ")", "\n", "x", "=", "ops", ".", "batch_norm", "(", "x", ",", "scope", "=", "'bn1'", ")", "\n", "x", "=", "tf", ".", "nn", ".", "relu", "(", "x", ")", "\n", "x", "=", "ops", ".", "conv2d", "(", "x", ",", "output_filters", ",", "3", ",", "scope", "=", "'conv2'", ")", "\n", "x", "=", "ops", ".", "batch_norm", "(", "x", ",", "scope", "=", "'bn2'", ")", "\n", "if", "is_training", ":", "\n", "    ", "x", "=", "x", "*", "rand_backward", "+", "tf", ".", "stop_gradient", "(", "x", "*", "rand_forward", "-", "\n", "x", "*", "rand_backward", ")", "\n", "", "else", ":", "\n", "    ", "x", "*=", "1.0", "/", "2", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_shake._shake_shake_block": [[72, 102], ["tensorflow.add_n", "tensorflow.add_n", "zip", "enumerate", "shake_shake._shake_shake_skip_connection", "tensorflow.shape", "tensorflow.random_uniform", "tensorflow.random_uniform", "tensorflow.add_n", "range", "range", "tensorflow.variable_scope", "shake_shake._shake_shake_branch", "branches.append"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_shake._shake_shake_skip_connection", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_shake._shake_shake_branch"], ["", "def", "_shake_shake_block", "(", "x", ",", "output_filters", ",", "stride", ",", "is_training", ")", ":", "\n", "  ", "\"\"\"Builds a full shake-shake sub layer.\"\"\"", "\n", "batch_size", "=", "tf", ".", "shape", "(", "x", ")", "[", "0", "]", "\n", "\n", "# Generate random numbers for scaling the branches", "\n", "rand_forward", "=", "[", "\n", "tf", ".", "random_uniform", "(", "\n", "[", "batch_size", ",", "1", ",", "1", ",", "1", "]", ",", "minval", "=", "0", ",", "maxval", "=", "1", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "for", "_", "in", "range", "(", "2", ")", "\n", "]", "\n", "rand_backward", "=", "[", "\n", "tf", ".", "random_uniform", "(", "\n", "[", "batch_size", ",", "1", ",", "1", ",", "1", "]", ",", "minval", "=", "0", ",", "maxval", "=", "1", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "for", "_", "in", "range", "(", "2", ")", "\n", "]", "\n", "# Normalize so that all sum to 1", "\n", "total_forward", "=", "tf", ".", "add_n", "(", "rand_forward", ")", "\n", "total_backward", "=", "tf", ".", "add_n", "(", "rand_backward", ")", "\n", "rand_forward", "=", "[", "samp", "/", "total_forward", "for", "samp", "in", "rand_forward", "]", "\n", "rand_backward", "=", "[", "samp", "/", "total_backward", "for", "samp", "in", "rand_backward", "]", "\n", "zipped_rand", "=", "zip", "(", "rand_forward", ",", "rand_backward", ")", "\n", "\n", "branches", "=", "[", "]", "\n", "for", "branch", ",", "(", "r_forward", ",", "r_backward", ")", "in", "enumerate", "(", "zipped_rand", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "'branch_{}'", ".", "format", "(", "branch", ")", ")", ":", "\n", "      ", "b", "=", "_shake_shake_branch", "(", "x", ",", "output_filters", ",", "stride", ",", "r_forward", ",", "r_backward", ",", "\n", "is_training", ")", "\n", "branches", ".", "append", "(", "b", ")", "\n", "", "", "res", "=", "_shake_shake_skip_connection", "(", "x", ",", "output_filters", ",", "stride", ")", "\n", "return", "res", "+", "tf", ".", "add_n", "(", "branches", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_shake._shake_shake_layer": [[104, 113], ["range", "tensorflow.variable_scope", "shake_shake._shake_shake_block"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_shake._shake_shake_block"], ["", "def", "_shake_shake_layer", "(", "x", ",", "output_filters", ",", "num_blocks", ",", "stride", ",", "\n", "is_training", ")", ":", "\n", "  ", "\"\"\"Builds many sub layers into one full layer.\"\"\"", "\n", "for", "block_num", "in", "range", "(", "num_blocks", ")", ":", "\n", "    ", "curr_stride", "=", "stride", "if", "(", "block_num", "==", "0", ")", "else", "1", "\n", "with", "tf", ".", "variable_scope", "(", "'layer_{}'", ".", "format", "(", "block_num", ")", ")", ":", "\n", "      ", "x", "=", "_shake_shake_block", "(", "x", ",", "output_filters", ",", "curr_stride", ",", "\n", "is_training", ")", "\n", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_shake.build_shake_shake_model": [[115, 150], ["int", "randaugment.custom_ops.conv2d", "randaugment.custom_ops.batch_norm", "tensorflow.nn.relu", "randaugment.custom_ops.global_avg_pool", "randaugment.custom_ops.fc", "tensorflow.variable_scope", "shake_shake._shake_shake_layer", "tensorflow.variable_scope", "shake_shake._shake_shake_layer", "tensorflow.variable_scope", "shake_shake._shake_shake_layer"], "function", ["home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.conv2d", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.batch_norm", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.global_avg_pool", "home.repos.pwc.inspect_result.google-research_uda.randaugment.custom_ops.fc", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_shake._shake_shake_layer", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_shake._shake_shake_layer", "home.repos.pwc.inspect_result.google-research_uda.randaugment.shake_shake._shake_shake_layer"], ["", "def", "build_shake_shake_model", "(", "images", ",", "num_classes", ",", "hparams", ",", "is_training", ")", ":", "\n", "  ", "\"\"\"Builds the Shake-Shake model.\n\n  Build the Shake-Shake model from https://arxiv.org/abs/1705.07485.\n\n  Args:\n    images: Tensor of images that will be fed into the Wide ResNet Model.\n    num_classes: Number of classed that the model needs to predict.\n    hparams: tf.HParams object that contains additional hparams needed to\n      construct the model. In this case it is the `shake_shake_widen_factor`\n      that is used to determine how many filters the model has.\n    is_training: Is the model training or not.\n\n  Returns:\n    The logits of the Shake-Shake model.\n  \"\"\"", "\n", "depth", "=", "26", "\n", "k", "=", "hparams", ".", "shake_shake_widen_factor", "# The widen factor", "\n", "n", "=", "int", "(", "(", "depth", "-", "2", ")", "/", "6", ")", "\n", "x", "=", "images", "\n", "\n", "x", "=", "ops", ".", "conv2d", "(", "x", ",", "16", ",", "3", ",", "scope", "=", "'init_conv'", ")", "\n", "x", "=", "ops", ".", "batch_norm", "(", "x", ",", "scope", "=", "'init_bn'", ")", "\n", "with", "tf", ".", "variable_scope", "(", "'L1'", ")", ":", "\n", "    ", "x", "=", "_shake_shake_layer", "(", "x", ",", "16", "*", "k", ",", "n", ",", "1", ",", "is_training", ")", "\n", "", "with", "tf", ".", "variable_scope", "(", "'L2'", ")", ":", "\n", "    ", "x", "=", "_shake_shake_layer", "(", "x", ",", "32", "*", "k", ",", "n", ",", "2", ",", "is_training", ")", "\n", "", "with", "tf", ".", "variable_scope", "(", "'L3'", ")", ":", "\n", "    ", "x", "=", "_shake_shake_layer", "(", "x", ",", "64", "*", "k", ",", "n", ",", "2", ",", "is_training", ")", "\n", "", "x", "=", "tf", ".", "nn", ".", "relu", "(", "x", ")", "\n", "x", "=", "ops", ".", "global_avg_pool", "(", "x", ")", "\n", "\n", "# Fully connected", "\n", "logits", "=", "ops", ".", "fc", "(", "x", ",", "num_classes", ")", "\n", "return", "logits", "\n", "", ""]]}