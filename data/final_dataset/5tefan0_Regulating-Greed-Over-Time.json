{"home.repos.pwc.inspect_result.5tefan0_Regulating-Greed-Over-Time.None.RGOT.RGOT": [[30, 51], ["arms.create_arms", "rewards.create_reward_history", "rewards.create_tradeoff_history", "range", "rewards.initialize_mean_reward", "pandas.DataFrame", "arms.get_best_estimate_arm_index", "greed.compute_z", "choose_arm_and_tradeoff", "rewards.get_reward", "rewards.update_arm"], "function", ["home.repos.pwc.inspect_result.5tefan0_Regulating-Greed-Over-Time.None.rewards.create_reward_history", "home.repos.pwc.inspect_result.5tefan0_Regulating-Greed-Over-Time.None.rewards.create_tradeoff_history", "home.repos.pwc.inspect_result.5tefan0_Regulating-Greed-Over-Time.None.rewards.initialize_mean_reward", "home.repos.pwc.inspect_result.5tefan0_Regulating-Greed-Over-Time.None.greed.compute_z", "home.repos.pwc.inspect_result.5tefan0_Regulating-Greed-Over-Time.None.policy.choose_arm_and_tradeoff", "home.repos.pwc.inspect_result.5tefan0_Regulating-Greed-Over-Time.None.rewards.get_reward", "home.repos.pwc.inspect_result.5tefan0_Regulating-Greed-Over-Time.None.rewards.update_arm"], ["def", "RGOT", "(", "G", ",", "number_of_turns", "=", "1500", ",", "number_of_arms", "=", "10", ",", "arms_behavior", "=", "\"Bernoulli\"", ",", "policies", "=", "[", "\"Epsilon_greedy\"", ",", "\"UCB\"", ",", "\"Epsilon_z_greedy\"", ",", "\"UCB_z\"", ",", "\"Epsilon_soft_greedy\"", ",", "\"UCB_soft\"", ",", "\"variable_pool\"", "]", ")", ":", "# set the policies you want to play", "\n", "    ", "arms", "=", "create_arms", "(", "number_of_arms", ",", "arms_behavior", ",", "policies", ")", "\n", "rewards_history", "=", "create_reward_history", "(", "number_of_turns", ",", "policies", ")", "\n", "tradeoff_history", "=", "create_tradeoff_history", "(", "number_of_turns", ",", "policies", ")", "\n", "#pool_size = [0]*number_of_turns", "\n", "for", "policy", "in", "policies", ":", "\n", "        ", "arms", "=", "initialize_mean_reward", "(", "arms", ",", "G", ",", "rewards_history", ",", "tradeoff_history", ",", "policy", ")", "## add option to not initialize", "\n", "", "for", "t", "in", "range", "(", "number_of_arms", ",", "number_of_turns", ")", ":", "\n", "        ", "for", "policy", "in", "policies", ":", "\n", "            ", "best_arm_so_far", "=", "get_best_estimate_arm_index", "(", "arms", ",", "policy", ")", "\n", "z", "=", "compute_z", "(", "G", ")", "# may depend on policy", "\n", "arm_to_play", ",", "tradeoff", "=", "choose_arm_and_tradeoff", "(", "t", ",", "policy", ",", "arms", ",", "best_arm_so_far", ",", "G", ",", "z", ")", "\n", "x_t", "=", "get_reward", "(", "arms", "[", "arm_to_play", "]", ")", "# reward for the arm played", "\n", "rewards_history", "[", "policy", "]", "[", "t", "]", "=", "x_t", "*", "G", "[", "t", "]", "# actual reward you get modified by the greed function", "\n", "tradeoff_history", "[", "policy", "]", "[", "t", "]", "=", "tradeoff", "*", "x_t", "*", "G", "[", "t", "]", "\n", "update_arm", "(", "arms", "[", "arm_to_play", "]", ",", "x_t", ",", "t", ",", "policy", ")", "# update the arm performance under this policy", "\n", "", "", "return", "pd", ".", "DataFrame", "(", "arms", ")", ",", "rewards_history", ",", "tradeoff_history", "\n", "#-------------------------------------------------------------------------------", "\n"]], "home.repos.pwc.inspect_result.5tefan0_Regulating-Greed-Over-Time.None.RGOT.compare_algorithms": [[65, 82], ["xrange", "print", "RGOT.RGOT", "sum", "sum", "str"], "function", ["home.repos.pwc.inspect_result.5tefan0_Regulating-Greed-Over-Time.None.RGOT.RGOT"], ["def", "compare_algorithms", "(", "number_of_games", ",", "G", ",", "number_of_turns", "=", "1000", ",", "number_of_arms", "=", "10", ",", "\n", "arms_behavior", "=", "\"Bernoulli\"", ",", "policies", "=", "[", "\"Epsilon_greedy\"", ",", "\"UCB\"", ",", "\"Epsilon_z_greedy\"", ",", "\"UCB_z\"", ",", "\"Epsilon_soft_greedy\"", ",", "\"UCB_soft\"", ",", "\"variable_pool\"", "]", ")", ":", "\n", "    ", "cumulative_rewards", "=", "{", "}", "\n", "trade_off", "=", "{", "}", "\n", "for", "policy", "in", "policies", ":", "\n", "        ", "cumulative_rewards", "[", "policy", "]", "=", "[", "False", "]", "*", "number_of_games", "# storage for the cululative rewards at the end of each game", "\n", "trade_off", "[", "policy", "]", "=", "[", "False", "]", "*", "number_of_games", "\n", "", "for", "game", "in", "xrange", "(", "number_of_games", ")", ":", "\n", "        ", "print", "(", "\"\\nPlaying game #\"", "+", "str", "(", "game", "+", "1", ")", ")", "\n", "df", ",", "rewards_history", ",", "tradeoff_history", "=", "RGOT", "(", "G", ",", "number_of_turns", "=", "number_of_turns", ",", "number_of_arms", "=", "number_of_arms", ",", "arms_behavior", "=", "arms_behavior", ",", "policies", "=", "policies", ")", "\n", "for", "policy", "in", "policies", ":", "\n", "            ", "cumulative_rewards", "[", "policy", "]", "[", "game", "]", "=", "sum", "(", "rewards_history", "[", "policy", "]", ")", "\n", "trade_off", "[", "policy", "]", "[", "game", "]", "=", "sum", "(", "tradeoff_history", "[", "policy", "]", ")", "\n", "", "", "return", "cumulative_rewards", ",", "trade_off", "\n", "\n"]], "home.repos.pwc.inspect_result.5tefan0_Regulating-Greed-Over-Time.None.policy.choose_arm_and_tradeoff": [[9, 115], ["len", "min", "numpy.random.binomial", "numpy.random.randint", "min", "numpy.random.binomial", "min", "numpy.random.binomial", "len", "numpy.log", "numpy.log", "xrange", "numpy.random.randint", "numpy.random.randint", "len", "len", "len", "numpy.sqrt", "xrange", "xrange", "numpy.min", "len", "len", "min", "print", "numpy.sqrt", "numpy.sqrt", "max", "numpy.log", "int", "range", "heapq.nlargest", "numpy.floor", "len", "xrange", "numpy.random.randint", "numpy.log", "numpy.log", "len"], "function", ["None"], ["def", "choose_arm_and_tradeoff", "(", "t", ",", "policy", ",", "arms", ",", "best_arm_so_far", ",", "G", ",", "z", ")", ":", "\n", "    ", "\"\"\" Returns True if the policy exploited the best arm found so far. \"\"\"", "\n", "m", "=", "len", "(", "arms", ")", "\n", "if", "policy", "==", "\"Epsilon_greedy\"", ":", "\n", "# set algorithm parameters", "\n", "        ", "c", "=", "11.0", "\n", "d", "=", "0.1", "\n", "k", "=", "c", "/", "d", "\n", "probability_of_random_exploration", "=", "min", "(", "1.0", ",", "k", "*", "m", "/", "t", ")", "\n", "exploration", "=", "np", ".", "random", ".", "binomial", "(", "1", ",", "probability_of_random_exploration", ",", "size", "=", "None", ")", "\n", "if", "exploration", "==", "0", ":", "\n", "            ", "return", "best_arm_so_far", ",", "True", "\n", "", "else", ":", "\n", "            ", "return", "np", ".", "random", ".", "randint", "(", "0", ",", "len", "(", "arms", ")", ")", ",", "False", "\n", "", "", "elif", "policy", "==", "\"Epsilon_z_greedy\"", ":", "\n", "# set algorithm parameters", "\n", "        ", "c", "=", "11.0", "\n", "d", "=", "0.1", "\n", "k", "=", "c", "/", "d", "\n", "if", "G", "[", "t", "]", ">", "z", ":", "\n", "            ", "return", "best_arm_so_far", ",", "True", "\n", "", "else", ":", "\n", "            ", "probability_of_random_exploration", "=", "min", "(", "1.0", ",", "k", "*", "m", "/", "t", ")", "\n", "exploration", "=", "np", ".", "random", ".", "binomial", "(", "1", ",", "probability_of_random_exploration", ",", "size", "=", "None", ")", "\n", "if", "exploration", "==", "0", ":", "\n", "                ", "return", "best_arm_so_far", ",", "True", "\n", "", "else", ":", "\n", "                ", "return", "np", ".", "random", ".", "randint", "(", "0", ",", "len", "(", "arms", ")", ")", ",", "False", "\n", "", "", "", "elif", "policy", "==", "\"Epsilon_soft_greedy\"", ":", "\n", "# set algorithm parameters", "\n", "        ", "c", "=", "11.0", "\n", "d", "=", "0.1", "\n", "k", "=", "c", "/", "d", "\n", "psi_t", "=", "np", ".", "log", "(", "1.0", "+", "1.0", "/", "G", "[", "t", "]", ")", "/", "np", ".", "log", "(", "1.0", "+", "1.0", "/", "np", ".", "min", "(", "G", ")", ")", "## min over all", "\n", "probability_of_random_exploration", "=", "min", "(", "psi_t", ",", "k", "*", "m", "/", "t", ")", "\n", "exploration", "=", "np", ".", "random", ".", "binomial", "(", "1", ",", "probability_of_random_exploration", ",", "size", "=", "None", ")", "\n", "if", "exploration", "==", "0", ":", "\n", "            ", "return", "best_arm_so_far", ",", "True", "\n", "", "else", ":", "\n", "            ", "return", "np", ".", "random", ".", "randint", "(", "0", ",", "len", "(", "arms", ")", ")", ",", "False", "\n", "", "", "elif", "policy", "==", "\"UCB\"", ":", "\n", "        ", "arm_to_play", "=", "-", "1", "\n", "second_best_arm", "=", "-", "1", "\n", "best_UCB_found", "=", "-", "1", "\n", "for", "j", "in", "xrange", "(", "len", "(", "arms", ")", ")", ":", "\n", "            ", "T_j", "=", "arms", "[", "j", "]", "[", "\"total_pulls_\"", "+", "policy", "]", "\n", "UCB_j", "=", "arms", "[", "j", "]", "[", "\"mean_reward_\"", "+", "policy", "]", "+", "np", ".", "sqrt", "(", "2.0", "*", "np", ".", "log", "(", "t", ")", "/", "T_j", ")", "\n", "if", "UCB_j", ">", "best_UCB_found", ":", "\n", "                ", "second_best_arm", "=", "arm_to_play", "# the arm you thought you were playing now is second best", "\n", "arm_to_play", "=", "j", "# update best arm", "\n", "best_UCB_found", "=", "UCB_j", "\n", "", "", "if", "arms", "[", "arm_to_play", "]", "[", "\"mean_reward_\"", "+", "policy", "]", ">", "arms", "[", "second_best_arm", "]", "[", "\"mean_reward_\"", "+", "policy", "]", ":", "#\\", "\n", "#+ np.sqrt( 2.0 * np.log( t ) / arms[second_best_arm][\"total_pulls_\"+policy] ):", "\n", "            ", "Exploitation", "=", "True", "\n", "", "else", ":", "\n", "            ", "Exploitation", "=", "False", "\n", "", "return", "arm_to_play", ",", "Exploitation", "\n", "", "elif", "policy", "==", "\"UCB_z\"", ":", "\n", "        ", "if", "G", "[", "t", "]", ">", "z", ":", "\n", "            ", "Exploitation", "=", "True", "\n", "return", "best_arm_so_far", ",", "Exploitation", "\n", "", "else", ":", "\n", "            ", "arm_to_play", "=", "-", "1", "\n", "second_best_arm", "=", "-", "1", "\n", "best_UCB_found", "=", "-", "1", "\n", "for", "j", "in", "xrange", "(", "len", "(", "arms", ")", ")", ":", "\n", "                ", "T_j", "=", "arms", "[", "j", "]", "[", "\"total_pulls_\"", "+", "policy", "]", "\n", "UCB_j", "=", "arms", "[", "j", "]", "[", "\"mean_reward_\"", "+", "policy", "]", "+", "np", ".", "sqrt", "(", "2.0", "*", "np", ".", "log", "(", "t", ")", "/", "T_j", ")", "\n", "if", "UCB_j", ">", "best_UCB_found", ":", "\n", "                    ", "second_best_arm", "=", "arm_to_play", "# the arm you thought you were playing now is second best", "\n", "arm_to_play", "=", "j", "# update best arm", "\n", "best_UCB_found", "=", "UCB_j", "\n", "", "", "if", "arms", "[", "arm_to_play", "]", "[", "\"mean_reward_\"", "+", "policy", "]", ">", "arms", "[", "second_best_arm", "]", "[", "\"mean_reward_\"", "+", "policy", "]", ":", "#\\", "\n", "                ", "Exploitation", "=", "True", "\n", "", "else", ":", "\n", "                ", "Exploitation", "=", "False", "\n", "", "return", "arm_to_play", ",", "Exploitation", "\n", "", "", "elif", "policy", "==", "\"UCB_soft\"", ":", "\n", "        ", "xi_t", "=", "(", "1.0", "+", "t", "/", "G", "[", "t", "]", ")", "\n", "arm_to_play", "=", "-", "1", "\n", "second_best_arm", "=", "-", "1", "\n", "best_UCB_found", "=", "-", "1", "\n", "for", "j", "in", "xrange", "(", "len", "(", "arms", ")", ")", ":", "\n", "            ", "T_j", "=", "arms", "[", "j", "]", "[", "\"total_pulls_\"", "+", "policy", "]", "\n", "UCB_j", "=", "arms", "[", "j", "]", "[", "\"mean_reward_\"", "+", "policy", "]", "+", "np", ".", "sqrt", "(", "2.0", "*", "np", ".", "log", "(", "xi_t", ")", "/", "T_j", ")", "\n", "if", "UCB_j", ">", "best_UCB_found", ":", "\n", "                ", "second_best_arm", "=", "arm_to_play", "# the arm you thought you were playing now is second best", "\n", "arm_to_play", "=", "j", "# update best arm", "\n", "best_UCB_found", "=", "UCB_j", "\n", "", "", "if", "arms", "[", "arm_to_play", "]", "[", "\"mean_reward_\"", "+", "policy", "]", ">", "arms", "[", "second_best_arm", "]", "[", "\"mean_reward_\"", "+", "policy", "]", ":", "#\\", "\n", "            ", "Exploitation", "=", "True", "\n", "", "else", ":", "\n", "            ", "Exploitation", "=", "False", "\n", "", "return", "arm_to_play", ",", "Exploitation", "\n", "", "elif", "policy", "==", "\"variable_pool\"", ":", "\n", "        ", "c", "=", "1.1", "\n", "m_t", "=", "min", "(", "m", ",", "max", "(", "1", ",", "int", "(", "np", ".", "floor", "(", "c", "*", "m", "/", "t", "*", "G", "[", "t", "]", ")", ")", ")", ")", "# pool size decreasing", "\n", "if", "m_t", "==", "1", ":", "\n", "            ", "Exploitation", "=", "True", "\n", "", "else", ":", "\n", "            ", "Exploitation", "=", "False", "\n", "", "A", "=", "[", "arms", "[", "i", "]", "[", "\"mean_reward_\"", "+", "policy", "]", "for", "i", "in", "range", "(", "len", "(", "arms", ")", ")", "]", "\n", "return", "heapq", ".", "nlargest", "(", "m_t", ",", "xrange", "(", "len", "(", "A", ")", ")", ",", "key", "=", "A", ".", "__getitem__", ")", "[", "np", ".", "random", ".", "randint", "(", "0", ",", "m_t", ")", "]", ",", "Exploitation", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"\\n Policy not found \\n\"", ")", "\n", "return", "\n", "", "", ""]], "home.repos.pwc.inspect_result.5tefan0_Regulating-Greed-Over-Time.None.rewards.get_reward": [[4, 9], ["numpy.random.binomial"], "function", ["None"], ["def", "get_reward", "(", "arm_j", ")", ":", "\n", "    ", "behavior", "=", "arm_j", "[", "\"distribution\"", "]", "\n", "if", "behavior", "==", "\"Bernoulli\"", ":", "\n", "        ", "reward", "=", "np", ".", "random", ".", "binomial", "(", "1", ",", "arm_j", "[", "\"p\"", "]", ")", "\n", "", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.5tefan0_Regulating-Greed-Over-Time.None.rewards.initialize_mean_reward": [[10, 17], ["range", "len", "rewards.get_reward", "rewards.update_arm"], "function", ["home.repos.pwc.inspect_result.5tefan0_Regulating-Greed-Over-Time.None.rewards.get_reward", "home.repos.pwc.inspect_result.5tefan0_Regulating-Greed-Over-Time.None.rewards.update_arm"], ["", "def", "initialize_mean_reward", "(", "arms", ",", "G", ",", "rewards_history", ",", "tradeoff_history", ",", "policy", ")", ":", "\n", "    ", "for", "j", "in", "range", "(", "len", "(", "arms", ")", ")", ":", "\n", "        ", "reward", "=", "get_reward", "(", "arms", "[", "j", "]", ")", "\n", "update_arm", "(", "arms", "[", "j", "]", ",", "reward", ",", "j", ",", "policy", ")", "\n", "rewards_history", "[", "policy", "]", "[", "j", "]", "=", "reward", "*", "G", "[", "j", "]", "\n", "tradeoff_history", "[", "policy", "]", "[", "j", "]", "=", "False", "\n", "", "return", "arms", "\n", "\n"]], "home.repos.pwc.inspect_result.5tefan0_Regulating-Greed-Over-Time.None.rewards.update_arm": [[18, 24], ["arm_j[].append", "arm_j[].append", "numpy.mean"], "function", ["None"], ["", "def", "update_arm", "(", "arm_j", ",", "reward", ",", "time", ",", "policy", ")", ":", "\n", "    ", "arm_j", "[", "\"total_pulls_\"", "+", "policy", "]", "+=", "1", "\n", "arm_j", "[", "\"rounds_pulled_\"", "+", "policy", "]", ".", "append", "(", "time", ")", "\n", "arm_j", "[", "\"rewards_\"", "+", "policy", "]", ".", "append", "(", "reward", ")", "\n", "arm_j", "[", "\"mean_reward_\"", "+", "policy", "]", "=", "np", ".", "mean", "(", "arm_j", "[", "\"rewards_\"", "+", "policy", "]", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.5tefan0_Regulating-Greed-Over-Time.None.rewards.create_reward_history": [[25, 32], ["dict"], "function", ["None"], ["", "def", "create_reward_history", "(", "number_of_turns", ",", "policies", ")", ":", "\n", "    ", "\"\"\" Creates a dictionary that stores the rewards history for each\n    policy applied \"\"\"", "\n", "rewards_history", "=", "dict", "(", ")", "\n", "for", "policy", "in", "policies", ":", "\n", "        ", "rewards_history", "[", "policy", "]", "=", "[", "False", "]", "*", "number_of_turns", "\n", "", "return", "rewards_history", "\n", "\n"]], "home.repos.pwc.inspect_result.5tefan0_Regulating-Greed-Over-Time.None.rewards.create_tradeoff_history": [[33, 40], ["dict"], "function", ["None"], ["", "def", "create_tradeoff_history", "(", "number_of_turns", ",", "policies", ")", ":", "\n", "    ", "\"\"\" Creates a dictionary that stores the trade-off history\n    (exploration VS exploitation) for each policy applied \"\"\"", "\n", "tradeoff_history", "=", "dict", "(", ")", "\n", "for", "policy", "in", "policies", ":", "\n", "        ", "tradeoff_history", "[", "policy", "]", "=", "[", "False", "]", "*", "number_of_turns", "\n", "", "return", "tradeoff_history", "\n", "", ""]], "home.repos.pwc.inspect_result.5tefan0_Regulating-Greed-Over-Time.None.greed.set_G_as": [[13, 41], ["xrange", "range", "xrange", "xrange", "print", "numpy.sin", "numpy.sin"], "function", ["None"], ["def", "set_G_as", "(", "number_of_turns", ",", "keyword", ")", ":", "\n", "    ", "\"\"\" You can choose a greed function of type 'Wave', 'Christmas', or 'Step' \"\"\"", "\n", "G", "=", "[", "0", "for", "i", "in", "range", "(", "number_of_turns", ")", "]", "\n", "if", "keyword", "==", "\"Wave\"", ":", "\n", "        ", "for", "t", "in", "xrange", "(", "number_of_turns", ")", ":", "\n", "            ", "G", "[", "t", "]", "=", "21", "+", "20", "*", "np", ".", "sin", "(", "0.25", "*", "t", ")", "\n", "", "", "elif", "keyword", "==", "\"Christmas\"", ":", "\n", "        ", "christmas_peak", "=", "1000", "\n", "for", "t", "in", "xrange", "(", "number_of_turns", ")", ":", "\n", "            ", "G", "[", "t", "]", "=", "21", "+", "20", "*", "np", ".", "sin", "(", "0.25", "*", "t", ")", "\n", "if", "t", ">=", "number_of_turns", "*", "0.5", "and", "t", "<", "number_of_turns", "*", "0.6", ":", "\n", "                ", "G", "[", "t", "]", "=", "christmas_peak", "\n", "", "", "", "elif", "keyword", "==", "\"Step\"", ":", "\n", "        ", "low_value", "=", "20", "\n", "high_value", "=", "40", "\n", "for", "t", "in", "xrange", "(", "number_of_turns", ")", ":", "\n", "            ", "if", "t", ">=", "number_of_turns", "*", "0.25", "and", "t", "<", "number_of_turns", "*", "0.5", ":", "\n", "                ", "G", "[", "t", "]", "=", "high_value", "\n", "", "elif", "t", ">=", "number_of_turns", "*", "0.75", ":", "\n", "                ", "G", "[", "t", "]", "=", "high_value", "\n", "", "else", ":", "\n", "                ", "G", "[", "t", "]", "=", "low_value", "\n", "", "", "", "else", ":", "\n", "        ", "print", "(", "\"Keyword not found. You can choose a greed function of type \\\n         'Wave', 'Christmas', or 'Step'. Alternatively you can choose your \\\n         greed function G(t).\"", ")", "\n", "return", "\n", "", "return", "G", "\n", "\n"]], "home.repos.pwc.inspect_result.5tefan0_Regulating-Greed-Over-Time.None.greed.compute_z": [[46, 48], ["numpy.percentile"], "function", ["None"], ["def", "compute_z", "(", "G", ",", "percentile", "=", "65", ")", ":", "\n", "    ", "return", "np", ".", "percentile", "(", "G", ",", "percentile", ")", "\n", "", ""]]}