{"home.repos.pwc.inspect_result.ufal_kazitext.None.apply_m2_edits.processEdits": [[6, 24], ["edit.split.split", "[].split", "int", "int", "edit_dict.keys", "edit_dict[].append"], "function", ["None"], ["def", "processEdits", "(", "edits", ")", ":", "\n", "    ", "edit_dict", "=", "{", "}", "\n", "for", "edit", "in", "edits", ":", "\n", "        ", "edit", "=", "edit", ".", "split", "(", "\"|||\"", ")", "\n", "span", "=", "edit", "[", "0", "]", "[", "2", ":", "]", ".", "split", "(", ")", "# [2:] ignore the leading \"A \"", "\n", "start", "=", "int", "(", "span", "[", "0", "]", ")", "\n", "end", "=", "int", "(", "span", "[", "1", "]", ")", "\n", "cat", "=", "edit", "[", "1", "]", "\n", "cor", "=", "edit", "[", "2", "]", "\n", "id", "=", "edit", "[", "-", "1", "]", "\n", "# Save the useful info as a list", "\n", "proc_edit", "=", "[", "start", ",", "end", ",", "cat", ",", "cor", "]", "\n", "# Save the proc edit inside the edit_dict using coder id.", "\n", "if", "id", "in", "edit_dict", ".", "keys", "(", ")", ":", "\n", "            ", "edit_dict", "[", "id", "]", ".", "append", "(", "proc_edit", ")", "\n", "", "else", ":", "\n", "            ", "edit_dict", "[", "id", "]", "=", "[", "proc_edit", "]", "\n", "", "", "return", "edit_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.None.apply_m2_edits.processM2": [[31, 74], ["info.split.split", "[].split", "apply_m2_edits.processEdits", "processEdits.items", "edit[].split", "gold_edits.append", "gold_edits.append", "len", "len"], "function", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.apply_m2_edits.processEdits"], ["", "def", "processM2", "(", "info", ",", "ignore_edit_types", ")", ":", "\n", "    ", "info", "=", "info", ".", "split", "(", "\"\\n\"", ")", "\n", "orig_sent", "=", "info", "[", "0", "]", "[", "2", ":", "]", ".", "split", "(", ")", "# [2:] ignore the leading \"S \"", "\n", "all_edits", "=", "info", "[", "1", ":", "]", "\n", "# Simplify the edits and group by coder id.", "\n", "edit_dict", "=", "processEdits", "(", "all_edits", ")", "\n", "out_dict", "=", "{", "}", "\n", "# Loop through each coder and their edits.", "\n", "for", "coder", ",", "edits", "in", "edit_dict", ".", "items", "(", ")", ":", "\n", "# Copy orig_sent. We will apply the edits to it to make cor_sent", "\n", "        ", "cor_sent", "=", "orig_sent", "[", ":", "]", "\n", "gold_edits", "=", "[", "]", "\n", "offset", "=", "0", "\n", "for", "edit", "in", "edits", ":", "\n", "# Do not apply noop or Um edits, but save them", "\n", "            ", "if", "edit", "[", "2", "]", "in", "{", "\"noop\"", ",", "\"Um\"", "}", ":", "\n", "                ", "gold_edits", ".", "append", "(", "edit", "+", "[", "-", "1", ",", "-", "1", "]", ")", "\n", "continue", "\n", "\n", "", "if", "ignore_edit_types", ":", "\n", "                ", "is_ignore_edit_type", "=", "False", "\n", "for", "ignore_edit_type", "in", "ignore_edit_types", ":", "\n", "                    ", "if", "ignore_edit_type", "in", "edit", "[", "2", "]", ":", "# substring match", "\n", "                        ", "is_ignore_edit_type", "=", "True", "\n", "\n", "", "", "if", "is_ignore_edit_type", ":", "\n", "                    ", "continue", "\n", "\n", "", "", "orig_start", "=", "edit", "[", "0", "]", "\n", "orig_end", "=", "edit", "[", "1", "]", "\n", "cor_toks", "=", "edit", "[", "3", "]", ".", "split", "(", ")", "\n", "# Apply the edit.", "\n", "cor_sent", "[", "orig_start", "+", "offset", ":", "orig_end", "+", "offset", "]", "=", "cor_toks", "\n", "# Get the cor token start and end positions in cor_sent", "\n", "cor_start", "=", "orig_start", "+", "offset", "\n", "cor_end", "=", "cor_start", "+", "len", "(", "cor_toks", ")", "\n", "# Keep track of how this affects orig edit offsets.", "\n", "offset", "=", "offset", "-", "(", "orig_end", "-", "orig_start", ")", "+", "len", "(", "cor_toks", ")", "\n", "# Save the edit with cor_start and cor_end", "\n", "gold_edits", ".", "append", "(", "edit", "+", "[", "cor_start", "]", "+", "[", "cor_end", "]", ")", "\n", "# Save the cor_sent and gold_edits for each annotator in the out_dict.", "\n", "", "out_dict", "[", "coder", "]", "=", "(", "cor_sent", ",", "gold_edits", ")", "\n", "", "return", "orig_sent", ",", "out_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.None.apply_m2_edits.main": [[76, 96], ["open", "print", "open().read().strip().split", "open.close", "apply_m2_edits.processM2", "open().read().strip", "sorted", "coder_dict.items", "open.write", "open().read", "open"], "function", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.apply_m2_edits.processM2"], ["", "def", "main", "(", "args", ")", ":", "\n", "# Setup output m2 file", "\n", "    ", "out_parallel", "=", "open", "(", "args", ".", "out", ",", "\"w\"", ")", "\n", "\n", "print", "(", "\"Processing files...\"", ")", "\n", "# Open the m2 file and split into sentence+edit chunks.", "\n", "m2_file", "=", "open", "(", "args", ".", "m2", ")", ".", "read", "(", ")", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\\n\"", ")", "\n", "for", "info", "in", "m2_file", ":", "\n", "# Get the original and corrected sentence + edits for each annotator.", "\n", "        ", "orig_sent", ",", "coder_dict", "=", "processM2", "(", "info", ",", "args", ".", "ignore_edit_types", ")", "\n", "# Save info about types of edit groups seen", "\n", "# Only process sentences with edits.", "\n", "if", "coder_dict", ":", "\n", "# Save marked up original sentence here, if required.", "\n", "# Loop through the annotators", "\n", "            ", "for", "coder", ",", "coder_info", "in", "sorted", "(", "coder_dict", ".", "items", "(", ")", ")", ":", "\n", "                ", "cor_sent", "=", "coder_info", "[", "0", "]", "\n", "out_parallel", ".", "write", "(", "\" \"", ".", "join", "(", "orig_sent", ")", "+", "\"\\t\"", "+", "\" \"", ".", "join", "(", "cor_sent", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "", "", "out_parallel", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.None.introduce_errors.get_truncated_normal": [[11, 17], ["scipy.stats.truncnorm"], "function", ["None"], ["def", "get_truncated_normal", "(", "mean", "=", "0", ",", "sd", "=", "1", ",", "low", "=", "0", ",", "upp", "=", "10", ")", ":", "\n", "    ", "if", "sd", "==", "0", ":", "\n", "        ", "sd", "=", "0.00001", "\n", "\n", "", "return", "truncnorm", "(", "\n", "(", "low", "-", "mean", ")", "/", "sd", ",", "(", "upp", "-", "mean", ")", "/", "sd", ",", "loc", "=", "mean", ",", "scale", "=", "sd", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.None.introduce_errors.get_aspects_generator": [[19, 60], ["range", "introduce_errors.load_basic_aspects", "max", "max", "introduce_errors.load_basic_aspects", "alphas.append", "introduce_errors.get_truncated_normal", "numpy.random.uniform", "numpy.random.choice", "get_truncated_normal.rvs", "min", "list", "aspects.values", "abs"], "function", ["home.repos.pwc.inspect_result.ufal_kazitext.None.introduce_errors.load_basic_aspects", "home.repos.pwc.inspect_result.ufal_kazitext.None.introduce_errors.load_basic_aspects", "home.repos.pwc.inspect_result.ufal_kazitext.None.introduce_errors.get_truncated_normal"], ["", "def", "get_aspects_generator", "(", "profile_file", ",", "lang", ",", "alpha_mean", ",", "beta", ",", "strip_all_diacritics", ",", "spelling_detailed_ratio", ",", "alpha_min", "=", "None", ",", "\n", "alpha_max", "=", "None", ",", "alpha_std", "=", "None", ",", "alpha_uniformity_prob", "=", "0", ",", "num_aspects", "=", "1000", ")", ":", "\n", "    ", "'''\n     Returns generator that when called, returns next aspect to be used for noising\n    '''", "\n", "\n", "if", "alpha_std", "==", "0", ":", "\n", "        ", "aspect", "=", "load_basic_aspects", "(", "profile_file", ",", "lang", ",", "alpha_mean", ",", "beta", ",", "strip_all_diacritics", ",", "spelling_detailed_ratio", ")", "\n", "return", "lambda", ":", "aspect", "\n", "\n", "# if alpha_min / alpha_max are not specified, set them to cover most of the probability mass", "\n", "", "if", "alpha_min", "is", "None", ":", "\n", "        ", "alpha_min", "=", "max", "(", "0", ",", "alpha_mean", "-", "3", "*", "alpha_std", ")", "\n", "", "if", "alpha_max", "is", "None", ":", "\n", "        ", "alpha_max", "=", "max", "(", "0", ",", "alpha_mean", "+", "3", "*", "alpha_std", ")", "\n", "\n", "# divide into num_chunks in [alpha_min, alpha_max]", "\n", "", "chunk_size", "=", "(", "alpha_max", "-", "alpha_min", ")", "/", "(", "num_aspects", "-", "1", ")", "\n", "\n", "alphas", "=", "[", "]", "\n", "aspects", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "num_aspects", ")", ":", "\n", "        ", "cur_alpha", "=", "alpha_min", "+", "i", "*", "chunk_size", "\n", "aspects", "[", "cur_alpha", "]", "=", "load_basic_aspects", "(", "profile_file", ",", "lang", ",", "cur_alpha", ",", "beta", ",", "strip_all_diacritics", ",", "spelling_detailed_ratio", ")", "\n", "alphas", ".", "append", "(", "cur_alpha", ")", "\n", "\n", "", "if", "alpha_uniformity_prob", "<", "1", ":", "\n", "# if uniform is not used always, instantiate generator of truncated normal values", "\n", "        ", "trunc_norm_alpha_generator", "=", "get_truncated_normal", "(", "alpha_mean", ",", "alpha_std", ",", "alpha_min", ",", "alpha_max", ")", "\n", "\n", "", "def", "get_next_aspect", "(", ")", ":", "\n", "        ", "if", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "alpha_uniformity_prob", ":", "\n", "# select aspect (alpha) uniformly from whole interval", "\n", "            ", "return", "np", ".", "random", ".", "choice", "(", "list", "(", "aspects", ".", "values", "(", ")", ")", ")", "\n", "", "else", ":", "\n", "# sample alpha according to (truncated) normal normal distribution and return aspect with closest alpha", "\n", "            ", "sampled_alpha", "=", "trunc_norm_alpha_generator", ".", "rvs", "(", ")", "\n", "closest_alpha", "=", "min", "(", "alphas", ",", "key", "=", "lambda", "x", ":", "abs", "(", "x", "-", "sampled_alpha", ")", ")", "\n", "return", "aspects", "[", "closest_alpha", "]", "\n", "\n", "", "", "return", "get_next_aspect", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.None.introduce_errors.load_basic_aspects": [[62, 83], ["open", "json.load", "aspects.Casing", "aspects.CommonOther", "aspects.Diacritics", "aspects.Punctuation", "aspects.Spelling", "aspects.SuffixPrefix", "aspects.Whitespace", "aspects.WordOrder"], "function", ["None"], ["", "def", "load_basic_aspects", "(", "profile_file", ",", "lang", ",", "alpha", ",", "beta", ",", "strip_all_diacritics", ",", "spelling_detailed_ratio", ")", ":", "\n", "    ", "with", "open", "(", "profile_file", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "profile", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "aspects", "=", "{", "\n", "'casing'", ":", "Casing", "(", "profile", ",", "lang", ",", "alpha", ",", "beta", ")", ",", "\n", "'common_other'", ":", "CommonOther", "(", "profile", ",", "lang", ",", "alpha", ",", "beta", ")", ",", "\n", "'diacritics'", ":", "Diacritics", "(", "profile", ",", "lang", ",", "alpha", ",", "beta", ")", ",", "\n", "'punctuation'", ":", "Punctuation", "(", "profile", ",", "lang", ",", "alpha", ",", "beta", ")", ",", "\n", "'spelling'", ":", "Spelling", "(", "profile", ",", "lang", ",", "alpha", ",", "beta", ")", ",", "\n", "'suffix_prefix'", ":", "SuffixPrefix", "(", "profile", ",", "lang", ",", "alpha", ",", "beta", ")", ",", "\n", "'whitespace'", ":", "Whitespace", "(", "profile", ",", "lang", ",", "alpha", ",", "beta", ")", ",", "\n", "'word_order'", ":", "WordOrder", "(", "profile", ",", "lang", ",", "alpha", ",", "beta", ")", "\n", "}", "\n", "\n", "if", "strip_all_diacritics", ":", "\n", "        ", "aspects", "[", "'diacritics'", "]", ".", "all_wo_diacritics_perc", "=", "1", "\n", "\n", "", "aspects", "[", "'spelling'", "]", ".", "spelling_detailed_ratio", "=", "spelling_detailed_ratio", "\n", "\n", "return", "aspects", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.None.introduce_errors.load_tokenizer": [[85, 87], ["udpipe_tokenizer.UDPipeTokenizer"], "function", ["None"], ["", "def", "load_tokenizer", "(", "lang", ")", ":", "\n", "    ", "return", "udpipe_tokenizer", ".", "UDPipeTokenizer", "(", "lang", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.None.introduce_errors._tokenize_line_and_get_whitespace_info": [[89, 115], ["tokenizer.tokenize", "range", "line.split", "line.startswith", "line.split.append", "len", "len", "len", "token.string.replace"], "function", ["home.repos.pwc.inspect_result.ufal_kazitext.None.udpipe_tokenizer.UDPipeTokenizer.tokenize"], ["", "def", "_tokenize_line_and_get_whitespace_info", "(", "line", ",", "tokenizer", ")", ":", "\n", "# tokenize input line and store space mapping", "\n", "\n", "    ", "if", "tokenizer", "is", "None", ":", "\n", "        ", "tokens", "=", "line", ".", "split", "(", "' '", ")", "\n", "text_whitespace_info", "=", "[", "True", "]", "*", "(", "len", "(", "tokens", ")", "-", "1", ")", "\n", "return", "line", ",", "text_whitespace_info", "\n", "\n", "", "tokens", "=", "[", "]", "\n", "for", "sentence_tokens", "in", "tokenizer", ".", "tokenize", "(", "line", ")", ":", "\n", "        ", "for", "token", "in", "sentence_tokens", ":", "\n", "            ", "tokens", ".", "append", "(", "token", ".", "string", ".", "replace", "(", "' '", ",", "''", ")", ")", "\n", "\n", "", "", "text_whitespace_info", "=", "[", "True", "]", "*", "(", "\n", "len", "(", "tokens", ")", "-", "1", ")", "# stores information on whether there was space between adjacent tokens in text", "\n", "current_status", "=", "tokens", "[", "0", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "tokens", ")", "-", "1", ")", ":", "\n", "        ", "if", "line", ".", "startswith", "(", "current_status", "+", "tokens", "[", "i", "+", "1", "]", ")", ":", "# no space in between", "\n", "            ", "text_whitespace_info", "[", "i", "]", "=", "False", "\n", "current_status", "+=", "tokens", "[", "i", "+", "1", "]", "\n", "", "else", ":", "\n", "            ", "current_status", "+=", "\" \"", "+", "tokens", "[", "i", "+", "1", "]", "\n", "\n", "", "", "tokenized_line", "=", "\" \"", ".", "join", "(", "tokens", ")", "\n", "\n", "return", "tokenized_line", ",", "text_whitespace_info", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.None.introduce_errors.introduce_errors_in_line": [[117, 207], ["introduce_errors._tokenize_line_and_get_whitespace_info", "numpy.random.uniform", "print", "len", "range", "len", "aspects[].apply", "line_changes.extend", "len", "aspects[].apply", "line_changes.extend", "len", "aspects[].apply", "line_changes.extend", "len", "aspects[].apply", "line_changes.extend", "len", "aspects[].apply", "line_changes.extend", "len", "aspects[].apply", "line_changes.extend", "len", "aspects[].apply", "line_changes.extend", "len", "aspects[].apply", "line_changes.extend", "len", "len", "original_tokenized_line.split", "len", "len", "len", "len", "len", "len", "len", "len", "len", "abs", "len", "len", "tokenized_line.split", "tokenized_line.split", "tokenized_line.split", "tokenized_line.split", "tokenized_line.split", "tokenized_line.split", "tokenized_line.split", "tokenized_line.split", "tokenized_line.split", "tokenized_line.split", "len"], "function", ["home.repos.pwc.inspect_result.ufal_kazitext.None.introduce_errors._tokenize_line_and_get_whitespace_info", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation.apply", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation.apply", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation.apply", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation.apply", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation.apply", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation.apply", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation.apply", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation.apply"], ["", "def", "introduce_errors_in_line", "(", "line", ",", "tokenizer", ",", "aspects", ",", "no_diacritics", ",", "no_spelling", ",", "no_casing", ",", "no_whitespace", ",", "no_punctuation", ",", "no_word_order", ",", "\n", "no_suffix_prefix", ",", "no_common_other", ",", "verbose", "=", "False", ",", "no_error_sentence_boost", "=", "0", ")", ":", "\n", "    ", "original_tokenized_line", ",", "original_text_whitespace_info", "=", "_tokenize_line_and_get_whitespace_info", "(", "line", ",", "tokenizer", ")", "\n", "\n", "if", "verbose", ":", "\n", "        ", "print", "(", "'Incoming line: {}'", ".", "format", "(", "original_tokenized_line", ")", ",", "flush", "=", "True", ")", "\n", "\n", "", "assert", "len", "(", "original_text_whitespace_info", ")", "==", "len", "(", "original_tokenized_line", ".", "split", "(", "' '", ")", ")", "-", "1", "\n", "\n", "num_iterations_done", "=", "0", "\n", "max_iterations_to_try", "=", "500", "\n", "random_number", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "while", "True", ":", "\n", "        ", "tokenized_line", "=", "original_tokenized_line", "\n", "text_whitespace_info", "=", "original_text_whitespace_info", "\n", "line_changes", "=", "[", "]", "\n", "\n", "if", "not", "no_common_other", ":", "\n", "            ", "tokenized_line", ",", "changes", ",", "text_whitespace_info", "=", "aspects", "[", "'common_other'", "]", ".", "apply", "(", "tokenized_line", ",", "text_whitespace_info", ")", "\n", "line_changes", ".", "extend", "(", "changes", ")", "\n", "\n", "", "assert", "len", "(", "text_whitespace_info", ")", "==", "len", "(", "tokenized_line", ".", "split", "(", "' '", ")", ")", "-", "1", "\n", "\n", "if", "not", "no_suffix_prefix", ":", "\n", "            ", "tokenized_line", ",", "changes", ",", "text_whitespace_info", "=", "aspects", "[", "'suffix_prefix'", "]", ".", "apply", "(", "tokenized_line", ",", "text_whitespace_info", ")", "\n", "line_changes", ".", "extend", "(", "changes", ")", "\n", "\n", "", "assert", "len", "(", "text_whitespace_info", ")", "==", "len", "(", "tokenized_line", ".", "split", "(", "' '", ")", ")", "-", "1", "\n", "\n", "if", "not", "no_spelling", ":", "\n", "            ", "tokenized_line", ",", "changes", ",", "text_whitespace_info", "=", "aspects", "[", "'spelling'", "]", ".", "apply", "(", "tokenized_line", ",", "text_whitespace_info", ")", "\n", "line_changes", ".", "extend", "(", "changes", ")", "\n", "\n", "", "assert", "len", "(", "text_whitespace_info", ")", "==", "len", "(", "tokenized_line", ".", "split", "(", "' '", ")", ")", "-", "1", "\n", "\n", "if", "not", "no_word_order", ":", "\n", "            ", "tokenized_line", ",", "changes", ",", "text_whitespace_info", "=", "aspects", "[", "'word_order'", "]", ".", "apply", "(", "tokenized_line", ",", "text_whitespace_info", ")", "\n", "line_changes", ".", "extend", "(", "changes", ")", "\n", "\n", "", "assert", "len", "(", "text_whitespace_info", ")", "==", "len", "(", "tokenized_line", ".", "split", "(", "' '", ")", ")", "-", "1", "\n", "\n", "if", "not", "no_diacritics", ":", "\n", "            ", "tokenized_line", ",", "changes", ",", "text_whitespace_info", "=", "aspects", "[", "'diacritics'", "]", ".", "apply", "(", "tokenized_line", ",", "text_whitespace_info", ")", "\n", "line_changes", ".", "extend", "(", "changes", ")", "\n", "\n", "", "assert", "len", "(", "text_whitespace_info", ")", "==", "len", "(", "tokenized_line", ".", "split", "(", "' '", ")", ")", "-", "1", "\n", "\n", "if", "not", "no_casing", ":", "\n", "            ", "tokenized_line", ",", "changes", ",", "text_whitespace_info", "=", "aspects", "[", "'casing'", "]", ".", "apply", "(", "tokenized_line", ",", "text_whitespace_info", ")", "\n", "line_changes", ".", "extend", "(", "changes", ")", "\n", "\n", "", "assert", "len", "(", "text_whitespace_info", ")", "==", "len", "(", "tokenized_line", ".", "split", "(", "' '", ")", ")", "-", "1", "\n", "\n", "if", "not", "no_whitespace", ":", "\n", "            ", "tokenized_line", ",", "changes", ",", "text_whitespace_info", "=", "aspects", "[", "'whitespace'", "]", ".", "apply", "(", "tokenized_line", ",", "text_whitespace_info", ")", "\n", "line_changes", ".", "extend", "(", "changes", ")", "\n", "\n", "", "assert", "len", "(", "text_whitespace_info", ")", "==", "len", "(", "tokenized_line", ".", "split", "(", "' '", ")", ")", "-", "1", "\n", "\n", "if", "not", "no_punctuation", ":", "\n", "            ", "tokenized_line", ",", "changes", ",", "text_whitespace_info", "=", "aspects", "[", "'punctuation'", "]", ".", "apply", "(", "tokenized_line", ",", "text_whitespace_info", ")", "\n", "line_changes", ".", "extend", "(", "changes", ")", "\n", "\n", "", "assert", "len", "(", "text_whitespace_info", ")", "==", "len", "(", "tokenized_line", ".", "split", "(", "' '", ")", ")", "-", "1", "\n", "\n", "num_iterations_done", "+=", "1", "\n", "if", "num_iterations_done", "<", "max_iterations_to_try", "and", "no_error_sentence_boost", "<", "0", "and", "len", "(", "line_changes", ")", "==", "0", "and", "abs", "(", "\n", "no_error_sentence_boost", ")", "<", "random_number", ":", "\n", "# we did not introduce any error but should have, try again", "\n", "            ", "continue", "\n", "\n", "", "if", "no_error_sentence_boost", ">", "0", "and", "len", "(", "line_changes", ")", "!=", "0", "and", "random_number", "<", "no_error_sentence_boost", ":", "\n", "# we introduced some errors but to make the distribution more similar to reference, we remove the errors from the sentence", "\n", "            ", "tokenized_line", ",", "text_whitespace_info", "=", "original_tokenized_line", ",", "original_text_whitespace_info", "\n", "\n", "# detokenize text", "\n", "# if no tokenizer was provided, the text should be in a tokenized form and space should be in-between all tokens", "\n", "", "if", "not", "tokenizer", ":", "\n", "            ", "text_whitespace_info", "=", "[", "True", "]", "*", "len", "(", "text_whitespace_info", ")", "\n", "\n", "", "detokenized_line", "=", "''", "\n", "for", "i", "in", "range", "(", "len", "(", "tokenized_line", ".", "split", "(", ")", ")", ")", ":", "\n", "            ", "detokenized_line", "+=", "tokenized_line", ".", "split", "(", "' '", ")", "[", "i", "]", "\n", "\n", "if", "i", "<", "len", "(", "text_whitespace_info", ")", "and", "text_whitespace_info", "[", "i", "]", ":", "\n", "                ", "detokenized_line", "+=", "\" \"", "\n", "\n", "", "", "break", "\n", "\n", "", "return", "detokenized_line", ",", "line_changes", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.None.introduce_errors.introduce_errors_into_file": [[209, 253], ["numpy.random.seed", "introduce_errors.get_aspects_generator", "introduce_errors.load_tokenizer", "open", "open", "time.time", "get_aspects_generator.", "introduce_errors.introduce_errors_in_line", "time_stats.append", "line.strip", "outfile.write", "outfile.write", "outfile.write", "print", "outfile.write", "outfile.write", "time.time", "print", "noised_line.strip", "numpy.mean", "line.strip"], "function", ["home.repos.pwc.inspect_result.ufal_kazitext.None.introduce_errors.get_aspects_generator", "home.repos.pwc.inspect_result.ufal_kazitext.None.introduce_errors.load_tokenizer", "home.repos.pwc.inspect_result.ufal_kazitext.None.introduce_errors.introduce_errors_in_line"], ["", "def", "introduce_errors_into_file", "(", "infile", ",", "outfile", ",", "profile_file", ",", "lang", ",", "debug", ",", "alpha", ",", "beta", ",", "save_input", ",", "strip_all_diacritics", ",", "no_diacritics", ",", "\n", "no_spelling", ",", "no_casing", ",", "no_whitespace", ",", "no_punctuation", ",", "no_word_order", ",", "no_suffix_prefix", ",", "no_common_other", ",", "\n", "spelling_detailed_ratio", ",", "verbose", "=", "False", ",", "random_seed", "=", "42", ",", "alpha_min", "=", "None", ",", "alpha_max", "=", "None", ",", "alpha_std", "=", "0", ",", "\n", "alpha_uniformity_prob", "=", "0", ",", "no_error_sentence_boost", "=", "0", ")", ":", "\n", "    ", "np", ".", "random", ".", "seed", "(", "random_seed", ")", "\n", "\n", "aspects_generator", "=", "get_aspects_generator", "(", "profile_file", ",", "lang", ",", "alpha", ",", "beta", ",", "strip_all_diacritics", ",", "spelling_detailed_ratio", ",", "alpha_min", ",", "\n", "alpha_max", ",", "alpha_std", ",", "alpha_uniformity_prob", ")", "\n", "\n", "tokenizer", "=", "load_tokenizer", "(", "lang", ")", "\n", "\n", "time_stats", "=", "[", "]", "\n", "with", "open", "(", "infile", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "infile", ",", "open", "(", "outfile", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "outfile", ":", "\n", "        ", "line_ind", "=", "0", "\n", "for", "line", "in", "infile", ":", "\n", "            ", "if", "not", "line", ".", "strip", "(", ")", ":", "# if empty line, just copy it", "\n", "                ", "outfile", ".", "write", "(", "\"\\n\"", ")", "\n", "continue", "\n", "\n", "", "line_ind", "+=", "1", "\n", "start_line_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "cur_aspects", "=", "aspects_generator", "(", ")", "\n", "noised_line", ",", "line_changes", "=", "introduce_errors_in_line", "(", "line", ",", "tokenizer", ",", "cur_aspects", ",", "no_diacritics", ",", "no_spelling", ",", "no_casing", ",", "\n", "no_whitespace", ",", "no_punctuation", ",", "no_word_order", ",", "no_suffix_prefix", ",", "\n", "no_common_other", ",", "verbose", ",", "no_error_sentence_boost", ")", "\n", "\n", "if", "save_input", ":", "\n", "                ", "outfile", ".", "write", "(", "line", ".", "strip", "(", ")", "+", "\"\\t\"", "+", "noised_line", ".", "strip", "(", ")", "+", "\"\\n\"", ")", "\n", "", "else", ":", "\n", "                ", "outfile", ".", "write", "(", "noised_line", "+", "\"\\n\"", ")", "\n", "\n", "", "if", "debug", ":", "\n", "                ", "print", "(", "line_changes", ")", "\n", "outfile", ".", "write", "(", "\";\"", ".", "join", "(", "[", "\";\"", ".", "join", "(", "x", ")", "for", "x", "in", "line_changes", "]", ")", ")", "\n", "outfile", ".", "write", "(", "\"\\n\"", ")", "\n", "\n", "", "time_stats", ".", "append", "(", "time", ".", "time", "(", ")", "-", "start_line_time", ")", "\n", "\n", "if", "verbose", ":", "\n", "                ", "if", "line_ind", "%", "100", "==", "0", ":", "\n", "                    ", "print", "(", "\"{} processed. 1 line took on average {}\"", ".", "format", "(", "line_ind", ",", "np", ".", "mean", "(", "time_stats", ")", ")", ",", "flush", "=", "True", ")", "\n", "\n", "time_stats", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.None.compute_error_rate.get_edits_info": [[6, 35], ["glob.glob", "open().read().strip().split", "apply_m2_edits.processM2", "len", "open().read().strip", "simple_edits.append", "detailed_edits.append", "simple_edits.append", "detailed_edits.append", "list", "open().read", "coder_dict.keys", "len", "open"], "function", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.apply_m2_edits.processM2"], ["def", "get_edits_info", "(", "m2_pattern", ")", ":", "\n", "    ", "simple_edits", "=", "[", "]", "# ratios of bad / all_tokens", "\n", "detailed_edits", "=", "[", "]", "# in each edit we also count its span both in original and corrected (whole sentence rewrite is one simple but many detailed edits)", "\n", "\n", "for", "f", "in", "glob", ".", "glob", "(", "m2_pattern", "+", "\"*\"", ")", ":", "\n", "        ", "m2_file", "=", "open", "(", "f", ")", ".", "read", "(", ")", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\\n\"", ")", "\n", "for", "info", "in", "m2_file", ":", "\n", "            ", "orig_sent", ",", "coder_dict", "=", "apply_m2_edits", ".", "processM2", "(", "info", ",", "[", "]", ")", "\n", "num_tokens", "=", "len", "(", "orig_sent", ")", "# in tokens", "\n", "\n", "if", "coder_dict", ":", "\n", "                ", "coder_id", "=", "list", "(", "coder_dict", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "\n", "edits", "=", "coder_dict", "[", "coder_id", "]", "[", "1", "]", "\n", "filtered_edits", "=", "[", "edit", "for", "edit", "in", "edits", "if", "edit", "[", "2", "]", "!=", "'noop'", "]", "\n", "simple_edits", ".", "append", "(", "len", "(", "filtered_edits", ")", "/", "num_tokens", ")", "\n", "\n", "num_detailed_edits", "=", "0", "\n", "for", "edit", "in", "filtered_edits", ":", "\n", "                    ", "orig_start", ",", "orig_end", ",", "error_type", ",", "cor_tok", ",", "cor_start", ",", "cor_end", "=", "edit", "\n", "num_detailed_edits", "+=", "orig_end", "-", "orig_start", "\n", "num_detailed_edits", "+=", "cor_end", "-", "cor_start", "\n", "\n", "", "detailed_edits", ".", "append", "(", "num_detailed_edits", "/", "num_tokens", ")", "\n", "", "else", ":", "\n", "                ", "simple_edits", ".", "append", "(", "0", ")", "\n", "detailed_edits", ".", "append", "(", "0", ")", "\n", "\n", "", "", "", "return", "simple_edits", ",", "detailed_edits", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.None.noise_squad.find_answer_borders_in_context": [[8, 23], ["len"], "function", ["None"], ["def", "find_answer_borders_in_context", "(", "answers", ")", ":", "\n", "    ", "min_left_border", "=", "1e10", "\n", "max_right_border", "=", "0", "\n", "\n", "for", "answer", "in", "answers", ":", "\n", "        ", "left_border", "=", "answer", "[", "'answer_start'", "]", "\n", "right_border", "=", "left_border", "+", "len", "(", "answer", "[", "'text'", "]", ")", "\n", "\n", "if", "left_border", "<", "min_left_border", ":", "\n", "            ", "min_left_border", "=", "left_border", "\n", "\n", "", "if", "right_border", ">", "max_right_border", ":", "\n", "            ", "max_right_border", "=", "right_border", "\n", "\n", "", "", "return", "min_left_border", ",", "max_right_border", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.None.udpipe_tokenizer.UDPipeTokenizer.__init__": [[19, 21], ["ufal.udpipe.Model.load"], "methods", ["None"], ["", "", "def", "__init__", "(", "self", ",", "lang", ")", ":", "\n", "        ", "self", ".", "_model", "=", "ufal", ".", "udpipe", ".", "Model", ".", "load", "(", "self", ".", "MODELS", "[", "lang", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.None.udpipe_tokenizer.UDPipeTokenizer.tokenize": [[22, 54], ["udpipe_tokenizer.UDPipeTokenizer._model.newTokenizer", "udpipe_tokenizer.UDPipeTokenizer.setText", "ufal.udpipe.ProcessingError", "ufal.udpipe.Sentence", "udpipe_tokenizer.UDPipeTokenizer.nextSentence", "ufal.udpipe.ProcessingError.occurred", "RuntimeError", "sentences.append", "RuntimeError", "sentences[].append", "udpipe_tokenizer.UDPipeTokenizer.Token", "len", "len", "word.getTokenRangeStart", "word.getTokenRangeEnd"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\" Return tokenized text as a list of sentences, each a list of tokens. \"\"\"", "\n", "\n", "tokenizer", "=", "self", ".", "_model", ".", "newTokenizer", "(", "self", ".", "_model", ".", "TOKENIZER_RANGES", ")", "\n", "if", "not", "tokenizer", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"The model does not have a tokenizer\"", ")", "\n", "\n", "", "tokenizer", ".", "setText", "(", "text", ")", "\n", "error", "=", "ufal", ".", "udpipe", ".", "ProcessingError", "(", ")", "\n", "sentences", "=", "[", "]", "\n", "\n", "sentence", "=", "ufal", ".", "udpipe", ".", "Sentence", "(", ")", "\n", "while", "tokenizer", ".", "nextSentence", "(", "sentence", ",", "error", ")", ":", "\n", "            ", "sentences", ".", "append", "(", "[", "]", ")", "\n", "\n", "multiword_token", "=", "0", "\n", "for", "word", "in", "sentence", ".", "words", "[", "1", ":", "]", ":", "\n", "                ", "while", "multiword_token", "<", "len", "(", "sentence", ".", "multiwordTokens", ")", "and", "word", ".", "id", ">", "sentence", ".", "multiwordTokens", "[", "multiword_token", "]", ".", "idLast", ":", "\n", "                    ", "multiword_token", "+=", "1", "\n", "", "if", "multiword_token", "<", "len", "(", "sentence", ".", "multiwordTokens", ")", "and", "word", ".", "id", ">=", "sentence", ".", "multiwordTokens", "[", "multiword_token", "]", ".", "idFirst", "and", "word", ".", "id", "<=", "sentence", ".", "multiwordTokens", "[", "multiword_token", "]", ".", "idLast", ":", "\n", "                    ", "if", "word", ".", "id", ">", "sentence", ".", "multiwordTokens", "[", "multiword_token", "]", ".", "idFirst", ":", "\n", "                        ", "continue", "\n", "", "word", "=", "sentence", ".", "multiwordTokens", "[", "multiword_token", "]", "\n", "", "sentences", "[", "-", "1", "]", ".", "append", "(", "self", ".", "Token", "(", "word", ".", "form", ",", "word", ".", "getTokenRangeStart", "(", ")", ",", "word", ".", "getTokenRangeEnd", "(", ")", ")", ")", "\n", "\n", "", "", "if", "error", ".", "occurred", "(", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "error", ".", "message", ")", "\n", "\n", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.None.noise_conll_ner.additional_postprocess": [[7, 18], ["original.strip", "numpy.random.rand", "noisy.replace"], "function", ["None"], ["def", "additional_postprocess", "(", "original", ",", "noisy", ")", ":", "\n", "\n", "# ignore docstart lines (in german conll files)", "\n", "    ", "if", "original", ".", "strip", "(", ")", "==", "'-DOCSTART-'", ":", "\n", "        ", "return", "original", "\n", "\n", "", "if", "'\\\\'", "in", "noisy", "and", "'\\\\'", "not", "in", "\" \"", ".", "join", "(", "original", ")", ":", "\n", "        ", "if", "np", ".", "random", ".", "rand", "(", ")", "<", "0.95", ":", "\n", "            ", "return", "noisy", ".", "replace", "(", "'\\\\'", ",", "''", ")", "\n", "\n", "", "", "return", "noisy", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.base.Aspect.__init__": [[9, 11], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "profile", ",", "lang", ",", "alpha", "=", "1", ",", "beta", "=", "0", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.base.Aspect.apply": [[12, 18], ["None"], "methods", ["None"], ["", "def", "apply", "(", "self", ",", "text", ",", "whitespace_info", ")", ":", "\n", "        ", "\"\"\"\n            Apply specific noise to given tokenized text.\n            Whitespace_info stores information on whether space should be inserted between adjacent tokens when detokenizing.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.base.Aspect.estimate_probabilities": [[19, 22], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "estimate_probabilities", "(", "m2_records", ")", ":", "\n", "        ", "pass", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.apply_m2_edits.processEdits": [[6, 24], ["edit.split.split", "[].split", "int", "int", "edit_dict.keys", "edit_dict[].append"], "function", ["None"], ["def", "processEdits", "(", "edits", ")", ":", "\n", "    ", "edit_dict", "=", "{", "}", "\n", "for", "edit", "in", "edits", ":", "\n", "        ", "edit", "=", "edit", ".", "split", "(", "\"|||\"", ")", "\n", "span", "=", "edit", "[", "0", "]", "[", "2", ":", "]", ".", "split", "(", ")", "# [2:] ignore the leading \"A \"", "\n", "start", "=", "int", "(", "span", "[", "0", "]", ")", "\n", "end", "=", "int", "(", "span", "[", "1", "]", ")", "\n", "cat", "=", "edit", "[", "1", "]", "\n", "cor", "=", "edit", "[", "2", "]", "\n", "id", "=", "edit", "[", "-", "1", "]", "\n", "# Save the useful info as a list", "\n", "proc_edit", "=", "[", "start", ",", "end", ",", "cat", ",", "cor", "]", "\n", "# Save the proc edit inside the edit_dict using coder id.", "\n", "if", "id", "in", "edit_dict", ".", "keys", "(", ")", ":", "\n", "            ", "edit_dict", "[", "id", "]", ".", "append", "(", "proc_edit", ")", "\n", "", "else", ":", "\n", "            ", "edit_dict", "[", "id", "]", "=", "[", "proc_edit", "]", "\n", "", "", "return", "edit_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.apply_m2_edits.processM2": [[31, 74], ["info.split.split", "[].split", "apply_m2_edits.processEdits", "processEdits.items", "edit[].split", "gold_edits.append", "gold_edits.append", "len", "len"], "function", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.apply_m2_edits.processEdits"], ["", "def", "processM2", "(", "info", ",", "ignore_edit_types", ")", ":", "\n", "    ", "info", "=", "info", ".", "split", "(", "\"\\n\"", ")", "\n", "orig_sent", "=", "info", "[", "0", "]", "[", "2", ":", "]", ".", "split", "(", ")", "# [2:] ignore the leading \"S \"", "\n", "all_edits", "=", "info", "[", "1", ":", "]", "\n", "# Simplify the edits and group by coder id.", "\n", "edit_dict", "=", "processEdits", "(", "all_edits", ")", "\n", "out_dict", "=", "{", "}", "\n", "# Loop through each coder and their edits.", "\n", "for", "coder", ",", "edits", "in", "edit_dict", ".", "items", "(", ")", ":", "\n", "# Copy orig_sent. We will apply the edits to it to make cor_sent", "\n", "        ", "cor_sent", "=", "orig_sent", "[", ":", "]", "\n", "gold_edits", "=", "[", "]", "\n", "offset", "=", "0", "\n", "for", "edit", "in", "edits", ":", "\n", "# Do not apply noop or Um edits, but save them", "\n", "            ", "if", "edit", "[", "2", "]", "in", "{", "\"noop\"", ",", "\"Um\"", "}", ":", "\n", "                ", "gold_edits", ".", "append", "(", "edit", "+", "[", "-", "1", ",", "-", "1", "]", ")", "\n", "continue", "\n", "\n", "", "if", "ignore_edit_types", ":", "\n", "                ", "is_ignore_edit_type", "=", "False", "\n", "for", "ignore_edit_type", "in", "ignore_edit_types", ":", "\n", "                    ", "if", "ignore_edit_type", "in", "edit", "[", "2", "]", ":", "# substring match", "\n", "                        ", "is_ignore_edit_type", "=", "True", "\n", "\n", "", "", "if", "is_ignore_edit_type", ":", "\n", "                    ", "continue", "\n", "\n", "", "", "orig_start", "=", "edit", "[", "0", "]", "\n", "orig_end", "=", "edit", "[", "1", "]", "\n", "cor_toks", "=", "edit", "[", "3", "]", ".", "split", "(", ")", "\n", "# Apply the edit.", "\n", "cor_sent", "[", "orig_start", "+", "offset", ":", "orig_end", "+", "offset", "]", "=", "cor_toks", "\n", "# Get the cor token start and end positions in cor_sent", "\n", "cor_start", "=", "orig_start", "+", "offset", "\n", "cor_end", "=", "cor_start", "+", "len", "(", "cor_toks", ")", "\n", "# Keep track of how this affects orig edit offsets.", "\n", "offset", "=", "offset", "-", "(", "orig_end", "-", "orig_start", ")", "+", "len", "(", "cor_toks", ")", "\n", "# Save the edit with cor_start and cor_end", "\n", "gold_edits", ".", "append", "(", "edit", "+", "[", "cor_start", "]", "+", "[", "cor_end", "]", ")", "\n", "# Save the cor_sent and gold_edits for each annotator in the out_dict.", "\n", "", "out_dict", "[", "coder", "]", "=", "(", "cor_sent", ",", "gold_edits", ")", "\n", "", "return", "orig_sent", ",", "out_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.apply_m2_edits.main": [[76, 96], ["open", "print", "open().read().strip().split", "open.close", "apply_m2_edits.processM2", "open().read().strip", "sorted", "coder_dict.items", "open.write", "open().read", "open"], "function", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.apply_m2_edits.processM2"], ["", "def", "main", "(", "args", ")", ":", "\n", "# Setup output m2 file", "\n", "    ", "out_parallel", "=", "open", "(", "args", ".", "out", ",", "\"w\"", ")", "\n", "\n", "print", "(", "\"Processing files...\"", ")", "\n", "# Open the m2 file and split into sentence+edit chunks.", "\n", "m2_file", "=", "open", "(", "args", ".", "m2", ")", ".", "read", "(", ")", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\\n\"", ")", "\n", "for", "info", "in", "m2_file", ":", "\n", "# Get the original and corrected sentence + edits for each annotator.", "\n", "        ", "orig_sent", ",", "coder_dict", "=", "processM2", "(", "info", ",", "args", ".", "ignore_edit_types", ")", "\n", "# Save info about types of edit groups seen", "\n", "# Only process sentences with edits.", "\n", "if", "coder_dict", ":", "\n", "# Save marked up original sentence here, if required.", "\n", "# Loop through the annotators", "\n", "            ", "for", "coder", ",", "coder_info", "in", "sorted", "(", "coder_dict", ".", "items", "(", ")", ")", ":", "\n", "                ", "cor_sent", "=", "coder_info", "[", "0", "]", "\n", "out_parallel", ".", "write", "(", "\" \"", ".", "join", "(", "orig_sent", ")", "+", "\"\\t\"", "+", "\" \"", ".", "join", "(", "cor_sent", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "", "", "out_parallel", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.suffix_prefix.SuffixPrefix.__init__": [[9, 20], ["aspects.base.Aspect.__init__"], "methods", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation.__init__"], ["    ", "def", "__init__", "(", "self", ",", "profile", ",", "lang", ",", "alpha", "=", "1", ",", "beta", "=", "0", ")", ":", "\n", "        ", "super", "(", "SuffixPrefix", ",", "self", ")", ".", "__init__", "(", "profile", ",", "alpha", ",", "beta", ")", "\n", "\n", "self", ".", "suffix_table", "=", "profile", "[", "'suffix_prefix'", "]", "[", "'suffix_table'", "]", "\n", "self", ".", "suffix_occurence_counts", "=", "profile", "[", "'suffix_prefix'", "]", "[", "'suffix_occurence_counts'", "]", "\n", "self", ".", "prefix_table", "=", "profile", "[", "'suffix_prefix'", "]", "[", "'prefix_table'", "]", "\n", "self", ".", "prefix_occurence_counts", "=", "profile", "[", "'suffix_prefix'", "]", "[", "'prefix_occurence_counts'", "]", "\n", "\n", "# alfa-self.beta smoothing must be done inside each call (because of counts instead of probabilities)", "\n", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "beta", "=", "beta", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.suffix_prefix.SuffixPrefix.apply": [[21, 94], ["suffix_prefix.SuffixPrefix.apply._introduce_suffix_errors"], "methods", ["None"], ["", "def", "apply", "(", "self", ",", "text", ",", "whitespace_info", ")", ":", "\n", "        ", "def", "_introduce_suffix_errors", "(", "local_text", ",", "suffix_table", ",", "suffix_occurence_counts", ")", ":", "\n", "            ", "new_text", "=", "[", "]", "\n", "changes", "=", "[", "]", "\n", "# first introduce suffix errors", "\n", "for", "word", "in", "local_text", ".", "split", "(", "' '", ")", ":", "\n", "                ", "word_suffixes", "=", "[", "word", "[", "i", ":", "]", "for", "i", "in", "range", "(", "1", ",", "len", "(", "word", ")", ")", "]", "+", "[", "\"\"", "]", "\n", "found_word_suffixes", "=", "[", "]", "\n", "word_suffixes_probs", "=", "[", "]", "\n", "\n", "sum_rewrites_that_go_on", "=", "0", "\n", "last_match_sum_applicable", "=", "0", "\n", "for", "word_suffix", "in", "word_suffixes", ":", "\n", "                    ", "if", "word_suffix", "in", "suffix_table", ":", "\n", "                        ", "found_word_suffixes", ".", "append", "(", "word_suffix", ")", "\n", "sum_rewrites_in_data", "=", "np", ".", "sum", "(", "list", "(", "suffix_table", "[", "word_suffix", "]", ".", "values", "(", ")", ")", ")", "-", "sum_rewrites_that_go_on", "\n", "sum_applicable_in_data", "=", "suffix_occurence_counts", "[", "word_suffix", "]", "-", "last_match_sum_applicable", "\n", "\n", "sum_rewrites_that_go_on", "=", "0", "\n", "if", "word_suffix", ":", "# if empty, this is an inserting of the suffix and is the last element of the for-cycle", "\n", "                            ", "for", "word_suffix_alternative", "in", "suffix_table", "[", "word_suffix", "]", ":", "\n", "                                ", "if", "len", "(", "word_suffix_alternative", ")", ">", "0", "and", "word_suffix_alternative", "[", "0", "]", "==", "word_suffix", "[", "0", "]", ":", "\n", "                                    ", "sum_rewrites_that_go_on", "+=", "suffix_table", "[", "word_suffix", "]", "[", "word_suffix_alternative", "]", "\n", "\n", "", "", "", "last_match_sum_applicable", "=", "sum_applicable_in_data", "\n", "\n", "if", "sum_applicable_in_data", "==", "0", ":", "\n", "                            ", "word_suffixes_probs", ".", "append", "(", "0", ")", "\n", "", "else", ":", "\n", "                            ", "word_suffixes_probs", ".", "append", "(", "sum_rewrites_in_data", "/", "sum_applicable_in_data", ")", "\n", "\n", "", "", "", "if", "not", "word_suffixes_probs", ":", "\n", "# no edit is applicable", "\n", "                    ", "new_text", ".", "append", "(", "word", ")", "\n", "continue", "\n", "\n", "# select suffix (according to probability distribution)", "\n", "", "word_suffixes_probs", "=", "np", ".", "array", "(", "word_suffixes_probs", ")", "\n", "word_suffixes_probs_normalized", "=", "word_suffixes_probs", "/", "np", ".", "sum", "(", "word_suffixes_probs", ")", "\n", "word_suffixes_probs_normalized_smoothed", "=", "utils", ".", "_apply_smoothing", "(", "word_suffixes_probs_normalized", ",", "self", ".", "alpha", ",", "self", ".", "beta", ")", "\n", "chosen_suffix_ind", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "found_word_suffixes", ")", ",", "p", "=", "word_suffixes_probs_normalized_smoothed", ")", "\n", "\n", "word_suffixes_probs_smoothed", "=", "utils", ".", "_apply_smoothing", "(", "word_suffixes_probs", ",", "self", ".", "alpha", ",", "self", ".", "beta", ")", "\n", "chosen_suffix", ",", "chosen_suffix_sum_prob", "=", "found_word_suffixes", "[", "chosen_suffix_ind", "]", ",", "word_suffixes_probs_smoothed", "[", "\n", "chosen_suffix_ind", "]", "\n", "\n", "# toss a coin for the chosen suffix", "\n", "chosen_suffix_sum_prob_smoothed", "=", "utils", ".", "_apply_smoothing", "(", "[", "chosen_suffix_sum_prob", "]", ",", "self", ".", "alpha", ",", "self", ".", "beta", ")", "[", "0", "]", "\n", "if", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "chosen_suffix_sum_prob_smoothed", ":", "\n", "# choose what to rewrite the suffix into", "\n", "                    ", "rewrite_into_probs", "=", "np", ".", "array", "(", "list", "(", "suffix_table", "[", "chosen_suffix", "]", ".", "values", "(", ")", ")", ")", "/", "np", ".", "sum", "(", "\n", "np", ".", "array", "(", "list", "(", "suffix_table", "[", "chosen_suffix", "]", ".", "values", "(", ")", ")", ")", ")", "\n", "\n", "rewrite_into_probs_smoothed", "=", "utils", ".", "_apply_smoothing", "(", "rewrite_into_probs", ",", "self", ".", "alpha", ",", "self", ".", "beta", ")", "\n", "chosen_rewrite_into_tokens", "=", "np", ".", "random", ".", "choice", "(", "list", "(", "suffix_table", "[", "chosen_suffix", "]", ".", "keys", "(", ")", ")", ",", "p", "=", "rewrite_into_probs_smoothed", ")", "\n", "\n", "if", "len", "(", "chosen_suffix", ")", "==", "0", ":", "# inserting suffix after this word", "\n", "                        ", "new_word", "=", "word", "+", "chosen_rewrite_into_tokens", "\n", "", "else", ":", "\n", "                        ", "new_word", "=", "word", "[", ":", "-", "len", "(", "chosen_suffix", ")", "]", "+", "chosen_rewrite_into_tokens", "\n", "", "new_text", ".", "append", "(", "new_word", ")", "\n", "changes", ".", "append", "(", "[", "'SUFFIX'", ",", "'change {} -> {}'", ".", "format", "(", "word", ",", "new_word", ")", "]", ")", "\n", "", "else", ":", "\n", "                    ", "new_text", ".", "append", "(", "word", ")", "\n", "\n", "", "", "return", "\" \"", ".", "join", "(", "new_text", ")", ",", "changes", "\n", "\n", "", "text", ",", "suffix_changes", ",", "=", "_introduce_suffix_errors", "(", "text", ",", "self", ".", "suffix_table", ",", "self", ".", "suffix_occurence_counts", ")", "\n", "\n", "text", "=", "\" \"", ".", "join", "(", "[", "\"\"", ".", "join", "(", "reversed", "(", "word", ")", ")", "for", "word", "in", "text", ".", "split", "(", "' '", ")", "]", ")", "\n", "text", ",", "prefix_changes", "=", "_introduce_suffix_errors", "(", "text", ",", "self", ".", "prefix_table", ",", "self", ".", "prefix_occurence_counts", ")", "\n", "text", "=", "\" \"", ".", "join", "(", "[", "\"\"", ".", "join", "(", "reversed", "(", "word", ")", ")", "for", "word", "in", "text", ".", "split", "(", "' '", ")", "]", ")", "\n", "return", "text", ",", "suffix_changes", "+", "prefix_changes", ",", "whitespace_info", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.suffix_prefix.SuffixPrefix._get_occurence_count_of_tokens_in_text": [[95, 114], ["text.index", "len", "text[].isalpha", "len", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_occurence_count_of_tokens_in_text", "(", "text", ",", "suffix", ")", ":", "\n", "        ", "'''\n        Get all occurences of suffix in text.\n\n        e.g. ed is suffix in \"I have walked\"; but is not suffix in \"I love reddish\"\n        '''", "\n", "num_occurences_of_suffix_in_cor", "=", "0", "\n", "last_occurence_start_index", "=", "-", "1", "\n", "while", "suffix", "in", "text", "[", "last_occurence_start_index", "+", "1", ":", "]", ":", "\n", "            ", "start_index", "=", "text", ".", "index", "(", "suffix", ",", "last_occurence_start_index", "+", "1", ")", "\n", "\n", "# matched suffix is either suffix of whole text or the character after the matched suffix is not self.alpha", "\n", "if", "(", "start_index", "+", "len", "(", "suffix", ")", "==", "len", "(", "text", ")", ")", "or", "not", "text", "[", "start_index", "+", "len", "(", "suffix", ")", "]", ".", "isalpha", "(", ")", ":", "\n", "                ", "num_occurences_of_suffix_in_cor", "+=", "1", "\n", "\n", "", "last_occurence_start_index", "=", "start_index", "\n", "\n", "", "return", "num_occurences_of_suffix_in_cor", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.suffix_prefix.SuffixPrefix._get_xfix_occurence_counts": [[115, 144], ["list", "coder_dict.keys", "cor_toks_xfix.strip", "sum", "suffix_prefix.SuffixPrefix._get_occurence_count_of_tokens_in_text", "reversed", "cor_sent.split", "x.isalpha"], "methods", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.common_other.CommonOther._get_occurence_count_of_tokens_in_text"], ["", "@", "staticmethod", "\n", "def", "_get_xfix_occurence_counts", "(", "xfix_table", ",", "coder_dicts", ",", "reverse", "=", "False", ")", ":", "\n", "        ", "''''\n        For each possible xfix (in xfix_table), count, how many times this suffix appeared in all corrected sentences (coder_dicts).\n        '''", "\n", "num_occurence", "=", "{", "}", "\n", "\n", "for", "coder_dict", "in", "coder_dicts", ":", "\n", "            ", "if", "coder_dict", ":", "\n", "                ", "coder_id", "=", "list", "(", "coder_dict", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "\n", "cor_sent", "=", "coder_dict", "[", "coder_id", "]", "[", "0", "]", "\n", "\n", "if", "reverse", ":", "\n", "                    ", "cor_sent", "=", "[", "\"\"", ".", "join", "(", "reversed", "(", "word", ")", ")", "for", "word", "in", "cor_sent", "]", "\n", "\n", "", "cor_sent", "=", "\" \"", ".", "join", "(", "cor_sent", ")", ".", "lower", "(", ")", "\n", "\n", "for", "cor_toks_xfix", "in", "xfix_table", ":", "\n", "                    ", "if", "cor_toks_xfix", "not", "in", "num_occurence", ":", "\n", "                        ", "num_occurence", "[", "cor_toks_xfix", "]", "=", "0", "\n", "\n", "", "if", "not", "cor_toks_xfix", ".", "strip", "(", ")", ":", "# empty suffix replace with some other (=insert some substring after token)", "\n", "                        ", "num_alpha_in_sent", "=", "sum", "(", "[", "1", "for", "x", "in", "cor_sent", ".", "split", "(", "' '", ")", "if", "x", ".", "isalpha", "(", ")", "]", ")", "\n", "num_occurence", "[", "cor_toks_xfix", "]", "+=", "num_alpha_in_sent", "\n", "", "else", ":", "\n", "                        ", "num_occurence", "[", "cor_toks_xfix", "]", "+=", "SuffixPrefix", ".", "_get_occurence_count_of_tokens_in_text", "(", "cor_sent", ",", "cor_toks_xfix", ")", "\n", "\n", "", "", "", "", "return", "num_occurence", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.suffix_prefix.SuffixPrefix._update_xfix_table": [[145, 210], ["cor_toks.lower.lower.lower", "orig_toks.isalpha", "cor_toks.lower.lower.isalpha", "len", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_update_xfix_table", "(", "edit", ",", "orig_sent", ",", "xfix_table", ")", ":", "\n", "        ", "'''\n        Update suffix/prefix table with all xfix changes that appeared in the edit.\n\n        Although this method is called update_xfix_table, it internally computes everything as suffixes and when used for prefixes, orig_sent\n        and cor_tok in edit must be reversed.\n\n        e.g. given edit \"hezk\u00fd\" for original token \"hezkej\", it increments all following pairs: (ezk\u00fd, ezkej), (zk\u00fd, zkej), (k\u00fd, kej), (\u00fd, ej)\n        :param edit:\n        :param orig_sent:\n        :param xfix_table: dict in format xfix_table[cor_xfix][orig_xfix] = count\n        :return:\n        '''", "\n", "orig_start", ",", "orig_end", ",", "error_type", ",", "cor_toks", ",", "_", ",", "_", "=", "edit", "\n", "\n", "# lower down all tokens to cope with lack of data", "\n", "orig_toks", "=", "\" \"", ".", "join", "(", "orig_sent", "[", "orig_start", ":", "orig_end", "]", ")", ".", "lower", "(", ")", "\n", "cor_toks", "=", "cor_toks", ".", "lower", "(", ")", "\n", "\n", "if", "orig_toks", "==", "cor_toks", ":", "\n", "# this should not happen if data were annotated by Errant", "\n", "            ", "return", "\n", "\n", "# # we allow whitespaces only in original (noisy) text and do not allow them in corrected (difficult to apply). Both tokens must be self.alpha", "\n", "# if not orig_toks.replace(\" \", \"\").isalpha() or not cor_toks.isalpha():", "\n", "#     return", "\n", "\n", "# we do not allow whitespaces in original (noisy) text and also do not allow them in corrected. Both tokens must be self.alpha", "\n", "", "if", "not", "orig_toks", ".", "isalpha", "(", ")", "or", "not", "cor_toks", ".", "isalpha", "(", ")", ":", "\n", "            ", "return", "\n", "\n", "# if the first characters are not same, this is not suffix", "\n", "", "if", "orig_toks", "[", "0", "]", "!=", "cor_toks", "[", "0", "]", ":", "\n", "            ", "return", "\n", "\n", "", "i", "=", "1", "\n", "end_after_this_iteration", "=", "False", "\n", "'''\n        We iterate through the orig_toks and cor_toks until we pass the first non-matching-block and then immediately stop\n        e.g.\n        Given orig = akdy\u017e and cor = a\u017e, we want to save (kdy\u017e, \u017e) as frequent pair, but do not want to proceed and do not want to save (dy\u017e, )\n        Similarly, orig = hezk\u00fd, cor = hezkej, we want to save all pairs up to (\u00fd, ej)\n        '''", "\n", "\n", "while", "True", ":", "\n", "            ", "cor_toks_current_suffix", "=", "cor_toks", "[", "i", ":", "]", "\n", "orig_toks_current_suffix", "=", "orig_toks", "[", "i", ":", "]", "\n", "\n", "if", "len", "(", "cor_toks_current_suffix", ")", "==", "0", "or", "len", "(", "orig_toks_current_suffix", ")", "==", "0", "or", "cor_toks_current_suffix", "[", "0", "]", "!=", "orig_toks_current_suffix", "[", "0", "]", ":", "\n", "                ", "end_after_this_iteration", "=", "True", "\n", "\n", "", "if", "cor_toks_current_suffix", "not", "in", "xfix_table", ":", "\n", "                ", "xfix_table", "[", "cor_toks_current_suffix", "]", "=", "{", "}", "\n", "\n", "", "if", "orig_toks_current_suffix", "not", "in", "xfix_table", "[", "cor_toks_current_suffix", "]", ":", "\n", "                ", "xfix_table", "[", "cor_toks_current_suffix", "]", "[", "orig_toks_current_suffix", "]", "=", "0", "\n", "\n", "", "xfix_table", "[", "cor_toks_current_suffix", "]", "[", "orig_toks_current_suffix", "]", "+=", "1", "\n", "\n", "i", "+=", "1", "\n", "\n", "if", "end_after_this_iteration", ":", "\n", "                ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.suffix_prefix.SuffixPrefix.estimate_probabilities": [[211, 268], ["suffix_prefix.SuffixPrefix.estimate_probabilities.filter_table_by_min_occurence_count"], "methods", ["None"], ["", "", "", "@", "staticmethod", "\n", "def", "estimate_probabilities", "(", "m2_records", ")", ":", "\n", "        ", "suffix_table", "=", "{", "}", "\n", "prefix_table", "=", "{", "}", "\n", "\n", "cached_coder_dicts", "=", "[", "]", "\n", "for", "m2_file", "in", "m2_records", ":", "\n", "            ", "for", "info", "in", "m2_file", ":", "\n", "                ", "orig_sent", ",", "coder_dict", "=", "apply_m2_edits", ".", "processM2", "(", "info", ",", "[", "]", ")", "\n", "if", "coder_dict", ":", "\n", "                    ", "coder_id", "=", "list", "(", "coder_dict", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "\n", "cached_coder_dicts", ".", "append", "(", "coder_dict", ")", "\n", "\n", "for", "edit", "in", "coder_dict", "[", "coder_id", "]", "[", "1", "]", ":", "\n", "                        ", "orig_start", ",", "orig_end", ",", "error_type", ",", "cor_tok", ",", "cor_start", ",", "cor_end", "=", "edit", "\n", "\n", "exclude_error_types", "=", "[", "'noop'", ",", "'PUNCT'", ",", "'CASING'", ",", "'DIACR'", ",", "'UNK'", "]", "\n", "\n", "if", "all", "(", "[", "x", "not", "in", "error_type", "for", "x", "in", "exclude_error_types", "]", ")", ":", "\n", "                            ", "SuffixPrefix", ".", "_update_xfix_table", "(", "edit", ",", "orig_sent", ",", "suffix_table", ")", "\n", "\n", "# reverse the sentence to update prefix table", "\n", "orig_sent_reversed_words", "=", "[", "\"\"", ".", "join", "(", "reversed", "(", "word", ")", ")", "for", "word", "in", "orig_sent", "]", "\n", "cor_tok_reversed", "=", "\" \"", ".", "join", "(", "[", "\"\"", ".", "join", "(", "reversed", "(", "word", ")", ")", "for", "word", "in", "cor_tok", ".", "split", "(", "' '", ")", "]", ")", "\n", "edit", "=", "(", "orig_start", ",", "orig_end", ",", "error_type", ",", "cor_tok_reversed", ",", "cor_start", ",", "cor_end", ")", "\n", "\n", "SuffixPrefix", ".", "_update_xfix_table", "(", "edit", ",", "orig_sent_reversed_words", ",", "prefix_table", ")", "\n", "\n", "# filter out edits that were done not often enough (are rather noise)", "\n", "", "", "", "", "", "def", "filter_table_by_min_occurence_count", "(", "table", ",", "local_filter_out_min_occ_count", ")", ":", "\n", "            ", "table_filtered", "=", "{", "}", "\n", "for", "cor_tok_local", "in", "table", ":", "\n", "                ", "if", "np", ".", "sum", "(", "list", "(", "table", "[", "cor_tok_local", "]", ".", "values", "(", ")", ")", ")", "<", "local_filter_out_min_occ_count", ":", "\n", "                    ", "continue", "\n", "\n", "", "for", "orig_tok", "in", "table", "[", "cor_tok_local", "]", ":", "\n", "                    ", "if", "table", "[", "cor_tok_local", "]", "[", "orig_tok", "]", ">=", "local_filter_out_min_occ_count", ":", "\n", "                        ", "if", "cor_tok_local", "not", "in", "table_filtered", ":", "\n", "                            ", "table_filtered", "[", "cor_tok_local", "]", "=", "{", "}", "\n", "\n", "", "table_filtered", "[", "cor_tok_local", "]", "[", "orig_tok", "]", "=", "table", "[", "cor_tok_local", "]", "[", "orig_tok", "]", "\n", "\n", "", "", "", "return", "table_filtered", "\n", "\n", "", "filter_out_min_occ_count", "=", "3", "\n", "suffix_table_filtered", "=", "filter_table_by_min_occurence_count", "(", "suffix_table", ",", "filter_out_min_occ_count", ")", "\n", "prefix_table_filtered", "=", "filter_table_by_min_occurence_count", "(", "prefix_table", ",", "filter_out_min_occ_count", ")", "\n", "\n", "suffix_occurence_counts", "=", "SuffixPrefix", ".", "_get_xfix_occurence_counts", "(", "suffix_table_filtered", ",", "cached_coder_dicts", ")", "\n", "prefix_occurence_counts", "=", "SuffixPrefix", ".", "_get_xfix_occurence_counts", "(", "prefix_table_filtered", ",", "cached_coder_dicts", ",", "reverse", "=", "True", ")", "\n", "\n", "return", "'suffix_prefix'", ",", "{", "\n", "'suffix_table'", ":", "suffix_table_filtered", ",", "\n", "'suffix_occurence_counts'", ":", "suffix_occurence_counts", ",", "\n", "'prefix_table'", ":", "prefix_table_filtered", ",", "\n", "'prefix_occurence_counts'", ":", "prefix_occurence_counts", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.casing.Casing.__init__": [[10, 22], ["aspects.base.Aspect.__init__", "word_casing_probs.keys", "aspects.utils._apply_smoothing_on_simple_dict", "aspects.utils._apply_smoothing"], "methods", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation.__init__", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing_on_simple_dict", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing"], ["    ", "def", "__init__", "(", "self", ",", "profile", ",", "lang", ",", "alpha", "=", "1", ",", "beta", "=", "0", ")", ":", "\n", "        ", "super", "(", "Casing", ",", "self", ")", ".", "__init__", "(", "profile", ",", "alpha", ",", "beta", ")", "\n", "\n", "word_casing_probs", "=", "profile", "[", "'casing'", "]", "[", "'word_casing_probs'", "]", "\n", "self", ".", "word_casing_probs", "=", "{", "}", "\n", "for", "k", "in", "word_casing_probs", ".", "keys", "(", ")", ":", "\n", "            ", "self", ".", "word_casing_probs", "[", "k", "]", "=", "utils", ".", "_apply_smoothing_on_simple_dict", "(", "word_casing_probs", "[", "k", "]", ",", "alpha", ",", "beta", ")", "\n", "\n", "", "char_change_case_prob", "=", "profile", "[", "'casing'", "]", "[", "'char_change_case_prob'", "]", "\n", "self", ".", "char_change_case_prob", "=", "utils", ".", "_apply_smoothing", "(", "[", "char_change_case_prob", "]", ",", "alpha", ",", "beta", ")", "[", "0", "]", "\n", "\n", "self", ".", "final_punctuation_marks", "=", "[", "'.'", ",", "'!'", ",", "'?'", "]", "+", "[", "\"\\\"\"", ",", "\"\u201e\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.casing.Casing.apply": [[23, 61], ["enumerate", "text.split", "word[].isupper", "new_text.append", "changes.append", "len", "numpy.random.uniform", "new_text.append", "changes.append", "word[].lower", "len", "word.lower", "numpy.random.uniform", "word.lower", "list", "new_text.append", "changes.append", "new_text.append", "word.lower", "word.upper", "numpy.random.uniform", "list", "enumerate", "numpy.random.uniform", "char.isupper", "char.lower", "char.upper"], "methods", ["None"], ["", "def", "apply", "(", "self", ",", "text", ",", "whitespace_info", ")", ":", "\n", "        ", "changes", "=", "[", "]", "\n", "new_text", "=", "[", "]", "\n", "for", "word_ind", ",", "word", "in", "enumerate", "(", "text", ".", "split", "(", ")", ")", ":", "\n", "\n", "# if not word.isalpha():", "\n", "#     new_text.append(word)", "\n", "#     continue", "\n", "\n", "            ", "if", "word_ind", "==", "0", "or", "text", "[", "word_ind", "-", "1", "]", "in", "self", ".", "final_punctuation_marks", ":", "\n", "                ", "applicability_place", "=", "'start'", "\n", "", "else", ":", "\n", "                ", "applicability_place", "=", "'other'", "\n", "\n", "", "if", "len", "(", "word", ")", ">", "0", "and", "word", "[", "0", "]", ".", "isupper", "(", ")", "and", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "self", ".", "word_casing_probs", "[", "applicability_place", "]", "[", "'first_lower'", "]", ":", "\n", "                ", "new_text", ".", "append", "(", "word", "[", "0", "]", ".", "lower", "(", ")", "+", "word", "[", "1", ":", "]", ")", "\n", "changes", ".", "append", "(", "[", "'CASING'", ",", "'first_lower {}'", ".", "format", "(", "word", ")", "]", ")", "\n", "", "elif", "len", "(", "word", ")", ">", "0", "and", "word", ".", "lower", "(", ")", "!=", "word", "and", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "self", ".", "word_casing_probs", "[", "applicability_place", "]", "[", "'all_lower'", "]", ":", "\n", "                ", "new_text", ".", "append", "(", "word", ".", "lower", "(", ")", ")", "\n", "changes", ".", "append", "(", "[", "'CASING'", ",", "'all_lower {}'", ".", "format", "(", "word", ")", "]", ")", "\n", "# note that when doing mixed casing, we need to check that the word's upper- and lower- cased version actually differ (e.g. \u9214)", "\n", "", "elif", "word", ".", "lower", "(", ")", "!=", "word", ".", "upper", "(", ")", "and", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "self", ".", "word_casing_probs", "[", "applicability_place", "]", "[", "'other'", "]", ":", "\n", "                ", "new_word", "=", "list", "(", "word", ")", "\n", "\n", "while", "new_word", "==", "list", "(", "word", ")", ":", "\n", "                    ", "for", "char_ind", ",", "char", "in", "enumerate", "(", "word", ")", ":", "\n", "                        ", "if", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "self", ".", "char_change_case_prob", ":", "\n", "                            ", "if", "char", ".", "isupper", "(", ")", ":", "\n", "                                ", "new_word", "[", "char_ind", "]", "=", "char", ".", "lower", "(", ")", "\n", "", "else", ":", "\n", "                                ", "new_word", "[", "char_ind", "]", "=", "char", ".", "upper", "(", ")", "\n", "\n", "", "", "", "", "new_text", ".", "append", "(", "\"\"", ".", "join", "(", "new_word", ")", ")", "\n", "changes", ".", "append", "(", "[", "'CASING'", ",", "'other {} -> {}'", ".", "format", "(", "word", ",", "new_text", "[", "-", "1", "]", ")", "]", ")", "\n", "", "else", ":", "\n", "                ", "new_text", ".", "append", "(", "word", ")", "\n", "\n", "", "", "return", "\" \"", ".", "join", "(", "new_text", ")", ",", "changes", ",", "whitespace_info", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.casing.Casing.estimate_probabilities": [[62, 175], ["numpy.mean", "zip", "len", "aspects.apply_m2_edits.processM2", "sum", "sum", "sum", "list", "sum", "coder_dict.keys", "cor_tok[].lower", "cor_tok.lower", "char_change_case_probs.append", "x[].isupper", "x.isalpha", "casing.Casing.estimate_probabilities.get_change_char_case_prob_for_pair"], "methods", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.apply_m2_edits.processM2"], ["", "@", "staticmethod", "\n", "def", "estimate_probabilities", "(", "m2_records", ")", ":", "\n", "        ", "'''\n\n            :param m2_files_loaded: list of loaded m2_files\n             i.e. one item of the list was obtained by sequence of:\n                m2_file = open(m2_file).read().strip().split(\"\\n\\n\")\n            :return:\n            '''", "\n", "\n", "'''\n        We differentiate between casing errors on the token right after dot or at the start of whole text, and casing errors in other places.\n        We also note, whether the error is in the first letter or whole word was lowered or some other error.\n        '''", "\n", "casing_errors", "=", "{", "\n", "'start'", ":", "{", "\n", "'first_lower'", ":", "0", ",", "# user wrote a lower-cased first letter of the word but should have written upper-cased letter", "\n", "'all_lower'", ":", "0", ",", "# user wrote whole word in lower-case but some chars should have been uppercased", "\n", "'other'", ":", "0", "# all other cases", "\n", "}", ",", "\n", "'other'", ":", "{", "\n", "'first_lower'", ":", "0", ",", "\n", "'all_lower'", ":", "0", ",", "\n", "'other'", ":", "0", "\n", "}", ",", "\n", "}", "\n", "\n", "num_all_alpha_words_wo_start", "=", "0", "# all words whose casing could be changed", "\n", "num_tokens_with_first_upper_wo_start", "=", "0", "# all words starting with upper case letter", "\n", "num_tokens_with_any_upper_wo_start", "=", "0", "# all words containing at least one upper letter", "\n", "num_start_tokens", "=", "0", "\n", "\n", "'''\n        First lower and all_lower are revertible (we know what to do), but other is non-revertible, so we estimate\n        number of chars that changed its casing\n        '''", "\n", "char_change_case_probs", "=", "[", "]", "\n", "\n", "def", "get_change_char_case_prob_for_pair", "(", "orig", ",", "cor", ")", ":", "\n", "            ", "num_change", "=", "0", "\n", "for", "o", ",", "c", "in", "zip", "(", "orig", ",", "cor", ")", ":", "\n", "                ", "if", "o", "!=", "c", ":", "\n", "                    ", "num_change", "+=", "1", "\n", "\n", "", "", "return", "num_change", "/", "len", "(", "orig", ")", "\n", "\n", "", "final_punctuation_marks", "=", "[", "'.'", ",", "'!'", ",", "'?'", "]", "+", "[", "\"\\\"\"", ",", "\"\u201e\"", "]", "# \" e.g. \u201eByls u\u017e v Gr\u00f3nsku?\u201c", "\n", "\n", "for", "m2_file", "in", "m2_records", ":", "\n", "\n", "            ", "for", "info", "in", "m2_file", ":", "\n", "                ", "orig_sent", ",", "coder_dict", "=", "apply_m2_edits", ".", "processM2", "(", "info", ",", "[", "]", ")", "\n", "\n", "if", "coder_dict", ":", "\n", "                    ", "coder_id", "=", "list", "(", "coder_dict", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "cor_sent", "=", "coder_dict", "[", "coder_id", "]", "[", "0", "]", "\n", "\n", "for", "edit", "in", "coder_dict", "[", "coder_id", "]", "[", "1", "]", ":", "\n", "                        ", "orig_start", ",", "orig_end", ",", "error_type", ",", "cor_tok", ",", "cor_start", ",", "cor_end", "=", "edit", "\n", "\n", "if", "'ORTH:CASING'", "in", "error_type", "or", "(", "\n", "'ORTH'", "in", "error_type", "and", "\" \"", ".", "join", "(", "orig_sent", "[", "orig_start", ":", "orig_end", "]", ")", ".", "lower", "(", ")", "==", "\" \"", ".", "join", "(", "\n", "cor_sent", "[", "cor_start", ":", "cor_end", "]", ")", ".", "lower", "(", ")", ")", ":", "\n", "                            ", "if", "(", "orig_start", "==", "0", ")", "or", "(", "orig_sent", "[", "orig_start", "-", "1", "]", "in", "final_punctuation_marks", ")", ":", "\n", "                                ", "applicability_place", "=", "'start'", "\n", "", "else", ":", "\n", "                                ", "applicability_place", "=", "'other'", "\n", "\n", "", "cor_tok_first_lower", "=", "cor_tok", "[", "0", "]", ".", "lower", "(", ")", "+", "cor_tok", "[", "1", ":", "]", "\n", "if", "\" \"", ".", "join", "(", "orig_sent", "[", "orig_start", ":", "orig_end", "]", ")", "==", "cor_tok_first_lower", ":", "\n", "                                ", "diacr_type", "=", "'first_lower'", "\n", "", "elif", "\" \"", ".", "join", "(", "orig_sent", "[", "orig_start", ":", "orig_end", "]", ")", "==", "cor_tok", ".", "lower", "(", ")", ":", "\n", "                                ", "diacr_type", "=", "'all_lower'", "\n", "", "else", ":", "\n", "                                ", "diacr_type", "=", "'other'", "\n", "char_change_case_probs", ".", "append", "(", "\n", "get_change_char_case_prob_for_pair", "(", "\" \"", ".", "join", "(", "orig_sent", "[", "orig_start", ":", "orig_end", "]", ")", ",", "cor_tok", ")", ")", "\n", "\n", "", "casing_errors", "[", "applicability_place", "]", "[", "diacr_type", "]", "+=", "1", "\n", "\n", "", "", "num_start_tokens", "+=", "sum", "(", "[", "1", "for", "x", "in", "orig_sent", "if", "\n", "x", "in", "final_punctuation_marks", "]", ")", "# no + 1 for the first token, because there is no token after last punct", "\n", "\n", "num_tokens_with_first_upper_wo_start", "+=", "sum", "(", "[", "1", "for", "x", "in", "orig_sent", "[", "1", ":", "]", "if", "\n", "len", "(", "x", ")", ">", "0", "and", "x", "[", "0", "]", ".", "isupper", "(", ")", "]", ")", "\n", "\n", "num_tokens_with_any_upper_wo_start", "+=", "sum", "(", "[", "1", "for", "x", "in", "orig_sent", "[", "1", ":", "]", "if", "\n", "len", "(", "x", ")", ">", "0", "and", "x", ".", "lower", "(", ")", "!=", "x", "]", ")", "\n", "\n", "num_all_alpha_words_wo_start", "+=", "sum", "(", "[", "1", "for", "x", "in", "orig_sent", "if", "\n", "x", ".", "isalpha", "(", ")", "]", ")", "-", "1", "# -1 for the first token (punct tokens are filtered out as they are not alpha)", "\n", "\n", "", "", "", "word_casing_probs", "=", "{", "}", "\n", "for", "applicability_place", "in", "casing_errors", ":", "\n", "            ", "word_casing_probs", "[", "applicability_place", "]", "=", "{", "}", "\n", "for", "diacr_type", "in", "casing_errors", "[", "applicability_place", "]", ":", "\n", "                ", "if", "applicability_place", "==", "'start'", ":", "\n", "                    ", "word_casing_probs", "[", "applicability_place", "]", "[", "diacr_type", "]", "=", "casing_errors", "[", "applicability_place", "]", "[", "\n", "diacr_type", "]", "/", "num_start_tokens", "\n", "", "else", ":", "\n", "                    ", "if", "diacr_type", "==", "'first_lower'", ":", "\n", "                        ", "word_casing_probs", "[", "applicability_place", "]", "[", "diacr_type", "]", "=", "casing_errors", "[", "applicability_place", "]", "[", "\n", "diacr_type", "]", "/", "num_tokens_with_first_upper_wo_start", "\n", "", "elif", "diacr_type", "==", "'all_lower'", ":", "\n", "                        ", "word_casing_probs", "[", "applicability_place", "]", "[", "diacr_type", "]", "=", "casing_errors", "[", "applicability_place", "]", "[", "\n", "diacr_type", "]", "/", "num_tokens_with_any_upper_wo_start", "\n", "", "else", ":", "\n", "                        ", "word_casing_probs", "[", "applicability_place", "]", "[", "diacr_type", "]", "=", "casing_errors", "[", "applicability_place", "]", "[", "\n", "diacr_type", "]", "/", "num_all_alpha_words_wo_start", "\n", "\n", "", "", "", "", "char_change_case_prob", "=", "np", ".", "mean", "(", "char_change_case_probs", ")", "\n", "return", "'casing'", ",", "{", "'word_casing_probs'", ":", "word_casing_probs", ",", "\n", "'char_change_case_prob'", ":", "char_change_case_prob", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.spelling.Spelling.__init__": [[13, 57], ["aspects.base.Aspect.__init__", "aspects.utils._apply_smoothing", "aspects.utils._apply_smoothing_on_simple_dict", "spelling.Spelling.spelling_noise_operation_detailed_probs[].items", "spelling.Spelling.spelling_noise_operation_detailed_probs[].items", "spelling.Spelling.spelling_noise_operation_detailed_probs[].items", "aspell.Speller", "aspects.utils._apply_smoothing_on_simple_dict", "aspects.utils._apply_smoothing_on_simple_dict", "aspects.utils._apply_smoothing", "list", "list", "list", "string.ascii_lowercase.upper", "czech_chars_with_diacritics.upper", "list", "list", "russian_special.upper", "list"], "methods", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation.__init__", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing_on_simple_dict", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing_on_simple_dict", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing_on_simple_dict", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing"], ["    ", "def", "__init__", "(", "self", ",", "profile", ",", "lang", ",", "alpha", "=", "1", ",", "beta", "=", "0", ")", ":", "\n", "        ", "super", "(", "Spelling", ",", "self", ")", ".", "__init__", "(", "profile", ",", "alpha", ",", "beta", ")", "\n", "\n", "spelling_word_to_invalid_word", "=", "profile", "[", "'spelling'", "]", "[", "'spelling_word_to_invalid_word'", "]", "\n", "spelling_word_to_other_valid_word", "=", "profile", "[", "'spelling'", "]", "[", "'spelling_word_to_other_valid_word'", "]", "\n", "self", ".", "spelling_word_to_invalid_word", ",", "self", ".", "spelling_word_to_other_valid_word", "=", "utils", ".", "_apply_smoothing", "(", "\n", "[", "spelling_word_to_invalid_word", ",", "spelling_word_to_other_valid_word", "]", ",", "alpha", ",", "beta", ")", "\n", "\n", "spelling_noise_operation_probs", "=", "profile", "[", "'spelling'", "]", "[", "'spelling_noise_operation_probs'", "]", "\n", "self", ".", "spelling_noise_operation_probs", "=", "utils", ".", "_apply_smoothing_on_simple_dict", "(", "spelling_noise_operation_probs", ",", "alpha", ",", "\n", "beta", ")", "\n", "\n", "self", ".", "spelling_noise_operation_detailed_probs", "=", "profile", "[", "'spelling'", "]", "[", "'spelling_noise_operation_detailed_probs'", "]", "\n", "self", ".", "spelling_noise_operation_detailed_probs", "=", "{", "'D'", ":", "{", "}", ",", "'I'", ":", "{", "}", ",", "'S'", ":", "{", "}", "}", "\n", "\n", "for", "char", ",", "char_delete_prob", "in", "self", ".", "spelling_noise_operation_detailed_probs", "[", "'D'", "]", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "spelling_noise_operation_detailed_probs", "[", "'D'", "]", "[", "char", "]", "=", "utils", ".", "_apply_smoothing", "(", "[", "char_delete_prob", "]", ",", "alpha", ",", "beta", ")", "[", "0", "]", "\n", "\n", "", "for", "from_char", ",", "v", "in", "self", ".", "spelling_noise_operation_detailed_probs", "[", "'S'", "]", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "spelling_noise_operation_detailed_probs", "[", "'S'", "]", "[", "from_char", "]", "=", "utils", ".", "_apply_smoothing_on_simple_dict", "(", "v", ",", "alpha", ",", "\n", "beta", ")", "\n", "\n", "", "for", "context", ",", "v", "in", "self", ".", "spelling_noise_operation_detailed_probs", "[", "'I'", "]", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "spelling_noise_operation_detailed_probs", "[", "'I'", "]", "[", "context", "]", "=", "utils", ".", "_apply_smoothing_on_simple_dict", "(", "v", ",", "alpha", ",", "\n", "beta", ")", "\n", "\n", "", "self", ".", "aspell_speller", "=", "aspell", ".", "Speller", "(", "'lang'", ",", "lang", ")", "\n", "self", ".", "spelling_detailed_ratio", "=", "0.3", "\n", "\n", "if", "lang", "==", "'cs'", ":", "\n", "            ", "czech_chars_with_diacritics", "=", "'\u00e1\u010d\u010f\u011b\u00e9\u00ed\u0148\u00f3\u0161\u0159\u0165\u016f\u00fa\u00fd\u017e'", "\n", "self", ".", "all_chars_in_language", "=", "list", "(", "string", ".", "ascii_lowercase", "+", "czech_chars_with_diacritics", ")", "+", "list", "(", "\n", "string", ".", "ascii_lowercase", ".", "upper", "(", ")", "+", "czech_chars_with_diacritics", ".", "upper", "(", ")", ")", "\n", "", "elif", "lang", "==", "'en'", ":", "\n", "            ", "self", ".", "all_chars_in_language", "=", "list", "(", "string", ".", "ascii_lowercase", "+", "string", ".", "ascii_uppercase", ")", "\n", "", "elif", "lang", "==", "'de'", ":", "\n", "            ", "self", ".", "all_chars_in_language", "=", "list", "(", "string", ".", "ascii_lowercase", "+", "'\u00e4\u00f6\u00fc\u00df'", ")", "+", "list", "(", "string", ".", "ascii_uppercase", "+", "'\u00c4\u00d6\u00dc\u1e9e'", ")", "\n", "", "elif", "lang", "==", "'ru'", ":", "\n", "            ", "russian_special", "=", "'\u0431\u0432\u0433\u0434\u0436\u0437\u043a\u043b\u043c\u043d\u043f\u0440\u0441\u0442\u0444\u0445\u0446\u0447\u0448\u0449\u0430\u044d\u044b\u0443\u043e\u044f\u0435\u0451\u044e\u0438\u0439'", "\n", "russian_special", "+=", "russian_special", ".", "upper", "(", ")", "\n", "russian_special", "+=", "'\u042c\u044c\u042a\u044a'", "\n", "self", ".", "all_chars_in_language", "=", "list", "(", "russian_special", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "all_chars_in_language", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.spelling.Spelling.apply": [[58, 209], ["set", "text.split", "set.update", "word.isalpha", "new_text.append", "numpy.random.uniform", "c.isalpha", "spelling.Spelling.aspell_speller.suggest", "top_aspell_suggestions.remove", "any", "numpy.random.choice", "new_text.append", "changes.append", "new_text.append", "numpy.random.uniform", "list", "new_text.append", "len", "numpy.random.choice.isalpha", "numpy.random.choice", "len", "new_text.append", "changes.append", "new_text.append", "changes.append", "x.isalpha", "len", "numpy.random.uniform", "list", "range", "list", "range", "len", "len", "max", "numpy.random.choice", "numpy.sum", "numpy.array", "numpy.random.choice", "set.difference", "list", "numpy.random.uniform", "numpy.sum", "list", "numpy.sum", "list", "numpy.random.choice", "[].values", "list", "[].values", "[].keys", "numpy.random.uniform", "list", "[].values", "numpy.random.uniform", "numpy.array", "numpy.random.choice", "set.difference", "len", "numpy.random.uniform", "len", "len", "numpy.random.uniform", "numpy.sum", "list", "numpy.sum", "list", "numpy.random.choice", "numpy.random.choice", "list", "[].values", "[].keys", "list", "list", "[].values", "set.difference", "set.difference"], "methods", ["None"], ["", "", "def", "apply", "(", "self", ",", "text", ",", "whitespace_info", ")", ":", "\n", "\n", "        ", "changes", "=", "[", "]", "\n", "new_text", "=", "[", "]", "\n", "all_alpha_chars_in_text_and_language", "=", "set", "(", "[", "c", "for", "c", "in", "text", "if", "c", ".", "isalpha", "(", ")", "]", ")", "\n", "if", "self", ".", "all_chars_in_language", ":", "\n", "            ", "all_alpha_chars_in_text_and_language", ".", "update", "(", "self", ".", "all_chars_in_language", ")", "\n", "\n", "", "for", "word", "in", "text", ".", "split", "(", ")", ":", "\n", "            ", "if", "not", "word", ".", "isalpha", "(", ")", ":", "\n", "                ", "new_text", ".", "append", "(", "word", ")", "\n", "continue", "\n", "\n", "", "if", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "self", ".", "spelling_word_to_other_valid_word", ":", "\n", "                ", "top_aspell_suggestions", "=", "self", ".", "aspell_speller", ".", "suggest", "(", "word", ")", "[", ":", "10", "]", "\n", "\n", "if", "word", "in", "top_aspell_suggestions", ":", "\n", "                    ", "top_aspell_suggestions", ".", "remove", "(", "word", ")", "\n", "\n", "# for some Words, Aspell does not provide any alternative and it also sometimes provides \"multi-token alternatives\" (e.g \"za\u017e\u00edvac\u00edho\" -> \"za\u017e\u00edvac\u00ed ho\")", "\n", "", "if", "len", "(", "top_aspell_suggestions", ")", ">", "0", "and", "any", "(", "[", "x", ".", "isalpha", "(", ")", "for", "x", "in", "top_aspell_suggestions", "]", ")", ":", "\n", "                    ", "chosen_suggestion", "=", "np", ".", "random", ".", "choice", "(", "top_aspell_suggestions", ")", "\n", "while", "not", "chosen_suggestion", ".", "isalpha", "(", ")", ":", "\n", "                        ", "chosen_suggestion", "=", "np", ".", "random", ".", "choice", "(", "top_aspell_suggestions", ")", "\n", "\n", "", "new_text", ".", "append", "(", "chosen_suggestion", ")", "\n", "changes", ".", "append", "(", "[", "'SPELL'", ",", "'Aspell replace {} with {}'", ".", "format", "(", "word", ",", "chosen_suggestion", ")", "]", ")", "\n", "", "else", ":", "\n", "                    ", "new_text", ".", "append", "(", "word", ")", "\n", "", "", "elif", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "self", ".", "spelling_word_to_invalid_word", ":", "\n", "                ", "new_word", "=", "list", "(", "word", ")", "\n", "\n", "detailed_spelling_applicable", "=", "False", "\n", "if", "len", "(", "word", ")", ">=", "2", ":", "\n", "                    ", "detailed_spelling_applicable", "=", "True", "\n", "", "elif", "len", "(", "word", ")", "==", "1", ":", "\n", "# we need to be sure that the substitute/delete probability is high enough (so that we do not cycle here too long)", "\n", "                    ", "if", "new_word", "[", "0", "]", "in", "self", ".", "spelling_noise_operation_detailed_probs", "[", "'S'", "]", "and", "np", ".", "sum", "(", "\n", "list", "(", "self", ".", "spelling_noise_operation_detailed_probs", "[", "'S'", "]", "[", "new_word", "[", "0", "]", "]", ".", "values", "(", ")", ")", ")", ">", "0.1", ":", "\n", "                        ", "detailed_spelling_applicable", "=", "True", "\n", "\n", "", "if", "new_word", "[", "0", "]", "in", "self", ".", "spelling_noise_operation_detailed_probs", "[", "'D'", "]", "and", "self", ".", "spelling_noise_operation_detailed_probs", "[", "'D'", "]", "[", "\n", "new_word", "[", "0", "]", "]", ">", "0.1", ":", "\n", "                        ", "detailed_spelling_applicable", "=", "True", "\n", "\n", "", "", "if", "detailed_spelling_applicable", "and", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "1", "-", "self", ".", "spelling_detailed_ratio", ":", "\n", "                    ", "num_iterations_spent", "=", "0", "\n", "# we must ensure that once we select the word to noisy, it will be actually noised and not an empty world", "\n", "while", "''", ".", "join", "(", "new_word", ")", "==", "word", "or", "not", "''", ".", "join", "(", "new_word", ")", ".", "strip", "(", ")", ":", "\n", "                        ", "new_word", "=", "list", "(", "word", ")", "\n", "num_iterations_spent", "+=", "1", "\n", "if", "num_iterations_spent", ">", "1e4", ":", "\n", "# it should not happen very often, better check whether we do not cycle here too long", "\n", "                            ", "break", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "new_word", ")", ")", ":", "\n", "# try substitute", "\n", "                            ", "if", "new_word", "[", "i", "]", "in", "self", ".", "spelling_noise_operation_detailed_probs", "[", "'S'", "]", "and", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "np", ".", "sum", "(", "\n", "list", "(", "self", ".", "spelling_noise_operation_detailed_probs", "[", "'S'", "]", "[", "new_word", "[", "i", "]", "]", ".", "values", "(", ")", ")", ")", ":", "\n", "\n", "                                ", "substitute_probabilites", "=", "np", ".", "array", "(", "\n", "list", "(", "self", ".", "spelling_noise_operation_detailed_probs", "[", "'S'", "]", "[", "new_word", "[", "i", "]", "]", ".", "values", "(", ")", ")", ")", "\n", "substitute_probabilites_normalized", "=", "substitute_probabilites", "/", "np", ".", "sum", "(", "substitute_probabilites", ")", "\n", "\n", "new_word", "[", "i", "]", "=", "np", ".", "random", ".", "choice", "(", "list", "(", "self", ".", "spelling_noise_operation_detailed_probs", "[", "'S'", "]", "[", "new_word", "[", "i", "]", "]", ".", "keys", "(", ")", ")", ",", "\n", "p", "=", "substitute_probabilites_normalized", ")", "\n", "continue", "\n", "# try delete", "\n", "", "elif", "new_word", "[", "i", "]", "in", "self", ".", "spelling_noise_operation_detailed_probs", "[", "'D'", "]", "and", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "self", ".", "spelling_noise_operation_detailed_probs", "[", "'D'", "]", "[", "new_word", "[", "i", "]", "]", ":", "\n", "\n", "                                ", "new_word", "[", "i", "]", "=", "''", "\n", "continue", "\n", "# try transpose", "\n", "", "elif", "i", "<", "len", "(", "word", ")", "-", "1", "and", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "self", ".", "spelling_noise_operation_probs", "[", "'T'", "]", ":", "\n", "                                ", "temp", "=", "new_word", "[", "i", "]", "\n", "new_word", "[", "i", "]", "=", "new_word", "[", "i", "+", "1", "]", "\n", "new_word", "[", "i", "+", "1", "]", "=", "temp", "\n", "continue", "\n", "# try insert", "\n", "", "else", ":", "\n", "                                ", "left_context", "=", "'^'", "if", "i", "==", "0", "else", "word", "[", "i", "-", "1", "]", "\n", "right_context", "=", "\"$\"", "if", "i", ">=", "len", "(", "word", ")", "else", "word", "[", "i", "]", "\n", "context", "=", "left_context", "+", "right_context", "\n", "\n", "if", "context", "in", "self", ".", "spelling_noise_operation_detailed_probs", "[", "'I'", "]", "and", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "np", ".", "sum", "(", "\n", "list", "(", "self", ".", "spelling_noise_operation_detailed_probs", "[", "'I'", "]", "[", "context", "]", ".", "values", "(", ")", ")", ")", ":", "\n", "                                    ", "insert_probabilites", "=", "np", ".", "array", "(", "\n", "list", "(", "self", ".", "spelling_noise_operation_detailed_probs", "[", "'I'", "]", "[", "context", "]", ".", "values", "(", ")", ")", ")", "\n", "insert_probabilites_normalized", "=", "insert_probabilites", "/", "np", ".", "sum", "(", "insert_probabilites", ")", "\n", "\n", "insert_char", "=", "np", ".", "random", ".", "choice", "(", "list", "(", "self", ".", "spelling_noise_operation_detailed_probs", "[", "'I'", "]", "[", "context", "]", ".", "keys", "(", ")", ")", ",", "\n", "p", "=", "insert_probabilites_normalized", ")", "\n", "new_word", "[", "i", "]", "=", "insert_char", "+", "new_word", "[", "i", "]", "\n", "", "continue", "\n", "\n", "", "", "", "new_text", ".", "append", "(", "''", ".", "join", "(", "new_word", ")", ")", "\n", "changes", ".", "append", "(", "[", "'SPELL detailed'", ",", "'Char replace {} with {}'", ".", "format", "(", "word", ",", "new_text", "[", "-", "1", "]", ")", "]", ")", "\n", "", "else", ":", "\n", "                    ", "num_iterations_spent", "=", "0", "\n", "# we must ensure that once we select the word to noisy, it will be actually noised and not an empty world", "\n", "while", "''", ".", "join", "(", "new_word", ")", "==", "word", "or", "not", "''", ".", "join", "(", "new_word", ")", ".", "strip", "(", ")", ":", "\n", "                        ", "new_word", "=", "list", "(", "word", ")", "\n", "num_iterations_spent", "+=", "1", "\n", "if", "num_iterations_spent", ">", "1e4", ":", "\n", "# it should not happen very often, better check whether we do not cycle here too long", "\n", "                            ", "break", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "new_word", ")", ")", ":", "\n", "\n", "                            ", "no_op_prob", "=", "max", "(", "0", ",", "1", "-", "self", ".", "spelling_noise_operation_probs", "[", "'S'", "]", "-", "self", ".", "spelling_noise_operation_probs", "[", "'T'", "]", "-", "self", ".", "spelling_noise_operation_probs", "[", "'I'", "]", "-", "self", ".", "spelling_noise_operation_probs", "[", "'D'", "]", ")", "\n", "op_type", "=", "np", ".", "random", ".", "choice", "(", "[", "'0'", ",", "'S'", ",", "'T'", ",", "'T'", ",", "'D'", "]", ",", "\n", "p", "=", "[", "no_op_prob", ",", "self", ".", "spelling_noise_operation_probs", "[", "'S'", "]", ",", "\n", "self", ".", "spelling_noise_operation_probs", "[", "'T'", "]", ",", "\n", "self", ".", "spelling_noise_operation_probs", "[", "'I'", "]", ",", "\n", "self", ".", "spelling_noise_operation_probs", "[", "'D'", "]", "]", ")", "\n", "# substitute", "\n", "if", "op_type", "==", "'S'", ":", "\n", "                                ", "if", "all_alpha_chars_in_text_and_language", ".", "difference", "(", "word", "[", "i", "]", ")", ":", "\n", "                                    ", "new_word", "[", "i", "]", "=", "np", ".", "random", ".", "choice", "(", "list", "(", "all_alpha_chars_in_text_and_language", ".", "difference", "(", "word", "[", "i", "]", ")", ")", ")", "\n", "", "continue", "\n", "# transpose", "\n", "", "elif", "op_type", "==", "'T'", "and", "i", "<", "len", "(", "word", ")", "-", "1", ":", "\n", "                                ", "temp", "=", "new_word", "[", "i", "]", "\n", "new_word", "[", "i", "]", "=", "new_word", "[", "i", "+", "1", "]", "\n", "new_word", "[", "i", "+", "1", "]", "=", "temp", "\n", "continue", "\n", "# insert", "\n", "", "elif", "op_type", "==", "'I'", ":", "\n", "                                ", "if", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "0.5", ":", "# insert to the left of the current char", "\n", "                                    ", "new_word", "[", "i", "]", "=", "np", ".", "random", ".", "choice", "(", "list", "(", "all_alpha_chars_in_text_and_language", ".", "difference", "(", "word", "[", "i", "]", ")", ")", ")", "+", "new_word", "[", "i", "]", "\n", "", "else", ":", "\n", "                                    ", "new_word", "[", "i", "]", "=", "new_word", "[", "i", "]", "+", "np", ".", "random", ".", "choice", "(", "\n", "list", "(", "all_alpha_chars_in_text_and_language", ".", "difference", "(", "word", "[", "i", "]", ")", ")", ")", "\n", "", "continue", "\n", "# delete", "\n", "", "elif", "op_type", "==", "'D'", ":", "\n", "                                ", "new_word", "[", "i", "]", "=", "''", "\n", "continue", "\n", "", "", "", "new_text", ".", "append", "(", "''", ".", "join", "(", "new_word", ")", ")", "\n", "changes", ".", "append", "(", "[", "'SPELL generalized'", ",", "'Char replace {} with {}'", ".", "format", "(", "word", ",", "new_text", "[", "-", "1", "]", ")", "]", ")", "\n", "\n", "", "", "else", ":", "\n", "                ", "new_text", ".", "append", "(", "word", ")", "\n", "\n", "", "", "return", "\" \"", ".", "join", "(", "new_text", ")", ",", "changes", ",", "whitespace_info", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.spelling.Spelling.estimate_probabilities": [[210, 389], ["collections.Counter", "collections.Counter", "spelling_noise_operation_distrib.items", "spelling_noise_operation_detailed[].items", "spelling_noise_operation_detailed[].items", "spelling_noise_operation_detailed[].items", "numpy.mean", "[].items", "[].items", "aspects.apply_m2_edits.processM2", "sum", "list", "coder_dict.keys", "cached_coder_dicts.append", "aspects.utils.get_cheapest_align_seq", "len", "sum", "sum", "sum", "sum", "spelling_noise_operation_distrib[].append", "spelling_noise_operation_distrib[].append", "spelling_noise_operation_distrib[].append", "spelling_noise_operation_distrib[].append", "collections.Counter.update", "collections.Counter.update", "collections.Counter.update", "x.isalpha", "all", "len", "difflib.SequenceMatcher().ratio", "range", "difflib.SequenceMatcher", "len"], "methods", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.apply_m2_edits.processM2", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils.get_cheapest_align_seq"], ["", "@", "staticmethod", "\n", "def", "estimate_probabilities", "(", "m2_records", ")", ":", "\n", "        ", "num_files", "=", "0", "\n", "num_spelling_incorrect_word", "=", "0", "# number of words that contain spelling error and the misspelled word is not a valid word", "\n", "num_spelling_valid_word", "=", "0", "# number of words that contain spelling error but the misspelled word is a valid word", "\n", "\n", "num_all_alpha_words", "=", "0", "\n", "\n", "spelling_noise_operation_distrib", "=", "{", "\n", "'S'", ":", "[", "]", ",", "\n", "'I'", ":", "[", "]", ",", "\n", "'D'", ":", "[", "]", ",", "\n", "'T'", ":", "[", "]", "\n", "}", "\n", "\n", "spelling_noise_operation_detailed", "=", "{", "\n", "'S'", ":", "{", "}", ",", "\n", "'I'", ":", "{", "}", ",", "\n", "'D'", ":", "{", "}", "\n", "}", "\n", "\n", "character_counter_in_corrected_spelling", "=", "Counter", "(", ")", "\n", "twocharacter_counter_in_corrected_spelling", "=", "Counter", "(", ")", "\n", "\n", "cached_coder_dicts", "=", "[", "]", "\n", "for", "m2_file", "in", "m2_records", ":", "\n", "            ", "num_files", "+=", "1", "\n", "\n", "for", "info", "in", "m2_file", ":", "\n", "                ", "orig_sent", ",", "coder_dict", "=", "apply_m2_edits", ".", "processM2", "(", "info", ",", "[", "]", ")", "\n", "if", "coder_dict", ":", "\n", "                    ", "coder_id", "=", "list", "(", "coder_dict", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "cor_sent", "=", "coder_dict", "[", "coder_id", "]", "[", "0", "]", "\n", "\n", "for", "edit", "in", "coder_dict", "[", "coder_id", "]", "[", "1", "]", ":", "\n", "                        ", "orig_start", ",", "orig_end", ",", "error_type", ",", "cor_tok", ",", "cor_start", ",", "cor_end", "=", "edit", "\n", "\n", "exclude_error_types", "=", "[", "'noop'", ",", "'DIACR'", ",", "'PUNCT'", ",", "'ORTH'", "]", "\n", "# print(orig_sent, orig_start)", "\n", "# print(cor_sent, cor_start)", "\n", "'''\n                        There are two types of spelling error:\n                            1. invalid_word -> valid_word (e.g. thiis -> this) (Errant: SPELL)\n                            2. valid_word -> valid_word (e.g. than -> then)\n\n                        For the first type, we also estimate the noise in terms of number of bad insertions, deletions and transpositions.\n\n                        '''", "\n", "if", "\"SPELL\"", "in", "error_type", "and", "\"WO:SPELL\"", "not", "in", "error_type", ":", "\n", "                            ", "num_spelling_incorrect_word", "+=", "1", "\n", "\n", "cached_coder_dicts", ".", "append", "(", "coder_dict", ")", "\n", "\n", "# ! NOTE that it is important to calculate it from corrected to original (as we are gonna use it in this way)", "\n", "align_seq", "=", "get_cheapest_align_seq", "(", "\" \"", ".", "join", "(", "cor_sent", "[", "cor_start", ":", "cor_end", "]", ")", ",", "\n", "\" \"", ".", "join", "(", "orig_sent", "[", "orig_start", ":", "orig_end", "]", ")", ")", "\n", "\n", "# first fill in spelling_noise_operation (which only stores probabilities of substituting individual char, inserting ...)", "\n", "cor_len", "=", "len", "(", "\" \"", ".", "join", "(", "cor_sent", "[", "cor_start", ":", "cor_end", "]", ")", ")", "\n", "\n", "num_substitute", "=", "sum", "(", "[", "1", "for", "x", "in", "align_seq", "if", "x", "[", "0", "]", "==", "'S'", "]", ")", "\n", "num_insert", "=", "sum", "(", "[", "1", "for", "x", "in", "align_seq", "if", "x", "[", "0", "]", "==", "'I'", "]", ")", "\n", "num_delete", "=", "sum", "(", "[", "1", "for", "x", "in", "align_seq", "if", "x", "[", "0", "]", "==", "'D'", "]", ")", "\n", "num_transpose", "=", "sum", "(", "[", "1", "for", "x", "in", "align_seq", "if", "x", "[", "0", "]", "==", "'T'", "]", ")", "\n", "\n", "spelling_noise_operation_distrib", "[", "'S'", "]", ".", "append", "(", "num_substitute", "/", "cor_len", ")", "\n", "spelling_noise_operation_distrib", "[", "'I'", "]", ".", "append", "(", "num_insert", "/", "cor_len", ")", "\n", "spelling_noise_operation_distrib", "[", "'D'", "]", ".", "append", "(", "num_delete", "/", "cor_len", ")", "\n", "spelling_noise_operation_distrib", "[", "'T'", "]", ".", "append", "(", "num_transpose", "/", "cor_len", ")", "\n", "\n", "# then fill in detailed occurence counts for substitute, insert and delete", "\n", "original_word", "=", "\" \"", ".", "join", "(", "orig_sent", "[", "orig_start", ":", "orig_end", "]", ")", "\n", "corrected_word", "=", "\" \"", ".", "join", "(", "cor_sent", "[", "cor_start", ":", "cor_end", "]", ")", "\n", "character_counter_in_corrected_spelling", ".", "update", "(", "corrected_word", ")", "\n", "twocharacter_counter_in_corrected_spelling", ".", "update", "(", "\n", "[", "corrected_word", "[", "i", "]", "+", "corrected_word", "[", "i", "+", "1", "]", "for", "i", "in", "range", "(", "len", "(", "corrected_word", ")", "-", "1", ")", "]", ")", "\n", "twocharacter_counter_in_corrected_spelling", ".", "update", "(", "[", "'^'", "+", "corrected_word", "[", "0", "]", ",", "corrected_word", "[", "-", "1", "]", "+", "'$'", "]", ")", "\n", "\n", "for", "operation", "in", "align_seq", ":", "\n", "                                ", "op_type", ",", "cor_start", ",", "cor_end", ",", "orig_start", ",", "orig_end", "=", "operation", "\n", "\n", "if", "op_type", "==", "'S'", ":", "\n", "                                    ", "cor_char", "=", "corrected_word", "[", "cor_start", "]", "\n", "orig_char", "=", "original_word", "[", "orig_start", "]", "\n", "\n", "if", "cor_char", "not", "in", "spelling_noise_operation_detailed", "[", "'S'", "]", ":", "\n", "                                        ", "spelling_noise_operation_detailed", "[", "'S'", "]", "[", "cor_char", "]", "=", "{", "}", "\n", "\n", "", "if", "orig_char", "not", "in", "spelling_noise_operation_detailed", "[", "'S'", "]", "[", "cor_char", "]", ":", "\n", "                                        ", "spelling_noise_operation_detailed", "[", "'S'", "]", "[", "cor_char", "]", "[", "orig_char", "]", "=", "0", "\n", "\n", "", "spelling_noise_operation_detailed", "[", "'S'", "]", "[", "cor_char", "]", "[", "orig_char", "]", "+=", "1", "\n", "", "elif", "op_type", "==", "'I'", ":", "\n", "                                    ", "left_context", "=", "\"^\"", "if", "cor_start", "==", "0", "else", "corrected_word", "[", "cor_start", "-", "1", "]", "\n", "right_context", "=", "\"$\"", "if", "cor_start", ">=", "cor_len", "else", "corrected_word", "[", "cor_start", "]", "\n", "\n", "insert_into_tuple", "=", "left_context", "+", "right_context", "\n", "if", "insert_into_tuple", "not", "in", "spelling_noise_operation_detailed", "[", "'I'", "]", ":", "\n", "                                        ", "spelling_noise_operation_detailed", "[", "'I'", "]", "[", "insert_into_tuple", "]", "=", "{", "}", "\n", "\n", "", "insert_char", "=", "original_word", "[", "orig_start", "]", "\n", "\n", "if", "insert_char", "not", "in", "spelling_noise_operation_detailed", "[", "'I'", "]", "[", "insert_into_tuple", "]", ":", "\n", "                                        ", "spelling_noise_operation_detailed", "[", "'I'", "]", "[", "insert_into_tuple", "]", "[", "insert_char", "]", "=", "0", "\n", "\n", "", "spelling_noise_operation_detailed", "[", "'I'", "]", "[", "insert_into_tuple", "]", "[", "insert_char", "]", "+=", "1", "\n", "", "elif", "op_type", "==", "'D'", ":", "\n", "                                    ", "delete_char", "=", "corrected_word", "[", "cor_start", "]", "\n", "\n", "if", "delete_char", "not", "in", "spelling_noise_operation_detailed", "[", "'D'", "]", ":", "\n", "                                        ", "spelling_noise_operation_detailed", "[", "'D'", "]", "[", "delete_char", "]", "=", "0", "\n", "\n", "", "spelling_noise_operation_detailed", "[", "'D'", "]", "[", "delete_char", "]", "+=", "1", "\n", "\n", "# # non alpha edits", "\n", "# if not \" \".join(cor_sent[cor_start:cor_end]).isalpha():", "\n", "#     print('not alpha cor', edit)", "\n", "#", "\n", "# if not \" \".join(orig_sent[orig_start:orig_end]).isalpha():", "\n", "#     print('not alpha orig', edit)", "\n", "\n", "", "", "", "elif", "all", "(", "[", "x", "not", "in", "error_type", "for", "x", "in", "exclude_error_types", "]", ")", "and", "orig_start", "==", "orig_end", "-", "1", "and", "orig_start", "<", "len", "(", "orig_sent", ")", "and", "cor_start", "==", "cor_end", "-", "1", "and", "SequenceMatcher", "(", "None", ",", "orig_sent", "[", "orig_start", "]", ",", "cor_sent", "[", "cor_start", "]", ")", ".", "ratio", "(", ")", ">", "0.5", ":", "\n", "                            ", "num_spelling_valid_word", "+=", "1", "\n", "# print(orig_sent[orig_start], cor_sent[cor_start])", "\n", "\n", "# # non alpha edits", "\n", "# if not \" \".join(cor_sent[cor_start:cor_end]).isalpha():", "\n", "#     print('not alpha cor', edit)", "\n", "#", "\n", "# if not \" \".join(orig_sent[orig_start:orig_end]).isalpha():", "\n", "#     print('not alpha orig', edit)", "\n", "\n", "# TODO WO:SPELL", "\n", "\n", "", "", "", "num_all_alpha_words", "+=", "sum", "(", "[", "1", "for", "x", "in", "orig_sent", "if", "x", ".", "isalpha", "(", ")", "]", ")", "\n", "\n", "# normalization to probabilities for word_to_other_valid_word and word_to_invalid_word", "\n", "", "", "spelling_word_to_other_valid_word", "=", "num_spelling_valid_word", "/", "num_all_alpha_words", "\n", "spelling_word_to_invalid_word", "=", "num_spelling_incorrect_word", "/", "num_all_alpha_words", "\n", "\n", "# normalization to probabilities for aggregated spelling noise operations", "\n", "spelling_noise_operation_probs", "=", "{", "}", "\n", "\n", "for", "k", ",", "v", "in", "spelling_noise_operation_distrib", ".", "items", "(", ")", ":", "\n", "            ", "spelling_noise_operation_probs", "[", "k", "]", "=", "np", ".", "mean", "(", "v", ")", "\n", "\n", "# normalization to probabilities for detailed spelling noise operations", "\n", "", "spelling_noise_operation_detailed_probs", "=", "{", "\n", "'S'", ":", "{", "}", ",", "\n", "'I'", ":", "{", "}", ",", "\n", "'D'", ":", "{", "}", "\n", "}", "\n", "## substitutes", "\n", "for", "cor_from", ",", "v", "in", "spelling_noise_operation_detailed", "[", "'S'", "]", ".", "items", "(", ")", ":", "\n", "            ", "spelling_noise_operation_detailed_probs", "[", "'S'", "]", "[", "cor_from", "]", "=", "{", "}", "\n", "for", "cor_to", ",", "val", "in", "spelling_noise_operation_detailed", "[", "'S'", "]", "[", "cor_from", "]", ".", "items", "(", ")", ":", "\n", "                ", "spelling_noise_operation_detailed_probs", "[", "'S'", "]", "[", "cor_from", "]", "[", "cor_to", "]", "=", "spelling_noise_operation_detailed", "[", "'S'", "]", "[", "cor_from", "]", "[", "\n", "cor_to", "]", "/", "character_counter_in_corrected_spelling", "[", "cor_from", "]", "\n", "## inserts", "\n", "", "", "for", "two_char_context", ",", "v", "in", "spelling_noise_operation_detailed", "[", "'I'", "]", ".", "items", "(", ")", ":", "\n", "            ", "spelling_noise_operation_detailed_probs", "[", "'I'", "]", "[", "two_char_context", "]", "=", "{", "}", "\n", "for", "insert_char", ",", "val", "in", "spelling_noise_operation_detailed", "[", "'I'", "]", "[", "two_char_context", "]", ".", "items", "(", ")", ":", "\n", "                ", "spelling_noise_operation_detailed_probs", "[", "'I'", "]", "[", "two_char_context", "]", "[", "insert_char", "]", "=", "spelling_noise_operation_detailed", "[", "'I'", "]", "[", "two_char_context", "]", "[", "insert_char", "]", "/", "twocharacter_counter_in_corrected_spelling", "[", "two_char_context", "]", "\n", "\n", "## deletes", "\n", "", "", "for", "delete_char", ",", "v", "in", "spelling_noise_operation_detailed", "[", "'D'", "]", ".", "items", "(", ")", ":", "\n", "            ", "spelling_noise_operation_detailed_probs", "[", "'D'", "]", "[", "delete_char", "]", "=", "spelling_noise_operation_detailed", "[", "'D'", "]", "[", "delete_char", "]", "/", "character_counter_in_corrected_spelling", "[", "delete_char", "]", "\n", "\n", "", "return", "'spelling'", ",", "{", "\n", "'spelling_word_to_other_valid_word'", ":", "spelling_word_to_other_valid_word", ",", "\n", "'spelling_word_to_invalid_word'", ":", "spelling_word_to_invalid_word", ",", "\n", "'spelling_noise_operation_probs'", ":", "spelling_noise_operation_probs", ",", "\n", "'spelling_noise_operation_detailed_probs'", ":", "spelling_noise_operation_detailed_probs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.word_order.WordOrder.__init__": [[10, 18], ["aspects.base.Aspect.__init__", "aspects.utils._apply_smoothing_on_simple_dict", "aspects.utils._apply_smoothing"], "methods", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation.__init__", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing_on_simple_dict", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing"], ["    ", "def", "__init__", "(", "self", ",", "profile", ",", "lang", ",", "alpha", "=", "1", ",", "beta", "=", "0", ")", ":", "\n", "        ", "super", "(", "WordOrder", ",", "self", ")", ".", "__init__", "(", "profile", ",", "alpha", ",", "beta", ")", "\n", "\n", "tuples_with_wo_percentage", "=", "profile", "[", "'word_order'", "]", "[", "'tuples_with_wo_percentage'", "]", "\n", "self", ".", "tuples_with_wo_percentage", "=", "utils", ".", "_apply_smoothing", "(", "[", "tuples_with_wo_percentage", "]", ",", "alpha", ",", "beta", ")", "[", "0", "]", "\n", "\n", "num_words_per_wo_change_distrib", "=", "profile", "[", "'word_order'", "]", "[", "'num_words_per_wo_change_distrib'", "]", "\n", "self", ".", "num_words_per_wo_change_distrib", "=", "utils", ".", "_apply_smoothing_on_simple_dict", "(", "num_words_per_wo_change_distrib", ",", "alpha", ",", "beta", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.word_order.WordOrder.apply": [[19, 65], ["text.split", "enumerate", "len", "numpy.random.uniform", "int", "enumerate", "range", "enumerate", "changes.append", "len", "numpy.array", "numpy.sum", "numpy.random.choice", "numpy.random.permutation", "word_order.WordOrder.num_words_per_wo_change_distrib.items", "list", "list", "list", "numpy.array_equal", "int", "this_wo_possible_tuples_probs.values", "this_wo_possible_tuples_probs.values", "this_wo_possible_tuples_probs.keys", "numpy.array", "len", "len", "range", "text.split"], "methods", ["None"], ["", "def", "apply", "(", "self", ",", "text", ",", "whitespace_info", ")", ":", "\n", "        ", "changes", "=", "[", "]", "\n", "text_words", "=", "text", ".", "split", "(", "' '", ")", "\n", "if", "len", "(", "text_words", ")", "<", "2", ":", "\n", "            ", "return", "text", ",", "changes", ",", "whitespace_info", "\n", "\n", "", "for", "start_word_i", ",", "start_word", "in", "enumerate", "(", "text_words", ")", ":", "\n", "            ", "remaining_words", "=", "len", "(", "text_words", ")", "-", "1", "-", "start_word_i", "\n", "\n", "if", "remaining_words", "<", "2", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "self", ".", "tuples_with_wo_percentage", ":", "\n", "                ", "this_wo_possible_tuples_probs", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "num_words_per_wo_change_distrib", ".", "items", "(", ")", "if", "int", "(", "k", ")", "<=", "remaining_words", "}", "\n", "normalized_probabilities", "=", "np", ".", "array", "(", "list", "(", "this_wo_possible_tuples_probs", ".", "values", "(", ")", ")", ")", "/", "np", ".", "sum", "(", "\n", "list", "(", "this_wo_possible_tuples_probs", ".", "values", "(", ")", ")", ")", "\n", "num_words_in_word_order_error", "=", "int", "(", "np", ".", "random", ".", "choice", "(", "list", "(", "this_wo_possible_tuples_probs", ".", "keys", "(", ")", ")", ",", "\n", "p", "=", "normalized_probabilities", ")", ")", "\n", "\n", "while", "True", ":", "\n", "# we do not want the permutation to \"do nothing\"", "\n", "                    ", "perm", "=", "np", ".", "random", ".", "permutation", "(", "num_words_in_word_order_error", ")", "\n", "if", "not", "np", ".", "array_equal", "(", "perm", ",", "np", ".", "array", "(", "range", "(", "num_words_in_word_order_error", ")", ")", ")", ":", "\n", "                        ", "break", "\n", "\n", "", "", "new_words", "=", "[", "''", "]", "*", "num_words_in_word_order_error", "\n", "for", "original_word_index", ",", "p_index", "in", "enumerate", "(", "perm", ")", ":", "\n", "                    ", "new_words", "[", "original_word_index", "]", "=", "text_words", "[", "start_word_i", "+", "p_index", "]", "\n", "\n", "", "for", "i", "in", "range", "(", "num_words_in_word_order_error", ")", ":", "\n", "                    ", "text_words", "[", "start_word_i", "+", "i", "]", "=", "new_words", "[", "i", "]", "\n", "\n", "# \"fix\" whitespace_info (try to copy it as it was originally)", "\n", "# this is definitely suboptimal", "\n", "", "for", "i", ",", "p_index", "in", "enumerate", "(", "perm", ")", ":", "\n", "                    ", "if", "(", "start_word_i", "+", "i", ")", ">", "0", "and", "(", "start_word_i", "+", "p_index", ")", ">", "0", ":", "\n", "                        ", "whitespace_info", "[", "start_word_i", "+", "i", "-", "1", "]", "=", "whitespace_info", "[", "start_word_i", "+", "p_index", "-", "1", "]", "\n", "\n", "", "if", "(", "start_word_i", "+", "i", ")", "<", "len", "(", "whitespace_info", ")", "and", "(", "start_word_i", "+", "p_index", ")", "<", "len", "(", "whitespace_info", ")", ":", "\n", "                        ", "whitespace_info", "[", "start_word_i", "+", "i", "]", "=", "whitespace_info", "[", "start_word_i", "+", "p_index", "]", "\n", "\n", "", "", "changes", ".", "append", "(", "[", "'WO'", ",", "'replace {} with {}'", ".", "format", "(", "\n", "\" \"", ".", "join", "(", "text", ".", "split", "(", "' '", ")", "[", "start_word_i", ":", "start_word_i", "+", "num_words_in_word_order_error", "]", ")", ",", "\n", "\" \"", ".", "join", "(", "text_words", "[", "start_word_i", ":", "start_word_i", "+", "num_words_in_word_order_error", "]", ")", ")", "]", ")", "\n", "\n", "", "", "return", "' '", ".", "join", "(", "text_words", ")", ",", "changes", ",", "whitespace_info", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.word_order.WordOrder.estimate_probabilities": [[66, 105], ["numpy.mean", "collections.Counter", "collections.Counter.most_common", "aspects.apply_m2_edits.processM2", "len", "len", "wo_percentage.append", "list", "coder_dict.keys", "num_words_per_wo_errors.append"], "methods", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.apply_m2_edits.processM2"], ["", "@", "staticmethod", "\n", "def", "estimate_probabilities", "(", "m2_records", ")", ":", "\n", "        ", "num_words_per_wo_errors", "=", "[", "]", "\n", "wo_percentage", "=", "[", "]", "\n", "\n", "for", "m2_file", "in", "m2_records", ":", "\n", "\n", "            ", "for", "info", "in", "m2_file", ":", "\n", "                ", "orig_sent", ",", "coder_dict", "=", "apply_m2_edits", ".", "processM2", "(", "info", ",", "[", "]", ")", "\n", "\n", "if", "coder_dict", ":", "\n", "                    ", "coder_id", "=", "list", "(", "coder_dict", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "cor_sent", "=", "coder_dict", "[", "coder_id", "]", "[", "0", "]", "\n", "paragraph_len", "=", "len", "(", "cor_sent", ")", "\n", "\n", "if", "paragraph_len", "<", "2", ":", "\n", "                        ", "continue", "\n", "\n", "", "num_wo_in_paragraph", "=", "0", "\n", "for", "edit", "in", "coder_dict", "[", "coder_id", "]", "[", "1", "]", ":", "\n", "                        ", "orig_start", ",", "orig_end", ",", "error_type", ",", "cor_tok", ",", "cor_start", ",", "cor_end", "=", "edit", "\n", "\n", "if", "'WO'", "in", "error_type", ":", "\n", "                            ", "num_words_per_wo_errors", ".", "append", "(", "orig_end", "-", "orig_start", ")", "\n", "num_wo_in_paragraph", "+=", "1", "\n", "\n", "", "", "wo_percentage", ".", "append", "(", "num_wo_in_paragraph", "/", "(", "paragraph_len", "-", "1", ")", ")", "\n", "\n", "", "", "", "tuples_with_wo_percentage", "=", "np", ".", "mean", "(", "wo_percentage", ")", "\n", "\n", "cnt_num_words_per_wo_errors", "=", "Counter", "(", "num_words_per_wo_errors", ")", "\n", "\n", "num_words_per_wo_change_distrib", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "cnt_num_words_per_wo_errors", ".", "most_common", "(", ")", ":", "\n", "            ", "num_words_per_wo_change_distrib", "[", "k", "]", "=", "v", "/", "len", "(", "num_words_per_wo_errors", ")", "\n", "\n", "", "return", "'word_order'", ",", "{", "\n", "'tuples_with_wo_percentage'", ":", "tuples_with_wo_percentage", ",", "\n", "'num_words_per_wo_change_distrib'", ":", "num_words_per_wo_change_distrib", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.diacritics.Diacritics.__init__": [[9, 22], ["aspects.base.Aspect.__init__", "aspects.utils._apply_smoothing", "aspects.utils._apply_smoothing", "aspects.utils._apply_smoothing_on_simple_dict"], "methods", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation.__init__", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing_on_simple_dict"], ["    ", "def", "__init__", "(", "self", ",", "profile", ",", "lang", ",", "alpha", "=", "1", ",", "beta", "=", "0", ")", ":", "\n", "        ", "super", "(", "Diacritics", ",", "self", ")", ".", "__init__", "(", "profile", ",", "alpha", ",", "beta", ")", "\n", "\n", "all_wo_diacritics_perc", "=", "profile", "[", "'diacritics'", "]", "[", "'all_wo_diacritics_perc'", "]", "\n", "self", ".", "all_wo_diacritics_perc", "=", "utils", ".", "_apply_smoothing", "(", "[", "all_wo_diacritics_perc", "]", ",", "alpha", ",", "beta", ")", "[", "0", "]", "\n", "\n", "wrong_char_diacritics_perc", "=", "profile", "[", "'diacritics'", "]", "[", "'wrong_char_diacritics_perc'", "]", "\n", "self", ".", "wrong_char_diacritics_perc", "=", "utils", ".", "_apply_smoothing", "(", "[", "wrong_char_diacritics_perc", "]", ",", "alpha", ",", "beta", ")", "[", "0", "]", "\n", "\n", "wrongly_diacritized_chars_probs", "=", "profile", "[", "'diacritics'", "]", "[", "'wrongly_diacritized_chars_probs'", "]", "\n", "self", ".", "wrongly_diacritized_chars_probs", "=", "{", "\n", "k", ":", "utils", ".", "_apply_smoothing_on_simple_dict", "(", "wrongly_diacritized_chars_probs", "[", "k", "]", ",", "alpha", ",", "beta", ")", "for", "k", "in", "\n", "wrongly_diacritized_chars_probs", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.diacritics.Diacritics.apply": [[23, 58], ["changes.append", "aspects.diacritization_stripping.strip_diacritics_single_line", "numpy.random.uniform", "aspects.diacritization_stripping.strip_diacritics_single_line", "changes.append", "numpy.random.uniform", "numpy.random.choice", "c.lower", "list", "numpy.random.uniform", "numpy.random.choice().upper", "c.upper", "diacritics.Diacritics.wrongly_diacritized_chars_probs[].keys", "list", "numpy.random.uniform", "numpy.random.choice().lower", "diacritics.Diacritics.wrongly_diacritized_chars_probs[].values", "numpy.random.choice", "list", "numpy.random.choice", "diacritics.Diacritics.wrongly_diacritized_chars_probs[].keys", "list", "list", "diacritics.Diacritics.wrongly_diacritized_chars_probs[].values", "diacritics.Diacritics.wrongly_diacritized_chars_probs[].keys", "list", "diacritics.Diacritics.wrongly_diacritized_chars_probs[].values", "c.lower", "c.lower", "c.upper", "c.upper"], "methods", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.diacritization_stripping.strip_diacritics_single_line", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.diacritization_stripping.strip_diacritics_single_line"], ["", "def", "apply", "(", "self", ",", "text", ",", "whitespace_info", ")", ":", "\n", "        ", "changes", "=", "[", "]", "\n", "\n", "if", "strip_diacritics_single_line", "(", "text", ")", "!=", "text", "and", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "self", ".", "all_wo_diacritics_perc", ":", "\n", "            ", "changes", ".", "append", "(", "[", "'DIACR'", ",", "'all_strip_diacritics'", "]", ")", "\n", "return", "strip_diacritics_single_line", "(", "text", ")", ",", "changes", ",", "whitespace_info", "\n", "\n", "", "new_text", "=", "''", "\n", "\n", "for", "c", "in", "text", ":", "\n", "            ", "if", "c", "in", "self", ".", "wrongly_diacritized_chars_probs", ":", "\n", "                ", "if", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "self", ".", "wrong_char_diacritics_perc", ":", "\n", "                    ", "new_text", "+=", "np", ".", "random", ".", "choice", "(", "list", "(", "self", ".", "wrongly_diacritized_chars_probs", "[", "c", "]", ".", "keys", "(", ")", ")", ",", "\n", "p", "=", "list", "(", "self", ".", "wrongly_diacritized_chars_probs", "[", "c", "]", ".", "values", "(", ")", ")", ")", "\n", "", "else", ":", "\n", "                    ", "new_text", "+=", "c", "\n", "", "", "elif", "c", ".", "lower", "(", ")", "in", "self", ".", "wrongly_diacritized_chars_probs", ":", "\n", "                ", "if", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "self", ".", "wrong_char_diacritics_perc", ":", "\n", "                    ", "new_text", "+=", "np", ".", "random", ".", "choice", "(", "list", "(", "self", ".", "wrongly_diacritized_chars_probs", "[", "c", ".", "lower", "(", ")", "]", ".", "keys", "(", ")", ")", ",", "\n", "p", "=", "list", "(", "self", ".", "wrongly_diacritized_chars_probs", "[", "c", ".", "lower", "(", ")", "]", ".", "values", "(", ")", ")", ")", ".", "upper", "(", ")", "\n", "", "else", ":", "\n", "                    ", "new_text", "+=", "c", "\n", "", "", "elif", "c", ".", "upper", "(", ")", "in", "self", ".", "wrongly_diacritized_chars_probs", ":", "\n", "                ", "if", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "self", ".", "wrong_char_diacritics_perc", ":", "\n", "                    ", "new_text", "+=", "np", ".", "random", ".", "choice", "(", "list", "(", "self", ".", "wrongly_diacritized_chars_probs", "[", "c", ".", "upper", "(", ")", "]", ".", "keys", "(", ")", ")", ",", "\n", "p", "=", "list", "(", "self", ".", "wrongly_diacritized_chars_probs", "[", "c", ".", "upper", "(", ")", "]", ".", "values", "(", ")", ")", ")", ".", "lower", "(", ")", "\n", "", "else", ":", "\n", "                    ", "new_text", "+=", "c", "\n", "", "", "else", ":", "\n", "                ", "new_text", "+=", "c", "\n", "\n", "", "if", "new_text", "[", "-", "1", "]", "!=", "c", ":", "\n", "                ", "changes", ".", "append", "(", "[", "'DIACR'", ",", "'replace {} with {}'", ".", "format", "(", "c", ",", "new_text", "[", "-", "1", "]", ")", "]", ")", "\n", "\n", "", "", "return", "new_text", ",", "changes", ",", "whitespace_info", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.diacritics.Diacritics.estimate_probabilities": [[59, 145], ["numpy.sum", "char.upper", "aspects.apply_m2_edits.processM2", "original_paragraphs.append", "corrected_paragraphs_wo_diacr.append", "aspects.apply_m2_edits.processM2", "corrected_paragraphs.append", "aspects.diacritization_stripping.strip_diacritics_single_line", "zip", "numpy.sum", "list", "zip", "list", "filtered_wrongly_diacritized_chars_map[].values", "filtered_wrongly_diacritized_chars_map[].items", "list", "list", "wrongly_diacritized_chars_map[].values", "wrongly_diacritized_chars_map[].items", "coder_dict.keys", "coder_dict.keys", "aspects.diacritization_stripping.strip_diacritics_single_line", "aspects.diacritization_stripping.strip_diacritics_single_line"], "methods", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.apply_m2_edits.processM2", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.apply_m2_edits.processM2", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.diacritization_stripping.strip_diacritics_single_line", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.diacritization_stripping.strip_diacritics_single_line", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.diacritization_stripping.strip_diacritics_single_line"], ["", "@", "staticmethod", "\n", "def", "estimate_probabilities", "(", "m2_records", ")", ":", "\n", "\n", "        ", "czech_diacritics_tuples", "=", "[", "(", "'a'", ",", "'\u00e1'", ")", ",", "(", "'c'", ",", "'\u010d'", ")", ",", "(", "'d'", ",", "'\u010f'", ")", ",", "(", "'e'", ",", "'\u00e9'", ",", "'\u011b'", ")", ",", "(", "'i'", ",", "'\u00ed'", ")", ",", "(", "'n'", ",", "'\u0148'", ")", ",", "(", "'o'", ",", "'\u00f3'", ")", ",", "(", "'r'", ",", "'\u0159'", ")", ",", "\n", "(", "'s'", ",", "'\u0161'", ")", ",", "(", "'t'", ",", "'\u0165'", ")", ",", "(", "'u'", ",", "'\u016f'", ",", "'\u00fa'", ")", ",", "(", "'y'", ",", "'\u00fd'", ")", ",", "(", "'z'", ",", "'\u017e'", ")", "]", "\n", "czech_diacritizables_chars", "=", "[", "char", "for", "sublist", "in", "czech_diacritics_tuples", "for", "char", "in", "sublist", "]", "+", "[", "char", ".", "upper", "(", ")", "for", "sublist", "in", "\n", "czech_diacritics_tuples", "for", "char", "\n", "in", "sublist", "]", "\n", "num_all_wo_diacritics", "=", "0", "\n", "num_files", "=", "0", "\n", "num_badly_diacritized_chars", "=", "0", "\n", "num_could_be_diacritized_char", "=", "0", "\n", "\n", "wrongly_diacritized_chars_map", "=", "{", "}", "\n", "\n", "for", "m2_file", "in", "m2_records", ":", "\n", "            ", "num_files", "+=", "1", "\n", "\n", "original_paragraphs", ",", "corrected_paragraphs_wo_diacr", ",", "corrected_paragraphs", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "info", "in", "m2_file", ":", "\n", "# Get the original and corrected sentence + edits for each annotator.", "\n", "                ", "orig_sent", ",", "coder_dict", "=", "apply_m2_edits", ".", "processM2", "(", "info", ",", "[", "\"DIACR\"", "]", ")", "\n", "orig_sent", "=", "\" \"", ".", "join", "(", "orig_sent", ")", "\n", "if", "coder_dict", ":", "\n", "                    ", "coder_id", "=", "list", "(", "coder_dict", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "cor_sent", "=", "\" \"", ".", "join", "(", "coder_dict", "[", "coder_id", "]", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                    ", "cor_sent", "=", "orig_sent", "\n", "\n", "", "original_paragraphs", ".", "append", "(", "orig_sent", ")", "\n", "corrected_paragraphs_wo_diacr", ".", "append", "(", "cor_sent", ")", "\n", "\n", "_", ",", "coder_dict", "=", "apply_m2_edits", ".", "processM2", "(", "info", ",", "[", "]", ")", "\n", "if", "coder_dict", ":", "\n", "                    ", "coder_id", "=", "list", "(", "coder_dict", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "cor_sent", "=", "\" \"", ".", "join", "(", "coder_dict", "[", "coder_id", "]", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                    ", "cor_sent", "=", "orig_sent", "\n", "", "corrected_paragraphs", ".", "append", "(", "cor_sent", ")", "\n", "\n", "", "this_text_is_all_wo_diacritics_and_should_contain_some", "=", "False", "\n", "# if original sentence does not contain diacritics", "\n", "if", "strip_diacritics_single_line", "(", "\" \"", ".", "join", "(", "original_paragraphs", ")", ")", "==", "\" \"", ".", "join", "(", "original_paragraphs", ")", ":", "\n", "# and there is a change in diacritics", "\n", "                ", "if", "(", "corrected_paragraphs_wo_diacr", "!=", "corrected_paragraphs", ")", ":", "\n", "                    ", "num_all_wo_diacritics", "+=", "1", "\n", "this_text_is_all_wo_diacritics_and_should_contain_some", "=", "True", "\n", "\n", "# individual characters", "\n", "", "", "if", "not", "this_text_is_all_wo_diacritics_and_should_contain_some", ":", "\n", "                ", "for", "p_orig", ",", "p_cor", "in", "zip", "(", "corrected_paragraphs_wo_diacr", ",", "corrected_paragraphs", ")", ":", "\n", "                    ", "for", "c_orig", ",", "c_cor", "in", "zip", "(", "p_orig", ",", "p_cor", ")", ":", "\n", "                        ", "if", "c_orig", "!=", "c_cor", "and", "strip_diacritics_single_line", "(", "c_orig", ")", "==", "strip_diacritics_single_line", "(", "c_cor", ")", ":", "\n", "                            ", "num_badly_diacritized_chars", "+=", "1", "\n", "num_could_be_diacritized_char", "+=", "1", "\n", "\n", "if", "c_cor", "not", "in", "wrongly_diacritized_chars_map", ":", "\n", "                                ", "wrongly_diacritized_chars_map", "[", "c_cor", "]", "=", "{", "}", "\n", "\n", "", "if", "c_orig", "not", "in", "wrongly_diacritized_chars_map", "[", "c_cor", "]", ":", "\n", "                                ", "wrongly_diacritized_chars_map", "[", "c_cor", "]", "[", "c_orig", "]", "=", "0", "\n", "\n", "", "wrongly_diacritized_chars_map", "[", "c_cor", "]", "[", "c_orig", "]", "+=", "1", "\n", "", "elif", "c_cor", "in", "czech_diacritizables_chars", ":", "\n", "                            ", "num_could_be_diacritized_char", "+=", "1", "\n", "\n", "", "", "", "", "", "all_wo_diacritics_perc", "=", "num_all_wo_diacritics", "/", "num_files", "\n", "wrong_char_diacritics_perc", "=", "num_badly_diacritized_chars", "/", "num_could_be_diacritized_char", "\n", "\n", "# filter out characters whose correction in diacritics appeared too little (<3)", "\n", "filtered_wrongly_diacritized_chars_map", "=", "{", "}", "\n", "for", "c_cor", "in", "wrongly_diacritized_chars_map", ":", "\n", "            ", "if", "np", ".", "sum", "(", "list", "(", "wrongly_diacritized_chars_map", "[", "c_cor", "]", ".", "values", "(", ")", ")", ")", ">", "3", ":", "\n", "                ", "filtered_wrongly_diacritized_chars_map", "[", "c_cor", "]", "=", "{", "c_orig", ":", "c_orig_value", "for", "c_orig", ",", "c_orig_value", "in", "\n", "wrongly_diacritized_chars_map", "[", "c_cor", "]", ".", "items", "(", ")", "}", "\n", "\n", "# normalize wrongly_diacritized_chars_map", "\n", "", "", "wrongly_diacritized_chars_probs", "=", "{", "}", "\n", "for", "c_cor", "in", "filtered_wrongly_diacritized_chars_map", ":", "\n", "            ", "normalize_sum", "=", "np", ".", "sum", "(", "list", "(", "filtered_wrongly_diacritized_chars_map", "[", "c_cor", "]", ".", "values", "(", ")", ")", ")", "\n", "wrongly_diacritized_chars_probs", "[", "c_cor", "]", "=", "{", "c_orig", ":", "c_orig_value", "/", "normalize_sum", "for", "c_orig", ",", "c_orig_value", "in", "\n", "filtered_wrongly_diacritized_chars_map", "[", "c_cor", "]", ".", "items", "(", ")", "}", "\n", "\n", "", "return", "'diacritics'", ",", "{", "'all_wo_diacritics_perc'", ":", "all_wo_diacritics_perc", ",", "\n", "'wrong_char_diacritics_perc'", ":", "wrong_char_diacritics_perc", ",", "\n", "'wrongly_diacritized_chars_probs'", ":", "wrongly_diacritized_chars_probs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.common_other.CommonOther.__init__": [[8, 17], ["aspects.base.Aspect.__init__", "aspects.utils._apply_smoothing_on_simple_dict"], "methods", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation.__init__", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing_on_simple_dict"], ["    ", "def", "__init__", "(", "self", ",", "profile", ",", "lang", ",", "alpha", "=", "1", ",", "beta", "=", "0", ")", ":", "\n", "        ", "super", "(", "CommonOther", ",", "self", ")", ".", "__init__", "(", "profile", ",", "alpha", ",", "beta", ")", "\n", "\n", "common_other_all_pairs_probs", "=", "profile", "[", "'common_other'", "]", "[", "'all_pairs_probs'", "]", "\n", "\n", "self", ".", "common_other_all_pairs_probs", "=", "{", "}", "\n", "for", "cor_from", "in", "common_other_all_pairs_probs", ":", "\n", "            ", "self", ".", "common_other_all_pairs_probs", "[", "cor_from", "]", "=", "utils", ".", "_apply_smoothing_on_simple_dict", "(", "common_other_all_pairs_probs", "[", "cor_from", "]", ",", "\n", "alpha", ",", "beta", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.common_other.CommonOther.apply": [[18, 123], ["enumerate", "common_other.CommonOther.apply._get_occurence_start_indices_of_tokens_in_text"], "methods", ["None"], ["", "", "def", "apply", "(", "self", ",", "text", ",", "whitespace_info", ")", ":", "\n", "        ", "def", "_get_occurence_start_indices_of_tokens_in_text", "(", "text", ",", "substring", ")", ":", "\n", "            ", "'''\n            Return all occurences of substring in text, but make sure that each occurence of substring is bordered by non-alpha characters, so\n            that substring is not a part of another word (e.g. for substring \"se\" we do not want to count its occurence in text \"prase\")\n            '''", "\n", "occurence_start_indices", "=", "[", "]", "\n", "last_occurence_start_index", "=", "-", "1", "\n", "while", "substring", "in", "text", "[", "last_occurence_start_index", "+", "1", ":", "]", ":", "\n", "                ", "start_index", "=", "text", ".", "index", "(", "substring", ",", "last_occurence_start_index", "+", "1", ")", "\n", "\n", "if", "(", "start_index", "==", "0", "or", "(", "start_index", "!=", "0", "and", "[", "start_index", "-", "1", "]", "==", "' '", ")", ")", "and", "(", "\n", "(", "start_index", "+", "len", "(", "substring", ")", "==", "len", "(", "text", ")", ")", "or", "text", "[", "start_index", "+", "len", "(", "substring", ")", "]", "==", "' '", ")", ":", "\n", "                    ", "occurence_start_indices", ".", "append", "(", "start_index", ")", "\n", "\n", "", "last_occurence_start_index", "=", "start_index", "\n", "\n", "", "return", "occurence_start_indices", "\n", "\n", "", "changes", "=", "[", "]", "\n", "'''\n        Go over all keys (corrected tokens) in all_pairs_probs and try to apply each of them on its each occurence in text.\n        '''", "\n", "# substitutes / deletes", "\n", "for", "cor_tok", "in", "self", ".", "common_other_all_pairs_probs", ":", "\n", "            ", "if", "not", "cor_tok", ":", "# insertions are done separately", "\n", "                ", "continue", "\n", "\n", "", "occurence_start_indices", "=", "_get_occurence_start_indices_of_tokens_in_text", "(", "text", ".", "lower", "(", ")", ",", "cor_tok", ")", "\n", "if", "len", "(", "occurence_start_indices", ")", ">", "0", ":", "\n", "                ", "char_relative_change", "=", "0", "\n", "for", "start_index", "in", "occurence_start_indices", ":", "\n", "                    ", "start_index", "=", "start_index", "+", "char_relative_change", "\n", "if", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "np", ".", "sum", "(", "list", "(", "self", ".", "common_other_all_pairs_probs", "[", "cor_tok", "]", ".", "values", "(", ")", ")", ")", ":", "\n", "                        ", "replace_tokens_probs", "=", "np", ".", "array", "(", "list", "(", "self", ".", "common_other_all_pairs_probs", "[", "cor_tok", "]", ".", "values", "(", ")", ")", ")", "\n", "replace_tokens_probs_normalized", "=", "replace_tokens_probs", "/", "np", ".", "sum", "(", "replace_tokens_probs", ")", "\n", "\n", "chosen_replace_tokens", "=", "np", ".", "random", ".", "choice", "(", "list", "(", "self", ".", "common_other_all_pairs_probs", "[", "cor_tok", "]", ".", "keys", "(", ")", ")", ",", "\n", "p", "=", "replace_tokens_probs_normalized", ")", "\n", "\n", "# if this is delete and would delete whole text, do not perform it", "\n", "if", "len", "(", "chosen_replace_tokens", ")", "==", "0", "and", "len", "(", "cor_tok", ")", "==", "len", "(", "text", ")", ":", "\n", "                            ", "continue", "\n", "\n", "", "if", "text", "[", "start_index", "]", ".", "isupper", "(", ")", "and", "len", "(", "chosen_replace_tokens", ")", ">", "0", ":", "\n", "                            ", "chosen_replace_tokens", "=", "chosen_replace_tokens", "[", "0", "]", ".", "upper", "(", ")", "+", "chosen_replace_tokens", "[", "1", ":", "]", "\n", "\n", "", "if", "len", "(", "chosen_replace_tokens", ")", "==", "0", ":", "# delete", "\n", "                            ", "if", "start_index", "==", "0", ":", "\n", "                                ", "text", "=", "text", "[", "start_index", "+", "1", "+", "len", "(", "cor_tok", ")", ":", "]", "\n", "", "else", ":", "\n", "                                ", "text", "=", "text", "[", ":", "start_index", "-", "1", "]", "+", "chosen_replace_tokens", "+", "text", "[", "start_index", "+", "len", "(", "cor_tok", ")", ":", "]", "\n", "", "", "else", ":", "\n", "                            ", "text", "=", "text", "[", ":", "start_index", "]", "+", "chosen_replace_tokens", "+", "text", "[", "start_index", "+", "len", "(", "cor_tok", ")", ":", "]", "\n", "\n", "# fix whitespace info", "\n", "", "num_tokens_in_correct", "=", "len", "(", "cor_tok", ".", "split", "(", "' '", ")", ")", "\n", "num_tokens_in_noised", "=", "len", "(", "chosen_replace_tokens", ".", "split", "(", "' '", ")", ")", "if", "chosen_replace_tokens", "else", "0", "\n", "\n", "start_index_token_num", "=", "np", ".", "sum", "(", "[", "1", "for", "x", "in", "text", "[", ":", "start_index", "]", "if", "x", "==", "' '", "]", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "if", "num_tokens_in_correct", ">", "num_tokens_in_noised", ":", "\n", "                            ", "for", "_", "in", "range", "(", "num_tokens_in_correct", "-", "num_tokens_in_noised", ")", ":", "\n", "                                ", "del", "whitespace_info", "[", "start_index_token_num", "]", "\n", "", "", "elif", "num_tokens_in_correct", "<", "num_tokens_in_noised", ":", "\n", "                            ", "for", "_", "in", "range", "(", "num_tokens_in_noised", "-", "num_tokens_in_correct", ")", ":", "\n", "                                ", "whitespace_info", ".", "insert", "(", "start_index_token_num", ",", "True", ")", "\n", "\n", "", "", "char_relative_change", "+=", "len", "(", "chosen_replace_tokens", ")", "-", "len", "(", "cor_tok", ")", "\n", "if", "len", "(", "chosen_replace_tokens", ")", "==", "0", ":", "\n", "                            ", "char_relative_change", "-=", "1", "# if we delete a token, we also remove one space next to it", "\n", "\n", "", "if", "len", "(", "chosen_replace_tokens", ")", "==", "0", ":", "\n", "                            ", "changes", ".", "append", "(", "[", "'COMMON-OTHER'", ",", "'delete {}'", ".", "format", "(", "cor_tok", ")", "]", ")", "\n", "", "else", ":", "\n", "                            ", "changes", ".", "append", "(", "[", "'COMMON-OTHER'", ",", "'change {} -> {}'", ".", "format", "(", "cor_tok", ",", "chosen_replace_tokens", ")", "]", ")", "\n", "\n", "# inserts", "\n", "", "", "", "", "", "insert_into_whitespace", "=", "[", "None", "]", "*", "len", "(", "whitespace_info", ")", "\n", "if", "''", "in", "self", ".", "common_other_all_pairs_probs", ":", "\n", "            ", "for", "whitespace_ind", "in", "range", "(", "len", "(", "insert_into_whitespace", ")", ")", ":", "\n", "                ", "for", "tokens_to", "in", "np", ".", "random", ".", "permutation", "(", "\n", "list", "(", "self", ".", "common_other_all_pairs_probs", "[", "''", "]", ".", "keys", "(", ")", ")", ")", ":", "# do permutation to allow all tokens when alpha-smoothing is high", "\n", "                    ", "if", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "self", ".", "common_other_all_pairs_probs", "[", "''", "]", "[", "tokens_to", "]", ":", "\n", "                        ", "insert_into_whitespace", "[", "whitespace_ind", "]", "=", "tokens_to", "\n", "break", "# do just one insert per each whitespace", "\n", "\n", "", "", "", "", "num_inserted_spaces", "=", "0", "\n", "new_text", "=", "\"\"", "\n", "for", "token_ind", ",", "token", "in", "enumerate", "(", "text", ".", "split", "(", "' '", ")", ")", ":", "\n", "            ", "if", "token_ind", "!=", "0", ":", "\n", "                ", "new_text", "+=", "\" \"", "\n", "\n", "", "new_text", "+=", "token", "\n", "\n", "if", "token_ind", "<", "len", "(", "insert_into_whitespace", ")", "and", "insert_into_whitespace", "[", "token_ind", "]", ":", "\n", "                ", "new_text", "+=", "\" \"", "+", "insert_into_whitespace", "[", "token_ind", "]", "\n", "\n", "for", "_", "in", "range", "(", "len", "(", "insert_into_whitespace", "[", "token_ind", "]", ".", "split", "(", "' '", ")", ")", ")", ":", "\n", "                    ", "whitespace_info", ".", "insert", "(", "token_ind", "+", "num_inserted_spaces", ",", "True", ")", "\n", "\n", "", "num_inserted_spaces", "+=", "len", "(", "insert_into_whitespace", "[", "token_ind", "]", ".", "split", "(", "' '", ")", ")", "\n", "\n", "changes", ".", "append", "(", "[", "'COMMON-OTHER'", ",", "'insert {}'", ".", "format", "(", "insert_into_whitespace", "[", "token_ind", "]", ")", "]", ")", "\n", "\n", "", "", "return", "new_text", ",", "changes", ",", "whitespace_info", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.common_other.CommonOther._get_occurence_count_of_tokens_in_text": [[124, 142], ["text.index", "len", "text[].isalpha", "text[].isalpha", "len", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_occurence_count_of_tokens_in_text", "(", "text", ",", "substring", ")", ":", "\n", "        ", "'''\n        Get all occurences of substring in text, but make sure that each occurence of substring is bordered by non-alpha characters, so that\n        substring is not a part of another word (e.g. for substring \"se\" we do not want to count its occurence in text \"prase\")\n        '''", "\n", "num_occurences_of_cor_tok_in_cor", "=", "0", "\n", "last_occurence_start_index", "=", "-", "1", "\n", "while", "substring", "in", "text", "[", "last_occurence_start_index", "+", "1", ":", "]", ":", "\n", "            ", "start_index", "=", "text", ".", "index", "(", "substring", ",", "last_occurence_start_index", "+", "1", ")", "\n", "\n", "if", "(", "start_index", "==", "0", "or", "(", "start_index", "!=", "0", "and", "not", "text", "[", "start_index", "-", "1", "]", ".", "isalpha", "(", ")", ")", ")", "and", "(", "\n", "(", "start_index", "+", "len", "(", "substring", ")", "==", "len", "(", "text", ")", ")", "or", "not", "text", "[", "start_index", "+", "len", "(", "substring", ")", "]", ".", "isalpha", "(", ")", ")", ":", "\n", "                ", "num_occurences_of_cor_tok_in_cor", "+=", "1", "\n", "\n", "", "last_occurence_start_index", "=", "start_index", "\n", "\n", "", "return", "num_occurences_of_cor_tok_in_cor", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.common_other.CommonOther._get_occurence_counts": [[143, 164], ["list", "coder_dict.keys", "cor_tok.strip", "common_other.CommonOther._get_occurence_count_of_tokens_in_text", "len", "cor_sent.split"], "methods", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.common_other.CommonOther._get_occurence_count_of_tokens_in_text"], ["", "@", "staticmethod", "\n", "def", "_get_occurence_counts", "(", "cor_toks", ",", "coder_dicts", ")", ":", "\n", "        ", "num_occurence", "=", "{", "}", "\n", "\n", "for", "coder_dict", "in", "coder_dicts", ":", "\n", "            ", "if", "coder_dict", ":", "\n", "                ", "coder_id", "=", "list", "(", "coder_dict", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "\n", "cor_sent", "=", "coder_dict", "[", "coder_id", "]", "[", "0", "]", "\n", "\n", "cor_sent", "=", "\" \"", ".", "join", "(", "cor_sent", ")", ".", "lower", "(", ")", "\n", "\n", "for", "cor_tok", "in", "cor_toks", ":", "\n", "                    ", "if", "cor_tok", "not", "in", "num_occurence", ":", "\n", "                        ", "num_occurence", "[", "cor_tok", "]", "=", "0", "\n", "\n", "", "if", "not", "cor_tok", ".", "strip", "(", ")", ":", "# replace empty string with some tokens (= insert)", "\n", "                        ", "num_occurence", "[", "cor_tok", "]", "+=", "len", "(", "cor_sent", ".", "split", "(", "' '", ")", ")", "+", "1", "# number of places where the token could be inserted", "\n", "", "else", ":", "\n", "                        ", "num_occurence", "[", "cor_tok", "]", "+=", "CommonOther", ".", "_get_occurence_count_of_tokens_in_text", "(", "cor_sent", ",", "cor_tok", ")", "\n", "", "", "", "", "return", "num_occurence", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.common_other.CommonOther.estimate_probabilities": [[165, 225], ["common_other.CommonOther._get_occurence_counts", "list", "aspects.apply_m2_edits.processM2", "sum", "all_pairs.keys", "numpy.sum", "cached_coder_dicts.append", "list", "cor_toks.strip", "len", "list", "cor_tok.lower.lower.lower", "all", "all_pairs[].values", "coder_dict.keys", "x.isalpha"], "methods", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.common_other.CommonOther._get_occurence_counts", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.apply_m2_edits.processM2"], ["", "@", "staticmethod", "\n", "def", "estimate_probabilities", "(", "m2_records", ")", ":", "\n", "        ", "num_files", "=", "0", "\n", "exclude_error_types", "=", "[", "'noop'", ",", "'PUNCT'", ",", "'CASING'", "]", "\n", "num_all_alpha_words", "=", "0", "\n", "\n", "all_pairs", "=", "{", "}", "\n", "cached_coder_dicts", "=", "[", "]", "\n", "\n", "for", "m2_file", "in", "m2_records", ":", "\n", "            ", "num_files", "+=", "1", "\n", "\n", "for", "info", "in", "m2_file", ":", "\n", "                ", "orig_sent", ",", "coder_dict", "=", "apply_m2_edits", ".", "processM2", "(", "info", ",", "[", "]", ")", "\n", "if", "coder_dict", ":", "\n", "                    ", "cached_coder_dicts", ".", "append", "(", "coder_dict", ")", "\n", "coder_id", "=", "list", "(", "coder_dict", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "\n", "for", "edit", "in", "coder_dict", "[", "coder_id", "]", "[", "1", "]", ":", "\n", "                        ", "orig_start", ",", "orig_end", ",", "error_type", ",", "cor_tok", ",", "cor_start", ",", "cor_end", "=", "edit", "\n", "cor_tok", "=", "cor_tok", ".", "lower", "(", ")", "\n", "\n", "if", "all", "(", "[", "x", "not", "in", "error_type", "for", "x", "in", "exclude_error_types", "]", ")", ":", "\n", "                            ", "orig_toks", "=", "\" \"", ".", "join", "(", "orig_sent", "[", "orig_start", ":", "orig_end", "]", ")", ".", "lower", "(", ")", "\n", "\n", "# # check that this error is not already handled by suffix/prefix estimator", "\n", "# if (orig_end - orig_start == 0) or (cor_end - cor_start == 0) or (", "\n", "#                 cor_tok[0] != orig_toks[0] and cor_tok[-1] != cor_tok[-1]) or (orig_end - orig_start) > 1:", "\n", "\n", "if", "cor_tok", "not", "in", "all_pairs", ":", "\n", "                                ", "all_pairs", "[", "cor_tok", "]", "=", "{", "}", "\n", "\n", "", "if", "orig_toks", "not", "in", "all_pairs", "[", "cor_tok", "]", ":", "\n", "                                ", "all_pairs", "[", "cor_tok", "]", "[", "orig_toks", "]", "=", "0", "\n", "\n", "", "all_pairs", "[", "cor_tok", "]", "[", "orig_toks", "]", "+=", "1", "\n", "\n", "", "", "", "num_all_alpha_words", "+=", "sum", "(", "[", "1", "for", "x", "in", "orig_sent", "if", "x", ".", "isalpha", "(", ")", "]", ")", "\n", "\n", "", "", "num_occurence", "=", "CommonOther", ".", "_get_occurence_counts", "(", "list", "(", "all_pairs", ".", "keys", "(", ")", ")", ",", "cached_coder_dicts", ")", "\n", "\n", "all_pairs_probs", "=", "{", "}", "\n", "filter_out_min_occ_count", "=", "3", "\n", "for", "cor_toks", "in", "all_pairs", ":", "\n", "            ", "if", "np", ".", "sum", "(", "list", "(", "all_pairs", "[", "cor_toks", "]", ".", "values", "(", ")", ")", ")", ">=", "filter_out_min_occ_count", ":", "\n", "                ", "all_pairs_probs", "[", "cor_toks", "]", "=", "{", "}", "\n", "if", "not", "cor_toks", ".", "strip", "(", ")", ":", "\n", "                    ", "for", "orig_toks", "in", "all_pairs", "[", "cor_toks", "]", ":", "\n", "                        ", "if", "all_pairs", "[", "cor_toks", "]", "[", "orig_toks", "]", ">=", "filter_out_min_occ_count", ":", "\n", "                            ", "all_pairs_probs", "[", "cor_toks", "]", "[", "orig_toks", "]", "=", "all_pairs", "[", "cor_toks", "]", "[", "orig_toks", "]", "/", "num_all_alpha_words", "\n", "", "", "", "else", ":", "\n", "                    ", "for", "orig_toks", "in", "all_pairs", "[", "cor_toks", "]", ":", "\n", "                        ", "if", "all_pairs", "[", "cor_toks", "]", "[", "orig_toks", "]", ">=", "filter_out_min_occ_count", ":", "\n", "                            ", "all_pairs_probs", "[", "cor_toks", "]", "[", "orig_toks", "]", "=", "all_pairs", "[", "cor_toks", "]", "[", "orig_toks", "]", "/", "num_occurence", "[", "cor_toks", "]", "\n", "\n", "", "", "", "if", "len", "(", "all_pairs_probs", "[", "cor_toks", "]", ")", "==", "0", ":", "\n", "                    ", "del", "all_pairs_probs", "[", "cor_toks", "]", "\n", "\n", "", "", "", "return", "'common_other'", ",", "{", "\n", "\"all_pairs_probs\"", ":", "all_pairs_probs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.diacritization_stripping.strip_diacritics": [[5, 15], ["None"], "function", ["None"], ["def", "strip_diacritics", "(", "list_of_texts", ")", ":", "\n", "    ", "for", "line", "in", "list_of_texts", ":", "\n", "        ", "output", "=", "\"\"", "\n", "for", "c", "in", "line", ":", "\n", "            ", "if", "c", "in", "diacritization_stripping_data", ".", "strip_diacritization_uninames", ":", "\n", "                ", "output", "+=", "diacritization_stripping_data", ".", "strip_diacritization_uninames", "[", "c", "]", "\n", "", "else", ":", "\n", "                ", "output", "+=", "c", "\n", "\n", "", "", "yield", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.diacritization_stripping.strip_diacritics_single_line": [[16, 25], ["None"], "function", ["None"], ["", "", "def", "strip_diacritics_single_line", "(", "textline", ")", ":", "\n", "    ", "output", "=", "\"\"", "\n", "for", "c", "in", "textline", ":", "\n", "        ", "if", "c", "in", "diacritization_stripping_data", ".", "strip_diacritization_uninames", ":", "\n", "            ", "output", "+=", "diacritization_stripping_data", ".", "strip_diacritization_uninames", "[", "c", "]", "\n", "", "else", ":", "\n", "           ", "output", "+=", "c", "\n", "\n", "", "", "return", "output", "\n", "", ""]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils.align": [[4, 57], ["len", "len", "range", "range", "range", "range", "range", "range", "range", "range", "costs.index", "sorted", "sorted", "float", "min", "orig[].lower", "cor[].lower"], "function", ["None"], ["def", "align", "(", "orig", ",", "cor", ")", ":", "\n", "# Sentence lengths", "\n", "    ", "o_len", "=", "len", "(", "orig", ")", "\n", "c_len", "=", "len", "(", "cor", ")", "\n", "\n", "# Create the cost_matrix and the op_matrix", "\n", "cost_matrix", "=", "[", "[", "0.0", "for", "j", "in", "range", "(", "c_len", "+", "1", ")", "]", "for", "i", "in", "range", "(", "o_len", "+", "1", ")", "]", "\n", "op_matrix", "=", "[", "[", "\"O\"", "for", "j", "in", "range", "(", "c_len", "+", "1", ")", "]", "for", "i", "in", "range", "(", "o_len", "+", "1", ")", "]", "\n", "# Fill in the edges", "\n", "for", "i", "in", "range", "(", "1", ",", "o_len", "+", "1", ")", ":", "\n", "        ", "cost_matrix", "[", "i", "]", "[", "0", "]", "=", "cost_matrix", "[", "i", "-", "1", "]", "[", "0", "]", "+", "1", "\n", "op_matrix", "[", "i", "]", "[", "0", "]", "=", "\"D\"", "\n", "", "for", "j", "in", "range", "(", "1", ",", "c_len", "+", "1", ")", ":", "\n", "        ", "cost_matrix", "[", "0", "]", "[", "j", "]", "=", "cost_matrix", "[", "0", "]", "[", "j", "-", "1", "]", "+", "1", "\n", "op_matrix", "[", "0", "]", "[", "j", "]", "=", "\"I\"", "\n", "\n", "# Loop through the cost_matrix", "\n", "", "for", "i", "in", "range", "(", "o_len", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "c_len", ")", ":", "\n", "# Matches", "\n", "            ", "if", "orig", "[", "i", "]", "==", "cor", "[", "j", "]", ":", "\n", "                ", "cost_matrix", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", "=", "cost_matrix", "[", "i", "]", "[", "j", "]", "\n", "op_matrix", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", "=", "\"M\"", "\n", "# Non-matches", "\n", "", "else", ":", "\n", "                ", "del_cost", "=", "cost_matrix", "[", "i", "]", "[", "j", "+", "1", "]", "+", "1", "\n", "ins_cost", "=", "cost_matrix", "[", "i", "+", "1", "]", "[", "j", "]", "+", "1", "\n", "# Standard Levenshtein (S = 1)", "\n", "sub_cost", "=", "cost_matrix", "[", "i", "]", "[", "j", "]", "+", "1", "\n", "\n", "# Transpositions require >=2 tokens", "\n", "# Traverse the diagonal while there is not a Match.", "\n", "if", "sorted", "(", "orig", "[", "i", "-", "1", ":", "i", "+", "1", "]", ".", "lower", "(", ")", ")", "==", "sorted", "(", "cor", "[", "j", "-", "1", ":", "j", "+", "1", "]", ".", "lower", "(", ")", ")", ":", "\n", "                    ", "trans_cost", "=", "cost_matrix", "[", "i", "-", "1", "]", "[", "j", "-", "1", "]", "+", "1", "\n", "", "else", ":", "\n", "                    ", "trans_cost", "=", "float", "(", "\"inf\"", ")", "\n", "\n", "# Costs", "\n", "", "costs", "=", "[", "trans_cost", ",", "sub_cost", ",", "ins_cost", ",", "del_cost", "]", "\n", "# Get the index of the cheapest (first cheapest if tied)", "\n", "l", "=", "costs", ".", "index", "(", "min", "(", "costs", ")", ")", "\n", "# Save the cost and the op in the matrices", "\n", "cost_matrix", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", "=", "costs", "[", "l", "]", "\n", "if", "l", "==", "0", ":", "\n", "                    ", "op_matrix", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", "=", "\"T\"", "\n", "", "elif", "l", "==", "1", ":", "\n", "                    ", "op_matrix", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", "=", "\"S\"", "\n", "", "elif", "l", "==", "2", ":", "\n", "                    ", "op_matrix", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", "=", "\"I\"", "\n", "", "else", ":", "\n", "                    ", "op_matrix", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", "=", "\"D\"", "\n", "# Return the matrices", "\n", "", "", "", "", "return", "cost_matrix", ",", "op_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils.get_cheapest_align_seq": [[61, 92], ["utils.align", "align_seq.reverse", "len", "len", "align_seq.append", "align_seq.append", "align_seq.append", "align_seq.append"], "function", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils.align"], ["", "def", "get_cheapest_align_seq", "(", "orig", ",", "cor", ")", ":", "\n", "    ", "_", ",", "op_matrix", "=", "align", "(", "orig", ",", "cor", ")", "\n", "i", "=", "len", "(", "op_matrix", ")", "-", "1", "\n", "j", "=", "len", "(", "op_matrix", "[", "0", "]", ")", "-", "1", "\n", "align_seq", "=", "[", "]", "\n", "# Work backwards from bottom right until we hit top left", "\n", "while", "i", "+", "j", "!=", "0", ":", "\n", "# Get the edit operation in the current cell", "\n", "        ", "op", "=", "op_matrix", "[", "i", "]", "[", "j", "]", "\n", "# Matches and substitutions", "\n", "if", "op", "in", "{", "\"M\"", ",", "\"S\"", "}", ":", "\n", "            ", "align_seq", ".", "append", "(", "(", "op", ",", "i", "-", "1", ",", "i", ",", "j", "-", "1", ",", "j", ")", ")", "\n", "i", "-=", "1", "\n", "j", "-=", "1", "\n", "# Deletions", "\n", "", "elif", "op", "==", "\"D\"", ":", "\n", "            ", "align_seq", ".", "append", "(", "(", "op", ",", "i", "-", "1", ",", "i", ",", "j", ",", "j", ")", ")", "\n", "i", "-=", "1", "\n", "# Insertions", "\n", "", "elif", "op", "==", "\"I\"", ":", "\n", "            ", "align_seq", ".", "append", "(", "(", "op", ",", "i", ",", "i", ",", "j", "-", "1", ",", "j", ")", ")", "\n", "j", "-=", "1", "\n", "# Transpositions", "\n", "", "else", ":", "\n", "# Get the size of the transposition", "\n", "            ", "align_seq", ".", "append", "(", "(", "op", ",", "i", "-", "1", ",", "i", ",", "j", "-", "1", ",", "j", ")", ")", "\n", "i", "-=", "1", "\n", "j", "-=", "1", "\n", "# Reverse the list to go from left to right and return", "\n", "", "", "align_seq", ".", "reverse", "(", ")", "\n", "return", "align_seq", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing": [[94, 121], ["numpy.sum", "len", "smoothed_unnormalized_probs.append", "numpy.isclose", "min", "range", "len", "len"], "function", ["None"], ["", "def", "_apply_smoothing", "(", "unnormalized_probs", ",", "alpha", ",", "beta", ")", ":", "\n", "    ", "'''\n    :param unnormalized_probs: list with (un-normalized) \"probability\" values for each class. This list can also contain counts instead of\n        probabilities, in this case, no alpha smoothing is done (as it would normalize the sum of these counts to 1).\n    :param alpha: multiplication factor for all probabilities (how greater / smaller they will be). Note that if probabilities sum up to 1,\n    :param beta: uniformity smoothing factor (if 1 new distribution is uniform, when 0 it is kept as is)\n    :return:\n    '''", "\n", "\n", "if", "len", "(", "unnormalized_probs", ")", "==", "0", ":", "\n", "        ", "return", "[", "]", "\n", "\n", "# first apply beta uniformity smoothing", "\n", "", "unnormalized_probs_sum", "=", "np", ".", "sum", "(", "unnormalized_probs", ")", "\n", "smoothed_unnormalized_probs", "=", "[", "]", "\n", "for", "unnormalized_prob", "in", "unnormalized_probs", ":", "\n", "        ", "smoothed_value", "=", "unnormalized_prob", "*", "(", "1", "-", "beta", ")", "+", "beta", "*", "(", "unnormalized_probs_sum", "/", "len", "(", "unnormalized_probs", ")", ")", "\n", "smoothed_unnormalized_probs", ".", "append", "(", "smoothed_value", ")", "\n", "\n", "# then apply alpha multiplication (and potential clipping)", "\n", "# do not apply it when the sum is already 1 (full distribution)", "\n", "", "if", "not", "np", ".", "isclose", "(", "unnormalized_probs_sum", ",", "1.", ")", ":", "\n", "        ", "multiplication_factor", "=", "min", "(", "alpha", ",", "1", "/", "(", "unnormalized_probs_sum", "+", "1e-6", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "smoothed_unnormalized_probs", ")", ")", ":", "\n", "            ", "smoothed_unnormalized_probs", "[", "i", "]", "*=", "multiplication_factor", "\n", "\n", "", "", "return", "smoothed_unnormalized_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing_on_simple_dict": [[123, 125], ["zip", "simple_dict.keys", "utils._apply_smoothing", "list", "simple_dict.values"], "function", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing"], ["", "def", "_apply_smoothing_on_simple_dict", "(", "simple_dict", ",", "alpha", ",", "beta", ")", ":", "\n", "    ", "return", "{", "k", ":", "v", "for", "k", ",", "v", "in", "zip", "(", "simple_dict", ".", "keys", "(", ")", ",", "_apply_smoothing", "(", "list", "(", "simple_dict", ".", "values", "(", ")", ")", ",", "alpha", ",", "beta", ")", ")", "}", "\n", "", ""]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.whitespace.Whitespace.__init__": [[8, 18], ["aspects.base.Aspect.__init__", "aspects.utils._apply_smoothing_on_simple_dict", "probs_whitespace_in_other.keys", "aspects.utils._apply_smoothing_on_simple_dict"], "methods", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation.__init__", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing_on_simple_dict", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing_on_simple_dict"], ["    ", "def", "__init__", "(", "self", ",", "profile", ",", "lang", ",", "alpha", "=", "1", ",", "beta", "=", "0", ")", ":", "\n", "        ", "super", "(", "Whitespace", ",", "self", ")", ".", "__init__", "(", "profile", ",", "alpha", ",", "beta", ")", "\n", "\n", "whitespace_errors_probs", "=", "profile", "[", "'whitespace'", "]", "[", "'whitespace_errors_probs'", "]", "\n", "self", ".", "whitespace_errors_probs", "=", "utils", ".", "_apply_smoothing_on_simple_dict", "(", "whitespace_errors_probs", ",", "alpha", ",", "beta", ")", "\n", "\n", "probs_whitespace_in_other", "=", "profile", "[", "'whitespace'", "]", "[", "'probs_whitespace_in_other'", "]", "\n", "self", ".", "probs_whitespace_in_other", "=", "{", "}", "\n", "for", "k", "in", "probs_whitespace_in_other", ".", "keys", "(", ")", ":", "\n", "            ", "self", ".", "probs_whitespace_in_other", "[", "k", "]", "=", "utils", ".", "_apply_smoothing_on_simple_dict", "(", "probs_whitespace_in_other", "[", "k", "]", ",", "alpha", ",", "beta", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.whitespace.Whitespace.apply": [[19, 147], ["text.split", "len", "word.isalpha", "new_text.append", "numpy.random.randint", "new_text.append", "changes.append", "len", "isinstance", "range", "len", "numpy.random.uniform", "len", "whitespace_info.insert", "word.isalpha", "text_words[].isalpha", "new_text.append", "changes.append", "len", "whitespace_info.insert", "len", "numpy.random.uniform", "word.isalpha", "text_words[].isalpha", "range", "range", "new_text.append", "changes.append", "new_text.append", "len", "numpy.random.uniform", "text_words[].isalpha", "numpy.array", "numpy.sum", "numpy.random.choice", "map", "range", "len", "len", "whitespace.Whitespace.probs_whitespace_in_other.items", "list", "list", "list", "numpy.random.choice.split", "len", "numpy.random.randint", "int", "this_word_whitespace_probs_flattened.values", "this_word_whitespace_probs_flattened.values", "this_word_whitespace_probs_flattened.keys", "len", "range", "len"], "methods", ["None"], ["", "", "def", "apply", "(", "self", ",", "text", ",", "whitespace_info", ")", ":", "\n", "        ", "changes", "=", "[", "]", "\n", "new_text", "=", "[", "]", "\n", "text_words", "=", "text", ".", "split", "(", "' '", ")", "\n", "word_ind", "=", "0", "\n", "while", "True", ":", "\n", "            ", "if", "word_ind", ">=", "len", "(", "text_words", ")", ":", "\n", "                ", "break", "\n", "\n", "", "word", "=", "text_words", "[", "word_ind", "]", "\n", "\n", "if", "not", "word", ".", "isalpha", "(", ")", ":", "\n", "                ", "new_text", ".", "append", "(", "word", ")", "\n", "word_ind", "+=", "1", "\n", "continue", "\n", "\n", "", "if", "len", "(", "word", ")", ">=", "2", "and", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "self", ".", "whitespace_errors_probs", "[", "'insert'", "]", ":", "\n", "# insert whitespace", "\n", "                ", "sep_index", "=", "np", ".", "random", ".", "randint", "(", "1", ",", "len", "(", "word", ")", ")", "\n", "new_text", ".", "append", "(", "word", "[", ":", "sep_index", "]", "+", "\" \"", "+", "word", "[", "sep_index", ":", "]", ")", "\n", "\n", "# if this is a last word, we must handle it differently", "\n", "if", "word_ind", "==", "len", "(", "text_words", ")", "-", "1", ":", "\n", "                    ", "whitespace_info", ".", "insert", "(", "word_ind", ",", "True", ")", "# word_ind==len(whitespace_info)", "\n", "", "else", ":", "\n", "                    ", "whitespace_info", "[", "word_ind", "]", "=", "[", "'I'", ",", "True", ",", "1", ",", "whitespace_info", "[", "word_ind", "]", "]", "\n", "", "word_ind", "+=", "1", "\n", "changes", ".", "append", "(", "[", "'WHITESPACE'", ",", "\"insert: {}\"", ".", "format", "(", "new_text", "[", "-", "1", "]", ")", "]", ")", "\n", "", "elif", "word_ind", "<", "len", "(", "text_words", ")", "-", "1", "and", "word", ".", "isalpha", "(", ")", "and", "text_words", "[", "word_ind", "+", "1", "]", ".", "isalpha", "(", ")", "and", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "self", ".", "whitespace_errors_probs", "[", "'delete'", "]", ":", "\n", "# delete", "\n", "                ", "new_text", ".", "append", "(", "word", "+", "text_words", "[", "word_ind", "+", "1", "]", ")", "\n", "whitespace_info", "[", "word_ind", "]", "=", "'D'", "\n", "word_ind", "+=", "2", "\n", "changes", ".", "append", "(", "[", "'WHITESPACE'", ",", "\"delete: {}\"", ".", "format", "(", "new_text", "[", "-", "1", "]", ")", "]", ")", "\n", "", "elif", "word_ind", "<", "len", "(", "text_words", ")", "-", "1", "and", "word", ".", "isalpha", "(", ")", "and", "text_words", "[", "word_ind", "+", "1", "]", ".", "isalpha", "(", ")", "and", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "self", ".", "whitespace_errors_probs", "[", "'other'", "]", ":", "\n", "# remove spaces between multiple following tokens and insert some spaces at random", "\n", "\n", "# check alpha adjacent (we already checked that there are at least two of them)", "\n", "                ", "max_applicability", "=", "2", "\n", "for", "token_right_plus", "in", "range", "(", "2", ",", "len", "(", "text_words", ")", "-", "word_ind", ")", ":", "\n", "                    ", "if", "text_words", "[", "word_ind", "+", "token_right_plus", "]", ".", "isalpha", "(", ")", ":", "\n", "                        ", "max_applicability", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "break", "\n", "\n", "", "", "this_word_whitespace_probs", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "probs_whitespace_in_other", ".", "items", "(", ")", "if", "int", "(", "k", ")", "<=", "max_applicability", "}", "\n", "\n", "this_word_whitespace_probs_flattened", "=", "{", "}", "# { [num_space_in_cor, num_spaces_in_orig] = prob, ...}", "\n", "for", "num_spaces_in_cor", "in", "this_word_whitespace_probs", ":", "\n", "                    ", "for", "num_spaces_in_orig", "in", "this_word_whitespace_probs", "[", "num_spaces_in_cor", "]", ":", "\n", "                        ", "this_word_whitespace_probs_flattened", "[", "num_spaces_in_cor", "+", "\"-\"", "+", "num_spaces_in_orig", "]", "=", "this_word_whitespace_probs", "[", "num_spaces_in_cor", "]", "[", "num_spaces_in_orig", "]", "\n", "\n", "", "", "this_word_whitespace_probs_flattened_normalized", "=", "np", ".", "array", "(", "list", "(", "this_word_whitespace_probs_flattened", ".", "values", "(", ")", ")", ")", "/", "np", ".", "sum", "(", "\n", "list", "(", "this_word_whitespace_probs_flattened", ".", "values", "(", ")", ")", ")", "\n", "\n", "# select how many words to take from corrected and to how many words to transform them", "\n", "# while-cycle is to make sure that we do not select single token with single character in the corrected text", "\n", "while", "True", ":", "\n", "                    ", "num_words_to_take_tuple", "=", "np", ".", "random", ".", "choice", "(", "list", "(", "this_word_whitespace_probs_flattened", ".", "keys", "(", ")", ")", ",", "\n", "p", "=", "this_word_whitespace_probs_flattened_normalized", ")", "\n", "\n", "num_spaces_in_cor", ",", "num_spaces_in_orig", "=", "map", "(", "int", ",", "num_words_to_take_tuple", ".", "split", "(", "'-'", ")", ")", "\n", "no_space_cor", "=", "\"\"", ".", "join", "(", "text_words", "[", "word_ind", ":", "word_ind", "+", "num_spaces_in_cor", "+", "1", "]", ")", "\n", "if", "len", "(", "no_space_cor", ")", ">", "2", ":", "\n", "                        ", "break", "\n", "\n", "# insert spaces on random, but be sure, that it is not the first, last or next to a whitespace", "\n", "", "", "for", "_", "in", "range", "(", "num_spaces_in_orig", "-", "1", ")", ":", "\n", "                    ", "while", "True", ":", "\n", "                        ", "index_to_insert_space", "=", "np", ".", "random", ".", "randint", "(", "1", ",", "len", "(", "no_space_cor", ")", ")", "\n", "if", "no_space_cor", "[", "index_to_insert_space", "-", "1", "]", "!=", "' '", "and", "no_space_cor", "[", "index_to_insert_space", "]", "!=", "' '", ":", "\n", "                            ", "break", "\n", "", "else", ":", "\n", "# check if there are still two adjacent non-space characters", "\n", "                            ", "insert_space_possible", "=", "False", "\n", "for", "j", "in", "range", "(", "len", "(", "no_space_cor", ")", "-", "1", ")", ":", "\n", "                                ", "if", "no_space_cor", "[", "j", "]", "!=", "' '", "and", "no_space_cor", "[", "j", "+", "1", "]", "!=", "' '", ":", "\n", "                                    ", "insert_space_possible", "=", "True", "\n", "break", "\n", "\n", "", "", "if", "not", "insert_space_possible", ":", "\n", "                                ", "index_to_insert_space", "=", "None", "\n", "break", "\n", "\n", "", "", "", "if", "not", "index_to_insert_space", ":", "\n", "                        ", "break", "\n", "\n", "", "no_space_cor", "=", "no_space_cor", "[", ":", "index_to_insert_space", "]", "+", "\" \"", "+", "no_space_cor", "[", "index_to_insert_space", ":", "]", "\n", "\n", "", "new_text", ".", "append", "(", "no_space_cor", ")", "\n", "if", "num_spaces_in_cor", ">", "num_spaces_in_orig", ":", "\n", "# new text has (num_spaces_in_cor -  num_spaces_in_orig) less tokens", "\n", "                    ", "for", "j", "in", "range", "(", "num_spaces_in_cor", "-", "num_spaces_in_orig", ")", ":", "\n", "                        ", "whitespace_info", "[", "word_ind", "+", "j", "]", "=", "'D'", "\n", "", "", "elif", "num_spaces_in_cor", "<", "num_spaces_in_orig", ":", "\n", "# new text has (num_spaces_in_orig - num_spaces_in_cor) more tokens", "\n", "                    ", "whitespace_info", "[", "word_ind", "]", "=", "[", "'I'", ",", "True", ",", "num_spaces_in_orig", "-", "num_spaces_in_cor", ",", "True", "]", "\n", "", "changes", ".", "append", "(", "\n", "[", "'WHITESPACE'", ",", "\"other: {} -> {}\"", ".", "format", "(", "\" \"", ".", "join", "(", "text_words", "[", "word_ind", ":", "word_ind", "+", "num_spaces_in_cor", "]", ")", ",", "new_text", "[", "-", "1", "]", ")", "]", ")", "\n", "\n", "word_ind", "+=", "num_spaces_in_cor", "\n", "\n", "", "else", ":", "\n", "                ", "new_text", ".", "append", "(", "word", ")", "\n", "word_ind", "+=", "1", "\n", "\n", "# process whitespace_info", "\n", "## first deletes", "\n", "", "", "whitespace_info", "=", "[", "x", "for", "x", "in", "whitespace_info", "if", "x", "!=", "'D'", "]", "\n", "##then inserts", "\n", "i", "=", "0", "\n", "while", "True", ":", "\n", "            ", "if", "i", ">=", "len", "(", "whitespace_info", ")", ":", "\n", "                ", "break", "\n", "\n", "", "if", "isinstance", "(", "whitespace_info", "[", "i", "]", ",", "list", ")", "and", "len", "(", "whitespace_info", "[", "i", "]", ")", "==", "4", ":", "\n", "                ", "_", ",", "insert_type", ",", "count", ",", "cur_type", "=", "whitespace_info", "[", "i", "]", "\n", "whitespace_info", "[", "i", "]", "=", "cur_type", "\n", "\n", "for", "_", "in", "range", "(", "count", ")", ":", "\n", "                    ", "whitespace_info", ".", "insert", "(", "i", ",", "insert_type", ")", "\n", "\n", "", "", "i", "+=", "1", "\n", "\n", "", "return", "\" \"", ".", "join", "(", "new_text", ")", ",", "changes", ",", "whitespace_info", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.whitespace.Whitespace.estimate_probabilities": [[148, 220], ["aspects.apply_m2_edits.processM2", "sum", "sum", "list", "coder_dict.keys", "x.isalpha", "enumerate", "x.isalpha", "orig_sent[].isalpha"], "methods", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.apply_m2_edits.processM2"], ["", "@", "staticmethod", "\n", "def", "estimate_probabilities", "(", "m2_records", ")", ":", "\n", "# TODO now we allow to delete space only between two alpha words, which may not be the best method", "\n", "\n", "        ", "whitespace_errors", "=", "{", "\n", "'insert'", ":", "0", ",", "# a single space must be inserted to make the sentence correct", "\n", "'delete'", ":", "0", ",", "# a single space must be deleted to make the sentence correct", "\n", "'other'", ":", "0", "# some spaces must be deleted and some inserted to make the sentence correct", "\n", "}", "\n", "\n", "'''\n        When other category is detected, multiple source tokens must be transformed into multiple output tokens. In this category, we collect\n        statistics on number of tokens in source and in output (correction).\n        '''", "\n", "stats_whitespaces_in_other", "=", "{", "}", "\n", "\n", "num_all_alpha_words", "=", "0", "# all words whose casing could be changed", "\n", "num_all_alpha_neighboring_pairs", "=", "0", "# number of adjacent words where both words are alpha", "\n", "\n", "for", "m2_file", "in", "m2_records", ":", "\n", "\n", "            ", "for", "info", "in", "m2_file", ":", "\n", "                ", "orig_sent", ",", "coder_dict", "=", "apply_m2_edits", ".", "processM2", "(", "info", ",", "[", "]", ")", "\n", "\n", "if", "coder_dict", ":", "\n", "                    ", "coder_id", "=", "list", "(", "coder_dict", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "cor_sent", "=", "coder_dict", "[", "coder_id", "]", "[", "0", "]", "\n", "\n", "for", "edit", "in", "coder_dict", "[", "coder_id", "]", "[", "1", "]", ":", "\n", "                        ", "orig_start", ",", "orig_end", ",", "error_type", ",", "cor_tok", ",", "cor_start", ",", "cor_end", "=", "edit", "\n", "\n", "if", "'ORTH:WSPACE'", "in", "error_type", "or", "(", "\n", "'ORTH'", "in", "error_type", "and", "\"\"", ".", "join", "(", "orig_sent", "[", "orig_start", ":", "orig_end", "]", ")", "==", "\"\"", ".", "join", "(", "\n", "cor_sent", "[", "cor_start", ":", "cor_end", "]", ")", ")", ":", "\n", "                            ", "if", "orig_end", "-", "orig_start", "==", "1", ":", "\n", "                                ", "whitespace_errors", "[", "'insert'", "]", "+=", "1", "\n", "", "elif", "cor_end", "-", "cor_start", "==", "1", ":", "\n", "                                ", "whitespace_errors", "[", "'delete'", "]", "+=", "1", "\n", "", "else", ":", "\n", "                                ", "whitespace_errors", "[", "'other'", "]", "+=", "1", "\n", "\n", "num_spaces_in_orig", "=", "orig_end", "-", "orig_start", "-", "1", "\n", "num_spaces_in_cor", "=", "cor_end", "-", "cor_start", "-", "1", "\n", "if", "num_spaces_in_cor", "not", "in", "stats_whitespaces_in_other", ":", "\n", "                                    ", "stats_whitespaces_in_other", "[", "num_spaces_in_cor", "]", "=", "{", "}", "\n", "\n", "", "if", "num_spaces_in_orig", "not", "in", "stats_whitespaces_in_other", "[", "num_spaces_in_cor", "]", ":", "\n", "                                    ", "stats_whitespaces_in_other", "[", "num_spaces_in_cor", "]", "[", "num_spaces_in_orig", "]", "=", "0", "\n", "\n", "", "stats_whitespaces_in_other", "[", "num_spaces_in_cor", "]", "[", "num_spaces_in_orig", "]", "+=", "1", "\n", "\n", "", "", "", "num_all_alpha_words", "+=", "sum", "(", "[", "1", "for", "x", "in", "orig_sent", "if", "x", ".", "isalpha", "(", ")", "]", ")", "\n", "num_all_alpha_neighboring_pairs", "+=", "sum", "(", "\n", "[", "1", "for", "i", ",", "x", "in", "enumerate", "(", "orig_sent", ")", "if", "i", ">", "1", "and", "x", ".", "isalpha", "(", ")", "and", "orig_sent", "[", "i", "-", "1", "]", ".", "isalpha", "(", ")", "]", ")", "\n", "\n", "# ! NOTE that we are already switching insert and delete to be used in the direction corrected -> original", "\n", "", "", "", "whitespace_errors_probs", "=", "{", "\n", "'delete'", ":", "whitespace_errors", "[", "'insert'", "]", "/", "num_all_alpha_words", ",", "\n", "'insert'", ":", "whitespace_errors", "[", "'delete'", "]", "/", "num_all_alpha_neighboring_pairs", ",", "\n", "'other'", ":", "whitespace_errors", "[", "'other'", "]", "/", "num_all_alpha_neighboring_pairs", ",", "# these are mostly two-tuples (approximation)", "\n", "}", "\n", "\n", "probs_whitespace_in_other", "=", "{", "}", "\n", "for", "num_spaces_in_cor", "in", "stats_whitespaces_in_other", ":", "\n", "            ", "probs_whitespace_in_other", "[", "num_spaces_in_cor", "]", "=", "{", "}", "\n", "for", "num_spaces_in_orig", "in", "stats_whitespaces_in_other", "[", "num_spaces_in_cor", "]", ":", "\n", "                ", "probs_whitespace_in_other", "[", "num_spaces_in_cor", "]", "[", "num_spaces_in_orig", "]", "=", "stats_whitespaces_in_other", "[", "num_spaces_in_cor", "]", "[", "\n", "num_spaces_in_orig", "]", "/", "whitespace_errors", "[", "'other'", "]", "\n", "\n", "", "", "return", "'whitespace'", ",", "{", "\n", "'whitespace_errors_probs'", ":", "whitespace_errors_probs", ",", "\n", "'probs_whitespace_in_other'", ":", "probs_whitespace_in_other", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation.__init__": [[13, 45], ["aspects.base.Aspect.__init__", "aspects.utils._apply_smoothing_on_simple_dict", "aspects.utils._apply_smoothing_on_simple_dict", "aspects.utils._apply_smoothing", "aspects.utils._apply_smoothing_on_simple_dict", "aspects.utils._apply_smoothing"], "methods", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation.__init__", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing_on_simple_dict", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing_on_simple_dict", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing_on_simple_dict", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.utils._apply_smoothing"], ["    ", "def", "__init__", "(", "self", ",", "profile", ",", "lang", ",", "alpha", "=", "1", ",", "beta", "=", "0", ")", ":", "\n", "        ", "super", "(", "Punctuation", ",", "self", ")", ".", "__init__", "(", "profile", ",", "alpha", ",", "beta", ")", "\n", "\n", "punct_errors_aggregated_probs", "=", "profile", "[", "'punctuation'", "]", "[", "'punct_errors_aggregated_probs'", "]", "\n", "self", ".", "punct_errors_aggregated_probs", "=", "{", "}", "\n", "for", "k", "in", "punct_errors_aggregated_probs", ":", "\n", "            ", "self", ".", "punct_errors_aggregated_probs", "[", "k", "]", "=", "{", "}", "\n", "\n", "self", ".", "punct_errors_aggregated_probs", "[", "k", "]", "[", "'I'", "]", "=", "utils", ".", "_apply_smoothing", "(", "[", "punct_errors_aggregated_probs", "[", "k", "]", "[", "'I'", "]", "]", ",", "alpha", ",", "\n", "beta", ")", "[", "0", "]", "\n", "\n", "self", ".", "punct_errors_aggregated_probs", "[", "k", "]", "[", "'S'", "]", "=", "utils", ".", "_apply_smoothing_on_simple_dict", "(", "punct_errors_aggregated_probs", "[", "k", "]", "[", "'S'", "]", ",", "\n", "alpha", ",", "beta", ")", "\n", "\n", "", "punct_errors_detailed_probs", "=", "profile", "[", "'punctuation'", "]", "[", "'punct_errors_detailed_probs'", "]", "\n", "self", ".", "punct_errors_detailed_probs", "=", "{", "}", "\n", "for", "k", "in", "punct_errors_detailed_probs", ":", "\n", "            ", "self", ".", "punct_errors_detailed_probs", "[", "k", "]", "=", "{", "}", "\n", "\n", "self", ".", "punct_errors_detailed_probs", "[", "k", "]", "[", "'I'", "]", "=", "utils", ".", "_apply_smoothing_on_simple_dict", "(", "punct_errors_detailed_probs", "[", "k", "]", "[", "'I'", "]", ",", "\n", "alpha", ",", "beta", ")", "\n", "self", ".", "punct_errors_detailed_probs", "[", "k", "]", "[", "'D'", "]", "=", "{", "}", "\n", "for", "kk", "in", "punct_errors_detailed_probs", "[", "k", "]", "[", "'D'", "]", ":", "\n", "                ", "self", ".", "punct_errors_detailed_probs", "[", "k", "]", "[", "'D'", "]", "[", "kk", "]", "=", "utils", ".", "_apply_smoothing", "(", "[", "punct_errors_detailed_probs", "[", "k", "]", "[", "'D'", "]", "[", "kk", "]", "]", ",", "alpha", ",", "beta", ")", "[", "0", "]", "\n", "\n", "", "self", ".", "punct_errors_detailed_probs", "[", "k", "]", "[", "'S'", "]", "=", "{", "}", "\n", "for", "kk", "in", "punct_errors_detailed_probs", "[", "k", "]", "[", "'S'", "]", ":", "\n", "                ", "self", ".", "punct_errors_detailed_probs", "[", "k", "]", "[", "'S'", "]", "[", "kk", "]", "=", "utils", ".", "_apply_smoothing_on_simple_dict", "(", "\n", "punct_errors_detailed_probs", "[", "k", "]", "[", "'S'", "]", "[", "kk", "]", ",", "alpha", ",", "beta", ")", "\n", "\n", "", "", "self", ".", "final_punctuation_marks", "=", "[", "'.'", ",", "'!'", ",", "'?'", "]", "+", "[", "\"\\\"\"", ",", "\"\u201e\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation.apply": [[46, 162], ["text.split", "len", "text.split.copy", "enumerate", "sum", "numpy.random.uniform", "numpy.random.choice", "punct_token.replace.replace.replace", "changes.append", "len", "list", "changes.append", "x.isalpha", "[].keys", "list", "whitespace_info.insert", "whitespace_info.insert", "whitespace_info.insert", "whitespace_info.insert", "numpy.random.uniform", "numpy.random.choice", "changes.append", "[].values", "numpy.random.uniform", "[].upper", "token[].upper", "[].lower", "numpy.random.uniform", "list", "[].keys", "list", "[].lower", "[].values", "[].upper", "len"], "methods", ["None"], ["", "def", "apply", "(", "self", ",", "text", ",", "whitespace_info", ")", ":", "\n", "\n", "        ", "'''\n            Most of the punctuation tokens (in Czech) appends to the previous token (e.g. dot, question mark, colon).\n            The following list stores those characters that in contrary append to the following token.\n            '''", "\n", "punct_tokens_that_append_to_next_token", "=", "[", "'\u201e'", ",", "'('", ",", "'{'", ",", "'['", "]", "\n", "\n", "changes", "=", "[", "]", "\n", "original_text_splitted_into_tokens", "=", "text", ".", "split", "(", "' '", ")", "\n", "num_tokens_in_original_text", "=", "len", "(", "original_text_splitted_into_tokens", ")", "\n", "\n", "new_text", "=", "original_text_splitted_into_tokens", ".", "copy", "(", ")", "\n", "whitespace_ind_difference", "=", "0", "\n", "\n", "delete_applicable", "=", "sum", "(", "[", "1", "if", "x", ".", "isalpha", "(", ")", "else", "0", "for", "x", "in", "new_text", "]", ")", ">", "0", "# apply delete punctuation if sure that whole text is not deleted", "\n", "for", "token_ind", ",", "token", "in", "enumerate", "(", "original_text_splitted_into_tokens", ")", ":", "\n", "            ", "if", "token_ind", "==", "len", "(", "text", ")", "-", "1", ":", "\n", "                ", "applicability_place", "=", "'eos'", "\n", "", "else", ":", "\n", "                ", "applicability_place", "=", "'middle'", "\n", "\n", "# Insert", "\n", "", "if", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "self", ".", "punct_errors_aggregated_probs", "[", "applicability_place", "]", "[", "'I'", "]", ":", "\n", "# select one of punctuation-tokens according to its distribution", "\n", "                ", "punct_token", "=", "np", ".", "random", ".", "choice", "(", "list", "(", "self", ".", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "'I'", "]", ".", "keys", "(", ")", ")", ",", "\n", "p", "=", "list", "(", "self", ".", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "'I'", "]", ".", "values", "(", ")", ")", ")", "\n", "\n", "punct_token", "=", "punct_token", ".", "replace", "(", "\" \"", ",", "\"\"", ")", "\n", "\n", "# do not insert anything before the first token (and do not insert anything after the last token)", "\n", "if", "(", "token_ind", "==", "0", "and", "num_tokens_in_original_text", ">", "1", ")", "or", "(", "\n", "(", "token_ind", "!=", "num_tokens_in_original_text", "-", "1", ")", "and", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "0.5", ")", ":", "\n", "                    ", "new_text", "[", "\n", "token_ind", "]", "=", "token", "+", "\" \"", "+", "punct_token", "\n", "\n", "# if are about to insert \"final-punctuation\" token, we need to upper-case the following token", "\n", "if", "punct_token", "in", "self", ".", "final_punctuation_marks", ":", "\n", "                        ", "new_text", "[", "token_ind", "+", "1", "]", "=", "new_text", "[", "token_ind", "+", "1", "]", "[", "0", "]", ".", "upper", "(", ")", "+", "new_text", "[", "token_ind", "+", "1", "]", "[", "1", ":", "]", "\n", "\n", "", "if", "punct_token", "in", "punct_tokens_that_append_to_next_token", ":", "\n", "                        ", "whitespace_info", ".", "insert", "(", "token_ind", "+", "1", "+", "whitespace_ind_difference", ",", "\n", "False", ")", "# ] = ['I', False, whitespace_info[token_ind + 1]] # .insert(token_ind + 1, False)", "\n", "", "else", ":", "\n", "                        ", "whitespace_info", ".", "insert", "(", "token_ind", "+", "whitespace_ind_difference", ",", "\n", "False", ")", "# ] = ['I', False, whitespace_info[token_ind]] # .insert(token_ind, False)", "\n", "\n", "", "", "else", ":", "\n", "                    ", "new_text", "[", "token_ind", "]", "=", "punct_token", "+", "\" \"", "\n", "\n", "if", "punct_token", "in", "self", ".", "final_punctuation_marks", ":", "\n", "                        ", "new_text", "[", "token_ind", "]", "+=", "token", "[", "0", "]", ".", "upper", "(", ")", "+", "token", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "                        ", "new_text", "[", "token_ind", "]", "+=", "token", "\n", "\n", "", "if", "punct_token", "in", "punct_tokens_that_append_to_next_token", ":", "\n", "                        ", "whitespace_info", ".", "insert", "(", "token_ind", "+", "whitespace_ind_difference", ",", "\n", "False", ")", "# ] = ['I', False, whitespace_info[token_ind]] # .insert(token_ind, False)", "\n", "", "else", ":", "\n", "                        ", "whitespace_info", ".", "insert", "(", "token_ind", "-", "1", "+", "whitespace_ind_difference", ",", "\n", "False", ")", "# ] = ['I', False, whitespace_info[token_ind - 1]] # .insert(token_ind - 1, False)", "\n", "\n", "", "", "whitespace_ind_difference", "+=", "1", "\n", "changes", ".", "append", "(", "[", "'PUNCT'", ",", "'insert {} around {}'", ".", "format", "(", "punct_token", ",", "token", ")", "]", ")", "\n", "\n", "# Delete", "\n", "", "elif", "delete_applicable", "and", "token", "in", "self", ".", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "'D'", "]", "and", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "self", ".", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "'D'", "]", "[", "token", "]", ":", "\n", "# if we are about to delete a \"final-punctuation\" token, we need to lower-case the following letter", "\n", "                ", "if", "token", "in", "self", ".", "final_punctuation_marks", "and", "token_ind", "<", "num_tokens_in_original_text", "-", "1", ":", "\n", "                    ", "new_text", "[", "token_ind", "+", "1", "]", "=", "new_text", "[", "token_ind", "+", "1", "]", "[", "0", "]", ".", "lower", "(", ")", "+", "new_text", "[", "token_ind", "+", "1", "]", "[", "1", ":", "]", "\n", "\n", "", "new_text", "[", "token_ind", "]", "=", "''", "\n", "\n", "if", "token", "in", "punct_tokens_that_append_to_next_token", "and", "token_ind", "<", "num_tokens_in_original_text", "-", "1", ":", "\n", "                    ", "del", "whitespace_info", "[", "token_ind", "+", "whitespace_ind_difference", "]", "\n", "", "else", ":", "\n", "                    ", "del", "whitespace_info", "[", "token_ind", "-", "1", "+", "whitespace_ind_difference", "]", "\n", "\n", "", "whitespace_ind_difference", "-=", "1", "\n", "changes", ".", "append", "(", "\n", "[", "'PUNCT'", ",", "\n", "'delete {} around {}'", ".", "format", "(", "token", ",", "\" \"", ".", "join", "(", "original_text_splitted_into_tokens", "[", "token_ind", "-", "4", ":", "token_ind", "+", "4", "]", ")", ")", "]", ")", "\n", "\n", "# Substitute", "\n", "", "elif", "token", "in", "self", ".", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "'S'", "]", "and", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "self", ".", "punct_errors_aggregated_probs", "[", "applicability_place", "]", "[", "'S'", "]", "[", "token", "]", ":", "\n", "                ", "replace_token", "=", "np", ".", "random", ".", "choice", "(", "list", "(", "self", ".", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "'S'", "]", "[", "token", "]", ".", "keys", "(", ")", ")", ",", "\n", "p", "=", "list", "(", "self", ".", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "'S'", "]", "[", "token", "]", ".", "values", "(", ")", ")", ")", "\n", "\n", "new_text", "[", "token_ind", "]", "=", "replace_token", "\n", "\n", "# if we substituted a \"final-punctuation\" token into \"non-final-punctuation\", or vica versa, we need to change casing of the following letter", "\n", "if", "token_ind", ">=", "num_tokens_in_original_text", "-", "1", ":", "\n", "                    ", "break", "\n", "\n", "", "if", "token", "in", "self", ".", "final_punctuation_marks", "and", "replace_token", "not", "in", "self", ".", "final_punctuation_marks", ":", "\n", "                    ", "new_text", "[", "token_ind", "+", "1", "]", "=", "new_text", "[", "token_ind", "+", "1", "]", "[", "0", "]", ".", "lower", "(", ")", "+", "new_text", "[", "token_ind", "+", "1", "]", "[", "1", ":", "]", "\n", "", "elif", "replace_token", "in", "self", ".", "final_punctuation_marks", "and", "token", "not", "in", "self", ".", "final_punctuation_marks", ":", "\n", "                    ", "new_text", "[", "token_ind", "+", "1", "]", "=", "new_text", "[", "token_ind", "+", "1", "]", "[", "0", "]", ".", "upper", "(", ")", "+", "new_text", "[", "token_ind", "+", "1", "]", "[", "1", ":", "]", "\n", "\n", "# if we substituted \"normal\" punctuation token (that appends to the token on the left) with one of punct_tokens_that_append_to_next_token,", "\n", "# we need to swap two adjacent info in whitespace_info (or vica versa)", "\n", "", "if", "(", "token", "in", "punct_tokens_that_append_to_next_token", "and", "replace_token", "not", "in", "punct_tokens_that_append_to_next_token", ")", "or", "(", "\n", "token", "not", "in", "punct_tokens_that_append_to_next_token", "and", "replace_token", "in", "punct_tokens_that_append_to_next_token", ")", ":", "\n", "                    ", "if", "token_ind", ">", "0", ":", "\n", "                        ", "whitespace_info", "[", "token_ind", "-", "1", "+", "whitespace_ind_difference", "]", "=", "not", "whitespace_info", "[", "\n", "token_ind", "-", "1", "+", "whitespace_ind_difference", "]", "\n", "\n", "", "if", "token_ind", "<", "len", "(", "original_text_splitted_into_tokens", ")", "-", "1", ":", "\n", "                        ", "whitespace_info", "[", "token_ind", "+", "whitespace_ind_difference", "]", "=", "not", "whitespace_info", "[", "\n", "token_ind", "+", "whitespace_ind_difference", "]", "\n", "\n", "", "", "changes", ".", "append", "(", "[", "'PUNCT'", ",", "'replace {} with {}'", ".", "format", "(", "token", ",", "replace_token", ")", "]", ")", "\n", "\n", "", "", "return", "\" \"", ".", "join", "(", "[", "x", "for", "x", "in", "new_text", "if", "x", "]", ")", ",", "changes", ",", "whitespace_info", "# ignore empty (deleted) tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation._get_punctuation_probabilities": [[163, 281], ["punct_errors.items", "punct_errors[].items", "numpy.sum", "tok[].split", "list", "numpy.sum", "collections.Counter", "list", "len", "[].values", "list", "list", "tok[].split", "coder_dict.keys", "[].values", "map", "collections.Counter"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_punctuation_probabilities", "(", "punct_errors", ",", "coder_dicts", ")", ":", "\n", "        ", "punct_errors_chars_only", "=", "{", "}", "\n", "for", "applicability_place", ",", "v", "in", "punct_errors", ".", "items", "(", ")", ":", "\n", "            ", "punct_errors_chars_only", "[", "applicability_place", "]", "=", "{", "}", "\n", "for", "op_type", ",", "v1", "in", "punct_errors", "[", "applicability_place", "]", ".", "items", "(", ")", ":", "\n", "                ", "if", "op_type", "==", "'I'", "or", "op_type", "==", "'D'", ":", "\n", "                    ", "punct_errors_chars_only", "[", "applicability_place", "]", "[", "op_type", "]", "=", "Counter", "(", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "0", "]", ",", "v1", ")", ")", ")", "\n", "", "else", ":", "# S", "\n", "                    ", "punct_errors_chars_only", "[", "applicability_place", "]", "[", "op_type", "]", "=", "{", "}", "\n", "for", "tok", "in", "v1", ":", "\n", "                        ", "token_from", ",", "token_to", "=", "tok", "[", "0", "]", ".", "split", "(", "'SEPARATOR'", ")", "\n", "if", "token_from", "not", "in", "punct_errors_chars_only", "[", "applicability_place", "]", "[", "op_type", "]", ":", "\n", "                            ", "punct_errors_chars_only", "[", "applicability_place", "]", "[", "op_type", "]", "[", "token_from", "]", "=", "Counter", "(", ")", "\n", "\n", "", "punct_errors_chars_only", "[", "applicability_place", "]", "[", "op_type", "]", "[", "token_from", "]", "[", "token_to", "]", "+=", "1", "\n", "\n", "# # verbose dump", "\n", "# for applicability_place, v in punct_errors.items():", "\n", "#     print(applicability_place)", "\n", "#     for op_type, v1 in punct_errors[applicability_place].items():", "\n", "#         print(op_type)", "\n", "#         if op_type == 'I' or op_type == 'D':", "\n", "#             for k, v in punct_errors_chars_only[applicability_place][op_type].most_common():", "\n", "#                 print(k, v)", "\n", "#         else:", "\n", "#             for token_from in punct_errors_chars_only[applicability_place][op_type]:", "\n", "#                 print(token_from)", "\n", "#                 for k,v in punct_errors_chars_only[applicability_place][op_type][token_from].most_common():", "\n", "#                     print(\"   -> {} {}\".format(k,v))", "\n", "\n", "\n", "", "", "", "", "num_applicable", "=", "{", "\n", "'eos'", ":", "{", "\n", "'I'", ":", "0", ",", "# num_para", "\n", "'D'", ":", "{", "}", ",", "\n", "'S'", ":", "{", "}", "\n", "}", ",", "\n", "'middle'", ":", "{", "\n", "'I'", ":", "0", ",", "# num_tokens - 1 (last one is separated)", "\n", "'D'", ":", "{", "}", ",", "\n", "'S'", ":", "{", "}", "\n", "}", ",", "\n", "}", "\n", "\n", "for", "coder_dict", "in", "coder_dicts", ":", "# coder dict for each corrected sentence (paragraph)", "\n", "            ", "if", "coder_dict", ":", "\n", "                ", "coder_id", "=", "list", "(", "coder_dict", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "cor_sent", "=", "coder_dict", "[", "coder_id", "]", "[", "0", "]", "\n", "\n", "num_applicable", "[", "'eos'", "]", "[", "'I'", "]", "+=", "1", "\n", "num_applicable", "[", "'middle'", "]", "[", "'I'", "]", "+=", "len", "(", "cor_sent", ")", "-", "1", "# -1 as we do not apply it on the last token", "\n", "\n", "for", "tok", "in", "cor_sent", ":", "\n", "                    ", "for", "applicability_place", "in", "punct_errors_chars_only", ":", "# for each applicability type", "\n", "                        ", "for", "op_type", "in", "[", "'S'", ",", "'D'", "]", ":", "\n", "                            ", "if", "tok", "in", "punct_errors_chars_only", "[", "applicability_place", "]", "[", "op_type", "]", ":", "\n", "                                ", "if", "tok", "not", "in", "num_applicable", "[", "applicability_place", "]", "[", "op_type", "]", ":", "\n", "                                    ", "num_applicable", "[", "applicability_place", "]", "[", "op_type", "]", "[", "tok", "]", "=", "0", "\n", "\n", "", "num_applicable", "[", "applicability_place", "]", "[", "op_type", "]", "[", "tok", "]", "+=", "1", "\n", "\n", "# create detailed (up to [applicability_place][op_type][token_from][token_to]) probability dictionary", "\n", "", "", "", "", "", "", "punct_errors_detailed_probs", "=", "{", "}", "\n", "for", "applicability_place", "in", "punct_errors_chars_only", ":", "# for each applicability type", "\n", "            ", "punct_errors_detailed_probs", "[", "applicability_place", "]", "=", "{", "}", "\n", "\n", "for", "op_type", "in", "[", "'I'", ",", "'D'", ",", "'S'", "]", ":", "\n", "                ", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "op_type", "]", "=", "{", "}", "\n", "\n", "", "op_type", "=", "'I'", "\n", "for", "tok", "in", "punct_errors_chars_only", "[", "applicability_place", "]", "[", "op_type", "]", ":", "\n", "                ", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "op_type", "]", "[", "tok", "]", "=", "punct_errors_chars_only", "[", "applicability_place", "]", "[", "op_type", "]", "[", "\n", "tok", "]", "/", "num_applicable", "[", "applicability_place", "]", "[", "op_type", "]", "\n", "\n", "", "op_type", "=", "'D'", "\n", "for", "tok", "in", "punct_errors_chars_only", "[", "applicability_place", "]", "[", "op_type", "]", ":", "\n", "                ", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "op_type", "]", "[", "tok", "]", "=", "punct_errors_chars_only", "[", "applicability_place", "]", "[", "op_type", "]", "[", "\n", "tok", "]", "/", "num_applicable", "[", "applicability_place", "]", "[", "op_type", "]", "[", "tok", "]", "\n", "", "op_type", "=", "'S'", "\n", "for", "tok", "in", "punct_errors", "[", "applicability_place", "]", "[", "op_type", "]", ":", "\n", "                ", "token_from", ",", "token_to", "=", "tok", "[", "0", "]", ".", "split", "(", "'SEPARATOR'", ")", "\n", "if", "token_from", "not", "in", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "op_type", "]", ":", "\n", "                    ", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "op_type", "]", "[", "token_from", "]", "=", "{", "}", "\n", "\n", "", "if", "token_from", "in", "num_applicable", "[", "applicability_place", "]", "[", "op_type", "]", ":", "\n", "                    ", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "op_type", "]", "[", "token_from", "]", "[", "token_to", "]", "=", "punct_errors_chars_only", "[", "applicability_place", "]", "[", "op_type", "]", "[", "token_from", "]", "[", "token_to", "]", "/", "num_applicable", "[", "applicability_place", "]", "[", "op_type", "]", "[", "token_from", "]", "\n", "\n", "# create shallow (up to [applicability_place][op_type]) probability dictionary", "\n", "", "", "", "punct_errors_aggregated_probs", "=", "{", "}", "\n", "for", "applicability_place", "in", "punct_errors_detailed_probs", ":", "# for each applicability type", "\n", "            ", "punct_errors_aggregated_probs", "[", "applicability_place", "]", "=", "{", "}", "\n", "\n", "punct_errors_aggregated_probs", "[", "applicability_place", "]", "[", "'I'", "]", "=", "np", ".", "sum", "(", "\n", "list", "(", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "'I'", "]", ".", "values", "(", ")", ")", ")", "\n", "punct_errors_aggregated_probs", "[", "applicability_place", "]", "[", "'S'", "]", "=", "{", "}", "\n", "\n", "for", "token_from", "in", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "'S'", "]", ":", "\n", "                ", "punct_errors_aggregated_probs", "[", "applicability_place", "]", "[", "'S'", "]", "[", "token_from", "]", "=", "np", ".", "sum", "(", "\n", "list", "(", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "'S'", "]", "[", "token_from", "]", ".", "values", "(", ")", ")", ")", "\n", "\n", "# normalize insert and substitute detailed probabilities to sum up to 1", "\n", "", "", "for", "applicability_place", "in", "punct_errors_chars_only", ":", "# for each applicability type", "\n", "# insert", "\n", "            ", "for", "token", "in", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "'I'", "]", ":", "\n", "                ", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "'I'", "]", "[", "token", "]", "/=", "punct_errors_aggregated_probs", "[", "applicability_place", "]", "[", "'I'", "]", "\n", "\n", "# substitute", "\n", "", "for", "token_from", "in", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "'S'", "]", ":", "\n", "                ", "for", "token_to", "in", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "'S'", "]", "[", "token_from", "]", ":", "\n", "                    ", "punct_errors_detailed_probs", "[", "applicability_place", "]", "[", "'S'", "]", "[", "token_from", "]", "[", "token_to", "]", "/=", "punct_errors_aggregated_probs", "[", "applicability_place", "]", "[", "'S'", "]", "[", "token_from", "]", "\n", "\n", "", "", "", "return", "punct_errors_aggregated_probs", ",", "punct_errors_detailed_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation._get_operation_type_and_char": [[282, 371], ["print", "len", "cor_tok.split", "[].islower", "[].isupper", "char.split", "len", "cor_tok.split", "[].isupper", "cor_tok[].islower", "cor_tok.split", "cor_tok.split", "len", "[].islower", "[].isupper", "cor_tok.split", "len", "[].isupper", "[].islower", "cor_tok.split", "cor_tok.split", "cor_tok.split", "len", "cor_tok.split", "cor_tok.split", "len", "orig[].lower", "[].lower", "cor_tok.split", "cor_tok.split", "cor_tok.split", "cor_tok.split", "len", "orig[].lower", "[].lower", "cor_tok.split", "cor_tok.split", "cor_tok.split", "cor_tok.split", "cor_tok.split", "cor_tok.split"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_operation_type_and_char", "(", "edit", ",", "orig", ")", ":", "\n", "        ", "final_punctuation_marks", "=", "[", "'.'", ",", "'!'", ",", "'?'", "]", "\n", "quotation_marks", "=", "[", "\"\\\"\"", ",", "\"\u201e\"", "]", "\n", "\n", "orig_start", ",", "orig_end", ",", "error_type", ",", "cor_tok", ",", "cor_start", ",", "cor_end", "=", "edit", "\n", "\n", "if", "orig_start", "==", "orig_end", ":", "\n", "            ", "op_type", "=", "'I'", "\n", "\n", "if", "len", "(", "cor_tok", ".", "split", "(", "' '", ")", ")", ">", "1", ":", "\n", "# TODO this is mainly because of emoji, which are now splitted into : - ( (three symbols), fix this after solving tokenisation", "\n", "                ", "return", "None", ",", "None", ",", "None", "\n", "\n", "", "char", "=", "cor_tok", "\n", "next_token_change_casing", "=", "False", "\n", "\n", "", "elif", "cor_start", "==", "cor_end", ":", "\n", "            ", "op_type", "=", "'D'", "\n", "char", "=", "\" \"", ".", "join", "(", "orig", "[", "orig_start", ":", "orig_end", "]", ")", "\n", "next_token_change_casing", "=", "False", "\n", "", "else", ":", "\n", "            ", "'''\n            Punctuation errors are by ERRANT sometimes merged with the following word (if it makes sense as a single error, handle these special\n            cases prior to defining this error type as substitute.\n\n            e.g.\n            word -> . Word (insert dot and upper-case next token)\n            '''", "\n", "\n", "# insert eos (any from final_punctuation_marks) (e.g. monkey -> . Monkey)", "\n", "if", "orig_end", "-", "orig_start", "==", "1", "and", "len", "(", "cor_tok", ".", "split", "(", ")", ")", "==", "2", "and", "orig", "[", "orig_start", "]", "[", "0", "]", ".", "islower", "(", ")", "and", "cor_tok", ".", "split", "(", ")", "[", "0", "]", "in", "final_punctuation_marks", "and", "cor_tok", ".", "split", "(", ")", "[", "1", "]", "[", "0", "]", ".", "isupper", "(", ")", ":", "\n", "                ", "op_type", "=", "'I'", "\n", "char", "=", "cor_tok", ".", "split", "(", ")", "[", "0", "]", "\n", "next_token_change_casing", "=", "True", "\n", "# delete . (e.g. . Monkey -> monkey)", "\n", "", "elif", "orig_end", "-", "orig_start", "==", "2", "and", "len", "(", "cor_tok", ".", "split", "(", ")", ")", "==", "1", "and", "orig", "[", "orig_start", "]", "in", "final_punctuation_marks", "and", "orig", "[", "orig_start", "+", "1", "]", "[", "0", "]", ".", "isupper", "(", ")", "and", "cor_tok", "[", "0", "]", ".", "islower", "(", ")", ":", "\n", "                ", "op_type", "=", "'D'", "\n", "char", "=", "orig", "[", "orig_start", "]", "\n", "next_token_change_casing", "=", "True", "\n", "", "else", ":", "\n", "                ", "op_type", "=", "'S'", "\n", "\n", "# several hand-crafted rules", "\n", "# 1. , because -> . Because", "\n", "if", "orig_end", "-", "orig_start", "==", "2", "and", "len", "(", "cor_tok", ".", "split", "(", ")", ")", "==", "2", "and", "orig", "[", "orig_start", "]", "==", "','", "and", "orig", "[", "orig_start", "+", "1", "]", "[", "\n", "0", "]", ".", "islower", "(", ")", "and", "cor_tok", ".", "split", "(", ")", "[", "0", "]", "==", "'.'", "and", "cor_tok", ".", "split", "(", ")", "[", "1", "]", "[", "0", "]", ".", "isupper", "(", ")", ":", "\n", "                    ", "char", "=", "','", "+", "\"SEPARATOR\"", "+", "'.'", "\n", "next_token_change_casing", "=", "True", "\n", "# 2. . Because -> , because", "\n", "", "elif", "orig_end", "-", "orig_start", "==", "2", "and", "len", "(", "cor_tok", ".", "split", "(", ")", ")", "==", "2", "and", "orig", "[", "orig_start", "]", "==", "'.'", "and", "orig", "[", "orig_start", "+", "1", "]", "[", "\n", "0", "]", ".", "isupper", "(", ")", "and", "cor_tok", ".", "split", "(", ")", "[", "0", "]", "==", "','", "and", "cor_tok", ".", "split", "(", ")", "[", "1", "]", "[", "0", "]", ".", "islower", "(", ")", ":", "\n", "                    ", "char", "=", "'.'", "+", "\"SEPARATOR\"", "+", "','", "\n", "next_token_change_casing", "=", "True", "\n", "# 3. . Because -> ? because (both punctuation and casing)", "\n", "", "elif", "orig_end", "-", "orig_start", "==", "2", "and", "len", "(", "cor_tok", ".", "split", "(", ")", ")", "==", "2", "and", "orig", "[", "orig_start", "]", "in", "final_punctuation_marks", "and", "cor_tok", ".", "split", "(", ")", "[", "0", "]", "in", "final_punctuation_marks", "and", "orig", "[", "orig_start", "+", "1", "]", ".", "lower", "(", ")", "==", "cor_tok", ".", "split", "(", ")", "[", "\n", "1", "]", ".", "lower", "(", ")", ":", "\n", "                    ", "char", "=", "orig", "[", "orig_start", "]", "+", "\"SEPARATOR\"", "+", "cor_tok", ".", "split", "(", ")", "[", "0", "]", "\n", "next_token_change_casing", "=", "True", "\n", "# 4. \" Car -> \u201e car (Czech phenomenon)", "\n", "", "elif", "orig_end", "-", "orig_start", "==", "2", "and", "len", "(", "cor_tok", ".", "split", "(", ")", ")", "==", "2", "and", "orig", "[", "orig_start", "]", "in", "quotation_marks", "and", "cor_tok", ".", "split", "(", ")", "[", "0", "]", "in", "quotation_marks", "and", "orig", "[", "orig_start", "+", "1", "]", ".", "lower", "(", ")", "==", "cor_tok", ".", "split", "(", ")", "[", "1", "]", ".", "lower", "(", ")", ":", "\n", "                    ", "char", "=", "orig", "[", "orig_start", "]", "+", "\"SEPARATOR\"", "+", "cor_tok", ".", "split", "(", ")", "[", "0", "]", "\n", "next_token_change_casing", "=", "True", "\n", "# In all other cases, we cannot be more specific, but for the sake of simplicity, we allow only 1:1 punctuation substititions", "\n", "", "elif", "orig_end", "-", "orig_start", "==", "1", "and", "cor_end", "-", "cor_start", "==", "1", ":", "\n", "                    ", "char", "=", "orig", "[", "orig_start", "]", "+", "\"SEPARATOR\"", "+", "cor_tok", "\n", "next_token_change_casing", "=", "False", "\n", "", "else", ":", "\n", "# TODO \u2026---. . . (tri tecky za znak trojtecky?)", "\n", "                    ", "return", "None", ",", "None", ",", "None", "\n", "\n", "# ! Note that currently we go in direction orig -> cor, but in fact, our noising script is supposed to run in opposite direction", "\n", "", "", "", "if", "op_type", "==", "'I'", ":", "\n", "            ", "op_type", "=", "'D'", "\n", "", "elif", "op_type", "==", "'D'", ":", "\n", "            ", "op_type", "=", "'I'", "\n", "", "elif", "op_type", "==", "'S'", ":", "\n", "            ", "orig", ",", "cor", "=", "char", ".", "split", "(", "'SEPARATOR'", ")", "\n", "char", "=", "cor", "+", "'SEPARATOR'", "+", "orig", "\n", "\n", "", "if", "char", "==", "'; - )'", ":", "\n", "            ", "print", "(", "'WTF'", ",", "edit", ")", "\n", "", "return", "op_type", ",", "char", ",", "next_token_change_casing", "\n", "\n"]], "home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation.estimate_probabilities": [[372, 426], ["Punctuation._get_punctuation_probabilities", "aspects.apply_m2_edits.processM2", "cached_edits.append", "len", "list", "coder_dict.keys", "punctuation.Punctuation._get_operation_type_and_char", "len", "[].append"], "methods", ["home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation._get_punctuation_probabilities", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.apply_m2_edits.processM2", "home.repos.pwc.inspect_result.ufal_kazitext.aspects.punctuation.Punctuation._get_operation_type_and_char"], ["", "@", "staticmethod", "\n", "def", "estimate_probabilities", "(", "m2_records", ")", ":", "\n", "        ", "'''\n            We differentiate between punctuation errors on the end of the text and in the middle of the text.\n            '''", "\n", "punct_errors", "=", "{", "\n", "'eos'", ":", "{", "\n", "'I'", ":", "[", "]", ",", "\n", "'D'", ":", "[", "]", ",", "\n", "'S'", ":", "[", "]", "\n", "}", ",", "\n", "'middle'", ":", "{", "\n", "'I'", ":", "[", "]", ",", "\n", "'D'", ":", "[", "]", ",", "\n", "'S'", ":", "[", "]", "\n", "}", ",", "\n", "}", "\n", "\n", "num_files", "=", "0", "# where eos is applicable", "\n", "num_tokens", "=", "0", "# where middle is applicable", "\n", "\n", "cached_edits", "=", "[", "]", "\n", "\n", "for", "m2_file", "in", "m2_records", ":", "\n", "            ", "for", "info", "in", "m2_file", ":", "\n", "                ", "orig_sent", ",", "coder_dict", "=", "apply_m2_edits", ".", "processM2", "(", "info", ",", "[", "]", ")", "\n", "\n", "cached_edits", ".", "append", "(", "coder_dict", ")", "\n", "\n", "if", "coder_dict", ":", "\n", "                    ", "num_files", "+=", "1", "\n", "coder_id", "=", "list", "(", "coder_dict", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "cor_sent", "=", "coder_dict", "[", "coder_id", "]", "[", "0", "]", "\n", "num_tokens", "+=", "len", "(", "cor_sent", ")", "\n", "\n", "for", "edit", "in", "coder_dict", "[", "coder_id", "]", "[", "1", "]", ":", "\n", "                        ", "orig_start", ",", "orig_end", ",", "error_type", ",", "cor_tok", ",", "cor_start", ",", "cor_end", "=", "edit", "\n", "\n", "if", "'PUNCT'", "in", "error_type", ":", "\n", "                            ", "if", "cor_end", ">=", "len", "(", "cor_sent", ")", ":", "\n", "                                ", "applicability_place", "=", "'eos'", "\n", "", "else", ":", "\n", "                                ", "applicability_place", "=", "'middle'", "\n", "\n", "", "op_type", ",", "char", ",", "next_token_change_casing", "=", "Punctuation", ".", "_get_operation_type_and_char", "(", "edit", ",", "orig_sent", ")", "\n", "\n", "if", "op_type", ":", "\n", "                                ", "punct_errors", "[", "applicability_place", "]", "[", "op_type", "]", ".", "append", "(", "[", "char", ",", "next_token_change_casing", "]", ")", "\n", "\n", "", "", "", "", "", "", "punct_errors_aggregated_probs", ",", "punct_errors_detailed_probs", "=", "Punctuation", ".", "_get_punctuation_probabilities", "(", "punct_errors", ",", "cached_edits", ")", "\n", "\n", "return", "'punctuation'", ",", "{", "\n", "'punct_errors_aggregated_probs'", ":", "punct_errors_aggregated_probs", ",", "\n", "'punct_errors_detailed_probs'", ":", "punct_errors_detailed_probs", "\n", "}", "\n"]]}