{"home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.graphs.thresholds.plot_thresholds": [[20, 40], ["sorted", "print", "ax.plot", "glob.glob", "list", "int", "os.path.join", "tensorflow.train.summary_iterator", "re.sub", "x.append", "y.append"], "function", ["None"], ["def", "plot_thresholds", "(", "sparsity", ",", "ax", ")", ":", "\n", "    ", "model_dir", "=", "f\"models/pretrain/gradual_prune_{int(sparsity*100)}/\"", "\n", "event_paths", "=", "sorted", "(", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "model_dir", ",", "\"event*\"", ")", ")", ")", "\n", "\n", "# Just get the second last event of the last event file", "\n", "# (we only care about the last value of thresholds for now)", "\n", "event", "=", "list", "(", "tf", ".", "train", ".", "summary_iterator", "(", "event_paths", "[", "-", "1", "]", ")", ")", "[", "-", "2", "]", "\n", "\n", "print", "(", "f'Training step {event.step}'", ")", "\n", "\n", "x", ",", "y", "=", "[", "]", ",", "[", "]", "\n", "for", "value", "in", "event", ".", "summary", ".", "value", ":", "\n", "        ", "if", "'threshold'", "in", "value", ".", "tag", ":", "\n", "            ", "tag", "=", "re", ".", "sub", "(", "r'model_pruning_summaries/bert/(.*)/threshold/threshold'", ",", "r'\\1'", ",", "value", ".", "tag", ")", "\n", "if", "tag", "!=", "'embeddings/'", ":", "\n", "                ", "x", ".", "append", "(", "value", ".", "simple_value", ")", "\n", "y", ".", "append", "(", "tag", ")", "\n", "# ax.scatter(value.simple_value, tag, color=color_map[sparsity])", "\n", "\n", "", "", "", "ax", ".", "plot", "(", "x", ",", "y", ",", "color", "=", "color_map", "[", "sparsity", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.graphs.weight_change.sort_order_movements": [[9, 13], ["weight_change.cosort", "numpy.argsort", "numpy.abs", "numpy.arange"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.graphs.weight_change.cosort"], ["def", "sort_order_movements", "(", "x", ",", "y", ")", ":", "\n", "    ", "y_xsorted", "=", "cosort", "(", "x", ",", "y", ")", "\n", "y_argsort", "=", "np", ".", "argsort", "(", "y_xsorted", ")", "\n", "return", "np", ".", "abs", "(", "y_argsort", "-", "np", ".", "arange", "(", "y_argsort", ".", "size", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.graphs.weight_change.cosort": [[14, 25], ["numpy.abs", "numpy.abs", "numpy.argsort", "np.abs.flatten", "np.abs.flatten"], "function", ["None"], ["", "def", "cosort", "(", "x", ",", "y", ")", ":", "\n", "    ", "\"\"\"Sort x, and apply the same swapping operations to y. Return y.\"\"\"", "\n", "assert", "x", ".", "shape", "==", "y", ".", "shape", "\n", "x", "=", "np", ".", "abs", "(", "x", ".", "flatten", "(", ")", ")", "\n", "y", "=", "np", ".", "abs", "(", "y", ".", "flatten", "(", ")", ")", "\n", "\n", "# If we sorted x, this is the order we would look at the elements", "\n", "x_argsort", "=", "np", ".", "argsort", "(", "x", ")", "\n", "\n", "# Now, take the elements of y in that order", "\n", "return", "y", "[", "x_argsort", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.graphs.weight_change.weight_change": [[26, 58], ["numpy.zeros", "numpy.array", "matplotlib.subplots", "ax.set_title", "ax.set_xlabel", "ax.set_ylabel", "ax.bar", "matplotlib.savefig", "open", "tensorflow.train.list_variables", "print", "print", "range", "var_name.endswith", "numpy.sum", "tensorflow.contrib.framework.load_variable", "tensorflow.contrib.framework.load_variable", "weight_change.sort_order_movements", "numpy.mean", "numpy.std", "print", "print", "numpy.concatenate", "int", "range", "numpy.sum", "numpy.mean", "numpy.std"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.graphs.weight_change.sort_order_movements"], ["", "def", "weight_change", "(", "first_model", ",", "second_model", ",", "filename", ")", ":", "\n", "    ", "BIN_SIZE", "=", "100", "\n", "heat_map", "=", "np", ".", "zeros", "(", "BIN_SIZE", ")", "\n", "percentages", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "with", "open", "(", "f'{filename}.txt'", ",", "'w+'", ")", "as", "out_f", ":", "\n", "        ", "for", "var_name", ",", "_", "in", "tf", ".", "train", ".", "list_variables", "(", "first_model", ")", ":", "\n", "            ", "if", "var_name", ".", "endswith", "(", "'/weights'", ")", ":", "\n", "                ", "first_tensor", "=", "tf", ".", "contrib", ".", "framework", ".", "load_variable", "(", "first_model", ",", "var_name", ")", "\n", "second_tensor", "=", "tf", ".", "contrib", ".", "framework", ".", "load_variable", "(", "second_model", ",", "var_name", ")", "\n", "\n", "size", "=", "first_tensor", ".", "size", "\n", "movements", "=", "sort_order_movements", "(", "first_tensor", ",", "second_tensor", ")", "\n", "avg_movement", "=", "np", ".", "mean", "(", "movements", ")", "\n", "std_movement", "=", "np", ".", "std", "(", "movements", ")", "\n", "print", "(", "f'{avg_movement:.0f} +- {std_movement:.0f} / {size} = {avg_movement / size:.3f} +- {std_movement / size:.3f}'", ",", "file", "=", "out_f", ")", "\n", "print", "(", "f'{avg_movement:.0f} +- {std_movement:.0f} / {size} = {avg_movement / size:.3f} +- {std_movement / size:.3f}'", ")", "\n", "\n", "percentages", "=", "np", ".", "concatenate", "(", "(", "percentages", ",", "movements", "/", "size", ")", ")", "\n", "\n", "bin_width", "=", "int", "(", "size", "/", "BIN_SIZE", ")", "\n", "for", "i", "in", "range", "(", "BIN_SIZE", ")", ":", "\n", "                    ", "heat_map", "[", "i", "]", "+=", "np", ".", "sum", "(", "movements", "[", "i", "*", "bin_width", ":", "(", "i", "+", "1", ")", "*", "bin_width", "]", ")", "\n", "\n", "", "", "", "print", "(", "f'Average: {np.mean(percentages)}'", ",", "file", "=", "out_f", ")", "\n", "print", "(", "f'Std: {np.std(percentages)}'", ",", "file", "=", "out_f", ")", "\n", "\n", "", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "\n", "ax", ".", "set_title", "(", "'Distribution of Sort Order Movements'", ")", "\n", "ax", ".", "set_xlabel", "(", "'Starting Magnitude Sort Order Position'", ")", "\n", "ax", ".", "set_ylabel", "(", "'% of Movement'", ")", "\n", "ax", ".", "bar", "(", "range", "(", "BIN_SIZE", ")", ",", "heat_map", "/", "np", ".", "sum", "(", "heat_map", ")", ")", "\n", "plt", ".", "savefig", "(", "filename", ")", "\n", "# return total_inversions / possible_inversions", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.graphs.avg_glue.plot_avg_acc": [[13, 25], ["eval_ax.plot", "print", "loss_ax.plot", "tables.common.grid_search_eval", "eval_avgs.append", "loss_avgs.append", "eval_path_str.format", "sum", "len", "sum", "len", "int"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.tables.common.grid_search_eval"], ["def", "plot_avg_acc", "(", "eval_path_str", ",", "label", ",", "eval_ax", ",", "loss_ax", ")", ":", "\n", "    ", "eval_avgs", "=", "[", "]", "\n", "loss_avgs", "=", "[", "]", "\n", "sparsities", "=", "[", "0", ",", ".1", ",", ".2", ",", ".3", ",", ".4", ",", ".5", ",", ".6", ",", ".7", ",", ".8", ",", ".9", "]", "\n", "for", "sparsity", "in", "sparsities", ":", "\n", "        ", "eval_accs", ",", "losses", "=", "grid_search_eval", "(", "lambda", "task", ",", "lr", ":", "eval_path_str", ".", "format", "(", "task", "=", "task", ",", "sparsity", "=", "int", "(", "sparsity", "*", "100", ")", ",", "lr", "=", "lr", ")", ")", "\n", "eval_avgs", ".", "append", "(", "sum", "(", "eval_accs", ")", "/", "len", "(", "eval_accs", ")", ")", "\n", "loss_avgs", ".", "append", "(", "sum", "(", "losses", ")", "/", "len", "(", "losses", ")", ")", "\n", "\n", "", "eval_ax", ".", "plot", "(", "sparsities", ",", "eval_avgs", ",", "label", "=", "label", ")", "\n", "print", "(", "loss_avgs", ")", "\n", "loss_ax", ".", "plot", "(", "sparsities", ",", "loss_avgs", ",", "label", "=", "label", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.graphs.weight_change2.weight_change": [[9, 22], ["numpy.array", "open", "tensorflow.train.list_variables", "print", "print", "var_name.endswith", "tensorflow.contrib.framework.load_variable", "tensorflow.contrib.framework.load_variable", "graphs.weight_change.sort_order_movements", "numpy.concatenate", "numpy.mean", "numpy.std"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.graphs.weight_change.sort_order_movements"], ["def", "weight_change", "(", "first_model", ",", "second_model", ",", "filename", ")", ":", "\n", "    ", "percentages", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "with", "open", "(", "f'{filename}.txt'", ",", "'w+'", ")", "as", "out_f", ":", "\n", "        ", "for", "var_name", ",", "_", "in", "tf", ".", "train", ".", "list_variables", "(", "first_model", ")", ":", "\n", "            ", "if", "var_name", ".", "endswith", "(", "'/weights'", ")", ":", "\n", "                ", "first_tensor", "=", "tf", ".", "contrib", ".", "framework", ".", "load_variable", "(", "first_model", ",", "var_name", ")", "\n", "second_tensor", "=", "tf", ".", "contrib", ".", "framework", ".", "load_variable", "(", "second_model", ",", "var_name", ")", "\n", "size", "=", "first_tensor", ".", "size", "\n", "movements", "=", "sort_order_movements", "(", "first_tensor", ",", "second_tensor", ")", "\n", "percentages", "=", "np", ".", "concatenate", "(", "(", "percentages", ",", "movements", "/", "size", ")", ")", "\n", "\n", "", "", "print", "(", "f'Average: {np.mean(percentages)}'", ",", "file", "=", "out_f", ")", "\n", "print", "(", "f'Std: {np.std(percentages)}'", ",", "file", "=", "out_f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.scripts.preprocess_pretrain_data.write_doc": [[26, 39], ["print", "sent.text.replace", "print", "print"], "function", ["None"], ["def", "write_doc", "(", "doc", ")", ":", "\n", "    ", "\"\"\"From the BERT readme:\n    The input is a plain text file, with one sentence per line. (It is\n    important that these be actual sentences for the \"next sentence prediction\"\n    task). Documents are delimited by empty lines.\n    \"\"\"", "\n", "for", "sent", "in", "doc", ".", "sents", ":", "\n", "        ", "text", "=", "sent", ".", "text", ".", "replace", "(", "'\\n'", ",", "' '", ")", "\n", "if", "text", "==", "'\"'", ":", "\n", "            ", "print", "(", "text", ",", "end", "=", "''", ")", "\n", "", "elif", "text", ":", "\n", "            ", "print", "(", "text", ")", "\n", "", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.scripts.preprocess_pretrain_data.process_docs": [[40, 64], ["os.listdir", "os.listdir", "os.listdir", "os.path.join", "os.path.isfile", "os.path.join", "os.path.join", "os.path.isfile", "open", "preprocess_pretrain_data.write_doc", "nlp", "json.loads", "preprocess_pretrain_data.write_doc", "open().read", "nlp_lg", "open"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.scripts.preprocess_pretrain_data.write_doc", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.scripts.preprocess_pretrain_data.write_doc"], ["", "def", "process_docs", "(", "chunk_id", ":", "int", ",", "total_chunks", ":", "int", ",", "wiki_loc", ",", "book_loc", ")", ":", "\n", "    ", "assert", "chunk_id", "<", "total_chunks", "\n", "doc_counter", "=", "0", "\n", "# Do all the wikipedia stuff", "\n", "for", "subdir", "in", "os", ".", "listdir", "(", "wiki_loc", ")", ":", "\n", "        ", "for", "wiki_fname", "in", "os", ".", "listdir", "(", "os", ".", "path", ".", "join", "(", "wiki_loc", ",", "subdir", ")", ")", ":", "\n", "            ", "input_path", "=", "os", ".", "path", ".", "join", "(", "wiki_loc", ",", "subdir", ",", "wiki_fname", ")", "\n", "if", "os", ".", "path", ".", "isfile", "(", "input_path", ")", ":", "\n", "# Every line is a json object representing a wikipedia article", "\n", "                ", "for", "line", "in", "open", "(", "input_path", ",", "'r'", ")", ":", "\n", "                    ", "if", "doc_counter", "%", "total_chunks", "==", "chunk_id", ":", "\n", "                        ", "doc", "=", "json", ".", "loads", "(", "line", ")", "\n", "# Use the large spacy model for wikipedia data, since it's supposed to be nice.", "\n", "write_doc", "(", "nlp_lg", "(", "doc", "[", "'text'", "]", ")", ")", "\n", "", "doc_counter", "+=", "1", "\n", "\n", "# Do all the bookcorpus stuff", "\n", "", "", "", "", "for", "book_fname", "in", "os", ".", "listdir", "(", "book_loc", ")", ":", "\n", "        ", "input_path", "=", "os", ".", "path", ".", "join", "(", "book_loc", ",", "book_fname", ")", "\n", "if", "os", ".", "path", ".", "isfile", "(", "input_path", ")", ":", "\n", "            ", "if", "doc_counter", "%", "total_chunks", "==", "chunk_id", ":", "\n", "# Use the rule-based sentecizer for books, because it handles quotes better.", "\n", "                ", "write_doc", "(", "nlp", "(", "open", "(", "input_path", ")", ".", "read", "(", ")", ")", ")", "\n", "", "doc_counter", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.scripts.get_glue.download_and_extract": [[44, 52], ["print", "urllib.request.urlretrieve", "os.remove", "print", "zipfile.ZipFile", "zip_ref.extractall"], "function", ["None"], ["def", "download_and_extract", "(", "task", ",", "data_dir", ")", ":", "\n", "    ", "print", "(", "\"Downloading and extracting %s...\"", "%", "task", ")", "\n", "data_file", "=", "\"%s.zip\"", "%", "task", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "TASK2PATH", "[", "task", "]", ",", "data_file", ")", "\n", "with", "zipfile", ".", "ZipFile", "(", "data_file", ")", "as", "zip_ref", ":", "\n", "        ", "zip_ref", ".", "extractall", "(", "data_dir", ")", "\n", "", "os", ".", "remove", "(", "data_file", ")", "\n", "print", "(", "\"\\tCompleted!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.scripts.get_glue.format_mrpc": [[53, 97], ["print", "os.path.join", "os.path.isfile", "os.path.isfile", "urllib.request.urlretrieve", "print", "os.path.isdir", "os.mkdir", "os.path.join", "os.path.join", "print", "os.path.join", "os.path.join", "urllib.request.urlretrieve", "urllib.request.urlretrieve", "os.path.join", "open", "open", "open", "open", "data_fh.readline", "train_fh.write", "dev_fh.write", "open", "open", "data_fh.readline", "test_fh.write", "enumerate", "os.path.join", "dev_ids.append", "os.path.join", "os.path.join", "row.strip().split", "os.path.join", "row.strip().split", "test_fh.write", "row.strip().split", "dev_fh.write", "train_fh.write", "row.strip", "row.strip", "row.strip"], "function", ["None"], ["", "def", "format_mrpc", "(", "data_dir", ",", "path_to_data", ")", ":", "\n", "    ", "print", "(", "\"Processing MRPC...\"", ")", "\n", "mrpc_dir", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"MRPC\"", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "mrpc_dir", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "mrpc_dir", ")", "\n", "", "if", "path_to_data", ":", "\n", "        ", "mrpc_train_file", "=", "os", ".", "path", ".", "join", "(", "path_to_data", ",", "\"msr_paraphrase_train.txt\"", ")", "\n", "mrpc_test_file", "=", "os", ".", "path", ".", "join", "(", "path_to_data", ",", "\"msr_paraphrase_test.txt\"", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Local MRPC data not specified, downloading data from %s\"", "%", "MRPC_TRAIN", ")", "\n", "mrpc_train_file", "=", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"msr_paraphrase_train.txt\"", ")", "\n", "mrpc_test_file", "=", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"msr_paraphrase_test.txt\"", ")", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "MRPC_TRAIN", ",", "mrpc_train_file", ")", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "MRPC_TEST", ",", "mrpc_test_file", ")", "\n", "", "assert", "os", ".", "path", ".", "isfile", "(", "mrpc_train_file", ")", ",", "\"Train data not found at %s\"", "%", "mrpc_train_file", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "mrpc_test_file", ")", ",", "\"Test data not found at %s\"", "%", "mrpc_test_file", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "TASK2PATH", "[", "\"MRPC\"", "]", ",", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"dev_ids.tsv\"", ")", ")", "\n", "\n", "dev_ids", "=", "[", "]", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"dev_ids.tsv\"", ")", ",", "encoding", "=", "\"utf8\"", ")", "as", "ids_fh", ":", "\n", "        ", "for", "row", "in", "ids_fh", ":", "\n", "            ", "dev_ids", ".", "append", "(", "row", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", ")", "\n", "\n", "", "", "with", "open", "(", "mrpc_train_file", ",", "encoding", "=", "\"utf8\"", ")", "as", "data_fh", ",", "open", "(", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"train.tsv\"", ")", ",", "'w'", ",", "encoding", "=", "\"utf8\"", ")", "as", "train_fh", ",", "open", "(", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"dev.tsv\"", ")", ",", "'w'", ",", "encoding", "=", "\"utf8\"", ")", "as", "dev_fh", ":", "\n", "        ", "header", "=", "data_fh", ".", "readline", "(", ")", "\n", "train_fh", ".", "write", "(", "header", ")", "\n", "dev_fh", ".", "write", "(", "header", ")", "\n", "for", "row", "in", "data_fh", ":", "\n", "            ", "label", ",", "id1", ",", "id2", ",", "s1", ",", "s2", "=", "row", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "[", "id1", ",", "id2", "]", "in", "dev_ids", ":", "\n", "                ", "dev_fh", ".", "write", "(", "\"%s\\t%s\\t%s\\t%s\\t%s\\n\"", "%", "(", "label", ",", "id1", ",", "id2", ",", "s1", ",", "s2", ")", ")", "\n", "", "else", ":", "\n", "                ", "train_fh", ".", "write", "(", "\"%s\\t%s\\t%s\\t%s\\t%s\\n\"", "%", "(", "label", ",", "id1", ",", "id2", ",", "s1", ",", "s2", ")", ")", "\n", "\n", "", "", "", "with", "open", "(", "mrpc_test_file", ",", "encoding", "=", "\"utf8\"", ")", "as", "data_fh", ",", "open", "(", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"test.tsv\"", ")", ",", "'w'", ",", "encoding", "=", "\"utf8\"", ")", "as", "test_fh", ":", "\n", "        ", "header", "=", "data_fh", ".", "readline", "(", ")", "\n", "test_fh", ".", "write", "(", "\"index\\t#1 ID\\t#2 ID\\t#1 String\\t#2 String\\n\"", ")", "\n", "for", "idx", ",", "row", "in", "enumerate", "(", "data_fh", ")", ":", "\n", "            ", "label", ",", "id1", ",", "id2", ",", "s1", ",", "s2", "=", "row", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "test_fh", ".", "write", "(", "\"%d\\t%s\\t%s\\t%s\\t%s\\n\"", "%", "(", "idx", ",", "id1", ",", "id2", ",", "s1", ",", "s2", ")", ")", "\n", "", "", "print", "(", "\"\\tCompleted!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.scripts.get_glue.download_diagnostic": [[98, 106], ["print", "os.path.join", "urllib.request.urlretrieve", "print", "os.path.isdir", "os.mkdir", "os.path.join", "os.path.join"], "function", ["None"], ["", "def", "download_diagnostic", "(", "data_dir", ")", ":", "\n", "    ", "print", "(", "\"Downloading and extracting diagnostic...\"", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"diagnostic\"", ")", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"diagnostic\"", ")", ")", "\n", "", "data_file", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"diagnostic\"", ",", "\"diagnostic.tsv\"", ")", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "TASK2PATH", "[", "\"diagnostic\"", "]", ",", "data_file", ")", "\n", "print", "(", "\"\\tCompleted!\"", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.scripts.get_glue.get_tasks": [[107, 117], ["task_names.split.split", "tasks.append"], "function", ["None"], ["", "def", "get_tasks", "(", "task_names", ")", ":", "\n", "    ", "task_names", "=", "task_names", ".", "split", "(", "','", ")", "\n", "if", "\"all\"", "in", "task_names", ":", "\n", "        ", "tasks", "=", "TASKS", "\n", "", "else", ":", "\n", "        ", "tasks", "=", "[", "]", "\n", "for", "task_name", "in", "task_names", ":", "\n", "            ", "assert", "task_name", "in", "TASKS", ",", "\"Task %s not found!\"", "%", "task_name", "\n", "tasks", ".", "append", "(", "task_name", ")", "\n", "", "", "return", "tasks", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.scripts.get_glue.main": [[118, 138], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "get_glue.get_tasks", "os.path.isdir", "os.mkdir", "get_glue.format_mrpc", "get_glue.download_diagnostic", "get_glue.download_and_extract"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.scripts.get_glue.get_tasks", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.scripts.get_glue.format_mrpc", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.scripts.get_glue.download_diagnostic", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.scripts.get_glue.download_and_extract"], ["", "def", "main", "(", "arguments", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--data_dir'", ",", "help", "=", "'directory to save data to'", ",", "type", "=", "str", ",", "default", "=", "'glue_data'", ")", "\n", "parser", ".", "add_argument", "(", "'--tasks'", ",", "help", "=", "'tasks to download data for as a comma separated string'", ",", "\n", "type", "=", "str", ",", "default", "=", "'all'", ")", "\n", "parser", ".", "add_argument", "(", "'--path_to_mrpc'", ",", "help", "=", "'path to directory containing extracted MRPC data, msr_paraphrase_train.txt and msr_paraphrase_text.txt'", ",", "\n", "type", "=", "str", ",", "default", "=", "''", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", "arguments", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "args", ".", "data_dir", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "args", ".", "data_dir", ")", "\n", "", "tasks", "=", "get_tasks", "(", "args", ".", "tasks", ")", "\n", "\n", "for", "task", "in", "tasks", ":", "\n", "        ", "if", "task", "==", "'MRPC'", ":", "\n", "            ", "format_mrpc", "(", "args", ".", "data_dir", ",", "args", ".", "path_to_mrpc", ")", "\n", "", "elif", "task", "==", "'diagnostic'", ":", "\n", "            ", "download_diagnostic", "(", "args", ".", "data_dir", ")", "\n", "", "else", ":", "\n", "            ", "download_and_extract", "(", "task", ",", "args", ".", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.TrainingInstance.__init__": [[67, 74], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "tokens", ",", "segment_ids", ",", "masked_lm_positions", ",", "masked_lm_labels", ",", "\n", "is_random_next", ")", ":", "\n", "    ", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "is_random_next", "=", "is_random_next", "\n", "self", ".", "masked_lm_positions", "=", "masked_lm_positions", "\n", "self", ".", "masked_lm_labels", "=", "masked_lm_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.TrainingInstance.__str__": [[75, 87], ["tokenization.printable_text", "str", "str", "tokenization.printable_text"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.printable_text", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.printable_text"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "    ", "s", "=", "\"\"", "\n", "s", "+=", "\"tokens: %s\\n\"", "%", "(", "\" \"", ".", "join", "(", "\n", "[", "tokenization", ".", "printable_text", "(", "x", ")", "for", "x", "in", "self", ".", "tokens", "]", ")", ")", "\n", "s", "+=", "\"segment_ids: %s\\n\"", "%", "(", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "self", ".", "segment_ids", "]", ")", ")", "\n", "s", "+=", "\"is_random_next: %s\\n\"", "%", "self", ".", "is_random_next", "\n", "s", "+=", "\"masked_lm_positions: %s\\n\"", "%", "(", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "self", ".", "masked_lm_positions", "]", ")", ")", "\n", "s", "+=", "\"masked_lm_labels: %s\\n\"", "%", "(", "\" \"", ".", "join", "(", "\n", "[", "tokenization", ".", "printable_text", "(", "x", ")", "for", "x", "in", "self", ".", "masked_lm_labels", "]", ")", ")", "\n", "s", "+=", "\"\\n\"", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.TrainingInstance.__repr__": [[88, 90], ["create_pretraining_data.TrainingInstance.__str__"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.SquadExample.__str__"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "__str__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.write_instance_to_example_files": [[92, 163], ["enumerate", "tensorflow.logging.info", "writers.append", "tokenizer.convert_tokens_to_ids", "list", "list", "tokenizer.convert_tokens_to_ids", "collections.OrderedDict", "create_pretraining_data.create_int_feature", "create_pretraining_data.create_int_feature", "create_pretraining_data.create_int_feature", "create_pretraining_data.create_int_feature", "create_pretraining_data.create_int_feature", "create_pretraining_data.create_float_feature", "create_pretraining_data.create_int_feature", "tensorflow.train.Example", "writers[].write", "writer.close", "tensorflow.python_io.TFRecordWriter", "len", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "list.append", "len", "len", "len", "len", "len", "list.append", "tokenizer.convert_tokens_to_ids.append", "masked_lm_weights.append", "tf.train.Example.SerializeToString", "len", "tensorflow.logging.info", "tensorflow.logging.info", "collections.OrderedDict.keys", "tensorflow.train.Features", "tensorflow.logging.info", "tokenization.printable_text", "str"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.create_int_feature", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.create_int_feature", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.create_int_feature", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.create_int_feature", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.create_int_feature", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.create_float_feature", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.create_int_feature", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.FeatureWriter.close", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.printable_text"], ["", "", "def", "write_instance_to_example_files", "(", "instances", ",", "tokenizer", ",", "max_seq_length", ",", "\n", "max_predictions_per_seq", ",", "output_files", ")", ":", "\n", "  ", "\"\"\"Create TF example files from `TrainingInstance`s.\"\"\"", "\n", "writers", "=", "[", "]", "\n", "for", "output_file", "in", "output_files", ":", "\n", "    ", "writers", ".", "append", "(", "tf", ".", "python_io", ".", "TFRecordWriter", "(", "output_file", ")", ")", "\n", "\n", "", "writer_index", "=", "0", "\n", "\n", "total_written", "=", "0", "\n", "for", "(", "inst_index", ",", "instance", ")", "in", "enumerate", "(", "instances", ")", ":", "\n", "    ", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "instance", ".", "tokens", ")", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "segment_ids", "=", "list", "(", "instance", ".", "segment_ids", ")", "\n", "assert", "len", "(", "input_ids", ")", "<=", "max_seq_length", "\n", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "      ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "\n", "masked_lm_positions", "=", "list", "(", "instance", ".", "masked_lm_positions", ")", "\n", "masked_lm_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "instance", ".", "masked_lm_labels", ")", "\n", "masked_lm_weights", "=", "[", "1.0", "]", "*", "len", "(", "masked_lm_ids", ")", "\n", "\n", "while", "len", "(", "masked_lm_positions", ")", "<", "max_predictions_per_seq", ":", "\n", "      ", "masked_lm_positions", ".", "append", "(", "0", ")", "\n", "masked_lm_ids", ".", "append", "(", "0", ")", "\n", "masked_lm_weights", ".", "append", "(", "0.0", ")", "\n", "\n", "", "next_sentence_label", "=", "1", "if", "instance", ".", "is_random_next", "else", "0", "\n", "\n", "features", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "features", "[", "\"input_ids\"", "]", "=", "create_int_feature", "(", "input_ids", ")", "\n", "features", "[", "\"input_mask\"", "]", "=", "create_int_feature", "(", "input_mask", ")", "\n", "features", "[", "\"segment_ids\"", "]", "=", "create_int_feature", "(", "segment_ids", ")", "\n", "features", "[", "\"masked_lm_positions\"", "]", "=", "create_int_feature", "(", "masked_lm_positions", ")", "\n", "features", "[", "\"masked_lm_ids\"", "]", "=", "create_int_feature", "(", "masked_lm_ids", ")", "\n", "features", "[", "\"masked_lm_weights\"", "]", "=", "create_float_feature", "(", "masked_lm_weights", ")", "\n", "features", "[", "\"next_sentence_labels\"", "]", "=", "create_int_feature", "(", "[", "next_sentence_label", "]", ")", "\n", "\n", "tf_example", "=", "tf", ".", "train", ".", "Example", "(", "features", "=", "tf", ".", "train", ".", "Features", "(", "feature", "=", "features", ")", ")", "\n", "\n", "writers", "[", "writer_index", "]", ".", "write", "(", "tf_example", ".", "SerializeToString", "(", ")", ")", "\n", "writer_index", "=", "(", "writer_index", "+", "1", ")", "%", "len", "(", "writers", ")", "\n", "\n", "total_written", "+=", "1", "\n", "\n", "if", "inst_index", "<", "20", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "tokenization", ".", "printable_text", "(", "x", ")", "for", "x", "in", "instance", ".", "tokens", "]", ")", ")", "\n", "\n", "for", "feature_name", "in", "features", ".", "keys", "(", ")", ":", "\n", "        ", "feature", "=", "features", "[", "feature_name", "]", "\n", "values", "=", "[", "]", "\n", "if", "feature", ".", "int64_list", ".", "value", ":", "\n", "          ", "values", "=", "feature", ".", "int64_list", ".", "value", "\n", "", "elif", "feature", ".", "float_list", ".", "value", ":", "\n", "          ", "values", "=", "feature", ".", "float_list", ".", "value", "\n", "", "tf", ".", "logging", ".", "info", "(", "\n", "\"%s: %s\"", "%", "(", "feature_name", ",", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "values", "]", ")", ")", ")", "\n", "\n", "", "", "", "for", "writer", "in", "writers", ":", "\n", "    ", "writer", ".", "close", "(", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"Wrote %d total instances\"", ",", "total_written", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.create_int_feature": [[165, 168], ["tensorflow.train.Feature", "tensorflow.train.Int64List", "list"], "function", ["None"], ["", "def", "create_int_feature", "(", "values", ")", ":", "\n", "  ", "feature", "=", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "list", "(", "values", ")", ")", ")", "\n", "return", "feature", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.create_float_feature": [[170, 173], ["tensorflow.train.Feature", "tensorflow.train.FloatList", "list"], "function", ["None"], ["", "def", "create_float_feature", "(", "values", ")", ":", "\n", "  ", "feature", "=", "tf", ".", "train", ".", "Feature", "(", "float_list", "=", "tf", ".", "train", ".", "FloatList", "(", "value", "=", "list", "(", "values", ")", ")", ")", "\n", "return", "feature", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.create_training_instances": [[175, 217], ["rng.shuffle", "list", "range", "rng.shuffle", "tokenizer.vocab.keys", "range", "tensorflow.gfile.GFile", "len", "instances.extend", "tokenization.convert_to_unicode", "line.strip.strip", "tokenizer.tokenize", "create_pretraining_data.create_instances_from_document", "reader.readline", "all_documents.append", "all_documents[].append"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.create_instances_from_document"], ["", "def", "create_training_instances", "(", "input_files", ",", "tokenizer", ",", "max_seq_length", ",", "\n", "dupe_factor", ",", "short_seq_prob", ",", "masked_lm_prob", ",", "\n", "max_predictions_per_seq", ",", "rng", ")", ":", "\n", "  ", "\"\"\"Create `TrainingInstance`s from raw text.\"\"\"", "\n", "all_documents", "=", "[", "[", "]", "]", "\n", "\n", "# Input file format:", "\n", "# (1) One sentence per line. These should ideally be actual sentences, not", "\n", "# entire paragraphs or arbitrary spans of text. (Because we use the", "\n", "# sentence boundaries for the \"next sentence prediction\" task).", "\n", "# (2) Blank lines between documents. Document boundaries are needed so", "\n", "# that the \"next sentence prediction\" task doesn't span between documents.", "\n", "for", "input_file", "in", "input_files", ":", "\n", "    ", "with", "tf", ".", "gfile", ".", "GFile", "(", "input_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "      ", "while", "True", ":", "\n", "        ", "line", "=", "tokenization", ".", "convert_to_unicode", "(", "reader", ".", "readline", "(", ")", ")", "\n", "if", "not", "line", ":", "\n", "          ", "break", "\n", "", "line", "=", "line", ".", "strip", "(", ")", "\n", "\n", "# Empty lines are used as document delimiters", "\n", "if", "not", "line", ":", "\n", "          ", "all_documents", ".", "append", "(", "[", "]", ")", "\n", "", "tokens", "=", "tokenizer", ".", "tokenize", "(", "line", ")", "\n", "if", "tokens", ":", "\n", "          ", "all_documents", "[", "-", "1", "]", ".", "append", "(", "tokens", ")", "\n", "\n", "# Remove empty documents", "\n", "", "", "", "", "all_documents", "=", "[", "x", "for", "x", "in", "all_documents", "if", "x", "]", "\n", "rng", ".", "shuffle", "(", "all_documents", ")", "\n", "\n", "vocab_words", "=", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", ")", ")", "\n", "instances", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "dupe_factor", ")", ":", "\n", "    ", "for", "document_index", "in", "range", "(", "len", "(", "all_documents", ")", ")", ":", "\n", "      ", "instances", ".", "extend", "(", "\n", "create_instances_from_document", "(", "\n", "all_documents", ",", "document_index", ",", "max_seq_length", ",", "short_seq_prob", ",", "\n", "masked_lm_prob", ",", "max_predictions_per_seq", ",", "vocab_words", ",", "rng", ")", ")", "\n", "\n", "", "", "rng", ".", "shuffle", "(", "instances", ")", "\n", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.create_instances_from_document": [[219, 332], ["rng.random", "rng.randint", "len", "current_chunk.append", "len", "range", "create_pretraining_data.truncate_seq_pair", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "create_pretraining_data.create_masked_lm_predictions", "create_pretraining_data.TrainingInstance", "instances.append", "len", "len", "rng.randint", "tokens_a.extend", "range", "rng.randint", "range", "range", "len", "len", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "len", "rng.random", "len", "rng.randint", "len", "tokens_b.extend", "len", "len", "tokens_b.extend", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.truncate_seq_pair", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.create_masked_lm_predictions"], ["", "def", "create_instances_from_document", "(", "\n", "all_documents", ",", "document_index", ",", "max_seq_length", ",", "short_seq_prob", ",", "\n", "masked_lm_prob", ",", "max_predictions_per_seq", ",", "vocab_words", ",", "rng", ")", ":", "\n", "  ", "\"\"\"Creates `TrainingInstance`s for a single document.\"\"\"", "\n", "document", "=", "all_documents", "[", "document_index", "]", "\n", "\n", "# Account for [CLS], [SEP], [SEP]", "\n", "max_num_tokens", "=", "max_seq_length", "-", "3", "\n", "\n", "# We *usually* want to fill up the entire sequence since we are padding", "\n", "# to `max_seq_length` anyways, so short sequences are generally wasted", "\n", "# computation. However, we *sometimes*", "\n", "# (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter", "\n", "# sequences to minimize the mismatch between pre-training and fine-tuning.", "\n", "# The `target_seq_length` is just a rough target however, whereas", "\n", "# `max_seq_length` is a hard limit.", "\n", "target_seq_length", "=", "max_num_tokens", "\n", "if", "rng", ".", "random", "(", ")", "<", "short_seq_prob", ":", "\n", "    ", "target_seq_length", "=", "rng", ".", "randint", "(", "2", ",", "max_num_tokens", ")", "\n", "\n", "# We DON'T just concatenate all of the tokens from a document into a long", "\n", "# sequence and choose an arbitrary split point because this would make the", "\n", "# next sentence prediction task too easy. Instead, we split the input into", "\n", "# segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user", "\n", "# input.", "\n", "", "instances", "=", "[", "]", "\n", "current_chunk", "=", "[", "]", "\n", "current_length", "=", "0", "\n", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "document", ")", ":", "\n", "    ", "segment", "=", "document", "[", "i", "]", "\n", "current_chunk", ".", "append", "(", "segment", ")", "\n", "current_length", "+=", "len", "(", "segment", ")", "\n", "if", "i", "==", "len", "(", "document", ")", "-", "1", "or", "current_length", ">=", "target_seq_length", ":", "\n", "      ", "if", "current_chunk", ":", "\n", "# `a_end` is how many segments from `current_chunk` go into the `A`", "\n", "# (first) sentence.", "\n", "        ", "a_end", "=", "1", "\n", "if", "len", "(", "current_chunk", ")", ">=", "2", ":", "\n", "          ", "a_end", "=", "rng", ".", "randint", "(", "1", ",", "len", "(", "current_chunk", ")", "-", "1", ")", "\n", "\n", "", "tokens_a", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "a_end", ")", ":", "\n", "          ", "tokens_a", ".", "extend", "(", "current_chunk", "[", "j", "]", ")", "\n", "\n", "", "tokens_b", "=", "[", "]", "\n", "# Random next", "\n", "is_random_next", "=", "False", "\n", "if", "len", "(", "current_chunk", ")", "==", "1", "or", "rng", ".", "random", "(", ")", "<", "0.5", ":", "\n", "          ", "is_random_next", "=", "True", "\n", "target_b_length", "=", "target_seq_length", "-", "len", "(", "tokens_a", ")", "\n", "\n", "# This should rarely go for more than one iteration for large", "\n", "# corpora. However, just to be careful, we try to make sure that", "\n", "# the random document is not the same as the document", "\n", "# we're processing.", "\n", "for", "_", "in", "range", "(", "10", ")", ":", "\n", "            ", "random_document_index", "=", "rng", ".", "randint", "(", "0", ",", "len", "(", "all_documents", ")", "-", "1", ")", "\n", "if", "random_document_index", "!=", "document_index", ":", "\n", "              ", "break", "\n", "\n", "", "", "random_document", "=", "all_documents", "[", "random_document_index", "]", "\n", "random_start", "=", "rng", ".", "randint", "(", "0", ",", "len", "(", "random_document", ")", "-", "1", ")", "\n", "for", "j", "in", "range", "(", "random_start", ",", "len", "(", "random_document", ")", ")", ":", "\n", "            ", "tokens_b", ".", "extend", "(", "random_document", "[", "j", "]", ")", "\n", "if", "len", "(", "tokens_b", ")", ">=", "target_b_length", ":", "\n", "              ", "break", "\n", "# We didn't actually use these segments so we \"put them back\" so", "\n", "# they don't go to waste.", "\n", "", "", "num_unused_segments", "=", "len", "(", "current_chunk", ")", "-", "a_end", "\n", "i", "-=", "num_unused_segments", "\n", "# Actual next", "\n", "", "else", ":", "\n", "          ", "is_random_next", "=", "False", "\n", "for", "j", "in", "range", "(", "a_end", ",", "len", "(", "current_chunk", ")", ")", ":", "\n", "            ", "tokens_b", ".", "extend", "(", "current_chunk", "[", "j", "]", ")", "\n", "", "", "truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_num_tokens", ",", "rng", ")", "\n", "\n", "assert", "len", "(", "tokens_a", ")", ">=", "1", "\n", "assert", "len", "(", "tokens_b", ")", ">=", "1", "\n", "\n", "tokens", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "tokens_a", ":", "\n", "          ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "for", "token", "in", "tokens_b", ":", "\n", "          ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "\n", "(", "tokens", ",", "masked_lm_positions", ",", "\n", "masked_lm_labels", ")", "=", "create_masked_lm_predictions", "(", "\n", "tokens", ",", "masked_lm_prob", ",", "max_predictions_per_seq", ",", "vocab_words", ",", "rng", ")", "\n", "instance", "=", "TrainingInstance", "(", "\n", "tokens", "=", "tokens", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "is_random_next", "=", "is_random_next", ",", "\n", "masked_lm_positions", "=", "masked_lm_positions", ",", "\n", "masked_lm_labels", "=", "masked_lm_labels", ")", "\n", "instances", ".", "append", "(", "instance", ")", "\n", "", "current_chunk", "=", "[", "]", "\n", "current_length", "=", "0", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.create_masked_lm_predictions": [[338, 389], ["enumerate", "rng.shuffle", "list", "min", "set", "sorted", "cand_indexes.append", "max", "set.add", "sorted.append", "masked_lm_positions.append", "masked_lm_labels.append", "int", "len", "rng.random", "MaskedLmInstance", "round", "rng.random", "len", "rng.randint", "len"], "function", ["None"], ["def", "create_masked_lm_predictions", "(", "tokens", ",", "masked_lm_prob", ",", "\n", "max_predictions_per_seq", ",", "vocab_words", ",", "rng", ")", ":", "\n", "  ", "\"\"\"Creates the predictions for the masked LM objective.\"\"\"", "\n", "\n", "cand_indexes", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "tokens", ")", ":", "\n", "    ", "if", "token", "==", "\"[CLS]\"", "or", "token", "==", "\"[SEP]\"", ":", "\n", "      ", "continue", "\n", "", "cand_indexes", ".", "append", "(", "i", ")", "\n", "\n", "", "rng", ".", "shuffle", "(", "cand_indexes", ")", "\n", "\n", "output_tokens", "=", "list", "(", "tokens", ")", "\n", "\n", "num_to_predict", "=", "min", "(", "max_predictions_per_seq", ",", "\n", "max", "(", "1", ",", "int", "(", "round", "(", "len", "(", "tokens", ")", "*", "masked_lm_prob", ")", ")", ")", ")", "\n", "\n", "masked_lms", "=", "[", "]", "\n", "covered_indexes", "=", "set", "(", ")", "\n", "for", "index", "in", "cand_indexes", ":", "\n", "    ", "if", "len", "(", "masked_lms", ")", ">=", "num_to_predict", ":", "\n", "      ", "break", "\n", "", "if", "index", "in", "covered_indexes", ":", "\n", "      ", "continue", "\n", "", "covered_indexes", ".", "add", "(", "index", ")", "\n", "\n", "masked_token", "=", "None", "\n", "# 80% of the time, replace with [MASK]", "\n", "if", "rng", ".", "random", "(", ")", "<", "0.8", ":", "\n", "      ", "masked_token", "=", "\"[MASK]\"", "\n", "", "else", ":", "\n", "# 10% of the time, keep original", "\n", "      ", "if", "rng", ".", "random", "(", ")", "<", "0.5", ":", "\n", "        ", "masked_token", "=", "tokens", "[", "index", "]", "\n", "# 10% of the time, replace with random word", "\n", "", "else", ":", "\n", "        ", "masked_token", "=", "vocab_words", "[", "rng", ".", "randint", "(", "0", ",", "len", "(", "vocab_words", ")", "-", "1", ")", "]", "\n", "\n", "", "", "output_tokens", "[", "index", "]", "=", "masked_token", "\n", "\n", "masked_lms", ".", "append", "(", "MaskedLmInstance", "(", "index", "=", "index", ",", "label", "=", "tokens", "[", "index", "]", ")", ")", "\n", "\n", "", "masked_lms", "=", "sorted", "(", "masked_lms", ",", "key", "=", "lambda", "x", ":", "x", ".", "index", ")", "\n", "\n", "masked_lm_positions", "=", "[", "]", "\n", "masked_lm_labels", "=", "[", "]", "\n", "for", "p", "in", "masked_lms", ":", "\n", "    ", "masked_lm_positions", ".", "append", "(", "p", ".", "index", ")", "\n", "masked_lm_labels", ".", "append", "(", "p", ".", "label", ")", "\n", "\n", "", "return", "(", "output_tokens", ",", "masked_lm_positions", ",", "masked_lm_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.truncate_seq_pair": [[391, 407], ["len", "len", "len", "rng.random", "trunc_tokens.pop", "len", "len"], "function", ["None"], ["", "def", "truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_num_tokens", ",", "rng", ")", ":", "\n", "  ", "\"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"", "\n", "while", "True", ":", "\n", "    ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_num_tokens", ":", "\n", "      ", "break", "\n", "\n", "", "trunc_tokens", "=", "tokens_a", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", "else", "tokens_b", "\n", "assert", "len", "(", "trunc_tokens", ")", ">=", "1", "\n", "\n", "# We want to sometimes truncate from the front and sometimes from the", "\n", "# back to add more randomness and avoid biases.", "\n", "if", "rng", ".", "random", "(", ")", "<", "0.5", ":", "\n", "      ", "del", "trunc_tokens", "[", "0", "]", "\n", "", "else", ":", "\n", "      ", "trunc_tokens", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.main": [[409, 436], ["tensorflow.logging.set_verbosity", "tokenization.FullTokenizer", "FLAGS.input_file.split", "tensorflow.logging.info", "random.Random", "create_pretraining_data.create_training_instances", "FLAGS.output_file.split", "tensorflow.logging.info", "create_pretraining_data.write_instance_to_example_files", "input_files.extend", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.gfile.Glob"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.create_training_instances", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.write_instance_to_example_files"], ["", "", "", "def", "main", "(", "_", ")", ":", "\n", "  ", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "\n", "tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "FLAGS", ".", "vocab_file", ",", "do_lower_case", "=", "FLAGS", ".", "do_lower_case", ")", "\n", "\n", "input_files", "=", "[", "]", "\n", "for", "input_pattern", "in", "FLAGS", ".", "input_file", ".", "split", "(", "\",\"", ")", ":", "\n", "    ", "input_files", ".", "extend", "(", "tf", ".", "gfile", ".", "Glob", "(", "input_pattern", ")", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"*** Reading from input files ***\"", ")", "\n", "for", "input_file", "in", "input_files", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"  %s\"", ",", "input_file", ")", "\n", "\n", "", "rng", "=", "random", ".", "Random", "(", "FLAGS", ".", "random_seed", ")", "\n", "instances", "=", "create_training_instances", "(", "\n", "input_files", ",", "tokenizer", ",", "FLAGS", ".", "max_seq_length", ",", "FLAGS", ".", "dupe_factor", ",", "\n", "FLAGS", ".", "short_seq_prob", ",", "FLAGS", ".", "masked_lm_prob", ",", "FLAGS", ".", "max_predictions_per_seq", ",", "\n", "rng", ")", "\n", "\n", "output_files", "=", "FLAGS", ".", "output_file", ".", "split", "(", "\",\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"*** Writing to output files ***\"", ")", "\n", "for", "output_file", "in", "output_files", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"  %s\"", ",", "output_file", ")", "\n", "\n", "", "write_instance_to_example_files", "(", "instances", ",", "tokenizer", ",", "FLAGS", ".", "max_seq_length", ",", "\n", "FLAGS", ".", "max_predictions_per_seq", ",", "output_files", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertConfig.__init__": [[38, 85], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "16", ",", "\n", "initializer_range", "=", "0.02", ")", ":", "\n", "    ", "\"\"\"Constructs BertConfig.\n\n    Args:\n      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n      hidden_size: Size of the encoder layers and the pooler layer.\n      num_hidden_layers: Number of hidden layers in the Transformer encoder.\n      num_attention_heads: Number of attention heads for each attention layer in\n        the Transformer encoder.\n      intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n        layer in the Transformer encoder.\n      hidden_act: The non-linear activation function (function or string) in the\n        encoder and pooler.\n      hidden_dropout_prob: The dropout probability for all fully connected\n        layers in the embeddings, encoder, and pooler.\n      attention_probs_dropout_prob: The dropout ratio for the attention\n        probabilities.\n      max_position_embeddings: The maximum sequence length that this model might\n        ever be used with. Typically set this to something large just in case\n        (e.g., 512 or 1024 or 2048).\n      type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n        `BertModel`.\n      initializer_range: The stdev of the truncated_normal_initializer for\n        initializing all weight matrices.\n    \"\"\"", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertConfig.from_dict": [[86, 93], ["modeling.BertConfig", "six.iteritems"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "    ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size", "=", "None", ")", "\n", "for", "(", "key", ",", "value", ")", "in", "six", ".", "iteritems", "(", "json_object", ")", ":", "\n", "      ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertConfig.from_json_file": [[94, 100], ["cls.from_dict", "tensorflow.gfile.GFile", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "    ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "json_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "      ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertConfig.to_dict": [[101, 105], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "    ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertConfig.to_json_string": [[106, 109], ["json.dumps", "modeling.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "    ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertModel.__init__": [[135, 237], ["copy.deepcopy", "modeling.get_shape_list", "tensorflow.ones", "tensorflow.zeros", "tensorflow.variable_scope", "tensorflow.variable_scope", "modeling.embedding_lookup", "modeling.embedding_postprocessor", "tensorflow.variable_scope", "modeling.create_attention_mask_from_input_mask", "modeling.transformer_model", "tensorflow.variable_scope", "tensorflow.squeeze", "tensorflow.contrib.model_pruning.python.layers.layers.masked_fully_connected", "modeling.get_activation", "modeling.create_initializer"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.embedding_lookup", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.embedding_postprocessor", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.create_attention_mask_from_input_mask", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.transformer_model", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.get_activation", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.create_initializer"], ["def", "__init__", "(", "self", ",", "\n", "config", ",", "\n", "is_training", ",", "\n", "input_ids", ",", "\n", "input_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "use_one_hot_embeddings", "=", "False", ",", "\n", "scope", "=", "None", ")", ":", "\n", "    ", "\"\"\"Constructor for BertModel.\n\n    Args:\n      config: `BertConfig` instance.\n      is_training: bool. true for training model, false for eval model. Controls\n        whether dropout will be applied.\n      input_ids: int32 Tensor of shape [batch_size, seq_length].\n      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n        embeddings or tf.embedding_lookup() for the word embeddings.\n      scope: (optional) variable scope. Defaults to \"bert\".\n\n    Raises:\n      ValueError: The config is invalid or one of the input tensor shapes\n        is invalid.\n    \"\"\"", "\n", "config", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "if", "not", "is_training", ":", "\n", "      ", "config", ".", "hidden_dropout_prob", "=", "0.0", "\n", "config", ".", "attention_probs_dropout_prob", "=", "0.0", "\n", "\n", "", "input_shape", "=", "get_shape_list", "(", "input_ids", ",", "expected_rank", "=", "2", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "\n", "if", "input_mask", "is", "None", ":", "\n", "      ", "input_mask", "=", "tf", ".", "ones", "(", "shape", "=", "[", "batch_size", ",", "seq_length", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "      ", "token_type_ids", "=", "tf", ".", "zeros", "(", "shape", "=", "[", "batch_size", ",", "seq_length", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"bert\"", ")", ":", "\n", "      ", "with", "tf", ".", "variable_scope", "(", "\"embeddings\"", ")", ":", "\n", "# Perform embedding lookup on the word ids.", "\n", "        ", "(", "self", ".", "embedding_output", ",", "self", ".", "embedding_table", ")", "=", "embedding_lookup", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "vocab_size", "=", "config", ".", "vocab_size", ",", "\n", "embedding_size", "=", "config", ".", "hidden_size", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "word_embedding_name", "=", "\"word_embeddings\"", ",", "\n", "use_one_hot_embeddings", "=", "use_one_hot_embeddings", ")", "\n", "\n", "# Add positional embeddings and token type embeddings, then layer", "\n", "# normalize and perform dropout.", "\n", "self", ".", "embedding_output", "=", "embedding_postprocessor", "(", "\n", "input_tensor", "=", "self", ".", "embedding_output", ",", "\n", "use_token_type", "=", "True", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "token_type_vocab_size", "=", "config", ".", "type_vocab_size", ",", "\n", "token_type_embedding_name", "=", "\"token_type_embeddings\"", ",", "\n", "use_position_embeddings", "=", "True", ",", "\n", "position_embedding_name", "=", "\"position_embeddings\"", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "max_position_embeddings", "=", "config", ".", "max_position_embeddings", ",", "\n", "dropout_prob", "=", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"encoder\"", ")", ":", "\n", "# This converts a 2D mask of shape [batch_size, seq_length] to a 3D", "\n", "# mask of shape [batch_size, seq_length, seq_length] which is used", "\n", "# for the attention scores.", "\n", "        ", "attention_mask", "=", "create_attention_mask_from_input_mask", "(", "\n", "input_ids", ",", "input_mask", ")", "\n", "\n", "# Run the stacked transformer.", "\n", "# `sequence_output` shape = [batch_size, seq_length, hidden_size].", "\n", "self", ".", "all_encoder_layers", "=", "transformer_model", "(", "\n", "input_tensor", "=", "self", ".", "embedding_output", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "hidden_size", "=", "config", ".", "hidden_size", ",", "\n", "num_hidden_layers", "=", "config", ".", "num_hidden_layers", ",", "\n", "num_attention_heads", "=", "config", ".", "num_attention_heads", ",", "\n", "intermediate_size", "=", "config", ".", "intermediate_size", ",", "\n", "intermediate_act_fn", "=", "get_activation", "(", "config", ".", "hidden_act", ")", ",", "\n", "hidden_dropout_prob", "=", "config", ".", "hidden_dropout_prob", ",", "\n", "attention_probs_dropout_prob", "=", "config", ".", "attention_probs_dropout_prob", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "do_return_all_layers", "=", "True", ")", "\n", "\n", "", "self", ".", "sequence_output", "=", "self", ".", "all_encoder_layers", "[", "-", "1", "]", "\n", "# The \"pooler\" converts the encoded sequence tensor of shape", "\n", "# [batch_size, seq_length, hidden_size] to a tensor of shape", "\n", "# [batch_size, hidden_size]. This is necessary for segment-level", "\n", "# (or segment-pair-level) classification tasks where we need a fixed", "\n", "# dimensional representation of the segment.", "\n", "with", "tf", ".", "variable_scope", "(", "\"pooler\"", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token. We assume that this has been pre-trained", "\n", "        ", "first_token_tensor", "=", "tf", ".", "squeeze", "(", "self", ".", "sequence_output", "[", ":", ",", "0", ":", "1", ",", ":", "]", ",", "axis", "=", "1", ")", "\n", "self", ".", "pooled_output", "=", "masked_fully_connected", "(", "\n", "first_token_tensor", ",", "\n", "config", ".", "hidden_size", ",", "\n", "activation_fn", "=", "tf", ".", "tanh", ",", "\n", "weights_initializer", "=", "create_initializer", "(", "config", ".", "initializer_range", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertModel.get_pooled_output": [[238, 240], ["None"], "methods", ["None"], ["", "", "", "def", "get_pooled_output", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertModel.get_sequence_output": [[241, 249], ["None"], "methods", ["None"], ["", "def", "get_sequence_output", "(", "self", ")", ":", "\n", "    ", "\"\"\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \"\"\"", "\n", "return", "self", ".", "sequence_output", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertModel.get_all_encoder_layers": [[250, 252], ["None"], "methods", ["None"], ["", "def", "get_all_encoder_layers", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "all_encoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertModel.get_embedding_output": [[253, 263], ["None"], "methods", ["None"], ["", "def", "get_embedding_output", "(", "self", ")", ":", "\n", "    ", "\"\"\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \"\"\"", "\n", "return", "self", ".", "embedding_output", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertModel.get_embedding_table": [[264, 266], ["None"], "methods", ["None"], ["", "def", "get_embedding_table", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "embedding_table", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.gelu": [[268, 282], ["tensorflow.tanh", "numpy.sqrt", "tensorflow.pow"], "function", ["None"], ["", "", "def", "gelu", "(", "x", ")", ":", "\n", "  ", "\"\"\"Gaussian Error Linear Unit.\n\n  This is a smoother version of the RELU.\n  Original paper: https://arxiv.org/abs/1606.08415\n  Args:\n    x: float Tensor to perform activation.\n\n  Returns:\n    `x` with the GELU activation applied.\n  \"\"\"", "\n", "cdf", "=", "0.5", "*", "(", "1.0", "+", "tf", ".", "tanh", "(", "\n", "(", "np", ".", "sqrt", "(", "2", "/", "np", ".", "pi", ")", "*", "(", "x", "+", "0.044715", "*", "tf", ".", "pow", "(", "x", ",", "3", ")", ")", ")", ")", ")", "\n", "return", "x", "*", "cdf", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.get_activation": [[284, 319], ["activation_string.lower", "isinstance", "ValueError"], "function", ["None"], ["", "def", "get_activation", "(", "activation_string", ")", ":", "\n", "  ", "\"\"\"Maps a string to a Python function, e.g., \"relu\" => `tf.nn.relu`.\n\n  Args:\n    activation_string: String name of the activation function.\n\n  Returns:\n    A Python function corresponding to the activation function. If\n    `activation_string` is None, empty, or \"linear\", this will return None.\n    If `activation_string` is not a string, it will return `activation_string`.\n\n  Raises:\n    ValueError: The `activation_string` does not correspond to a known\n      activation.\n  \"\"\"", "\n", "\n", "# We assume that anything that\"s not a string is already an activation", "\n", "# function, so we just return it.", "\n", "if", "not", "isinstance", "(", "activation_string", ",", "six", ".", "string_types", ")", ":", "\n", "    ", "return", "activation_string", "\n", "\n", "", "if", "not", "activation_string", ":", "\n", "    ", "return", "None", "\n", "\n", "", "act", "=", "activation_string", ".", "lower", "(", ")", "\n", "if", "act", "==", "\"linear\"", ":", "\n", "    ", "return", "None", "\n", "", "elif", "act", "==", "\"relu\"", ":", "\n", "    ", "return", "tf", ".", "nn", ".", "relu", "\n", "", "elif", "act", "==", "\"gelu\"", ":", "\n", "    ", "return", "gelu", "\n", "", "elif", "act", "==", "\"tanh\"", ":", "\n", "    ", "return", "tf", ".", "tanh", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Unsupported activation: %s\"", "%", "act", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.get_assignment_map_from_checkpoint": [[321, 355], ["tensorflow.global_variables", "collections.OrderedDict", "tensorflow.train.list_variables", "collections.OrderedDict", "re.match", "tvars.append", "re.match.group"], "function", ["None"], ["", "", "def", "get_assignment_map_from_checkpoint", "(", "tvars", ",", "init_checkpoint", ")", ":", "\n", "  ", "\"\"\"Compute the union of the current variables and checkpoint variables.\"\"\"", "\n", "assignment_map", "=", "{", "}", "\n", "initialized_variable_names", "=", "{", "}", "\n", "\n", "# Add mask variables to the list of \"trainable\" variables.", "\n", "# Even though they're not \"trainable\" by gradient descent,", "\n", "# we still need to load them into our model.", "\n", "for", "var", "in", "tf", ".", "global_variables", "(", ")", ":", "\n", "    ", "if", "var", "not", "in", "tvars", "and", "'mask'", "in", "var", ".", "name", ":", "\n", "      ", "tvars", ".", "append", "(", "var", ")", "\n", "\n", "", "", "name_to_variable", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "var", "in", "tvars", ":", "\n", "    ", "name", "=", "var", ".", "name", "\n", "m", "=", "re", ".", "match", "(", "\"^(.*):\\\\d+$\"", ",", "name", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "      ", "name", "=", "m", ".", "group", "(", "1", ")", "\n", "", "name_to_variable", "[", "name", "]", "=", "var", "\n", "\n", "", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "init_checkpoint", ")", "\n", "\n", "assignment_map", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "x", "in", "init_vars", ":", "\n", "    ", "(", "name", ",", "var", ")", "=", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", "\n", "assert", "name", "!=", "\"bert/embeddings//mask\"", ",", "\"Please run checkpoint_utils/fix_embed_name.py on this checkpoint.\"", "\n", "if", "name", "not", "in", "name_to_variable", ":", "\n", "      ", "continue", "\n", "", "assignment_map", "[", "name", "]", "=", "name", "\n", "initialized_variable_names", "[", "name", "]", "=", "1", "\n", "initialized_variable_names", "[", "name", "+", "\":0\"", "]", "=", "1", "\n", "\n", "\n", "", "return", "(", "assignment_map", ",", "initialized_variable_names", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.dropout": [[357, 373], ["tensorflow.nn.dropout"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.dropout"], ["", "def", "dropout", "(", "input_tensor", ",", "dropout_prob", ")", ":", "\n", "  ", "\"\"\"Perform dropout.\n\n  Args:\n    input_tensor: float Tensor.\n    dropout_prob: Python float. The probability of dropping out a value (NOT of\n      *keeping* a dimension as in `tf.nn.dropout`).\n\n  Returns:\n    A version of `input_tensor` with dropout applied.\n  \"\"\"", "\n", "if", "dropout_prob", "is", "None", "or", "dropout_prob", "==", "0.0", ":", "\n", "    ", "return", "input_tensor", "\n", "\n", "", "output", "=", "tf", ".", "nn", ".", "dropout", "(", "input_tensor", ",", "1.0", "-", "dropout_prob", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.layer_norm": [[375, 379], ["tensorflow.contrib.layers.layer_norm"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.layer_norm"], ["", "def", "layer_norm", "(", "input_tensor", ",", "name", "=", "None", ")", ":", "\n", "  ", "\"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"", "\n", "return", "tf", ".", "contrib", ".", "layers", ".", "layer_norm", "(", "\n", "inputs", "=", "input_tensor", ",", "begin_norm_axis", "=", "-", "1", ",", "begin_params_axis", "=", "-", "1", ",", "scope", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.layer_norm_and_dropout": [[381, 386], ["modeling.layer_norm", "modeling.dropout"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.layer_norm", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.dropout"], ["", "def", "layer_norm_and_dropout", "(", "input_tensor", ",", "dropout_prob", ",", "name", "=", "None", ")", ":", "\n", "  ", "\"\"\"Runs layer normalization followed by dropout.\"\"\"", "\n", "output_tensor", "=", "layer_norm", "(", "input_tensor", ",", "name", ")", "\n", "output_tensor", "=", "dropout", "(", "output_tensor", ",", "dropout_prob", ")", "\n", "return", "output_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.create_initializer": [[388, 391], ["tensorflow.truncated_normal_initializer"], "function", ["None"], ["", "def", "create_initializer", "(", "initializer_range", "=", "0.02", ")", ":", "\n", "  ", "\"\"\"Creates a `truncated_normal_initializer` with the given range.\"\"\"", "\n", "return", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "initializer_range", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.embedding_lookup": [[393, 439], ["tensorflow.contrib.model_pruning.python.pruning.apply_mask", "tensorflow.reshape", "modeling.get_shape_list", "tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.get_variable", "tensorflow.one_hot", "tensorflow.matmul", "tensorflow.gather", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.create_initializer"], ["", "def", "embedding_lookup", "(", "input_ids", ",", "\n", "vocab_size", ",", "\n", "embedding_size", "=", "128", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "word_embedding_name", "=", "\"word_embeddings\"", ",", "\n", "use_one_hot_embeddings", "=", "False", ")", ":", "\n", "  ", "\"\"\"Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.gather()`.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  \"\"\"", "\n", "# This function assumes that the input is of shape [batch_size, seq_length,", "\n", "# num_inputs].", "\n", "#", "\n", "# If the input is a 2D tensor of shape [batch_size, seq_length], we", "\n", "# reshape to [batch_size, seq_length, 1].", "\n", "if", "input_ids", ".", "shape", ".", "ndims", "==", "2", ":", "\n", "    ", "input_ids", "=", "tf", ".", "expand_dims", "(", "input_ids", ",", "axis", "=", "[", "-", "1", "]", ")", "\n", "\n", "", "embedding_table", "=", "apply_mask", "(", "tf", ".", "get_variable", "(", "\n", "name", "=", "word_embedding_name", ",", "\n", "shape", "=", "[", "vocab_size", ",", "embedding_size", "]", ",", "\n", "initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", ",", "scope", "=", "'embed_mask'", ")", "\n", "\n", "flat_input_ids", "=", "tf", ".", "reshape", "(", "input_ids", ",", "[", "-", "1", "]", ")", "\n", "if", "use_one_hot_embeddings", ":", "\n", "    ", "one_hot_input_ids", "=", "tf", ".", "one_hot", "(", "flat_input_ids", ",", "depth", "=", "vocab_size", ")", "\n", "output", "=", "tf", ".", "matmul", "(", "one_hot_input_ids", ",", "embedding_table", ")", "\n", "", "else", ":", "\n", "    ", "output", "=", "tf", ".", "gather", "(", "embedding_table", ",", "flat_input_ids", ")", "\n", "\n", "", "input_shape", "=", "get_shape_list", "(", "input_ids", ")", "\n", "\n", "output", "=", "tf", ".", "reshape", "(", "output", ",", "\n", "input_shape", "[", "0", ":", "-", "1", "]", "+", "[", "input_shape", "[", "-", "1", "]", "*", "embedding_size", "]", ")", "\n", "return", "(", "output", ",", "embedding_table", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.embedding_postprocessor": [[441, 535], ["modeling.get_shape_list", "modeling.layer_norm_and_dropout", "tensorflow.get_variable", "tensorflow.reshape", "tensorflow.one_hot", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.assert_less_equal", "ValueError", "tensorflow.control_dependencies", "tensorflow.get_variable", "tensorflow.slice", "len", "range", "position_broadcast_shape.extend", "tensorflow.reshape", "modeling.create_initializer", "layer_norm_and_dropout.shape.as_list", "position_broadcast_shape.append", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.layer_norm_and_dropout", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.create_initializer", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.create_initializer"], ["", "def", "embedding_postprocessor", "(", "input_tensor", ",", "\n", "use_token_type", "=", "False", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "token_type_vocab_size", "=", "16", ",", "\n", "token_type_embedding_name", "=", "\"token_type_embeddings\"", ",", "\n", "use_position_embeddings", "=", "True", ",", "\n", "position_embedding_name", "=", "\"position_embeddings\"", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "dropout_prob", "=", "0.1", ")", ":", "\n", "  ", "\"\"\"Performs various post-processing on a word embedding tensor.\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length,\n      embedding_size].\n    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      Must be specified if `use_token_type` is True.\n    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n    token_type_embedding_name: string. The name of the embedding table variable\n      for token type ids.\n    use_position_embeddings: bool. Whether to add position embeddings for the\n      position of each token in the sequence.\n    position_embedding_name: string. The name of the embedding table variable\n      for positional embeddings.\n    initializer_range: float. Range of the weight initialization.\n    max_position_embeddings: int. Maximum sequence length that might ever be\n      used with this model. This can be longer than the sequence length of\n      input_tensor, but cannot be shorter.\n    dropout_prob: float. Dropout probability applied to the final output tensor.\n\n  Returns:\n    float tensor with same shape as `input_tensor`.\n\n  Raises:\n    ValueError: One of the tensor shapes or input values is invalid.\n  \"\"\"", "\n", "input_shape", "=", "get_shape_list", "(", "input_tensor", ",", "expected_rank", "=", "3", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "width", "=", "input_shape", "[", "2", "]", "\n", "\n", "output", "=", "input_tensor", "\n", "\n", "if", "use_token_type", ":", "\n", "    ", "if", "token_type_ids", "is", "None", ":", "\n", "      ", "raise", "ValueError", "(", "\"`token_type_ids` must be specified if\"", "\n", "\"`use_token_type` is True.\"", ")", "\n", "", "token_type_table", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "token_type_embedding_name", ",", "\n", "shape", "=", "[", "token_type_vocab_size", ",", "width", "]", ",", "\n", "initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "# This vocab will be small so we always do one-hot here, since it is always", "\n", "# faster for a small vocabulary.", "\n", "flat_token_type_ids", "=", "tf", ".", "reshape", "(", "token_type_ids", ",", "[", "-", "1", "]", ")", "\n", "one_hot_ids", "=", "tf", ".", "one_hot", "(", "flat_token_type_ids", ",", "depth", "=", "token_type_vocab_size", ")", "\n", "token_type_embeddings", "=", "tf", ".", "matmul", "(", "one_hot_ids", ",", "token_type_table", ")", "\n", "token_type_embeddings", "=", "tf", ".", "reshape", "(", "token_type_embeddings", ",", "\n", "[", "batch_size", ",", "seq_length", ",", "width", "]", ")", "\n", "output", "+=", "token_type_embeddings", "\n", "\n", "", "if", "use_position_embeddings", ":", "\n", "    ", "assert_op", "=", "tf", ".", "assert_less_equal", "(", "seq_length", ",", "max_position_embeddings", ")", "\n", "with", "tf", ".", "control_dependencies", "(", "[", "assert_op", "]", ")", ":", "\n", "      ", "full_position_embeddings", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "position_embedding_name", ",", "\n", "shape", "=", "[", "max_position_embeddings", ",", "width", "]", ",", "\n", "initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "# Since the position embedding table is a learned variable, we create it", "\n", "# using a (long) sequence length `max_position_embeddings`. The actual", "\n", "# sequence length might be shorter than this, for faster training of", "\n", "# tasks that do not have long sequences.", "\n", "#", "\n", "# So `full_position_embeddings` is effectively an embedding table", "\n", "# for position [0, 1, 2, ..., max_position_embeddings-1], and the current", "\n", "# sequence has positions [0, 1, 2, ... seq_length-1], so we can just", "\n", "# perform a slice.", "\n", "position_embeddings", "=", "tf", ".", "slice", "(", "full_position_embeddings", ",", "[", "0", ",", "0", "]", ",", "\n", "[", "seq_length", ",", "-", "1", "]", ")", "\n", "num_dims", "=", "len", "(", "output", ".", "shape", ".", "as_list", "(", ")", ")", "\n", "\n", "# Only the last two dimensions are relevant (`seq_length` and `width`), so", "\n", "# we broadcast among the first dimensions, which is typically just", "\n", "# the batch size.", "\n", "position_broadcast_shape", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "num_dims", "-", "2", ")", ":", "\n", "        ", "position_broadcast_shape", ".", "append", "(", "1", ")", "\n", "", "position_broadcast_shape", ".", "extend", "(", "[", "seq_length", ",", "width", "]", ")", "\n", "position_embeddings", "=", "tf", ".", "reshape", "(", "position_embeddings", ",", "\n", "position_broadcast_shape", ")", "\n", "output", "+=", "position_embeddings", "\n", "\n", "", "", "output", "=", "layer_norm_and_dropout", "(", "output", ",", "dropout_prob", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.create_attention_mask_from_input_mask": [[537, 569], ["modeling.get_shape_list", "modeling.get_shape_list", "tensorflow.cast", "tensorflow.ones", "tensorflow.reshape"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.get_shape_list"], ["", "def", "create_attention_mask_from_input_mask", "(", "from_tensor", ",", "to_mask", ")", ":", "\n", "  ", "\"\"\"Create 3D attention mask from a 2D tensor mask.\n\n  Args:\n    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n  \"\"\"", "\n", "from_shape", "=", "get_shape_list", "(", "from_tensor", ",", "expected_rank", "=", "[", "2", ",", "3", "]", ")", "\n", "batch_size", "=", "from_shape", "[", "0", "]", "\n", "from_seq_length", "=", "from_shape", "[", "1", "]", "\n", "\n", "to_shape", "=", "get_shape_list", "(", "to_mask", ",", "expected_rank", "=", "2", ")", "\n", "to_seq_length", "=", "to_shape", "[", "1", "]", "\n", "\n", "to_mask", "=", "tf", ".", "cast", "(", "\n", "tf", ".", "reshape", "(", "to_mask", ",", "[", "batch_size", ",", "1", ",", "to_seq_length", "]", ")", ",", "tf", ".", "float32", ")", "\n", "\n", "# We don't assume that `from_tensor` is a mask (although it could be). We", "\n", "# don't actually care if we attend *from* padding tokens (only *to* padding)", "\n", "# tokens so we create a tensor of all ones.", "\n", "#", "\n", "# `broadcast_ones` = [batch_size, from_seq_length, 1]", "\n", "broadcast_ones", "=", "tf", ".", "ones", "(", "\n", "shape", "=", "[", "batch_size", ",", "from_seq_length", ",", "1", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "# Here we broadcast along two dimensions to create the mask.", "\n", "mask", "=", "broadcast_ones", "*", "to_mask", "\n", "\n", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.attention_layer": [[571, 765], ["modeling.get_shape_list", "modeling.get_shape_list", "modeling.reshape_to_matrix", "modeling.reshape_to_matrix", "tensorflow.contrib.model_pruning.python.layers.layers.masked_fully_connected", "tensorflow.contrib.model_pruning.python.layers.layers.masked_fully_connected", "tensorflow.contrib.model_pruning.python.layers.layers.masked_fully_connected", "modeling.attention_layer.transpose_for_scores"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.reshape_to_matrix", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.reshape_to_matrix"], ["", "def", "attention_layer", "(", "from_tensor", ",", "\n", "to_tensor", ",", "\n", "attention_mask", "=", "None", ",", "\n", "num_attention_heads", "=", "1", ",", "\n", "size_per_head", "=", "512", ",", "\n", "query_act", "=", "None", ",", "\n", "key_act", "=", "None", ",", "\n", "value_act", "=", "None", ",", "\n", "attention_probs_dropout_prob", "=", "0.0", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "do_return_2d_tensor", "=", "False", ",", "\n", "batch_size", "=", "None", ",", "\n", "from_seq_length", "=", "None", ",", "\n", "to_seq_length", "=", "None", ")", ":", "\n", "  ", "\"\"\"Performs multi-headed attention from `from_tensor` to `to_tensor`.\n\n  This is an implementation of multi-headed attention based on \"Attention\n  is all you Need\". If `from_tensor` and `to_tensor` are the same, then\n  this is self-attention. Each timestep in `from_tensor` attends to the\n  corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n\n  This function first projects `from_tensor` into a \"query\" tensor and\n  `to_tensor` into \"key\" and \"value\" tensors. These are (effectively) a list\n  of tensors of length `num_attention_heads`, where each tensor is of shape\n  [batch_size, seq_length, size_per_head].\n\n  Then, the query and key tensors are dot-producted and scaled. These are\n  softmaxed to obtain attention probabilities. The value tensors are then\n  interpolated by these probabilities, then concatenated back to a single\n  tensor and returned.\n\n  In practice, the multi-headed attention are done with transposes and\n  reshapes rather than actual separate tensors.\n\n  Args:\n    from_tensor: float Tensor of shape [batch_size, from_seq_length,\n      from_width].\n    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n    attention_mask: (optional) int32 Tensor of shape [batch_size,\n      from_seq_length, to_seq_length]. The values should be 1 or 0. The\n      attention scores will effectively be set to -infinity for any positions in\n      the mask that are 0, and will be unchanged for positions that are 1.\n    num_attention_heads: int. Number of attention heads.\n    size_per_head: int. Size of each attention head.\n    query_act: (optional) Activation function for the query transform.\n    key_act: (optional) Activation function for the key transform.\n    value_act: (optional) Activation function for the value transform.\n    attention_probs_dropout_prob: (optional) float. Dropout probability of the\n      attention probabilities.\n    initializer_range: float. Range of the weight initializer.\n    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n      * from_seq_length, num_attention_heads * size_per_head]. If False, the\n      output will be of shape [batch_size, from_seq_length, num_attention_heads\n      * size_per_head].\n    batch_size: (Optional) int. If the input is 2D, this might be the batch size\n      of the 3D version of the `from_tensor` and `to_tensor`.\n    from_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `from_tensor`.\n    to_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `to_tensor`.\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length,\n      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n      true, this will be of shape [batch_size * from_seq_length,\n      num_attention_heads * size_per_head]).\n\n  Raises:\n    ValueError: Any of the arguments or tensor shapes are invalid.\n  \"\"\"", "\n", "\n", "def", "transpose_for_scores", "(", "input_tensor", ",", "batch_size", ",", "num_attention_heads", ",", "\n", "seq_length", ",", "width", ")", ":", "\n", "    ", "output_tensor", "=", "tf", ".", "reshape", "(", "\n", "input_tensor", ",", "[", "batch_size", ",", "seq_length", ",", "num_attention_heads", ",", "width", "]", ")", "\n", "\n", "output_tensor", "=", "tf", ".", "transpose", "(", "output_tensor", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "return", "output_tensor", "\n", "\n", "", "from_shape", "=", "get_shape_list", "(", "from_tensor", ",", "expected_rank", "=", "[", "2", ",", "3", "]", ")", "\n", "to_shape", "=", "get_shape_list", "(", "to_tensor", ",", "expected_rank", "=", "[", "2", ",", "3", "]", ")", "\n", "\n", "if", "len", "(", "from_shape", ")", "!=", "len", "(", "to_shape", ")", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"The rank of `from_tensor` must match the rank of `to_tensor`.\"", ")", "\n", "\n", "", "if", "len", "(", "from_shape", ")", "==", "3", ":", "\n", "    ", "batch_size", "=", "from_shape", "[", "0", "]", "\n", "from_seq_length", "=", "from_shape", "[", "1", "]", "\n", "to_seq_length", "=", "to_shape", "[", "1", "]", "\n", "", "elif", "len", "(", "from_shape", ")", "==", "2", ":", "\n", "    ", "if", "(", "batch_size", "is", "None", "or", "from_seq_length", "is", "None", "or", "to_seq_length", "is", "None", ")", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "\"When passing in rank 2 tensors to attention_layer, the values \"", "\n", "\"for `batch_size`, `from_seq_length`, and `to_seq_length` \"", "\n", "\"must all be specified.\"", ")", "\n", "\n", "# Scalar dimensions referenced here:", "\n", "#   B = batch size (number of sequences)", "\n", "#   F = `from_tensor` sequence length", "\n", "#   T = `to_tensor` sequence length", "\n", "#   N = `num_attention_heads`", "\n", "#   H = `size_per_head`", "\n", "\n", "", "", "from_tensor_2d", "=", "reshape_to_matrix", "(", "from_tensor", ")", "\n", "to_tensor_2d", "=", "reshape_to_matrix", "(", "to_tensor", ")", "\n", "\n", "# `query_layer` = [B*F, N*H]", "\n", "query_layer", "=", "masked_fully_connected", "(", "\n", "from_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation_fn", "=", "query_act", ",", "\n", "scope", "=", "\"query\"", ",", "\n", "weights_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# `key_layer` = [B*T, N*H]", "\n", "key_layer", "=", "masked_fully_connected", "(", "\n", "to_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation_fn", "=", "key_act", ",", "\n", "scope", "=", "\"key\"", ",", "\n", "weights_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# `value_layer` = [B*T, N*H]", "\n", "value_layer", "=", "masked_fully_connected", "(", "\n", "to_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation_fn", "=", "value_act", ",", "\n", "scope", "=", "\"value\"", ",", "\n", "weights_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# `query_layer` = [B, N, F, H]", "\n", "query_layer", "=", "transpose_for_scores", "(", "query_layer", ",", "batch_size", ",", "\n", "num_attention_heads", ",", "from_seq_length", ",", "\n", "size_per_head", ")", "\n", "\n", "# `key_layer` = [B, N, T, H]", "\n", "key_layer", "=", "transpose_for_scores", "(", "key_layer", ",", "batch_size", ",", "num_attention_heads", ",", "\n", "to_seq_length", ",", "size_per_head", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw", "\n", "# attention scores.", "\n", "# `attention_scores` = [B, N, F, T]", "\n", "attention_scores", "=", "tf", ".", "matmul", "(", "query_layer", ",", "key_layer", ",", "transpose_b", "=", "True", ")", "\n", "attention_scores", "=", "tf", ".", "multiply", "(", "attention_scores", ",", "\n", "1.0", "/", "math", ".", "sqrt", "(", "float", "(", "size_per_head", ")", ")", ")", "\n", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "# `attention_mask` = [B, 1, F, T]", "\n", "    ", "attention_mask", "=", "tf", ".", "expand_dims", "(", "attention_mask", ",", "axis", "=", "[", "1", "]", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "adder", "=", "(", "1.0", "-", "tf", ".", "cast", "(", "attention_mask", ",", "tf", ".", "float32", ")", ")", "*", "-", "10000.0", "\n", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "attention_scores", "+=", "adder", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "# `attention_probs` = [B, N, F, T]", "\n", "", "attention_probs", "=", "tf", ".", "nn", ".", "softmax", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "dropout", "(", "attention_probs", ",", "attention_probs_dropout_prob", ")", "\n", "\n", "# `value_layer` = [B, T, N, H]", "\n", "value_layer", "=", "tf", ".", "reshape", "(", "\n", "value_layer", ",", "\n", "[", "batch_size", ",", "to_seq_length", ",", "num_attention_heads", ",", "size_per_head", "]", ")", "\n", "\n", "# `value_layer` = [B, N, T, H]", "\n", "value_layer", "=", "tf", ".", "transpose", "(", "value_layer", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n", "# `context_layer` = [B, N, F, H]", "\n", "context_layer", "=", "tf", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "\n", "# `context_layer` = [B, F, N, H]", "\n", "context_layer", "=", "tf", ".", "transpose", "(", "context_layer", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n", "if", "do_return_2d_tensor", ":", "\n", "# `context_layer` = [B*F, N*H]", "\n", "    ", "context_layer", "=", "tf", ".", "reshape", "(", "\n", "context_layer", ",", "\n", "[", "batch_size", "*", "from_seq_length", ",", "num_attention_heads", "*", "size_per_head", "]", ")", "\n", "", "else", ":", "\n", "# `context_layer` = [B, F, N*H]", "\n", "    ", "context_layer", "=", "tf", ".", "reshape", "(", "\n", "context_layer", ",", "\n", "[", "batch_size", ",", "from_seq_length", ",", "num_attention_heads", "*", "size_per_head", "]", ")", "\n", "\n", "", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.transformer_model": [[767, 906], ["int", "modeling.get_shape_list", "modeling.reshape_to_matrix", "range", "ValueError", "ValueError", "modeling.reshape_from_matrix", "tensorflow.variable_scope", "modeling.reshape_from_matrix", "final_outputs.append", "tensorflow.variable_scope", "tensorflow.variable_scope", "tensorflow.contrib.model_pruning.python.layers.layers.masked_fully_connected", "tensorflow.variable_scope", "tensorflow.contrib.model_pruning.python.layers.layers.masked_fully_connected", "modeling.dropout", "modeling.layer_norm", "all_layer_outputs.append", "tensorflow.variable_scope", "modeling.attention_layer", "attention_heads.append", "len", "tensorflow.concat", "tensorflow.variable_scope", "tensorflow.contrib.model_pruning.python.layers.layers.masked_fully_connected", "modeling.dropout", "modeling.layer_norm", "modeling.create_initializer", "modeling.create_initializer", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.reshape_to_matrix", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.reshape_from_matrix", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.reshape_from_matrix", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.dropout", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.layer_norm", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.attention_layer", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.dropout", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.layer_norm", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.create_initializer", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.create_initializer", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.create_initializer"], ["", "def", "transformer_model", "(", "input_tensor", ",", "\n", "attention_mask", "=", "None", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "intermediate_act_fn", "=", "gelu", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "do_return_all_layers", "=", "False", ")", ":", "\n", "  ", "\"\"\"Multi-headed, multi-layer Transformer from \"Attention is All You Need\".\n\n  This is almost an exact implementation of the original Transformer encoder.\n\n  See the original paper:\n  https://arxiv.org/abs/1706.03762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the \"intermediate\" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  \"\"\"", "\n", "if", "hidden_size", "%", "num_attention_heads", "!=", "0", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "hidden_size", ",", "num_attention_heads", ")", ")", "\n", "\n", "", "attention_head_size", "=", "int", "(", "hidden_size", "/", "num_attention_heads", ")", "\n", "input_shape", "=", "get_shape_list", "(", "input_tensor", ",", "expected_rank", "=", "3", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "input_width", "=", "input_shape", "[", "2", "]", "\n", "\n", "# The Transformer performs sum residuals on all layers so the input needs", "\n", "# to be the same as the hidden size.", "\n", "if", "input_width", "!=", "hidden_size", ":", "\n", "    ", "raise", "ValueError", "(", "\"The width of the input tensor (%d) != hidden size (%d)\"", "%", "\n", "(", "input_width", ",", "hidden_size", ")", ")", "\n", "\n", "# We keep the representation as a 2D tensor to avoid re-shaping it back and", "\n", "# forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on", "\n", "# the GPU/CPU but may not be free on the TPU, so we want to minimize them to", "\n", "# help the optimizer.", "\n", "", "prev_output", "=", "reshape_to_matrix", "(", "input_tensor", ")", "\n", "\n", "all_layer_outputs", "=", "[", "]", "\n", "for", "layer_idx", "in", "range", "(", "num_hidden_layers", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "\"layer_%d\"", "%", "layer_idx", ")", ":", "\n", "      ", "layer_input", "=", "prev_output", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"attention\"", ")", ":", "\n", "        ", "attention_heads", "=", "[", "]", "\n", "with", "tf", ".", "variable_scope", "(", "\"self\"", ")", ":", "\n", "          ", "attention_head", "=", "attention_layer", "(", "\n", "from_tensor", "=", "layer_input", ",", "\n", "to_tensor", "=", "layer_input", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "num_attention_heads", "=", "num_attention_heads", ",", "\n", "size_per_head", "=", "attention_head_size", ",", "\n", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", ",", "\n", "initializer_range", "=", "initializer_range", ",", "\n", "do_return_2d_tensor", "=", "True", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "from_seq_length", "=", "seq_length", ",", "\n", "to_seq_length", "=", "seq_length", ")", "\n", "attention_heads", ".", "append", "(", "attention_head", ")", "\n", "\n", "", "attention_output", "=", "None", "\n", "if", "len", "(", "attention_heads", ")", "==", "1", ":", "\n", "          ", "attention_output", "=", "attention_heads", "[", "0", "]", "\n", "", "else", ":", "\n", "# In the case where we have other sequences, we just concatenate", "\n", "# them to the self-attention head before the projection.", "\n", "          ", "attention_output", "=", "tf", ".", "concat", "(", "attention_heads", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# Run a linear projection of `hidden_size` then add a residual", "\n", "# with `layer_input`.", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"output\"", ")", ":", "\n", "          ", "attention_output", "=", "masked_fully_connected", "(", "\n", "attention_output", ",", "\n", "hidden_size", ",", "\n", "weights_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "attention_output", "=", "dropout", "(", "attention_output", ",", "hidden_dropout_prob", ")", "\n", "attention_output", "=", "layer_norm", "(", "attention_output", "+", "layer_input", ")", "\n", "\n", "# The activation is only applied to the \"intermediate\" hidden layer.", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"intermediate\"", ")", ":", "\n", "        ", "intermediate_output", "=", "masked_fully_connected", "(", "\n", "attention_output", ",", "\n", "intermediate_size", ",", "\n", "activation_fn", "=", "intermediate_act_fn", ",", "\n", "weights_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# Down-project back to `hidden_size` then add the residual.", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"output\"", ")", ":", "\n", "        ", "layer_output", "=", "masked_fully_connected", "(", "\n", "intermediate_output", ",", "\n", "hidden_size", ",", "\n", "weights_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "layer_output", "=", "dropout", "(", "layer_output", ",", "hidden_dropout_prob", ")", "\n", "layer_output", "=", "layer_norm", "(", "layer_output", "+", "attention_output", ")", "\n", "prev_output", "=", "layer_output", "\n", "all_layer_outputs", ".", "append", "(", "layer_output", ")", "\n", "\n", "", "", "", "if", "do_return_all_layers", ":", "\n", "    ", "final_outputs", "=", "[", "]", "\n", "for", "layer_output", "in", "all_layer_outputs", ":", "\n", "      ", "final_output", "=", "reshape_from_matrix", "(", "layer_output", ",", "input_shape", ")", "\n", "final_outputs", ".", "append", "(", "final_output", ")", "\n", "", "return", "final_outputs", "\n", "", "else", ":", "\n", "    ", "final_output", "=", "reshape_from_matrix", "(", "prev_output", ",", "input_shape", ")", "\n", "return", "final_output", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.get_shape_list": [[908, 943], ["tensor.shape.as_list", "enumerate", "tensorflow.shape", "modeling.assert_rank", "non_static_indexes.append"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.assert_rank"], ["", "", "def", "get_shape_list", "(", "tensor", ",", "expected_rank", "=", "None", ",", "name", "=", "None", ")", ":", "\n", "  ", "\"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n\n  Args:\n    tensor: A tf.Tensor object to find the shape of.\n    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n      specified and the `tensor` has a different rank, and exception will be\n      thrown.\n    name: Optional name of the tensor for the error message.\n\n  Returns:\n    A list of dimensions of the shape of tensor. All static dimensions will\n    be returned as python integers, and dynamic dimensions will be returned\n    as tf.Tensor scalars.\n  \"\"\"", "\n", "if", "name", "is", "None", ":", "\n", "    ", "name", "=", "tensor", ".", "name", "\n", "\n", "", "if", "expected_rank", "is", "not", "None", ":", "\n", "    ", "assert_rank", "(", "tensor", ",", "expected_rank", ",", "name", ")", "\n", "\n", "", "shape", "=", "tensor", ".", "shape", ".", "as_list", "(", ")", "\n", "\n", "non_static_indexes", "=", "[", "]", "\n", "for", "(", "index", ",", "dim", ")", "in", "enumerate", "(", "shape", ")", ":", "\n", "    ", "if", "dim", "is", "None", ":", "\n", "      ", "non_static_indexes", ".", "append", "(", "index", ")", "\n", "\n", "", "", "if", "not", "non_static_indexes", ":", "\n", "    ", "return", "shape", "\n", "\n", "", "dyn_shape", "=", "tf", ".", "shape", "(", "tensor", ")", "\n", "for", "index", "in", "non_static_indexes", ":", "\n", "    ", "shape", "[", "index", "]", "=", "dyn_shape", "[", "index", "]", "\n", "", "return", "shape", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.reshape_to_matrix": [[945, 957], ["tensorflow.reshape", "ValueError"], "function", ["None"], ["", "def", "reshape_to_matrix", "(", "input_tensor", ")", ":", "\n", "  ", "\"\"\"Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).\"\"\"", "\n", "ndims", "=", "input_tensor", ".", "shape", ".", "ndims", "\n", "if", "ndims", "<", "2", ":", "\n", "    ", "raise", "ValueError", "(", "\"Input tensor must have at least rank 2. Shape = %s\"", "%", "\n", "(", "input_tensor", ".", "shape", ")", ")", "\n", "", "if", "ndims", "==", "2", ":", "\n", "    ", "return", "input_tensor", "\n", "\n", "", "width", "=", "input_tensor", ".", "shape", "[", "-", "1", "]", "\n", "output_tensor", "=", "tf", ".", "reshape", "(", "input_tensor", ",", "[", "-", "1", ",", "width", "]", ")", "\n", "return", "output_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.reshape_from_matrix": [[959, 970], ["modeling.get_shape_list", "tensorflow.reshape", "len"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.get_shape_list"], ["", "def", "reshape_from_matrix", "(", "output_tensor", ",", "orig_shape_list", ")", ":", "\n", "  ", "\"\"\"Reshapes a rank 2 tensor back to its original rank >= 2 tensor.\"\"\"", "\n", "if", "len", "(", "orig_shape_list", ")", "==", "2", ":", "\n", "    ", "return", "output_tensor", "\n", "\n", "", "output_shape", "=", "get_shape_list", "(", "output_tensor", ")", "\n", "\n", "orig_dims", "=", "orig_shape_list", "[", "0", ":", "-", "1", "]", "\n", "width", "=", "output_shape", "[", "-", "1", "]", "\n", "\n", "return", "tf", ".", "reshape", "(", "output_tensor", ",", "orig_dims", "+", "[", "width", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.assert_rank": [[972, 1000], ["isinstance", "ValueError", "tensorflow.get_variable_scope", "str", "str"], "function", ["None"], ["", "def", "assert_rank", "(", "tensor", ",", "expected_rank", ",", "name", "=", "None", ")", ":", "\n", "  ", "\"\"\"Raises an exception if the tensor rank is not of the expected rank.\n\n  Args:\n    tensor: A tf.Tensor to check the rank of.\n    expected_rank: Python integer or list of integers, expected rank.\n    name: Optional name of the tensor for the error message.\n\n  Raises:\n    ValueError: If the expected shape doesn't match the actual shape.\n  \"\"\"", "\n", "if", "name", "is", "None", ":", "\n", "    ", "name", "=", "tensor", ".", "name", "\n", "\n", "", "expected_rank_dict", "=", "{", "}", "\n", "if", "isinstance", "(", "expected_rank", ",", "six", ".", "integer_types", ")", ":", "\n", "    ", "expected_rank_dict", "[", "expected_rank", "]", "=", "True", "\n", "", "else", ":", "\n", "    ", "for", "x", "in", "expected_rank", ":", "\n", "      ", "expected_rank_dict", "[", "x", "]", "=", "True", "\n", "\n", "", "", "actual_rank", "=", "tensor", ".", "shape", ".", "ndims", "\n", "if", "actual_rank", "not", "in", "expected_rank_dict", ":", "\n", "    ", "scope_name", "=", "tf", ".", "get_variable_scope", "(", ")", ".", "name", "\n", "raise", "ValueError", "(", "\n", "\"For the tensor `%s` in scope `%s`, the actual rank \"", "\n", "\"`%d` (shape = %s) is not equal to the expected rank `%s`\"", "%", "\n", "(", "name", ",", "scope_name", ",", "actual_rank", ",", "str", "(", "tensor", ".", "shape", ")", ",", "str", "(", "expected_rank", ")", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.gradients_speed": [[23, 25], ["tensorflow.python.ops.gradients"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.gradients"], ["def", "gradients_speed", "(", "ys", ",", "xs", ",", "grad_ys", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "gradients", "(", "ys", ",", "xs", ",", "grad_ys", ",", "checkpoints", "=", "'speed'", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.gradients_memory": [[26, 28], ["tensorflow.python.ops.gradients"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.gradients"], ["", "def", "gradients_memory", "(", "ys", ",", "xs", ",", "grad_ys", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "gradients", "(", "ys", ",", "xs", ",", "grad_ys", ",", "checkpoints", "=", "'memory'", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.gradients_collection": [[29, 31], ["tensorflow.python.ops.gradients"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.gradients"], ["", "def", "gradients_collection", "(", "ys", ",", "xs", ",", "grad_ys", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "gradients", "(", "ys", ",", "xs", ",", "grad_ys", ",", "checkpoints", "=", "'collection'", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.gradients": [[32, 301], ["tensorflow.get_backward_walk_ops", "memory_saving_gradients.debug_print", "tensorflow.get_forward_walk_ops", "memory_saving_gradients.debug_print", "memory_saving_gradients._to_ops", "tensorflow.filter_ts", "list", "isinstance", "memory_saving_gradients.debug_print", "set().intersection", "set().intersection", "memory_saving_gradients.debug_print", "list", "memory_saving_gradients.fast_backward_ops", "memory_saving_gradients.debug_print", "memory_saving_gradients.debug_print", "memory_saving_gradients.debug_print", "tensorflow.copy_with_input_replacements", "info._transformed_ops.items", "info._transformed_ops.values", "memory_saving_gradients.debug_print", "tensorflow.reroute_ts", "memory_saving_gradients.debug_print", "list", "tf_gradients", "memory_saving_gradients.debug_print", "memory_saving_gradients.debug_print", "memory_saving_gradients.debug_print", "memory_saving_gradients.my_add_control_inputs", "memory_saving_gradients.tf_toposort", "isinstance", "isinstance", "set", "type", "set().intersection", "set", "memory_saving_gradients.debug_print", "set", "memory_saving_gradients.debug_print", "Exception", "len", "tensorflow.sgv", "op._set_device", "checkpoints_disconnected.values", "checkpoints_disconnected.keys", "checkpoints_disconnected.values", "checkpoints_disconnected.keys", "checkpoints_disconnected.values", "list", "memory_saving_gradients.debug_print", "memory_saving_gradients.fast_backward_ops", "memory_saving_gradients.debug_print", "memory_saving_gradients.debug_print", "tensorflow.copy_with_input_replacements", "info._transformed_ops.items", "info._transformed_ops.values", "memory_saving_gradients.debug_print", "tensorflow.reroute_ts", "memory_saving_gradients.debug_print", "tf_gradients", "memory_saving_gradients.debug_print", "memory_saving_gradients.debug_print", "memory_saving_gradients.debug_print", "memory_saving_gradients.debug_print", "memory_saving_gradients.my_add_control_inputs", "zip", "range", "set", "set", "tensorflow.get_collection", "set", "set", "memory_saving_gradients.format_ops", "set", "tensorflow.stop_gradient", "tensorflow.stop_gradient", "zip", "len", "len", "tensorflow.sgv", "op._set_device", "list", "tensorflow.scatter_nd", "len", "tensorflow.filter_ts_from_regex", "set", "set", "set", "checkpoints_disconnected.keys", "isinstance", "tensorflow.expand_dims", "len", "list", "memory_saving_gradients.debug_print", "memory_saving_gradients.tf_toposort", "len", "Exception", "len", "memory_saving_gradients.gradients._unsparsify"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients._to_ops", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.fast_backward_ops", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.my_add_control_inputs", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.tf_toposort", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.fast_backward_ops", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.my_add_control_inputs", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.format_ops", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.tf_toposort"], ["", "def", "gradients", "(", "ys", ",", "xs", ",", "grad_ys", "=", "None", ",", "checkpoints", "=", "'collection'", ",", "**", "kwargs", ")", ":", "\n", "    ", "'''\n    Authors: Tim Salimans & Yaroslav Bulatov\n\n    memory efficient gradient implementation inspired by \"Training Deep Nets with Sublinear Memory Cost\"\n    by Chen et al. 2016 (https://arxiv.org/abs/1604.06174)\n\n    ys,xs,grad_ys,kwargs are the arguments to standard tensorflow tf.gradients\n    (https://www.tensorflow.org/versions/r0.12/api_docs/python/train.html#gradients)\n\n    'checkpoints' can either be\n        - a list consisting of tensors from the forward pass of the neural net\n          that we should re-use when calculating the gradients in the backward pass\n          all other tensors that do not appear in this list will be re-computed\n        - a string specifying how this list should be determined. currently we support\n            - 'speed':  checkpoint all outputs of convolutions and matmuls. these ops are usually the most expensive,\n                        so checkpointing them maximizes the running speed\n                        (this is a good option if nonlinearities, concats, batchnorms, etc are taking up a lot of memory)\n            - 'memory': try to minimize the memory usage\n                        (currently using a very simple strategy that identifies a number of bottleneck tensors in the graph to checkpoint)\n            - 'collection': look for a tensorflow collection named 'checkpoints', which holds the tensors to checkpoint\n    '''", "\n", "\n", "#    print(\"Calling memsaving gradients with\", checkpoints)", "\n", "if", "not", "isinstance", "(", "ys", ",", "list", ")", ":", "\n", "        ", "ys", "=", "[", "ys", "]", "\n", "", "if", "not", "isinstance", "(", "xs", ",", "list", ")", ":", "\n", "        ", "xs", "=", "[", "xs", "]", "\n", "\n", "", "bwd_ops", "=", "ge", ".", "get_backward_walk_ops", "(", "[", "y", ".", "op", "for", "y", "in", "ys", "]", ",", "\n", "inclusive", "=", "True", ")", "\n", "\n", "debug_print", "(", "\"bwd_ops: %s\"", ",", "bwd_ops", ")", "\n", "\n", "# forward ops are all ops that are candidates for recomputation", "\n", "fwd_ops", "=", "ge", ".", "get_forward_walk_ops", "(", "[", "x", ".", "op", "for", "x", "in", "xs", "]", ",", "\n", "inclusive", "=", "True", ",", "\n", "within_ops", "=", "bwd_ops", ")", "\n", "debug_print", "(", "\"fwd_ops: %s\"", ",", "fwd_ops", ")", "\n", "\n", "# exclude ops with no inputs", "\n", "fwd_ops", "=", "[", "op", "for", "op", "in", "fwd_ops", "if", "op", ".", "inputs", "]", "\n", "\n", "# don't recompute xs, remove variables", "\n", "xs_ops", "=", "_to_ops", "(", "xs", ")", "\n", "fwd_ops", "=", "[", "op", "for", "op", "in", "fwd_ops", "if", "not", "op", "in", "xs_ops", "]", "\n", "fwd_ops", "=", "[", "op", "for", "op", "in", "fwd_ops", "if", "not", "'/assign'", "in", "op", ".", "name", "]", "\n", "fwd_ops", "=", "[", "op", "for", "op", "in", "fwd_ops", "if", "not", "'/Assign'", "in", "op", ".", "name", "]", "\n", "fwd_ops", "=", "[", "op", "for", "op", "in", "fwd_ops", "if", "not", "'/read'", "in", "op", ".", "name", "]", "\n", "ts_all", "=", "ge", ".", "filter_ts", "(", "fwd_ops", ",", "True", ")", "# get the tensors", "\n", "ts_all", "=", "[", "t", "for", "t", "in", "ts_all", "if", "'/read'", "not", "in", "t", ".", "name", "]", "\n", "ts_all", "=", "set", "(", "ts_all", ")", "-", "set", "(", "xs", ")", "-", "set", "(", "ys", ")", "\n", "\n", "# construct list of tensors to checkpoint during forward pass, if not", "\n", "# given as input", "\n", "if", "type", "(", "checkpoints", ")", "is", "not", "list", ":", "\n", "        ", "if", "checkpoints", "==", "'collection'", ":", "\n", "            ", "checkpoints", "=", "tf", ".", "get_collection", "(", "'checkpoints'", ")", "\n", "\n", "", "elif", "checkpoints", "==", "'speed'", ":", "\n", "# checkpoint all expensive ops to maximize running speed", "\n", "            ", "checkpoints", "=", "ge", ".", "filter_ts_from_regex", "(", "fwd_ops", ",", "'conv2d|Conv|MatMul'", ")", "\n", "\n", "", "elif", "checkpoints", "==", "'memory'", ":", "\n", "\n", "# remove very small tensors and some weird ops", "\n", "            ", "def", "fixdims", "(", "t", ")", ":", "# tf.Dimension values are not compatible with int, convert manually", "\n", "                ", "try", ":", "\n", "                    ", "return", "[", "int", "(", "e", "if", "e", ".", "value", "is", "not", "None", "else", "64", ")", "for", "e", "in", "t", "]", "\n", "", "except", ":", "\n", "                    ", "return", "[", "0", "]", "# unknown shape", "\n", "", "", "ts_all", "=", "[", "t", "for", "t", "in", "ts_all", "if", "np", ".", "prod", "(", "fixdims", "(", "t", ".", "shape", ")", ")", ">", "MIN_CHECKPOINT_NODE_SIZE", "]", "\n", "ts_all", "=", "[", "t", "for", "t", "in", "ts_all", "if", "'L2Loss'", "not", "in", "t", ".", "name", "]", "\n", "ts_all", "=", "[", "t", "for", "t", "in", "ts_all", "if", "'entropy'", "not", "in", "t", ".", "name", "]", "\n", "ts_all", "=", "[", "t", "for", "t", "in", "ts_all", "if", "'FusedBatchNorm'", "not", "in", "t", ".", "name", "]", "\n", "ts_all", "=", "[", "t", "for", "t", "in", "ts_all", "if", "'Switch'", "not", "in", "t", ".", "name", "]", "\n", "ts_all", "=", "[", "t", "for", "t", "in", "ts_all", "if", "'dropout'", "not", "in", "t", ".", "name", "]", "\n", "# DV: FP16_FIX - need to add 'Cast' layer here to make it work for FP16", "\n", "ts_all", "=", "[", "t", "for", "t", "in", "ts_all", "if", "'Cast'", "not", "in", "t", ".", "name", "]", "\n", "\n", "# filter out all tensors that are inputs of the backward graph", "\n", "with", "util", ".", "capture_ops", "(", ")", "as", "bwd_ops", ":", "\n", "                ", "tf_gradients", "(", "ys", ",", "xs", ",", "grad_ys", ",", "**", "kwargs", ")", "\n", "\n", "", "bwd_inputs", "=", "[", "t", "for", "op", "in", "bwd_ops", "for", "t", "in", "op", ".", "inputs", "]", "\n", "# list of tensors in forward graph that is in input to bwd graph", "\n", "ts_filtered", "=", "list", "(", "set", "(", "bwd_inputs", ")", ".", "intersection", "(", "ts_all", ")", ")", "\n", "debug_print", "(", "\"Using tensors %s\"", ",", "ts_filtered", ")", "\n", "\n", "# try two slightly different ways of getting bottlenecks tensors", "\n", "# to checkpoint", "\n", "for", "ts", "in", "[", "ts_filtered", ",", "ts_all", "]", ":", "\n", "\n", "# get all bottlenecks in the graph", "\n", "                ", "bottleneck_ts", "=", "[", "]", "\n", "for", "t", "in", "ts", ":", "\n", "                    ", "b", "=", "set", "(", "ge", ".", "get_backward_walk_ops", "(", "t", ".", "op", ",", "inclusive", "=", "True", ",", "within_ops", "=", "fwd_ops", ")", ")", "\n", "f", "=", "set", "(", "ge", ".", "get_forward_walk_ops", "(", "t", ".", "op", ",", "inclusive", "=", "False", ",", "within_ops", "=", "fwd_ops", ")", ")", "\n", "# check that there are not shortcuts", "\n", "b_inp", "=", "set", "(", "[", "inp", "for", "op", "in", "b", "for", "inp", "in", "op", ".", "inputs", "]", ")", ".", "intersection", "(", "ts_all", ")", "\n", "f_inp", "=", "set", "(", "[", "inp", "for", "op", "in", "f", "for", "inp", "in", "op", ".", "inputs", "]", ")", ".", "intersection", "(", "ts_all", ")", "\n", "if", "not", "set", "(", "b_inp", ")", ".", "intersection", "(", "f_inp", ")", "and", "len", "(", "b_inp", ")", "+", "len", "(", "f_inp", ")", ">=", "len", "(", "ts_all", ")", ":", "\n", "                        ", "bottleneck_ts", ".", "append", "(", "t", ")", "# we have a bottleneck!", "\n", "", "else", ":", "\n", "                        ", "debug_print", "(", "\"Rejected bottleneck candidate and ops %s\"", ",", "[", "t", "]", "+", "list", "(", "set", "(", "ts_all", ")", "-", "set", "(", "b_inp", ")", "-", "set", "(", "f_inp", ")", ")", ")", "\n", "\n", "# success? or try again without filtering?", "\n", "", "", "if", "len", "(", "bottleneck_ts", ")", ">=", "np", ".", "sqrt", "(", "len", "(", "ts_filtered", ")", ")", ":", "# yes, enough bottlenecks found!", "\n", "                    ", "break", "\n", "\n", "", "", "if", "not", "bottleneck_ts", ":", "\n", "                ", "raise", "Exception", "(", "'unable to find bottleneck tensors! please provide checkpoint nodes manually, or use checkpoints=\"speed\".'", ")", "\n", "\n", "# sort the bottlenecks", "\n", "", "bottlenecks_sorted_lists", "=", "tf_toposort", "(", "bottleneck_ts", ",", "within_ops", "=", "fwd_ops", ")", "\n", "sorted_bottlenecks", "=", "[", "t", "for", "ts", "in", "bottlenecks_sorted_lists", "for", "t", "in", "ts", "]", "\n", "\n", "# save an approximately optimal number ~ sqrt(N)", "\n", "N", "=", "len", "(", "ts_filtered", ")", "\n", "if", "len", "(", "bottleneck_ts", ")", "<=", "np", ".", "ceil", "(", "np", ".", "sqrt", "(", "N", ")", ")", ":", "\n", "                ", "checkpoints", "=", "sorted_bottlenecks", "\n", "", "else", ":", "\n", "                ", "step", "=", "int", "(", "np", ".", "ceil", "(", "len", "(", "bottleneck_ts", ")", "/", "np", ".", "sqrt", "(", "N", ")", ")", ")", "\n", "checkpoints", "=", "sorted_bottlenecks", "[", "step", ":", ":", "step", "]", "\n", "\n", "", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "'%s is unsupported input for \"checkpoints\"'", "%", "(", "checkpoints", ",", ")", ")", "\n", "\n", "", "", "checkpoints", "=", "list", "(", "set", "(", "checkpoints", ")", ".", "intersection", "(", "ts_all", ")", ")", "\n", "\n", "# at this point automatic selection happened and checkpoints is list of nodes", "\n", "assert", "isinstance", "(", "checkpoints", ",", "list", ")", "\n", "\n", "debug_print", "(", "\"Checkpoint nodes used: %s\"", ",", "checkpoints", ")", "\n", "# better error handling of special cases", "\n", "# xs are already handled as checkpoint nodes, so no need to include them", "\n", "xs_intersect_checkpoints", "=", "set", "(", "xs", ")", ".", "intersection", "(", "set", "(", "checkpoints", ")", ")", "\n", "if", "xs_intersect_checkpoints", ":", "\n", "        ", "debug_print", "(", "\"Warning, some input nodes are also checkpoint nodes: %s\"", ",", "\n", "xs_intersect_checkpoints", ")", "\n", "", "ys_intersect_checkpoints", "=", "set", "(", "ys", ")", ".", "intersection", "(", "set", "(", "checkpoints", ")", ")", "\n", "debug_print", "(", "\"ys: %s, checkpoints: %s, intersect: %s\"", ",", "ys", ",", "checkpoints", ",", "\n", "ys_intersect_checkpoints", ")", "\n", "# saving an output node (ys) gives no benefit in memory while creating", "\n", "# new edge cases, exclude them", "\n", "if", "ys_intersect_checkpoints", ":", "\n", "        ", "debug_print", "(", "\"Warning, some output nodes are also checkpoints nodes: %s\"", ",", "\n", "format_ops", "(", "ys_intersect_checkpoints", ")", ")", "\n", "\n", "# remove initial and terminal nodes from checkpoints list if present", "\n", "", "checkpoints", "=", "list", "(", "set", "(", "checkpoints", ")", "-", "set", "(", "ys", ")", "-", "set", "(", "xs", ")", ")", "\n", "\n", "# check that we have some nodes to checkpoint", "\n", "if", "not", "checkpoints", ":", "\n", "        ", "raise", "Exception", "(", "'no checkpoints nodes found or given as input! '", ")", "\n", "\n", "# disconnect dependencies between checkpointed tensors", "\n", "", "checkpoints_disconnected", "=", "{", "}", "\n", "for", "x", "in", "checkpoints", ":", "\n", "        ", "if", "x", ".", "op", "and", "x", ".", "op", ".", "name", "is", "not", "None", ":", "\n", "            ", "grad_node", "=", "tf", ".", "stop_gradient", "(", "x", ",", "name", "=", "x", ".", "op", ".", "name", "+", "\"_sg\"", ")", "\n", "", "else", ":", "\n", "            ", "grad_node", "=", "tf", ".", "stop_gradient", "(", "x", ")", "\n", "", "checkpoints_disconnected", "[", "x", "]", "=", "grad_node", "\n", "\n", "# partial derivatives to the checkpointed tensors and xs", "\n", "", "ops_to_copy", "=", "fast_backward_ops", "(", "seed_ops", "=", "[", "y", ".", "op", "for", "y", "in", "ys", "]", ",", "\n", "stop_at_ts", "=", "checkpoints", ",", "within_ops", "=", "fwd_ops", ")", "\n", "debug_print", "(", "\"Found %s ops to copy within fwd_ops %s, seed %s, stop_at %s\"", ",", "\n", "len", "(", "ops_to_copy", ")", ",", "fwd_ops", ",", "[", "r", ".", "op", "for", "r", "in", "ys", "]", ",", "checkpoints", ")", "\n", "debug_print", "(", "\"ops_to_copy = %s\"", ",", "ops_to_copy", ")", "\n", "debug_print", "(", "\"Processing list %s\"", ",", "ys", ")", "\n", "copied_sgv", ",", "info", "=", "ge", ".", "copy_with_input_replacements", "(", "ge", ".", "sgv", "(", "ops_to_copy", ")", ",", "{", "}", ")", "\n", "for", "origin_op", ",", "op", "in", "info", ".", "_transformed_ops", ".", "items", "(", ")", ":", "\n", "        ", "op", ".", "_set_device", "(", "origin_op", ".", "node_def", ".", "device", ")", "\n", "", "copied_ops", "=", "info", ".", "_transformed_ops", ".", "values", "(", ")", "\n", "debug_print", "(", "\"Copied %s to %s\"", ",", "ops_to_copy", ",", "copied_ops", ")", "\n", "ge", ".", "reroute_ts", "(", "checkpoints_disconnected", ".", "values", "(", ")", ",", "checkpoints_disconnected", ".", "keys", "(", ")", ",", "can_modify", "=", "copied_ops", ")", "\n", "debug_print", "(", "\"Rewired %s in place of %s restricted to %s\"", ",", "\n", "checkpoints_disconnected", ".", "values", "(", ")", ",", "checkpoints_disconnected", ".", "keys", "(", ")", ",", "copied_ops", ")", "\n", "\n", "# get gradients with respect to current boundary + original x's", "\n", "copied_ys", "=", "[", "info", ".", "_transformed_ops", "[", "y", ".", "op", "]", ".", "_outputs", "[", "0", "]", "for", "y", "in", "ys", "]", "\n", "boundary", "=", "list", "(", "checkpoints_disconnected", ".", "values", "(", ")", ")", "\n", "dv", "=", "tf_gradients", "(", "ys", "=", "copied_ys", ",", "xs", "=", "boundary", "+", "xs", ",", "grad_ys", "=", "grad_ys", ",", "**", "kwargs", ")", "\n", "debug_print", "(", "\"Got gradients %s\"", ",", "dv", ")", "\n", "debug_print", "(", "\"for %s\"", ",", "copied_ys", ")", "\n", "debug_print", "(", "\"with respect to %s\"", ",", "boundary", "+", "xs", ")", "\n", "\n", "inputs_to_do_before", "=", "[", "y", ".", "op", "for", "y", "in", "ys", "]", "\n", "if", "grad_ys", "is", "not", "None", ":", "\n", "        ", "inputs_to_do_before", "+=", "grad_ys", "\n", "", "wait_to_do_ops", "=", "list", "(", "copied_ops", ")", "+", "[", "g", ".", "op", "for", "g", "in", "dv", "if", "g", "is", "not", "None", "]", "\n", "my_add_control_inputs", "(", "wait_to_do_ops", ",", "inputs_to_do_before", ")", "\n", "\n", "# partial derivatives to the checkpointed nodes", "\n", "# dictionary of \"node: backprop\" for nodes in the boundary", "\n", "d_checkpoints", "=", "{", "r", ":", "dr", "for", "r", ",", "dr", "in", "zip", "(", "checkpoints_disconnected", ".", "keys", "(", ")", ",", "\n", "dv", "[", ":", "len", "(", "checkpoints_disconnected", ")", "]", ")", "}", "\n", "# partial derivatives to xs (usually the params of the neural net)", "\n", "d_xs", "=", "dv", "[", "len", "(", "checkpoints_disconnected", ")", ":", "]", "\n", "\n", "# incorporate derivatives flowing through the checkpointed nodes", "\n", "checkpoints_sorted_lists", "=", "tf_toposort", "(", "checkpoints", ",", "within_ops", "=", "fwd_ops", ")", "\n", "for", "ts", "in", "checkpoints_sorted_lists", "[", ":", ":", "-", "1", "]", ":", "\n", "        ", "debug_print", "(", "\"Processing list %s\"", ",", "ts", ")", "\n", "checkpoints_other", "=", "[", "r", "for", "r", "in", "checkpoints", "if", "r", "not", "in", "ts", "]", "\n", "checkpoints_disconnected_other", "=", "[", "checkpoints_disconnected", "[", "r", "]", "for", "r", "in", "checkpoints_other", "]", "\n", "\n", "# copy part of the graph below current checkpoint node, stopping at", "\n", "# other checkpoints nodes", "\n", "ops_to_copy", "=", "fast_backward_ops", "(", "within_ops", "=", "fwd_ops", ",", "seed_ops", "=", "[", "r", ".", "op", "for", "r", "in", "ts", "]", ",", "stop_at_ts", "=", "checkpoints_other", ")", "\n", "debug_print", "(", "\"Found %s ops to copy within %s, seed %s, stop_at %s\"", ",", "\n", "len", "(", "ops_to_copy", ")", ",", "fwd_ops", ",", "[", "r", ".", "op", "for", "r", "in", "ts", "]", ",", "\n", "checkpoints_other", ")", "\n", "debug_print", "(", "\"ops_to_copy = %s\"", ",", "ops_to_copy", ")", "\n", "if", "not", "ops_to_copy", ":", "# we're done!", "\n", "            ", "break", "\n", "", "copied_sgv", ",", "info", "=", "ge", ".", "copy_with_input_replacements", "(", "ge", ".", "sgv", "(", "ops_to_copy", ")", ",", "{", "}", ")", "\n", "for", "origin_op", ",", "op", "in", "info", ".", "_transformed_ops", ".", "items", "(", ")", ":", "\n", "            ", "op", ".", "_set_device", "(", "origin_op", ".", "node_def", ".", "device", ")", "\n", "", "copied_ops", "=", "info", ".", "_transformed_ops", ".", "values", "(", ")", "\n", "debug_print", "(", "\"Copied %s to %s\"", ",", "ops_to_copy", ",", "copied_ops", ")", "\n", "ge", ".", "reroute_ts", "(", "checkpoints_disconnected_other", ",", "checkpoints_other", ",", "can_modify", "=", "copied_ops", ")", "\n", "debug_print", "(", "\"Rewired %s in place of %s restricted to %s\"", ",", "\n", "checkpoints_disconnected_other", ",", "checkpoints_other", ",", "copied_ops", ")", "\n", "\n", "# gradient flowing through the checkpointed node", "\n", "boundary", "=", "[", "info", ".", "_transformed_ops", "[", "r", ".", "op", "]", ".", "_outputs", "[", "0", "]", "for", "r", "in", "ts", "]", "\n", "substitute_backprops", "=", "[", "d_checkpoints", "[", "r", "]", "for", "r", "in", "ts", "]", "\n", "dv", "=", "tf_gradients", "(", "boundary", ",", "\n", "checkpoints_disconnected_other", "+", "xs", ",", "\n", "grad_ys", "=", "substitute_backprops", ",", "**", "kwargs", ")", "\n", "debug_print", "(", "\"Got gradients %s\"", ",", "dv", ")", "\n", "debug_print", "(", "\"for %s\"", ",", "boundary", ")", "\n", "debug_print", "(", "\"with respect to %s\"", ",", "checkpoints_disconnected_other", "+", "xs", ")", "\n", "debug_print", "(", "\"with boundary backprop substitutions %s\"", ",", "substitute_backprops", ")", "\n", "\n", "inputs_to_do_before", "=", "[", "d_checkpoints", "[", "r", "]", ".", "op", "for", "r", "in", "ts", "]", "\n", "wait_to_do_ops", "=", "list", "(", "copied_ops", ")", "+", "[", "g", ".", "op", "for", "g", "in", "dv", "if", "g", "is", "not", "None", "]", "\n", "my_add_control_inputs", "(", "wait_to_do_ops", ",", "inputs_to_do_before", ")", "\n", "\n", "# partial derivatives to the checkpointed nodes", "\n", "for", "r", ",", "dr", "in", "zip", "(", "checkpoints_other", ",", "dv", "[", ":", "len", "(", "checkpoints_other", ")", "]", ")", ":", "\n", "            ", "if", "dr", "is", "not", "None", ":", "\n", "                ", "if", "d_checkpoints", "[", "r", "]", "is", "None", ":", "\n", "                    ", "d_checkpoints", "[", "r", "]", "=", "dr", "\n", "", "else", ":", "\n", "                    ", "d_checkpoints", "[", "r", "]", "+=", "dr", "\n", "", "", "", "def", "_unsparsify", "(", "x", ")", ":", "\n", "            ", "if", "not", "isinstance", "(", "x", ",", "tf", ".", "IndexedSlices", ")", ":", "\n", "                ", "return", "x", "\n", "", "assert", "x", ".", "dense_shape", "is", "not", "None", ",", "\"memory_saving_gradients encountered sparse gradients of unknown shape\"", "\n", "indices", "=", "x", ".", "indices", "\n", "while", "indices", ".", "shape", ".", "ndims", "<", "x", ".", "values", ".", "shape", ".", "ndims", ":", "\n", "                ", "indices", "=", "tf", ".", "expand_dims", "(", "indices", ",", "-", "1", ")", "\n", "", "return", "tf", ".", "scatter_nd", "(", "indices", ",", "x", ".", "values", ",", "x", ".", "dense_shape", ")", "\n", "\n", "# partial derivatives to xs (usually the params of the neural net)", "\n", "", "d_xs_new", "=", "dv", "[", "len", "(", "checkpoints_other", ")", ":", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "xs", ")", ")", ":", "\n", "            ", "if", "d_xs_new", "[", "j", "]", "is", "not", "None", ":", "\n", "                ", "if", "d_xs", "[", "j", "]", "is", "None", ":", "\n", "                    ", "d_xs", "[", "j", "]", "=", "_unsparsify", "(", "d_xs_new", "[", "j", "]", ")", "\n", "", "else", ":", "\n", "                    ", "d_xs", "[", "j", "]", "+=", "_unsparsify", "(", "d_xs_new", "[", "j", "]", ")", "\n", "\n", "\n", "", "", "", "", "return", "d_xs", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.tf_toposort": [[302, 319], ["tensorflow.get_forward_walk_ops", "toposort.toposort", "list", "set", "set().intersection", "ts_sorted_lists.append", "set"], "function", ["None"], ["", "def", "tf_toposort", "(", "ts", ",", "within_ops", "=", "None", ")", ":", "\n", "    ", "all_ops", "=", "ge", ".", "get_forward_walk_ops", "(", "[", "x", ".", "op", "for", "x", "in", "ts", "]", ",", "within_ops", "=", "within_ops", ")", "\n", "\n", "deps", "=", "{", "}", "\n", "for", "op", "in", "all_ops", ":", "\n", "        ", "for", "o", "in", "op", ".", "outputs", ":", "\n", "            ", "deps", "[", "o", "]", "=", "set", "(", "op", ".", "inputs", ")", "\n", "", "", "sorted_ts", "=", "toposort", "(", "deps", ")", "\n", "\n", "# only keep the tensors from our original list", "\n", "ts_sorted_lists", "=", "[", "]", "\n", "for", "l", "in", "sorted_ts", ":", "\n", "        ", "keep", "=", "list", "(", "set", "(", "l", ")", ".", "intersection", "(", "ts", ")", ")", "\n", "if", "keep", ":", "\n", "            ", "ts_sorted_lists", ".", "append", "(", "keep", ")", "\n", "\n", "", "", "return", "ts_sorted_lists", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.fast_backward_ops": [[320, 324], ["set", "set.intersection().difference", "list", "tensorflow.get_backward_walk_ops", "set.intersection"], "function", ["None"], ["", "def", "fast_backward_ops", "(", "within_ops", ",", "seed_ops", ",", "stop_at_ts", ")", ":", "\n", "    ", "bwd_ops", "=", "set", "(", "ge", ".", "get_backward_walk_ops", "(", "seed_ops", ",", "stop_at_ts", "=", "stop_at_ts", ")", ")", "\n", "ops", "=", "bwd_ops", ".", "intersection", "(", "within_ops", ")", ".", "difference", "(", "[", "t", ".", "op", "for", "t", "in", "stop_at_ts", "]", ")", "\n", "return", "list", "(", "ops", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.capture_ops": [[325, 341], ["int", "str", "tensorflow.get_default_graph", "op_list.extend", "tensorflow.name_scope", "tensorflow.select_ops", "time.time"], "function", ["None"], ["", "@", "contextlib", ".", "contextmanager", "\n", "def", "capture_ops", "(", ")", ":", "\n", "  ", "\"\"\"Decorator to capture ops created in the block.\n  with capture_ops() as ops:\n    # create some ops\n  print(ops) # => prints ops created.\n  \"\"\"", "\n", "\n", "micros", "=", "int", "(", "time", ".", "time", "(", ")", "*", "10", "**", "6", ")", "\n", "scope_name", "=", "str", "(", "micros", ")", "\n", "op_list", "=", "[", "]", "\n", "with", "tf", ".", "name_scope", "(", "scope_name", ")", ":", "\n", "    ", "yield", "op_list", "\n", "\n", "", "g", "=", "tf", ".", "get_default_graph", "(", ")", "\n", "op_list", ".", "extend", "(", "ge", ".", "select_ops", "(", "scope_name", "+", "\"/.*\"", ",", "graph", "=", "g", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients._to_op": [[342, 346], ["hasattr"], "function", ["None"], ["", "def", "_to_op", "(", "tensor_or_op", ")", ":", "\n", "  ", "if", "hasattr", "(", "tensor_or_op", ",", "\"op\"", ")", ":", "\n", "    ", "return", "tensor_or_op", ".", "op", "\n", "", "return", "tensor_or_op", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients._to_ops": [[347, 351], ["memory_saving_gradients._is_iterable", "memory_saving_gradients._to_op"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients._is_iterable", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients._to_op"], ["", "def", "_to_ops", "(", "iterable", ")", ":", "\n", "  ", "if", "not", "_is_iterable", "(", "iterable", ")", ":", "\n", "    ", "return", "iterable", "\n", "", "return", "[", "_to_op", "(", "i", ")", "for", "i", "in", "iterable", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients._is_iterable": [[352, 358], ["iter"], "function", ["None"], ["", "def", "_is_iterable", "(", "o", ")", ":", "\n", "  ", "try", ":", "\n", "    ", "_", "=", "iter", "(", "o", ")", "\n", "", "except", "Exception", ":", "\n", "    ", "return", "False", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.debug_print": [[360, 371], ["print", "memory_saving_gradients.format_ops", "tuple"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.format_ops"], ["def", "debug_print", "(", "s", ",", "*", "args", ")", ":", "\n", "  ", "\"\"\"Like logger.log, but also replaces all TensorFlow ops/tensors with their\n  names. Sensitive to value of DEBUG_LOGGING, see enable_debug/disable_debug\n\n  Usage:\n    debug_print(\"see tensors %s for %s\", tensorlist, [1,2,3])\n  \"\"\"", "\n", "\n", "if", "DEBUG_LOGGING", ":", "\n", "    ", "formatted_args", "=", "[", "format_ops", "(", "arg", ")", "for", "arg", "in", "args", "]", "\n", "print", "(", "\"DEBUG \"", "+", "s", "%", "tuple", "(", "formatted_args", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.format_ops": [[372, 383], ["hasattr", "isinstance", "sorted", "hasattr", "str", "hasattr", "str"], "function", ["None"], ["", "", "def", "format_ops", "(", "ops", ",", "sort_outputs", "=", "True", ")", ":", "\n", "  ", "\"\"\"Helper method for printing ops. Converts Tensor/Operation op to op.name,\n  rest to str(op).\"\"\"", "\n", "\n", "if", "hasattr", "(", "ops", ",", "'__iter__'", ")", "and", "not", "isinstance", "(", "ops", ",", "str", ")", ":", "\n", "    ", "l", "=", "[", "(", "op", ".", "name", "if", "hasattr", "(", "op", ",", "\"name\"", ")", "else", "str", "(", "op", ")", ")", "for", "op", "in", "ops", "]", "\n", "if", "sort_outputs", ":", "\n", "      ", "return", "sorted", "(", "l", ")", "\n", "", "return", "l", "\n", "", "else", ":", "\n", "    ", "return", "ops", ".", "name", "if", "hasattr", "(", "ops", ",", "\"name\"", ")", "else", "str", "(", "ops", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.my_add_control_inputs": [[384, 388], ["tensorflow.add_control_inputs"], "function", ["None"], ["", "", "def", "my_add_control_inputs", "(", "wait_to_do_ops", ",", "inputs_to_do_before", ")", ":", "\n", "    ", "for", "op", "in", "wait_to_do_ops", ":", "\n", "        ", "ci", "=", "[", "i", "for", "i", "in", "inputs_to_do_before", "if", "op", ".", "control_inputs", "is", "None", "or", "i", "not", "in", "op", ".", "control_inputs", "]", "\n", "ge", ".", "add_control_inputs", "(", "op", ",", "ci", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.optimization.AdamWeightDecayOptimizer.__init__": [[110, 127], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.FeatureWriter.__init__"], ["def", "__init__", "(", "self", ",", "\n", "learning_rate", ",", "\n", "weight_decay_rate", "=", "0.0", ",", "\n", "beta_1", "=", "0.9", ",", "\n", "beta_2", "=", "0.999", ",", "\n", "epsilon", "=", "1e-6", ",", "\n", "exclude_from_weight_decay", "=", "None", ",", "\n", "name", "=", "\"AdamWeightDecayOptimizer\"", ")", ":", "\n", "    ", "\"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"", "\n", "super", "(", "AdamWeightDecayOptimizer", ",", "self", ")", ".", "__init__", "(", "False", ",", "name", ")", "\n", "\n", "self", ".", "learning_rate", "=", "learning_rate", "\n", "self", ".", "weight_decay_rate", "=", "weight_decay_rate", "\n", "self", ".", "beta_1", "=", "beta_1", "\n", "self", ".", "beta_2", "=", "beta_2", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "exclude_from_weight_decay", "=", "exclude_from_weight_decay", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.optimization.AdamWeightDecayOptimizer.apply_gradients": [[128, 178], ["tensorflow.group", "optimization.AdamWeightDecayOptimizer._get_variable_name", "tensorflow.get_variable", "tensorflow.get_variable", "optimization.AdamWeightDecayOptimizer._do_use_weight_decay", "assignments.extend", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.multiply", "param.shape.as_list", "tensorflow.zeros_initializer", "param.shape.as_list", "tensorflow.zeros_initializer", "tensorflow.square", "tensorflow.sqrt", "param.assign", "tensorflow.get_variable.assign", "tensorflow.get_variable.assign"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.optimization.AdamWeightDecayOptimizer._get_variable_name", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.optimization.AdamWeightDecayOptimizer._do_use_weight_decay"], ["", "def", "apply_gradients", "(", "self", ",", "grads_and_vars", ",", "global_step", "=", "None", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "assignments", "=", "[", "]", "\n", "for", "(", "grad", ",", "param", ")", "in", "grads_and_vars", ":", "\n", "      ", "if", "grad", "is", "None", "or", "param", "is", "None", ":", "\n", "        ", "continue", "\n", "\n", "", "param_name", "=", "self", ".", "_get_variable_name", "(", "param", ".", "name", ")", "\n", "\n", "m", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "param_name", "+", "\"/adam_m\"", ",", "\n", "shape", "=", "param", ".", "shape", ".", "as_list", "(", ")", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "trainable", "=", "False", ",", "\n", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "v", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "param_name", "+", "\"/adam_v\"", ",", "\n", "shape", "=", "param", ".", "shape", ".", "as_list", "(", ")", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "trainable", "=", "False", ",", "\n", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "\n", "# Standard Adam update.", "\n", "next_m", "=", "(", "\n", "tf", ".", "multiply", "(", "self", ".", "beta_1", ",", "m", ")", "+", "tf", ".", "multiply", "(", "1.0", "-", "self", ".", "beta_1", ",", "grad", ")", ")", "\n", "next_v", "=", "(", "\n", "tf", ".", "multiply", "(", "self", ".", "beta_2", ",", "v", ")", "+", "tf", ".", "multiply", "(", "1.0", "-", "self", ".", "beta_2", ",", "\n", "tf", ".", "square", "(", "grad", ")", ")", ")", "\n", "\n", "update", "=", "next_m", "/", "(", "tf", ".", "sqrt", "(", "next_v", ")", "+", "self", ".", "epsilon", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want ot decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "if", "self", ".", "_do_use_weight_decay", "(", "param_name", ")", ":", "\n", "        ", "update", "+=", "self", ".", "weight_decay_rate", "*", "param", "\n", "\n", "", "update_with_lr", "=", "self", ".", "learning_rate", "*", "update", "\n", "\n", "next_param", "=", "param", "-", "update_with_lr", "\n", "\n", "assignments", ".", "extend", "(", "\n", "[", "param", ".", "assign", "(", "next_param", ")", ",", "\n", "m", ".", "assign", "(", "next_m", ")", ",", "\n", "v", ".", "assign", "(", "next_v", ")", "]", ")", "\n", "", "return", "tf", ".", "group", "(", "*", "assignments", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.optimization.AdamWeightDecayOptimizer._do_use_weight_decay": [[179, 188], ["re.search"], "methods", ["None"], ["", "def", "_do_use_weight_decay", "(", "self", ",", "param_name", ")", ":", "\n", "    ", "\"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"", "\n", "if", "not", "self", ".", "weight_decay_rate", ":", "\n", "      ", "return", "False", "\n", "", "if", "self", ".", "exclude_from_weight_decay", ":", "\n", "      ", "for", "r", "in", "self", ".", "exclude_from_weight_decay", ":", "\n", "        ", "if", "re", ".", "search", "(", "r", ",", "param_name", ")", "is", "not", "None", ":", "\n", "          ", "return", "False", "\n", "", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.optimization.AdamWeightDecayOptimizer._get_variable_name": [[189, 195], ["re.match", "re.match.group"], "methods", ["None"], ["", "def", "_get_variable_name", "(", "self", ",", "param_name", ")", ":", "\n", "    ", "\"\"\"Get the variable name from the tensor name.\"\"\"", "\n", "m", "=", "re", ".", "match", "(", "\"^(.*):\\\\d+$\"", ",", "param_name", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "      ", "param_name", "=", "m", ".", "group", "(", "1", ")", "\n", "", "return", "param_name", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.optimization.create_optimizer": [[30, 105], ["tensorflow.train.get_or_create_global_step", "tensorflow.constant", "tensorflow.train.polynomial_decay", "optimization.AdamWeightDecayOptimizer", "tensorflow.trainable_variables", "os.getenv", "tensorflow.clip_by_global_norm", "tf.contrib.tpu.CrossShardOptimizer.apply_gradients", "tensorflow.group", "tensorflow.cast", "tensorflow.constant", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.contrib.tpu.CrossShardOptimizer", "tensorflow.gradients", "memory_saving_gradients.gradients", "zip", "tensorflow.logging.info", "tensorflow.contrib.model_pruning.python.pruning.get_pruning_hparams().parse", "tensorflow.contrib.model_pruning.python.pruning.Pruning", "tensorflow.contrib.model_pruning.python.pruning.Pruning.conditional_mask_update_op", "tensorflow.contrib.model_pruning.python.pruning.Pruning.add_pruning_summaries", "tensorflow.logging.info", "tensorflow.no_op", "tf.train.get_or_create_global_step.assign", "tensorflow.contrib.model_pruning.python.pruning.get_pruning_hparams"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.optimization.AdamWeightDecayOptimizer.apply_gradients", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.gradients", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.gradients"], ["def", "create_optimizer", "(", "loss", ",", "init_lr", ",", "num_train_steps", ",", "num_warmup_steps", ",", "use_tpu", ",", "prune_config_flag", ")", ":", "\n", "  ", "\"\"\"Creates an optimizer training op.\"\"\"", "\n", "global_step", "=", "tf", ".", "train", ".", "get_or_create_global_step", "(", ")", "\n", "\n", "learning_rate", "=", "tf", ".", "constant", "(", "value", "=", "init_lr", ",", "shape", "=", "[", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "# Implements linear decay of the learning rate.", "\n", "learning_rate", "=", "tf", ".", "train", ".", "polynomial_decay", "(", "\n", "learning_rate", ",", "\n", "global_step", ",", "\n", "num_train_steps", ",", "\n", "end_learning_rate", "=", "0.0", ",", "\n", "power", "=", "1.0", ",", "\n", "cycle", "=", "False", ")", "\n", "\n", "# Implements linear warmup. I.e., if global_step < num_warmup_steps, the", "\n", "# learning rate will be `global_step/num_warmup_steps * init_lr`.", "\n", "if", "num_warmup_steps", ":", "\n", "    ", "global_steps_int", "=", "tf", ".", "cast", "(", "global_step", ",", "tf", ".", "int32", ")", "\n", "warmup_steps_int", "=", "tf", ".", "constant", "(", "num_warmup_steps", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "global_steps_float", "=", "tf", ".", "cast", "(", "global_steps_int", ",", "tf", ".", "float32", ")", "\n", "warmup_steps_float", "=", "tf", ".", "cast", "(", "warmup_steps_int", ",", "tf", ".", "float32", ")", "\n", "\n", "warmup_percent_done", "=", "global_steps_float", "/", "warmup_steps_float", "\n", "warmup_learning_rate", "=", "init_lr", "*", "warmup_percent_done", "\n", "\n", "is_warmup", "=", "tf", ".", "cast", "(", "global_steps_int", "<", "warmup_steps_int", ",", "tf", ".", "float32", ")", "\n", "learning_rate", "=", "(", "\n", "(", "1.0", "-", "is_warmup", ")", "*", "learning_rate", "+", "is_warmup", "*", "warmup_learning_rate", ")", "\n", "\n", "# It is recommended that you use this optimizer for fine tuning, since this", "\n", "# is how the model was trained (note that the Adam m/v variables are NOT", "\n", "# loaded from init_checkpoint.)", "\n", "", "optimizer", "=", "AdamWeightDecayOptimizer", "(", "\n", "learning_rate", "=", "learning_rate", ",", "\n", "weight_decay_rate", "=", "0.01", ",", "\n", "beta_1", "=", "0.9", ",", "\n", "beta_2", "=", "0.999", ",", "\n", "epsilon", "=", "1e-6", ",", "\n", "exclude_from_weight_decay", "=", "[", "\"LayerNorm\"", ",", "\"layer_norm\"", ",", "\"bias\"", "]", ")", "\n", "\n", "if", "use_tpu", ":", "\n", "    ", "optimizer", "=", "tf", ".", "contrib", ".", "tpu", ".", "CrossShardOptimizer", "(", "optimizer", ")", "\n", "\n", "# memory_saving_gradients.DEBUG_LOGGING = True", "\n", "", "tvars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "if", "os", ".", "getenv", "(", "'DISABLE_GRAD_CHECKPOINT'", ")", ":", "\n", "    ", "grads", "=", "tf", ".", "gradients", "(", "loss", ",", "tvars", ")", "\n", "", "else", ":", "\n", "    ", "grads", "=", "memory_saving_gradients", ".", "gradients", "(", "loss", ",", "tvars", ",", "checkpoints", "=", "'memory'", ")", "\n", "\n", "# This is how the model was pre-trained.", "\n", "", "(", "grads", ",", "_", ")", "=", "tf", ".", "clip_by_global_norm", "(", "grads", ",", "clip_norm", "=", "1.0", ")", "\n", "\n", "train_op", "=", "optimizer", ".", "apply_gradients", "(", "\n", "zip", "(", "grads", ",", "tvars", ")", ",", "global_step", "=", "global_step", ")", "\n", "\n", "# Pruning mask update ops", "\n", "if", "prune_config_flag", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "f'Pruning with configs {prune_config_flag}'", ")", "\n", "prune_config", "=", "get_pruning_hparams", "(", ")", ".", "parse", "(", "prune_config_flag", ")", "\n", "prune", "=", "Pruning", "(", "prune_config", ",", "global_step", "=", "global_step", ")", "\n", "mask_update_op", "=", "prune", ".", "conditional_mask_update_op", "(", ")", "\n", "prune", ".", "add_pruning_summaries", "(", ")", "\n", "", "else", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "'No pruning config provided, skipping pruning'", ")", "\n", "mask_update_op", "=", "tf", ".", "no_op", "(", ")", "\n", "\n", "# Normally the global step update is done inside of `apply_gradients`.", "\n", "# However, `AdamWeightDecayOptimizer` doesn't do this. But if you use", "\n", "# a different optimizer, you should probably take this line out.", "\n", "", "new_global_step", "=", "global_step", "+", "1", "\n", "train_op", "=", "tf", ".", "group", "(", "train_op", ",", "mask_update_op", ",", "[", "global_step", ".", "assign", "(", "new_global_step", ")", "]", ")", "\n", "return", "train_op", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.extract_features.InputExample.__init__": [[83, 87], ["None"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ",", "unique_id", ",", "text_a", ",", "text_b", ")", ":", "\n", "    ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "text_a", "=", "text_a", "\n", "self", ".", "text_b", "=", "text_b", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.extract_features.InputFeatures.__init__": [[92, 98], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "unique_id", ",", "tokens", ",", "input_ids", ",", "input_mask", ",", "input_type_ids", ")", ":", "\n", "    ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "input_type_ids", "=", "input_type_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.extract_features.input_fn_builder": [[100, 146], ["all_unique_ids.append", "all_input_ids.append", "all_input_mask.append", "all_input_type_ids.append", "len", "tensorflow.data.Dataset.from_tensor_slices", "d.batch.batch", "tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "tensorflow.constant"], "function", ["None"], ["", "", "def", "input_fn_builder", "(", "features", ",", "seq_length", ")", ":", "\n", "  ", "\"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"", "\n", "\n", "all_unique_ids", "=", "[", "]", "\n", "all_input_ids", "=", "[", "]", "\n", "all_input_mask", "=", "[", "]", "\n", "all_input_type_ids", "=", "[", "]", "\n", "\n", "for", "feature", "in", "features", ":", "\n", "    ", "all_unique_ids", ".", "append", "(", "feature", ".", "unique_id", ")", "\n", "all_input_ids", ".", "append", "(", "feature", ".", "input_ids", ")", "\n", "all_input_mask", ".", "append", "(", "feature", ".", "input_mask", ")", "\n", "all_input_type_ids", ".", "append", "(", "feature", ".", "input_type_ids", ")", "\n", "\n", "", "def", "input_fn", "(", "params", ")", ":", "\n", "    ", "\"\"\"The actual input function.\"\"\"", "\n", "batch_size", "=", "params", "[", "\"batch_size\"", "]", "\n", "\n", "num_examples", "=", "len", "(", "features", ")", "\n", "\n", "# This is for demo purposes and does NOT scale to large data sets. We do", "\n", "# not use Dataset.from_generator() because that uses tf.py_func which is", "\n", "# not TPU compatible. The right way to load data is with TFRecordReader.", "\n", "d", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "{", "\n", "\"unique_ids\"", ":", "\n", "tf", ".", "constant", "(", "all_unique_ids", ",", "shape", "=", "[", "num_examples", "]", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "\n", "\"input_ids\"", ":", "\n", "tf", ".", "constant", "(", "\n", "all_input_ids", ",", "shape", "=", "[", "num_examples", ",", "seq_length", "]", ",", "\n", "dtype", "=", "tf", ".", "int32", ")", ",", "\n", "\"input_mask\"", ":", "\n", "tf", ".", "constant", "(", "\n", "all_input_mask", ",", "\n", "shape", "=", "[", "num_examples", ",", "seq_length", "]", ",", "\n", "dtype", "=", "tf", ".", "int32", ")", ",", "\n", "\"input_type_ids\"", ":", "\n", "tf", ".", "constant", "(", "\n", "all_input_type_ids", ",", "\n", "shape", "=", "[", "num_examples", ",", "seq_length", "]", ",", "\n", "dtype", "=", "tf", ".", "int32", ")", ",", "\n", "}", ")", "\n", "\n", "d", "=", "d", ".", "batch", "(", "batch_size", "=", "batch_size", ",", "drop_remainder", "=", "False", ")", "\n", "return", "d", "\n", "\n", "", "return", "input_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.extract_features.model_fn_builder": [[148, 208], ["modeling.BertModel", "tensorflow.trainable_variables", "modeling.get_assignment_map_from_checkpoint", "tensorflow.logging.info", "modeling.BertModel.get_all_encoder_layers", "enumerate", "tensorflow.contrib.tpu.TPUEstimatorSpec", "ValueError", "tensorflow.train.init_from_checkpoint", "tensorflow.logging.info", "tensorflow.train.init_from_checkpoint", "tensorflow.train.Scaffold"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.get_assignment_map_from_checkpoint", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertModel.get_all_encoder_layers"], ["", "def", "model_fn_builder", "(", "bert_config", ",", "init_checkpoint", ",", "layer_indexes", ",", "use_tpu", ",", "\n", "use_one_hot_embeddings", ")", ":", "\n", "  ", "\"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"", "\n", "\n", "def", "model_fn", "(", "features", ",", "labels", ",", "mode", ",", "params", ")", ":", "# pylint: disable=unused-argument", "\n", "    ", "\"\"\"The `model_fn` for TPUEstimator.\"\"\"", "\n", "\n", "unique_ids", "=", "features", "[", "\"unique_ids\"", "]", "\n", "input_ids", "=", "features", "[", "\"input_ids\"", "]", "\n", "input_mask", "=", "features", "[", "\"input_mask\"", "]", "\n", "input_type_ids", "=", "features", "[", "\"input_type_ids\"", "]", "\n", "\n", "model", "=", "modeling", ".", "BertModel", "(", "\n", "config", "=", "bert_config", ",", "\n", "is_training", "=", "False", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "token_type_ids", "=", "input_type_ids", ",", "\n", "use_one_hot_embeddings", "=", "use_one_hot_embeddings", ")", "\n", "\n", "if", "mode", "!=", "tf", ".", "estimator", ".", "ModeKeys", ".", "PREDICT", ":", "\n", "      ", "raise", "ValueError", "(", "\"Only PREDICT modes are supported: %s\"", "%", "(", "mode", ")", ")", "\n", "\n", "", "tvars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "scaffold_fn", "=", "None", "\n", "(", "assignment_map", ",", "\n", "initialized_variable_names", ")", "=", "modeling", ".", "get_assignment_map_from_checkpoint", "(", "\n", "tvars", ",", "init_checkpoint", ")", "\n", "if", "use_tpu", ":", "\n", "\n", "      ", "def", "tpu_scaffold", "(", ")", ":", "\n", "        ", "tf", ".", "train", ".", "init_from_checkpoint", "(", "init_checkpoint", ",", "assignment_map", ")", "\n", "return", "tf", ".", "train", ".", "Scaffold", "(", ")", "\n", "\n", "", "scaffold_fn", "=", "tpu_scaffold", "\n", "", "else", ":", "\n", "      ", "tf", ".", "train", ".", "init_from_checkpoint", "(", "init_checkpoint", ",", "assignment_map", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"**** Trainable Variables ****\"", ")", "\n", "for", "var", "in", "tvars", ":", "\n", "      ", "init_string", "=", "\"\"", "\n", "if", "var", ".", "name", "in", "initialized_variable_names", ":", "\n", "        ", "init_string", "=", "\", *INIT_FROM_CKPT*\"", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"  name = %s, shape = %s%s\"", ",", "var", ".", "name", ",", "var", ".", "shape", ",", "\n", "init_string", ")", "\n", "\n", "", "all_layers", "=", "model", ".", "get_all_encoder_layers", "(", ")", "\n", "\n", "predictions", "=", "{", "\n", "\"unique_id\"", ":", "unique_ids", ",", "\n", "}", "\n", "\n", "for", "(", "i", ",", "layer_index", ")", "in", "enumerate", "(", "layer_indexes", ")", ":", "\n", "      ", "predictions", "[", "\"layer_output_%d\"", "%", "i", "]", "=", "all_layers", "[", "layer_index", "]", "\n", "\n", "", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "predictions", "=", "predictions", ",", "scaffold_fn", "=", "scaffold_fn", ")", "\n", "return", "output_spec", "\n", "\n", "", "return", "model_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.extract_features.convert_examples_to_features": [[210, 300], ["enumerate", "tokenizer.tokenize", "tokens.append", "input_type_ids.append", "tokens.append", "input_type_ids.append", "tokenizer.convert_tokens_to_ids", "features.append", "tokenizer.tokenize", "extract_features._truncate_seq_pair", "tokens.append", "input_type_ids.append", "tokens.append", "input_type_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "input_type_ids.append", "len", "len", "len", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "extract_features.InputFeatures", "len", "tokens.append", "input_type_ids.append", "tokenization.printable_text", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier._truncate_seq_pair", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.printable_text"], ["", "def", "convert_examples_to_features", "(", "examples", ",", "seq_length", ",", "tokenizer", ")", ":", "\n", "  ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "    ", "tokens_a", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_a", ")", "\n", "\n", "tokens_b", "=", "None", "\n", "if", "example", ".", "text_b", ":", "\n", "      ", "tokens_b", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_b", ")", "\n", "\n", "", "if", "tokens_b", ":", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total", "\n", "# length is less than the specified length.", "\n", "# Account for [CLS], [SEP], [SEP] with \"- 3\"", "\n", "      ", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "seq_length", "-", "3", ")", "\n", "", "else", ":", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "      ", "if", "len", "(", "tokens_a", ")", ">", "seq_length", "-", "2", ":", "\n", "        ", "tokens_a", "=", "tokens_a", "[", "0", ":", "(", "seq_length", "-", "2", ")", "]", "\n", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] the dog is hairy . [SEP]", "\n", "#  type_ids: 0     0   0   0  0     0 0", "\n", "#", "\n", "# Where \"type_ids\" are used to indicate whether this is the first", "\n", "# sequence or the second sequence. The embedding vectors for `type=0` and", "\n", "# `type=1` were learned during pre-training and are added to the wordpiece", "\n", "# embedding vector (and position vector). This is not *strictly* necessary", "\n", "# since the [SEP] token unambiguously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "#", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "", "", "tokens", "=", "[", "]", "\n", "input_type_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "tokens_a", ":", "\n", "      ", "tokens", ".", "append", "(", "token", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "if", "tokens_b", ":", "\n", "      ", "for", "token", "in", "tokens_b", ":", "\n", "        ", "tokens", ".", "append", "(", "token", ")", "\n", "input_type_ids", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_type_ids", ".", "append", "(", "1", ")", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "seq_length", ":", "\n", "      ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_type_ids", ")", "==", "seq_length", "\n", "\n", "if", "ex_index", "<", "5", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"unique_id: %s\"", "%", "(", "example", ".", "unique_id", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "tokenization", ".", "printable_text", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "\"input_type_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_type_ids", "]", ")", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures", "(", "\n", "unique_id", "=", "example", ".", "unique_id", ",", "\n", "tokens", "=", "tokens", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "input_type_ids", "=", "input_type_ids", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.extract_features._truncate_seq_pair": [[302, 317], ["len", "len", "len", "len", "tokens_a.pop", "tokens_b.pop"], "function", ["None"], ["", "def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_length", ")", ":", "\n", "  ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "    ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "      ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "      ", "tokens_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "      ", "tokens_b", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.extract_features.read_examples": [[319, 341], ["tensorflow.gfile.GFile", "tokenization.convert_to_unicode", "line.strip.strip", "re.match", "examples.append", "reader.readline", "re.match.group", "re.match.group", "extract_features.InputExample"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode"], ["", "", "", "def", "read_examples", "(", "input_file", ")", ":", "\n", "  ", "\"\"\"Read a list of `InputExample`s from an input file.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "unique_id", "=", "0", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "input_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "    ", "while", "True", ":", "\n", "      ", "line", "=", "tokenization", ".", "convert_to_unicode", "(", "reader", ".", "readline", "(", ")", ")", "\n", "if", "not", "line", ":", "\n", "        ", "break", "\n", "", "line", "=", "line", ".", "strip", "(", ")", "\n", "text_a", "=", "None", "\n", "text_b", "=", "None", "\n", "m", "=", "re", ".", "match", "(", "r\"^(.*) \\|\\|\\| (.*)$\"", ",", "line", ")", "\n", "if", "m", "is", "None", ":", "\n", "        ", "text_a", "=", "line", "\n", "", "else", ":", "\n", "        ", "text_a", "=", "m", ".", "group", "(", "1", ")", "\n", "text_b", "=", "m", ".", "group", "(", "2", ")", "\n", "", "examples", ".", "append", "(", "\n", "InputExample", "(", "unique_id", "=", "unique_id", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ")", ")", "\n", "unique_id", "+=", "1", "\n", "", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.extract_features.main": [[343, 411], ["tensorflow.logging.set_verbosity", "modeling.BertConfig.from_json_file", "tokenization.FullTokenizer", "tensorflow.contrib.tpu.RunConfig", "extract_features.read_examples", "extract_features.convert_examples_to_features", "extract_features.model_fn_builder", "tensorflow.contrib.tpu.TPUEstimator", "extract_features.input_fn_builder", "int", "tf.contrib.tpu.TPUEstimator.predict", "FLAGS.layers.split", "tensorflow.contrib.tpu.TPUConfig", "codecs.getwriter", "tensorflow.gfile.Open", "int", "collections.OrderedDict", "enumerate", "writer.write", "enumerate", "collections.OrderedDict", "all_features.append", "collections.OrderedDict", "all_layers.append", "json.dumps", "round", "float"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertConfig.from_json_file", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.extract_features.read_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.convert_examples_to_features", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.model_fn_builder", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.input_fn_builder"], ["", "def", "main", "(", "_", ")", ":", "\n", "  ", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "\n", "layer_indexes", "=", "[", "int", "(", "x", ")", "for", "x", "in", "FLAGS", ".", "layers", ".", "split", "(", "\",\"", ")", "]", "\n", "\n", "bert_config", "=", "modeling", ".", "BertConfig", ".", "from_json_file", "(", "FLAGS", ".", "bert_config_file", ")", "\n", "\n", "tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "FLAGS", ".", "vocab_file", ",", "do_lower_case", "=", "FLAGS", ".", "do_lower_case", ")", "\n", "\n", "is_per_host", "=", "tf", ".", "contrib", ".", "tpu", ".", "InputPipelineConfig", ".", "PER_HOST_V2", "\n", "run_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "RunConfig", "(", "\n", "master", "=", "FLAGS", ".", "master", ",", "\n", "tpu_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUConfig", "(", "\n", "num_shards", "=", "FLAGS", ".", "num_tpu_cores", ",", "\n", "per_host_input_for_training", "=", "is_per_host", ")", ")", "\n", "\n", "examples", "=", "read_examples", "(", "FLAGS", ".", "input_file", ")", "\n", "\n", "features", "=", "convert_examples_to_features", "(", "\n", "examples", "=", "examples", ",", "seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "tokenizer", "=", "tokenizer", ")", "\n", "\n", "unique_id_to_feature", "=", "{", "}", "\n", "for", "feature", "in", "features", ":", "\n", "    ", "unique_id_to_feature", "[", "feature", ".", "unique_id", "]", "=", "feature", "\n", "\n", "", "model_fn", "=", "model_fn_builder", "(", "\n", "bert_config", "=", "bert_config", ",", "\n", "init_checkpoint", "=", "FLAGS", ".", "init_checkpoint", ",", "\n", "layer_indexes", "=", "layer_indexes", ",", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "use_one_hot_embeddings", "=", "FLAGS", ".", "use_one_hot_embeddings", ")", "\n", "\n", "# If TPU is not available, this will fall back to normal Estimator on CPU", "\n", "# or GPU.", "\n", "estimator", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimator", "(", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "model_fn", "=", "model_fn", ",", "\n", "config", "=", "run_config", ",", "\n", "predict_batch_size", "=", "FLAGS", ".", "batch_size", ")", "\n", "\n", "input_fn", "=", "input_fn_builder", "(", "\n", "features", "=", "features", ",", "seq_length", "=", "FLAGS", ".", "max_seq_length", ")", "\n", "\n", "with", "codecs", ".", "getwriter", "(", "\"utf-8\"", ")", "(", "tf", ".", "gfile", ".", "Open", "(", "FLAGS", ".", "output_file", ",", "\n", "\"w\"", ")", ")", "as", "writer", ":", "\n", "    ", "for", "result", "in", "estimator", ".", "predict", "(", "input_fn", ",", "yield_single_examples", "=", "True", ")", ":", "\n", "      ", "unique_id", "=", "int", "(", "result", "[", "\"unique_id\"", "]", ")", "\n", "feature", "=", "unique_id_to_feature", "[", "unique_id", "]", "\n", "output_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "output_json", "[", "\"linex_index\"", "]", "=", "unique_id", "\n", "all_features", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "feature", ".", "tokens", ")", ":", "\n", "        ", "all_layers", "=", "[", "]", "\n", "for", "(", "j", ",", "layer_index", ")", "in", "enumerate", "(", "layer_indexes", ")", ":", "\n", "          ", "layer_output", "=", "result", "[", "\"layer_output_%d\"", "%", "j", "]", "\n", "layers", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "layers", "[", "\"index\"", "]", "=", "layer_index", "\n", "layers", "[", "\"values\"", "]", "=", "[", "\n", "round", "(", "float", "(", "x", ")", ",", "6", ")", "for", "x", "in", "layer_output", "[", "i", ":", "(", "i", "+", "1", ")", "]", ".", "flat", "\n", "]", "\n", "all_layers", ".", "append", "(", "layers", ")", "\n", "", "features", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "features", "[", "\"token\"", "]", "=", "token", "\n", "features", "[", "\"layers\"", "]", "=", "all_layers", "\n", "all_features", ".", "append", "(", "features", ")", "\n", "", "output_json", "[", "\"features\"", "]", "=", "all_features", "\n", "writer", ".", "write", "(", "json", ".", "dumps", "(", "output_json", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.InputExample.__init__": [[140, 156], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "guid", ",", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "None", ")", ":", "\n", "    ", "\"\"\"Constructs a InputExample.\n\n    Args:\n      guid: Unique id for the example.\n      text_a: string. The untokenized text of the first sequence. For single\n        sequence tasks, only this sequence must be specified.\n      text_b: (Optional) string. The untokenized text of the second sequence.\n        Only must be specified for sequence pair tasks.\n      label: (Optional) string. The label of the example. This should be\n        specified for train and dev examples, but not for test examples.\n    \"\"\"", "\n", "self", ".", "guid", "=", "guid", "\n", "self", ".", "text_a", "=", "text_a", "\n", "self", ".", "text_b", "=", "text_b", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.InputFeatures.__init__": [[174, 185], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "input_ids", ",", "\n", "input_mask", ",", "\n", "segment_ids", ",", "\n", "label_id", ",", "\n", "is_real_example", "=", "True", ")", ":", "\n", "    ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "label_id", "=", "label_id", "\n", "self", ".", "is_real_example", "=", "is_real_example", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor.get_train_examples": [[190, 193], ["NotImplementedError"], "methods", ["None"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "    ", "\"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor.get_dev_examples": [[194, 197], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "    ", "\"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor.get_test_examples": [[198, 201], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "    ", "\"\"\"Gets a collection of `InputExample`s for prediction.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor.get_labels": [[202, 205], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "    ", "\"\"\"Gets the list of labels for this data set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv": [[206, 215], ["tensorflow.gfile.Open", "csv.reader", "lines.append"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "_read_tsv", "(", "cls", ",", "input_file", ",", "quotechar", "=", "None", ")", ":", "\n", "    ", "\"\"\"Reads a tab separated value file.\"\"\"", "\n", "with", "tf", ".", "gfile", ".", "Open", "(", "input_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "      ", "reader", "=", "csv", ".", "reader", "(", "f", ",", "delimiter", "=", "\"\\t\"", ",", "quotechar", "=", "quotechar", ")", "\n", "lines", "=", "[", "]", "\n", "for", "line", "in", "reader", ":", "\n", "        ", "lines", ".", "append", "(", "line", ")", "\n", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.XnliProcessor.__init__": [[220, 222], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "    ", "self", ".", "language", "=", "\"zh\"", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.XnliProcessor.get_train_examples": [[223, 241], ["run_classifier.XnliProcessor._read_tsv", "enumerate", "os.path.join", "tokenization.convert_to_unicode", "tokenization.convert_to_unicode", "tokenization.convert_to_unicode", "examples.append", "tokenization.convert_to_unicode", "tokenization.convert_to_unicode", "run_classifier.InputExample"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode"], ["", "def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "lines", "=", "self", ".", "_read_tsv", "(", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"multinli\"", ",", "\n", "\"multinli.train.%s.tsv\"", "%", "self", ".", "language", ")", ")", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "      ", "if", "i", "==", "0", ":", "\n", "        ", "continue", "\n", "", "guid", "=", "\"train-%d\"", "%", "(", "i", ")", "\n", "text_a", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "0", "]", ")", "\n", "text_b", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "1", "]", ")", "\n", "label", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "2", "]", ")", "\n", "if", "label", "==", "tokenization", ".", "convert_to_unicode", "(", "\"contradictory\"", ")", ":", "\n", "        ", "label", "=", "tokenization", ".", "convert_to_unicode", "(", "\"contradiction\"", ")", "\n", "", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.XnliProcessor.get_dev_examples": [[242, 259], ["run_classifier.XnliProcessor._read_tsv", "enumerate", "os.path.join", "tokenization.convert_to_unicode", "tokenization.convert_to_unicode", "tokenization.convert_to_unicode", "tokenization.convert_to_unicode", "examples.append", "tokenization.convert_to_unicode", "run_classifier.InputExample"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "lines", "=", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"xnli.dev.tsv\"", ")", ")", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "      ", "if", "i", "==", "0", ":", "\n", "        ", "continue", "\n", "", "guid", "=", "\"dev-%d\"", "%", "(", "i", ")", "\n", "language", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "0", "]", ")", "\n", "if", "language", "!=", "tokenization", ".", "convert_to_unicode", "(", "self", ".", "language", ")", ":", "\n", "        ", "continue", "\n", "", "text_a", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "6", "]", ")", "\n", "text_b", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "7", "]", ")", "\n", "label", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "1", "]", ")", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.XnliProcessor.get_labels": [[260, 263], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "\"contradiction\"", ",", "\"entailment\"", ",", "\"neutral\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.MnliProcessor.get_train_examples": [[268, 272], ["run_classifier.MnliProcessor._create_examples", "run_classifier.MnliProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.tsv\"", ")", ")", ",", "\"train\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.MnliProcessor.get_dev_examples": [[273, 278], ["run_classifier.MnliProcessor._create_examples", "run_classifier.MnliProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"dev_matched.tsv\"", ")", ")", ",", "\n", "\"dev_matched\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.MnliProcessor.get_test_examples": [[279, 283], ["run_classifier.MnliProcessor._create_examples", "run_classifier.MnliProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"test_matched.tsv\"", ")", ")", ",", "\"test\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.MnliProcessor.get_labels": [[284, 287], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "\"contradiction\"", ",", "\"entailment\"", ",", "\"neutral\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.MnliProcessor._create_examples": [[288, 304], ["enumerate", "tokenization.convert_to_unicode", "tokenization.convert_to_unicode", "examples.append", "tokenization.convert_to_unicode", "run_classifier.InputExample", "tokenization.convert_to_unicode"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "    ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "      ", "if", "i", "==", "0", ":", "\n", "        ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "0", "]", ")", ")", "\n", "text_a", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "8", "]", ")", "\n", "text_b", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "9", "]", ")", "\n", "if", "set_type", "==", "\"test\"", ":", "\n", "        ", "label", "=", "\"contradiction\"", "\n", "", "else", ":", "\n", "        ", "label", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "-", "1", "]", ")", "\n", "", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.MrpcProcessor.get_train_examples": [[309, 313], ["run_classifier.MrpcProcessor._create_examples", "run_classifier.MrpcProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.tsv\"", ")", ")", ",", "\"train\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.MrpcProcessor.get_dev_examples": [[314, 318], ["run_classifier.MrpcProcessor._create_examples", "run_classifier.MrpcProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"dev.tsv\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.MrpcProcessor.get_test_examples": [[319, 323], ["run_classifier.MrpcProcessor._create_examples", "run_classifier.MrpcProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"test.tsv\"", ")", ")", ",", "\"test\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.MrpcProcessor.get_labels": [[324, 327], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "\"0\"", ",", "\"1\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.MrpcProcessor._create_examples": [[328, 344], ["enumerate", "tokenization.convert_to_unicode", "tokenization.convert_to_unicode", "examples.append", "tokenization.convert_to_unicode", "run_classifier.InputExample"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "    ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "      ", "if", "i", "==", "0", ":", "\n", "        ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "text_a", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "3", "]", ")", "\n", "text_b", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "4", "]", ")", "\n", "if", "set_type", "==", "\"test\"", ":", "\n", "        ", "label", "=", "\"0\"", "\n", "", "else", ":", "\n", "        ", "label", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "0", "]", ")", "\n", "", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.ColaProcessor.get_train_examples": [[349, 353], ["run_classifier.ColaProcessor._create_examples", "run_classifier.ColaProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.tsv\"", ")", ")", ",", "\"train\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.ColaProcessor.get_dev_examples": [[354, 358], ["run_classifier.ColaProcessor._create_examples", "run_classifier.ColaProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"dev.tsv\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.ColaProcessor.get_test_examples": [[359, 363], ["run_classifier.ColaProcessor._create_examples", "run_classifier.ColaProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"test.tsv\"", ")", ")", ",", "\"test\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.ColaProcessor.get_labels": [[364, 367], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "\"0\"", ",", "\"1\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.ColaProcessor._create_examples": [[368, 385], ["enumerate", "examples.append", "tokenization.convert_to_unicode", "tokenization.convert_to_unicode", "tokenization.convert_to_unicode", "run_classifier.InputExample"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "    ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "# Only the test set has a header", "\n", "      ", "if", "set_type", "==", "\"test\"", "and", "i", "==", "0", ":", "\n", "        ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "if", "set_type", "==", "\"test\"", ":", "\n", "        ", "text_a", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "1", "]", ")", "\n", "label", "=", "\"0\"", "\n", "", "else", ":", "\n", "        ", "text_a", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "3", "]", ")", "\n", "label", "=", "tokenization", ".", "convert_to_unicode", "(", "line", "[", "1", "]", ")", "\n", "", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.Sst2Processor.get_train_examples": [[389, 393], ["run_classifier.Sst2Processor._create_examples", "run_classifier.Sst2Processor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.tsv\"", ")", ")", ",", "\"train\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.Sst2Processor.get_dev_examples": [[394, 398], ["run_classifier.Sst2Processor._create_examples", "run_classifier.Sst2Processor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"dev.tsv\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.Sst2Processor.get_labels": [[399, 402], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "\"0\"", ",", "\"1\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.Sst2Processor._create_examples": [[403, 415], ["enumerate", "examples.append", "run_classifier.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "text_a", "=", "line", "[", "0", "]", "\n", "label", "=", "line", "[", "1", "]", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.StsbProcessor.get_train_examples": [[420, 424], ["run_classifier.StsbProcessor._create_examples", "run_classifier.StsbProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.tsv\"", ")", ")", ",", "\"train\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.StsbProcessor.get_dev_examples": [[425, 429], ["run_classifier.StsbProcessor._create_examples", "run_classifier.StsbProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"dev.tsv\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.StsbProcessor.get_labels": [[430, 433], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "None", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.StsbProcessor._create_examples": [[434, 447], ["enumerate", "examples.append", "run_classifier.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "line", "[", "0", "]", ")", "\n", "text_a", "=", "line", "[", "7", "]", "\n", "text_b", "=", "line", "[", "8", "]", "\n", "label", "=", "line", "[", "-", "1", "]", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.QqpProcessor.get_train_examples": [[452, 456], ["run_classifier.QqpProcessor._create_examples", "run_classifier.QqpProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.tsv\"", ")", ")", ",", "\"train\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.QqpProcessor.get_dev_examples": [[457, 461], ["run_classifier.QqpProcessor._create_examples", "run_classifier.QqpProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"dev.tsv\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.QqpProcessor.get_labels": [[462, 465], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "\"0\"", ",", "\"1\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.QqpProcessor._create_examples": [[466, 482], ["enumerate", "examples.append", "run_classifier.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "line", "[", "0", "]", ")", "\n", "try", ":", "\n", "                ", "text_a", "=", "line", "[", "3", "]", "\n", "text_b", "=", "line", "[", "4", "]", "\n", "label", "=", "line", "[", "5", "]", "\n", "", "except", "IndexError", ":", "\n", "                ", "continue", "\n", "", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.QnliProcessor.get_train_examples": [[487, 491], ["run_classifier.QnliProcessor._create_examples", "run_classifier.QnliProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.tsv\"", ")", ")", ",", "\"train\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.QnliProcessor.get_dev_examples": [[492, 497], ["run_classifier.QnliProcessor._create_examples", "run_classifier.QnliProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"dev.tsv\"", ")", ")", ",", "\n", "\"dev_matched\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.QnliProcessor.get_labels": [[498, 501], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "\"entailment\"", ",", "\"not_entailment\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.QnliProcessor._create_examples": [[502, 515], ["enumerate", "examples.append", "run_classifier.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "line", "[", "0", "]", ")", "\n", "text_a", "=", "line", "[", "1", "]", "\n", "text_b", "=", "line", "[", "2", "]", "\n", "label", "=", "line", "[", "-", "1", "]", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.RteProcessor.get_train_examples": [[520, 524], ["run_classifier.RteProcessor._create_examples", "run_classifier.RteProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.tsv\"", ")", ")", ",", "\"train\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.RteProcessor.get_dev_examples": [[525, 529], ["run_classifier.RteProcessor._create_examples", "run_classifier.RteProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"dev.tsv\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.RteProcessor.get_labels": [[530, 533], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "\"entailment\"", ",", "\"not_entailment\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.RteProcessor._create_examples": [[534, 547], ["enumerate", "examples.append", "run_classifier.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "line", "[", "0", "]", ")", "\n", "text_a", "=", "line", "[", "1", "]", "\n", "text_b", "=", "line", "[", "2", "]", "\n", "label", "=", "line", "[", "-", "1", "]", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor.get_train_examples": [[552, 556], ["run_classifier.WnliProcessor._create_examples", "run_classifier.WnliProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.tsv\"", ")", ")", ",", "\"train\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor.get_dev_examples": [[557, 561], ["run_classifier.WnliProcessor._create_examples", "run_classifier.WnliProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.DataProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"dev.tsv\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor.get_labels": [[562, 565], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "\"0\"", ",", "\"1\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor._create_examples": [[566, 579], ["enumerate", "examples.append", "run_classifier.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "line", "[", "0", "]", ")", "\n", "text_a", "=", "line", "[", "1", "]", "\n", "text_b", "=", "line", "[", "2", "]", "\n", "label", "=", "line", "[", "-", "1", "]", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.convert_single_example": [[581, 681], ["isinstance", "enumerate", "tokenizer.tokenize", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "tokenizer.convert_tokens_to_ids", "run_classifier.InputFeatures", "run_classifier.InputFeatures", "tokenizer.tokenize", "run_classifier._truncate_seq_pair", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "len", "len", "len", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "len", "tokens.append", "segment_ids.append", "tokenization.printable_text", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier._truncate_seq_pair", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.printable_text"], ["", "", "def", "convert_single_example", "(", "ex_index", ",", "example", ",", "label_list", ",", "max_seq_length", ",", "\n", "tokenizer", ")", ":", "\n", "  ", "\"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"", "\n", "\n", "if", "isinstance", "(", "example", ",", "PaddingInputExample", ")", ":", "\n", "    ", "return", "InputFeatures", "(", "\n", "input_ids", "=", "[", "0", "]", "*", "max_seq_length", ",", "\n", "input_mask", "=", "[", "0", "]", "*", "max_seq_length", ",", "\n", "segment_ids", "=", "[", "0", "]", "*", "max_seq_length", ",", "\n", "label_id", "=", "0", ",", "\n", "is_real_example", "=", "False", ")", "\n", "\n", "", "label_map", "=", "{", "}", "\n", "for", "(", "i", ",", "label", ")", "in", "enumerate", "(", "label_list", ")", ":", "\n", "    ", "label_map", "[", "label", "]", "=", "i", "\n", "\n", "", "tokens_a", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_a", ")", "\n", "tokens_b", "=", "None", "\n", "if", "example", ".", "text_b", ":", "\n", "    ", "tokens_b", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_b", ")", "\n", "\n", "", "if", "tokens_b", ":", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total", "\n", "# length is less than the specified length.", "\n", "# Account for [CLS], [SEP], [SEP] with \"- 3\"", "\n", "    ", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_seq_length", "-", "3", ")", "\n", "", "else", ":", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "    ", "if", "len", "(", "tokens_a", ")", ">", "max_seq_length", "-", "2", ":", "\n", "      ", "tokens_a", "=", "tokens_a", "[", "0", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] the dog is hairy . [SEP]", "\n", "#  type_ids: 0     0   0   0  0     0 0", "\n", "#", "\n", "# Where \"type_ids\" are used to indicate whether this is the first", "\n", "# sequence or the second sequence. The embedding vectors for `type=0` and", "\n", "# `type=1` were learned during pre-training and are added to the wordpiece", "\n", "# embedding vector (and position vector). This is not *strictly* necessary", "\n", "# since the [SEP] token unambiguously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "#", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "", "", "tokens", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "tokens_a", ":", "\n", "    ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "if", "tokens_b", ":", "\n", "    ", "for", "token", "in", "tokens_b", ":", "\n", "      ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "    ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "\n", "label_id", "=", "label_map", "[", "example", ".", "label", "]", "\n", "if", "ex_index", "<", "5", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"guid: %s\"", "%", "(", "example", ".", "guid", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "tokenization", ".", "printable_text", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"label: %s (id = %d)\"", "%", "(", "example", ".", "label", ",", "label_id", ")", ")", "\n", "\n", "", "feature", "=", "InputFeatures", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "label_id", "=", "label_id", ",", "\n", "is_real_example", "=", "True", ")", "\n", "return", "feature", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.file_based_convert_examples_to_features": [[683, 711], ["tensorflow.python_io.TFRecordWriter", "enumerate", "tf.python_io.TFRecordWriter.close", "run_classifier.convert_single_example", "collections.OrderedDict", "run_classifier.file_based_convert_examples_to_features.create_int_feature"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.FeatureWriter.close", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.convert_single_example", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.create_int_feature"], ["", "def", "file_based_convert_examples_to_features", "(", "\n", "examples", ",", "label_list", ",", "max_seq_length", ",", "tokenizer", ",", "output_file", ")", ":", "\n", "  ", "\"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"", "\n", "\n", "writer", "=", "tf", ".", "python_io", ".", "TFRecordWriter", "(", "output_file", ")", "\n", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "    ", "if", "ex_index", "%", "10000", "==", "0", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"Writing example %d of %d\"", "%", "(", "ex_index", ",", "len", "(", "examples", ")", ")", ")", "\n", "\n", "", "feature", "=", "convert_single_example", "(", "ex_index", ",", "example", ",", "label_list", ",", "\n", "max_seq_length", ",", "tokenizer", ")", "\n", "\n", "def", "create_int_feature", "(", "values", ")", ":", "\n", "      ", "f", "=", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "list", "(", "values", ")", ")", ")", "\n", "return", "f", "\n", "\n", "", "features", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "features", "[", "\"input_ids\"", "]", "=", "create_int_feature", "(", "feature", ".", "input_ids", ")", "\n", "features", "[", "\"input_mask\"", "]", "=", "create_int_feature", "(", "feature", ".", "input_mask", ")", "\n", "features", "[", "\"segment_ids\"", "]", "=", "create_int_feature", "(", "feature", ".", "segment_ids", ")", "\n", "features", "[", "\"label_ids\"", "]", "=", "create_int_feature", "(", "[", "feature", ".", "label_id", "]", ")", "\n", "features", "[", "\"is_real_example\"", "]", "=", "create_int_feature", "(", "\n", "[", "int", "(", "feature", ".", "is_real_example", ")", "]", ")", "\n", "\n", "tf_example", "=", "tf", ".", "train", ".", "Example", "(", "features", "=", "tf", ".", "train", ".", "Features", "(", "feature", "=", "features", ")", ")", "\n", "writer", ".", "write", "(", "tf_example", ".", "SerializeToString", "(", ")", ")", "\n", "", "writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.file_based_input_fn_builder": [[713, 759], ["tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.parse_single_example", "list", "tensorflow.data.TFRecordDataset", "d.shuffle.apply", "tf.parse_single_example.keys", "d.shuffle.repeat", "d.shuffle.shuffle", "tensorflow.contrib.data.map_and_batch", "tensorflow.to_int32", "run_classifier.file_based_input_fn_builder._decode_record"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_pretraining._decode_record"], ["", "def", "file_based_input_fn_builder", "(", "input_file", ",", "seq_length", ",", "is_training", ",", "\n", "drop_remainder", ")", ":", "\n", "  ", "\"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"", "\n", "\n", "name_to_features", "=", "{", "\n", "\"input_ids\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"input_mask\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"segment_ids\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"label_ids\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"is_real_example\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "tf", ".", "int64", ")", ",", "\n", "}", "\n", "\n", "def", "_decode_record", "(", "record", ",", "name_to_features", ")", ":", "\n", "    ", "\"\"\"Decodes a record to a TensorFlow example.\"\"\"", "\n", "example", "=", "tf", ".", "parse_single_example", "(", "record", ",", "name_to_features", ")", "\n", "\n", "# tf.Example only supports tf.int64, but the TPU only supports tf.int32.", "\n", "# So cast all int64 to int32.", "\n", "for", "name", "in", "list", "(", "example", ".", "keys", "(", ")", ")", ":", "\n", "      ", "t", "=", "example", "[", "name", "]", "\n", "if", "t", ".", "dtype", "==", "tf", ".", "int64", ":", "\n", "        ", "t", "=", "tf", ".", "to_int32", "(", "t", ")", "\n", "", "example", "[", "name", "]", "=", "t", "\n", "\n", "", "return", "example", "\n", "\n", "", "def", "input_fn", "(", "params", ")", ":", "\n", "    ", "\"\"\"The actual input function.\"\"\"", "\n", "batch_size", "=", "params", "[", "\"batch_size\"", "]", "\n", "\n", "# For training, we want a lot of parallel reading and shuffling.", "\n", "# For eval, we want no shuffling and parallel reading doesn't matter.", "\n", "d", "=", "tf", ".", "data", ".", "TFRecordDataset", "(", "input_file", ")", "\n", "if", "is_training", ":", "\n", "      ", "d", "=", "d", ".", "repeat", "(", ")", "\n", "d", "=", "d", ".", "shuffle", "(", "buffer_size", "=", "100", ")", "\n", "\n", "", "d", "=", "d", ".", "apply", "(", "\n", "tf", ".", "contrib", ".", "data", ".", "map_and_batch", "(", "\n", "lambda", "record", ":", "_decode_record", "(", "record", ",", "name_to_features", ")", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "drop_remainder", "=", "drop_remainder", ")", ")", "\n", "\n", "return", "d", "\n", "\n", "", "return", "input_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier._truncate_seq_pair": [[761, 776], ["len", "len", "len", "len", "tokens_a.pop", "tokens_b.pop"], "function", ["None"], ["", "def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_length", ")", ":", "\n", "  ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "    ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "      ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "      ", "tokens_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "      ", "tokens_b", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.create_model": [[778, 821], ["modeling.BertModel", "modeling.BertModel.get_pooled_output", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.matmul", "tensorflow.nn.bias_add", "tensorflow.nn.softmax", "tensorflow.nn.log_softmax", "tensorflow.one_hot", "tensorflow.reduce_mean", "tensorflow.truncated_normal_initializer", "tensorflow.zeros_initializer", "tensorflow.nn.dropout", "tensorflow.reduce_sum"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertModel.get_pooled_output", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.dropout"], ["", "", "", "def", "create_model", "(", "bert_config", ",", "is_training", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "\n", "labels", ",", "num_labels", ",", "use_one_hot_embeddings", ")", ":", "\n", "  ", "\"\"\"Creates a classification model.\"\"\"", "\n", "model", "=", "modeling", ".", "BertModel", "(", "\n", "config", "=", "bert_config", ",", "\n", "is_training", "=", "is_training", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "token_type_ids", "=", "segment_ids", ",", "\n", "use_one_hot_embeddings", "=", "use_one_hot_embeddings", ")", "\n", "\n", "# In the demo, we are doing a simple classification task on the entire", "\n", "# segment.", "\n", "#", "\n", "# If you want to use the token-level output, use model.get_sequence_output()", "\n", "# instead.", "\n", "output_layer", "=", "model", ".", "get_pooled_output", "(", ")", "\n", "\n", "hidden_size", "=", "output_layer", ".", "shape", "[", "-", "1", "]", ".", "value", "\n", "\n", "output_weights", "=", "tf", ".", "get_variable", "(", "\n", "\"output_weights\"", ",", "[", "num_labels", ",", "hidden_size", "]", ",", "\n", "initializer", "=", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "0.02", ")", ")", "\n", "\n", "output_bias", "=", "tf", ".", "get_variable", "(", "\n", "\"output_bias\"", ",", "[", "num_labels", "]", ",", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"loss\"", ")", ":", "\n", "    ", "if", "is_training", ":", "\n", "# I.e., 0.1 dropout", "\n", "      ", "output_layer", "=", "tf", ".", "nn", ".", "dropout", "(", "output_layer", ",", "keep_prob", "=", "0.9", ")", "\n", "\n", "", "logits", "=", "tf", ".", "matmul", "(", "output_layer", ",", "output_weights", ",", "transpose_b", "=", "True", ")", "\n", "logits", "=", "tf", ".", "nn", ".", "bias_add", "(", "logits", ",", "output_bias", ")", "\n", "probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "logits", ",", "axis", "=", "-", "1", ")", "\n", "log_probs", "=", "tf", ".", "nn", ".", "log_softmax", "(", "logits", ",", "axis", "=", "-", "1", ")", "\n", "\n", "one_hot_labels", "=", "tf", ".", "one_hot", "(", "labels", ",", "depth", "=", "num_labels", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "per_example_loss", "=", "-", "tf", ".", "reduce_sum", "(", "one_hot_labels", "*", "log_probs", ",", "axis", "=", "-", "1", ")", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "per_example_loss", ")", "\n", "\n", "return", "(", "loss", ",", "per_example_loss", ",", "logits", ",", "probabilities", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.model_fn_builder": [[823, 913], ["tensorflow.logging.info", "sorted", "run_classifier.create_model", "tensorflow.trainable_variables", "tensorflow.logging.info", "features.keys", "tensorflow.logging.info", "tensorflow.cast", "tensorflow.ones", "modeling.get_assignment_map_from_checkpoint", "tensorflow.logging.info", "optimization.create_optimizer", "tensorflow.contrib.tpu.TPUEstimatorSpec", "tensorflow.shape", "tensorflow.train.init_from_checkpoint", "tensorflow.contrib.tpu.TPUEstimatorSpec", "tensorflow.contrib.tpu.TPUEstimatorSpec", "tensorflow.train.init_from_checkpoint", "tensorflow.train.Scaffold", "tensorflow.argmax", "tensorflow.metrics.accuracy", "tensorflow.metrics.mean"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.create_model", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.get_assignment_map_from_checkpoint", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.optimization.create_optimizer"], ["", "", "def", "model_fn_builder", "(", "bert_config", ",", "num_labels", ",", "init_checkpoint", ",", "learning_rate", ",", "\n", "num_train_steps", ",", "num_warmup_steps", ",", "use_tpu", ",", "\n", "use_one_hot_embeddings", ",", "prune_config_flag", ")", ":", "\n", "  ", "\"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"", "\n", "\n", "def", "model_fn", "(", "features", ",", "labels", ",", "mode", ",", "params", ")", ":", "# pylint: disable=unused-argument", "\n", "    ", "\"\"\"The `model_fn` for TPUEstimator.\"\"\"", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"*** Features ***\"", ")", "\n", "for", "name", "in", "sorted", "(", "features", ".", "keys", "(", ")", ")", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"  name = %s, shape = %s\"", "%", "(", "name", ",", "features", "[", "name", "]", ".", "shape", ")", ")", "\n", "\n", "", "input_ids", "=", "features", "[", "\"input_ids\"", "]", "\n", "input_mask", "=", "features", "[", "\"input_mask\"", "]", "\n", "segment_ids", "=", "features", "[", "\"segment_ids\"", "]", "\n", "label_ids", "=", "features", "[", "\"label_ids\"", "]", "\n", "is_real_example", "=", "None", "\n", "if", "\"is_real_example\"", "in", "features", ":", "\n", "      ", "is_real_example", "=", "tf", ".", "cast", "(", "features", "[", "\"is_real_example\"", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "", "else", ":", "\n", "      ", "is_real_example", "=", "tf", ".", "ones", "(", "tf", ".", "shape", "(", "label_ids", ")", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "", "is_training", "=", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ")", "\n", "\n", "(", "total_loss", ",", "per_example_loss", ",", "logits", ",", "probabilities", ")", "=", "create_model", "(", "\n", "bert_config", ",", "is_training", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_ids", ",", "\n", "num_labels", ",", "use_one_hot_embeddings", ")", "\n", "\n", "tvars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "initialized_variable_names", "=", "{", "}", "\n", "scaffold_fn", "=", "None", "\n", "if", "init_checkpoint", ":", "\n", "      ", "(", "assignment_map", ",", "initialized_variable_names", "\n", ")", "=", "modeling", ".", "get_assignment_map_from_checkpoint", "(", "tvars", ",", "init_checkpoint", ")", "\n", "if", "use_tpu", ":", "\n", "\n", "        ", "def", "tpu_scaffold", "(", ")", ":", "\n", "          ", "tf", ".", "train", ".", "init_from_checkpoint", "(", "init_checkpoint", ",", "assignment_map", ")", "\n", "return", "tf", ".", "train", ".", "Scaffold", "(", ")", "\n", "\n", "", "scaffold_fn", "=", "tpu_scaffold", "\n", "", "else", ":", "\n", "        ", "tf", ".", "train", ".", "init_from_checkpoint", "(", "init_checkpoint", ",", "assignment_map", ")", "\n", "\n", "", "", "tf", ".", "logging", ".", "info", "(", "\"**** Trainable Variables ****\"", ")", "\n", "for", "var", "in", "tvars", ":", "\n", "      ", "init_string", "=", "\"\"", "\n", "if", "var", ".", "name", "in", "initialized_variable_names", ":", "\n", "        ", "init_string", "=", "\", *INIT_FROM_CKPT*\"", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"  name = %s, shape = %s%s\"", ",", "var", ".", "name", ",", "var", ".", "shape", ",", "\n", "init_string", ")", "\n", "\n", "", "output_spec", "=", "None", "\n", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "\n", "      ", "train_op", "=", "optimization", ".", "create_optimizer", "(", "\n", "total_loss", ",", "learning_rate", ",", "num_train_steps", ",", "num_warmup_steps", ",", "use_tpu", ",", "prune_config_flag", ")", "\n", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "\n", "loss", "=", "total_loss", ",", "\n", "train_op", "=", "train_op", ",", "\n", "scaffold_fn", "=", "scaffold_fn", ")", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "\n", "      ", "def", "metric_fn", "(", "per_example_loss", ",", "label_ids", ",", "logits", ",", "is_real_example", ")", ":", "\n", "        ", "predictions", "=", "tf", ".", "argmax", "(", "logits", ",", "axis", "=", "-", "1", ",", "output_type", "=", "tf", ".", "int32", ")", "\n", "accuracy", "=", "tf", ".", "metrics", ".", "accuracy", "(", "\n", "labels", "=", "label_ids", ",", "predictions", "=", "predictions", ",", "weights", "=", "is_real_example", ")", "\n", "loss", "=", "tf", ".", "metrics", ".", "mean", "(", "values", "=", "per_example_loss", ",", "weights", "=", "is_real_example", ")", "\n", "return", "{", "\n", "\"eval_accuracy\"", ":", "accuracy", ",", "\n", "\"eval_loss\"", ":", "loss", ",", "\n", "}", "\n", "\n", "", "eval_metrics", "=", "(", "metric_fn", ",", "\n", "[", "per_example_loss", ",", "label_ids", ",", "logits", ",", "is_real_example", "]", ")", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "\n", "loss", "=", "total_loss", ",", "\n", "eval_metrics", "=", "eval_metrics", ",", "\n", "scaffold_fn", "=", "scaffold_fn", ")", "\n", "", "else", ":", "\n", "      ", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "\n", "predictions", "=", "{", "\"probabilities\"", ":", "probabilities", "}", ",", "\n", "scaffold_fn", "=", "scaffold_fn", ")", "\n", "", "return", "output_spec", "\n", "\n", "", "return", "model_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.input_fn_builder": [[917, 967], ["all_input_ids.append", "all_input_mask.append", "all_segment_ids.append", "all_label_ids.append", "len", "tensorflow.data.Dataset.from_tensor_slices", "d.shuffle.batch", "d.shuffle.repeat", "d.shuffle.shuffle", "tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "tensorflow.constant"], "function", ["None"], ["", "def", "input_fn_builder", "(", "features", ",", "seq_length", ",", "is_training", ",", "drop_remainder", ")", ":", "\n", "  ", "\"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"", "\n", "\n", "all_input_ids", "=", "[", "]", "\n", "all_input_mask", "=", "[", "]", "\n", "all_segment_ids", "=", "[", "]", "\n", "all_label_ids", "=", "[", "]", "\n", "\n", "for", "feature", "in", "features", ":", "\n", "    ", "all_input_ids", ".", "append", "(", "feature", ".", "input_ids", ")", "\n", "all_input_mask", ".", "append", "(", "feature", ".", "input_mask", ")", "\n", "all_segment_ids", ".", "append", "(", "feature", ".", "segment_ids", ")", "\n", "all_label_ids", ".", "append", "(", "feature", ".", "label_id", ")", "\n", "\n", "", "def", "input_fn", "(", "params", ")", ":", "\n", "    ", "\"\"\"The actual input function.\"\"\"", "\n", "batch_size", "=", "params", "[", "\"batch_size\"", "]", "\n", "\n", "num_examples", "=", "len", "(", "features", ")", "\n", "\n", "# This is for demo purposes and does NOT scale to large data sets. We do", "\n", "# not use Dataset.from_generator() because that uses tf.py_func which is", "\n", "# not TPU compatible. The right way to load data is with TFRecordReader.", "\n", "d", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "{", "\n", "\"input_ids\"", ":", "\n", "tf", ".", "constant", "(", "\n", "all_input_ids", ",", "shape", "=", "[", "num_examples", ",", "seq_length", "]", ",", "\n", "dtype", "=", "tf", ".", "int32", ")", ",", "\n", "\"input_mask\"", ":", "\n", "tf", ".", "constant", "(", "\n", "all_input_mask", ",", "\n", "shape", "=", "[", "num_examples", ",", "seq_length", "]", ",", "\n", "dtype", "=", "tf", ".", "int32", ")", ",", "\n", "\"segment_ids\"", ":", "\n", "tf", ".", "constant", "(", "\n", "all_segment_ids", ",", "\n", "shape", "=", "[", "num_examples", ",", "seq_length", "]", ",", "\n", "dtype", "=", "tf", ".", "int32", ")", ",", "\n", "\"label_ids\"", ":", "\n", "tf", ".", "constant", "(", "all_label_ids", ",", "shape", "=", "[", "num_examples", "]", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "\n", "}", ")", "\n", "\n", "if", "is_training", ":", "\n", "      ", "d", "=", "d", ".", "repeat", "(", ")", "\n", "d", "=", "d", ".", "shuffle", "(", "buffer_size", "=", "100", ")", "\n", "\n", "", "d", "=", "d", ".", "batch", "(", "batch_size", "=", "batch_size", ",", "drop_remainder", "=", "drop_remainder", ")", "\n", "return", "d", "\n", "\n", "", "return", "input_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.convert_examples_to_features": [[971, 985], ["enumerate", "run_classifier.convert_single_example", "features.append", "tensorflow.logging.info", "len"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.convert_single_example"], ["", "def", "convert_examples_to_features", "(", "examples", ",", "label_list", ",", "max_seq_length", ",", "\n", "tokenizer", ")", ":", "\n", "  ", "\"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"", "\n", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "    ", "if", "ex_index", "%", "10000", "==", "0", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"Writing example %d of %d\"", "%", "(", "ex_index", ",", "len", "(", "examples", ")", ")", ")", "\n", "\n", "", "feature", "=", "convert_single_example", "(", "ex_index", ",", "example", ",", "label_list", ",", "\n", "max_seq_length", ",", "tokenizer", ")", "\n", "\n", "features", ".", "append", "(", "feature", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.main": [[987, 1192], ["tensorflow.logging.set_verbosity", "tokenization.validate_case_matches_checkpoint", "modeling.BertConfig.from_json_file", "tensorflow.gfile.MakeDirs", "FLAGS.task_name.lower", "processor.get_labels", "tokenization.FullTokenizer", "tensorflow.contrib.tpu.RunConfig", "run_classifier.model_fn_builder", "tensorflow.contrib.tpu.TPUEstimator", "ValueError", "ValueError", "ValueError", "tensorflow.contrib.cluster_resolver.TPUClusterResolver", "processor.get_train_examples", "int", "int", "os.path.join", "run_classifier.file_based_convert_examples_to_features", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "run_classifier.file_based_input_fn_builder", "tf.contrib.tpu.TPUEstimator.train", "processor.get_dev_examples", "len", "os.path.join", "run_classifier.file_based_convert_examples_to_features", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "run_classifier.file_based_input_fn_builder", "tf.contrib.tpu.TPUEstimator.evaluate", "os.path.join", "processor.get_test_examples", "len", "os.path.join", "run_classifier.file_based_convert_examples_to_features", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "run_classifier.file_based_input_fn_builder", "tf.contrib.tpu.TPUEstimator.predict", "os.path.join", "tensorflow.contrib.tpu.TPUConfig", "len", "len", "len", "int", "tensorflow.logging.info", "os.path.join", "tensorflow.gfile.GFile", "tensorflow.logging.info", "sorted", "len", "tensorflow.gfile.GFile", "tensorflow.logging.info", "enumerate", "processor.get_dev_examples.append", "len", "estimator.predict.keys", "tensorflow.logging.info", "writer.write", "processor.get_test_examples.append", "len", "writer.write", "len", "len", "run_classifier.PaddingInputExample", "len", "len", "str", "len", "run_classifier.PaddingInputExample", "str", "str"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.validate_case_matches_checkpoint", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertConfig.from_json_file", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor.get_labels", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.model_fn_builder", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor.get_train_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.file_based_convert_examples_to_features", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.file_based_input_fn_builder", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor.get_dev_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.file_based_convert_examples_to_features", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.file_based_input_fn_builder", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.ColaProcessor.get_test_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.file_based_convert_examples_to_features", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.file_based_input_fn_builder"], ["", "def", "main", "(", "_", ")", ":", "\n", "  ", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "\n", "processors", "=", "{", "\n", "\"cola\"", ":", "ColaProcessor", ",", "\n", "\"mnli\"", ":", "MnliProcessor", ",", "\n", "\"mrpc\"", ":", "MrpcProcessor", ",", "\n", "\"xnli\"", ":", "XnliProcessor", ",", "\n", "\"sst-2\"", ":", "Sst2Processor", ",", "\n", "\"sts-b\"", ":", "StsbProcessor", ",", "\n", "\"qqp\"", ":", "QqpProcessor", ",", "\n", "\"qnli\"", ":", "QnliProcessor", ",", "\n", "\"rte\"", ":", "RteProcessor", ",", "\n", "\"wnli\"", ":", "WnliProcessor", ",", "\n", "}", "\n", "\n", "tokenization", ".", "validate_case_matches_checkpoint", "(", "FLAGS", ".", "do_lower_case", ",", "\n", "FLAGS", ".", "init_checkpoint", ")", "\n", "\n", "if", "not", "FLAGS", ".", "do_train", "and", "not", "FLAGS", ".", "do_eval", "and", "not", "FLAGS", ".", "do_predict", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"At least one of `do_train`, `do_eval` or `do_predict' must be True.\"", ")", "\n", "\n", "", "bert_config", "=", "modeling", ".", "BertConfig", ".", "from_json_file", "(", "FLAGS", ".", "bert_config_file", ")", "\n", "\n", "if", "FLAGS", ".", "max_seq_length", ">", "bert_config", ".", "max_position_embeddings", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"Cannot use sequence length %d because the BERT model \"", "\n", "\"was only trained up to sequence length %d\"", "%", "\n", "(", "FLAGS", ".", "max_seq_length", ",", "bert_config", ".", "max_position_embeddings", ")", ")", "\n", "\n", "", "tf", ".", "gfile", ".", "MakeDirs", "(", "FLAGS", ".", "output_dir", ")", "\n", "\n", "task_name", "=", "FLAGS", ".", "task_name", ".", "lower", "(", ")", "\n", "\n", "if", "task_name", "not", "in", "processors", ":", "\n", "    ", "raise", "ValueError", "(", "\"Task not found: %s\"", "%", "(", "task_name", ")", ")", "\n", "\n", "", "processor", "=", "processors", "[", "task_name", "]", "(", ")", "\n", "\n", "label_list", "=", "processor", ".", "get_labels", "(", ")", "\n", "\n", "tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "FLAGS", ".", "vocab_file", ",", "do_lower_case", "=", "FLAGS", ".", "do_lower_case", ")", "\n", "\n", "tpu_cluster_resolver", "=", "None", "\n", "if", "FLAGS", ".", "use_tpu", "and", "FLAGS", ".", "tpu_name", ":", "\n", "    ", "tpu_cluster_resolver", "=", "tf", ".", "contrib", ".", "cluster_resolver", ".", "TPUClusterResolver", "(", "\n", "FLAGS", ".", "tpu_name", ",", "zone", "=", "FLAGS", ".", "tpu_zone", ",", "project", "=", "FLAGS", ".", "gcp_project", ")", "\n", "\n", "", "is_per_host", "=", "tf", ".", "contrib", ".", "tpu", ".", "InputPipelineConfig", ".", "PER_HOST_V2", "\n", "run_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "RunConfig", "(", "\n", "cluster", "=", "tpu_cluster_resolver", ",", "\n", "master", "=", "FLAGS", ".", "master", ",", "\n", "model_dir", "=", "FLAGS", ".", "output_dir", ",", "\n", "save_checkpoints_steps", "=", "FLAGS", ".", "save_checkpoints_steps", ",", "\n", "keep_checkpoint_max", "=", "FLAGS", ".", "keep_checkpoint_max", ",", "\n", "tpu_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUConfig", "(", "\n", "iterations_per_loop", "=", "FLAGS", ".", "iterations_per_loop", ",", "\n", "num_shards", "=", "FLAGS", ".", "num_tpu_cores", ",", "\n", "per_host_input_for_training", "=", "is_per_host", ")", ")", "\n", "\n", "train_examples", "=", "None", "\n", "num_train_steps", "=", "None", "\n", "num_warmup_steps", "=", "None", "\n", "if", "FLAGS", ".", "do_train", ":", "\n", "    ", "train_examples", "=", "processor", ".", "get_train_examples", "(", "FLAGS", ".", "data_dir", ")", "\n", "num_train_steps", "=", "int", "(", "\n", "len", "(", "train_examples", ")", "/", "FLAGS", ".", "train_batch_size", "*", "FLAGS", ".", "num_train_epochs", ")", "\n", "num_warmup_steps", "=", "int", "(", "num_train_steps", "*", "FLAGS", ".", "warmup_proportion", ")", "\n", "\n", "", "model_fn", "=", "model_fn_builder", "(", "\n", "bert_config", "=", "bert_config", ",", "\n", "num_labels", "=", "len", "(", "label_list", ")", ",", "\n", "init_checkpoint", "=", "FLAGS", ".", "init_checkpoint", ",", "\n", "learning_rate", "=", "FLAGS", ".", "learning_rate", ",", "\n", "num_train_steps", "=", "num_train_steps", ",", "\n", "num_warmup_steps", "=", "num_warmup_steps", ",", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "use_one_hot_embeddings", "=", "FLAGS", ".", "use_tpu", ",", "\n", "prune_config_flag", "=", "FLAGS", ".", "pruning_hparams", ",", "\n", ")", "\n", "\n", "# If TPU is not available, this will fall back to normal Estimator on CPU", "\n", "# or GPU.", "\n", "estimator", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimator", "(", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "model_fn", "=", "model_fn", ",", "\n", "config", "=", "run_config", ",", "\n", "train_batch_size", "=", "FLAGS", ".", "train_batch_size", ",", "\n", "eval_batch_size", "=", "FLAGS", ".", "eval_batch_size", ",", "\n", "predict_batch_size", "=", "FLAGS", ".", "predict_batch_size", ")", "\n", "\n", "if", "FLAGS", ".", "do_train", ":", "\n", "    ", "train_file", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"train.tf_record\"", ")", "\n", "file_based_convert_examples_to_features", "(", "\n", "train_examples", ",", "label_list", ",", "FLAGS", ".", "max_seq_length", ",", "tokenizer", ",", "train_file", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_examples", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Batch size = %d\"", ",", "FLAGS", ".", "train_batch_size", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_steps", ")", "\n", "train_input_fn", "=", "file_based_input_fn_builder", "(", "\n", "input_file", "=", "train_file", ",", "\n", "seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "is_training", "=", "True", ",", "\n", "drop_remainder", "=", "True", ")", "\n", "estimator", ".", "train", "(", "input_fn", "=", "train_input_fn", ",", "max_steps", "=", "num_train_steps", ")", "\n", "\n", "", "if", "FLAGS", ".", "do_eval", ":", "\n", "    ", "eval_examples", "=", "processor", ".", "get_dev_examples", "(", "FLAGS", ".", "data_dir", ")", "\n", "num_actual_eval_examples", "=", "len", "(", "eval_examples", ")", "\n", "if", "FLAGS", ".", "use_tpu", ":", "\n", "# TPU requires a fixed batch size for all batches, therefore the number", "\n", "# of examples must be a multiple of the batch size, or else examples", "\n", "# will get dropped. So we pad with fake examples which are ignored", "\n", "# later on. These do NOT count towards the metric (all tf.metrics", "\n", "# support a per-instance weight, and these get a weight of 0.0).", "\n", "      ", "while", "len", "(", "eval_examples", ")", "%", "FLAGS", ".", "eval_batch_size", "!=", "0", ":", "\n", "        ", "eval_examples", ".", "append", "(", "PaddingInputExample", "(", ")", ")", "\n", "\n", "", "", "eval_file", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"eval.tf_record\"", ")", "\n", "file_based_convert_examples_to_features", "(", "\n", "eval_examples", ",", "label_list", ",", "FLAGS", ".", "max_seq_length", ",", "tokenizer", ",", "eval_file", ")", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** Running evaluation *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num examples = %d (%d actual, %d padding)\"", ",", "\n", "len", "(", "eval_examples", ")", ",", "num_actual_eval_examples", ",", "\n", "len", "(", "eval_examples", ")", "-", "num_actual_eval_examples", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Batch size = %d\"", ",", "FLAGS", ".", "eval_batch_size", ")", "\n", "\n", "# This tells the estimator to run through the entire set.", "\n", "eval_steps", "=", "None", "\n", "# However, if running eval on the TPU, you will need to specify the", "\n", "# number of steps.", "\n", "if", "FLAGS", ".", "use_tpu", ":", "\n", "      ", "assert", "len", "(", "eval_examples", ")", "%", "FLAGS", ".", "eval_batch_size", "==", "0", "\n", "eval_steps", "=", "int", "(", "len", "(", "eval_examples", ")", "//", "FLAGS", ".", "eval_batch_size", ")", "\n", "\n", "# This is a little hacky, but sometimes we want to evaluate on the training set", "\n", "# instead of the dev set. So ignore the file we made above and just use the training file.", "\n", "", "if", "FLAGS", ".", "eval_train_data", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"Oh nevermind, we're running eval on the training data instead.\"", ")", "\n", "eval_file", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"train.tf_record\"", ")", "\n", "\n", "", "eval_drop_remainder", "=", "True", "if", "FLAGS", ".", "use_tpu", "else", "False", "\n", "eval_input_fn", "=", "file_based_input_fn_builder", "(", "\n", "input_file", "=", "eval_file", ",", "\n", "seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "is_training", "=", "False", ",", "\n", "drop_remainder", "=", "eval_drop_remainder", ")", "\n", "\n", "result", "=", "estimator", ".", "evaluate", "(", "input_fn", "=", "eval_input_fn", ",", "steps", "=", "eval_steps", ")", "\n", "\n", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"eval_train_results.txt\"", "if", "FLAGS", ".", "eval_train_data", "else", "\"eval_results.txt\"", ")", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"***** Eval results *****\"", ")", "\n", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "\n", "", "", "", "if", "FLAGS", ".", "do_predict", ":", "\n", "    ", "predict_examples", "=", "processor", ".", "get_test_examples", "(", "FLAGS", ".", "data_dir", ")", "\n", "num_actual_predict_examples", "=", "len", "(", "predict_examples", ")", "\n", "if", "FLAGS", ".", "use_tpu", ":", "\n", "# TPU requires a fixed batch size for all batches, therefore the number", "\n", "# of examples must be a multiple of the batch size, or else examples", "\n", "# will get dropped. So we pad with fake examples which are ignored", "\n", "# later on.", "\n", "      ", "while", "len", "(", "predict_examples", ")", "%", "FLAGS", ".", "predict_batch_size", "!=", "0", ":", "\n", "        ", "predict_examples", ".", "append", "(", "PaddingInputExample", "(", ")", ")", "\n", "\n", "", "", "predict_file", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"predict.tf_record\"", ")", "\n", "file_based_convert_examples_to_features", "(", "predict_examples", ",", "label_list", ",", "\n", "FLAGS", ".", "max_seq_length", ",", "tokenizer", ",", "\n", "predict_file", ")", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** Running prediction*****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num examples = %d (%d actual, %d padding)\"", ",", "\n", "len", "(", "predict_examples", ")", ",", "num_actual_predict_examples", ",", "\n", "len", "(", "predict_examples", ")", "-", "num_actual_predict_examples", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Batch size = %d\"", ",", "FLAGS", ".", "predict_batch_size", ")", "\n", "\n", "predict_drop_remainder", "=", "True", "if", "FLAGS", ".", "use_tpu", "else", "False", "\n", "predict_input_fn", "=", "file_based_input_fn_builder", "(", "\n", "input_file", "=", "predict_file", ",", "\n", "seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "is_training", "=", "False", ",", "\n", "drop_remainder", "=", "predict_drop_remainder", ")", "\n", "\n", "result", "=", "estimator", ".", "predict", "(", "input_fn", "=", "predict_input_fn", ")", "\n", "\n", "output_predict_file", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"test_results.tsv\"", ")", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "output_predict_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "      ", "num_written_lines", "=", "0", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** Predict results *****\"", ")", "\n", "for", "(", "i", ",", "prediction", ")", "in", "enumerate", "(", "result", ")", ":", "\n", "        ", "probabilities", "=", "prediction", "[", "\"probabilities\"", "]", "\n", "if", "i", ">=", "num_actual_predict_examples", ":", "\n", "          ", "break", "\n", "", "output_line", "=", "\"\\t\"", ".", "join", "(", "\n", "str", "(", "class_probability", ")", "\n", "for", "class_probability", "in", "probabilities", ")", "+", "\"\\n\"", "\n", "writer", ".", "write", "(", "output_line", ")", "\n", "num_written_lines", "+=", "1", "\n", "", "", "assert", "num_written_lines", "==", "num_actual_predict_examples", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization_test.TokenizationTest.test_full_tokenizer": [[28, 50], ["tokenization.FullTokenizer", "os.unlink", "tokenization.FullTokenizer.tokenize", "tokenization_test.TokenizationTest.assertAllEqual", "tokenization_test.TokenizationTest.assertAllEqual", "tempfile.NamedTemporaryFile", "tokenization.FullTokenizer.convert_tokens_to_ids", "vocab_writer.write", "vocab_writer.write"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_tokens_to_ids"], ["  ", "def", "test_full_tokenizer", "(", "self", ")", ":", "\n", "    ", "vocab_tokens", "=", "[", "\n", "\"[UNK]\"", ",", "\"[CLS]\"", ",", "\"[SEP]\"", ",", "\"want\"", ",", "\"##want\"", ",", "\"##ed\"", ",", "\"wa\"", ",", "\"un\"", ",", "\"runn\"", ",", "\n", "\"##ing\"", ",", "\",\"", "\n", "]", "\n", "with", "tempfile", ".", "NamedTemporaryFile", "(", "delete", "=", "False", ")", "as", "vocab_writer", ":", "\n", "      ", "if", "six", ".", "PY2", ":", "\n", "        ", "vocab_writer", ".", "write", "(", "\"\"", ".", "join", "(", "[", "x", "+", "\"\\n\"", "for", "x", "in", "vocab_tokens", "]", ")", ")", "\n", "", "else", ":", "\n", "        ", "vocab_writer", ".", "write", "(", "\"\"", ".", "join", "(", "\n", "[", "x", "+", "\"\\n\"", "for", "x", "in", "vocab_tokens", "]", ")", ".", "encode", "(", "\"utf-8\"", ")", ")", "\n", "\n", "", "vocab_file", "=", "vocab_writer", ".", "name", "\n", "\n", "", "tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "vocab_file", ")", "\n", "os", ".", "unlink", "(", "vocab_file", ")", "\n", "\n", "tokens", "=", "tokenizer", ".", "tokenize", "(", "u\"UNwant\\u00E9d,running\"", ")", "\n", "self", ".", "assertAllEqual", "(", "tokens", ",", "[", "\"un\"", ",", "\"##want\"", ",", "\"##ed\"", ",", "\",\"", ",", "\"runn\"", ",", "\"##ing\"", "]", ")", "\n", "\n", "self", ".", "assertAllEqual", "(", "\n", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", ",", "[", "7", ",", "4", ",", "5", ",", "10", ",", "8", ",", "9", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization_test.TokenizationTest.test_chinese": [[51, 57], ["tokenization.BasicTokenizer", "tokenization_test.TokenizationTest.assertAllEqual", "tokenization.BasicTokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "test_chinese", "(", "self", ")", ":", "\n", "    ", "tokenizer", "=", "tokenization", ".", "BasicTokenizer", "(", ")", "\n", "\n", "self", ".", "assertAllEqual", "(", "\n", "tokenizer", ".", "tokenize", "(", "u\"ah\\u535A\\u63A8zz\"", ")", ",", "\n", "[", "u\"ah\"", ",", "u\"\\u535A\"", ",", "u\"\\u63A8\"", ",", "u\"zz\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization_test.TokenizationTest.test_basic_tokenizer_lower": [[58, 65], ["tokenization.BasicTokenizer", "tokenization_test.TokenizationTest.assertAllEqual", "tokenization_test.TokenizationTest.assertAllEqual", "tokenization.BasicTokenizer.tokenize", "tokenization.BasicTokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "test_basic_tokenizer_lower", "(", "self", ")", ":", "\n", "    ", "tokenizer", "=", "tokenization", ".", "BasicTokenizer", "(", "do_lower_case", "=", "True", ")", "\n", "\n", "self", ".", "assertAllEqual", "(", "\n", "tokenizer", ".", "tokenize", "(", "u\" \\tHeLLo!how  \\n Are yoU?  \"", ")", ",", "\n", "[", "\"hello\"", ",", "\"!\"", ",", "\"how\"", ",", "\"are\"", ",", "\"you\"", ",", "\"?\"", "]", ")", "\n", "self", ".", "assertAllEqual", "(", "tokenizer", ".", "tokenize", "(", "u\"H\\u00E9llo\"", ")", ",", "[", "\"hello\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization_test.TokenizationTest.test_basic_tokenizer_no_lower": [[66, 72], ["tokenization.BasicTokenizer", "tokenization_test.TokenizationTest.assertAllEqual", "tokenization.BasicTokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "test_basic_tokenizer_no_lower", "(", "self", ")", ":", "\n", "    ", "tokenizer", "=", "tokenization", ".", "BasicTokenizer", "(", "do_lower_case", "=", "False", ")", "\n", "\n", "self", ".", "assertAllEqual", "(", "\n", "tokenizer", ".", "tokenize", "(", "u\" \\tHeLLo!how  \\n Are yoU?  \"", ")", ",", "\n", "[", "\"HeLLo\"", ",", "\"!\"", ",", "\"how\"", ",", "\"Are\"", ",", "\"yoU\"", ",", "\"?\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization_test.TokenizationTest.test_wordpiece_tokenizer": [[73, 92], ["enumerate", "tokenization.WordpieceTokenizer", "tokenization_test.TokenizationTest.assertAllEqual", "tokenization_test.TokenizationTest.assertAllEqual", "tokenization_test.TokenizationTest.assertAllEqual", "tokenization.WordpieceTokenizer.tokenize", "tokenization.WordpieceTokenizer.tokenize", "tokenization.WordpieceTokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "test_wordpiece_tokenizer", "(", "self", ")", ":", "\n", "    ", "vocab_tokens", "=", "[", "\n", "\"[UNK]\"", ",", "\"[CLS]\"", ",", "\"[SEP]\"", ",", "\"want\"", ",", "\"##want\"", ",", "\"##ed\"", ",", "\"wa\"", ",", "\"un\"", ",", "\"runn\"", ",", "\n", "\"##ing\"", "\n", "]", "\n", "\n", "vocab", "=", "{", "}", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "vocab_tokens", ")", ":", "\n", "      ", "vocab", "[", "token", "]", "=", "i", "\n", "", "tokenizer", "=", "tokenization", ".", "WordpieceTokenizer", "(", "vocab", "=", "vocab", ")", "\n", "\n", "self", ".", "assertAllEqual", "(", "tokenizer", ".", "tokenize", "(", "\"\"", ")", ",", "[", "]", ")", "\n", "\n", "self", ".", "assertAllEqual", "(", "\n", "tokenizer", ".", "tokenize", "(", "\"unwanted running\"", ")", ",", "\n", "[", "\"un\"", ",", "\"##want\"", ",", "\"##ed\"", ",", "\"runn\"", ",", "\"##ing\"", "]", ")", "\n", "\n", "self", ".", "assertAllEqual", "(", "\n", "tokenizer", ".", "tokenize", "(", "\"unwantedX running\"", ")", ",", "[", "\"[UNK]\"", ",", "\"runn\"", ",", "\"##ing\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization_test.TokenizationTest.test_convert_tokens_to_ids": [[93, 106], ["enumerate", "tokenization_test.TokenizationTest.assertAllEqual", "tokenization.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_tokens_to_ids"], ["", "def", "test_convert_tokens_to_ids", "(", "self", ")", ":", "\n", "    ", "vocab_tokens", "=", "[", "\n", "\"[UNK]\"", ",", "\"[CLS]\"", ",", "\"[SEP]\"", ",", "\"want\"", ",", "\"##want\"", ",", "\"##ed\"", ",", "\"wa\"", ",", "\"un\"", ",", "\"runn\"", ",", "\n", "\"##ing\"", "\n", "]", "\n", "\n", "vocab", "=", "{", "}", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "vocab_tokens", ")", ":", "\n", "      ", "vocab", "[", "token", "]", "=", "i", "\n", "\n", "", "self", ".", "assertAllEqual", "(", "\n", "tokenization", ".", "convert_tokens_to_ids", "(", "\n", "vocab", ",", "[", "\"un\"", ",", "\"##want\"", ",", "\"##ed\"", ",", "\"runn\"", ",", "\"##ing\"", "]", ")", ",", "[", "7", ",", "4", ",", "5", ",", "8", ",", "9", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization_test.TokenizationTest.test_is_whitespace": [[107, 116], ["tokenization_test.TokenizationTest.assertTrue", "tokenization_test.TokenizationTest.assertTrue", "tokenization_test.TokenizationTest.assertTrue", "tokenization_test.TokenizationTest.assertTrue", "tokenization_test.TokenizationTest.assertTrue", "tokenization_test.TokenizationTest.assertFalse", "tokenization_test.TokenizationTest.assertFalse", "tokenization._is_whitespace", "tokenization._is_whitespace", "tokenization._is_whitespace", "tokenization._is_whitespace", "tokenization._is_whitespace", "tokenization._is_whitespace", "tokenization._is_whitespace"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_whitespace", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_whitespace", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_whitespace", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_whitespace", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_whitespace", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_whitespace", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_whitespace"], ["", "def", "test_is_whitespace", "(", "self", ")", ":", "\n", "    ", "self", ".", "assertTrue", "(", "tokenization", ".", "_is_whitespace", "(", "u\" \"", ")", ")", "\n", "self", ".", "assertTrue", "(", "tokenization", ".", "_is_whitespace", "(", "u\"\\t\"", ")", ")", "\n", "self", ".", "assertTrue", "(", "tokenization", ".", "_is_whitespace", "(", "u\"\\r\"", ")", ")", "\n", "self", ".", "assertTrue", "(", "tokenization", ".", "_is_whitespace", "(", "u\"\\n\"", ")", ")", "\n", "self", ".", "assertTrue", "(", "tokenization", ".", "_is_whitespace", "(", "u\"\\u00A0\"", ")", ")", "\n", "\n", "self", ".", "assertFalse", "(", "tokenization", ".", "_is_whitespace", "(", "u\"A\"", ")", ")", "\n", "self", ".", "assertFalse", "(", "tokenization", ".", "_is_whitespace", "(", "u\"-\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization_test.TokenizationTest.test_is_control": [[117, 125], ["tokenization_test.TokenizationTest.assertTrue", "tokenization_test.TokenizationTest.assertFalse", "tokenization_test.TokenizationTest.assertFalse", "tokenization_test.TokenizationTest.assertFalse", "tokenization_test.TokenizationTest.assertFalse", "tokenization_test.TokenizationTest.assertFalse", "tokenization._is_control", "tokenization._is_control", "tokenization._is_control", "tokenization._is_control", "tokenization._is_control", "tokenization._is_control"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_control", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_control", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_control", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_control", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_control", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_control"], ["", "def", "test_is_control", "(", "self", ")", ":", "\n", "    ", "self", ".", "assertTrue", "(", "tokenization", ".", "_is_control", "(", "u\"\\u0005\"", ")", ")", "\n", "\n", "self", ".", "assertFalse", "(", "tokenization", ".", "_is_control", "(", "u\"A\"", ")", ")", "\n", "self", ".", "assertFalse", "(", "tokenization", ".", "_is_control", "(", "u\" \"", ")", ")", "\n", "self", ".", "assertFalse", "(", "tokenization", ".", "_is_control", "(", "u\"\\t\"", ")", ")", "\n", "self", ".", "assertFalse", "(", "tokenization", ".", "_is_control", "(", "u\"\\r\"", ")", ")", "\n", "self", ".", "assertFalse", "(", "tokenization", ".", "_is_control", "(", "u\"\\U0001F4A9\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization_test.TokenizationTest.test_is_punctuation": [[126, 134], ["tokenization_test.TokenizationTest.assertTrue", "tokenization_test.TokenizationTest.assertTrue", "tokenization_test.TokenizationTest.assertTrue", "tokenization_test.TokenizationTest.assertTrue", "tokenization_test.TokenizationTest.assertFalse", "tokenization_test.TokenizationTest.assertFalse", "tokenization._is_punctuation", "tokenization._is_punctuation", "tokenization._is_punctuation", "tokenization._is_punctuation", "tokenization._is_punctuation", "tokenization._is_punctuation"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_punctuation", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_punctuation", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_punctuation", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_punctuation", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_punctuation", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_punctuation"], ["", "def", "test_is_punctuation", "(", "self", ")", ":", "\n", "    ", "self", ".", "assertTrue", "(", "tokenization", ".", "_is_punctuation", "(", "u\"-\"", ")", ")", "\n", "self", ".", "assertTrue", "(", "tokenization", ".", "_is_punctuation", "(", "u\"$\"", ")", ")", "\n", "self", ".", "assertTrue", "(", "tokenization", ".", "_is_punctuation", "(", "u\"`\"", ")", ")", "\n", "self", ".", "assertTrue", "(", "tokenization", ".", "_is_punctuation", "(", "u\".\"", ")", ")", "\n", "\n", "self", ".", "assertFalse", "(", "tokenization", ".", "_is_punctuation", "(", "u\"A\"", ")", ")", "\n", "self", ".", "assertFalse", "(", "tokenization", ".", "_is_punctuation", "(", "u\" \"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.FullTokenizer.__init__": [[164, 169], ["tokenization.load_vocab", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer", "tokenization.FullTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ")", ":", "\n", "    ", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "inv_vocab", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "vocab", ".", "items", "(", ")", "}", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.FullTokenizer.tokenize": [[170, 177], ["tokenization.FullTokenizer.basic_tokenizer.tokenize", "tokenization.FullTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "      ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "        ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.FullTokenizer.convert_tokens_to_ids": [[178, 180], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_by_vocab"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.FullTokenizer.convert_ids_to_tokens": [[181, 183], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_by_vocab"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "self", ".", "inv_vocab", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.BasicTokenizer.__init__": [[188, 195], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ")", ":", "\n", "    ", "\"\"\"Constructs a BasicTokenizer.\n\n    Args:\n      do_lower_case: Whether to lower case the input.\n    \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.BasicTokenizer.tokenize": [[196, 219], ["tokenization.convert_to_unicode", "tokenization.BasicTokenizer._clean_text", "tokenization.BasicTokenizer._tokenize_chinese_chars", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.BasicTokenizer._tokenize_chinese_chars", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "      ", "if", "self", ".", "do_lower_case", ":", "\n", "        ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.BasicTokenizer._run_strip_accents": [[220, 230], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "        ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.BasicTokenizer._run_split_on_punc": [[231, 250], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "      ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "        ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "        ", "if", "start_new_word", ":", "\n", "          ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.BasicTokenizer._tokenize_chinese_chars": [[251, 263], ["ord", "tokenization.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "        ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "        ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.BasicTokenizer._is_chinese_char": [[264, 285], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "    ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "      ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.BasicTokenizer._clean_text": [[286, 298], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_whitespace", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "        ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "        ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "        ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.WordpieceTokenizer.__init__": [[303, 307], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "200", ")", ":", "\n", "    ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.WordpieceTokenizer.tokenize": [[308, 360], ["tokenization.convert_to_unicode", "tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n    This uses a greedy longest-match-first algorithm to perform tokenization\n    using the given vocabulary.\n\n    For example:\n      input = \"unaffable\"\n      output = [\"un\", \"##aff\", \"##able\"]\n\n    Args:\n      text: A single token or whitespace separated tokens. This should have\n        already been passed through `BasicTokenizer.\n\n    Returns:\n      A list of wordpiece tokens.\n    \"\"\"", "\n", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "      ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "        ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "        ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "          ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "            ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "            ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "          ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "        ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "        ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.validate_case_matches_checkpoint": [[28, 76], ["re.match", "re.match.group", "ValueError"], "function", ["None"], ["def", "validate_case_matches_checkpoint", "(", "do_lower_case", ",", "init_checkpoint", ")", ":", "\n", "  ", "\"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"", "\n", "\n", "# The casing has to be passed in by the user and there is no explicit check", "\n", "# as to whether it matches the checkpoint. The casing information probably", "\n", "# should have been stored in the bert_config.json file, but it's not, so", "\n", "# we have to heuristically detect it to validate.", "\n", "\n", "if", "not", "init_checkpoint", ":", "\n", "    ", "return", "\n", "\n", "", "m", "=", "re", ".", "match", "(", "\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\"", ",", "init_checkpoint", ")", "\n", "if", "m", "is", "None", ":", "\n", "    ", "return", "\n", "\n", "", "model_name", "=", "m", ".", "group", "(", "1", ")", "\n", "\n", "lower_models", "=", "[", "\n", "\"uncased_L-24_H-1024_A-16\"", ",", "\"uncased_L-12_H-768_A-12\"", ",", "\n", "\"multilingual_L-12_H-768_A-12\"", ",", "\"chinese_L-12_H-768_A-12\"", "\n", "]", "\n", "\n", "cased_models", "=", "[", "\n", "\"cased_L-12_H-768_A-12\"", ",", "\"cased_L-24_H-1024_A-16\"", ",", "\n", "\"multi_cased_L-12_H-768_A-12\"", "\n", "]", "\n", "\n", "is_bad_config", "=", "False", "\n", "if", "model_name", "in", "lower_models", "and", "not", "do_lower_case", ":", "\n", "    ", "is_bad_config", "=", "True", "\n", "actual_flag", "=", "\"False\"", "\n", "case_name", "=", "\"lowercased\"", "\n", "opposite_flag", "=", "\"True\"", "\n", "\n", "", "if", "model_name", "in", "cased_models", "and", "do_lower_case", ":", "\n", "    ", "is_bad_config", "=", "True", "\n", "actual_flag", "=", "\"True\"", "\n", "case_name", "=", "\"cased\"", "\n", "opposite_flag", "=", "\"False\"", "\n", "\n", "", "if", "is_bad_config", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"", "\n", "\"However, `%s` seems to be a %s model, so you \"", "\n", "\"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"", "\n", "\"how the model was pre-training. If this error is wrong, please \"", "\n", "\"just comment out this check.\"", "%", "(", "actual_flag", ",", "init_checkpoint", ",", "\n", "model_name", ",", "case_name", ",", "opposite_flag", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode": [[78, 96], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "text.decode", "isinstance", "ValueError", "type", "type"], "function", ["None"], ["", "", "def", "convert_to_unicode", "(", "text", ")", ":", "\n", "  ", "\"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"", "\n", "if", "six", ".", "PY3", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "      ", "return", "text", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.printable_text": [[98, 119], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "isinstance", "text.encode", "ValueError", "type", "type"], "function", ["None"], ["", "", "def", "printable_text", "(", "text", ")", ":", "\n", "  ", "\"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"", "\n", "\n", "# These functions want `str` for both Python2 and Python3, but in one case", "\n", "# it's a Unicode string and in the other it's a byte string.", "\n", "if", "six", ".", "PY3", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "      ", "return", "text", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.load_vocab": [[121, 134], ["collections.OrderedDict", "tensorflow.gfile.GFile", "tokenization.convert_to_unicode", "token.strip.strip", "reader.readline"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_to_unicode"], ["", "", "def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "  ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "vocab_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "    ", "while", "True", ":", "\n", "      ", "token", "=", "convert_to_unicode", "(", "reader", ".", "readline", "(", ")", ")", "\n", "if", "not", "token", ":", "\n", "        ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_by_vocab": [[136, 142], ["output.append"], "function", ["None"], ["", "def", "convert_by_vocab", "(", "vocab", ",", "items", ")", ":", "\n", "  ", "\"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "item", "in", "items", ":", "\n", "    ", "output", ".", "append", "(", "vocab", "[", "item", "]", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_tokens_to_ids": [[144, 146], ["tokenization.convert_by_vocab"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_by_vocab"], ["", "def", "convert_tokens_to_ids", "(", "vocab", ",", "tokens", ")", ":", "\n", "  ", "return", "convert_by_vocab", "(", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_ids_to_tokens": [[148, 150], ["tokenization.convert_by_vocab"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_by_vocab"], ["", "def", "convert_ids_to_tokens", "(", "inv_vocab", ",", "ids", ")", ":", "\n", "  ", "return", "convert_by_vocab", "(", "inv_vocab", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.whitespace_tokenize": [[152, 159], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "  ", "\"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "    ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_whitespace": [[362, 372], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "    ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_control": [[374, 384], ["unicodedata.category"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "    ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "in", "(", "\"Cc\"", ",", "\"Cf\"", ")", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization._is_punctuation": [[386, 400], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "    ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.optimization_test.OptimizationTest.test_adam": [[25, 45], ["optimization_test.OptimizationTest.test_session", "tensorflow.get_variable", "tensorflow.constant", "tensorflow.reduce_mean", "tensorflow.trainable_variables", "tensorflow.gradients", "tensorflow.train.get_or_create_global_step", "optimization.AdamWeightDecayOptimizer", "optimization.AdamWeightDecayOptimizer.apply_gradients", "tensorflow.group", "sess.run", "range", "sess.run", "optimization_test.OptimizationTest.assertAllClose", "tensorflow.square", "zip", "tensorflow.global_variables_initializer", "tensorflow.local_variables_initializer", "sess.run", "tensorflow.constant_initializer"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.memory_saving_gradients.gradients", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.optimization.AdamWeightDecayOptimizer.apply_gradients"], ["  ", "def", "test_adam", "(", "self", ")", ":", "\n", "    ", "with", "self", ".", "test_session", "(", ")", "as", "sess", ":", "\n", "      ", "w", "=", "tf", ".", "get_variable", "(", "\n", "\"w\"", ",", "\n", "shape", "=", "[", "3", "]", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "[", "0.1", ",", "-", "0.2", ",", "-", "0.1", "]", ")", ")", "\n", "x", "=", "tf", ".", "constant", "(", "[", "0.4", ",", "0.2", ",", "-", "0.5", "]", ")", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "x", "-", "w", ")", ")", "\n", "tvars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "grads", "=", "tf", ".", "gradients", "(", "loss", ",", "tvars", ")", "\n", "global_step", "=", "tf", ".", "train", ".", "get_or_create_global_step", "(", ")", "\n", "optimizer", "=", "optimization", ".", "AdamWeightDecayOptimizer", "(", "learning_rate", "=", "0.2", ")", "\n", "train_op", "=", "optimizer", ".", "apply_gradients", "(", "zip", "(", "grads", ",", "tvars", ")", ",", "global_step", ")", "\n", "init_op", "=", "tf", ".", "group", "(", "tf", ".", "global_variables_initializer", "(", ")", ",", "\n", "tf", ".", "local_variables_initializer", "(", ")", ")", "\n", "sess", ".", "run", "(", "init_op", ")", "\n", "for", "_", "in", "range", "(", "100", ")", ":", "\n", "        ", "sess", ".", "run", "(", "train_op", ")", "\n", "", "w_np", "=", "sess", ".", "run", "(", "w", ")", "\n", "self", ".", "assertAllClose", "(", "w_np", ".", "flat", ",", "[", "0.4", ",", "0.2", ",", "-", "0.5", "]", ",", "rtol", "=", "1e-2", ",", "atol", "=", "1e-2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_pretraining.model_fn_builder": [[118, 247], ["tensorflow.logging.info", "sorted", "modeling.BertModel", "run_pretraining.get_masked_lm_output", "run_pretraining.get_next_sentence_output", "tensorflow.trainable_variables", "tensorflow.logging.info", "features.keys", "tensorflow.logging.info", "modeling.BertModel.get_sequence_output", "modeling.BertModel.get_embedding_table", "modeling.BertModel.get_pooled_output", "modeling.get_assignment_map_from_checkpoint", "tensorflow.logging.info", "optimization.create_optimizer", "tensorflow.contrib.tpu.TPUEstimatorSpec", "tensorflow.train.init_from_checkpoint", "tensorflow.contrib.tpu.TPUEstimatorSpec", "ValueError", "tensorflow.train.init_from_checkpoint", "tensorflow.train.Scaffold", "tensorflow.reshape", "tensorflow.argmax", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.metrics.accuracy", "tensorflow.metrics.mean", "tensorflow.reshape", "tensorflow.argmax", "tensorflow.reshape", "tensorflow.metrics.accuracy", "tensorflow.metrics.mean"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_pretraining.get_masked_lm_output", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_pretraining.get_next_sentence_output", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertModel.get_sequence_output", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertModel.get_embedding_table", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertModel.get_pooled_output", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.get_assignment_map_from_checkpoint", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.optimization.create_optimizer"], ["def", "model_fn_builder", "(", "bert_config", ",", "init_checkpoint", ",", "learning_rate", ",", "\n", "num_train_steps", ",", "num_warmup_steps", ",", "use_tpu", ",", "\n", "use_one_hot_embeddings", ",", "prune_config_flag", ")", ":", "\n", "  ", "\"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"", "\n", "\n", "def", "model_fn", "(", "features", ",", "labels", ",", "mode", ",", "params", ")", ":", "# pylint: disable=unused-argument", "\n", "    ", "\"\"\"The `model_fn` for TPUEstimator.\"\"\"", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"*** Features ***\"", ")", "\n", "for", "name", "in", "sorted", "(", "features", ".", "keys", "(", ")", ")", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"  name = %s, shape = %s\"", "%", "(", "name", ",", "features", "[", "name", "]", ".", "shape", ")", ")", "\n", "\n", "", "input_ids", "=", "features", "[", "\"input_ids\"", "]", "\n", "input_mask", "=", "features", "[", "\"input_mask\"", "]", "\n", "segment_ids", "=", "features", "[", "\"segment_ids\"", "]", "\n", "masked_lm_positions", "=", "features", "[", "\"masked_lm_positions\"", "]", "\n", "masked_lm_ids", "=", "features", "[", "\"masked_lm_ids\"", "]", "\n", "masked_lm_weights", "=", "features", "[", "\"masked_lm_weights\"", "]", "\n", "next_sentence_labels", "=", "features", "[", "\"next_sentence_labels\"", "]", "\n", "\n", "is_training", "=", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ")", "\n", "\n", "model", "=", "modeling", ".", "BertModel", "(", "\n", "config", "=", "bert_config", ",", "\n", "is_training", "=", "is_training", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "token_type_ids", "=", "segment_ids", ",", "\n", "use_one_hot_embeddings", "=", "use_one_hot_embeddings", ")", "\n", "\n", "(", "masked_lm_loss", ",", "\n", "masked_lm_example_loss", ",", "masked_lm_log_probs", ")", "=", "get_masked_lm_output", "(", "\n", "bert_config", ",", "model", ".", "get_sequence_output", "(", ")", ",", "model", ".", "get_embedding_table", "(", ")", ",", "\n", "masked_lm_positions", ",", "masked_lm_ids", ",", "masked_lm_weights", ")", "\n", "\n", "(", "next_sentence_loss", ",", "next_sentence_example_loss", ",", "\n", "next_sentence_log_probs", ")", "=", "get_next_sentence_output", "(", "\n", "bert_config", ",", "model", ".", "get_pooled_output", "(", ")", ",", "next_sentence_labels", ")", "\n", "\n", "total_loss", "=", "masked_lm_loss", "+", "next_sentence_loss", "\n", "\n", "tvars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "\n", "initialized_variable_names", "=", "{", "}", "\n", "scaffold_fn", "=", "None", "\n", "if", "init_checkpoint", ":", "\n", "      ", "(", "assignment_map", ",", "initialized_variable_names", "\n", ")", "=", "modeling", ".", "get_assignment_map_from_checkpoint", "(", "tvars", ",", "init_checkpoint", ")", "\n", "if", "use_tpu", ":", "\n", "\n", "        ", "def", "tpu_scaffold", "(", ")", ":", "\n", "          ", "tf", ".", "train", ".", "init_from_checkpoint", "(", "init_checkpoint", ",", "assignment_map", ")", "\n", "return", "tf", ".", "train", ".", "Scaffold", "(", ")", "\n", "\n", "", "scaffold_fn", "=", "tpu_scaffold", "\n", "", "else", ":", "\n", "        ", "tf", ".", "train", ".", "init_from_checkpoint", "(", "init_checkpoint", ",", "assignment_map", ")", "\n", "\n", "", "", "tf", ".", "logging", ".", "info", "(", "\"**** Trainable Variables ****\"", ")", "\n", "for", "var", "in", "tvars", ":", "\n", "      ", "init_string", "=", "\"\"", "\n", "if", "var", ".", "name", "in", "initialized_variable_names", ":", "\n", "        ", "init_string", "=", "\", *INIT_FROM_CKPT*\"", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"  name = %s, shape = %s%s\"", ",", "var", ".", "name", ",", "var", ".", "shape", ",", "\n", "init_string", ")", "\n", "\n", "", "output_spec", "=", "None", "\n", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "      ", "train_op", "=", "optimization", ".", "create_optimizer", "(", "\n", "total_loss", ",", "learning_rate", ",", "num_train_steps", ",", "num_warmup_steps", ",", "use_tpu", ",", "prune_config_flag", ")", "\n", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "\n", "loss", "=", "total_loss", ",", "\n", "train_op", "=", "train_op", ",", "\n", "scaffold_fn", "=", "scaffold_fn", ")", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "\n", "      ", "def", "metric_fn", "(", "masked_lm_example_loss", ",", "masked_lm_log_probs", ",", "masked_lm_ids", ",", "\n", "masked_lm_weights", ",", "next_sentence_example_loss", ",", "\n", "next_sentence_log_probs", ",", "next_sentence_labels", ")", ":", "\n", "        ", "\"\"\"Computes the loss and accuracy of the model.\"\"\"", "\n", "masked_lm_log_probs", "=", "tf", ".", "reshape", "(", "masked_lm_log_probs", ",", "\n", "[", "-", "1", ",", "masked_lm_log_probs", ".", "shape", "[", "-", "1", "]", "]", ")", "\n", "masked_lm_predictions", "=", "tf", ".", "argmax", "(", "\n", "masked_lm_log_probs", ",", "axis", "=", "-", "1", ",", "output_type", "=", "tf", ".", "int32", ")", "\n", "masked_lm_example_loss", "=", "tf", ".", "reshape", "(", "masked_lm_example_loss", ",", "[", "-", "1", "]", ")", "\n", "masked_lm_ids", "=", "tf", ".", "reshape", "(", "masked_lm_ids", ",", "[", "-", "1", "]", ")", "\n", "masked_lm_weights", "=", "tf", ".", "reshape", "(", "masked_lm_weights", ",", "[", "-", "1", "]", ")", "\n", "masked_lm_accuracy", "=", "tf", ".", "metrics", ".", "accuracy", "(", "\n", "labels", "=", "masked_lm_ids", ",", "\n", "predictions", "=", "masked_lm_predictions", ",", "\n", "weights", "=", "masked_lm_weights", ")", "\n", "masked_lm_mean_loss", "=", "tf", ".", "metrics", ".", "mean", "(", "\n", "values", "=", "masked_lm_example_loss", ",", "weights", "=", "masked_lm_weights", ")", "\n", "\n", "next_sentence_log_probs", "=", "tf", ".", "reshape", "(", "\n", "next_sentence_log_probs", ",", "[", "-", "1", ",", "next_sentence_log_probs", ".", "shape", "[", "-", "1", "]", "]", ")", "\n", "next_sentence_predictions", "=", "tf", ".", "argmax", "(", "\n", "next_sentence_log_probs", ",", "axis", "=", "-", "1", ",", "output_type", "=", "tf", ".", "int32", ")", "\n", "next_sentence_labels", "=", "tf", ".", "reshape", "(", "next_sentence_labels", ",", "[", "-", "1", "]", ")", "\n", "next_sentence_accuracy", "=", "tf", ".", "metrics", ".", "accuracy", "(", "\n", "labels", "=", "next_sentence_labels", ",", "predictions", "=", "next_sentence_predictions", ")", "\n", "next_sentence_mean_loss", "=", "tf", ".", "metrics", ".", "mean", "(", "\n", "values", "=", "next_sentence_example_loss", ")", "\n", "\n", "return", "{", "\n", "\"masked_lm_accuracy\"", ":", "masked_lm_accuracy", ",", "\n", "\"masked_lm_loss\"", ":", "masked_lm_mean_loss", ",", "\n", "\"next_sentence_accuracy\"", ":", "next_sentence_accuracy", ",", "\n", "\"next_sentence_loss\"", ":", "next_sentence_mean_loss", ",", "\n", "}", "\n", "\n", "", "eval_metrics", "=", "(", "metric_fn", ",", "[", "\n", "masked_lm_example_loss", ",", "masked_lm_log_probs", ",", "masked_lm_ids", ",", "\n", "masked_lm_weights", ",", "next_sentence_example_loss", ",", "\n", "next_sentence_log_probs", ",", "next_sentence_labels", "\n", "]", ")", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "\n", "loss", "=", "total_loss", ",", "\n", "eval_metrics", "=", "eval_metrics", ",", "\n", "scaffold_fn", "=", "scaffold_fn", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Only TRAIN and EVAL modes are supported: %s\"", "%", "(", "mode", ")", ")", "\n", "\n", "", "return", "output_spec", "\n", "\n", "", "return", "model_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_pretraining.get_masked_lm_output": [[249, 292], ["run_pretraining.gather_indexes", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.matmul", "tensorflow.nn.bias_add", "tensorflow.nn.log_softmax", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.one_hot", "tensorflow.reduce_sum", "tensorflow.variable_scope", "tensorflow.layers.dense", "modeling.layer_norm", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.zeros_initializer", "modeling.get_activation", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_pretraining.gather_indexes", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.layer_norm", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.get_activation", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.create_initializer"], ["", "def", "get_masked_lm_output", "(", "bert_config", ",", "input_tensor", ",", "output_weights", ",", "positions", ",", "\n", "label_ids", ",", "label_weights", ")", ":", "\n", "  ", "\"\"\"Get loss and log probs for the masked LM.\"\"\"", "\n", "input_tensor", "=", "gather_indexes", "(", "input_tensor", ",", "positions", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"cls/predictions\"", ")", ":", "\n", "# We apply one more non-linear transformation before the output layer.", "\n", "# This matrix is not used after pre-training.", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "\"transform\"", ")", ":", "\n", "      ", "input_tensor", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "input_tensor", ",", "\n", "units", "=", "bert_config", ".", "hidden_size", ",", "\n", "activation", "=", "modeling", ".", "get_activation", "(", "bert_config", ".", "hidden_act", ")", ",", "\n", "kernel_initializer", "=", "modeling", ".", "create_initializer", "(", "\n", "bert_config", ".", "initializer_range", ")", ")", "\n", "input_tensor", "=", "modeling", ".", "layer_norm", "(", "input_tensor", ")", "\n", "\n", "# The output weights are the same as the input embeddings, but there is", "\n", "# an output-only bias for each token.", "\n", "", "output_bias", "=", "tf", ".", "get_variable", "(", "\n", "\"output_bias\"", ",", "\n", "shape", "=", "[", "bert_config", ".", "vocab_size", "]", ",", "\n", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "logits", "=", "tf", ".", "matmul", "(", "input_tensor", ",", "output_weights", ",", "transpose_b", "=", "True", ")", "\n", "logits", "=", "tf", ".", "nn", ".", "bias_add", "(", "logits", ",", "output_bias", ")", "\n", "log_probs", "=", "tf", ".", "nn", ".", "log_softmax", "(", "logits", ",", "axis", "=", "-", "1", ")", "\n", "\n", "label_ids", "=", "tf", ".", "reshape", "(", "label_ids", ",", "[", "-", "1", "]", ")", "\n", "label_weights", "=", "tf", ".", "reshape", "(", "label_weights", ",", "[", "-", "1", "]", ")", "\n", "\n", "one_hot_labels", "=", "tf", ".", "one_hot", "(", "\n", "label_ids", ",", "depth", "=", "bert_config", ".", "vocab_size", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "# The `positions` tensor might be zero-padded (if the sequence is too", "\n", "# short to have the maximum number of predictions). The `label_weights`", "\n", "# tensor has a value of 1.0 for every real prediction and 0.0 for the", "\n", "# padding predictions.", "\n", "per_example_loss", "=", "-", "tf", ".", "reduce_sum", "(", "log_probs", "*", "one_hot_labels", ",", "axis", "=", "[", "-", "1", "]", ")", "\n", "numerator", "=", "tf", ".", "reduce_sum", "(", "label_weights", "*", "per_example_loss", ")", "\n", "denominator", "=", "tf", ".", "reduce_sum", "(", "label_weights", ")", "+", "1e-5", "\n", "loss", "=", "numerator", "/", "denominator", "\n", "\n", "", "return", "(", "loss", ",", "per_example_loss", ",", "log_probs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_pretraining.get_next_sentence_output": [[294, 315], ["tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.matmul", "tensorflow.nn.bias_add", "tensorflow.nn.log_softmax", "tensorflow.reshape", "tensorflow.one_hot", "tensorflow.reduce_mean", "tensorflow.reduce_sum", "modeling.create_initializer", "tensorflow.zeros_initializer"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.create_initializer"], ["", "def", "get_next_sentence_output", "(", "bert_config", ",", "input_tensor", ",", "labels", ")", ":", "\n", "  ", "\"\"\"Get loss and log probs for the next sentence prediction.\"\"\"", "\n", "\n", "# Simple binary classification. Note that 0 is \"next sentence\" and 1 is", "\n", "# \"random sentence\". This weight matrix is not used after pre-training.", "\n", "with", "tf", ".", "variable_scope", "(", "\"cls/seq_relationship\"", ")", ":", "\n", "    ", "output_weights", "=", "tf", ".", "get_variable", "(", "\n", "\"output_weights\"", ",", "\n", "shape", "=", "[", "2", ",", "bert_config", ".", "hidden_size", "]", ",", "\n", "initializer", "=", "modeling", ".", "create_initializer", "(", "bert_config", ".", "initializer_range", ")", ")", "\n", "output_bias", "=", "tf", ".", "get_variable", "(", "\n", "\"output_bias\"", ",", "shape", "=", "[", "2", "]", ",", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "\n", "logits", "=", "tf", ".", "matmul", "(", "input_tensor", ",", "output_weights", ",", "transpose_b", "=", "True", ")", "\n", "logits", "=", "tf", ".", "nn", ".", "bias_add", "(", "logits", ",", "output_bias", ")", "\n", "log_probs", "=", "tf", ".", "nn", ".", "log_softmax", "(", "logits", ",", "axis", "=", "-", "1", ")", "\n", "labels", "=", "tf", ".", "reshape", "(", "labels", ",", "[", "-", "1", "]", ")", "\n", "one_hot_labels", "=", "tf", ".", "one_hot", "(", "labels", ",", "depth", "=", "2", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "per_example_loss", "=", "-", "tf", ".", "reduce_sum", "(", "one_hot_labels", "*", "log_probs", ",", "axis", "=", "-", "1", ")", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "per_example_loss", ")", "\n", "return", "(", "loss", ",", "per_example_loss", ",", "log_probs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_pretraining.gather_indexes": [[317, 331], ["modeling.get_shape_list", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.gather", "tensorflow.range"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.get_shape_list"], ["", "", "def", "gather_indexes", "(", "sequence_tensor", ",", "positions", ")", ":", "\n", "  ", "\"\"\"Gathers the vectors at the specific positions over a minibatch.\"\"\"", "\n", "sequence_shape", "=", "modeling", ".", "get_shape_list", "(", "sequence_tensor", ",", "expected_rank", "=", "3", ")", "\n", "batch_size", "=", "sequence_shape", "[", "0", "]", "\n", "seq_length", "=", "sequence_shape", "[", "1", "]", "\n", "width", "=", "sequence_shape", "[", "2", "]", "\n", "\n", "flat_offsets", "=", "tf", ".", "reshape", "(", "\n", "tf", ".", "range", "(", "0", ",", "batch_size", ",", "dtype", "=", "tf", ".", "int32", ")", "*", "seq_length", ",", "[", "-", "1", ",", "1", "]", ")", "\n", "flat_positions", "=", "tf", ".", "reshape", "(", "positions", "+", "flat_offsets", ",", "[", "-", "1", "]", ")", "\n", "flat_sequence_tensor", "=", "tf", ".", "reshape", "(", "sequence_tensor", ",", "\n", "[", "batch_size", "*", "seq_length", ",", "width", "]", ")", "\n", "output_tensor", "=", "tf", ".", "gather", "(", "flat_sequence_tensor", ",", "flat_positions", ")", "\n", "return", "output_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_pretraining.input_fn_builder": [[333, 398], ["d.repeat.apply", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.data.Dataset.from_tensor_slices", "d.repeat.repeat", "d.repeat.shuffle", "min", "d.repeat.apply", "d.repeat.shuffle", "tensorflow.data.TFRecordDataset", "d.repeat.repeat", "tensorflow.contrib.data.map_and_batch", "tensorflow.constant", "len", "tensorflow.contrib.data.parallel_interleave", "len", "run_pretraining._decode_record"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_pretraining._decode_record"], ["", "def", "input_fn_builder", "(", "input_files", ",", "\n", "max_seq_length", ",", "\n", "max_predictions_per_seq", ",", "\n", "is_training", ",", "\n", "num_cpu_threads", "=", "4", ")", ":", "\n", "  ", "\"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"", "\n", "\n", "def", "input_fn", "(", "params", ")", ":", "\n", "    ", "\"\"\"The actual input function.\"\"\"", "\n", "batch_size", "=", "params", "[", "\"batch_size\"", "]", "\n", "\n", "name_to_features", "=", "{", "\n", "\"input_ids\"", ":", "\n", "tf", ".", "FixedLenFeature", "(", "[", "max_seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"input_mask\"", ":", "\n", "tf", ".", "FixedLenFeature", "(", "[", "max_seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"segment_ids\"", ":", "\n", "tf", ".", "FixedLenFeature", "(", "[", "max_seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"masked_lm_positions\"", ":", "\n", "tf", ".", "FixedLenFeature", "(", "[", "max_predictions_per_seq", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"masked_lm_ids\"", ":", "\n", "tf", ".", "FixedLenFeature", "(", "[", "max_predictions_per_seq", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"masked_lm_weights\"", ":", "\n", "tf", ".", "FixedLenFeature", "(", "[", "max_predictions_per_seq", "]", ",", "tf", ".", "float32", ")", ",", "\n", "\"next_sentence_labels\"", ":", "\n", "tf", ".", "FixedLenFeature", "(", "[", "1", "]", ",", "tf", ".", "int64", ")", ",", "\n", "}", "\n", "\n", "# For training, we want a lot of parallel reading and shuffling.", "\n", "# For eval, we want no shuffling and parallel reading doesn't matter.", "\n", "if", "is_training", ":", "\n", "      ", "d", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "tf", ".", "constant", "(", "input_files", ")", ")", "\n", "d", "=", "d", ".", "repeat", "(", ")", "\n", "d", "=", "d", ".", "shuffle", "(", "buffer_size", "=", "len", "(", "input_files", ")", ")", "\n", "\n", "# `cycle_length` is the number of parallel files that get read.", "\n", "cycle_length", "=", "min", "(", "num_cpu_threads", ",", "len", "(", "input_files", ")", ")", "\n", "\n", "# `sloppy` mode means that the interleaving is not exact. This adds", "\n", "# even more randomness to the training pipeline.", "\n", "d", "=", "d", ".", "apply", "(", "\n", "tf", ".", "contrib", ".", "data", ".", "parallel_interleave", "(", "\n", "tf", ".", "data", ".", "TFRecordDataset", ",", "\n", "sloppy", "=", "is_training", ",", "\n", "cycle_length", "=", "cycle_length", ")", ")", "\n", "d", "=", "d", ".", "shuffle", "(", "buffer_size", "=", "100", ")", "\n", "", "else", ":", "\n", "      ", "d", "=", "tf", ".", "data", ".", "TFRecordDataset", "(", "input_files", ")", "\n", "# Since we evaluate for a fixed number of steps we don't want to encounter", "\n", "# out-of-range exceptions.", "\n", "d", "=", "d", ".", "repeat", "(", ")", "\n", "\n", "# We must `drop_remainder` on training because the TPU requires fixed", "\n", "# size dimensions. For eval, we assume we are evaluating on the CPU or GPU", "\n", "# and we *don't* want to drop the remainder, otherwise we wont cover", "\n", "# every sample.", "\n", "", "d", "=", "d", ".", "apply", "(", "\n", "tf", ".", "contrib", ".", "data", ".", "map_and_batch", "(", "\n", "lambda", "record", ":", "_decode_record", "(", "record", ",", "name_to_features", ")", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "num_parallel_batches", "=", "num_cpu_threads", ",", "\n", "drop_remainder", "=", "True", ")", ")", "\n", "return", "d", "\n", "\n", "", "return", "input_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_pretraining._decode_record": [[400, 413], ["tensorflow.parse_single_example", "list", "tf.parse_single_example.keys", "tensorflow.to_int32"], "function", ["None"], ["", "def", "_decode_record", "(", "record", ",", "name_to_features", ")", ":", "\n", "  ", "\"\"\"Decodes a record to a TensorFlow example.\"\"\"", "\n", "example", "=", "tf", ".", "parse_single_example", "(", "record", ",", "name_to_features", ")", "\n", "\n", "# tf.Example only supports tf.int64, but the TPU only supports tf.int32.", "\n", "# So cast all int64 to int32.", "\n", "for", "name", "in", "list", "(", "example", ".", "keys", "(", ")", ")", ":", "\n", "    ", "t", "=", "example", "[", "name", "]", "\n", "if", "t", ".", "dtype", "==", "tf", ".", "int64", ":", "\n", "      ", "t", "=", "tf", ".", "to_int32", "(", "t", ")", "\n", "", "example", "[", "name", "]", "=", "t", "\n", "\n", "", "return", "example", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_pretraining.main": [[415, 498], ["tensorflow.logging.set_verbosity", "modeling.BertConfig.from_json_file", "tensorflow.gfile.MakeDirs", "FLAGS.input_file.split", "tensorflow.logging.info", "tensorflow.contrib.tpu.RunConfig", "run_pretraining.model_fn_builder", "tensorflow.contrib.tpu.TPUEstimator", "ValueError", "input_files.extend", "tensorflow.logging.info", "tensorflow.contrib.cluster_resolver.TPUClusterResolver", "tensorflow.logging.info", "tensorflow.logging.info", "run_pretraining.input_fn_builder", "tf.contrib.tpu.TPUEstimator.train", "tensorflow.logging.info", "tensorflow.logging.info", "run_pretraining.input_fn_builder", "tf.contrib.tpu.TPUEstimator.evaluate", "os.path.join", "tensorflow.gfile.Glob", "tensorflow.contrib.tpu.TPUConfig", "tensorflow.gfile.GFile", "tensorflow.logging.info", "sorted", "estimator.evaluate.keys", "tensorflow.logging.info", "writer.write", "str", "str"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertConfig.from_json_file", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.model_fn_builder", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.input_fn_builder", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.input_fn_builder"], ["", "def", "main", "(", "_", ")", ":", "\n", "  ", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "\n", "if", "not", "FLAGS", ".", "do_train", "and", "not", "FLAGS", ".", "do_eval", ":", "\n", "    ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "bert_config", "=", "modeling", ".", "BertConfig", ".", "from_json_file", "(", "FLAGS", ".", "bert_config_file", ")", "\n", "\n", "tf", ".", "gfile", ".", "MakeDirs", "(", "FLAGS", ".", "output_dir", ")", "\n", "\n", "input_files", "=", "[", "]", "\n", "for", "input_pattern", "in", "FLAGS", ".", "input_file", ".", "split", "(", "\",\"", ")", ":", "\n", "    ", "input_files", ".", "extend", "(", "tf", ".", "gfile", ".", "Glob", "(", "input_pattern", ")", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"*** Input Files ***\"", ")", "\n", "for", "input_file", "in", "input_files", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"  %s\"", "%", "input_file", ")", "\n", "\n", "", "tpu_cluster_resolver", "=", "None", "\n", "if", "FLAGS", ".", "use_tpu", "and", "FLAGS", ".", "tpu_name", ":", "\n", "    ", "tpu_cluster_resolver", "=", "tf", ".", "contrib", ".", "cluster_resolver", ".", "TPUClusterResolver", "(", "\n", "FLAGS", ".", "tpu_name", ",", "zone", "=", "FLAGS", ".", "tpu_zone", ",", "project", "=", "FLAGS", ".", "gcp_project", ")", "\n", "\n", "", "is_per_host", "=", "tf", ".", "contrib", ".", "tpu", ".", "InputPipelineConfig", ".", "PER_HOST_V2", "\n", "run_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "RunConfig", "(", "\n", "cluster", "=", "tpu_cluster_resolver", ",", "\n", "master", "=", "FLAGS", ".", "master", ",", "\n", "model_dir", "=", "FLAGS", ".", "output_dir", ",", "\n", "save_checkpoints_steps", "=", "FLAGS", ".", "save_checkpoints_steps", ",", "\n", "keep_checkpoint_max", "=", "FLAGS", ".", "keep_checkpoint_max", ",", "\n", "tpu_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUConfig", "(", "\n", "iterations_per_loop", "=", "FLAGS", ".", "iterations_per_loop", ",", "\n", "num_shards", "=", "FLAGS", ".", "num_tpu_cores", ",", "\n", "per_host_input_for_training", "=", "is_per_host", ")", ")", "\n", "\n", "model_fn", "=", "model_fn_builder", "(", "\n", "bert_config", "=", "bert_config", ",", "\n", "init_checkpoint", "=", "FLAGS", ".", "init_checkpoint", ",", "\n", "learning_rate", "=", "FLAGS", ".", "learning_rate", ",", "\n", "num_train_steps", "=", "FLAGS", ".", "num_train_steps", ",", "\n", "num_warmup_steps", "=", "FLAGS", ".", "num_warmup_steps", ",", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "use_one_hot_embeddings", "=", "FLAGS", ".", "use_tpu", ",", "\n", "prune_config_flag", "=", "FLAGS", ".", "pruning_hparams", ")", "\n", "\n", "# If TPU is not available, this will fall back to normal Estimator on CPU", "\n", "# or GPU.", "\n", "estimator", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimator", "(", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "model_fn", "=", "model_fn", ",", "\n", "config", "=", "run_config", ",", "\n", "train_batch_size", "=", "FLAGS", ".", "train_batch_size", ",", "\n", "eval_batch_size", "=", "FLAGS", ".", "eval_batch_size", ")", "\n", "\n", "if", "FLAGS", ".", "do_train", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Batch size = %d\"", ",", "FLAGS", ".", "train_batch_size", ")", "\n", "train_input_fn", "=", "input_fn_builder", "(", "\n", "input_files", "=", "input_files", ",", "\n", "max_seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "max_predictions_per_seq", "=", "FLAGS", ".", "max_predictions_per_seq", ",", "\n", "is_training", "=", "True", ")", "\n", "estimator", ".", "train", "(", "input_fn", "=", "train_input_fn", ",", "max_steps", "=", "FLAGS", ".", "num_train_steps", ")", "\n", "\n", "", "if", "FLAGS", ".", "do_eval", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"***** Running evaluation *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Batch size = %d\"", ",", "FLAGS", ".", "eval_batch_size", ")", "\n", "\n", "eval_input_fn", "=", "input_fn_builder", "(", "\n", "input_files", "=", "input_files", ",", "\n", "max_seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "max_predictions_per_seq", "=", "FLAGS", ".", "max_predictions_per_seq", ",", "\n", "is_training", "=", "False", ")", "\n", "\n", "result", "=", "estimator", ".", "evaluate", "(", "\n", "input_fn", "=", "eval_input_fn", ",", "steps", "=", "FLAGS", ".", "max_eval_steps", ")", "\n", "\n", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"eval_results.txt\"", ")", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"***** Eval results *****\"", ")", "\n", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier_with_tfhub.create_model": [[37, 85], ["set", "tensorflow_hub.Module", "dict", "hub.Module.", "tensorflow.get_variable", "tensorflow.get_variable", "set.add", "tensorflow.variable_scope", "tensorflow.matmul", "tensorflow.nn.bias_add", "tensorflow.nn.softmax", "tensorflow.nn.log_softmax", "tensorflow.one_hot", "tensorflow.reduce_mean", "tensorflow.truncated_normal_initializer", "tensorflow.zeros_initializer", "tensorflow.nn.dropout", "tensorflow.reduce_sum"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.dropout"], ["def", "create_model", "(", "is_training", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "labels", ",", "\n", "num_labels", ",", "bert_hub_module_handle", ")", ":", "\n", "  ", "\"\"\"Creates a classification model.\"\"\"", "\n", "tags", "=", "set", "(", ")", "\n", "if", "is_training", ":", "\n", "    ", "tags", ".", "add", "(", "\"train\"", ")", "\n", "", "bert_module", "=", "hub", ".", "Module", "(", "bert_hub_module_handle", ",", "tags", "=", "tags", ",", "trainable", "=", "True", ")", "\n", "bert_inputs", "=", "dict", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ")", "\n", "bert_outputs", "=", "bert_module", "(", "\n", "inputs", "=", "bert_inputs", ",", "\n", "signature", "=", "\"tokens\"", ",", "\n", "as_dict", "=", "True", ")", "\n", "\n", "# In the demo, we are doing a simple classification task on the entire", "\n", "# segment.", "\n", "#", "\n", "# If you want to use the token-level output, use", "\n", "# bert_outputs[\"sequence_output\"] instead.", "\n", "output_layer", "=", "bert_outputs", "[", "\"pooled_output\"", "]", "\n", "\n", "hidden_size", "=", "output_layer", ".", "shape", "[", "-", "1", "]", ".", "value", "\n", "\n", "output_weights", "=", "tf", ".", "get_variable", "(", "\n", "\"output_weights\"", ",", "[", "num_labels", ",", "hidden_size", "]", ",", "\n", "initializer", "=", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "0.02", ")", ")", "\n", "\n", "output_bias", "=", "tf", ".", "get_variable", "(", "\n", "\"output_bias\"", ",", "[", "num_labels", "]", ",", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"loss\"", ")", ":", "\n", "    ", "if", "is_training", ":", "\n", "# I.e., 0.1 dropout", "\n", "      ", "output_layer", "=", "tf", ".", "nn", ".", "dropout", "(", "output_layer", ",", "keep_prob", "=", "0.9", ")", "\n", "\n", "", "logits", "=", "tf", ".", "matmul", "(", "output_layer", ",", "output_weights", ",", "transpose_b", "=", "True", ")", "\n", "logits", "=", "tf", ".", "nn", ".", "bias_add", "(", "logits", ",", "output_bias", ")", "\n", "probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "logits", ",", "axis", "=", "-", "1", ")", "\n", "log_probs", "=", "tf", ".", "nn", ".", "log_softmax", "(", "logits", ",", "axis", "=", "-", "1", ")", "\n", "\n", "one_hot_labels", "=", "tf", ".", "one_hot", "(", "labels", ",", "depth", "=", "num_labels", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "per_example_loss", "=", "-", "tf", ".", "reduce_sum", "(", "one_hot_labels", "*", "log_probs", ",", "axis", "=", "-", "1", ")", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "per_example_loss", ")", "\n", "\n", "return", "(", "loss", ",", "per_example_loss", ",", "logits", ",", "probabilities", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier_with_tfhub.model_fn_builder": [[87, 144], ["tensorflow.logging.info", "sorted", "run_classifier_with_tfhub.create_model", "features.keys", "tensorflow.logging.info", "optimization.create_optimizer", "tensorflow.contrib.tpu.TPUEstimatorSpec", "tensorflow.contrib.tpu.TPUEstimatorSpec", "tensorflow.argmax", "tensorflow.metrics.accuracy", "tensorflow.metrics.mean", "tensorflow.contrib.tpu.TPUEstimatorSpec", "ValueError"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.create_model", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.optimization.create_optimizer"], ["", "", "def", "model_fn_builder", "(", "num_labels", ",", "learning_rate", ",", "num_train_steps", ",", "\n", "num_warmup_steps", ",", "use_tpu", ",", "bert_hub_module_handle", ")", ":", "\n", "  ", "\"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"", "\n", "\n", "def", "model_fn", "(", "features", ",", "labels", ",", "mode", ",", "params", ")", ":", "# pylint: disable=unused-argument", "\n", "    ", "\"\"\"The `model_fn` for TPUEstimator.\"\"\"", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"*** Features ***\"", ")", "\n", "for", "name", "in", "sorted", "(", "features", ".", "keys", "(", ")", ")", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"  name = %s, shape = %s\"", "%", "(", "name", ",", "features", "[", "name", "]", ".", "shape", ")", ")", "\n", "\n", "", "input_ids", "=", "features", "[", "\"input_ids\"", "]", "\n", "input_mask", "=", "features", "[", "\"input_mask\"", "]", "\n", "segment_ids", "=", "features", "[", "\"segment_ids\"", "]", "\n", "label_ids", "=", "features", "[", "\"label_ids\"", "]", "\n", "\n", "is_training", "=", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ")", "\n", "\n", "(", "total_loss", ",", "per_example_loss", ",", "logits", ",", "probabilities", ")", "=", "create_model", "(", "\n", "is_training", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_ids", ",", "num_labels", ",", "\n", "bert_hub_module_handle", ")", "\n", "\n", "output_spec", "=", "None", "\n", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "      ", "train_op", "=", "optimization", ".", "create_optimizer", "(", "\n", "total_loss", ",", "learning_rate", ",", "num_train_steps", ",", "num_warmup_steps", ",", "use_tpu", ")", "\n", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "\n", "loss", "=", "total_loss", ",", "\n", "train_op", "=", "train_op", ")", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "\n", "      ", "def", "metric_fn", "(", "per_example_loss", ",", "label_ids", ",", "logits", ")", ":", "\n", "        ", "predictions", "=", "tf", ".", "argmax", "(", "logits", ",", "axis", "=", "-", "1", ",", "output_type", "=", "tf", ".", "int32", ")", "\n", "accuracy", "=", "tf", ".", "metrics", ".", "accuracy", "(", "label_ids", ",", "predictions", ")", "\n", "loss", "=", "tf", ".", "metrics", ".", "mean", "(", "per_example_loss", ")", "\n", "return", "{", "\n", "\"eval_accuracy\"", ":", "accuracy", ",", "\n", "\"eval_loss\"", ":", "loss", ",", "\n", "}", "\n", "\n", "", "eval_metrics", "=", "(", "metric_fn", ",", "[", "per_example_loss", ",", "label_ids", ",", "logits", "]", ")", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "\n", "loss", "=", "total_loss", ",", "\n", "eval_metrics", "=", "eval_metrics", ")", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "PREDICT", ":", "\n", "      ", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "predictions", "=", "{", "\"probabilities\"", ":", "probabilities", "}", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "\"Only TRAIN, EVAL and PREDICT modes are supported: %s\"", "%", "(", "mode", ")", ")", "\n", "\n", "", "return", "output_spec", "\n", "\n", "", "return", "model_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier_with_tfhub.create_tokenizer_from_hub_module": [[146, 156], ["tokenization.FullTokenizer", "tensorflow.Graph().as_default", "tensorflow_hub.Module", "hub.Module.", "tensorflow.Session", "sess.run", "tensorflow.Graph"], "function", ["None"], ["", "def", "create_tokenizer_from_hub_module", "(", "bert_hub_module_handle", ")", ":", "\n", "  ", "\"\"\"Get the vocab file and casing info from the Hub module.\"\"\"", "\n", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "    ", "bert_module", "=", "hub", ".", "Module", "(", "bert_hub_module_handle", ")", "\n", "tokenization_info", "=", "bert_module", "(", "signature", "=", "\"tokenization_info\"", ",", "as_dict", "=", "True", ")", "\n", "with", "tf", ".", "Session", "(", ")", "as", "sess", ":", "\n", "      ", "vocab_file", ",", "do_lower_case", "=", "sess", ".", "run", "(", "[", "tokenization_info", "[", "\"vocab_file\"", "]", ",", "\n", "tokenization_info", "[", "\"do_lower_case\"", "]", "]", ")", "\n", "", "", "return", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "vocab_file", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier_with_tfhub.main": [[158, 307], ["tensorflow.logging.set_verbosity", "tensorflow.gfile.MakeDirs", "FLAGS.task_name.lower", "processor.get_labels", "run_classifier_with_tfhub.create_tokenizer_from_hub_module", "tensorflow.contrib.tpu.RunConfig", "run_classifier_with_tfhub.model_fn_builder", "tensorflow.contrib.tpu.TPUEstimator", "ValueError", "ValueError", "tensorflow.contrib.cluster_resolver.TPUClusterResolver", "processor.get_train_examples", "int", "int", "run_classifier.convert_examples_to_features", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "run_classifier.input_fn_builder", "tf.contrib.tpu.TPUEstimator.train", "processor.get_dev_examples", "run_classifier.convert_examples_to_features", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "run_classifier.input_fn_builder", "tf.contrib.tpu.TPUEstimator.evaluate", "os.path.join", "processor.get_test_examples", "os.path.join", "run_classifier.file_based_convert_examples_to_features", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "run_classifier.file_based_input_fn_builder", "tf.contrib.tpu.TPUEstimator.predict", "os.path.join", "tensorflow.contrib.tpu.TPUConfig", "len", "len", "len", "int", "tensorflow.gfile.GFile", "tensorflow.logging.info", "sorted", "len", "len", "tensorflow.gfile.GFile", "tensorflow.logging.info", "estimator.predict.keys", "tensorflow.logging.info", "writer.write", "writer.write", "len", "len", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor.get_labels", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier_with_tfhub.create_tokenizer_from_hub_module", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.model_fn_builder", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor.get_train_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.convert_examples_to_features", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.input_fn_builder", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.WnliProcessor.get_dev_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.convert_examples_to_features", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.input_fn_builder", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.ColaProcessor.get_test_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.file_based_convert_examples_to_features", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_classifier.file_based_input_fn_builder"], ["", "def", "main", "(", "_", ")", ":", "\n", "  ", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "\n", "processors", "=", "{", "\n", "\"cola\"", ":", "run_classifier", ".", "ColaProcessor", ",", "\n", "\"mnli\"", ":", "run_classifier", ".", "MnliProcessor", ",", "\n", "\"mrpc\"", ":", "run_classifier", ".", "MrpcProcessor", ",", "\n", "}", "\n", "\n", "if", "not", "FLAGS", ".", "do_train", "and", "not", "FLAGS", ".", "do_eval", ":", "\n", "    ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "tf", ".", "gfile", ".", "MakeDirs", "(", "FLAGS", ".", "output_dir", ")", "\n", "\n", "task_name", "=", "FLAGS", ".", "task_name", ".", "lower", "(", ")", "\n", "\n", "if", "task_name", "not", "in", "processors", ":", "\n", "    ", "raise", "ValueError", "(", "\"Task not found: %s\"", "%", "(", "task_name", ")", ")", "\n", "\n", "", "processor", "=", "processors", "[", "task_name", "]", "(", ")", "\n", "\n", "label_list", "=", "processor", ".", "get_labels", "(", ")", "\n", "\n", "tokenizer", "=", "create_tokenizer_from_hub_module", "(", "FLAGS", ".", "bert_hub_module_handle", ")", "\n", "\n", "tpu_cluster_resolver", "=", "None", "\n", "if", "FLAGS", ".", "use_tpu", "and", "FLAGS", ".", "tpu_name", ":", "\n", "    ", "tpu_cluster_resolver", "=", "tf", ".", "contrib", ".", "cluster_resolver", ".", "TPUClusterResolver", "(", "\n", "FLAGS", ".", "tpu_name", ",", "zone", "=", "FLAGS", ".", "tpu_zone", ",", "project", "=", "FLAGS", ".", "gcp_project", ")", "\n", "\n", "", "is_per_host", "=", "tf", ".", "contrib", ".", "tpu", ".", "InputPipelineConfig", ".", "PER_HOST_V2", "\n", "run_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "RunConfig", "(", "\n", "cluster", "=", "tpu_cluster_resolver", ",", "\n", "master", "=", "FLAGS", ".", "master", ",", "\n", "model_dir", "=", "FLAGS", ".", "output_dir", ",", "\n", "save_checkpoints_steps", "=", "FLAGS", ".", "save_checkpoints_steps", ",", "\n", "tpu_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUConfig", "(", "\n", "iterations_per_loop", "=", "FLAGS", ".", "iterations_per_loop", ",", "\n", "num_shards", "=", "FLAGS", ".", "num_tpu_cores", ",", "\n", "per_host_input_for_training", "=", "is_per_host", ")", ")", "\n", "\n", "train_examples", "=", "None", "\n", "num_train_steps", "=", "None", "\n", "num_warmup_steps", "=", "None", "\n", "if", "FLAGS", ".", "do_train", ":", "\n", "    ", "train_examples", "=", "processor", ".", "get_train_examples", "(", "FLAGS", ".", "data_dir", ")", "\n", "num_train_steps", "=", "int", "(", "\n", "len", "(", "train_examples", ")", "/", "FLAGS", ".", "train_batch_size", "*", "FLAGS", ".", "num_train_epochs", ")", "\n", "num_warmup_steps", "=", "int", "(", "num_train_steps", "*", "FLAGS", ".", "warmup_proportion", ")", "\n", "\n", "", "model_fn", "=", "model_fn_builder", "(", "\n", "num_labels", "=", "len", "(", "label_list", ")", ",", "\n", "learning_rate", "=", "FLAGS", ".", "learning_rate", ",", "\n", "num_train_steps", "=", "num_train_steps", ",", "\n", "num_warmup_steps", "=", "num_warmup_steps", ",", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "bert_hub_module_handle", "=", "FLAGS", ".", "bert_hub_module_handle", ")", "\n", "\n", "# If TPU is not available, this will fall back to normal Estimator on CPU", "\n", "# or GPU.", "\n", "estimator", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimator", "(", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "model_fn", "=", "model_fn", ",", "\n", "config", "=", "run_config", ",", "\n", "train_batch_size", "=", "FLAGS", ".", "train_batch_size", ",", "\n", "eval_batch_size", "=", "FLAGS", ".", "eval_batch_size", ",", "\n", "predict_batch_size", "=", "FLAGS", ".", "predict_batch_size", ")", "\n", "\n", "if", "FLAGS", ".", "do_train", ":", "\n", "    ", "train_features", "=", "run_classifier", ".", "convert_examples_to_features", "(", "\n", "train_examples", ",", "label_list", ",", "FLAGS", ".", "max_seq_length", ",", "tokenizer", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_examples", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Batch size = %d\"", ",", "FLAGS", ".", "train_batch_size", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_steps", ")", "\n", "train_input_fn", "=", "run_classifier", ".", "input_fn_builder", "(", "\n", "features", "=", "train_features", ",", "\n", "seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "is_training", "=", "True", ",", "\n", "drop_remainder", "=", "True", ")", "\n", "estimator", ".", "train", "(", "input_fn", "=", "train_input_fn", ",", "max_steps", "=", "num_train_steps", ")", "\n", "\n", "", "if", "FLAGS", ".", "do_eval", ":", "\n", "    ", "eval_examples", "=", "processor", ".", "get_dev_examples", "(", "FLAGS", ".", "data_dir", ")", "\n", "eval_features", "=", "run_classifier", ".", "convert_examples_to_features", "(", "\n", "eval_examples", ",", "label_list", ",", "FLAGS", ".", "max_seq_length", ",", "tokenizer", ")", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** Running evaluation *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Batch size = %d\"", ",", "FLAGS", ".", "eval_batch_size", ")", "\n", "\n", "# This tells the estimator to run through the entire set.", "\n", "eval_steps", "=", "None", "\n", "# However, if running eval on the TPU, you will need to specify the", "\n", "# number of steps.", "\n", "if", "FLAGS", ".", "use_tpu", ":", "\n", "# Eval will be slightly WRONG on the TPU because it will truncate", "\n", "# the last batch.", "\n", "      ", "eval_steps", "=", "int", "(", "len", "(", "eval_examples", ")", "/", "FLAGS", ".", "eval_batch_size", ")", "\n", "\n", "", "eval_drop_remainder", "=", "True", "if", "FLAGS", ".", "use_tpu", "else", "False", "\n", "eval_input_fn", "=", "run_classifier", ".", "input_fn_builder", "(", "\n", "features", "=", "eval_features", ",", "\n", "seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "is_training", "=", "False", ",", "\n", "drop_remainder", "=", "eval_drop_remainder", ")", "\n", "\n", "result", "=", "estimator", ".", "evaluate", "(", "input_fn", "=", "eval_input_fn", ",", "steps", "=", "eval_steps", ")", "\n", "\n", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"eval_results.txt\"", ")", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"***** Eval results *****\"", ")", "\n", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "\n", "", "", "", "if", "FLAGS", ".", "do_predict", ":", "\n", "    ", "predict_examples", "=", "processor", ".", "get_test_examples", "(", "FLAGS", ".", "data_dir", ")", "\n", "if", "FLAGS", ".", "use_tpu", ":", "\n", "# Discard batch remainder if running on TPU", "\n", "      ", "n", "=", "len", "(", "predict_examples", ")", "\n", "predict_examples", "=", "predict_examples", "[", ":", "(", "n", "-", "n", "%", "FLAGS", ".", "predict_batch_size", ")", "]", "\n", "\n", "", "predict_file", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"predict.tf_record\"", ")", "\n", "run_classifier", ".", "file_based_convert_examples_to_features", "(", "\n", "predict_examples", ",", "label_list", ",", "FLAGS", ".", "max_seq_length", ",", "tokenizer", ",", "\n", "predict_file", ")", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** Running prediction*****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "predict_examples", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Batch size = %d\"", ",", "FLAGS", ".", "predict_batch_size", ")", "\n", "\n", "predict_input_fn", "=", "run_classifier", ".", "file_based_input_fn_builder", "(", "\n", "input_file", "=", "predict_file", ",", "\n", "seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "is_training", "=", "False", ",", "\n", "drop_remainder", "=", "FLAGS", ".", "use_tpu", ")", "\n", "\n", "result", "=", "estimator", ".", "predict", "(", "input_fn", "=", "predict_input_fn", ")", "\n", "\n", "output_predict_file", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"test_results.tsv\"", ")", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "output_predict_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"***** Predict results *****\"", ")", "\n", "for", "prediction", "in", "result", ":", "\n", "        ", "probabilities", "=", "prediction", "[", "\"probabilities\"", "]", "\n", "output_line", "=", "\"\\t\"", ".", "join", "(", "\n", "str", "(", "class_probability", ")", "\n", "for", "class_probability", "in", "probabilities", ")", "+", "\"\\n\"", "\n", "writer", ".", "write", "(", "output_line", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling_test.BertModelTest.test_default": [[126, 128], ["modeling_test.BertModelTest.run_tester", "BertModelTest.BertModelTester"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling_test.BertModelTest.run_tester"], ["", "", "def", "test_default", "(", "self", ")", ":", "\n", "    ", "self", ".", "run_tester", "(", "BertModelTest", ".", "BertModelTester", "(", "self", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling_test.BertModelTest.test_config_to_json_string": [[129, 134], ["modeling.BertConfig", "json.loads", "modeling_test.BertModelTest.assertEqual", "modeling_test.BertModelTest.assertEqual", "modeling.BertConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertConfig.to_json_string"], ["", "def", "test_config_to_json_string", "(", "self", ")", ":", "\n", "    ", "config", "=", "modeling", ".", "BertConfig", "(", "vocab_size", "=", "99", ",", "hidden_size", "=", "37", ")", "\n", "obj", "=", "json", ".", "loads", "(", "config", ".", "to_json_string", "(", ")", ")", "\n", "self", ".", "assertEqual", "(", "obj", "[", "\"vocab_size\"", "]", ",", "99", ")", "\n", "self", ".", "assertEqual", "(", "obj", "[", "\"hidden_size\"", "]", ",", "37", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling_test.BertModelTest.run_tester": [[135, 145], ["modeling_test.BertModelTest.test_session", "tester.create_model", "tensorflow.group", "sess.run", "sess.run", "tester.check_output", "modeling_test.BertModelTest.assert_all_tensors_reachable", "tensorflow.global_variables_initializer", "tensorflow.local_variables_initializer"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.create_model", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling_test.BertModelTest.assert_all_tensors_reachable"], ["", "def", "run_tester", "(", "self", ",", "tester", ")", ":", "\n", "    ", "with", "self", ".", "test_session", "(", ")", "as", "sess", ":", "\n", "      ", "ops", "=", "tester", ".", "create_model", "(", ")", "\n", "init_op", "=", "tf", ".", "group", "(", "tf", ".", "global_variables_initializer", "(", ")", ",", "\n", "tf", ".", "local_variables_initializer", "(", ")", ")", "\n", "sess", ".", "run", "(", "init_op", ")", "\n", "output_result", "=", "sess", ".", "run", "(", "ops", ")", "\n", "tester", ".", "check_output", "(", "output_result", ")", "\n", "\n", "self", ".", "assert_all_tensors_reachable", "(", "sess", ",", "[", "init_op", ",", "ops", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling_test.BertModelTest.ids_tensor": [[146, 161], ["range", "tensorflow.constant", "random.Random", "values.append", "random.Random.randint"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "ids_tensor", "(", "cls", ",", "shape", ",", "vocab_size", ",", "rng", "=", "None", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"Creates a random int32 tensor of the shape within the vocab size.\"\"\"", "\n", "if", "rng", "is", "None", ":", "\n", "      ", "rng", "=", "random", ".", "Random", "(", ")", "\n", "\n", "", "total_dims", "=", "1", "\n", "for", "dim", "in", "shape", ":", "\n", "      ", "total_dims", "*=", "dim", "\n", "\n", "", "values", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "total_dims", ")", ":", "\n", "      ", "values", ".", "append", "(", "rng", ".", "randint", "(", "0", ",", "vocab_size", "-", "1", ")", ")", "\n", "\n", "", "return", "tf", ".", "constant", "(", "value", "=", "values", ",", "dtype", "=", "tf", ".", "int32", ",", "shape", "=", "shape", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling_test.BertModelTest.assert_all_tensors_reachable": [[162, 192], ["modeling_test.BertModelTest.get_unreachable_ops", "modeling_test.BertModelTest.assertEqual", "re.compile", "filtered_unreachable.append", "len", "r.match"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling_test.BertModelTest.get_unreachable_ops"], ["", "def", "assert_all_tensors_reachable", "(", "self", ",", "sess", ",", "outputs", ")", ":", "\n", "    ", "\"\"\"Checks that all the tensors in the graph are reachable from outputs.\"\"\"", "\n", "graph", "=", "sess", ".", "graph", "\n", "\n", "ignore_strings", "=", "[", "\n", "\"^.*/assert_less_equal/.*$\"", ",", "\n", "\"^.*/dilation_rate$\"", ",", "\n", "\"^.*/Tensordot/concat$\"", ",", "\n", "\"^.*/Tensordot/concat/axis$\"", ",", "\n", "\"^testing/.*$\"", ",", "\n", "]", "\n", "\n", "ignore_regexes", "=", "[", "re", ".", "compile", "(", "x", ")", "for", "x", "in", "ignore_strings", "]", "\n", "\n", "unreachable", "=", "self", ".", "get_unreachable_ops", "(", "graph", ",", "outputs", ")", "\n", "filtered_unreachable", "=", "[", "]", "\n", "for", "x", "in", "unreachable", ":", "\n", "      ", "do_ignore", "=", "False", "\n", "for", "r", "in", "ignore_regexes", ":", "\n", "        ", "m", "=", "r", ".", "match", "(", "x", ".", "name", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "          ", "do_ignore", "=", "True", "\n", "", "", "if", "do_ignore", ":", "\n", "        ", "continue", "\n", "", "filtered_unreachable", ".", "append", "(", "x", ")", "\n", "", "unreachable", "=", "filtered_unreachable", "\n", "\n", "self", ".", "assertEqual", "(", "\n", "len", "(", "unreachable", ")", ",", "0", ",", "\"The following ops are unreachable: %s\"", "%", "\n", "(", "\" \"", ".", "join", "(", "[", "x", ".", "name", "for", "x", "in", "unreachable", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling_test.BertModelTest.get_unreachable_ops": [[193, 255], ["cls.flatten_recursive", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "graph.get_operations", "collections.defaultdict", "collections.defaultdict.keys", "graph.get_operations", "stack.pop", "op_to_all[].append", "output_to_op[].append", "op_to_all[].append", "str", "assign_groups[].append", "unreachable_ops.append", "expanded_names.append", "stack.append", "assign_out_to_in[].append", "assign_groups[].append", "stack.append"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling_test.BertModelTest.flatten_recursive"], ["", "@", "classmethod", "\n", "def", "get_unreachable_ops", "(", "cls", ",", "graph", ",", "outputs", ")", ":", "\n", "    ", "\"\"\"Finds all of the tensors in graph that are unreachable from outputs.\"\"\"", "\n", "outputs", "=", "cls", ".", "flatten_recursive", "(", "outputs", ")", "\n", "output_to_op", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "op_to_all", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "assign_out_to_in", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "\n", "for", "op", "in", "graph", ".", "get_operations", "(", ")", ":", "\n", "      ", "for", "x", "in", "op", ".", "inputs", ":", "\n", "        ", "op_to_all", "[", "op", ".", "name", "]", ".", "append", "(", "x", ".", "name", ")", "\n", "", "for", "y", "in", "op", ".", "outputs", ":", "\n", "        ", "output_to_op", "[", "y", ".", "name", "]", ".", "append", "(", "op", ".", "name", ")", "\n", "op_to_all", "[", "op", ".", "name", "]", ".", "append", "(", "y", ".", "name", ")", "\n", "", "if", "str", "(", "op", ".", "type", ")", "==", "\"Assign\"", ":", "\n", "        ", "for", "y", "in", "op", ".", "outputs", ":", "\n", "          ", "for", "x", "in", "op", ".", "inputs", ":", "\n", "            ", "assign_out_to_in", "[", "y", ".", "name", "]", ".", "append", "(", "x", ".", "name", ")", "\n", "\n", "", "", "", "", "assign_groups", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "out_name", "in", "assign_out_to_in", ".", "keys", "(", ")", ":", "\n", "      ", "name_group", "=", "assign_out_to_in", "[", "out_name", "]", "\n", "for", "n1", "in", "name_group", ":", "\n", "        ", "assign_groups", "[", "n1", "]", ".", "append", "(", "out_name", ")", "\n", "for", "n2", "in", "name_group", ":", "\n", "          ", "if", "n1", "!=", "n2", ":", "\n", "            ", "assign_groups", "[", "n1", "]", ".", "append", "(", "n2", ")", "\n", "\n", "", "", "", "", "seen_tensors", "=", "{", "}", "\n", "stack", "=", "[", "x", ".", "name", "for", "x", "in", "outputs", "]", "\n", "while", "stack", ":", "\n", "      ", "name", "=", "stack", ".", "pop", "(", ")", "\n", "if", "name", "in", "seen_tensors", ":", "\n", "        ", "continue", "\n", "", "seen_tensors", "[", "name", "]", "=", "True", "\n", "\n", "if", "name", "in", "output_to_op", ":", "\n", "        ", "for", "op_name", "in", "output_to_op", "[", "name", "]", ":", "\n", "          ", "if", "op_name", "in", "op_to_all", ":", "\n", "            ", "for", "input_name", "in", "op_to_all", "[", "op_name", "]", ":", "\n", "              ", "if", "input_name", "not", "in", "stack", ":", "\n", "                ", "stack", ".", "append", "(", "input_name", ")", "\n", "\n", "", "", "", "", "", "expanded_names", "=", "[", "]", "\n", "if", "name", "in", "assign_groups", ":", "\n", "        ", "for", "assign_name", "in", "assign_groups", "[", "name", "]", ":", "\n", "          ", "expanded_names", ".", "append", "(", "assign_name", ")", "\n", "\n", "", "", "for", "expanded_name", "in", "expanded_names", ":", "\n", "        ", "if", "expanded_name", "not", "in", "stack", ":", "\n", "          ", "stack", ".", "append", "(", "expanded_name", ")", "\n", "\n", "", "", "", "unreachable_ops", "=", "[", "]", "\n", "for", "op", "in", "graph", ".", "get_operations", "(", ")", ":", "\n", "      ", "is_unreachable", "=", "False", "\n", "all_names", "=", "[", "x", ".", "name", "for", "x", "in", "op", ".", "inputs", "]", "+", "[", "x", ".", "name", "for", "x", "in", "op", ".", "outputs", "]", "\n", "for", "name", "in", "all_names", ":", "\n", "        ", "if", "name", "not", "in", "seen_tensors", ":", "\n", "          ", "is_unreachable", "=", "True", "\n", "", "", "if", "is_unreachable", ":", "\n", "        ", "unreachable_ops", ".", "append", "(", "op", ")", "\n", "", "", "return", "unreachable_ops", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling_test.BertModelTest.flatten_recursive": [[256, 274], ["isinstance", "output.extend", "isinstance", "flat_output.extend", "output.extend", "isinstance", "cls.flatten_recursive", "list", "six.iteritems", "output.append"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling_test.BertModelTest.flatten_recursive"], ["", "@", "classmethod", "\n", "def", "flatten_recursive", "(", "cls", ",", "item", ")", ":", "\n", "    ", "\"\"\"Flattens (potentially nested) a tuple/dictionary/list to a list.\"\"\"", "\n", "output", "=", "[", "]", "\n", "if", "isinstance", "(", "item", ",", "list", ")", ":", "\n", "      ", "output", ".", "extend", "(", "item", ")", "\n", "", "elif", "isinstance", "(", "item", ",", "tuple", ")", ":", "\n", "      ", "output", ".", "extend", "(", "list", "(", "item", ")", ")", "\n", "", "elif", "isinstance", "(", "item", ",", "dict", ")", ":", "\n", "      ", "for", "(", "_", ",", "v", ")", "in", "six", ".", "iteritems", "(", "item", ")", ":", "\n", "        ", "output", ".", "append", "(", "v", ")", "\n", "", "", "else", ":", "\n", "      ", "return", "[", "item", "]", "\n", "\n", "", "flat_output", "=", "[", "]", "\n", "for", "x", "in", "output", ":", "\n", "      ", "flat_output", ".", "extend", "(", "cls", ".", "flatten_recursive", "(", "x", ")", ")", "\n", "", "return", "flat_output", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.SquadExample.__init__": [[163, 178], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "qas_id", ",", "\n", "question_text", ",", "\n", "doc_tokens", ",", "\n", "orig_answer_text", "=", "None", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ",", "\n", "is_impossible", "=", "False", ")", ":", "\n", "    ", "self", ".", "qas_id", "=", "qas_id", "\n", "self", ".", "question_text", "=", "question_text", "\n", "self", ".", "doc_tokens", "=", "doc_tokens", "\n", "self", ".", "orig_answer_text", "=", "orig_answer_text", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "self", ".", "is_impossible", "=", "is_impossible", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.SquadExample.__str__": [[179, 181], ["run_squad.SquadExample.__repr__"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.SquadExample.__repr__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "__repr__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.SquadExample.__repr__": [[182, 195], ["tokenization.printable_text", "tokenization.printable_text"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.printable_text", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.printable_text"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "    ", "s", "=", "\"\"", "\n", "s", "+=", "\"qas_id: %s\"", "%", "(", "tokenization", ".", "printable_text", "(", "self", ".", "qas_id", ")", ")", "\n", "s", "+=", "\", question_text: %s\"", "%", "(", "\n", "tokenization", ".", "printable_text", "(", "self", ".", "question_text", ")", ")", "\n", "s", "+=", "\", doc_tokens: [%s]\"", "%", "(", "\" \"", ".", "join", "(", "self", ".", "doc_tokens", ")", ")", "\n", "if", "self", ".", "start_position", ":", "\n", "      ", "s", "+=", "\", start_position: %d\"", "%", "(", "self", ".", "start_position", ")", "\n", "", "if", "self", ".", "start_position", ":", "\n", "      ", "s", "+=", "\", end_position: %d\"", "%", "(", "self", ".", "end_position", ")", "\n", "", "if", "self", ".", "start_position", ":", "\n", "      ", "s", "+=", "\", is_impossible: %r\"", "%", "(", "self", ".", "is_impossible", ")", "\n", "", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.InputFeatures.__init__": [[200, 225], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "unique_id", ",", "\n", "example_index", ",", "\n", "doc_span_index", ",", "\n", "tokens", ",", "\n", "token_to_orig_map", ",", "\n", "token_is_max_context", ",", "\n", "input_ids", ",", "\n", "input_mask", ",", "\n", "segment_ids", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ",", "\n", "is_impossible", "=", "None", ")", ":", "\n", "    ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "example_index", "=", "example_index", "\n", "self", ".", "doc_span_index", "=", "doc_span_index", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "token_to_orig_map", "=", "token_to_orig_map", "\n", "self", ".", "token_is_max_context", "=", "token_is_max_context", "\n", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "self", ".", "is_impossible", "=", "is_impossible", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.FeatureWriter.__init__": [[1061, 1066], ["tensorflow.python_io.TFRecordWriter"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "filename", ",", "is_training", ")", ":", "\n", "    ", "self", ".", "filename", "=", "filename", "\n", "self", ".", "is_training", "=", "is_training", "\n", "self", ".", "num_features", "=", "0", "\n", "self", ".", "_writer", "=", "tf", ".", "python_io", ".", "TFRecordWriter", "(", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.FeatureWriter.process_feature": [[1067, 1092], ["collections.OrderedDict", "run_squad.FeatureWriter.process_feature.create_int_feature"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.create_pretraining_data.create_int_feature"], ["", "def", "process_feature", "(", "self", ",", "feature", ")", ":", "\n", "    ", "\"\"\"Write a InputFeature to the TFRecordWriter as a tf.train.Example.\"\"\"", "\n", "self", ".", "num_features", "+=", "1", "\n", "\n", "def", "create_int_feature", "(", "values", ")", ":", "\n", "      ", "feature", "=", "tf", ".", "train", ".", "Feature", "(", "\n", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "list", "(", "values", ")", ")", ")", "\n", "return", "feature", "\n", "\n", "", "features", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "features", "[", "\"unique_ids\"", "]", "=", "create_int_feature", "(", "[", "feature", ".", "unique_id", "]", ")", "\n", "features", "[", "\"input_ids\"", "]", "=", "create_int_feature", "(", "feature", ".", "input_ids", ")", "\n", "features", "[", "\"input_mask\"", "]", "=", "create_int_feature", "(", "feature", ".", "input_mask", ")", "\n", "features", "[", "\"segment_ids\"", "]", "=", "create_int_feature", "(", "feature", ".", "segment_ids", ")", "\n", "\n", "if", "self", ".", "is_training", ":", "\n", "      ", "features", "[", "\"start_positions\"", "]", "=", "create_int_feature", "(", "[", "feature", ".", "start_position", "]", ")", "\n", "features", "[", "\"end_positions\"", "]", "=", "create_int_feature", "(", "[", "feature", ".", "end_position", "]", ")", "\n", "impossible", "=", "0", "\n", "if", "feature", ".", "is_impossible", ":", "\n", "        ", "impossible", "=", "1", "\n", "", "features", "[", "\"is_impossible\"", "]", "=", "create_int_feature", "(", "[", "impossible", "]", ")", "\n", "\n", "", "tf_example", "=", "tf", ".", "train", ".", "Example", "(", "features", "=", "tf", ".", "train", ".", "Features", "(", "feature", "=", "features", ")", ")", "\n", "self", ".", "_writer", ".", "write", "(", "tf_example", ".", "SerializeToString", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.FeatureWriter.close": [[1093, 1095], ["run_squad.FeatureWriter._writer.close"], "methods", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.FeatureWriter.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "    ", "self", ".", "_writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.read_squad_examples": [[227, 307], ["tensorflow.gfile.Open", "json.load", "ord", "run_squad.read_squad_examples.is_whitespace"], "function", ["None"], ["", "", "def", "read_squad_examples", "(", "input_file", ",", "is_training", ")", ":", "\n", "  ", "\"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"", "\n", "with", "tf", ".", "gfile", ".", "Open", "(", "input_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "    ", "input_data", "=", "json", ".", "load", "(", "reader", ")", "[", "\"data\"", "]", "\n", "\n", "", "def", "is_whitespace", "(", "c", ")", ":", "\n", "    ", "if", "c", "==", "\" \"", "or", "c", "==", "\"\\t\"", "or", "c", "==", "\"\\r\"", "or", "c", "==", "\"\\n\"", "or", "ord", "(", "c", ")", "==", "0x202F", ":", "\n", "      ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "examples", "=", "[", "]", "\n", "for", "entry", "in", "input_data", ":", "\n", "    ", "for", "paragraph", "in", "entry", "[", "\"paragraphs\"", "]", ":", "\n", "      ", "paragraph_text", "=", "paragraph", "[", "\"context\"", "]", "\n", "doc_tokens", "=", "[", "]", "\n", "char_to_word_offset", "=", "[", "]", "\n", "prev_is_whitespace", "=", "True", "\n", "for", "c", "in", "paragraph_text", ":", "\n", "        ", "if", "is_whitespace", "(", "c", ")", ":", "\n", "          ", "prev_is_whitespace", "=", "True", "\n", "", "else", ":", "\n", "          ", "if", "prev_is_whitespace", ":", "\n", "            ", "doc_tokens", ".", "append", "(", "c", ")", "\n", "", "else", ":", "\n", "            ", "doc_tokens", "[", "-", "1", "]", "+=", "c", "\n", "", "prev_is_whitespace", "=", "False", "\n", "", "char_to_word_offset", ".", "append", "(", "len", "(", "doc_tokens", ")", "-", "1", ")", "\n", "\n", "", "for", "qa", "in", "paragraph", "[", "\"qas\"", "]", ":", "\n", "        ", "qas_id", "=", "qa", "[", "\"id\"", "]", "\n", "question_text", "=", "qa", "[", "\"question\"", "]", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "orig_answer_text", "=", "None", "\n", "is_impossible", "=", "False", "\n", "if", "is_training", ":", "\n", "\n", "          ", "if", "FLAGS", ".", "version_2_with_negative", ":", "\n", "            ", "is_impossible", "=", "qa", "[", "\"is_impossible\"", "]", "\n", "", "if", "(", "len", "(", "qa", "[", "\"answers\"", "]", ")", "!=", "1", ")", "and", "(", "not", "is_impossible", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"For training, each question should have exactly 1 answer.\"", ")", "\n", "", "if", "not", "is_impossible", ":", "\n", "            ", "answer", "=", "qa", "[", "\"answers\"", "]", "[", "0", "]", "\n", "orig_answer_text", "=", "answer", "[", "\"text\"", "]", "\n", "answer_offset", "=", "answer", "[", "\"answer_start\"", "]", "\n", "answer_length", "=", "len", "(", "orig_answer_text", ")", "\n", "start_position", "=", "char_to_word_offset", "[", "answer_offset", "]", "\n", "end_position", "=", "char_to_word_offset", "[", "answer_offset", "+", "answer_length", "-", "\n", "1", "]", "\n", "# Only add answers where the text can be exactly recovered from the", "\n", "# document. If this CAN'T happen it's likely due to weird Unicode", "\n", "# stuff so we will just skip the example.", "\n", "#", "\n", "# Note that this means for training mode, every example is NOT", "\n", "# guaranteed to be preserved.", "\n", "actual_text", "=", "\" \"", ".", "join", "(", "\n", "doc_tokens", "[", "start_position", ":", "(", "end_position", "+", "1", ")", "]", ")", "\n", "cleaned_answer_text", "=", "\" \"", ".", "join", "(", "\n", "tokenization", ".", "whitespace_tokenize", "(", "orig_answer_text", ")", ")", "\n", "if", "actual_text", ".", "find", "(", "cleaned_answer_text", ")", "==", "-", "1", ":", "\n", "              ", "tf", ".", "logging", ".", "warning", "(", "\"Could not find answer: '%s' vs. '%s'\"", ",", "\n", "actual_text", ",", "cleaned_answer_text", ")", "\n", "continue", "\n", "", "", "else", ":", "\n", "            ", "start_position", "=", "-", "1", "\n", "end_position", "=", "-", "1", "\n", "orig_answer_text", "=", "\"\"", "\n", "\n", "", "", "example", "=", "SquadExample", "(", "\n", "qas_id", "=", "qas_id", ",", "\n", "question_text", "=", "question_text", ",", "\n", "doc_tokens", "=", "doc_tokens", ",", "\n", "orig_answer_text", "=", "orig_answer_text", ",", "\n", "start_position", "=", "start_position", ",", "\n", "end_position", "=", "end_position", ",", "\n", "is_impossible", "=", "is_impossible", ")", "\n", "examples", ".", "append", "(", "example", ")", "\n", "\n", "", "", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.convert_examples_to_features": [[309, 474], ["enumerate", "tokenizer.tokenize", "enumerate", "collections.namedtuple", "enumerate", "len", "orig_to_tok_index.append", "tokenizer.tokenize", "run_squad._improve_answer_span", "len", "doc_spans.append", "min", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "range", "tokens.append", "segment_ids.append", "tokenizer.convert_tokens_to_ids", "run_squad.InputFeatures", "output_fn", "len", "tok_to_orig_index.append", "all_doc_tokens.append", "len", "len", "collections.namedtuple.", "len", "tokens.append", "segment_ids.append", "run_squad._check_is_max_context", "tokens.append", "segment_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "len", "len", "len", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "len", "len", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "len", "len", "len", "tokenization.printable_text", "tokenization.printable_text", "str", "str", "str", "six.iteritems", "six.iteritems"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad._improve_answer_span", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad._check_is_max_context", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.printable_text", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.printable_text"], ["", "def", "convert_examples_to_features", "(", "examples", ",", "tokenizer", ",", "max_seq_length", ",", "\n", "doc_stride", ",", "max_query_length", ",", "is_training", ",", "\n", "output_fn", ")", ":", "\n", "  ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "unique_id", "=", "1000000000", "\n", "\n", "for", "(", "example_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "    ", "query_tokens", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "question_text", ")", "\n", "\n", "if", "len", "(", "query_tokens", ")", ">", "max_query_length", ":", "\n", "      ", "query_tokens", "=", "query_tokens", "[", "0", ":", "max_query_length", "]", "\n", "\n", "", "tok_to_orig_index", "=", "[", "]", "\n", "orig_to_tok_index", "=", "[", "]", "\n", "all_doc_tokens", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "example", ".", "doc_tokens", ")", ":", "\n", "      ", "orig_to_tok_index", ".", "append", "(", "len", "(", "all_doc_tokens", ")", ")", "\n", "sub_tokens", "=", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "for", "sub_token", "in", "sub_tokens", ":", "\n", "        ", "tok_to_orig_index", ".", "append", "(", "i", ")", "\n", "all_doc_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "tok_start_position", "=", "None", "\n", "tok_end_position", "=", "None", "\n", "if", "is_training", "and", "example", ".", "is_impossible", ":", "\n", "      ", "tok_start_position", "=", "-", "1", "\n", "tok_end_position", "=", "-", "1", "\n", "", "if", "is_training", "and", "not", "example", ".", "is_impossible", ":", "\n", "      ", "tok_start_position", "=", "orig_to_tok_index", "[", "example", ".", "start_position", "]", "\n", "if", "example", ".", "end_position", "<", "len", "(", "example", ".", "doc_tokens", ")", "-", "1", ":", "\n", "        ", "tok_end_position", "=", "orig_to_tok_index", "[", "example", ".", "end_position", "+", "1", "]", "-", "1", "\n", "", "else", ":", "\n", "        ", "tok_end_position", "=", "len", "(", "all_doc_tokens", ")", "-", "1", "\n", "", "(", "tok_start_position", ",", "tok_end_position", ")", "=", "_improve_answer_span", "(", "\n", "all_doc_tokens", ",", "tok_start_position", ",", "tok_end_position", ",", "tokenizer", ",", "\n", "example", ".", "orig_answer_text", ")", "\n", "\n", "# The -3 accounts for [CLS], [SEP] and [SEP]", "\n", "", "max_tokens_for_doc", "=", "max_seq_length", "-", "len", "(", "query_tokens", ")", "-", "3", "\n", "\n", "# We can have documents that are longer than the maximum sequence length.", "\n", "# To deal with this we do a sliding window approach, where we take chunks", "\n", "# of the up to our max length with a stride of `doc_stride`.", "\n", "_DocSpan", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"DocSpan\"", ",", "[", "\"start\"", ",", "\"length\"", "]", ")", "\n", "doc_spans", "=", "[", "]", "\n", "start_offset", "=", "0", "\n", "while", "start_offset", "<", "len", "(", "all_doc_tokens", ")", ":", "\n", "      ", "length", "=", "len", "(", "all_doc_tokens", ")", "-", "start_offset", "\n", "if", "length", ">", "max_tokens_for_doc", ":", "\n", "        ", "length", "=", "max_tokens_for_doc", "\n", "", "doc_spans", ".", "append", "(", "_DocSpan", "(", "start", "=", "start_offset", ",", "length", "=", "length", ")", ")", "\n", "if", "start_offset", "+", "length", "==", "len", "(", "all_doc_tokens", ")", ":", "\n", "        ", "break", "\n", "", "start_offset", "+=", "min", "(", "length", ",", "doc_stride", ")", "\n", "\n", "", "for", "(", "doc_span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "      ", "tokens", "=", "[", "]", "\n", "token_to_orig_map", "=", "{", "}", "\n", "token_is_max_context", "=", "{", "}", "\n", "segment_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "query_tokens", ":", "\n", "        ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "for", "i", "in", "range", "(", "doc_span", ".", "length", ")", ":", "\n", "        ", "split_token_index", "=", "doc_span", ".", "start", "+", "i", "\n", "token_to_orig_map", "[", "len", "(", "tokens", ")", "]", "=", "tok_to_orig_index", "[", "split_token_index", "]", "\n", "\n", "is_max_context", "=", "_check_is_max_context", "(", "doc_spans", ",", "doc_span_index", ",", "\n", "split_token_index", ")", "\n", "token_is_max_context", "[", "len", "(", "tokens", ")", "]", "=", "is_max_context", "\n", "tokens", ".", "append", "(", "all_doc_tokens", "[", "split_token_index", "]", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "        ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "if", "is_training", "and", "not", "example", ".", "is_impossible", ":", "\n", "# For training, if our document chunk does not contain an annotation", "\n", "# we throw it out, since there is nothing to predict.", "\n", "        ", "doc_start", "=", "doc_span", ".", "start", "\n", "doc_end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "out_of_span", "=", "False", "\n", "if", "not", "(", "tok_start_position", ">=", "doc_start", "and", "\n", "tok_end_position", "<=", "doc_end", ")", ":", "\n", "          ", "out_of_span", "=", "True", "\n", "", "if", "out_of_span", ":", "\n", "          ", "start_position", "=", "0", "\n", "end_position", "=", "0", "\n", "", "else", ":", "\n", "          ", "doc_offset", "=", "len", "(", "query_tokens", ")", "+", "2", "\n", "start_position", "=", "tok_start_position", "-", "doc_start", "+", "doc_offset", "\n", "end_position", "=", "tok_end_position", "-", "doc_start", "+", "doc_offset", "\n", "\n", "", "", "if", "is_training", "and", "example", ".", "is_impossible", ":", "\n", "        ", "start_position", "=", "0", "\n", "end_position", "=", "0", "\n", "\n", "", "if", "example_index", "<", "20", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"unique_id: %s\"", "%", "(", "unique_id", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"example_index: %s\"", "%", "(", "example_index", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"doc_span_index: %s\"", "%", "(", "doc_span_index", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "tokenization", ".", "printable_text", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"token_to_orig_map: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "\"%d:%d\"", "%", "(", "x", ",", "y", ")", "for", "(", "x", ",", "y", ")", "in", "six", ".", "iteritems", "(", "token_to_orig_map", ")", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"token_is_max_context: %s\"", "%", "\" \"", ".", "join", "(", "[", "\n", "\"%d:%s\"", "%", "(", "x", ",", "y", ")", "for", "(", "x", ",", "y", ")", "in", "six", ".", "iteritems", "(", "token_is_max_context", ")", "\n", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "if", "is_training", "and", "example", ".", "is_impossible", ":", "\n", "          ", "tf", ".", "logging", ".", "info", "(", "\"impossible example\"", ")", "\n", "", "if", "is_training", "and", "not", "example", ".", "is_impossible", ":", "\n", "          ", "answer_text", "=", "\" \"", ".", "join", "(", "tokens", "[", "start_position", ":", "(", "end_position", "+", "1", ")", "]", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"start_position: %d\"", "%", "(", "start_position", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"end_position: %d\"", "%", "(", "end_position", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "\"answer: %s\"", "%", "(", "tokenization", ".", "printable_text", "(", "answer_text", ")", ")", ")", "\n", "\n", "", "", "feature", "=", "InputFeatures", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "example_index", "=", "example_index", ",", "\n", "doc_span_index", "=", "doc_span_index", ",", "\n", "tokens", "=", "tokens", ",", "\n", "token_to_orig_map", "=", "token_to_orig_map", ",", "\n", "token_is_max_context", "=", "token_is_max_context", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "start_position", "=", "start_position", ",", "\n", "end_position", "=", "end_position", ",", "\n", "is_impossible", "=", "example", ".", "is_impossible", ")", "\n", "\n", "# Run callback", "\n", "output_fn", "(", "feature", ")", "\n", "\n", "unique_id", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad._improve_answer_span": [[476, 511], ["range", "tokenizer.tokenize", "range"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.WordpieceTokenizer.tokenize"], ["", "", "", "def", "_improve_answer_span", "(", "doc_tokens", ",", "input_start", ",", "input_end", ",", "tokenizer", ",", "\n", "orig_answer_text", ")", ":", "\n", "  ", "\"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"", "\n", "\n", "# The SQuAD annotations are character based. We first project them to", "\n", "# whitespace-tokenized words. But then after WordPiece tokenization, we can", "\n", "# often find a \"better match\". For example:", "\n", "#", "\n", "#   Question: What year was John Smith born?", "\n", "#   Context: The leader was John Smith (1895-1943).", "\n", "#   Answer: 1895", "\n", "#", "\n", "# The original whitespace-tokenized answer will be \"(1895-1943).\". However", "\n", "# after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match", "\n", "# the exact answer, 1895.", "\n", "#", "\n", "# However, this is not always possible. Consider the following:", "\n", "#", "\n", "#   Question: What country is the top exporter of electornics?", "\n", "#   Context: The Japanese electronics industry is the lagest in the world.", "\n", "#   Answer: Japan", "\n", "#", "\n", "# In this case, the annotator chose \"Japan\" as a character sub-span of", "\n", "# the word \"Japanese\". Since our WordPiece tokenizer does not split", "\n", "# \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare", "\n", "# in SQuAD, but does happen.", "\n", "tok_answer_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "orig_answer_text", ")", ")", "\n", "\n", "for", "new_start", "in", "range", "(", "input_start", ",", "input_end", "+", "1", ")", ":", "\n", "    ", "for", "new_end", "in", "range", "(", "input_end", ",", "new_start", "-", "1", ",", "-", "1", ")", ":", "\n", "      ", "text_span", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "new_start", ":", "(", "new_end", "+", "1", ")", "]", ")", "\n", "if", "text_span", "==", "tok_answer_text", ":", "\n", "        ", "return", "(", "new_start", ",", "new_end", ")", "\n", "\n", "", "", "", "return", "(", "input_start", ",", "input_end", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad._check_is_max_context": [[513, 548], ["enumerate", "min"], "function", ["None"], ["", "def", "_check_is_max_context", "(", "doc_spans", ",", "cur_span_index", ",", "position", ")", ":", "\n", "  ", "\"\"\"Check if this is the 'max context' doc span for the token.\"\"\"", "\n", "\n", "# Because of the sliding window approach taken to scoring documents, a single", "\n", "# token can appear in multiple documents. E.g.", "\n", "#  Doc: the man went to the store and bought a gallon of milk", "\n", "#  Span A: the man went to the", "\n", "#  Span B: to the store and bought", "\n", "#  Span C: and bought a gallon of", "\n", "#  ...", "\n", "#", "\n", "# Now the word 'bought' will have two scores from spans B and C. We only", "\n", "# want to consider the score with \"maximum context\", which we define as", "\n", "# the *minimum* of its left and right context (the *sum* of left and", "\n", "# right context will always be the same, of course).", "\n", "#", "\n", "# In the example the maximum context for 'bought' would be span C since", "\n", "# it has 1 left context and 3 right context, while span B has 4 left context", "\n", "# and 0 right context.", "\n", "best_score", "=", "None", "\n", "best_span_index", "=", "None", "\n", "for", "(", "span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "    ", "end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "if", "position", "<", "doc_span", ".", "start", ":", "\n", "      ", "continue", "\n", "", "if", "position", ">", "end", ":", "\n", "      ", "continue", "\n", "", "num_left_context", "=", "position", "-", "doc_span", ".", "start", "\n", "num_right_context", "=", "end", "-", "position", "\n", "score", "=", "min", "(", "num_left_context", ",", "num_right_context", ")", "+", "0.01", "*", "doc_span", ".", "length", "\n", "if", "best_score", "is", "None", "or", "score", ">", "best_score", ":", "\n", "      ", "best_score", "=", "score", "\n", "best_span_index", "=", "span_index", "\n", "\n", "", "", "return", "cur_span_index", "==", "best_span_index", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.create_model": [[550, 588], ["modeling.BertModel", "modeling.BertModel.get_sequence_output", "modeling.get_shape_list", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.nn.bias_add", "tensorflow.reshape", "tensorflow.transpose", "tensorflow.unstack", "tensorflow.truncated_normal_initializer", "tensorflow.zeros_initializer"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertModel.get_sequence_output", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.get_shape_list"], ["", "def", "create_model", "(", "bert_config", ",", "is_training", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "\n", "use_one_hot_embeddings", ")", ":", "\n", "  ", "\"\"\"Creates a classification model.\"\"\"", "\n", "model", "=", "modeling", ".", "BertModel", "(", "\n", "config", "=", "bert_config", ",", "\n", "is_training", "=", "is_training", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "token_type_ids", "=", "segment_ids", ",", "\n", "use_one_hot_embeddings", "=", "use_one_hot_embeddings", ")", "\n", "\n", "final_hidden", "=", "model", ".", "get_sequence_output", "(", ")", "\n", "\n", "final_hidden_shape", "=", "modeling", ".", "get_shape_list", "(", "final_hidden", ",", "expected_rank", "=", "3", ")", "\n", "batch_size", "=", "final_hidden_shape", "[", "0", "]", "\n", "seq_length", "=", "final_hidden_shape", "[", "1", "]", "\n", "hidden_size", "=", "final_hidden_shape", "[", "2", "]", "\n", "\n", "output_weights", "=", "tf", ".", "get_variable", "(", "\n", "\"cls/squad/output_weights\"", ",", "[", "2", ",", "hidden_size", "]", ",", "\n", "initializer", "=", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "0.02", ")", ")", "\n", "\n", "output_bias", "=", "tf", ".", "get_variable", "(", "\n", "\"cls/squad/output_bias\"", ",", "[", "2", "]", ",", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "\n", "final_hidden_matrix", "=", "tf", ".", "reshape", "(", "final_hidden", ",", "\n", "[", "batch_size", "*", "seq_length", ",", "hidden_size", "]", ")", "\n", "logits", "=", "tf", ".", "matmul", "(", "final_hidden_matrix", ",", "output_weights", ",", "transpose_b", "=", "True", ")", "\n", "logits", "=", "tf", ".", "nn", ".", "bias_add", "(", "logits", ",", "output_bias", ")", "\n", "\n", "logits", "=", "tf", ".", "reshape", "(", "logits", ",", "[", "batch_size", ",", "seq_length", ",", "2", "]", ")", "\n", "logits", "=", "tf", ".", "transpose", "(", "logits", ",", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "\n", "unstacked_logits", "=", "tf", ".", "unstack", "(", "logits", ",", "axis", "=", "0", ")", "\n", "\n", "(", "start_logits", ",", "end_logits", ")", "=", "(", "unstacked_logits", "[", "0", "]", ",", "unstacked_logits", "[", "1", "]", ")", "\n", "\n", "return", "(", "start_logits", ",", "end_logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.model_fn_builder": [[590, 685], ["tensorflow.logging.info", "sorted", "run_squad.create_model", "tensorflow.trainable_variables", "tensorflow.logging.info", "features.keys", "tensorflow.logging.info", "modeling.get_assignment_map_from_checkpoint", "tensorflow.logging.info", "compute_loss"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.create_model", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.get_assignment_map_from_checkpoint"], ["", "def", "model_fn_builder", "(", "bert_config", ",", "init_checkpoint", ",", "learning_rate", ",", "\n", "num_train_steps", ",", "num_warmup_steps", ",", "use_tpu", ",", "\n", "use_one_hot_embeddings", ")", ":", "\n", "  ", "\"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"", "\n", "\n", "def", "model_fn", "(", "features", ",", "labels", ",", "mode", ",", "params", ")", ":", "# pylint: disable=unused-argument", "\n", "    ", "\"\"\"The `model_fn` for TPUEstimator.\"\"\"", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"*** Features ***\"", ")", "\n", "for", "name", "in", "sorted", "(", "features", ".", "keys", "(", ")", ")", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"  name = %s, shape = %s\"", "%", "(", "name", ",", "features", "[", "name", "]", ".", "shape", ")", ")", "\n", "\n", "", "unique_ids", "=", "features", "[", "\"unique_ids\"", "]", "\n", "input_ids", "=", "features", "[", "\"input_ids\"", "]", "\n", "input_mask", "=", "features", "[", "\"input_mask\"", "]", "\n", "segment_ids", "=", "features", "[", "\"segment_ids\"", "]", "\n", "\n", "is_training", "=", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ")", "\n", "\n", "(", "start_logits", ",", "end_logits", ")", "=", "create_model", "(", "\n", "bert_config", "=", "bert_config", ",", "\n", "is_training", "=", "is_training", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "use_one_hot_embeddings", "=", "use_one_hot_embeddings", ")", "\n", "\n", "tvars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "\n", "initialized_variable_names", "=", "{", "}", "\n", "scaffold_fn", "=", "None", "\n", "if", "init_checkpoint", ":", "\n", "      ", "(", "assignment_map", ",", "initialized_variable_names", "\n", ")", "=", "modeling", ".", "get_assignment_map_from_checkpoint", "(", "tvars", ",", "init_checkpoint", ")", "\n", "if", "use_tpu", ":", "\n", "\n", "        ", "def", "tpu_scaffold", "(", ")", ":", "\n", "          ", "tf", ".", "train", ".", "init_from_checkpoint", "(", "init_checkpoint", ",", "assignment_map", ")", "\n", "return", "tf", ".", "train", ".", "Scaffold", "(", ")", "\n", "\n", "", "scaffold_fn", "=", "tpu_scaffold", "\n", "", "else", ":", "\n", "        ", "tf", ".", "train", ".", "init_from_checkpoint", "(", "init_checkpoint", ",", "assignment_map", ")", "\n", "\n", "", "", "tf", ".", "logging", ".", "info", "(", "\"**** Trainable Variables ****\"", ")", "\n", "for", "var", "in", "tvars", ":", "\n", "      ", "init_string", "=", "\"\"", "\n", "if", "var", ".", "name", "in", "initialized_variable_names", ":", "\n", "        ", "init_string", "=", "\", *INIT_FROM_CKPT*\"", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"  name = %s, shape = %s%s\"", ",", "var", ".", "name", ",", "var", ".", "shape", ",", "\n", "init_string", ")", "\n", "\n", "", "output_spec", "=", "None", "\n", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "      ", "seq_length", "=", "modeling", ".", "get_shape_list", "(", "input_ids", ")", "[", "1", "]", "\n", "\n", "def", "compute_loss", "(", "logits", ",", "positions", ")", ":", "\n", "        ", "one_hot_positions", "=", "tf", ".", "one_hot", "(", "\n", "positions", ",", "depth", "=", "seq_length", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "log_probs", "=", "tf", ".", "nn", ".", "log_softmax", "(", "logits", ",", "axis", "=", "-", "1", ")", "\n", "loss", "=", "-", "tf", ".", "reduce_mean", "(", "\n", "tf", ".", "reduce_sum", "(", "one_hot_positions", "*", "log_probs", ",", "axis", "=", "-", "1", ")", ")", "\n", "return", "loss", "\n", "\n", "", "start_positions", "=", "features", "[", "\"start_positions\"", "]", "\n", "end_positions", "=", "features", "[", "\"end_positions\"", "]", "\n", "\n", "start_loss", "=", "compute_loss", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "compute_loss", "(", "end_logits", ",", "end_positions", ")", "\n", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2.0", "\n", "\n", "train_op", "=", "optimization", ".", "create_optimizer", "(", "\n", "total_loss", ",", "learning_rate", ",", "num_train_steps", ",", "num_warmup_steps", ",", "use_tpu", ")", "\n", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "\n", "loss", "=", "total_loss", ",", "\n", "train_op", "=", "train_op", ",", "\n", "scaffold_fn", "=", "scaffold_fn", ")", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "PREDICT", ":", "\n", "      ", "predictions", "=", "{", "\n", "\"unique_ids\"", ":", "unique_ids", ",", "\n", "\"start_logits\"", ":", "start_logits", ",", "\n", "\"end_logits\"", ":", "end_logits", ",", "\n", "}", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "predictions", "=", "predictions", ",", "scaffold_fn", "=", "scaffold_fn", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "\"Only TRAIN and PREDICT modes are supported: %s\"", "%", "(", "mode", ")", ")", "\n", "\n", "", "return", "output_spec", "\n", "\n", "", "return", "model_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.input_fn_builder": [[687, 735], ["tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.parse_single_example", "list", "tensorflow.data.TFRecordDataset", "d.shuffle.apply", "tf.parse_single_example.keys", "d.shuffle.repeat", "d.shuffle.shuffle", "tensorflow.contrib.data.map_and_batch", "tensorflow.to_int32", "run_squad.input_fn_builder._decode_record"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_pretraining._decode_record"], ["", "def", "input_fn_builder", "(", "input_file", ",", "seq_length", ",", "is_training", ",", "drop_remainder", ")", ":", "\n", "  ", "\"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"", "\n", "\n", "name_to_features", "=", "{", "\n", "\"unique_ids\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"input_ids\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"input_mask\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"segment_ids\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "}", "\n", "\n", "if", "is_training", ":", "\n", "    ", "name_to_features", "[", "\"start_positions\"", "]", "=", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "tf", ".", "int64", ")", "\n", "name_to_features", "[", "\"end_positions\"", "]", "=", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "tf", ".", "int64", ")", "\n", "\n", "", "def", "_decode_record", "(", "record", ",", "name_to_features", ")", ":", "\n", "    ", "\"\"\"Decodes a record to a TensorFlow example.\"\"\"", "\n", "example", "=", "tf", ".", "parse_single_example", "(", "record", ",", "name_to_features", ")", "\n", "\n", "# tf.Example only supports tf.int64, but the TPU only supports tf.int32.", "\n", "# So cast all int64 to int32.", "\n", "for", "name", "in", "list", "(", "example", ".", "keys", "(", ")", ")", ":", "\n", "      ", "t", "=", "example", "[", "name", "]", "\n", "if", "t", ".", "dtype", "==", "tf", ".", "int64", ":", "\n", "        ", "t", "=", "tf", ".", "to_int32", "(", "t", ")", "\n", "", "example", "[", "name", "]", "=", "t", "\n", "\n", "", "return", "example", "\n", "\n", "", "def", "input_fn", "(", "params", ")", ":", "\n", "    ", "\"\"\"The actual input function.\"\"\"", "\n", "batch_size", "=", "params", "[", "\"batch_size\"", "]", "\n", "\n", "# For training, we want a lot of parallel reading and shuffling.", "\n", "# For eval, we want no shuffling and parallel reading doesn't matter.", "\n", "d", "=", "tf", ".", "data", ".", "TFRecordDataset", "(", "input_file", ")", "\n", "if", "is_training", ":", "\n", "      ", "d", "=", "d", ".", "repeat", "(", ")", "\n", "d", "=", "d", ".", "shuffle", "(", "buffer_size", "=", "100", ")", "\n", "\n", "", "d", "=", "d", ".", "apply", "(", "\n", "tf", ".", "contrib", ".", "data", ".", "map_and_batch", "(", "\n", "lambda", "record", ":", "_decode_record", "(", "record", ",", "name_to_features", ")", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "drop_remainder", "=", "drop_remainder", ")", ")", "\n", "\n", "return", "d", "\n", "\n", "", "return", "input_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.write_predictions": [[741, 925], ["tensorflow.logging.info", "tensorflow.logging.info", "collections.defaultdict", "collections.namedtuple", "collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "enumerate", "example_index_to_features[].append", "enumerate", "sorted", "collections.namedtuple", "run_squad._compute_softmax", "enumerate", "tensorflow.gfile.GFile", "writer.write", "tensorflow.gfile.GFile", "writer.write", "run_squad._get_best_indexes", "run_squad._get_best_indexes", "sorted.append", "nbest.append", "nbest.append", "len", "total_scores.append", "collections.OrderedDict", "nbest_json.append", "len", "tensorflow.gfile.GFile", "writer.write", "collections.namedtuple.", "len", "tok_text.strip.replace", "tok_text.strip.replace", "tok_text.strip.strip", "run_squad.get_final_text", "collections.namedtuple.", "nbest.append", "collections.namedtuple.", "json.dumps", "json.dumps", "sorted.append", "tok_text.strip.split", "collections.namedtuple.", "json.dumps", "len", "len", "feature.token_is_max_context.get", "collections.namedtuple."], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad._compute_softmax", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad._get_best_indexes", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad._get_best_indexes", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.get_final_text"], ["def", "write_predictions", "(", "all_examples", ",", "all_features", ",", "all_results", ",", "n_best_size", ",", "\n", "max_answer_length", ",", "do_lower_case", ",", "output_prediction_file", ",", "\n", "output_nbest_file", ",", "output_null_log_odds_file", ")", ":", "\n", "  ", "\"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"", "\n", "tf", ".", "logging", ".", "info", "(", "\"Writing predictions to: %s\"", "%", "(", "output_prediction_file", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Writing nbest to: %s\"", "%", "(", "output_nbest_file", ")", ")", "\n", "\n", "example_index_to_features", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "feature", "in", "all_features", ":", "\n", "    ", "example_index_to_features", "[", "feature", ".", "example_index", "]", ".", "append", "(", "feature", ")", "\n", "\n", "", "unique_id_to_result", "=", "{", "}", "\n", "for", "result", "in", "all_results", ":", "\n", "    ", "unique_id_to_result", "[", "result", ".", "unique_id", "]", "=", "result", "\n", "\n", "", "_PrelimPrediction", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"PrelimPrediction\"", ",", "\n", "[", "\"feature_index\"", ",", "\"start_index\"", ",", "\"end_index\"", ",", "\"start_logit\"", ",", "\"end_logit\"", "]", ")", "\n", "\n", "all_predictions", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "all_nbest_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "scores_diff_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "\n", "for", "(", "example_index", ",", "example", ")", "in", "enumerate", "(", "all_examples", ")", ":", "\n", "    ", "features", "=", "example_index_to_features", "[", "example_index", "]", "\n", "\n", "prelim_predictions", "=", "[", "]", "\n", "# keep track of the minimum score of null start+end of position 0", "\n", "score_null", "=", "1000000", "# large and positive", "\n", "min_null_feature_index", "=", "0", "# the paragraph slice with min mull score", "\n", "null_start_logit", "=", "0", "# the start logit at the slice with min null score", "\n", "null_end_logit", "=", "0", "# the end logit at the slice with min null score", "\n", "for", "(", "feature_index", ",", "feature", ")", "in", "enumerate", "(", "features", ")", ":", "\n", "      ", "result", "=", "unique_id_to_result", "[", "feature", ".", "unique_id", "]", "\n", "start_indexes", "=", "_get_best_indexes", "(", "result", ".", "start_logits", ",", "n_best_size", ")", "\n", "end_indexes", "=", "_get_best_indexes", "(", "result", ".", "end_logits", ",", "n_best_size", ")", "\n", "# if we could have irrelevant answers, get the min score of irrelevant", "\n", "if", "FLAGS", ".", "version_2_with_negative", ":", "\n", "        ", "feature_null_score", "=", "result", ".", "start_logits", "[", "0", "]", "+", "result", ".", "end_logits", "[", "0", "]", "\n", "if", "feature_null_score", "<", "score_null", ":", "\n", "          ", "score_null", "=", "feature_null_score", "\n", "min_null_feature_index", "=", "feature_index", "\n", "null_start_logit", "=", "result", ".", "start_logits", "[", "0", "]", "\n", "null_end_logit", "=", "result", ".", "end_logits", "[", "0", "]", "\n", "", "", "for", "start_index", "in", "start_indexes", ":", "\n", "        ", "for", "end_index", "in", "end_indexes", ":", "\n", "# We could hypothetically create invalid predictions, e.g., predict", "\n", "# that the start of the span is in the question. We throw out all", "\n", "# invalid predictions.", "\n", "          ", "if", "start_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "            ", "continue", "\n", "", "if", "end_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "            ", "continue", "\n", "", "if", "start_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "            ", "continue", "\n", "", "if", "end_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "            ", "continue", "\n", "", "if", "not", "feature", ".", "token_is_max_context", ".", "get", "(", "start_index", ",", "False", ")", ":", "\n", "            ", "continue", "\n", "", "if", "end_index", "<", "start_index", ":", "\n", "            ", "continue", "\n", "", "length", "=", "end_index", "-", "start_index", "+", "1", "\n", "if", "length", ">", "max_answer_length", ":", "\n", "            ", "continue", "\n", "", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "\n", "feature_index", "=", "feature_index", ",", "\n", "start_index", "=", "start_index", ",", "\n", "end_index", "=", "end_index", ",", "\n", "start_logit", "=", "result", ".", "start_logits", "[", "start_index", "]", ",", "\n", "end_logit", "=", "result", ".", "end_logits", "[", "end_index", "]", ")", ")", "\n", "\n", "", "", "", "if", "FLAGS", ".", "version_2_with_negative", ":", "\n", "      ", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "\n", "feature_index", "=", "min_null_feature_index", ",", "\n", "start_index", "=", "0", ",", "\n", "end_index", "=", "0", ",", "\n", "start_logit", "=", "null_start_logit", ",", "\n", "end_logit", "=", "null_end_logit", ")", ")", "\n", "", "prelim_predictions", "=", "sorted", "(", "\n", "prelim_predictions", ",", "\n", "key", "=", "lambda", "x", ":", "(", "x", ".", "start_logit", "+", "x", ".", "end_logit", ")", ",", "\n", "reverse", "=", "True", ")", "\n", "\n", "_NbestPrediction", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"NbestPrediction\"", ",", "[", "\"text\"", ",", "\"start_logit\"", ",", "\"end_logit\"", "]", ")", "\n", "\n", "seen_predictions", "=", "{", "}", "\n", "nbest", "=", "[", "]", "\n", "for", "pred", "in", "prelim_predictions", ":", "\n", "      ", "if", "len", "(", "nbest", ")", ">=", "n_best_size", ":", "\n", "        ", "break", "\n", "", "feature", "=", "features", "[", "pred", ".", "feature_index", "]", "\n", "if", "pred", ".", "start_index", ">", "0", ":", "# this is a non-null prediction", "\n", "        ", "tok_tokens", "=", "feature", ".", "tokens", "[", "pred", ".", "start_index", ":", "(", "pred", ".", "end_index", "+", "1", ")", "]", "\n", "orig_doc_start", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "start_index", "]", "\n", "orig_doc_end", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "end_index", "]", "\n", "orig_tokens", "=", "example", ".", "doc_tokens", "[", "orig_doc_start", ":", "(", "orig_doc_end", "+", "1", ")", "]", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tok_tokens", ")", "\n", "\n", "# De-tokenize WordPieces that have been split off.", "\n", "tok_text", "=", "tok_text", ".", "replace", "(", "\" ##\"", ",", "\"\"", ")", "\n", "tok_text", "=", "tok_text", ".", "replace", "(", "\"##\"", ",", "\"\"", ")", "\n", "\n", "# Clean whitespace", "\n", "tok_text", "=", "tok_text", ".", "strip", "(", ")", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tok_text", ".", "split", "(", ")", ")", "\n", "orig_text", "=", "\" \"", ".", "join", "(", "orig_tokens", ")", "\n", "\n", "final_text", "=", "get_final_text", "(", "tok_text", ",", "orig_text", ",", "do_lower_case", ")", "\n", "if", "final_text", "in", "seen_predictions", ":", "\n", "          ", "continue", "\n", "\n", "", "seen_predictions", "[", "final_text", "]", "=", "True", "\n", "", "else", ":", "\n", "        ", "final_text", "=", "\"\"", "\n", "seen_predictions", "[", "final_text", "]", "=", "True", "\n", "\n", "", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "\n", "text", "=", "final_text", ",", "\n", "start_logit", "=", "pred", ".", "start_logit", ",", "\n", "end_logit", "=", "pred", ".", "end_logit", ")", ")", "\n", "\n", "# if we didn't inlude the empty option in the n-best, inlcude it", "\n", "", "if", "FLAGS", ".", "version_2_with_negative", ":", "\n", "      ", "if", "\"\"", "not", "in", "seen_predictions", ":", "\n", "        ", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "\n", "text", "=", "\"\"", ",", "start_logit", "=", "null_start_logit", ",", "\n", "end_logit", "=", "null_end_logit", ")", ")", "\n", "# In very rare edge cases we could have no valid predictions. So we", "\n", "# just create a nonce prediction in this case to avoid failure.", "\n", "", "", "if", "not", "nbest", ":", "\n", "      ", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "text", "=", "\"empty\"", ",", "start_logit", "=", "0.0", ",", "end_logit", "=", "0.0", ")", ")", "\n", "\n", "", "assert", "len", "(", "nbest", ")", ">=", "1", "\n", "\n", "total_scores", "=", "[", "]", "\n", "best_non_null_entry", "=", "None", "\n", "for", "entry", "in", "nbest", ":", "\n", "      ", "total_scores", ".", "append", "(", "entry", ".", "start_logit", "+", "entry", ".", "end_logit", ")", "\n", "if", "not", "best_non_null_entry", ":", "\n", "        ", "if", "entry", ".", "text", ":", "\n", "          ", "best_non_null_entry", "=", "entry", "\n", "\n", "", "", "", "probs", "=", "_compute_softmax", "(", "total_scores", ")", "\n", "\n", "nbest_json", "=", "[", "]", "\n", "for", "(", "i", ",", "entry", ")", "in", "enumerate", "(", "nbest", ")", ":", "\n", "      ", "output", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "output", "[", "\"text\"", "]", "=", "entry", ".", "text", "\n", "output", "[", "\"probability\"", "]", "=", "probs", "[", "i", "]", "\n", "output", "[", "\"start_logit\"", "]", "=", "entry", ".", "start_logit", "\n", "output", "[", "\"end_logit\"", "]", "=", "entry", ".", "end_logit", "\n", "nbest_json", ".", "append", "(", "output", ")", "\n", "\n", "", "assert", "len", "(", "nbest_json", ")", ">=", "1", "\n", "\n", "if", "not", "FLAGS", ".", "version_2_with_negative", ":", "\n", "      ", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "nbest_json", "[", "0", "]", "[", "\"text\"", "]", "\n", "", "else", ":", "\n", "# predict \"\" iff the null score - the score of best non-null > threshold", "\n", "      ", "score_diff", "=", "score_null", "-", "best_non_null_entry", ".", "start_logit", "-", "(", "\n", "best_non_null_entry", ".", "end_logit", ")", "\n", "scores_diff_json", "[", "example", ".", "qas_id", "]", "=", "score_diff", "\n", "if", "score_diff", ">", "FLAGS", ".", "null_score_diff_threshold", ":", "\n", "        ", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "\"\"", "\n", "", "else", ":", "\n", "        ", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "best_non_null_entry", ".", "text", "\n", "\n", "", "", "all_nbest_json", "[", "example", ".", "qas_id", "]", "=", "nbest_json", "\n", "\n", "", "with", "tf", ".", "gfile", ".", "GFile", "(", "output_prediction_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "    ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "all_predictions", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "with", "tf", ".", "gfile", ".", "GFile", "(", "output_nbest_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "    ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "all_nbest_json", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "if", "FLAGS", ".", "version_2_with_negative", ":", "\n", "    ", "with", "tf", ".", "gfile", ".", "GFile", "(", "output_null_log_odds_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "      ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "scores_diff_json", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.get_final_text": [[927, 1021], ["tokenization.BasicTokenizer", "tok_text.find", "run_squad.get_final_text._strip_spaces"], "function", ["None"], ["", "", "", "def", "get_final_text", "(", "pred_text", ",", "orig_text", ",", "do_lower_case", ")", ":", "\n", "  ", "\"\"\"Project the tokenized prediction back to the original text.\"\"\"", "\n", "\n", "# When we created the data, we kept track of the alignment between original", "\n", "# (whitespace tokenized) tokens and our WordPiece tokenized tokens. So", "\n", "# now `orig_text` contains the span of our original text corresponding to the", "\n", "# span that we predicted.", "\n", "#", "\n", "# However, `orig_text` may contain extra characters that we don't want in", "\n", "# our prediction.", "\n", "#", "\n", "# For example, let's say:", "\n", "#   pred_text = steve smith", "\n", "#   orig_text = Steve Smith's", "\n", "#", "\n", "# We don't want to return `orig_text` because it contains the extra \"'s\".", "\n", "#", "\n", "# We don't want to return `pred_text` because it's already been normalized", "\n", "# (the SQuAD eval script also does punctuation stripping/lower casing but", "\n", "# our tokenizer does additional normalization like stripping accent", "\n", "# characters).", "\n", "#", "\n", "# What we really want to return is \"Steve Smith\".", "\n", "#", "\n", "# Therefore, we have to apply a semi-complicated alignment heruistic between", "\n", "# `pred_text` and `orig_text` to get a character-to-charcter alignment. This", "\n", "# can fail in certain cases in which case we just return `orig_text`.", "\n", "\n", "def", "_strip_spaces", "(", "text", ")", ":", "\n", "    ", "ns_chars", "=", "[", "]", "\n", "ns_to_s_map", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "(", "i", ",", "c", ")", "in", "enumerate", "(", "text", ")", ":", "\n", "      ", "if", "c", "==", "\" \"", ":", "\n", "        ", "continue", "\n", "", "ns_to_s_map", "[", "len", "(", "ns_chars", ")", "]", "=", "i", "\n", "ns_chars", ".", "append", "(", "c", ")", "\n", "", "ns_text", "=", "\"\"", ".", "join", "(", "ns_chars", ")", "\n", "return", "(", "ns_text", ",", "ns_to_s_map", ")", "\n", "\n", "# We first tokenize `orig_text`, strip whitespace from the result", "\n", "# and `pred_text`, and check if they are the same length. If they are", "\n", "# NOT the same length, the heuristic has failed. If they are the same", "\n", "# length, we assume the characters are one-to-one aligned.", "\n", "", "tokenizer", "=", "tokenization", ".", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "orig_text", ")", ")", "\n", "\n", "start_position", "=", "tok_text", ".", "find", "(", "pred_text", ")", "\n", "if", "start_position", "==", "-", "1", ":", "\n", "    ", "if", "FLAGS", ".", "verbose_logging", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\n", "\"Unable to find text: '%s' in '%s'\"", "%", "(", "pred_text", ",", "orig_text", ")", ")", "\n", "", "return", "orig_text", "\n", "", "end_position", "=", "start_position", "+", "len", "(", "pred_text", ")", "-", "1", "\n", "\n", "(", "orig_ns_text", ",", "orig_ns_to_s_map", ")", "=", "_strip_spaces", "(", "orig_text", ")", "\n", "(", "tok_ns_text", ",", "tok_ns_to_s_map", ")", "=", "_strip_spaces", "(", "tok_text", ")", "\n", "\n", "if", "len", "(", "orig_ns_text", ")", "!=", "len", "(", "tok_ns_text", ")", ":", "\n", "    ", "if", "FLAGS", ".", "verbose_logging", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"Length not equal after stripping spaces: '%s' vs '%s'\"", ",", "\n", "orig_ns_text", ",", "tok_ns_text", ")", "\n", "", "return", "orig_text", "\n", "\n", "# We then project the characters in `pred_text` back to `orig_text` using", "\n", "# the character-to-character alignment.", "\n", "", "tok_s_to_ns_map", "=", "{", "}", "\n", "for", "(", "i", ",", "tok_index", ")", "in", "six", ".", "iteritems", "(", "tok_ns_to_s_map", ")", ":", "\n", "    ", "tok_s_to_ns_map", "[", "tok_index", "]", "=", "i", "\n", "\n", "", "orig_start_position", "=", "None", "\n", "if", "start_position", "in", "tok_s_to_ns_map", ":", "\n", "    ", "ns_start_position", "=", "tok_s_to_ns_map", "[", "start_position", "]", "\n", "if", "ns_start_position", "in", "orig_ns_to_s_map", ":", "\n", "      ", "orig_start_position", "=", "orig_ns_to_s_map", "[", "ns_start_position", "]", "\n", "\n", "", "", "if", "orig_start_position", "is", "None", ":", "\n", "    ", "if", "FLAGS", ".", "verbose_logging", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"Couldn't map start position\"", ")", "\n", "", "return", "orig_text", "\n", "\n", "", "orig_end_position", "=", "None", "\n", "if", "end_position", "in", "tok_s_to_ns_map", ":", "\n", "    ", "ns_end_position", "=", "tok_s_to_ns_map", "[", "end_position", "]", "\n", "if", "ns_end_position", "in", "orig_ns_to_s_map", ":", "\n", "      ", "orig_end_position", "=", "orig_ns_to_s_map", "[", "ns_end_position", "]", "\n", "\n", "", "", "if", "orig_end_position", "is", "None", ":", "\n", "    ", "if", "FLAGS", ".", "verbose_logging", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"Couldn't map end position\"", ")", "\n", "", "return", "orig_text", "\n", "\n", "", "output_text", "=", "orig_text", "[", "orig_start_position", ":", "(", "orig_end_position", "+", "1", ")", "]", "\n", "return", "output_text", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad._get_best_indexes": [[1023, 1033], ["sorted", "range", "enumerate", "len", "best_indexes.append"], "function", ["None"], ["", "def", "_get_best_indexes", "(", "logits", ",", "n_best_size", ")", ":", "\n", "  ", "\"\"\"Get the n-best logits from a list.\"\"\"", "\n", "index_and_score", "=", "sorted", "(", "enumerate", "(", "logits", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "best_indexes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "index_and_score", ")", ")", ":", "\n", "    ", "if", "i", ">=", "n_best_size", ":", "\n", "      ", "break", "\n", "", "best_indexes", ".", "append", "(", "index_and_score", "[", "i", "]", "[", "0", "]", ")", "\n", "", "return", "best_indexes", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad._compute_softmax": [[1035, 1056], ["math.exp", "exp_scores.append", "probs.append"], "function", ["None"], ["", "def", "_compute_softmax", "(", "scores", ")", ":", "\n", "  ", "\"\"\"Compute softmax probability over raw logits.\"\"\"", "\n", "if", "not", "scores", ":", "\n", "    ", "return", "[", "]", "\n", "\n", "", "max_score", "=", "None", "\n", "for", "score", "in", "scores", ":", "\n", "    ", "if", "max_score", "is", "None", "or", "score", ">", "max_score", ":", "\n", "      ", "max_score", "=", "score", "\n", "\n", "", "", "exp_scores", "=", "[", "]", "\n", "total_sum", "=", "0.0", "\n", "for", "score", "in", "scores", ":", "\n", "    ", "x", "=", "math", ".", "exp", "(", "score", "-", "max_score", ")", "\n", "exp_scores", ".", "append", "(", "x", ")", "\n", "total_sum", "+=", "x", "\n", "\n", "", "probs", "=", "[", "]", "\n", "for", "score", "in", "exp_scores", ":", "\n", "    ", "probs", ".", "append", "(", "score", "/", "total_sum", ")", "\n", "", "return", "probs", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.validate_flags_or_throw": [[1097, 1124], ["tokenization.validate_case_matches_checkpoint", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.tokenization.validate_case_matches_checkpoint"], ["", "", "def", "validate_flags_or_throw", "(", "bert_config", ")", ":", "\n", "  ", "\"\"\"Validate the input FLAGS or throw an exception.\"\"\"", "\n", "tokenization", ".", "validate_case_matches_checkpoint", "(", "FLAGS", ".", "do_lower_case", ",", "\n", "FLAGS", ".", "init_checkpoint", ")", "\n", "\n", "if", "not", "FLAGS", ".", "do_train", "and", "not", "FLAGS", ".", "do_predict", ":", "\n", "    ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_predict` must be True.\"", ")", "\n", "\n", "", "if", "FLAGS", ".", "do_train", ":", "\n", "    ", "if", "not", "FLAGS", ".", "train_file", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "\"If `do_train` is True, then `train_file` must be specified.\"", ")", "\n", "", "", "if", "FLAGS", ".", "do_predict", ":", "\n", "    ", "if", "not", "FLAGS", ".", "predict_file", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "\"If `do_predict` is True, then `predict_file` must be specified.\"", ")", "\n", "\n", "", "", "if", "FLAGS", ".", "max_seq_length", ">", "bert_config", ".", "max_position_embeddings", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"Cannot use sequence length %d because the BERT model \"", "\n", "\"was only trained up to sequence length %d\"", "%", "\n", "(", "FLAGS", ".", "max_seq_length", ",", "bert_config", ".", "max_position_embeddings", ")", ")", "\n", "\n", "", "if", "FLAGS", ".", "max_seq_length", "<=", "FLAGS", ".", "max_query_length", "+", "3", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"The max_seq_length (%d) must be greater than max_query_length \"", "\n", "\"(%d) + 3\"", "%", "(", "FLAGS", ".", "max_seq_length", ",", "FLAGS", ".", "max_query_length", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.main": [[1126, 1277], ["tensorflow.logging.set_verbosity", "modeling.BertConfig.from_json_file", "run_squad.validate_flags_or_throw", "tensorflow.gfile.MakeDirs", "tokenization.FullTokenizer", "tensorflow.contrib.tpu.RunConfig", "run_squad.model_fn_builder", "tensorflow.contrib.tpu.TPUEstimator", "tensorflow.contrib.cluster_resolver.TPUClusterResolver", "run_squad.read_squad_examples", "int", "int", "random.Random", "random.Random.shuffle", "run_squad.FeatureWriter", "run_squad.convert_examples_to_features", "run_squad.FeatureWriter.close", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "run_squad.input_fn_builder", "tf.contrib.tpu.TPUEstimator.train", "run_squad.read_squad_examples", "run_squad.FeatureWriter", "run_squad.convert_examples_to_features", "run_squad.FeatureWriter.close", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "run_squad.input_fn_builder", "tf.contrib.tpu.TPUEstimator.predict", "os.path.join", "os.path.join", "os.path.join", "run_squad.write_predictions", "tensorflow.contrib.tpu.TPUConfig", "len", "eval_features.append", "run_squad.FeatureWriter.process_feature", "len", "len", "int", "all_results.append", "os.path.join", "os.path.join", "tensorflow.logging.info", "float", "float", "RawResult", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.modeling.BertConfig.from_json_file", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.validate_flags_or_throw", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.model_fn_builder", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.read_squad_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.convert_examples_to_features", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.FeatureWriter.close", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.input_fn_builder", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.read_squad_examples", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.convert_examples_to_features", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.FeatureWriter.close", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.input_fn_builder", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.write_predictions", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.FeatureWriter.process_feature"], ["", "", "def", "main", "(", "_", ")", ":", "\n", "  ", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "\n", "bert_config", "=", "modeling", ".", "BertConfig", ".", "from_json_file", "(", "FLAGS", ".", "bert_config_file", ")", "\n", "\n", "validate_flags_or_throw", "(", "bert_config", ")", "\n", "\n", "tf", ".", "gfile", ".", "MakeDirs", "(", "FLAGS", ".", "output_dir", ")", "\n", "\n", "tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "FLAGS", ".", "vocab_file", ",", "do_lower_case", "=", "FLAGS", ".", "do_lower_case", ")", "\n", "\n", "tpu_cluster_resolver", "=", "None", "\n", "if", "FLAGS", ".", "use_tpu", "and", "FLAGS", ".", "tpu_name", ":", "\n", "    ", "tpu_cluster_resolver", "=", "tf", ".", "contrib", ".", "cluster_resolver", ".", "TPUClusterResolver", "(", "\n", "FLAGS", ".", "tpu_name", ",", "zone", "=", "FLAGS", ".", "tpu_zone", ",", "project", "=", "FLAGS", ".", "gcp_project", ")", "\n", "\n", "", "is_per_host", "=", "tf", ".", "contrib", ".", "tpu", ".", "InputPipelineConfig", ".", "PER_HOST_V2", "\n", "run_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "RunConfig", "(", "\n", "cluster", "=", "tpu_cluster_resolver", ",", "\n", "master", "=", "FLAGS", ".", "master", ",", "\n", "model_dir", "=", "FLAGS", ".", "output_dir", ",", "\n", "save_checkpoints_steps", "=", "FLAGS", ".", "save_checkpoints_steps", ",", "\n", "tpu_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUConfig", "(", "\n", "iterations_per_loop", "=", "FLAGS", ".", "iterations_per_loop", ",", "\n", "num_shards", "=", "FLAGS", ".", "num_tpu_cores", ",", "\n", "per_host_input_for_training", "=", "is_per_host", ")", ")", "\n", "\n", "train_examples", "=", "None", "\n", "num_train_steps", "=", "None", "\n", "num_warmup_steps", "=", "None", "\n", "if", "FLAGS", ".", "do_train", ":", "\n", "    ", "train_examples", "=", "read_squad_examples", "(", "\n", "input_file", "=", "FLAGS", ".", "train_file", ",", "is_training", "=", "True", ")", "\n", "num_train_steps", "=", "int", "(", "\n", "len", "(", "train_examples", ")", "/", "FLAGS", ".", "train_batch_size", "*", "FLAGS", ".", "num_train_epochs", ")", "\n", "num_warmup_steps", "=", "int", "(", "num_train_steps", "*", "FLAGS", ".", "warmup_proportion", ")", "\n", "\n", "# Pre-shuffle the input to avoid having to make a very large shuffle", "\n", "# buffer in in the `input_fn`.", "\n", "rng", "=", "random", ".", "Random", "(", "12345", ")", "\n", "rng", ".", "shuffle", "(", "train_examples", ")", "\n", "\n", "", "model_fn", "=", "model_fn_builder", "(", "\n", "bert_config", "=", "bert_config", ",", "\n", "init_checkpoint", "=", "FLAGS", ".", "init_checkpoint", ",", "\n", "learning_rate", "=", "FLAGS", ".", "learning_rate", ",", "\n", "num_train_steps", "=", "num_train_steps", ",", "\n", "num_warmup_steps", "=", "num_warmup_steps", ",", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "use_one_hot_embeddings", "=", "FLAGS", ".", "use_tpu", ")", "\n", "\n", "# If TPU is not available, this will fall back to normal Estimator on CPU", "\n", "# or GPU.", "\n", "estimator", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimator", "(", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "model_fn", "=", "model_fn", ",", "\n", "config", "=", "run_config", ",", "\n", "train_batch_size", "=", "FLAGS", ".", "train_batch_size", ",", "\n", "predict_batch_size", "=", "FLAGS", ".", "predict_batch_size", ")", "\n", "\n", "if", "FLAGS", ".", "do_train", ":", "\n", "# We write to a temporary file to avoid storing very large constant tensors", "\n", "# in memory.", "\n", "    ", "train_writer", "=", "FeatureWriter", "(", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"train.tf_record\"", ")", ",", "\n", "is_training", "=", "True", ")", "\n", "convert_examples_to_features", "(", "\n", "examples", "=", "train_examples", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "max_seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "doc_stride", "=", "FLAGS", ".", "doc_stride", ",", "\n", "max_query_length", "=", "FLAGS", ".", "max_query_length", ",", "\n", "is_training", "=", "True", ",", "\n", "output_fn", "=", "train_writer", ".", "process_feature", ")", "\n", "train_writer", ".", "close", "(", ")", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num orig examples = %d\"", ",", "len", "(", "train_examples", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num split examples = %d\"", ",", "train_writer", ".", "num_features", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Batch size = %d\"", ",", "FLAGS", ".", "train_batch_size", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_steps", ")", "\n", "del", "train_examples", "\n", "\n", "train_input_fn", "=", "input_fn_builder", "(", "\n", "input_file", "=", "train_writer", ".", "filename", ",", "\n", "seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "is_training", "=", "True", ",", "\n", "drop_remainder", "=", "True", ")", "\n", "estimator", ".", "train", "(", "input_fn", "=", "train_input_fn", ",", "max_steps", "=", "num_train_steps", ")", "\n", "\n", "", "if", "FLAGS", ".", "do_predict", ":", "\n", "    ", "eval_examples", "=", "read_squad_examples", "(", "\n", "input_file", "=", "FLAGS", ".", "predict_file", ",", "is_training", "=", "False", ")", "\n", "\n", "eval_writer", "=", "FeatureWriter", "(", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"eval.tf_record\"", ")", ",", "\n", "is_training", "=", "False", ")", "\n", "eval_features", "=", "[", "]", "\n", "\n", "def", "append_feature", "(", "feature", ")", ":", "\n", "      ", "eval_features", ".", "append", "(", "feature", ")", "\n", "eval_writer", ".", "process_feature", "(", "feature", ")", "\n", "\n", "", "convert_examples_to_features", "(", "\n", "examples", "=", "eval_examples", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "max_seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "doc_stride", "=", "FLAGS", ".", "doc_stride", ",", "\n", "max_query_length", "=", "FLAGS", ".", "max_query_length", ",", "\n", "is_training", "=", "False", ",", "\n", "output_fn", "=", "append_feature", ")", "\n", "eval_writer", ".", "close", "(", ")", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** Running predictions *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num orig examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num split examples = %d\"", ",", "len", "(", "eval_features", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Batch size = %d\"", ",", "FLAGS", ".", "predict_batch_size", ")", "\n", "\n", "all_results", "=", "[", "]", "\n", "\n", "predict_input_fn", "=", "input_fn_builder", "(", "\n", "input_file", "=", "eval_writer", ".", "filename", ",", "\n", "seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "is_training", "=", "False", ",", "\n", "drop_remainder", "=", "False", ")", "\n", "\n", "# If running eval on the TPU, you will need to specify the number of", "\n", "# steps.", "\n", "all_results", "=", "[", "]", "\n", "for", "result", "in", "estimator", ".", "predict", "(", "\n", "predict_input_fn", ",", "yield_single_examples", "=", "True", ")", ":", "\n", "      ", "if", "len", "(", "all_results", ")", "%", "1000", "==", "0", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"Processing example: %d\"", "%", "(", "len", "(", "all_results", ")", ")", ")", "\n", "", "unique_id", "=", "int", "(", "result", "[", "\"unique_ids\"", "]", ")", "\n", "start_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "result", "[", "\"start_logits\"", "]", ".", "flat", "]", "\n", "end_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "result", "[", "\"end_logits\"", "]", ".", "flat", "]", "\n", "all_results", ".", "append", "(", "\n", "RawResult", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "start_logits", "=", "start_logits", ",", "\n", "end_logits", "=", "end_logits", ")", ")", "\n", "\n", "", "output_prediction_file", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"predictions.json\"", ")", "\n", "output_nbest_file", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"nbest_predictions.json\"", ")", "\n", "output_null_log_odds_file", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"null_odds.json\"", ")", "\n", "\n", "write_predictions", "(", "eval_examples", ",", "eval_features", ",", "all_results", ",", "\n", "FLAGS", ".", "n_best_size", ",", "FLAGS", ".", "max_answer_length", ",", "\n", "FLAGS", ".", "do_lower_case", ",", "output_prediction_file", ",", "\n", "output_nbest_file", ",", "output_null_log_odds_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.tables.common.parse_file": [[11, 18], ["open", "parse.parse", "print", "f.read"], "function", ["None"], ["def", "parse_file", "(", "fname", ",", "template", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "with", "open", "(", "fname", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "return", "parse", "(", "template", ",", "f", ".", "read", "(", ")", ")", "\n", "", "", "except", "FileNotFoundError", ":", "\n", "        ", "print", "(", "f\"Missing {fname}\"", ")", "\n", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.tables.common.grid_search_eval": [[19, 40], ["max", "eval_entries.append", "train_losses.append", "common.parse_file", "common.parse_file", "eval_path_fn", "float", "grid_search_res.append", "eval_path_fn"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.tables.common.parse_file", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.tables.common.parse_file"], ["", "", "def", "grid_search_eval", "(", "eval_path_fn", ")", ":", "\n", "    ", "\"\"\"Given eval_path_fn(task, lr), searches over some eval results and\n    returns the best one for each task\"\"\"", "\n", "eval_entries", ",", "train_losses", "=", "[", "]", ",", "[", "]", "\n", "for", "task", "in", "[", "'MNLI'", ",", "'QQP'", ",", "'QNLI'", ",", "'SST-2'", ",", "'CoLA'", "]", ":", "# 'MRPC', 'RTE', 'STS-B'", "\n", "# Select the model with the best accuracy among the grid search", "\n", "        ", "grid_search_res", "=", "[", "]", "\n", "for", "lr", "in", "[", "'2e-5'", ",", "'3e-5'", ",", "'4e-5'", ",", "'5e-5'", "]", ":", "\n", "            ", "downstream_results", "=", "parse_file", "(", "eval_path_fn", "(", "task", ",", "lr", ")", ",", "EVAL_RESULTS_TEMPLATE", ")", "\n", "train_eval", "=", "parse_file", "(", "eval_path_fn", "(", "task", ",", "lr", ")", "[", ":", "-", "16", "]", "+", "'eval_train_results.txt'", ",", "EVAL_RESULTS_TEMPLATE", ")", "\n", "train_loss", "=", "train_eval", "[", "'loss'", "]", "if", "train_eval", "else", "float", "(", "'inf'", ")", "\n", "train_step", "=", "train_eval", "[", "'step'", "]", "if", "train_eval", "else", "0", "\n", "if", "downstream_results", ":", "\n", "                ", "grid_search_res", ".", "append", "(", "(", "downstream_results", "[", "'eval_accuracy'", "]", ",", "train_loss", ",", "lr", ",", "downstream_results", "[", "'step'", "]", ",", "train_step", ")", ")", "\n", "\n", "", "", "best", "=", "max", "(", "grid_search_res", ",", "default", "=", "(", "0", ",", "0", ",", "''", ",", "0", ",", "0", ")", ")", "\n", "# print(f'Using {task} with lr {best[2]} eval_step {best[3]} train_step {best[4]}')", "\n", "eval_entries", ".", "append", "(", "best", "[", "0", "]", ")", "\n", "train_losses", ".", "append", "(", "best", "[", "1", "]", ")", "\n", "\n", "", "return", "eval_entries", ",", "train_losses", "\n", "", ""]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.delete_all_but_last.delete_all_but_last": [[7, 22], ["tensorflow.python.training.checkpoint_management.latest_checkpoint", "glob.glob", "re.match().group", "path.startswith", "print", "re.match", "os.remove", "print"], "function", ["None"], ["def", "delete_all_but_last", "(", "*", "model_dirs", ")", ":", "\n", "    ", "\"\"\"Deletes all checkpoints except for the last one\"\"\"", "\n", "for", "model_dir", "in", "model_dirs", ":", "\n", "        ", "latest", "=", "checkpoint_management", ".", "latest_checkpoint", "(", "model_dir", ")", "\n", "try", ":", "\n", "            ", "prefix", "=", "re", ".", "match", "(", "r'(.*)-\\d*$'", ",", "latest", ")", ".", "group", "(", "1", ")", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "", "for", "path", "in", "glob", ".", "glob", "(", "prefix", "+", "'*'", ")", ":", "\n", "            ", "if", "not", "path", ".", "startswith", "(", "latest", ")", ":", "\n", "                ", "print", "(", "f'Removing {path}'", ")", "\n", "try", ":", "\n", "                    ", "os", ".", "remove", "(", "path", ")", "\n", "", "except", ":", "\n", "                    ", "print", "(", "f'Unable to remvoe {path}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.diff_masks.diff_masks": [[6, 21], ["tensorflow.train.list_variables", "var_name.endswith", "tensorflow.contrib.framework.load_variable", "tensorflow.contrib.framework.load_variable", "numpy.sum"], "function", ["None"], ["fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "\n", "\n", "for", "task", "in", "[", "'MNLI'", ",", "'QQP'", ",", "'QNLI'", ",", "'SST-2'", ",", "'CoLA'", "]", ":", "# 'STS-B'", "\n", "    ", "print", "(", ")", "\n", "print", "(", "task", ")", "\n", "x", "=", "[", "]", "\n", "y", "=", "[", "]", "\n", "for", "sparsity", "in", "[", "0", ",", ".1", ",", ".2", ",", ".3", ",", ".4", ",", ".5", ",", ".6", ",", ".7", ",", ".8", ",", ".9", "]", ":", "\n", "        ", "print", "(", ")", "\n", "print", "(", "f'SPARSITY {sparsity}'", ")", "\n", "for", "lr", "in", "[", "'2e-5'", ",", "'3e-5'", ",", "'4e-5'", ",", "'5e-5'", "]", ":", "\n", "            ", "mask_diff_res", "=", "parse_file", "(", "\n", "f'models/{task}/downstream_prune_{int(sparsity*100)}_lr_{lr}/mask_diff.txt'", ",", "\n", "'{diff:f}'", ")", "\n", "\n", "if", "not", "mask_diff_res", ":", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.common.prune": [[4, 10], ["numpy.abs", "int", "numpy.partition", "np.abs.flatten"], "function", ["None"], ["import", "os", "\n", "\n", "EVAL_RESULTS_TEMPLATE", "=", "\"\"\"eval_accuracy = {eval_accuracy:f}\neval_loss = {eval_loss:f}\nglobal_step = {step:d}\nloss = {loss:f}\"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.weight_graphs.anim_max": [[16, 18], ["None"], "function", ["None"], ["def", "anim_max", "(", "tensor", ",", "model_dir", ",", "var_name", ")", ":", "\n", "    ", "pass", "# Ressurectable from git history", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.weight_graphs.abs_value_sort": [[19, 27], ["numpy.sort", "numpy.arange", "matplotlib.subplots", "ax.scatter", "ax.set_ylim", "fig.savefig", "matplotlib.close", "numpy.abs", "tensor.flatten"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.FeatureWriter.close"], ["", "def", "abs_value_sort", "(", "tensor", ",", "model_dir", ",", "var_name", ")", ":", "\n", "    ", "y", "=", "np", ".", "sort", "(", "np", ".", "abs", "(", "tensor", ".", "flatten", "(", ")", ")", ")", "\n", "x", "=", "np", ".", "arange", "(", "tensor", ".", "size", ")", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "\n", "ax", ".", "scatter", "(", "x", ",", "y", ",", "s", "=", "1", ")", "\n", "ax", ".", "set_ylim", "(", "(", "0", ",", "1", ")", ")", "\n", "fig", ".", "savefig", "(", "f'graphs/weights/{model_dir}/{var_name}_sort.png'", ")", "\n", "plt", ".", "close", "(", "fig", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.weight_graphs.hist": [[28, 42], ["numpy.abs", "matplotlib.subplots", "ax.hist", "ax.set_ylim", "fig.savefig", "matplotlib.close", "tensor.flatten", "matplotlib.subplots", "ax.set_ylim", "ax.hist", "fig.savefig", "matplotlib.close", "tensor.flatten"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.weight_graphs.hist", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.FeatureWriter.close", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.weight_graphs.hist", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.FeatureWriter.close"], ["", "def", "hist", "(", "tensor", ",", "model_dir", ",", "var_name", ")", ":", "\n", "    ", "y", "=", "np", ".", "abs", "(", "tensor", ".", "flatten", "(", ")", ")", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "\n", "ax", ".", "hist", "(", "y", ",", "density", "=", "True", ",", "range", "=", "(", "0", ",", "0.5", ")", ",", "bins", "=", "100", ")", "\n", "ax", ".", "set_ylim", "(", "(", "0", ",", "25", ")", ")", "\n", "fig", ".", "savefig", "(", "f'graphs/weights/{model_dir}/{var_name}_hist.png'", ")", "\n", "plt", ".", "close", "(", "fig", ")", "\n", "\n", "y", "=", "tensor", ".", "flatten", "(", ")", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "\n", "ax", ".", "set_ylim", "(", "(", "0", ",", "12", ")", ")", "\n", "ax", ".", "hist", "(", "y", ",", "density", "=", "True", ",", "range", "=", "(", "-", "0.5", ",", "0.5", ")", ",", "bins", "=", "100", ")", "\n", "fig", ".", "savefig", "(", "f'graphs/weights/{model_dir}/{var_name}_hist_signed.png'", ")", "\n", "plt", ".", "close", "(", "fig", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.weight_graphs.plot_3d": [[43, 45], ["None"], "function", ["None"], ["", "def", "plot_3d", "(", "tensor", ",", "model_dir", ",", "var_name", ")", ":", "\n", "    ", "pass", "# Ressurectable from git history", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.weight_graphs.max_k": [[46, 48], ["None"], "function", ["None"], ["", "def", "max_k", "(", "tensor", ",", "k", ",", "model_dir", ",", "var_name", ",", "reverse", "=", "False", ")", ":", "\n", "    ", "pass", "# Ressurectable from git history", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.weight_graphs.heatmap": [[49, 57], ["matplotlib.subplots", "seaborn.heatmap", "ax.set_title", "ax.set_ylabel", "ax.set_xlabel", "fig.savefig", "matplotlib.close", "numpy.abs"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.weight_graphs.heatmap", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.FeatureWriter.close"], ["", "def", "heatmap", "(", "tensor", ",", "model_dir", ",", "var_name", ")", ":", "\n", "    ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "figsize", "=", "(", "15", ",", "15", ")", ")", "\n", "sns", ".", "heatmap", "(", "np", ".", "abs", "(", "tensor", ")", ",", "vmax", "=", "0.1", ",", "cmap", "=", "\"YlGnBu\"", ",", "ax", "=", "ax", ")", "\n", "ax", ".", "set_title", "(", "'Parameter Matrix Magnitude Heatmap'", ",", "fontsize", "=", "20", ")", "\n", "ax", ".", "set_ylabel", "(", "'Row'", ",", "fontsize", "=", "20", ")", "\n", "ax", ".", "set_xlabel", "(", "'Column'", ",", "fontsize", "=", "20", ")", "\n", "fig", ".", "savefig", "(", "f'graphs/weights/{model_dir}/{var_name}_heatmap.png'", ")", "\n", "plt", ".", "close", "(", "fig", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.weight_graphs.attention_head_plot": [[64, 102], ["print", "matplotlib.subplots", "numpy.array", "numpy.abs", "ax.errorbar", "ax.set_title", "ax.set_xlabel", "ax.set_ylabel", "fig.savefig", "matplotlib.close", "print", "numpy.zeros", "range", "prune_percents.flatten.flatten", "matplotlib.subplots", "ax2.hist", "fig2.savefig", "matplotlib.close", "y_avg.append", "y_min.append", "y_max.append", "tuple", "tuple", "range", "numpy.mean", "numpy.min", "numpy.max", "checkpoint_utils.prune_attn_heads.attn_head_weight", "checkpoint_utils.prune_attn_heads.attn_head_weight", "checkpoint_utils.common.prune", "numpy.ones_like", "checkpoint_utils.prune_attn_heads.params_for_attn"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.FeatureWriter.close", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.weight_graphs.hist", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.FeatureWriter.close", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.prune_attn_heads.attn_head_weight", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.prune_attn_heads.attn_head_weight", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.common.prune", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.prune_attn_heads.params_for_attn"], ["", "def", "attention_head_plot", "(", "ledger", ",", "model_dir", ",", "layers", "=", "12", ",", "heads", "=", "12", ")", ":", "\n", "    ", "print", "(", "f\"Warning: assuming BERT has {layers} layers and {heads} attention heads\"", ")", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "\n", "sparsities", "=", "[", "0", ",", ".1", ",", ".2", ",", ".3", ",", ".4", ",", ".5", ",", ".6", ",", ".7", ",", ".8", ",", ".9", "]", "\n", "y_avg", "=", "[", "]", "\n", "y_min", "=", "[", "]", "\n", "y_max", "=", "[", "]", "\n", "for", "sparsity", "in", "sparsities", ":", "\n", "        ", "print", "(", "sparsity", ")", "\n", "\n", "prune_percents", "=", "np", ".", "zeros", "(", "(", "12", ",", "12", ")", ")", "# 12 layers, 12 heads each", "\n", "for", "layer", "in", "range", "(", "12", ")", ":", "\n", "            ", "parameter_masks", "=", "tuple", "(", "prune", "(", "mat", ",", "sparsity", ")", "for", "mat", "in", "params_for_attn", "(", "ledger", ",", "layer", ")", ")", "\n", "ones", "=", "tuple", "(", "np", ".", "ones_like", "(", "mat", ")", "for", "mat", "in", "parameter_masks", ")", "\n", "\n", "for", "head", "in", "range", "(", "12", ")", ":", "\n", "                ", "not_pruned", "=", "attn_head_weight", "(", "*", "parameter_masks", ",", "head", ")", "\n", "total", "=", "attn_head_weight", "(", "*", "ones", ",", "head", ")", "\n", "prune_percents", "[", "layer", ",", "head", "]", "=", "(", "total", "-", "not_pruned", ")", "/", "total", "\n", "\n", "", "", "prune_percents", "=", "prune_percents", ".", "flatten", "(", ")", "\n", "fig2", ",", "ax2", "=", "plt", ".", "subplots", "(", ")", "\n", "ax2", ".", "hist", "(", "prune_percents", ",", "range", "=", "(", "0", ",", "1", ")", ",", "bins", "=", "100", ")", "\n", "fig2", ".", "savefig", "(", "f'graphs/weights/{model_dir}/attn_head_pruning_{sparsity}.png'", ")", "\n", "plt", ".", "close", "(", "fig2", ")", "\n", "y_avg", ".", "append", "(", "np", ".", "mean", "(", "prune_percents", ")", ")", "\n", "y_min", ".", "append", "(", "np", ".", "min", "(", "prune_percents", ")", ")", "\n", "y_max", ".", "append", "(", "np", ".", "max", "(", "prune_percents", ")", ")", "\n", "\n", "# The size of the error bars, not the min and max", "\n", "", "y_err", "=", "np", ".", "array", "(", "[", "y_min", ",", "y_max", "]", ")", "\n", "y_err", "=", "np", ".", "abs", "(", "y_err", "-", "y_avg", ")", "\n", "ax", ".", "errorbar", "(", "sparsities", ",", "y_avg", ",", "yerr", "=", "y_err", ")", "\n", "ax", ".", "set_title", "(", "'% of Individual Attention Head Pruned'", ")", "\n", "ax", ".", "set_xlabel", "(", "'Sparsity'", ")", "\n", "ax", ".", "set_ylabel", "(", "'% of Attn Head Pruned'", ")", "\n", "fig", ".", "savefig", "(", "f'graphs/weights/{model_dir}/attn_head_pruning.png'", ")", "\n", "plt", ".", "close", "(", "fig", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.weight_graphs.pruned_weights_sum": [[103, 129], ["numpy.linspace", "matplotlib.subplots", "ax.plot", "ax.plot", "ax.set_title", "ax.set_xlabel", "ax.set_ylabel", "ax.plot", "ax.legend", "fig.savefig", "matplotlib.close", "ledger.items", "y1.append", "y2.append", "numpy.sum", "numpy.sum", "checkpoint_utils.common.prune", "numpy.abs"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.bert.run_squad.FeatureWriter.close", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.common.prune"], ["", "def", "pruned_weights_sum", "(", "ledger", ",", "model_dir", ")", ":", "\n", "    ", "sparsities", "=", "np", ".", "linspace", "(", "0", ",", "1", ",", "endpoint", "=", "False", ")", "\n", "y1", "=", "[", "]", "\n", "y2", "=", "[", "]", "\n", "for", "sparsity", "in", "sparsities", ":", "\n", "        ", "sum_abs", "=", "0", "\n", "sum_", "=", "0", "\n", "for", "var_name", ",", "tensor", "in", "ledger", ".", "items", "(", ")", ":", "\n", "            ", "pruned", "=", "prune", "(", "tensor", ",", "sparsity", ")", "==", "0", "\n", "sum_abs", "+=", "np", ".", "sum", "(", "np", ".", "abs", "(", "tensor", "[", "mask", "]", ")", ")", "\n", "sum_", "+=", "np", ".", "sum", "(", "tensor", "[", "mask", "]", ")", "\n", "\n", "", "y1", ".", "append", "(", "sum_abs", ")", "\n", "y2", ".", "append", "(", "sum_", ")", "\n", "\n", "", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "\n", "ax", ".", "plot", "(", "sparsities", ",", "y1", ",", "label", "=", "'Sum of Absolute'", ")", "\n", "ax", ".", "plot", "(", "sparsities", ",", "y2", ",", "label", "=", "'Sum of Signed'", ")", "\n", "ax", ".", "set_title", "(", "'Sum of Weights Pruned'", ")", "\n", "ax", ".", "set_xlabel", "(", "'Sparsity'", ")", "\n", "ax", ".", "set_ylabel", "(", "'Sum'", ")", "\n", "# ax.set_ylim((-60000, 60000))", "\n", "ax", ".", "plot", "(", "[", "0", ",", "1", "]", ",", "[", "0", ",", "0", "]", ",", "'--'", ",", "label", "=", "'y=0'", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "fig", ".", "savefig", "(", "f'graphs/weights/{model_dir}/sum_weights_pruned.png'", ")", "\n", "plt", ".", "close", "(", "fig", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.weight_graphs.main": [[132, 158], ["weight_graphs.attention_head_plot", "os.makedirs", "open", "tensorflow.train.list_variables", "re.sub.endswith", "re.sub.endswith", "tensorflow.contrib.framework.load_variable", "print", "re.sub.replace", "re.sub"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.weight_graphs.attention_head_plot"], ["def", "main", "(", "model_dir", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "os", ".", "makedirs", "(", "f'graphs/weights/{model_dir}'", ")", "\n", "", "except", "FileExistsError", ":", "\n", "        ", "pass", "\n", "", "with", "open", "(", "f'graphs/weights/{model_dir}/normals.txt'", ",", "'w+'", ")", "as", "normals_f", ":", "\n", "        ", "for", "var_name", ",", "_", "in", "tf", ".", "train", ".", "list_variables", "(", "model_dir", ")", ":", "\n", "            ", "if", "var_name", ".", "endswith", "(", "'/weights'", ")", "or", "var_name", ".", "endswith", "(", "'/mask'", ")", ":", "# or var_name.endswith('/word_embeddings'):", "\n", "                ", "tensor", "=", "tf", ".", "contrib", ".", "framework", ".", "load_variable", "(", "model_dir", ",", "var_name", ")", "\n", "ledger", "[", "var_name", "]", "=", "tensor", "\n", "print", "(", "var_name", ")", "\n", "\n", "# print(var_name, end=' & ', file=normals_f)", "\n", "# print(f'{np.mean(tensor):.4f}', end=' & ', file=normals_f)", "\n", "# print(f'{np.std(tensor):.3f}', file=normals_f)", "\n", "\n", "# Make the variable name more like a filename.", "\n", "var_name", "=", "var_name", ".", "replace", "(", "'/'", ",", "'_'", ")", "\n", "# Make all layer numbers two digits wide so the filenames sort nicely", "\n", "var_name", "=", "re", ".", "sub", "(", "r'_(\\d)_'", ",", "r'_0\\1_'", ",", "var_name", ")", "\n", "\n", "\n", "# hist(tensor, model_dir, var_name)", "\n", "# heatmap(tensor, model_dir, var_name)", "\n", "\n", "", "", "", "attention_head_plot", "(", "ledger", ",", "model_dir", ")", "\n", "# pruned_weights_sum(ledger, model_dir)", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.random_masks.random_masks": [[9, 35], ["model_dir.rstrip.rstrip", "tensorflow.Session", "tensorflow.train.list_variables", "tensorflow.train.Saver", "sess.run", "os.mkdir", "tf.train.Saver.save", "tensorflow.contrib.framework.load_variable", "var_name.endswith", "tensorflow.global_variables_initializer", "os.path.join", "int", "numpy.concatenate", "numpy.random.shuffle", "new_mask.reshape().astype.reshape().astype", "tensorflow.Variable", "tensorflow.Variable", "numpy.zeros", "numpy.ones", "new_mask.reshape().astype.reshape"], "function", ["None"], ["def", "random_masks", "(", "model_dir", ",", "out_dir", ",", "sparsity", ":", "float", ")", ":", "\n", "    ", "\"\"\"Prunes a random [sparsity] of of weights in each matrix of [model_dir].\n    Makes a new checkpoint [out_dir].\n    \"\"\"", "\n", "model_dir", "=", "model_dir", ".", "rstrip", "(", "'/'", ")", "\n", "\n", "with", "tf", ".", "Session", "(", ")", "as", "sess", ":", "\n", "\n", "# Load all the variables from the checkpoint", "\n", "        ", "for", "var_name", ",", "_", "in", "tf", ".", "train", ".", "list_variables", "(", "model_dir", ")", ":", "\n", "            ", "tensor", "=", "tf", ".", "contrib", ".", "framework", ".", "load_variable", "(", "model_dir", ",", "var_name", ")", "\n", "\n", "if", "var_name", ".", "endswith", "(", "'/mask'", ")", ":", "\n", "                ", "num_zeros", "=", "int", "(", "tensor", ".", "size", "*", "sparsity", ")", "\n", "new_mask", "=", "np", ".", "concatenate", "(", "(", "np", ".", "zeros", "(", "num_zeros", ")", ",", "np", ".", "ones", "(", "tensor", ".", "size", "-", "num_zeros", ")", ")", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "new_mask", ")", "\n", "new_mask", "=", "new_mask", ".", "reshape", "(", "tensor", ".", "shape", ")", ".", "astype", "(", "tensor", ".", "dtype", ")", "\n", "var", "=", "tf", ".", "Variable", "(", "new_mask", ",", "name", "=", "var_name", ")", "\n", "", "else", ":", "\n", "                ", "var", "=", "tf", ".", "Variable", "(", "tensor", ",", "name", "=", "var_name", ")", "\n", "\n", "# Save these new variables", "\n", "", "", "saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "\n", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "os", ".", "mkdir", "(", "out_dir", ")", "\n", "saver", ".", "save", "(", "sess", ",", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'random_prune.ckpt'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.compare_features.layers": [[6, 24], ["open", "open", "zip", "json.loads", "json.loads", "zip", "zip", "numpy.array", "numpy.array"], "function", ["None"], ["def", "layers", "(", "first_fname", ",", "second_fname", ")", ":", "\n", "    ", "\"\"\"Returns an iterator over pairs of layer activations from BERT feature files\"\"\"", "\n", "first_file", "=", "open", "(", "first_fname", ")", "\n", "second_file", "=", "open", "(", "second_fname", ")", "\n", "for", "first_line", ",", "second_line", "in", "zip", "(", "first_file", ",", "second_file", ")", ":", "\n", "        ", "first_json", "=", "json", ".", "loads", "(", "first_line", ")", "\n", "second_json", "=", "json", ".", "loads", "(", "second_line", ")", "\n", "\n", "for", "first_feature", ",", "second_feature", "in", "zip", "(", "first_json", "[", "'features'", "]", ",", "second_json", "[", "'features'", "]", ")", ":", "\n", "            ", "assert", "first_feature", "[", "'token'", "]", "==", "second_feature", "[", "'token'", "]", "\n", "\n", "for", "first_layer", ",", "second_layer", "in", "zip", "(", "first_feature", "[", "'layers'", "]", ",", "second_feature", "[", "'layers'", "]", ")", ":", "\n", "                ", "assert", "first_layer", "[", "'index'", "]", "==", "second_layer", "[", "'index'", "]", "\n", "\n", "first_vec", "=", "np", ".", "array", "(", "first_layer", "[", "'values'", "]", ")", "\n", "second_vec", "=", "np", ".", "array", "(", "second_layer", "[", "'values'", "]", ")", "\n", "\n", "yield", "first_vec", ",", "second_vec", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.compare_features.main": [[25, 43], ["compare_features.layers", "print", "numpy.dot", "print", "print", "numpy.linalg.norm", "numpy.linalg.norm"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.compare_features.layers"], ["", "", "", "", "def", "main", "(", "first_fname", ",", "second_fname", ")", ":", "\n", "    ", "total_sim", "=", "0", "\n", "layer_count", "=", "0", "\n", "\n", "for", "first_vec", ",", "second_vec", "in", "layers", "(", "first_fname", ",", "second_fname", ")", ":", "\n", "# dist = np.linalg.norm(first_vec - second_vec)", "\n", "        ", "cos", "=", "np", ".", "dot", "(", "first_vec", ",", "second_vec", ")", "/", "(", "np", ".", "linalg", ".", "norm", "(", "first_vec", ")", "*", "np", ".", "linalg", ".", "norm", "(", "second_vec", ")", ")", "\n", "\n", "total_sim", "+=", "cos", "\n", "layer_count", "+=", "1", "\n", "if", "layer_count", "%", "10000", "==", "0", ":", "\n", "            ", "print", "(", "layer_count", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "", "if", "layer_count", ">", "300000", ":", "\n", "            ", "print", "(", "'Passed 300k layers, ignoring the rest.'", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "break", "\n", "# print(f\"{first_feature['token']} ({first_layer['index']}): {cos}\")", "\n", "\n", "", "", "print", "(", "f'{total_sim} / {layer_count} = {total_sim / layer_count}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.prune_attn_heads.params_for_attn": [[12, 19], ["None"], "function", ["None"], ["def", "params_for_attn", "(", "ledger", ",", "layer", ",", "masks", "=", "False", ")", ":", "\n", "    ", "end", "=", "'mask'", "if", "masks", "else", "'weights'", "\n", "return", "(", "\n", "ledger", "[", "f'bert/encoder/layer_{layer}/attention/self/key/{end}'", "]", ",", "\n", "ledger", "[", "f'bert/encoder/layer_{layer}/attention/self/query/{end}'", "]", ",", "\n", "ledger", "[", "f'bert/encoder/layer_{layer}/attention/self/value/{end}'", "]", ",", "\n", "ledger", "[", "f'bert/encoder/layer_{layer}/attention/output/fully_connected/{end}'", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.prune_attn_heads.extract_single_head": [[21, 27], ["all", "tuple"], "function", ["None"], ["", "def", "extract_single_head", "(", "key", ",", "query", ",", "value", ",", "FC", ",", "head_ind", ")", ":", "\n", "    ", "assert", "all", "(", "[", "tensor", ".", "shape", "==", "(", "768", ",", "768", ")", "for", "tensor", "in", "[", "key", ",", "query", ",", "value", ",", "FC", "]", "]", ")", "\n", "\n", "return", "tuple", "(", "tensor", "[", ":", ",", "SIZE_PER_HEAD", "*", "head_ind", ":", "SIZE_PER_HEAD", "*", "(", "head_ind", "+", "1", ")", "]", "\n", "for", "tensor", "in", "[", "key", ",", "query", ",", "value", "]", "\n", ")", "+", "(", "FC", "[", "SIZE_PER_HEAD", "*", "head_ind", ":", "SIZE_PER_HEAD", "*", "(", "head_ind", "+", "1", ")", ",", ":", "]", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.prune_attn_heads.attn_head_weight": [[29, 35], ["prune_attn_heads.extract_single_head", "numpy.sum", "numpy.abs"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.prune_attn_heads.extract_single_head"], ["", "def", "attn_head_weight", "(", "key", ",", "query", ",", "value", ",", "FC", ",", "head_ind", ")", ":", "\n", "    ", "total", "=", "0", "\n", "for", "tensor", "in", "extract_single_head", "(", "key", ",", "query", ",", "value", ",", "FC", ",", "head_ind", ")", ":", "\n", "        ", "total", "+=", "np", ".", "sum", "(", "np", ".", "abs", "(", "tensor", ")", ")", "\n", "\n", "", "return", "total", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.prune_attn_heads.prune_single_head": [[37, 44], ["prune_attn_heads.params_for_attn"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.prune_attn_heads.params_for_attn"], ["", "def", "prune_single_head", "(", "ledger", ",", "layer", ",", "head", ")", ":", "\n", "    ", "key", ",", "query", ",", "value", ",", "FC", "=", "params_for_attn", "(", "ledger", ",", "layer", ",", "masks", "=", "True", ")", "\n", "\n", "for", "tensor", "in", "[", "key", ",", "query", ",", "value", "]", ":", "\n", "        ", "tensor", "[", ":", ",", "SIZE_PER_HEAD", "*", "head", ":", "SIZE_PER_HEAD", "*", "(", "head", "+", "1", ")", "]", "=", "0", "\n", "\n", "", "FC", "[", "SIZE_PER_HEAD", "*", "head", ":", "SIZE_PER_HEAD", "*", "(", "head", "+", "1", ")", ",", ":", "]", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.prune_attn_heads.prune_attn_heads": [[46, 90], ["model_dir.rstrip.rstrip", "tensorflow.Session", "tensorflow.train.list_variables", "numpy.zeros", "range", "numpy.sum", "checkpoint_utils.common.prune", "numpy.nonzero", "zip", "ledger.items", "tensorflow.train.Saver", "sess.run", "os.mkdir", "tf.train.Saver.save", "tensorflow.contrib.framework.load_variable", "prune_attn_heads.params_for_attn", "range", "prune_attn_heads.prune_single_head", "tensorflow.Variable", "tensorflow.global_variables_initializer", "os.path.join", "prune_attn_heads.attn_head_weight"], "function", ["home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.common.prune", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.prune_attn_heads.params_for_attn", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.prune_attn_heads.prune_single_head", "home.repos.pwc.inspect_result.mitchellgordon95_bert-prune.checkpoint_utils.prune_attn_heads.attn_head_weight"], ["", "def", "prune_attn_heads", "(", "model_dir", ",", "out_dir", ",", "sparsity", ":", "float", ")", ":", "\n", "    ", "\"\"\"Prunes [sparsity] of the number of attention heads in [model_dir].\n    Makes a new checkpoint [out_dir]\n    \"\"\"", "\n", "model_dir", "=", "model_dir", ".", "rstrip", "(", "'/'", ")", "\n", "\n", "with", "tf", ".", "Session", "(", ")", "as", "sess", ":", "\n", "\n", "# Load all the variables from the checkpoint", "\n", "        ", "ledger", "=", "{", "}", "\n", "for", "var_name", ",", "_", "in", "tf", ".", "train", ".", "list_variables", "(", "model_dir", ")", ":", "\n", "            ", "ledger", "[", "var_name", "]", "=", "tf", ".", "contrib", ".", "framework", ".", "load_variable", "(", "model_dir", ",", "var_name", ")", "\n", "\n", "", "head_weights", "=", "np", ".", "zeros", "(", "(", "12", ",", "12", ")", ")", "# 12 layers, 12 heads each", "\n", "# layer_stds = np.zeros(12) # 12 layers", "\n", "for", "layer", "in", "range", "(", "12", ")", ":", "\n", "            ", "params", "=", "params_for_attn", "(", "ledger", ",", "layer", ")", "\n", "# layer_stds[layer] = np.std(np.concatenate(params).flatten())", "\n", "for", "head", "in", "range", "(", "12", ")", ":", "\n", "                ", "head_weights", "[", "layer", ",", "head", "]", "=", "attn_head_weight", "(", "*", "params", ",", "head", ")", "\n", "\n", "# layer_stds /= np.linalg.norm(layer_stds, keepdims=True)", "\n", "\n", "# TODO: normalize by layer", "\n", "", "", "layer_norms", "=", "np", ".", "sum", "(", "head_weights", ",", "axis", "=", "1", ",", "keepdims", "=", "True", ")", "\n", "# layer_norms = np.linalg.norm(head_weights, axis=1, keepdims=True)", "\n", "head_weights", "/=", "layer_norms", "\n", "# head_weights *= layer_stds", "\n", "mask", "=", "prune", "(", "head_weights", ",", "sparsity", ")", "\n", "\n", "# Non-zero gives us the indices of non-zero elements like", "\n", "# ([row indices], [column indices])", "\n", "to_prune", "=", "np", ".", "nonzero", "(", "mask", "==", "0", ")", "\n", "for", "layer", ",", "head", "in", "zip", "(", "to_prune", "[", "0", "]", ",", "to_prune", "[", "1", "]", ")", ":", "\n", "            ", "prune_single_head", "(", "ledger", ",", "layer", ",", "head", ")", "\n", "\n", "", "for", "var_name", ",", "var_tensor_np", "in", "ledger", ".", "items", "(", ")", ":", "\n", "            ", "var", "=", "tf", ".", "Variable", "(", "var_tensor_np", ",", "name", "=", "var_name", ")", "\n", "\n", "# Save these new variables", "\n", "", "saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "\n", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "os", ".", "mkdir", "(", "out_dir", ")", "\n", "saver", ".", "save", "(", "sess", ",", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'head_pruned.ckpt'", ")", ")", "\n", "\n"]]}