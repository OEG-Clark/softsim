{"home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_engspan.read_jsonlines": [[10, 17], ["print", "jsonlines.open", "lines.append"], "function", ["None"], ["def", "read_jsonlines", "(", "eval_file_name", ")", ":", "\n", "    ", "lines", "=", "[", "]", "\n", "print", "(", "\"loading examples from {0}\"", ".", "format", "(", "eval_file_name", ")", ")", "\n", "with", "jsonlines", ".", "open", "(", "eval_file_name", ")", "as", "reader", ":", "\n", "        ", "for", "obj", "in", "reader", ":", "\n", "            ", "lines", ".", "append", "(", "obj", ")", "\n", "", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_engspan.normalize_answer": [[19, 35], ["eval_xor_engspan.normalize_answer.white_space_fix"], "function", ["None"], ["", "def", "normalize_answer", "(", "s", ")", ":", "\n", "    ", "\"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"", "\n", "def", "remove_articles", "(", "text", ")", ":", "\n", "        ", "return", "re", ".", "sub", "(", "r'\\b(a|an|the)\\b'", ",", "' '", ",", "text", ")", "\n", "\n", "", "def", "white_space_fix", "(", "text", ")", ":", "\n", "        ", "return", "' '", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "\n", "", "def", "remove_punc", "(", "text", ")", ":", "\n", "        ", "exclude", "=", "set", "(", "string", ".", "punctuation", ")", "\n", "return", "''", ".", "join", "(", "ch", "for", "ch", "in", "text", "if", "ch", "not", "in", "exclude", ")", "\n", "\n", "", "def", "lower", "(", "text", ")", ":", "\n", "        ", "return", "text", ".", "lower", "(", ")", "\n", "\n", "", "return", "white_space_fix", "(", "remove_articles", "(", "remove_punc", "(", "lower", "(", "s", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_engspan.f1_score": [[37, 49], ["normalize_answer().split", "normalize_answer().split", "sum", "collections.Counter", "collections.Counter", "common.values", "len", "len", "eval_xor_engspan.normalize_answer", "eval_xor_engspan.normalize_answer"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.normalize_answer", "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.normalize_answer"], ["", "def", "f1_score", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "prediction_tokens", "=", "normalize_answer", "(", "prediction", ")", ".", "split", "(", ")", "\n", "ground_truth_tokens", "=", "normalize_answer", "(", "ground_truth", ")", ".", "split", "(", ")", "\n", "common", "=", "Counter", "(", "prediction_tokens", ")", "&", "Counter", "(", "ground_truth_tokens", ")", "\n", "num_same", "=", "sum", "(", "common", ".", "values", "(", ")", ")", "\n", "if", "num_same", "==", "0", ":", "\n", "        ", "return", "0", ",", "0", ",", "0", "\n", "", "precision", "=", "1.0", "*", "num_same", "/", "len", "(", "prediction_tokens", ")", "\n", "recall", "=", "1.0", "*", "num_same", "/", "len", "(", "ground_truth_tokens", ")", "\n", "f1", "=", "(", "2", "*", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", ")", "\n", "#print(precision)", "\n", "return", "f1", ",", "precision", ",", "recall", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_engspan.exact_match_score": [[51, 53], ["eval_xor_engspan.normalize_answer", "eval_xor_engspan.normalize_answer"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.normalize_answer", "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.normalize_answer"], ["", "def", "exact_match_score", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "return", "(", "normalize_answer", "(", "prediction", ")", "==", "normalize_answer", "(", "ground_truth", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_engspan.metric_max_over_ground_truths": [[55, 71], ["scores_for_ground_truths.append", "max", "scores_prec.append", "scores_recall.append", "max", "max", "max", "eval_xor_engspan.exact_match_score", "eval_xor_engspan.f1_score"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.data.qa_validation.exact_match_score", "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.f1_score"], ["", "def", "metric_max_over_ground_truths", "(", "metric_fn", ",", "prediction", ",", "ground_truths", ")", ":", "\n", "    ", "scores_for_ground_truths", "=", "[", "]", "\n", "scores_prec", "=", "[", "]", "\n", "scores_recall", "=", "[", "]", "\n", "for", "ground_truth", "in", "ground_truths", ":", "\n", "        ", "if", "metric_fn", "==", "f1_score", ":", "\n", "            ", "score", ",", "prec", ",", "recall", "=", "metric_fn", "(", "prediction", ",", "ground_truth", ")", "\n", "scores_prec", ".", "append", "(", "prec", ")", "\n", "scores_recall", ".", "append", "(", "recall", ")", "\n", "", "else", ":", "\n", "            ", "score", "=", "metric_fn", "(", "prediction", ",", "ground_truth", ")", "\n", "", "scores_for_ground_truths", ".", "append", "(", "score", ")", "\n", "", "if", "metric_fn", "==", "f1_score", ":", "\n", "        ", "return", "max", "(", "scores_for_ground_truths", ")", ",", "max", "(", "scores_prec", ")", ",", "max", "(", "scores_recall", ")", "\n", "", "else", ":", "\n", "        ", "return", "max", "(", "scores_for_ground_truths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_engspan.evaluate": [[73, 103], ["eval_xor_engspan.metric_max_over_ground_truths", "eval_xor_engspan.metric_max_over_ground_truths", "print", "print", "isinstance"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.metric_max_over_ground_truths", "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.metric_max_over_ground_truths"], ["", "", "def", "evaluate", "(", "dataset", ",", "predictions", ",", "per_lang", "=", "False", ",", "lang", "=", "\"ja\"", ")", ":", "\n", "    ", "f1", "=", "exact_match", "=", "total", "=", "0", "\n", "precision", "=", "recall", "=", "0", "\n", "for", "qa", "in", "dataset", ":", "\n", "        ", "if", "per_lang", "is", "True", "and", "lang", "!=", "qa", "[", "\"lang\"", "]", ":", "\n", "            ", "continue", "\n", "", "total", "+=", "1", "\n", "if", "qa", "[", "'id'", "]", "not", "in", "predictions", ":", "\n", "            ", "message", "=", "'Unanswered question '", "+", "qa", "[", "'id'", "]", "+", "' will receive score 0.'", "\n", "print", "(", "message", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "continue", "\n", "\n", "", "ground_truths", "=", "qa", "[", "\"answers\"", "]", "\n", "prediction", "=", "predictions", "[", "qa", "[", "'id'", "]", "]", "[", "\"answer\"", "]", "if", "isinstance", "(", "predictions", "[", "qa", "[", "'id'", "]", "]", ",", "dict", ")", "else", "predictions", "[", "qa", "[", "'id'", "]", "]", "\n", "exact_match", "+=", "metric_max_over_ground_truths", "(", "\n", "exact_match_score", ",", "prediction", ",", "ground_truths", ")", "\n", "f1_tmp", ",", "prec_tmp", ",", "recall_tmp", "=", "metric_max_over_ground_truths", "(", "\n", "f1_score", ",", "prediction", ",", "ground_truths", ")", "\n", "f1", "+=", "f1_tmp", "\n", "precision", "+=", "prec_tmp", "\n", "recall", "+=", "recall_tmp", "\n", "", "if", "total", "==", "0", ":", "\n", "        ", "print", "(", "\"no examples in this language\"", ")", "\n", "return", "False", "\n", "", "exact_match", "=", "100.0", "*", "exact_match", "/", "total", "\n", "f1", "=", "100.0", "*", "f1", "/", "total", "\n", "precision", "=", "100.0", "*", "precision", "/", "total", "\n", "recall", "=", "100.0", "*", "recall", "/", "total", "\n", "return", "{", "'exact_match'", ":", "exact_match", ",", "'f1'", ":", "f1", ",", "\"precision\"", ":", "precision", ",", "\"recall\"", ":", "recall", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.read_jsonlines": [[25, 32], ["print", "jsonlines.open", "lines.append"], "function", ["None"], ["def", "read_jsonlines", "(", "eval_file_name", ")", ":", "\n", "    ", "lines", "=", "[", "]", "\n", "print", "(", "\"loading examples from {0}\"", ".", "format", "(", "eval_file_name", ")", ")", "\n", "with", "jsonlines", ".", "open", "(", "eval_file_name", ")", "as", "reader", ":", "\n", "        ", "for", "obj", "in", "reader", ":", "\n", "            ", "lines", ".", "append", "(", "obj", ")", "\n", "", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.load_tydi_answer": [[33, 39], ["eval_xor_full.read_jsonlines"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.read_jsonlines"], ["", "def", "load_tydi_answer", "(", "tydi_eval_open_domain_data", ")", ":", "\n", "    ", "answer_dict", "=", "{", "}", "\n", "eval_data", "=", "read_jsonlines", "(", "tydi_eval_open_domain_data", ")", "\n", "for", "item", "in", "eval_data", ":", "\n", "        ", "answer_dict", "[", "item", "[", "\"id\"", "]", "]", "=", "item", "[", "\"answers\"", "]", "\n", "", "return", "answer_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.normalize_answer": [[41, 57], ["eval_xor_full.normalize_answer.white_space_fix"], "function", ["None"], ["", "def", "normalize_answer", "(", "s", ")", ":", "\n", "# TODO: should we keep those counter removal? ", "\n", "    ", "def", "remove_counter", "(", "text", ")", ":", "\n", "        ", "return", "text", ".", "replace", "(", "\"\u5e74\"", ",", "\"\"", ")", ".", "replace", "(", "\"\u6b73\"", ",", "\"\"", ")", ".", "replace", "(", "\"\u4eba\"", ",", "\"\"", ")", ".", "replace", "(", "\"\ub144\"", ",", "\"\"", ")", "\n", "\n", "", "def", "white_space_fix", "(", "text", ")", ":", "\n", "        ", "return", "' '", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "\n", "", "def", "remove_punc", "(", "text", ")", ":", "\n", "        ", "exclude", "=", "set", "(", "string", ".", "punctuation", ")", "\n", "return", "''", ".", "join", "(", "ch", "for", "ch", "in", "text", "if", "ch", "not", "in", "exclude", ")", "\n", "\n", "", "def", "lower", "(", "text", ")", ":", "\n", "        ", "return", "text", ".", "lower", "(", ")", "\n", "\n", "", "return", "white_space_fix", "(", "remove_counter", "(", "remove_punc", "(", "lower", "(", "s", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.exact_match_score": [[59, 61], ["eval_xor_full.normalize_answer", "eval_xor_full.normalize_answer"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.normalize_answer", "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.normalize_answer"], ["", "def", "exact_match_score", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "return", "(", "normalize_answer", "(", "prediction", ")", "==", "normalize_answer", "(", "ground_truth", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.f1_score": [[63, 75], ["normalize_answer().split", "normalize_answer().split", "sum", "collections.Counter", "collections.Counter", "common.values", "len", "len", "eval_xor_full.normalize_answer", "eval_xor_full.normalize_answer"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.normalize_answer", "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.normalize_answer"], ["", "def", "f1_score", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "prediction_tokens", "=", "normalize_answer", "(", "prediction", ")", ".", "split", "(", ")", "\n", "ground_truth_tokens", "=", "normalize_answer", "(", "ground_truth", ")", ".", "split", "(", ")", "\n", "common", "=", "Counter", "(", "prediction_tokens", ")", "&", "Counter", "(", "ground_truth_tokens", ")", "\n", "num_same", "=", "sum", "(", "common", ".", "values", "(", ")", ")", "\n", "\n", "if", "num_same", "==", "0", ":", "\n", "        ", "return", "0", "\n", "", "precision", "=", "1.0", "*", "num_same", "/", "len", "(", "prediction_tokens", ")", "\n", "recall", "=", "1.0", "*", "num_same", "/", "len", "(", "ground_truth_tokens", ")", "\n", "f1", "=", "(", "2", "*", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", ")", "\n", "return", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.metric_max_over_ground_truths": [[77, 83], ["max", "scores_for_ground_truths.append", "eval_xor_full.f1_score", "eval_xor_full.exact_match_score"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.f1_score", "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.qa_validation.exact_match_score"], ["", "def", "metric_max_over_ground_truths", "(", "metric_fn", ",", "prediction", ",", "ground_truths", ")", ":", "\n", "    ", "scores_for_ground_truths", "=", "[", "]", "\n", "for", "ground_truth", "in", "ground_truths", ":", "\n", "        ", "score", "=", "metric_fn", "(", "prediction", ",", "ground_truth", ")", "\n", "scores_for_ground_truths", ".", "append", "(", "score", ")", "\n", "", "return", "max", "(", "scores_for_ground_truths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.calculate_f1_em_bleu": [[86, 126], ["lang_dict.items", "isinstance", "eval_xor_full.metric_max_over_ground_truths", "nltk.translate.bleu", "eval_xor_full.metric_max_over_ground_truths", "lang_dic.values", "print", "wakati.parse", "wakati.parse", "final_gts.append", "pred.replace().replace", "pred.replace"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.metric_max_over_ground_truths", "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.metric_max_over_ground_truths"], ["", "def", "calculate_f1_em_bleu", "(", "dataset", ",", "predictions", ")", ":", "\n", "    ", "lang_dict", "=", "{", "lang", ":", "{", "\"count\"", ":", "0", ",", "\"f1\"", ":", "0", ",", "\"bleu\"", ":", "0", ",", "\"em\"", ":", "0", "}", "\n", "for", "lang", "in", "lang_dic", ".", "values", "(", ")", "}", "\n", "\n", "\n", "for", "qa", "in", "dataset", ":", "\n", "        ", "lang", "=", "qa", "[", "\"lang\"", "]", "\n", "gts", "=", "qa", "[", "\"answers\"", "]", "\n", "q_id", "=", "qa", "[", "\"id\"", "]", "\n", "lang_dict", "[", "lang", "]", "[", "\"count\"", "]", "+=", "1", "\n", "if", "q_id", "not", "in", "predictions", ":", "\n", "            ", "print", "(", "\"no answers\"", ")", "\n", "continue", "\n", "", "pred", "=", "predictions", "[", "q_id", "]", "\n", "if", "isinstance", "(", "gts", ",", "str", ")", ":", "\n", "            ", "gts", "=", "[", "gts", "]", "\n", "\n", "", "final_gts", "=", "[", "]", "\n", "# for japanese, we need to tokenize the input as there are no white spaces.", "\n", "if", "lang", "==", "\"ja\"", ":", "\n", "            ", "for", "gt", "in", "gts", ":", "\n", "                ", "gt", "=", "wakati", ".", "parse", "(", "gt", ")", "\n", "final_gts", ".", "append", "(", "gt", ")", "\n", "", "final_pred", "=", "wakati", ".", "parse", "(", "pred", ".", "replace", "(", "\"\u30fb\"", ",", "\" \"", ")", ".", "replace", "(", "\"\u3001\"", ",", "\",\"", ")", ")", "\n", "", "else", ":", "\n", "            ", "final_gts", "=", "gts", "\n", "final_pred", "=", "pred", "\n", "", "lang_dict", "[", "lang", "]", "[", "\"f1\"", "]", "+=", "metric_max_over_ground_truths", "(", "\n", "f1_score", ",", "final_pred", ",", "final_gts", ")", "\n", "lang_dict", "[", "lang", "]", "[", "\"bleu\"", "]", "+=", "bleu", "(", "final_gts", ",", "pred", ")", "\n", "lang_dict", "[", "lang", "]", "[", "\"em\"", "]", "+=", "metric_max_over_ground_truths", "(", "\n", "exact_match_score", ",", "final_pred", ",", "final_gts", ")", "\n", "# finalize scores", "\n", "", "for", "lang", ",", "scores", "in", "lang_dict", ".", "items", "(", ")", ":", "\n", "        ", "if", "scores", "[", "\"count\"", "]", "==", "0", ":", "\n", "            ", "continue", "\n", "", "for", "score_key", "in", "scores", ":", "\n", "            ", "if", "\"count\"", "!=", "score_key", ":", "\n", "                ", "lang_dict", "[", "lang", "]", "[", "score_key", "]", "=", "scores", "[", "score_key", "]", "/", "scores", "[", "\"count\"", "]", "\n", "", "", "", "return", "lang_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.main": [[127, 155], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "eval_xor_full.read_jsonlines", "eval_xor_full.calculate_f1_em_bleu", "print", "print", "print", "open", "json.load", "print", "print", "id.split", "json.load.items"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.read_jsonlines", "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_full.calculate_f1_em_bleu"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--data_file\"", ",", "\n", "default", "=", "None", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--pred_file\"", ",", "\n", "default", "=", "None", ",", "type", "=", "str", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "# load dpr results", "\n", "dataset", "=", "read_jsonlines", "(", "args", ".", "data_file", ")", "\n", "with", "open", "(", "args", ".", "pred_file", ")", "as", "prediction_file", ":", "\n", "        ", "predictions", "=", "json", ".", "load", "(", "prediction_file", ")", "\n", "", "predictions", "=", "{", "id", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", ":", "pred", "for", "id", ",", "pred", "in", "predictions", ".", "items", "(", ")", "}", "\n", "\n", "results", "=", "calculate_f1_em_bleu", "(", "dataset", ",", "predictions", ")", "\n", "\n", "f1_total", ",", "em_total", ",", "bleu_total", "=", "0.0", ",", "0.0", ",", "0.0", "\n", "total_num", "=", "0", "\n", "for", "lang", "in", "results", ":", "\n", "        ", "f1_total", "+=", "results", "[", "lang", "]", "[", "\"f1\"", "]", "\n", "em_total", "+=", "results", "[", "lang", "]", "[", "\"em\"", "]", "\n", "bleu_total", "+=", "results", "[", "lang", "]", "[", "\"bleu\"", "]", "\n", "total_num", "+=", "results", "[", "lang", "]", "[", "\"count\"", "]", "\n", "print", "(", "\"Evaluating the performance on {}\"", ".", "format", "(", "lang", ")", ")", "\n", "print", "(", "\"F1: {0}, EM:{1}, BLEU:{2}\"", ".", "format", "(", "results", "[", "lang", "]", "[", "\"f1\"", "]", "*", "100", ",", "results", "[", "lang", "]", "[", "\"em\"", "]", "*", "100", ",", "results", "[", "lang", "]", "[", "\"bleu\"", "]", "*", "100", ")", ")", "\n", "", "print", "(", "\"avg f1: {}\"", ".", "format", "(", "f1_total", "/", "7", "*", "100", ")", ")", "\n", "print", "(", "\"avg em: {}\"", ".", "format", "(", "em_total", "/", "7", "*", "100", ")", ")", "\n", "print", "(", "\"avg bleu: {}\"", ".", "format", "(", "bleu_total", "/", "7", "*", "100", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_retrieve.read_jsonlines": [[9, 16], ["print", "jsonlines.open", "lines.append"], "function", ["None"], ["def", "read_jsonlines", "(", "eval_file_name", ")", ":", "\n", "    ", "lines", "=", "[", "]", "\n", "print", "(", "\"loading examples from {0}\"", ".", "format", "(", "eval_file_name", ")", ")", "\n", "with", "jsonlines", ".", "open", "(", "eval_file_name", ")", "as", "reader", ":", "\n", "        ", "for", "obj", "in", "reader", ":", "\n", "            ", "lines", ".", "append", "(", "obj", ")", "\n", "", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_retrieve.evaluate_top_k_hit": [[18, 60], ["tqdm.tqdm", "per_lang.setdefault", "len", "nltk.tokenize.word_tokenize", "per_lang.items", "span_answers.append", "len"], "function", ["None"], ["", "def", "evaluate_top_k_hit", "(", "results", ",", "gt_answers", ",", "max_token_num", "=", "5000", ",", "difficulty_dict", "=", "{", "}", ")", ":", "\n", "    ", "per_lang", "=", "{", "}", "\n", "for", "item", "in", "tqdm", "(", "results", ")", ":", "\n", "        ", "q_id", "=", "item", "[", "\"id\"", "]", "\n", "lang", "=", "item", "[", "\"lang\"", "]", "\n", "per_lang", ".", "setdefault", "(", "lang", ",", "{", "\"count\"", ":", "0", ",", "\"hit\"", ":", "0", "}", ")", "\n", "ctxs", "=", "item", "[", "\"ctxs\"", "]", "\n", "\n", "if", "q_id", "not", "in", "gt_answers", ":", "\n", "            ", "continue", "\n", "\n", "", "answers", "=", "gt_answers", "[", "q_id", "]", "\n", "\n", "span_answers", "=", "[", "]", "\n", "# Skip yes/no examples during XOR-Retrieve evaluations", "\n", "for", "answer", "in", "answers", ":", "\n", "            ", "if", "answer", "not", "in", "[", "\"yes\"", ",", "\"no\"", "]", ":", "\n", "                ", "span_answers", ".", "append", "(", "answer", ")", "\n", "", "", "if", "len", "(", "span_answers", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "\n", "", "per_lang", "[", "lang", "]", "[", "\"count\"", "]", "+=", "1", "\n", "\n", "concat_string_tokens", "=", "[", "]", "\n", "for", "ctx_text", "in", "ctxs", ":", "\n", "            ", "tokenized_text", "=", "word_tokenize", "(", "ctx_text", ")", "\n", "concat_string_tokens", "+=", "tokenized_text", "\n", "if", "len", "(", "concat_string_tokens", ")", ">=", "max_token_num", ":", "\n", "                ", "break", "\n", "", "", "concat_string_tokens", "=", "concat_string_tokens", "[", ":", "max_token_num", "]", "\n", "concat_string", "=", "\" \"", ".", "join", "(", "concat_string_tokens", ")", "\n", "hit", "=", "False", "\n", "for", "answer", "in", "span_answers", ":", "\n", "            ", "if", "answer", "in", "concat_string", ":", "\n", "                ", "hit", "=", "True", "\n", "", "", "if", "hit", "is", "True", ":", "\n", "            ", "per_lang", "[", "lang", "]", "[", "\"hit\"", "]", "+=", "1", "\n", "\n", "", "", "final_results", "=", "{", "lang", ":", "result", "for", "lang", ",", "\n", "result", "in", "per_lang", ".", "items", "(", ")", "if", "result", "[", "\"count\"", "]", ">", "0", "}", "\n", "\n", "return", "final_results", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_retrieve.main": [[62, 90], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "json.load", "eval_xor_retrieve.read_jsonlines", "open", "print", "eval_xor_retrieve.evaluate_top_k_hit", "print", "print", "print", "print", "avg_scores.append", "statistics.mean"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.read_jsonlines", "home.repos.pwc.inspect_result.AkariAsai_XORQA.evals.eval_xor_retrieve.evaluate_top_k_hit"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--data_file\"", ",", "\n", "default", "=", "None", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--pred_file\"", ",", "\n", "default", "=", "None", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_token_num\"", ",", "\n", "default", "=", "5000", ",", "type", "=", "int", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "predictions", "=", "json", ".", "load", "(", "open", "(", "args", ".", "pred_file", ")", ")", "\n", "input_data", "=", "read_jsonlines", "(", "args", ".", "data_file", ")", "\n", "# convert input open-domain data into the qid2answer dictionary", "\n", "qid2answers", "=", "{", "item", "[", "\"id\"", "]", ":", "item", "[", "\"answers\"", "]", "for", "item", "in", "input_data", "}", "\n", "for", "topk", "in", "[", "2", ",", "5", "]", ":", "\n", "        ", "print", "(", "\"Evaluating R@{}kt\"", ".", "format", "(", "topk", ")", ")", "\n", "pred_per_lang_results", "=", "evaluate_top_k_hit", "(", "\n", "predictions", ",", "qid2answers", ",", "topk", "*", "1000", ")", "\n", "avg_scores", "=", "[", "]", "\n", "for", "lang", "in", "pred_per_lang_results", ":", "\n", "            ", "print", "(", "\"performance on {0} ({1} examples)\"", ".", "format", "(", "lang", ",", "pred_per_lang_results", "[", "lang", "]", "[", "\"count\"", "]", ")", ")", "\n", "per_lang_score", "=", "(", "pred_per_lang_results", "[", "lang", "]", "[", "\"hit\"", "]", "/", "pred_per_lang_results", "[", "lang", "]", "[", "\"count\"", "]", ")", "*", "100", "\n", "print", "(", "per_lang_score", ")", "\n", "\n", "avg_scores", ".", "append", "(", "per_lang_score", ")", "\n", "\n", "", "print", "(", "\"Final macro averaged score: \"", ")", "\n", "print", "(", "mean", "(", "avg_scores", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.build_es.build_index": [[10, 46], ["doc_db.DocDB", "elasticsearch.Elasticsearch", "elasticsearch.Elasticsearch.indices.exists", "elasticsearch.Elasticsearch.indices.create", "elasticsearch.Elasticsearch.ping", "doc_db.DocDB.get_doc_ids", "tqdm.tqdm", "print", "elasticsearch.Elasticsearch.indices.delete", "doc_db.DocDB.get_doc_text_section_separations", "elasticsearch.Elasticsearch.count", "enumerate", "doc_id.split", "elasticsearch.Elasticsearch.index", "print"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.get_doc_ids", "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.get_doc_text_section_separations"], ["def", "build_index", "(", "index_prefix", ",", "db_path", ",", "lcode", ",", "es_index_settings", ",", "port", "=", "9200", ")", ":", "\n", "    ", "db", "=", "DocDB", "(", "db_path", ")", "\n", "\n", "# initialize the elastic search", "\n", "config", "=", "{", "'host'", ":", "'localhost'", ",", "'port'", ":", "port", "}", "\n", "es", "=", "Elasticsearch", "(", "[", "config", "]", ")", "\n", "index_name", "=", "\"{0}_{1}\"", ".", "format", "(", "index_prefix", ",", "lcode", ")", "\n", "if", "es", ".", "indices", ".", "exists", "(", "index", "=", "index_name", ")", ":", "\n", "        ", "es", ".", "indices", ".", "delete", "(", "index", "=", "index_name", ")", "\n", "", "index_settings", "=", "es_index_settings", "\n", "\n", "es", ".", "indices", ".", "create", "(", "index", "=", "index_name", ",", "body", "=", "{", "\"settings\"", ":", "index_settings", "[", "\"settings\"", "]", "}", ")", "\n", "# populate index", "\n", "# load DB and index in Elastic Search", "\n", "es", ".", "ping", "(", ")", "\n", "doc_ids", "=", "db", ".", "get_doc_ids", "(", ")", "\n", "count", "=", "0", "\n", "for", "doc_id", "in", "tqdm", "(", "doc_ids", ")", ":", "\n", "        ", "sections_paras", "=", "db", ".", "get_doc_text_section_separations", "(", "doc_id", ")", "\n", "for", "section", "in", "sections_paras", ":", "\n", "            ", "section_name", "=", "section", "[", "\"section_name\"", "]", "\n", "parent_section_name", "=", "section", "[", "\"parent_section_name\"", "]", "\n", "paragraphs", "=", "section", "[", "\"paragraphs\"", "]", "\n", "title", "=", "doc_id", ".", "split", "(", "\"_0\"", ")", "[", "0", "]", "\n", "for", "para_idx", ",", "para", "in", "enumerate", "(", "paragraphs", ")", ":", "\n", "                ", "para_title_id", "=", "\"title:{0}_parentSection:{1}_sectionName:{2}_sectionIndex:{3}\"", ".", "format", "(", "title", ",", "parent_section_name", ",", "section_name", ",", "para_idx", ")", "\n", "rec", "=", "{", "\"document_text\"", ":", "para", ",", "\"document_title\"", ":", "para_title_id", "}", "\n", "try", ":", "\n", "                    ", "index_status", "=", "es", ".", "index", "(", "index", "=", "index_name", ",", "id", "=", "count", ",", "body", "=", "rec", ")", "\n", "count", "+=", "1", "\n", "", "except", ":", "\n", "                    ", "print", "(", "f'Unable to load document {para_title_id}.'", ")", "\n", "\n", "", "", "", "", "n_records", "=", "es", ".", "count", "(", "index", "=", "index_name", ")", "[", "'count'", "]", "\n", "print", "(", "f'Successfully loaded {n_records} into {index_name}'", ")", "\n", "return", "es", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.build_es.search_es": [[47, 60], ["es_obj.search"], "function", ["None"], ["", "def", "search_es", "(", "es_obj", ",", "index_name", ",", "question_text", ",", "n_results", "=", "5", ")", ":", "\n", "# construct query", "\n", "    ", "query", "=", "{", "\n", "'query'", ":", "{", "\n", "'match'", ":", "{", "\n", "'document_text'", ":", "question_text", "\n", "}", "\n", "}", "\n", "}", "\n", "\n", "res", "=", "es_obj", ".", "search", "(", "index", "=", "index_name", ",", "body", "=", "query", ",", "size", "=", "n_results", ")", "\n", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.build_es.main": [[61, 79], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "build_es.build_index", "build_es.search_es", "print", "json.load", "open"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.build_es.build_index", "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.es_search_multi.search_es"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--db_path'", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "'Path to sqlite db holding document texts'", ")", "\n", "parser", ".", "add_argument", "(", "'--lcode'", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "'language code'", ")", "\n", "parser", ".", "add_argument", "(", "'--config_file_path'", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "'path to the congig file'", ")", "\n", "parser", ".", "add_argument", "(", "'--port'", ",", "type", "=", "int", ",", "required", "=", "True", ",", "\n", "help", "=", "'path to the congig file'", ")", "\n", "parser", ".", "add_argument", "(", "'--index_prefix'", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "'path to the congig file'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "question_text", "=", "\"What did Ron Paul majored in college?\"", "\n", "es", "=", "build_index", "(", "args", ".", "index_prefix", ",", "args", ".", "db_path", ",", "args", ".", "lcode", ",", "json", ".", "load", "(", "open", "(", "args", ".", "config_file_path", ")", ")", ",", "port", "=", "args", ".", "port", ")", "\n", "res", "=", "search_es", "(", "es_obj", "=", "es", ",", "index_name", "=", "\"{0}_{1}\"", ".", "format", "(", "args", ".", "index_prefix", ",", "args", ".", "lcode", ")", ",", "question_text", "=", "question_text", ",", "n_results", "=", "10", ")", "\n", "print", "(", "res", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.es_search_multi.read_jsonlines": [[10, 17], ["print", "jsonlines.open", "lines.append"], "function", ["None"], ["def", "read_jsonlines", "(", "file_name", ")", ":", "\n", "    ", "lines", "=", "[", "]", "\n", "print", "(", "\"loading examples from {0}\"", ".", "format", "(", "file_name", ")", ")", "\n", "with", "jsonlines", ".", "open", "(", "file_name", ")", "as", "reader", ":", "\n", "        ", "for", "obj", "in", "reader", ":", "\n", "            ", "lines", ".", "append", "(", "obj", ")", "\n", "", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.es_search_multi.search_es": [[18, 31], ["es_obj.search"], "function", ["None"], ["", "def", "search_es", "(", "es_obj", ",", "index_name", ",", "question_text", ",", "n_results", "=", "5", ")", ":", "\n", "# construct query", "\n", "    ", "query", "=", "{", "\n", "'query'", ":", "{", "\n", "'match'", ":", "{", "\n", "'document_text'", ":", "question_text", "\n", "}", "\n", "}", "\n", "}", "\n", "\n", "res", "=", "es_obj", ".", "search", "(", "index", "=", "index_name", ",", "body", "=", "query", ",", "size", "=", "n_results", ")", "\n", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.es_search_multi.main": [[32, 94], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "es_search_multi.read_jsonlines", "elasticsearch.Elasticsearch", "tqdm.tqdm", "print", "print", "es_search_multi.search_es", "open", "json.dump", "open", "json.dump", "len", "len", "len", "squad_style_dev_data[].append", "len", "[].find", "result.items", "result.items", "result.items"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.read_jsonlines", "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.es_search_multi.search_es"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--index_name_prefix'", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "'Path to index'", ")", "\n", "parser", ".", "add_argument", "(", "'--input_data_file_name'", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "'Path to index'", ")", "\n", "parser", ".", "add_argument", "(", "'--port'", ",", "type", "=", "int", ",", "required", "=", "True", ",", "\n", "help", "=", "'Path to index'", ")", "\n", "parser", ".", "add_argument", "(", "'--output_fp'", ",", "type", "=", "str", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "input_data", "=", "read_jsonlines", "(", "args", ".", "input_data_file_name", ")", "\n", "config", "=", "{", "'host'", ":", "'localhost'", ",", "'port'", ":", "args", ".", "port", "}", "\n", "es", "=", "Elasticsearch", "(", "[", "config", "]", ")", "\n", "result", "=", "{", "}", "\n", "# telugu is not supported.", "\n", "langs", "=", "[", "\"ar\"", ",", "\"fi\"", ",", "\"ja\"", ",", "\"ko\"", ",", "\"ru\"", ",", "\"bn\"", "]", "\n", "squad_style_dev_data", "=", "{", "'data'", ":", "[", "]", ",", "'version'", ":", "'v2.0'", "}", "\n", "for", "item", "in", "tqdm", "(", "input_data", ")", ":", "\n", "        ", "question", "=", "item", "[", "\"question\"", "]", "\n", "lang", "=", "item", "[", "\"lang\"", "]", "\n", "if", "lang", "not", "in", "langs", ":", "\n", "            ", "continue", "\n", "", "index_name", "=", "args", ".", "index_name_prefix", "+", "\"_\"", "+", "lang", "\n", "res", "=", "search_es", "(", "es_obj", "=", "es", ",", "index_name", "=", "index_name", ",", "question_text", "=", "question", ",", "n_results", "=", "20", ")", "\n", "result", "[", "item", "[", "\"id\"", "]", "]", "=", "{", "\"hits\"", ":", "res", "[", "\"hits\"", "]", "[", "\"hits\"", "]", ",", "\"answers\"", ":", "item", "[", "\"answers\"", "]", ",", "\"has_answer\"", ":", "False", ",", "\"question\"", ":", "question", "}", "\n", "for", "hit", "in", "res", "[", "\"hits\"", "]", "[", "\"hits\"", "]", ":", "\n", "            ", "answers", "=", "[", "{", "\"text\"", ":", "answer", ",", "\"answer_start\"", ":", "hit", "[", "\"_source\"", "]", "[", "\"document_text\"", "]", ".", "find", "(", "answer", ")", "}", "for", "answer", "in", "item", "[", "\"answers\"", "]", "]", "\n", "squad_example", "=", "{", "'context'", ":", "hit", "[", "\"_source\"", "]", "[", "\"document_text\"", "]", ",", "\n", "'qas'", ":", "[", "{", "'question'", ":", "question", ",", "'is_impossible'", ":", "False", ",", "\n", "'answers'", ":", "answers", ",", "\n", "'id'", ":", "item", "[", "\"id\"", "]", "}", "]", "}", "\n", "squad_style_dev_data", "[", "\"data\"", "]", ".", "append", "(", "\n", "{", "\"title\"", ":", "hit", "[", "\"_source\"", "]", "[", "\"document_title\"", "]", ",", "'paragraphs'", ":", "[", "squad_example", "]", "}", ")", "\n", "\n", "# evaluate top 20 accuracy", "\n", "", "", "for", "q_id", "in", "result", ":", "\n", "        ", "hits", "=", "result", "[", "q_id", "]", "[", "\"hits\"", "]", "\n", "answers", "=", "result", "[", "q_id", "]", "[", "\"answers\"", "]", "\n", "for", "answer", "in", "answers", ":", "\n", "            ", "for", "hit", "in", "hits", ":", "\n", "                ", "if", "answer", "in", "hit", "[", "\"_source\"", "]", "[", "\"document_text\"", "]", ":", "\n", "                    ", "result", "[", "q_id", "]", "[", "\"has_answer\"", "]", "=", "True", "\n", "break", "\n", "\n", "", "", "", "", "with", "open", "(", "args", ".", "output_fp", ",", "'w'", ")", "as", "outfile", ":", "\n", "        ", "json", ".", "dump", "(", "result", ",", "outfile", ")", "\n", "", "with", "open", "(", "\"_squad_format\"", "+", "args", ".", "output_fp", ",", "'w'", ")", "as", "outfile", ":", "\n", "        ", "json", ".", "dump", "(", "squad_style_dev_data", ",", "outfile", ")", "\n", "\n", "\n", "# calc top 20 recall ", "\n", "", "top_20_accuracy", "=", "len", "(", "[", "q_id", "for", "q_id", ",", "item", "in", "result", ".", "items", "(", ")", "if", "item", "[", "\"has_answer\"", "]", "is", "True", "]", ")", "/", "len", "(", "result", ")", "\n", "# per language performance ", "\n", "langs", "=", "[", "\"ar\"", ",", "\"bn\"", ",", "\"fi\"", ",", "\"ja\"", ",", "\"ko\"", ",", "\"ru\"", ",", "\"te\"", "]", "\n", "per_lang_performance", "=", "{", "}", "\n", "for", "lang", "in", "langs", ":", "\n", "        ", "question_count", "=", "len", "(", "[", "q_id", "for", "q_id", ",", "item", "in", "result", ".", "items", "(", ")", "if", "lang", "in", "q_id", "]", ")", "\n", "top_20_accuracy_lang", "=", "len", "(", "[", "q_id", "for", "q_id", ",", "item", "in", "result", ".", "items", "(", ")", "if", "item", "[", "\"has_answer\"", "]", "is", "True", "and", "lang", "in", "q_id", "]", ")", "/", "question_count", "\n", "per_lang_performance", "[", "lang", "]", "=", "top_20_accuracy_lang", "\n", "", "print", "(", "top_20_accuracy", ")", "\n", "print", "(", "per_lang_performance", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.build_db.init": [[37, 41], ["build_db.import_module"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.build_db.import_module"], ["def", "init", "(", "filename", ")", ":", "\n", "    ", "global", "PREPROCESS_FN", "\n", "if", "filename", ":", "\n", "        ", "PREPROCESS_FN", "=", "import_module", "(", "filename", ")", ".", "preprocess", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.build_db.import_module": [[43, 49], ["importlib.util.spec_from_file_location", "importlib.util.module_from_spec", "importlib.util.spec_from_file_location.loader.exec_module"], "function", ["None"], ["", "", "def", "import_module", "(", "filename", ")", ":", "\n", "    ", "\"\"\"Import a module given a full path to the file.\"\"\"", "\n", "spec", "=", "importlib", ".", "util", ".", "spec_from_file_location", "(", "'doc_filter'", ",", "filename", ")", "\n", "module", "=", "importlib", ".", "util", ".", "module_from_spec", "(", "spec", ")", "\n", "spec", ".", "loader", ".", "exec_module", "(", "module", ")", "\n", "return", "module", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.build_db.iter_files": [[56, 66], ["os.path.isfile", "os.path.isdir", "os.walk", "RuntimeError", "os.path.join"], "function", ["None"], ["", "def", "iter_files", "(", "path", ")", ":", "\n", "    ", "\"\"\"Walk through all files located under a root path.\"\"\"", "\n", "if", "os", ".", "path", ".", "isfile", "(", "path", ")", ":", "\n", "        ", "yield", "path", "\n", "", "elif", "os", ".", "path", ".", "isdir", "(", "path", ")", ":", "\n", "        ", "for", "dirpath", ",", "_", ",", "filenames", "in", "os", ".", "walk", "(", "path", ")", ":", "\n", "            ", "for", "f", "in", "filenames", ":", "\n", "                ", "yield", "os", ".", "path", ".", "join", "(", "dirpath", ",", "f", ")", "\n", "", "", "", "else", ":", "\n", "        ", "raise", "RuntimeError", "(", "'Path %s is invalid'", "%", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.build_db.get_contents_hotpotqa": [[68, 82], ["process_jsonlines_hotpotqa", "documents.append"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.process_jsonlines_hotpotqa"], ["", "", "def", "get_contents_hotpotqa", "(", "filename", ")", ":", "\n", "    ", "\"\"\"Parse the contents of a file. Each line is a JSON encoded document.\"\"\"", "\n", "global", "PREPROCESS_FN", "\n", "documents", "=", "[", "]", "\n", "extracted_items", "=", "process_jsonlines_hotpotqa", "(", "filename", ")", "\n", "for", "extracted_item", "in", "extracted_items", ":", "\n", "        ", "title", "=", "extracted_item", "[", "\"title\"", "]", "\n", "text", "=", "extracted_item", "[", "\"plain_text\"", "]", "\n", "original_title", "=", "extracted_item", "[", "\"original_title\"", "]", "\n", "hyper_linked_titles", "=", "extracted_item", "[", "\"hyper_linked_titles\"", "]", "\n", "\n", "documents", ".", "append", "(", "(", "title", ",", "text", ",", "\n", "hyper_linked_titles", ",", "original_title", ")", ")", "\n", "", "return", "documents", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.build_db.get_contents": [[83, 97], ["process_jsonlines", "documents.append"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.process_jsonlines"], ["", "def", "get_contents", "(", "filename", ")", ":", "\n", "    ", "\"\"\"Parse the contents of a file. Each line is a JSON encoded document.\"\"\"", "\n", "global", "PREPROCESS_FN", "\n", "documents", "=", "[", "]", "\n", "extracted_items", "=", "process_jsonlines", "(", "filename", ")", "\n", "for", "extracted_item", "in", "extracted_items", ":", "\n", "        ", "title", "=", "extracted_item", "[", "\"title\"", "]", "\n", "text", "=", "extracted_item", "[", "\"plain_text\"", "]", "\n", "original_title", "=", "extracted_item", "[", "\"original_title\"", "]", "\n", "hyper_linked_titles", "=", "extracted_item", "[", "\"hyper_linked_titles\"", "]", "\n", "\n", "documents", ".", "append", "(", "(", "title", ",", "text", ",", "\n", "hyper_linked_titles", ",", "original_title", ")", ")", "\n", "", "return", "documents", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.build_db.store_contents": [[98, 140], ["os.path.isfile", "logger.info", "sqlite3.connect", "sqlite3.connect.cursor", "conn.cursor.execute", "multiprocessing.Pool", "logger.info", "logger.info", "sqlite3.connect.commit", "sqlite3.connect.close", "RuntimeError", "tqdm.tqdm", "tqdm.tqdm", "glob.glob", "multiprocessing.Pool.imap_unordered", "len", "conn.cursor.executemany", "pbar.update", "len"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.close"], ["", "def", "store_contents", "(", "wiki_dir", ",", "save_path", ",", "preprocess", ",", "num_workers", "=", "None", ",", "hotpotqa_format", "=", "False", ")", ":", "\n", "    ", "\"\"\"Preprocess and store a corpus of documents in sqlite.\n\n    Args:\n        data_path: Root path to directory (or directory of directories) of files\n          containing json encoded documents (must have `id` and `text` fields).\n        save_path: Path to output sqlite db.\n        preprocess: Path to file defining a custom `preprocess` function. Takes\n          in and outputs a structured doc.\n        num_workers: Number of parallel processes to use when reading docs.\n    \"\"\"", "\n", "filenames", "=", "[", "f", "for", "f", "in", "glob", ".", "glob", "(", "\n", "wiki_dir", "+", "\"/*/wiki_*\"", ",", "recursive", "=", "True", ")", "if", "\".bz2\"", "not", "in", "f", "]", "\n", "if", "os", ".", "path", ".", "isfile", "(", "save_path", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'%s already exists! Not overwriting.'", "%", "save_path", ")", "\n", "\n", "", "logger", ".", "info", "(", "'Reading into database...'", ")", "\n", "conn", "=", "sqlite3", ".", "connect", "(", "save_path", ")", "\n", "c", "=", "conn", ".", "cursor", "(", ")", "\n", "c", ".", "execute", "(", "\n", "\"CREATE TABLE documents (id PRIMARY KEY, text, linked_title, original_title);\"", ")", "\n", "\n", "workers", "=", "ProcessPool", "(", "num_workers", ",", "initializer", "=", "init", ",", "\n", "initargs", "=", "(", "preprocess", ",", ")", ")", "\n", "count", "=", "0", "\n", "# Due to the slight difference of input format between preprocessed HotpotQA wikipedia data and ", "\n", "# the ones by Wikiextractor, we call different functions for data process.", "\n", "if", "hotpotqa_format", "is", "True", ":", "\n", "        ", "content_processing_method", "=", "get_contents_hotpotqa", "\n", "", "else", ":", "\n", "        ", "content_processing_method", "=", "get_contents", "\n", "\n", "", "with", "tqdm", "(", "total", "=", "len", "(", "filenames", ")", ")", "as", "pbar", ":", "\n", "        ", "for", "pairs", "in", "tqdm", "(", "workers", ".", "imap_unordered", "(", "content_processing_method", ",", "filenames", ")", ")", ":", "\n", "            ", "count", "+=", "len", "(", "pairs", ")", "\n", "c", ".", "executemany", "(", "\n", "\"INSERT OR REPLACE INTO documents VALUES (?,?,?,?)\"", ",", "pairs", ")", "\n", "pbar", ".", "update", "(", ")", "\n", "", "", "logger", ".", "info", "(", "'Read %d docs.'", "%", "count", ")", "\n", "logger", ".", "info", "(", "'Committing...'", ")", "\n", "conn", ".", "commit", "(", ")", "\n", "conn", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.normalize": [[10, 14], ["unicodedata.normalize", "text[].capitalize"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.normalize"], ["def", "normalize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Resolve different type of unicode encodings / capitarization in HotpotQA data.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "'NFD'", ",", "text", ")", "\n", "return", "text", "[", "0", "]", ".", "capitalize", "(", ")", "+", "text", "[", "1", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.make_wiki_id": [[16, 19], ["utils.normalize"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.normalize"], ["", "def", "make_wiki_id", "(", "title", ",", "para_index", ")", ":", "\n", "    ", "title_id", "=", "\"{0}_{1}\"", ".", "format", "(", "normalize", "(", "title", ")", ",", "para_index", ")", "\n", "return", "title_id", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.find_hyper_linked_titles": [[21, 26], ["re.findall", "urllib.parse.unquote", "title[].capitalize"], "function", ["None"], ["", "def", "find_hyper_linked_titles", "(", "text_w_links", ")", ":", "\n", "    ", "titles", "=", "re", ".", "findall", "(", "r'href=[\\'\"]?([^\\'\" >]+)'", ",", "text_w_links", ")", "\n", "titles", "=", "[", "unquote", "(", "title", ")", "for", "title", "in", "titles", "]", "\n", "titles", "=", "[", "title", "[", "0", "]", ".", "capitalize", "(", ")", "+", "title", "[", "1", ":", "]", "for", "title", "in", "titles", "]", "\n", "return", "titles", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.remove_tags": [[31, 33], ["TAG_RE.sub"], "function", ["None"], ["def", "remove_tags", "(", "text", ")", ":", "\n", "    ", "return", "TAG_RE", ".", "sub", "(", "''", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.process_jsonlines": [[35, 66], ["jsonlines.open", "utils.make_wiki_id", "extracted_items.append"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.make_wiki_id"], ["", "def", "process_jsonlines", "(", "filename", ")", ":", "\n", "    ", "\"\"\"\n    This is process_jsonlines method for extracted Wikipedia file.\n    After extracting items by using Wikiextractor (with `--json` and `--links` options), \n    you will get the files named with wiki_xx, where each line contains the information of each article.\n    e.g., \n    {\"id\": \"316\", \"url\": \"https://en.wikipedia.org/wiki?curid=316\", \"title\": \"Academy Award for Best Production Design\", \n    \"text\": \"Academy Award for Best Production Design\\n\\nThe <a href=\\\"Academy%20Awards\\\">Academy Award</a> for \n    Best Production Design recognizes achievement for <a href=\\\"art%20direction\\\">art direction</a> \\n\\n\"}\n    This function takes these input and extract items.\n    Each article contains one or more than one paragraphs, and each paragraphs are separeated by \\n\\n.\n    \"\"\"", "\n", "# item should be nested list", "\n", "extracted_items", "=", "[", "]", "\n", "with", "jsonlines", ".", "open", "(", "filename", ")", "as", "reader", ":", "\n", "        ", "for", "obj", "in", "reader", ":", "\n", "            ", "wiki_id", "=", "obj", "[", "\"id\"", "]", "\n", "title", "=", "obj", "[", "\"title\"", "]", "\n", "title_id", "=", "make_wiki_id", "(", "title", ",", "0", ")", "\n", "text_with_links", "=", "obj", "[", "\"text\"", "]", "\n", "\n", "hyper_linked_titles_text", "=", "\"\"", "\n", "# When we consider the whole article as a document unit (e.g., SQuAD Open, Natural Questions Open)", "\n", "# we'll keep the links with the original articles, and dynamically process and extract the links", "\n", "# when we process with our selector.", "\n", "extracted_items", ".", "append", "(", "{", "\"wiki_id\"", ":", "wiki_id", ",", "\"title\"", ":", "title_id", ",", "\n", "\"plain_text\"", ":", "text_with_links", ",", "\n", "\"hyper_linked_titles\"", ":", "hyper_linked_titles_text", ",", "\n", "\"original_title\"", ":", "title", "}", ")", "\n", "\n", "", "", "return", "extracted_items", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.process_jsonlines_hotpotqa": [[68, 102], ["jsonlines.open", "utils.make_wiki_id", "utils.find_hyper_linked_titles", "extracted_items.append", "len"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.make_wiki_id", "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.find_hyper_linked_titles"], ["", "def", "process_jsonlines_hotpotqa", "(", "filename", ")", ":", "\n", "    ", "\"\"\"\n    This is process_jsonlines method for intro-only processed_wikipedia file.\n    The item example:\n    {\"id\": \"45668011\", \"url\": \"https://en.wikipedia.org/wiki?curid=45668011\", \"title\": \"Flouch Roundabout\",\n     \"text\": [\"Flouch Roundabout is a roundabout near Penistone, South Yorkshire, England, where the A628 meets the A616.\"],\n     \"charoffset\": [[[0, 6],...]]\n     \"text_with_links\" : [\"Flouch Roundabout is a roundabout near <a href=\\\"Penistone\\\">Penistone</a>,\n     <a href=\\\"South%20Yorkshire\\\">South Yorkshire</a>, England, where the <a href=\\\"A628%20road\\\">A628</a>\n     meets the <a href=\\\"A616%20road\\\">A616</a>.\"],\n        \"charoffset_with_links\": [[[0, 6], ... [213, 214]]]}\n    \"\"\"", "\n", "# item should be nested list", "\n", "extracted_items", "=", "[", "]", "\n", "with", "jsonlines", ".", "open", "(", "filename", ")", "as", "reader", ":", "\n", "        ", "for", "obj", "in", "reader", ":", "\n", "            ", "wiki_id", "=", "obj", "[", "\"id\"", "]", "\n", "title", "=", "obj", "[", "\"title\"", "]", "\n", "title_id", "=", "make_wiki_id", "(", "title", ",", "0", ")", "\n", "plain_text", "=", "\"\\t\"", ".", "join", "(", "obj", "[", "\"text\"", "]", ")", "\n", "text_with_links", "=", "\"\\t\"", ".", "join", "(", "obj", "[", "\"text_with_links\"", "]", ")", "\n", "\n", "hyper_linked_titles", "=", "[", "]", "\n", "hyper_linked_titles", "=", "find_hyper_linked_titles", "(", "text_with_links", ")", "\n", "if", "len", "(", "hyper_linked_titles", ")", ">", "0", ":", "\n", "                ", "hyper_linked_titles_text", "=", "\"\\t\"", ".", "join", "(", "hyper_linked_titles", ")", "\n", "", "else", ":", "\n", "                ", "hyper_linked_titles_text", "=", "\"\"", "\n", "", "extracted_items", ".", "append", "(", "{", "\"wiki_id\"", ":", "wiki_id", ",", "\"title\"", ":", "title_id", ",", "\n", "\"plain_text\"", ":", "plain_text", ",", "\n", "\"hyper_linked_titles\"", ":", "hyper_linked_titles_text", ",", "\n", "\"original_title\"", ":", "title", "}", ")", "\n", "\n", "", "", "return", "extracted_items", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.save_sparse_csr": [[109, 118], ["numpy.savez"], "function", ["None"], ["", "def", "save_sparse_csr", "(", "filename", ",", "matrix", ",", "metadata", "=", "None", ")", ":", "\n", "    ", "data", "=", "{", "\n", "'data'", ":", "matrix", ".", "data", ",", "\n", "'indices'", ":", "matrix", ".", "indices", ",", "\n", "'indptr'", ":", "matrix", ".", "indptr", ",", "\n", "'shape'", ":", "matrix", ".", "shape", ",", "\n", "'metadata'", ":", "metadata", ",", "\n", "}", "\n", "np", ".", "savez", "(", "filename", ",", "**", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.load_sparse_csr": [[120, 125], ["numpy.load", "scipy.csr_matrix", "loader[].item"], "function", ["None"], ["", "def", "load_sparse_csr", "(", "filename", ")", ":", "\n", "    ", "loader", "=", "np", ".", "load", "(", "filename", ",", "allow_pickle", "=", "True", ")", "\n", "matrix", "=", "sp", ".", "csr_matrix", "(", "(", "loader", "[", "'data'", "]", ",", "loader", "[", "'indices'", "]", ",", "\n", "loader", "[", "'indptr'", "]", ")", ",", "shape", "=", "loader", "[", "'shape'", "]", ")", "\n", "return", "matrix", ",", "loader", "[", "'metadata'", "]", ".", "item", "(", "0", ")", "if", "'metadata'", "in", "loader", "else", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.hash": [[131, 134], ["sklearn.utils.murmurhash3_32"], "function", ["None"], ["", "def", "hash", "(", "token", ",", "num_buckets", ")", ":", "\n", "    ", "\"\"\"Unsigned 32 bit murmurhash for feature hashing.\"\"\"", "\n", "return", "murmurhash3_32", "(", "token", ",", "positive", "=", "True", ")", "%", "num_buckets", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.filter_word": [[161, 169], ["utils.normalize", "regex.match", "normalize.lower"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.normalize"], ["def", "filter_word", "(", "text", ")", ":", "\n", "    ", "\"\"\"Take out english stopwords, punctuation, and compound endings.\"\"\"", "\n", "text", "=", "normalize", "(", "text", ")", "\n", "if", "regex", ".", "match", "(", "r'^\\p{P}+$'", ",", "text", ")", ":", "\n", "        ", "return", "True", "\n", "", "if", "text", ".", "lower", "(", ")", "in", "STOPWORDS", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.filter_ngram": [[171, 189], ["utils.filter_word", "any", "all", "ValueError"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.filter_word"], ["", "def", "filter_ngram", "(", "gram", ",", "mode", "=", "'any'", ")", ":", "\n", "    ", "\"\"\"Decide whether to keep or discard an n-gram.\n    Args:\n        gram: list of tokens (length N)\n        mode: Option to throw out ngram if\n          'any': any single token passes filter_word\n          'all': all tokens pass filter_word\n          'ends': book-ended by filterable tokens\n    \"\"\"", "\n", "filtered", "=", "[", "filter_word", "(", "w", ")", "for", "w", "in", "gram", "]", "\n", "if", "mode", "==", "'any'", ":", "\n", "        ", "return", "any", "(", "filtered", ")", "\n", "", "elif", "mode", "==", "'all'", ":", "\n", "        ", "return", "all", "(", "filtered", ")", "\n", "", "elif", "mode", "==", "'ends'", ":", "\n", "        ", "return", "filtered", "[", "0", "]", "or", "filtered", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Invalid mode: %s'", "%", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.get_field": [[191, 202], ["isinstance", "d.copy"], "function", ["None"], ["", "", "def", "get_field", "(", "d", ",", "field_list", ")", ":", "\n", "    ", "\"\"\"get the subfield associated to a list of elastic fields\n        E.g. ['file', 'filename'] to d['file']['filename']\n    \"\"\"", "\n", "if", "isinstance", "(", "field_list", ",", "str", ")", ":", "\n", "        ", "return", "d", "[", "field_list", "]", "\n", "", "else", ":", "\n", "        ", "idx", "=", "d", ".", "copy", "(", ")", "\n", "for", "field", "in", "field_list", ":", "\n", "            ", "idx", "=", "idx", "[", "field", "]", "\n", "", "return", "idx", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.load_para_collections_from_tfidf_id_intro_only": [[204, 211], ["db.get_doc_text", "logger.warning", "db.get_doc_text().split", "db.get_doc_text"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.get_doc_text", "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.get_doc_text"], ["", "", "def", "load_para_collections_from_tfidf_id_intro_only", "(", "tfidf_id", ",", "db", ")", ":", "\n", "    ", "if", "\"_0\"", "not", "in", "tfidf_id", ":", "\n", "        ", "tfidf_id", "=", "\"{0}_0\"", ".", "format", "(", "tfidf_id", ")", "\n", "", "if", "db", ".", "get_doc_text", "(", "tfidf_id", ")", "is", "None", ":", "\n", "        ", "logger", ".", "warning", "(", "\"{0} is missing\"", ".", "format", "(", "tfidf_id", ")", ")", "\n", "return", "[", "]", "\n", "", "return", "[", "[", "tfidf_id", ",", "db", ".", "get_doc_text", "(", "tfidf_id", ")", ".", "split", "(", "\"\\t\"", ")", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.load_linked_titles_from_tfidf_id": [[213, 221], ["db.get_paras_with_article", "db.get_hyper_linked", "len", "db.get_hyper_linked.split"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.get_hyper_linked"], ["", "def", "load_linked_titles_from_tfidf_id", "(", "tfidf_id", ",", "db", ")", ":", "\n", "    ", "para_titles", "=", "db", ".", "get_paras_with_article", "(", "tfidf_id", ")", "\n", "linked_titles_all", "=", "[", "]", "\n", "for", "para_title", "in", "para_titles", ":", "\n", "        ", "linked_title_per_para", "=", "db", ".", "get_hyper_linked", "(", "para_title", ")", "\n", "if", "len", "(", "linked_title_per_para", ")", ">", "0", ":", "\n", "            ", "linked_titles_all", "+=", "linked_title_per_para", ".", "split", "(", "\"\\t\"", ")", "\n", "", "", "return", "linked_titles_all", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.load_para_and_linked_titles_dict_from_tfidf_id": [[223, 247], ["db.get_doc_text_hyper_linked_titles_for_articles", "enumerate", "len", "logger.warning", "tfidf_id.split", "zip"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.get_doc_text_hyper_linked_titles_for_articles"], ["", "def", "load_para_and_linked_titles_dict_from_tfidf_id", "(", "tfidf_id", ",", "db", ")", ":", "\n", "    ", "\"\"\"\n    load paragraphs and hyperlinked titles from DB. \n    This method is mainly for Natural Questions Open benchmark.\n    \"\"\"", "\n", "# will be fixed in the later version; current tfidf weights use indexed titles as keys.", "\n", "if", "\"_0\"", "not", "in", "tfidf_id", ":", "\n", "        ", "tfidf_id", "=", "\"{0}_0\"", ".", "format", "(", "tfidf_id", ")", "\n", "", "paras", ",", "linked_titles", "=", "db", ".", "get_doc_text_hyper_linked_titles_for_articles", "(", "\n", "tfidf_id", ")", "\n", "if", "len", "(", "paras", ")", "==", "0", ":", "\n", "        ", "logger", ".", "warning", "(", "\"{0} is missing\"", ".", "format", "(", "tfidf_id", ")", ")", "\n", "return", "[", "]", ",", "[", "]", "\n", "\n", "", "paras_dict", "=", "{", "}", "\n", "linked_titles_dict", "=", "{", "}", "\n", "article_name", "=", "tfidf_id", ".", "split", "(", "\"_0\"", ")", "[", "0", "]", "\n", "# store the para_dict and linked_titles_dict; skip the first para (title)", "\n", "for", "para_idx", ",", "(", "para", ",", "linked_title_list", ")", "in", "enumerate", "(", "zip", "(", "paras", "[", "1", ":", "]", ",", "linked_titles", "[", "1", ":", "]", ")", ")", ":", "\n", "        ", "paras_dict", "[", "\"{0}_{1}\"", ".", "format", "(", "article_name", ",", "para_idx", ")", "]", "=", "para", "\n", "linked_titles_dict", "[", "\"{0}_{1}\"", ".", "format", "(", "\n", "article_name", ",", "para_idx", ")", "]", "=", "linked_title_list", "\n", "\n", "", "return", "paras_dict", ",", "linked_titles_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.prune_top_k_paragraphs": [[249, 261], ["list", "list", "tfidf_vectorizer.prune", "paragraphs.keys", "paragraphs.values"], "function", ["None"], ["", "def", "prune_top_k_paragraphs", "(", "question_text", ",", "paragraphs", ",", "tfidf_vectorizer", ",", "pruning_l", "=", "10", ")", ":", "\n", "    ", "para_titles", ",", "para_text", "=", "list", "(", "paragraphs", ".", "keys", "(", ")", ")", ",", "list", "(", "paragraphs", ".", "values", "(", ")", ")", "\n", "# prune top l paragraphs using the question as query to reduce the search space.", "\n", "top_tfidf_para_indices", "=", "tfidf_vectorizer", ".", "prune", "(", "\n", "question_text", ",", "para_text", ")", "[", ":", "pruning_l", "]", "\n", "para_title_text_pairs_pruned", "=", "{", "}", "\n", "\n", "# store the selected paras into dictionary.", "\n", "for", "idx", "in", "top_tfidf_para_indices", ":", "\n", "        ", "para_title_text_pairs_pruned", "[", "para_titles", "[", "idx", "]", "]", "=", "para_text", "[", "idx", "]", "\n", "\n", "", "return", "para_title_text_pairs_pruned", "\n", "", ""]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.__init__": [[17, 20], ["sqlite3.connect"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "db_path", "=", "None", ")", ":", "\n", "        ", "self", ".", "path", "=", "db_path", "\n", "self", ".", "connection", "=", "sqlite3", ".", "connect", "(", "self", ".", "path", ",", "check_same_thread", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.__enter__": [[21, 23], ["None"], "methods", ["None"], ["", "def", "__enter__", "(", "self", ")", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.__exit__": [[24, 26], ["doc_db.DocDB.close"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.close"], ["", "def", "__exit__", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "self", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.close": [[27, 30], ["doc_db.DocDB.connection.close"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "\"\"\"Close the connection to the database.\"\"\"", "\n", "self", ".", "connection", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.get_doc_ids": [[31, 38], ["doc_db.DocDB.connection.cursor", "doc_db.DocDB.execute", "doc_db.DocDB.close", "doc_db.DocDB.fetchall"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.close"], ["", "def", "get_doc_ids", "(", "self", ")", ":", "\n", "        ", "\"\"\"Fetch all ids of docs stored in the db.\"\"\"", "\n", "cursor", "=", "self", ".", "connection", ".", "cursor", "(", ")", "\n", "cursor", ".", "execute", "(", "\"SELECT id FROM documents\"", ")", "\n", "results", "=", "[", "r", "[", "0", "]", "for", "r", "in", "cursor", ".", "fetchall", "(", ")", "]", "\n", "cursor", ".", "close", "(", ")", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.get_doc_text": [[39, 49], ["doc_db.DocDB.connection.cursor", "doc_db.DocDB.execute", "doc_db.DocDB.fetchone", "doc_db.DocDB.close"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.close"], ["", "def", "get_doc_text", "(", "self", ",", "doc_id", ")", ":", "\n", "        ", "\"\"\"Fetch the raw text of the doc for 'doc_id'.\"\"\"", "\n", "cursor", "=", "self", ".", "connection", ".", "cursor", "(", ")", "\n", "cursor", ".", "execute", "(", "\n", "\"SELECT text FROM documents WHERE id = ?\"", ",", "\n", "(", "doc_id", ",", ")", "\n", ")", "\n", "result", "=", "cursor", ".", "fetchone", "(", ")", "\n", "cursor", ".", "close", "(", ")", "\n", "return", "result", "if", "result", "is", "None", "else", "result", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.get_hyper_linked": [[50, 60], ["doc_db.DocDB.connection.cursor", "doc_db.DocDB.execute", "doc_db.DocDB.fetchone", "doc_db.DocDB.close", "normalize", "len", "result[].split"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.close", "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.normalize"], ["", "def", "get_hyper_linked", "(", "self", ",", "doc_id", ")", ":", "\n", "        ", "\"\"\"Fetch the hyper-linked titles of the doc for 'doc_id'.\"\"\"", "\n", "cursor", "=", "self", ".", "connection", ".", "cursor", "(", ")", "\n", "cursor", ".", "execute", "(", "\n", "\"SELECT linked_title FROM documents WHERE id = ?\"", ",", "\n", "(", "doc_id", ",", ")", "\n", ")", "\n", "result", "=", "cursor", ".", "fetchone", "(", ")", "\n", "cursor", ".", "close", "(", ")", "\n", "return", "result", "if", "(", "result", "is", "None", "or", "len", "(", "result", "[", "0", "]", ")", "==", "0", ")", "else", "[", "normalize", "(", "title", ")", "for", "title", "in", "result", "[", "0", "]", ".", "split", "(", "\"\\t\"", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.get_original_title": [[61, 71], ["doc_db.DocDB.connection.cursor", "doc_db.DocDB.execute", "doc_db.DocDB.fetchone", "doc_db.DocDB.close"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.close"], ["", "def", "get_original_title", "(", "self", ",", "doc_id", ")", ":", "\n", "        ", "\"\"\"Fetch the original title name  of the doc.\"\"\"", "\n", "cursor", "=", "self", ".", "connection", ".", "cursor", "(", ")", "\n", "cursor", ".", "execute", "(", "\n", "\"SELECT original_title FROM documents WHERE id = ?\"", ",", "\n", "(", "doc_id", ",", ")", "\n", ")", "\n", "result", "=", "cursor", ".", "fetchone", "(", ")", "\n", "cursor", ".", "close", "(", ")", "\n", "return", "result", "if", "result", "is", "None", "else", "result", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.get_doc_text_hyper_linked_titles_for_articles": [[72, 101], ["doc_db.DocDB.connection.cursor", "doc_db.DocDB.execute", "doc_db.DocDB.fetchone", "doc_db.DocDB.close", "result[].split", "paragraphs.append", "hyper_linked_titles.append", "remove_tags", "normalize", "find_hyper_linked_titles"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.close", "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.remove_tags", "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.normalize", "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.find_hyper_linked_titles"], ["", "def", "get_doc_text_hyper_linked_titles_for_articles", "(", "self", ",", "doc_id", ")", ":", "\n", "        ", "\"\"\"\n        fetch all of the paragraphs with their corresponding hyperlink titles.\n        e.g., \n        >>> paras, links = db.get_doc_text_hyper_linked_titles_for_articles(\"Tokyo Imperial Palace_0\")\n        >>> paras[2]\n        'It is built on the site of the old Edo Castle. The total area including the gardens is . During the height of the 1980s Japanese property bubble, the palace grounds were valued by some to be more than the value of all of the real estate in the state of California.'\n        >>> links[2]\n        ['Edo Castle', 'Japanese asset price bubble', 'Real estate', 'California']\n        \"\"\"", "\n", "cursor", "=", "self", ".", "connection", ".", "cursor", "(", ")", "\n", "cursor", ".", "execute", "(", "\n", "\"SELECT text FROM documents WHERE id = ?\"", ",", "\n", "(", "doc_id", ",", ")", "\n", ")", "\n", "result", "=", "cursor", ".", "fetchone", "(", ")", "\n", "cursor", ".", "close", "(", ")", "\n", "if", "result", "is", "None", ":", "\n", "            ", "return", "[", "]", ",", "[", "]", "\n", "", "else", ":", "\n", "            ", "hyper_linked_paragraphs", "=", "result", "[", "0", "]", ".", "split", "(", "\"\\n\\n\"", ")", "\n", "paragraphs", ",", "hyper_linked_titles", "=", "[", "]", ",", "[", "]", "\n", "\n", "for", "hyper_linked_paragraph", "in", "hyper_linked_paragraphs", ":", "\n", "                ", "paragraphs", ".", "append", "(", "remove_tags", "(", "hyper_linked_paragraph", ")", ")", "\n", "hyper_linked_titles", ".", "append", "(", "[", "normalize", "(", "title", ")", "for", "title", "in", "find_hyper_linked_titles", "(", "\n", "hyper_linked_paragraph", ")", "]", ")", "\n", "\n", "", "return", "paragraphs", ",", "hyper_linked_titles", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.get_doc_text_section_separations": [[102, 156], ["doc_db.DocDB.connection.cursor", "doc_db.DocDB.execute", "doc_db.DocDB.fetchone", "doc_db.DocDB.close", "result[].split", "enumerate", "output_data.append", "re.compile().search().group", "re.sub", "len", "len", "len", "output_data.append", "output_data.append", "section.split", "re.compile().search", "section.split", "section.split", "re.compile", "re.sub.split", "len"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.doc_db.DocDB.close"], ["", "", "def", "get_doc_text_section_separations", "(", "self", ",", "doc_id", ")", ":", "\n", "# WIP: we might have better formats to keep the information.", "\n", "        ", "\"\"\"\n        fetch all of the paragraphs with section level separations\n        e.g., \n        >>> sectioned_paragraphs = db.get_doc_text_hyper_linked_titles_for_articles(\"Tokyo Imperial Palace_0\")\n        >>> sectioned_paragraphs[0]\n        {\"section_name\":\"Early life and sumo background.\", \n        \"parent_section_name\": None:,\n        \"paragraphs\": [\"Tatsu Ry\u014dya was born in Kanazawa, Ishikawa and is the youngest of three children. \n        His father was a truck driver. He was a normal-sized baby but grew quickly so that when \n        attending kindergarten he had difficulty fitting into the uniform. He first began \n        practicing sumo whilst in the first grade of elementary school.\",\n        \"By the age of thirteen, when he ended his \n        first year at junior high school he stood , and weighed . \n        After competing successfully in junior high school sumo he gave up formal education \n        at the age of fifteen and entered the Takadagawa stable to pursue a professional career.\"\n        \"type\": \"child\"}\n        \"\"\"", "\n", "cursor", "=", "self", ".", "connection", ".", "cursor", "(", ")", "\n", "cursor", ".", "execute", "(", "\n", "\"SELECT text FROM documents WHERE id = ?\"", ",", "\n", "(", "doc_id", ",", ")", "\n", ")", "\n", "result", "=", "cursor", ".", "fetchone", "(", ")", "\n", "cursor", ".", "close", "(", ")", "\n", "if", "result", "is", "None", ":", "\n", "            ", "return", "[", "]", "\n", "", "else", ":", "\n", "            ", "output_data", "=", "[", "]", "\n", "section_separated_context", "=", "result", "[", "0", "]", ".", "split", "(", "\"Section::::\"", ")", "\n", "\n", "parent_section", "=", "\"\"", "\n", "for", "s_idx", ",", "section", "in", "enumerate", "(", "section_separated_context", ")", ":", "\n", "# the first sections are likely to be introductory paragraphs.", "\n", "                ", "if", "s_idx", "==", "0", "and", "len", "(", "section", ".", "split", "(", "\"\\n\\n\"", ")", ")", ">", "1", "and", "len", "(", "section", ".", "split", "(", "\"\\n\\n\"", ")", "[", "1", "]", ")", ">", "0", ":", "\n", "                    ", "section_name", "=", "\"Introduction\"", "\n", "parent_section", "=", "\"Introduction\"", "\n", "output_data", ".", "append", "(", "\n", "{", "\"section_name\"", ":", "section_name", ",", "\"paragraphs\"", ":", "section", ".", "split", "(", "\"\\n\\n\"", ")", "[", "1", ":", "]", ",", "\n", "\"type\"", ":", "\"intro\"", ",", "\"parent_section_name\"", ":", "parent_section", "}", ")", "\n", "", "else", ":", "\n", "                    ", "section_name", "=", "re", ".", "compile", "(", "\n", "\"(.*)\\n\"", ")", ".", "search", "(", "section", ")", ".", "group", "(", "1", ")", "\n", "section_text", "=", "re", ".", "sub", "(", "\"(.*)\\n\"", ",", "\"\"", ",", "section", ",", "1", ")", "\n", "if", "len", "(", "section_text", ")", "==", "0", ":", "\n", "# this is section header", "\n", "                        ", "parent_section", "=", "section_name", "\n", "output_data", ".", "append", "(", "{", "\"section_name\"", ":", "section_name", ",", "\"paragraphs\"", ":", "[", "]", ",", "\n", "\"type\"", ":", "\"parent\"", ",", "\"parent_section_name\"", ":", "None", "}", ")", "\n", "", "else", ":", "\n", "                        ", "output_data", ".", "append", "(", "{", "\"section_name\"", ":", "section_name", ",", "\"paragraphs\"", ":", "[", "para", "for", "para", "in", "section_text", ".", "split", "(", "\"\\n\\n\"", ")", "if", "len", "(", "para", ")", ">", "10", "]", ",", "\n", "\"type\"", ":", "\"child\"", ",", "\"parent_section_name\"", ":", "parent_section", "}", ")", "\n", "", "", "", "return", "output_data", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.preprocess_reader_data.main": [[21, 29], ["dpr.models.init_tenzorizer", "dpr.models.init_tenzorizer.set_pad_to_max", "dpr.data.reader_data.convert_retriever_results"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.__init__.init_tenzorizer", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.set_pad_to_max", "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data.convert_retriever_results"], ["def", "main", "(", "args", ")", ":", "\n", "    ", "tensorizer", "=", "init_tenzorizer", "(", "args", ".", "encoder_model_type", ",", "args", ")", "\n", "\n", "# disable auto-padding to save disk space of serialized files", "\n", "tensorizer", ".", "set_pad_to_max", "(", "False", ")", "\n", "\n", "convert_retriever_results", "(", "args", ".", "is_train_set", ",", "args", ".", "retriever_results", ",", "args", ".", "out_file", ",", "args", ".", "gold_passages_src", ",", "\n", "tensorizer", ",", "args", ".", "num_workers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.generate_dense_embeddings.gen_ctx_vectors": [[38, 71], ["len", "enumerate", "range", "dpr.utils.model_utils.move_to_device", "dpr.utils.model_utils.move_to_device", "dpr.utils.model_utils.move_to_device", "out.cpu.cpu", "len", "results.extend", "tensorizer.text_to_tensor", "torch.stack", "torch.zeros_like", "tensorizer.get_attn_mask", "torch.no_grad", "model", "len", "out.cpu.size", "logger.info", "out[].view().numpy", "range", "out.cpu.size", "out[].view"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.move_to_device", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.move_to_device", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.move_to_device", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.text_to_tensor", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.get_attn_mask"], ["def", "gen_ctx_vectors", "(", "ctx_rows", ":", "List", "[", "Tuple", "[", "object", ",", "str", ",", "str", "]", "]", ",", "model", ":", "nn", ".", "Module", ",", "tensorizer", ":", "Tensorizer", ",", "\n", "insert_title", ":", "bool", "=", "True", ")", "->", "List", "[", "Tuple", "[", "object", ",", "np", ".", "array", "]", "]", ":", "\n", "    ", "n", "=", "len", "(", "ctx_rows", ")", "\n", "bsz", "=", "args", ".", "batch_size", "\n", "total", "=", "0", "\n", "results", "=", "[", "]", "\n", "for", "j", ",", "batch_start", "in", "enumerate", "(", "range", "(", "0", ",", "n", ",", "bsz", ")", ")", ":", "\n", "\n", "        ", "batch_token_tensors", "=", "[", "tensorizer", ".", "text_to_tensor", "(", "ctx", "[", "1", "]", ",", "title", "=", "ctx", "[", "2", "]", "if", "insert_title", "else", "None", ")", "for", "ctx", "in", "\n", "ctx_rows", "[", "batch_start", ":", "batch_start", "+", "bsz", "]", "]", "\n", "\n", "ctx_ids_batch", "=", "move_to_device", "(", "torch", ".", "stack", "(", "batch_token_tensors", ",", "dim", "=", "0", ")", ",", "args", ".", "device", ")", "\n", "ctx_seg_batch", "=", "move_to_device", "(", "torch", ".", "zeros_like", "(", "ctx_ids_batch", ")", ",", "args", ".", "device", ")", "\n", "ctx_attn_mask", "=", "move_to_device", "(", "tensorizer", ".", "get_attn_mask", "(", "ctx_ids_batch", ")", ",", "args", ".", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "_", ",", "out", ",", "_", "=", "model", "(", "ctx_ids_batch", ",", "ctx_seg_batch", ",", "ctx_attn_mask", ")", "\n", "", "out", "=", "out", ".", "cpu", "(", ")", "\n", "\n", "ctx_ids", "=", "[", "r", "[", "0", "]", "for", "r", "in", "ctx_rows", "[", "batch_start", ":", "batch_start", "+", "bsz", "]", "]", "\n", "\n", "assert", "len", "(", "ctx_ids", ")", "==", "out", ".", "size", "(", "0", ")", "\n", "\n", "total", "+=", "len", "(", "ctx_ids", ")", "\n", "\n", "results", ".", "extend", "(", "[", "\n", "(", "ctx_ids", "[", "i", "]", ",", "out", "[", "i", "]", ".", "view", "(", "-", "1", ")", ".", "numpy", "(", ")", ")", "\n", "for", "i", "in", "range", "(", "out", ".", "size", "(", "0", ")", ")", "\n", "]", ")", "\n", "\n", "if", "total", "%", "10", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "'Encoded passages %d'", ",", "total", ")", "\n", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.generate_dense_embeddings.main": [[73, 122], ["dpr.utils.model_utils.load_states_from_checkpoint", "dpr.options.set_encoder_params_from_state", "dpr.options.print_args", "dpr.models.init_biencoder_components", "dpr.utils.model_utils.setup_for_distributed_mode", "encoder.eval", "dpr.utils.model_utils.get_model_obj", "logger.info", "logger.debug", "len", "dpr.utils.model_utils.get_model_obj.load_state_dict", "logger.info", "int", "logger.info", "generate_dense_embeddings.gen_ctx_vectors", "pathlib.Path().mkdir", "logger.info", "logger.info", "dpr.utils.model_utils.load_states_from_checkpoint.model_dict.keys", "open", "csv.reader", "rows.extend", "len", "str", "open", "pickle.dump", "len", "dpr.utils.model_utils.load_states_from_checkpoint.model_dict.items", "key.startswith", "len", "pathlib.Path", "os.path.dirname"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.load_states_from_checkpoint", "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.set_encoder_params_from_state", "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.print_args", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.__init__.init_biencoder_components", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.setup_for_distributed_mode", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.get_model_obj", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.generate_dense_embeddings.gen_ctx_vectors"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "saved_state", "=", "load_states_from_checkpoint", "(", "args", ".", "model_file", ")", "\n", "set_encoder_params_from_state", "(", "saved_state", ".", "encoder_params", ",", "args", ")", "\n", "print_args", "(", "args", ")", "\n", "\n", "tensorizer", ",", "encoder", ",", "_", "=", "init_biencoder_components", "(", "args", ".", "encoder_model_type", ",", "args", ",", "inference_only", "=", "True", ")", "\n", "\n", "encoder", "=", "encoder", ".", "ctx_model", "\n", "\n", "encoder", ",", "_", "=", "setup_for_distributed_mode", "(", "encoder", ",", "None", ",", "args", ".", "device", ",", "args", ".", "n_gpu", ",", "\n", "args", ".", "local_rank", ",", "\n", "args", ".", "fp16", ",", "\n", "args", ".", "fp16_opt_level", ")", "\n", "encoder", ".", "eval", "(", ")", "\n", "\n", "# load weights from the model file", "\n", "model_to_load", "=", "get_model_obj", "(", "encoder", ")", "\n", "logger", ".", "info", "(", "'Loading saved model state ...'", ")", "\n", "logger", ".", "debug", "(", "'saved model keys =%s'", ",", "saved_state", ".", "model_dict", ".", "keys", "(", ")", ")", "\n", "\n", "prefix_len", "=", "len", "(", "'ctx_model.'", ")", "\n", "ctx_state", "=", "{", "key", "[", "prefix_len", ":", "]", ":", "value", "for", "(", "key", ",", "value", ")", "in", "saved_state", ".", "model_dict", ".", "items", "(", ")", "if", "\n", "key", ".", "startswith", "(", "'ctx_model.'", ")", "}", "\n", "model_to_load", ".", "load_state_dict", "(", "ctx_state", ")", "\n", "\n", "logger", ".", "info", "(", "'reading data from file=%s'", ",", "args", ".", "ctx_file", ")", "\n", "\n", "rows", "=", "[", "]", "\n", "with", "open", "(", "args", ".", "ctx_file", ")", "as", "tsvfile", ":", "\n", "        ", "reader", "=", "csv", ".", "reader", "(", "tsvfile", ",", "delimiter", "=", "'\\t'", ")", "\n", "# file format: doc_id, doc_text, title", "\n", "rows", ".", "extend", "(", "[", "(", "row", "[", "0", "]", ",", "row", "[", "1", "]", ",", "row", "[", "2", "]", ")", "for", "row", "in", "reader", "if", "row", "[", "0", "]", "!=", "'id'", "]", ")", "\n", "\n", "", "shard_size", "=", "int", "(", "len", "(", "rows", ")", "/", "args", ".", "num_shards", ")", "\n", "start_idx", "=", "args", ".", "shard_id", "*", "shard_size", "\n", "end_idx", "=", "start_idx", "+", "shard_size", "\n", "\n", "logger", ".", "info", "(", "'Producing encodings for passages range: %d to %d (out of total %d)'", ",", "start_idx", ",", "end_idx", ",", "len", "(", "rows", ")", ")", "\n", "rows", "=", "rows", "[", "start_idx", ":", "end_idx", "]", "\n", "\n", "data", "=", "gen_ctx_vectors", "(", "rows", ",", "encoder", ",", "tensorizer", ",", "True", ")", "\n", "\n", "file", "=", "args", ".", "out_file", "+", "'_'", "+", "str", "(", "args", ".", "shard_id", ")", "\n", "pathlib", ".", "Path", "(", "os", ".", "path", ".", "dirname", "(", "file", ")", ")", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "logger", ".", "info", "(", "'Writing results to %s'", "%", "file", ")", "\n", "with", "open", "(", "file", ",", "mode", "=", "'wb'", ")", "as", "f", ":", "\n", "        ", "pickle", ".", "dump", "(", "data", ",", "f", ")", "\n", "\n", "", "logger", ".", "info", "(", "'Total passages processed %d. Written to %s'", ",", "len", "(", "data", ")", ",", "file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_reader.ReaderTrainer.__init__": [[47, 79], ["logger.info", "dpr.utils.model_utils.get_model_file", "dpr.models.init_reader_components", "dpr.utils.model_utils.setup_for_distributed_mode", "dpr.utils.model_utils.load_states_from_checkpoint", "dpr.options.set_encoder_params_from_state", "train_reader.ReaderTrainer._load_saved_state"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.get_model_file", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.__init__.init_reader_components", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.setup_for_distributed_mode", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.load_states_from_checkpoint", "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.set_encoder_params_from_state", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer._load_saved_state"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "\n", "self", ".", "shard_id", "=", "args", ".", "local_rank", "if", "args", ".", "local_rank", "!=", "-", "1", "else", "0", "\n", "self", ".", "distributed_factor", "=", "args", ".", "distributed_world_size", "or", "1", "\n", "\n", "logger", ".", "info", "(", "\"***** Initializing components for training *****\"", ")", "\n", "\n", "model_file", "=", "get_model_file", "(", "self", ".", "args", ",", "self", ".", "args", ".", "checkpoint_file_name", ")", "\n", "saved_state", "=", "None", "\n", "if", "model_file", ":", "\n", "            ", "saved_state", "=", "load_states_from_checkpoint", "(", "model_file", ")", "\n", "set_encoder_params_from_state", "(", "saved_state", ".", "encoder_params", ",", "args", ")", "\n", "\n", "", "tensorizer", ",", "reader", ",", "optimizer", "=", "init_reader_components", "(", "args", ".", "encoder_model_type", ",", "args", ")", "\n", "\n", "reader", ",", "optimizer", "=", "setup_for_distributed_mode", "(", "reader", ",", "optimizer", ",", "args", ".", "device", ",", "args", ".", "n_gpu", ",", "\n", "args", ".", "local_rank", ",", "\n", "args", ".", "fp16", ",", "\n", "args", ".", "fp16_opt_level", ")", "\n", "self", ".", "reader", "=", "reader", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "tensorizer", "=", "tensorizer", "\n", "self", ".", "start_epoch", "=", "0", "\n", "self", ".", "start_batch", "=", "0", "\n", "self", ".", "scheduler_state", "=", "None", "\n", "self", ".", "best_validation_result", "=", "None", "\n", "self", ".", "best_cp_name", "=", "None", "\n", "self", ".", "restart", "=", "self", ".", "args", ".", "restart", "\n", "# load state dict as well if the initialize_state is False.", "\n", "if", "saved_state", ":", "\n", "            ", "self", ".", "_load_saved_state", "(", "saved_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_reader.ReaderTrainer.get_data_iterator": [[80, 97], ["glob.glob", "logger.info", "train_reader.ReaderTrainer._get_preprocessed_files", "dpr.utils.data_utils.read_serialized_data_from_files", "dpr.utils.data_utils.ShardedDataIterator", "dpr.utils.data_utils.ShardedDataIterator.apply", "RuntimeError", "sample.on_deserialize"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_reader.ReaderTrainer._get_preprocessed_files", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.read_serialized_data_from_files", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.ShardedDataIterator.apply", "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data.ReaderSample.on_deserialize"], ["", "", "def", "get_data_iterator", "(", "self", ",", "path", ":", "str", ",", "batch_size", ":", "int", ",", "is_train", ":", "bool", ",", "shuffle", "=", "True", ",", "\n", "shuffle_seed", ":", "int", "=", "0", ",", "\n", "offset", ":", "int", "=", "0", ")", "->", "ShardedDataIterator", ":", "\n", "        ", "data_files", "=", "glob", ".", "glob", "(", "path", ")", "\n", "logger", ".", "info", "(", "\"Data files: %s\"", ",", "data_files", ")", "\n", "if", "not", "data_files", ":", "\n", "            ", "raise", "RuntimeError", "(", "'No Data files found'", ")", "\n", "", "preprocessed_data_files", "=", "self", ".", "_get_preprocessed_files", "(", "data_files", ",", "is_train", ")", "\n", "data", "=", "read_serialized_data_from_files", "(", "preprocessed_data_files", ")", "\n", "\n", "iterator", "=", "ShardedDataIterator", "(", "data", ",", "shard_id", "=", "self", ".", "shard_id", ",", "\n", "num_shards", "=", "self", ".", "distributed_factor", ",", "\n", "batch_size", "=", "batch_size", ",", "shuffle", "=", "shuffle", ",", "shuffle_seed", "=", "shuffle_seed", ",", "offset", "=", "offset", ")", "\n", "\n", "# apply deserialization hook", "\n", "iterator", ".", "apply", "(", "lambda", "sample", ":", "sample", ".", "on_deserialize", "(", ")", ")", "\n", "return", "iterator", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_reader.ReaderTrainer.run_train": [[98, 134], ["train_reader.ReaderTrainer.get_data_iterator", "logger.info", "logger.info", "dpr.utils.model_utils.get_schedule_linear", "logger.info", "logger.info", "range", "logger.info", "dpr.utils.model_utils.get_schedule_linear.load_state_dict", "int", "logger.info", "train_reader.ReaderTrainer._train_epoch", "logger.info"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer.get_data_iterator", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.get_schedule_linear", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer._train_epoch"], ["", "def", "run_train", "(", "self", ")", ":", "\n", "        ", "args", "=", "self", ".", "args", "\n", "\n", "train_iterator", "=", "self", ".", "get_data_iterator", "(", "args", ".", "train_file", ",", "args", ".", "batch_size", ",", "\n", "True", ",", "\n", "shuffle", "=", "True", ",", "\n", "shuffle_seed", "=", "args", ".", "seed", ",", "offset", "=", "self", ".", "start_batch", ")", "\n", "\n", "num_train_epochs", "=", "args", ".", "num_train_epochs", "-", "self", ".", "start_epoch", "\n", "\n", "logger", ".", "info", "(", "\"Total iterations per epoch=%d\"", ",", "train_iterator", ".", "max_iterations", ")", "\n", "updates_per_epoch", "=", "train_iterator", ".", "max_iterations", "//", "args", ".", "gradient_accumulation_steps", "\n", "total_updates", "=", "updates_per_epoch", "*", "num_train_epochs", "-", "self", ".", "start_batch", "\n", "logger", ".", "info", "(", "\" Total updates=%d\"", ",", "total_updates", ")", "\n", "\n", "warmup_steps", "=", "args", ".", "warmup_steps", "\n", "scheduler", "=", "get_schedule_linear", "(", "self", ".", "optimizer", ",", "warmup_steps", "=", "warmup_steps", ",", "\n", "training_steps", "=", "total_updates", ")", "\n", "if", "self", ".", "scheduler_state", ":", "\n", "            ", "logger", ".", "info", "(", "\"Loading scheduler state %s\"", ",", "self", ".", "scheduler_state", ")", "\n", "scheduler", ".", "load_state_dict", "(", "self", ".", "scheduler_state", ")", "\n", "\n", "", "eval_step", "=", "args", ".", "eval_step", "\n", "logger", ".", "info", "(", "\"  Eval step = %d\"", ",", "eval_step", ")", "\n", "logger", ".", "info", "(", "\"***** Training *****\"", ")", "\n", "\n", "global_step", "=", "self", ".", "start_epoch", "*", "updates_per_epoch", "+", "self", ".", "start_batch", "\n", "\n", "for", "epoch", "in", "range", "(", "self", ".", "start_epoch", ",", "int", "(", "args", ".", "num_train_epochs", ")", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Epoch %d *****\"", ",", "epoch", ")", "\n", "global_step", "=", "self", ".", "_train_epoch", "(", "scheduler", ",", "epoch", ",", "eval_step", ",", "train_iterator", ",", "global_step", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "            ", "logger", ".", "info", "(", "'Training finished. Best validation checkpoint %s'", ",", "self", ".", "best_cp_name", ")", "\n", "\n", "", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_reader.ReaderTrainer.validate_and_save": [[135, 150], ["train_reader.ReaderTrainer.validate", "train_reader.ReaderTrainer._save_checkpoint", "logger.info", "logger.info"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.validate", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer._save_checkpoint"], ["", "def", "validate_and_save", "(", "self", ",", "epoch", ":", "int", ",", "iteration", ":", "int", ",", "scheduler", ")", ":", "\n", "        ", "args", "=", "self", ".", "args", "\n", "# in distributed DDP mode, save checkpoint for only one process", "\n", "save_cp", "=", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", "\n", "reader_validation_score", "=", "self", ".", "validate", "(", ")", "\n", "self", ".", "best_validation_result", "=", "reader_validation_score", "\n", "\n", "if", "save_cp", ":", "\n", "            ", "cp_name", "=", "self", ".", "_save_checkpoint", "(", "scheduler", ",", "epoch", ",", "iteration", ")", "\n", "logger", ".", "info", "(", "'Saved checkpoint to %s'", ",", "cp_name", ")", "\n", "\n", "if", "reader_validation_score", "<", "(", "self", ".", "best_validation_result", "or", "0", ")", ":", "\n", "                ", "self", ".", "best_validation_result", "=", "reader_validation_score", "\n", "self", ".", "best_cp_name", "=", "cp_name", "\n", "logger", ".", "info", "(", "'New Best validation checkpoint %s'", ",", "cp_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_reader.ReaderTrainer.validate": [[151, 202], ["logger.info", "train_reader.ReaderTrainer.reader.eval", "train_reader.ReaderTrainer.get_data_iterator", "enumerate", "collections.defaultdict", "sorted", "train_reader.ReaderTrainer.iterate_data", "dpr.models.reader.create_reader_input", "dpr.models.reader.ReaderBatch", "train_reader.ReaderTrainer.tensorizer.get_attn_mask", "train_reader.ReaderTrainer._get_best_prediction", "all_results.extend", "span_predictions.items", "collections.defaultdict.keys", "numpy.mean", "logger.info", "train_reader.ReaderTrainer._save_predictions", "train_reader.ReaderTrainer.tensorizer.get_pad_id", "torch.no_grad", "train_reader.ReaderTrainer.reader", "logger.info", "len", "max", "ems[].append", "dpr.utils.model_utils.move_to_device", "dpr.models.reader.ReaderBatch._asdict", "dpr.data.qa_validation.exact_match_score"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer.get_data_iterator", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.ShardedDataIterator.iterate_data", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader.create_reader_input", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.get_attn_mask", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_reader.ReaderTrainer._get_best_prediction", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_reader.ReaderTrainer._save_predictions", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.get_pad_id", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.move_to_device", "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.qa_validation.exact_match_score"], ["", "", "", "def", "validate", "(", "self", ")", ":", "\n", "        ", "logger", ".", "info", "(", "'Validation ...'", ")", "\n", "args", "=", "self", ".", "args", "\n", "self", ".", "reader", ".", "eval", "(", ")", "\n", "data_iterator", "=", "self", ".", "get_data_iterator", "(", "args", ".", "dev_file", ",", "args", ".", "dev_batch_size", ",", "False", ",", "shuffle", "=", "False", ")", "\n", "\n", "log_result_step", "=", "args", ".", "log_batch_step", "\n", "all_results", "=", "[", "]", "\n", "\n", "eval_top_docs", "=", "args", ".", "eval_top_docs", "\n", "for", "i", ",", "samples_batch", "in", "enumerate", "(", "data_iterator", ".", "iterate_data", "(", ")", ")", ":", "\n", "            ", "input", "=", "create_reader_input", "(", "self", ".", "tensorizer", ".", "get_pad_id", "(", ")", ",", "\n", "samples_batch", ",", "\n", "args", ".", "passages_per_question_predict", ",", "\n", "args", ".", "sequence_length", ",", "\n", "args", ".", "max_n_answers", ",", "\n", "is_train", "=", "False", ",", "shuffle", "=", "False", ")", "\n", "\n", "input", "=", "ReaderBatch", "(", "**", "move_to_device", "(", "input", ".", "_asdict", "(", ")", ",", "args", ".", "device", ")", ")", "\n", "attn_mask", "=", "self", ".", "tensorizer", ".", "get_attn_mask", "(", "input", ".", "input_ids", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "start_logits", ",", "end_logits", ",", "relevance_logits", "=", "self", ".", "reader", "(", "input", ".", "input_ids", ",", "attn_mask", ")", "\n", "\n", "", "batch_predictions", "=", "self", ".", "_get_best_prediction", "(", "start_logits", ",", "end_logits", ",", "relevance_logits", ",", "samples_batch", ",", "\n", "passage_thresholds", "=", "eval_top_docs", ")", "\n", "\n", "all_results", ".", "extend", "(", "batch_predictions", ")", "\n", "\n", "if", "(", "i", "+", "1", ")", "%", "log_result_step", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "'Eval step: %d '", ",", "i", ")", "\n", "\n", "", "", "ems", "=", "defaultdict", "(", "list", ")", "\n", "\n", "for", "q_predictions", "in", "all_results", ":", "\n", "            ", "gold_answers", "=", "q_predictions", ".", "gold_answers", "\n", "if", "len", "(", "gold_answers", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "", "span_predictions", "=", "q_predictions", ".", "predictions", "# {top docs threshold -> SpanPrediction()}", "\n", "for", "(", "n", ",", "span_prediction", ")", "in", "span_predictions", ".", "items", "(", ")", ":", "\n", "                ", "em_hit", "=", "max", "(", "[", "exact_match_score", "(", "span_prediction", ".", "prediction_text", ",", "ga", ")", "for", "ga", "in", "gold_answers", "]", ")", "\n", "ems", "[", "n", "]", ".", "append", "(", "em_hit", ")", "\n", "", "", "em", "=", "0", "\n", "for", "n", "in", "sorted", "(", "ems", ".", "keys", "(", ")", ")", ":", "\n", "            ", "em", "=", "np", ".", "mean", "(", "ems", "[", "n", "]", ")", "\n", "logger", ".", "info", "(", "\"n=%d\\tEM %.2f\"", "%", "(", "n", ",", "em", "*", "100", ")", ")", "\n", "\n", "", "if", "args", ".", "prediction_results_file", ":", "\n", "            ", "self", ".", "_save_predictions", "(", "args", ".", "prediction_results_file", ",", "all_results", ")", "\n", "\n", "", "return", "em", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_reader.ReaderTrainer._train_epoch": [[203, 278], ["train_reader.ReaderTrainer.reader.train", "enumerate", "logger.info", "train_data_iterator.iterate_data", "train_data_iterator.get_iteration", "dpr.models.reader.create_reader_input", "train_reader.ReaderTrainer._calc_loss", "train_reader.ReaderTrainer.item", "train_reader.ReaderTrainer.item", "numpy.random.seed", "torch.manual_seed", "train_reader.ReaderTrainer.tensorizer.get_pad_id", "train_reader.ReaderTrainer.backward", "train_reader.ReaderTrainer.optimizer.step", "scheduler.step", "train_reader.ReaderTrainer.reader.zero_grad", "logger.info", "logger.info", "logger.info", "logger.info", "train_reader.ReaderTrainer.validate_and_save", "train_reader.ReaderTrainer.reader.train", "torch.cuda.manual_seed_all", "amp.scale_loss", "scaled_loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "train_data_iterator.get_iteration", "amp.master_params", "train_reader.ReaderTrainer.reader.parameters"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.ShardedDataIterator.iterate_data", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.ShardedDataIterator.get_iteration", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader.create_reader_input", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder._calc_loss", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.get_pad_id", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer.validate_and_save", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.ShardedDataIterator.get_iteration"], ["", "def", "_train_epoch", "(", "self", ",", "scheduler", ",", "epoch", ":", "int", ",", "eval_step", ":", "int", ",", "\n", "train_data_iterator", ":", "ShardedDataIterator", ",", "global_step", ":", "int", ")", ":", "\n", "        ", "args", "=", "self", ".", "args", "\n", "rolling_train_loss", "=", "0.0", "\n", "epoch_loss", "=", "0", "\n", "log_result_step", "=", "args", ".", "log_batch_step", "\n", "rolling_loss_step", "=", "args", ".", "train_rolling_loss_step", "\n", "\n", "self", ".", "reader", ".", "train", "(", ")", "\n", "epoch_batches", "=", "train_data_iterator", ".", "max_iterations", "\n", "\n", "for", "i", ",", "samples_batch", "in", "enumerate", "(", "train_data_iterator", ".", "iterate_data", "(", "epoch", "=", "epoch", ")", ")", ":", "\n", "\n", "            ", "data_iteration", "=", "train_data_iterator", ".", "get_iteration", "(", ")", "\n", "\n", "# enables to resume to exactly same train state", "\n", "if", "args", ".", "fully_resumable", ":", "\n", "                ", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", "+", "global_step", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", "+", "global_step", ")", "\n", "if", "args", ".", "n_gpu", ">", "0", ":", "\n", "                    ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", "+", "global_step", ")", "\n", "\n", "", "", "input", "=", "create_reader_input", "(", "self", ".", "tensorizer", ".", "get_pad_id", "(", ")", ",", "\n", "samples_batch", ",", "\n", "args", ".", "passages_per_question", ",", "\n", "args", ".", "sequence_length", ",", "\n", "args", ".", "max_n_answers", ",", "\n", "is_train", "=", "True", ",", "shuffle", "=", "True", ")", "\n", "# skip empty examples", "\n", "if", "input", "is", "None", ":", "\n", "                ", "continue", "\n", "\n", "", "loss", "=", "self", ".", "_calc_loss", "(", "input", ")", "\n", "\n", "epoch_loss", "+=", "loss", ".", "item", "(", ")", "\n", "rolling_train_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "if", "args", ".", "fp16", ":", "\n", "                ", "from", "apex", "import", "amp", "\n", "with", "amp", ".", "scale_loss", "(", "loss", ",", "self", ".", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                    ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "if", "args", ".", "max_grad_norm", ">", "0", ":", "\n", "                    ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "amp", ".", "master_params", "(", "self", ".", "optimizer", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "", "", "else", ":", "\n", "                ", "loss", ".", "backward", "(", ")", "\n", "if", "args", ".", "max_grad_norm", ">", "0", ":", "\n", "                    ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "reader", ".", "parameters", "(", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "\n", "", "", "global_step", "+=", "1", "\n", "\n", "if", "(", "i", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", ")", "\n", "self", ".", "reader", ".", "zero_grad", "(", ")", "\n", "\n", "", "if", "global_step", "%", "log_result_step", "==", "0", ":", "\n", "                ", "lr", "=", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "logger", ".", "info", "(", "\n", "'Epoch: %d: Step: %d/%d, global_step=%d, lr=%f'", ",", "epoch", ",", "data_iteration", ",", "epoch_batches", ",", "global_step", ",", "\n", "lr", ")", "\n", "\n", "", "if", "(", "i", "+", "1", ")", "%", "rolling_loss_step", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "'Train batch %d'", ",", "data_iteration", ")", "\n", "latest_rolling_train_av_loss", "=", "rolling_train_loss", "/", "rolling_loss_step", "\n", "logger", ".", "info", "(", "'Avg. loss per last %d batches: %f'", ",", "rolling_loss_step", ",", "latest_rolling_train_av_loss", ")", "\n", "rolling_train_loss", "=", "0.0", "\n", "\n", "", "if", "global_step", "%", "eval_step", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "'Validation: Epoch: %d Step: %d/%d'", ",", "epoch", ",", "data_iteration", ",", "epoch_batches", ")", "\n", "self", ".", "validate_and_save", "(", "epoch", ",", "train_data_iterator", ".", "get_iteration", "(", ")", ",", "scheduler", ")", "\n", "self", ".", "reader", ".", "train", "(", ")", "\n", "\n", "", "", "epoch_loss", "=", "(", "epoch_loss", "/", "epoch_batches", ")", "if", "epoch_batches", ">", "0", "else", "0", "\n", "logger", ".", "info", "(", "'Av Loss per epoch=%f'", ",", "epoch_loss", ")", "\n", "return", "global_step", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_reader.ReaderTrainer._save_checkpoint": [[279, 292], ["dpr.utils.model_utils.get_model_obj", "os.path.join", "dpr.options.get_encoder_params_state", "dpr.utils.model_utils.CheckpointState", "torch.save", "dpr.utils.model_utils.get_model_obj.state_dict", "train_reader.ReaderTrainer.optimizer.state_dict", "scheduler.state_dict", "dpr.utils.model_utils.CheckpointState._asdict", "str", "str"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.get_model_obj", "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.get_encoder_params_state"], ["", "def", "_save_checkpoint", "(", "self", ",", "scheduler", ",", "epoch", ":", "int", ",", "offset", ":", "int", ")", "->", "str", ":", "\n", "        ", "args", "=", "self", ".", "args", "\n", "model_to_save", "=", "get_model_obj", "(", "self", ".", "reader", ")", "\n", "cp", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\n", "args", ".", "checkpoint_file_name", "+", "'.'", "+", "str", "(", "epoch", ")", "+", "(", "'.'", "+", "str", "(", "offset", ")", "if", "offset", ">", "0", "else", "''", ")", ")", "\n", "\n", "meta_params", "=", "get_encoder_params_state", "(", "args", ")", "\n", "\n", "state", "=", "CheckpointState", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "self", ".", "optimizer", ".", "state_dict", "(", ")", ",", "scheduler", ".", "state_dict", "(", ")", ",", "offset", ",", "\n", "epoch", ",", "meta_params", "\n", ")", "\n", "torch", ".", "save", "(", "state", ".", "_asdict", "(", ")", ",", "cp", ")", "\n", "return", "cp", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_reader.ReaderTrainer._load_saved_state": [[293, 314], ["logger.info", "dpr.utils.model_utils.get_model_obj", "logger.info", "dpr.utils.model_utils.get_model_obj.load_state_dict", "logger.info", "train_reader.ReaderTrainer.optimizer.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.get_model_obj"], ["", "def", "_load_saved_state", "(", "self", ",", "saved_state", ":", "CheckpointState", ")", ":", "\n", "        ", "epoch", "=", "saved_state", ".", "epoch", "\n", "offset", "=", "saved_state", ".", "offset", "\n", "if", "offset", "==", "0", ":", "# epoch has been completed", "\n", "            ", "epoch", "+=", "1", "\n", "", "logger", ".", "info", "(", "'Loading checkpoint @ batch=%s and epoch=%s'", ",", "offset", ",", "epoch", ")", "\n", "if", "self", ".", "restart", "is", "False", ":", "\n", "            ", "self", ".", "start_epoch", "=", "epoch", "\n", "self", ".", "start_batch", "=", "offset", "\n", "\n", "", "model_to_load", "=", "get_model_obj", "(", "self", ".", "reader", ")", "\n", "if", "saved_state", ".", "model_dict", ":", "\n", "            ", "logger", ".", "info", "(", "'Loading model weights from saved state ...'", ")", "\n", "model_to_load", ".", "load_state_dict", "(", "saved_state", ".", "model_dict", ")", "\n", "\n", "", "if", "self", ".", "restart", "is", "False", "and", "saved_state", ".", "optimizer_dict", ":", "\n", "            ", "logger", ".", "info", "(", "'Loading saved optimizer state ...'", ")", "\n", "self", ".", "optimizer", ".", "load_state_dict", "(", "saved_state", ".", "optimizer_dict", ")", "\n", "\n", "", "if", "self", ".", "restart", "is", "False", "and", "saved_state", ".", "scheduler_dict", ":", "\n", "            ", "self", ".", "scheduler_state", "=", "saved_state", ".", "scheduler_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_reader.ReaderTrainer._get_best_prediction": [[315, 364], ["relevance_logits.size", "torch.sort", "range", "len", "range", "batch_results.append", "idxs[].item", "sequence_ids.size", "dpr.data.reader_data.get_best_spans", "nbest.extend", "ReaderQuestionPredictions", "start_logits[].tolist", "end_logits[].tolist", "sequence_ids.tolist", "relevance_logits[].item", "len", "len", "dpr.data.reader_data.SpanPrediction"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data.get_best_spans"], ["", "", "def", "_get_best_prediction", "(", "self", ",", "start_logits", ",", "end_logits", ",", "relevance_logits", ",", "\n", "samples_batch", ":", "List", "[", "ReaderSample", "]", ",", "passage_thresholds", ":", "List", "[", "int", "]", "=", "None", ")", "->", "List", "[", "ReaderQuestionPredictions", "]", ":", "\n", "\n", "        ", "args", "=", "self", ".", "args", "\n", "max_answer_length", "=", "args", ".", "max_answer_length", "\n", "questions_num", ",", "passages_per_question", "=", "relevance_logits", ".", "size", "(", ")", "\n", "\n", "_", ",", "idxs", "=", "torch", ".", "sort", "(", "relevance_logits", ",", "dim", "=", "1", ",", "descending", "=", "True", ",", ")", "\n", "\n", "batch_results", "=", "[", "]", "\n", "for", "q", "in", "range", "(", "questions_num", ")", ":", "\n", "            ", "sample", "=", "samples_batch", "[", "q", "]", "\n", "\n", "non_empty_passages_num", "=", "len", "(", "sample", ".", "passages", ")", "\n", "nbest", "=", "[", "]", "\n", "for", "p", "in", "range", "(", "passages_per_question", ")", ":", "\n", "                ", "passage_idx", "=", "idxs", "[", "q", ",", "p", "]", ".", "item", "(", ")", "\n", "if", "passage_idx", ">=", "non_empty_passages_num", ":", "# empty passage selected, skip", "\n", "                    ", "continue", "\n", "", "reader_passage", "=", "sample", ".", "passages", "[", "passage_idx", "]", "\n", "sequence_ids", "=", "reader_passage", ".", "sequence_ids", "\n", "sequence_len", "=", "sequence_ids", ".", "size", "(", "0", ")", "\n", "# assuming question & title information is at the beginning of the sequence", "\n", "passage_offset", "=", "reader_passage", ".", "passage_offset", "\n", "\n", "p_start_logits", "=", "start_logits", "[", "q", ",", "passage_idx", "]", ".", "tolist", "(", ")", "[", "passage_offset", ":", "sequence_len", "]", "\n", "p_end_logits", "=", "end_logits", "[", "q", ",", "passage_idx", "]", ".", "tolist", "(", ")", "[", "passage_offset", ":", "sequence_len", "]", "\n", "\n", "ctx_ids", "=", "sequence_ids", ".", "tolist", "(", ")", "[", "passage_offset", ":", "]", "\n", "best_spans", "=", "get_best_spans", "(", "self", ".", "tensorizer", ",", "p_start_logits", ",", "p_end_logits", ",", "ctx_ids", ",", "max_answer_length", ",", "\n", "passage_idx", ",", "relevance_logits", "[", "q", ",", "passage_idx", "]", ".", "item", "(", ")", ",", "top_spans", "=", "10", ")", "\n", "nbest", ".", "extend", "(", "best_spans", ")", "\n", "if", "len", "(", "nbest", ")", ">", "0", "and", "not", "passage_thresholds", ":", "\n", "                    ", "break", "\n", "\n", "", "", "if", "passage_thresholds", ":", "\n", "                ", "passage_rank_matches", "=", "{", "}", "\n", "for", "n", "in", "passage_thresholds", ":", "\n", "                    ", "curr_nbest", "=", "[", "pred", "for", "pred", "in", "nbest", "if", "pred", ".", "passage_index", "<", "n", "]", "\n", "passage_rank_matches", "[", "n", "]", "=", "curr_nbest", "[", "0", "]", "\n", "", "predictions", "=", "passage_rank_matches", "\n", "", "else", ":", "\n", "                ", "if", "len", "(", "nbest", ")", "==", "0", ":", "\n", "                    ", "predictions", "=", "{", "passages_per_question", ":", "SpanPrediction", "(", "''", ",", "-", "1", ",", "-", "1", ",", "-", "1", ",", "''", ")", "}", "\n", "", "else", ":", "\n", "                    ", "predictions", "=", "{", "passages_per_question", ":", "nbest", "[", "0", "]", "}", "\n", "", "", "batch_results", ".", "append", "(", "ReaderQuestionPredictions", "(", "sample", ".", "question", ",", "predictions", ",", "sample", ".", "answers", ",", "sample", ".", "q_id", ")", ")", "\n", "", "return", "batch_results", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_reader.ReaderTrainer._calc_loss": [[365, 391], ["dpr.models.reader.ReaderBatch", "train_reader.ReaderTrainer.tensorizer.get_attn_mask", "dpr.models.reader.ReaderBatch.input_ids.size", "train_reader.ReaderTrainer.reader", "dpr.models.reader.compute_loss", "loss.mean.mean.mean", "dpr.utils.model_utils.move_to_device", "torch.no_grad", "train_reader.ReaderTrainer.reader", "dpr.models.reader.ReaderBatch._asdict"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.get_attn_mask", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader.compute_loss", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.move_to_device"], ["", "def", "_calc_loss", "(", "self", ",", "input", ":", "ReaderBatch", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "args", "=", "self", ".", "args", "\n", "input", "=", "ReaderBatch", "(", "**", "move_to_device", "(", "input", ".", "_asdict", "(", ")", ",", "args", ".", "device", ")", ")", "\n", "attn_mask", "=", "self", ".", "tensorizer", ".", "get_attn_mask", "(", "input", ".", "input_ids", ")", "\n", "questions_num", ",", "passages_per_question", ",", "_", "=", "input", ".", "input_ids", ".", "size", "(", ")", "\n", "\n", "if", "self", ".", "reader", ".", "training", ":", "\n", "# start_logits, end_logits, rank_logits = self.reader(input.input_ids, attn_mask)", "\n", "            ", "loss", "=", "self", ".", "reader", "(", "input", ".", "input_ids", ",", "attn_mask", ",", "input", ".", "start_positions", ",", "input", ".", "end_positions", ",", "\n", "input", ".", "answers_mask", ")", "\n", "\n", "", "else", ":", "\n", "# TODO: remove?", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "start_logits", ",", "end_logits", ",", "rank_logits", "=", "self", ".", "reader", "(", "input", ".", "input_ids", ",", "attn_mask", ")", "\n", "\n", "", "loss", "=", "compute_loss", "(", "input", ".", "start_positions", ",", "input", ".", "end_positions", ",", "input", ".", "answers_mask", ",", "start_logits", ",", "\n", "end_logits", ",", "\n", "rank_logits", ",", "\n", "questions_num", ",", "passages_per_question", ")", "\n", "", "if", "args", ".", "n_gpu", ">", "1", ":", "\n", "            ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "            ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_reader.ReaderTrainer._get_preprocessed_files": [[392, 441], ["train_reader.ReaderTrainer._get_preprocessed_files._find_cached_files"], "methods", ["None"], ["", "def", "_get_preprocessed_files", "(", "self", ",", "data_files", ":", "List", ",", "is_train", ":", "bool", ",", ")", ":", "\n", "\n", "        ", "serialized_files", "=", "[", "file", "for", "file", "in", "data_files", "if", "file", ".", "endswith", "(", "'.pkl'", ")", "]", "\n", "if", "serialized_files", ":", "\n", "            ", "return", "serialized_files", "\n", "", "assert", "len", "(", "data_files", ")", "==", "1", ",", "'Only 1 source file pre-processing is supported.'", "\n", "\n", "# data may have been serialized and cached before, try to find ones from same dir", "\n", "def", "_find_cached_files", "(", "path", ":", "str", ")", ":", "\n", "            ", "dir_path", ",", "base_name", "=", "os", ".", "path", ".", "split", "(", "path", ")", "\n", "base_name", "=", "base_name", ".", "replace", "(", "'.json'", ",", "''", ")", "\n", "out_file_prefix", "=", "os", ".", "path", ".", "join", "(", "dir_path", ",", "base_name", ")", "\n", "out_file_pattern", "=", "out_file_prefix", "+", "'*.pkl'", "\n", "return", "glob", ".", "glob", "(", "out_file_pattern", ")", ",", "out_file_prefix", "\n", "\n", "", "serialized_files", ",", "out_file_prefix", "=", "_find_cached_files", "(", "data_files", "[", "0", "]", ")", "\n", "if", "serialized_files", ":", "\n", "            ", "logger", ".", "info", "(", "'Found preprocessed files. %s'", ",", "serialized_files", ")", "\n", "return", "serialized_files", "\n", "\n", "", "gold_passages_src", "=", "None", "\n", "if", "self", ".", "args", ".", "gold_passages_src", ":", "\n", "            ", "gold_passages_src", "=", "self", ".", "args", ".", "gold_passages_src", "if", "is_train", "else", "self", ".", "args", ".", "gold_passages_src_dev", "\n", "assert", "os", ".", "path", ".", "exists", "(", "gold_passages_src", ")", ",", "'Please specify valid gold_passages_src/gold_passages_src_dev'", "\n", "", "logger", ".", "info", "(", "'Data are not preprocessed for reader training. Start pre-processing ...'", ")", "\n", "\n", "# start pre-processing and save results", "\n", "def", "_run_preprocessing", "(", "tensorizer", ":", "Tensorizer", ")", ":", "\n", "# temporarily disable auto-padding to save disk space usage of serialized files", "\n", "            ", "tensorizer", ".", "set_pad_to_max", "(", "False", ")", "\n", "serialized_files", "=", "convert_retriever_results", "(", "is_train", ",", "data_files", "[", "0", "]", ",", "out_file_prefix", ",", "\n", "gold_passages_src", ",", "\n", "self", ".", "tensorizer", ",", "\n", "num_workers", "=", "self", ".", "args", ".", "num_workers", ")", "\n", "tensorizer", ".", "set_pad_to_max", "(", "True", ")", "\n", "return", "serialized_files", "\n", "\n", "", "if", "self", ".", "distributed_factor", ">", "1", ":", "\n", "# only one node in DDP model will do pre-processing", "\n", "            ", "if", "self", ".", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "                ", "serialized_files", "=", "_run_preprocessing", "(", "self", ".", "tensorizer", ")", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "", "else", ":", "\n", "                ", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "serialized_files", "=", "_find_cached_files", "(", "data_files", "[", "0", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "serialized_files", "=", "_run_preprocessing", "(", "self", ".", "tensorizer", ")", "\n", "\n", "", "return", "serialized_files", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_reader.ReaderTrainer._save_predictions": [[442, 470], ["logger.info", "open", "output.write", "r.predictions.items", "open", "output.write", "save_results.append", "json.dumps", "os.path.basename", "json.dumps", "out_file.split", "r.predictions.items", "train_reader.ReaderTrainer.tensorizer.to_string"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.to_string"], ["", "def", "_save_predictions", "(", "self", ",", "out_file", ":", "str", ",", "prediction_results", ":", "List", "[", "ReaderQuestionPredictions", "]", ")", ":", "\n", "        ", "logger", ".", "info", "(", "'Saving prediction results to  %s'", ",", "out_file", ")", "\n", "with", "open", "(", "out_file", ",", "'w'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "output", ":", "\n", "            ", "save_results", "=", "[", "]", "\n", "for", "r", "in", "prediction_results", ":", "\n", "                ", "save_results", ".", "append", "(", "{", "\n", "'question'", ":", "r", ".", "id", ",", "\n", "'gold_answers'", ":", "r", ".", "gold_answers", ",", "\n", "'predictions'", ":", "[", "{", "\n", "'top_k'", ":", "top_k", ",", "\n", "'prediction'", ":", "{", "\n", "'text'", ":", "span_pred", ".", "prediction_text", ",", "\n", "'score'", ":", "span_pred", ".", "span_score", ",", "\n", "'relevance_score'", ":", "span_pred", ".", "relevance_score", ",", "\n", "'passage_idx'", ":", "span_pred", ".", "passage_index", ",", "\n", "'passage'", ":", "self", ".", "tensorizer", ".", "to_string", "(", "span_pred", ".", "passage_token_ids", ")", "\n", "}", "\n", "}", "for", "top_k", ",", "span_pred", "in", "r", ".", "predictions", ".", "items", "(", ")", "]", "\n", "}", ")", "\n", "", "output", ".", "write", "(", "json", ".", "dumps", "(", "save_results", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "# output SQuAD format", "\n", "", "reader_save_results", "=", "{", "}", "\n", "for", "r", "in", "prediction_results", ":", "\n", "            ", "for", "top_k", ",", "span_pred", "in", "r", ".", "predictions", ".", "items", "(", ")", ":", "\n", "                ", "reader_save_results", "[", "r", ".", "q_id", "]", "=", "span_pred", ".", "prediction_text", "\n", "", "", "with", "open", "(", "\"{}_predictions.json\"", ".", "format", "(", "os", ".", "path", ".", "basename", "(", "out_file", ".", "split", "(", "\".\"", ")", "[", "0", "]", ")", ")", ",", "'w'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "output", ":", "\n", "            ", "output", ".", "write", "(", "json", ".", "dumps", "(", "reader_save_results", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_reader.main": [[471, 523], ["argparse.ArgumentParser", "dpr.options.add_encoder_params", "dpr.options.add_training_params", "dpr.options.add_tokenizer_params", "dpr.options.add_reader_preprocessing_params", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "dpr.options.setup_args_gpu", "dpr.options.set_seed", "dpr.options.print_args", "train_reader.ReaderTrainer", "os.makedirs", "train_reader.ReaderTrainer.run_train", "logger.info", "train_reader.ReaderTrainer.validate", "logger.warning"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.add_encoder_params", "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.add_training_params", "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.add_tokenizer_params", "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.add_reader_preprocessing_params", "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.setup_args_gpu", "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.set_seed", "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.print_args", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer.run_train", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.validate"], ["", "", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "add_encoder_params", "(", "parser", ")", "\n", "add_training_params", "(", "parser", ")", "\n", "add_tokenizer_params", "(", "parser", ")", "\n", "add_reader_preprocessing_params", "(", "parser", ")", "\n", "\n", "# reader specific params", "\n", "parser", ".", "add_argument", "(", "\"--max_n_answers\"", ",", "default", "=", "10", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Max amount of answer spans to marginalize per singe passage\"", ")", "\n", "parser", ".", "add_argument", "(", "'--passages_per_question'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "\n", "help", "=", "\"Total amount of positive and negative passages per question\"", ")", "\n", "parser", ".", "add_argument", "(", "'--passages_per_question_predict'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "\n", "help", "=", "\"Total amount of positive and negative passages per question for evaluation\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_answer_length\"", ",", "default", "=", "10", ",", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum length of an answer that can be generated. This is needed because the start \"", "\n", "\"and end predictions are not conditioned on one another.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--eval_top_docs'", ",", "nargs", "=", "'+'", ",", "type", "=", "int", ",", "\n", "help", "=", "\"top retrival passages thresholds to analyze prediction results for\"", ")", "\n", "parser", ".", "add_argument", "(", "'--checkpoint_file_name'", ",", "type", "=", "str", ",", "default", "=", "'dpr_reader'", ")", "\n", "parser", ".", "add_argument", "(", "'--prediction_results_file'", ",", "type", "=", "str", ",", "help", "=", "'path to a file to write prediction results to'", ")", "\n", "\n", "# training parameters", "\n", "parser", ".", "add_argument", "(", "\"--eval_step\"", ",", "default", "=", "2000", ",", "type", "=", "int", ",", "\n", "help", "=", "\"batch steps to run validation and save checkpoint\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The output directory where the model checkpoints will be written to\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--fully_resumable'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Enables resumable mode by specifying global step dependent random seed before shuffling \"", "\n", "\"in-batch data\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--restart\"", ",", "action", "=", "'store_true'", ",", "help", "=", "\"set true if you want to reset the optimizer and schedular states when you start fine-tuning from pre-trained models\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "output_dir", "is", "not", "None", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "setup_args_gpu", "(", "args", ")", "\n", "set_seed", "(", "args", ")", "\n", "print_args", "(", "args", ")", "\n", "\n", "trainer", "=", "ReaderTrainer", "(", "args", ")", "\n", "\n", "if", "args", ".", "train_file", "is", "not", "None", ":", "\n", "        ", "trainer", ".", "run_train", "(", ")", "\n", "", "elif", "args", ".", "dev_file", ":", "\n", "        ", "logger", ".", "info", "(", "\"No train files are specified. Run validation.\"", ")", "\n", "trainer", ".", "validate", "(", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "warning", "(", "\"Neither train_file or (model_file & dev_file) parameters are specified. Nothing to do.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer.__init__": [[53, 84], ["logger.info", "dpr.utils.model_utils.get_model_file", "dpr.models.init_biencoder_components", "dpr.utils.model_utils.setup_for_distributed_mode", "dpr.utils.model_utils.load_states_from_checkpoint", "dpr.options.set_encoder_params_from_state", "train_dense_encoder.BiEncoderTrainer._load_saved_state"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.get_model_file", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.__init__.init_biencoder_components", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.setup_for_distributed_mode", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.load_states_from_checkpoint", "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.set_encoder_params_from_state", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer._load_saved_state"], ["def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "shard_id", "=", "args", ".", "local_rank", "if", "args", ".", "local_rank", "!=", "-", "1", "else", "0", "\n", "self", ".", "distributed_factor", "=", "args", ".", "distributed_world_size", "or", "1", "\n", "\n", "logger", ".", "info", "(", "\"***** Initializing components for training *****\"", ")", "\n", "\n", "# if model file is specified, encoder parameters from saved state should be used for initialization", "\n", "model_file", "=", "get_model_file", "(", "self", ".", "args", ",", "self", ".", "args", ".", "checkpoint_file_name", ")", "\n", "saved_state", "=", "None", "\n", "if", "model_file", ":", "\n", "            ", "saved_state", "=", "load_states_from_checkpoint", "(", "model_file", ")", "\n", "set_encoder_params_from_state", "(", "saved_state", ".", "encoder_params", ",", "args", ")", "\n", "\n", "", "tensorizer", ",", "model", ",", "optimizer", "=", "init_biencoder_components", "(", "args", ".", "encoder_model_type", ",", "args", ")", "\n", "\n", "model", ",", "optimizer", "=", "setup_for_distributed_mode", "(", "model", ",", "optimizer", ",", "args", ".", "device", ",", "args", ".", "n_gpu", ",", "\n", "args", ".", "local_rank", ",", "\n", "args", ".", "fp16", ",", "\n", "args", ".", "fp16_opt_level", ")", "\n", "self", ".", "biencoder", "=", "model", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "tensorizer", "=", "tensorizer", "\n", "self", ".", "start_epoch", "=", "0", "\n", "self", ".", "start_batch", "=", "0", "\n", "self", ".", "scheduler_state", "=", "None", "\n", "self", ".", "best_validation_result", "=", "None", "\n", "self", ".", "best_cp_name", "=", "None", "\n", "self", ".", "restart", "=", "self", ".", "args", ".", "restart", "\n", "if", "saved_state", ":", "\n", "            ", "self", ".", "_load_saved_state", "(", "saved_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer.get_data_iterator": [[85, 99], ["glob.glob", "dpr.utils.data_utils.read_data_from_json_files", "logger.info", "dpr.utils.data_utils.ShardedDataIterator", "len", "len"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.read_data_from_json_files"], ["", "", "def", "get_data_iterator", "(", "self", ",", "path", ":", "str", ",", "batch_size", ":", "int", ",", "shuffle", "=", "True", ",", "\n", "shuffle_seed", ":", "int", "=", "0", ",", "\n", "offset", ":", "int", "=", "0", ",", "upsample_rates", ":", "list", "=", "None", ")", "->", "ShardedDataIterator", ":", "\n", "        ", "data_files", "=", "glob", ".", "glob", "(", "path", ")", "\n", "data", "=", "read_data_from_json_files", "(", "data_files", ",", "upsample_rates", ")", "\n", "\n", "# filter those without positive ctx", "\n", "data", "=", "[", "r", "for", "r", "in", "data", "if", "len", "(", "r", "[", "'positive_ctxs'", "]", ")", ">", "0", "]", "\n", "logger", ".", "info", "(", "'Total cleaned data size: {}'", ".", "format", "(", "len", "(", "data", ")", ")", ")", "\n", "\n", "return", "ShardedDataIterator", "(", "data", ",", "shard_id", "=", "self", ".", "shard_id", ",", "\n", "num_shards", "=", "self", ".", "distributed_factor", ",", "\n", "batch_size", "=", "batch_size", ",", "shuffle", "=", "shuffle", ",", "shuffle_seed", "=", "shuffle_seed", ",", "offset", "=", "offset", ",", "\n", "strict_batch_size", "=", "True", ",", "# this is not really necessary, one can probably disable it", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer.run_train": [[101, 134], ["train_dense_encoder.BiEncoderTrainer.get_data_iterator", "logger.info", "logger.info", "dpr.utils.model_utils.get_schedule_linear", "math.ceil", "logger.info", "logger.info", "range", "eval", "max", "logger.info", "dpr.utils.model_utils.get_schedule_linear.load_state_dict", "int", "logger.info", "train_dense_encoder.BiEncoderTrainer._train_epoch", "logger.info"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer.get_data_iterator", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.get_schedule_linear", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer._train_epoch"], ["", "def", "run_train", "(", "self", ",", ")", ":", "\n", "        ", "args", "=", "self", ".", "args", "\n", "upsample_rates", "=", "None", "\n", "if", "args", ".", "train_files_upsample_rates", "is", "not", "None", ":", "\n", "            ", "upsample_rates", "=", "eval", "(", "args", ".", "train_files_upsample_rates", ")", "\n", "\n", "", "train_iterator", "=", "self", ".", "get_data_iterator", "(", "args", ".", "train_file", ",", "args", ".", "batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "shuffle_seed", "=", "args", ".", "seed", ",", "offset", "=", "self", ".", "start_batch", ",", "\n", "upsample_rates", "=", "upsample_rates", ")", "\n", "\n", "logger", ".", "info", "(", "\"  Total iterations per epoch=%d\"", ",", "train_iterator", ".", "max_iterations", ")", "\n", "updates_per_epoch", "=", "train_iterator", ".", "max_iterations", "//", "args", ".", "gradient_accumulation_steps", "\n", "total_updates", "=", "max", "(", "updates_per_epoch", "*", "(", "args", ".", "num_train_epochs", "-", "self", ".", "start_epoch", "-", "1", ")", ",", "0", ")", "+", "(", "train_iterator", ".", "max_iterations", "-", "self", ".", "start_batch", ")", "//", "args", ".", "gradient_accumulation_steps", "\n", "logger", ".", "info", "(", "\" Total updates=%d\"", ",", "total_updates", ")", "\n", "warmup_steps", "=", "args", ".", "warmup_steps", "\n", "scheduler", "=", "get_schedule_linear", "(", "self", ".", "optimizer", ",", "warmup_steps", ",", "total_updates", ")", "\n", "\n", "if", "self", ".", "scheduler_state", "and", "self", ".", "restart", "is", "False", ":", "\n", "            ", "logger", ".", "info", "(", "\"Loading scheduler state %s\"", ",", "self", ".", "scheduler_state", ")", "\n", "scheduler", ".", "load_state_dict", "(", "self", ".", "scheduler_state", ")", "\n", "\n", "", "eval_step", "=", "math", ".", "ceil", "(", "updates_per_epoch", "/", "args", ".", "eval_per_epoch", ")", "\n", "logger", ".", "info", "(", "\"  Eval step = %d\"", ",", "eval_step", ")", "\n", "logger", ".", "info", "(", "\"***** Training *****\"", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "self", ".", "start_epoch", ",", "int", "(", "args", ".", "num_train_epochs", ")", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Epoch %d *****\"", ",", "epoch", ")", "\n", "self", ".", "_train_epoch", "(", "scheduler", ",", "epoch", ",", "eval_step", ",", "train_iterator", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "            ", "logger", ".", "info", "(", "'Training finished. Best validation checkpoint %s'", ",", "self", ".", "best_cp_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer.validate_and_save": [[135, 156], ["train_dense_encoder.BiEncoderTrainer.validate_average_rank", "train_dense_encoder.BiEncoderTrainer.validate_nll", "train_dense_encoder.BiEncoderTrainer._save_checkpoint", "logger.info", "logger.info"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer.validate_average_rank", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer.validate_nll", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer._save_checkpoint"], ["", "", "def", "validate_and_save", "(", "self", ",", "epoch", ":", "int", ",", "iteration", ":", "int", ",", "scheduler", ")", ":", "\n", "        ", "args", "=", "self", ".", "args", "\n", "# for distributed mode, save checkpoint for only one process", "\n", "save_cp", "=", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", "\n", "\n", "if", "epoch", "==", "args", ".", "val_av_rank_start_epoch", ":", "\n", "            ", "self", ".", "best_validation_result", "=", "None", "\n", "\n", "", "if", "epoch", ">=", "args", ".", "val_av_rank_start_epoch", ":", "\n", "            ", "validation_loss", "=", "self", ".", "validate_average_rank", "(", ")", "\n", "", "else", ":", "\n", "            ", "validation_loss", "=", "self", ".", "validate_nll", "(", ")", "\n", "\n", "", "if", "save_cp", ":", "\n", "            ", "cp_name", "=", "self", ".", "_save_checkpoint", "(", "scheduler", ",", "epoch", ",", "iteration", ")", "\n", "logger", ".", "info", "(", "'Saved checkpoint to %s'", ",", "cp_name", ")", "\n", "\n", "if", "validation_loss", "<", "(", "self", ".", "best_validation_result", "or", "validation_loss", "+", "1", ")", ":", "\n", "                ", "self", ".", "best_validation_result", "=", "validation_loss", "\n", "self", ".", "best_cp_name", "=", "cp_name", "\n", "logger", ".", "info", "(", "'New Best validation checkpoint %s'", ",", "cp_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer.validate_nll": [[158, 192], ["logger.info", "train_dense_encoder.BiEncoderTrainer.biencoder.eval", "train_dense_encoder.BiEncoderTrainer.get_data_iterator", "time.time", "enumerate", "float", "logger.info", "train_dense_encoder.BiEncoderTrainer.iterate_data", "dpr.models.biencoder.BiEncoder.create_biencoder_input", "train_dense_encoder._do_biencoder_fwd_pass", "loss.item", "logger.info", "loss.item", "time.time"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer.get_data_iterator", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.ShardedDataIterator.iterate_data", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.biencoder.BiEncoder.create_biencoder_input", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder._do_biencoder_fwd_pass"], ["", "", "", "def", "validate_nll", "(", "self", ")", "->", "float", ":", "\n", "        ", "logger", ".", "info", "(", "'NLL validation ...'", ")", "\n", "args", "=", "self", ".", "args", "\n", "self", ".", "biencoder", ".", "eval", "(", ")", "\n", "data_iterator", "=", "self", ".", "get_data_iterator", "(", "args", ".", "dev_file", ",", "args", ".", "dev_batch_size", ",", "shuffle", "=", "False", ")", "\n", "\n", "total_loss", "=", "0.0", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "total_correct_predictions", "=", "0", "\n", "num_hard_negatives", "=", "args", ".", "hard_negatives", "\n", "num_other_negatives", "=", "args", ".", "other_negatives", "\n", "log_result_step", "=", "args", ".", "log_batch_step", "\n", "batches", "=", "0", "\n", "for", "i", ",", "samples_batch", "in", "enumerate", "(", "data_iterator", ".", "iterate_data", "(", ")", ")", ":", "\n", "            ", "biencoder_input", "=", "BiEncoder", ".", "create_biencoder_input", "(", "samples_batch", ",", "self", ".", "tensorizer", ",", "\n", "True", ",", "\n", "num_hard_negatives", ",", "num_other_negatives", ",", "shuffle", "=", "False", ")", "\n", "\n", "loss", ",", "correct_cnt", "=", "_do_biencoder_fwd_pass", "(", "self", ".", "biencoder", ",", "biencoder_input", ",", "self", ".", "tensorizer", ",", "args", ")", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "total_correct_predictions", "+=", "correct_cnt", "\n", "batches", "+=", "1", "\n", "if", "(", "i", "+", "1", ")", "%", "log_result_step", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "'Eval step: %d , used_time=%f sec., loss=%f '", ",", "i", ",", "time", ".", "time", "(", ")", "-", "start_time", ",", "loss", ".", "item", "(", ")", ")", "\n", "\n", "", "", "total_loss", "=", "total_loss", "/", "batches", "\n", "total_samples", "=", "batches", "*", "args", ".", "dev_batch_size", "*", "self", ".", "distributed_factor", "\n", "correct_ratio", "=", "float", "(", "total_correct_predictions", "/", "total_samples", ")", "\n", "logger", ".", "info", "(", "'NLL Validation: loss = %f. correct prediction ratio  %d/%d ~  %f'", ",", "total_loss", ",", "\n", "total_correct_predictions", ",", "\n", "total_samples", ",", "\n", "correct_ratio", "\n", ")", "\n", "return", "total_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer.validate_average_rank": [[193, 298], ["logger.info", "train_dense_encoder.BiEncoderTrainer.biencoder.eval", "train_dense_encoder.BiEncoderTrainer.get_data_iterator", "dpr.models.biencoder.BiEncoderNllLoss.get_similarity_function", "enumerate", "torch.cat", "torch.cat", "logger.info", "logger.info", "torch.cat.size", "dpr.models.biencoder.BiEncoderNllLoss.get_similarity_function.", "torch.sort", "enumerate", "float", "logger.info", "train_dense_encoder.BiEncoderTrainer.iterate_data", "dpr.models.biencoder.BiEncoder.create_biencoder_input", "len", "ctxs_ids.size", "enumerate", "positive_idx_per_question.extend", "torch.cat.size", "torch.cat.size", "len", "gold_idx.item", "dpr.utils.dist_utils.all_gather_list", "enumerate", "len", "range", "train_dense_encoder.BiEncoderTrainer.tensorizer.get_attn_mask", "train_dense_encoder.BiEncoderTrainer.tensorizer.get_attn_mask", "torch.cat.extend", "logger.info", "torch.no_grad", "train_dense_encoder.BiEncoderTrainer.biencoder", "torch.cat.extend", "ctx_dense.cpu().split", "len", "len", "q_ids.size", "q_dense.cpu().split", "ctx_dense.cpu", "q_dense.cpu"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer.get_data_iterator", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.biencoder.BiEncoderNllLoss.get_similarity_function", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.ShardedDataIterator.iterate_data", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.biencoder.BiEncoder.create_biencoder_input", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.dist_utils.all_gather_list", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.get_attn_mask", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.get_attn_mask"], ["", "def", "validate_average_rank", "(", "self", ")", "->", "float", ":", "\n", "        ", "\"\"\"\n        Validates biencoder model using each question's gold passage's rank across the set of passages from the dataset.\n        It generates vectors for specified amount of negative passages from each question (see --val_av_rank_xxx params)\n        and stores them in RAM as well as question vectors.\n        Then the similarity scores are calculted for the entire\n        num_questions x (num_questions x num_passages_per_question) matrix and sorted per quesrtion.\n        Each question's gold passage rank in that  sorted list of scores is averaged across all the questions.\n        :return: averaged rank number\n        \"\"\"", "\n", "logger", ".", "info", "(", "'Average rank validation ...'", ")", "\n", "\n", "args", "=", "self", ".", "args", "\n", "self", ".", "biencoder", ".", "eval", "(", ")", "\n", "distributed_factor", "=", "self", ".", "distributed_factor", "\n", "\n", "data_iterator", "=", "self", ".", "get_data_iterator", "(", "args", ".", "dev_file", ",", "args", ".", "dev_batch_size", ",", "shuffle", "=", "False", ")", "\n", "\n", "sub_batch_size", "=", "args", ".", "val_av_rank_bsz", "\n", "sim_score_f", "=", "BiEncoderNllLoss", ".", "get_similarity_function", "(", ")", "\n", "q_represenations", "=", "[", "]", "\n", "ctx_represenations", "=", "[", "]", "\n", "positive_idx_per_question", "=", "[", "]", "\n", "\n", "num_hard_negatives", "=", "args", ".", "val_av_rank_hard_neg", "\n", "num_other_negatives", "=", "args", ".", "val_av_rank_other_neg", "\n", "\n", "log_result_step", "=", "args", ".", "log_batch_step", "\n", "\n", "for", "i", ",", "samples_batch", "in", "enumerate", "(", "data_iterator", ".", "iterate_data", "(", ")", ")", ":", "\n", "# samples += 1", "\n", "            ", "if", "len", "(", "q_represenations", ")", ">", "args", ".", "val_av_rank_max_qs", "/", "distributed_factor", ":", "\n", "                ", "break", "\n", "\n", "", "biencoder_input", "=", "BiEncoder", ".", "create_biencoder_input", "(", "samples_batch", ",", "self", ".", "tensorizer", ",", "\n", "True", ",", "\n", "num_hard_negatives", ",", "num_other_negatives", ",", "shuffle", "=", "False", ")", "\n", "total_ctxs", "=", "len", "(", "ctx_represenations", ")", "\n", "ctxs_ids", "=", "biencoder_input", ".", "context_ids", "\n", "ctxs_segments", "=", "biencoder_input", ".", "ctx_segments", "\n", "bsz", "=", "ctxs_ids", ".", "size", "(", "0", ")", "\n", "\n", "# split contexts batch into sub batches since it is supposed to be too large to be processed in one batch", "\n", "for", "j", ",", "batch_start", "in", "enumerate", "(", "range", "(", "0", ",", "bsz", ",", "sub_batch_size", ")", ")", ":", "\n", "\n", "                ", "q_ids", ",", "q_segments", "=", "(", "biencoder_input", ".", "question_ids", ",", "biencoder_input", ".", "question_segments", ")", "if", "j", "==", "0", "else", "(", "None", ",", "None", ")", "\n", "\n", "if", "j", "==", "0", "and", "args", ".", "n_gpu", ">", "1", "and", "q_ids", ".", "size", "(", "0", ")", "==", "1", ":", "\n", "# if we are in DP (but not in DDP) mode, all model input tensors should have batch size >1 or 0,", "\n", "# otherwise the other input tensors will be split but only the first split will be called", "\n", "                    ", "continue", "\n", "\n", "", "ctx_ids_batch", "=", "ctxs_ids", "[", "batch_start", ":", "batch_start", "+", "sub_batch_size", "]", "\n", "ctx_seg_batch", "=", "ctxs_segments", "[", "batch_start", ":", "batch_start", "+", "sub_batch_size", "]", "\n", "\n", "q_attn_mask", "=", "self", ".", "tensorizer", ".", "get_attn_mask", "(", "q_ids", ")", "\n", "ctx_attn_mask", "=", "self", ".", "tensorizer", ".", "get_attn_mask", "(", "ctx_ids_batch", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "q_dense", ",", "ctx_dense", "=", "self", ".", "biencoder", "(", "q_ids", ",", "q_segments", ",", "q_attn_mask", ",", "ctx_ids_batch", ",", "ctx_seg_batch", ",", "\n", "ctx_attn_mask", ")", "\n", "\n", "", "if", "q_dense", "is", "not", "None", ":", "\n", "                    ", "q_represenations", ".", "extend", "(", "q_dense", ".", "cpu", "(", ")", ".", "split", "(", "1", ",", "dim", "=", "0", ")", ")", "\n", "\n", "", "ctx_represenations", ".", "extend", "(", "ctx_dense", ".", "cpu", "(", ")", ".", "split", "(", "1", ",", "dim", "=", "0", ")", ")", "\n", "\n", "", "batch_positive_idxs", "=", "biencoder_input", ".", "is_positive", "\n", "positive_idx_per_question", ".", "extend", "(", "[", "total_ctxs", "+", "v", "for", "v", "in", "batch_positive_idxs", "]", ")", "\n", "\n", "if", "(", "i", "+", "1", ")", "%", "log_result_step", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "'Av.rank validation: step %d, computed ctx_vectors %d, q_vectors %d'", ",", "i", ",", "\n", "len", "(", "ctx_represenations", ")", ",", "len", "(", "q_represenations", ")", ")", "\n", "\n", "", "", "ctx_represenations", "=", "torch", ".", "cat", "(", "ctx_represenations", ",", "dim", "=", "0", ")", "\n", "q_represenations", "=", "torch", ".", "cat", "(", "q_represenations", ",", "dim", "=", "0", ")", "\n", "\n", "logger", ".", "info", "(", "'Av.rank validation: total q_vectors size=%s'", ",", "q_represenations", ".", "size", "(", ")", ")", "\n", "logger", ".", "info", "(", "'Av.rank validation: total ctx_vectors size=%s'", ",", "ctx_represenations", ".", "size", "(", ")", ")", "\n", "\n", "q_num", "=", "q_represenations", ".", "size", "(", "0", ")", "\n", "assert", "q_num", "==", "len", "(", "positive_idx_per_question", ")", "\n", "\n", "scores", "=", "sim_score_f", "(", "q_represenations", ",", "ctx_represenations", ")", "\n", "values", ",", "indices", "=", "torch", ".", "sort", "(", "scores", ",", "dim", "=", "1", ",", "descending", "=", "True", ")", "\n", "\n", "rank", "=", "0", "\n", "for", "i", ",", "idx", "in", "enumerate", "(", "positive_idx_per_question", ")", ":", "\n", "# aggregate the rank of the known gold passage in the sorted results for each question", "\n", "            ", "gold_idx", "=", "(", "indices", "[", "i", "]", "==", "idx", ")", ".", "nonzero", "(", ")", "\n", "rank", "+=", "gold_idx", ".", "item", "(", ")", "\n", "\n", "", "if", "distributed_factor", ">", "1", ":", "\n", "# each node calcuated its own rank, exchange the information between node and calculate the \"global\" average rank", "\n", "# NOTE: the set of passages is still unique for every node", "\n", "            ", "eval_stats", "=", "all_gather_list", "(", "[", "rank", ",", "q_num", "]", ",", "max_size", "=", "100", ")", "\n", "for", "i", ",", "item", "in", "enumerate", "(", "eval_stats", ")", ":", "\n", "                ", "remote_rank", ",", "remote_q_num", "=", "item", "\n", "if", "i", "!=", "args", ".", "local_rank", ":", "\n", "                    ", "rank", "+=", "remote_rank", "\n", "q_num", "+=", "remote_q_num", "\n", "\n", "", "", "", "av_rank", "=", "float", "(", "rank", "/", "q_num", ")", "\n", "logger", ".", "info", "(", "'Av.rank validation: average rank %s, total questions=%d'", ",", "av_rank", ",", "q_num", ")", "\n", "return", "av_rank", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer._train_epoch": [[299, 369], ["train_dense_encoder.BiEncoderTrainer.biencoder.train", "enumerate", "train_dense_encoder.BiEncoderTrainer.validate_and_save", "logger.info", "logger.info", "train_data_iterator.iterate_data", "train_data_iterator.get_iteration", "random.seed", "dpr.models.biencoder.BiEncoder.create_biencoder_input", "train_dense_encoder._do_biencoder_fwd_pass", "loss.item", "loss.item", "loss.backward", "train_dense_encoder.BiEncoderTrainer.optimizer.step", "scheduler.step", "train_dense_encoder.BiEncoderTrainer.biencoder.zero_grad", "logger.info", "logger.info", "logger.info", "logger.info", "train_dense_encoder.BiEncoderTrainer.validate_and_save", "train_dense_encoder.BiEncoderTrainer.biencoder.train", "amp.scale_loss", "scaled_loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "loss.item", "train_data_iterator.get_iteration", "amp.master_params", "train_dense_encoder.BiEncoderTrainer.biencoder.parameters"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer.validate_and_save", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.ShardedDataIterator.iterate_data", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.ShardedDataIterator.get_iteration", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.biencoder.BiEncoder.create_biencoder_input", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder._do_biencoder_fwd_pass", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer.validate_and_save", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.ShardedDataIterator.get_iteration"], ["", "def", "_train_epoch", "(", "self", ",", "scheduler", ",", "epoch", ":", "int", ",", "eval_step", ":", "int", ",", "\n", "train_data_iterator", ":", "ShardedDataIterator", ",", ")", ":", "\n", "\n", "        ", "args", "=", "self", ".", "args", "\n", "rolling_train_loss", "=", "0.0", "\n", "epoch_loss", "=", "0", "\n", "epoch_correct_predictions", "=", "0", "\n", "\n", "log_result_step", "=", "args", ".", "log_batch_step", "\n", "rolling_loss_step", "=", "args", ".", "train_rolling_loss_step", "\n", "num_hard_negatives", "=", "args", ".", "hard_negatives", "\n", "num_other_negatives", "=", "args", ".", "other_negatives", "\n", "seed", "=", "args", ".", "seed", "\n", "self", ".", "biencoder", ".", "train", "(", ")", "\n", "epoch_batches", "=", "train_data_iterator", ".", "max_iterations", "\n", "data_iteration", "=", "0", "\n", "for", "i", ",", "samples_batch", "in", "enumerate", "(", "train_data_iterator", ".", "iterate_data", "(", "epoch", "=", "epoch", ")", ")", ":", "\n", "\n", "# to be able to resume shuffled ctx- pools", "\n", "            ", "data_iteration", "=", "train_data_iterator", ".", "get_iteration", "(", ")", "\n", "random", ".", "seed", "(", "seed", "+", "epoch", "+", "data_iteration", ")", "\n", "biencoder_batch", "=", "BiEncoder", ".", "create_biencoder_input", "(", "samples_batch", ",", "self", ".", "tensorizer", ",", "\n", "True", ",", "\n", "num_hard_negatives", ",", "num_other_negatives", ",", "shuffle", "=", "True", ",", "\n", "shuffle_positives", "=", "args", ".", "shuffle_positive_ctx", "\n", ")", "\n", "\n", "loss", ",", "correct_cnt", "=", "_do_biencoder_fwd_pass", "(", "self", ".", "biencoder", ",", "biencoder_batch", ",", "self", ".", "tensorizer", ",", "args", ")", "\n", "\n", "epoch_correct_predictions", "+=", "correct_cnt", "\n", "epoch_loss", "+=", "loss", ".", "item", "(", ")", "\n", "rolling_train_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "if", "args", ".", "fp16", ":", "\n", "                ", "from", "apex", "import", "amp", "\n", "with", "amp", ".", "scale_loss", "(", "loss", ",", "self", ".", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                    ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "if", "args", ".", "max_grad_norm", ">", "0", ":", "\n", "                    ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "amp", ".", "master_params", "(", "self", ".", "optimizer", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "", "", "else", ":", "\n", "                ", "loss", ".", "backward", "(", ")", "\n", "if", "args", ".", "max_grad_norm", ">", "0", ":", "\n", "                    ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "biencoder", ".", "parameters", "(", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "\n", "", "", "if", "(", "i", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", ")", "\n", "self", ".", "biencoder", ".", "zero_grad", "(", ")", "\n", "\n", "", "if", "i", "%", "log_result_step", "==", "0", ":", "\n", "                ", "lr", "=", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "logger", ".", "info", "(", "\n", "'Epoch: %d: Step: %d/%d, loss=%f, lr=%f'", ",", "epoch", ",", "data_iteration", ",", "epoch_batches", ",", "loss", ".", "item", "(", ")", ",", "lr", ")", "\n", "\n", "", "if", "(", "i", "+", "1", ")", "%", "rolling_loss_step", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "'Train batch %d'", ",", "data_iteration", ")", "\n", "latest_rolling_train_av_loss", "=", "rolling_train_loss", "/", "rolling_loss_step", "\n", "logger", ".", "info", "(", "'Avg. loss per last %d batches: %f'", ",", "rolling_loss_step", ",", "latest_rolling_train_av_loss", ")", "\n", "rolling_train_loss", "=", "0.0", "\n", "\n", "", "if", "data_iteration", "%", "eval_step", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "'Validation: Epoch: %d Step: %d/%d'", ",", "epoch", ",", "data_iteration", ",", "epoch_batches", ")", "\n", "self", ".", "validate_and_save", "(", "epoch", ",", "train_data_iterator", ".", "get_iteration", "(", ")", ",", "scheduler", ")", "\n", "self", ".", "biencoder", ".", "train", "(", ")", "\n", "\n", "", "", "self", ".", "validate_and_save", "(", "epoch", ",", "data_iteration", ",", "scheduler", ")", "\n", "\n", "epoch_loss", "=", "(", "epoch_loss", "/", "epoch_batches", ")", "if", "epoch_batches", ">", "0", "else", "0", "\n", "logger", ".", "info", "(", "'Av Loss per epoch=%f'", ",", "epoch_loss", ")", "\n", "logger", ".", "info", "(", "'epoch total correct predictions=%d'", ",", "epoch_correct_predictions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer._save_checkpoint": [[370, 387], ["dpr.utils.model_utils.get_model_obj", "os.path.join", "dpr.options.get_encoder_params_state", "dpr.utils.model_utils.CheckpointState", "torch.save", "logger.info", "dpr.utils.model_utils.get_model_obj.state_dict", "train_dense_encoder.BiEncoderTrainer.optimizer.state_dict", "scheduler.state_dict", "dpr.utils.model_utils.CheckpointState._asdict", "str", "str"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.get_model_obj", "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.get_encoder_params_state"], ["", "def", "_save_checkpoint", "(", "self", ",", "scheduler", ",", "epoch", ":", "int", ",", "offset", ":", "int", ")", "->", "str", ":", "\n", "        ", "args", "=", "self", ".", "args", "\n", "model_to_save", "=", "get_model_obj", "(", "self", ".", "biencoder", ")", "\n", "cp", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\n", "args", ".", "checkpoint_file_name", "+", "'.'", "+", "str", "(", "epoch", ")", "+", "(", "'.'", "+", "str", "(", "offset", ")", "if", "offset", ">", "0", "else", "''", ")", ")", "\n", "\n", "meta_params", "=", "get_encoder_params_state", "(", "args", ")", "\n", "\n", "state", "=", "CheckpointState", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "\n", "self", ".", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "scheduler", ".", "state_dict", "(", ")", ",", "\n", "offset", ",", "\n", "epoch", ",", "meta_params", "\n", ")", "\n", "torch", ".", "save", "(", "state", ".", "_asdict", "(", ")", ",", "cp", ")", "\n", "logger", ".", "info", "(", "'Saved checkpoint at %s'", ",", "cp", ")", "\n", "return", "cp", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer._load_saved_state": [[388, 409], ["logger.info", "dpr.utils.model_utils.get_model_obj", "logger.info", "dpr.utils.model_utils.get_model_obj.load_state_dict", "logger.info", "train_dense_encoder.BiEncoderTrainer.optimizer.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.get_model_obj"], ["", "def", "_load_saved_state", "(", "self", ",", "saved_state", ":", "CheckpointState", ")", ":", "\n", "        ", "epoch", "=", "saved_state", ".", "epoch", "\n", "offset", "=", "saved_state", ".", "offset", "\n", "if", "offset", "==", "0", ":", "# epoch has been completed", "\n", "            ", "epoch", "+=", "1", "\n", "", "logger", ".", "info", "(", "'Loading checkpoint @ batch=%s and epoch=%s'", ",", "offset", ",", "epoch", ")", "\n", "\n", "if", "self", ".", "restart", "is", "False", ":", "\n", "            ", "self", ".", "start_epoch", "=", "epoch", "\n", "self", ".", "start_batch", "=", "offset", "\n", "\n", "", "model_to_load", "=", "get_model_obj", "(", "self", ".", "biencoder", ")", "\n", "logger", ".", "info", "(", "'Loading saved model state ...'", ")", "\n", "model_to_load", ".", "load_state_dict", "(", "saved_state", ".", "model_dict", ")", "# set strict=False if you use extra projection", "\n", "\n", "if", "self", ".", "restart", "is", "False", "and", "saved_state", ".", "optimizer_dict", ":", "\n", "            ", "logger", ".", "info", "(", "'Loading saved optimizer state ...'", ")", "\n", "self", ".", "optimizer", ".", "load_state_dict", "(", "saved_state", ".", "optimizer_dict", ")", "\n", "\n", "", "if", "self", ".", "restart", "is", "False", "and", "saved_state", ".", "scheduler_dict", ":", "\n", "            ", "self", ".", "scheduler_state", "=", "saved_state", ".", "scheduler_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder._calc_loss": [[411, 464], ["loss_function.calc", "torch.empty_like().cpu().copy_().detach_", "torch.empty_like().cpu().copy_().detach_", "dpr.utils.dist_utils.all_gather_list", "enumerate", "torch.cat", "torch.cat", "ctx_vectors.size", "torch.empty_like().cpu().copy_", "torch.empty_like().cpu().copy_", "torch.cat.append", "torch.cat.append", "positive_idx_per_question.extend", "hard_negatives_per_question.extend", "torch.cat.append", "torch.cat.append", "positive_idx_per_question.extend", "hard_negatives_per_question.extend", "q_vector.to", "ctx_vectors.to", "torch.empty_like().cpu", "torch.empty_like().cpu", "torch.empty_like", "torch.empty_like"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.biencoder.BiEncoderNllLoss.calc", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.dist_utils.all_gather_list"], ["", "", "", "def", "_calc_loss", "(", "args", ",", "loss_function", ",", "local_q_vector", ",", "local_ctx_vectors", ",", "local_positive_idxs", ",", "\n", "local_hard_negatives_idxs", ":", "list", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "T", ",", "bool", "]", ":", "\n", "    ", "\"\"\"\n    Calculates In-batch negatives schema loss and supports to run it in DDP mode by exchanging the representations\n    across all the nodes.\n    \"\"\"", "\n", "distributed_world_size", "=", "args", ".", "distributed_world_size", "or", "1", "\n", "if", "distributed_world_size", ">", "1", ":", "\n", "        ", "q_vector_to_send", "=", "torch", ".", "empty_like", "(", "local_q_vector", ")", ".", "cpu", "(", ")", ".", "copy_", "(", "local_q_vector", ")", ".", "detach_", "(", ")", "\n", "ctx_vector_to_send", "=", "torch", ".", "empty_like", "(", "local_ctx_vectors", ")", ".", "cpu", "(", ")", ".", "copy_", "(", "local_ctx_vectors", ")", ".", "detach_", "(", ")", "\n", "\n", "global_question_ctx_vectors", "=", "all_gather_list", "(", "\n", "[", "q_vector_to_send", ",", "ctx_vector_to_send", ",", "local_positive_idxs", ",", "local_hard_negatives_idxs", "]", ",", "\n", "max_size", "=", "args", ".", "global_loss_buf_sz", ")", "\n", "\n", "global_q_vector", "=", "[", "]", "\n", "global_ctxs_vector", "=", "[", "]", "\n", "\n", "# ctxs_per_question = local_ctx_vectors.size(0)", "\n", "positive_idx_per_question", "=", "[", "]", "\n", "hard_negatives_per_question", "=", "[", "]", "\n", "\n", "total_ctxs", "=", "0", "\n", "\n", "for", "i", ",", "item", "in", "enumerate", "(", "global_question_ctx_vectors", ")", ":", "\n", "            ", "q_vector", ",", "ctx_vectors", ",", "positive_idx", ",", "hard_negatives_idxs", "=", "item", "\n", "\n", "if", "i", "!=", "args", ".", "local_rank", ":", "\n", "                ", "global_q_vector", ".", "append", "(", "q_vector", ".", "to", "(", "local_q_vector", ".", "device", ")", ")", "\n", "global_ctxs_vector", ".", "append", "(", "ctx_vectors", ".", "to", "(", "local_q_vector", ".", "device", ")", ")", "\n", "positive_idx_per_question", ".", "extend", "(", "[", "v", "+", "total_ctxs", "for", "v", "in", "positive_idx", "]", ")", "\n", "hard_negatives_per_question", ".", "extend", "(", "[", "[", "v", "+", "total_ctxs", "for", "v", "in", "l", "]", "for", "l", "in", "hard_negatives_idxs", "]", ")", "\n", "", "else", ":", "\n", "                ", "global_q_vector", ".", "append", "(", "local_q_vector", ")", "\n", "global_ctxs_vector", ".", "append", "(", "local_ctx_vectors", ")", "\n", "positive_idx_per_question", ".", "extend", "(", "[", "v", "+", "total_ctxs", "for", "v", "in", "local_positive_idxs", "]", ")", "\n", "hard_negatives_per_question", ".", "extend", "(", "[", "[", "v", "+", "total_ctxs", "for", "v", "in", "l", "]", "for", "l", "in", "local_hard_negatives_idxs", "]", ")", "\n", "", "total_ctxs", "+=", "ctx_vectors", ".", "size", "(", "0", ")", "\n", "\n", "", "global_q_vector", "=", "torch", ".", "cat", "(", "global_q_vector", ",", "dim", "=", "0", ")", "\n", "global_ctxs_vector", "=", "torch", ".", "cat", "(", "global_ctxs_vector", ",", "dim", "=", "0", ")", "\n", "\n", "", "else", ":", "\n", "        ", "global_q_vector", "=", "local_q_vector", "\n", "global_ctxs_vector", "=", "local_ctx_vectors", "\n", "positive_idx_per_question", "=", "local_positive_idxs", "\n", "hard_negatives_per_question", "=", "local_hard_negatives_idxs", "\n", "\n", "", "loss", ",", "is_correct", "=", "loss_function", ".", "calc", "(", "global_q_vector", ",", "global_ctxs_vector", ",", "positive_idx_per_question", ",", "\n", "hard_negatives_per_question", ")", "\n", "\n", "return", "loss", ",", "is_correct", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder._do_biencoder_fwd_pass": [[466, 496], ["dpr.models.biencoder.BiEncoderBatch", "tensorizer.get_attn_mask", "tensorizer.get_attn_mask", "dpr.models.biencoder.BiEncoderNllLoss", "train_dense_encoder._calc_loss", "is_correct.sum().item.sum().item", "model", "loss.mean.mean", "dpr.utils.model_utils.move_to_device", "torch.no_grad", "model", "is_correct.sum().item.sum", "dpr.models.biencoder.BiEncoderBatch._asdict"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.get_attn_mask", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.get_attn_mask", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder._calc_loss", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.move_to_device"], ["", "def", "_do_biencoder_fwd_pass", "(", "model", ":", "nn", ".", "Module", ",", "input", ":", "BiEncoderBatch", ",", "tensorizer", ":", "Tensorizer", ",", "args", ")", "->", "(", "\n", "torch", ".", "Tensor", ",", "int", ")", ":", "\n", "    ", "input", "=", "BiEncoderBatch", "(", "**", "move_to_device", "(", "input", ".", "_asdict", "(", ")", ",", "args", ".", "device", ")", ")", "\n", "\n", "q_attn_mask", "=", "tensorizer", ".", "get_attn_mask", "(", "input", ".", "question_ids", ")", "\n", "ctx_attn_mask", "=", "tensorizer", ".", "get_attn_mask", "(", "input", ".", "context_ids", ")", "\n", "\n", "if", "model", ".", "training", ":", "\n", "        ", "model_out", "=", "model", "(", "input", ".", "question_ids", ",", "input", ".", "question_segments", ",", "q_attn_mask", ",", "input", ".", "context_ids", ",", "\n", "input", ".", "ctx_segments", ",", "ctx_attn_mask", ")", "\n", "", "else", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "model_out", "=", "model", "(", "input", ".", "question_ids", ",", "input", ".", "question_segments", ",", "q_attn_mask", ",", "input", ".", "context_ids", ",", "\n", "input", ".", "ctx_segments", ",", "ctx_attn_mask", ")", "\n", "\n", "", "", "local_q_vector", ",", "local_ctx_vectors", "=", "model_out", "\n", "\n", "loss_function", "=", "BiEncoderNllLoss", "(", ")", "\n", "\n", "loss", ",", "is_correct", "=", "_calc_loss", "(", "args", ",", "loss_function", ",", "local_q_vector", ",", "local_ctx_vectors", ",", "input", ".", "is_positive", ",", "\n", "input", ".", "hard_negatives", ")", "\n", "\n", "is_correct", "=", "is_correct", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "if", "args", ".", "n_gpu", ">", "1", ":", "\n", "        ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "        ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "return", "loss", ",", "is_correct", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.main": [[498, 565], ["argparse.ArgumentParser", "dpr.options.add_encoder_params", "dpr.options.add_training_params", "dpr.options.add_tokenizer_params", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "dpr.options.setup_args_gpu", "dpr.options.set_seed", "dpr.options.print_args", "train_dense_encoder.BiEncoderTrainer", "ValueError", "os.makedirs", "train_dense_encoder.BiEncoderTrainer.run_train", "logger.info", "train_dense_encoder.BiEncoderTrainer.validate_nll", "train_dense_encoder.BiEncoderTrainer.validate_average_rank", "logger.warning"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.add_encoder_params", "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.add_training_params", "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.add_tokenizer_params", "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.setup_args_gpu", "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.set_seed", "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.print_args", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer.run_train", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer.validate_nll", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.train_dense_encoder.BiEncoderTrainer.validate_average_rank"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "add_encoder_params", "(", "parser", ")", "\n", "add_training_params", "(", "parser", ")", "\n", "add_tokenizer_params", "(", "parser", ")", "\n", "\n", "# biencoder specific training features", "\n", "parser", ".", "add_argument", "(", "\"--eval_per_epoch\"", ",", "default", "=", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "\"How many times it evaluates on dev set per epoch and saves a checkpoint\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--global_loss_buf_sz\"", ",", "type", "=", "int", ",", "default", "=", "150000", ",", "\n", "help", "=", "'Buffer size for distributed mode representations al gather operation. \\\n                                Increase this if you see errors like \"encoded data exceeds max_size ...\"'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--fix_ctx_encoder\"", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "\"--shuffle_positive_ctx\"", ",", "action", "=", "'store_true'", ")", "\n", "\n", "# input/output src params", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The output directory where the model checkpoints will be written or resumed from\"", ")", "\n", "\n", "# data handling parameters", "\n", "parser", ".", "add_argument", "(", "\"--hard_negatives\"", ",", "default", "=", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "\"amount of hard negative ctx per question\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--other_negatives\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"amount of 'other' negative ctx per question\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_files_upsample_rates\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"list of up-sample rates per each train file. Example: [1,2,1]\"", ")", "\n", "\n", "# parameters for Av.rank validation method", "\n", "parser", ".", "add_argument", "(", "\"--val_av_rank_start_epoch\"", ",", "type", "=", "int", ",", "default", "=", "10000", ",", "\n", "help", "=", "\"Av.rank validation: the epoch from which to enable this validation\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--val_av_rank_hard_neg\"", ",", "type", "=", "int", ",", "default", "=", "30", ",", "\n", "help", "=", "\"Av.rank validation: how many hard negatives to take from each question pool\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--val_av_rank_other_neg\"", ",", "type", "=", "int", ",", "default", "=", "30", ",", "\n", "help", "=", "\"Av.rank validation: how many 'other' negatives to take from each question pool\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--val_av_rank_bsz\"", ",", "type", "=", "int", ",", "default", "=", "128", ",", "\n", "help", "=", "\"Av.rank validation: batch size to process passages\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--val_av_rank_max_qs\"", ",", "type", "=", "int", ",", "default", "=", "10000", ",", "\n", "help", "=", "\"Av.rank validation: max num of questions\"", ")", "\n", "parser", ".", "add_argument", "(", "'--checkpoint_file_name'", ",", "type", "=", "str", ",", "default", "=", "'dpr_biencoder'", ",", "help", "=", "\"Checkpoints file prefix\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--restart\"", ",", "action", "=", "'store_true'", ",", "help", "=", "\"set true if you want to reset the optimizer and schedular states when you start fine-tuning from pre-trained models\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "if", "args", ".", "output_dir", "is", "not", "None", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "setup_args_gpu", "(", "args", ")", "\n", "set_seed", "(", "args", ")", "\n", "print_args", "(", "args", ")", "\n", "\n", "trainer", "=", "BiEncoderTrainer", "(", "args", ")", "\n", "\n", "if", "args", ".", "train_file", "is", "not", "None", ":", "\n", "        ", "trainer", ".", "run_train", "(", ")", "\n", "", "elif", "args", ".", "model_file", "and", "args", ".", "dev_file", ":", "\n", "        ", "logger", ".", "info", "(", "\"No train files are specified. Run 2 types of validation for specified model file\"", ")", "\n", "trainer", ".", "validate_nll", "(", ")", "\n", "trainer", ".", "validate_average_rank", "(", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "warning", "(", "\"Neither train_file or (model_file & dev_file) parameters are specified. Nothing to do.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.DenseRetriever.__init__": [[49, 54], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "question_encoder", ":", "nn", ".", "Module", ",", "batch_size", ":", "int", ",", "tensorizer", ":", "Tensorizer", ",", "index", ":", "DenseIndexer", ")", ":", "\n", "        ", "self", ".", "question_encoder", "=", "question_encoder", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "tensorizer", "=", "tensorizer", "\n", "self", ".", "index", "=", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.DenseRetriever.generate_question_vectors": [[55, 84], ["len", "dense_retriever.DenseRetriever.question_encoder.eval", "torch.cat", "logger.info", "torch.no_grad", "enumerate", "torch.cat.size", "torch.cat.size", "len", "range", "torch.stack().cuda", "torch.zeros_like().cuda", "dense_retriever.DenseRetriever.tensorizer.get_attn_mask", "dense_retriever.DenseRetriever.question_encoder", "query_vectors.extend", "dense_retriever.DenseRetriever.tensorizer.text_to_tensor", "out.cpu().split", "logger.info", "torch.stack", "torch.zeros_like", "len", "len", "out.cpu"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.get_attn_mask", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.text_to_tensor"], ["", "def", "generate_question_vectors", "(", "self", ",", "questions", ":", "List", "[", "str", "]", ")", "->", "T", ":", "\n", "        ", "n", "=", "len", "(", "questions", ")", "\n", "bsz", "=", "self", ".", "batch_size", "\n", "query_vectors", "=", "[", "]", "\n", "\n", "self", ".", "question_encoder", ".", "eval", "(", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "j", ",", "batch_start", "in", "enumerate", "(", "range", "(", "0", ",", "n", ",", "bsz", ")", ")", ":", "\n", "\n", "                ", "batch_token_tensors", "=", "[", "self", ".", "tensorizer", ".", "text_to_tensor", "(", "q", ")", "for", "q", "in", "\n", "questions", "[", "batch_start", ":", "batch_start", "+", "bsz", "]", "]", "\n", "\n", "q_ids_batch", "=", "torch", ".", "stack", "(", "batch_token_tensors", ",", "dim", "=", "0", ")", ".", "cuda", "(", ")", "\n", "q_seg_batch", "=", "torch", ".", "zeros_like", "(", "q_ids_batch", ")", ".", "cuda", "(", ")", "\n", "q_attn_mask", "=", "self", ".", "tensorizer", ".", "get_attn_mask", "(", "q_ids_batch", ")", "\n", "_", ",", "out", ",", "_", "=", "self", ".", "question_encoder", "(", "q_ids_batch", ",", "q_seg_batch", ",", "q_attn_mask", ")", "\n", "\n", "query_vectors", ".", "extend", "(", "out", ".", "cpu", "(", ")", ".", "split", "(", "1", ",", "dim", "=", "0", ")", ")", "\n", "\n", "if", "len", "(", "query_vectors", ")", "%", "100", "==", "0", ":", "\n", "                    ", "logger", ".", "info", "(", "'Encoded queries %d'", ",", "len", "(", "query_vectors", ")", ")", "\n", "\n", "", "", "", "query_tensor", "=", "torch", ".", "cat", "(", "query_vectors", ",", "dim", "=", "0", ")", "\n", "\n", "logger", ".", "info", "(", "'Total encoded queries tensor %s'", ",", "query_tensor", ".", "size", "(", ")", ")", "\n", "\n", "assert", "query_tensor", ".", "size", "(", "0", ")", "==", "len", "(", "questions", ")", "\n", "return", "query_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.DenseRetriever.index_encoded_data": [[85, 101], ["enumerate", "dense_retriever.DenseRetriever.index.index_data", "logger.info", "dense_retriever.iterate_encoded_files", "buffer.append", "len", "dense_retriever.DenseRetriever.index.index_data"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseHNSWFlatIndexer.index_data", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.iterate_encoded_files", "home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseHNSWFlatIndexer.index_data"], ["", "def", "index_encoded_data", "(", "self", ",", "vector_files", ":", "List", "[", "str", "]", ",", "buffer_size", ":", "int", "=", "50000", ")", ":", "\n", "        ", "\"\"\"\n        Indexes encoded passages takes form a list of files\n        :param vector_files: file names to get passages vectors from\n        :param buffer_size: size of a buffer (amount of passages) to send for the indexing at once\n        :return:\n        \"\"\"", "\n", "buffer", "=", "[", "]", "\n", "for", "i", ",", "item", "in", "enumerate", "(", "iterate_encoded_files", "(", "vector_files", ")", ")", ":", "\n", "            ", "db_id", ",", "doc_vector", "=", "item", "\n", "buffer", ".", "append", "(", "(", "db_id", ",", "doc_vector", ")", ")", "\n", "if", "0", "<", "buffer_size", "==", "len", "(", "buffer", ")", ":", "\n", "                ", "self", ".", "index", ".", "index_data", "(", "buffer", ")", "\n", "buffer", "=", "[", "]", "\n", "", "", "self", ".", "index", ".", "index_data", "(", "buffer", ")", "\n", "logger", ".", "info", "(", "'Data indexing completed.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.DenseRetriever.get_top_docs": [[102, 113], ["time.time", "dense_retriever.DenseRetriever.index.search_knn", "logger.info", "time.time"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseHNSWFlatIndexer.search_knn"], ["", "def", "get_top_docs", "(", "self", ",", "query_vectors", ":", "np", ".", "array", ",", "top_docs", ":", "int", "=", "100", ")", "->", "List", "[", "Tuple", "[", "List", "[", "object", "]", ",", "List", "[", "float", "]", "]", "]", ":", "\n", "        ", "\"\"\"\n        Does the retrieval of the best matching passages given the query vectors batch\n        :param query_vectors:\n        :param top_docs:\n        :return:\n        \"\"\"", "\n", "time0", "=", "time", ".", "time", "(", ")", "\n", "results", "=", "self", ".", "index", ".", "search_knn", "(", "query_vectors", ",", "top_docs", ")", "\n", "logger", ".", "info", "(", "'index search time: %f sec.'", ",", "time", ".", "time", "(", ")", "-", "time0", ")", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.parse_qa_csv_file": [[115, 122], ["open", "csv.reader", "eval"], "function", ["None"], ["", "", "def", "parse_qa_csv_file", "(", "location", ")", "->", "Iterator", "[", "Tuple", "[", "str", ",", "List", "[", "str", "]", "]", "]", ":", "\n", "    ", "with", "open", "(", "location", ")", "as", "ifile", ":", "\n", "        ", "reader", "=", "csv", ".", "reader", "(", "ifile", ",", "delimiter", "=", "'\\t'", ")", "\n", "for", "row", "in", "reader", ":", "\n", "            ", "question", "=", "row", "[", "0", "]", "\n", "answers", "=", "eval", "(", "row", "[", "1", "]", ")", "\n", "yield", "question", ",", "answers", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.read_jsonlines": [[123, 130], ["print", "jsonlines.open", "lines.append"], "function", ["None"], ["", "", "", "def", "read_jsonlines", "(", "eval_file_name", ")", ":", "\n", "    ", "lines", "=", "[", "]", "\n", "print", "(", "\"loading examples from {0}\"", ".", "format", "(", "eval_file_name", ")", ")", "\n", "with", "jsonlines", ".", "open", "(", "eval_file_name", ")", "as", "reader", ":", "\n", "        ", "for", "obj", "in", "reader", ":", "\n", "            ", "lines", ".", "append", "(", "obj", ")", "\n", "", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.parse_qa_jsonlines_file": [[131, 139], ["dense_retriever.read_jsonlines"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.read_jsonlines"], ["", "def", "parse_qa_jsonlines_file", "(", "location", ")", "->", "Iterator", "[", "Tuple", "[", "str", ",", "str", ",", "List", "[", "str", "]", ",", "str", "]", "]", ":", "\n", "    ", "data", "=", "read_jsonlines", "(", "location", ")", "\n", "for", "row", "in", "data", ":", "\n", "        ", "question", "=", "row", "[", "\"question\"", "]", "\n", "answers", "=", "row", "[", "\"answers\"", "]", "\n", "q_id", "=", "row", "[", "\"id\"", "]", "\n", "lang", "=", "row", "[", "\"lang\"", "]", "\n", "yield", "question", ",", "q_id", ",", "answers", ",", "lang", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.validate": [[140, 150], ["dpr.data.qa_validation.calculate_matches", "logger.info", "logger.info", "len"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.data.qa_validation.calculate_matches"], ["", "", "def", "validate", "(", "passages", ":", "Dict", "[", "object", ",", "Tuple", "[", "str", ",", "str", "]", "]", ",", "answers", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "result_ctx_ids", ":", "List", "[", "Tuple", "[", "List", "[", "object", "]", ",", "List", "[", "float", "]", "]", "]", ",", "\n", "workers_num", ":", "int", ",", "match_type", ":", "str", ")", "->", "List", "[", "List", "[", "bool", "]", "]", ":", "\n", "    ", "match_stats", "=", "calculate_matches", "(", "passages", ",", "answers", ",", "result_ctx_ids", ",", "workers_num", ",", "match_type", ")", "\n", "top_k_hits", "=", "match_stats", ".", "top_k_hits", "\n", "\n", "logger", ".", "info", "(", "'Validation results: top k documents hits %s'", ",", "top_k_hits", ")", "\n", "top_k_hits", "=", "[", "v", "/", "len", "(", "result_ctx_ids", ")", "for", "v", "in", "top_k_hits", "]", "\n", "logger", ".", "info", "(", "'Validation results: top k documents hits accuracy %s'", ",", "top_k_hits", ")", "\n", "return", "match_stats", ".", "questions_doc_hits", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.load_passages": [[152, 170], ["logger.info", "ctx_file.startswith", "gzip.open", "csv.reader", "open", "csv.reader"], "function", ["None"], ["", "def", "load_passages", "(", "ctx_file", ":", "str", ")", "->", "Dict", "[", "object", ",", "Tuple", "[", "str", ",", "str", "]", "]", ":", "\n", "    ", "docs", "=", "{", "}", "\n", "logger", ".", "info", "(", "'Reading data from: %s'", ",", "ctx_file", ")", "\n", "if", "ctx_file", ".", "startswith", "(", "\".gz\"", ")", ":", "\n", "        ", "with", "gzip", ".", "open", "(", "ctx_file", ")", "as", "tsvfile", ":", "\n", "            ", "reader", "=", "csv", ".", "reader", "(", "tsvfile", ",", "delimiter", "=", "'\\t'", ",", ")", "\n", "# file format: doc_id, doc_text, title", "\n", "for", "row", "in", "reader", ":", "\n", "                ", "if", "row", "[", "0", "]", "!=", "'id'", ":", "\n", "                    ", "docs", "[", "row", "[", "0", "]", "]", "=", "(", "row", "[", "1", "]", ",", "row", "[", "2", "]", ")", "\n", "", "", "", "", "else", ":", "\n", "        ", "with", "open", "(", "ctx_file", ")", "as", "tsvfile", ":", "\n", "            ", "reader", "=", "csv", ".", "reader", "(", "tsvfile", ",", "delimiter", "=", "'\\t'", ",", ")", "\n", "# file format: doc_id, doc_text, title", "\n", "for", "row", "in", "reader", ":", "\n", "                ", "if", "row", "[", "0", "]", "!=", "'id'", ":", "\n", "                    ", "docs", "[", "row", "[", "0", "]", "]", "=", "(", "row", "[", "1", "]", ",", "row", "[", "2", "]", ")", "\n", "", "", "", "", "return", "docs", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.save_results": [[172, 221], ["enumerate", "logger.info", "len", "len", "len", "len", "merged_data.append", "open", "writer.write", "xor_output_prediction_format.append", "open", "json.dump", "str", "json.dumps", "out_file.split", "range"], "function", ["None"], ["", "def", "save_results", "(", "passages", ":", "Dict", "[", "object", ",", "Tuple", "[", "str", ",", "str", "]", "]", ",", "questions", ":", "List", "[", "str", "]", ",", "q_ids", ":", "List", "[", "str", "]", ",", "answers", ":", "List", "[", "List", "[", "str", "]", "]", ",", "languages", ":", "List", "[", "str", "]", ",", "\n", "top_passages_and_scores", ":", "List", "[", "Tuple", "[", "List", "[", "object", "]", ",", "List", "[", "float", "]", "]", "]", ",", "per_question_hits", ":", "List", "[", "List", "[", "bool", "]", "]", ",", "\n", "out_file", ":", "str", "\n", ")", ":", "\n", "# join passages text with the result ids, their questions and assigning has|no answer labels", "\n", "    ", "merged_data", "=", "[", "]", "\n", "sqaud_style_data", "=", "{", "'data'", ":", "[", "]", ",", "'version'", ":", "'v1.1'", "}", "\n", "assert", "len", "(", "per_question_hits", ")", "==", "len", "(", "questions", ")", "==", "len", "(", "answers", ")", "\n", "for", "i", ",", "q", "in", "enumerate", "(", "questions", ")", ":", "\n", "        ", "q_answers", "=", "answers", "[", "i", "]", "\n", "q_id", "=", "q_ids", "[", "i", "]", "\n", "lang", "=", "languages", "[", "i", "]", "\n", "results_and_scores", "=", "top_passages_and_scores", "[", "i", "]", "\n", "hits", "=", "per_question_hits", "[", "i", "]", "\n", "docs", "=", "[", "passages", "[", "doc_id", "]", "for", "doc_id", "in", "results_and_scores", "[", "0", "]", "]", "\n", "scores", "=", "[", "str", "(", "score", ")", "for", "score", "in", "results_and_scores", "[", "1", "]", "]", "\n", "ctxs_num", "=", "len", "(", "hits", ")", "\n", "\n", "merged_data", ".", "append", "(", "{", "\n", "'q_id'", ":", "q_id", ",", "\n", "'question'", ":", "q", ",", "\n", "'answers'", ":", "q_answers", ",", "\n", "'lang'", ":", "lang", ",", "\n", "'ctxs'", ":", "[", "\n", "{", "\n", "'id'", ":", "results_and_scores", "[", "0", "]", "[", "c", "]", ",", "\n", "'title'", ":", "docs", "[", "c", "]", "[", "1", "]", ",", "\n", "'text'", ":", "docs", "[", "c", "]", "[", "0", "]", ",", "\n", "'score'", ":", "scores", "[", "c", "]", ",", "\n", "'has_answer'", ":", "hits", "[", "c", "]", ",", "\n", "}", "for", "c", "in", "range", "(", "ctxs_num", ")", "\n", "]", "\n", "}", ")", "\n", "\n", "", "with", "open", "(", "out_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "        ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "merged_data", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "# Generate a prediction file in the XOR retrieve output format.", "\n", "", "xor_output_prediction_format", "=", "[", "]", "\n", "for", "example", "in", "merged_data", ":", "\n", "        ", "q_id", "=", "example", "[", "\"q_id\"", "]", "\n", "ctxs", "=", "[", "ctx", "[", "\"text\"", "]", "for", "ctx", "in", "example", "[", "\"ctxs\"", "]", "]", "\n", "lang", "=", "example", "[", "\"lang\"", "]", "\n", "xor_output_prediction_format", ".", "append", "(", "{", "\"id\"", ":", "q_id", ",", "\"lang\"", ":", "lang", ",", "\"ctxs\"", ":", "ctxs", "}", ")", "\n", "\n", "", "with", "open", "(", "\"{}_xor_retrieve_results.json\"", ".", "format", "(", "out_file", ".", "split", "(", "\".\"", ")", "[", "0", "]", ")", ",", "'w'", ")", "as", "outfile", ":", "\n", "        ", "json", ".", "dump", "(", "xor_output_prediction_format", ",", "outfile", ")", "\n", "\n", "", "logger", ".", "info", "(", "'Saved results * scores  to %s'", ",", "out_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.iterate_encoded_files": [[223, 231], ["enumerate", "logger.info", "open", "pickle.load"], "function", ["None"], ["", "def", "iterate_encoded_files", "(", "vector_files", ":", "list", ")", "->", "Iterator", "[", "Tuple", "[", "object", ",", "np", ".", "array", "]", "]", ":", "\n", "    ", "for", "i", ",", "file", "in", "enumerate", "(", "vector_files", ")", ":", "\n", "        ", "logger", ".", "info", "(", "'Reading file %s'", ",", "file", ")", "\n", "with", "open", "(", "file", ",", "\"rb\"", ")", "as", "reader", ":", "\n", "            ", "doc_vectors", "=", "pickle", ".", "load", "(", "reader", ")", "\n", "for", "doc", "in", "doc_vectors", ":", "\n", "                ", "db_id", ",", "doc_vector", "=", "doc", "\n", "yield", "db_id", ",", "doc_vector", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.main": [[233, 307], ["dpr.utils.model_utils.load_states_from_checkpoint", "dpr.options.set_encoder_params_from_state", "dpr.models.init_biencoder_components", "dpr.utils.model_utils.setup_for_distributed_mode", "encoder.eval", "dpr.utils.model_utils.get_model_obj", "logger.info", "len", "dpr.utils.model_utils.get_model_obj.load_state_dict", "dpr.utils.model_utils.get_model_obj.get_out_size", "logger.info", "dense_retriever.DenseRetriever", "glob.glob", "dense_retriever.parse_qa_jsonlines_file", "dense_retriever.DenseRetriever.generate_question_vectors", "dense_retriever.DenseRetriever.get_top_docs", "dense_retriever.load_passages", "dense_retriever.validate", "dpr.indexer.faiss_indexers.DenseHNSWFlatIndexer", "dpr.indexer.faiss_indexers.DenseFlatIndexer", "os.path.exists", "object.index", "logger.info", "dense_retriever.DenseRetriever.index_encoded_data", "questions.append", "q_ids.append", "question_answers.append", "question_languages.append", "retriever.generate_question_vectors.numpy", "len", "RuntimeError", "dense_retriever.save_results", "dpr.utils.model_utils.load_states_from_checkpoint.model_dict.items", "key.startswith", "input_paths[].split", "object.index"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.load_states_from_checkpoint", "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.set_encoder_params_from_state", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.__init__.init_biencoder_components", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.setup_for_distributed_mode", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.get_model_obj", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.pytext_models.PytextBertEncoder.get_out_size", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.parse_qa_jsonlines_file", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.DenseRetriever.generate_question_vectors", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.DenseRetriever.get_top_docs", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.load_passages", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.validate", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.DenseRetriever.index_encoded_data", "home.repos.pwc.inspect_result.AkariAsai_XORQA.DPR.dense_retriever.save_results"], ["", "", "", "", "def", "main", "(", "args", ")", ":", "\n", "    ", "saved_state", "=", "load_states_from_checkpoint", "(", "args", ".", "model_file", ")", "\n", "set_encoder_params_from_state", "(", "saved_state", ".", "encoder_params", ",", "args", ")", "\n", "\n", "tensorizer", ",", "encoder", ",", "_", "=", "init_biencoder_components", "(", "args", ".", "encoder_model_type", ",", "args", ",", "inference_only", "=", "True", ")", "\n", "\n", "encoder", "=", "encoder", ".", "question_model", "\n", "\n", "encoder", ",", "_", "=", "setup_for_distributed_mode", "(", "encoder", ",", "None", ",", "args", ".", "device", ",", "args", ".", "n_gpu", ",", "\n", "args", ".", "local_rank", ",", "\n", "args", ".", "fp16", ")", "\n", "encoder", ".", "eval", "(", ")", "\n", "\n", "# load weights from the model file", "\n", "model_to_load", "=", "get_model_obj", "(", "encoder", ")", "\n", "logger", ".", "info", "(", "'Loading saved model state ...'", ")", "\n", "\n", "prefix_len", "=", "len", "(", "'question_model.'", ")", "\n", "question_encoder_state", "=", "{", "key", "[", "prefix_len", ":", "]", ":", "value", "for", "(", "key", ",", "value", ")", "in", "saved_state", ".", "model_dict", ".", "items", "(", ")", "if", "\n", "key", ".", "startswith", "(", "'question_model.'", ")", "}", "\n", "model_to_load", ".", "load_state_dict", "(", "question_encoder_state", ")", "\n", "vector_size", "=", "model_to_load", ".", "get_out_size", "(", ")", "\n", "logger", ".", "info", "(", "'Encoder vector_size=%d'", ",", "vector_size", ")", "\n", "\n", "index_buffer_sz", "=", "args", ".", "index_buffer", "\n", "if", "args", ".", "hnsw_index", ":", "\n", "        ", "index", "=", "DenseHNSWFlatIndexer", "(", "vector_size", ")", "\n", "index_buffer_sz", "=", "-", "1", "# encode all at once", "\n", "", "else", ":", "\n", "        ", "index", "=", "DenseFlatIndexer", "(", "vector_size", ")", "\n", "\n", "", "retriever", "=", "DenseRetriever", "(", "encoder", ",", "args", ".", "batch_size", ",", "tensorizer", ",", "index", ")", "\n", "\n", "\n", "# index all passages", "\n", "ctx_files_pattern", "=", "args", ".", "encoded_ctx_file", "\n", "input_paths", "=", "glob", ".", "glob", "(", "ctx_files_pattern", ")", "\n", "\n", "index_path", "=", "\"_\"", ".", "join", "(", "input_paths", "[", "0", "]", ".", "split", "(", "\"_\"", ")", "[", ":", "-", "1", "]", ")", "\n", "if", "args", ".", "save_or_load_index", "and", "os", ".", "path", ".", "exists", "(", "index_path", ")", ":", "\n", "        ", "retriever", ".", "index", ".", "deserialize", "(", "index_path", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "'Reading all passages data from files: %s'", ",", "input_paths", ")", "\n", "retriever", ".", "index_encoded_data", "(", "input_paths", ",", "buffer_size", "=", "index_buffer_sz", ")", "\n", "if", "args", ".", "save_or_load_index", ":", "\n", "            ", "retriever", ".", "index", ".", "serialize", "(", "index_path", ")", "\n", "# get questions & answers", "\n", "", "", "questions", "=", "[", "]", "\n", "question_answers", "=", "[", "]", "\n", "question_languages", "=", "[", "]", "\n", "q_ids", "=", "[", "]", "\n", "\n", "for", "ds_item", "in", "parse_qa_jsonlines_file", "(", "args", ".", "qa_file", ")", ":", "\n", "        ", "question", ",", "q_id", ",", "answers", ",", "language", "=", "ds_item", "\n", "questions", ".", "append", "(", "question", ")", "\n", "q_ids", ".", "append", "(", "q_id", ")", "\n", "question_answers", ".", "append", "(", "answers", ")", "\n", "question_languages", ".", "append", "(", "language", ")", "\n", "\n", "", "questions_tensor", "=", "retriever", ".", "generate_question_vectors", "(", "questions", ")", "\n", "\n", "# get top k results", "\n", "top_ids_and_scores", "=", "retriever", ".", "get_top_docs", "(", "questions_tensor", ".", "numpy", "(", ")", ",", "args", ".", "n_docs", ")", "\n", "\n", "all_passages", "=", "load_passages", "(", "args", ".", "ctx_file", ")", "\n", "\n", "if", "len", "(", "all_passages", ")", "==", "0", ":", "\n", "        ", "raise", "RuntimeError", "(", "'No passages data found. Please specify ctx_file param properly.'", ")", "\n", "\n", "", "questions_doc_hits", "=", "validate", "(", "all_passages", ",", "question_answers", ",", "top_ids_and_scores", ",", "args", ".", "validation_workers", ",", "\n", "args", ".", "match", ")", "\n", "\n", "if", "args", ".", "out_file", ":", "\n", "        ", "save_results", "(", "all_passages", ",", "questions", ",", "q_ids", ",", "question_answers", ",", "question_languages", ",", "top_ids_and_scores", ",", "questions_doc_hits", ",", "args", ".", "out_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.add_tokenizer_params": [[24, 27], ["parser.add_argument"], "function", ["None"], ["def", "add_tokenizer_params", "(", "parser", ":", "argparse", ".", "ArgumentParser", ")", ":", "\n", "    ", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to lower case the input text. True for uncased models, False for cased models.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.add_encoder_params": [[29, 42], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], ["", "def", "add_encoder_params", "(", "parser", ":", "argparse", ".", "ArgumentParser", ")", ":", "\n", "    ", "\"\"\"\n        Common parameters to initialize an encoder-based model\n    \"\"\"", "\n", "parser", ".", "add_argument", "(", "\"--pretrained_model_cfg\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "help", "=", "\"config name for model initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--encoder_model_type\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"model type. One of [hf_bert, pytext_bert, fairseq_roberta]\"", ")", "\n", "parser", ".", "add_argument", "(", "'--pretrained_file'", ",", "type", "=", "str", ",", "help", "=", "\"Some encoders need to be initialized from a file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Saved bi-encoder checkpoint file to initialize the model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--projection_dim\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Extra linear layer on top of standard bert/roberta encoder\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--sequence_length\"", ",", "type", "=", "int", ",", "default", "=", "512", ",", "help", "=", "\"Max length of the encoder input sequence\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.add_training_params": [[44, 73], ["options.add_cuda_params", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.add_cuda_params"], ["", "def", "add_training_params", "(", "parser", ":", "argparse", ".", "ArgumentParser", ")", ":", "\n", "    ", "\"\"\"\n        Common parameters for training\n    \"\"\"", "\n", "add_cuda_params", "(", "parser", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "help", "=", "\"File pattern for the train set\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dev_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "help", "=", "\"\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "default", "=", "2", ",", "type", "=", "int", ",", "help", "=", "\"Amount of questions per batch\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dev_batch_size\"", ",", "type", "=", "int", ",", "default", "=", "4", ",", "\n", "help", "=", "\"amount of questions per batch for dev set validation\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "\"random seed for initialization and dataset shuffling\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--adam_eps\"", ",", "default", "=", "1e-8", ",", "type", "=", "float", ",", "help", "=", "\"Epsilon for Adam optimizer.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--adam_betas\"", ",", "default", "=", "'(0.9, 0.999)'", ",", "type", "=", "str", ",", "help", "=", "\"Betas for Adam optimizer.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--max_grad_norm\"", ",", "default", "=", "1.0", ",", "type", "=", "float", ",", "help", "=", "\"Max gradient norm.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--log_batch_step\"", ",", "default", "=", "100", ",", "type", "=", "int", ",", "help", "=", "\"\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_rolling_loss_step\"", ",", "default", "=", "100", ",", "type", "=", "int", ",", "help", "=", "\"\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "help", "=", "\"\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "1e-5", ",", "type", "=", "float", ",", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--warmup_steps\"", ",", "default", "=", "100", ",", "type", "=", "int", ",", "help", "=", "\"Linear warmup over warmup_steps.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dropout\"", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "help", "=", "\"\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "default", "=", "3.0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.add_cuda_params": [[75, 83], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], ["", "def", "add_cuda_params", "(", "parser", ":", "argparse", ".", "ArgumentParser", ")", ":", "\n", "    ", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "action", "=", "'store_true'", ",", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--fp16_opt_level'", ",", "type", "=", "str", ",", "default", "=", "'O1'", ",", "\n", "help", "=", "\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"", "\n", "\"See details at https://nvidia.github.io/apex/amp.html\"", ")", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.add_reader_preprocessing_params": [[86, 93], ["parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], ["", "def", "add_reader_preprocessing_params", "(", "parser", ":", "argparse", ".", "ArgumentParser", ")", ":", "\n", "    ", "parser", ".", "add_argument", "(", "\"--gold_passages_src\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"File with the original dataset passages (json format). Required for train set\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--gold_passages_src_dev\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"File with the original dataset passages (json format). Required for dev set\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_workers\"", ",", "type", "=", "int", ",", "default", "=", "16", ",", "\n", "help", "=", "\"number of parallel processes to binarize reader data\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.get_encoder_checkpoint_params_names": [[95, 99], ["None"], "function", ["None"], ["", "def", "get_encoder_checkpoint_params_names", "(", ")", ":", "\n", "    ", "return", "[", "'do_lower_case'", ",", "'pretrained_model_cfg'", ",", "'encoder_model_type'", ",", "\n", "'pretrained_file'", ",", "\n", "'projection_dim'", ",", "'sequence_length'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.get_encoder_params_state": [[101, 113], ["options.get_encoder_checkpoint_params_names", "getattr"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.get_encoder_checkpoint_params_names"], ["", "def", "get_encoder_params_state", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n     Selects the param values to be saved in a checkpoint, so that a trained model faile can be used for downstream\n     tasks without the need to specify these parameter again\n    :return: Dict of params to memorize in a checkpoint\n    \"\"\"", "\n", "params_to_save", "=", "get_encoder_checkpoint_params_names", "(", ")", "\n", "\n", "r", "=", "{", "}", "\n", "for", "param", "in", "params_to_save", ":", "\n", "        ", "r", "[", "param", "]", "=", "getattr", "(", "args", ",", "param", ")", "\n", "", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.set_encoder_params_from_state": [[115, 127], ["options.get_encoder_checkpoint_params_names", "hasattr", "setattr", "logger.warning"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.get_encoder_checkpoint_params_names"], ["", "def", "set_encoder_params_from_state", "(", "state", ",", "args", ")", ":", "\n", "    ", "if", "not", "state", ":", "\n", "        ", "return", "\n", "", "params_to_save", "=", "get_encoder_checkpoint_params_names", "(", ")", "\n", "\n", "override_params", "=", "[", "(", "param", ",", "state", "[", "param", "]", ")", "for", "param", "in", "params_to_save", "if", "param", "in", "state", "and", "state", "[", "param", "]", "]", "\n", "for", "param", ",", "value", "in", "override_params", ":", "\n", "        ", "if", "hasattr", "(", "args", ",", "param", ")", ":", "\n", "            ", "logger", ".", "warning", "(", "'Overriding args parameter value from checkpoint state. Param = %s, value = %s'", ",", "param", ",", "\n", "value", ")", "\n", "", "setattr", "(", "args", ",", "param", ",", "value", ")", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.set_seed": [[129, 136], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all"], "function", ["None"], ["", "def", "set_seed", "(", "args", ")", ":", "\n", "    ", "seed", "=", "args", ".", "seed", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "if", "args", ".", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.setup_args_gpu": [[138, 162], ["os.environ.get", "logger.info", "logger.info", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "int", "socket.gethostname", "torch.cuda.is_available"], "function", ["None"], ["", "", "def", "setup_args_gpu", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n     Setup arguments CUDA, GPU & distributed training\n    \"\"\"", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "# single-node multi-gpu (or cpu) mode", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "args", ".", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "# distributed mode", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "\"nccl\"", ")", "\n", "args", ".", "n_gpu", "=", "1", "\n", "", "args", ".", "device", "=", "device", "\n", "ws", "=", "os", ".", "environ", ".", "get", "(", "'WORLD_SIZE'", ")", "\n", "\n", "args", ".", "distributed_world_size", "=", "int", "(", "ws", ")", "if", "ws", "else", "1", "\n", "\n", "logger", ".", "info", "(", "\n", "'Initialized host %s as d.rank %d on device=%s, n_gpu=%d, world size=%d'", ",", "socket", ".", "gethostname", "(", ")", ",", "\n", "args", ".", "local_rank", ",", "device", ",", "\n", "args", ".", "n_gpu", ",", "\n", "args", ".", "distributed_world_size", ")", "\n", "logger", ".", "info", "(", "\"16-bits training: %s \"", ",", "args", ".", "fp16", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.dpr.options.print_args": [[164, 170], ["logger.info", "sorted", "logger.info", "vars().items", "logger.info", "vars", "len"], "function", ["None"], ["", "def", "print_args", "(", "args", ")", ":", "\n", "    ", "logger", ".", "info", "(", "\" **************** CONFIGURATION **************** \"", ")", "\n", "for", "key", ",", "val", "in", "sorted", "(", "vars", "(", "args", ")", ".", "items", "(", ")", ")", ":", "\n", "        ", "keystr", "=", "\"{}\"", ".", "format", "(", "key", ")", "+", "(", "\" \"", "*", "(", "30", "-", "len", "(", "key", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"%s -->   %s\"", ",", "keystr", ",", "val", ")", "\n", "", "logger", ".", "info", "(", "\" **************** CONFIGURATION **************** \"", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.dist_utils.get_rank": [[18, 20], ["torch.get_rank"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.dist_utils.get_rank"], ["def", "get_rank", "(", ")", ":", "\n", "    ", "return", "dist", ".", "get_rank", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.dist_utils.get_world_size": [[22, 24], ["torch.get_world_size"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.dist_utils.get_world_size"], ["", "def", "get_world_size", "(", ")", ":", "\n", "    ", "return", "dist", ".", "get_world_size", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.dist_utils.get_default_group": [[26, 28], ["None"], "function", ["None"], ["", "def", "get_default_group", "(", ")", ":", "\n", "    ", "return", "dist", ".", "group", ".", "WORLD", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.dist_utils.all_reduce": [[30, 34], ["torch.all_reduce", "dist_utils.get_default_group"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.dist_utils.all_reduce", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.dist_utils.get_default_group"], ["", "def", "all_reduce", "(", "tensor", ",", "group", "=", "None", ")", ":", "\n", "    ", "if", "group", "is", "None", ":", "\n", "        ", "group", "=", "get_default_group", "(", ")", "\n", "", "return", "dist", ".", "all_reduce", "(", "tensor", ",", "group", "=", "group", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.dist_utils.all_gather_list": [[36, 91], ["pickle.dumps", "len", "dist_utils.get_rank", "dist_utils.get_world_size", "buffer.zero_", "len.to_bytes", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "buffer[].copy_", "dist_utils.all_reduce", "ValueError", "torch.cuda.ByteTensor", "torch.cuda.ByteTensor", "torch.ByteTensor().pin_memory", "torch.ByteTensor().pin_memory", "list", "list", "range", "hasattr", "all_gather_list._buffer.numel", "int.from_bytes", "Exception", "torch.ByteTensor", "torch.ByteTensor", "result.append", "pickle.loads", "bytes", "out_buffer[].tolist"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.dist_utils.get_rank", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.dist_utils.get_world_size", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.dist_utils.all_reduce"], ["", "def", "all_gather_list", "(", "data", ",", "group", "=", "None", ",", "max_size", "=", "16384", ")", ":", "\n", "    ", "\"\"\"Gathers arbitrary data from all nodes into a list.\n    Similar to :func:`~torch.distributed.all_gather` but for arbitrary Python\n    data. Note that *data* must be picklable.\n    Args:\n        data (Any): data from the local worker to be gathered on other workers\n        group (optional): group of the collective\n    \"\"\"", "\n", "SIZE_STORAGE_BYTES", "=", "4", "# int32 to encode the payload size", "\n", "\n", "enc", "=", "pickle", ".", "dumps", "(", "data", ")", "\n", "enc_size", "=", "len", "(", "enc", ")", "\n", "\n", "if", "enc_size", "+", "SIZE_STORAGE_BYTES", ">", "max_size", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "'encoded data exceeds max_size, this can be fixed by increasing buffer size: {}'", ".", "format", "(", "enc_size", ")", ")", "\n", "\n", "", "rank", "=", "get_rank", "(", ")", "\n", "world_size", "=", "get_world_size", "(", ")", "\n", "buffer_size", "=", "max_size", "*", "world_size", "\n", "\n", "if", "not", "hasattr", "(", "all_gather_list", ",", "'_buffer'", ")", "or", "all_gather_list", ".", "_buffer", ".", "numel", "(", ")", "<", "buffer_size", ":", "\n", "        ", "all_gather_list", ".", "_buffer", "=", "torch", ".", "cuda", ".", "ByteTensor", "(", "buffer_size", ")", "\n", "all_gather_list", ".", "_cpu_buffer", "=", "torch", ".", "ByteTensor", "(", "max_size", ")", ".", "pin_memory", "(", ")", "\n", "\n", "", "buffer", "=", "all_gather_list", ".", "_buffer", "\n", "buffer", ".", "zero_", "(", ")", "\n", "cpu_buffer", "=", "all_gather_list", ".", "_cpu_buffer", "\n", "\n", "assert", "enc_size", "<", "256", "**", "SIZE_STORAGE_BYTES", ",", "'Encoded object size should be less than {} bytes'", ".", "format", "(", "\n", "256", "**", "SIZE_STORAGE_BYTES", ")", "\n", "\n", "size_bytes", "=", "enc_size", ".", "to_bytes", "(", "SIZE_STORAGE_BYTES", ",", "byteorder", "=", "'big'", ")", "\n", "\n", "cpu_buffer", "[", "0", ":", "SIZE_STORAGE_BYTES", "]", "=", "torch", ".", "ByteTensor", "(", "list", "(", "size_bytes", ")", ")", "\n", "cpu_buffer", "[", "SIZE_STORAGE_BYTES", ":", "enc_size", "+", "SIZE_STORAGE_BYTES", "]", "=", "torch", ".", "ByteTensor", "(", "list", "(", "enc", ")", ")", "\n", "\n", "start", "=", "rank", "*", "max_size", "\n", "size", "=", "enc_size", "+", "SIZE_STORAGE_BYTES", "\n", "buffer", "[", "start", ":", "start", "+", "size", "]", ".", "copy_", "(", "cpu_buffer", "[", ":", "size", "]", ")", "\n", "\n", "all_reduce", "(", "buffer", ",", "group", "=", "group", ")", "\n", "\n", "try", ":", "\n", "        ", "result", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "world_size", ")", ":", "\n", "            ", "out_buffer", "=", "buffer", "[", "i", "*", "max_size", ":", "(", "i", "+", "1", ")", "*", "max_size", "]", "\n", "size", "=", "int", ".", "from_bytes", "(", "out_buffer", "[", "0", ":", "SIZE_STORAGE_BYTES", "]", ",", "byteorder", "=", "'big'", ")", "\n", "if", "size", ">", "0", ":", "\n", "                ", "result", ".", "append", "(", "pickle", ".", "loads", "(", "bytes", "(", "out_buffer", "[", "SIZE_STORAGE_BYTES", ":", "size", "+", "SIZE_STORAGE_BYTES", "]", ".", "tolist", "(", ")", ")", ")", ")", "\n", "", "", "return", "result", "\n", "", "except", "pickle", ".", "UnpicklingError", ":", "\n", "        ", "raise", "Exception", "(", "\n", "'Unable to unpickle data from other workers. all_gather_list requires all '", "\n", "'workers to enter the function together, so this error usually indicates '", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.setup_for_distributed_mode": [[26, 49], ["torch.nn.parallel.DistributedDataParallel.to", "amp.initialize", "torch.nn.DataParallel", "torch.nn.parallel.DistributedDataParallel", "apex.amp.register_half_function", "ImportError"], "function", ["None"], ["def", "setup_for_distributed_mode", "(", "model", ":", "nn", ".", "Module", ",", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "device", ":", "object", ",", "n_gpu", ":", "int", "=", "1", ",", "\n", "local_rank", ":", "int", "=", "-", "1", ",", "\n", "fp16", ":", "bool", "=", "False", ",", "\n", "fp16_opt_level", ":", "str", "=", "\"O1\"", ")", "->", "(", "nn", ".", "Module", ",", "torch", ".", "optim", ".", "Optimizer", ")", ":", "\n", "    ", "model", ".", "to", "(", "device", ")", "\n", "if", "fp16", ":", "\n", "        ", "try", ":", "\n", "            ", "import", "apex", "\n", "from", "apex", "import", "amp", "\n", "apex", ".", "amp", ".", "register_half_function", "(", "torch", ",", "\"einsum\"", ")", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"", ")", "\n", "\n", "", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "optimizer", ",", "opt_level", "=", "fp16_opt_level", ")", "\n", "\n", "", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "if", "local_rank", "!=", "-", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "model", ",", "device_ids", "=", "[", "local_rank", "]", ",", "\n", "output_device", "=", "local_rank", ",", "\n", "find_unused_parameters", "=", "True", ")", "\n", "", "return", "model", ",", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.move_to_cuda": [[51, 71], ["model_utils.move_to_cuda._move_to_cuda"], "function", ["None"], ["", "def", "move_to_cuda", "(", "sample", ")", ":", "\n", "    ", "if", "len", "(", "sample", ")", "==", "0", ":", "\n", "        ", "return", "{", "}", "\n", "\n", "", "def", "_move_to_cuda", "(", "maybe_tensor", ")", ":", "\n", "        ", "if", "torch", ".", "is_tensor", "(", "maybe_tensor", ")", ":", "\n", "            ", "return", "maybe_tensor", ".", "cuda", "(", ")", "\n", "", "elif", "isinstance", "(", "maybe_tensor", ",", "dict", ")", ":", "\n", "            ", "return", "{", "\n", "key", ":", "_move_to_cuda", "(", "value", ")", "\n", "for", "key", ",", "value", "in", "maybe_tensor", ".", "items", "(", ")", "\n", "}", "\n", "", "elif", "isinstance", "(", "maybe_tensor", ",", "list", ")", ":", "\n", "            ", "return", "[", "_move_to_cuda", "(", "x", ")", "for", "x", "in", "maybe_tensor", "]", "\n", "", "elif", "isinstance", "(", "maybe_tensor", ",", "tuple", ")", ":", "\n", "            ", "return", "[", "_move_to_cuda", "(", "x", ")", "for", "x", "in", "maybe_tensor", "]", "\n", "", "else", ":", "\n", "            ", "return", "maybe_tensor", "\n", "\n", "", "", "return", "_move_to_cuda", "(", "sample", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.move_to_device": [[73, 93], ["model_utils.move_to_device._move_to_device"], "function", ["None"], ["", "def", "move_to_device", "(", "sample", ",", "device", ")", ":", "\n", "    ", "if", "len", "(", "sample", ")", "==", "0", ":", "\n", "        ", "return", "{", "}", "\n", "\n", "", "def", "_move_to_device", "(", "maybe_tensor", ",", "device", ")", ":", "\n", "        ", "if", "torch", ".", "is_tensor", "(", "maybe_tensor", ")", ":", "\n", "            ", "return", "maybe_tensor", ".", "to", "(", "device", ")", "\n", "", "elif", "isinstance", "(", "maybe_tensor", ",", "dict", ")", ":", "\n", "            ", "return", "{", "\n", "key", ":", "_move_to_device", "(", "value", ",", "device", ")", "\n", "for", "key", ",", "value", "in", "maybe_tensor", ".", "items", "(", ")", "\n", "}", "\n", "", "elif", "isinstance", "(", "maybe_tensor", ",", "list", ")", ":", "\n", "            ", "return", "[", "_move_to_device", "(", "x", ",", "device", ")", "for", "x", "in", "maybe_tensor", "]", "\n", "", "elif", "isinstance", "(", "maybe_tensor", ",", "tuple", ")", ":", "\n", "            ", "return", "[", "_move_to_device", "(", "x", ",", "device", ")", "for", "x", "in", "maybe_tensor", "]", "\n", "", "else", ":", "\n", "            ", "return", "maybe_tensor", "\n", "\n", "", "", "return", "_move_to_device", "(", "sample", ",", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.get_schedule_linear": [[95, 108], ["torch.optim.lr_scheduler.LambdaLR", "max", "float", "float", "float", "float", "max", "max"], "function", ["None"], ["", "def", "get_schedule_linear", "(", "optimizer", ",", "warmup_steps", ",", "training_steps", ",", "last_epoch", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\" Create a schedule with a learning rate that decreases linearly after\n    linearly increasing during a warmup period.\n    \"\"\"", "\n", "\n", "def", "lr_lambda", "(", "current_step", ")", ":", "\n", "        ", "if", "current_step", "<", "warmup_steps", ":", "\n", "            ", "return", "float", "(", "current_step", ")", "/", "float", "(", "max", "(", "1", ",", "warmup_steps", ")", ")", "\n", "", "return", "max", "(", "\n", "0.0", ",", "float", "(", "training_steps", "-", "current_step", ")", "/", "float", "(", "max", "(", "1", ",", "training_steps", "-", "warmup_steps", ")", ")", "\n", ")", "\n", "\n", "", "return", "LambdaLR", "(", "optimizer", ",", "lr_lambda", ",", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.init_weights": [[110, 119], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "function", ["None"], ["", "def", "init_weights", "(", "modules", ":", "List", ")", ":", "\n", "    ", "for", "module", "in", "modules", ":", "\n", "        ", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "0.02", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.get_model_obj": [[121, 123], ["hasattr"], "function", ["None"], ["", "", "", "def", "get_model_obj", "(", "model", ":", "nn", ".", "Module", ")", ":", "\n", "    ", "return", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.get_model_file": [[125, 136], ["logger.info", "os.path.exists", "glob.glob", "len", "max", "os.path.join"], "function", ["None"], ["", "def", "get_model_file", "(", "args", ",", "file_prefix", ")", "->", "str", ":", "\n", "    ", "if", "args", ".", "model_file", "and", "os", ".", "path", ".", "exists", "(", "args", ".", "model_file", ")", ":", "\n", "        ", "return", "args", ".", "model_file", "\n", "\n", "", "out_cp_files", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "file_prefix", "+", "'*'", ")", ")", "if", "args", ".", "output_dir", "else", "[", "]", "\n", "logger", ".", "info", "(", "'Checkpoint files %s'", ",", "out_cp_files", ")", "\n", "model_file", "=", "None", "\n", "\n", "if", "len", "(", "out_cp_files", ")", ">", "0", ":", "\n", "        ", "model_file", "=", "max", "(", "out_cp_files", ",", "key", "=", "os", ".", "path", ".", "getctime", ")", "\n", "", "return", "model_file", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.load_states_from_checkpoint": [[138, 143], ["logger.info", "torch.load", "logger.info", "CheckpointState", "torch.load.keys", "torch.serialization.default_restore_location"], "function", ["None"], ["", "def", "load_states_from_checkpoint", "(", "model_file", ":", "str", ")", "->", "CheckpointState", ":", "\n", "    ", "logger", ".", "info", "(", "'Reading saved model from %s'", ",", "model_file", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "model_file", ",", "map_location", "=", "lambda", "s", ",", "l", ":", "default_restore_location", "(", "s", ",", "'cpu'", ")", ")", "\n", "logger", ".", "info", "(", "'model_state_dict keys %s'", ",", "state_dict", ".", "keys", "(", ")", ")", "\n", "return", "CheckpointState", "(", "**", "state_dict", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.ShardedDataIterator.__init__": [[62, 95], ["len", "max", "max", "math.ceil", "min", "logger.debug", "math.ceil", "int"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "data", ":", "list", ",", "shard_id", ":", "int", "=", "0", ",", "num_shards", ":", "int", "=", "1", ",", "batch_size", ":", "int", "=", "1", ",", "shuffle", "=", "True", ",", "\n", "shuffle_seed", ":", "int", "=", "0", ",", "offset", ":", "int", "=", "0", ",", "\n", "strict_batch_size", ":", "bool", "=", "False", "\n", ")", ":", "\n", "\n", "        ", "self", ".", "data", "=", "data", "\n", "total_size", "=", "len", "(", "data", ")", "\n", "\n", "self", ".", "shards_num", "=", "max", "(", "num_shards", ",", "1", ")", "\n", "self", ".", "shard_id", "=", "max", "(", "shard_id", ",", "0", ")", "\n", "\n", "samples_per_shard", "=", "math", ".", "ceil", "(", "total_size", "/", "self", ".", "shards_num", ")", "\n", "\n", "self", ".", "shard_start_idx", "=", "self", ".", "shard_id", "*", "samples_per_shard", "\n", "\n", "self", ".", "shard_end_idx", "=", "min", "(", "self", ".", "shard_start_idx", "+", "samples_per_shard", ",", "total_size", ")", "\n", "\n", "if", "strict_batch_size", ":", "\n", "            ", "self", ".", "max_iterations", "=", "math", ".", "ceil", "(", "samples_per_shard", "/", "batch_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "max_iterations", "=", "int", "(", "samples_per_shard", "/", "batch_size", ")", "\n", "\n", "", "logger", ".", "debug", "(", "\n", "'samples_per_shard=%d, shard_start_idx=%d, shard_end_idx=%d, max_iterations=%d'", ",", "samples_per_shard", ",", "\n", "self", ".", "shard_start_idx", ",", "\n", "self", ".", "shard_end_idx", ",", "\n", "self", ".", "max_iterations", ")", "\n", "\n", "self", ".", "iteration", "=", "offset", "# to track in-shard iteration status", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "shuffle_seed", "=", "shuffle_seed", "\n", "self", ".", "strict_batch_size", "=", "strict_batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.ShardedDataIterator.total_data_len": [[96, 98], ["len"], "methods", ["None"], ["", "def", "total_data_len", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.ShardedDataIterator.iterate_data": [[99, 128], ["range", "logger.debug", "random.Random", "random.Random.shuffle", "len", "logger.debug", "logger.debug", "items.extend", "len", "len"], "methods", ["None"], ["", "def", "iterate_data", "(", "self", ",", "epoch", ":", "int", "=", "0", ")", "->", "Iterator", "[", "List", "]", ":", "\n", "        ", "if", "self", ".", "shuffle", ":", "\n", "# to be able to resume, same shuffling should be used when starting from a failed/stopped iteration", "\n", "            ", "epoch_rnd", "=", "random", ".", "Random", "(", "self", ".", "shuffle_seed", "+", "epoch", ")", "\n", "epoch_rnd", ".", "shuffle", "(", "self", ".", "data", ")", "\n", "\n", "# if resuming iteration somewhere in the middle of epoch, one needs to adjust max_iterations", "\n", "\n", "", "max_iterations", "=", "self", ".", "max_iterations", "-", "self", ".", "iteration", "\n", "\n", "shard_samples", "=", "self", ".", "data", "[", "self", ".", "shard_start_idx", ":", "self", ".", "shard_end_idx", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "iteration", "*", "self", ".", "batch_size", ",", "len", "(", "shard_samples", ")", ",", "self", ".", "batch_size", ")", ":", "\n", "            ", "items", "=", "shard_samples", "[", "i", ":", "i", "+", "self", ".", "batch_size", "]", "\n", "if", "self", ".", "strict_batch_size", "and", "len", "(", "items", ")", "<", "self", ".", "batch_size", ":", "\n", "                ", "logger", ".", "debug", "(", "'Extending batch to max size'", ")", "\n", "items", ".", "extend", "(", "shard_samples", "[", "0", ":", "self", ".", "batch_size", "-", "len", "(", "items", ")", "]", ")", "\n", "", "self", ".", "iteration", "+=", "1", "\n", "yield", "items", "\n", "\n", "# some shards may done iterating while the others are at the last batch. Just return the first batch", "\n", "", "while", "self", ".", "iteration", "<", "max_iterations", ":", "\n", "            ", "logger", ".", "debug", "(", "'Fulfilling non complete shard='", ".", "format", "(", "self", ".", "shard_id", ")", ")", "\n", "self", ".", "iteration", "+=", "1", "\n", "batch", "=", "shard_samples", "[", "0", ":", "self", ".", "batch_size", "]", "\n", "yield", "batch", "\n", "\n", "", "logger", ".", "debug", "(", "'Finished iterating, iteration={}, shard={}'", ".", "format", "(", "self", ".", "iteration", ",", "self", ".", "shard_id", ")", ")", "\n", "# reset the iteration status", "\n", "self", ".", "iteration", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.ShardedDataIterator.get_iteration": [[129, 131], ["None"], "methods", ["None"], ["", "def", "get_iteration", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "iteration", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.ShardedDataIterator.apply": [[132, 135], ["visitor_func"], "methods", ["None"], ["", "def", "apply", "(", "self", ",", "visitor_func", ":", "Callable", ")", ":", "\n", "        ", "for", "sample", "in", "self", ".", "data", ":", "\n", "            ", "visitor_func", "(", "sample", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.Tensorizer.text_to_tensor": [[149, 151], ["None"], "methods", ["None"], ["def", "text_to_tensor", "(", "self", ",", "text", ":", "str", ",", "title", ":", "str", "=", "None", ",", "add_special_tokens", ":", "bool", "=", "True", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.Tensorizer.get_pair_separator_ids": [[152, 154], ["None"], "methods", ["None"], ["", "def", "get_pair_separator_ids", "(", "self", ")", "->", "T", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.Tensorizer.get_pad_id": [[155, 157], ["None"], "methods", ["None"], ["", "def", "get_pad_id", "(", "self", ")", "->", "int", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.Tensorizer.get_attn_mask": [[158, 160], ["None"], "methods", ["None"], ["", "def", "get_attn_mask", "(", "self", ",", "tokens_tensor", ":", "T", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.Tensorizer.is_sub_word_id": [[161, 163], ["None"], "methods", ["None"], ["", "def", "is_sub_word_id", "(", "self", ",", "token_id", ":", "int", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.Tensorizer.to_string": [[164, 166], ["None"], "methods", ["None"], ["", "def", "to_string", "(", "self", ",", "token_ids", ",", "skip_special_tokens", "=", "True", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.Tensorizer.set_pad_to_max": [[167, 169], ["None"], "methods", ["None"], ["", "def", "set_pad_to_max", "(", "self", ",", "pad", ":", "bool", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.read_serialized_data_from_files": [[24, 34], ["enumerate", "logger.info", "open", "logger.info", "pickle.load", "results.extend", "logger.info", "len", "len"], "function", ["None"], ["def", "read_serialized_data_from_files", "(", "paths", ":", "List", "[", "str", "]", ")", "->", "List", ":", "\n", "    ", "results", "=", "[", "]", "\n", "for", "i", ",", "path", "in", "enumerate", "(", "paths", ")", ":", "\n", "        ", "with", "open", "(", "path", ",", "\"rb\"", ")", "as", "reader", ":", "\n", "            ", "logger", ".", "info", "(", "'Reading file %s'", ",", "path", ")", "\n", "data", "=", "pickle", ".", "load", "(", "reader", ")", "\n", "results", ".", "extend", "(", "data", ")", "\n", "logger", ".", "info", "(", "'Aggregated data size: {}'", ".", "format", "(", "len", "(", "results", ")", ")", ")", "\n", "", "", "logger", ".", "info", "(", "'Total data size: {}'", ".", "format", "(", "len", "(", "results", ")", ")", ")", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.read_data_from_json_files": [[36, 52], ["enumerate", "len", "len", "len", "open", "logger.info", "json.load", "int", "results.extend", "logger.info", "len"], "function", ["None"], ["", "def", "read_data_from_json_files", "(", "paths", ":", "List", "[", "str", "]", ",", "upsample_rates", ":", "List", "=", "None", ")", "->", "List", ":", "\n", "    ", "results", "=", "[", "]", "\n", "if", "upsample_rates", "is", "None", ":", "\n", "        ", "upsample_rates", "=", "[", "1", "]", "*", "len", "(", "paths", ")", "\n", "\n", "", "assert", "len", "(", "upsample_rates", ")", "==", "len", "(", "paths", ")", ",", "'up-sample rates parameter doesn\\'t match input files amount'", "\n", "\n", "for", "i", ",", "path", "in", "enumerate", "(", "paths", ")", ":", "\n", "        ", "with", "open", "(", "path", ",", "'r'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "logger", ".", "info", "(", "'Reading file %s'", "%", "path", ")", "\n", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "upsample_factor", "=", "int", "(", "upsample_rates", "[", "i", "]", ")", "\n", "data", "=", "data", "*", "upsample_factor", "\n", "results", ".", "extend", "(", "data", ")", "\n", "logger", ".", "info", "(", "'Aggregated data size: {}'", ".", "format", "(", "len", "(", "results", ")", ")", ")", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.normalize_question": [[137, 141], ["None"], "function", ["None"], ["", "", "", "def", "normalize_question", "(", "question", ":", "str", ")", "->", "str", ":", "\n", "    ", "if", "question", "[", "-", "1", "]", "==", "'?'", ":", "\n", "        ", "question", "=", "question", "[", ":", "-", "1", "]", "\n", "", "return", "question", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokens.__init__": [[31, 35], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "data", ",", "annotators", ",", "opts", "=", "None", ")", ":", "\n", "        ", "self", ".", "data", "=", "data", "\n", "self", ".", "annotators", "=", "annotators", "\n", "self", ".", "opts", "=", "opts", "or", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokens.__len__": [[36, 39], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"The number of tokens.\"\"\"", "\n", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokens.slice": [[40, 45], ["copy.copy"], "methods", ["None"], ["", "def", "slice", "(", "self", ",", "i", "=", "None", ",", "j", "=", "None", ")", ":", "\n", "        ", "\"\"\"Return a view of the list of tokens from [i, j).\"\"\"", "\n", "new_tokens", "=", "copy", ".", "copy", "(", "self", ")", "\n", "new_tokens", ".", "data", "=", "self", ".", "data", "[", "i", ":", "j", "]", "\n", "return", "new_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokens.untokenize": [[46, 49], ["None"], "methods", ["None"], ["", "def", "untokenize", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the original text (with whitespace reinserted).\"\"\"", "\n", "return", "''", ".", "join", "(", "[", "t", "[", "self", ".", "TEXT_WS", "]", "for", "t", "in", "self", ".", "data", "]", ")", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokens.words": [[50, 60], ["t[].lower"], "methods", ["None"], ["", "def", "words", "(", "self", ",", "uncased", "=", "False", ")", ":", "\n", "        ", "\"\"\"Returns a list of the text of each token\n\n        Args:\n            uncased: lower cases text\n        \"\"\"", "\n", "if", "uncased", ":", "\n", "            ", "return", "[", "t", "[", "self", ".", "TEXT", "]", ".", "lower", "(", ")", "for", "t", "in", "self", ".", "data", "]", "\n", "", "else", ":", "\n", "            ", "return", "[", "t", "[", "self", ".", "TEXT", "]", "for", "t", "in", "self", ".", "data", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokens.offsets": [[61, 64], ["None"], "methods", ["None"], ["", "", "def", "offsets", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns a list of [start, end) character offsets of each token.\"\"\"", "\n", "return", "[", "t", "[", "self", ".", "SPAN", "]", "for", "t", "in", "self", ".", "data", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokens.pos": [[65, 72], ["None"], "methods", ["None"], ["", "def", "pos", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns a list of part-of-speech tags of each token.\n        Returns None if this annotation was not included.\n        \"\"\"", "\n", "if", "'pos'", "not", "in", "self", ".", "annotators", ":", "\n", "            ", "return", "None", "\n", "", "return", "[", "t", "[", "self", ".", "POS", "]", "for", "t", "in", "self", ".", "data", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokens.lemmas": [[73, 80], ["None"], "methods", ["None"], ["", "def", "lemmas", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns a list of the lemmatized text of each token.\n        Returns None if this annotation was not included.\n        \"\"\"", "\n", "if", "'lemma'", "not", "in", "self", ".", "annotators", ":", "\n", "            ", "return", "None", "\n", "", "return", "[", "t", "[", "self", ".", "LEMMA", "]", "for", "t", "in", "self", ".", "data", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokens.entities": [[81, 88], ["None"], "methods", ["None"], ["", "def", "entities", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns a list of named-entity-recognition tags of each token.\n        Returns None if this annotation was not included.\n        \"\"\"", "\n", "if", "'ner'", "not", "in", "self", ".", "annotators", ":", "\n", "            ", "return", "None", "\n", "", "return", "[", "t", "[", "self", ".", "NER", "]", "for", "t", "in", "self", ".", "data", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokens.ngrams": [[89, 116], ["tokenizers.Tokens.words", "filter_fn", "range", "range", "len", "min", "tokenizers.Tokens.ngrams._skip"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokens.words"], ["", "def", "ngrams", "(", "self", ",", "n", "=", "1", ",", "uncased", "=", "False", ",", "filter_fn", "=", "None", ",", "as_strings", "=", "True", ")", ":", "\n", "        ", "\"\"\"Returns a list of all ngrams from length 1 to n.\n\n        Args:\n            n: upper limit of ngram length\n            uncased: lower cases text\n            filter_fn: user function that takes in an ngram list and returns\n              True or False to keep or not keep the ngram\n            as_string: return the ngram as a string vs list\n        \"\"\"", "\n", "\n", "def", "_skip", "(", "gram", ")", ":", "\n", "            ", "if", "not", "filter_fn", ":", "\n", "                ", "return", "False", "\n", "", "return", "filter_fn", "(", "gram", ")", "\n", "\n", "", "words", "=", "self", ".", "words", "(", "uncased", ")", "\n", "ngrams", "=", "[", "(", "s", ",", "e", "+", "1", ")", "\n", "for", "s", "in", "range", "(", "len", "(", "words", ")", ")", "\n", "for", "e", "in", "range", "(", "s", ",", "min", "(", "s", "+", "n", ",", "len", "(", "words", ")", ")", ")", "\n", "if", "not", "_skip", "(", "words", "[", "s", ":", "e", "+", "1", "]", ")", "]", "\n", "\n", "# Concatenate into strings", "\n", "if", "as_strings", ":", "\n", "            ", "ngrams", "=", "[", "'{}'", ".", "format", "(", "' '", ".", "join", "(", "words", "[", "s", ":", "e", "]", ")", ")", "for", "(", "s", ",", "e", ")", "in", "ngrams", "]", "\n", "\n", "", "return", "ngrams", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokens.entity_groups": [[117, 137], ["tokenizers.Tokens.entities", "tokenizers.Tokens.opts.get", "len", "groups.append", "len", "tokenizers.Tokens.slice().untokenize", "tokenizers.Tokens.slice"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokens.entities", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokens.untokenize", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokens.slice"], ["", "def", "entity_groups", "(", "self", ")", ":", "\n", "        ", "\"\"\"Group consecutive entity tokens with the same NER tag.\"\"\"", "\n", "entities", "=", "self", ".", "entities", "(", ")", "\n", "if", "not", "entities", ":", "\n", "            ", "return", "None", "\n", "", "non_ent", "=", "self", ".", "opts", ".", "get", "(", "'non_ent'", ",", "'O'", ")", "\n", "groups", "=", "[", "]", "\n", "idx", "=", "0", "\n", "while", "idx", "<", "len", "(", "entities", ")", ":", "\n", "            ", "ner_tag", "=", "entities", "[", "idx", "]", "\n", "# Check for entity tag", "\n", "if", "ner_tag", "!=", "non_ent", ":", "\n", "# Chomp the sequence", "\n", "                ", "start", "=", "idx", "\n", "while", "(", "idx", "<", "len", "(", "entities", ")", "and", "entities", "[", "idx", "]", "==", "ner_tag", ")", ":", "\n", "                    ", "idx", "+=", "1", "\n", "", "groups", ".", "append", "(", "(", "self", ".", "slice", "(", "start", ",", "idx", ")", ".", "untokenize", "(", ")", ",", "ner_tag", ")", ")", "\n", "", "else", ":", "\n", "                ", "idx", "+=", "1", "\n", "", "", "return", "groups", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokenizer.tokenize": [[144, 146], ["None"], "methods", ["None"], ["def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokenizer.shutdown": [[147, 149], ["None"], "methods", ["None"], ["", "def", "shutdown", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokenizer.__del__": [[150, 152], ["tokenizers.Tokenizer.shutdown"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokenizer.shutdown"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "self", ".", "shutdown", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.SimpleTokenizer.__init__": [[158, 171], ["regex.compile", "set", "len", "logger.warning", "kwargs.get", "kwargs.get", "type"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            annotators: None or empty set (only tokenizes).\n        \"\"\"", "\n", "self", ".", "_regexp", "=", "regex", ".", "compile", "(", "\n", "'(%s)|(%s)'", "%", "(", "self", ".", "ALPHA_NUM", ",", "self", ".", "NON_WS", ")", ",", "\n", "flags", "=", "regex", ".", "IGNORECASE", "+", "regex", ".", "UNICODE", "+", "regex", ".", "MULTILINE", "\n", ")", "\n", "if", "len", "(", "kwargs", ".", "get", "(", "'annotators'", ",", "{", "}", ")", ")", ">", "0", ":", "\n", "            ", "logger", ".", "warning", "(", "'%s only tokenizes! Skipping annotators: %s'", "%", "\n", "(", "type", "(", "self", ")", ".", "__name__", ",", "kwargs", ".", "get", "(", "'annotators'", ")", ")", ")", "\n", "", "self", ".", "annotators", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.SimpleTokenizer.tokenize": [[172, 194], ["range", "tokenizers.Tokens", "len", "matches[].group", "matches[].span", "data.append", "tokenizers.SimpleTokenizer._regexp.finditer", "len", "matches[].span"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "data", "=", "[", "]", "\n", "matches", "=", "[", "m", "for", "m", "in", "self", ".", "_regexp", ".", "finditer", "(", "text", ")", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "matches", ")", ")", ":", "\n", "# Get text", "\n", "            ", "token", "=", "matches", "[", "i", "]", ".", "group", "(", ")", "\n", "\n", "# Get whitespace", "\n", "span", "=", "matches", "[", "i", "]", ".", "span", "(", ")", "\n", "start_ws", "=", "span", "[", "0", "]", "\n", "if", "i", "+", "1", "<", "len", "(", "matches", ")", ":", "\n", "                ", "end_ws", "=", "matches", "[", "i", "+", "1", "]", ".", "span", "(", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "end_ws", "=", "span", "[", "1", "]", "\n", "\n", "# Format data", "\n", "", "data", ".", "append", "(", "(", "\n", "token", ",", "\n", "text", "[", "start_ws", ":", "end_ws", "]", ",", "\n", "span", ",", "\n", ")", ")", "\n", "", "return", "Tokens", "(", "data", ",", "self", ".", "annotators", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.SpacyTokenizer.__init__": [[198, 212], ["kwargs.get", "copy.deepcopy", "spacy.load", "kwargs.get", "any", "set"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            annotators: set that can include pos, lemma, and ner.\n            model: spaCy model to use (either path, or keyword like 'en').\n        \"\"\"", "\n", "model", "=", "kwargs", ".", "get", "(", "'model'", ",", "'en'", ")", "\n", "self", ".", "annotators", "=", "copy", ".", "deepcopy", "(", "kwargs", ".", "get", "(", "'annotators'", ",", "set", "(", ")", ")", ")", "\n", "nlp_kwargs", "=", "{", "'parser'", ":", "False", "}", "\n", "if", "not", "any", "(", "[", "p", "in", "self", ".", "annotators", "for", "p", "in", "[", "'lemma'", ",", "'pos'", ",", "'ner'", "]", "]", ")", ":", "\n", "            ", "nlp_kwargs", "[", "'tagger'", "]", "=", "False", "\n", "", "if", "'ner'", "not", "in", "self", ".", "annotators", ":", "\n", "            ", "nlp_kwargs", "[", "'entity'", "]", "=", "False", "\n", "", "self", ".", "nlp", "=", "spacy", ".", "load", "(", "model", ",", "**", "nlp_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.SpacyTokenizer.tokenize": [[213, 242], ["text.replace", "tokenizers.SpacyTokenizer.nlp.tokenizer", "any", "range", "tokenizers.Tokens", "tokenizers.SpacyTokenizer.nlp.tagger", "tokenizers.SpacyTokenizer.nlp.entity", "len", "data.append", "len", "len", "len"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "# We don't treat new lines as tokens.", "\n", "        ", "clean_text", "=", "text", ".", "replace", "(", "'\\n'", ",", "' '", ")", "\n", "tokens", "=", "self", ".", "nlp", ".", "tokenizer", "(", "clean_text", ")", "\n", "if", "any", "(", "[", "p", "in", "self", ".", "annotators", "for", "p", "in", "[", "'lemma'", ",", "'pos'", ",", "'ner'", "]", "]", ")", ":", "\n", "            ", "self", ".", "nlp", ".", "tagger", "(", "tokens", ")", "\n", "", "if", "'ner'", "in", "self", ".", "annotators", ":", "\n", "            ", "self", ".", "nlp", ".", "entity", "(", "tokens", ")", "\n", "\n", "", "data", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "tokens", ")", ")", ":", "\n", "# Get whitespace", "\n", "            ", "start_ws", "=", "tokens", "[", "i", "]", ".", "idx", "\n", "if", "i", "+", "1", "<", "len", "(", "tokens", ")", ":", "\n", "                ", "end_ws", "=", "tokens", "[", "i", "+", "1", "]", ".", "idx", "\n", "", "else", ":", "\n", "                ", "end_ws", "=", "tokens", "[", "i", "]", ".", "idx", "+", "len", "(", "tokens", "[", "i", "]", ".", "text", ")", "\n", "\n", "", "data", ".", "append", "(", "(", "\n", "tokens", "[", "i", "]", ".", "text", ",", "\n", "text", "[", "start_ws", ":", "end_ws", "]", ",", "\n", "(", "tokens", "[", "i", "]", ".", "idx", ",", "tokens", "[", "i", "]", ".", "idx", "+", "len", "(", "tokens", "[", "i", "]", ".", "text", ")", ")", ",", "\n", "tokens", "[", "i", "]", ".", "tag_", ",", "\n", "tokens", "[", "i", "]", ".", "lemma_", ",", "\n", "tokens", "[", "i", "]", ".", "ent_type_", ",", "\n", ")", ")", "\n", "\n", "# Set special option for non-entity tag: '' vs 'O' in spaCy", "\n", "", "return", "Tokens", "(", "data", ",", "self", ".", "annotators", ",", "opts", "=", "{", "'non_ent'", ":", "''", "}", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.fairseq_models.RobertaEncoder.__init__": [[45, 48], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseHNSWFlatIndexer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "fairseq_roberta_hub", ":", "RobertaHubInterface", ")", ":", "\n", "        ", "super", "(", "RobertaEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fairseq_roberta", "=", "fairseq_roberta_hub", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.fairseq_models.RobertaEncoder.from_pretrained": [[49, 53], ["fairseq.models.roberta.model.RobertaModel.from_pretrained", "cls"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.fairseq_models.RobertaEncoder.from_pretrained"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_dir_path", ":", "str", ")", ":", "\n", "        ", "model", "=", "FaiseqRobertaModel", ".", "from_pretrained", "(", "pretrained_dir_path", ")", "\n", "return", "cls", "(", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.fairseq_models.RobertaEncoder.forward": [[54, 58], ["fairseq_models.RobertaEncoder.fairseq_roberta.extract_features"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ":", "T", ",", "token_type_ids", ":", "T", ",", "attention_mask", ":", "T", ")", "->", "Tuple", "[", "T", ",", "...", "]", ":", "\n", "        ", "roberta_out", "=", "self", ".", "fairseq_roberta", ".", "extract_features", "(", "input_ids", ")", "\n", "cls_out", "=", "roberta_out", "[", ":", ",", "0", ",", ":", "]", "\n", "return", "roberta_out", ",", "cls_out", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.fairseq_models.RobertaEncoder.get_out_size": [[59, 61], ["None"], "methods", ["None"], ["", "def", "get_out_size", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.fairseq_models.get_roberta_biencoder_components": [[27, 36], ["fairseq_models.RobertaEncoder.from_pretrained", "fairseq_models.RobertaEncoder.from_pretrained", "biencoder.BiEncoder", "dpr.models.hf_models.get_roberta_tensorizer", "fairseq_models.get_fairseq_adamw_optimizer"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.fairseq_models.RobertaEncoder.from_pretrained", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.fairseq_models.RobertaEncoder.from_pretrained", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.get_roberta_tensorizer", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.fairseq_models.get_fairseq_adamw_optimizer"], ["def", "get_roberta_biencoder_components", "(", "args", ",", "inference_only", ":", "bool", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "question_encoder", "=", "RobertaEncoder", ".", "from_pretrained", "(", "args", ".", "pretrained_file", ")", "\n", "ctx_encoder", "=", "RobertaEncoder", ".", "from_pretrained", "(", "args", ".", "pretrained_file", ")", "\n", "biencoder", "=", "BiEncoder", "(", "question_encoder", ",", "ctx_encoder", ")", "\n", "optimizer", "=", "get_fairseq_adamw_optimizer", "(", "biencoder", ",", "args", ")", "if", "not", "inference_only", "else", "None", "\n", "\n", "tensorizer", "=", "get_roberta_tensorizer", "(", "args", ")", "\n", "\n", "return", "tensorizer", ",", "biencoder", ",", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.fairseq_models.get_fairseq_adamw_optimizer": [[38, 41], ["setattr", "fairseq.optim.adam.FairseqAdam", "model.parameters"], "function", ["None"], ["", "def", "get_fairseq_adamw_optimizer", "(", "model", ":", "nn", ".", "Module", ",", "args", ")", ":", "\n", "    ", "setattr", "(", "args", ",", "'lr'", ",", "[", "args", ".", "learning_rate", "]", ")", "\n", "return", "FairseqAdam", "(", "args", ",", "model", ".", "parameters", "(", ")", ")", ".", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.biencoder.BiEncoder.__init__": [[54, 61], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseHNSWFlatIndexer.__init__"], ["def", "__init__", "(", "self", ",", "question_model", ":", "nn", ".", "Module", ",", "ctx_model", ":", "nn", ".", "Module", ",", "fix_q_encoder", ":", "bool", "=", "False", ",", "\n", "fix_ctx_encoder", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "BiEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "question_model", "=", "question_model", "\n", "self", ".", "ctx_model", "=", "ctx_model", "\n", "self", ".", "fix_q_encoder", "=", "fix_q_encoder", "\n", "self", ".", "fix_ctx_encoder", "=", "fix_ctx_encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.biencoder.BiEncoder.get_representation": [[62, 80], ["sub_model", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "sub_model", "sequence_output.requires_grad_", "pooled_output.requires_grad_"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_representation", "(", "sub_model", ":", "nn", ".", "Module", ",", "ids", ":", "T", ",", "segments", ":", "T", ",", "attn_mask", ":", "T", ",", "fix_encoder", ":", "bool", "=", "False", ")", "->", "(", "\n", "T", ",", "T", ",", "T", ")", ":", "\n", "        ", "sequence_output", "=", "None", "\n", "pooled_output", "=", "None", "\n", "hidden_states", "=", "None", "\n", "if", "ids", "is", "not", "None", ":", "\n", "            ", "if", "fix_encoder", ":", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "sequence_output", ",", "pooled_output", ",", "hidden_states", "=", "sub_model", "(", "ids", ",", "segments", ",", "attn_mask", ")", "\n", "\n", "", "if", "sub_model", ".", "training", ":", "\n", "                    ", "sequence_output", ".", "requires_grad_", "(", "requires_grad", "=", "True", ")", "\n", "pooled_output", ".", "requires_grad_", "(", "requires_grad", "=", "True", ")", "\n", "", "", "else", ":", "\n", "                ", "sequence_output", ",", "pooled_output", ",", "hidden_states", "=", "sub_model", "(", "ids", ",", "segments", ",", "attn_mask", ")", "\n", "\n", "", "", "return", "sequence_output", ",", "pooled_output", ",", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.biencoder.BiEncoder.forward": [[81, 90], ["biencoder.BiEncoder.get_representation", "biencoder.BiEncoder.get_representation"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.biencoder.BiEncoder.get_representation", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.biencoder.BiEncoder.get_representation"], ["", "def", "forward", "(", "self", ",", "question_ids", ":", "T", ",", "question_segments", ":", "T", ",", "question_attn_mask", ":", "T", ",", "context_ids", ":", "T", ",", "ctx_segments", ":", "T", ",", "\n", "ctx_attn_mask", ":", "T", ")", "->", "Tuple", "[", "T", ",", "T", "]", ":", "\n", "\n", "        ", "_q_seq", ",", "q_pooled_out", ",", "_q_hidden", "=", "self", ".", "get_representation", "(", "self", ".", "question_model", ",", "question_ids", ",", "question_segments", ",", "\n", "question_attn_mask", ",", "self", ".", "fix_q_encoder", ")", "\n", "_ctx_seq", ",", "ctx_pooled_out", ",", "_ctx_hidden", "=", "self", ".", "get_representation", "(", "self", ".", "ctx_model", ",", "context_ids", ",", "ctx_segments", ",", "\n", "ctx_attn_mask", ",", "self", ".", "fix_ctx_encoder", ")", "\n", "\n", "return", "q_pooled_out", ",", "ctx_pooled_out", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.biencoder.BiEncoder.create_biencoder_input": [[91, 163], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "BiEncoderBatch", "dpr.utils.data_utils.normalize_question", "len", "ctx_tensors.extend", "positive_ctx_indices.append", "hard_neg_ctx_indices.append", "question_tensors.append", "random.shuffle", "random.shuffle", "len", "tensorizer.text_to_tensor", "tensorizer.text_to_tensor", "ctx.view", "q.view", "numpy.random.choice", "range", "len"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.data_utils.normalize_question", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.text_to_tensor", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.text_to_tensor"], ["", "@", "classmethod", "\n", "def", "create_biencoder_input", "(", "cls", ",", "\n", "samples", ":", "List", ",", "\n", "tensorizer", ":", "Tensorizer", ",", "\n", "insert_title", ":", "bool", ",", "\n", "num_hard_negatives", ":", "int", "=", "0", ",", "\n", "num_other_negatives", ":", "int", "=", "0", ",", "\n", "shuffle", ":", "bool", "=", "True", ",", "\n", "shuffle_positives", ":", "bool", "=", "False", ",", "\n", ")", "->", "BiEncoderBatch", ":", "\n", "        ", "\"\"\"\n        Creates a batch of the biencoder training tuple.\n        :param samples: list of data items (from json) to create the batch for\n        :param tensorizer: components to create model input tensors from a text sequence\n        :param insert_title: enables title insertion at the beginning of the context sequences\n        :param num_hard_negatives: amount of hard negatives per question (taken from samples' pools)\n        :param num_other_negatives: amount of other negatives per question (taken from samples' pools)\n        :param shuffle: shuffles negative passages pools\n        :param shuffle_positives: shuffles positive passages pools\n        :return: BiEncoderBatch tuple\n        \"\"\"", "\n", "question_tensors", "=", "[", "]", "\n", "ctx_tensors", "=", "[", "]", "\n", "positive_ctx_indices", "=", "[", "]", "\n", "hard_neg_ctx_indices", "=", "[", "]", "\n", "\n", "for", "sample", "in", "samples", ":", "\n", "# ctx+ & [ctx-] composition", "\n", "# as of now, take the first(gold) ctx+ only", "\n", "            ", "if", "shuffle", "and", "shuffle_positives", ":", "\n", "                ", "positive_ctxs", "=", "sample", "[", "'positive_ctxs'", "]", "\n", "positive_ctx", "=", "positive_ctxs", "[", "np", ".", "random", ".", "choice", "(", "len", "(", "positive_ctxs", ")", ")", "]", "\n", "", "else", ":", "\n", "                ", "positive_ctx", "=", "sample", "[", "'positive_ctxs'", "]", "[", "0", "]", "\n", "\n", "", "neg_ctxs", "=", "sample", "[", "'negative_ctxs'", "]", "\n", "hard_neg_ctxs", "=", "sample", "[", "'hard_negative_ctxs'", "]", "\n", "question", "=", "normalize_question", "(", "sample", "[", "'question'", "]", ")", "\n", "\n", "if", "shuffle", ":", "\n", "                ", "random", ".", "shuffle", "(", "neg_ctxs", ")", "\n", "random", ".", "shuffle", "(", "hard_neg_ctxs", ")", "\n", "\n", "", "neg_ctxs", "=", "neg_ctxs", "[", "0", ":", "num_other_negatives", "]", "\n", "hard_neg_ctxs", "=", "hard_neg_ctxs", "[", "0", ":", "num_hard_negatives", "]", "\n", "\n", "all_ctxs", "=", "[", "positive_ctx", "]", "+", "neg_ctxs", "+", "hard_neg_ctxs", "\n", "hard_negatives_start_idx", "=", "1", "\n", "hard_negatives_end_idx", "=", "1", "+", "len", "(", "hard_neg_ctxs", ")", "\n", "\n", "current_ctxs_len", "=", "len", "(", "ctx_tensors", ")", "\n", "\n", "sample_ctxs_tensors", "=", "[", "tensorizer", ".", "text_to_tensor", "(", "ctx", "[", "'text'", "]", ",", "title", "=", "ctx", "[", "'title'", "]", "if", "insert_title", "else", "None", ")", "\n", "for", "\n", "ctx", "in", "all_ctxs", "]", "\n", "\n", "ctx_tensors", ".", "extend", "(", "sample_ctxs_tensors", ")", "\n", "positive_ctx_indices", ".", "append", "(", "current_ctxs_len", ")", "\n", "hard_neg_ctx_indices", ".", "append", "(", "\n", "[", "i", "for", "i", "in", "\n", "range", "(", "current_ctxs_len", "+", "hard_negatives_start_idx", ",", "current_ctxs_len", "+", "hard_negatives_end_idx", ")", "]", ")", "\n", "\n", "question_tensors", ".", "append", "(", "tensorizer", ".", "text_to_tensor", "(", "question", ")", ")", "\n", "\n", "", "ctxs_tensor", "=", "torch", ".", "cat", "(", "[", "ctx", ".", "view", "(", "1", ",", "-", "1", ")", "for", "ctx", "in", "ctx_tensors", "]", ",", "dim", "=", "0", ")", "\n", "questions_tensor", "=", "torch", ".", "cat", "(", "[", "q", ".", "view", "(", "1", ",", "-", "1", ")", "for", "q", "in", "question_tensors", "]", ",", "dim", "=", "0", ")", "\n", "\n", "ctx_segments", "=", "torch", ".", "zeros_like", "(", "ctxs_tensor", ")", "\n", "question_segments", "=", "torch", ".", "zeros_like", "(", "questions_tensor", ")", "\n", "\n", "return", "BiEncoderBatch", "(", "questions_tensor", ",", "question_segments", ",", "ctxs_tensor", ",", "ctx_segments", ",", "positive_ctx_indices", ",", "\n", "hard_neg_ctx_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.biencoder.BiEncoderNllLoss.calc": [[167, 189], ["biencoder.BiEncoderNllLoss.get_scores", "torch.log_softmax", "torch.log_softmax", "torch.nll_loss", "torch.nll_loss", "torch.max", "torch.max", "torch.max", "torch.max", "len", "q_vectors.size", "scores.view.view.view", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "q_vectors.size", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.biencoder.BiEncoderNllLoss.get_scores"], ["    ", "def", "calc", "(", "self", ",", "q_vectors", ":", "T", ",", "ctx_vectors", ":", "T", ",", "positive_idx_per_question", ":", "list", ",", "\n", "hard_negatice_idx_per_question", ":", "list", "=", "None", ")", "->", "Tuple", "[", "T", ",", "int", "]", ":", "\n", "        ", "\"\"\"\n        Computes nll loss for the given lists of question and ctx vectors.\n        Note that although hard_negatice_idx_per_question in not currently in use, one can use it for the\n        loss modifications. For example - weighted NLL with different factors for hard vs regular negatives.\n        :return: a tuple of loss value and amount of correct predictions per batch\n        \"\"\"", "\n", "scores", "=", "self", ".", "get_scores", "(", "q_vectors", ",", "ctx_vectors", ")", "\n", "\n", "if", "len", "(", "q_vectors", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "            ", "q_num", "=", "q_vectors", ".", "size", "(", "0", ")", "\n", "scores", "=", "scores", ".", "view", "(", "q_num", ",", "-", "1", ")", "\n", "\n", "", "softmax_scores", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "1", ")", "\n", "\n", "loss", "=", "F", ".", "nll_loss", "(", "softmax_scores", ",", "torch", ".", "tensor", "(", "positive_idx_per_question", ")", ".", "to", "(", "softmax_scores", ".", "device", ")", ",", "\n", "reduction", "=", "'mean'", ")", "\n", "\n", "max_score", ",", "max_idxs", "=", "torch", ".", "max", "(", "softmax_scores", ",", "1", ")", "\n", "correct_predictions_count", "=", "(", "max_idxs", "==", "torch", ".", "tensor", "(", "positive_idx_per_question", ")", ".", "to", "(", "max_idxs", ".", "device", ")", ")", ".", "sum", "(", ")", "\n", "return", "loss", ",", "correct_predictions_count", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.biencoder.BiEncoderNllLoss.get_scores": [[190, 194], ["biencoder.BiEncoderNllLoss.get_similarity_function", "biencoder.BiEncoderNllLoss.get_similarity_function"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.biencoder.BiEncoderNllLoss.get_similarity_function", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.biencoder.BiEncoderNllLoss.get_similarity_function"], ["", "@", "staticmethod", "\n", "def", "get_scores", "(", "q_vector", ":", "T", ",", "ctx_vectors", ":", "T", ")", "->", "T", ":", "\n", "        ", "f", "=", "BiEncoderNllLoss", ".", "get_similarity_function", "(", ")", "\n", "return", "f", "(", "q_vector", ",", "ctx_vectors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.biencoder.BiEncoderNllLoss.get_similarity_function": [[195, 198], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_similarity_function", "(", ")", ":", "\n", "        ", "return", "dot_product_scores", "\n", "", "", ""]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.biencoder.dot_product_scores": [[33, 43], ["torch.matmul", "torch.matmul", "torch.transpose", "torch.transpose"], "function", ["None"], ["def", "dot_product_scores", "(", "q_vectors", ":", "T", ",", "ctx_vectors", ":", "T", ")", "->", "T", ":", "\n", "    ", "\"\"\"\n    calculates q->ctx scores for every row in ctx_vector\n    :param q_vector:\n    :param ctx_vector:\n    :return:\n    \"\"\"", "\n", "# q_vector: n1 x D, ctx_vectors: n2 x D, result n1 x n2", "\n", "r", "=", "torch", ".", "matmul", "(", "q_vectors", ",", "torch", ".", "transpose", "(", "ctx_vectors", ",", "0", ",", "1", ")", ")", "\n", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.biencoder.cosine_scores": [[45, 48], ["torch.cosine_similarity"], "function", ["None"], ["", "def", "cosine_scores", "(", "q_vector", ":", "T", ",", "ctx_vectors", ":", "T", ")", ":", "\n", "# q_vector: n1 x D, ctx_vectors: n2 x D, result n1 x n2", "\n", "    ", "return", "F", ".", "cosine_similarity", "(", "q_vector", ",", "ctx_vectors", ",", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.HFBertEncoder.__init__": [[104, 109], ["transformers.modeling_bert.BertModel.__init__", "hf_models.HFBertEncoder.init_weights", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseHNSWFlatIndexer.__init__", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "project_dim", ":", "int", "=", "0", ")", ":", "\n", "        ", "BertModel", ".", "__init__", "(", "self", ",", "config", ")", "\n", "assert", "config", ".", "hidden_size", ">", "0", ",", "'Encoder hidden_size can\\'t be zero'", "\n", "self", ".", "encode_proj", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "project_dim", ")", "if", "project_dim", "!=", "0", "else", "None", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.HFBertEncoder.init_encoder": [[110, 117], ["transformers.modeling_bert.BertConfig.from_pretrained", "cls.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.fairseq_models.RobertaEncoder.from_pretrained", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.fairseq_models.RobertaEncoder.from_pretrained"], ["", "@", "classmethod", "\n", "def", "init_encoder", "(", "cls", ",", "cfg_name", ":", "str", ",", "projection_dim", ":", "int", "=", "0", ",", "dropout", ":", "float", "=", "0.1", ",", "**", "kwargs", ")", "->", "BertModel", ":", "\n", "        ", "cfg", "=", "BertConfig", ".", "from_pretrained", "(", "cfg_name", "if", "cfg_name", "else", "'bert-base-uncased'", ")", "\n", "if", "dropout", "!=", "0", ":", "\n", "            ", "cfg", ".", "attention_probs_dropout_prob", "=", "dropout", "\n", "cfg", ".", "hidden_dropout_prob", "=", "dropout", "\n", "", "return", "cls", ".", "from_pretrained", "(", "cfg_name", ",", "config", "=", "cfg", ",", "project_dim", "=", "projection_dim", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.HFBertEncoder.forward": [[118, 132], ["super().forward", "super().forward", "hf_models.HFBertEncoder.encode_proj"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader.ReaderYesNo.forward", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader.ReaderYesNo.forward"], ["", "def", "forward", "(", "self", ",", "input_ids", ":", "T", ",", "token_type_ids", ":", "T", ",", "attention_mask", ":", "T", ")", "->", "Tuple", "[", "T", ",", "...", "]", ":", "\n", "        ", "if", "self", ".", "config", ".", "output_hidden_states", ":", "\n", "            ", "sequence_output", ",", "pooled_output", ",", "hidden_states", "=", "super", "(", ")", ".", "forward", "(", "input_ids", "=", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ")", "\n", "", "else", ":", "\n", "            ", "hidden_states", "=", "None", "\n", "sequence_output", ",", "pooled_output", "=", "super", "(", ")", ".", "forward", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ")", "\n", "\n", "", "pooled_output", "=", "sequence_output", "[", ":", ",", "0", ",", ":", "]", "\n", "if", "self", ".", "encode_proj", ":", "\n", "            ", "pooled_output", "=", "self", ".", "encode_proj", "(", "pooled_output", ")", "\n", "", "return", "sequence_output", ",", "pooled_output", ",", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.HFBertEncoder.get_out_size": [[133, 137], ["None"], "methods", ["None"], ["", "def", "get_out_size", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "encode_proj", ":", "\n", "            ", "return", "self", ".", "encode_proj", ".", "out_features", "\n", "", "return", "self", ".", "config", ".", "hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.__init__": [[140, 144], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "tokenizer", ":", "BertTokenizer", ",", "max_length", ":", "int", ",", "pad_to_max", ":", "bool", "=", "True", ")", ":", "\n", "        ", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "pad_to_max", "=", "pad_to_max", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.text_to_tensor": [[145, 168], ["text.strip.strip.strip", "torch.tensor", "isinstance", "hf_models.BertTensorizer.tokenizer.encode", "hf_models.BertTensorizer.tokenizer.encode", "len", "len", "len", "len"], "methods", ["None"], ["", "def", "text_to_tensor", "(", "self", ",", "text", ":", "str", ",", "title", ":", "str", "=", "None", ",", "add_special_tokens", ":", "bool", "=", "True", ")", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "list", ")", "and", "len", "(", "text", ")", "==", "1", ":", "\n", "            ", "text", "=", "text", "[", "0", "]", "\n", "", "text", "=", "text", ".", "strip", "(", ")", "\n", "\n", "# tokenizer automatic padding is explicitly disabled since its inconsistent behavior", "\n", "# FIXME: temporary enabling the tokenizer's truncation.", "\n", "if", "title", ":", "\n", "            ", "token_ids", "=", "self", ".", "tokenizer", ".", "encode", "(", "title", ",", "text_pair", "=", "text", ",", "add_special_tokens", "=", "add_special_tokens", ",", "\n", "max_length", "=", "self", ".", "max_length", ",", "\n", "pad_to_max_length", "=", "False", ",", "truncation", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "token_ids", "=", "self", ".", "tokenizer", ".", "encode", "(", "text", ",", "add_special_tokens", "=", "add_special_tokens", ",", "max_length", "=", "self", ".", "max_length", ",", "\n", "pad_to_max_length", "=", "False", ",", "truncation", "=", "True", ")", "\n", "\n", "", "seq_len", "=", "self", ".", "max_length", "\n", "if", "self", ".", "pad_to_max", "and", "len", "(", "token_ids", ")", "<", "seq_len", ":", "\n", "            ", "token_ids", "=", "token_ids", "+", "[", "self", ".", "tokenizer", ".", "pad_token_id", "]", "*", "(", "seq_len", "-", "len", "(", "token_ids", ")", ")", "\n", "", "if", "len", "(", "token_ids", ")", ">", "seq_len", ":", "\n", "            ", "token_ids", "=", "token_ids", "[", "0", ":", "seq_len", "]", "\n", "token_ids", "[", "-", "1", "]", "=", "self", ".", "tokenizer", ".", "sep_token_id", "\n", "\n", "", "return", "torch", ".", "tensor", "(", "token_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.get_pair_separator_ids": [[169, 171], ["torch.tensor"], "methods", ["None"], ["", "def", "get_pair_separator_ids", "(", "self", ")", "->", "T", ":", "\n", "        ", "return", "torch", ".", "tensor", "(", "[", "self", ".", "tokenizer", ".", "sep_token_id", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.get_pad_id": [[172, 174], ["None"], "methods", ["None"], ["", "def", "get_pad_id", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "tokenizer", ".", "pad_token_type_id", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.get_attn_mask": [[175, 177], ["hf_models.BertTensorizer.get_pad_id"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.get_pad_id"], ["", "def", "get_attn_mask", "(", "self", ",", "tokens_tensor", ":", "T", ")", "->", "T", ":", "\n", "        ", "return", "tokens_tensor", "!=", "self", ".", "get_pad_id", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.is_sub_word_id": [[178, 181], ["hf_models.BertTensorizer.tokenizer.convert_ids_to_tokens", "token.startswith", "token.startswith"], "methods", ["None"], ["", "def", "is_sub_word_id", "(", "self", ",", "token_id", ":", "int", ")", ":", "\n", "        ", "token", "=", "self", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "[", "token_id", "]", ")", "[", "0", "]", "\n", "return", "token", ".", "startswith", "(", "\"##\"", ")", "or", "token", ".", "startswith", "(", "\" ##\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.to_string": [[182, 184], ["hf_models.BertTensorizer.tokenizer.decode"], "methods", ["None"], ["", "def", "to_string", "(", "self", ",", "token_ids", ",", "skip_special_tokens", "=", "True", ")", ":", "\n", "        ", "return", "self", ".", "tokenizer", ".", "decode", "(", "token_ids", ",", "skip_special_tokens", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.set_pad_to_max": [[185, 187], ["None"], "methods", ["None"], ["", "def", "set_pad_to_max", "(", "self", ",", "do_pad", ":", "bool", ")", ":", "\n", "        ", "self", ".", "pad_to_max", "=", "do_pad", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.RobertaTensorizer.__init__": [[190, 192], ["hf_models.BertTensorizer.__init__"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseHNSWFlatIndexer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "tokenizer", ",", "max_length", ":", "int", ",", "pad_to_max", ":", "bool", "=", "True", ")", ":", "\n", "        ", "super", "(", "RobertaTensorizer", ",", "self", ")", ".", "__init__", "(", "tokenizer", ",", "max_length", ",", "pad_to_max", "=", "pad_to_max", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.get_bert_biencoder_components": [[31, 49], ["hf_models.HFBertEncoder.init_encoder", "hf_models.HFBertEncoder.init_encoder", "biencoder.BiEncoder", "hf_models.get_bert_tensorizer", "hasattr", "hasattr", "hf_models.get_optimizer"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.pytext_models.PytextBertEncoder.init_encoder", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.pytext_models.PytextBertEncoder.init_encoder", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.get_bert_tensorizer", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.pytext_models.get_optimizer"], ["def", "get_bert_biencoder_components", "(", "args", ",", "inference_only", ":", "bool", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "dropout", "=", "args", ".", "dropout", "if", "hasattr", "(", "args", ",", "'dropout'", ")", "else", "0.0", "\n", "question_encoder", "=", "HFBertEncoder", ".", "init_encoder", "(", "args", ".", "pretrained_model_cfg", ",", "\n", "projection_dim", "=", "args", ".", "projection_dim", ",", "dropout", "=", "dropout", ",", "**", "kwargs", ")", "\n", "ctx_encoder", "=", "HFBertEncoder", ".", "init_encoder", "(", "args", ".", "pretrained_model_cfg", ",", "\n", "projection_dim", "=", "args", ".", "projection_dim", ",", "dropout", "=", "dropout", ",", "**", "kwargs", ")", "\n", "\n", "fix_ctx_encoder", "=", "args", ".", "fix_ctx_encoder", "if", "hasattr", "(", "args", ",", "'fix_ctx_encoder'", ")", "else", "False", "\n", "biencoder", "=", "BiEncoder", "(", "question_encoder", ",", "ctx_encoder", ",", "fix_ctx_encoder", "=", "fix_ctx_encoder", ")", "\n", "\n", "optimizer", "=", "get_optimizer", "(", "biencoder", ",", "\n", "learning_rate", "=", "args", ".", "learning_rate", ",", "\n", "adam_eps", "=", "args", ".", "adam_eps", ",", "weight_decay", "=", "args", ".", "weight_decay", ",", "\n", ")", "if", "not", "inference_only", "else", "None", "\n", "\n", "tensorizer", "=", "get_bert_tensorizer", "(", "args", ")", "\n", "\n", "return", "tensorizer", ",", "biencoder", ",", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.get_bert_reader_components": [[51, 66], ["hf_models.HFBertEncoder.init_encoder", "reader.Reader", "hf_models.get_bert_tensorizer", "hasattr", "hf_models.get_optimizer"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.pytext_models.PytextBertEncoder.init_encoder", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.get_bert_tensorizer", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.pytext_models.get_optimizer"], ["", "def", "get_bert_reader_components", "(", "args", ",", "inference_only", ":", "bool", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "dropout", "=", "args", ".", "dropout", "if", "hasattr", "(", "args", ",", "'dropout'", ")", "else", "0.0", "\n", "encoder", "=", "HFBertEncoder", ".", "init_encoder", "(", "args", ".", "pretrained_model_cfg", ",", "\n", "projection_dim", "=", "args", ".", "projection_dim", ",", "dropout", "=", "dropout", ")", "\n", "\n", "hidden_size", "=", "encoder", ".", "config", ".", "hidden_size", "\n", "reader", "=", "Reader", "(", "encoder", ",", "hidden_size", ")", "\n", "\n", "optimizer", "=", "get_optimizer", "(", "reader", ",", "\n", "learning_rate", "=", "args", ".", "learning_rate", ",", "\n", "adam_eps", "=", "args", ".", "adam_eps", ",", "weight_decay", "=", "args", ".", "weight_decay", ",", "\n", ")", "if", "not", "inference_only", "else", "None", "\n", "\n", "tensorizer", "=", "get_bert_tensorizer", "(", "args", ")", "\n", "return", "tensorizer", ",", "reader", ",", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.get_bert_tensorizer": [[68, 72], ["hf_models.BertTensorizer", "hf_models.get_bert_tokenizer"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.get_bert_tokenizer"], ["", "def", "get_bert_tensorizer", "(", "args", ",", "tokenizer", "=", "None", ")", ":", "\n", "    ", "if", "not", "tokenizer", ":", "\n", "        ", "tokenizer", "=", "get_bert_tokenizer", "(", "args", ".", "pretrained_model_cfg", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "", "return", "BertTensorizer", "(", "tokenizer", ",", "args", ".", "sequence_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.get_roberta_tensorizer": [[74, 78], ["hf_models.RobertaTensorizer", "hf_models.get_roberta_tokenizer"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.get_roberta_tokenizer"], ["", "def", "get_roberta_tensorizer", "(", "args", ",", "tokenizer", "=", "None", ")", ":", "\n", "    ", "if", "not", "tokenizer", ":", "\n", "        ", "tokenizer", "=", "get_roberta_tokenizer", "(", "args", ".", "pretrained_model_cfg", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "", "return", "RobertaTensorizer", "(", "tokenizer", ",", "args", ".", "sequence_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.get_optimizer": [[80, 91], ["transformers.optimization.AdamW", "model.named_parameters", "model.named_parameters", "any", "any"], "function", ["None"], ["", "def", "get_optimizer", "(", "model", ":", "nn", ".", "Module", ",", "learning_rate", ":", "float", "=", "1e-5", ",", "adam_eps", ":", "float", "=", "1e-8", ",", "\n", "weight_decay", ":", "float", "=", "0.0", ",", ")", "->", "torch", ".", "optim", ".", "Optimizer", ":", "\n", "    ", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.weight'", "]", "\n", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "'weight_decay'", ":", "weight_decay", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "optimizer", "=", "AdamW", "(", "optimizer_grouped_parameters", ",", "lr", "=", "learning_rate", ",", "eps", "=", "adam_eps", ")", "\n", "return", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.get_bert_tokenizer": [[93, 95], ["transformers.tokenization_bert.BertTokenizer.from_pretrained"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.fairseq_models.RobertaEncoder.from_pretrained"], ["", "def", "get_bert_tokenizer", "(", "pretrained_cfg_name", ":", "str", ",", "do_lower_case", ":", "bool", "=", "True", ")", ":", "\n", "    ", "return", "BertTokenizer", ".", "from_pretrained", "(", "pretrained_cfg_name", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.get_roberta_tokenizer": [[97, 100], ["transformers.tokenization_roberta.RobertaTokenizer.from_pretrained"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.fairseq_models.RobertaEncoder.from_pretrained"], ["", "def", "get_roberta_tokenizer", "(", "pretrained_cfg_name", ":", "str", ",", "do_lower_case", ":", "bool", "=", "True", ")", ":", "\n", "# still uses HF code for tokenizer since they are the same", "\n", "    ", "return", "RobertaTokenizer", ".", "from_pretrained", "(", "pretrained_cfg_name", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.pytext_models.PytextBertEncoder.__init__": [[88, 100], ["pytext.models.representations.transformer_sentence_encoder.TransformerSentenceEncoder.__init__", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseHNSWFlatIndexer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ":", "TransformerSentenceEncoder", ".", "Config", ",", "\n", "padding_idx", ":", "int", ",", "\n", "vocab_size", ":", "int", ",", "\n", "projection_dim", ":", "int", "=", "0", ",", "\n", "*", "args", ",", "\n", "**", "kwarg", "\n", ")", ":", "\n", "\n", "        ", "TransformerSentenceEncoder", ".", "__init__", "(", "self", ",", "config", ",", "False", ",", "padding_idx", ",", "vocab_size", ",", "*", "args", ",", "**", "kwarg", ")", "\n", "\n", "assert", "config", ".", "embedding_dim", ">", "0", ",", "'Encoder hidden_size can\\'t be zero'", "\n", "self", ".", "encode_proj", "=", "nn", ".", "Linear", "(", "config", ".", "embedding_dim", ",", "projection_dim", ")", "if", "projection_dim", "!=", "0", "else", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.pytext_models.PytextBertEncoder.init_encoder": [[101, 119], ["pytext_models.get_pytext_bert_base_cfg", "cls", "logger.info", "torch.load", "cls.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.pytext_models.get_pytext_bert_base_cfg"], ["", "@", "classmethod", "\n", "def", "init_encoder", "(", "cls", ",", "pretrained_file", ":", "str", "=", "None", ",", "projection_dim", ":", "int", "=", "0", ",", "dropout", ":", "float", "=", "0.1", ",", "\n", "vocab_size", ":", "int", "=", "0", ",", "\n", "padding_idx", ":", "int", "=", "0", ",", "**", "kwargs", ")", ":", "\n", "        ", "cfg", "=", "get_pytext_bert_base_cfg", "(", ")", "\n", "\n", "if", "dropout", "!=", "0", ":", "\n", "            ", "cfg", ".", "dropout", "=", "dropout", "\n", "cfg", ".", "attention_dropout", "=", "dropout", "\n", "cfg", ".", "activation_dropout", "=", "dropout", "\n", "\n", "", "encoder", "=", "cls", "(", "cfg", ",", "padding_idx", ",", "vocab_size", ",", "projection_dim", ",", "**", "kwargs", ")", "\n", "\n", "if", "pretrained_file", ":", "\n", "            ", "logger", ".", "info", "(", "'Loading pre-trained pytext encoder state from %s'", ",", "pretrained_file", ")", "\n", "state", "=", "torch", ".", "load", "(", "pretrained_file", ")", "\n", "encoder", ".", "load_state_dict", "(", "state", ")", "\n", "", "return", "encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.pytext_models.PytextBertEncoder.forward": [[120, 126], ["super().forward", "pytext_models.PytextBertEncoder.encode_proj"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader.ReaderYesNo.forward"], ["", "def", "forward", "(", "self", ",", "input_ids", ":", "T", ",", "token_type_ids", ":", "T", ",", "attention_mask", ":", "T", ")", "->", "Tuple", "[", "T", ",", "...", "]", ":", "\n", "        ", "pooled_output", "=", "super", "(", ")", ".", "forward", "(", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "None", ")", ")", "[", "0", "]", "\n", "if", "self", ".", "encode_proj", ":", "\n", "            ", "pooled_output", "=", "self", ".", "encode_proj", "(", "pooled_output", ")", "\n", "\n", "", "return", "None", ",", "pooled_output", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.pytext_models.PytextBertEncoder.get_out_size": [[127, 131], ["None"], "methods", ["None"], ["", "def", "get_out_size", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "encode_proj", ":", "\n", "            ", "return", "self", ".", "encode_proj", ".", "out_features", "\n", "", "return", "self", ".", "representation_dim", "\n", "", "", ""]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.pytext_models.get_bert_biencoder_components": [[26, 53], ["get_tokenizer", "pytext_models.PytextBertEncoder.init_encoder", "pytext_models.PytextBertEncoder.init_encoder", "biencoder.BiEncoder", "BertTensorizer", "pytext_models.get_optimizer"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.pytext_models.PytextBertEncoder.init_encoder", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.pytext_models.PytextBertEncoder.init_encoder", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.pytext_models.get_optimizer"], ["def", "get_bert_biencoder_components", "(", "args", ",", "inference_only", ":", "bool", "=", "False", ")", ":", "\n", "# since bert tokenizer is the same in HF and pytext/fairseq, just use HF's implementation here for now", "\n", "    ", "from", ".", "hf_models", "import", "get_tokenizer", ",", "BertTensorizer", "\n", "\n", "tokenizer", "=", "get_tokenizer", "(", "args", ".", "pretrained_model_cfg", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "question_encoder", "=", "PytextBertEncoder", ".", "init_encoder", "(", "args", ".", "pretrained_file", ",", "\n", "projection_dim", "=", "args", ".", "projection_dim", ",", "dropout", "=", "args", ".", "dropout", ",", "\n", "vocab_size", "=", "tokenizer", ".", "vocab_size", ",", "\n", "padding_idx", "=", "tokenizer", ".", "pad_token_type_id", "\n", ")", "\n", "\n", "ctx_encoder", "=", "PytextBertEncoder", ".", "init_encoder", "(", "args", ".", "pretrained_file", ",", "\n", "projection_dim", "=", "args", ".", "projection_dim", ",", "dropout", "=", "args", ".", "dropout", ",", "\n", "vocab_size", "=", "tokenizer", ".", "vocab_size", ",", "\n", "padding_idx", "=", "tokenizer", ".", "pad_token_type_id", "\n", ")", "\n", "\n", "biencoder", "=", "BiEncoder", "(", "question_encoder", ",", "ctx_encoder", ")", "\n", "\n", "optimizer", "=", "get_optimizer", "(", "biencoder", ",", "\n", "learning_rate", "=", "args", ".", "learning_rate", ",", "\n", "adam_eps", "=", "args", ".", "adam_eps", ",", "weight_decay", "=", "args", ".", "weight_decay", ",", "\n", ")", "if", "not", "inference_only", "else", "None", "\n", "\n", "tensorizer", "=", "BertTensorizer", "(", "tokenizer", ",", "args", ".", "sequence_length", ")", "\n", "return", "tensorizer", ",", "biencoder", ",", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.pytext_models.get_optimizer": [[55, 63], ["pytext.optimizer.optimizers.AdamW.Config", "pytext.optimizer.optimizers.AdamW.from_config"], "function", ["None"], ["", "def", "get_optimizer", "(", "model", ":", "nn", ".", "Module", ",", "learning_rate", ":", "float", "=", "1e-5", ",", "adam_eps", ":", "float", "=", "1e-8", ",", "\n", "weight_decay", ":", "float", "=", "0.0", ")", "->", "torch", ".", "optim", ".", "Optimizer", ":", "\n", "    ", "cfg", "=", "AdamW", ".", "Config", "(", ")", "\n", "cfg", ".", "lr", "=", "learning_rate", "\n", "cfg", ".", "weight_decay", "=", "weight_decay", "\n", "cfg", ".", "eps", "=", "adam_eps", "\n", "optimizer", "=", "AdamW", ".", "from_config", "(", "cfg", ",", "model", ")", "\n", "return", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.pytext_models.get_pytext_bert_base_cfg": [[65, 84], ["pytext.models.representations.transformer_sentence_encoder.TransformerSentenceEncoder.Config"], "function", ["None"], ["", "def", "get_pytext_bert_base_cfg", "(", ")", ":", "\n", "    ", "cfg", "=", "TransformerSentenceEncoder", ".", "Config", "(", ")", "\n", "cfg", ".", "embedding_dim", "=", "768", "\n", "cfg", ".", "ffn_embedding_dim", "=", "3072", "\n", "cfg", ".", "num_encoder_layers", "=", "12", "\n", "cfg", ".", "num_attention_heads", "=", "12", "\n", "cfg", ".", "num_segments", "=", "2", "\n", "cfg", ".", "use_position_embeddings", "=", "True", "\n", "cfg", ".", "offset_positions_by_padding", "=", "True", "\n", "cfg", ".", "apply_bert_init", "=", "True", "\n", "cfg", ".", "encoder_normalize_before", "=", "True", "\n", "cfg", ".", "activation_fn", "=", "\"gelu\"", "\n", "cfg", ".", "projection_dim", "=", "0", "\n", "cfg", ".", "max_seq_len", "=", "512", "\n", "cfg", ".", "multilingual", "=", "False", "\n", "cfg", ".", "freeze_embeddings", "=", "False", "\n", "cfg", ".", "n_trans_layers_to_freeze", "=", "0", "\n", "cfg", ".", "use_torchscript", "=", "False", "\n", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.__init__.init_hf_bert_biencoder": [[15, 20], ["get_bert_biencoder_components", "importlib.util.find_spec", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.pytext_models.get_bert_biencoder_components"], []], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.__init__.init_hf_bert_reader": [[22, 27], ["get_bert_reader_components", "importlib.util.find_spec", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.get_bert_reader_components"], []], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.__init__.init_pytext_bert_biencoder": [[29, 34], ["get_bert_biencoder_components", "importlib.util.find_spec", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.pytext_models.get_bert_biencoder_components"], []], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.__init__.init_fairseq_roberta_biencoder": [[36, 41], ["get_roberta_biencoder_components", "importlib.util.find_spec", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.fairseq_models.get_roberta_biencoder_components"], []], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.__init__.init_hf_bert_tenzorizer": [[43, 48], ["get_bert_tensorizer", "importlib.util.find_spec", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.get_bert_tensorizer"], []], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.__init__.init_hf_roberta_tenzorizer": [[50, 55], ["get_roberta_tensorizer", "importlib.util.find_spec", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.get_roberta_tensorizer"], []], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.__init__.init_comp": [[75, 80], ["RuntimeError"], "function", ["None"], []], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.__init__.init_biencoder_components": [[82, 84], ["__init__.init_comp"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.__init__.init_comp"], []], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.__init__.init_reader_components": [[86, 88], ["__init__.init_comp"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.__init__.init_comp"], []], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.__init__.init_tenzorizer": [[90, 92], ["__init__.init_comp"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.__init__.init_comp"], []], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader.Reader.__init__": [[32, 38], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "dpr.utils.model_utils.init_weights"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseHNSWFlatIndexer.__init__", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ":", "nn", ".", "Module", ",", "hidden_size", ")", ":", "\n", "        ", "super", "(", "Reader", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "qa_outputs", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "2", ")", "\n", "self", ".", "qa_classifier", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "1", ")", "\n", "init_weights", "(", "[", "self", ".", "qa_outputs", ",", "self", ".", "qa_classifier", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader.Reader.forward": [[39, 49], ["input_ids.size", "reader.Reader._forward", "input_ids.view", "attention_mask.view", "reader.compute_loss", "start_logits.view", "end_logits.view", "relevance_logits.view"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader.ReaderYesNo._forward", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader.compute_loss"], ["", "def", "forward", "(", "self", ",", "input_ids", ":", "T", ",", "attention_mask", ":", "T", ",", "start_positions", "=", "None", ",", "end_positions", "=", "None", ",", "answer_mask", "=", "None", ")", ":", "\n", "# notations: N - number of questions in a batch, M - number of passages per questions, L - sequence length", "\n", "        ", "N", ",", "M", ",", "L", "=", "input_ids", ".", "size", "(", ")", "\n", "start_logits", ",", "end_logits", ",", "relevance_logits", "=", "self", ".", "_forward", "(", "input_ids", ".", "view", "(", "N", "*", "M", ",", "L", ")", ",", "\n", "attention_mask", ".", "view", "(", "N", "*", "M", ",", "L", ")", ")", "\n", "if", "self", ".", "training", ":", "\n", "            ", "return", "compute_loss", "(", "start_positions", ",", "end_positions", ",", "answer_mask", ",", "start_logits", ",", "end_logits", ",", "relevance_logits", ",", "\n", "N", ",", "M", ")", "\n", "\n", "", "return", "start_logits", ".", "view", "(", "N", ",", "M", ",", "L", ")", ",", "end_logits", ".", "view", "(", "N", ",", "M", ",", "L", ")", ",", "relevance_logits", ".", "view", "(", "N", ",", "M", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader.Reader._forward": [[50, 59], ["reader.Reader.encoder", "reader.Reader.qa_outputs", "reader.Reader.split", "start_logits.squeeze.squeeze.squeeze", "end_logits.squeeze.squeeze.squeeze", "reader.Reader.qa_classifier"], "methods", ["None"], ["", "def", "_forward", "(", "self", ",", "input_ids", ",", "attention_mask", ")", ":", "\n", "# TODO: provide segment values", "\n", "        ", "sequence_output", ",", "_pooled_output", ",", "_hidden_states", "=", "self", ".", "encoder", "(", "input_ids", ",", "None", ",", "attention_mask", ")", "\n", "logits", "=", "self", ".", "qa_outputs", "(", "sequence_output", ")", "\n", "start_logits", ",", "end_logits", "=", "logits", ".", "split", "(", "1", ",", "dim", "=", "-", "1", ")", "\n", "start_logits", "=", "start_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "rank_logits", "=", "self", ".", "qa_classifier", "(", "sequence_output", "[", ":", ",", "0", ",", ":", "]", ")", "\n", "return", "start_logits", ",", "end_logits", ",", "rank_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader.ReaderYesNo.__init__": [[99, 105], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "dpr.utils.model_utils.init_weights"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseHNSWFlatIndexer.__init__", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.model_utils.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ":", "nn", ".", "Module", ",", "hidden_size", ",", "n_class", ")", ":", "\n", "        ", "super", "(", "ReaderYesNo", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "qa_outputs", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "2", ")", "\n", "self", ".", "qa_classifier", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "n_class", ")", "\n", "init_weights", "(", "[", "self", ".", "qa_outputs", ",", "self", ".", "qa_classifier", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader.ReaderYesNo.forward": [[106, 116], ["input_ids.size", "reader.ReaderYesNo._forward", "input_ids.view", "attention_mask.view", "reader.compute_loss_yes_no", "start_logits.view", "end_logits.view", "relevance_logits.view"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader.ReaderYesNo._forward", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader.compute_loss_yes_no"], ["", "def", "forward", "(", "self", ",", "input_ids", ":", "T", ",", "attention_mask", ":", "T", ",", "start_positions", "=", "None", ",", "end_positions", "=", "None", ",", "answer_mask", "=", "None", ",", "switch_labels", "=", "None", ")", ":", "\n", "# notations: N - number of questions in a batch, M - number of passages per questions, L - sequence length", "\n", "        ", "N", ",", "M", ",", "L", "=", "input_ids", ".", "size", "(", ")", "\n", "start_logits", ",", "end_logits", ",", "relevance_logits", "=", "self", ".", "_forward", "(", "input_ids", ".", "view", "(", "N", "*", "M", ",", "L", ")", ",", "\n", "attention_mask", ".", "view", "(", "N", "*", "M", ",", "L", ")", ")", "\n", "if", "self", ".", "training", ":", "\n", "            ", "return", "compute_loss_yes_no", "(", "start_positions", ",", "end_positions", ",", "switch_labels", ",", "answer_mask", ",", "start_logits", ",", "end_logits", ",", "relevance_logits", ",", "\n", "N", ",", "M", ")", "\n", "\n", "", "return", "start_logits", ".", "view", "(", "N", ",", "M", ",", "L", ")", ",", "end_logits", ".", "view", "(", "N", ",", "M", ",", "L", ")", ",", "relevance_logits", ".", "view", "(", "N", ",", "M", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader.ReaderYesNo._forward": [[117, 126], ["reader.ReaderYesNo.encoder", "reader.ReaderYesNo.qa_outputs", "reader.ReaderYesNo.split", "start_logits.squeeze.squeeze.squeeze", "end_logits.squeeze.squeeze.squeeze", "reader.ReaderYesNo.qa_classifier"], "methods", ["None"], ["", "def", "_forward", "(", "self", ",", "input_ids", ",", "attention_mask", ")", ":", "\n", "# TODO: provide segment values", "\n", "        ", "sequence_output", ",", "_pooled_output", ",", "_hidden_states", "=", "self", ".", "encoder", "(", "input_ids", ",", "None", ",", "attention_mask", ")", "\n", "logits", "=", "self", ".", "qa_outputs", "(", "sequence_output", ")", "\n", "start_logits", ",", "end_logits", "=", "logits", ".", "split", "(", "1", ",", "dim", "=", "-", "1", ")", "\n", "start_logits", "=", "start_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "rank_logits", "=", "self", ".", "qa_classifier", "(", "sequence_output", "[", ":", ",", "0", ",", ":", "]", ")", "\n", "return", "start_logits", ",", "end_logits", ",", "rank_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader.compute_loss": [[61, 96], ["start_positions.view.view", "end_positions.view.view", "answer_mask.type().cuda.view", "start_logits.view.view", "end_logits.view.view", "relevance_logits.view.view", "answer_mask.type().cuda.type().cuda", "start_logits.view.size", "start_positions.view.clamp_", "end_positions.view.clamp_", "torch.nn.CrossEntropyLoss", "relevance_logits.view.view", "torch.zeros().cuda", "torch.zeros().cuda", "torch.sum", "torch.sum", "reader._calc_mml", "torch.nn.CrossEntropyLoss.", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "loss_tensor.view().max", "answer_mask.type().cuda.type", "torch.zeros", "torch.zeros", "torch.nn.CrossEntropyLoss.", "zip", "torch.nn.CrossEntropyLoss.", "zip", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "t.unsqueeze", "t.unsqueeze", "loss_tensor.view"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._calc_mml"], ["", "", "def", "compute_loss", "(", "start_positions", ",", "end_positions", ",", "answer_mask", ",", "start_logits", ",", "end_logits", ",", "relevance_logits", ",", "N", ",", "M", ")", ":", "\n", "    ", "start_positions", "=", "start_positions", ".", "view", "(", "N", "*", "M", ",", "-", "1", ")", "\n", "end_positions", "=", "end_positions", ".", "view", "(", "N", "*", "M", ",", "-", "1", ")", "\n", "answer_mask", "=", "answer_mask", ".", "view", "(", "N", "*", "M", ",", "-", "1", ")", "\n", "\n", "start_logits", "=", "start_logits", ".", "view", "(", "N", "*", "M", ",", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "view", "(", "N", "*", "M", ",", "-", "1", ")", "\n", "relevance_logits", "=", "relevance_logits", ".", "view", "(", "N", "*", "M", ")", "\n", "\n", "answer_mask", "=", "answer_mask", ".", "type", "(", "torch", ".", "FloatTensor", ")", ".", "cuda", "(", ")", "\n", "\n", "ignored_index", "=", "start_logits", ".", "size", "(", "1", ")", "\n", "start_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "end_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "reduce", "=", "False", ",", "ignore_index", "=", "ignored_index", ")", "\n", "\n", "# compute switch loss", "\n", "relevance_logits", "=", "relevance_logits", ".", "view", "(", "N", ",", "M", ")", "\n", "switch_labels", "=", "torch", ".", "zeros", "(", "N", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", "\n", "switch_loss", "=", "torch", ".", "sum", "(", "loss_fct", "(", "relevance_logits", ",", "switch_labels", ")", ")", "\n", "\n", "# compute span loss", "\n", "start_losses", "=", "[", "(", "loss_fct", "(", "start_logits", ",", "_start_positions", ")", "*", "_span_mask", ")", "\n", "for", "(", "_start_positions", ",", "_span_mask", ")", "\n", "in", "zip", "(", "torch", ".", "unbind", "(", "start_positions", ",", "dim", "=", "1", ")", ",", "torch", ".", "unbind", "(", "answer_mask", ",", "dim", "=", "1", ")", ")", "]", "\n", "\n", "end_losses", "=", "[", "(", "loss_fct", "(", "end_logits", ",", "_end_positions", ")", "*", "_span_mask", ")", "\n", "for", "(", "_end_positions", ",", "_span_mask", ")", "\n", "in", "zip", "(", "torch", ".", "unbind", "(", "end_positions", ",", "dim", "=", "1", ")", ",", "torch", ".", "unbind", "(", "answer_mask", ",", "dim", "=", "1", ")", ")", "]", "\n", "loss_tensor", "=", "torch", ".", "cat", "(", "[", "t", ".", "unsqueeze", "(", "1", ")", "for", "t", "in", "start_losses", "]", ",", "dim", "=", "1", ")", "+", "torch", ".", "cat", "(", "[", "t", ".", "unsqueeze", "(", "1", ")", "for", "t", "in", "end_losses", "]", ",", "dim", "=", "1", ")", "\n", "\n", "loss_tensor", "=", "loss_tensor", ".", "view", "(", "N", ",", "M", ",", "-", "1", ")", ".", "max", "(", "dim", "=", "1", ")", "[", "0", "]", "\n", "span_loss", "=", "_calc_mml", "(", "loss_tensor", ")", "\n", "return", "span_loss", "+", "switch_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader.compute_loss_yes_no": [[128, 167], ["start_positions.view.view", "end_positions.view.view", "answer_mask.type().cuda.view", "start_logits.view.view", "end_logits.view.view", "relevance_logits.view.view", "answer_mask.type().cuda.type().cuda", "start_logits.view.size", "start_positions.view.clamp_", "end_positions.view.clamp_", "torch.nn.CrossEntropyLoss", "relevance_logits.view.view", "torch.sum", "torch.sum", "reader._calc_mml", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "loss_tensor.view().max", "answer_mask.type().cuda.type", "torch.nn.CrossEntropyLoss.", "zip", "torch.nn.CrossEntropyLoss.", "zip", "torch.nn.CrossEntropyLoss.", "zip", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "t.unsqueeze", "t.unsqueeze", "loss_tensor.view"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._calc_mml"], ["", "", "def", "compute_loss_yes_no", "(", "start_positions", ",", "end_positions", ",", "switch_labels", ",", "answer_mask", ",", "start_logits", ",", "end_logits", ",", "relevance_logits", ",", "N", ",", "M", ")", ":", "\n", "    ", "start_positions", "=", "start_positions", ".", "view", "(", "N", "*", "M", ",", "-", "1", ")", "\n", "end_positions", "=", "end_positions", ".", "view", "(", "N", "*", "M", ",", "-", "1", ")", "\n", "answer_mask", "=", "answer_mask", ".", "view", "(", "N", "*", "M", ",", "-", "1", ")", "\n", "\n", "start_logits", "=", "start_logits", ".", "view", "(", "N", "*", "M", ",", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "view", "(", "N", "*", "M", ",", "-", "1", ")", "\n", "relevance_logits", "=", "relevance_logits", ".", "view", "(", "N", "*", "M", ")", "\n", "\n", "answer_mask", "=", "answer_mask", ".", "type", "(", "torch", ".", "FloatTensor", ")", ".", "cuda", "(", ")", "\n", "\n", "ignored_index", "=", "start_logits", ".", "size", "(", "1", ")", "\n", "start_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "end_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "reduce", "=", "False", ",", "ignore_index", "=", "ignored_index", ")", "\n", "\n", "# compute switch loss", "\n", "relevance_logits", "=", "relevance_logits", ".", "view", "(", "N", ",", "M", ")", "\n", "\n", "# compute span loss", "\n", "start_losses", "=", "[", "(", "loss_fct", "(", "start_logits", ",", "_start_positions", ")", "*", "_span_mask", ")", "\n", "for", "(", "_start_positions", ",", "_span_mask", ")", "\n", "in", "zip", "(", "torch", ".", "unbind", "(", "start_positions", ",", "dim", "=", "1", ")", ",", "torch", ".", "unbind", "(", "answer_mask", ",", "dim", "=", "1", ")", ")", "]", "\n", "\n", "end_losses", "=", "[", "(", "loss_fct", "(", "end_logits", ",", "_end_positions", ")", "*", "_span_mask", ")", "\n", "for", "(", "_end_positions", ",", "_span_mask", ")", "\n", "in", "zip", "(", "torch", ".", "unbind", "(", "end_positions", ",", "dim", "=", "1", ")", ",", "torch", ".", "unbind", "(", "answer_mask", ",", "dim", "=", "1", ")", ")", "]", "\n", "\n", "switch_losses", "=", "[", "(", "loss_fct", "(", "relevance_logits", ",", "_switch", ")", "*", "_answer_mask", ")", "for", "(", "_switch", ",", "_answer_mask", ")", "in", "zip", "(", "torch", ".", "unbind", "(", "switch_labels", ",", "dim", "=", "1", ")", ",", "torch", ".", "unbind", "(", "answer_mask", ",", "dim", "=", "1", ")", ")", "]", "\n", "switch_loss", "=", "torch", ".", "sum", "(", "switch_losses", ")", "\n", "\n", "loss_tensor", "=", "torch", ".", "cat", "(", "[", "t", ".", "unsqueeze", "(", "1", ")", "for", "t", "in", "start_losses", "]", ",", "dim", "=", "1", ")", "+", "torch", ".", "cat", "(", "[", "t", ".", "unsqueeze", "(", "1", ")", "for", "t", "in", "end_losses", "]", ",", "dim", "=", "1", ")", "\n", "\n", "loss_tensor", "=", "loss_tensor", ".", "view", "(", "N", ",", "M", ",", "-", "1", ")", ".", "max", "(", "dim", "=", "1", ")", "[", "0", "]", "\n", "span_loss", "=", "_calc_mml", "(", "loss_tensor", ")", "\n", "return", "span_loss", "+", "switch_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader.create_reader_input": [[169, 225], ["torch.Tensor().new_full", "torch.Tensor().new_full", "torch.cat", "torch.cat", "ReaderBatch", "reader._create_question_passages_tensors", "torch.cat.append", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.Tensor", "torch.Tensor", "logger.warning", "torch.stack.append", "torch.stack.append", "torch.stack.append", "len", "ids.unsqueeze"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._create_question_passages_tensors"], ["", "def", "create_reader_input", "(", "pad_token_id", ":", "int", ",", "\n", "samples", ":", "List", "[", "ReaderSample", "]", ",", "\n", "passages_per_question", ":", "int", ",", "\n", "max_length", ":", "int", ",", "\n", "max_n_answers", ":", "int", ",", "\n", "is_train", ":", "bool", ",", "\n", "shuffle", ":", "bool", ",", "\n", ")", "->", "ReaderBatch", ":", "\n", "    ", "\"\"\"\n    Creates a reader batch instance out of a list of ReaderSample-s\n    :param pad_token_id: id of the padding token\n    :param samples: list of samples to create the batch for\n    :param passages_per_question: amount of passages for every question in a batch\n    :param max_length: max model input sequence length\n    :param max_n_answers: max num of answers per single question\n    :param is_train: if the samples are for a train set\n    :param shuffle: should passages selection be randomized\n    :return: ReaderBatch instance\n    \"\"\"", "\n", "input_ids", "=", "[", "]", "\n", "start_positions", "=", "[", "]", "\n", "end_positions", "=", "[", "]", "\n", "answers_masks", "=", "[", "]", "\n", "empty_sequence", "=", "torch", ".", "Tensor", "(", ")", ".", "new_full", "(", "(", "max_length", ",", ")", ",", "pad_token_id", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "for", "sample", "in", "samples", ":", "\n", "        ", "positive_ctxs", "=", "sample", ".", "positive_passages", "\n", "negative_ctxs", "=", "sample", ".", "negative_passages", "if", "is_train", "else", "sample", ".", "passages", "\n", "\n", "sample_tensors", "=", "_create_question_passages_tensors", "(", "positive_ctxs", ",", "\n", "negative_ctxs", ",", "\n", "passages_per_question", ",", "\n", "empty_sequence", ",", "\n", "max_n_answers", ",", "\n", "pad_token_id", ",", "\n", "is_train", ",", "\n", "is_random", "=", "shuffle", ")", "\n", "if", "not", "sample_tensors", ":", "\n", "            ", "logger", ".", "warning", "(", "'No valid passages combination for question=%s '", ",", "sample", ".", "question", ")", "\n", "continue", "\n", "", "sample_input_ids", ",", "starts_tensor", ",", "ends_tensor", ",", "answer_mask", "=", "sample_tensors", "\n", "input_ids", ".", "append", "(", "sample_input_ids", ")", "\n", "if", "is_train", ":", "\n", "            ", "start_positions", ".", "append", "(", "starts_tensor", ")", "\n", "end_positions", ".", "append", "(", "ends_tensor", ")", "\n", "answers_masks", ".", "append", "(", "answer_mask", ")", "\n", "", "", "if", "len", "(", "input_ids", ")", "==", "0", "and", "is_train", "is", "True", ":", "\n", "        ", "return", "None", "\n", "", "input_ids", "=", "torch", ".", "cat", "(", "[", "ids", ".", "unsqueeze", "(", "0", ")", "for", "ids", "in", "input_ids", "]", ",", "dim", "=", "0", ")", "\n", "\n", "if", "is_train", ":", "\n", "        ", "start_positions", "=", "torch", ".", "stack", "(", "start_positions", ",", "dim", "=", "0", ")", "\n", "end_positions", "=", "torch", ".", "stack", "(", "end_positions", ",", "dim", "=", "0", ")", "\n", "answers_masks", "=", "torch", ".", "stack", "(", "answers_masks", ",", "dim", "=", "0", ")", "\n", "\n", "", "return", "ReaderBatch", "(", "input_ids", ",", "start_positions", ",", "end_positions", ",", "answers_masks", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader.create_reader_input_yes_no": [[226, 284], ["torch.Tensor().new_full", "torch.Tensor().new_full", "torch.cat", "torch.cat", "ReaderBatch", "reader._create_question_passages_tensors_yes_no", "torch.cat.append", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.Tensor", "torch.Tensor", "logger.warning", "torch.stack.append", "torch.stack.append", "torch.stack.append", "torch.stack.append", "ids.unsqueeze"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._create_question_passages_tensors_yes_no"], ["", "def", "create_reader_input_yes_no", "(", "pad_token_id", ":", "int", ",", "\n", "samples", ":", "List", "[", "ReaderSample", "]", ",", "\n", "passages_per_question", ":", "int", ",", "\n", "max_length", ":", "int", ",", "\n", "max_n_answers", ":", "int", ",", "\n", "is_train", ":", "bool", ",", "\n", "shuffle", ":", "bool", ",", "\n", ")", "->", "ReaderBatch", ":", "\n", "    ", "\"\"\"\n    Creates a reader batch instance out of a list of ReaderSample-s\n    :param pad_token_id: id of the padding token\n    :param samples: list of samples to create the batch for\n    :param passages_per_question: amount of passages for every question in a batch\n    :param max_length: max model input sequence length\n    :param max_n_answers: max num of answers per single question\n    :param is_train: if the samples are for a train set\n    :param shuffle: should passages selection be randomized\n    :return: ReaderBatch instance\n    \"\"\"", "\n", "input_ids", "=", "[", "]", "\n", "start_positions", "=", "[", "]", "\n", "end_positions", "=", "[", "]", "\n", "answers_masks", "=", "[", "]", "\n", "swtich_labels", "=", "[", "]", "\n", "empty_sequence", "=", "torch", ".", "Tensor", "(", ")", ".", "new_full", "(", "(", "max_length", ",", ")", ",", "pad_token_id", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "for", "sample", "in", "samples", ":", "\n", "        ", "positive_ctxs", "=", "sample", ".", "positive_passages", "\n", "negative_ctxs", "=", "sample", ".", "negative_passages", "if", "is_train", "else", "sample", ".", "passages", "\n", "is_yes_no", "=", "sample", ".", "is_yes_no", "\n", "\n", "sample_tensors", "=", "_create_question_passages_tensors_yes_no", "(", "positive_ctxs", ",", "\n", "negative_ctxs", ",", "\n", "passages_per_question", ",", "\n", "empty_sequence", ",", "\n", "max_n_answers", ",", "\n", "pad_token_id", ",", "\n", "is_train", ",", "\n", "is_random", "=", "shuffle", ",", "is_yes_no", "=", "is_yes_no", ")", "\n", "if", "not", "sample_tensors", ":", "\n", "            ", "logger", ".", "warning", "(", "'No valid passages combination for question=%s '", ",", "sample", ".", "question", ")", "\n", "continue", "\n", "", "sample_input_ids", ",", "starts_tensor", ",", "ends_tensor", ",", "switch_tensor", ",", "answer_mask", "=", "sample_tensors", "\n", "input_ids", ".", "append", "(", "sample_input_ids", ")", "\n", "if", "is_train", ":", "\n", "            ", "start_positions", ".", "append", "(", "starts_tensor", ")", "\n", "end_positions", ".", "append", "(", "ends_tensor", ")", "\n", "swtich_labels", ".", "append", "(", "switch_tensor", ")", "\n", "answers_masks", ".", "append", "(", "answer_mask", ")", "\n", "", "", "input_ids", "=", "torch", ".", "cat", "(", "[", "ids", ".", "unsqueeze", "(", "0", ")", "for", "ids", "in", "input_ids", "]", ",", "dim", "=", "0", ")", "\n", "\n", "if", "is_train", ":", "\n", "        ", "start_positions", "=", "torch", ".", "stack", "(", "start_positions", ",", "dim", "=", "0", ")", "\n", "end_positions", "=", "torch", ".", "stack", "(", "end_positions", ",", "dim", "=", "0", ")", "\n", "answers_masks", "=", "torch", ".", "stack", "(", "answers_masks", ",", "dim", "=", "0", ")", "\n", "swtich_labels", "=", "torch", ".", "stack", "(", "swtich_labels", ",", "dim", "=", "0", ")", "\n", "\n", "", "return", "ReaderBatch", "(", "input_ids", ",", "start_positions", ",", "end_positions", ",", "swtich_labels", ",", "answers_masks", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._calc_mml": [[285, 290], ["torch.sum", "torch.sum", "torch.exp", "torch.exp", "torch.sum", "torch.sum", "torch.log", "torch.log", "torch.ones().cuda", "torch.ones().cuda", "torch.ones", "torch.ones", "loss_tensor.size"], "function", ["None"], ["", "def", "_calc_mml", "(", "loss_tensor", ")", ":", "\n", "    ", "marginal_likelihood", "=", "torch", ".", "sum", "(", "torch", ".", "exp", "(", "\n", "-", "loss_tensor", "-", "1e10", "*", "(", "loss_tensor", "==", "0", ")", ".", "float", "(", ")", ")", ",", "1", ")", "\n", "return", "-", "torch", ".", "sum", "(", "torch", ".", "log", "(", "marginal_likelihood", "+", "\n", "torch", ".", "ones", "(", "loss_tensor", ".", "size", "(", "0", ")", ")", ".", "cuda", "(", ")", "*", "(", "marginal_likelihood", "==", "0", ")", ".", "float", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._pad_to_len": [[292, 297], ["seq.size", "torch.cat", "torch.cat", "torch.Tensor().new_full", "torch.Tensor().new_full", "torch.Tensor", "torch.Tensor"], "function", ["None"], ["", "def", "_pad_to_len", "(", "seq", ":", "T", ",", "pad_id", ":", "int", ",", "max_len", ":", "int", ")", ":", "\n", "    ", "s_len", "=", "seq", ".", "size", "(", "0", ")", "\n", "if", "s_len", ">", "max_len", ":", "\n", "        ", "return", "seq", "[", "0", ":", "max_len", "]", "\n", "", "return", "torch", ".", "cat", "(", "[", "seq", ",", "torch", ".", "Tensor", "(", ")", ".", "new_full", "(", "(", "max_len", "-", "s_len", ",", ")", ",", "pad_id", ",", "dtype", "=", "torch", ".", "long", ")", "]", ",", "dim", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._get_answer_spans": [[299, 302], ["None"], "function", ["None"], ["", "def", "_get_answer_spans", "(", "idx", ",", "positives", ":", "List", "[", "ReaderPassage", "]", ",", "max_len", ":", "int", ")", ":", "\n", "    ", "positive_a_spans", "=", "positives", "[", "idx", "]", ".", "answers_spans", "\n", "return", "[", "span", "for", "span", "in", "positive_a_spans", "if", "(", "span", "[", "0", "]", "<", "max_len", "and", "span", "[", "1", "]", "<", "max_len", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._get_positive_idx": [[303, 312], ["numpy.random.choice", "reader._get_answer_spans", "next", "len", "range", "reader._get_answer_spans", "len"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._get_answer_spans", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._get_answer_spans"], ["", "def", "_get_positive_idx", "(", "positives", ":", "List", "[", "ReaderPassage", "]", ",", "max_len", ":", "int", ",", "is_random", ":", "bool", ")", ":", "\n", "# select just one positive", "\n", "    ", "positive_idx", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "positives", ")", ")", "if", "is_random", "else", "0", "\n", "\n", "if", "not", "_get_answer_spans", "(", "positive_idx", ",", "positives", ",", "max_len", ")", ":", "\n", "# question may be too long, find the first positive with at least one valid span", "\n", "        ", "positive_idx", "=", "next", "(", "(", "i", "for", "i", "in", "range", "(", "len", "(", "positives", ")", ")", "if", "_get_answer_spans", "(", "i", ",", "positives", ",", "max_len", ")", ")", ",", "\n", "None", ")", "\n", "", "return", "positive_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._get_positive_idx_has_answer": [[313, 317], ["numpy.random.choice", "len"], "function", ["None"], ["", "def", "_get_positive_idx_has_answer", "(", "positives", ":", "List", "[", "ReaderPassage", "]", ",", "is_random", ":", "bool", ")", ":", "\n", "# select just one positive", "\n", "    ", "positive_idx", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "positives", ")", ")", "if", "is_random", "else", "0", "\n", "return", "positive_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._create_question_passages_tensors": [[318, 371], ["empty_ids.size", "len", "torch.stack", "torch.stack", "reader._get_positive_idx", "all", "all", "reader._pad_to_len", "torch.zeros().long", "torch.zeros().long", "torch.tensor", "torch.tensor", "torch.zeros().long", "torch.zeros().long", "torch.tensor", "torch.tensor", "torch.zeros", "torch.zeros", "torch.tensor", "torch.tensor", "numpy.random.permutation", "range", "reader._pad_to_len", "len", "negatives_selected.append", "reader._get_answer_spans", "range", "empty_ids.clone", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "len", "len", "len", "len", "len", "range", "len"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._get_positive_idx", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._pad_to_len", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._pad_to_len", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._get_answer_spans"], ["", "def", "_create_question_passages_tensors", "(", "positives", ":", "List", "[", "ReaderPassage", "]", ",", "negatives", ":", "List", "[", "ReaderPassage", "]", ",", "total_size", ":", "int", ",", "\n", "empty_ids", ":", "T", ",", "\n", "max_n_answers", ":", "int", ",", "\n", "pad_token_id", ":", "int", ",", "\n", "is_train", ":", "bool", ",", "\n", "is_random", ":", "bool", "=", "True", ")", ":", "\n", "    ", "max_len", "=", "empty_ids", ".", "size", "(", "0", ")", "\n", "if", "is_train", ":", "\n", "# select just one positive", "\n", "        ", "positive_idx", "=", "_get_positive_idx", "(", "positives", ",", "max_len", ",", "is_random", ")", "\n", "if", "positive_idx", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "positive_a_spans", "=", "_get_answer_spans", "(", "positive_idx", ",", "positives", ",", "max_len", ")", "[", "0", ":", "max_n_answers", "]", "\n", "\n", "answer_starts", "=", "[", "span", "[", "0", "]", "for", "span", "in", "positive_a_spans", "]", "\n", "answer_ends", "=", "[", "span", "[", "1", "]", "for", "span", "in", "positive_a_spans", "]", "\n", "\n", "assert", "all", "(", "s", "<", "max_len", "for", "s", "in", "answer_starts", ")", "\n", "assert", "all", "(", "e", "<", "max_len", "for", "e", "in", "answer_ends", ")", "\n", "\n", "positive_input_ids", "=", "_pad_to_len", "(", "positives", "[", "positive_idx", "]", ".", "sequence_ids", ",", "pad_token_id", ",", "max_len", ")", "\n", "\n", "answer_starts_tensor", "=", "torch", ".", "zeros", "(", "(", "total_size", ",", "max_n_answers", ")", ")", ".", "long", "(", ")", "\n", "answer_starts_tensor", "[", "0", ",", "0", ":", "len", "(", "answer_starts", ")", "]", "=", "torch", ".", "tensor", "(", "answer_starts", ")", "\n", "\n", "answer_ends_tensor", "=", "torch", ".", "zeros", "(", "(", "total_size", ",", "max_n_answers", ")", ")", ".", "long", "(", ")", "\n", "answer_ends_tensor", "[", "0", ",", "0", ":", "len", "(", "answer_ends", ")", "]", "=", "torch", ".", "tensor", "(", "answer_ends", ")", "\n", "\n", "answer_mask", "=", "torch", ".", "zeros", "(", "(", "total_size", ",", "max_n_answers", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "answer_mask", "[", "0", ",", "0", ":", "len", "(", "answer_starts", ")", "]", "=", "torch", ".", "tensor", "(", "[", "1", "for", "_", "in", "range", "(", "len", "(", "answer_starts", ")", ")", "]", ")", "\n", "\n", "positives_selected", "=", "[", "positive_input_ids", "]", "\n", "\n", "", "else", ":", "\n", "        ", "positives_selected", "=", "[", "]", "\n", "answer_starts_tensor", "=", "None", "\n", "answer_ends_tensor", "=", "None", "\n", "answer_mask", "=", "None", "\n", "\n", "", "positives_num", "=", "len", "(", "positives_selected", ")", "\n", "negative_idxs", "=", "np", ".", "random", ".", "permutation", "(", "range", "(", "len", "(", "negatives", ")", ")", ")", "if", "is_random", "else", "range", "(", "\n", "len", "(", "negatives", ")", "-", "positives_num", ")", "\n", "\n", "negative_idxs", "=", "negative_idxs", "[", ":", "total_size", "-", "positives_num", "]", "\n", "\n", "negatives_selected", "=", "[", "_pad_to_len", "(", "negatives", "[", "i", "]", ".", "sequence_ids", ",", "pad_token_id", ",", "max_len", ")", "for", "i", "in", "negative_idxs", "]", "\n", "\n", "while", "len", "(", "negatives_selected", ")", "<", "total_size", "-", "positives_num", ":", "\n", "        ", "negatives_selected", ".", "append", "(", "empty_ids", ".", "clone", "(", ")", ")", "\n", "\n", "", "input_ids", "=", "torch", ".", "stack", "(", "[", "t", "for", "t", "in", "positives_selected", "+", "negatives_selected", "]", ",", "dim", "=", "0", ")", "\n", "return", "input_ids", ",", "answer_starts_tensor", ",", "answer_ends_tensor", ",", "answer_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._create_question_passages_tensors_yes_no": [[373, 434], ["empty_ids.size", "len", "torch.stack", "torch.stack", "reader._get_positive_idx", "all", "all", "reader._pad_to_len", "torch.zeros().long", "torch.zeros().long", "torch.tensor", "torch.tensor", "torch.zeros().long", "torch.zeros().long", "torch.tensor", "torch.tensor", "torch.zeros", "torch.zeros", "torch.tensor", "torch.tensor", "numpy.random.permutation", "range", "reader._pad_to_len", "len", "negatives_selected.append", "reader._get_answer_spans", "reader._get_positive_idx_has_answer", "reader._pad_to_len", "range", "empty_ids.clone", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "len", "len", "len", "len", "len", "range", "len"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._get_positive_idx", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._pad_to_len", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._pad_to_len", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._get_answer_spans", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._get_positive_idx_has_answer", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.reader._pad_to_len"], ["", "def", "_create_question_passages_tensors_yes_no", "(", "positives", ":", "List", "[", "ReaderPassage", "]", ",", "negatives", ":", "List", "[", "ReaderPassage", "]", ",", "total_size", ":", "int", ",", "\n", "empty_ids", ":", "T", ",", "\n", "max_n_answers", ":", "int", ",", "\n", "pad_token_id", ":", "int", ",", "\n", "is_train", ":", "bool", ",", "\n", "is_random", ":", "bool", "=", "True", ",", "is_yes_no", ":", "bool", "=", "False", ")", ":", "\n", "    ", "max_len", "=", "empty_ids", ".", "size", "(", "0", ")", "\n", "if", "is_train", "and", "is_yes_no", "is", "False", ":", "\n", "# select just one positive; add yes. no information", "\n", "        ", "positive_idx", "=", "_get_positive_idx", "(", "positives", ",", "max_len", ",", "is_random", ")", "\n", "if", "positive_idx", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "positive_a_spans", "=", "_get_answer_spans", "(", "positive_idx", ",", "positives", ",", "max_len", ")", "[", "0", ":", "max_n_answers", "]", "\n", "\n", "answer_starts", "=", "[", "span", "[", "0", "]", "for", "span", "in", "positive_a_spans", "]", "\n", "answer_ends", "=", "[", "span", "[", "1", "]", "for", "span", "in", "positive_a_spans", "]", "\n", "\n", "assert", "all", "(", "s", "<", "max_len", "for", "s", "in", "answer_starts", ")", "\n", "assert", "all", "(", "e", "<", "max_len", "for", "e", "in", "answer_ends", ")", "\n", "\n", "positive_input_ids", "=", "_pad_to_len", "(", "positives", "[", "positive_idx", "]", ".", "sequence_ids", ",", "pad_token_id", ",", "max_len", ")", "\n", "\n", "answer_starts_tensor", "=", "torch", ".", "zeros", "(", "(", "total_size", ",", "max_n_answers", ")", ")", ".", "long", "(", ")", "\n", "answer_starts_tensor", "[", "0", ",", "0", ":", "len", "(", "answer_starts", ")", "]", "=", "torch", ".", "tensor", "(", "answer_starts", ")", "\n", "\n", "answer_ends_tensor", "=", "torch", ".", "zeros", "(", "(", "total_size", ",", "max_n_answers", ")", ")", ".", "long", "(", ")", "\n", "answer_ends_tensor", "[", "0", ",", "0", ":", "len", "(", "answer_ends", ")", "]", "=", "torch", ".", "tensor", "(", "answer_ends", ")", "\n", "\n", "answer_mask", "=", "torch", ".", "zeros", "(", "(", "total_size", ",", "max_n_answers", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "answer_mask", "[", "0", ",", "0", ":", "len", "(", "answer_starts", ")", "]", "=", "torch", ".", "tensor", "(", "[", "1", "for", "_", "in", "range", "(", "len", "(", "answer_starts", ")", ")", "]", ")", "\n", "\n", "positives_selected", "=", "[", "positive_input_ids", "]", "\n", "\n", "", "elif", "is_train", "and", "is_yes_no", "is", "True", ":", "\n", "        ", "positive_idx", "=", "_get_positive_idx_has_answer", "(", "positives", ",", "is_random", ")", "\n", "positive_input_ids", "=", "_pad_to_len", "(", "positives", "[", "positive_idx", "]", ".", "sequence_ids", ",", "pad_token_id", ",", "max_len", ")", "\n", "positives_selected", "=", "[", "positive_input_ids", "]", "\n", "answer_starts_tensor", "=", "None", "\n", "answer_ends_tensor", "=", "None", "\n", "answer_mask", "=", "None", "\n", "", "else", ":", "\n", "# I think you should put yes/no selection here; negatives are limited so we need to add it carefully.", "\n", "        ", "positives_selected", "=", "[", "]", "\n", "answer_starts_tensor", "=", "None", "\n", "answer_ends_tensor", "=", "None", "\n", "answer_mask", "=", "None", "\n", "\n", "", "positives_num", "=", "len", "(", "positives_selected", ")", "\n", "negative_idxs", "=", "np", ".", "random", ".", "permutation", "(", "range", "(", "len", "(", "negatives", ")", ")", ")", "if", "is_random", "else", "range", "(", "\n", "len", "(", "negatives", ")", "-", "positives_num", ")", "\n", "\n", "negative_idxs", "=", "negative_idxs", "[", ":", "total_size", "-", "positives_num", "]", "\n", "\n", "negatives_selected", "=", "[", "_pad_to_len", "(", "negatives", "[", "i", "]", ".", "sequence_ids", ",", "pad_token_id", ",", "max_len", ")", "for", "i", "in", "negative_idxs", "]", "\n", "\n", "while", "len", "(", "negatives_selected", ")", "<", "total_size", "-", "positives_num", ":", "\n", "        ", "negatives_selected", ".", "append", "(", "empty_ids", ".", "clone", "(", ")", ")", "\n", "\n", "", "input_ids", "=", "torch", ".", "stack", "(", "[", "t", "for", "t", "in", "positives_selected", "+", "negatives_selected", "]", ",", "dim", "=", "0", ")", "\n", "return", "input_ids", ",", "answer_starts_tensor", ",", "answer_ends_tensor", ",", "answer_mask", "", "", ""]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.qa_validation.calculate_matches": [[29, 73], ["dpr.utils.tokenizers.SimpleTokenizer", "multiprocessing.Pool", "logger.info", "functools.partial", "zip", "multiprocessing.Pool.map", "logger.info", "len", "QAMatchStats", "len", "next", "enumerate"], "function", ["None"], ["def", "calculate_matches", "(", "all_docs", ":", "Dict", "[", "object", ",", "Tuple", "[", "str", ",", "str", "]", "]", ",", "answers", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "closest_docs", ":", "List", "[", "Tuple", "[", "List", "[", "object", "]", ",", "List", "[", "float", "]", "]", "]", ",", "workers_num", ":", "int", ",", "\n", "match_type", ":", "str", ")", "->", "QAMatchStats", ":", "\n", "    ", "\"\"\"\n    Evaluates answers presence in the set of documents. This function is supposed to be used with a large collection of\n    documents and results. It internally forks multiple sub-processes for evaluation and then merges results\n    :param all_docs: dictionary of the entire documents database. doc_id -> (doc_text, title)\n    :param answers: list of answers's list. One list per question\n    :param closest_docs: document ids of the top results along with their scores\n    :param workers_num: amount of parallel threads to process data\n    :param match_type: type of answer matching. Refer to has_answer code for available options\n    :return: matching information tuple.\n    top_k_hits - a list where the index is the amount of top documents retrieved and the value is the total amount of\n    valid matches across an entire dataset.\n    questions_doc_hits - more detailed info with answer matches for every question and every retrieved document\n    \"\"\"", "\n", "global", "dpr_all_documents", "\n", "dpr_all_documents", "=", "all_docs", "\n", "\n", "tok_opts", "=", "{", "}", "\n", "tokenizer", "=", "SimpleTokenizer", "(", "**", "tok_opts", ")", "\n", "\n", "processes", "=", "ProcessPool", "(", "\n", "processes", "=", "workers_num", ",", "\n", ")", "\n", "\n", "logger", ".", "info", "(", "'Matching answers in top docs...'", ")", "\n", "\n", "get_score_partial", "=", "partial", "(", "check_answer", ",", "match_type", "=", "match_type", ",", "tokenizer", "=", "tokenizer", ")", "\n", "\n", "questions_answers_docs", "=", "zip", "(", "answers", ",", "closest_docs", ")", "\n", "\n", "scores", "=", "processes", ".", "map", "(", "get_score_partial", ",", "questions_answers_docs", ")", "\n", "\n", "logger", ".", "info", "(", "'Per question validation results len=%d'", ",", "len", "(", "scores", ")", ")", "\n", "\n", "n_docs", "=", "len", "(", "closest_docs", "[", "0", "]", "[", "0", "]", ")", "\n", "top_k_hits", "=", "[", "0", "]", "*", "n_docs", "\n", "for", "question_hits", "in", "scores", ":", "\n", "        ", "best_hit", "=", "next", "(", "(", "i", "for", "i", ",", "x", "in", "enumerate", "(", "question_hits", ")", "if", "x", ")", ",", "None", ")", "\n", "if", "best_hit", "is", "not", "None", ":", "\n", "            ", "top_k_hits", "[", "best_hit", ":", "]", "=", "[", "v", "+", "1", "for", "v", "in", "top_k_hits", "[", "best_hit", ":", "]", "]", "\n", "\n", "", "", "return", "QAMatchStats", "(", "top_k_hits", ",", "scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.qa_validation.check_answer": [[75, 96], ["enumerate", "qa_validation.has_answer", "hits.append", "logger.warning", "hits.append"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.data.qa_validation.has_answer"], ["", "def", "check_answer", "(", "questions_answers_docs", ",", "tokenizer", ",", "match_type", ")", "->", "List", "[", "bool", "]", ":", "\n", "    ", "\"\"\"Search through all the top docs to see if they have any of the answers.\"\"\"", "\n", "answers", ",", "(", "doc_ids", ",", "doc_scores", ")", "=", "questions_answers_docs", "\n", "\n", "global", "dpr_all_documents", "\n", "hits", "=", "[", "]", "\n", "\n", "for", "i", ",", "doc_id", "in", "enumerate", "(", "doc_ids", ")", ":", "\n", "        ", "doc", "=", "dpr_all_documents", "[", "doc_id", "]", "\n", "text", "=", "doc", "[", "0", "]", "\n", "\n", "answer_found", "=", "False", "\n", "if", "text", "is", "None", ":", "# cannot find the document for some reason", "\n", "            ", "logger", ".", "warning", "(", "\"no doc in db\"", ")", "\n", "hits", ".", "append", "(", "False", ")", "\n", "continue", "\n", "\n", "", "if", "has_answer", "(", "answers", ",", "text", ",", "tokenizer", ",", "match_type", ")", ":", "\n", "            ", "answer_found", "=", "True", "\n", "", "hits", ".", "append", "(", "answer_found", ")", "\n", "", "return", "hits", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.qa_validation.has_answer": [[98, 125], ["qa_validation._normalize", "tokenizer.tokenize().words", "qa_validation._normalize", "tokenizer.tokenize", "_normalize.words", "range", "tokenizer.tokenize", "qa_validation._normalize", "qa_validation.regex_match", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.data.qa_validation._normalize", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokens.words", "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.qa_validation._normalize", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.SpacyTokenizer.tokenize", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.Tokens.words", "home.repos.pwc.inspect_result.AkariAsai_XORQA.utils.tokenizers.SpacyTokenizer.tokenize", "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.qa_validation._normalize", "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.qa_validation.regex_match"], ["", "def", "has_answer", "(", "answers", ",", "text", ",", "tokenizer", ",", "match_type", ")", "->", "bool", ":", "\n", "    ", "\"\"\"Check if a document contains an answer string.\n    If `match_type` is string, token matching is done between the text and answer.\n    If `match_type` is regex, we search the whole text with the regex.\n    \"\"\"", "\n", "text", "=", "_normalize", "(", "text", ")", "\n", "\n", "if", "match_type", "==", "'string'", ":", "\n", "# Answer is a list of possible strings", "\n", "        ", "text", "=", "tokenizer", ".", "tokenize", "(", "text", ")", ".", "words", "(", "uncased", "=", "True", ")", "\n", "\n", "for", "single_answer", "in", "answers", ":", "\n", "            ", "single_answer", "=", "_normalize", "(", "single_answer", ")", "\n", "single_answer", "=", "tokenizer", ".", "tokenize", "(", "single_answer", ")", "\n", "single_answer", "=", "single_answer", ".", "words", "(", "uncased", "=", "True", ")", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "text", ")", "-", "len", "(", "single_answer", ")", "+", "1", ")", ":", "\n", "                ", "if", "single_answer", "==", "text", "[", "i", ":", "i", "+", "len", "(", "single_answer", ")", "]", ":", "\n", "                    ", "return", "True", "\n", "\n", "", "", "", "", "elif", "match_type", "==", "'regex'", ":", "\n", "# Answer is a regex", "\n", "        ", "for", "single_answer", "in", "answers", ":", "\n", "            ", "single_answer", "=", "_normalize", "(", "single_answer", ")", "\n", "if", "regex_match", "(", "text", ",", "single_answer", ")", ":", "\n", "                ", "return", "True", "\n", "", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.qa_validation.regex_match": [[127, 137], ["regex.compile", "re.compile.search"], "function", ["None"], ["", "def", "regex_match", "(", "text", ",", "pattern", ")", ":", "\n", "    ", "\"\"\"Test if a regex pattern is contained within a text.\"\"\"", "\n", "try", ":", "\n", "        ", "pattern", "=", "re", ".", "compile", "(", "\n", "pattern", ",", "\n", "flags", "=", "re", ".", "IGNORECASE", "+", "re", ".", "UNICODE", "+", "re", ".", "MULTILINE", ",", "\n", ")", "\n", "", "except", "BaseException", ":", "\n", "        ", "return", "False", "\n", "", "return", "pattern", ".", "search", "(", "text", ")", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.qa_validation.exact_match_score": [[140, 142], ["qa_validation._normalize_answer", "qa_validation._normalize_answer"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.data.qa_validation._normalize_answer", "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.qa_validation._normalize_answer"], ["", "def", "exact_match_score", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "return", "_normalize_answer", "(", "prediction", ")", "==", "_normalize_answer", "(", "ground_truth", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.qa_validation._normalize_answer": [[144, 159], ["qa_validation._normalize_answer.white_space_fix"], "function", ["None"], ["", "def", "_normalize_answer", "(", "s", ")", ":", "\n", "    ", "def", "remove_articles", "(", "text", ")", ":", "\n", "        ", "return", "re", ".", "sub", "(", "r'\\b(a|an|the)\\b'", ",", "' '", ",", "text", ")", "\n", "\n", "", "def", "white_space_fix", "(", "text", ")", ":", "\n", "        ", "return", "' '", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "\n", "", "def", "remove_punc", "(", "text", ")", ":", "\n", "        ", "exclude", "=", "set", "(", "string", ".", "punctuation", ")", "\n", "return", "''", ".", "join", "(", "ch", "for", "ch", "in", "text", "if", "ch", "not", "in", "exclude", ")", "\n", "\n", "", "def", "lower", "(", "text", ")", ":", "\n", "        ", "return", "text", ".", "lower", "(", ")", "\n", "\n", "", "return", "white_space_fix", "(", "remove_articles", "(", "remove_punc", "(", "lower", "(", "s", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.qa_validation._normalize": [[161, 163], ["unicodedata.normalize"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.bm25.utils.normalize"], ["", "def", "_normalize", "(", "text", ")", ":", "\n", "    ", "return", "unicodedata", ".", "normalize", "(", "'NFD'", ",", "text", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data.ReaderPassage.__init__": [[35, 49], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "id", "=", "None", ",", "text", ":", "str", "=", "None", ",", "title", ":", "str", "=", "None", ",", "score", "=", "None", ",", "\n", "has_answer", ":", "bool", "=", "None", ")", ":", "\n", "        ", "self", ".", "id", "=", "id", "\n", "# string passage representations", "\n", "self", ".", "passage_text", "=", "text", "\n", "self", ".", "title", "=", "title", "\n", "self", ".", "score", "=", "score", "\n", "self", ".", "has_answer", "=", "has_answer", "\n", "self", ".", "passage_token_ids", "=", "None", "\n", "# offset of the actual passage (i.e. not a question or may be title) in the sequence_ids", "\n", "self", ".", "passage_offset", "=", "None", "\n", "self", ".", "answers_spans", "=", "None", "\n", "# passage token ids", "\n", "self", ".", "sequence_ids", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data.ReaderPassage.on_serialize": [[50, 56], ["reader_data.ReaderPassage.sequence_ids.numpy"], "methods", ["None"], ["", "def", "on_serialize", "(", "self", ")", ":", "\n", "# store only final sequence_ids and the ctx offset", "\n", "        ", "self", ".", "sequence_ids", "=", "self", ".", "sequence_ids", ".", "numpy", "(", ")", "\n", "self", ".", "passage_text", "=", "None", "\n", "self", ".", "title", "=", "None", "\n", "self", ".", "passage_token_ids", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data.ReaderPassage.on_deserialize": [[57, 59], ["torch.tensor"], "methods", ["None"], ["", "def", "on_deserialize", "(", "self", ")", ":", "\n", "        ", "self", ".", "sequence_ids", "=", "torch", ".", "tensor", "(", "self", ".", "sequence_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data.ReaderSample.__init__": [[66, 76], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "question", ":", "str", ",", "answers", ":", "List", ",", "q_id", ":", "str", ",", "positive_passages", ":", "List", "[", "ReaderPassage", "]", "=", "[", "]", ",", "\n", "negative_passages", ":", "List", "[", "ReaderPassage", "]", "=", "[", "]", ",", "\n", "passages", ":", "List", "[", "ReaderPassage", "]", "=", "[", "]", ",", "\n", ")", ":", "\n", "        ", "self", ".", "question", "=", "question", "\n", "self", ".", "answers", "=", "answers", "\n", "self", ".", "q_id", "=", "q_id", "\n", "self", ".", "positive_passages", "=", "positive_passages", "\n", "self", ".", "negative_passages", "=", "negative_passages", "\n", "self", ".", "passages", "=", "passages", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data.ReaderSample.on_serialize": [[77, 80], ["passage.on_serialize"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data.ReaderSample.on_serialize"], ["", "def", "on_serialize", "(", "self", ")", ":", "\n", "        ", "for", "passage", "in", "self", ".", "passages", "+", "self", ".", "positive_passages", "+", "self", ".", "negative_passages", ":", "\n", "            ", "passage", ".", "on_serialize", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data.ReaderSample.on_deserialize": [[81, 84], ["passage.on_deserialize"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data.ReaderSample.on_deserialize"], ["", "", "def", "on_deserialize", "(", "self", ")", ":", "\n", "        ", "for", "passage", "in", "self", ".", "passages", "+", "self", ".", "positive_passages", "+", "self", ".", "negative_passages", ":", "\n", "            ", "passage", ".", "on_deserialize", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data.preprocess_retriever_data": [[104, 176], ["tensorizer.get_pair_separator_ids", "logger.info", "logger.info", "reader_data._get_gold_ctx_dict", "tensorizer.text_to_tensor", "reader_data._concat_pair", "reader_data._select_reader_passages", "next", "tensorizer.text_to_tensor", "reader_data.preprocess_retriever_data.create_reader_sample_ids"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.get_pair_separator_ids", "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data._get_gold_ctx_dict", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.text_to_tensor", "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data._concat_pair", "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data._select_reader_passages", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.text_to_tensor"], ["def", "preprocess_retriever_data", "(", "samples", ":", "List", "[", "Dict", "]", ",", "gold_info_file", ":", "Optional", "[", "str", "]", ",", "tensorizer", ":", "Tensorizer", ",", "\n", "cfg", ":", "ReaderPreprocessingCfg", "=", "DEFAULT_PREPROCESSING_CFG_TRAIN", ",", "\n", "is_train_set", ":", "bool", "=", "True", ",", "\n", ")", "->", "Iterable", "[", "ReaderSample", "]", ":", "\n", "    ", "\"\"\"\n    Converts retriever results into reader training data.\n    :param samples: samples from the retriever's json file results\n    :param gold_info_file: optional path for the 'gold passages & questions' file. Required to get best results for NQ\n    :param tensorizer: Tensorizer object for text to model input tensors conversions\n    :param cfg: ReaderPreprocessingCfg object with positive and negative passage selection parameters\n    :param is_train_set: if the data should be processed as a train set\n    :return: iterable of ReaderSample objects which can be consumed by the reader model\n    \"\"\"", "\n", "sep_tensor", "=", "tensorizer", ".", "get_pair_separator_ids", "(", ")", "# separator can be a multi token", "\n", "\n", "gold_passage_map", ",", "canonical_questions", "=", "_get_gold_ctx_dict", "(", "gold_info_file", ")", "if", "gold_info_file", "else", "(", "{", "}", ",", "{", "}", ")", "\n", "\n", "no_positive_passages", "=", "0", "\n", "positives_from_gold", "=", "0", "\n", "\n", "def", "create_reader_sample_ids", "(", "sample", ":", "ReaderPassage", ",", "question", ":", "str", ")", ":", "\n", "        ", "question_and_title", "=", "tensorizer", ".", "text_to_tensor", "(", "sample", ".", "title", ",", "title", "=", "question", ",", "add_special_tokens", "=", "True", ")", "\n", "if", "sample", ".", "passage_token_ids", "is", "None", ":", "\n", "            ", "sample", ".", "passage_token_ids", "=", "tensorizer", ".", "text_to_tensor", "(", "sample", ".", "passage_text", ",", "add_special_tokens", "=", "False", ")", "\n", "\n", "", "all_concatenated", ",", "shift", "=", "_concat_pair", "(", "question_and_title", ",", "sample", ".", "passage_token_ids", ",", "\n", "tailing_sep", "=", "sep_tensor", "if", "cfg", ".", "use_tailing_sep", "else", "None", ")", "\n", "\n", "sample", ".", "sequence_ids", "=", "all_concatenated", "\n", "sample", ".", "passage_offset", "=", "shift", "\n", "assert", "shift", ">", "1", "\n", "if", "sample", ".", "has_answer", "and", "is_train_set", ":", "\n", "            ", "sample", ".", "answers_spans", "=", "[", "(", "span", "[", "0", "]", "+", "shift", ",", "span", "[", "1", "]", "+", "shift", ")", "for", "span", "in", "sample", ".", "answers_spans", "]", "\n", "", "return", "sample", "\n", "\n", "", "for", "sample", "in", "samples", ":", "\n", "        ", "question", "=", "sample", "[", "'question'", "]", "\n", "q_id", "=", "sample", "[", "'q_id'", "]", "\n", "\n", "if", "question", "in", "canonical_questions", ":", "\n", "            ", "question", "=", "canonical_questions", "[", "question", "]", "\n", "\n", "", "positive_passages", ",", "negative_passages", "=", "_select_reader_passages", "(", "sample", ",", "question", ",", "\n", "tensorizer", ",", "\n", "gold_passage_map", ",", "\n", "cfg", ".", "gold_page_only_positives", ",", "\n", "cfg", ".", "max_positives", ",", "cfg", ".", "max_negatives", ",", "\n", "cfg", ".", "min_negatives", ",", "\n", "cfg", ".", "max_retriever_passages", ",", "\n", "cfg", ".", "include_gold_passage", ",", "\n", "is_train_set", ",", "\n", ")", "\n", "# create concatenated sequence ids for each passage and adjust answer spans", "\n", "positive_passages", "=", "[", "create_reader_sample_ids", "(", "s", ",", "question", ")", "for", "s", "in", "positive_passages", "]", "\n", "negative_passages", "=", "[", "create_reader_sample_ids", "(", "s", ",", "question", ")", "for", "s", "in", "negative_passages", "]", "\n", "\n", "if", "is_train_set", "and", "len", "(", "positive_passages", ")", "==", "0", ":", "\n", "            ", "no_positive_passages", "+=", "1", "\n", "if", "cfg", ".", "skip_no_positves", ":", "\n", "                ", "continue", "\n", "\n", "", "", "if", "next", "(", "iter", "(", "ctx", "for", "ctx", "in", "positive_passages", "if", "ctx", ".", "score", "==", "-", "1", ")", ",", "None", ")", ":", "\n", "            ", "positives_from_gold", "+=", "1", "\n", "\n", "", "if", "is_train_set", ":", "\n", "            ", "yield", "ReaderSample", "(", "question", ",", "sample", "[", "'answers'", "]", ",", "q_id", ",", "positive_passages", "=", "positive_passages", ",", "\n", "negative_passages", "=", "negative_passages", ")", "\n", "", "else", ":", "\n", "            ", "yield", "ReaderSample", "(", "question", ",", "sample", "[", "'answers'", "]", ",", "q_id", ",", "passages", "=", "negative_passages", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "'no positive passages samples: %d'", ",", "no_positive_passages", ")", "\n", "logger", ".", "info", "(", "'positive passages from gold samples: %d'", ",", "positives_from_gold", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data.convert_retriever_results": [[178, 218], ["logger.info", "multiprocessing.Pool", "len", "max", "logger.info", "functools.partial", "multiprocessing.Pool.map", "logger.info", "open", "json.loads", "len", "math.ceil", "len", "serialized_files.append", "logger.info", "logger.info", "range", "range", "f.readlines", "len"], "function", ["None"], ["", "def", "convert_retriever_results", "(", "is_train_set", ":", "bool", ",", "input_file", ":", "str", ",", "out_file_prefix", ":", "str", ",", "\n", "gold_passages_file", ":", "str", ",", "\n", "tensorizer", ":", "Tensorizer", ",", "\n", "num_workers", ":", "int", "=", "8", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Converts the file with dense retriever(or any compatible file format) results into the reader input data and\n    serializes them into a set of files.\n    Conversion splits the input data into multiple chunks and processes them in parallel. Each chunk results are stored\n    in a separate file with name out_file_prefix.{number}.pkl\n    :param is_train_set: if the data should be processed for a train set (i.e. with answer span detection)\n    :param input_file: path to a json file with data to convert\n    :param out_file_prefix: output path prefix.\n    :param gold_passages_file: optional path for the 'gold passages & questions' file. Required to get best results for NQ\n    :param tensorizer: Tensorizer object for text to model input tensors conversions\n    :param num_workers: the number of parallel processes for conversion\n    :return: names of files with serialized results\n    \"\"\"", "\n", "with", "open", "(", "input_file", ",", "'r'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "samples", "=", "json", ".", "loads", "(", "\"\"", ".", "join", "(", "f", ".", "readlines", "(", ")", ")", ")", "\n", "", "logger", ".", "info", "(", "\"Loaded %d questions + retrieval results from %s\"", ",", "len", "(", "samples", ")", ",", "input_file", ")", "\n", "workers", "=", "multiprocessing", ".", "Pool", "(", "num_workers", ")", "\n", "ds_size", "=", "len", "(", "samples", ")", "\n", "step", "=", "max", "(", "math", ".", "ceil", "(", "ds_size", "/", "num_workers", ")", ",", "1", ")", "\n", "chunks", "=", "[", "samples", "[", "i", ":", "i", "+", "step", "]", "for", "i", "in", "range", "(", "0", ",", "ds_size", ",", "step", ")", "]", "\n", "chunks", "=", "[", "(", "i", ",", "chunks", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "chunks", ")", ")", "]", "\n", "\n", "logger", ".", "info", "(", "\"Split data into %d chunks\"", ",", "len", "(", "chunks", ")", ")", "\n", "\n", "processed", "=", "0", "\n", "_parse_batch", "=", "partial", "(", "_preprocess_reader_samples_chunk", ",", "out_file_prefix", "=", "out_file_prefix", ",", "\n", "gold_passages_file", "=", "gold_passages_file", ",", "tensorizer", "=", "tensorizer", ",", "\n", "is_train_set", "=", "is_train_set", ")", "\n", "serialized_files", "=", "[", "]", "\n", "for", "file_name", "in", "workers", ".", "map", "(", "_parse_batch", ",", "chunks", ")", ":", "\n", "        ", "processed", "+=", "1", "\n", "serialized_files", ".", "append", "(", "file_name", ")", "\n", "logger", ".", "info", "(", "'Chunks processed %d'", ",", "processed", ")", "\n", "logger", ".", "info", "(", "'Data saved to %s'", ",", "file_name", ")", "\n", "", "logger", ".", "info", "(", "'Preprocessed data stored in %s'", ",", "serialized_files", ")", "\n", "return", "serialized_files", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data.get_best_spans": [[220, 256], ["enumerate", "sorted", "enumerate", "any", "reader_data._extend_span_to_full_words", "tensorizer.to_string", "best_spans.append", "chosen_span_intervals.append", "sorted.append", "SpanPrediction", "len"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data._extend_span_to_full_words", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.to_string"], ["", "def", "get_best_spans", "(", "tensorizer", ":", "Tensorizer", ",", "start_logits", ":", "List", ",", "end_logits", ":", "List", ",", "ctx_ids", ":", "List", ",", "max_answer_length", ":", "int", ",", "\n", "passage_idx", ":", "int", ",", "relevance_score", ":", "float", ",", "top_spans", ":", "int", "=", "1", ")", "->", "List", "[", "SpanPrediction", "]", ":", "\n", "    ", "\"\"\"\n    Finds the best answer span for the extractive Q&A model\n    \"\"\"", "\n", "scores", "=", "[", "]", "\n", "for", "(", "i", ",", "s", ")", "in", "enumerate", "(", "start_logits", ")", ":", "\n", "        ", "for", "(", "j", ",", "e", ")", "in", "enumerate", "(", "end_logits", "[", "i", ":", "i", "+", "max_answer_length", "]", ")", ":", "\n", "            ", "scores", ".", "append", "(", "(", "(", "i", ",", "i", "+", "j", ")", ",", "s", "+", "e", ")", ")", "\n", "\n", "", "", "scores", "=", "sorted", "(", "scores", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "chosen_span_intervals", "=", "[", "]", "\n", "best_spans", "=", "[", "]", "\n", "\n", "for", "(", "start_index", ",", "end_index", ")", ",", "score", "in", "scores", ":", "\n", "        ", "assert", "start_index", "<=", "end_index", "\n", "length", "=", "end_index", "-", "start_index", "+", "1", "\n", "assert", "length", "<=", "max_answer_length", "\n", "\n", "if", "any", "(", "[", "start_index", "<=", "prev_start_index", "<=", "prev_end_index", "<=", "end_index", "or", "\n", "prev_start_index", "<=", "start_index", "<=", "end_index", "<=", "prev_end_index", "\n", "for", "(", "prev_start_index", ",", "prev_end_index", ")", "in", "chosen_span_intervals", "]", ")", ":", "\n", "            ", "continue", "\n", "\n", "# extend bpe subtokens to full tokens", "\n", "", "start_index", ",", "end_index", "=", "_extend_span_to_full_words", "(", "tensorizer", ",", "ctx_ids", ",", "\n", "(", "start_index", ",", "end_index", ")", ")", "\n", "\n", "predicted_answer", "=", "tensorizer", ".", "to_string", "(", "ctx_ids", "[", "start_index", ":", "end_index", "+", "1", "]", ")", "\n", "best_spans", ".", "append", "(", "SpanPrediction", "(", "predicted_answer", ",", "score", ",", "relevance_score", ",", "passage_idx", ",", "ctx_ids", ")", ")", "\n", "chosen_span_intervals", ".", "append", "(", "(", "start_index", ",", "end_index", ")", ")", "\n", "\n", "if", "len", "(", "chosen_span_intervals", ")", "==", "top_spans", ":", "\n", "            ", "break", "\n", "", "", "return", "best_spans", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data._select_reader_passages": [[258, 331], ["list", "tensorizer.text_to_tensor", "list", "list", "list", "filter", "next", "min", "reader_data.ReaderPassage", "filter", "filter", "filter", "list", "bool", "list", "iter", "reader_data._select_reader_passages.find_answer_spans"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.text_to_tensor"], ["", "def", "_select_reader_passages", "(", "sample", ":", "Dict", ",", "\n", "question", ":", "str", ",", "\n", "tensorizer", ":", "Tensorizer", ",", "gold_passage_map", ":", "Dict", "[", "str", ",", "ReaderPassage", "]", ",", "\n", "gold_page_only_positives", ":", "bool", ",", "\n", "max_positives", ":", "int", ",", "\n", "max1_negatives", ":", "int", ",", "\n", "max2_negatives", ":", "int", ",", "\n", "max_retriever_passages", ":", "int", ",", "\n", "include_gold_passage", ":", "bool", ",", "\n", "is_train_set", ":", "bool", "\n", ")", "->", "Tuple", "[", "List", "[", "ReaderPassage", "]", ",", "List", "[", "ReaderPassage", "]", "]", ":", "\n", "    ", "answers", "=", "sample", "[", "'answers'", "]", "\n", "\n", "ctxs", "=", "[", "ReaderPassage", "(", "**", "ctx", ")", "for", "ctx", "in", "sample", "[", "'ctxs'", "]", "]", "[", "0", ":", "max_retriever_passages", "]", "\n", "answers_token_ids", "=", "[", "tensorizer", ".", "text_to_tensor", "(", "a", ",", "add_special_tokens", "=", "False", ")", "for", "a", "in", "answers", "]", "\n", "\n", "if", "is_train_set", ":", "\n", "        ", "positive_samples", "=", "list", "(", "filter", "(", "lambda", "ctx", ":", "ctx", ".", "has_answer", ",", "ctxs", ")", ")", "\n", "negative_samples", "=", "list", "(", "filter", "(", "lambda", "ctx", ":", "not", "ctx", ".", "has_answer", ",", "ctxs", ")", ")", "\n", "", "else", ":", "\n", "        ", "positive_samples", "=", "[", "]", "\n", "negative_samples", "=", "ctxs", "\n", "\n", "", "positive_ctxs_from_gold_page", "=", "list", "(", "\n", "filter", "(", "lambda", "ctx", ":", "_is_from_gold_wiki_page", "(", "gold_passage_map", ",", "ctx", ".", "title", ",", "question", ")", ",", "\n", "positive_samples", ")", ")", "if", "gold_page_only_positives", "else", "[", "]", "\n", "\n", "def", "find_answer_spans", "(", "ctx", ":", "ReaderPassage", ")", ":", "\n", "        ", "if", "ctx", ".", "has_answer", ":", "\n", "            ", "if", "ctx", ".", "passage_token_ids", "is", "None", ":", "\n", "                ", "ctx", ".", "passage_token_ids", "=", "tensorizer", ".", "text_to_tensor", "(", "ctx", ".", "passage_text", ",", "add_special_tokens", "=", "False", ")", "\n", "\n", "", "answer_spans", "=", "[", "_find_answer_positions", "(", "ctx", ".", "passage_token_ids", ",", "answers_token_ids", "[", "i", "]", ")", "for", "i", "in", "\n", "range", "(", "len", "(", "answers", ")", ")", "]", "\n", "\n", "# flatten spans list", "\n", "answer_spans", "=", "[", "item", "for", "sublist", "in", "answer_spans", "for", "item", "in", "sublist", "]", "\n", "answers_spans", "=", "list", "(", "filter", "(", "None", ",", "answer_spans", ")", ")", "\n", "ctx", ".", "answers_spans", "=", "answers_spans", "\n", "\n", "if", "not", "answers_spans", ":", "\n", "                ", "logger", ".", "warning", "(", "'No answer found in passage id=%s text=%s, answers=%s, question=%s'", ",", "ctx", ".", "id", ",", "\n", "ctx", ".", "passage_text", ",", "\n", "answers", ",", "question", ")", "\n", "\n", "", "ctx", ".", "has_answer", "=", "bool", "(", "answers_spans", ")", "\n", "\n", "", "return", "ctx", "\n", "\n", "# check if any of the selected ctx+ has answer spans", "\n", "", "selected_positive_ctxs", "=", "list", "(", "\n", "filter", "(", "lambda", "ctx", ":", "ctx", ".", "has_answer", ",", "[", "find_answer_spans", "(", "ctx", ")", "for", "ctx", "in", "positive_ctxs_from_gold_page", "]", ")", ")", "\n", "\n", "if", "not", "selected_positive_ctxs", ":", "# fallback to positive ctx not from gold pages", "\n", "        ", "selected_positive_ctxs", "=", "list", "(", "\n", "filter", "(", "lambda", "ctx", ":", "ctx", ".", "has_answer", ",", "[", "find_answer_spans", "(", "ctx", ")", "for", "ctx", "in", "positive_samples", "]", ")", "\n", ")", "[", "0", ":", "max_positives", "]", "\n", "\n", "# optionally include gold passage itself if it is still not in the positives list", "\n", "", "if", "include_gold_passage", "and", "question", "in", "gold_passage_map", ":", "\n", "        ", "gold_passage", "=", "gold_passage_map", "[", "question", "]", "\n", "included_gold_passage", "=", "next", "(", "iter", "(", "ctx", "for", "ctx", "in", "selected_positive_ctxs", "if", "ctx", ".", "id", "==", "gold_passage", ".", "id", ")", ",", "None", ")", "\n", "if", "not", "included_gold_passage", ":", "\n", "            ", "gold_passage", "=", "find_answer_spans", "(", "gold_passage", ")", "\n", "if", "not", "gold_passage", ".", "has_answer", ":", "\n", "                ", "logger", ".", "warning", "(", "'No answer found in gold passage %s'", ",", "gold_passage", ")", "\n", "", "else", ":", "\n", "                ", "selected_positive_ctxs", ".", "append", "(", "gold_passage", ")", "\n", "\n", "", "", "", "max_negatives", "=", "min", "(", "max", "(", "10", "*", "len", "(", "selected_positive_ctxs", ")", ",", "max1_negatives", ")", ",", "\n", "max2_negatives", ")", "if", "is_train_set", "else", "DEFAULT_EVAL_PASSAGES", "\n", "negative_samples", "=", "negative_samples", "[", "0", ":", "max_negatives", "]", "\n", "return", "selected_positive_ctxs", ",", "negative_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data._find_answer_positions": [[333, 341], ["ctx_ids.size", "answer.size", "range", "answer_occurences.append"], "function", ["None"], ["", "def", "_find_answer_positions", "(", "ctx_ids", ":", "T", ",", "answer", ":", "T", ")", "->", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ":", "\n", "    ", "c_len", "=", "ctx_ids", ".", "size", "(", "0", ")", "\n", "a_len", "=", "answer", ".", "size", "(", "0", ")", "\n", "answer_occurences", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "c_len", "-", "a_len", "+", "1", ")", ":", "\n", "        ", "if", "(", "answer", "==", "ctx_ids", "[", "i", ":", "i", "+", "a_len", "]", ")", ".", "all", "(", ")", ":", "\n", "            ", "answer_occurences", ".", "append", "(", "(", "i", ",", "i", "+", "a_len", "-", "1", ")", ")", "\n", "", "", "return", "answer_occurences", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data._concat_pair": [[343, 347], ["torch.cat", "t1.size", "len"], "function", ["None"], ["", "def", "_concat_pair", "(", "t1", ":", "T", ",", "t2", ":", "T", ",", "middle_sep", ":", "T", "=", "None", ",", "tailing_sep", ":", "T", "=", "None", ")", ":", "\n", "    ", "middle", "=", "(", "[", "middle_sep", "]", "if", "middle_sep", "else", "[", "]", ")", "\n", "r", "=", "[", "t1", "]", "+", "middle", "+", "[", "t2", "]", "+", "(", "[", "tailing_sep", "]", "if", "tailing_sep", "else", "[", "]", ")", "\n", "return", "torch", ".", "cat", "(", "r", ",", "dim", "=", "0", ")", ",", "t1", ".", "size", "(", "0", ")", "+", "len", "(", "middle", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data._get_gold_ctx_dict": [[349, 380], ["open", "logger.info", "sample[].lower", "reader_data.ReaderPassage", "json.load", "logger.info", "logger.info", "logger.info", "logger.info"], "function", ["None"], ["", "def", "_get_gold_ctx_dict", "(", "file", ":", "str", ")", "->", "Tuple", "[", "Dict", "[", "str", ",", "ReaderPassage", "]", ",", "Dict", "[", "str", ",", "str", "]", "]", ":", "\n", "    ", "gold_passage_infos", "=", "{", "}", "# question|question_tokens -> ReaderPassage (with title and gold ctx)", "\n", "\n", "# original NQ dataset has 2 forms of same question - original, and tokenized.", "\n", "# Tokenized form is not fully consisted with the original question if tokenized by some encoder tokenizers", "\n", "# Specifically, this is the case for the BERT tokenizer.", "\n", "# Depending of which form was used for retriever training and results generation, it may be useful to convert", "\n", "# all questions to the canonical original representation.", "\n", "original_questions", "=", "{", "}", "# question from tokens -> original question (NQ only)", "\n", "\n", "with", "open", "(", "file", ",", "'r'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "logger", ".", "info", "(", "'Reading file %s'", "%", "file", ")", "\n", "data", "=", "json", ".", "load", "(", "f", ")", "[", "'data'", "]", "\n", "\n", "", "for", "sample", "in", "data", ":", "\n", "        ", "question", "=", "sample", "[", "'question'", "]", "\n", "question_from_tokens", "=", "sample", "[", "'question_tokens'", "]", "if", "'question_tokens'", "in", "sample", "else", "question", "\n", "original_questions", "[", "question_from_tokens", "]", "=", "question", "\n", "title", "=", "sample", "[", "'title'", "]", ".", "lower", "(", ")", "\n", "context", "=", "sample", "[", "'context'", "]", "# Note: This one is cased", "\n", "rp", "=", "ReaderPassage", "(", "sample", "[", "'example_id'", "]", ",", "text", "=", "context", ",", "title", "=", "title", ")", "\n", "if", "question", "in", "gold_passage_infos", ":", "\n", "            ", "logger", ".", "info", "(", "'Duplicate question %s'", ",", "question", ")", "\n", "rp_exist", "=", "gold_passage_infos", "[", "question", "]", "\n", "logger", ".", "info", "(", "'Duplicate question gold info: title new =%s | old title=%s'", ",", "title", ",", "rp_exist", ".", "title", ")", "\n", "logger", ".", "info", "(", "'Duplicate question gold info: new ctx =%s '", ",", "context", ")", "\n", "logger", ".", "info", "(", "'Duplicate question gold info: old ctx =%s '", ",", "rp_exist", ".", "passage_text", ")", "\n", "\n", "", "gold_passage_infos", "[", "question", "]", "=", "rp", "\n", "gold_passage_infos", "[", "question_from_tokens", "]", "=", "rp", "\n", "", "return", "gold_passage_infos", ",", "original_questions", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data._is_from_gold_wiki_page": [[382, 387], ["gold_passage_map.get", "passage_title.lower", "gold_passage_map.get.title.lower"], "function", ["None"], ["", "def", "_is_from_gold_wiki_page", "(", "gold_passage_map", ":", "Dict", "[", "str", ",", "ReaderPassage", "]", ",", "passage_title", ":", "str", ",", "question", ":", "str", ")", ":", "\n", "    ", "gold_info", "=", "gold_passage_map", ".", "get", "(", "question", ",", "None", ")", "\n", "if", "gold_info", ":", "\n", "        ", "return", "passage_title", ".", "lower", "(", ")", "==", "gold_info", ".", "title", ".", "lower", "(", ")", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data._extend_span_to_full_words": [[389, 399], ["len", "tensorizer.is_sub_word_id", "tensorizer.is_sub_word_id"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.is_sub_word_id", "home.repos.pwc.inspect_result.AkariAsai_XORQA.models.hf_models.BertTensorizer.is_sub_word_id"], ["", "def", "_extend_span_to_full_words", "(", "tensorizer", ":", "Tensorizer", ",", "tokens", ":", "List", "[", "int", "]", ",", "span", ":", "Tuple", "[", "int", ",", "int", "]", ")", "->", "Tuple", "[", "int", ",", "int", "]", ":", "\n", "    ", "start_index", ",", "end_index", "=", "span", "\n", "max_len", "=", "len", "(", "tokens", ")", "\n", "while", "start_index", ">", "0", "and", "tensorizer", ".", "is_sub_word_id", "(", "tokens", "[", "start_index", "]", ")", ":", "\n", "        ", "start_index", "-=", "1", "\n", "\n", "", "while", "end_index", "<", "max_len", "-", "1", "and", "tensorizer", ".", "is_sub_word_id", "(", "tokens", "[", "end_index", "+", "1", "]", ")", ":", "\n", "        ", "end_index", "+=", "1", "\n", "\n", "", "return", "start_index", ",", "end_index", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data._preprocess_reader_samples_chunk": [[401, 425], ["logger.info", "reader_data.preprocess_retriever_data", "tqdm.tqdm", "enumerate", "len", "r.on_serialize", "results.append", "open", "logger.info", "pickle.dump", "str", "len"], "function", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data.preprocess_retriever_data", "home.repos.pwc.inspect_result.AkariAsai_XORQA.data.reader_data.ReaderSample.on_serialize"], ["", "def", "_preprocess_reader_samples_chunk", "(", "samples", ":", "List", ",", "out_file_prefix", ":", "str", ",", "gold_passages_file", ":", "str", ",", "\n", "tensorizer", ":", "Tensorizer", ",", "\n", "is_train_set", ":", "bool", ")", "->", "str", ":", "\n", "    ", "chunk_id", ",", "samples", "=", "samples", "\n", "logger", ".", "info", "(", "'Start batch %d'", ",", "len", "(", "samples", ")", ")", "\n", "iterator", "=", "preprocess_retriever_data", "(", "\n", "samples", ",", "\n", "gold_passages_file", ",", "\n", "tensorizer", ",", "\n", "is_train_set", "=", "is_train_set", ",", "\n", ")", "\n", "\n", "results", "=", "[", "]", "\n", "\n", "iterator", "=", "tqdm", "(", "iterator", ")", "\n", "for", "i", ",", "r", "in", "enumerate", "(", "iterator", ")", ":", "\n", "        ", "r", ".", "on_serialize", "(", ")", "\n", "results", ".", "append", "(", "r", ")", "\n", "\n", "", "out_file", "=", "out_file_prefix", "+", "'.'", "+", "str", "(", "chunk_id", ")", "+", "'.pkl'", "\n", "with", "open", "(", "out_file", ",", "mode", "=", "'wb'", ")", "as", "f", ":", "\n", "        ", "logger", ".", "info", "(", "'Serialize %d results to %s'", ",", "len", "(", "results", ")", ",", "out_file", ")", "\n", "pickle", ".", "dump", "(", "results", ",", "f", ")", "\n", "", "return", "out_file", "\n", "", ""]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseIndexer.__init__": [[25, 29], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "buffer_size", ":", "int", "=", "50000", ")", ":", "\n", "        ", "self", ".", "buffer_size", "=", "buffer_size", "\n", "self", ".", "index_id_to_db_id", "=", "[", "]", "\n", "self", ".", "index", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseIndexer.index_data": [[30, 32], ["None"], "methods", ["None"], ["", "def", "index_data", "(", "self", ",", "data", ":", "List", "[", "Tuple", "[", "object", ",", "np", ".", "array", "]", "]", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseIndexer.search_knn": [[33, 35], ["None"], "methods", ["None"], ["", "def", "search_knn", "(", "self", ",", "query_vectors", ":", "np", ".", "array", ",", "top_docs", ":", "int", ")", "->", "List", "[", "Tuple", "[", "List", "[", "object", "]", ",", "List", "[", "float", "]", "]", "]", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseIndexer.serialize": [[36, 49], ["logger.info", "os.path.isdir", "faiss.write_index", "os.path.join", "os.path.join", "open", "pickle.dump"], "methods", ["None"], ["", "def", "serialize", "(", "self", ",", "file", ":", "str", ")", ":", "\n", "        ", "logger", ".", "info", "(", "'Serializing index to %s'", ",", "file", ")", "\n", "\n", "if", "os", ".", "path", ".", "isdir", "(", "file", ")", ":", "\n", "            ", "index_file", "=", "os", ".", "path", ".", "join", "(", "file", ",", "\"index.dpr\"", ")", "\n", "meta_file", "=", "os", ".", "path", ".", "join", "(", "file", ",", "\"index_meta.dpr\"", ")", "\n", "", "else", ":", "\n", "            ", "index_file", "=", "file", "+", "'.index.dpr'", "\n", "meta_file", "=", "file", "+", "'.index_meta.dpr'", "\n", "\n", "", "faiss", ".", "write_index", "(", "self", ".", "index", ",", "index_file", ")", "\n", "with", "open", "(", "meta_file", ",", "mode", "=", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "self", ".", "index_id_to_db_id", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseIndexer.deserialize_from": [[50, 67], ["logger.info", "os.path.isdir", "faiss.read_index", "logger.info", "os.path.join", "os.path.join", "type", "open", "pickle.load", "len"], "methods", ["None"], ["", "", "def", "deserialize_from", "(", "self", ",", "file", ":", "str", ")", ":", "\n", "        ", "logger", ".", "info", "(", "'Loading index from %s'", ",", "file", ")", "\n", "\n", "if", "os", ".", "path", ".", "isdir", "(", "file", ")", ":", "\n", "            ", "index_file", "=", "os", ".", "path", ".", "join", "(", "file", ",", "\"index.dpr\"", ")", "\n", "meta_file", "=", "os", ".", "path", ".", "join", "(", "file", ",", "\"index_meta.dpr\"", ")", "\n", "", "else", ":", "\n", "            ", "index_file", "=", "file", "+", "'.index.dpr'", "\n", "meta_file", "=", "file", "+", "'.index_meta.dpr'", "\n", "\n", "", "self", ".", "index", "=", "faiss", ".", "read_index", "(", "index_file", ")", "\n", "logger", ".", "info", "(", "'Loaded index of type %s and size %d'", ",", "type", "(", "self", ".", "index", ")", ",", "self", ".", "index", ".", "ntotal", ")", "\n", "\n", "with", "open", "(", "meta_file", ",", "\"rb\"", ")", "as", "reader", ":", "\n", "            ", "self", ".", "index_id_to_db_id", "=", "pickle", ".", "load", "(", "reader", ")", "\n", "", "assert", "len", "(", "\n", "self", ".", "index_id_to_db_id", ")", "==", "self", ".", "index", ".", "ntotal", ",", "'Deserialized index_id_to_db_id should match faiss index size'", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseIndexer._update_id_mapping": [[68, 70], ["faiss_indexers.DenseIndexer.index_id_to_db_id.extend"], "methods", ["None"], ["", "def", "_update_id_mapping", "(", "self", ",", "db_ids", ":", "List", ")", ":", "\n", "        ", "self", ".", "index_id_to_db_id", ".", "extend", "(", "db_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseFlatIndexer.__init__": [[74, 77], ["faiss_indexers.DenseIndexer.__init__", "faiss.IndexFlatIP"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseHNSWFlatIndexer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "vector_sz", ":", "int", ",", "buffer_size", ":", "int", "=", "50000", ")", ":", "\n", "        ", "super", "(", "DenseFlatIndexer", ",", "self", ")", ".", "__init__", "(", "buffer_size", "=", "buffer_size", ")", "\n", "self", ".", "index", "=", "faiss", ".", "IndexFlatIP", "(", "vector_sz", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseFlatIndexer.index_data": [[78, 90], ["len", "range", "len", "logger.info", "numpy.concatenate", "faiss_indexers.DenseFlatIndexer._update_id_mapping", "faiss_indexers.DenseFlatIndexer.index.add", "numpy.reshape"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseIndexer._update_id_mapping"], ["", "def", "index_data", "(", "self", ",", "data", ":", "List", "[", "Tuple", "[", "object", ",", "np", ".", "array", "]", "]", ")", ":", "\n", "        ", "n", "=", "len", "(", "data", ")", "\n", "# indexing in batches is beneficial for many faiss index types", "\n", "for", "i", "in", "range", "(", "0", ",", "n", ",", "self", ".", "buffer_size", ")", ":", "\n", "            ", "db_ids", "=", "[", "t", "[", "0", "]", "for", "t", "in", "data", "[", "i", ":", "i", "+", "self", ".", "buffer_size", "]", "]", "\n", "vectors", "=", "[", "np", ".", "reshape", "(", "t", "[", "1", "]", ",", "(", "1", ",", "-", "1", ")", ")", "for", "t", "in", "data", "[", "i", ":", "i", "+", "self", ".", "buffer_size", "]", "]", "\n", "vectors", "=", "np", ".", "concatenate", "(", "vectors", ",", "axis", "=", "0", ")", "\n", "self", ".", "_update_id_mapping", "(", "db_ids", ")", "\n", "self", ".", "index", ".", "add", "(", "vectors", ")", "\n", "\n", "", "indexed_cnt", "=", "len", "(", "self", ".", "index_id_to_db_id", ")", "\n", "logger", ".", "info", "(", "'Total data indexed %d'", ",", "indexed_cnt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseFlatIndexer.search_knn": [[91, 97], ["faiss_indexers.DenseFlatIndexer.index.search", "range", "len"], "methods", ["None"], ["", "def", "search_knn", "(", "self", ",", "query_vectors", ":", "np", ".", "array", ",", "top_docs", ":", "int", ")", "->", "List", "[", "Tuple", "[", "List", "[", "object", "]", ",", "List", "[", "float", "]", "]", "]", ":", "\n", "        ", "scores", ",", "indexes", "=", "self", ".", "index", ".", "search", "(", "query_vectors", ",", "top_docs", ")", "\n", "# convert to external ids", "\n", "db_ids", "=", "[", "[", "self", ".", "index_id_to_db_id", "[", "i", "]", "for", "i", "in", "query_top_idxs", "]", "for", "query_top_idxs", "in", "indexes", "]", "\n", "result", "=", "[", "(", "db_ids", "[", "i", "]", ",", "scores", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "db_ids", ")", ")", "]", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseHNSWFlatIndexer.__init__": [[104, 115], ["faiss_indexers.DenseIndexer.__init__", "faiss.IndexHNSWFlat"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseHNSWFlatIndexer.__init__"], ["def", "__init__", "(", "self", ",", "vector_sz", ":", "int", ",", "buffer_size", ":", "int", "=", "50000", ",", "store_n", ":", "int", "=", "512", "\n", ",", "ef_search", ":", "int", "=", "128", ",", "ef_construction", ":", "int", "=", "200", ")", ":", "\n", "        ", "super", "(", "DenseHNSWFlatIndexer", ",", "self", ")", ".", "__init__", "(", "buffer_size", "=", "buffer_size", ")", "\n", "\n", "# IndexHNSWFlat supports L2 similarity only", "\n", "# so we have to apply DOT -> L2 similairy space conversion with the help of an extra dimension", "\n", "index", "=", "faiss", ".", "IndexHNSWFlat", "(", "vector_sz", "+", "1", ",", "store_n", ")", "\n", "index", ".", "hnsw", ".", "efSearch", "=", "ef_search", "\n", "index", ".", "hnsw", ".", "efConstruction", "=", "ef_construction", "\n", "self", ".", "index", "=", "index", "\n", "self", ".", "phi", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseHNSWFlatIndexer.index_data": [[116, 148], ["len", "enumerate", "logger.info", "range", "len", "logger.info", "RuntimeError", "max", "numpy.concatenate", "faiss_indexers.DenseHNSWFlatIndexer._update_id_mapping", "faiss_indexers.DenseHNSWFlatIndexer.index.add", "logger.info", "numpy.reshape", "numpy.sqrt", "numpy.hstack", "len", "enumerate", "aux_dims[].reshape"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseIndexer._update_id_mapping"], ["", "def", "index_data", "(", "self", ",", "data", ":", "List", "[", "Tuple", "[", "object", ",", "np", ".", "array", "]", "]", ")", ":", "\n", "        ", "n", "=", "len", "(", "data", ")", "\n", "\n", "# max norm is required before putting all vectors in the index to convert inner product similarity to L2", "\n", "if", "self", ".", "phi", ">", "0", ":", "\n", "            ", "raise", "RuntimeError", "(", "'DPR HNSWF index needs to index all data at once,'", "\n", "'results will be unpredictable otherwise.'", ")", "\n", "", "phi", "=", "0", "\n", "for", "i", ",", "item", "in", "enumerate", "(", "data", ")", ":", "\n", "            ", "id", ",", "doc_vector", "=", "item", "\n", "norms", "=", "(", "doc_vector", "**", "2", ")", ".", "sum", "(", ")", "\n", "phi", "=", "max", "(", "phi", ",", "norms", ")", "\n", "", "logger", ".", "info", "(", "'HNSWF DotProduct -> L2 space phi={}'", ".", "format", "(", "phi", ")", ")", "\n", "self", ".", "phi", "=", "0", "\n", "\n", "# indexing in batches is beneficial for many faiss index types", "\n", "for", "i", "in", "range", "(", "0", ",", "n", ",", "self", ".", "buffer_size", ")", ":", "\n", "            ", "db_ids", "=", "[", "t", "[", "0", "]", "for", "t", "in", "data", "[", "i", ":", "i", "+", "self", ".", "buffer_size", "]", "]", "\n", "vectors", "=", "[", "np", ".", "reshape", "(", "t", "[", "1", "]", ",", "(", "1", ",", "-", "1", ")", ")", "for", "t", "in", "data", "[", "i", ":", "i", "+", "self", ".", "buffer_size", "]", "]", "\n", "\n", "norms", "=", "[", "(", "doc_vector", "**", "2", ")", ".", "sum", "(", ")", "for", "doc_vector", "in", "vectors", "]", "\n", "aux_dims", "=", "[", "np", ".", "sqrt", "(", "phi", "-", "norm", ")", "for", "norm", "in", "norms", "]", "\n", "hnsw_vectors", "=", "[", "np", ".", "hstack", "(", "(", "doc_vector", ",", "aux_dims", "[", "i", "]", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ")", "for", "i", ",", "doc_vector", "in", "\n", "enumerate", "(", "vectors", ")", "]", "\n", "hnsw_vectors", "=", "np", ".", "concatenate", "(", "hnsw_vectors", ",", "axis", "=", "0", ")", "\n", "\n", "self", ".", "_update_id_mapping", "(", "db_ids", ")", "\n", "self", ".", "index", ".", "add", "(", "hnsw_vectors", ")", "\n", "logger", ".", "info", "(", "'data indexed %d'", ",", "len", "(", "self", ".", "index_id_to_db_id", ")", ")", "\n", "\n", "", "indexed_cnt", "=", "len", "(", "self", ".", "index_id_to_db_id", ")", "\n", "logger", ".", "info", "(", "'Total data indexed %d'", ",", "indexed_cnt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseHNSWFlatIndexer.search_knn": [[149, 159], ["numpy.zeros", "numpy.hstack", "logger.info", "faiss_indexers.DenseHNSWFlatIndexer.index.search", "len", "numpy.zeros.reshape", "range", "len"], "methods", ["None"], ["", "def", "search_knn", "(", "self", ",", "query_vectors", ":", "np", ".", "array", ",", "top_docs", ":", "int", ")", "->", "List", "[", "Tuple", "[", "List", "[", "object", "]", ",", "List", "[", "float", "]", "]", "]", ":", "\n", "\n", "        ", "aux_dim", "=", "np", ".", "zeros", "(", "len", "(", "query_vectors", ")", ",", "dtype", "=", "'float32'", ")", "\n", "query_nhsw_vectors", "=", "np", ".", "hstack", "(", "(", "query_vectors", ",", "aux_dim", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ")", "\n", "logger", ".", "info", "(", "'query_hnsw_vectors %s'", ",", "query_nhsw_vectors", ".", "shape", ")", "\n", "scores", ",", "indexes", "=", "self", ".", "index", ".", "search", "(", "query_nhsw_vectors", ",", "top_docs", ")", "\n", "# convert to external ids", "\n", "db_ids", "=", "[", "[", "self", ".", "index_id_to_db_id", "[", "i", "]", "for", "i", "in", "query_top_idxs", "]", "for", "query_top_idxs", "in", "indexes", "]", "\n", "result", "=", "[", "(", "db_ids", "[", "i", "]", ",", "scores", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "db_ids", ")", ")", "]", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseHNSWFlatIndexer.deserialize_from": [[160, 164], ["faiss_indexers.DenseIndexer.deserialize_from"], "methods", ["home.repos.pwc.inspect_result.AkariAsai_XORQA.indexer.faiss_indexers.DenseHNSWFlatIndexer.deserialize_from"], ["", "def", "deserialize_from", "(", "self", ",", "file", ":", "str", ")", ":", "\n", "        ", "super", "(", "DenseHNSWFlatIndexer", ",", "self", ")", ".", "deserialize_from", "(", "file", ")", "\n", "# to trigger warning on subsequent indexing", "\n", "self", ".", "phi", "=", "1", "\n", "", "", ""]]}