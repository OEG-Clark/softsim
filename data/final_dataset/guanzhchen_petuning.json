{"home.repos.pwc.inspect_result.guanzhchen_petuning.None.arguments.get_args": [[228, 235], ["transformers.HfArgumentParser", "transformers.HfArgumentParser.parse_args_into_dataclasses"], "function", ["None"], ["", "def", "get_args", "(", ")", ":", "\n", "    ", "\"\"\"Parse all the args.\"\"\"", "\n", "parser", "=", "HfArgumentParser", "(", "(", "ModelArguments", ",", "DataTrainingArguments", ",", "TrainingArguments", ",", "QuestionAnwseringArguments", ",", "AdapterArguments", ")", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args_into_dataclasses", "(", ")", "\n", "\n", "return", "args", "", "", ""]], "home.repos.pwc.inspect_result.guanzhchen_petuning.None.run.train": [[21, 32], ["trainer.train", "trainer.save_model"], "function", ["home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.train"], ["def", "train", "(", "trainer", ",", "resume_from_checkpoint", "=", "None", ",", "last_checkpoint", "=", "None", ")", ":", "\n", "    ", "checkpoint", "=", "None", "\n", "if", "resume_from_checkpoint", "is", "not", "None", ":", "\n", "        ", "checkpoint", "=", "resume_from_checkpoint", "\n", "", "elif", "last_checkpoint", "is", "not", "None", ":", "\n", "        ", "checkpoint", "=", "last_checkpoint", "\n", "", "train_result", "=", "trainer", ".", "train", "(", "resume_from_checkpoint", "=", "checkpoint", ")", "\n", "# trainer.save_model()", "\n", "\n", "# metrics = train_result.metrics", "\n", "trainer", ".", "save_model", "(", ")", "\n", "# trainer.log_metrics(\"train\", metrics)", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.None.run.evaluate": [[38, 44], ["logger.info", "trainer.evaluate", "trainer.log_metrics", "trainer.save_metrics"], "function", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.record_evaluation.evaluate"], ["", "def", "evaluate", "(", "trainer", ")", ":", "\n", "    ", "logger", ".", "info", "(", "\"*** Evaluate ***\"", ")", "\n", "metrics", "=", "trainer", ".", "evaluate", "(", ")", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"eval\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"eval\"", ",", "metrics", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.None.run.predict": [[45, 51], ["logger.info", "trainer.predict", "trainer.log_metrics", "trainer.save_metrics"], "function", ["home.repos.pwc.inspect_result.guanzhchen_petuning.None.run.predict"], ["", "def", "predict", "(", "trainer", ",", "predict_dataset", "=", "None", ")", ":", "\n", "    ", "logger", ".", "info", "(", "\"*** Predict ***\"", ")", "\n", "predictions", "=", "trainer", ".", "predict", "(", "predict_dataset", ",", "metric_key_prefix", "=", "\"predict\"", ")", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"predict\"", ",", "predictions", ".", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"predict\"", ",", "predictions", ".", "metrics", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.sequence_classification.RobertaPrefixForSequenceClassification.__init__": [[24, 53], ["transformers.adapters.model_mixin.ModelWithHeadsAdaptersMixin.__init__", "model.roberta.RobertaModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "sequence_classification.RobertaClassificationHead", "sequence_classification.RobertaPrefixForSequenceClassification.init_weights", "sequence_classification.RobertaPrefixForSequenceClassification.roberta.parameters", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "model.prefix_encoder.PrefixEncoder", "sequence_classification.RobertaPrefixForSequenceClassification.roberta.named_parameters", "sequence_classification.RobertaPrefixForSequenceClassification.named_parameters", "print", "param.numel", "param.numel", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "roberta", "=", "RobertaModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "# self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)", "\n", "self", ".", "classifier", "=", "RobertaClassificationHead", "(", "config", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n", "for", "param", "in", "self", ".", "roberta", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "self", ".", "pre_seq_len", "=", "config", ".", "pre_seq_len", "\n", "self", ".", "n_layer", "=", "config", ".", "num_hidden_layers", "\n", "self", ".", "n_head", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "n_embd", "=", "config", ".", "hidden_size", "//", "config", ".", "num_attention_heads", "\n", "\n", "self", ".", "prefix_tokens", "=", "torch", ".", "arange", "(", "self", ".", "pre_seq_len", ")", ".", "long", "(", ")", "\n", "self", ".", "prefix_encoder", "=", "PrefixEncoder", "(", "config", ")", "\n", "\n", "bert_param", "=", "0", "\n", "for", "name", ",", "param", "in", "self", ".", "roberta", ".", "named_parameters", "(", ")", ":", "\n", "            ", "bert_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "all_param", "=", "0", "\n", "for", "name", ",", "param", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "all_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "total_param", "=", "all_param", "-", "bert_param", "\n", "print", "(", "'total param is {}'", ".", "format", "(", "total_param", ")", ")", "# 9860105", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.sequence_classification.RobertaPrefixForSequenceClassification.get_prompt": [[55, 68], ["sequence_classification.RobertaPrefixForSequenceClassification.prefix_tokens.unsqueeze().expand().to", "sequence_classification.RobertaPrefixForSequenceClassification.prefix_encoder", "past_key_values.permute().split.permute().split.view", "sequence_classification.RobertaPrefixForSequenceClassification.dropout", "past_key_values.permute().split.permute().split.permute().split", "sequence_classification.RobertaPrefixForSequenceClassification.prefix_tokens.unsqueeze().expand", "past_key_values.permute().split.permute().split.permute", "sequence_classification.RobertaPrefixForSequenceClassification.prefix_tokens.unsqueeze"], "methods", ["None"], ["", "def", "get_prompt", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "prefix_tokens", "=", "self", ".", "prefix_tokens", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "batch_size", ",", "-", "1", ")", ".", "to", "(", "self", ".", "roberta", ".", "device", ")", "\n", "past_key_values", "=", "self", ".", "prefix_encoder", "(", "prefix_tokens", ")", "\n", "past_key_values", "=", "past_key_values", ".", "view", "(", "\n", "batch_size", ",", "\n", "self", ".", "pre_seq_len", ",", "\n", "self", ".", "n_layer", "*", "2", ",", "\n", "self", ".", "n_head", ",", "\n", "self", ".", "n_embd", "\n", ")", "\n", "past_key_values", "=", "self", ".", "dropout", "(", "past_key_values", ")", "\n", "past_key_values", "=", "past_key_values", ".", "permute", "(", "[", "2", ",", "0", ",", "3", ",", "1", ",", "4", "]", ")", ".", "split", "(", "2", ")", "\n", "return", "past_key_values", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.sequence_classification.RobertaPrefixForSequenceClassification.forward": [[69, 145], ["sequence_classification.RobertaPrefixForSequenceClassification.get_prompt", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "sequence_classification.RobertaPrefixForSequenceClassification.roberta", "sequence_classification.RobertaPrefixForSequenceClassification.classifier", "transformers.modeling_outputs.SequenceClassifierOutput", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "sequence_classification.RobertaPrefixForSequenceClassification.squeeze", "labels.squeeze", "sequence_classification.RobertaPrefixForSequenceClassification.view", "labels.view", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss."], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.RobertaModelWithHeads.get_prompt"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "adapter_names", "=", "None", ",", "\n", "head", "=", "None", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "batch_size", "=", "input_ids", ".", "shape", "[", "0", "]", "\n", "past_key_values", "=", "self", ".", "get_prompt", "(", "batch_size", "=", "batch_size", ")", "\n", "prefix_attention_mask", "=", "torch", ".", "ones", "(", "batch_size", ",", "self", ".", "pre_seq_len", ")", ".", "to", "(", "self", ".", "roberta", ".", "device", ")", "\n", "attention_mask", "=", "torch", ".", "cat", "(", "(", "prefix_attention_mask", ",", "attention_mask", ")", ",", "dim", "=", "1", ")", "\n", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "adapter_names", "=", "adapter_names", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", ")", "\n", "\n", "# pooled_output = outputs[1]", "\n", "\n", "# pooled_output = self.dropout(pooled_output)", "\n", "# logits = self.classifier(pooled_output)", "\n", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "config", ".", "problem_type", "is", "None", ":", "\n", "                ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"regression\"", "\n", "", "elif", "self", ".", "num_labels", ">", "1", "and", "(", "labels", ".", "dtype", "==", "torch", ".", "long", "or", "labels", ".", "dtype", "==", "torch", ".", "int", ")", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"single_label_classification\"", "\n", "", "else", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"multi_label_classification\"", "\n", "\n", "", "", "if", "self", ".", "config", ".", "problem_type", "==", "\"regression\"", ":", "\n", "                ", "loss_fct", "=", "MSELoss", "(", ")", "\n", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                    ", "loss", "=", "loss_fct", "(", "logits", ".", "squeeze", "(", ")", ",", "labels", ".", "squeeze", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "loss", "=", "loss_fct", "(", "logits", ",", "labels", ")", "\n", "", "", "elif", "self", ".", "config", ".", "problem_type", "==", "\"single_label_classification\"", ":", "\n", "                ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "elif", "self", ".", "config", ".", "problem_type", "==", "\"multi_label_classification\"", ":", "\n", "                ", "loss_fct", "=", "BCEWithLogitsLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ",", "labels", ")", "\n", "", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "SequenceClassifierOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.sequence_classification.RobertaClassificationHead.__init__": [[155, 163], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "classifier_dropout", "=", "(", "\n", "config", ".", "classifier_dropout", "if", "config", ".", "classifier_dropout", "is", "not", "None", "else", "config", ".", "hidden_dropout_prob", "\n", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "classifier_dropout", ")", "\n", "self", ".", "out_proj", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.sequence_classification.RobertaClassificationHead.forward": [[164, 172], ["sequence_classification.RobertaClassificationHead.dropout", "sequence_classification.RobertaClassificationHead.dense", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "sequence_classification.RobertaClassificationHead.dropout", "sequence_classification.RobertaClassificationHead.out_proj"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ",", "**", "kwargs", ")", ":", "\n", "        ", "x", "=", "features", "[", ":", ",", "0", ",", ":", "]", "# take <s> token (equiv. to [CLS])", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "x", "=", "self", ".", "dense", "(", "x", ")", "\n", "x", "=", "torch", ".", "tanh", "(", "x", ")", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "x", "=", "self", ".", "out_proj", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.sequence_classification.RobertaLoraForSequenceClassification.__init__": [[175, 197], ["transformers.adapters.model_mixin.ModelWithHeadsAdaptersMixin.__init__", "model.roberta.RobertaModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "sequence_classification.RobertaClassificationHead", "sequence_classification.RobertaLoraForSequenceClassification.init_weights", "sequence_classification.RobertaLoraForSequenceClassification.roberta.named_parameters", "sequence_classification.RobertaLoraForSequenceClassification.roberta.named_parameters", "sequence_classification.RobertaLoraForSequenceClassification.named_parameters", "print", "param.numel", "param.numel", "name.lower"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "roberta", "=", "RobertaModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "# self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)", "\n", "self", ".", "classifier", "=", "RobertaClassificationHead", "(", "config", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n", "for", "name", ",", "param", "in", "self", ".", "roberta", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "\"lora\"", "not", "in", "name", ".", "lower", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "bert_param", "=", "0", "\n", "for", "name", ",", "param", "in", "self", ".", "roberta", ".", "named_parameters", "(", ")", ":", "\n", "            ", "bert_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "all_param", "=", "0", "\n", "for", "name", ",", "param", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "all_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "total_param", "=", "all_param", "-", "bert_param", "\n", "print", "(", "'total param is {}'", ".", "format", "(", "total_param", ")", ")", "# 9860105", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.sequence_classification.RobertaLoraForSequenceClassification.forward": [[200, 265], ["sequence_classification.RobertaLoraForSequenceClassification.roberta", "sequence_classification.RobertaLoraForSequenceClassification.classifier", "transformers.modeling_outputs.SequenceClassifierOutput", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "sequence_classification.RobertaLoraForSequenceClassification.squeeze", "labels.squeeze", "sequence_classification.RobertaLoraForSequenceClassification.view", "labels.view", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss."], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "adapter_names", "=", "None", ",", "\n", "head", "=", "None", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "adapter_names", "=", "adapter_names", "\n", ")", "\n", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "config", ".", "problem_type", "is", "None", ":", "\n", "                ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"regression\"", "\n", "", "elif", "self", ".", "num_labels", ">", "1", "and", "(", "labels", ".", "dtype", "==", "torch", ".", "long", "or", "labels", ".", "dtype", "==", "torch", ".", "int", ")", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"single_label_classification\"", "\n", "", "else", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"multi_label_classification\"", "\n", "\n", "", "", "if", "self", ".", "config", ".", "problem_type", "==", "\"regression\"", ":", "\n", "                ", "loss_fct", "=", "MSELoss", "(", ")", "\n", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                    ", "loss", "=", "loss_fct", "(", "logits", ".", "squeeze", "(", ")", ",", "labels", ".", "squeeze", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "loss", "=", "loss_fct", "(", "logits", ",", "labels", ")", "\n", "", "", "elif", "self", ".", "config", ".", "problem_type", "==", "\"single_label_classification\"", ":", "\n", "                ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "elif", "self", ".", "config", ".", "problem_type", "==", "\"multi_label_classification\"", ":", "\n", "                ", "loss_fct", "=", "BCEWithLogitsLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ",", "labels", ")", "\n", "", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "SequenceClassifierOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", ""]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.multiple_choice.RobertaPrefixForMultipleChoice.__init__": [[21, 50], ["transformers.adapters.model_mixin.ModelWithHeadsAdaptersMixin.__init__", "model.roberta.RobertaModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "multiple_choice.RobertaPrefixForMultipleChoice.init_weights", "multiple_choice.RobertaPrefixForMultipleChoice.roberta.parameters", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "model.prefix_encoder.PrefixEncoder", "multiple_choice.RobertaPrefixForMultipleChoice.roberta.named_parameters", "multiple_choice.RobertaPrefixForMultipleChoice.named_parameters", "print", "param.numel", "param.numel", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.init_weights"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "roberta", "=", "RobertaModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "torch", ".", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n", "\n", "for", "param", "in", "self", ".", "roberta", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "self", ".", "pre_seq_len", "=", "config", ".", "pre_seq_len", "\n", "self", ".", "n_layer", "=", "config", ".", "num_hidden_layers", "\n", "self", ".", "n_head", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "n_embd", "=", "config", ".", "hidden_size", "//", "config", ".", "num_attention_heads", "\n", "\n", "self", ".", "prefix_tokens", "=", "torch", ".", "arange", "(", "self", ".", "pre_seq_len", ")", ".", "long", "(", ")", "\n", "self", ".", "prefix_encoder", "=", "PrefixEncoder", "(", "config", ")", "\n", "\n", "bert_param", "=", "0", "\n", "for", "name", ",", "param", "in", "self", ".", "roberta", ".", "named_parameters", "(", ")", ":", "\n", "            ", "bert_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "all_param", "=", "0", "\n", "for", "name", ",", "param", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "all_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "total_param", "=", "all_param", "-", "bert_param", "\n", "print", "(", "'total param is {}'", ".", "format", "(", "total_param", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.multiple_choice.RobertaPrefixForMultipleChoice.get_prompt": [[51, 64], ["multiple_choice.RobertaPrefixForMultipleChoice.prefix_tokens.unsqueeze().expand().to", "multiple_choice.RobertaPrefixForMultipleChoice.prefix_encoder", "past_key_values.permute().split.permute().split.view", "multiple_choice.RobertaPrefixForMultipleChoice.dropout", "past_key_values.permute().split.permute().split.permute().split", "multiple_choice.RobertaPrefixForMultipleChoice.prefix_tokens.unsqueeze().expand", "past_key_values.permute().split.permute().split.permute", "multiple_choice.RobertaPrefixForMultipleChoice.prefix_tokens.unsqueeze"], "methods", ["None"], ["", "def", "get_prompt", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "prefix_tokens", "=", "self", ".", "prefix_tokens", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "batch_size", ",", "-", "1", ")", ".", "to", "(", "self", ".", "roberta", ".", "device", ")", "\n", "past_key_values", "=", "self", ".", "prefix_encoder", "(", "prefix_tokens", ")", "\n", "past_key_values", "=", "past_key_values", ".", "view", "(", "\n", "batch_size", ",", "\n", "self", ".", "pre_seq_len", ",", "\n", "self", ".", "n_layer", "*", "2", ",", "\n", "self", ".", "n_head", ",", "\n", "self", ".", "n_embd", "\n", ")", "\n", "past_key_values", "=", "self", ".", "dropout", "(", "past_key_values", ")", "\n", "past_key_values", "=", "past_key_values", ".", "permute", "(", "[", "2", ",", "0", ",", "3", ",", "1", ",", "4", "]", ")", ".", "split", "(", "2", ")", "\n", "return", "past_key_values", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.multiple_choice.RobertaPrefixForMultipleChoice.forward": [[65, 137], ["multiple_choice.RobertaPrefixForMultipleChoice.get_prompt", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "multiple_choice.RobertaPrefixForMultipleChoice.roberta", "multiple_choice.RobertaPrefixForMultipleChoice.dropout", "multiple_choice.RobertaPrefixForMultipleChoice.classifier", "multiple_choice.RobertaPrefixForMultipleChoice.view", "transformers.modeling_outputs.MultipleChoiceModelOutput", "input_ids.view", "position_ids.view", "token_type_ids.view", "attention_mask.view", "inputs_embeds.view", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "input_ids.size", "position_ids.size", "token_type_ids.size", "attention_mask.size", "inputs_embeds.size", "inputs_embeds.size", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.RobertaModelWithHeads.get_prompt"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "adapter_names", "=", "None", ",", "\n", "head", "=", "None", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n            :obj:`input_ids` above)\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "batch_size", ",", "num_choices", "=", "input_ids", ".", "shape", "[", ":", "2", "]", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "shape", "[", ":", "2", "]", "\n", "\n", "flat_input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_ids", ".", "size", "(", "-", "1", ")", ")", "if", "input_ids", "is", "not", "None", "else", "None", "\n", "flat_position_ids", "=", "position_ids", ".", "view", "(", "-", "1", ",", "position_ids", ".", "size", "(", "-", "1", ")", ")", "if", "position_ids", "is", "not", "None", "else", "None", "\n", "flat_token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "token_type_ids", ".", "size", "(", "-", "1", ")", ")", "if", "token_type_ids", "is", "not", "None", "else", "None", "\n", "flat_attention_mask", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "attention_mask", ".", "size", "(", "-", "1", ")", ")", "if", "attention_mask", "is", "not", "None", "else", "None", "\n", "flat_inputs_embeds", "=", "(", "\n", "inputs_embeds", ".", "view", "(", "-", "1", ",", "inputs_embeds", ".", "size", "(", "-", "2", ")", ",", "inputs_embeds", ".", "size", "(", "-", "1", ")", ")", "\n", "if", "inputs_embeds", "is", "not", "None", "\n", "else", "None", "\n", ")", "\n", "\n", "past_key_values", "=", "self", ".", "get_prompt", "(", "batch_size", "=", "batch_size", "*", "num_choices", ")", "\n", "prefix_attention_mask", "=", "torch", ".", "ones", "(", "batch_size", "*", "num_choices", ",", "self", ".", "pre_seq_len", ")", ".", "to", "(", "self", ".", "roberta", ".", "device", ")", "\n", "flat_attention_mask", "=", "torch", ".", "cat", "(", "(", "prefix_attention_mask", ",", "flat_attention_mask", ")", ",", "dim", "=", "1", ")", "\n", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "flat_input_ids", ",", "\n", "position_ids", "=", "flat_position_ids", ",", "\n", "token_type_ids", "=", "flat_token_type_ids", ",", "\n", "attention_mask", "=", "flat_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "flat_inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "adapter_names", "=", "adapter_names", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", ")", "\n", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "reshaped_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "num_choices", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "reshaped_logits", ",", "labels", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "reshaped_logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "MultipleChoiceModelOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "reshaped_logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.multiple_choice.RobertaLoraForMultipleChoice.__init__": [[147, 170], ["transformers.adapters.model_mixin.ModelWithHeadsAdaptersMixin.__init__", "model.roberta.RobertaModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "multiple_choice.RobertaLoraForMultipleChoice.init_weights", "multiple_choice.RobertaLoraForMultipleChoice.roberta.named_parameters", "multiple_choice.RobertaLoraForMultipleChoice.roberta.named_parameters", "multiple_choice.RobertaLoraForMultipleChoice.named_parameters", "print", "param.numel", "param.numel", "name.lower"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.init_weights"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "roberta", "=", "RobertaModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "torch", ".", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n", "\n", "for", "name", ",", "param", "in", "self", ".", "roberta", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "\"lora\"", "not", "in", "name", ".", "lower", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "\n", "", "", "bert_param", "=", "0", "\n", "for", "name", ",", "param", "in", "self", ".", "roberta", ".", "named_parameters", "(", ")", ":", "\n", "            ", "bert_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "all_param", "=", "0", "\n", "for", "name", ",", "param", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "all_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "total_param", "=", "all_param", "-", "bert_param", "\n", "print", "(", "'total param is {}'", ".", "format", "(", "total_param", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.multiple_choice.RobertaLoraForMultipleChoice.forward": [[171, 238], ["multiple_choice.RobertaLoraForMultipleChoice.roberta", "multiple_choice.RobertaLoraForMultipleChoice.dropout", "multiple_choice.RobertaLoraForMultipleChoice.classifier", "multiple_choice.RobertaLoraForMultipleChoice.view", "transformers.modeling_outputs.MultipleChoiceModelOutput", "input_ids.view", "position_ids.view", "token_type_ids.view", "attention_mask.view", "inputs_embeds.view", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "input_ids.size", "position_ids.size", "token_type_ids.size", "attention_mask.size", "inputs_embeds.size", "inputs_embeds.size"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "adapter_names", "=", "None", ",", "\n", "head", "=", "None", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n            :obj:`input_ids` above)\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "batch_size", ",", "num_choices", "=", "input_ids", ".", "shape", "[", ":", "2", "]", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "shape", "[", ":", "2", "]", "\n", "\n", "flat_input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_ids", ".", "size", "(", "-", "1", ")", ")", "if", "input_ids", "is", "not", "None", "else", "None", "\n", "flat_position_ids", "=", "position_ids", ".", "view", "(", "-", "1", ",", "position_ids", ".", "size", "(", "-", "1", ")", ")", "if", "position_ids", "is", "not", "None", "else", "None", "\n", "flat_token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "token_type_ids", ".", "size", "(", "-", "1", ")", ")", "if", "token_type_ids", "is", "not", "None", "else", "None", "\n", "flat_attention_mask", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "attention_mask", ".", "size", "(", "-", "1", ")", ")", "if", "attention_mask", "is", "not", "None", "else", "None", "\n", "flat_inputs_embeds", "=", "(", "\n", "inputs_embeds", ".", "view", "(", "-", "1", ",", "inputs_embeds", ".", "size", "(", "-", "2", ")", ",", "inputs_embeds", ".", "size", "(", "-", "1", ")", ")", "\n", "if", "inputs_embeds", "is", "not", "None", "\n", "else", "None", "\n", ")", "\n", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "flat_input_ids", ",", "\n", "position_ids", "=", "flat_position_ids", ",", "\n", "token_type_ids", "=", "flat_token_type_ids", ",", "\n", "attention_mask", "=", "flat_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "flat_inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "adapter_names", "=", "adapter_names", ",", "\n", ")", "\n", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "reshaped_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "num_choices", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "reshaped_logits", ",", "labels", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "reshaped_logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "MultipleChoiceModelOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "reshaped_logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", ""]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.BertForSequenceClassification.__init__": [[19, 29], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "prompt.BertForSequenceClassification.init_weights"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "torch", ".", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.BertForSequenceClassification.forward": [[30, 99], ["prompt.BertForSequenceClassification.bert", "prompt.BertForSequenceClassification.dropout", "prompt.BertForSequenceClassification.classifier", "transformers.modeling_outputs.SequenceClassifierOutput", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "prompt.BertForSequenceClassification.squeeze", "labels.squeeze", "prompt.BertForSequenceClassification.view", "labels.view", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss."], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "outputs", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "config", ".", "problem_type", "is", "None", ":", "\n", "                ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"regression\"", "\n", "", "elif", "self", ".", "num_labels", ">", "1", "and", "(", "labels", ".", "dtype", "==", "torch", ".", "long", "or", "labels", ".", "dtype", "==", "torch", ".", "int", ")", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"single_label_classification\"", "\n", "", "else", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"multi_label_classification\"", "\n", "\n", "", "", "if", "self", ".", "config", ".", "problem_type", "==", "\"regression\"", ":", "\n", "                ", "loss_fct", "=", "MSELoss", "(", ")", "\n", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                    ", "loss", "=", "loss_fct", "(", "logits", ".", "squeeze", "(", ")", ",", "labels", ".", "squeeze", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "loss", "=", "loss_fct", "(", "logits", ",", "labels", ")", "\n", "", "", "elif", "self", ".", "config", ".", "problem_type", "==", "\"single_label_classification\"", ":", "\n", "                ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "elif", "self", ".", "config", ".", "problem_type", "==", "\"multi_label_classification\"", ":", "\n", "                ", "loss_fct", "=", "BCEWithLogitsLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ",", "labels", ")", "\n", "", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "SequenceClassifierOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.BertPrefixForSequenceClassification.__init__": [[103, 130], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "prompt.BertPrefixForSequenceClassification.bert.parameters", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "model.prefix_encoder.PrefixEncoder", "prompt.BertPrefixForSequenceClassification.bert.named_parameters", "prompt.BertPrefixForSequenceClassification.named_parameters", "print", "param.numel", "param.numel", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "torch", ".", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "\n", "for", "param", "in", "self", ".", "bert", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "self", ".", "pre_seq_len", "=", "config", ".", "pre_seq_len", "\n", "self", ".", "n_layer", "=", "config", ".", "num_hidden_layers", "\n", "self", ".", "n_head", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "n_embd", "=", "config", ".", "hidden_size", "//", "config", ".", "num_attention_heads", "\n", "\n", "self", ".", "prefix_tokens", "=", "torch", ".", "arange", "(", "self", ".", "pre_seq_len", ")", ".", "long", "(", ")", "\n", "self", ".", "prefix_encoder", "=", "PrefixEncoder", "(", "config", ")", "\n", "\n", "bert_param", "=", "0", "\n", "for", "name", ",", "param", "in", "self", ".", "bert", ".", "named_parameters", "(", ")", ":", "\n", "            ", "bert_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "all_param", "=", "0", "\n", "for", "name", ",", "param", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "all_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "total_param", "=", "all_param", "-", "bert_param", "\n", "print", "(", "'total param is {}'", ".", "format", "(", "total_param", ")", ")", "# 9860105", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.BertPrefixForSequenceClassification.get_prompt": [[131, 145], ["prompt.BertPrefixForSequenceClassification.prefix_tokens.unsqueeze().expand().to", "prompt.BertPrefixForSequenceClassification.prefix_encoder", "past_key_values.permute().split.permute().split.view", "prompt.BertPrefixForSequenceClassification.dropout", "past_key_values.permute().split.permute().split.permute().split", "prompt.BertPrefixForSequenceClassification.prefix_tokens.unsqueeze().expand", "past_key_values.permute().split.permute().split.permute", "prompt.BertPrefixForSequenceClassification.prefix_tokens.unsqueeze"], "methods", ["None"], ["", "def", "get_prompt", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "prefix_tokens", "=", "self", ".", "prefix_tokens", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "batch_size", ",", "-", "1", ")", ".", "to", "(", "self", ".", "bert", ".", "device", ")", "\n", "past_key_values", "=", "self", ".", "prefix_encoder", "(", "prefix_tokens", ")", "\n", "# bsz, seqlen, _ = past_key_values.shape", "\n", "past_key_values", "=", "past_key_values", ".", "view", "(", "\n", "batch_size", ",", "\n", "self", ".", "pre_seq_len", ",", "\n", "self", ".", "n_layer", "*", "2", ",", "\n", "self", ".", "n_head", ",", "\n", "self", ".", "n_embd", "\n", ")", "\n", "past_key_values", "=", "self", ".", "dropout", "(", "past_key_values", ")", "\n", "past_key_values", "=", "past_key_values", ".", "permute", "(", "[", "2", ",", "0", ",", "3", ",", "1", ",", "4", "]", ")", ".", "split", "(", "2", ")", "\n", "return", "past_key_values", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.BertPrefixForSequenceClassification.forward": [[146, 215], ["prompt.BertPrefixForSequenceClassification.get_prompt", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "prompt.BertPrefixForSequenceClassification.bert", "prompt.BertPrefixForSequenceClassification.dropout", "prompt.BertPrefixForSequenceClassification.classifier", "transformers.modeling_outputs.SequenceClassifierOutput", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "prompt.BertPrefixForSequenceClassification.squeeze", "labels.squeeze", "prompt.BertPrefixForSequenceClassification.view", "labels.view", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss."], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.RobertaModelWithHeads.get_prompt"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "batch_size", "=", "input_ids", ".", "shape", "[", "0", "]", "\n", "past_key_values", "=", "self", ".", "get_prompt", "(", "batch_size", "=", "batch_size", ")", "\n", "prefix_attention_mask", "=", "torch", ".", "ones", "(", "batch_size", ",", "self", ".", "pre_seq_len", ")", ".", "to", "(", "self", ".", "bert", ".", "device", ")", "\n", "attention_mask", "=", "torch", ".", "cat", "(", "(", "prefix_attention_mask", ",", "attention_mask", ")", ",", "dim", "=", "1", ")", "\n", "\n", "outputs", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", ")", "\n", "\n", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "config", ".", "problem_type", "is", "None", ":", "\n", "                ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"regression\"", "\n", "", "elif", "self", ".", "num_labels", ">", "1", "and", "(", "labels", ".", "dtype", "==", "torch", ".", "long", "or", "labels", ".", "dtype", "==", "torch", ".", "int", ")", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"single_label_classification\"", "\n", "", "else", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"multi_label_classification\"", "\n", "\n", "", "", "if", "self", ".", "config", ".", "problem_type", "==", "\"regression\"", ":", "\n", "                ", "loss_fct", "=", "MSELoss", "(", ")", "\n", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                    ", "loss", "=", "loss_fct", "(", "logits", ".", "squeeze", "(", ")", ",", "labels", ".", "squeeze", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "loss", "=", "loss_fct", "(", "logits", ",", "labels", ")", "\n", "", "", "elif", "self", ".", "config", ".", "problem_type", "==", "\"single_label_classification\"", ":", "\n", "                ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "elif", "self", ".", "config", ".", "problem_type", "==", "\"multi_label_classification\"", ":", "\n", "                ", "loss_fct", "=", "BCEWithLogitsLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ",", "labels", ")", "\n", "", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "SequenceClassifierOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.BertPromptForSequenceClassification.__init__": [[219, 237], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "prompt.BertPromptForSequenceClassification.bert.parameters", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "embeddings", "=", "self", ".", "bert", ".", "embeddings", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "torch", ".", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "\n", "for", "param", "in", "self", ".", "bert", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "self", ".", "pre_seq_len", "=", "config", ".", "pre_seq_len", "\n", "self", ".", "n_layer", "=", "config", ".", "num_hidden_layers", "\n", "self", ".", "n_head", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "n_embd", "=", "config", ".", "hidden_size", "//", "config", ".", "num_attention_heads", "\n", "\n", "self", ".", "prefix_tokens", "=", "torch", ".", "arange", "(", "self", ".", "pre_seq_len", ")", ".", "long", "(", ")", "\n", "self", ".", "prefix_encoder", "=", "torch", ".", "nn", ".", "Embedding", "(", "self", ".", "pre_seq_len", ",", "config", ".", "hidden_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.BertPromptForSequenceClassification.get_prompt": [[238, 242], ["prompt.BertPromptForSequenceClassification.prefix_tokens.unsqueeze().expand().to", "prompt.BertPromptForSequenceClassification.prefix_encoder", "prompt.BertPromptForSequenceClassification.prefix_tokens.unsqueeze().expand", "prompt.BertPromptForSequenceClassification.prefix_tokens.unsqueeze"], "methods", ["None"], ["", "def", "get_prompt", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "prefix_tokens", "=", "self", ".", "prefix_tokens", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "batch_size", ",", "-", "1", ")", ".", "to", "(", "self", ".", "bert", ".", "device", ")", "\n", "prompts", "=", "self", ".", "prefix_encoder", "(", "prefix_tokens", ")", "\n", "return", "prompts", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.BertPromptForSequenceClassification.forward": [[243, 318], ["prompt.BertPromptForSequenceClassification.embeddings", "prompt.BertPromptForSequenceClassification.get_prompt", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "prompt.BertPromptForSequenceClassification.bert", "prompt.BertPromptForSequenceClassification.dropout", "prompt.BertPromptForSequenceClassification.classifier", "transformers.modeling_outputs.SequenceClassifierOutput", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "prompt.BertPromptForSequenceClassification.squeeze", "labels.squeeze", "prompt.BertPromptForSequenceClassification.view", "labels.view", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss."], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.RobertaModelWithHeads.get_prompt"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "batch_size", "=", "input_ids", ".", "shape", "[", "0", "]", "\n", "raw_embedding", "=", "self", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", ")", "\n", "prompts", "=", "self", ".", "get_prompt", "(", "batch_size", "=", "batch_size", ")", "\n", "inputs_embeds", "=", "torch", ".", "cat", "(", "(", "prompts", ",", "raw_embedding", ")", ",", "dim", "=", "1", ")", "\n", "prefix_attention_mask", "=", "torch", ".", "ones", "(", "batch_size", ",", "self", ".", "pre_seq_len", ")", ".", "to", "(", "self", ".", "bert", ".", "device", ")", "\n", "attention_mask", "=", "torch", ".", "cat", "(", "(", "prefix_attention_mask", ",", "attention_mask", ")", ",", "dim", "=", "1", ")", "\n", "\n", "outputs", "=", "self", ".", "bert", "(", "\n", "# input_ids,", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "# token_type_ids=token_type_ids,", "\n", "# position_ids=position_ids,", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "# past_key_values=past_key_values,", "\n", ")", "\n", "\n", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "config", ".", "problem_type", "is", "None", ":", "\n", "                ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"regression\"", "\n", "", "elif", "self", ".", "num_labels", ">", "1", "and", "(", "labels", ".", "dtype", "==", "torch", ".", "long", "or", "labels", ".", "dtype", "==", "torch", ".", "int", ")", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"single_label_classification\"", "\n", "", "else", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"multi_label_classification\"", "\n", "\n", "", "", "if", "self", ".", "config", ".", "problem_type", "==", "\"regression\"", ":", "\n", "                ", "loss_fct", "=", "MSELoss", "(", ")", "\n", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                    ", "loss", "=", "loss_fct", "(", "logits", ".", "squeeze", "(", ")", ",", "labels", ".", "squeeze", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "loss", "=", "loss_fct", "(", "logits", ",", "labels", ")", "\n", "", "", "elif", "self", ".", "config", ".", "problem_type", "==", "\"single_label_classification\"", ":", "\n", "                ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "elif", "self", ".", "config", ".", "problem_type", "==", "\"multi_label_classification\"", ":", "\n", "                ", "loss_fct", "=", "BCEWithLogitsLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ",", "labels", ")", "\n", "", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "SequenceClassifierOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.RobertaPrefixForSequenceClassification.__init__": [[323, 351], ["transformers.RobertaPreTrainedModel.__init__", "transformers.RobertaModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "prompt.RobertaPrefixForSequenceClassification.init_weights", "prompt.RobertaPrefixForSequenceClassification.roberta.parameters", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "model.prefix_encoder.PrefixEncoder", "prompt.RobertaPrefixForSequenceClassification.roberta.named_parameters", "prompt.RobertaPrefixForSequenceClassification.named_parameters", "print", "param.numel", "param.numel", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "roberta", "=", "RobertaModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "torch", ".", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n", "for", "param", "in", "self", ".", "roberta", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "self", ".", "pre_seq_len", "=", "config", ".", "pre_seq_len", "\n", "self", ".", "n_layer", "=", "config", ".", "num_hidden_layers", "\n", "self", ".", "n_head", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "n_embd", "=", "config", ".", "hidden_size", "//", "config", ".", "num_attention_heads", "\n", "\n", "self", ".", "prefix_tokens", "=", "torch", ".", "arange", "(", "self", ".", "pre_seq_len", ")", ".", "long", "(", ")", "\n", "self", ".", "prefix_encoder", "=", "PrefixEncoder", "(", "config", ")", "\n", "\n", "bert_param", "=", "0", "\n", "for", "name", ",", "param", "in", "self", ".", "roberta", ".", "named_parameters", "(", ")", ":", "\n", "            ", "bert_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "all_param", "=", "0", "\n", "for", "name", ",", "param", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "all_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "total_param", "=", "all_param", "-", "bert_param", "\n", "print", "(", "'total param is {}'", ".", "format", "(", "total_param", ")", ")", "# 9860105", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.RobertaPrefixForSequenceClassification.get_prompt": [[353, 366], ["prompt.RobertaPrefixForSequenceClassification.prefix_tokens.unsqueeze().expand().to", "prompt.RobertaPrefixForSequenceClassification.prefix_encoder", "past_key_values.permute().split.permute().split.view", "prompt.RobertaPrefixForSequenceClassification.dropout", "past_key_values.permute().split.permute().split.permute().split", "prompt.RobertaPrefixForSequenceClassification.prefix_tokens.unsqueeze().expand", "past_key_values.permute().split.permute().split.permute", "prompt.RobertaPrefixForSequenceClassification.prefix_tokens.unsqueeze"], "methods", ["None"], ["", "def", "get_prompt", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "prefix_tokens", "=", "self", ".", "prefix_tokens", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "batch_size", ",", "-", "1", ")", ".", "to", "(", "self", ".", "roberta", ".", "device", ")", "\n", "past_key_values", "=", "self", ".", "prefix_encoder", "(", "prefix_tokens", ")", "\n", "past_key_values", "=", "past_key_values", ".", "view", "(", "\n", "batch_size", ",", "\n", "self", ".", "pre_seq_len", ",", "\n", "self", ".", "n_layer", "*", "2", ",", "\n", "self", ".", "n_head", ",", "\n", "self", ".", "n_embd", "\n", ")", "\n", "past_key_values", "=", "self", ".", "dropout", "(", "past_key_values", ")", "\n", "past_key_values", "=", "past_key_values", ".", "permute", "(", "[", "2", ",", "0", ",", "3", ",", "1", ",", "4", "]", ")", ".", "split", "(", "2", ")", "\n", "return", "past_key_values", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.RobertaPrefixForSequenceClassification.forward": [[367, 436], ["prompt.RobertaPrefixForSequenceClassification.get_prompt", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "prompt.RobertaPrefixForSequenceClassification.roberta", "prompt.RobertaPrefixForSequenceClassification.dropout", "prompt.RobertaPrefixForSequenceClassification.classifier", "transformers.modeling_outputs.SequenceClassifierOutput", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "prompt.RobertaPrefixForSequenceClassification.squeeze", "labels.squeeze", "prompt.RobertaPrefixForSequenceClassification.view", "labels.view", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss."], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.RobertaModelWithHeads.get_prompt"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "batch_size", "=", "input_ids", ".", "shape", "[", "0", "]", "\n", "past_key_values", "=", "self", ".", "get_prompt", "(", "batch_size", "=", "batch_size", ")", "\n", "prefix_attention_mask", "=", "torch", ".", "ones", "(", "batch_size", ",", "self", ".", "pre_seq_len", ")", ".", "to", "(", "self", ".", "roberta", ".", "device", ")", "\n", "attention_mask", "=", "torch", ".", "cat", "(", "(", "prefix_attention_mask", ",", "attention_mask", ")", ",", "dim", "=", "1", ")", "\n", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", ")", "\n", "\n", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "config", ".", "problem_type", "is", "None", ":", "\n", "                ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"regression\"", "\n", "", "elif", "self", ".", "num_labels", ">", "1", "and", "(", "labels", ".", "dtype", "==", "torch", ".", "long", "or", "labels", ".", "dtype", "==", "torch", ".", "int", ")", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"single_label_classification\"", "\n", "", "else", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"multi_label_classification\"", "\n", "\n", "", "", "if", "self", ".", "config", ".", "problem_type", "==", "\"regression\"", ":", "\n", "                ", "loss_fct", "=", "MSELoss", "(", ")", "\n", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                    ", "loss", "=", "loss_fct", "(", "logits", ".", "squeeze", "(", ")", ",", "labels", ".", "squeeze", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "loss", "=", "loss_fct", "(", "logits", ",", "labels", ")", "\n", "", "", "elif", "self", ".", "config", ".", "problem_type", "==", "\"single_label_classification\"", ":", "\n", "                ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "elif", "self", ".", "config", ".", "problem_type", "==", "\"multi_label_classification\"", ":", "\n", "                ", "loss_fct", "=", "BCEWithLogitsLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ",", "labels", ")", "\n", "", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "SequenceClassifierOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.RobertaPromptForSequenceClassification.__init__": [[441, 459], ["transformers.RobertaPreTrainedModel.__init__", "transformers.RobertaModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "prompt.RobertaPromptForSequenceClassification.roberta.parameters", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "self", ".", "roberta", "=", "RobertaModel", "(", "config", ")", "\n", "self", ".", "embeddings", "=", "self", ".", "roberta", ".", "embeddings", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "torch", ".", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "\n", "for", "param", "in", "self", ".", "roberta", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "self", ".", "pre_seq_len", "=", "config", ".", "pre_seq_len", "\n", "self", ".", "n_layer", "=", "config", ".", "num_hidden_layers", "\n", "self", ".", "n_head", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "n_embd", "=", "config", ".", "hidden_size", "//", "config", ".", "num_attention_heads", "\n", "\n", "self", ".", "prefix_tokens", "=", "torch", ".", "arange", "(", "self", ".", "pre_seq_len", ")", ".", "long", "(", ")", "\n", "self", ".", "prefix_encoder", "=", "torch", ".", "nn", ".", "Embedding", "(", "self", ".", "pre_seq_len", ",", "config", ".", "hidden_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.RobertaPromptForSequenceClassification.get_prompt": [[460, 464], ["prompt.RobertaPromptForSequenceClassification.prefix_tokens.unsqueeze().expand().to", "prompt.RobertaPromptForSequenceClassification.prefix_encoder", "prompt.RobertaPromptForSequenceClassification.prefix_tokens.unsqueeze().expand", "prompt.RobertaPromptForSequenceClassification.prefix_tokens.unsqueeze"], "methods", ["None"], ["", "def", "get_prompt", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "prefix_tokens", "=", "self", ".", "prefix_tokens", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "batch_size", ",", "-", "1", ")", ".", "to", "(", "self", ".", "roberta", ".", "device", ")", "\n", "prompts", "=", "self", ".", "prefix_encoder", "(", "prefix_tokens", ")", "\n", "return", "prompts", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.RobertaPromptForSequenceClassification.forward": [[465, 542], ["prompt.RobertaPromptForSequenceClassification.embeddings", "prompt.RobertaPromptForSequenceClassification.get_prompt", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "prompt.RobertaPromptForSequenceClassification.roberta", "prompt.RobertaPromptForSequenceClassification.dropout", "prompt.RobertaPromptForSequenceClassification.classifier", "transformers.modeling_outputs.SequenceClassifierOutput", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "prompt.RobertaPromptForSequenceClassification.squeeze", "labels.squeeze", "prompt.RobertaPromptForSequenceClassification.view", "labels.view", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss."], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.RobertaModelWithHeads.get_prompt"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "batch_size", "=", "input_ids", ".", "shape", "[", "0", "]", "\n", "raw_embedding", "=", "self", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", ")", "\n", "prompts", "=", "self", ".", "get_prompt", "(", "batch_size", "=", "batch_size", ")", "\n", "inputs_embeds", "=", "torch", ".", "cat", "(", "(", "prompts", ",", "raw_embedding", ")", ",", "dim", "=", "1", ")", "\n", "# print(input_embeddings.shape)", "\n", "# exit()", "\n", "prefix_attention_mask", "=", "torch", ".", "ones", "(", "batch_size", ",", "self", ".", "pre_seq_len", ")", ".", "to", "(", "self", ".", "roberta", ".", "device", ")", "\n", "attention_mask", "=", "torch", ".", "cat", "(", "(", "prefix_attention_mask", ",", "attention_mask", ")", ",", "dim", "=", "1", ")", "\n", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "# input_ids,", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "# token_type_ids=token_type_ids,", "\n", "# position_ids=position_ids,", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "# past_key_values=past_key_values,", "\n", ")", "\n", "\n", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "config", ".", "problem_type", "is", "None", ":", "\n", "                ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"regression\"", "\n", "", "elif", "self", ".", "num_labels", ">", "1", "and", "(", "labels", ".", "dtype", "==", "torch", ".", "long", "or", "labels", ".", "dtype", "==", "torch", ".", "int", ")", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"single_label_classification\"", "\n", "", "else", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"multi_label_classification\"", "\n", "\n", "", "", "if", "self", ".", "config", ".", "problem_type", "==", "\"regression\"", ":", "\n", "                ", "loss_fct", "=", "MSELoss", "(", ")", "\n", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                    ", "loss", "=", "loss_fct", "(", "logits", ".", "squeeze", "(", ")", ",", "labels", ".", "squeeze", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "loss", "=", "loss_fct", "(", "logits", ",", "labels", ")", "\n", "", "", "elif", "self", ".", "config", ".", "problem_type", "==", "\"single_label_classification\"", ":", "\n", "                ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "elif", "self", ".", "config", ".", "problem_type", "==", "\"multi_label_classification\"", ":", "\n", "                ", "loss_fct", "=", "BCEWithLogitsLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ",", "labels", ")", "\n", "", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "SequenceClassifierOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.DebertaPrefixForSequenceClassification.__init__": [[547, 577], ["DebertaPreTrainedModel.__init__", "DebertaModel", "ContextPooler", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "StableDropout", "prompt.DebertaPrefixForSequenceClassification.init_weights", "prompt.DebertaPrefixForSequenceClassification.deberta.parameters", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "model.prefix_encoder.PrefixEncoder", "prompt.DebertaPrefixForSequenceClassification.deberta.named_parameters", "prompt.DebertaPrefixForSequenceClassification.named_parameters", "print", "param.numel", "param.numel", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "deberta", "=", "DebertaModel", "(", "config", ")", "\n", "self", ".", "pooler", "=", "ContextPooler", "(", "config", ")", "\n", "output_dim", "=", "self", ".", "pooler", ".", "output_dim", "\n", "self", ".", "classifier", "=", "torch", ".", "nn", ".", "Linear", "(", "output_dim", ",", "self", ".", "num_labels", ")", "\n", "self", ".", "dropout", "=", "StableDropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n", "for", "param", "in", "self", ".", "deberta", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "self", ".", "pre_seq_len", "=", "config", ".", "pre_seq_len", "\n", "self", ".", "n_layer", "=", "config", ".", "num_hidden_layers", "\n", "self", ".", "n_head", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "n_embd", "=", "config", ".", "hidden_size", "//", "config", ".", "num_attention_heads", "\n", "\n", "self", ".", "prefix_tokens", "=", "torch", ".", "arange", "(", "self", ".", "pre_seq_len", ")", ".", "long", "(", ")", "\n", "self", ".", "prefix_encoder", "=", "PrefixEncoder", "(", "config", ")", "\n", "\n", "deberta_param", "=", "0", "\n", "for", "name", ",", "param", "in", "self", ".", "deberta", ".", "named_parameters", "(", ")", ":", "\n", "            ", "deberta_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "all_param", "=", "0", "\n", "for", "name", ",", "param", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "all_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "total_param", "=", "all_param", "-", "deberta_param", "\n", "print", "(", "'total param is {}'", ".", "format", "(", "total_param", ")", ")", "# 9860105", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.DebertaPrefixForSequenceClassification.get_prompt": [[578, 592], ["prompt.DebertaPrefixForSequenceClassification.prefix_tokens.unsqueeze().expand().to", "prompt.DebertaPrefixForSequenceClassification.prefix_encoder", "past_key_values.permute().split.permute().split.view", "prompt.DebertaPrefixForSequenceClassification.dropout", "past_key_values.permute().split.permute().split.permute().split", "prompt.DebertaPrefixForSequenceClassification.prefix_tokens.unsqueeze().expand", "past_key_values.permute().split.permute().split.permute", "prompt.DebertaPrefixForSequenceClassification.prefix_tokens.unsqueeze"], "methods", ["None"], ["", "def", "get_prompt", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "prefix_tokens", "=", "self", ".", "prefix_tokens", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "batch_size", ",", "-", "1", ")", ".", "to", "(", "self", ".", "deberta", ".", "device", ")", "\n", "past_key_values", "=", "self", ".", "prefix_encoder", "(", "prefix_tokens", ")", "\n", "# bsz, seqlen, _ = past_key_values.shape", "\n", "past_key_values", "=", "past_key_values", ".", "view", "(", "\n", "batch_size", ",", "\n", "self", ".", "pre_seq_len", ",", "\n", "self", ".", "n_layer", "*", "2", ",", "\n", "self", ".", "n_head", ",", "\n", "self", ".", "n_embd", "\n", ")", "\n", "past_key_values", "=", "self", ".", "dropout", "(", "past_key_values", ")", "\n", "past_key_values", "=", "past_key_values", ".", "permute", "(", "[", "2", ",", "0", ",", "3", ",", "1", ",", "4", "]", ")", ".", "split", "(", "2", ")", "\n", "return", "past_key_values", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.DebertaPrefixForSequenceClassification.forward": [[593, 659], ["prompt.DebertaPrefixForSequenceClassification.get_prompt", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "prompt.DebertaPrefixForSequenceClassification.deberta", "prompt.DebertaPrefixForSequenceClassification.pooler", "prompt.DebertaPrefixForSequenceClassification.dropout", "prompt.DebertaPrefixForSequenceClassification.classifier", "transformers.modeling_outputs.SequenceClassifierOutput", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "logits.view().to.view().to.view().to", "torch.nn.MSELoss.", "torch.nn.MSELoss.", "torch.nn.MSELoss.", "torch.gather.view", "torch.gather.view", "torch.gather.view", "torch.gather.long", "torch.gather.long", "torch.gather.long", "torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "logits.view().to.view().to.view", "torch.gather.dim", "torch.gather.dim", "torch.gather.dim", "torch.gather.size", "torch.gather.size", "torch.gather.size", "label_index.size", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "label_index.expand", "label_index.view", "torch.gather.view().float", "torch.gather.view().float", "torch.gather.view().float", "torch.gather.view", "torch.gather.view", "torch.gather.view", "label_index.size", "logits.view().to.view().to.size", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.gather.view", "torch.gather.view", "torch.gather.view", "torch.nn.LogSoftmax.", "torch.nn.LogSoftmax.", "torch.nn.LogSoftmax."], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.RobertaModelWithHeads.get_prompt"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "batch_size", "=", "input_ids", ".", "shape", "[", "0", "]", "\n", "past_key_values", "=", "self", ".", "get_prompt", "(", "batch_size", "=", "batch_size", ")", "\n", "prefix_attention_mask", "=", "torch", ".", "ones", "(", "batch_size", ",", "self", ".", "pre_seq_len", ")", ".", "to", "(", "self", ".", "deberta", ".", "device", ")", "\n", "attention_mask", "=", "torch", ".", "cat", "(", "(", "prefix_attention_mask", ",", "attention_mask", ")", ",", "dim", "=", "1", ")", "\n", "\n", "outputs", "=", "self", ".", "deberta", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", ")", "\n", "\n", "encoder_layer", "=", "outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "encoder_layer", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "# regression task", "\n", "                ", "loss_fn", "=", "torch", ".", "nn", ".", "MSELoss", "(", ")", "\n", "logits", "=", "logits", ".", "view", "(", "-", "1", ")", ".", "to", "(", "labels", ".", "dtype", ")", "\n", "loss", "=", "loss_fn", "(", "logits", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "elif", "labels", ".", "dim", "(", ")", "==", "1", "or", "labels", ".", "size", "(", "-", "1", ")", "==", "1", ":", "\n", "                ", "label_index", "=", "(", "labels", ">=", "0", ")", ".", "nonzero", "(", ")", "\n", "labels", "=", "labels", ".", "long", "(", ")", "\n", "if", "label_index", ".", "size", "(", "0", ")", ">", "0", ":", "\n", "                    ", "labeled_logits", "=", "torch", ".", "gather", "(", "logits", ",", "0", ",", "label_index", ".", "expand", "(", "label_index", ".", "size", "(", "0", ")", ",", "logits", ".", "size", "(", "1", ")", ")", ")", "\n", "labels", "=", "torch", ".", "gather", "(", "labels", ",", "0", ",", "label_index", ".", "view", "(", "-", "1", ")", ")", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "labeled_logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ".", "float", "(", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                    ", "loss", "=", "torch", ".", "tensor", "(", "0", ")", ".", "to", "(", "logits", ")", "\n", "", "", "else", ":", "\n", "                ", "log_softmax", "=", "torch", ".", "nn", ".", "LogSoftmax", "(", "-", "1", ")", "\n", "loss", "=", "-", "(", "(", "log_softmax", "(", "logits", ")", "*", "labels", ")", ".", "sum", "(", "-", "1", ")", ")", ".", "mean", "(", ")", "\n", "", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "outputs", "[", "1", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "", "else", ":", "\n", "            ", "return", "SequenceClassifierOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.RobertaPrefixWithHeads.__init__": [[663, 693], ["transformers.adapters.models.bert.BertModelHeadsMixin.__init__", "transformers.RobertaModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "prompt.RobertaPrefixWithHeads._init_head_modules", "prompt.RobertaPrefixWithHeads.init_weights", "prompt.RobertaPrefixWithHeads.roberta.parameters", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "model.prefix_encoder.PrefixEncoder", "prompt.RobertaPrefixWithHeads.roberta.named_parameters", "prompt.RobertaPrefixWithHeads.named_parameters", "print", "param.numel", "param.numel", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "roberta", "=", "RobertaModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "_init_head_modules", "(", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n", "for", "param", "in", "self", ".", "roberta", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "self", ".", "pre_seq_len", "=", "config", ".", "pre_seq_len", "\n", "self", ".", "n_layer", "=", "config", ".", "num_hidden_layers", "\n", "self", ".", "n_head", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "n_embd", "=", "config", ".", "hidden_size", "//", "config", ".", "num_attention_heads", "\n", "\n", "self", ".", "prefix_tokens", "=", "torch", ".", "arange", "(", "self", ".", "pre_seq_len", ")", ".", "long", "(", ")", "\n", "self", ".", "prefix_encoder", "=", "PrefixEncoder", "(", "config", ")", "\n", "\n", "bert_param", "=", "0", "\n", "for", "name", ",", "param", "in", "self", ".", "roberta", ".", "named_parameters", "(", ")", ":", "\n", "            ", "bert_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "all_param", "=", "0", "\n", "for", "name", ",", "param", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "all_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "total_param", "=", "all_param", "-", "bert_param", "\n", "print", "(", "'total param is {}'", ".", "format", "(", "total_param", ")", ")", "# 9860105", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.RobertaPrefixWithHeads.get_prompt": [[695, 708], ["prompt.RobertaPrefixWithHeads.prefix_tokens.unsqueeze().expand().to", "prompt.RobertaPrefixWithHeads.prefix_encoder", "past_key_values.permute().split.permute().split.view", "prompt.RobertaPrefixWithHeads.dropout", "past_key_values.permute().split.permute().split.permute().split", "prompt.RobertaPrefixWithHeads.prefix_tokens.unsqueeze().expand", "past_key_values.permute().split.permute().split.permute", "prompt.RobertaPrefixWithHeads.prefix_tokens.unsqueeze"], "methods", ["None"], ["", "def", "get_prompt", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "prefix_tokens", "=", "self", ".", "prefix_tokens", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "batch_size", ",", "-", "1", ")", ".", "to", "(", "self", ".", "roberta", ".", "device", ")", "\n", "past_key_values", "=", "self", ".", "prefix_encoder", "(", "prefix_tokens", ")", "\n", "past_key_values", "=", "past_key_values", ".", "view", "(", "\n", "batch_size", ",", "\n", "self", ".", "pre_seq_len", ",", "\n", "self", ".", "n_layer", "*", "2", ",", "\n", "self", ".", "n_head", ",", "\n", "self", ".", "n_embd", "\n", ")", "\n", "past_key_values", "=", "self", ".", "dropout", "(", "past_key_values", ")", "\n", "past_key_values", "=", "past_key_values", ".", "permute", "(", "[", "2", ",", "0", ",", "3", ",", "1", ",", "4", "]", ")", ".", "split", "(", "2", ")", "\n", "return", "past_key_values", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.RobertaPrefixWithHeads.forward": [[709, 766], ["prompt.RobertaPrefixWithHeads.get_prompt", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "prompt.RobertaPrefixWithHeads.roberta", "prompt.RobertaPrefixWithHeads.dropout", "prompt.RobertaPrefixWithHeads.forward_head", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.RobertaModelWithHeads.get_prompt"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "adapter_names", "=", "None", ",", "\n", "head", "=", "None", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "batch_size", "=", "input_ids", ".", "shape", "[", "0", "]", "\n", "past_key_values", "=", "self", ".", "get_prompt", "(", "batch_size", "=", "batch_size", ")", "\n", "prefix_attention_mask", "=", "torch", ".", "ones", "(", "batch_size", ",", "self", ".", "pre_seq_len", ")", ".", "to", "(", "self", ".", "roberta", ".", "device", ")", "\n", "attention_mask", "=", "torch", ".", "cat", "(", "(", "prefix_attention_mask", ",", "attention_mask", ")", ",", "dim", "=", "1", ")", "\n", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", ")", "\n", "# BERT & RoBERTa return the pooled output as second item, we don't need that in these heads", "\n", "if", "not", "return_dict", ":", "\n", "            ", "head_inputs", "=", "(", "outputs", "[", "0", "]", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "", "else", ":", "\n", "            ", "head_inputs", "=", "outputs", "\n", "\n", "", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "\n", "if", "head", "or", "self", ".", "active_head", ":", "\n", "            ", "head_outputs", "=", "self", ".", "forward_head", "(", "\n", "head_inputs", ",", "\n", "head_name", "=", "head", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "pooled_output", "=", "pooled_output", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "return", "head_outputs", "\n", "", "else", ":", "\n", "# in case no head is used just return the output of the base model (including pooler output)", "\n", "            ", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.RobertaModelWithHeads.__init__": [[769, 807], ["transformers.adapters.models.bert.BertModelHeadsMixin.__init__", "transformers.RobertaModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "prompt.RobertaModelWithHeads._init_head_modules", "prompt.RobertaModelWithHeads.init_weights", "prompt.RobertaModelWithHeads.roberta.parameters", "prompt.RobertaModelWithHeads.roberta.parameters", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "model.prefix_encoder.PrefixEncoder", "prompt.RobertaModelWithHeads.roberta.named_parameters", "prompt.RobertaModelWithHeads.named_parameters", "print", "param.numel", "param.numel", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "use_vocab", "=", "True", ",", "use_prompt", "=", "False", ",", "prompt_length", "=", "20", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "roberta", "=", "RobertaModel", "(", "config", ",", "use_prompt", "=", "use_prompt", ",", "prompt_length", "=", "prompt_length", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "_init_head_modules", "(", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n", "for", "param", "in", "self", ".", "roberta", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "self", ".", "use_prompt", "=", "use_prompt", "\n", "self", ".", "prompt_length", "=", "prompt_length", "\n", "for", "param", "in", "self", ".", "roberta", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "", "import", "random", "\n", "random_range", "=", "0.5", "\n", "self", ".", "prompt", "=", "None", "\n", "# soft_embeds = torch.FloatTensor(self.prompt_length, config.hidden_size).uniform_(-random_range, random_range)", "\n", "if", "use_prompt", ":", "\n", "            ", "self", ".", "pre_seq_len", "=", "config", ".", "pre_seq_len", "\n", "self", ".", "n_layer", "=", "config", ".", "num_hidden_layers", "\n", "self", ".", "n_head", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "n_embd", "=", "config", ".", "hidden_size", "//", "config", ".", "num_attention_heads", "\n", "\n", "self", ".", "prefix_tokens", "=", "torch", ".", "arange", "(", "self", ".", "pre_seq_len", ")", ".", "long", "(", ")", "\n", "self", ".", "prefix_encoder", "=", "PrefixEncoder", "(", "config", ")", "\n", "\n", "bert_param", "=", "0", "\n", "for", "name", ",", "param", "in", "self", ".", "roberta", ".", "named_parameters", "(", ")", ":", "\n", "                ", "bert_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "all_param", "=", "0", "\n", "for", "name", ",", "param", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "                ", "all_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "total_param", "=", "all_param", "-", "bert_param", "\n", "print", "(", "'total param is {}'", ".", "format", "(", "total_param", ")", ")", "# 9860105", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.RobertaModelWithHeads.get_prompt": [[809, 822], ["prompt.RobertaModelWithHeads.prefix_tokens.unsqueeze().expand().to", "prompt.RobertaModelWithHeads.prefix_encoder", "past_key_values.permute().split.permute().split.view", "prompt.RobertaModelWithHeads.dropout", "past_key_values.permute().split.permute().split.permute().split", "prompt.RobertaModelWithHeads.prefix_tokens.unsqueeze().expand", "past_key_values.permute().split.permute().split.permute", "prompt.RobertaModelWithHeads.prefix_tokens.unsqueeze"], "methods", ["None"], ["", "", "def", "get_prompt", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "prefix_tokens", "=", "self", ".", "prefix_tokens", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "batch_size", ",", "-", "1", ")", ".", "to", "(", "self", ".", "roberta", ".", "device", ")", "\n", "past_key_values", "=", "self", ".", "prefix_encoder", "(", "prefix_tokens", ")", "\n", "past_key_values", "=", "past_key_values", ".", "view", "(", "\n", "batch_size", ",", "\n", "self", ".", "pre_seq_len", ",", "\n", "self", ".", "n_layer", "*", "2", ",", "\n", "self", ".", "n_head", ",", "\n", "self", ".", "n_embd", "\n", ")", "\n", "past_key_values", "=", "self", ".", "dropout", "(", "past_key_values", ")", "\n", "past_key_values", "=", "past_key_values", ".", "permute", "(", "[", "2", ",", "0", ",", "3", ",", "1", ",", "4", "]", ")", ".", "split", "(", "2", ")", "\n", "return", "past_key_values", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prompt.RobertaModelWithHeads.forward": [[823, 883], ["prompt.RobertaModelWithHeads.roberta", "input_ids.view", "position_ids.view", "token_type_ids.view", "attention_mask.view", "inputs_embeds.view", "prompt.RobertaModelWithHeads.forward_head", "input_ids.size", "position_ids.size", "token_type_ids.size", "attention_mask.size", "inputs_embeds.size", "inputs_embeds.size"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "adapter_names", "=", "None", ",", "\n", "head", "=", "None", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_ids", ".", "size", "(", "-", "1", ")", ")", "if", "input_ids", "is", "not", "None", "else", "None", "\n", "position_ids", "=", "position_ids", ".", "view", "(", "-", "1", ",", "position_ids", ".", "size", "(", "-", "1", ")", ")", "if", "position_ids", "is", "not", "None", "else", "None", "\n", "token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "token_type_ids", ".", "size", "(", "-", "1", ")", ")", "if", "token_type_ids", "is", "not", "None", "else", "None", "\n", "attention_mask", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "attention_mask", ".", "size", "(", "-", "1", ")", ")", "if", "attention_mask", "is", "not", "None", "else", "None", "\n", "inputs_embeds", "=", "(", "\n", "inputs_embeds", ".", "view", "(", "-", "1", ",", "inputs_embeds", ".", "size", "(", "-", "2", ")", ",", "inputs_embeds", ".", "size", "(", "-", "1", ")", ")", "\n", "if", "inputs_embeds", "is", "not", "None", "\n", "else", "None", "\n", ")", "\n", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "adapter_names", "=", "adapter_names", ",", "\n", "prompt", "=", "self", ".", "prompt", "\n", ")", "\n", "# BERT & RoBERTa return the pooled output as second item, we don't need that in these heads", "\n", "if", "not", "return_dict", ":", "\n", "            ", "head_inputs", "=", "(", "outputs", "[", "0", "]", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "", "else", ":", "\n", "            ", "head_inputs", "=", "outputs", "\n", "", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "\n", "if", "head", "or", "self", ".", "active_head", ":", "\n", "            ", "head_outputs", "=", "self", ".", "forward_head", "(", "\n", "head_inputs", ",", "\n", "head_name", "=", "head", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "pooled_output", "=", "pooled_output", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "return", "head_outputs", "\n", "", "else", ":", "\n", "# in case no head is used just return the output of the base model (including pooler output)", "\n", "            ", "return", "outputs", "", "", "", "", ""]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prefix_encoder.PrefixEncoder.__init__": [[12, 25], ["super().__init__", "torch.nn.Embedding", "torch.nn.Sequential", "torch.nn.Embedding", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "prefix_projection", "=", "config", ".", "prefix_projection", "\n", "if", "self", ".", "prefix_projection", ":", "\n", "# Use a two-layer MLP to encode the prefix", "\n", "            ", "self", ".", "embedding", "=", "torch", ".", "nn", ".", "Embedding", "(", "config", ".", "pre_seq_len", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "trans", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "prefix_hidden_size", ")", ",", "\n", "torch", ".", "nn", ".", "Tanh", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "config", ".", "prefix_hidden_size", ",", "config", ".", "num_hidden_layers", "*", "2", "*", "config", ".", "hidden_size", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "embedding", "=", "torch", ".", "nn", ".", "Embedding", "(", "config", ".", "pre_seq_len", ",", "config", ".", "num_hidden_layers", "*", "2", "*", "config", ".", "hidden_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.prefix_encoder.PrefixEncoder.forward": [[26, 33], ["prefix_encoder.PrefixEncoder.embedding", "prefix_encoder.PrefixEncoder.trans", "prefix_encoder.PrefixEncoder.embedding"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "prefix", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "if", "self", ".", "prefix_projection", ":", "\n", "            ", "prefix_tokens", "=", "self", ".", "embedding", "(", "prefix", ")", "\n", "past_key_values", "=", "self", ".", "trans", "(", "prefix_tokens", ")", "\n", "", "else", ":", "\n", "            ", "past_key_values", "=", "self", ".", "embedding", "(", "prefix", ")", "\n", "", "return", "past_key_values", "", "", "", ""]], "home.repos.pwc.inspect_result.guanzhchen_petuning.model.utils.get_model": [[57, 111], ["model_class.from_pretrained", "model_class.from_pretrained", "model_class.from_pretrained", "model_class.from_pretrained.named_parameters", "print", "param.numel", "model_class.from_pretrained.bert.parameters", "model_class.from_pretrained.bert.named_parameters", "param.numel", "model_class.from_pretrained.roberta.parameters", "model_class.from_pretrained.roberta.named_parameters", "param.numel", "model_class.from_pretrained.deberta.parameters", "model_class.from_pretrained.deberta.named_parameters", "param.numel"], "function", ["None"], ["def", "get_model", "(", "model_args", ",", "task_type", ":", "TaskType", ",", "config", ":", "AutoConfig", ",", "fix_bert", ":", "bool", "=", "False", ")", ":", "\n", "    ", "if", "model_args", ".", "prefix", ":", "\n", "        ", "config", ".", "hidden_dropout_prob", "=", "model_args", ".", "hidden_dropout_prob", "\n", "config", ".", "pre_seq_len", "=", "model_args", ".", "pre_seq_len", "\n", "config", ".", "prefix_projection", "=", "model_args", ".", "prefix_projection", "\n", "config", ".", "prefix_hidden_size", "=", "model_args", ".", "prefix_hidden_size", "\n", "\n", "model_class", "=", "PREFIX_MODELS", "[", "config", ".", "model_type", "]", "[", "task_type", "]", "\n", "model", "=", "model_class", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "config", "=", "config", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", ")", "\n", "", "elif", "model_args", ".", "lora", ":", "\n", "        ", "config", ".", "lora", "=", "model_args", ".", "lora", "\n", "config", ".", "lora_r", "=", "model_args", ".", "lora_r", "\n", "config", ".", "lora_alpha", "=", "model_args", ".", "lora_alpha", "\n", "model_class", "=", "LORA_MODELS", "[", "config", ".", "model_type", "]", "[", "task_type", "]", "\n", "model", "=", "model_class", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "config", "=", "config", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "model_class", "=", "AUTO_MODELS", "[", "task_type", "]", "\n", "model", "=", "model_class", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "config", "=", "config", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", ")", "\n", "\n", "bert_param", "=", "0", "\n", "if", "fix_bert", ":", "\n", "            ", "if", "config", ".", "model_type", "==", "\"bert\"", ":", "\n", "                ", "for", "param", "in", "model", ".", "bert", ".", "parameters", "(", ")", ":", "\n", "                    ", "param", ".", "requires_grad", "=", "False", "\n", "", "for", "_", ",", "param", "in", "model", ".", "bert", ".", "named_parameters", "(", ")", ":", "\n", "                    ", "bert_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "", "elif", "config", ".", "model_type", "==", "\"roberta\"", ":", "\n", "                ", "for", "param", "in", "model", ".", "roberta", ".", "parameters", "(", ")", ":", "\n", "                    ", "param", ".", "requires_grad", "=", "False", "\n", "", "for", "_", ",", "param", "in", "model", ".", "roberta", ".", "named_parameters", "(", ")", ":", "\n", "                    ", "bert_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "", "elif", "config", ".", "model_type", "==", "\"deberta\"", ":", "\n", "                ", "for", "param", "in", "model", ".", "deberta", ".", "parameters", "(", ")", ":", "\n", "                    ", "param", ".", "requires_grad", "=", "False", "\n", "", "for", "_", ",", "param", "in", "model", ".", "deberta", ".", "named_parameters", "(", ")", ":", "\n", "                    ", "bert_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "", "", "all_param", "=", "0", "\n", "for", "_", ",", "param", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "            ", "all_param", "+=", "param", ".", "numel", "(", ")", "\n", "", "total_param", "=", "all_param", "-", "bert_param", "\n", "print", "(", "'***** total param is {} *****'", ".", "format", "(", "total_param", ")", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.LoRALayer.__init__": [[13, 30], ["torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "r", ":", "int", ",", "\n", "lora_alpha", ":", "int", ",", "\n", "lora_dropout", ":", "float", ",", "\n", "merge_weights", ":", "bool", ",", "\n", ")", ":", "\n", "        ", "self", ".", "r", "=", "r", "\n", "self", ".", "lora_alpha", "=", "lora_alpha", "\n", "# Optional dropout", "\n", "if", "lora_dropout", ">", "0.", ":", "\n", "            ", "self", ".", "lora_dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "lora_dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "lora_dropout", "=", "lambda", "x", ":", "x", "\n", "# Mark the weight as unmerged", "\n", "", "self", ".", "merged", "=", "False", "\n", "self", ".", "merge_weights", "=", "merge_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Embedding.__init__": [[34, 54], ["torch.Embedding.__init__", "torch.Embedding.__init__", "torch.Embedding.__init__", "layers.LoRALayer.__init__", "layers.Embedding.reset_parameters", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "layers.Embedding.weight.new_zeros", "layers.Embedding.weight.new_zeros"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.reset_parameters"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "num_embeddings", ":", "int", ",", "\n", "embedding_dim", ":", "int", ",", "\n", "r", ":", "int", "=", "0", ",", "\n", "lora_alpha", ":", "int", "=", "1", ",", "\n", "merge_weights", ":", "bool", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "nn", ".", "Embedding", ".", "__init__", "(", "self", ",", "num_embeddings", ",", "embedding_dim", ",", "**", "kwargs", ")", "\n", "LoRALayer", ".", "__init__", "(", "self", ",", "r", "=", "r", ",", "lora_alpha", "=", "lora_alpha", ",", "lora_dropout", "=", "0", ",", "\n", "merge_weights", "=", "merge_weights", ")", "\n", "# Actual trainable parameters", "\n", "if", "r", ">", "0", ":", "\n", "            ", "self", ".", "lora_A", "=", "nn", ".", "Parameter", "(", "self", ".", "weight", ".", "new_zeros", "(", "(", "r", ",", "num_embeddings", ")", ")", ")", "\n", "self", ".", "lora_B", "=", "nn", ".", "Parameter", "(", "self", ".", "weight", ".", "new_zeros", "(", "(", "embedding_dim", ",", "r", ")", ")", ")", "\n", "self", ".", "scaling", "=", "self", ".", "lora_alpha", "/", "self", ".", "r", "\n", "# Freezing the pre-trained weight matrix", "\n", "self", ".", "weight", ".", "requires_grad", "=", "False", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Embedding.reset_parameters": [[55, 61], ["torch.Embedding.reset_parameters", "torch.Embedding.reset_parameters", "torch.Embedding.reset_parameters", "hasattr", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.reset_parameters", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.reset_parameters", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.reset_parameters"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "Embedding", ".", "reset_parameters", "(", "self", ")", "\n", "if", "hasattr", "(", "self", ",", "'lora_A'", ")", ":", "\n", "# initialize A the same way as the default for nn.Linear and B to zero", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "lora_A", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "lora_B", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Embedding.train": [[62, 69], ["torch.Embedding.train", "torch.Embedding.train", "torch.Embedding.train"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.train", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.train", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.train"], ["", "", "def", "train", "(", "self", ",", "mode", ":", "bool", "=", "True", ")", ":", "\n", "        ", "nn", ".", "Embedding", ".", "train", "(", "self", ",", "mode", ")", "\n", "if", "self", ".", "merge_weights", "and", "self", ".", "merged", ":", "\n", "# Make sure that the weights are not merged", "\n", "            ", "if", "self", ".", "r", ">", "0", ":", "\n", "                ", "self", ".", "weight", ".", "data", "-=", "(", "self", ".", "lora_B", "@", "self", ".", "lora_A", ")", ".", "T", "*", "self", ".", "scaling", "\n", "", "self", ".", "merged", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Embedding.eval": [[70, 77], ["torch.Linear.eval", "torch.Linear.eval", "torch.Linear.eval"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.eval", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.eval", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.eval"], ["", "", "def", "eval", "(", "self", ")", ":", "\n", "        ", "nn", ".", "Linear", ".", "eval", "(", "self", ")", "\n", "if", "self", ".", "merge_weights", "and", "not", "self", ".", "merged", ":", "\n", "# Merge the weights and mark it", "\n", "            ", "if", "self", ".", "r", ">", "0", ":", "\n", "                ", "self", ".", "weight", ".", "data", "+=", "(", "self", ".", "lora_B", "@", "self", ".", "lora_A", ")", "*", "self", ".", "scaling", "\n", "", "self", ".", "merged", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Embedding.forward": [[78, 90], ["torch.Embedding.forward", "torch.Embedding.forward", "torch.Embedding.forward", "torch.Embedding.forward", "torch.Embedding.forward", "torch.Embedding.forward", "torch.embedding", "torch.embedding", "torch.embedding"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForQuestionAnswering.forward", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForQuestionAnswering.forward", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForQuestionAnswering.forward", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForQuestionAnswering.forward", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForQuestionAnswering.forward", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForQuestionAnswering.forward"], ["", "", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "if", "self", ".", "r", ">", "0", "and", "not", "self", ".", "merged", ":", "\n", "            ", "result", "=", "nn", ".", "Embedding", ".", "forward", "(", "self", ",", "x", ")", "\n", "if", "self", ".", "r", ">", "0", ":", "\n", "                ", "after_A", "=", "F", ".", "embedding", "(", "\n", "x", ",", "self", ".", "lora_A", ".", "T", ",", "self", ".", "padding_idx", ",", "self", ".", "max_norm", ",", "\n", "self", ".", "norm_type", ",", "self", ".", "scale_grad_by_freq", ",", "self", ".", "sparse", "\n", ")", "\n", "result", "+=", "(", "after_A", "@", "self", ".", "lora_B", ".", "T", ")", "*", "self", ".", "scaling", "\n", "", "return", "result", "\n", "", "else", ":", "\n", "            ", "return", "nn", ".", "Embedding", ".", "forward", "(", "self", ",", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Linear.__init__": [[94, 120], ["torch.Linear.__init__", "torch.Linear.__init__", "torch.Linear.__init__", "layers.LoRALayer.__init__", "layers.Linear.reset_parameters", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "layers.Linear.weight.new_zeros", "layers.Linear.weight.new_zeros"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.reset_parameters"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "in_features", ":", "int", ",", "\n", "out_features", ":", "int", ",", "\n", "r", ":", "int", "=", "0", ",", "\n", "lora_alpha", ":", "int", "=", "1", ",", "\n", "lora_dropout", ":", "float", "=", "0.", ",", "\n", "fan_in_fan_out", ":", "bool", "=", "False", ",", "# Set this to True if the layer to replace stores weight like (fan_in, fan_out)", "\n", "merge_weights", ":", "bool", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "nn", ".", "Linear", ".", "__init__", "(", "self", ",", "in_features", ",", "out_features", ",", "**", "kwargs", ")", "\n", "LoRALayer", ".", "__init__", "(", "self", ",", "r", "=", "r", ",", "lora_alpha", "=", "lora_alpha", ",", "lora_dropout", "=", "lora_dropout", ",", "\n", "merge_weights", "=", "merge_weights", ")", "\n", "\n", "self", ".", "fan_in_fan_out", "=", "fan_in_fan_out", "\n", "# Actual trainable parameters", "\n", "if", "r", ">", "0", ":", "\n", "            ", "self", ".", "lora_A", "=", "nn", ".", "Parameter", "(", "self", ".", "weight", ".", "new_zeros", "(", "(", "r", ",", "in_features", ")", ")", ")", "\n", "self", ".", "lora_B", "=", "nn", ".", "Parameter", "(", "self", ".", "weight", ".", "new_zeros", "(", "(", "out_features", ",", "r", ")", ")", ")", "\n", "self", ".", "scaling", "=", "self", ".", "lora_alpha", "/", "self", ".", "r", "\n", "# Freezing the pre-trained weight matrix", "\n", "self", ".", "weight", ".", "requires_grad", "=", "False", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "if", "fan_in_fan_out", ":", "\n", "            ", "self", ".", "weight", ".", "data", "=", "self", ".", "weight", ".", "data", ".", "T", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Linear.reset_parameters": [[121, 127], ["torch.Linear.reset_parameters", "torch.Linear.reset_parameters", "torch.Linear.reset_parameters", "hasattr", "torch.init.kaiming_uniform_", "torch.init.kaiming_uniform_", "torch.init.kaiming_uniform_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.reset_parameters", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.reset_parameters", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.reset_parameters"], ["", "", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "Linear", ".", "reset_parameters", "(", "self", ")", "\n", "if", "hasattr", "(", "self", ",", "'lora_A'", ")", ":", "\n", "# initialize A the same way as the default for nn.Linear and B to zero", "\n", "            ", "nn", ".", "init", ".", "kaiming_uniform_", "(", "self", ".", "lora_A", ",", "a", "=", "math", ".", "sqrt", "(", "5", ")", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "lora_B", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Linear.train": [[128, 137], ["torch.Linear.train", "torch.Linear.train", "torch.Linear.train", "layers.Linear.train.T"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.train", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.train", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.train"], ["", "", "def", "train", "(", "self", ",", "mode", ":", "bool", "=", "True", ")", ":", "\n", "        ", "def", "T", "(", "w", ")", ":", "\n", "            ", "return", "w", ".", "T", "if", "self", ".", "fan_in_fan_out", "else", "w", "\n", "", "nn", ".", "Linear", ".", "train", "(", "self", ",", "mode", ")", "\n", "if", "self", ".", "merge_weights", "and", "self", ".", "merged", ":", "\n", "# Make sure that the weights are not merged", "\n", "            ", "if", "self", ".", "r", ">", "0", ":", "\n", "                ", "self", ".", "weight", ".", "data", "-=", "T", "(", "self", ".", "lora_B", "@", "self", ".", "lora_A", ")", "*", "self", ".", "scaling", "\n", "", "self", ".", "merged", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Linear.eval": [[138, 147], ["torch.Linear.eval", "torch.Linear.eval", "torch.Linear.eval", "layers.Linear.train.T"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.eval", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.eval", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.eval"], ["", "", "def", "eval", "(", "self", ")", ":", "\n", "        ", "def", "T", "(", "w", ")", ":", "\n", "            ", "return", "w", ".", "T", "if", "self", ".", "fan_in_fan_out", "else", "w", "\n", "", "nn", ".", "Linear", ".", "eval", "(", "self", ")", "\n", "if", "self", ".", "merge_weights", "and", "not", "self", ".", "merged", ":", "\n", "# Merge the weights and mark it", "\n", "            ", "if", "self", ".", "r", ">", "0", ":", "\n", "                ", "self", ".", "weight", ".", "data", "+=", "T", "(", "self", ".", "lora_B", "@", "self", ".", "lora_A", ")", "*", "self", ".", "scaling", "\n", "", "self", ".", "merged", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Linear.forward": [[148, 158], ["torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "layers.Linear.train.T"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "def", "T", "(", "w", ")", ":", "\n", "            ", "return", "w", ".", "T", "if", "self", ".", "fan_in_fan_out", "else", "w", "\n", "", "if", "self", ".", "r", ">", "0", "and", "not", "self", ".", "merged", ":", "\n", "            ", "result", "=", "F", ".", "linear", "(", "x", ",", "T", "(", "self", ".", "weight", ")", ",", "bias", "=", "self", ".", "bias", ")", "\n", "if", "self", ".", "r", ">", "0", ":", "\n", "                ", "result", "+=", "(", "self", ".", "lora_dropout", "(", "x", ")", "@", "self", ".", "lora_A", ".", "T", "@", "self", ".", "lora_B", ".", "T", ")", "*", "self", ".", "scaling", "\n", "", "return", "result", "\n", "", "else", ":", "\n", "            ", "return", "F", ".", "linear", "(", "x", ",", "T", "(", "self", ".", "weight", ")", ",", "bias", "=", "self", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.MergedLinear.__init__": [[162, 200], ["torch.Linear.__init__", "torch.Linear.__init__", "torch.Linear.__init__", "layers.LoRALayer.__init__", "layers.MergedLinear.reset_parameters", "any", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "layers.MergedLinear.weight.new_zeros().view", "layers.MergedLinear.lora_ind.view", "len", "layers.MergedLinear.weight.new_zeros", "layers.MergedLinear.weight.new_zeros", "len", "layers.MergedLinear.weight.new_zeros", "sum", "sum", "len"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.reset_parameters"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "in_features", ":", "int", ",", "\n", "out_features", ":", "int", ",", "\n", "r", ":", "int", "=", "0", ",", "\n", "lora_alpha", ":", "int", "=", "1", ",", "\n", "lora_dropout", ":", "float", "=", "0.", ",", "\n", "enable_lora", ":", "List", "[", "bool", "]", "=", "[", "False", "]", ",", "\n", "fan_in_fan_out", ":", "bool", "=", "False", ",", "\n", "merge_weights", ":", "bool", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "nn", ".", "Linear", ".", "__init__", "(", "self", ",", "in_features", ",", "out_features", ",", "**", "kwargs", ")", "\n", "LoRALayer", ".", "__init__", "(", "self", ",", "r", "=", "r", ",", "lora_alpha", "=", "lora_alpha", ",", "lora_dropout", "=", "lora_dropout", ",", "\n", "merge_weights", "=", "merge_weights", ")", "\n", "assert", "out_features", "%", "len", "(", "enable_lora", ")", "==", "0", ",", "'The length of enable_lora must divide out_features'", "\n", "self", ".", "enable_lora", "=", "enable_lora", "\n", "self", ".", "fan_in_fan_out", "=", "fan_in_fan_out", "\n", "# Actual trainable parameters", "\n", "if", "r", ">", "0", "and", "any", "(", "enable_lora", ")", ":", "\n", "            ", "self", ".", "lora_A", "=", "nn", ".", "Parameter", "(", "\n", "self", ".", "weight", ".", "new_zeros", "(", "(", "r", "*", "sum", "(", "enable_lora", ")", ",", "in_features", ")", ")", ")", "\n", "self", ".", "lora_B", "=", "nn", ".", "Parameter", "(", "\n", "self", ".", "weight", ".", "new_zeros", "(", "(", "out_features", "//", "len", "(", "enable_lora", ")", "*", "sum", "(", "enable_lora", ")", ",", "r", ")", ")", "\n", ")", "# weights for Conv1D with groups=sum(enable_lora)", "\n", "self", ".", "scaling", "=", "self", ".", "lora_alpha", "/", "self", ".", "r", "\n", "# Freezing the pre-trained weight matrix", "\n", "self", ".", "weight", ".", "requires_grad", "=", "False", "\n", "# Compute the indices", "\n", "self", ".", "lora_ind", "=", "self", ".", "weight", ".", "new_zeros", "(", "\n", "(", "out_features", ",", ")", ",", "dtype", "=", "torch", ".", "bool", "\n", ")", ".", "view", "(", "len", "(", "enable_lora", ")", ",", "-", "1", ")", "\n", "self", ".", "lora_ind", "[", "enable_lora", ",", ":", "]", "=", "True", "\n", "self", ".", "lora_ind", "=", "self", ".", "lora_ind", ".", "view", "(", "-", "1", ")", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "if", "fan_in_fan_out", ":", "\n", "            ", "self", ".", "weight", ".", "data", "=", "self", ".", "weight", ".", "data", ".", "T", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.MergedLinear.reset_parameters": [[201, 207], ["torch.Linear.reset_parameters", "torch.Linear.reset_parameters", "torch.Linear.reset_parameters", "hasattr", "torch.init.kaiming_uniform_", "torch.init.kaiming_uniform_", "torch.init.kaiming_uniform_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.reset_parameters", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.reset_parameters", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.reset_parameters"], ["", "", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "Linear", ".", "reset_parameters", "(", "self", ")", "\n", "if", "hasattr", "(", "self", ",", "'lora_A'", ")", ":", "\n", "# initialize A the same way as the default for nn.Linear and B to zero", "\n", "            ", "nn", ".", "init", ".", "kaiming_uniform_", "(", "self", ".", "lora_A", ",", "a", "=", "math", ".", "sqrt", "(", "5", ")", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "lora_B", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.MergedLinear.zero_pad": [[208, 215], ["x.new_zeros", "result.view.view.view", "x.reshape", "result.view.view.view", "sum", "len"], "methods", ["None"], ["", "", "def", "zero_pad", "(", "self", ",", "x", ")", ":", "\n", "        ", "result", "=", "x", ".", "new_zeros", "(", "(", "*", "x", ".", "shape", "[", ":", "-", "1", "]", ",", "self", ".", "out_features", ")", ")", "\n", "result", "=", "result", ".", "view", "(", "-", "1", ",", "self", ".", "out_features", ")", "\n", "result", "[", ":", ",", "self", ".", "lora_ind", "]", "=", "x", ".", "reshape", "(", "\n", "-", "1", ",", "self", ".", "out_features", "//", "len", "(", "self", ".", "enable_lora", ")", "*", "sum", "(", "self", ".", "enable_lora", ")", "\n", ")", "\n", "return", "result", ".", "view", "(", "(", "*", "x", ".", "shape", "[", ":", "-", "1", "]", ",", "self", ".", "out_features", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.MergedLinear.train": [[216, 230], ["torch.Linear.train", "torch.Linear.train", "torch.Linear.train", "any", "torch.conv1d().squeeze", "torch.conv1d().squeeze", "torch.conv1d().squeeze", "layers.MergedLinear.zero_pad", "layers.MergedLinear.train.T"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.train", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.train", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.train", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.MergedLinear.zero_pad"], ["", "def", "train", "(", "self", ",", "mode", ":", "bool", "=", "True", ")", ":", "\n", "        ", "def", "T", "(", "w", ")", ":", "\n", "            ", "return", "w", ".", "T", "if", "self", ".", "fan_in_fan_out", "else", "w", "\n", "", "nn", ".", "Linear", ".", "train", "(", "self", ",", "mode", ")", "\n", "if", "self", ".", "merge_weights", "and", "self", ".", "merged", ":", "\n", "# Make sure that the weights are not merged", "\n", "            ", "if", "self", ".", "r", ">", "0", "and", "any", "(", "self", ".", "enable_lora", ")", ":", "\n", "                ", "delta_w", "=", "F", ".", "conv1d", "(", "\n", "self", ".", "lora_A", ".", "data", ".", "unsqueeze", "(", "0", ")", ",", "\n", "self", ".", "lora_B", ".", "data", ".", "unsqueeze", "(", "-", "1", ")", ",", "\n", "groups", "=", "sum", "(", "self", ".", "enable_lora", ")", "\n", ")", ".", "squeeze", "(", "0", ")", "\n", "self", ".", "weight", ".", "data", "-=", "self", ".", "zero_pad", "(", "T", "(", "delta_w", "*", "self", ".", "scaling", ")", ")", "\n", "", "self", ".", "merged", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.MergedLinear.eval": [[231, 245], ["torch.Linear.eval", "torch.Linear.eval", "torch.Linear.eval", "any", "torch.conv1d().squeeze", "torch.conv1d().squeeze", "torch.conv1d().squeeze", "layers.MergedLinear.zero_pad", "layers.MergedLinear.train.T"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.eval", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.eval", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.eval", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.MergedLinear.zero_pad"], ["", "", "def", "eval", "(", "self", ")", ":", "\n", "        ", "def", "T", "(", "w", ")", ":", "\n", "            ", "return", "w", ".", "T", "if", "self", ".", "fan_in_fan_out", "else", "w", "\n", "", "nn", ".", "Linear", ".", "eval", "(", "self", ")", "\n", "if", "self", ".", "merge_weights", "and", "not", "self", ".", "merged", ":", "\n", "# Merge the weights and mark it", "\n", "            ", "if", "self", ".", "r", ">", "0", "and", "any", "(", "self", ".", "enable_lora", ")", ":", "\n", "                ", "delta_w", "=", "F", ".", "conv1d", "(", "\n", "self", ".", "lora_A", ".", "data", ".", "unsqueeze", "(", "0", ")", ",", "\n", "self", ".", "lora_B", ".", "data", ".", "unsqueeze", "(", "-", "1", ")", ",", "\n", "groups", "=", "sum", "(", "self", ".", "enable_lora", ")", "\n", ")", ".", "squeeze", "(", "0", ")", "\n", "self", ".", "weight", ".", "data", "+=", "self", ".", "zero_pad", "(", "T", "(", "delta_w", "*", "self", ".", "scaling", ")", ")", "\n", "", "self", ".", "merged", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.MergedLinear.forward": [[246, 262], ["torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "layers.MergedLinear.train.T"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "def", "T", "(", "w", ")", ":", "\n", "            ", "return", "w", ".", "T", "if", "self", ".", "fan_in_fan_out", "else", "w", "\n", "", "if", "self", ".", "merged", ":", "\n", "            ", "return", "F", ".", "linear", "(", "x", ",", "T", "(", "self", ".", "weight", ")", ",", "bias", "=", "self", ".", "bias", ")", "\n", "", "else", ":", "\n", "            ", "result", "=", "F", ".", "linear", "(", "x", ",", "T", "(", "self", ".", "weight", ")", ",", "bias", "=", "self", ".", "bias", ")", "\n", "if", "self", ".", "r", ">", "0", ":", "\n", "                ", "after_A", "=", "F", ".", "linear", "(", "self", ".", "lora_dropout", "(", "x", ")", ",", "self", ".", "lora_A", ")", "\n", "after_B", "=", "F", ".", "conv1d", "(", "\n", "after_A", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ",", "\n", "self", ".", "lora_B", ".", "unsqueeze", "(", "-", "1", ")", ",", "\n", "groups", "=", "sum", "(", "self", ".", "enable_lora", ")", "\n", ")", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", "\n", "result", "+=", "self", ".", "zero_pad", "(", "after_B", ")", "*", "self", ".", "scaling", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.__init__": [[266, 293], ["torch.Conv2d.__init__", "torch.Conv2d.__init__", "torch.Conv2d.__init__", "layers.LoRALayer.__init__", "layers.Conv2d.reset_parameters", "type", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "layers.Conv2d.weight.new_zeros", "layers.Conv2d.weight.new_zeros"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.reset_parameters"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "in_channels", ":", "int", ",", "\n", "out_channels", ":", "int", ",", "\n", "kernel_size", ":", "int", ",", "\n", "r", ":", "int", "=", "0", ",", "\n", "lora_alpha", ":", "int", "=", "1", ",", "\n", "lora_dropout", ":", "float", "=", "0.", ",", "\n", "merge_weights", ":", "bool", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "nn", ".", "Conv2d", ".", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "LoRALayer", ".", "__init__", "(", "self", ",", "r", "=", "r", ",", "lora_alpha", "=", "lora_alpha", ",", "lora_dropout", "=", "lora_dropout", ",", "\n", "merge_weights", "=", "merge_weights", ")", "\n", "assert", "type", "(", "kernel_size", ")", "is", "int", "\n", "# Actual trainable parameters", "\n", "if", "r", ">", "0", ":", "\n", "            ", "self", ".", "lora_A", "=", "nn", ".", "Parameter", "(", "\n", "self", ".", "weight", ".", "new_zeros", "(", "(", "r", "*", "kernel_size", ",", "in_channels", "*", "kernel_size", ")", ")", "\n", ")", "\n", "self", ".", "lora_B", "=", "nn", ".", "Parameter", "(", "\n", "self", ".", "weight", ".", "new_zeros", "(", "(", "out_channels", "*", "kernel_size", ",", "r", "*", "kernel_size", ")", ")", "\n", ")", "\n", "self", ".", "scaling", "=", "self", ".", "lora_alpha", "/", "self", ".", "r", "\n", "# Freezing the pre-trained weight matrix", "\n", "self", ".", "weight", ".", "requires_grad", "=", "False", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.reset_parameters": [[294, 300], ["torch.Conv2d.reset_parameters", "torch.Conv2d.reset_parameters", "torch.Conv2d.reset_parameters", "hasattr", "torch.init.kaiming_uniform_", "torch.init.kaiming_uniform_", "torch.init.kaiming_uniform_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.reset_parameters", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.reset_parameters", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.reset_parameters"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "Conv2d", ".", "reset_parameters", "(", "self", ")", "\n", "if", "hasattr", "(", "self", ",", "'lora_A'", ")", ":", "\n", "# initialize A the same way as the default for nn.Linear and B to zero", "\n", "            ", "nn", ".", "init", ".", "kaiming_uniform_", "(", "self", ".", "lora_A", ",", "a", "=", "math", ".", "sqrt", "(", "5", ")", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "lora_B", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.train": [[301, 307], ["torch.Conv2d.train", "torch.Conv2d.train", "torch.Conv2d.train"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.train", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.train", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.train"], ["", "", "def", "train", "(", "self", ",", "mode", ":", "bool", "=", "True", ")", ":", "\n", "        ", "nn", ".", "Conv2d", ".", "train", "(", "self", ",", "mode", ")", "\n", "if", "self", ".", "merge_weights", "and", "self", ".", "merged", ":", "\n", "# Make sure that the weights are not merged", "\n", "            ", "self", ".", "weight", ".", "data", "-=", "(", "self", ".", "lora_B", "@", "self", ".", "lora_A", ")", ".", "view", "(", "self", ".", "weight", ".", "shape", ")", "*", "self", ".", "scaling", "\n", "self", ".", "merged", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.eval": [[308, 314], ["torch.Conv2d.eval", "torch.Conv2d.eval", "torch.Conv2d.eval"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.eval", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.eval", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.eval"], ["", "", "def", "eval", "(", "self", ")", ":", "\n", "        ", "nn", ".", "Conv2d", ".", "eval", "(", "self", ")", "\n", "if", "self", ".", "merge_weights", "and", "not", "self", ".", "merged", ":", "\n", "# Merge the weights and mark it", "\n", "            ", "self", ".", "weight", ".", "data", "+=", "(", "self", ".", "lora_B", "@", "self", ".", "lora_A", ")", ".", "view", "(", "self", ".", "weight", ".", "shape", ")", "*", "self", ".", "scaling", "\n", "self", ".", "merged", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.forward": [[315, 323], ["torch.Conv2d.forward", "torch.Conv2d.forward", "torch.Conv2d.forward", "torch.conv2d", "torch.conv2d", "torch.conv2d"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForQuestionAnswering.forward", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForQuestionAnswering.forward", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForQuestionAnswering.forward"], ["", "", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "if", "self", ".", "r", ">", "0", "and", "not", "self", ".", "merged", ":", "\n", "            ", "return", "F", ".", "conv2d", "(", "\n", "x", ",", "\n", "self", ".", "weight", "+", "(", "self", ".", "lora_B", "@", "self", ".", "lora_A", ")", ".", "view", "(", "self", ".", "weight", ".", "shape", ")", "*", "self", ".", "scaling", ",", "\n", "self", ".", "bias", ",", "self", ".", "stride", ",", "self", ".", "padding", ",", "self", ".", "dilation", ",", "self", ".", "groups", "\n", ")", "\n", "", "return", "nn", ".", "Conv2d", ".", "forward", "(", "self", ",", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.utils.mark_only_lora_as_trainable": [[13, 31], ["model.named_parameters", "model.named_parameters", "model.modules", "isinstance", "hasattr"], "function", ["None"], [")", "\n", "\n", "from", "transformers", "import", "(", "\n", "AutoConfig", ",", "\n", "AutoModelForTokenClassification", ",", "\n", "AutoModelForSequenceClassification", ",", "\n", "AutoModelForQuestionAnswering", ",", "\n", "AutoModelForMultipleChoice", "\n", ")", "\n", "\n", "\n", "\n", "class", "TaskType", "(", "Enum", ")", ":", "\n", "    ", "TOKEN_CLASSIFICATION", "=", "1", ",", "\n", "SEQUENCE_CLASSIFICATION", "=", "2", ",", "\n", "QUESTION_ANSWERING", "=", "3", ",", "\n", "MULTIPLE_CHOICE", "=", "4", "\n", "\n", "", "PREFIX_MODELS", "=", "{", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.utils.lora_state_dict": [[33, 50], ["model.state_dict", "k.split"], "function", ["None"], ["\"roberta\"", ":", "{", "\n", "TaskType", ".", "SEQUENCE_CLASSIFICATION", ":", "RobertaPrefixForSequenceClassification", ",", "\n", "TaskType", ".", "MULTIPLE_CHOICE", ":", "RobertaPrefixForMultipleChoice", ",", "\n", "\n", "}", ",", "\n", "}", "\n", "\n", "\n", "\n", "AUTO_MODELS", "=", "{", "\n", "TaskType", ".", "TOKEN_CLASSIFICATION", ":", "AutoModelForTokenClassification", ",", "\n", "TaskType", ".", "SEQUENCE_CLASSIFICATION", ":", "AutoModelForSequenceClassification", ",", "\n", "TaskType", ".", "QUESTION_ANSWERING", ":", "AutoModelForQuestionAnswering", ",", "\n", "TaskType", ".", "MULTIPLE_CHOICE", ":", "AutoModelForMultipleChoice", ",", "\n", "}", "\n", "\n", "LORA_MODELS", "=", "{", "\n", "\"roberta\"", ":", "{", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.convert_roberta_original_pytorch_checkpoint_to_pytorch.convert_roberta_checkpoint_to_pytorch": [[48, 161], ["fairseq.models.roberta.RobertaModel.from_pretrained", "FairseqRobertaModel.from_pretrained.eval", "transformers.RobertaConfig", "print", "model.eval", "torch.zeros_like", "range", "FairseqRobertaModel.from_pretrained.encode().unsqueeze", "print", "torch.max().item", "print", "torch.allclose", "print", "pathlib.Path().mkdir", "print", "model.save_pretrained", "transformers.RobertaForSequenceClassification", "transformers.RobertaForMaskedLM", "model", "Exception", "torch.Size", "FairseqRobertaModel.from_pretrained.encode", "FairseqRobertaModel.from_pretrained.extract_features", "FairseqRobertaModel.from_pretrained.model", "torch.max", "pathlib.Path", "torch.abs"], "function", ["home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.eval", "home.repos.pwc.inspect_result.guanzhchen_petuning.lora.layers.Conv2d.eval"], ["def", "convert_roberta_checkpoint_to_pytorch", "(", "\n", "roberta_checkpoint_path", ":", "str", ",", "pytorch_dump_folder_path", ":", "str", ",", "classification_head", ":", "bool", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Copy/paste/tweak roberta's weights to our BERT structure.\n    \"\"\"", "\n", "roberta", "=", "FairseqRobertaModel", ".", "from_pretrained", "(", "roberta_checkpoint_path", ")", "\n", "roberta", ".", "eval", "(", ")", "# disable dropout", "\n", "roberta_sent_encoder", "=", "roberta", ".", "model", ".", "encoder", ".", "sentence_encoder", "\n", "config", "=", "RobertaConfig", "(", "\n", "vocab_size", "=", "roberta_sent_encoder", ".", "embed_tokens", ".", "num_embeddings", ",", "\n", "hidden_size", "=", "roberta", ".", "args", ".", "encoder_embed_dim", ",", "\n", "num_hidden_layers", "=", "roberta", ".", "args", ".", "encoder_layers", ",", "\n", "num_attention_heads", "=", "roberta", ".", "args", ".", "encoder_attention_heads", ",", "\n", "intermediate_size", "=", "roberta", ".", "args", ".", "encoder_ffn_embed_dim", ",", "\n", "max_position_embeddings", "=", "514", ",", "\n", "type_vocab_size", "=", "1", ",", "\n", "layer_norm_eps", "=", "1e-5", ",", "# PyTorch default used in fairseq", "\n", ")", "\n", "if", "classification_head", ":", "\n", "        ", "config", ".", "num_labels", "=", "roberta", ".", "model", ".", "classification_heads", "[", "\"mnli\"", "]", ".", "out_proj", ".", "weight", ".", "shape", "[", "0", "]", "\n", "", "print", "(", "\"Our BERT config:\"", ",", "config", ")", "\n", "\n", "model", "=", "RobertaForSequenceClassification", "(", "config", ")", "if", "classification_head", "else", "RobertaForMaskedLM", "(", "config", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "# Now let's copy all the weights.", "\n", "# Embeddings", "\n", "model", ".", "roberta", ".", "embeddings", ".", "word_embeddings", ".", "weight", "=", "roberta_sent_encoder", ".", "embed_tokens", ".", "weight", "\n", "model", ".", "roberta", ".", "embeddings", ".", "position_embeddings", ".", "weight", "=", "roberta_sent_encoder", ".", "embed_positions", ".", "weight", "\n", "model", ".", "roberta", ".", "embeddings", ".", "token_type_embeddings", ".", "weight", ".", "data", "=", "torch", ".", "zeros_like", "(", "\n", "model", ".", "roberta", ".", "embeddings", ".", "token_type_embeddings", ".", "weight", "\n", ")", "# just zero them out b/c RoBERTa doesn't use them.", "\n", "model", ".", "roberta", ".", "embeddings", ".", "LayerNorm", ".", "weight", "=", "roberta_sent_encoder", ".", "emb_layer_norm", ".", "weight", "\n", "model", ".", "roberta", ".", "embeddings", ".", "LayerNorm", ".", "bias", "=", "roberta_sent_encoder", ".", "emb_layer_norm", ".", "bias", "\n", "\n", "for", "i", "in", "range", "(", "config", ".", "num_hidden_layers", ")", ":", "\n", "# Encoder: start of layer", "\n", "        ", "layer", ":", "BertLayer", "=", "model", ".", "roberta", ".", "encoder", ".", "layer", "[", "i", "]", "\n", "roberta_layer", ":", "TransformerSentenceEncoderLayer", "=", "roberta_sent_encoder", ".", "layers", "[", "i", "]", "\n", "\n", "# self attention", "\n", "self_attn", ":", "BertSelfAttention", "=", "layer", ".", "attention", ".", "self", "\n", "assert", "(", "\n", "roberta_layer", ".", "self_attn", ".", "k_proj", ".", "weight", ".", "data", ".", "shape", "\n", "==", "roberta_layer", ".", "self_attn", ".", "q_proj", ".", "weight", ".", "data", ".", "shape", "\n", "==", "roberta_layer", ".", "self_attn", ".", "v_proj", ".", "weight", ".", "data", ".", "shape", "\n", "==", "torch", ".", "Size", "(", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", ")", "\n", ")", "\n", "\n", "self_attn", ".", "query", ".", "weight", ".", "data", "=", "roberta_layer", ".", "self_attn", ".", "q_proj", ".", "weight", "\n", "self_attn", ".", "query", ".", "bias", ".", "data", "=", "roberta_layer", ".", "self_attn", ".", "q_proj", ".", "bias", "\n", "self_attn", ".", "key", ".", "weight", ".", "data", "=", "roberta_layer", ".", "self_attn", ".", "k_proj", ".", "weight", "\n", "self_attn", ".", "key", ".", "bias", ".", "data", "=", "roberta_layer", ".", "self_attn", ".", "k_proj", ".", "bias", "\n", "self_attn", ".", "value", ".", "weight", ".", "data", "=", "roberta_layer", ".", "self_attn", ".", "v_proj", ".", "weight", "\n", "self_attn", ".", "value", ".", "bias", ".", "data", "=", "roberta_layer", ".", "self_attn", ".", "v_proj", ".", "bias", "\n", "\n", "# self-attention output", "\n", "self_output", ":", "BertSelfOutput", "=", "layer", ".", "attention", ".", "output", "\n", "assert", "self_output", ".", "dense", ".", "weight", ".", "shape", "==", "roberta_layer", ".", "self_attn", ".", "out_proj", ".", "weight", ".", "shape", "\n", "self_output", ".", "dense", ".", "weight", "=", "roberta_layer", ".", "self_attn", ".", "out_proj", ".", "weight", "\n", "self_output", ".", "dense", ".", "bias", "=", "roberta_layer", ".", "self_attn", ".", "out_proj", ".", "bias", "\n", "self_output", ".", "LayerNorm", ".", "weight", "=", "roberta_layer", ".", "self_attn_layer_norm", ".", "weight", "\n", "self_output", ".", "LayerNorm", ".", "bias", "=", "roberta_layer", ".", "self_attn_layer_norm", ".", "bias", "\n", "\n", "# intermediate", "\n", "intermediate", ":", "BertIntermediate", "=", "layer", ".", "intermediate", "\n", "assert", "intermediate", ".", "dense", ".", "weight", ".", "shape", "==", "roberta_layer", ".", "fc1", ".", "weight", ".", "shape", "\n", "intermediate", ".", "dense", ".", "weight", "=", "roberta_layer", ".", "fc1", ".", "weight", "\n", "intermediate", ".", "dense", ".", "bias", "=", "roberta_layer", ".", "fc1", ".", "bias", "\n", "\n", "# output", "\n", "bert_output", ":", "BertOutput", "=", "layer", ".", "output", "\n", "assert", "bert_output", ".", "dense", ".", "weight", ".", "shape", "==", "roberta_layer", ".", "fc2", ".", "weight", ".", "shape", "\n", "bert_output", ".", "dense", ".", "weight", "=", "roberta_layer", ".", "fc2", ".", "weight", "\n", "bert_output", ".", "dense", ".", "bias", "=", "roberta_layer", ".", "fc2", ".", "bias", "\n", "bert_output", ".", "LayerNorm", ".", "weight", "=", "roberta_layer", ".", "final_layer_norm", ".", "weight", "\n", "bert_output", ".", "LayerNorm", ".", "bias", "=", "roberta_layer", ".", "final_layer_norm", ".", "bias", "\n", "# end of layer", "\n", "\n", "", "if", "classification_head", ":", "\n", "        ", "model", ".", "classifier", ".", "dense", ".", "weight", "=", "roberta", ".", "model", ".", "classification_heads", "[", "\"mnli\"", "]", ".", "dense", ".", "weight", "\n", "model", ".", "classifier", ".", "dense", ".", "bias", "=", "roberta", ".", "model", ".", "classification_heads", "[", "\"mnli\"", "]", ".", "dense", ".", "bias", "\n", "model", ".", "classifier", ".", "out_proj", ".", "weight", "=", "roberta", ".", "model", ".", "classification_heads", "[", "\"mnli\"", "]", ".", "out_proj", ".", "weight", "\n", "model", ".", "classifier", ".", "out_proj", ".", "bias", "=", "roberta", ".", "model", ".", "classification_heads", "[", "\"mnli\"", "]", ".", "out_proj", ".", "bias", "\n", "", "else", ":", "\n", "# LM Head", "\n", "        ", "model", ".", "lm_head", ".", "dense", ".", "weight", "=", "roberta", ".", "model", ".", "encoder", ".", "lm_head", ".", "dense", ".", "weight", "\n", "model", ".", "lm_head", ".", "dense", ".", "bias", "=", "roberta", ".", "model", ".", "encoder", ".", "lm_head", ".", "dense", ".", "bias", "\n", "model", ".", "lm_head", ".", "layer_norm", ".", "weight", "=", "roberta", ".", "model", ".", "encoder", ".", "lm_head", ".", "layer_norm", ".", "weight", "\n", "model", ".", "lm_head", ".", "layer_norm", ".", "bias", "=", "roberta", ".", "model", ".", "encoder", ".", "lm_head", ".", "layer_norm", ".", "bias", "\n", "model", ".", "lm_head", ".", "decoder", ".", "weight", "=", "roberta", ".", "model", ".", "encoder", ".", "lm_head", ".", "weight", "\n", "model", ".", "lm_head", ".", "decoder", ".", "bias", "=", "roberta", ".", "model", ".", "encoder", ".", "lm_head", ".", "bias", "\n", "\n", "# Let's check that we get the same results.", "\n", "", "input_ids", ":", "torch", ".", "Tensor", "=", "roberta", ".", "encode", "(", "SAMPLE_TEXT", ")", ".", "unsqueeze", "(", "0", ")", "# batch of size 1", "\n", "\n", "our_output", "=", "model", "(", "input_ids", ")", "[", "0", "]", "\n", "if", "classification_head", ":", "\n", "        ", "their_output", "=", "roberta", ".", "model", ".", "classification_heads", "[", "\"mnli\"", "]", "(", "roberta", ".", "extract_features", "(", "input_ids", ")", ")", "\n", "", "else", ":", "\n", "        ", "their_output", "=", "roberta", ".", "model", "(", "input_ids", ")", "[", "0", "]", "\n", "", "print", "(", "our_output", ".", "shape", ",", "their_output", ".", "shape", ")", "\n", "max_absolute_diff", "=", "torch", ".", "max", "(", "torch", ".", "abs", "(", "our_output", "-", "their_output", ")", ")", ".", "item", "(", ")", "\n", "print", "(", "f\"max_absolute_diff = {max_absolute_diff}\"", ")", "# ~ 1e-7", "\n", "success", "=", "torch", ".", "allclose", "(", "our_output", ",", "their_output", ",", "atol", "=", "1e-3", ")", "\n", "print", "(", "\"Do both models output the same tensors?\"", ",", "\"\ud83d\udd25\"", "if", "success", "else", "\"\ud83d\udca9\"", ")", "\n", "if", "not", "success", ":", "\n", "        ", "raise", "Exception", "(", "\"Something went wRoNg\"", ")", "\n", "\n", "", "pathlib", ".", "Path", "(", "pytorch_dump_folder_path", ")", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "print", "(", "f\"Saving model to {pytorch_dump_folder_path}\"", ")", "\n", "model", ".", "save_pretrained", "(", "pytorch_dump_folder_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaEmbeddings.__init__": [[78, 90], ["super().__init__", "tensorflow.keras.layers.Add", "tensorflow.keras.layers.LayerNormalization", "tensorflow.keras.layers.Dropout"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n", "self", ".", "padding_idx", "=", "1", "\n", "self", ".", "vocab_size", "=", "config", ".", "vocab_size", "\n", "self", ".", "type_vocab_size", "=", "config", ".", "type_vocab_size", "\n", "self", ".", "hidden_size", "=", "config", ".", "hidden_size", "\n", "self", ".", "max_position_embeddings", "=", "config", ".", "max_position_embeddings", "\n", "self", ".", "initializer_range", "=", "config", ".", "initializer_range", "\n", "self", ".", "embeddings_sum", "=", "tf", ".", "keras", ".", "layers", ".", "Add", "(", ")", "\n", "self", ".", "LayerNorm", "=", "tf", ".", "keras", ".", "layers", ".", "LayerNormalization", "(", "epsilon", "=", "config", ".", "layer_norm_eps", ",", "name", "=", "\"LayerNorm\"", ")", "\n", "self", ".", "dropout", "=", "tf", ".", "keras", ".", "layers", ".", "Dropout", "(", "rate", "=", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaEmbeddings.build": [[91, 114], ["super().build", "tensorflow.name_scope", "modeling_tf_roberta.TFRobertaEmbeddings.add_weight", "tensorflow.name_scope", "modeling_tf_roberta.TFRobertaEmbeddings.add_weight", "tensorflow.name_scope", "modeling_tf_roberta.TFRobertaEmbeddings.add_weight", "transformers.modeling_tf_utils.get_initializer", "transformers.modeling_tf_utils.get_initializer", "transformers.modeling_tf_utils.get_initializer"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaLMHead.build"], ["", "def", "build", "(", "self", ",", "input_shape", ":", "tf", ".", "TensorShape", ")", ":", "\n", "        ", "with", "tf", ".", "name_scope", "(", "\"word_embeddings\"", ")", ":", "\n", "            ", "self", ".", "weight", "=", "self", ".", "add_weight", "(", "\n", "name", "=", "\"weight\"", ",", "\n", "shape", "=", "[", "self", ".", "vocab_size", ",", "self", ".", "hidden_size", "]", ",", "\n", "initializer", "=", "get_initializer", "(", "self", ".", "initializer_range", ")", ",", "\n", ")", "\n", "\n", "", "with", "tf", ".", "name_scope", "(", "\"token_type_embeddings\"", ")", ":", "\n", "            ", "self", ".", "token_type_embeddings", "=", "self", ".", "add_weight", "(", "\n", "name", "=", "\"embeddings\"", ",", "\n", "shape", "=", "[", "self", ".", "type_vocab_size", ",", "self", ".", "hidden_size", "]", ",", "\n", "initializer", "=", "get_initializer", "(", "self", ".", "initializer_range", ")", ",", "\n", ")", "\n", "\n", "", "with", "tf", ".", "name_scope", "(", "\"position_embeddings\"", ")", ":", "\n", "            ", "self", ".", "position_embeddings", "=", "self", ".", "add_weight", "(", "\n", "name", "=", "\"embeddings\"", ",", "\n", "shape", "=", "[", "self", ".", "max_position_embeddings", ",", "self", ".", "hidden_size", "]", ",", "\n", "initializer", "=", "get_initializer", "(", "self", ".", "initializer_range", ")", ",", "\n", ")", "\n", "\n", "", "super", "(", ")", ".", "build", "(", "input_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaEmbeddings.create_position_ids_from_input_ids": [[115, 128], ["tensorflow.cast", "tensorflow.math.not_equal", "tensorflow.math.cumsum"], "methods", ["None"], ["", "def", "create_position_ids_from_input_ids", "(", "self", ",", "input_ids", ")", ":", "\n", "        ", "\"\"\"\n        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding\n        symbols are ignored. This is modified from fairseq's `utils.make_positions`.\n\n        Args:\n            input_ids: tf.Tensor\n        Returns: tf.Tensor\n        \"\"\"", "\n", "mask", "=", "tf", ".", "cast", "(", "tf", ".", "math", ".", "not_equal", "(", "input_ids", ",", "self", ".", "padding_idx", ")", ",", "dtype", "=", "input_ids", ".", "dtype", ")", "\n", "incremental_indices", "=", "tf", ".", "math", ".", "cumsum", "(", "mask", ",", "axis", "=", "1", ")", "*", "mask", "\n", "\n", "return", "incremental_indices", "+", "self", ".", "padding_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaEmbeddings.call": [[129, 163], ["tensorflow.gather", "tensorflow.gather", "modeling_tf_roberta.TFRobertaEmbeddings.embeddings_sum", "modeling_tf_roberta.TFRobertaEmbeddings.LayerNorm", "modeling_tf_roberta.TFRobertaEmbeddings.dropout", "tensorflow.gather", "transformers.modeling_tf_utils.shape_list", "tensorflow.fill", "modeling_tf_roberta.TFRobertaEmbeddings.create_position_ids_from_input_ids", "tensorflow.expand_dims", "tensorflow.tile", "tensorflow.range"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.create_position_ids_from_input_ids"], ["", "def", "call", "(", "self", ",", "input_ids", "=", "None", ",", "position_ids", "=", "None", ",", "token_type_ids", "=", "None", ",", "inputs_embeds", "=", "None", ",", "training", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Applies embedding based on inputs tensor.\n\n        Returns:\n            final_embeddings (:obj:`tf.Tensor`): output embedding tensor.\n        \"\"\"", "\n", "assert", "not", "(", "input_ids", "is", "None", "and", "inputs_embeds", "is", "None", ")", "\n", "\n", "if", "input_ids", "is", "not", "None", ":", "\n", "            ", "inputs_embeds", "=", "tf", ".", "gather", "(", "params", "=", "self", ".", "weight", ",", "indices", "=", "input_ids", ")", "\n", "\n", "", "input_shape", "=", "shape_list", "(", "inputs_embeds", ")", "[", ":", "-", "1", "]", "\n", "\n", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "tf", ".", "fill", "(", "dims", "=", "input_shape", ",", "value", "=", "0", ")", "\n", "\n", "", "if", "position_ids", "is", "None", ":", "\n", "            ", "if", "input_ids", "is", "not", "None", ":", "\n", "# Create the position ids from the input token ids. Any padded tokens remain padded.", "\n", "                ", "position_ids", "=", "self", ".", "create_position_ids_from_input_ids", "(", "input_ids", "=", "input_ids", ")", "\n", "", "else", ":", "\n", "                ", "position_ids", "=", "tf", ".", "expand_dims", "(", "\n", "tf", ".", "range", "(", "start", "=", "self", ".", "padding_idx", "+", "1", ",", "limit", "=", "input_shape", "[", "-", "1", "]", "+", "self", ".", "padding_idx", "+", "1", ")", ",", "axis", "=", "0", "\n", ")", "\n", "position_ids", "=", "tf", ".", "tile", "(", "input", "=", "position_ids", ",", "multiples", "=", "(", "input_shape", "[", "0", "]", ",", "1", ")", ")", "\n", "\n", "", "", "position_embeds", "=", "tf", ".", "gather", "(", "params", "=", "self", ".", "position_embeddings", ",", "indices", "=", "position_ids", ")", "\n", "token_type_embeds", "=", "tf", ".", "gather", "(", "params", "=", "self", ".", "token_type_embeddings", ",", "indices", "=", "token_type_ids", ")", "\n", "final_embeddings", "=", "self", ".", "embeddings_sum", "(", "inputs", "=", "[", "inputs_embeds", ",", "position_embeds", ",", "token_type_embeds", "]", ")", "\n", "final_embeddings", "=", "self", ".", "LayerNorm", "(", "inputs", "=", "final_embeddings", ")", "\n", "final_embeddings", "=", "self", ".", "dropout", "(", "inputs", "=", "final_embeddings", ",", "training", "=", "training", ")", "\n", "\n", "return", "final_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaPooler.__init__": [[167, 175], ["super().__init__", "tensorflow.keras.layers.Dense", "transformers.modeling_tf_utils.get_initializer"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ":", "RobertaConfig", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n", "self", ".", "dense", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "units", "=", "config", ".", "hidden_size", ",", "\n", "kernel_initializer", "=", "get_initializer", "(", "config", ".", "initializer_range", ")", ",", "\n", "activation", "=", "\"tanh\"", ",", "\n", "name", "=", "\"dense\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaPooler.call": [[177, 184], ["modeling_tf_roberta.TFRobertaPooler.dense"], "methods", ["None"], ["", "def", "call", "(", "self", ",", "hidden_states", ":", "tf", ".", "Tensor", ")", "->", "tf", ".", "Tensor", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "inputs", "=", "first_token_tensor", ")", "\n", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaSelfAttention.__init__": [[188, 212], ["super().__init__", "int", "math.sqrt", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.Dropout", "ValueError", "transformers.modeling_tf_utils.get_initializer", "transformers.modeling_tf_utils.get_initializer", "transformers.modeling_tf_utils.get_initializer"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ":", "RobertaConfig", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"", "\n", "f\"of attention heads ({config.num_attention_heads})\"", "\n", ")", "\n", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "self", ".", "sqrt_att_head_size", "=", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "\n", "self", ".", "query", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "units", "=", "self", ".", "all_head_size", ",", "kernel_initializer", "=", "get_initializer", "(", "config", ".", "initializer_range", ")", ",", "name", "=", "\"query\"", "\n", ")", "\n", "self", ".", "key", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "units", "=", "self", ".", "all_head_size", ",", "kernel_initializer", "=", "get_initializer", "(", "config", ".", "initializer_range", ")", ",", "name", "=", "\"key\"", "\n", ")", "\n", "self", ".", "value", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "units", "=", "self", ".", "all_head_size", ",", "kernel_initializer", "=", "get_initializer", "(", "config", ".", "initializer_range", ")", ",", "name", "=", "\"value\"", "\n", ")", "\n", "self", ".", "dropout", "=", "tf", ".", "keras", ".", "layers", ".", "Dropout", "(", "rate", "=", "config", ".", "attention_probs_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaSelfAttention.transpose_for_scores": [[213, 219], ["tensorflow.reshape", "tensorflow.transpose"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "tensor", ":", "tf", ".", "Tensor", ",", "batch_size", ":", "int", ")", "->", "tf", ".", "Tensor", ":", "\n", "# Reshape from [batch_size, seq_length, all_head_size] to [batch_size, seq_length, num_attention_heads, attention_head_size]", "\n", "        ", "tensor", "=", "tf", ".", "reshape", "(", "tensor", "=", "tensor", ",", "shape", "=", "(", "batch_size", ",", "-", "1", ",", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", ")", "\n", "\n", "# Transpose the tensor from [batch_size, seq_length, num_attention_heads, attention_head_size] to [batch_size, num_attention_heads, seq_length, attention_head_size]", "\n", "return", "tf", ".", "transpose", "(", "tensor", ",", "perm", "=", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaSelfAttention.call": [[220, 265], ["modeling_tf_roberta.TFRobertaSelfAttention.query", "modeling_tf_roberta.TFRobertaSelfAttention.key", "modeling_tf_roberta.TFRobertaSelfAttention.value", "modeling_tf_roberta.TFRobertaSelfAttention.transpose_for_scores", "modeling_tf_roberta.TFRobertaSelfAttention.transpose_for_scores", "modeling_tf_roberta.TFRobertaSelfAttention.transpose_for_scores", "tensorflow.matmul", "tensorflow.cast", "tensorflow.divide", "tensorflow.nn.softmax", "modeling_tf_roberta.TFRobertaSelfAttention.dropout", "tensorflow.matmul", "tensorflow.transpose", "tensorflow.reshape", "transformers.modeling_tf_utils.shape_list", "tensorflow.add", "tensorflow.multiply"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaSelfAttention.transpose_for_scores"], ["", "def", "call", "(", "\n", "self", ",", "\n", "hidden_states", ":", "tf", ".", "Tensor", ",", "\n", "attention_mask", ":", "tf", ".", "Tensor", ",", "\n", "head_mask", ":", "tf", ".", "Tensor", ",", "\n", "output_attentions", ":", "bool", ",", "\n", "training", ":", "bool", "=", "False", ",", "\n", ")", "->", "Tuple", "[", "tf", ".", "Tensor", "]", ":", "\n", "        ", "batch_size", "=", "shape_list", "(", "hidden_states", ")", "[", "0", "]", "\n", "mixed_query_layer", "=", "self", ".", "query", "(", "inputs", "=", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "inputs", "=", "hidden_states", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "inputs", "=", "hidden_states", ")", "\n", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ",", "batch_size", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ",", "batch_size", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ",", "batch_size", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "# (batch size, num_heads, seq_len_q, seq_len_k)", "\n", "attention_scores", "=", "tf", ".", "matmul", "(", "query_layer", ",", "key_layer", ",", "transpose_b", "=", "True", ")", "\n", "dk", "=", "tf", ".", "cast", "(", "self", ".", "sqrt_att_head_size", ",", "dtype", "=", "attention_scores", ".", "dtype", ")", "\n", "attention_scores", "=", "tf", ".", "divide", "(", "attention_scores", ",", "dk", ")", "\n", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "# Apply the attention mask is (precomputed for all layers in TFRobertaModel call() function)", "\n", "            ", "attention_scores", "=", "tf", ".", "add", "(", "attention_scores", ",", "attention_mask", ")", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "", "attention_probs", "=", "tf", ".", "nn", ".", "softmax", "(", "logits", "=", "attention_scores", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "inputs", "=", "attention_probs", ",", "training", "=", "training", ")", "\n", "\n", "# Mask heads if we want to", "\n", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "attention_probs", "=", "tf", ".", "multiply", "(", "attention_probs", ",", "head_mask", ")", "\n", "\n", "", "attention_output", "=", "tf", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "attention_output", "=", "tf", ".", "transpose", "(", "attention_output", ",", "perm", "=", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n", "# (batch_size, seq_len_q, all_head_size)", "\n", "attention_output", "=", "tf", ".", "reshape", "(", "tensor", "=", "attention_output", ",", "shape", "=", "(", "batch_size", ",", "-", "1", ",", "self", ".", "all_head_size", ")", ")", "\n", "outputs", "=", "(", "attention_output", ",", "attention_probs", ")", "if", "output_attentions", "else", "(", "attention_output", ",", ")", "\n", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaSelfOutput.__init__": [[269, 277], ["super().__init__", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.LayerNormalization", "tensorflow.keras.layers.Dropout", "transformers.modeling_tf_utils.get_initializer"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ":", "RobertaConfig", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n", "self", ".", "dense", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "units", "=", "config", ".", "hidden_size", ",", "kernel_initializer", "=", "get_initializer", "(", "config", ".", "initializer_range", ")", ",", "name", "=", "\"dense\"", "\n", ")", "\n", "self", ".", "LayerNorm", "=", "tf", ".", "keras", ".", "layers", ".", "LayerNormalization", "(", "epsilon", "=", "config", ".", "layer_norm_eps", ",", "name", "=", "\"LayerNorm\"", ")", "\n", "self", ".", "dropout", "=", "tf", ".", "keras", ".", "layers", ".", "Dropout", "(", "rate", "=", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaSelfOutput.call": [[278, 284], ["modeling_tf_roberta.TFRobertaSelfOutput.dense", "modeling_tf_roberta.TFRobertaSelfOutput.dropout", "modeling_tf_roberta.TFRobertaSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "call", "(", "self", ",", "hidden_states", ":", "tf", ".", "Tensor", ",", "input_tensor", ":", "tf", ".", "Tensor", ",", "training", ":", "bool", "=", "False", ")", "->", "tf", ".", "Tensor", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "inputs", "=", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "inputs", "=", "hidden_states", ",", "training", "=", "training", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "inputs", "=", "hidden_states", "+", "input_tensor", ")", "\n", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaAttention.__init__": [[288, 293], ["super().__init__", "modeling_tf_roberta.TFRobertaSelfAttention", "modeling_tf_roberta.TFRobertaSelfOutput"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ":", "RobertaConfig", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n", "self", ".", "self_attention", "=", "TFRobertaSelfAttention", "(", "config", ",", "name", "=", "\"self\"", ")", "\n", "self", ".", "dense_output", "=", "TFRobertaSelfOutput", "(", "config", ",", "name", "=", "\"output\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaAttention.prune_heads": [[294, 296], ["None"], "methods", ["None"], ["", "def", "prune_heads", "(", "self", ",", "heads", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaAttention.call": [[297, 318], ["modeling_tf_roberta.TFRobertaAttention.self_attention", "modeling_tf_roberta.TFRobertaAttention.dense_output"], "methods", ["None"], ["", "def", "call", "(", "\n", "self", ",", "\n", "input_tensor", ":", "tf", ".", "Tensor", ",", "\n", "attention_mask", ":", "tf", ".", "Tensor", ",", "\n", "head_mask", ":", "tf", ".", "Tensor", ",", "\n", "output_attentions", ":", "bool", ",", "\n", "training", ":", "bool", "=", "False", ",", "\n", ")", "->", "Tuple", "[", "tf", ".", "Tensor", "]", ":", "\n", "        ", "self_outputs", "=", "self", ".", "self_attention", "(", "\n", "hidden_states", "=", "input_tensor", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "training", "=", "training", ",", "\n", ")", "\n", "attention_output", "=", "self", ".", "dense_output", "(", "\n", "hidden_states", "=", "self_outputs", "[", "0", "]", ",", "input_tensor", "=", "input_tensor", ",", "training", "=", "training", "\n", ")", "\n", "outputs", "=", "(", "attention_output", ",", ")", "+", "self_outputs", "[", "1", ":", "]", "# add attentions if we output them", "\n", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaIntermediate.__init__": [[322, 333], ["super().__init__", "tensorflow.keras.layers.Dense", "isinstance", "transformers.activations_tf.get_tf_activation", "transformers.modeling_tf_utils.get_initializer"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ":", "RobertaConfig", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n", "self", ".", "dense", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "units", "=", "config", ".", "intermediate_size", ",", "kernel_initializer", "=", "get_initializer", "(", "config", ".", "initializer_range", ")", ",", "name", "=", "\"dense\"", "\n", ")", "\n", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "get_tf_activation", "(", "config", ".", "hidden_act", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaIntermediate.call": [[334, 339], ["modeling_tf_roberta.TFRobertaIntermediate.dense", "modeling_tf_roberta.TFRobertaIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "", "def", "call", "(", "self", ",", "hidden_states", ":", "tf", ".", "Tensor", ")", "->", "tf", ".", "Tensor", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "inputs", "=", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaOutput.__init__": [[343, 351], ["super().__init__", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.LayerNormalization", "tensorflow.keras.layers.Dropout", "transformers.modeling_tf_utils.get_initializer"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ":", "RobertaConfig", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n", "self", ".", "dense", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "units", "=", "config", ".", "hidden_size", ",", "kernel_initializer", "=", "get_initializer", "(", "config", ".", "initializer_range", ")", ",", "name", "=", "\"dense\"", "\n", ")", "\n", "self", ".", "LayerNorm", "=", "tf", ".", "keras", ".", "layers", ".", "LayerNormalization", "(", "epsilon", "=", "config", ".", "layer_norm_eps", ",", "name", "=", "\"LayerNorm\"", ")", "\n", "self", ".", "dropout", "=", "tf", ".", "keras", ".", "layers", ".", "Dropout", "(", "rate", "=", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaOutput.call": [[352, 358], ["modeling_tf_roberta.TFRobertaOutput.dense", "modeling_tf_roberta.TFRobertaOutput.dropout", "modeling_tf_roberta.TFRobertaOutput.LayerNorm"], "methods", ["None"], ["", "def", "call", "(", "self", ",", "hidden_states", ":", "tf", ".", "Tensor", ",", "input_tensor", ":", "tf", ".", "Tensor", ",", "training", ":", "bool", "=", "False", ")", "->", "tf", ".", "Tensor", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "inputs", "=", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "inputs", "=", "hidden_states", ",", "training", "=", "training", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "inputs", "=", "hidden_states", "+", "input_tensor", ")", "\n", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaLayer.__init__": [[362, 368], ["super().__init__", "modeling_tf_roberta.TFRobertaAttention", "modeling_tf_roberta.TFRobertaIntermediate", "modeling_tf_roberta.TFRobertaOutput"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ":", "RobertaConfig", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n", "self", ".", "attention", "=", "TFRobertaAttention", "(", "config", ",", "name", "=", "\"attention\"", ")", "\n", "self", ".", "intermediate", "=", "TFRobertaIntermediate", "(", "config", ",", "name", "=", "\"intermediate\"", ")", "\n", "self", ".", "bert_output", "=", "TFRobertaOutput", "(", "config", ",", "name", "=", "\"output\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaLayer.call": [[369, 392], ["modeling_tf_roberta.TFRobertaLayer.attention", "modeling_tf_roberta.TFRobertaLayer.intermediate", "modeling_tf_roberta.TFRobertaLayer.bert_output"], "methods", ["None"], ["", "def", "call", "(", "\n", "self", ",", "\n", "hidden_states", ":", "tf", ".", "Tensor", ",", "\n", "attention_mask", ":", "tf", ".", "Tensor", ",", "\n", "head_mask", ":", "tf", ".", "Tensor", ",", "\n", "output_attentions", ":", "bool", ",", "\n", "training", ":", "bool", "=", "False", ",", "\n", ")", "->", "Tuple", "[", "tf", ".", "Tensor", "]", ":", "\n", "        ", "attention_outputs", "=", "self", ".", "attention", "(", "\n", "input_tensor", "=", "hidden_states", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "training", "=", "training", ",", "\n", ")", "\n", "attention_output", "=", "attention_outputs", "[", "0", "]", "\n", "intermediate_output", "=", "self", ".", "intermediate", "(", "hidden_states", "=", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "bert_output", "(", "\n", "hidden_states", "=", "intermediate_output", ",", "input_tensor", "=", "attention_output", ",", "training", "=", "training", "\n", ")", "\n", "outputs", "=", "(", "layer_output", ",", ")", "+", "attention_outputs", "[", "1", ":", "]", "# add attentions if we output them", "\n", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaEncoder.__init__": [[396, 400], ["super().__init__", "modeling_tf_roberta.TFRobertaLayer", "range"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ":", "RobertaConfig", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n", "self", ".", "layer", "=", "[", "TFRobertaLayer", "(", "config", ",", "name", "=", "f\"layer_._{i}\"", ")", "for", "i", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaEncoder.call": [[401, 439], ["enumerate", "transformers.modeling_tf_outputs.TFBaseModelOutput", "layer_module", "tuple"], "methods", ["None"], ["", "def", "call", "(", "\n", "self", ",", "\n", "hidden_states", ":", "tf", ".", "Tensor", ",", "\n", "attention_mask", ":", "tf", ".", "Tensor", ",", "\n", "head_mask", ":", "tf", ".", "Tensor", ",", "\n", "output_attentions", ":", "bool", ",", "\n", "output_hidden_states", ":", "bool", ",", "\n", "return_dict", ":", "bool", ",", "\n", "training", ":", "bool", "=", "False", ",", "\n", ")", "->", "Union", "[", "TFBaseModelOutput", ",", "Tuple", "[", "tf", ".", "Tensor", "]", "]", ":", "\n", "        ", "all_hidden_states", "=", "(", ")", "if", "output_hidden_states", "else", "None", "\n", "all_attentions", "=", "(", ")", "if", "output_attentions", "else", "None", "\n", "\n", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "            ", "if", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "layer_outputs", "=", "layer_module", "(", "\n", "hidden_states", "=", "hidden_states", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "head_mask", "=", "head_mask", "[", "i", "]", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "training", "=", "training", ",", "\n", ")", "\n", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "\n", "if", "output_attentions", ":", "\n", "                ", "all_attentions", "=", "all_attentions", "+", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "\n", "# Add last layer", "\n", "", "", "if", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "return", "tuple", "(", "v", "for", "v", "in", "[", "hidden_states", ",", "all_hidden_states", ",", "all_attentions", "]", "if", "v", "is", "not", "None", ")", "\n", "\n", "", "return", "TFBaseModelOutput", "(", "\n", "last_hidden_state", "=", "hidden_states", ",", "hidden_states", "=", "all_hidden_states", ",", "attentions", "=", "all_attentions", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaMainLayer.__init__": [[446, 459], ["super().__init__", "modeling_tf_roberta.TFRobertaEncoder", "modeling_tf_roberta.TFRobertaEmbeddings", "modeling_tf_roberta.TFRobertaPooler"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "add_pooling_layer", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "num_hidden_layers", "=", "config", ".", "num_hidden_layers", "\n", "self", ".", "initializer_range", "=", "config", ".", "initializer_range", "\n", "self", ".", "output_attentions", "=", "config", ".", "output_attentions", "\n", "self", ".", "output_hidden_states", "=", "config", ".", "output_hidden_states", "\n", "self", ".", "return_dict", "=", "config", ".", "use_return_dict", "\n", "self", ".", "encoder", "=", "TFRobertaEncoder", "(", "config", ",", "name", "=", "\"encoder\"", ")", "\n", "self", ".", "pooler", "=", "TFRobertaPooler", "(", "config", ",", "name", "=", "\"pooler\"", ")", "if", "add_pooling_layer", "else", "None", "\n", "# The embeddings must be the last declaration in order to follow the weights order", "\n", "self", ".", "embeddings", "=", "TFRobertaEmbeddings", "(", "config", ",", "name", "=", "\"embeddings\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaMainLayer.get_input_embeddings": [[461, 463], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", "->", "tf", ".", "keras", ".", "layers", ".", "Layer", ":", "\n", "        ", "return", "self", ".", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaMainLayer.set_input_embeddings": [[465, 468], ["transformers.modeling_tf_utils.shape_list"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ":", "tf", ".", "Variable", ")", ":", "\n", "        ", "self", ".", "embeddings", ".", "weight", "=", "value", "\n", "self", ".", "embeddings", ".", "vocab_size", "=", "shape_list", "(", "value", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaMainLayer._prune_heads": [[470, 476], ["None"], "methods", ["None"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaMainLayer.call": [[478, 582], ["transformers.modeling_tf_utils.input_processing", "modeling_tf_roberta.TFRobertaMainLayer.embeddings", "tensorflow.reshape", "tensorflow.cast", "tensorflow.constant", "tensorflow.constant", "tensorflow.multiply", "modeling_tf_roberta.TFRobertaMainLayer.encoder", "transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling", "ValueError", "tensorflow.fill", "tensorflow.fill", "tensorflow.subtract", "modeling_tf_roberta.TFRobertaMainLayer.pooler", "transformers.modeling_tf_utils.shape_list", "ValueError", "transformers.modeling_tf_utils.shape_list"], "methods", ["None"], ["", "def", "call", "(", "\n", "self", ",", "\n", "input_ids", ":", "Optional", "[", "TFModelInputType", "]", "=", "None", ",", "\n", "attention_mask", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "tf", ".", "Tensor", "]", "]", "=", "None", ",", "\n", "token_type_ids", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "tf", ".", "Tensor", "]", "]", "=", "None", ",", "\n", "position_ids", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "tf", ".", "Tensor", "]", "]", "=", "None", ",", "\n", "head_mask", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "tf", ".", "Tensor", "]", "]", "=", "None", ",", "\n", "inputs_embeds", ":", "Optional", "[", "Union", "[", "np", ".", "ndarray", ",", "tf", ".", "Tensor", "]", "]", "=", "None", ",", "\n", "output_attentions", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "output_hidden_states", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "return_dict", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "training", ":", "bool", "=", "False", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Union", "[", "TFBaseModelOutputWithPooling", ",", "Tuple", "[", "tf", ".", "Tensor", "]", "]", ":", "\n", "        ", "inputs", "=", "input_processing", "(", "\n", "func", "=", "self", ".", "call", ",", "\n", "config", "=", "self", ".", "config", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "training", "=", "training", ",", "\n", "kwargs_call", "=", "kwargs", ",", "\n", ")", "\n", "\n", "if", "inputs", "[", "\"input_ids\"", "]", "is", "not", "None", "and", "inputs", "[", "\"inputs_embeds\"", "]", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both input_ids and inputs_embeds at the same time\"", ")", "\n", "", "elif", "inputs", "[", "\"input_ids\"", "]", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "shape_list", "(", "inputs", "[", "\"input_ids\"", "]", ")", "\n", "", "elif", "inputs", "[", "\"inputs_embeds\"", "]", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "shape_list", "(", "inputs", "[", "\"inputs_embeds\"", "]", ")", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either input_ids or inputs_embeds\"", ")", "\n", "\n", "", "if", "inputs", "[", "\"attention_mask\"", "]", "is", "None", ":", "\n", "            ", "inputs", "[", "\"attention_mask\"", "]", "=", "tf", ".", "fill", "(", "dims", "=", "input_shape", ",", "value", "=", "1", ")", "\n", "\n", "", "if", "inputs", "[", "\"token_type_ids\"", "]", "is", "None", ":", "\n", "            ", "inputs", "[", "\"token_type_ids\"", "]", "=", "tf", ".", "fill", "(", "dims", "=", "input_shape", ",", "value", "=", "0", ")", "\n", "\n", "", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", "=", "inputs", "[", "\"input_ids\"", "]", ",", "\n", "position_ids", "=", "inputs", "[", "\"position_ids\"", "]", ",", "\n", "token_type_ids", "=", "inputs", "[", "\"token_type_ids\"", "]", ",", "\n", "inputs_embeds", "=", "inputs", "[", "\"inputs_embeds\"", "]", ",", "\n", "training", "=", "inputs", "[", "\"training\"", "]", ",", "\n", ")", "\n", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "extended_attention_mask", "=", "tf", ".", "reshape", "(", "inputs", "[", "\"attention_mask\"", "]", ",", "(", "input_shape", "[", "0", "]", ",", "1", ",", "1", ",", "input_shape", "[", "1", "]", ")", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "extended_attention_mask", "=", "tf", ".", "cast", "(", "extended_attention_mask", ",", "dtype", "=", "embedding_output", ".", "dtype", ")", "\n", "one_cst", "=", "tf", ".", "constant", "(", "1.0", ",", "dtype", "=", "embedding_output", ".", "dtype", ")", "\n", "ten_thousand_cst", "=", "tf", ".", "constant", "(", "-", "10000.0", ",", "dtype", "=", "embedding_output", ".", "dtype", ")", "\n", "extended_attention_mask", "=", "tf", ".", "multiply", "(", "tf", ".", "subtract", "(", "one_cst", ",", "extended_attention_mask", ")", ",", "ten_thousand_cst", ")", "\n", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "if", "inputs", "[", "\"head_mask\"", "]", "is", "not", "None", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "else", ":", "\n", "            ", "inputs", "[", "\"head_mask\"", "]", "=", "[", "None", "]", "*", "self", ".", "config", ".", "num_hidden_layers", "\n", "\n", "", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "hidden_states", "=", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "inputs", "[", "\"head_mask\"", "]", ",", "\n", "output_attentions", "=", "inputs", "[", "\"output_attentions\"", "]", ",", "\n", "output_hidden_states", "=", "inputs", "[", "\"output_hidden_states\"", "]", ",", "\n", "return_dict", "=", "inputs", "[", "\"return_dict\"", "]", ",", "\n", "training", "=", "inputs", "[", "\"training\"", "]", ",", "\n", ")", "\n", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "hidden_states", "=", "sequence_output", ")", "if", "self", ".", "pooler", "is", "not", "None", "else", "None", "\n", "\n", "if", "not", "inputs", "[", "\"return_dict\"", "]", ":", "\n", "            ", "return", "(", "\n", "sequence_output", ",", "\n", "pooled_output", ",", "\n", ")", "+", "encoder_outputs", "[", "1", ":", "]", "\n", "\n", "", "return", "TFBaseModelOutputWithPooling", "(", "\n", "last_hidden_state", "=", "sequence_output", ",", "\n", "pooler_output", "=", "pooled_output", ",", "\n", "hidden_states", "=", "encoder_outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "encoder_outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaPreTrainedModel.serving": [[594, 606], ["tensorflow.function", "modeling_tf_roberta.TFRobertaPreTrainedModel.call", "modeling_tf_roberta.TFRobertaPreTrainedModel.serving_output", "tensorflow.TensorSpec", "tensorflow.TensorSpec"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForQuestionAnswering.call", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForQuestionAnswering.serving_output"], ["@", "tf", ".", "function", "(", "\n", "input_signature", "=", "[", "\n", "{", "\n", "\"input_ids\"", ":", "tf", ".", "TensorSpec", "(", "(", "None", ",", "None", ")", ",", "tf", ".", "int32", ",", "name", "=", "\"input_ids\"", ")", ",", "\n", "\"attention_mask\"", ":", "tf", ".", "TensorSpec", "(", "(", "None", ",", "None", ")", ",", "tf", ".", "int32", ",", "name", "=", "\"attention_mask\"", ")", ",", "\n", "}", "\n", "]", "\n", ")", "\n", "def", "serving", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "output", "=", "self", ".", "call", "(", "inputs", ")", "\n", "\n", "return", "self", ".", "serving_output", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaModel.__init__": [[706, 709], ["transformers.modeling_tf_utils.TFPreTrainedModel.__init__", "modeling_tf_roberta.TFRobertaMainLayer"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "self", ".", "roberta", "=", "TFRobertaMainLayer", "(", "config", ",", "name", "=", "\"roberta\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaModel.call": [[710, 760], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "transformers.modeling_tf_utils.input_processing", "modeling_tf_roberta.TFRobertaModel.roberta", "ROBERTA_INPUTS_DOCSTRING.format"], "methods", ["None"], ["", "@", "add_start_docstrings_to_model_forward", "(", "ROBERTA_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "_CHECKPOINT_FOR_DOC", ",", "\n", "output_type", "=", "TFBaseModelOutputWithPooling", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "call", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "training", "=", "False", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "inputs", "=", "input_processing", "(", "\n", "func", "=", "self", ".", "call", ",", "\n", "config", "=", "self", ".", "config", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "training", "=", "training", ",", "\n", "kwargs_call", "=", "kwargs", ",", "\n", ")", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "input_ids", "=", "inputs", "[", "\"input_ids\"", "]", ",", "\n", "attention_mask", "=", "inputs", "[", "\"attention_mask\"", "]", ",", "\n", "token_type_ids", "=", "inputs", "[", "\"token_type_ids\"", "]", ",", "\n", "position_ids", "=", "inputs", "[", "\"position_ids\"", "]", ",", "\n", "head_mask", "=", "inputs", "[", "\"head_mask\"", "]", ",", "\n", "inputs_embeds", "=", "inputs", "[", "\"inputs_embeds\"", "]", ",", "\n", "output_attentions", "=", "inputs", "[", "\"output_attentions\"", "]", ",", "\n", "output_hidden_states", "=", "inputs", "[", "\"output_hidden_states\"", "]", ",", "\n", "return_dict", "=", "inputs", "[", "\"return_dict\"", "]", ",", "\n", "training", "=", "inputs", "[", "\"training\"", "]", ",", "\n", ")", "\n", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaModel.serving_output": [[762, 771], ["transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor"], "methods", ["None"], ["", "def", "serving_output", "(", "self", ",", "output", ":", "TFBaseModelOutputWithPooling", ")", "->", "TFBaseModelOutputWithPooling", ":", "\n", "        ", "hs", "=", "tf", ".", "convert_to_tensor", "(", "output", ".", "hidden_states", ")", "if", "self", ".", "config", ".", "output_hidden_states", "else", "None", "\n", "attns", "=", "tf", ".", "convert_to_tensor", "(", "output", ".", "attentions", ")", "if", "self", ".", "config", ".", "output_attentions", "else", "None", "\n", "\n", "return", "TFBaseModelOutputWithPooling", "(", "\n", "last_hidden_state", "=", "output", ".", "last_hidden_state", ",", "\n", "pooler_output", "=", "output", ".", "pooler_output", ",", "\n", "hidden_states", "=", "hs", ",", "\n", "attentions", "=", "attns", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaLMHead.__init__": [[777, 791], ["super().__init__", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.LayerNormalization", "transformers.activations_tf.get_tf_activation", "transformers.modeling_tf_utils.get_initializer"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "input_embeddings", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n", "self", ".", "vocab_size", "=", "config", ".", "vocab_size", "\n", "self", ".", "hidden_size", "=", "config", ".", "hidden_size", "\n", "self", ".", "dense", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "config", ".", "hidden_size", ",", "kernel_initializer", "=", "get_initializer", "(", "config", ".", "initializer_range", ")", ",", "name", "=", "\"dense\"", "\n", ")", "\n", "self", ".", "layer_norm", "=", "tf", ".", "keras", ".", "layers", ".", "LayerNormalization", "(", "epsilon", "=", "config", ".", "layer_norm_eps", ",", "name", "=", "\"layer_norm\"", ")", "\n", "self", ".", "act", "=", "get_tf_activation", "(", "\"gelu\"", ")", "\n", "\n", "# The output weights are the same as the input embeddings, but there is", "\n", "# an output-only bias for each token.", "\n", "self", ".", "decoder", "=", "input_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaLMHead.build": [[792, 796], ["modeling_tf_roberta.TFRobertaLMHead.add_weight", "super().build"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaLMHead.build"], ["", "def", "build", "(", "self", ",", "input_shape", ")", ":", "\n", "        ", "self", ".", "bias", "=", "self", ".", "add_weight", "(", "shape", "=", "(", "self", ".", "vocab_size", ",", ")", ",", "initializer", "=", "\"zeros\"", ",", "trainable", "=", "True", ",", "name", "=", "\"bias\"", ")", "\n", "\n", "super", "(", ")", ".", "build", "(", "input_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaLMHead.get_output_embeddings": [[797, 799], ["None"], "methods", ["None"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaLMHead.set_output_embeddings": [[800, 803], ["transformers.modeling_tf_utils.shape_list"], "methods", ["None"], ["", "def", "set_output_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "decoder", ".", "weight", "=", "value", "\n", "self", ".", "decoder", ".", "vocab_size", "=", "shape_list", "(", "value", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaLMHead.get_bias": [[804, 806], ["None"], "methods", ["None"], ["", "def", "get_bias", "(", "self", ")", ":", "\n", "        ", "return", "{", "\"bias\"", ":", "self", ".", "bias", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaLMHead.set_bias": [[807, 810], ["transformers.modeling_tf_utils.shape_list"], "methods", ["None"], ["", "def", "set_bias", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "bias", "=", "value", "[", "\"bias\"", "]", "\n", "self", ".", "vocab_size", "=", "shape_list", "(", "value", "[", "\"bias\"", "]", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaLMHead.call": [[811, 824], ["modeling_tf_roberta.TFRobertaLMHead.dense", "modeling_tf_roberta.TFRobertaLMHead.act", "modeling_tf_roberta.TFRobertaLMHead.layer_norm", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.nn.bias_add", "transformers.modeling_tf_utils.shape_list"], "methods", ["None"], ["", "def", "call", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "act", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "layer_norm", "(", "hidden_states", ")", "\n", "\n", "# project back to size of vocabulary with bias", "\n", "seq_length", "=", "shape_list", "(", "tensor", "=", "hidden_states", ")", "[", "1", "]", "\n", "hidden_states", "=", "tf", ".", "reshape", "(", "tensor", "=", "hidden_states", ",", "shape", "=", "[", "-", "1", ",", "self", ".", "hidden_size", "]", ")", "\n", "hidden_states", "=", "tf", ".", "matmul", "(", "a", "=", "hidden_states", ",", "b", "=", "self", ".", "decoder", ".", "weight", ",", "transpose_b", "=", "True", ")", "\n", "hidden_states", "=", "tf", ".", "reshape", "(", "tensor", "=", "hidden_states", ",", "shape", "=", "[", "-", "1", ",", "seq_length", ",", "self", ".", "vocab_size", "]", ")", "\n", "hidden_states", "=", "tf", ".", "nn", ".", "bias_add", "(", "value", "=", "hidden_states", ",", "bias", "=", "self", ".", "bias", ")", "\n", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForMaskedLM.__init__": [[831, 836], ["transformers.modeling_tf_utils.TFPreTrainedModel.__init__", "modeling_tf_roberta.TFRobertaMainLayer", "modeling_tf_roberta.TFRobertaLMHead"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "roberta", "=", "TFRobertaMainLayer", "(", "config", ",", "add_pooling_layer", "=", "False", ",", "name", "=", "\"roberta\"", ")", "\n", "self", ".", "lm_head", "=", "TFRobertaLMHead", "(", "config", ",", "self", ".", "roberta", ".", "embeddings", ",", "name", "=", "\"lm_head\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForMaskedLM.get_lm_head": [[837, 839], ["None"], "methods", ["None"], ["", "def", "get_lm_head", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "lm_head", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForMaskedLM.get_prefix_bias_name": [[840, 843], ["warnings.warn"], "methods", ["None"], ["", "def", "get_prefix_bias_name", "(", "self", ")", ":", "\n", "        ", "warnings", ".", "warn", "(", "\"The method get_prefix_bias_name is deprecated. Please use `get_bias` instead.\"", ",", "FutureWarning", ")", "\n", "return", "self", ".", "name", "+", "\"/\"", "+", "self", ".", "lm_head", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForMaskedLM.call": [[844, 915], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "transformers.modeling_tf_utils.input_processing", "modeling_tf_roberta.TFRobertaForMaskedLM.roberta", "modeling_tf_roberta.TFRobertaForMaskedLM.lm_head", "transformers.modeling_tf_outputs.TFMaskedLMOutput", "ROBERTA_INPUTS_DOCSTRING.format", "modeling_tf_roberta.TFRobertaForMaskedLM.compute_loss"], "methods", ["None"], ["", "@", "add_start_docstrings_to_model_forward", "(", "ROBERTA_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "_CHECKPOINT_FOR_DOC", ",", "\n", "output_type", "=", "TFMaskedLMOutput", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "call", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "training", "=", "False", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n        \"\"\"", "\n", "inputs", "=", "input_processing", "(", "\n", "func", "=", "self", ".", "call", ",", "\n", "config", "=", "self", ".", "config", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "labels", "=", "labels", ",", "\n", "training", "=", "training", ",", "\n", "kwargs_call", "=", "kwargs", ",", "\n", ")", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "inputs", "[", "\"input_ids\"", "]", ",", "\n", "attention_mask", "=", "inputs", "[", "\"attention_mask\"", "]", ",", "\n", "token_type_ids", "=", "inputs", "[", "\"token_type_ids\"", "]", ",", "\n", "position_ids", "=", "inputs", "[", "\"position_ids\"", "]", ",", "\n", "head_mask", "=", "inputs", "[", "\"head_mask\"", "]", ",", "\n", "inputs_embeds", "=", "inputs", "[", "\"inputs_embeds\"", "]", ",", "\n", "output_attentions", "=", "inputs", "[", "\"output_attentions\"", "]", ",", "\n", "output_hidden_states", "=", "inputs", "[", "\"output_hidden_states\"", "]", ",", "\n", "return_dict", "=", "inputs", "[", "\"return_dict\"", "]", ",", "\n", "training", "=", "inputs", "[", "\"training\"", "]", ",", "\n", ")", "\n", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "prediction_scores", "=", "self", ".", "lm_head", "(", "sequence_output", ")", "\n", "\n", "loss", "=", "None", "if", "inputs", "[", "\"labels\"", "]", "is", "None", "else", "self", ".", "compute_loss", "(", "inputs", "[", "\"labels\"", "]", ",", "prediction_scores", ")", "\n", "\n", "if", "not", "inputs", "[", "\"return_dict\"", "]", ":", "\n", "            ", "output", "=", "(", "prediction_scores", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "TFMaskedLMOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "prediction_scores", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForMaskedLM.serving_output": [[918, 923], ["transformers.modeling_tf_outputs.TFMaskedLMOutput", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor"], "methods", ["None"], ["", "def", "serving_output", "(", "self", ",", "output", ":", "TFMaskedLMOutput", ")", "->", "TFMaskedLMOutput", ":", "\n", "        ", "hs", "=", "tf", ".", "convert_to_tensor", "(", "output", ".", "hidden_states", ")", "if", "self", ".", "config", ".", "output_hidden_states", "else", "None", "\n", "attns", "=", "tf", ".", "convert_to_tensor", "(", "output", ".", "attentions", ")", "if", "self", ".", "config", ".", "output_attentions", "else", "None", "\n", "\n", "return", "TFMaskedLMOutput", "(", "logits", "=", "output", ".", "logits", ",", "hidden_states", "=", "hs", ",", "attentions", "=", "attns", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaClassificationHead.__init__": [[928, 942], ["super().__init__", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.Dropout", "tensorflow.keras.layers.Dense", "transformers.modeling_tf_utils.get_initializer", "transformers.modeling_tf_utils.get_initializer"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "dense", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "config", ".", "hidden_size", ",", "\n", "kernel_initializer", "=", "get_initializer", "(", "config", ".", "initializer_range", ")", ",", "\n", "activation", "=", "\"tanh\"", ",", "\n", "name", "=", "\"dense\"", ",", "\n", ")", "\n", "classifier_dropout", "=", "(", "\n", "config", ".", "classifier_dropout", "if", "config", ".", "classifier_dropout", "is", "not", "None", "else", "config", ".", "hidden_dropout_prob", "\n", ")", "\n", "self", ".", "dropout", "=", "tf", ".", "keras", ".", "layers", ".", "Dropout", "(", "classifier_dropout", ")", "\n", "self", ".", "out_proj", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "config", ".", "num_labels", ",", "kernel_initializer", "=", "get_initializer", "(", "config", ".", "initializer_range", ")", ",", "name", "=", "\"out_proj\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaClassificationHead.call": [[944, 951], ["modeling_tf_roberta.TFRobertaClassificationHead.dropout", "modeling_tf_roberta.TFRobertaClassificationHead.dense", "modeling_tf_roberta.TFRobertaClassificationHead.dropout", "modeling_tf_roberta.TFRobertaClassificationHead.out_proj"], "methods", ["None"], ["", "def", "call", "(", "self", ",", "features", ",", "training", "=", "False", ")", ":", "\n", "        ", "x", "=", "features", "[", ":", ",", "0", ",", ":", "]", "# take <s> token (equiv. to [CLS])", "\n", "x", "=", "self", ".", "dropout", "(", "x", ",", "training", "=", "training", ")", "\n", "x", "=", "self", ".", "dense", "(", "x", ")", "\n", "x", "=", "self", ".", "dropout", "(", "x", ",", "training", "=", "training", ")", "\n", "x", "=", "self", ".", "out_proj", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForSequenceClassification.__init__": [[964, 970], ["transformers.modeling_tf_utils.TFPreTrainedModel.__init__", "modeling_tf_roberta.TFRobertaMainLayer", "modeling_tf_roberta.TFRobertaClassificationHead"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "\n", "self", ".", "roberta", "=", "TFRobertaMainLayer", "(", "config", ",", "add_pooling_layer", "=", "False", ",", "name", "=", "\"roberta\"", ")", "\n", "self", ".", "classifier", "=", "TFRobertaClassificationHead", "(", "config", ",", "name", "=", "\"classifier\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForSequenceClassification.call": [[971, 1041], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "transformers.modeling_tf_utils.input_processing", "modeling_tf_roberta.TFRobertaForSequenceClassification.roberta", "modeling_tf_roberta.TFRobertaForSequenceClassification.classifier", "transformers.modeling_tf_outputs.TFSequenceClassifierOutput", "ROBERTA_INPUTS_DOCSTRING.format", "modeling_tf_roberta.TFRobertaForSequenceClassification.compute_loss"], "methods", ["None"], ["", "@", "add_start_docstrings_to_model_forward", "(", "ROBERTA_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "_CHECKPOINT_FOR_DOC", ",", "\n", "output_type", "=", "TFSequenceClassifierOutput", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "call", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "training", "=", "False", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`tf.Tensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"", "\n", "inputs", "=", "input_processing", "(", "\n", "func", "=", "self", ".", "call", ",", "\n", "config", "=", "self", ".", "config", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "labels", "=", "labels", ",", "\n", "training", "=", "training", ",", "\n", "kwargs_call", "=", "kwargs", ",", "\n", ")", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "inputs", "[", "\"input_ids\"", "]", ",", "\n", "attention_mask", "=", "inputs", "[", "\"attention_mask\"", "]", ",", "\n", "token_type_ids", "=", "inputs", "[", "\"token_type_ids\"", "]", ",", "\n", "position_ids", "=", "inputs", "[", "\"position_ids\"", "]", ",", "\n", "head_mask", "=", "inputs", "[", "\"head_mask\"", "]", ",", "\n", "inputs_embeds", "=", "inputs", "[", "\"inputs_embeds\"", "]", ",", "\n", "output_attentions", "=", "inputs", "[", "\"output_attentions\"", "]", ",", "\n", "output_hidden_states", "=", "inputs", "[", "\"output_hidden_states\"", "]", ",", "\n", "return_dict", "=", "inputs", "[", "\"return_dict\"", "]", ",", "\n", "training", "=", "inputs", "[", "\"training\"", "]", ",", "\n", ")", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ",", "training", "=", "inputs", "[", "\"training\"", "]", ")", "\n", "\n", "loss", "=", "None", "if", "inputs", "[", "\"labels\"", "]", "is", "None", "else", "self", ".", "compute_loss", "(", "inputs", "[", "\"labels\"", "]", ",", "logits", ")", "\n", "\n", "if", "not", "inputs", "[", "\"return_dict\"", "]", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "TFSequenceClassifierOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForSequenceClassification.serving_output": [[1044, 1049], ["transformers.modeling_tf_outputs.TFSequenceClassifierOutput", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor"], "methods", ["None"], ["", "def", "serving_output", "(", "self", ",", "output", ":", "TFSequenceClassifierOutput", ")", "->", "TFSequenceClassifierOutput", ":", "\n", "        ", "hs", "=", "tf", ".", "convert_to_tensor", "(", "output", ".", "hidden_states", ")", "if", "self", ".", "config", ".", "output_hidden_states", "else", "None", "\n", "attns", "=", "tf", ".", "convert_to_tensor", "(", "output", ".", "attentions", ")", "if", "self", ".", "config", ".", "output_attentions", "else", "None", "\n", "\n", "return", "TFSequenceClassifierOutput", "(", "logits", "=", "output", ".", "logits", ",", "hidden_states", "=", "hs", ",", "attentions", "=", "attns", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForMultipleChoice.__init__": [[1063, 1070], ["transformers.modeling_tf_utils.TFPreTrainedModel.__init__", "modeling_tf_roberta.TFRobertaMainLayer", "tensorflow.keras.layers.Dropout", "tensorflow.keras.layers.Dense", "transformers.modeling_tf_utils.get_initializer"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "roberta", "=", "TFRobertaMainLayer", "(", "config", ",", "name", "=", "\"roberta\"", ")", "\n", "self", ".", "dropout", "=", "tf", ".", "keras", ".", "layers", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "1", ",", "kernel_initializer", "=", "get_initializer", "(", "config", ".", "initializer_range", ")", ",", "name", "=", "\"classifier\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForMultipleChoice.dummy_inputs": [[1072, 1081], ["tensorflow.constant"], "methods", ["None"], ["", "@", "property", "\n", "def", "dummy_inputs", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Dummy inputs to build the network.\n\n        Returns:\n            tf.Tensor with dummy inputs\n        \"\"\"", "\n", "return", "{", "\"input_ids\"", ":", "tf", ".", "constant", "(", "MULTIPLE_CHOICE_DUMMY_INPUTS", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForMultipleChoice.call": [[1082, 1172], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "transformers.modeling_tf_utils.input_processing", "modeling_tf_roberta.TFRobertaForMultipleChoice.roberta", "modeling_tf_roberta.TFRobertaForMultipleChoice.dropout", "modeling_tf_roberta.TFRobertaForMultipleChoice.classifier", "tensorflow.reshape", "transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput", "ROBERTA_INPUTS_DOCSTRING.format", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "modeling_tf_roberta.TFRobertaForMultipleChoice.compute_loss", "transformers.modeling_tf_utils.shape_list", "transformers.modeling_tf_utils.shape_list", "transformers.modeling_tf_utils.shape_list", "transformers.modeling_tf_utils.shape_list"], "methods", ["None"], ["", "@", "add_start_docstrings_to_model_forward", "(", "ROBERTA_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, num_choices, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "_CHECKPOINT_FOR_DOC", ",", "\n", "output_type", "=", "TFMultipleChoiceModelOutput", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "call", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "training", "=", "False", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`tf.Tensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n            num_choices]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n            :obj:`input_ids` above)\n        \"\"\"", "\n", "inputs", "=", "input_processing", "(", "\n", "func", "=", "self", ".", "call", ",", "\n", "config", "=", "self", ".", "config", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "labels", "=", "labels", ",", "\n", "training", "=", "training", ",", "\n", "kwargs_call", "=", "kwargs", ",", "\n", ")", "\n", "\n", "if", "inputs", "[", "\"input_ids\"", "]", "is", "not", "None", ":", "\n", "            ", "num_choices", "=", "shape_list", "(", "inputs", "[", "\"input_ids\"", "]", ")", "[", "1", "]", "\n", "seq_length", "=", "shape_list", "(", "inputs", "[", "\"input_ids\"", "]", ")", "[", "2", "]", "\n", "", "else", ":", "\n", "            ", "num_choices", "=", "shape_list", "(", "inputs_embeds", ")", "[", "1", "]", "\n", "seq_length", "=", "shape_list", "(", "inputs_embeds", ")", "[", "2", "]", "\n", "\n", "", "flat_input_ids", "=", "tf", ".", "reshape", "(", "inputs", "[", "\"input_ids\"", "]", ",", "(", "-", "1", ",", "seq_length", ")", ")", "if", "inputs", "[", "\"input_ids\"", "]", "is", "not", "None", "else", "None", "\n", "flat_attention_mask", "=", "(", "\n", "tf", ".", "reshape", "(", "inputs", "[", "\"attention_mask\"", "]", ",", "(", "-", "1", ",", "seq_length", ")", ")", "if", "inputs", "[", "\"attention_mask\"", "]", "is", "not", "None", "else", "None", "\n", ")", "\n", "flat_token_type_ids", "=", "(", "\n", "tf", ".", "reshape", "(", "inputs", "[", "\"token_type_ids\"", "]", ",", "(", "-", "1", ",", "seq_length", ")", ")", "if", "inputs", "[", "\"token_type_ids\"", "]", "is", "not", "None", "else", "None", "\n", ")", "\n", "flat_position_ids", "=", "(", "\n", "tf", ".", "reshape", "(", "inputs", "[", "\"position_ids\"", "]", ",", "(", "-", "1", ",", "seq_length", ")", ")", "if", "inputs", "[", "\"position_ids\"", "]", "is", "not", "None", "else", "None", "\n", ")", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "flat_input_ids", ",", "\n", "flat_attention_mask", ",", "\n", "flat_token_type_ids", ",", "\n", "flat_position_ids", ",", "\n", "inputs", "[", "\"head_mask\"", "]", ",", "\n", "inputs", "[", "\"inputs_embeds\"", "]", ",", "\n", "inputs", "[", "\"output_attentions\"", "]", ",", "\n", "inputs", "[", "\"output_hidden_states\"", "]", ",", "\n", "return_dict", "=", "inputs", "[", "\"return_dict\"", "]", ",", "\n", "training", "=", "inputs", "[", "\"training\"", "]", ",", "\n", ")", "\n", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ",", "training", "=", "inputs", "[", "\"training\"", "]", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "reshaped_logits", "=", "tf", ".", "reshape", "(", "logits", ",", "(", "-", "1", ",", "num_choices", ")", ")", "\n", "\n", "loss", "=", "None", "if", "inputs", "[", "\"labels\"", "]", "is", "None", "else", "self", ".", "compute_loss", "(", "inputs", "[", "\"labels\"", "]", ",", "reshaped_logits", ")", "\n", "\n", "if", "not", "inputs", "[", "\"return_dict\"", "]", ":", "\n", "            ", "output", "=", "(", "reshaped_logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "TFMultipleChoiceModelOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "reshaped_logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForMultipleChoice.serving": [[1174, 1186], ["tensorflow.function", "modeling_tf_roberta.TFRobertaForMultipleChoice.call", "modeling_tf_roberta.TFRobertaForMultipleChoice.serving_output", "tensorflow.TensorSpec", "tensorflow.TensorSpec"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForQuestionAnswering.call", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForQuestionAnswering.serving_output"], ["", "@", "tf", ".", "function", "(", "\n", "input_signature", "=", "[", "\n", "{", "\n", "\"input_ids\"", ":", "tf", ".", "TensorSpec", "(", "(", "None", ",", "None", ",", "None", ")", ",", "tf", ".", "int32", ",", "name", "=", "\"input_ids\"", ")", ",", "\n", "\"attention_mask\"", ":", "tf", ".", "TensorSpec", "(", "(", "None", ",", "None", ",", "None", ")", ",", "tf", ".", "int32", ",", "name", "=", "\"attention_mask\"", ")", ",", "\n", "}", "\n", "]", "\n", ")", "\n", "def", "serving", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "output", "=", "self", ".", "call", "(", "inputs", ")", "\n", "\n", "return", "self", ".", "serving_output", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForMultipleChoice.serving_output": [[1188, 1193], ["transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor"], "methods", ["None"], ["", "def", "serving_output", "(", "self", ",", "output", ":", "TFMultipleChoiceModelOutput", ")", "->", "TFMultipleChoiceModelOutput", ":", "\n", "        ", "hs", "=", "tf", ".", "convert_to_tensor", "(", "output", ".", "hidden_states", ")", "if", "self", ".", "config", ".", "output_hidden_states", "else", "None", "\n", "attns", "=", "tf", ".", "convert_to_tensor", "(", "output", ".", "attentions", ")", "if", "self", ".", "config", ".", "output_attentions", "else", "None", "\n", "\n", "return", "TFMultipleChoiceModelOutput", "(", "logits", "=", "output", ".", "logits", ",", "hidden_states", "=", "hs", ",", "attentions", "=", "attns", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForTokenClassification.__init__": [[1207, 1218], ["transformers.modeling_tf_utils.TFPreTrainedModel.__init__", "modeling_tf_roberta.TFRobertaMainLayer", "tensorflow.keras.layers.Dropout", "tensorflow.keras.layers.Dense", "transformers.modeling_tf_utils.get_initializer"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "\n", "self", ".", "roberta", "=", "TFRobertaMainLayer", "(", "config", ",", "add_pooling_layer", "=", "False", ",", "name", "=", "\"roberta\"", ")", "\n", "classifier_dropout", "=", "(", "\n", "config", ".", "classifier_dropout", "if", "config", ".", "classifier_dropout", "is", "not", "None", "else", "config", ".", "hidden_dropout_prob", "\n", ")", "\n", "self", ".", "dropout", "=", "tf", ".", "keras", ".", "layers", ".", "Dropout", "(", "classifier_dropout", ")", "\n", "self", ".", "classifier", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "config", ".", "num_labels", ",", "kernel_initializer", "=", "get_initializer", "(", "config", ".", "initializer_range", ")", ",", "name", "=", "\"classifier\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForTokenClassification.call": [[1220, 1291], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "transformers.modeling_tf_utils.input_processing", "modeling_tf_roberta.TFRobertaForTokenClassification.roberta", "modeling_tf_roberta.TFRobertaForTokenClassification.dropout", "modeling_tf_roberta.TFRobertaForTokenClassification.classifier", "transformers.modeling_tf_outputs.TFTokenClassifierOutput", "ROBERTA_INPUTS_DOCSTRING.format", "modeling_tf_roberta.TFRobertaForTokenClassification.compute_loss"], "methods", ["None"], ["", "@", "add_start_docstrings_to_model_forward", "(", "ROBERTA_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "_CHECKPOINT_FOR_DOC", ",", "\n", "output_type", "=", "TFTokenClassifierOutput", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "call", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "training", "=", "False", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n            1]``.\n        \"\"\"", "\n", "inputs", "=", "input_processing", "(", "\n", "func", "=", "self", ".", "call", ",", "\n", "config", "=", "self", ".", "config", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "labels", "=", "labels", ",", "\n", "training", "=", "training", ",", "\n", "kwargs_call", "=", "kwargs", ",", "\n", ")", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "inputs", "[", "\"input_ids\"", "]", ",", "\n", "attention_mask", "=", "inputs", "[", "\"attention_mask\"", "]", ",", "\n", "token_type_ids", "=", "inputs", "[", "\"token_type_ids\"", "]", ",", "\n", "position_ids", "=", "inputs", "[", "\"position_ids\"", "]", ",", "\n", "head_mask", "=", "inputs", "[", "\"head_mask\"", "]", ",", "\n", "inputs_embeds", "=", "inputs", "[", "\"inputs_embeds\"", "]", ",", "\n", "output_attentions", "=", "inputs", "[", "\"output_attentions\"", "]", ",", "\n", "output_hidden_states", "=", "inputs", "[", "\"output_hidden_states\"", "]", ",", "\n", "return_dict", "=", "inputs", "[", "\"return_dict\"", "]", ",", "\n", "training", "=", "inputs", "[", "\"training\"", "]", ",", "\n", ")", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "\n", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ",", "training", "=", "inputs", "[", "\"training\"", "]", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "loss", "=", "None", "if", "inputs", "[", "\"labels\"", "]", "is", "None", "else", "self", ".", "compute_loss", "(", "inputs", "[", "\"labels\"", "]", ",", "logits", ")", "\n", "\n", "if", "not", "inputs", "[", "\"return_dict\"", "]", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "TFTokenClassifierOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForTokenClassification.serving_output": [[1294, 1299], ["transformers.modeling_tf_outputs.TFTokenClassifierOutput", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor"], "methods", ["None"], ["", "def", "serving_output", "(", "self", ",", "output", ":", "TFTokenClassifierOutput", ")", "->", "TFTokenClassifierOutput", ":", "\n", "        ", "hs", "=", "tf", ".", "convert_to_tensor", "(", "output", ".", "hidden_states", ")", "if", "self", ".", "config", ".", "output_hidden_states", "else", "None", "\n", "attns", "=", "tf", ".", "convert_to_tensor", "(", "output", ".", "attentions", ")", "if", "self", ".", "config", ".", "output_attentions", "else", "None", "\n", "\n", "return", "TFTokenClassifierOutput", "(", "logits", "=", "output", ".", "logits", ",", "hidden_states", "=", "hs", ",", "attentions", "=", "attns", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForQuestionAnswering.__init__": [[1312, 1319], ["transformers.modeling_tf_utils.TFPreTrainedModel.__init__", "modeling_tf_roberta.TFRobertaMainLayer", "tensorflow.keras.layers.Dense", "transformers.modeling_tf_utils.get_initializer"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "\n", "self", ".", "roberta", "=", "TFRobertaMainLayer", "(", "config", ",", "add_pooling_layer", "=", "False", ",", "name", "=", "\"roberta\"", ")", "\n", "self", ".", "qa_outputs", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "config", ".", "num_labels", ",", "kernel_initializer", "=", "get_initializer", "(", "config", ".", "initializer_range", ")", ",", "name", "=", "\"qa_outputs\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForQuestionAnswering.call": [[1321, 1406], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "transformers.modeling_tf_utils.input_processing", "modeling_tf_roberta.TFRobertaForQuestionAnswering.roberta", "modeling_tf_roberta.TFRobertaForQuestionAnswering.qa_outputs", "tensorflow.split", "tensorflow.squeeze", "tensorflow.squeeze", "transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput", "ROBERTA_INPUTS_DOCSTRING.format", "modeling_tf_roberta.TFRobertaForQuestionAnswering.compute_loss"], "methods", ["None"], ["", "@", "add_start_docstrings_to_model_forward", "(", "ROBERTA_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "_CHECKPOINT_FOR_DOC", ",", "\n", "output_type", "=", "TFQuestionAnsweringModelOutput", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "call", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "start_positions", "=", "None", ",", "\n", "end_positions", "=", "None", ",", "\n", "training", "=", "False", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        start_positions (:obj:`tf.Tensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n            sequence are not taken into account for computing the loss.\n        end_positions (:obj:`tf.Tensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n            sequence are not taken into account for computing the loss.\n        \"\"\"", "\n", "inputs", "=", "input_processing", "(", "\n", "func", "=", "self", ".", "call", ",", "\n", "config", "=", "self", ".", "config", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "start_positions", "=", "start_positions", ",", "\n", "end_positions", "=", "end_positions", ",", "\n", "training", "=", "training", ",", "\n", "kwargs_call", "=", "kwargs", ",", "\n", ")", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "inputs", "[", "\"input_ids\"", "]", ",", "\n", "attention_mask", "=", "inputs", "[", "\"attention_mask\"", "]", ",", "\n", "token_type_ids", "=", "inputs", "[", "\"token_type_ids\"", "]", ",", "\n", "position_ids", "=", "inputs", "[", "\"position_ids\"", "]", ",", "\n", "head_mask", "=", "inputs", "[", "\"head_mask\"", "]", ",", "\n", "inputs_embeds", "=", "inputs", "[", "\"inputs_embeds\"", "]", ",", "\n", "output_attentions", "=", "inputs", "[", "\"output_attentions\"", "]", ",", "\n", "output_hidden_states", "=", "inputs", "[", "\"output_hidden_states\"", "]", ",", "\n", "return_dict", "=", "inputs", "[", "\"return_dict\"", "]", ",", "\n", "training", "=", "inputs", "[", "\"training\"", "]", ",", "\n", ")", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "\n", "logits", "=", "self", ".", "qa_outputs", "(", "sequence_output", ")", "\n", "start_logits", ",", "end_logits", "=", "tf", ".", "split", "(", "logits", ",", "2", ",", "axis", "=", "-", "1", ")", "\n", "start_logits", "=", "tf", ".", "squeeze", "(", "start_logits", ",", "axis", "=", "-", "1", ")", "\n", "end_logits", "=", "tf", ".", "squeeze", "(", "end_logits", ",", "axis", "=", "-", "1", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "inputs", "[", "\"start_positions\"", "]", "is", "not", "None", "and", "inputs", "[", "\"end_positions\"", "]", "is", "not", "None", ":", "\n", "            ", "labels", "=", "{", "\"start_position\"", ":", "inputs", "[", "\"start_positions\"", "]", "}", "\n", "labels", "[", "\"end_position\"", "]", "=", "inputs", "[", "\"end_positions\"", "]", "\n", "loss", "=", "self", ".", "compute_loss", "(", "labels", ",", "(", "start_logits", ",", "end_logits", ")", ")", "\n", "\n", "", "if", "not", "inputs", "[", "\"return_dict\"", "]", ":", "\n", "            ", "output", "=", "(", "start_logits", ",", "end_logits", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "TFQuestionAnsweringModelOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "start_logits", "=", "start_logits", ",", "\n", "end_logits", "=", "end_logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_tf_roberta.TFRobertaForQuestionAnswering.serving_output": [[1409, 1415], ["transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor"], "methods", ["None"], ["", "def", "serving_output", "(", "self", ",", "output", ":", "TFQuestionAnsweringModelOutput", ")", "->", "TFQuestionAnsweringModelOutput", ":", "\n", "        ", "hs", "=", "tf", ".", "convert_to_tensor", "(", "output", ".", "hidden_states", ")", "if", "self", ".", "config", ".", "output_hidden_states", "else", "None", "\n", "attns", "=", "tf", ".", "convert_to_tensor", "(", "output", ".", "attentions", ")", "if", "self", ".", "config", ".", "output_attentions", "else", "None", "\n", "\n", "return", "TFQuestionAnsweringModelOutput", "(", "\n", "start_logits", "=", "output", ".", "start_logits", ",", "end_logits", "=", "output", ".", "end_logits", ",", "hidden_states", "=", "hs", ",", "attentions", "=", "attns", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaEmbeddings.__init__": [[87, 111], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "getattr", "modeling_roberta.RobertaEmbeddings.register_buffer", "torch.nn.Embedding", "torch.nn.Embedding", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "packaging.version.parse", "packaging.version.parse", "modeling_roberta.RobertaEmbeddings.register_buffer", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "modeling_roberta.RobertaEmbeddings.position_ids.size"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ",", "padding_idx", "=", "config", ".", "pad_token_id", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "# position_ids (1, len position emb) is contiguous in memory and exported when serialized", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "\"absolute\"", ")", "\n", "self", ".", "register_buffer", "(", "\"position_ids\"", ",", "torch", ".", "arange", "(", "config", ".", "max_position_embeddings", ")", ".", "expand", "(", "(", "1", ",", "-", "1", ")", ")", ")", "\n", "if", "version", ".", "parse", "(", "torch", ".", "__version__", ")", ">", "version", ".", "parse", "(", "\"1.6.0\"", ")", ":", "\n", "            ", "self", ".", "register_buffer", "(", "\n", "\"token_type_ids\"", ",", "\n", "torch", ".", "zeros", "(", "self", ".", "position_ids", ".", "size", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "position_ids", ".", "device", ")", ",", "\n", "persistent", "=", "False", ",", "\n", ")", "\n", "\n", "# End copy", "\n", "", "self", ".", "padding_idx", "=", "config", ".", "pad_token_id", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "\n", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ",", "padding_idx", "=", "self", ".", "padding_idx", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaEmbeddings.forward": [[113, 152], ["modeling_roberta.RobertaEmbeddings.token_type_embeddings", "modeling_roberta.RobertaEmbeddings.LayerNorm", "modeling_roberta.RobertaEmbeddings.dropout", "input_ids.size", "hasattr", "modeling_roberta.RobertaEmbeddings.word_embeddings", "modeling_roberta.RobertaEmbeddings.position_embeddings", "modeling_roberta.create_position_ids_from_input_ids", "modeling_roberta.RobertaEmbeddings.create_position_ids_from_inputs_embeds", "modeling_roberta.RobertaEmbeddings.size", "buffered_token_type_ids.expand", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.create_position_ids_from_input_ids", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaEmbeddings.create_position_ids_from_inputs_embeds"], ["", "def", "forward", "(", "\n", "self", ",", "input_ids", "=", "None", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "inputs_embeds", "=", "None", ",", "past_key_values_length", "=", "0", "\n", ")", ":", "\n", "        ", "if", "position_ids", "is", "None", ":", "\n", "            ", "if", "input_ids", "is", "not", "None", ":", "\n", "# Create the position ids from the input token ids. Any padded tokens remain padded.", "\n", "                ", "position_ids", "=", "create_position_ids_from_input_ids", "(", "input_ids", ",", "self", ".", "padding_idx", ",", "past_key_values_length", ")", "\n", "", "else", ":", "\n", "                ", "position_ids", "=", "self", ".", "create_position_ids_from_inputs_embeds", "(", "inputs_embeds", ")", "\n", "\n", "", "", "if", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "", "else", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "\n", "", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "\n", "# Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs", "\n", "# when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves", "\n", "# issue #5664", "\n", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "if", "hasattr", "(", "self", ",", "\"token_type_ids\"", ")", ":", "\n", "                ", "buffered_token_type_ids", "=", "self", ".", "token_type_ids", "[", ":", ",", ":", "seq_length", "]", "\n", "buffered_token_type_ids_expanded", "=", "buffered_token_type_ids", ".", "expand", "(", "input_shape", "[", "0", "]", ",", "seq_length", ")", "\n", "token_type_ids", "=", "buffered_token_type_ids_expanded", "\n", "", "else", ":", "\n", "                ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "position_ids", ".", "device", ")", "\n", "\n", "", "", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "inputs_embeds", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "inputs_embeds", "+", "token_type_embeddings", "\n", "if", "self", ".", "position_embedding_type", "==", "\"absolute\"", ":", "\n", "            ", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "embeddings", "+=", "position_embeddings", "\n", "", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaEmbeddings.create_position_ids_from_inputs_embeds": [[153, 169], ["torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange.unsqueeze().expand", "torch.arange.unsqueeze().expand", "inputs_embeds.size", "torch.arange.unsqueeze", "torch.arange.unsqueeze"], "methods", ["None"], ["", "def", "create_position_ids_from_inputs_embeds", "(", "self", ",", "inputs_embeds", ")", ":", "\n", "        ", "\"\"\"\n        We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n\n        Args:\n            inputs_embeds: torch.Tensor\n\n        Returns: torch.Tensor\n        \"\"\"", "\n", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "sequence_length", "=", "input_shape", "[", "1", "]", "\n", "\n", "position_ids", "=", "torch", ".", "arange", "(", "\n", "self", ".", "padding_idx", "+", "1", ",", "sequence_length", "+", "self", ".", "padding_idx", "+", "1", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "inputs_embeds", ".", "device", "\n", ")", "\n", "return", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "input_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaSelfAttention.__init__": [[173, 211], ["torch.nn.Module.__init__", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "getattr", "ValueError", "model.Linear", "torch.nn.Linear", "torch.nn.Linear", "model.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Embedding", "torch.nn.Embedding", "hasattr"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", "and", "not", "hasattr", "(", "config", ",", "\"embedding_size\"", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"", "\n", "f\"heads ({config.num_attention_heads})\"", "\n", ")", "\n", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "# self.query = nn.Linear(config.hidden_size, self.all_head_size)", "\n", "# self.key = nn.Linear(config.hidden_size, self.all_head_size)", "\n", "\n", "if", "config", ".", "lora", ":", "\n", "            ", "self", ".", "query", "=", "lora", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ",", "config", ".", "lora_r", ",", "lora_alpha", "=", "config", ".", "lora_alpha", ")", "\n", "# print(config.lora_r)", "\n", "", "else", ":", "\n", "            ", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "if", "config", ".", "lora", ":", "\n", "            ", "self", ".", "value", "=", "lora", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ",", "config", ".", "lora_r", ",", "lora_alpha", "=", "config", ".", "lora_alpha", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "\n", "# self.value = nn.Linear(config.hidden_size, self.all_head_size)", "\n", "\n", "", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "\"absolute\"", ")", "\n", "if", "self", ".", "position_embedding_type", "==", "\"relative_key\"", "or", "self", ".", "position_embedding_type", "==", "\"relative_key_query\"", ":", "\n", "            ", "self", ".", "max_position_embeddings", "=", "config", ".", "max_position_embeddings", "\n", "self", ".", "distance_embedding", "=", "nn", ".", "Embedding", "(", "2", "*", "config", ".", "max_position_embeddings", "-", "1", ",", "self", ".", "attention_head_size", ")", "\n", "\n", "", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaSelfAttention.transpose_for_scores": [[212, 216], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaSelfAttention.forward": [[217, 310], ["modeling_roberta.RobertaSelfAttention.query", "modeling_roberta.RobertaSelfAttention.transpose_for_scores", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "modeling_roberta.RobertaSelfAttention.dropout", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "modeling_roberta.RobertaSelfAttention.transpose", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "modeling_roberta.RobertaSelfAttention.distance_embedding", "positional_embedding.to.to.to", "math.sqrt", "torch.nn.Softmax", "torch.nn.Softmax", "modeling_roberta.RobertaSelfAttention.transpose_for_scores", "modeling_roberta.RobertaSelfAttention.transpose_for_scores", "hidden_states.size", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "context_layer.view.view.permute", "context_layer.view.view.size", "modeling_roberta.RobertaSelfAttention.key", "modeling_roberta.RobertaSelfAttention.value", "modeling_roberta.RobertaSelfAttention.transpose_for_scores", "modeling_roberta.RobertaSelfAttention.transpose_for_scores", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling_roberta.RobertaSelfAttention.transpose_for_scores", "modeling_roberta.RobertaSelfAttention.transpose_for_scores", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "modeling_roberta.RobertaSelfAttention.key", "modeling_roberta.RobertaSelfAttention.value", "modeling_roberta.RobertaSelfAttention.key", "modeling_roberta.RobertaSelfAttention.value"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "        ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "\n", "# If this is instantiated as a cross-attention module, the keys", "\n", "# and values come from an encoder; the attention mask needs to be", "\n", "# such that the encoder's padding tokens are not attended to.", "\n", "is_cross_attention", "=", "encoder_hidden_states", "is", "not", "None", "\n", "\n", "if", "is_cross_attention", "and", "past_key_value", "is", "not", "None", ":", "\n", "# reuse k,v, cross_attentions", "\n", "            ", "key_layer", "=", "past_key_value", "[", "0", "]", "\n", "value_layer", "=", "past_key_value", "[", "1", "]", "\n", "attention_mask", "=", "encoder_attention_mask", "\n", "", "elif", "is_cross_attention", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "encoder_hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "encoder_hidden_states", ")", ")", "\n", "attention_mask", "=", "encoder_attention_mask", "\n", "", "elif", "past_key_value", "is", "not", "None", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "hidden_states", ")", ")", "\n", "key_layer", "=", "torch", ".", "cat", "(", "[", "past_key_value", "[", "0", "]", ",", "key_layer", "]", ",", "dim", "=", "2", ")", "\n", "value_layer", "=", "torch", ".", "cat", "(", "[", "past_key_value", "[", "1", "]", ",", "value_layer", "]", ",", "dim", "=", "2", ")", "\n", "", "else", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "hidden_states", ")", ")", "\n", "\n", "", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "\n", "if", "self", ".", "is_decoder", ":", "\n", "# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.", "\n", "# Further calls to cross_attention layer can then reuse all cross-attention", "\n", "# key/value_states (first \"if\" case)", "\n", "# if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of", "\n", "# all previous decoder key/value_states. Further calls to uni-directional self-attention", "\n", "# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)", "\n", "# if encoder bi-directional self-attention `past_key_value` is always `None`", "\n", "            ", "past_key_value", "=", "(", "key_layer", ",", "value_layer", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "\n", "if", "self", ".", "position_embedding_type", "==", "\"relative_key\"", "or", "self", ".", "position_embedding_type", "==", "\"relative_key_query\"", ":", "\n", "            ", "seq_length", "=", "hidden_states", ".", "size", "(", ")", "[", "1", "]", "\n", "position_ids_l", "=", "torch", ".", "arange", "(", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "hidden_states", ".", "device", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "position_ids_r", "=", "torch", ".", "arange", "(", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "hidden_states", ".", "device", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "distance", "=", "position_ids_l", "-", "position_ids_r", "\n", "positional_embedding", "=", "self", ".", "distance_embedding", "(", "distance", "+", "self", ".", "max_position_embeddings", "-", "1", ")", "\n", "positional_embedding", "=", "positional_embedding", ".", "to", "(", "dtype", "=", "query_layer", ".", "dtype", ")", "# fp16 compatibility", "\n", "\n", "if", "self", ".", "position_embedding_type", "==", "\"relative_key\"", ":", "\n", "                ", "relative_position_scores", "=", "torch", ".", "einsum", "(", "\"bhld,lrd->bhlr\"", ",", "query_layer", ",", "positional_embedding", ")", "\n", "attention_scores", "=", "attention_scores", "+", "relative_position_scores", "\n", "", "elif", "self", ".", "position_embedding_type", "==", "\"relative_key_query\"", ":", "\n", "                ", "relative_position_scores_query", "=", "torch", ".", "einsum", "(", "\"bhld,lrd->bhlr\"", ",", "query_layer", ",", "positional_embedding", ")", "\n", "relative_position_scores_key", "=", "torch", ".", "einsum", "(", "\"bhrd,lrd->bhlr\"", ",", "key_layer", ",", "positional_embedding", ")", "\n", "attention_scores", "=", "attention_scores", "+", "relative_position_scores_query", "+", "relative_position_scores_key", "\n", "\n", "", "", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "# Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)", "\n", "            ", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "# Mask heads if we want to", "\n", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "attention_probs", "=", "attention_probs", "*", "head_mask", "\n", "\n", "", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "\n", "outputs", "=", "(", "context_layer", ",", "attention_probs", ")", "if", "output_attentions", "else", "(", "context_layer", ",", ")", "\n", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "past_key_value", ",", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaSelfOutput.__init__": [[314, 322], ["super().__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "modeling_roberta.RobertaSelfOutput._init_adapter_modules"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "_init_adapter_modules", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaSelfOutput.forward": [[323, 328], ["modeling_roberta.RobertaSelfOutput.dense", "modeling_roberta.RobertaSelfOutput.dropout", "modeling_roberta.RobertaSelfOutput.adapters_forward"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ",", "**", "kwargs", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "adapters_forward", "(", "hidden_states", ",", "input_tensor", ",", "**", "kwargs", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaAttention.__init__": [[332, 337], ["torch.nn.Module.__init__", "modeling_roberta.RobertaSelfAttention", "modeling_roberta.RobertaSelfOutput", "set"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "RobertaSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "RobertaSelfOutput", "(", "config", ")", "\n", "self", ".", "pruned_heads", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaAttention.prune_heads": [[338, 355], ["transformers.modeling_utils.find_pruneable_heads_and_indices", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "modeling_roberta.RobertaAttention.pruned_heads.union", "len", "len"], "methods", ["None"], ["", "def", "prune_heads", "(", "self", ",", "heads", ")", ":", "\n", "        ", "if", "len", "(", "heads", ")", "==", "0", ":", "\n", "            ", "return", "\n", "", "heads", ",", "index", "=", "find_pruneable_heads_and_indices", "(", "\n", "heads", ",", "self", ".", "self", ".", "num_attention_heads", ",", "self", ".", "self", ".", "attention_head_size", ",", "self", ".", "pruned_heads", "\n", ")", "\n", "\n", "# Prune linear layers", "\n", "self", ".", "self", ".", "query", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "query", ",", "index", ")", "\n", "self", ".", "self", ".", "key", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "key", ",", "index", ")", "\n", "self", ".", "self", ".", "value", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "value", ",", "index", ")", "\n", "self", ".", "output", ".", "dense", "=", "prune_linear_layer", "(", "self", ".", "output", ".", "dense", ",", "index", ",", "dim", "=", "1", ")", "\n", "\n", "# Update hyper params and store pruned heads", "\n", "self", ".", "self", ".", "num_attention_heads", "=", "self", ".", "self", ".", "num_attention_heads", "-", "len", "(", "heads", ")", "\n", "self", ".", "self", ".", "all_head_size", "=", "self", ".", "self", ".", "attention_head_size", "*", "self", ".", "self", ".", "num_attention_heads", "\n", "self", ".", "pruned_heads", "=", "self", ".", "pruned_heads", ".", "union", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaAttention.forward": [[356, 379], ["modeling_roberta.RobertaAttention.self", "modeling_roberta.RobertaAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "self_outputs", "=", "self", ".", "self", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_outputs", "[", "0", "]", ",", "hidden_states", ",", "**", "kwargs", ")", "\n", "outputs", "=", "(", "attention_output", ",", ")", "+", "self_outputs", "[", "1", ":", "]", "# add attentions if we output them", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaIntermediate.__init__": [[383, 390], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaIntermediate.forward": [[391, 395], ["modeling_roberta.RobertaIntermediate.dense", "modeling_roberta.RobertaIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaOutput.__init__": [[399, 407], ["super().__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "modeling_roberta.RobertaOutput._init_adapter_modules"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "_init_adapter_modules", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaOutput.forward": [[408, 413], ["modeling_roberta.RobertaOutput.dense", "modeling_roberta.RobertaOutput.dropout", "modeling_roberta.RobertaOutput.adapters_forward"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ",", "**", "kwargs", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "adapters_forward", "(", "hidden_states", ",", "input_tensor", ",", "**", "kwargs", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaLayer.__init__": [[417, 429], ["super().__init__", "modeling_roberta.RobertaAttention", "modeling_roberta.RobertaIntermediate", "modeling_roberta.RobertaOutput", "modeling_roberta.RobertaAttention"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "chunk_size_feed_forward", "=", "config", ".", "chunk_size_feed_forward", "\n", "self", ".", "seq_len_dim", "=", "1", "\n", "self", ".", "attention", "=", "RobertaAttention", "(", "config", ")", "\n", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "self", ".", "add_cross_attention", "=", "config", ".", "add_cross_attention", "\n", "if", "self", ".", "add_cross_attention", ":", "\n", "            ", "assert", "self", ".", "is_decoder", ",", "f\"{self} should be used as a decoder model if cross attention is added\"", "\n", "self", ".", "crossattention", "=", "RobertaAttention", "(", "config", ")", "\n", "", "self", ".", "intermediate", "=", "RobertaIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "RobertaOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaLayer.forward": [[430, 494], ["modeling_roberta.RobertaLayer.attention", "transformers.modeling_utils.apply_chunking_to_forward", "hasattr", "modeling_roberta.RobertaLayer.crossattention"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "# decoder uni-directional self-attention cached key/values tuple is at positions 1,2", "\n", "        ", "self_attn_past_key_value", "=", "past_key_value", "[", ":", "2", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "self_attention_outputs", "=", "self", ".", "attention", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "past_key_value", "=", "self_attn_past_key_value", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "attention_output", "=", "self_attention_outputs", "[", "0", "]", "\n", "\n", "# if decoder, the last output is tuple of self-attn cache", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "self_attention_outputs", "[", "1", ":", "-", "1", "]", "\n", "present_key_value", "=", "self_attention_outputs", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "self_attention_outputs", "[", "1", ":", "]", "# add self attentions if we output attention weights", "\n", "\n", "", "cross_attn_present_key_value", "=", "None", "\n", "if", "self", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "assert", "hasattr", "(", "\n", "self", ",", "\"crossattention\"", "\n", ")", ",", "f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"", "\n", "\n", "# cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple", "\n", "cross_attn_past_key_value", "=", "past_key_value", "[", "-", "2", ":", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "cross_attention_outputs", "=", "self", ".", "crossattention", "(", "\n", "attention_output", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "cross_attn_past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "cross_attention_outputs", "[", "0", "]", "\n", "outputs", "=", "outputs", "+", "cross_attention_outputs", "[", "1", ":", "-", "1", "]", "# add cross attentions if we output attention weights", "\n", "\n", "# add cross-attn cache to positions 3,4 of present_key_value tuple", "\n", "cross_attn_present_key_value", "=", "cross_attention_outputs", "[", "-", "1", "]", "\n", "present_key_value", "=", "present_key_value", "+", "cross_attn_present_key_value", "\n", "\n", "", "layer_output", "=", "apply_chunking_to_forward", "(", "\n", "self", ".", "feed_forward_chunk", ",", "self", ".", "chunk_size_feed_forward", ",", "self", ".", "seq_len_dim", ",", "attention_output", ",", "**", "kwargs", "\n", ")", "\n", "outputs", "=", "(", "layer_output", ",", ")", "+", "outputs", "\n", "\n", "# if decoder, return the attn key/values as the last output", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "present_key_value", ",", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaLayer.feed_forward_chunk": [[495, 499], ["modeling_roberta.RobertaLayer.intermediate", "modeling_roberta.RobertaLayer.output"], "methods", ["None"], ["", "def", "feed_forward_chunk", "(", "self", ",", "attention_output", ",", "**", "kwargs", ")", ":", "\n", "        ", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ",", "**", "kwargs", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaEncoder.__init__": [[503, 509], ["super().__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "modeling_roberta.RobertaLayer", "range"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "RobertaLayer", "(", "config", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "self", ".", "gradient_checkpointing", "=", "False", "\n", "self", ".", "drop_layer", "=", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaEncoder.forward": [[510, 611], ["enumerate", "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions", "modeling_roberta.RobertaEncoder.adjust_attention_mask_for_parallel", "tuple", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "layer_module", "logger.warning", "modeling_roberta.RobertaEncoder.forward.create_custom_forward"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", "output_hidden_states", "=", "False", ",", "\n", "return_dict", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "all_hidden_states", "=", "(", ")", "if", "output_hidden_states", "else", "None", "\n", "all_self_attentions", "=", "(", ")", "if", "output_attentions", "else", "None", "\n", "all_cross_attentions", "=", "(", ")", "if", "output_attentions", "and", "self", ".", "config", ".", "add_cross_attention", "else", "None", "\n", "\n", "next_decoder_cache", "=", "(", ")", "if", "use_cache", "else", "None", "\n", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "            ", "if", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "layer_head_mask", "=", "head_mask", "[", "i", "]", "if", "head_mask", "is", "not", "None", "else", "None", "\n", "# rmlayers = [9,10,11]", "\n", "past_key_value", "=", "past_key_values", "[", "i", "]", "if", "past_key_values", "is", "not", "None", "else", "None", "\n", "\n", "# for j in range(i):", "\n", "#     past_key_value = past_key_value + past_key_values[j]", "\n", "# if i > 0:", "\n", "#     past_key_value = past_key_value + past_key_values[i-1]", "\n", "# if i in rmlayers:", "\n", "#     # print(attention_mask)", "\n", "#     attention_mask_new = attention_mask[:, :, :, self.config.pre_seq_len:]", "\n", "# else:", "\n", "#     attention_mask_new = attention_mask[:]", "\n", "if", "self", ".", "gradient_checkpointing", "and", "self", ".", "training", ":", "\n", "\n", "                ", "if", "use_cache", ":", "\n", "                    ", "logger", ".", "warning", "(", "\n", "\"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"", "\n", ")", "\n", "use_cache", "=", "False", "\n", "\n", "", "def", "create_custom_forward", "(", "module", ")", ":", "\n", "                    ", "def", "custom_forward", "(", "*", "inputs", ")", ":", "\n", "                        ", "return", "module", "(", "*", "inputs", ",", "past_key_value", ",", "output_attentions", ")", "\n", "\n", "", "return", "custom_forward", "\n", "\n", "", "layer_outputs", "=", "torch", ".", "utils", ".", "checkpoint", ".", "checkpoint", "(", "\n", "create_custom_forward", "(", "layer_module", ")", ",", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "layer_outputs", "=", "layer_module", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "past_key_value", ",", "\n", "output_attentions", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "\n", "", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "attention_mask", "=", "self", ".", "adjust_attention_mask_for_parallel", "(", "hidden_states", ",", "attention_mask", ")", "\n", "\n", "if", "use_cache", ":", "\n", "                ", "next_decoder_cache", "+=", "(", "layer_outputs", "[", "-", "1", "]", ",", ")", "\n", "", "if", "output_attentions", ":", "\n", "                ", "all_self_attentions", "=", "all_self_attentions", "+", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "if", "self", ".", "config", ".", "add_cross_attention", ":", "\n", "                    ", "all_cross_attentions", "=", "all_cross_attentions", "+", "(", "layer_outputs", "[", "2", "]", ",", ")", "\n", "\n", "", "", "", "if", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "return", "tuple", "(", "\n", "v", "\n", "for", "v", "in", "[", "\n", "hidden_states", ",", "\n", "next_decoder_cache", ",", "\n", "all_hidden_states", ",", "\n", "all_self_attentions", ",", "\n", "all_cross_attentions", ",", "\n", "]", "\n", "if", "v", "is", "not", "None", "\n", ")", "\n", "", "return", "BaseModelOutputWithPastAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "hidden_states", ",", "\n", "past_key_values", "=", "next_decoder_cache", ",", "\n", "hidden_states", "=", "all_hidden_states", ",", "\n", "attentions", "=", "all_self_attentions", ",", "\n", "cross_attentions", "=", "all_cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaPooler.__init__": [[616, 620], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaPooler.forward": [[621, 628], ["modeling_roberta.RobertaPooler.dense", "modeling_roberta.RobertaPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaPreTrainedModel._init_weights": [[641, 656], ["isinstance", "module.weight.data.normal_", "isinstance", "module.bias.data.zero_", "module.weight.data.normal_", "isinstance", "module.weight.data[].zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\"Initialize the weights\"\"\"", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Embedding", ")", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "module", ".", "padding_idx", "is", "not", "None", ":", "\n", "                ", "module", ".", "weight", ".", "data", "[", "module", ".", "padding_idx", "]", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaPreTrainedModel._set_gradient_checkpointing": [[657, 660], ["isinstance"], "methods", ["None"], ["", "", "def", "_set_gradient_checkpointing", "(", "self", ",", "module", ",", "value", "=", "False", ")", ":", "\n", "        ", "if", "isinstance", "(", "module", ",", "RobertaEncoder", ")", ":", "\n", "            ", "module", ".", "gradient_checkpointing", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaPreTrainedModel.update_keys_to_ignore": [[661, 668], ["None"], "methods", ["None"], ["", "", "def", "update_keys_to_ignore", "(", "self", ",", "config", ",", "del_keys_to_ignore", ")", ":", "\n", "        ", "\"\"\"Remove some keys from ignore list\"\"\"", "\n", "if", "not", "config", ".", "tie_word_embeddings", ":", "\n", "# must make a new list, or the class variable gets modified!", "\n", "            ", "self", ".", "_keys_to_ignore_on_save", "=", "[", "k", "for", "k", "in", "self", ".", "_keys_to_ignore_on_save", "if", "k", "not", "in", "del_keys_to_ignore", "]", "\n", "self", ".", "_keys_to_ignore_on_load_missing", "=", "[", "\n", "k", "for", "k", "in", "self", ".", "_keys_to_ignore_on_load_missing", "if", "k", "not", "in", "del_keys_to_ignore", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaModel.__init__": [[763, 775], ["transformers.adapters.models.bert.BertModelAdaptersMixin.__init__", "modeling_roberta.RobertaEmbeddings", "modeling_roberta.RobertaEncoder", "modeling_roberta.RobertaModel._init_adapter_modules", "modeling_roberta.RobertaModel.init_weights", "modeling_roberta.RobertaPooler"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.init_weights"], ["def", "__init__", "(", "self", ",", "config", ",", "add_pooling_layer", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "embeddings", "=", "RobertaEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "RobertaEncoder", "(", "config", ")", "\n", "\n", "self", ".", "pooler", "=", "RobertaPooler", "(", "config", ")", "if", "add_pooling_layer", "else", "None", "\n", "\n", "self", ".", "_init_adapter_modules", "(", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaModel.get_input_embeddings": [[776, 778], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaModel.set_input_embeddings": [[779, 781], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "embeddings", ".", "word_embeddings", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaModel._prune_heads": [[782, 789], ["heads_to_prune.items", "modeling_roberta.RobertaModel.encoder.layer[].attention.prune_heads"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaAttention.prune_heads"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "encoder", ".", "layer", "[", "layer", "]", ".", "attention", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaModel.forward": [[790, 930], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "modeling_roberta.RobertaModel.pre_transformer_forward", "modeling_roberta.RobertaModel.get_extended_attention_mask", "modeling_roberta.RobertaModel.get_head_mask", "modeling_roberta.RobertaModel.embeddings", "modeling_roberta.RobertaModel.invertible_adapters_forward", "modeling_roberta.RobertaModel.encoder", "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions", "ROBERTA_INPUTS_DOCSTRING.format", "ValueError", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "hasattr", "encoder_hidden_states.size", "modeling_roberta.RobertaModel.invert_attention_mask", "modeling_roberta.RobertaModel.pooler", "input_ids.size", "buffered_token_type_ids.expand", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "ValueError", "inputs_embeds.size"], "methods", ["None"], ["", "", "@", "add_start_docstrings_to_model_forward", "(", "ROBERTA_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "_CHECKPOINT_FOR_DOC", ",", "\n", "output_type", "=", "BaseModelOutputWithPoolingAndCrossAttentions", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "# Copied from transformers.models.bert.modeling_bert.BertModel.forward", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        \"\"\"", "\n", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "self", ".", "pre_transformer_forward", "(", "**", "kwargs", ")", "\n", "\n", "if", "self", ".", "config", ".", "is_decoder", ":", "\n", "            ", "use_cache", "=", "use_cache", "if", "use_cache", "is", "not", "None", "else", "self", ".", "config", ".", "use_cache", "\n", "", "else", ":", "\n", "            ", "use_cache", "=", "False", "\n", "\n", "", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both input_ids and inputs_embeds at the same time\"", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either input_ids or inputs_embeds\"", ")", "\n", "\n", "", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "device", "=", "input_ids", ".", "device", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "device", "\n", "\n", "# past_key_values_length", "\n", "past_key_values_length", "=", "past_key_values", "[", "0", "]", "[", "0", "]", ".", "shape", "[", "2", "]", "if", "past_key_values", "is", "not", "None", "else", "0", "\n", "\n", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones", "(", "(", "(", "batch_size", ",", "seq_length", "+", "past_key_values_length", ")", ")", ",", "device", "=", "device", ")", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "if", "hasattr", "(", "self", ".", "embeddings", ",", "\"token_type_ids\"", ")", ":", "\n", "                ", "buffered_token_type_ids", "=", "self", ".", "embeddings", ".", "token_type_ids", "[", ":", ",", ":", "seq_length", "]", "\n", "buffered_token_type_ids_expanded", "=", "buffered_token_type_ids", ".", "expand", "(", "batch_size", ",", "seq_length", ")", "\n", "token_type_ids", "=", "buffered_token_type_ids_expanded", "\n", "", "else", ":", "\n", "                ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "", "", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "input_shape", ",", "device", ")", "\n", "\n", "# If a 2D or 3D attention mask is provided for the cross-attention", "\n", "# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]", "\n", "if", "self", ".", "config", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "encoder_batch_size", ",", "encoder_sequence_length", ",", "_", "=", "encoder_hidden_states", ".", "size", "(", ")", "\n", "encoder_hidden_shape", "=", "(", "encoder_batch_size", ",", "encoder_sequence_length", ")", "\n", "if", "encoder_attention_mask", "is", "None", ":", "\n", "                ", "encoder_attention_mask", "=", "torch", ".", "ones", "(", "encoder_hidden_shape", ",", "device", "=", "device", ")", "\n", "", "encoder_extended_attention_mask", "=", "self", ".", "invert_attention_mask", "(", "encoder_attention_mask", ")", "\n", "", "else", ":", "\n", "            ", "encoder_extended_attention_mask", "=", "None", "\n", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "", "head_mask", "=", "self", ".", "get_head_mask", "(", "head_mask", ",", "self", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "past_key_values_length", "=", "past_key_values_length", ",", "\n", ")", "\n", "embedding_output", "=", "self", ".", "invertible_adapters_forward", "(", "embedding_output", ")", "\n", "\n", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "encoder_hidden_states", "=", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", "=", "encoder_extended_attention_mask", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "pooler", "is", "not", "None", "else", "None", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "(", "sequence_output", ",", "pooled_output", ")", "+", "encoder_outputs", "[", "1", ":", "]", "\n", "\n", "", "return", "BaseModelOutputWithPoolingAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "sequence_output", ",", "\n", "pooler_output", "=", "pooled_output", ",", "\n", "past_key_values", "=", "encoder_outputs", ".", "past_key_values", ",", "\n", "hidden_states", "=", "encoder_outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "encoder_outputs", ".", "attentions", ",", "\n", "cross_attentions", "=", "encoder_outputs", ".", "cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaModelWithHeads.__init__": [[938, 946], ["transformers.adapters.models.bert.BertModelHeadsMixin.__init__", "modeling_roberta.RobertaModel", "modeling_roberta.RobertaModelWithHeads._init_head_modules", "modeling_roberta.RobertaModelWithHeads.init_weights"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "roberta", "=", "RobertaModel", "(", "config", ")", "\n", "\n", "self", ".", "_init_head_modules", "(", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaModelWithHeads.forward": [[947, 1013], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "modeling_roberta.RobertaModelWithHeads.roberta", "ROBERTA_INPUTS_DOCSTRING.format", "input_ids.view", "position_ids.view", "token_type_ids.view", "attention_mask.view", "inputs_embeds.view", "modeling_roberta.RobertaModelWithHeads.forward_head", "input_ids.size", "position_ids.size", "token_type_ids.size", "attention_mask.size", "inputs_embeds.size", "inputs_embeds.size"], "methods", ["None"], ["", "@", "add_start_docstrings_to_model_forward", "(", "ROBERTA_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "\"roberta-base\"", ",", "\n", "output_type", "=", "ModelOutput", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "adapter_names", "=", "None", ",", "\n", "head", "=", "None", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_ids", ".", "size", "(", "-", "1", ")", ")", "if", "input_ids", "is", "not", "None", "else", "None", "\n", "position_ids", "=", "position_ids", ".", "view", "(", "-", "1", ",", "position_ids", ".", "size", "(", "-", "1", ")", ")", "if", "position_ids", "is", "not", "None", "else", "None", "\n", "token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "token_type_ids", ".", "size", "(", "-", "1", ")", ")", "if", "token_type_ids", "is", "not", "None", "else", "None", "\n", "attention_mask", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "attention_mask", ".", "size", "(", "-", "1", ")", ")", "if", "attention_mask", "is", "not", "None", "else", "None", "\n", "inputs_embeds", "=", "(", "\n", "inputs_embeds", ".", "view", "(", "-", "1", ",", "inputs_embeds", ".", "size", "(", "-", "2", ")", ",", "inputs_embeds", ".", "size", "(", "-", "1", ")", ")", "\n", "if", "inputs_embeds", "is", "not", "None", "\n", "else", "None", "\n", ")", "\n", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "adapter_names", "=", "adapter_names", ",", "\n", ")", "\n", "# BERT & RoBERTa return the pooled output as second item, we don't need that in these heads", "\n", "if", "not", "return_dict", ":", "\n", "            ", "head_inputs", "=", "(", "outputs", "[", "0", "]", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "", "else", ":", "\n", "            ", "head_inputs", "=", "outputs", "\n", "", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "\n", "if", "head", "or", "self", ".", "active_head", ":", "\n", "            ", "head_outputs", "=", "self", ".", "forward_head", "(", "\n", "head_inputs", ",", "\n", "head_name", "=", "head", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "pooled_output", "=", "pooled_output", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "return", "head_outputs", "\n", "", "else", ":", "\n", "# in case no head is used just return the output of the base model (including pooler output)", "\n", "            ", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForCausalLM.__init__": [[1023, 1036], ["transformers.adapters.model_mixin.ModelWithHeadsAdaptersMixin.__init__", "modeling_roberta.RobertaModel", "modeling_roberta.RobertaLMHead", "modeling_roberta.RobertaForCausalLM.update_keys_to_ignore", "modeling_roberta.RobertaForCausalLM.init_weights", "logger.warning"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaPreTrainedModel.update_keys_to_ignore", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.init_weights"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "if", "not", "config", ".", "is_decoder", ":", "\n", "            ", "logger", ".", "warning", "(", "\"If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\"", ")", "\n", "\n", "", "self", ".", "roberta", "=", "RobertaModel", "(", "config", ",", "add_pooling_layer", "=", "False", ")", "\n", "self", ".", "lm_head", "=", "RobertaLMHead", "(", "config", ")", "\n", "\n", "# The LM head weights require special treatment only when they are tied with the word embeddings", "\n", "self", ".", "update_keys_to_ignore", "(", "config", ",", "[", "\"lm_head.decoder.weight\"", "]", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForCausalLM.get_output_embeddings": [[1037, 1039], ["None"], "methods", ["None"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "lm_head", ".", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForCausalLM.set_output_embeddings": [[1040, 1042], ["None"], "methods", ["None"], ["", "def", "set_output_embeddings", "(", "self", ",", "new_embeddings", ")", ":", "\n", "        ", "self", ".", "lm_head", ".", "decoder", "=", "new_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForCausalLM.forward": [[1043, 1151], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.replace_return_docstrings", "modeling_roberta.RobertaForCausalLM.roberta", "modeling_roberta.RobertaForCausalLM.lm_head", "transformers.modeling_outputs.CausalLMOutputWithCrossAttentions", "ROBERTA_INPUTS_DOCSTRING.format", "prediction_scores[].contiguous", "labels[].contiguous", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling_roberta.RobertaForCausalLM.roberta.get_invertible_adapter", "prediction_scores[].contiguous.view", "labels[].contiguous.view"], "methods", ["None"], ["", "@", "add_start_docstrings_to_model_forward", "(", "ROBERTA_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "replace_return_docstrings", "(", "output_type", "=", "CausalLMOutputWithCrossAttentions", ",", "config_class", "=", "_CONFIG_FOR_DOC", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "adapter_names", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n            ignored (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n\n        Returns:\n\n        Example::\n\n            >>> from transformers import RobertaTokenizer, RobertaForCausalLM, RobertaConfig\n            >>> import torch\n\n            >>> tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n            >>> config = RobertaConfig.from_pretrained(\"roberta-base\")\n            >>> config.is_decoder = True\n            >>> model = RobertaForCausalLM.from_pretrained('roberta-base', config=config)\n\n            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n\n            >>> prediction_logits = outputs.logits\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "use_cache", "=", "False", "\n", "\n", "", "outputs", "=", "self", ".", "roberta", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "encoder_hidden_states", "=", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", "=", "encoder_attention_mask", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "adapter_names", "=", "adapter_names", ",", "\n", ")", "\n", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "prediction_scores", "=", "self", ".", "lm_head", "(", "\n", "sequence_output", ",", "\n", "inv_lang_adapter", "=", "self", ".", "roberta", ".", "get_invertible_adapter", "(", ")", ",", "\n", ")", "\n", "\n", "lm_loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "# we are doing next-token prediction; shift prediction scores and input ids by one", "\n", "            ", "shifted_prediction_scores", "=", "prediction_scores", "[", ":", ",", ":", "-", "1", ",", ":", "]", ".", "contiguous", "(", ")", "\n", "labels", "=", "labels", "[", ":", ",", "1", ":", "]", ".", "contiguous", "(", ")", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "lm_loss", "=", "loss_fct", "(", "shifted_prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "prediction_scores", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "lm_loss", ",", ")", "+", "output", ")", "if", "lm_loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "CausalLMOutputWithCrossAttentions", "(", "\n", "loss", "=", "lm_loss", ",", "\n", "logits", "=", "prediction_scores", ",", "\n", "past_key_values", "=", "outputs", ".", "past_key_values", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", "cross_attentions", "=", "outputs", ".", "cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForCausalLM.prepare_inputs_for_generation": [[1153, 1164], ["input_ids.new_ones"], "methods", ["None"], ["", "def", "prepare_inputs_for_generation", "(", "self", ",", "input_ids", ",", "past", "=", "None", ",", "attention_mask", "=", "None", ",", "**", "model_kwargs", ")", ":", "\n", "        ", "input_shape", "=", "input_ids", ".", "shape", "\n", "# if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly", "\n", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "input_ids", ".", "new_ones", "(", "input_shape", ")", "\n", "\n", "# cut decoder_input_ids if past is used", "\n", "", "if", "past", "is", "not", "None", ":", "\n", "            ", "input_ids", "=", "input_ids", "[", ":", ",", "-", "1", ":", "]", "\n", "\n", "", "return", "{", "\"input_ids\"", ":", "input_ids", ",", "\"attention_mask\"", ":", "attention_mask", ",", "\"past_key_values\"", ":", "past", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForCausalLM._reorder_cache": [[1165, 1170], ["tuple", "past_state.index_select"], "methods", ["None"], ["", "def", "_reorder_cache", "(", "self", ",", "past", ",", "beam_idx", ")", ":", "\n", "        ", "reordered_past", "=", "(", ")", "\n", "for", "layer_past", "in", "past", ":", "\n", "            ", "reordered_past", "+=", "(", "tuple", "(", "past_state", ".", "index_select", "(", "0", ",", "beam_idx", ")", "for", "past_state", "in", "layer_past", ")", ",", ")", "\n", "", "return", "reordered_past", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForMaskedLM.__init__": [[1178, 1194], ["transformers.adapters.model_mixin.ModelWithHeadsAdaptersMixin.__init__", "modeling_roberta.RobertaModel", "modeling_roberta.RobertaLMHead", "modeling_roberta.RobertaForMaskedLM.update_keys_to_ignore", "modeling_roberta.RobertaForMaskedLM.init_weights", "logger.warning"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaPreTrainedModel.update_keys_to_ignore", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.init_weights"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "if", "config", ".", "is_decoder", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"If you want to use `RobertaForMaskedLM` make sure `config.is_decoder=False` for \"", "\n", "\"bi-directional self-attention.\"", "\n", ")", "\n", "\n", "", "self", ".", "roberta", "=", "RobertaModel", "(", "config", ",", "add_pooling_layer", "=", "False", ")", "\n", "self", ".", "lm_head", "=", "RobertaLMHead", "(", "config", ")", "\n", "\n", "# The LM head weights require special treatment only when they are tied with the word embeddings", "\n", "self", ".", "update_keys_to_ignore", "(", "config", ",", "[", "\"lm_head.decoder.weight\"", "]", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForMaskedLM.get_output_embeddings": [[1195, 1197], ["None"], "methods", ["None"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "lm_head", ".", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForMaskedLM.set_output_embeddings": [[1198, 1200], ["None"], "methods", ["None"], ["", "def", "set_output_embeddings", "(", "self", ",", "new_embeddings", ")", ":", "\n", "        ", "self", ".", "lm_head", ".", "decoder", "=", "new_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForMaskedLM.forward": [[1201, 1269], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "modeling_roberta.RobertaForMaskedLM.roberta", "modeling_roberta.RobertaForMaskedLM.lm_head", "transformers.modeling_outputs.MaskedLMOutput", "ROBERTA_INPUTS_DOCSTRING.format", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling_roberta.RobertaForMaskedLM.roberta.get_invertible_adapter", "modeling_roberta.RobertaForMaskedLM.view", "labels.view"], "methods", ["None"], ["", "@", "add_start_docstrings_to_model_forward", "(", "ROBERTA_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "_CHECKPOINT_FOR_DOC", ",", "\n", "output_type", "=", "MaskedLMOutput", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", "mask", "=", "\"<mask>\"", ",", "\n", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "adapter_names", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n            Used to hide legacy arguments that have been deprecated.\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "encoder_hidden_states", "=", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", "=", "encoder_attention_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "adapter_names", "=", "adapter_names", ",", "\n", ")", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "prediction_scores", "=", "self", ".", "lm_head", "(", "\n", "sequence_output", ",", "\n", "inv_lang_adapter", "=", "self", ".", "roberta", ".", "get_invertible_adapter", "(", ")", ",", "\n", ")", "\n", "\n", "masked_lm_loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "prediction_scores", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "masked_lm_loss", ",", ")", "+", "output", ")", "if", "masked_lm_loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "MaskedLMOutput", "(", "\n", "loss", "=", "masked_lm_loss", ",", "\n", "logits", "=", "prediction_scores", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaLMHead.__init__": [[1275, 1283], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Parameter", "torch.nn.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "layer_norm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "vocab_size", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "config", ".", "vocab_size", ")", ")", "\n", "self", ".", "decoder", ".", "bias", "=", "self", ".", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaLMHead.forward": [[1284, 1296], ["modeling_roberta.RobertaLMHead.dense", "transformers.activations.gelu", "modeling_roberta.RobertaLMHead.layer_norm", "modeling_roberta.RobertaLMHead.decoder", "inv_lang_adapter"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ",", "inv_lang_adapter", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "x", "=", "self", ".", "dense", "(", "features", ")", "\n", "x", "=", "gelu", "(", "x", ")", "\n", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "\n", "if", "inv_lang_adapter", ":", "\n", "            ", "x", "=", "inv_lang_adapter", "(", "x", ",", "rev", "=", "True", ")", "\n", "\n", "# project back to size of vocabulary with bias", "\n", "", "x", "=", "self", ".", "decoder", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaLMHead._tie_weights": [[1297, 1300], ["None"], "methods", ["None"], ["", "def", "_tie_weights", "(", "self", ")", ":", "\n", "# To tie those two weights if they get disconnected (on TPU or when the bias is resized)", "\n", "        ", "self", ".", "bias", "=", "self", ".", "decoder", ".", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForSequenceClassification.__init__": [[1312, 1321], ["transformers.adapters.model_mixin.ModelWithHeadsAdaptersMixin.__init__", "modeling_roberta.RobertaModel", "modeling_roberta.RobertaClassificationHead", "modeling_roberta.RobertaForSequenceClassification.init_weights"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.init_weights"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "roberta", "=", "RobertaModel", "(", "config", ",", "add_pooling_layer", "=", "False", ")", "\n", "self", ".", "classifier", "=", "RobertaClassificationHead", "(", "config", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForSequenceClassification.forward": [[1322, 1398], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "modeling_roberta.RobertaForSequenceClassification.roberta", "modeling_roberta.RobertaForSequenceClassification.classifier", "transformers.modeling_outputs.SequenceClassifierOutput", "ROBERTA_INPUTS_DOCSTRING.format", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss.", "modeling_roberta.RobertaForSequenceClassification.squeeze", "labels.squeeze", "modeling_roberta.RobertaForSequenceClassification.view", "labels.view", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.BCEWithLogitsLoss."], "methods", ["None"], ["", "@", "add_start_docstrings_to_model_forward", "(", "ROBERTA_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "_CHECKPOINT_FOR_DOC", ",", "\n", "output_type", "=", "SequenceClassifierOutput", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "adapter_names", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "adapter_names", "=", "adapter_names", ",", "\n", ")", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "config", ".", "problem_type", "is", "None", ":", "\n", "                ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"regression\"", "\n", "", "elif", "self", ".", "num_labels", ">", "1", "and", "(", "labels", ".", "dtype", "==", "torch", ".", "long", "or", "labels", ".", "dtype", "==", "torch", ".", "int", ")", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"single_label_classification\"", "\n", "", "else", ":", "\n", "                    ", "self", ".", "config", ".", "problem_type", "=", "\"multi_label_classification\"", "\n", "\n", "", "", "if", "self", ".", "config", ".", "problem_type", "==", "\"regression\"", ":", "\n", "                ", "loss_fct", "=", "MSELoss", "(", ")", "\n", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                    ", "loss", "=", "loss_fct", "(", "logits", ".", "squeeze", "(", ")", ",", "labels", ".", "squeeze", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "loss", "=", "loss_fct", "(", "logits", ",", "labels", ")", "\n", "", "", "elif", "self", ".", "config", ".", "problem_type", "==", "\"single_label_classification\"", ":", "\n", "                ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "elif", "self", ".", "config", ".", "problem_type", "==", "\"multi_label_classification\"", ":", "\n", "                ", "loss_fct", "=", "BCEWithLogitsLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ",", "labels", ")", "\n", "\n", "", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "SequenceClassifierOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForMultipleChoice.__init__": [[1411, 1419], ["transformers.adapters.model_mixin.ModelWithHeadsAdaptersMixin.__init__", "modeling_roberta.RobertaModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "modeling_roberta.RobertaForMultipleChoice.init_weights"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.init_weights"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "roberta", "=", "RobertaModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForMultipleChoice.forward": [[1420, 1492], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "modeling_roberta.RobertaForMultipleChoice.roberta", "modeling_roberta.RobertaForMultipleChoice.dropout", "modeling_roberta.RobertaForMultipleChoice.classifier", "modeling_roberta.RobertaForMultipleChoice.view", "transformers.modeling_outputs.MultipleChoiceModelOutput", "ROBERTA_INPUTS_DOCSTRING.format", "input_ids.view", "position_ids.view", "token_type_ids.view", "attention_mask.view", "inputs_embeds.view", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "input_ids.size", "position_ids.size", "token_type_ids.size", "attention_mask.size", "inputs_embeds.size", "inputs_embeds.size"], "methods", ["None"], ["", "@", "add_start_docstrings_to_model_forward", "(", "ROBERTA_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, num_choices, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "_CHECKPOINT_FOR_DOC", ",", "\n", "output_type", "=", "MultipleChoiceModelOutput", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "adapter_names", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n            :obj:`input_ids` above)\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "num_choices", "=", "input_ids", ".", "shape", "[", "1", "]", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "shape", "[", "1", "]", "\n", "\n", "flat_input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_ids", ".", "size", "(", "-", "1", ")", ")", "if", "input_ids", "is", "not", "None", "else", "None", "\n", "flat_position_ids", "=", "position_ids", ".", "view", "(", "-", "1", ",", "position_ids", ".", "size", "(", "-", "1", ")", ")", "if", "position_ids", "is", "not", "None", "else", "None", "\n", "flat_token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "token_type_ids", ".", "size", "(", "-", "1", ")", ")", "if", "token_type_ids", "is", "not", "None", "else", "None", "\n", "flat_attention_mask", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "attention_mask", ".", "size", "(", "-", "1", ")", ")", "if", "attention_mask", "is", "not", "None", "else", "None", "\n", "flat_inputs_embeds", "=", "(", "\n", "inputs_embeds", ".", "view", "(", "-", "1", ",", "inputs_embeds", ".", "size", "(", "-", "2", ")", ",", "inputs_embeds", ".", "size", "(", "-", "1", ")", ")", "\n", "if", "inputs_embeds", "is", "not", "None", "\n", "else", "None", "\n", ")", "\n", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "flat_input_ids", ",", "\n", "position_ids", "=", "flat_position_ids", ",", "\n", "token_type_ids", "=", "flat_token_type_ids", ",", "\n", "attention_mask", "=", "flat_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "flat_inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "adapter_names", "=", "adapter_names", ",", "\n", ")", "\n", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "reshaped_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "num_choices", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "reshaped_logits", ",", "labels", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "reshaped_logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "MultipleChoiceModelOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "reshaped_logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForTokenClassification.__init__": [[1506, 1518], ["transformers.adapters.model_mixin.ModelWithHeadsAdaptersMixin.__init__", "modeling_roberta.RobertaModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "modeling_roberta.RobertaForTokenClassification.init_weights"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.init_weights"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "\n", "self", ".", "roberta", "=", "RobertaModel", "(", "config", ",", "add_pooling_layer", "=", "False", ")", "\n", "classifier_dropout", "=", "(", "\n", "config", ".", "classifier_dropout", "if", "config", ".", "classifier_dropout", "is", "not", "None", "else", "config", ".", "hidden_dropout_prob", "\n", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "classifier_dropout", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForTokenClassification.forward": [[1519, 1588], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "modeling_roberta.RobertaForTokenClassification.roberta", "modeling_roberta.RobertaForTokenClassification.dropout", "modeling_roberta.RobertaForTokenClassification.classifier", "transformers.modeling_outputs.TokenClassifierOutput", "ROBERTA_INPUTS_DOCSTRING.format", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "modeling_roberta.RobertaForTokenClassification.view", "torch.where", "torch.where", "torch.where", "torch.where", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "attention_mask.view", "labels.view", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "modeling_roberta.RobertaForTokenClassification.view", "labels.view", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "@", "add_start_docstrings_to_model_forward", "(", "ROBERTA_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "_CHECKPOINT_FOR_DOC", ",", "\n", "output_type", "=", "TokenClassifierOutput", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "adapter_names", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n            1]``.\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "adapter_names", "=", "adapter_names", ",", "\n", ")", "\n", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "\n", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "# Only keep active parts of the loss", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "\n", "active_labels", "=", "torch", ".", "where", "(", "\n", "active_loss", ",", "labels", ".", "view", "(", "-", "1", ")", ",", "torch", ".", "tensor", "(", "loss_fct", ".", "ignore_index", ")", ".", "type_as", "(", "labels", ")", "\n", ")", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "TokenClassifierOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaClassificationHead.__init__": [[1594, 1602], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "classifier_dropout", "=", "(", "\n", "config", ".", "classifier_dropout", "if", "config", ".", "classifier_dropout", "is", "not", "None", "else", "config", ".", "hidden_dropout_prob", "\n", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "classifier_dropout", ")", "\n", "self", ".", "out_proj", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaClassificationHead.forward": [[1603, 1611], ["modeling_roberta.RobertaClassificationHead.dropout", "modeling_roberta.RobertaClassificationHead.dense", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "modeling_roberta.RobertaClassificationHead.dropout", "modeling_roberta.RobertaClassificationHead.out_proj"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ",", "**", "kwargs", ")", ":", "\n", "        ", "x", "=", "features", "[", ":", ",", "0", ",", ":", "]", "# take <s> token (equiv. to [CLS])", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "x", "=", "self", ".", "dense", "(", "x", ")", "\n", "x", "=", "torch", ".", "tanh", "(", "x", ")", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "x", "=", "self", ".", "out_proj", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForQuestionAnswering.__init__": [[1624, 1632], ["transformers.adapters.model_mixin.ModelWithHeadsAdaptersMixin.__init__", "modeling_roberta.RobertaModel", "torch.nn.Linear", "torch.nn.Linear", "modeling_roberta.RobertaForQuestionAnswering.init_weights"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__", "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.init_weights"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "\n", "self", ".", "roberta", "=", "RobertaModel", "(", "config", ",", "add_pooling_layer", "=", "False", ")", "\n", "self", ".", "qa_outputs", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.RobertaForQuestionAnswering.forward": [[1633, 1713], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "modeling_roberta.RobertaForQuestionAnswering.roberta", "modeling_roberta.RobertaForQuestionAnswering.qa_outputs", "modeling_roberta.RobertaForQuestionAnswering.split", "start_logits.squeeze().contiguous.squeeze().contiguous.squeeze().contiguous", "end_logits.squeeze().contiguous.squeeze().contiguous.squeeze().contiguous", "transformers.modeling_outputs.QuestionAnsweringModelOutput", "ROBERTA_INPUTS_DOCSTRING.format", "start_logits.squeeze().contiguous.squeeze().contiguous.size", "start_positions.squeeze.squeeze.clamp", "end_positions.squeeze.squeeze.clamp", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "start_logits.squeeze().contiguous.squeeze().contiguous.squeeze", "end_logits.squeeze().contiguous.squeeze().contiguous.squeeze", "len", "start_positions.squeeze.squeeze.squeeze", "len", "end_positions.squeeze.squeeze.squeeze", "start_positions.squeeze.squeeze.size", "end_positions.squeeze.squeeze.size"], "methods", ["None"], ["", "@", "add_start_docstrings_to_model_forward", "(", "ROBERTA_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "_CHECKPOINT_FOR_DOC", ",", "\n", "output_type", "=", "QuestionAnsweringModelOutput", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "start_positions", "=", "None", ",", "\n", "end_positions", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "adapter_names", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n            sequence are not taken into account for computing the loss.\n        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n            sequence are not taken into account for computing the loss.\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "\n", "logits", "=", "self", ".", "qa_outputs", "(", "sequence_output", ")", "\n", "start_logits", ",", "end_logits", "=", "logits", ".", "split", "(", "1", ",", "dim", "=", "-", "1", ")", "\n", "start_logits", "=", "start_logits", ".", "squeeze", "(", "-", "1", ")", ".", "contiguous", "(", ")", "\n", "end_logits", "=", "end_logits", ".", "squeeze", "(", "-", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "total_loss", "=", "None", "\n", "if", "start_positions", "is", "not", "None", "and", "end_positions", "is", "not", "None", ":", "\n", "# If we are on multi-GPU, split add a dimension", "\n", "            ", "if", "len", "(", "start_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "start_positions", "=", "start_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "", "if", "len", "(", "end_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "end_positions", "=", "end_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "# sometimes the start/end positions are outside our model inputs, we ignore these terms", "\n", "", "ignored_index", "=", "start_logits", ".", "size", "(", "1", ")", "\n", "start_positions", "=", "start_positions", ".", "clamp", "(", "0", ",", "ignored_index", ")", "\n", "end_positions", "=", "end_positions", ".", "clamp", "(", "0", ",", "ignored_index", ")", "\n", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "ignored_index", ")", "\n", "start_loss", "=", "loss_fct", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "loss_fct", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "start_logits", ",", "end_logits", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "total_loss", ",", ")", "+", "output", ")", "if", "total_loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "QuestionAnsweringModelOutput", "(", "\n", "loss", "=", "total_loss", ",", "\n", "start_logits", "=", "start_logits", ",", "\n", "end_logits", "=", "end_logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_roberta.create_position_ids_from_input_ids": [[1716, 1730], ["input_ids.ne().int", "incremental_indices.long", "input_ids.ne", "torch.cumsum().type_as", "torch.cumsum().type_as", "torch.cumsum", "torch.cumsum"], "function", ["None"], ["", "", "def", "create_position_ids_from_input_ids", "(", "input_ids", ",", "padding_idx", ",", "past_key_values_length", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n    are ignored. This is modified from fairseq's `utils.make_positions`.\n\n    Args:\n        x: torch.Tensor x:\n\n    Returns: torch.Tensor\n    \"\"\"", "\n", "# The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.", "\n", "mask", "=", "input_ids", ".", "ne", "(", "padding_idx", ")", ".", "int", "(", ")", "\n", "incremental_indices", "=", "(", "torch", ".", "cumsum", "(", "mask", ",", "dim", "=", "1", ")", ".", "type_as", "(", "mask", ")", "+", "past_key_values_length", ")", "*", "mask", "\n", "return", "incremental_indices", ".", "long", "(", ")", "+", "padding_idx", "\n", "", ""]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaEmbeddings.setup": [[137, 158], ["flax.Embed", "flax.Embed", "flax.Embed", "flax.LayerNorm", "flax.Dropout", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal"], "methods", ["None"], ["def", "setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "word_embeddings", "=", "nn", ".", "Embed", "(", "\n", "self", ".", "config", ".", "vocab_size", ",", "\n", "self", ".", "config", ".", "hidden_size", ",", "\n", "embedding_init", "=", "jax", ".", "nn", ".", "initializers", ".", "normal", "(", "stddev", "=", "self", ".", "config", ".", "initializer_range", ")", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embed", "(", "\n", "self", ".", "config", ".", "max_position_embeddings", ",", "\n", "self", ".", "config", ".", "hidden_size", ",", "\n", "embedding_init", "=", "jax", ".", "nn", ".", "initializers", ".", "normal", "(", "stddev", "=", "self", ".", "config", ".", "initializer_range", ")", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embed", "(", "\n", "self", ".", "config", ".", "type_vocab_size", ",", "\n", "self", ".", "config", ".", "hidden_size", ",", "\n", "embedding_init", "=", "jax", ".", "nn", ".", "initializers", ".", "normal", "(", "stddev", "=", "self", ".", "config", ".", "initializer_range", ")", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "epsilon", "=", "self", ".", "config", ".", "layer_norm_eps", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "rate", "=", "self", ".", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaEmbeddings.__call__": [[159, 172], ["modeling_flax_roberta.FlaxRobertaEmbeddings.word_embeddings", "modeling_flax_roberta.FlaxRobertaEmbeddings.position_embeddings", "modeling_flax_roberta.FlaxRobertaEmbeddings.token_type_embeddings", "modeling_flax_roberta.FlaxRobertaEmbeddings.LayerNorm", "modeling_flax_roberta.FlaxRobertaEmbeddings.dropout", "input_ids.astype", "position_ids.astype", "token_type_ids.astype"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "deterministic", ":", "bool", "=", "True", ")", ":", "\n", "# Embed", "\n", "        ", "inputs_embeds", "=", "self", ".", "word_embeddings", "(", "input_ids", ".", "astype", "(", "\"i4\"", ")", ")", "\n", "position_embeds", "=", "self", ".", "position_embeddings", "(", "position_ids", ".", "astype", "(", "\"i4\"", ")", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ".", "astype", "(", "\"i4\"", ")", ")", "\n", "\n", "# Sum all embeddings", "\n", "hidden_states", "=", "inputs_embeds", "+", "token_type_embeddings", "+", "position_embeds", "\n", "\n", "# Layer Norm", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ",", "deterministic", "=", "deterministic", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaSelfAttention.setup": [[179, 200], ["flax.Dense", "flax.Dense", "flax.Dense", "ValueError", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal"], "methods", ["None"], ["def", "setup", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "config", ".", "hidden_size", "%", "self", ".", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"`config.hidden_size`: {self.config.hidden_size} has to be a multiple of `config.num_attention_heads`\\\n                    : {self.config.num_attention_heads}\"", "\n", ")", "\n", "\n", "", "self", ".", "query", "=", "nn", ".", "Dense", "(", "\n", "self", ".", "config", ".", "hidden_size", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", "kernel_init", "=", "jax", ".", "nn", ".", "initializers", ".", "normal", "(", "self", ".", "config", ".", "initializer_range", ",", "self", ".", "dtype", ")", ",", "\n", ")", "\n", "self", ".", "key", "=", "nn", ".", "Dense", "(", "\n", "self", ".", "config", ".", "hidden_size", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", "kernel_init", "=", "jax", ".", "nn", ".", "initializers", ".", "normal", "(", "self", ".", "config", ".", "initializer_range", ",", "self", ".", "dtype", ")", ",", "\n", ")", "\n", "self", ".", "value", "=", "nn", ".", "Dense", "(", "\n", "self", ".", "config", ".", "hidden_size", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", "kernel_init", "=", "jax", ".", "nn", ".", "initializers", ".", "normal", "(", "self", ".", "config", ".", "initializer_range", ",", "self", ".", "dtype", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaSelfAttention.__call__": [[202, 248], ["modeling_flax_roberta.FlaxRobertaSelfAttention.query().reshape", "modeling_flax_roberta.FlaxRobertaSelfAttention.value().reshape", "modeling_flax_roberta.FlaxRobertaSelfAttention.key().reshape", "flax.linen.attention.dot_product_attention_weights", "jax.einsum", "jax.einsum", "attn_output.reshape.reshape.reshape", "jax.expand_dims", "jax.expand_dims", "jax.lax.select", "jax.lax.select", "modeling_flax_roberta.FlaxRobertaSelfAttention.make_rng", "modeling_flax_roberta.FlaxRobertaSelfAttention.query", "modeling_flax_roberta.FlaxRobertaSelfAttention.value", "modeling_flax_roberta.FlaxRobertaSelfAttention.key", "jax.full().astype", "jax.full().astype", "jax.full().astype", "jax.full().astype", "jax.full", "jax.full", "jax.full", "jax.full"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "deterministic", "=", "True", ",", "output_attentions", ":", "bool", "=", "False", ")", ":", "\n", "        ", "head_dim", "=", "self", ".", "config", ".", "hidden_size", "//", "self", ".", "config", ".", "num_attention_heads", "\n", "\n", "query_states", "=", "self", ".", "query", "(", "hidden_states", ")", ".", "reshape", "(", "\n", "hidden_states", ".", "shape", "[", ":", "2", "]", "+", "(", "self", ".", "config", ".", "num_attention_heads", ",", "head_dim", ")", "\n", ")", "\n", "value_states", "=", "self", ".", "value", "(", "hidden_states", ")", ".", "reshape", "(", "\n", "hidden_states", ".", "shape", "[", ":", "2", "]", "+", "(", "self", ".", "config", ".", "num_attention_heads", ",", "head_dim", ")", "\n", ")", "\n", "key_states", "=", "self", ".", "key", "(", "hidden_states", ")", ".", "reshape", "(", "\n", "hidden_states", ".", "shape", "[", ":", "2", "]", "+", "(", "self", ".", "config", ".", "num_attention_heads", ",", "head_dim", ")", "\n", ")", "\n", "\n", "# Convert the boolean attention mask to an attention bias.", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "# attention mask in the form of attention bias", "\n", "            ", "attention_mask", "=", "jnp", ".", "expand_dims", "(", "attention_mask", ",", "axis", "=", "(", "-", "3", ",", "-", "2", ")", ")", "\n", "attention_bias", "=", "lax", ".", "select", "(", "\n", "attention_mask", ">", "0", ",", "\n", "jnp", ".", "full", "(", "attention_mask", ".", "shape", ",", "0.0", ")", ".", "astype", "(", "self", ".", "dtype", ")", ",", "\n", "jnp", ".", "full", "(", "attention_mask", ".", "shape", ",", "-", "1e10", ")", ".", "astype", "(", "self", ".", "dtype", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "attention_bias", "=", "None", "\n", "\n", "", "dropout_rng", "=", "None", "\n", "if", "not", "deterministic", "and", "self", ".", "config", ".", "attention_probs_dropout_prob", ">", "0.0", ":", "\n", "            ", "dropout_rng", "=", "self", ".", "make_rng", "(", "\"dropout\"", ")", "\n", "\n", "", "attn_weights", "=", "dot_product_attention_weights", "(", "\n", "query_states", ",", "\n", "key_states", ",", "\n", "bias", "=", "attention_bias", ",", "\n", "dropout_rng", "=", "dropout_rng", ",", "\n", "dropout_rate", "=", "self", ".", "config", ".", "attention_probs_dropout_prob", ",", "\n", "broadcast_dropout", "=", "True", ",", "\n", "deterministic", "=", "deterministic", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", "precision", "=", "None", ",", "\n", ")", "\n", "\n", "attn_output", "=", "jnp", ".", "einsum", "(", "\"...hqk,...khd->...qhd\"", ",", "attn_weights", ",", "value_states", ")", "\n", "attn_output", "=", "attn_output", ".", "reshape", "(", "attn_output", ".", "shape", "[", ":", "2", "]", "+", "(", "-", "1", ",", ")", ")", "\n", "\n", "outputs", "=", "(", "attn_output", ",", "attn_weights", ")", "if", "output_attentions", "else", "(", "attn_output", ",", ")", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaSelfOutput.setup": [[255, 263], ["flax.Dense", "flax.LayerNorm", "flax.Dropout", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal"], "methods", ["None"], ["def", "setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "dense", "=", "nn", ".", "Dense", "(", "\n", "self", ".", "config", ".", "hidden_size", ",", "\n", "kernel_init", "=", "jax", ".", "nn", ".", "initializers", ".", "normal", "(", "self", ".", "config", ".", "initializer_range", ",", "self", ".", "dtype", ")", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "epsilon", "=", "self", ".", "config", ".", "layer_norm_eps", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "rate", "=", "self", ".", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaSelfOutput.__call__": [[264, 269], ["modeling_flax_roberta.FlaxRobertaSelfOutput.dense", "modeling_flax_roberta.FlaxRobertaSelfOutput.dropout", "modeling_flax_roberta.FlaxRobertaSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "hidden_states", ",", "input_tensor", ",", "deterministic", ":", "bool", "=", "True", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ",", "deterministic", "=", "deterministic", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaAttention.setup": [[276, 279], ["modeling_flax_roberta.FlaxRobertaSelfAttention", "modeling_flax_roberta.FlaxRobertaSelfOutput"], "methods", ["None"], ["def", "setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "self", "=", "FlaxRobertaSelfAttention", "(", "self", ".", "config", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "self", ".", "output", "=", "FlaxRobertaSelfOutput", "(", "self", ".", "config", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaAttention.__call__": [[280, 296], ["modeling_flax_roberta.FlaxRobertaAttention.self", "modeling_flax_roberta.FlaxRobertaAttention.output"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "deterministic", "=", "True", ",", "output_attentions", ":", "bool", "=", "False", ")", ":", "\n", "# Attention mask comes in as attention_mask.shape == (*batch_sizes, kv_length)", "\n", "# FLAX expects: attention_mask.shape == (*batch_sizes, 1, 1, kv_length) such that it is broadcastable", "\n", "# with attn_weights.shape == (*batch_sizes, num_heads, q_length, kv_length)", "\n", "        ", "attn_outputs", "=", "self", ".", "self", "(", "\n", "hidden_states", ",", "attention_mask", ",", "deterministic", "=", "deterministic", ",", "output_attentions", "=", "output_attentions", "\n", ")", "\n", "attn_output", "=", "attn_outputs", "[", "0", "]", "\n", "hidden_states", "=", "self", ".", "output", "(", "attn_output", ",", "hidden_states", ",", "deterministic", "=", "deterministic", ")", "\n", "\n", "outputs", "=", "(", "hidden_states", ",", ")", "\n", "\n", "if", "output_attentions", ":", "\n", "            ", "outputs", "+=", "(", "attn_outputs", "[", "1", "]", ",", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaIntermediate.setup": [[303, 310], ["flax.Dense", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal"], "methods", ["None"], ["def", "setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "dense", "=", "nn", ".", "Dense", "(", "\n", "self", ".", "config", ".", "intermediate_size", ",", "\n", "kernel_init", "=", "jax", ".", "nn", ".", "initializers", ".", "normal", "(", "self", ".", "config", ".", "initializer_range", ",", "self", ".", "dtype", ")", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", ")", "\n", "self", ".", "activation", "=", "ACT2FN", "[", "self", ".", "config", ".", "hidden_act", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaIntermediate.__call__": [[311, 315], ["modeling_flax_roberta.FlaxRobertaIntermediate.dense", "modeling_flax_roberta.FlaxRobertaIntermediate.activation"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "activation", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaOutput.setup": [[322, 330], ["flax.Dense", "flax.Dropout", "flax.LayerNorm", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal"], "methods", ["None"], ["def", "setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "dense", "=", "nn", ".", "Dense", "(", "\n", "self", ".", "config", ".", "hidden_size", ",", "\n", "kernel_init", "=", "jax", ".", "nn", ".", "initializers", ".", "normal", "(", "self", ".", "config", ".", "initializer_range", ",", "self", ".", "dtype", ")", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "rate", "=", "self", ".", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "epsilon", "=", "self", ".", "config", ".", "layer_norm_eps", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaOutput.__call__": [[331, 336], ["modeling_flax_roberta.FlaxRobertaOutput.dense", "modeling_flax_roberta.FlaxRobertaOutput.dropout", "modeling_flax_roberta.FlaxRobertaOutput.LayerNorm"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "hidden_states", ",", "attention_output", ",", "deterministic", ":", "bool", "=", "True", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ",", "deterministic", "=", "deterministic", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "attention_output", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaLayer.setup": [[343, 347], ["modeling_flax_roberta.FlaxRobertaAttention", "modeling_flax_roberta.FlaxRobertaIntermediate", "modeling_flax_roberta.FlaxRobertaOutput"], "methods", ["None"], ["def", "setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "attention", "=", "FlaxRobertaAttention", "(", "self", ".", "config", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "self", ".", "intermediate", "=", "FlaxRobertaIntermediate", "(", "self", ".", "config", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "self", ".", "output", "=", "FlaxRobertaOutput", "(", "self", ".", "config", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaLayer.__call__": [[348, 362], ["modeling_flax_roberta.FlaxRobertaLayer.attention", "modeling_flax_roberta.FlaxRobertaLayer.intermediate", "modeling_flax_roberta.FlaxRobertaLayer.output"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "deterministic", ":", "bool", "=", "True", ",", "output_attentions", ":", "bool", "=", "False", ")", ":", "\n", "        ", "attention_outputs", "=", "self", ".", "attention", "(", "\n", "hidden_states", ",", "attention_mask", ",", "deterministic", "=", "deterministic", ",", "output_attentions", "=", "output_attentions", "\n", ")", "\n", "attention_output", "=", "attention_outputs", "[", "0", "]", "\n", "\n", "hidden_states", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "hidden_states", "=", "self", ".", "output", "(", "hidden_states", ",", "attention_output", ",", "deterministic", "=", "deterministic", ")", "\n", "\n", "outputs", "=", "(", "hidden_states", ",", ")", "\n", "\n", "if", "output_attentions", ":", "\n", "            ", "outputs", "+=", "(", "attention_outputs", "[", "1", "]", ",", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaLayerCollection.setup": [[369, 372], ["modeling_flax_roberta.FlaxRobertaLayer", "range", "str"], "methods", ["None"], ["def", "setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "layers", "=", "[", "\n", "FlaxRobertaLayer", "(", "self", ".", "config", ",", "name", "=", "str", "(", "i", ")", ",", "dtype", "=", "self", ".", "dtype", ")", "for", "i", "in", "range", "(", "self", ".", "config", ".", "num_hidden_layers", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaLayerCollection.__call__": [[374, 409], ["enumerate", "modeling_flax_outputs.FlaxBaseModelOutput", "layer", "tuple"], "methods", ["None"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "deterministic", ":", "bool", "=", "True", ",", "\n", "output_attentions", ":", "bool", "=", "False", ",", "\n", "output_hidden_states", ":", "bool", "=", "False", ",", "\n", "return_dict", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "all_attentions", "=", "(", ")", "if", "output_attentions", "else", "None", "\n", "all_hidden_states", "=", "(", ")", "if", "output_hidden_states", "else", "None", "\n", "\n", "for", "i", ",", "layer", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "            ", "if", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "+=", "(", "hidden_states", ",", ")", "\n", "\n", "", "layer_outputs", "=", "layer", "(", "\n", "hidden_states", ",", "attention_mask", ",", "deterministic", "=", "deterministic", ",", "output_attentions", "=", "output_attentions", "\n", ")", "\n", "\n", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "\n", "if", "output_attentions", ":", "\n", "                ", "all_attentions", "+=", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "\n", "", "", "if", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "+=", "(", "hidden_states", ",", ")", "\n", "\n", "", "outputs", "=", "(", "hidden_states", ",", ")", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "tuple", "(", "v", "for", "v", "in", "outputs", "if", "v", "is", "not", "None", ")", "\n", "\n", "", "return", "FlaxBaseModelOutput", "(", "\n", "last_hidden_state", "=", "hidden_states", ",", "hidden_states", "=", "all_hidden_states", ",", "attentions", "=", "all_attentions", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaEncoder.setup": [[417, 419], ["modeling_flax_roberta.FlaxRobertaLayerCollection"], "methods", ["None"], ["def", "setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "layer", "=", "FlaxRobertaLayerCollection", "(", "self", ".", "config", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaEncoder.__call__": [[420, 436], ["modeling_flax_roberta.FlaxRobertaEncoder.layer"], "methods", ["None"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "deterministic", ":", "bool", "=", "True", ",", "\n", "output_attentions", ":", "bool", "=", "False", ",", "\n", "output_hidden_states", ":", "bool", "=", "False", ",", "\n", "return_dict", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "return", "self", ".", "layer", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "deterministic", "=", "deterministic", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPooler.setup": [[444, 449], ["flax.Dense", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal"], "methods", ["None"], ["def", "setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "dense", "=", "nn", ".", "Dense", "(", "\n", "self", ".", "config", ".", "hidden_size", ",", "\n", "kernel_init", "=", "jax", ".", "nn", ".", "initializers", ".", "normal", "(", "self", ".", "config", ".", "initializer_range", ",", "self", ".", "dtype", ")", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPooler.__call__": [[451, 455], ["modeling_flax_roberta.FlaxRobertaPooler.dense", "flax.tanh"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "cls_hidden_state", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "cls_hidden_state", "=", "self", ".", "dense", "(", "cls_hidden_state", ")", "\n", "return", "nn", ".", "tanh", "(", "cls_hidden_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaLMHead.setup": [[462, 476], ["flax.Dense", "flax.LayerNorm", "flax.Dense", "modeling_flax_roberta.FlaxRobertaLMHead.param", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal"], "methods", ["None"], ["def", "setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "dense", "=", "nn", ".", "Dense", "(", "\n", "self", ".", "config", ".", "hidden_size", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", "kernel_init", "=", "jax", ".", "nn", ".", "initializers", ".", "normal", "(", "self", ".", "config", ".", "initializer_range", ",", "self", ".", "dtype", ")", ",", "\n", ")", "\n", "self", ".", "layer_norm", "=", "nn", ".", "LayerNorm", "(", "epsilon", "=", "self", ".", "config", ".", "layer_norm_eps", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "self", ".", "decoder", "=", "nn", ".", "Dense", "(", "\n", "self", ".", "config", ".", "vocab_size", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", "use_bias", "=", "False", ",", "\n", "kernel_init", "=", "jax", ".", "nn", ".", "initializers", ".", "normal", "(", "self", ".", "config", ".", "initializer_range", ",", "self", ".", "dtype", ")", ",", "\n", ")", "\n", "self", ".", "bias", "=", "self", ".", "param", "(", "\"bias\"", ",", "self", ".", "bias_init", ",", "(", "self", ".", "config", ".", "vocab_size", ",", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaLMHead.__call__": [[477, 489], ["modeling_flax_roberta.FlaxRobertaLMHead.dense", "modeling_flax_roberta.FlaxRobertaLMHead.layer_norm", "modeling_flax_roberta.FlaxRobertaLMHead.decoder.apply", "modeling_flax_roberta.FlaxRobertaLMHead.decoder"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "hidden_states", ",", "shared_embedding", "=", "None", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "ACT2FN", "[", "\"gelu\"", "]", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "layer_norm", "(", "hidden_states", ")", "\n", "\n", "if", "shared_embedding", "is", "not", "None", ":", "\n", "            ", "hidden_states", "=", "self", ".", "decoder", ".", "apply", "(", "{", "\"params\"", ":", "{", "\"kernel\"", ":", "shared_embedding", ".", "T", "}", "}", ",", "hidden_states", ")", "\n", "", "else", ":", "\n", "            ", "hidden_states", "=", "self", ".", "decoder", "(", "hidden_states", ")", "\n", "\n", "", "hidden_states", "+=", "self", ".", "bias", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaClassificationHead.setup": [[495, 511], ["flax.Dense", "flax.Dropout", "flax.Dense", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal", "jax.nn.initializers.normal"], "methods", ["None"], ["def", "setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "dense", "=", "nn", ".", "Dense", "(", "\n", "self", ".", "config", ".", "hidden_size", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", "kernel_init", "=", "jax", ".", "nn", ".", "initializers", ".", "normal", "(", "self", ".", "config", ".", "initializer_range", ",", "self", ".", "dtype", ")", ",", "\n", ")", "\n", "classifier_dropout", "=", "(", "\n", "self", ".", "config", ".", "classifier_dropout", "\n", "if", "self", ".", "config", ".", "classifier_dropout", "is", "not", "None", "\n", "else", "self", ".", "config", ".", "hidden_dropout_prob", "\n", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "rate", "=", "classifier_dropout", ")", "\n", "self", ".", "out_proj", "=", "nn", ".", "Dense", "(", "\n", "self", ".", "config", ".", "num_labels", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", "kernel_init", "=", "jax", ".", "nn", ".", "initializers", ".", "normal", "(", "self", ".", "config", ".", "initializer_range", ",", "self", ".", "dtype", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaClassificationHead.__call__": [[513, 521], ["modeling_flax_roberta.FlaxRobertaClassificationHead.dropout", "modeling_flax_roberta.FlaxRobertaClassificationHead.dense", "flax.tanh", "modeling_flax_roberta.FlaxRobertaClassificationHead.dropout", "modeling_flax_roberta.FlaxRobertaClassificationHead.out_proj"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "hidden_states", ",", "deterministic", "=", "True", ")", ":", "\n", "        ", "hidden_states", "=", "hidden_states", "[", ":", ",", "0", ",", ":", "]", "# take <s> token (equiv. to [CLS])", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ",", "deterministic", "=", "deterministic", ")", "\n", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "nn", ".", "tanh", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ",", "deterministic", "=", "deterministic", ")", "\n", "hidden_states", "=", "self", ".", "out_proj", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.__init__": [[534, 544], ["modeling_flax_roberta.FlaxRobertaPreTrainedModel.module_class", "modeling_flax_utils.FlaxPreTrainedModel.__init__"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "config", ":", "RobertaConfig", ",", "\n", "input_shape", ":", "Tuple", "=", "(", "1", ",", "1", ")", ",", "\n", "seed", ":", "int", "=", "0", ",", "\n", "dtype", ":", "jnp", ".", "dtype", "=", "jnp", ".", "float32", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "module", "=", "self", ".", "module_class", "(", "config", "=", "config", ",", "dtype", "=", "dtype", ",", "**", "kwargs", ")", "\n", "super", "(", ")", ".", "__init__", "(", "config", ",", "module", ",", "input_shape", "=", "input_shape", ",", "seed", "=", "seed", ",", "dtype", "=", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.init_weights": [[545, 557], ["jax.zeros", "jax.zeros", "jax.ones_like", "jax.ones_like", "modeling_flax_roberta.create_position_ids_from_input_ids", "jax.ones_like", "jax.ones_like", "jax.random.split", "jax.random.split", "jax.random.split", "jax.random.split", "modeling_flax_roberta.FlaxRobertaPreTrainedModel.module.init"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.create_position_ids_from_input_ids"], ["", "def", "init_weights", "(", "self", ",", "rng", ":", "jax", ".", "random", ".", "PRNGKey", ",", "input_shape", ":", "Tuple", ")", "->", "FrozenDict", ":", "\n", "# init input tensors", "\n", "        ", "input_ids", "=", "jnp", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "\"i4\"", ")", "\n", "token_type_ids", "=", "jnp", ".", "ones_like", "(", "input_ids", ")", "\n", "position_ids", "=", "create_position_ids_from_input_ids", "(", "input_ids", ",", "self", ".", "config", ".", "pad_token_id", ")", "\n", "attention_mask", "=", "jnp", ".", "ones_like", "(", "input_ids", ")", "\n", "\n", "params_rng", ",", "dropout_rng", "=", "jax", ".", "random", ".", "split", "(", "rng", ")", "\n", "rngs", "=", "{", "\"params\"", ":", "params_rng", ",", "\"dropout\"", ":", "dropout_rng", "}", "\n", "\n", "return", "self", ".", "module", ".", "init", "(", "rngs", ",", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "position_ids", ",", "return_dict", "=", "False", ")", "[", "\n", "\"params\"", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.__call__": [[559, 605], ["file_utils.add_start_docstrings_to_model_forward", "modeling_flax_roberta.FlaxRobertaPreTrainedModel.module.apply", "ROBERTA_INPUTS_DOCSTRING.format", "jax.zeros_like", "jax.zeros_like", "modeling_flax_roberta.create_position_ids_from_input_ids", "jax.ones_like", "jax.ones_like", "jax.array", "jax.array", "jax.array", "jax.array", "jax.array", "jax.array", "jax.array", "jax.array"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.create_position_ids_from_input_ids"], ["", "@", "add_start_docstrings_to_model_forward", "(", "ROBERTA_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "def", "__call__", "(", "\n", "self", ",", "\n", "input_ids", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "params", ":", "dict", "=", "None", ",", "\n", "dropout_rng", ":", "PRNGKey", "=", "None", ",", "\n", "train", ":", "bool", "=", "False", ",", "\n", "output_attentions", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "output_hidden_states", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "return_dict", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "return_dict", "\n", "\n", "# init input tensors if not passed", "\n", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "jnp", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "create_position_ids_from_input_ids", "(", "input_ids", ",", "self", ".", "config", ".", "pad_token_id", ")", "\n", "\n", "", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "jnp", ".", "ones_like", "(", "input_ids", ")", "\n", "\n", "# Handle any PRNG if needed", "\n", "", "rngs", "=", "{", "}", "\n", "if", "dropout_rng", "is", "not", "None", ":", "\n", "            ", "rngs", "[", "\"dropout\"", "]", "=", "dropout_rng", "\n", "\n", "", "return", "self", ".", "module", ".", "apply", "(", "\n", "{", "\"params\"", ":", "params", "or", "self", ".", "params", "}", ",", "\n", "jnp", ".", "array", "(", "input_ids", ",", "dtype", "=", "\"i4\"", ")", ",", "\n", "jnp", ".", "array", "(", "attention_mask", ",", "dtype", "=", "\"i4\"", ")", ",", "\n", "jnp", ".", "array", "(", "token_type_ids", ",", "dtype", "=", "\"i4\"", ")", ",", "\n", "jnp", ".", "array", "(", "position_ids", ",", "dtype", "=", "\"i4\"", ")", ",", "\n", "not", "train", ",", "\n", "output_attentions", ",", "\n", "output_hidden_states", ",", "\n", "return_dict", ",", "\n", "rngs", "=", "rngs", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaModule.setup": [[614, 618], ["modeling_flax_roberta.FlaxRobertaEmbeddings", "modeling_flax_roberta.FlaxRobertaEncoder", "modeling_flax_roberta.FlaxRobertaPooler"], "methods", ["None"], ["def", "setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "embeddings", "=", "FlaxRobertaEmbeddings", "(", "self", ".", "config", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "self", ".", "encoder", "=", "FlaxRobertaEncoder", "(", "self", ".", "config", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "self", ".", "pooler", "=", "FlaxRobertaPooler", "(", "self", ".", "config", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaModule.__call__": [[619, 663], ["modeling_flax_roberta.FlaxRobertaModule.embeddings", "modeling_flax_roberta.FlaxRobertaModule.encoder", "modeling_flax_outputs.FlaxBaseModelOutputWithPooling", "jax.zeros_like", "jax.zeros_like", "jax.broadcast_to", "jax.broadcast_to", "modeling_flax_roberta.FlaxRobertaModule.pooler", "jax.arange", "jax.arange", "jax.atleast_2d", "jax.atleast_2d"], "methods", ["None"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "input_ids", ",", "\n", "attention_mask", ",", "\n", "token_type_ids", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "position_ids", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "deterministic", ":", "bool", "=", "True", ",", "\n", "output_attentions", ":", "bool", "=", "False", ",", "\n", "output_hidden_states", ":", "bool", "=", "False", ",", "\n", "return_dict", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "# make sure `token_type_ids` is correctly initialized when not passed", "\n", "        ", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "jnp", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "# make sure `position_ids` is correctly initialized when not passed", "\n", "", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "jnp", ".", "broadcast_to", "(", "jnp", ".", "arange", "(", "jnp", ".", "atleast_2d", "(", "input_ids", ")", ".", "shape", "[", "-", "1", "]", ")", ",", "input_ids", ".", "shape", ")", "\n", "\n", "", "hidden_states", "=", "self", ".", "embeddings", "(", "\n", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "deterministic", "=", "deterministic", "\n", ")", "\n", "outputs", "=", "self", ".", "encoder", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "deterministic", "=", "deterministic", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "hidden_states", "=", "outputs", "[", "0", "]", "\n", "pooled", "=", "self", ".", "pooler", "(", "hidden_states", ")", "if", "self", ".", "add_pooling_layer", "else", "None", "\n", "\n", "if", "not", "return_dict", ":", "\n", "# if pooled is None, don't return it", "\n", "            ", "if", "pooled", "is", "None", ":", "\n", "                ", "return", "(", "hidden_states", ",", ")", "+", "outputs", "[", "1", ":", "]", "\n", "", "return", "(", "hidden_states", ",", "pooled", ")", "+", "outputs", "[", "1", ":", "]", "\n", "\n", "", "return", "FlaxBaseModelOutputWithPooling", "(", "\n", "last_hidden_state", "=", "hidden_states", ",", "\n", "pooler_output", "=", "pooled", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaForMaskedLMModule.setup": [[683, 686], ["modeling_flax_roberta.FlaxRobertaModule", "modeling_flax_roberta.FlaxRobertaLMHead"], "methods", ["None"], ["def", "setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "roberta", "=", "FlaxRobertaModule", "(", "config", "=", "self", ".", "config", ",", "add_pooling_layer", "=", "False", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "self", ".", "lm_head", "=", "FlaxRobertaLMHead", "(", "config", "=", "self", ".", "config", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaForMaskedLMModule.__call__": [[687, 726], ["modeling_flax_roberta.FlaxRobertaForMaskedLMModule.roberta", "modeling_flax_roberta.FlaxRobertaForMaskedLMModule.lm_head", "modeling_flax_outputs.FlaxMaskedLMOutput"], "methods", ["None"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "input_ids", ",", "\n", "attention_mask", ",", "\n", "token_type_ids", ",", "\n", "position_ids", ",", "\n", "deterministic", ":", "bool", "=", "True", ",", "\n", "output_attentions", ":", "bool", "=", "False", ",", "\n", "output_hidden_states", ":", "bool", "=", "False", ",", "\n", "return_dict", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "# Model", "\n", "        ", "outputs", "=", "self", ".", "roberta", "(", "\n", "input_ids", ",", "\n", "attention_mask", ",", "\n", "token_type_ids", ",", "\n", "position_ids", ",", "\n", "deterministic", "=", "deterministic", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "hidden_states", "=", "outputs", "[", "0", "]", "\n", "if", "self", ".", "config", ".", "tie_word_embeddings", ":", "\n", "            ", "shared_embedding", "=", "self", ".", "roberta", ".", "variables", "[", "\"params\"", "]", "[", "\"embeddings\"", "]", "[", "\"word_embeddings\"", "]", "[", "\"embedding\"", "]", "\n", "", "else", ":", "\n", "            ", "shared_embedding", "=", "None", "\n", "\n", "# Compute the prediction scores", "\n", "", "logits", "=", "self", ".", "lm_head", "(", "hidden_states", ",", "shared_embedding", "=", "shared_embedding", ")", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "(", "logits", ",", ")", "+", "outputs", "[", "1", ":", "]", "\n", "\n", "", "return", "FlaxMaskedLMOutput", "(", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaForSequenceClassificationModule.setup": [[748, 751], ["modeling_flax_roberta.FlaxRobertaModule", "modeling_flax_roberta.FlaxRobertaClassificationHead"], "methods", ["None"], ["def", "setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "roberta", "=", "FlaxRobertaModule", "(", "config", "=", "self", ".", "config", ",", "dtype", "=", "self", ".", "dtype", ",", "add_pooling_layer", "=", "False", ")", "\n", "self", ".", "classifier", "=", "FlaxRobertaClassificationHead", "(", "config", "=", "self", ".", "config", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaForSequenceClassificationModule.__call__": [[752, 785], ["modeling_flax_roberta.FlaxRobertaForSequenceClassificationModule.roberta", "modeling_flax_roberta.FlaxRobertaForSequenceClassificationModule.classifier", "modeling_flax_outputs.FlaxSequenceClassifierOutput"], "methods", ["None"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "input_ids", ",", "\n", "attention_mask", ",", "\n", "token_type_ids", ",", "\n", "position_ids", ",", "\n", "deterministic", ":", "bool", "=", "True", ",", "\n", "output_attentions", ":", "bool", "=", "False", ",", "\n", "output_hidden_states", ":", "bool", "=", "False", ",", "\n", "return_dict", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "# Model", "\n", "        ", "outputs", "=", "self", ".", "roberta", "(", "\n", "input_ids", ",", "\n", "attention_mask", ",", "\n", "token_type_ids", ",", "\n", "position_ids", ",", "\n", "deterministic", "=", "deterministic", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ",", "deterministic", "=", "deterministic", ")", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "(", "logits", ",", ")", "+", "outputs", "[", "1", ":", "]", "\n", "\n", "", "return", "FlaxSequenceClassifierOutput", "(", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaForMultipleChoiceModule.setup": [[813, 817], ["modeling_flax_roberta.FlaxRobertaModule", "flax.Dropout", "flax.Dense"], "methods", ["None"], ["def", "setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "roberta", "=", "FlaxRobertaModule", "(", "config", "=", "self", ".", "config", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "rate", "=", "self", ".", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Dense", "(", "1", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaForMultipleChoiceModule.__call__": [[818, 860], ["modeling_flax_roberta.FlaxRobertaForMultipleChoiceModule.roberta", "modeling_flax_roberta.FlaxRobertaForMultipleChoiceModule.dropout", "modeling_flax_roberta.FlaxRobertaForMultipleChoiceModule.classifier", "modeling_flax_roberta.FlaxRobertaForMultipleChoiceModule.reshape", "modeling_flax_outputs.FlaxMultipleChoiceModelOutput", "input_ids.reshape", "attention_mask.reshape", "token_type_ids.reshape", "position_ids.reshape"], "methods", ["None"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "input_ids", ",", "\n", "attention_mask", ",", "\n", "token_type_ids", ",", "\n", "position_ids", ",", "\n", "deterministic", ":", "bool", "=", "True", ",", "\n", "output_attentions", ":", "bool", "=", "False", ",", "\n", "output_hidden_states", ":", "bool", "=", "False", ",", "\n", "return_dict", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "num_choices", "=", "input_ids", ".", "shape", "[", "1", "]", "\n", "input_ids", "=", "input_ids", ".", "reshape", "(", "-", "1", ",", "input_ids", ".", "shape", "[", "-", "1", "]", ")", "if", "input_ids", "is", "not", "None", "else", "None", "\n", "attention_mask", "=", "attention_mask", ".", "reshape", "(", "-", "1", ",", "attention_mask", ".", "shape", "[", "-", "1", "]", ")", "if", "attention_mask", "is", "not", "None", "else", "None", "\n", "token_type_ids", "=", "token_type_ids", ".", "reshape", "(", "-", "1", ",", "token_type_ids", ".", "shape", "[", "-", "1", "]", ")", "if", "token_type_ids", "is", "not", "None", "else", "None", "\n", "position_ids", "=", "position_ids", ".", "reshape", "(", "-", "1", ",", "position_ids", ".", "shape", "[", "-", "1", "]", ")", "if", "position_ids", "is", "not", "None", "else", "None", "\n", "\n", "# Model", "\n", "outputs", "=", "self", ".", "roberta", "(", "\n", "input_ids", ",", "\n", "attention_mask", ",", "\n", "token_type_ids", ",", "\n", "position_ids", ",", "\n", "deterministic", "=", "deterministic", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ",", "deterministic", "=", "deterministic", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "reshaped_logits", "=", "logits", ".", "reshape", "(", "-", "1", ",", "num_choices", ")", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "(", "reshaped_logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "\n", "", "return", "FlaxMultipleChoiceModelOutput", "(", "\n", "logits", "=", "reshaped_logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaForTokenClassificationModule.setup": [[891, 900], ["modeling_flax_roberta.FlaxRobertaModule", "flax.Dropout", "flax.Dense"], "methods", ["None"], ["def", "setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "roberta", "=", "FlaxRobertaModule", "(", "config", "=", "self", ".", "config", ",", "dtype", "=", "self", ".", "dtype", ",", "add_pooling_layer", "=", "False", ")", "\n", "classifier_dropout", "=", "(", "\n", "self", ".", "config", ".", "classifier_dropout", "\n", "if", "self", ".", "config", ".", "classifier_dropout", "is", "not", "None", "\n", "else", "self", ".", "config", ".", "hidden_dropout_prob", "\n", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "rate", "=", "classifier_dropout", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Dense", "(", "self", ".", "config", ".", "num_labels", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaForTokenClassificationModule.__call__": [[901, 935], ["modeling_flax_roberta.FlaxRobertaForTokenClassificationModule.roberta", "modeling_flax_roberta.FlaxRobertaForTokenClassificationModule.dropout", "modeling_flax_roberta.FlaxRobertaForTokenClassificationModule.classifier", "modeling_flax_outputs.FlaxTokenClassifierOutput"], "methods", ["None"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "input_ids", ",", "\n", "attention_mask", ",", "\n", "token_type_ids", ",", "\n", "position_ids", ",", "\n", "deterministic", ":", "bool", "=", "True", ",", "\n", "output_attentions", ":", "bool", "=", "False", ",", "\n", "output_hidden_states", ":", "bool", "=", "False", ",", "\n", "return_dict", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "# Model", "\n", "        ", "outputs", "=", "self", ".", "roberta", "(", "\n", "input_ids", ",", "\n", "attention_mask", ",", "\n", "token_type_ids", ",", "\n", "position_ids", ",", "\n", "deterministic", "=", "deterministic", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "hidden_states", "=", "outputs", "[", "0", "]", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ",", "deterministic", "=", "deterministic", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "hidden_states", ")", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "(", "logits", ",", ")", "+", "outputs", "[", "1", ":", "]", "\n", "\n", "", "return", "FlaxTokenClassifierOutput", "(", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaForQuestionAnsweringModule.setup": [[963, 966], ["modeling_flax_roberta.FlaxRobertaModule", "flax.Dense"], "methods", ["None"], ["def", "setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "roberta", "=", "FlaxRobertaModule", "(", "config", "=", "self", ".", "config", ",", "dtype", "=", "self", ".", "dtype", ",", "add_pooling_layer", "=", "False", ")", "\n", "self", ".", "qa_outputs", "=", "nn", ".", "Dense", "(", "self", ".", "config", ".", "num_labels", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.FlaxRobertaForQuestionAnsweringModule.__call__": [[967, 1005], ["modeling_flax_roberta.FlaxRobertaForQuestionAnsweringModule.roberta", "modeling_flax_roberta.FlaxRobertaForQuestionAnsweringModule.qa_outputs", "modeling_flax_roberta.FlaxRobertaForQuestionAnsweringModule.split", "start_logits.squeeze.squeeze.squeeze", "end_logits.squeeze.squeeze.squeeze", "modeling_flax_outputs.FlaxQuestionAnsweringModelOutput"], "methods", ["None"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "input_ids", ",", "\n", "attention_mask", ",", "\n", "token_type_ids", ",", "\n", "position_ids", ",", "\n", "deterministic", ":", "bool", "=", "True", ",", "\n", "output_attentions", ":", "bool", "=", "False", ",", "\n", "output_hidden_states", ":", "bool", "=", "False", ",", "\n", "return_dict", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "# Model", "\n", "        ", "outputs", "=", "self", ".", "roberta", "(", "\n", "input_ids", ",", "\n", "attention_mask", ",", "\n", "token_type_ids", ",", "\n", "position_ids", ",", "\n", "deterministic", "=", "deterministic", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "hidden_states", "=", "outputs", "[", "0", "]", "\n", "\n", "logits", "=", "self", ".", "qa_outputs", "(", "hidden_states", ")", "\n", "start_logits", ",", "end_logits", "=", "logits", ".", "split", "(", "self", ".", "config", ".", "num_labels", ",", "axis", "=", "-", "1", ")", "\n", "start_logits", "=", "start_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "(", "start_logits", ",", "end_logits", ")", "+", "outputs", "[", "1", ":", "]", "\n", "\n", "", "return", "FlaxQuestionAnsweringModelOutput", "(", "\n", "start_logits", "=", "start_logits", ",", "\n", "end_logits", "=", "end_logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.modeling_flax_roberta.create_position_ids_from_input_ids": [[49, 71], ["mask.reshape.reshape", "incremental_indices.reshape.reshape", "incremental_indices.reshape.astype", "jax.cumsum().astype", "jax.cumsum().astype", "jax.cumsum", "jax.cumsum"], "function", ["None"], ["def", "create_position_ids_from_input_ids", "(", "input_ids", ",", "padding_idx", ")", ":", "\n", "    ", "\"\"\"\n    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n    are ignored. This is modified from fairseq's `utils.make_positions`.\n\n    Args:\n        input_ids: jnp.ndarray\n        padding_idx: int\n\n    Returns: jnp.ndarray\n    \"\"\"", "\n", "# The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.", "\n", "mask", "=", "(", "input_ids", "!=", "padding_idx", ")", ".", "astype", "(", "\"i4\"", ")", "\n", "\n", "if", "mask", ".", "ndim", ">", "2", ":", "\n", "        ", "mask", "=", "mask", ".", "reshape", "(", "(", "-", "1", ",", "mask", ".", "shape", "[", "-", "1", "]", ")", ")", "\n", "incremental_indices", "=", "jnp", ".", "cumsum", "(", "mask", ",", "axis", "=", "1", ")", ".", "astype", "(", "\"i4\"", ")", "*", "mask", "\n", "incremental_indices", "=", "incremental_indices", ".", "reshape", "(", "input_ids", ".", "shape", ")", "\n", "", "else", ":", "\n", "        ", "incremental_indices", "=", "jnp", ".", "cumsum", "(", "mask", ",", "axis", "=", "1", ")", ".", "astype", "(", "\"i4\"", ")", "*", "mask", "\n", "\n", "", "return", "incremental_indices", ".", "astype", "(", "\"i4\"", ")", "+", "padding_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.tokenization_roberta.RobertaTokenizer.__init__": [[134, 172], ["transformers.GPT2Tokenizer.__init__", "isinstance", "transformers.tokenization_utils.AddedToken", "isinstance", "transformers.tokenization_utils.AddedToken", "isinstance", "transformers.tokenization_utils.AddedToken", "isinstance", "transformers.tokenization_utils.AddedToken", "isinstance", "transformers.tokenization_utils.AddedToken", "isinstance", "transformers.tokenization_utils.AddedToken", "isinstance", "transformers.tokenization_utils.AddedToken"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab_file", ",", "\n", "merges_file", ",", "\n", "errors", "=", "\"replace\"", ",", "\n", "bos_token", "=", "\"<s>\"", ",", "\n", "eos_token", "=", "\"</s>\"", ",", "\n", "sep_token", "=", "\"</s>\"", ",", "\n", "cls_token", "=", "\"<s>\"", ",", "\n", "unk_token", "=", "\"<unk>\"", ",", "\n", "pad_token", "=", "\"<pad>\"", ",", "\n", "mask_token", "=", "\"<mask>\"", ",", "\n", "add_prefix_space", "=", "False", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "bos_token", "=", "AddedToken", "(", "bos_token", ",", "lstrip", "=", "False", ",", "rstrip", "=", "False", ")", "if", "isinstance", "(", "bos_token", ",", "str", ")", "else", "bos_token", "\n", "eos_token", "=", "AddedToken", "(", "eos_token", ",", "lstrip", "=", "False", ",", "rstrip", "=", "False", ")", "if", "isinstance", "(", "eos_token", ",", "str", ")", "else", "eos_token", "\n", "sep_token", "=", "AddedToken", "(", "sep_token", ",", "lstrip", "=", "False", ",", "rstrip", "=", "False", ")", "if", "isinstance", "(", "sep_token", ",", "str", ")", "else", "sep_token", "\n", "cls_token", "=", "AddedToken", "(", "cls_token", ",", "lstrip", "=", "False", ",", "rstrip", "=", "False", ")", "if", "isinstance", "(", "cls_token", ",", "str", ")", "else", "cls_token", "\n", "unk_token", "=", "AddedToken", "(", "unk_token", ",", "lstrip", "=", "False", ",", "rstrip", "=", "False", ")", "if", "isinstance", "(", "unk_token", ",", "str", ")", "else", "unk_token", "\n", "pad_token", "=", "AddedToken", "(", "pad_token", ",", "lstrip", "=", "False", ",", "rstrip", "=", "False", ")", "if", "isinstance", "(", "pad_token", ",", "str", ")", "else", "pad_token", "\n", "\n", "# Mask token behave like a normal word, i.e. include the space before it", "\n", "mask_token", "=", "AddedToken", "(", "mask_token", ",", "lstrip", "=", "True", ",", "rstrip", "=", "False", ")", "if", "isinstance", "(", "mask_token", ",", "str", ")", "else", "mask_token", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "vocab_file", "=", "vocab_file", ",", "\n", "merges_file", "=", "merges_file", ",", "\n", "errors", "=", "errors", ",", "\n", "bos_token", "=", "bos_token", ",", "\n", "eos_token", "=", "eos_token", ",", "\n", "unk_token", "=", "unk_token", ",", "\n", "sep_token", "=", "sep_token", ",", "\n", "cls_token", "=", "cls_token", ",", "\n", "pad_token", "=", "pad_token", ",", "\n", "mask_token", "=", "mask_token", ",", "\n", "add_prefix_space", "=", "add_prefix_space", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.tokenization_roberta.RobertaTokenizer.build_inputs_with_special_tokens": [[174, 198], ["None"], "methods", ["None"], ["", "def", "build_inputs_with_special_tokens", "(", "\n", "self", ",", "token_ids_0", ":", "List", "[", "int", "]", ",", "token_ids_1", ":", "Optional", "[", "List", "[", "int", "]", "]", "=", "None", "\n", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n        adding special tokens. A RoBERTa sequence has the following format:\n\n        - single sequence: ``<s> X </s>``\n        - pair of sequences: ``<s> A </s></s> B </s>``\n\n        Args:\n            token_ids_0 (:obj:`List[int]`):\n                List of IDs to which the special tokens will be added.\n            token_ids_1 (:obj:`List[int]`, `optional`):\n                Optional second list of IDs for sequence pairs.\n\n        Returns:\n            :obj:`List[int]`: List of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n        \"\"\"", "\n", "if", "token_ids_1", "is", "None", ":", "\n", "            ", "return", "[", "self", ".", "cls_token_id", "]", "+", "token_ids_0", "+", "[", "self", ".", "sep_token_id", "]", "\n", "", "cls", "=", "[", "self", ".", "cls_token_id", "]", "\n", "sep", "=", "[", "self", ".", "sep_token_id", "]", "\n", "return", "cls", "+", "token_ids_0", "+", "sep", "+", "sep", "+", "token_ids_1", "+", "sep", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.tokenization_roberta.RobertaTokenizer.get_special_tokens_mask": [[199, 225], ["super().get_special_tokens_mask", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.tokenization_roberta.RobertaTokenizer.get_special_tokens_mask"], ["", "def", "get_special_tokens_mask", "(", "\n", "self", ",", "token_ids_0", ":", "List", "[", "int", "]", ",", "token_ids_1", ":", "Optional", "[", "List", "[", "int", "]", "]", "=", "None", ",", "already_has_special_tokens", ":", "bool", "=", "False", "\n", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n        special tokens using the tokenizer ``prepare_for_model`` method.\n\n        Args:\n            token_ids_0 (:obj:`List[int]`):\n                List of IDs.\n            token_ids_1 (:obj:`List[int]`, `optional`):\n                Optional second list of IDs for sequence pairs.\n            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n                Whether or not the token list is already formatted with special tokens for the model.\n\n        Returns:\n            :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n        \"\"\"", "\n", "if", "already_has_special_tokens", ":", "\n", "            ", "return", "super", "(", ")", ".", "get_special_tokens_mask", "(", "\n", "token_ids_0", "=", "token_ids_0", ",", "token_ids_1", "=", "token_ids_1", ",", "already_has_special_tokens", "=", "True", "\n", ")", "\n", "\n", "", "if", "token_ids_1", "is", "None", ":", "\n", "            ", "return", "[", "1", "]", "+", "(", "[", "0", "]", "*", "len", "(", "token_ids_0", ")", ")", "+", "[", "1", "]", "\n", "", "return", "[", "1", "]", "+", "(", "[", "0", "]", "*", "len", "(", "token_ids_0", ")", ")", "+", "[", "1", ",", "1", "]", "+", "(", "[", "0", "]", "*", "len", "(", "token_ids_1", ")", ")", "+", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.tokenization_roberta.RobertaTokenizer.create_token_type_ids_from_sequences": [[226, 248], ["len", "len"], "methods", ["None"], ["", "def", "create_token_type_ids_from_sequences", "(", "\n", "self", ",", "token_ids_0", ":", "List", "[", "int", "]", ",", "token_ids_1", ":", "Optional", "[", "List", "[", "int", "]", "]", "=", "None", "\n", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        Create a mask from the two sequences passed to be used in a sequence-pair classification task. RoBERTa does not\n        make use of token type ids, therefore a list of zeros is returned.\n\n        Args:\n            token_ids_0 (:obj:`List[int]`):\n                List of IDs.\n            token_ids_1 (:obj:`List[int]`, `optional`):\n                Optional second list of IDs for sequence pairs.\n\n        Returns:\n            :obj:`List[int]`: List of zeros.\n        \"\"\"", "\n", "sep", "=", "[", "self", ".", "sep_token_id", "]", "\n", "cls", "=", "[", "self", ".", "cls_token_id", "]", "\n", "\n", "if", "token_ids_1", "is", "None", ":", "\n", "            ", "return", "len", "(", "cls", "+", "token_ids_0", "+", "sep", ")", "*", "[", "0", "]", "\n", "", "return", "len", "(", "cls", "+", "token_ids_0", "+", "sep", "+", "sep", "+", "token_ids_1", "+", "sep", ")", "*", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.tokenization_roberta.RobertaTokenizer.prepare_for_tokenization": [[249, 254], ["kwargs.pop", "len", "text[].isspace"], "methods", ["None"], ["", "def", "prepare_for_tokenization", "(", "self", ",", "text", ",", "is_split_into_words", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "add_prefix_space", "=", "kwargs", ".", "pop", "(", "\"add_prefix_space\"", ",", "self", ".", "add_prefix_space", ")", "\n", "if", "(", "is_split_into_words", "or", "add_prefix_space", ")", "and", "(", "len", "(", "text", ")", ">", "0", "and", "not", "text", "[", "0", "]", ".", "isspace", "(", ")", ")", ":", "\n", "            ", "text", "=", "\" \"", "+", "text", "\n", "", "return", "(", "text", ",", "kwargs", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.configuration_roberta.RobertaConfig.__init__": [[65, 68], ["transformers.BertConfig.__init__"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["def", "__init__", "(", "self", ",", "pad_token_id", "=", "1", ",", "bos_token_id", "=", "0", ",", "eos_token_id", "=", "2", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Constructs RobertaConfig.\"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "pad_token_id", "=", "pad_token_id", ",", "bos_token_id", "=", "bos_token_id", ",", "eos_token_id", "=", "eos_token_id", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.configuration_roberta.RobertaOnnxConfig.inputs": [[71, 77], ["collections.OrderedDict"], "methods", ["None"], ["    ", "@", "property", "\n", "def", "inputs", "(", "self", ")", "->", "Mapping", "[", "str", ",", "Mapping", "[", "int", ",", "str", "]", "]", ":", "\n", "        ", "return", "OrderedDict", "(", "\n", "[", "\n", "(", "\"input_ids\"", ",", "{", "0", ":", "\"batch\"", ",", "1", ":", "\"sequence\"", "}", ")", ",", "\n", "(", "\"attention_mask\"", ",", "{", "0", ":", "\"batch\"", ",", "1", ":", "\"sequence\"", "}", ")", ",", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.configuration_roberta.RobertaOnnxConfig.outputs": [[80, 83], ["collections.OrderedDict"], "methods", ["None"], ["", "@", "property", "\n", "def", "outputs", "(", "self", ")", "->", "Mapping", "[", "str", ",", "Mapping", "[", "int", ",", "str", "]", "]", ":", "\n", "        ", "return", "OrderedDict", "(", "[", "(", "\"last_hidden_state\"", ",", "{", "0", ":", "\"batch\"", ",", "1", ":", "\"sequence\"", "}", ")", ",", "(", "\"pooler_output\"", ",", "{", "0", ":", "\"batch\"", "}", ")", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.tokenization_roberta_fast.RobertaTokenizerFast.__init__": [[144, 174], ["transformers.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast.__init__"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab_file", "=", "None", ",", "\n", "merges_file", "=", "None", ",", "\n", "tokenizer_file", "=", "None", ",", "\n", "errors", "=", "\"replace\"", ",", "\n", "bos_token", "=", "\"<s>\"", ",", "\n", "eos_token", "=", "\"</s>\"", ",", "\n", "sep_token", "=", "\"</s>\"", ",", "\n", "cls_token", "=", "\"<s>\"", ",", "\n", "unk_token", "=", "\"<unk>\"", ",", "\n", "pad_token", "=", "\"<pad>\"", ",", "\n", "mask_token", "=", "\"<mask>\"", ",", "\n", "add_prefix_space", "=", "False", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "vocab_file", ",", "\n", "merges_file", ",", "\n", "tokenizer_file", "=", "tokenizer_file", ",", "\n", "errors", "=", "errors", ",", "\n", "bos_token", "=", "bos_token", ",", "\n", "eos_token", "=", "eos_token", ",", "\n", "sep_token", "=", "sep_token", ",", "\n", "cls_token", "=", "cls_token", ",", "\n", "unk_token", "=", "unk_token", ",", "\n", "pad_token", "=", "pad_token", ",", "\n", "mask_token", "=", "mask_token", ",", "\n", "add_prefix_space", "=", "add_prefix_space", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.tokenization_roberta_fast.RobertaTokenizerFast.mask_token": [[190, 201], ["isinstance", "transformers.tokenization_utils_base.AddedToken"], "methods", ["None"], ["", "@", "mask_token", ".", "setter", "\n", "def", "mask_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\"\n        Overriding the default behavior of the mask token to have it eat the space before it.\n\n        This is needed to preserve backward compatibility with all the previously used models based on Roberta.\n        \"\"\"", "\n", "# Mask token behave like a normal word, i.e. include the space before it", "\n", "# So we set lstrip to True", "\n", "value", "=", "AddedToken", "(", "value", ",", "lstrip", "=", "True", ",", "rstrip", "=", "False", ")", "if", "isinstance", "(", "value", ",", "str", ")", "else", "value", "\n", "self", ".", "_mask_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.tokenization_roberta_fast.RobertaTokenizerFast.build_inputs_with_special_tokens": [[202, 208], ["None"], "methods", ["None"], ["", "def", "build_inputs_with_special_tokens", "(", "self", ",", "token_ids_0", ",", "token_ids_1", "=", "None", ")", ":", "\n", "        ", "output", "=", "[", "self", ".", "bos_token_id", "]", "+", "token_ids_0", "+", "[", "self", ".", "eos_token_id", "]", "\n", "if", "token_ids_1", "is", "None", ":", "\n", "            ", "return", "output", "\n", "\n", "", "return", "output", "+", "[", "self", ".", "eos_token_id", "]", "+", "token_ids_1", "+", "[", "self", ".", "eos_token_id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.roberta.tokenization_roberta_fast.RobertaTokenizerFast.create_token_type_ids_from_sequences": [[209, 231], ["len", "len"], "methods", ["None"], ["", "def", "create_token_type_ids_from_sequences", "(", "\n", "self", ",", "token_ids_0", ":", "List", "[", "int", "]", ",", "token_ids_1", ":", "Optional", "[", "List", "[", "int", "]", "]", "=", "None", "\n", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        Create a mask from the two sequences passed to be used in a sequence-pair classification task. RoBERTa does not\n        make use of token type ids, therefore a list of zeros is returned.\n\n        Args:\n            token_ids_0 (:obj:`List[int]`):\n                List of IDs.\n            token_ids_1 (:obj:`List[int]`, `optional`):\n                Optional second list of IDs for sequence pairs.\n\n        Returns:\n            :obj:`List[int]`: List of zeros.\n        \"\"\"", "\n", "sep", "=", "[", "self", ".", "sep_token_id", "]", "\n", "cls", "=", "[", "self", ".", "cls_token_id", "]", "\n", "\n", "if", "token_ids_1", "is", "None", ":", "\n", "            ", "return", "len", "(", "cls", "+", "token_ids_0", "+", "sep", ")", "*", "[", "0", "]", "\n", "", "return", "len", "(", "cls", "+", "token_ids_0", "+", "sep", "+", "sep", "+", "token_ids_1", "+", "sep", ")", "*", "[", "0", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.guanzhchen_petuning.training.trainer_base.BaseTrainer.__init__": [[13, 20], ["transformers.Trainer.__init__", "typing.OrderedDict"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "predict_dataset", "=", "None", ",", "test_key", "=", "\"accuracy\"", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "predict_dataset", "=", "predict_dataset", "\n", "self", ".", "test_key", "=", "test_key", "\n", "self", ".", "best_metrics", "=", "OrderedDict", "(", "{", "\n", "\"best_epoch\"", ":", "0", ",", "\n", "f\"best_eval_{self.test_key}\"", ":", "0", ",", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.training.trainer_base.BaseTrainer.log_best_metrics": [[22, 25], ["trainer_base.BaseTrainer.log_metrics", "trainer_base.BaseTrainer.save_metrics"], "methods", ["None"], ["", "def", "log_best_metrics", "(", "self", ")", ":", "\n", "        ", "self", ".", "log_metrics", "(", "\"best\"", ",", "self", ".", "best_metrics", ")", "\n", "self", ".", "save_metrics", "(", "\"best\"", ",", "self", ".", "best_metrics", ",", "combined", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.training.trainer_base.BaseTrainer._maybe_log_save_evaluate": [[28, 73], ["trainer_base.BaseTrainer._nested_gather().mean().item", "round", "trainer_base.BaseTrainer._get_learning_rate", "trainer_base.BaseTrainer.store_flos", "trainer_base.BaseTrainer.log", "trainer_base.BaseTrainer.evaluate", "trainer_base.BaseTrainer._report_to_hp_search", "logger.info", "trainer_base.BaseTrainer.best_metrics.items", "trainer_base.BaseTrainer.log", "trainer_base.BaseTrainer._save_checkpoint", "trainer_base.BaseTrainer.callback_handler.on_save", "logger.info", "trainer_base.BaseTrainer._nested_gather().mean", "isinstance", "trainer_base.BaseTrainer.predict_dataset.items", "trainer_base.BaseTrainer.predict", "trainer_base.BaseTrainer._nested_gather", "trainer_base.BaseTrainer.predict"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.record_evaluation.evaluate", "home.repos.pwc.inspect_result.guanzhchen_petuning.None.run.predict", "home.repos.pwc.inspect_result.guanzhchen_petuning.None.run.predict"], ["", "def", "_maybe_log_save_evaluate", "(", "self", ",", "tr_loss", ",", "model", ",", "trial", ",", "epoch", ",", "ignore_keys_for_eval", ")", ":", "\n", "        ", "if", "self", ".", "control", ".", "should_log", ":", "\n", "            ", "logs", ":", "Dict", "[", "str", ",", "float", "]", "=", "{", "}", "\n", "\n", "\n", "tr_loss_scalar", "=", "self", ".", "_nested_gather", "(", "tr_loss", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n", "# reset tr_loss to zero", "\n", "tr_loss", "-=", "tr_loss", "\n", "\n", "logs", "[", "\"loss\"", "]", "=", "round", "(", "tr_loss_scalar", "/", "(", "self", ".", "state", ".", "global_step", "-", "self", ".", "_globalstep_last_logged", ")", ",", "4", ")", "\n", "logs", "[", "\"learning_rate\"", "]", "=", "self", ".", "_get_learning_rate", "(", ")", "\n", "\n", "self", ".", "_total_loss_scalar", "+=", "tr_loss_scalar", "\n", "self", ".", "_globalstep_last_logged", "=", "self", ".", "state", ".", "global_step", "\n", "self", ".", "store_flos", "(", ")", "\n", "\n", "self", ".", "log", "(", "logs", ")", "\n", "\n", "", "eval_metrics", "=", "None", "\n", "if", "self", ".", "control", ".", "should_evaluate", ":", "\n", "            ", "eval_metrics", "=", "self", ".", "evaluate", "(", "ignore_keys", "=", "ignore_keys_for_eval", ")", "\n", "self", ".", "_report_to_hp_search", "(", "trial", ",", "epoch", ",", "eval_metrics", ")", "\n", "\n", "if", "eval_metrics", "[", "\"eval_\"", "+", "self", ".", "test_key", "]", ">", "self", ".", "best_metrics", "[", "\"best_eval_\"", "+", "self", ".", "test_key", "]", ":", "\n", "                ", "self", ".", "best_metrics", "[", "\"best_epoch\"", "]", "=", "epoch", "\n", "self", ".", "best_metrics", "[", "\"best_eval_\"", "+", "self", ".", "test_key", "]", "=", "eval_metrics", "[", "\"eval_\"", "+", "self", ".", "test_key", "]", "\n", "\n", "if", "self", ".", "predict_dataset", "is", "not", "None", ":", "\n", "                    ", "if", "isinstance", "(", "self", ".", "predict_dataset", ",", "dict", ")", ":", "\n", "                        ", "for", "dataset_name", ",", "dataset", "in", "self", ".", "predict_dataset", ".", "items", "(", ")", ":", "\n", "                            ", "_", ",", "_", ",", "test_metrics", "=", "self", ".", "predict", "(", "dataset", ",", "metric_key_prefix", "=", "\"test\"", ")", "\n", "self", ".", "best_metrics", "[", "f\"best_test_{dataset_name}_{self.test_key}\"", "]", "=", "test_metrics", "[", "\"test_\"", "+", "self", ".", "test_key", "]", "\n", "", "", "else", ":", "\n", "                        ", "_", ",", "_", ",", "test_metrics", "=", "self", ".", "predict", "(", "self", ".", "predict_dataset", ",", "metric_key_prefix", "=", "\"test\"", ")", "\n", "self", ".", "best_metrics", "[", "\"best_test_\"", "+", "self", ".", "test_key", "]", "=", "test_metrics", "[", "\"test_\"", "+", "self", ".", "test_key", "]", "\n", "\n", "", "", "", "logger", ".", "info", "(", "f\"***** Epoch {epoch}: Best results *****\"", ")", "\n", "for", "key", ",", "value", "in", "self", ".", "best_metrics", ".", "items", "(", ")", ":", "\n", "                ", "logger", ".", "info", "(", "f\"{key} = {value}\"", ")", "\n", "", "self", ".", "log", "(", "self", ".", "best_metrics", ")", "\n", "\n", "", "if", "self", ".", "control", ".", "should_save", ":", "\n", "            ", "self", ".", "_save_checkpoint", "(", "model", ",", "trial", ",", "metrics", "=", "eval_metrics", ")", "\n", "self", ".", "control", "=", "self", ".", "callback_handler", ".", "on_save", "(", "self", ".", "args", ",", "self", ".", "state", ",", "self", ".", "control", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.training.trainer_base.BaseAdapterTrainer.__init__": [[75, 82], ["transformers.AdapterTrainer.__init__", "typing.OrderedDict"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "predict_dataset", "=", "None", ",", "test_key", "=", "\"accuracy\"", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "predict_dataset", "=", "predict_dataset", "\n", "self", ".", "test_key", "=", "test_key", "\n", "self", ".", "best_metrics", "=", "OrderedDict", "(", "{", "\n", "\"best_epoch\"", ":", "0", ",", "\n", "f\"best_eval_{self.test_key}\"", ":", "0", ",", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.training.trainer_base.BaseAdapterTrainer.log_best_metrics": [[84, 87], ["trainer_base.BaseAdapterTrainer.log_metrics", "trainer_base.BaseAdapterTrainer.save_metrics"], "methods", ["None"], ["", "def", "log_best_metrics", "(", "self", ")", ":", "\n", "        ", "self", ".", "log_metrics", "(", "\"best\"", ",", "self", ".", "best_metrics", ")", "\n", "self", ".", "save_metrics", "(", "\"best\"", ",", "self", ".", "best_metrics", ",", "combined", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.training.trainer_base.BaseAdapterTrainer._maybe_log_save_evaluate": [[90, 135], ["trainer_base.BaseAdapterTrainer._nested_gather().mean().item", "round", "trainer_base.BaseAdapterTrainer._get_learning_rate", "trainer_base.BaseAdapterTrainer.store_flos", "trainer_base.BaseAdapterTrainer.log", "trainer_base.BaseAdapterTrainer.evaluate", "trainer_base.BaseAdapterTrainer._report_to_hp_search", "logger.info", "trainer_base.BaseAdapterTrainer.best_metrics.items", "trainer_base.BaseAdapterTrainer.log", "trainer_base.BaseAdapterTrainer._save_checkpoint", "trainer_base.BaseAdapterTrainer.callback_handler.on_save", "logger.info", "trainer_base.BaseAdapterTrainer._nested_gather().mean", "isinstance", "trainer_base.BaseAdapterTrainer.predict_dataset.items", "trainer_base.BaseAdapterTrainer.predict", "trainer_base.BaseAdapterTrainer._nested_gather", "trainer_base.BaseAdapterTrainer.predict"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.record_evaluation.evaluate", "home.repos.pwc.inspect_result.guanzhchen_petuning.None.run.predict", "home.repos.pwc.inspect_result.guanzhchen_petuning.None.run.predict"], ["", "def", "_maybe_log_save_evaluate", "(", "self", ",", "tr_loss", ",", "model", ",", "trial", ",", "epoch", ",", "ignore_keys_for_eval", ")", ":", "\n", "        ", "if", "self", ".", "control", ".", "should_log", ":", "\n", "            ", "logs", ":", "Dict", "[", "str", ",", "float", "]", "=", "{", "}", "\n", "\n", "\n", "tr_loss_scalar", "=", "self", ".", "_nested_gather", "(", "tr_loss", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n", "# reset tr_loss to zero", "\n", "tr_loss", "-=", "tr_loss", "\n", "\n", "logs", "[", "\"loss\"", "]", "=", "round", "(", "tr_loss_scalar", "/", "(", "self", ".", "state", ".", "global_step", "-", "self", ".", "_globalstep_last_logged", ")", ",", "4", ")", "\n", "logs", "[", "\"learning_rate\"", "]", "=", "self", ".", "_get_learning_rate", "(", ")", "\n", "\n", "self", ".", "_total_loss_scalar", "+=", "tr_loss_scalar", "\n", "self", ".", "_globalstep_last_logged", "=", "self", ".", "state", ".", "global_step", "\n", "self", ".", "store_flos", "(", ")", "\n", "\n", "self", ".", "log", "(", "logs", ")", "\n", "\n", "", "eval_metrics", "=", "None", "\n", "if", "self", ".", "control", ".", "should_evaluate", ":", "\n", "            ", "eval_metrics", "=", "self", ".", "evaluate", "(", "ignore_keys", "=", "ignore_keys_for_eval", ")", "\n", "self", ".", "_report_to_hp_search", "(", "trial", ",", "epoch", ",", "eval_metrics", ")", "\n", "\n", "if", "eval_metrics", "[", "\"eval_\"", "+", "self", ".", "test_key", "]", ">", "self", ".", "best_metrics", "[", "\"best_eval_\"", "+", "self", ".", "test_key", "]", ":", "\n", "                ", "self", ".", "best_metrics", "[", "\"best_epoch\"", "]", "=", "epoch", "\n", "self", ".", "best_metrics", "[", "\"best_eval_\"", "+", "self", ".", "test_key", "]", "=", "eval_metrics", "[", "\"eval_\"", "+", "self", ".", "test_key", "]", "\n", "\n", "if", "self", ".", "predict_dataset", "is", "not", "None", ":", "\n", "                    ", "if", "isinstance", "(", "self", ".", "predict_dataset", ",", "dict", ")", ":", "\n", "                        ", "for", "dataset_name", ",", "dataset", "in", "self", ".", "predict_dataset", ".", "items", "(", ")", ":", "\n", "                            ", "_", ",", "_", ",", "test_metrics", "=", "self", ".", "predict", "(", "dataset", ",", "metric_key_prefix", "=", "\"test\"", ")", "\n", "self", ".", "best_metrics", "[", "f\"best_test_{dataset_name}_{self.test_key}\"", "]", "=", "test_metrics", "[", "\"test_\"", "+", "self", ".", "test_key", "]", "\n", "", "", "else", ":", "\n", "                        ", "_", ",", "_", ",", "test_metrics", "=", "self", ".", "predict", "(", "self", ".", "predict_dataset", ",", "metric_key_prefix", "=", "\"test\"", ")", "\n", "self", ".", "best_metrics", "[", "\"best_test_\"", "+", "self", ".", "test_key", "]", "=", "test_metrics", "[", "\"test_\"", "+", "self", ".", "test_key", "]", "\n", "\n", "", "", "", "logger", ".", "info", "(", "f\"***** Epoch {epoch}: Best results *****\"", ")", "\n", "for", "key", ",", "value", "in", "self", ".", "best_metrics", ".", "items", "(", ")", ":", "\n", "                ", "logger", ".", "info", "(", "f\"{key} = {value}\"", ")", "\n", "", "self", ".", "log", "(", "self", ".", "best_metrics", ")", "\n", "\n", "", "if", "self", ".", "control", ".", "should_save", ":", "\n", "            ", "self", ".", "_save_checkpoint", "(", "model", ",", "trial", ",", "metrics", "=", "eval_metrics", ")", "\n", "self", ".", "control", "=", "self", ".", "callback_handler", ".", "on_save", "(", "self", ".", "args", ",", "self", ".", "state", ",", "self", ".", "control", ")", "", "", "", "", ""]], "home.repos.pwc.inspect_result.guanzhchen_petuning.glue.dataset.GlueDataset.__init__": [[31, 104], ["super().__init__", "datasets.load.load_dataset", "min", "raw_datasets.map.map.map", "datasets.load.load_metric", "len", "logger.warning", "raw_datasets[].train_test_split", "dataset.GlueDataset.train_dataset.select", "dataset.GlueDataset.predict_dataset.select", "transformers.DataCollatorWithPadding", "enumerate", "dataset.GlueDataset.label2id.items", "range", "range"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "tokenizer", ":", "AutoTokenizer", ",", "data_args", ",", "training_args", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "raw_datasets", "=", "load_dataset", "(", "\"glue\"", ",", "data_args", ".", "dataset_name", ")", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "data_args", "=", "data_args", "\n", "#labels", "\n", "self", ".", "is_regression", "=", "data_args", ".", "dataset_name", "==", "\"stsb\"", "\n", "if", "not", "self", ".", "is_regression", ":", "\n", "            ", "self", ".", "label_list", "=", "raw_datasets", "[", "\"train\"", "]", ".", "features", "[", "\"label\"", "]", ".", "names", "\n", "self", ".", "num_labels", "=", "len", "(", "self", ".", "label_list", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_labels", "=", "1", "\n", "\n", "# Preprocessing the raw_datasets", "\n", "", "self", ".", "sentence1_key", ",", "self", ".", "sentence2_key", "=", "task_to_keys", "[", "data_args", ".", "dataset_name", "]", "\n", "\n", "# Padding strategy", "\n", "if", "data_args", ".", "pad_to_max_length", ":", "\n", "            ", "self", ".", "padding", "=", "\"max_length\"", "\n", "", "else", ":", "\n", "# We will pad later, dynamically at batch creation, to the max sequence length in each batch", "\n", "            ", "self", ".", "padding", "=", "False", "\n", "\n", "# Some models have set the order of the labels to use, so let's make sure we do use it.", "\n", "", "if", "not", "self", ".", "is_regression", ":", "\n", "            ", "self", ".", "label2id", "=", "{", "l", ":", "i", "for", "i", ",", "l", "in", "enumerate", "(", "self", ".", "label_list", ")", "}", "\n", "self", ".", "id2label", "=", "{", "id", ":", "label", "for", "label", ",", "id", "in", "self", ".", "label2id", ".", "items", "(", ")", "}", "\n", "\n", "", "if", "data_args", ".", "max_seq_length", ">", "tokenizer", ".", "model_max_length", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"", "\n", "f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"", "\n", ")", "\n", "", "self", ".", "max_seq_length", "=", "min", "(", "data_args", ".", "max_seq_length", ",", "tokenizer", ".", "model_max_length", ")", "\n", "\n", "raw_datasets", "=", "raw_datasets", ".", "map", "(", "\n", "self", ".", "preprocess_function", ",", "\n", "batched", "=", "True", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", "desc", "=", "\"Running tokenizer on dataset\"", ",", "\n", ")", "\n", "if", "training_args", ".", "do_train", ":", "\n", "            ", "train_dataset", "=", "raw_datasets", "[", "\"train\"", "]", ".", "train_test_split", "(", "test_size", "=", "0.1", ",", "shuffle", "=", "False", ")", "\n", "self", ".", "train_dataset", ",", "self", ".", "eval_dataset", "=", "train_dataset", "=", "train_dataset", "[", "'train'", "]", ",", "train_dataset", "[", "'test'", "]", "\n", "if", "data_args", ".", "max_train_samples", "is", "not", "None", ":", "\n", "                ", "self", ".", "train_dataset", "=", "self", ".", "train_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_train_samples", ")", ")", "\n", "\n", "", "", "if", "training_args", ".", "do_predict", ":", "\n", "            ", "self", ".", "predict_dataset", "=", "raw_datasets", "[", "\"validation_matched\"", "if", "data_args", ".", "dataset_name", "==", "\"mnli\"", "else", "\"validation\"", "]", "\n", "if", "data_args", ".", "max_predict_samples", "is", "not", "None", ":", "\n", "                ", "self", ".", "predict_dataset", "=", "self", ".", "predict_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_predict_samples", ")", ")", "\n", "\n", "# if training_args.do_train:", "\n", "#     self.train_dataset = raw_datasets[\"train\"]", "\n", "#     if data_args.max_train_samples is not None:", "\n", "#         self.train_dataset = self.train_dataset.select(range(data_args.max_train_samples))", "\n", "\n", "# if training_args.do_eval:", "\n", "#     self.eval_dataset = raw_datasets[\"validation_matched\" if data_args.dataset_name == \"mnli\" else \"validation\"]", "\n", "#     if data_args.max_eval_samples is not None:", "\n", "#         self.eval_dataset = self.eval_dataset.select(range(data_args.max_eval_samples))", "\n", "\n", "# if training_args.do_predict or data_args.dataset_name is not None or data_args.test_file is not None:", "\n", "#     self.predict_dataset = raw_datasets[\"test_matched\" if data_args.dataset_name == \"mnli\" else \"test\"]", "\n", "#     if data_args.max_predict_samples is not None:", "\n", "#         self.predict_dataset = self.predict_dataset.select(range(data_args.max_predict_samples))", "\n", "\n", "", "", "self", ".", "metric", "=", "load_metric", "(", "\"tasks/glue/glue.py\"", ",", "data_args", ".", "dataset_name", ")", "\n", "\n", "if", "data_args", ".", "pad_to_max_length", ":", "\n", "            ", "self", ".", "data_collator", "=", "default_data_collator", "\n", "", "elif", "training_args", ".", "fp16", ":", "\n", "            ", "self", ".", "data_collator", "=", "DataCollatorWithPadding", "(", "tokenizer", ",", "pad_to_multiple_of", "=", "8", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.glue.dataset.GlueDataset.preprocess_function": [[106, 114], ["dataset.GlueDataset.tokenizer"], "methods", ["None"], ["", "", "def", "preprocess_function", "(", "self", ",", "examples", ")", ":", "\n", "# Tokenize the texts", "\n", "        ", "args", "=", "(", "\n", "(", "examples", "[", "self", ".", "sentence1_key", "]", ",", ")", "if", "self", ".", "sentence2_key", "is", "None", "else", "(", "examples", "[", "self", ".", "sentence1_key", "]", ",", "examples", "[", "self", ".", "sentence2_key", "]", ")", "\n", ")", "\n", "result", "=", "self", ".", "tokenizer", "(", "*", "args", ",", "padding", "=", "self", ".", "padding", ",", "max_length", "=", "self", ".", "max_seq_length", ",", "truncation", "=", "True", ")", "\n", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.glue.dataset.GlueDataset.compute_metrics": [[115, 127], ["isinstance", "numpy.squeeze", "numpy.argmax", "dataset.GlueDataset.metric.compute", "len", "numpy.mean().item", "numpy.mean", "list", "dataset.GlueDataset.values"], "methods", ["None"], ["", "def", "compute_metrics", "(", "self", ",", "p", ":", "EvalPrediction", ")", ":", "\n", "        ", "preds", "=", "p", ".", "predictions", "[", "0", "]", "if", "isinstance", "(", "p", ".", "predictions", ",", "tuple", ")", "else", "p", ".", "predictions", "\n", "preds", "=", "np", ".", "squeeze", "(", "preds", ")", "if", "self", ".", "is_regression", "else", "np", ".", "argmax", "(", "preds", ",", "axis", "=", "1", ")", "\n", "if", "self", ".", "data_args", ".", "dataset_name", "is", "not", "None", ":", "\n", "            ", "result", "=", "self", ".", "metric", ".", "compute", "(", "predictions", "=", "preds", ",", "references", "=", "p", ".", "label_ids", ")", "\n", "if", "len", "(", "result", ")", ">", "1", ":", "\n", "                ", "result", "[", "\"combined_score\"", "]", "=", "np", ".", "mean", "(", "list", "(", "result", ".", "values", "(", ")", ")", ")", ".", "item", "(", ")", "\n", "", "return", "result", "\n", "", "elif", "self", ".", "is_regression", ":", "\n", "            ", "return", "{", "\"mse\"", ":", "(", "(", "preds", "-", "p", ".", "label_ids", ")", "**", "2", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "}", "\n", "", "else", ":", "\n", "            ", "return", "{", "\"accuracy\"", ":", "(", "preds", "==", "p", ".", "label_ids", ")", ".", "astype", "(", "np", ".", "float32", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.glue.glue.Glue._info": [[107, 140], ["datasets.MetricInfo", "KeyError", "datasets.Features", "datasets.Value", "datasets.Value"], "methods", ["None"], ["    ", "def", "_info", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "config_name", "not", "in", "[", "\n", "\"sst2\"", ",", "\n", "\"mnli\"", ",", "\n", "\"mnli_mismatched\"", ",", "\n", "\"mnli_matched\"", ",", "\n", "\"cola\"", ",", "\n", "\"stsb\"", ",", "\n", "\"mrpc\"", ",", "\n", "\"qqp\"", ",", "\n", "\"qnli\"", ",", "\n", "\"rte\"", ",", "\n", "\"wnli\"", ",", "\n", "\"hans\"", ",", "\n", "]", ":", "\n", "            ", "raise", "KeyError", "(", "\n", "\"You should supply a configuration name selected in \"", "\n", "'[\"sst2\", \"mnli\", \"mnli_mismatched\", \"mnli_matched\", '", "\n", "'\"cola\", \"stsb\", \"mrpc\", \"qqp\", \"qnli\", \"rte\", \"wnli\", \"hans\"]'", "\n", ")", "\n", "", "return", "datasets", ".", "MetricInfo", "(", "\n", "description", "=", "_DESCRIPTION", ",", "\n", "citation", "=", "_CITATION", ",", "\n", "inputs_description", "=", "_KWARGS_DESCRIPTION", ",", "\n", "features", "=", "datasets", ".", "Features", "(", "\n", "{", "\n", "\"predictions\"", ":", "datasets", ".", "Value", "(", "\"int64\"", "if", "self", ".", "config_name", "!=", "\"stsb\"", "else", "\"float32\"", ")", ",", "\n", "\"references\"", ":", "datasets", ".", "Value", "(", "\"int64\"", "if", "self", ".", "config_name", "!=", "\"stsb\"", "else", "\"float32\"", ")", ",", "\n", "}", "\n", ")", ",", "\n", "codebase_urls", "=", "[", "]", ",", "\n", "reference_urls", "=", "[", "]", ",", "\n", "format", "=", "\"numpy\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.glue.glue.Glue._compute": [[142, 154], ["sklearn.metrics.matthews_corrcoef", "glue.pearson_and_spearman", "glue.acc_and_f1", "KeyError", "glue.simple_accuracy"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.glue.glue.pearson_and_spearman", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue_metric.acc_and_f1", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue_metric.simple_accuracy"], ["", "def", "_compute", "(", "self", ",", "predictions", ",", "references", ")", ":", "\n", "        ", "if", "self", ".", "config_name", "==", "\"cola\"", ":", "\n", "            ", "return", "{", "\"matthews_correlation\"", ":", "matthews_corrcoef", "(", "references", ",", "predictions", ")", "}", "\n", "", "elif", "self", ".", "config_name", "==", "\"stsb\"", ":", "\n", "            ", "return", "pearson_and_spearman", "(", "predictions", ",", "references", ")", "\n", "", "elif", "self", ".", "config_name", "in", "[", "\"mrpc\"", ",", "\"qqp\"", "]", ":", "\n", "            ", "return", "acc_and_f1", "(", "predictions", ",", "references", ")", "\n", "", "elif", "self", ".", "config_name", "in", "[", "\"sst2\"", ",", "\"mnli\"", ",", "\"mnli_mismatched\"", ",", "\"mnli_matched\"", ",", "\"qnli\"", ",", "\"rte\"", ",", "\"wnli\"", ",", "\"hans\"", "]", ":", "\n", "            ", "return", "{", "\"accuracy\"", ":", "simple_accuracy", "(", "predictions", ",", "references", ")", "}", "\n", "", "else", ":", "\n", "            ", "raise", "KeyError", "(", "\n", "\"You should supply a configuration name selected in \"", "\n", "'[\"sst2\", \"mnli\", \"mnli_mismatched\", \"mnli_matched\", '", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.glue.glue.simple_accuracy": [[83, 85], ["float"], "function", ["None"], ["def", "simple_accuracy", "(", "preds", ",", "labels", ")", ":", "\n", "    ", "return", "float", "(", "(", "preds", "==", "labels", ")", ".", "mean", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.glue.glue.acc_and_f1": [[87, 93], ["glue.simple_accuracy", "float", "sklearn.metrics.f1_score"], "function", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue_metric.simple_accuracy", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.f1_score"], ["", "def", "acc_and_f1", "(", "preds", ",", "labels", ")", ":", "\n", "    ", "acc", "=", "simple_accuracy", "(", "preds", ",", "labels", ")", "\n", "f1", "=", "float", "(", "f1_score", "(", "y_true", "=", "labels", ",", "y_pred", "=", "preds", ")", ")", "\n", "return", "{", "\n", "\"accuracy\"", ":", "acc", ",", "\n", "\"f1\"", ":", "f1", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.glue.glue.pearson_and_spearman": [[96, 102], ["float", "float", "scipy.stats.pearsonr", "scipy.stats.spearmanr"], "function", ["None"], ["", "def", "pearson_and_spearman", "(", "preds", ",", "labels", ")", ":", "\n", "    ", "pearson_corr", "=", "float", "(", "pearsonr", "(", "preds", ",", "labels", ")", "[", "0", "]", ")", "\n", "spearman_corr", "=", "float", "(", "spearmanr", "(", "preds", ",", "labels", ")", "[", "0", "]", ")", "\n", "return", "{", "\n", "\"pearson\"", ":", "pearson_corr", ",", "\n", "\"spearmanr\"", ":", "spearman_corr", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.glue.get_trainer.get_trainer": [[19, 103], ["transformers.AutoTokenizer.from_pretrained", "tasks.glue.dataset.GlueDataset", "model.utils.get_model", "list", "logger.info", "trainer_cls", "transformers.AutoConfig.from_pretrained", "transformers.AutoConfig.from_pretrained", "logger.info", "model.utils.get_model.train_adapter", "model.utils.get_model.set_active_adapters", "model.utils.get_model.named_parameters", "model.utils.get_model.named_parameters", "transformers.AdapterConfig.load", "model.utils.get_model.add_adapter", "ValueError", "logger.info", "name.startswith", "transformers.EarlyStoppingCallback", "name.lower"], "function", ["home.repos.pwc.inspect_result.guanzhchen_petuning.model.utils.get_model"], ["def", "get_trainer", "(", "args", ")", ":", "\n", "    ", "model_args", ",", "data_args", ",", "training_args", ",", "_", ",", "adapter_args", "=", "args", "\n", "\n", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "use_fast", "=", "model_args", ".", "use_fast_tokenizer", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", ")", "\n", "dataset", "=", "GlueDataset", "(", "tokenizer", ",", "data_args", ",", "training_args", ")", "\n", "\n", "if", "not", "dataset", ".", "is_regression", ":", "\n", "        ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "num_labels", "=", "dataset", ".", "num_labels", ",", "\n", "label2id", "=", "dataset", ".", "label2id", ",", "\n", "id2label", "=", "dataset", ".", "id2label", ",", "\n", "finetuning_task", "=", "data_args", ".", "dataset_name", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "num_labels", "=", "dataset", ".", "num_labels", ",", "\n", "finetuning_task", "=", "data_args", ".", "dataset_name", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", ")", "\n", "", "config", ".", "lora", "=", "False", "\n", "model", "=", "get_model", "(", "model_args", ",", "TaskType", ".", "SEQUENCE_CLASSIFICATION", ",", "config", ")", "\n", "\n", "if", "adapter_args", ".", "train_adapter", ":", "\n", "        ", "logger", ".", "info", "(", "f\"Reduction Factor: {adapter_args.adapter_reduction_factor}\"", ")", "\n", "task_name", "=", "data_args", ".", "task_name", "or", "\"superglue\"", "\n", "# check if adapter already exists, otherwise add it", "\n", "if", "task_name", "not", "in", "model", ".", "config", ".", "adapters", ":", "\n", "# resolve the adapter config", "\n", "            ", "adapter_config", "=", "AdapterConfig", ".", "load", "(", "\n", "adapter_args", ".", "adapter_config", ",", "\n", "non_linearity", "=", "adapter_args", ".", "adapter_non_linearity", ",", "\n", "reduction_factor", "=", "adapter_args", ".", "adapter_reduction_factor", ",", "\n", ")", "\n", "# load a pre-trained from Hub if specified", "\n", "# if adapter_args.load_adapter:", "\n", "#     model.load_adapter(", "\n", "#         adapter_args.load_adapter,", "\n", "#         config=adapter_config,", "\n", "#         load_as=task_name,", "\n", "#     )", "\n", "# # otherwise, add a fresh adapter", "\n", "# else:", "\n", "model", ".", "add_adapter", "(", "task_name", ",", "config", "=", "adapter_config", ")", "\n", "# Freeze all model weights except of those of this adapter", "\n", "", "model", ".", "train_adapter", "(", "[", "task_name", "]", ")", "\n", "# Set the adapters to be used in every forward pass", "\n", "model", ".", "set_active_adapters", "(", "task_name", ")", "\n", "", "else", ":", "\n", "        ", "if", "adapter_args", ".", "load_adapter", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Adapters can only be loaded in adapters training mode.\"", "\n", "\"Use --train_adapter to enable adapter training\"", "\n", ")", "\n", "", "", "if", "model_args", ".", "bitfit", ":", "\n", "        ", "for", "name", ",", "param", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "name", ".", "startswith", "(", "'roberta'", ")", "and", "\"bias\"", "not", "in", "name", ".", "lower", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "", "", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "logger", ".", "info", "(", "\"Trainable parameters:\"", ")", "\n", "for", "n", ",", "p", "in", "param_optimizer", ":", "\n", "        ", "if", "p", ".", "requires_grad", ":", "\n", "            ", "logger", ".", "info", "(", "f\"{n}\"", ")", "\n", "# print(n)", "\n", "\n", "", "", "trainer_cls", "=", "AdapterTrainer", "if", "adapter_args", ".", "train_adapter", "else", "Trainer", "\n", "trainer", "=", "trainer_cls", "(", "\n", "model", "=", "model", ",", "\n", "args", "=", "training_args", ",", "\n", "train_dataset", "=", "dataset", ".", "train_dataset", "if", "training_args", ".", "do_train", "else", "None", ",", "\n", "eval_dataset", "=", "dataset", ".", "eval_dataset", "if", "training_args", ".", "do_eval", "else", "None", ",", "\n", "compute_metrics", "=", "dataset", ".", "compute_metrics", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "data_collator", "=", "dataset", ".", "data_collator", ",", "\n", "callbacks", "=", "[", "EarlyStoppingCallback", "(", "early_stopping_patience", "=", "10", ")", "]", "\n", ")", "\n", "\n", "return", "trainer", ",", "dataset", ".", "predict_dataset", "", "", ""]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue_metric.SuperGlue._info": [[149, 175], ["datasets.MetricInfo", "KeyError", "datasets.Features", "super_glue_metric.SuperGlue._get_feature_types"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue_metric.SuperGlue._get_feature_types"], ["    ", "def", "_info", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "config_name", "not", "in", "[", "\n", "\"boolq\"", ",", "\n", "\"cb\"", ",", "\n", "\"copa\"", ",", "\n", "\"multirc\"", ",", "\n", "\"record\"", ",", "\n", "\"rte\"", ",", "\n", "\"wic\"", ",", "\n", "\"wsc\"", ",", "\n", "\"wsc.fixed\"", ",", "\n", "\"axb\"", ",", "\n", "\"axg\"", ",", "\n", "]", ":", "\n", "            ", "raise", "KeyError", "(", "\n", "\"You should supply a configuration name selected in \"", "\n", "'[\"boolq\", \"cb\", \"copa\", \"multirc\", \"record\", \"rte\", \"wic\", \"wsc\", \"wsc.fixed\", \"axb\", \"axg\",]'", "\n", ")", "\n", "", "return", "datasets", ".", "MetricInfo", "(", "\n", "description", "=", "_DESCRIPTION", ",", "\n", "citation", "=", "_CITATION", ",", "\n", "inputs_description", "=", "_KWARGS_DESCRIPTION", ",", "\n", "features", "=", "datasets", ".", "Features", "(", "self", ".", "_get_feature_types", "(", ")", ")", ",", "\n", "codebase_urls", "=", "[", "]", ",", "\n", "reference_urls", "=", "[", "]", ",", "\n", "format", "=", "\"numpy\"", "if", "not", "self", ".", "config_name", "==", "\"record\"", "and", "not", "self", ".", "config_name", "==", "\"multirc\"", "else", "None", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue_metric.SuperGlue._get_feature_types": [[177, 211], ["datasets.Value", "datasets.Sequence", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value"], "methods", ["None"], ["", "def", "_get_feature_types", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "config_name", "==", "\"record\"", ":", "\n", "            ", "return", "{", "\n", "\"predictions\"", ":", "{", "\n", "\"idx\"", ":", "{", "\n", "\"passage\"", ":", "datasets", ".", "Value", "(", "\"int64\"", ")", ",", "\n", "\"query\"", ":", "datasets", ".", "Value", "(", "\"int64\"", ")", ",", "\n", "}", ",", "\n", "\"prediction_text\"", ":", "datasets", ".", "Value", "(", "\"string\"", ")", ",", "\n", "}", ",", "\n", "\"references\"", ":", "{", "\n", "\"idx\"", ":", "{", "\n", "\"passage\"", ":", "datasets", ".", "Value", "(", "\"int64\"", ")", ",", "\n", "\"query\"", ":", "datasets", ".", "Value", "(", "\"int64\"", ")", ",", "\n", "}", ",", "\n", "\"answers\"", ":", "datasets", ".", "Sequence", "(", "datasets", ".", "Value", "(", "\"string\"", ")", ")", ",", "\n", "}", ",", "\n", "}", "\n", "", "elif", "self", ".", "config_name", "==", "\"multirc\"", ":", "\n", "            ", "return", "{", "\n", "\"predictions\"", ":", "{", "\n", "\"idx\"", ":", "{", "\n", "\"answer\"", ":", "datasets", ".", "Value", "(", "\"int64\"", ")", ",", "\n", "\"paragraph\"", ":", "datasets", ".", "Value", "(", "\"int64\"", ")", ",", "\n", "\"question\"", ":", "datasets", ".", "Value", "(", "\"int64\"", ")", ",", "\n", "}", ",", "\n", "\"prediction\"", ":", "datasets", ".", "Value", "(", "\"int64\"", ")", ",", "\n", "}", ",", "\n", "\"references\"", ":", "datasets", ".", "Value", "(", "\"int64\"", ")", ",", "\n", "}", "\n", "", "else", ":", "\n", "            ", "return", "{", "\n", "\"predictions\"", ":", "datasets", ".", "Value", "(", "\"int64\"", ")", ",", "\n", "\"references\"", ":", "datasets", ".", "Value", "(", "\"int64\"", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue_metric.SuperGlue._compute": [[213, 236], ["sklearn.metrics.matthews_corrcoef", "super_glue_metric.acc_and_f1", "record_evaluation.evaluate", "super_glue_metric.evaluate_multirc", "KeyError", "super_glue_metric.simple_accuracy"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue_metric.acc_and_f1", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.record_evaluation.evaluate", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue_metric.evaluate_multirc", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue_metric.simple_accuracy"], ["", "", "def", "_compute", "(", "self", ",", "predictions", ",", "references", ")", ":", "\n", "        ", "if", "self", ".", "config_name", "==", "\"axb\"", ":", "\n", "            ", "return", "{", "\"matthews_correlation\"", ":", "matthews_corrcoef", "(", "references", ",", "predictions", ")", "}", "\n", "", "elif", "self", ".", "config_name", "==", "\"cb\"", ":", "\n", "            ", "return", "acc_and_f1", "(", "predictions", ",", "references", ",", "f1_avg", "=", "\"macro\"", ")", "\n", "", "elif", "self", ".", "config_name", "==", "\"record\"", ":", "\n", "            ", "dataset", "=", "[", "\n", "{", "\n", "\"qas\"", ":", "[", "\n", "{", "\"id\"", ":", "ref", "[", "\"idx\"", "]", "[", "\"query\"", "]", ",", "\"answers\"", ":", "[", "{", "\"text\"", ":", "ans", "}", "for", "ans", "in", "ref", "[", "\"answers\"", "]", "]", "}", "\n", "for", "ref", "in", "references", "\n", "]", "\n", "}", "\n", "]", "\n", "predictions", "=", "{", "pred", "[", "\"idx\"", "]", "[", "\"query\"", "]", ":", "pred", "[", "\"prediction_text\"", "]", "for", "pred", "in", "predictions", "}", "\n", "return", "evaluate_record", "(", "dataset", ",", "predictions", ")", "[", "0", "]", "\n", "", "elif", "self", ".", "config_name", "==", "\"multirc\"", ":", "\n", "            ", "return", "evaluate_multirc", "(", "predictions", ",", "references", ")", "\n", "", "elif", "self", ".", "config_name", "in", "[", "\"copa\"", ",", "\"rte\"", ",", "\"wic\"", ",", "\"wsc\"", ",", "\"wsc.fixed\"", ",", "\"boolq\"", ",", "\"axg\"", "]", ":", "\n", "            ", "return", "{", "\"accuracy\"", ":", "simple_accuracy", "(", "predictions", ",", "references", ")", "}", "\n", "", "else", ":", "\n", "            ", "raise", "KeyError", "(", "\n", "\"You should supply a configuration name selected in \"", "\n", "'[\"boolq\", \"cb\", \"copa\", \"multirc\", \"record\", \"rte\", \"wic\", \"wsc\", \"wsc.fixed\", \"axb\", \"axg\",]'", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue_metric.simple_accuracy": [[109, 111], ["float"], "function", ["None"], ["def", "simple_accuracy", "(", "preds", ",", "labels", ")", ":", "\n", "    ", "return", "float", "(", "(", "preds", "==", "labels", ")", ".", "mean", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue_metric.acc_and_f1": [[113, 119], ["super_glue_metric.simple_accuracy", "float", "sklearn.metrics.f1_score"], "function", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue_metric.simple_accuracy", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.f1_score"], ["", "def", "acc_and_f1", "(", "preds", ",", "labels", ",", "f1_avg", "=", "\"binary\"", ")", ":", "\n", "    ", "acc", "=", "simple_accuracy", "(", "preds", ",", "labels", ")", "\n", "f1", "=", "float", "(", "f1_score", "(", "y_true", "=", "labels", ",", "y_pred", "=", "preds", ",", "average", "=", "f1_avg", ")", ")", "\n", "return", "{", "\n", "\"accuracy\"", ":", "acc", ",", "\n", "\"f1\"", ":", "f1", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue_metric.evaluate_multirc": [[122, 145], ["zip", "question_map.items", "float", "float", "zip", "sklearn.metrics.f1_score", "f1s.append", "int", "ems.append", "sum", "len", "sklearn.metrics.f1_score", "question_map[].append", "sum", "len", "sum", "len"], "function", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.f1_score", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.f1_score"], ["", "def", "evaluate_multirc", "(", "ids_preds", ",", "labels", ")", ":", "\n", "    ", "\"\"\"\n    Computes F1 score and Exact Match for MultiRC predictions.\n    \"\"\"", "\n", "question_map", "=", "{", "}", "\n", "for", "id_pred", ",", "label", "in", "zip", "(", "ids_preds", ",", "labels", ")", ":", "\n", "        ", "question_id", "=", "\"{}-{}\"", ".", "format", "(", "id_pred", "[", "\"idx\"", "]", "[", "\"paragraph\"", "]", ",", "id_pred", "[", "\"idx\"", "]", "[", "\"question\"", "]", ")", "\n", "pred", "=", "id_pred", "[", "\"prediction\"", "]", "\n", "if", "question_id", "in", "question_map", ":", "\n", "            ", "question_map", "[", "question_id", "]", ".", "append", "(", "(", "pred", ",", "label", ")", ")", "\n", "", "else", ":", "\n", "            ", "question_map", "[", "question_id", "]", "=", "[", "(", "pred", ",", "label", ")", "]", "\n", "", "", "f1s", ",", "ems", "=", "[", "]", ",", "[", "]", "\n", "for", "question", ",", "preds_labels", "in", "question_map", ".", "items", "(", ")", ":", "\n", "        ", "question_preds", ",", "question_labels", "=", "zip", "(", "*", "preds_labels", ")", "\n", "f1", "=", "f1_score", "(", "y_true", "=", "question_labels", ",", "y_pred", "=", "question_preds", ",", "average", "=", "\"macro\"", ")", "\n", "f1s", ".", "append", "(", "f1", ")", "\n", "em", "=", "int", "(", "sum", "(", "[", "p", "==", "l", "for", "p", ",", "l", "in", "preds_labels", "]", ")", "==", "len", "(", "preds_labels", ")", ")", "\n", "ems", ".", "append", "(", "em", ")", "\n", "", "f1_m", "=", "float", "(", "(", "sum", "(", "f1s", ")", "/", "len", "(", "f1s", ")", ")", ")", "\n", "em", "=", "sum", "(", "ems", ")", "/", "len", "(", "ems", ")", "\n", "f1_a", "=", "float", "(", "f1_score", "(", "y_true", "=", "labels", ",", "y_pred", "=", "[", "id_pred", "[", "\"prediction\"", "]", "for", "id_pred", "in", "ids_preds", "]", ")", ")", "\n", "return", "{", "\"exact_match\"", ":", "em", ",", "\"f1_m\"", ":", "f1_m", ",", "\"f1_a\"", ":", "f1_a", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.dataset.SuperGlueDataset.__init__": [[27, 113], ["super().__init__", "datasets.load.load_dataset", "min", "datasets.load.load_metric", "print", "print", "logger.warning", "raw_datasets.map.map.map", "raw_datasets.map.map.map", "logger.info", "raw_datasets[].train_test_split", "len", "dataset.SuperGlueDataset.train_dataset.select", "dataset.SuperGlueDataset.predict_dataset.select", "transformers.DataCollatorWithPadding", "enumerate", "dataset.SuperGlueDataset.label2id.items", "range", "range"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "\n", "\n", "class", "GlueDataset", "(", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "tokenizer", ":", "AutoTokenizer", ",", "data_args", ",", "training_args", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "raw_datasets", "=", "load_dataset", "(", "\"glue\"", ",", "data_args", ".", "dataset_name", ")", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "data_args", "=", "data_args", "\n", "#labels", "\n", "self", ".", "is_regression", "=", "data_args", ".", "dataset_name", "==", "\"stsb\"", "\n", "if", "not", "self", ".", "is_regression", ":", "\n", "            ", "self", ".", "label_list", "=", "raw_datasets", "[", "\"train\"", "]", ".", "features", "[", "\"label\"", "]", ".", "names", "\n", "self", ".", "num_labels", "=", "len", "(", "self", ".", "label_list", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_labels", "=", "1", "\n", "\n", "# Preprocessing the raw_datasets", "\n", "", "self", ".", "sentence1_key", ",", "self", ".", "sentence2_key", "=", "task_to_keys", "[", "data_args", ".", "dataset_name", "]", "\n", "\n", "# Padding strategy", "\n", "if", "data_args", ".", "pad_to_max_length", ":", "\n", "            ", "self", ".", "padding", "=", "\"max_length\"", "\n", "", "else", ":", "\n", "# We will pad later, dynamically at batch creation, to the max sequence length in each batch", "\n", "            ", "self", ".", "padding", "=", "False", "\n", "\n", "# Some models have set the order of the labels to use, so let's make sure we do use it.", "\n", "", "if", "not", "self", ".", "is_regression", ":", "\n", "            ", "self", ".", "label2id", "=", "{", "l", ":", "i", "for", "i", ",", "l", "in", "enumerate", "(", "self", ".", "label_list", ")", "}", "\n", "self", ".", "id2label", "=", "{", "id", ":", "label", "for", "label", ",", "id", "in", "self", ".", "label2id", ".", "items", "(", ")", "}", "\n", "\n", "", "if", "data_args", ".", "max_seq_length", ">", "tokenizer", ".", "model_max_length", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"", "\n", "f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"", "\n", ")", "\n", "", "self", ".", "max_seq_length", "=", "min", "(", "data_args", ".", "max_seq_length", ",", "tokenizer", ".", "model_max_length", ")", "\n", "\n", "raw_datasets", "=", "raw_datasets", ".", "map", "(", "\n", "self", ".", "preprocess_function", ",", "\n", "batched", "=", "True", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", "desc", "=", "\"Running tokenizer on dataset\"", ",", "\n", ")", "\n", "if", "training_args", ".", "do_train", ":", "\n", "            ", "train_dataset", "=", "raw_datasets", "[", "\"train\"", "]", ".", "train_test_split", "(", "test_size", "=", "0.1", ",", "shuffle", "=", "False", ")", "\n", "self", ".", "train_dataset", ",", "self", ".", "eval_dataset", "=", "train_dataset", "=", "train_dataset", "[", "'train'", "]", ",", "train_dataset", "[", "'test'", "]", "\n", "if", "data_args", ".", "max_train_samples", "is", "not", "None", ":", "\n", "                ", "self", ".", "train_dataset", "=", "self", ".", "train_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_train_samples", ")", ")", "\n", "\n", "", "", "if", "training_args", ".", "do_predict", ":", "\n", "            ", "self", ".", "predict_dataset", "=", "raw_datasets", "[", "\"validation_matched\"", "if", "data_args", ".", "dataset_name", "==", "\"mnli\"", "else", "\"validation\"", "]", "\n", "if", "data_args", ".", "max_predict_samples", "is", "not", "None", ":", "\n", "                ", "self", ".", "predict_dataset", "=", "self", ".", "predict_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_predict_samples", ")", ")", "\n", "\n", "# if training_args.do_train:", "\n", "#     self.train_dataset = raw_datasets[\"train\"]", "\n", "#     if data_args.max_train_samples is not None:", "\n", "#         self.train_dataset = self.train_dataset.select(range(data_args.max_train_samples))", "\n", "\n", "# if training_args.do_eval:", "\n", "#     self.eval_dataset = raw_datasets[\"validation_matched\" if data_args.dataset_name == \"mnli\" else \"validation\"]", "\n", "#     if data_args.max_eval_samples is not None:", "\n", "#         self.eval_dataset = self.eval_dataset.select(range(data_args.max_eval_samples))", "\n", "\n", "# if training_args.do_predict or data_args.dataset_name is not None or data_args.test_file is not None:", "\n", "#     self.predict_dataset = raw_datasets[\"test_matched\" if data_args.dataset_name == \"mnli\" else \"test\"]", "\n", "#     if data_args.max_predict_samples is not None:", "\n", "#         self.predict_dataset = self.predict_dataset.select(range(data_args.max_predict_samples))", "\n", "\n", "", "", "self", ".", "metric", "=", "load_metric", "(", "\"tasks/glue/glue.py\"", ",", "data_args", ".", "dataset_name", ")", "\n", "\n", "if", "data_args", ".", "pad_to_max_length", ":", "\n", "            ", "self", ".", "data_collator", "=", "default_data_collator", "\n", "", "elif", "training_args", ".", "fp16", ":", "\n", "            ", "self", ".", "data_collator", "=", "DataCollatorWithPadding", "(", "tokenizer", ",", "pad_to_multiple_of", "=", "8", ")", "\n", "\n", "\n", "", "", "def", "preprocess_function", "(", "self", ",", "examples", ")", ":", "\n", "# Tokenize the texts", "\n", "        ", "args", "=", "(", "\n", "(", "examples", "[", "self", ".", "sentence1_key", "]", ",", ")", "if", "self", ".", "sentence2_key", "is", "None", "else", "(", "examples", "[", "self", ".", "sentence1_key", "]", ",", "examples", "[", "self", ".", "sentence2_key", "]", ")", "\n", ")", "\n", "result", "=", "self", ".", "tokenizer", "(", "*", "args", ",", "padding", "=", "self", ".", "padding", ",", "max_length", "=", "self", ".", "max_seq_length", ",", "truncation", "=", "True", ")", "\n", "\n", "return", "result", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.dataset.SuperGlueDataset.preprocess_function": [[114, 169], ["dataset.SuperGlueDataset.tokenizer", "zip", "zip", "zip", "zip", "dataset.SuperGlueDataset.tokenizer", "dataset.SuperGlueDataset.tokenizer", "examples[].append", "examples[].append", "examples[].append", "examples[].append", "zip", "text.split", "examples[].append", "examples[].append", "examples[].append", "result[].append"], "methods", ["None"], ["\n", "", "def", "compute_metrics", "(", "self", ",", "p", ":", "EvalPrediction", ")", ":", "\n", "        ", "preds", "=", "p", ".", "predictions", "[", "0", "]", "if", "isinstance", "(", "p", ".", "predictions", ",", "tuple", ")", "else", "p", ".", "predictions", "\n", "preds", "=", "np", ".", "squeeze", "(", "preds", ")", "if", "self", ".", "is_regression", "else", "np", ".", "argmax", "(", "preds", ",", "axis", "=", "1", ")", "\n", "if", "self", ".", "data_args", ".", "dataset_name", "is", "not", "None", ":", "\n", "            ", "result", "=", "self", ".", "metric", ".", "compute", "(", "predictions", "=", "preds", ",", "references", "=", "p", ".", "label_ids", ")", "\n", "if", "len", "(", "result", ")", ">", "1", ":", "\n", "                ", "result", "[", "\"combined_score\"", "]", "=", "np", ".", "mean", "(", "list", "(", "result", ".", "values", "(", ")", ")", ")", ".", "item", "(", ")", "\n", "", "return", "result", "\n", "", "elif", "self", ".", "is_regression", ":", "\n", "            ", "return", "{", "\"mse\"", ":", "(", "(", "preds", "-", "p", ".", "label_ids", ")", "**", "2", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "}", "\n", "", "else", ":", "\n", "            ", "return", "{", "\"accuracy\"", ":", "(", "preds", "==", "p", ".", "label_ids", ")", ".", "astype", "(", "np", ".", "float32", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "}", "\n", "\n", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.dataset.SuperGlueDataset.compute_metrics": [[170, 190], ["numpy.argmax", "isinstance", "dataset.SuperGlueDataset.reocrd_compute_metrics", "dataset.SuperGlueDataset.metric.compute", "f1_score", "len", "numpy.mean().item", "numpy.mean", "list", "dataset.SuperGlueDataset.values"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.dataset.SuperGlueDataset.reocrd_compute_metrics", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.f1_score"], []], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.dataset.SuperGlueDataset.reocrd_compute_metrics": [[191, 215], ["collections.defaultdict", "zip", "isinstance", "qid2pred[].append", "sorted", "metric_max_over_ground_truths", "metric_max_over_ground_truths"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.metric_max_over_ground_truths", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.metric_max_over_ground_truths"], []], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.dataset.SuperGlueDataset.record_preprocess_function": [[216, 247], ["enumerate", "list", "list", "list", "list", "list", "list", "list", "passage.replace.replace.replace", "enumerate", "query.replace", "dataset.SuperGlueDataset.tokenizer", "results[].append", "results[].append", "results[].append", "results[].append", "results[].append", "results[].append", "results[].append", "results[].append"], "methods", ["None"], []], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.record_evaluation.normalize_answer": [[15, 32], ["record_evaluation.normalize_answer.white_space_fix"], "function", ["None"], ["def", "normalize_answer", "(", "s", ")", ":", "\n", "    ", "\"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"", "\n", "\n", "def", "remove_articles", "(", "text", ")", ":", "\n", "        ", "return", "re", ".", "sub", "(", "r\"\\b(a|an|the)\\b\"", ",", "\" \"", ",", "text", ")", "\n", "\n", "", "def", "white_space_fix", "(", "text", ")", ":", "\n", "        ", "return", "\" \"", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "\n", "", "def", "remove_punc", "(", "text", ")", ":", "\n", "        ", "exclude", "=", "set", "(", "string", ".", "punctuation", ")", "\n", "return", "\"\"", ".", "join", "(", "ch", "for", "ch", "in", "text", "if", "ch", "not", "in", "exclude", ")", "\n", "\n", "", "def", "lower", "(", "text", ")", ":", "\n", "        ", "return", "text", ".", "lower", "(", ")", "\n", "\n", "", "return", "white_space_fix", "(", "remove_articles", "(", "remove_punc", "(", "lower", "(", "s", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.record_evaluation.f1_score": [[34, 45], ["normalize_answer().split", "normalize_answer().split", "sum", "collections.Counter", "collections.Counter", "common.values", "len", "len", "record_evaluation.normalize_answer", "record_evaluation.normalize_answer"], "function", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.normalize_answer", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.normalize_answer"], ["", "def", "f1_score", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "prediction_tokens", "=", "normalize_answer", "(", "prediction", ")", ".", "split", "(", ")", "\n", "ground_truth_tokens", "=", "normalize_answer", "(", "ground_truth", ")", ".", "split", "(", ")", "\n", "common", "=", "Counter", "(", "prediction_tokens", ")", "&", "Counter", "(", "ground_truth_tokens", ")", "\n", "num_same", "=", "sum", "(", "common", ".", "values", "(", ")", ")", "\n", "if", "num_same", "==", "0", ":", "\n", "        ", "return", "0", "\n", "", "precision", "=", "1.0", "*", "num_same", "/", "len", "(", "prediction_tokens", ")", "\n", "recall", "=", "1.0", "*", "num_same", "/", "len", "(", "ground_truth_tokens", ")", "\n", "f1", "=", "(", "2", "*", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", ")", "\n", "return", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.record_evaluation.exact_match_score": [[47, 49], ["record_evaluation.normalize_answer", "record_evaluation.normalize_answer"], "function", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.normalize_answer", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.normalize_answer"], ["", "def", "exact_match_score", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "return", "normalize_answer", "(", "prediction", ")", "==", "normalize_answer", "(", "ground_truth", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.record_evaluation.metric_max_over_ground_truths": [[51, 57], ["max", "scores_for_ground_truths.append", "record_evaluation.exact_match_score", "record_evaluation.f1_score"], "function", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.exact_match_score", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.f1_score"], ["", "def", "metric_max_over_ground_truths", "(", "metric_fn", ",", "prediction", ",", "ground_truths", ")", ":", "\n", "    ", "scores_for_ground_truths", "=", "[", "]", "\n", "for", "ground_truth", "in", "ground_truths", ":", "\n", "        ", "score", "=", "metric_fn", "(", "prediction", ",", "ground_truth", ")", "\n", "scores_for_ground_truths", ".", "append", "(", "score", ")", "\n", "", "return", "max", "(", "scores_for_ground_truths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.record_evaluation.evaluate": [[59, 84], ["list", "record_evaluation.metric_max_over_ground_truths", "record_evaluation.metric_max_over_ground_truths", "print", "map", "int", "correct_ids.append"], "function", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.metric_max_over_ground_truths", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.metric_max_over_ground_truths"], ["", "def", "evaluate", "(", "dataset", ",", "predictions", ")", ":", "\n", "    ", "f1", "=", "exact_match", "=", "total", "=", "0", "\n", "correct_ids", "=", "[", "]", "\n", "for", "passage", "in", "dataset", ":", "\n", "        ", "for", "qa", "in", "passage", "[", "\"qas\"", "]", ":", "\n", "            ", "total", "+=", "1", "\n", "if", "qa", "[", "\"id\"", "]", "not", "in", "predictions", ":", "\n", "                ", "message", "=", "\"Unanswered question {} will receive score 0.\"", ".", "format", "(", "qa", "[", "\"id\"", "]", ")", "\n", "print", "(", "message", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "continue", "\n", "\n", "", "ground_truths", "=", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "\"text\"", "]", ",", "qa", "[", "\"answers\"", "]", ")", ")", "\n", "prediction", "=", "predictions", "[", "qa", "[", "\"id\"", "]", "]", "\n", "\n", "_exact_match", "=", "metric_max_over_ground_truths", "(", "exact_match_score", ",", "prediction", ",", "ground_truths", ")", "\n", "if", "int", "(", "_exact_match", ")", "==", "1", ":", "\n", "                ", "correct_ids", ".", "append", "(", "qa", "[", "\"id\"", "]", ")", "\n", "", "exact_match", "+=", "_exact_match", "\n", "\n", "f1", "+=", "metric_max_over_ground_truths", "(", "f1_score", ",", "prediction", ",", "ground_truths", ")", "\n", "\n", "", "", "exact_match", "=", "exact_match", "/", "total", "\n", "f1", "=", "f1", "/", "total", "\n", "\n", "return", "{", "\"exact_match\"", ":", "exact_match", ",", "\"f1\"", ":", "f1", "}", ",", "correct_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.get_trainer.get_trainer": [[22, 118], ["print", "transformers.set_seed", "training_args.get_process_log_level", "logger.setLevel", "transformers.AutoTokenizer.from_pretrained", "tasks.superglue.dataset.SuperGlueDataset", "list", "logger.info", "transformers.set_seed", "print", "trainer_cls", "transformers.AutoConfig.from_pretrained", "transformers.AutoConfig.from_pretrained", "model.utils.get_model", "model.utils.get_model", "logger.info", "model.utils.get_model.train_adapter", "model.utils.get_model.set_active_adapters", "model.utils.get_model.named_parameters", "model.utils.get_model.named_parameters", "transformers.AdapterConfig.load", "model.utils.get_model.add_adapter", "ValueError", "p.numel", "logger.info", "name.startswith", "transformers.EarlyStoppingCallback", "name.lower"], "function", ["home.repos.pwc.inspect_result.guanzhchen_petuning.model.utils.get_model", "home.repos.pwc.inspect_result.guanzhchen_petuning.model.utils.get_model"], ["tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "use_fast", "=", "model_args", ".", "use_fast_tokenizer", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", ")", "\n", "dataset", "=", "GlueDataset", "(", "tokenizer", ",", "data_args", ",", "training_args", ")", "\n", "\n", "if", "not", "dataset", ".", "is_regression", ":", "\n", "        ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "num_labels", "=", "dataset", ".", "num_labels", ",", "\n", "label2id", "=", "dataset", ".", "label2id", ",", "\n", "id2label", "=", "dataset", ".", "id2label", ",", "\n", "finetuning_task", "=", "data_args", ".", "dataset_name", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "num_labels", "=", "dataset", ".", "num_labels", ",", "\n", "finetuning_task", "=", "data_args", ".", "dataset_name", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", ")", "\n", "", "config", ".", "lora", "=", "False", "\n", "model", "=", "get_model", "(", "model_args", ",", "TaskType", ".", "SEQUENCE_CLASSIFICATION", ",", "config", ")", "\n", "\n", "if", "adapter_args", ".", "train_adapter", ":", "\n", "        ", "logger", ".", "info", "(", "f\"Reduction Factor: {adapter_args.adapter_reduction_factor}\"", ")", "\n", "task_name", "=", "data_args", ".", "task_name", "or", "\"superglue\"", "\n", "# check if adapter already exists, otherwise add it", "\n", "if", "task_name", "not", "in", "model", ".", "config", ".", "adapters", ":", "\n", "# resolve the adapter config", "\n", "            ", "adapter_config", "=", "AdapterConfig", ".", "load", "(", "\n", "adapter_args", ".", "adapter_config", ",", "\n", "non_linearity", "=", "adapter_args", ".", "adapter_non_linearity", ",", "\n", "reduction_factor", "=", "adapter_args", ".", "adapter_reduction_factor", ",", "\n", ")", "\n", "# load a pre-trained from Hub if specified", "\n", "# if adapter_args.load_adapter:", "\n", "#     model.load_adapter(", "\n", "#         adapter_args.load_adapter,", "\n", "#         config=adapter_config,", "\n", "#         load_as=task_name,", "\n", "#     )", "\n", "# # otherwise, add a fresh adapter", "\n", "# else:", "\n", "model", ".", "add_adapter", "(", "task_name", ",", "config", "=", "adapter_config", ")", "\n", "# Freeze all model weights except of those of this adapter", "\n", "", "model", ".", "train_adapter", "(", "[", "task_name", "]", ")", "\n", "# Set the adapters to be used in every forward pass", "\n", "model", ".", "set_active_adapters", "(", "task_name", ")", "\n", "", "else", ":", "\n", "        ", "if", "adapter_args", ".", "load_adapter", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Adapters can only be loaded in adapters training mode.\"", "\n", "\"Use --train_adapter to enable adapter training\"", "\n", ")", "\n", "", "", "if", "model_args", ".", "bitfit", ":", "\n", "        ", "for", "name", ",", "param", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "name", ".", "startswith", "(", "'roberta'", ")", "and", "\"bias\"", "not", "in", "name", ".", "lower", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "", "", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "logger", ".", "info", "(", "\"Trainable parameters:\"", ")", "\n", "for", "n", ",", "p", "in", "param_optimizer", ":", "\n", "        ", "if", "p", ".", "requires_grad", ":", "\n", "            ", "logger", ".", "info", "(", "f\"{n}\"", ")", "\n", "# print(n)", "\n", "\n", "", "", "trainer_cls", "=", "AdapterTrainer", "if", "adapter_args", ".", "train_adapter", "else", "Trainer", "\n", "trainer", "=", "trainer_cls", "(", "\n", "model", "=", "model", ",", "\n", "args", "=", "training_args", ",", "\n", "train_dataset", "=", "dataset", ".", "train_dataset", "if", "training_args", ".", "do_train", "else", "None", ",", "\n", "eval_dataset", "=", "dataset", ".", "eval_dataset", "if", "training_args", ".", "do_eval", "else", "None", ",", "\n", "compute_metrics", "=", "dataset", ".", "compute_metrics", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "data_collator", "=", "dataset", ".", "data_collator", ",", "\n", "callbacks", "=", "[", "EarlyStoppingCallback", "(", "early_stopping_patience", "=", "10", ")", "]", "\n", ")", "\n", "\n", "return", "trainer", ",", "dataset", ".", "predict_dataset", "", "", ""]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.normalize_answer": [[6, 23], ["utils.normalize_answer.white_space_fix"], "function", ["None"], ["RobertaLoraForSequenceClassification", "\n", ")", "\n", "\n", "\n", "from", "model", ".", "multiple_choice", "import", "(", "\n", "RobertaPrefixForMultipleChoice", ",", "\n", "RobertaLoraForMultipleChoice", "\n", ")", "\n", "\n", "from", "transformers", "import", "(", "\n", "AutoConfig", ",", "\n", "AutoModelForTokenClassification", ",", "\n", "AutoModelForSequenceClassification", ",", "\n", "AutoModelForQuestionAnswering", ",", "\n", "AutoModelForMultipleChoice", "\n", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.f1_score": [[24, 35], ["normalize_answer().split", "normalize_answer().split", "sum", "collections.Counter", "collections.Counter", "common.values", "len", "len", "utils.normalize_answer", "utils.normalize_answer"], "function", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.normalize_answer", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.normalize_answer"], ["\n", "class", "TaskType", "(", "Enum", ")", ":", "\n", "    ", "TOKEN_CLASSIFICATION", "=", "1", ",", "\n", "SEQUENCE_CLASSIFICATION", "=", "2", ",", "\n", "QUESTION_ANSWERING", "=", "3", ",", "\n", "MULTIPLE_CHOICE", "=", "4", "\n", "\n", "", "PREFIX_MODELS", "=", "{", "\n", "\n", "\"roberta\"", ":", "{", "\n", "TaskType", ".", "SEQUENCE_CLASSIFICATION", ":", "RobertaPrefixForSequenceClassification", ",", "\n", "TaskType", ".", "MULTIPLE_CHOICE", ":", "RobertaPrefixForMultipleChoice", ",", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.exact_match_score": [[37, 39], ["utils.normalize_answer", "utils.normalize_answer"], "function", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.normalize_answer", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.normalize_answer"], ["}", ",", "\n", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.utils.metric_max_over_ground_truths": [[41, 47], ["max", "metric_fn", "scores_for_ground_truths.append"], "function", ["None"], ["\n", "AUTO_MODELS", "=", "{", "\n", "TaskType", ".", "TOKEN_CLASSIFICATION", ":", "AutoModelForTokenClassification", ",", "\n", "TaskType", ".", "SEQUENCE_CLASSIFICATION", ":", "AutoModelForSequenceClassification", ",", "\n", "TaskType", ".", "QUESTION_ANSWERING", ":", "AutoModelForQuestionAnswering", ",", "\n", "TaskType", ".", "MULTIPLE_CHOICE", ":", "AutoModelForMultipleChoice", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__": [[284, 310], ["datasets.BuilderConfig.__init__", "datasets.Version"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlueConfig.__init__"], ["def", "__init__", "(", "self", ",", "features", ",", "data_url", ",", "citation", ",", "url", ",", "label_classes", "=", "(", "\"False\"", ",", "\"True\"", ")", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"BuilderConfig for SuperGLUE.\n\n        Args:\n          features: `list[string]`, list of the features that will appear in the\n            feature dict. Should not include \"label\".\n          data_url: `string`, url to download the zip file from.\n          citation: `string`, citation for the data set.\n          url: `string`, url for information about the data set.\n          label_classes: `list[string]`, the list of classes for the label if the\n            label is present as a string. Non-string labels will be cast to either\n            'False' or 'True'.\n          **kwargs: keyword arguments forwarded to super.\n        \"\"\"", "\n", "# Version history:", "\n", "# 1.0.2: Fixed non-nondeterminism in ReCoRD.", "\n", "# 1.0.1: Change from the pre-release trial version of SuperGLUE (v1.9) to", "\n", "#        the full release (v2.0).", "\n", "# 1.0.0: S3 (new shuffling, sharding and slicing mechanism).", "\n", "# 0.0.2: Initial version.", "\n", "super", "(", "SuperGlueConfig", ",", "self", ")", ".", "__init__", "(", "version", "=", "datasets", ".", "Version", "(", "\"1.0.2\"", ")", ",", "**", "kwargs", ")", "\n", "self", ".", "features", "=", "features", "\n", "self", ".", "label_classes", "=", "label_classes", "\n", "self", ".", "data_url", "=", "data_url", "\n", "self", ".", "citation", "=", "citation", "\n", "self", ".", "url", "=", "url", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlue._info": [[425, 466], ["super_glue.SuperGlue.config.name.startswith", "datasets.DatasetInfo", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value", "dict", "datasets.features.Sequence", "datasets.features.Sequence", "datasets.features.ClassLabel", "dict", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Features", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value", "datasets.Value"], "methods", ["None"], ["def", "_info", "(", "self", ")", ":", "\n", "        ", "features", "=", "{", "feature", ":", "datasets", ".", "Value", "(", "\"string\"", ")", "for", "feature", "in", "self", ".", "config", ".", "features", "}", "\n", "if", "self", ".", "config", ".", "name", ".", "startswith", "(", "\"wsc\"", ")", ":", "\n", "            ", "features", "[", "\"span1_index\"", "]", "=", "datasets", ".", "Value", "(", "\"int32\"", ")", "\n", "features", "[", "\"span2_index\"", "]", "=", "datasets", ".", "Value", "(", "\"int32\"", ")", "\n", "", "if", "self", ".", "config", ".", "name", "==", "\"wic\"", ":", "\n", "            ", "features", "[", "\"start1\"", "]", "=", "datasets", ".", "Value", "(", "\"int32\"", ")", "\n", "features", "[", "\"start2\"", "]", "=", "datasets", ".", "Value", "(", "\"int32\"", ")", "\n", "features", "[", "\"end1\"", "]", "=", "datasets", ".", "Value", "(", "\"int32\"", ")", "\n", "features", "[", "\"end2\"", "]", "=", "datasets", ".", "Value", "(", "\"int32\"", ")", "\n", "", "if", "self", ".", "config", ".", "name", "==", "\"multirc\"", ":", "\n", "            ", "features", "[", "\"idx\"", "]", "=", "dict", "(", "\n", "{", "\n", "\"paragraph\"", ":", "datasets", ".", "Value", "(", "\"int32\"", ")", ",", "\n", "\"question\"", ":", "datasets", ".", "Value", "(", "\"int32\"", ")", ",", "\n", "\"answer\"", ":", "datasets", ".", "Value", "(", "\"int32\"", ")", ",", "\n", "}", "\n", ")", "\n", "", "elif", "self", ".", "config", ".", "name", "==", "\"record\"", ":", "\n", "            ", "features", "[", "\"idx\"", "]", "=", "dict", "(", "\n", "{", "\n", "\"passage\"", ":", "datasets", ".", "Value", "(", "\"int32\"", ")", ",", "\n", "\"query\"", ":", "datasets", ".", "Value", "(", "\"int32\"", ")", ",", "\n", "}", "\n", ")", "\n", "", "else", ":", "\n", "            ", "features", "[", "\"idx\"", "]", "=", "datasets", ".", "Value", "(", "\"int32\"", ")", "\n", "\n", "", "if", "self", ".", "config", ".", "name", "==", "\"record\"", ":", "\n", "# Entities are the set of possible choices for the placeholder.", "\n", "            ", "features", "[", "\"entities\"", "]", "=", "datasets", ".", "features", ".", "Sequence", "(", "datasets", ".", "Value", "(", "\"string\"", ")", ")", "\n", "# Answers are the subset of entities that are correct.", "\n", "features", "[", "\"answers\"", "]", "=", "datasets", ".", "features", ".", "Sequence", "(", "datasets", ".", "Value", "(", "\"string\"", ")", ")", "\n", "", "else", ":", "\n", "            ", "features", "[", "\"label\"", "]", "=", "datasets", ".", "features", ".", "ClassLabel", "(", "names", "=", "self", ".", "config", ".", "label_classes", ")", "\n", "\n", "", "return", "datasets", ".", "DatasetInfo", "(", "\n", "description", "=", "_GLUE_DESCRIPTION", "+", "self", ".", "config", ".", "description", ",", "\n", "features", "=", "datasets", ".", "Features", "(", "features", ")", ",", "\n", "homepage", "=", "self", ".", "config", ".", "url", ",", "\n", "citation", "=", "self", ".", "config", ".", "citation", "+", "\"\\n\"", "+", "_SUPER_GLUE_CITATION", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlue._split_generators": [[468, 502], ["super_glue._get_task_name_from_data_url", "os.path.join", "dl_manager.download_and_extract", "datasets.SplitGenerator", "datasets.SplitGenerator", "datasets.SplitGenerator", "datasets.SplitGenerator", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue._get_task_name_from_data_url"], ["", "def", "_split_generators", "(", "self", ",", "dl_manager", ")", ":", "\n", "        ", "dl_dir", "=", "dl_manager", ".", "download_and_extract", "(", "self", ".", "config", ".", "data_url", ")", "or", "\"\"", "\n", "task_name", "=", "_get_task_name_from_data_url", "(", "self", ".", "config", ".", "data_url", ")", "\n", "dl_dir", "=", "os", ".", "path", ".", "join", "(", "dl_dir", ",", "task_name", ")", "\n", "if", "self", ".", "config", ".", "name", "in", "[", "\"axb\"", ",", "\"axg\"", "]", ":", "\n", "            ", "return", "[", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "TEST", ",", "\n", "gen_kwargs", "=", "{", "\n", "\"data_file\"", ":", "os", ".", "path", ".", "join", "(", "dl_dir", ",", "\"{}.jsonl\"", ".", "format", "(", "task_name", ")", ")", ",", "\n", "\"split\"", ":", "datasets", ".", "Split", ".", "TEST", ",", "\n", "}", ",", "\n", ")", ",", "\n", "]", "\n", "", "return", "[", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "TRAIN", ",", "\n", "gen_kwargs", "=", "{", "\n", "\"data_file\"", ":", "os", ".", "path", ".", "join", "(", "dl_dir", ",", "\"train.jsonl\"", ")", ",", "\n", "\"split\"", ":", "datasets", ".", "Split", ".", "TRAIN", ",", "\n", "}", ",", "\n", ")", ",", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "VALIDATION", ",", "\n", "gen_kwargs", "=", "{", "\n", "\"data_file\"", ":", "os", ".", "path", ".", "join", "(", "dl_dir", ",", "\"val.jsonl\"", ")", ",", "\n", "\"split\"", ":", "datasets", ".", "Split", ".", "VALIDATION", ",", "\n", "}", ",", "\n", ")", ",", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "TEST", ",", "\n", "gen_kwargs", "=", "{", "\n", "\"data_file\"", ":", "os", ".", "path", ".", "join", "(", "dl_dir", ",", "\"test.jsonl\"", ")", ",", "\n", "\"split\"", ":", "datasets", ".", "Split", ".", "TEST", ",", "\n", "}", ",", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue.SuperGlue._generate_examples": [[506, 551], ["open", "json.loads", "super_glue.SuperGlue.config.name.startswith", "answer.get", "json.loads.update", "super_glue._fix_wst", "super_glue._cast_label", "super_glue._get_record_entities", "super_glue._get_record_answers", "super_glue._cast_label", "bool"], "methods", ["home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue._fix_wst", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue._cast_label", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue._get_record_entities", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue._get_record_answers", "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue._cast_label"], ["", "def", "_generate_examples", "(", "self", ",", "data_file", ",", "split", ")", ":", "\n", "        ", "with", "open", "(", "data_file", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "row", "=", "json", ".", "loads", "(", "line", ")", "\n", "\n", "if", "self", ".", "config", ".", "name", "==", "\"multirc\"", ":", "\n", "                    ", "paragraph", "=", "row", "[", "\"passage\"", "]", "\n", "for", "question", "in", "paragraph", "[", "\"questions\"", "]", ":", "\n", "                        ", "for", "answer", "in", "question", "[", "\"answers\"", "]", ":", "\n", "                            ", "label", "=", "answer", ".", "get", "(", "\"label\"", ")", "\n", "key", "=", "\"%s_%s_%s\"", "%", "(", "row", "[", "\"idx\"", "]", ",", "question", "[", "\"idx\"", "]", ",", "answer", "[", "\"idx\"", "]", ")", "\n", "yield", "key", ",", "{", "\n", "\"paragraph\"", ":", "paragraph", "[", "\"text\"", "]", ",", "\n", "\"question\"", ":", "question", "[", "\"question\"", "]", ",", "\n", "\"answer\"", ":", "answer", "[", "\"text\"", "]", ",", "\n", "\"label\"", ":", "-", "1", "if", "label", "is", "None", "else", "_cast_label", "(", "bool", "(", "label", ")", ")", ",", "\n", "\"idx\"", ":", "{", "\"paragraph\"", ":", "row", "[", "\"idx\"", "]", ",", "\"question\"", ":", "question", "[", "\"idx\"", "]", ",", "\"answer\"", ":", "answer", "[", "\"idx\"", "]", "}", ",", "\n", "}", "\n", "", "", "", "elif", "self", ".", "config", ".", "name", "==", "\"record\"", ":", "\n", "                    ", "passage", "=", "row", "[", "\"passage\"", "]", "\n", "for", "qa", "in", "row", "[", "\"qas\"", "]", ":", "\n", "                        ", "yield", "qa", "[", "\"idx\"", "]", ",", "{", "\n", "\"passage\"", ":", "passage", "[", "\"text\"", "]", ",", "\n", "\"query\"", ":", "qa", "[", "\"query\"", "]", ",", "\n", "\"entities\"", ":", "_get_record_entities", "(", "passage", ")", ",", "\n", "\"answers\"", ":", "_get_record_answers", "(", "qa", ")", ",", "\n", "\"idx\"", ":", "{", "\"passage\"", ":", "row", "[", "\"idx\"", "]", ",", "\"query\"", ":", "qa", "[", "\"idx\"", "]", "}", ",", "\n", "}", "\n", "", "", "else", ":", "\n", "                    ", "if", "self", ".", "config", ".", "name", ".", "startswith", "(", "\"wsc\"", ")", ":", "\n", "                        ", "row", ".", "update", "(", "row", "[", "\"target\"", "]", ")", "\n", "", "example", "=", "{", "feature", ":", "row", "[", "feature", "]", "for", "feature", "in", "self", ".", "config", ".", "features", "}", "\n", "if", "self", ".", "config", ".", "name", "==", "\"wsc.fixed\"", ":", "\n", "                        ", "example", "=", "_fix_wst", "(", "example", ")", "\n", "", "example", "[", "\"idx\"", "]", "=", "row", "[", "\"idx\"", "]", "\n", "\n", "if", "\"label\"", "in", "row", ":", "\n", "                        ", "if", "self", ".", "config", ".", "name", "==", "\"copa\"", ":", "\n", "                            ", "example", "[", "\"label\"", "]", "=", "\"choice2\"", "if", "row", "[", "\"label\"", "]", "else", "\"choice1\"", "\n", "", "else", ":", "\n", "                            ", "example", "[", "\"label\"", "]", "=", "_cast_label", "(", "row", "[", "\"label\"", "]", ")", "\n", "", "", "else", ":", "\n", "                        ", "assert", "split", "==", "datasets", ".", "Split", ".", "TEST", ",", "row", "\n", "example", "[", "\"label\"", "]", "=", "-", "1", "\n", "", "yield", "example", "[", "\"idx\"", "]", ",", "example", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue._fix_wst": [[553, 588], ["super_glue._fix_wst._fix_span_text"], "function", ["None"], ["", "", "", "", "", "def", "_fix_wst", "(", "ex", ")", ":", "\n", "    ", "\"\"\"Fixes most cases where spans are not actually substrings of text.\"\"\"", "\n", "\n", "def", "_fix_span_text", "(", "k", ")", ":", "\n", "        ", "\"\"\"Fixes a single span.\"\"\"", "\n", "text", "=", "ex", "[", "k", "+", "\"_text\"", "]", "\n", "index", "=", "ex", "[", "k", "+", "\"_index\"", "]", "\n", "\n", "if", "text", "in", "ex", "[", "\"text\"", "]", ":", "\n", "            ", "return", "\n", "\n", "", "if", "text", "in", "(", "\"Kamenev and Zinoviev\"", ",", "\"Kamenev, Zinoviev, and Stalin\"", ")", ":", "\n", "# There is no way to correct these examples since the subjects have", "\n", "# intervening text.", "\n", "            ", "return", "\n", "\n", "", "if", "\"theyscold\"", "in", "text", ":", "\n", "            ", "ex", "[", "\"text\"", "]", ".", "replace", "(", "\"theyscold\"", ",", "\"they scold\"", ")", "\n", "ex", "[", "\"span2_index\"", "]", "=", "10", "\n", "# Make sure case of the first words match.", "\n", "", "first_word", "=", "ex", "[", "\"text\"", "]", ".", "split", "(", ")", "[", "index", "]", "\n", "if", "first_word", "[", "0", "]", ".", "islower", "(", ")", ":", "\n", "            ", "text", "=", "text", "[", "0", "]", ".", "lower", "(", ")", "+", "text", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "text", "=", "text", "[", "0", "]", ".", "upper", "(", ")", "+", "text", "[", "1", ":", "]", "\n", "# Remove punctuation in span.", "\n", "", "text", "=", "text", ".", "rstrip", "(", "\".\"", ")", "\n", "# Replace incorrect whitespace character in span.", "\n", "text", "=", "text", ".", "replace", "(", "\"\\n\"", ",", "\" \"", ")", "\n", "ex", "[", "k", "+", "\"_text\"", "]", "=", "text", "\n", "assert", "ex", "[", "k", "+", "\"_text\"", "]", "in", "ex", "[", "\"text\"", "]", ",", "ex", "\n", "\n", "", "_fix_span_text", "(", "\"span1\"", ")", "\n", "_fix_span_text", "(", "\"span2\"", ")", "\n", "return", "ex", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue._cast_label": [[590, 601], ["isinstance", "isinstance", "isinstance", "str", "ValueError"], "function", ["None"], ["", "def", "_cast_label", "(", "label", ")", ":", "\n", "    ", "\"\"\"Converts the label into the appropriate string version.\"\"\"", "\n", "if", "isinstance", "(", "label", ",", "str", ")", ":", "\n", "        ", "return", "label", "\n", "", "elif", "isinstance", "(", "label", ",", "bool", ")", ":", "\n", "        ", "return", "\"True\"", "if", "label", "else", "\"False\"", "\n", "", "elif", "isinstance", "(", "label", ",", "int", ")", ":", "\n", "        ", "assert", "label", "in", "(", "0", ",", "1", ")", "\n", "return", "str", "(", "label", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid label format.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue._get_record_entities": [[603, 610], ["set", "sorted", "set.add"], "function", ["None"], ["", "", "def", "_get_record_entities", "(", "passage", ")", ":", "\n", "    ", "\"\"\"Returns the unique set of entities.\"\"\"", "\n", "text", "=", "passage", "[", "\"text\"", "]", "\n", "entities", "=", "set", "(", ")", "\n", "for", "entity", "in", "passage", "[", "\"entities\"", "]", ":", "\n", "        ", "entities", ".", "add", "(", "text", "[", "entity", "[", "\"start\"", "]", ":", "entity", "[", "\"end\"", "]", "+", "1", "]", ")", "\n", "", "return", "sorted", "(", "entities", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue._get_record_answers": [[612, 620], ["set", "sorted", "set.add"], "function", ["None"], ["", "def", "_get_record_answers", "(", "qa", ")", ":", "\n", "    ", "\"\"\"Returns the unique set of answers.\"\"\"", "\n", "if", "\"answers\"", "not", "in", "qa", ":", "\n", "        ", "return", "[", "]", "\n", "", "answers", "=", "set", "(", ")", "\n", "for", "answer", "in", "qa", "[", "\"answers\"", "]", ":", "\n", "        ", "answers", ".", "add", "(", "answer", "[", "\"text\"", "]", ")", "\n", "", "return", "sorted", "(", "answers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.guanzhchen_petuning.superglue.super_glue._get_task_name_from_data_url": [[622, 624], ["[].split", "data_url.split"], "function", ["None"], ["", "def", "_get_task_name_from_data_url", "(", "data_url", ")", ":", "\n", "    ", "return", "data_url", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ".", "split", "(", "\".\"", ")", "[", "0", "]", "", "", ""]]}