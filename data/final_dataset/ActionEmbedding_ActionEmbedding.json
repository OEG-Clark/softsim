{"home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.sac_discrete.DiscreteSoftAC.__init__": [[7, 26], ["sac_discrete.DiscreteSoftAC._build", "tf.train.Saver", "tf.Session", "sac_discrete.DiscreteSoftAC.sess.run", "sac_discrete.DiscreteSoftAC.sess.run", "tf.global_variables_initializer"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed._build"], ["    ", "def", "__init__", "(", "self", ",", "state_dim", ",", "action_dim", ",", "hiddens", ",", "state_embed_hiddens", ",", "\n", "gamma", "=", "0.99", ",", "actor_lr", "=", "0.00025", ",", "critic_lr", "=", "0.001", ",", "tau", "=", "0.999", ",", "alpha", "=", "0.2", ")", ":", "\n", "        ", "self", ".", "state_dim", "=", "state_dim", "\n", "self", ".", "state_embed_dim", "=", "5", "\n", "self", ".", "action_dim", "=", "action_dim", "\n", "self", ".", "hiddens", "=", "hiddens", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "tau", "=", "tau", "\n", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "alpha", "=", "0", "\n", "self", ".", "actor_lr", ",", "self", ".", "critic_lr", "=", "actor_lr", ",", "critic_lr", "\n", "\n", "# self.state_embed = StateEmbed(state_dim, 5, state_embed_hiddens, 0)", "\n", "\n", "self", ".", "_build", "(", ")", "\n", "self", ".", "saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "\n", "self", ".", "sess", "=", "tf", ".", "Session", "(", ")", "\n", "self", ".", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "self", ".", "sess", ".", "run", "(", "self", ".", "init_target", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.sac_discrete.DiscreteSoftAC._build": [[27, 110], ["tf.placeholder", "tf.placeholder", "tf.placeholder", "tf.placeholder", "tf.placeholder", "tf.one_hot", "tf.reduce_sum", "tf.reduce_sum", "tf.reduce_sum", "tf.reduce_sum", "tf.minimum", "tf.stop_gradient", "tf.stop_gradient", "tf.reduce_mean", "tf.train.AdamOptimizer", "tf.train.AdamOptimizer", "tf.train.AdamOptimizer.minimize", "tf.train.AdamOptimizer.minimize", "tf.summary.scalar", "tf.summary.scalar", "tf.summary.histogram", "tf.summary.merge_all", "tf.variable_scope", "tf.squeeze", "tf.variable_scope", "tf.squeeze", "tf.variable_scope", "utils.mlp", "tf.layers.dense", "tf.nn.softmax", "tf.nn.log_softmax", "tf.argmax", "tf.squeeze", "tf.one_hot", "tf.reduce_sum", "tf.variable_scope", "utils.mlp1", "tf.variable_scope", "utils.mlp1", "tf.reduce_mean", "tf.reduce_mean", "tf.reduce_mean", "tf.assign", "tf.assign", "utils.mlp1", "utils.mlp1", "tf.multinomial", "tf.square", "tf.square", "tf.square", "utils.get_vars", "zip", "zip", "utils.get_vars", "utils.get_vars", "utils.get_vars", "utils.get_vars", "utils.get_vars", "utils.get_vars"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.mlp", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.mlp1", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.mlp1", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.mlp1", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.mlp1", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.get_vars", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.get_vars", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.get_vars", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.get_vars", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.get_vars", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.get_vars", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.get_vars"], ["", "def", "_build", "(", "self", ")", ":", "\n", "        ", "self", ".", "states_pl", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "(", "None", ",", "self", ".", "state_dim", ")", ")", "\n", "self", ".", "next_states_pl", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "(", "None", ",", "self", ".", "state_dim", ")", ")", "\n", "self", ".", "a_pl", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "shape", "=", "(", "None", ",", ")", ")", "\n", "self", ".", "r_pl", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "(", "None", ",", ")", ")", "\n", "self", ".", "done_pl", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "(", "None", ",", ")", ")", "\n", "\n", "self", ".", "state", "=", "self", ".", "states_pl", "\n", "self", ".", "next_state", "=", "self", ".", "next_states_pl", "\n", "\n", "# self.state = self.state_embed(self.states_pl)", "\n", "# self.next_state = self.state_embed(self.next_states_pl)", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'main_v'", ")", ":", "\n", "            ", "self", ".", "v", "=", "tf", ".", "squeeze", "(", "mlp1", "(", "self", ".", "state", ",", "self", ".", "hiddens", "+", "[", "1", "]", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", ",", "axis", "=", "1", ")", "\n", "", "with", "tf", ".", "variable_scope", "(", "'target_v'", ")", ":", "\n", "            ", "target_v", "=", "tf", ".", "squeeze", "(", "mlp1", "(", "self", ".", "next_state", ",", "self", ".", "hiddens", "+", "[", "1", "]", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", ",", "axis", "=", "1", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'policy'", ")", ":", "\n", "            ", "temp", "=", "mlp", "(", "self", ".", "state", ",", "self", ".", "hiddens", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "action_logit", "=", "tf", ".", "layers", ".", "dense", "(", "temp", ",", "self", ".", "action_dim", ",", "activation", "=", "None", ")", "\n", "probs", "=", "tf", ".", "nn", ".", "softmax", "(", "logits", "=", "action_logit", ",", "axis", "=", "1", ")", "\n", "logp_probs", "=", "tf", ".", "nn", ".", "log_softmax", "(", "logits", "=", "action_logit", ",", "axis", "=", "1", ")", "\n", "self", ".", "max_prob_action", "=", "tf", ".", "argmax", "(", "action_logit", ",", "axis", "=", "1", ")", "\n", "pi", "=", "tf", ".", "squeeze", "(", "tf", ".", "multinomial", "(", "action_logit", ",", "1", ")", ",", "axis", "=", "1", ")", "\n", "\n", "# target_distribution = tf.distributions.Categorical(logits=self.alpha * q_min)", "\n", "one_hot_pi", "=", "tf", ".", "one_hot", "(", "pi", ",", "depth", "=", "self", ".", "action_dim", ")", "\n", "logp_pi", "=", "tf", ".", "reduce_sum", "(", "logp_probs", "*", "one_hot_pi", ",", "axis", "=", "1", ")", "\n", "\n", "self", ".", "action", "=", "pi", "\n", "\n", "# with tf.variable_scope('alpha'):", "\n", "#     log_alpha = tf.Variable(0., trainable=True, name='log_alpha')", "\n", "#     self.alpha = tf.exp(log_alpha)", "\n", "#     alpha_loss = tf.reduce_mean(-log_alpha * tf.stop_gradient(logp_pi + 0.2))", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'q1'", ")", ":", "\n", "            ", "q1_all", "=", "mlp1", "(", "self", ".", "state", ",", "self", ".", "hiddens", "+", "[", "self", ".", "action_dim", "]", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'q2'", ")", ":", "\n", "            ", "q2_all", "=", "mlp1", "(", "self", ".", "state", ",", "self", ".", "hiddens", "+", "[", "self", ".", "action_dim", "]", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "\n", "", "actions", "=", "tf", ".", "one_hot", "(", "self", ".", "a_pl", ",", "depth", "=", "self", ".", "action_dim", ")", "\n", "q1", "=", "tf", ".", "reduce_sum", "(", "q1_all", "*", "actions", ",", "axis", "=", "1", ")", "\n", "q2", "=", "tf", ".", "reduce_sum", "(", "q2_all", "*", "actions", ",", "axis", "=", "1", ")", "\n", "q1_pi", "=", "tf", ".", "reduce_sum", "(", "q1_all", "*", "one_hot_pi", ",", "axis", "=", "1", ")", "\n", "q2_pi", "=", "tf", ".", "reduce_sum", "(", "q2_all", "*", "one_hot_pi", ",", "axis", "=", "1", ")", "\n", "\n", "q_", "=", "tf", ".", "minimum", "(", "q1_pi", ",", "q2_pi", ")", "\n", "v_", "=", "tf", ".", "stop_gradient", "(", "q_", "-", "self", ".", "alpha", "*", "logp_pi", ")", "\n", "target_q", "=", "tf", ".", "stop_gradient", "(", "self", ".", "r_pl", "+", "self", ".", "gamma", "*", "(", "1", "-", "self", ".", "done_pl", ")", "*", "target_v", ")", "\n", "\n", "\n", "# refer to the code below", "\n", "# https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/blob/master/agents/actor_critic_agents/SAC_Discrete.py", "\n", "# min_q = tf.minimum(q1_all, q2_all)", "\n", "inside", "=", "self", ".", "alpha", "*", "logp_probs", "-", "q1_all", "\n", "self", ".", "actor_loss", "=", "tf", ".", "reduce_mean", "(", "probs", "*", "inside", ")", "\n", "\n", "v_loss", "=", "0.5", "*", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "v_", "-", "self", ".", "v", ")", ")", "\n", "q1_loss", "=", "0.5", "*", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "q1", "-", "target_q", ")", ")", "\n", "q2_loss", "=", "0.5", "*", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "q2", "-", "target_q", ")", ")", "\n", "self", ".", "critic_loss", "=", "v_loss", "+", "q1_loss", "+", "q2_loss", "\n", "\n", "# alpha_opt = tf.train.AdamOptimizer(self.critic_lr)", "\n", "actor_opt", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "self", ".", "actor_lr", ")", "\n", "critic_opt", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "self", ".", "critic_lr", ")", "\n", "\n", "# self.alpha_train_op = alpha_opt.minimize(alpha_loss)", "\n", "self", ".", "actor_train_op", "=", "actor_opt", ".", "minimize", "(", "self", ".", "actor_loss", ",", "var_list", "=", "get_vars", "(", "'policy'", ")", ")", "# , var_list=get_vars('policy') + get_vars('state_embed')", "\n", "\n", "self", ".", "critic_train_op", "=", "critic_opt", ".", "minimize", "(", "self", ".", "critic_loss", ",", "var_list", "=", "get_vars", "(", "'main_v'", ")", "+", "get_vars", "(", "'q'", ")", ")", "# , var_list=get_vars('main_v') + get_vars('q') + get_vars('state_embed')", "\n", "\n", "self", ".", "soft_update", "=", "[", "tf", ".", "assign", "(", "v_targ", ",", "self", ".", "tau", "*", "v_targ", "+", "(", "1", "-", "self", ".", "tau", ")", "*", "v", ")", "\n", "for", "v", ",", "v_targ", "in", "zip", "(", "get_vars", "(", "'main_v'", ")", ",", "get_vars", "(", "'target_v'", ")", ")", "]", "\n", "\n", "self", ".", "init_target", "=", "[", "tf", ".", "assign", "(", "v_targ", ",", "v", ")", "for", "v", ",", "v_targ", "in", "zip", "(", "get_vars", "(", "'main_v'", ")", ",", "get_vars", "(", "'target_v'", ")", ")", "]", "\n", "\n", "tf", ".", "summary", ".", "scalar", "(", "'critic_loss'", ",", "self", ".", "critic_loss", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "'actor_loss'", ",", "self", ".", "actor_loss", ")", "\n", "tf", ".", "summary", ".", "histogram", "(", "'actions'", ",", "self", ".", "action", ")", "\n", "self", ".", "summary_op", "=", "tf", ".", "summary", ".", "merge_all", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.sac_discrete.DiscreteSoftAC.train": [[111, 121], ["sac_discrete.DiscreteSoftAC.sess.run", "sac_discrete.DiscreteSoftAC.sess.run", "sac_discrete.DiscreteSoftAC.sess.run"], "methods", ["None"], ["", "def", "train", "(", "self", ",", "step", ",", "state", ",", "action", ",", "next_state", ",", "reward", ",", "done", ")", ":", "\n", "        ", "feeds", "=", "{", "self", ".", "states_pl", ":", "state", ",", "self", ".", "a_pl", ":", "action", ",", "\n", "self", ".", "next_states_pl", ":", "next_state", ",", "self", ".", "r_pl", ":", "reward", ",", "self", ".", "done_pl", ":", "done", "}", "\n", "\n", "# self.sess.run(self.alpha_train_op, feed_dict=feeds)", "\n", "actor_loss", ",", "_", "=", "self", ".", "sess", ".", "run", "(", "[", "self", ".", "actor_loss", ",", "self", ".", "actor_train_op", "]", ",", "feed_dict", "=", "feeds", ")", "\n", "summary", ",", "critic_loss", ",", "_", "=", "self", ".", "sess", ".", "run", "(", "[", "self", ".", "summary_op", ",", "self", ".", "critic_loss", ",", "self", ".", "critic_train_op", "]", ",", "feed_dict", "=", "feeds", ")", "\n", "self", ".", "sess", ".", "run", "(", "self", ".", "soft_update", ")", "\n", "\n", "return", "actor_loss", ",", "critic_loss", ",", "summary", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.sac_discrete.DiscreteSoftAC.act": [[122, 127], ["sac_discrete.DiscreteSoftAC.sess.run"], "methods", ["None"], ["", "def", "act", "(", "self", ",", "state", ")", ":", "\n", "        ", "feeds", "=", "{", "self", ".", "states_pl", ":", "state", "}", "\n", "a", "=", "self", ".", "sess", ".", "run", "(", "self", ".", "action", ",", "feed_dict", "=", "feeds", ")", "\n", "\n", "return", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.sac_discrete.DiscreteSoftAC.save": [[128, 130], ["sac_discrete.DiscreteSoftAC.saver.save"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.save"], ["", "def", "save", "(", "self", ",", "epoch", ",", "path", "=", "\"saved_models/sac\"", ")", ":", "\n", "        ", "self", ".", "saver", ".", "save", "(", "self", ".", "sess", ",", "path", ",", "global_step", "=", "epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.sac_discrete.DiscreteSoftAC.restore": [[131, 133], ["sac_discrete.DiscreteSoftAC.saver.restore"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.restore"], ["", "def", "restore", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "saver", ".", "restore", "(", "self", ".", "sess", ",", "path", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.agent.Agent.__init__": [[16, 36], ["numpy.arange", "sac.SoftAC", "action_embed.ActionEmbed", "experience.Expericence", "agent.Agent.action_embed.get_embedding", "len", "tensorflow.summary.FileWriter"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.get_embedding"], ["    ", "def", "__init__", "(", "self", ",", "envs", ")", ":", "\n", "        ", "self", ".", "envs", "=", "envs", "\n", "self", ".", "task_ids", "=", "np", ".", "arange", "(", "len", "(", "envs", ")", ",", "dtype", "=", "int", ")", "\n", "\n", "self", ".", "noise_std", "=", "1", "\n", "self", ".", "noise_decay", "=", "0.99999", "\n", "self", ".", "noise_min", "=", "0.001", "\n", "\n", "self", ".", "policy_net", "=", "SoftAC", "(", "Config", ".", "state_dims", ",", "Config", ".", "state_embed_dim", ",", "Config", ".", "action_embed_dim", ",", "self", ".", "task_ids", ",", "Config", ".", "state_embed_hiddens", ",", "\n", "Config", ".", "ac_hiddens", ",", "Config", ".", "gamma", ",", "Config", ".", "actor_lr", ",", "Config", ".", "critic_lr", ",", "Config", ".", "tau", ",", "Config", ".", "alpha", ")", "\n", "\n", "self", ".", "action_embed", "=", "ActionEmbed", "(", "Config", ".", "state_embed_dim", ",", "Config", ".", "action_embed_dim", ",", "Config", ".", "action_dims", ",", "self", ".", "task_ids", ",", "\n", "Config", ".", "seq_len", ",", "Config", ".", "action_embed_hiddens", ",", "Config", ".", "cell_num", ",", "Config", ".", "action_embed_lr", ")", "\n", "\n", "self", ".", "experiences", "=", "Expericence", "(", "Config", ".", "state_dims", ",", "Config", ".", "action_embed_dim", ",", "Config", ".", "seq_len", ",", "self", ".", "task_ids", ",", "Config", ".", "memory_size", ")", "\n", "# initialize the embedding", "\n", "self", ".", "action_embeddings", "=", "self", ".", "action_embed", ".", "get_embedding", "(", ")", "\n", "self", ".", "writer", "=", "None", "\n", "if", "args", ".", "summary", ":", "\n", "            ", "self", ".", "writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "Config", ".", "summary_folder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.agent.Agent.choose_action": [[37, 46], ["agent.Agent.policy_net.act", "agent.Agent.nearest", "numpy.random.choice", "state.reshape().astype", "state.reshape"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.sac.SoftAC.act", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.agent.Agent.nearest"], ["", "", "def", "choose_action", "(", "self", ",", "state", ",", "task_id", ",", "random", "=", "False", ")", ":", "\n", "        ", "if", "random", ":", "\n", "            ", "a", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "envs", "[", "task_id", "]", ".", "actions", ")", "\n", "return", "a", ",", "self", ".", "action_embeddings", "[", "task_id", "]", "[", "a", "]", "\n", "", "a_hat", "=", "self", ".", "policy_net", ".", "act", "(", "state", ".", "reshape", "(", "1", ",", "-", "1", ")", ".", "astype", "(", "np", ".", "float32", ")", ",", "task_id", ")", "\n", "\n", "embed_mat", "=", "self", ".", "action_embeddings", "[", "task_id", "]", "\n", "a", "=", "self", ".", "nearest", "(", "a_hat", ",", "embed_mat", ")", "\n", "return", "a", ",", "a_hat", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.agent.Agent.nearest": [[47, 51], ["numpy.argmin", "sklearn.metrics.pairwise.pairwise_distances"], "methods", ["None"], ["", "def", "nearest", "(", "self", ",", "a", ",", "embedding", ")", ":", "\n", "        ", "distance", "=", "pairwise_distances", "(", "a", ",", "embedding", ")", "[", "0", "]", "\n", "closest", "=", "np", ".", "argmin", "(", "distance", ")", "\n", "return", "closest", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.agent.Agent.train_policy": [[52, 60], ["agent.Agent.experiences.sample", "agent.Agent.policy_net.train", "agent.Agent.writer.add_summary"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.sample", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.discrete_agent.DiscreteAgent.train"], ["", "def", "train_policy", "(", "self", ",", "epoch", ",", "task_id", ")", ":", "\n", "        ", "s", ",", "a", ",", "a_embed", ",", "n_s", ",", "r", ",", "d", "=", "self", ".", "experiences", ".", "sample", "(", "Config", ".", "batch_size", ",", "task_id", ")", "\n", "loss_act", ",", "loss_crt", ",", "summary", "=", "self", ".", "policy_net", ".", "train", "(", "epoch", ",", "s", ",", "a", ",", "a_embed", ",", "n_s", ",", "r", ",", "d", ",", "task_id", ")", "# self.action_embeddings[task_id][a]", "\n", "if", "self", ".", "writer", ":", "\n", "            ", "self", ".", "writer", ".", "add_summary", "(", "summary", ",", "global_step", "=", "epoch", ")", "\n", "\n", "# print('trainning policy. epoch {}: actor loss: {}, critic loss: {}'.format(epoch, loss_act, loss_crt))", "\n", "", "return", "loss_act", ",", "loss_crt", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.agent.Agent.train_embedding": [[61, 79], ["agent.Agent.experiences.sample_traj", "agent.Agent.policy_net.get_state_embedding", "state_embed.reshape.reshape.reshape", "agent.Agent.action_embed.train", "agent.Agent.action_embed.get_embedding", "states.reshape", "agent.Agent.writer.add_summary"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.sample_traj", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.sac.SoftAC.get_state_embedding", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.discrete_agent.DiscreteAgent.train", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.get_embedding"], ["", "def", "train_embedding", "(", "self", ",", "epoch", ",", "task_id", ")", ":", "\n", "        ", "states", ",", "actions", ",", "length", "=", "self", ".", "experiences", ".", "sample_traj", "(", "Config", ".", "action_batch_size", ",", "task_id", ")", "\n", "shape", "=", "states", ".", "shape", "\n", "state_embed", "=", "self", ".", "policy_net", ".", "get_state_embedding", "(", "states", ".", "reshape", "(", "-", "1", ",", "shape", "[", "-", "1", "]", ")", ",", "task_id", ")", "\n", "state_embed", "=", "state_embed", ".", "reshape", "(", "shape", "[", "0", "]", ",", "shape", "[", "1", "]", ",", "-", "1", ")", "\n", "\n", "# if no state embedding for same state space", "\n", "# state_embed = states", "\n", "loss", ",", "summary", "=", "self", ".", "action_embed", ".", "train", "(", "epoch", ",", "state_embed", ",", "actions", "[", ":", ",", ":", "-", "1", "]", ",", "length", ",", "task_id", ")", "\n", "\n", "# update current embeddings", "\n", "self", ".", "action_embeddings", "=", "self", ".", "action_embed", ".", "get_embedding", "(", ")", "\n", "\n", "if", "self", ".", "writer", ":", "\n", "            ", "self", ".", "writer", ".", "add_summary", "(", "summary", ",", "global_step", "=", "epoch", ")", "\n", "\n", "# print('trainning embedding. epoch {}: embedding loss: {}'.format(epoch, loss))", "\n", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.agent.Agent.train": [[80, 132], ["range", "os.path.exists", "os.makedirs", "numpy.random.choice", "env.reset", "agent.Agent.experiences.finish", "print", "rewards.append", "numpy.mean", "tensorflow.Summary", "env.get_state", "agent.Agent.choose_action", "env.step", "agent.Agent.experiences.store", "len", "rewards.pop", "agent.Agent.writer.add_summary", "agent.Agent.experiences.get_size", "agent.Agent.train_policy", "agent.Agent.experiences.get_traj_size", "agent.Agent.train_embedding", "agent.Agent.experiences.finish", "agent.Agent.policy_net.save", "agent.Agent.action_embed.save", "tensorflow.Summary.Value", "str", "str"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.reset", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.finish", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.get_state", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.discrete_agent.DiscreteAgent.choose_action", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.step", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.store", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.get_size", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.discrete_agent.DiscreteAgent.train_policy", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.get_traj_size", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.train_embedding", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.finish", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.save", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.save"], ["", "def", "train", "(", "self", ",", "tasks", "=", "(", "1", ",", ")", ")", ":", "\n", "        ", "global_step", "=", "0", "\n", "rewards", "=", "[", "]", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "'data/{}/'", ".", "format", "(", "args", ".", "i", ")", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "'data/{}/'", ".", "format", "(", "args", ".", "i", ")", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "Config", ".", "epoches", ")", ":", "\n", "# todo: may need to modify", "\n", "            ", "task_id", "=", "np", ".", "random", ".", "choice", "(", "tasks", ")", "\n", "env", "=", "self", ".", "envs", "[", "task_id", "]", "\n", "env", ".", "reset", "(", ")", "\n", "total_r", ",", "done", ",", "step", "=", "0", ",", "False", ",", "0", "\n", "while", "not", "done", "and", "step", "<", "Config", ".", "max_step", ":", "#  // (task_id + 1)", "\n", "                ", "obs", "=", "env", ".", "get_state", "(", ")", "\n", "a", ",", "a_hat", "=", "self", ".", "choose_action", "(", "obs", ",", "task_id", ")", "\n", "n_obs", ",", "r", ",", "done", ",", "_", "=", "env", ".", "step", "(", "a", ")", "\n", "# self.envs[task_id].render()", "\n", "total_r", "+=", "r", "\n", "step", "+=", "1", "\n", "\n", "self", ".", "experiences", ".", "store", "(", "obs", ",", "a", ",", "a_hat", ",", "n_obs", ",", "r", ",", "done", ",", "task_id", ")", "\n", "\n", "# train policy", "\n", "if", "self", ".", "experiences", ".", "get_size", "(", "task_id", ")", ">", "Config", ".", "batch_size", "*", "2", ":", "\n", "                    ", "self", ".", "train_policy", "(", "global_step", ",", "task_id", ")", "\n", "\n", "# train embedding", "\n", "", "if", "self", ".", "experiences", ".", "get_traj_size", "(", "task_id", ")", ">", "Config", ".", "action_batch_size", ":", "\n", "                    ", "embed_loss", "=", "self", ".", "train_embedding", "(", "global_step", ",", "task_id", ")", "\n", "\n", "# if reach the maximum length of trajectory in the replay memory", "\n", "", "if", "step", "%", "Config", ".", "seq_len", "==", "0", ":", "\n", "                    ", "self", ".", "experiences", ".", "finish", "(", "task_id", ")", "\n", "\n", "", "global_step", "+=", "1", "\n", "if", "global_step", "%", "1000", "==", "0", ":", "\n", "                    ", "self", ".", "policy_net", ".", "save", "(", "global_step", ",", "path", "=", "Config", ".", "model_save_folder", "+", "task_name", "+", "\"/\"", "+", "str", "(", "args", ".", "seed", ")", "+", "\"/sac\"", ")", "\n", "self", ".", "action_embed", ".", "save", "(", "global_step", ",", "path", "=", "Config", ".", "model_save_folder", "+", "task_name", "+", "\"/\"", "+", "str", "(", "args", ".", "seed", ")", "+", "\"/embedding\"", ")", "\n", "\n", "", "", "self", ".", "experiences", ".", "finish", "(", "task_id", ")", "\n", "print", "(", "'epoch: {}, task id: {}, steps: {} total reward: {}'", ".", "format", "(", "i", ",", "task_id", ",", "step", ",", "total_r", ")", ")", "\n", "\n", "rewards", ".", "append", "(", "total_r", ")", "\n", "if", "len", "(", "rewards", ")", ">", "100", ":", "\n", "                ", "rewards", ".", "pop", "(", "0", ")", "\n", "", "avg_reward", "=", "np", ".", "mean", "(", "rewards", ")", "\n", "\n", "summary", "=", "tf", ".", "Summary", "(", "value", "=", "[", "tf", ".", "Summary", ".", "Value", "(", "tag", "=", "\"reward1\"", ",", "simple_value", "=", "avg_reward", ")", "]", ")", "\n", "\n", "if", "self", ".", "writer", ":", "\n", "                ", "self", ".", "writer", ".", "add_summary", "(", "summary", ",", "global_step", "=", "global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.agent.Agent.transfer": [[133, 141], ["agent.Agent.action_embed.get_embedding", "agent.Agent.train", "agent.Agent.action_embed.restore", "agent.Agent.policy_net.restore"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.get_embedding", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.discrete_agent.DiscreteAgent.train", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.restore", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.restore"], ["", "", "", "def", "transfer", "(", "self", ",", "task_id", ",", "embedding_path", "=", "None", ",", "policy_path", "=", "None", ")", ":", "\n", "        ", "if", "embedding_path", ":", "\n", "            ", "self", ".", "action_embed", ".", "restore", "(", "embedding_path", ")", "\n", "", "if", "policy_path", ":", "\n", "            ", "self", ".", "policy_net", ".", "restore", "(", "policy_path", ")", "\n", "\n", "", "self", ".", "action_embeddings", "=", "self", ".", "action_embed", ".", "get_embedding", "(", ")", "\n", "self", ".", "train", "(", "[", "task_id", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.agent.parse": [[143, 154], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["", "", "def", "parse", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'-i'", ",", "default", "=", "'0'", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "'-t'", ",", "default", "=", "0", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'-seed'", ",", "default", "=", "0", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'-summary'", ",", "default", "=", "True", ",", "type", "=", "bool", ")", "\n", "parser", ".", "add_argument", "(", "'-ckpt_path'", ",", "type", "=", "str", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-ckpt_step'", ",", "type", "=", "str", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-source_t'", ",", "type", "=", "int", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-transfer'", ",", "default", "=", "0", ",", "type", "=", "int", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.ExperienceCell.__init__": [[5, 24], ["numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "state_dim", ",", "embed_dim", ",", "seq_len", ",", "mem_size", ")", ":", "\n", "        ", "self", ".", "states", "=", "np", ".", "zeros", "(", "(", "mem_size", ",", "state_dim", ")", ")", "\n", "self", ".", "actions", "=", "np", ".", "zeros", "(", "(", "mem_size", ",", ")", ",", "dtype", "=", "int", ")", "\n", "self", ".", "action_embeddings", "=", "np", ".", "zeros", "(", "(", "mem_size", ",", "embed_dim", ")", ")", "\n", "self", ".", "next_states", "=", "np", ".", "zeros", "(", "(", "mem_size", ",", "state_dim", ")", ")", "\n", "self", ".", "rewards", "=", "np", ".", "zeros", "(", "(", "mem_size", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "terminals", "=", "np", ".", "zeros", "(", "(", "mem_size", ",", ")", ",", "dtype", "=", "int", ")", "\n", "\n", "self", ".", "cur", "=", "0", "\n", "self", ".", "cur_size", "=", "0", "\n", "self", ".", "max_size", "=", "mem_size", "\n", "\n", "self", ".", "seq_max", "=", "mem_size", "//", "seq_len", "\n", "self", ".", "seq_states", "=", "np", ".", "zeros", "(", "(", "self", ".", "seq_max", ",", "seq_len", "+", "1", ",", "state_dim", ")", ")", "\n", "self", ".", "seq_actions", "=", "np", ".", "zeros", "(", "(", "self", ".", "seq_max", ",", "seq_len", "+", "1", ")", ",", "dtype", "=", "int", ")", "\n", "self", ".", "seq_lengths", "=", "np", ".", "zeros", "(", "self", ".", "seq_max", ",", "dtype", "=", "int", ")", "\n", "self", ".", "seq_cur", "=", "0", "\n", "self", ".", "seq_len_cur", "=", "0", "\n", "self", ".", "seq_cur_size", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.ExperienceCell.store": [[25, 34], ["None"], "methods", ["None"], ["", "def", "store", "(", "self", ",", "state", ",", "action", ",", "action_embedding", ",", "next_state", ",", "reward", ",", "done", ")", ":", "\n", "        ", "self", ".", "states", "[", "self", ".", "cur", "]", "=", "state", "\n", "self", ".", "actions", "[", "self", ".", "cur", "]", "=", "action", "\n", "self", ".", "action_embeddings", "[", "self", ".", "cur", "]", "=", "action_embedding", "\n", "self", ".", "next_states", "[", "self", ".", "cur", "]", "=", "next_state", "\n", "self", ".", "rewards", "[", "self", ".", "cur", "]", "=", "reward", "\n", "self", ".", "terminals", "[", "self", ".", "cur", "]", "=", "done", "\n", "self", ".", "cur", "=", "(", "self", ".", "cur", "+", "1", ")", "%", "self", ".", "max_size", "\n", "self", ".", "cur_size", "=", "self", ".", "cur_size", "+", "1", "if", "self", ".", "cur_size", "+", "1", "<", "self", ".", "max_size", "else", "self", ".", "max_size", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.ExperienceCell.store_traj": [[35, 39], ["None"], "methods", ["None"], ["", "def", "store_traj", "(", "self", ",", "state", ",", "action", ")", ":", "\n", "        ", "self", ".", "seq_states", "[", "self", ".", "seq_cur", "]", "[", "self", ".", "seq_len_cur", "]", "=", "state", "\n", "self", ".", "seq_actions", "[", "self", ".", "seq_cur", "]", "[", "self", ".", "seq_len_cur", "]", "=", "action", "\n", "self", ".", "seq_len_cur", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.ExperienceCell.finish": [[40, 45], ["None"], "methods", ["None"], ["", "def", "finish", "(", "self", ")", ":", "\n", "        ", "self", ".", "seq_lengths", "[", "self", ".", "seq_cur", "]", "=", "self", ".", "seq_len_cur", "\n", "self", ".", "seq_cur", "=", "(", "self", ".", "seq_cur", "+", "1", ")", "%", "self", ".", "seq_max", "\n", "self", ".", "seq_cur_size", "=", "self", ".", "seq_cur_size", "+", "1", "if", "self", ".", "seq_cur_size", "+", "1", "<", "self", ".", "seq_max", "else", "self", ".", "seq_max", "\n", "self", ".", "seq_len_cur", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.ExperienceCell.sample": [[46, 50], ["numpy.random.choice", "numpy.arange"], "methods", ["None"], ["", "def", "sample", "(", "self", ",", "n", ")", ":", "\n", "        ", "inds", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "self", ".", "cur_size", ")", ",", "n", ")", "\n", "return", "self", ".", "states", "[", "inds", "]", ",", "self", ".", "actions", "[", "inds", "]", ",", "self", ".", "action_embeddings", "[", "inds", "]", ",", "self", ".", "next_states", "[", "inds", "]", ",", "self", ".", "rewards", "[", "inds", "]", ",", "self", ".", "terminals", "[", "inds", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.ExperienceCell.sample_traj": [[51, 55], ["min", "numpy.random.choice", "numpy.arange"], "methods", ["None"], ["", "def", "sample_traj", "(", "self", ",", "n", ")", ":", "\n", "        ", "n", "=", "min", "(", "n", ",", "self", ".", "seq_cur_size", ")", "\n", "inds", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "self", ".", "seq_cur_size", ")", ",", "n", ")", "\n", "return", "self", ".", "seq_states", "[", "inds", "]", ",", "self", ".", "seq_actions", "[", "inds", "]", ",", "self", ".", "seq_lengths", "[", "inds", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.__init__": [[58, 60], ["experience.ExperienceCell"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "state_dims", ",", "embed_dim", ",", "seq_len", ",", "task_ids", ",", "mem_size", ")", ":", "\n", "        ", "self", ".", "cells", "=", "[", "ExperienceCell", "(", "state_dims", "[", "i", "]", ",", "embed_dim", ",", "seq_len", ",", "mem_size", ")", "for", "i", "in", "task_ids", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.sample": [[61, 63], ["experience.Expericence.cells[].sample"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.sample"], ["", "def", "sample", "(", "self", ",", "n", ",", "task_id", ")", ":", "\n", "        ", "return", "self", ".", "cells", "[", "task_id", "]", ".", "sample", "(", "n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.sample_traj": [[64, 66], ["experience.Expericence.cells[].sample_traj"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.sample_traj"], ["", "def", "sample_traj", "(", "self", ",", "n", ",", "task_id", ")", ":", "\n", "        ", "return", "self", ".", "cells", "[", "task_id", "]", ".", "sample_traj", "(", "n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.store": [[67, 70], ["experience.Expericence.cells[].store", "experience.Expericence.cells[].store_traj"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.store", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.ExperienceCell.store_traj"], ["", "def", "store", "(", "self", ",", "state", ",", "action", ",", "action_embedding", ",", "next_state", ",", "reward", ",", "done", ",", "task_id", ")", ":", "\n", "        ", "self", ".", "cells", "[", "task_id", "]", ".", "store", "(", "state", ",", "action", ",", "action_embedding", ",", "next_state", ",", "reward", ",", "done", ")", "\n", "self", ".", "cells", "[", "task_id", "]", ".", "store_traj", "(", "state", ",", "action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.finish": [[71, 73], ["experience.Expericence.cells[].finish"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.finish"], ["", "def", "finish", "(", "self", ",", "task_id", ")", ":", "\n", "        ", "self", ".", "cells", "[", "task_id", "]", ".", "finish", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.get_size": [[74, 76], ["None"], "methods", ["None"], ["", "def", "get_size", "(", "self", ",", "task_id", ")", ":", "\n", "        ", "return", "self", ".", "cells", "[", "task_id", "]", ".", "cur_size", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.get_traj_size": [[77, 79], ["None"], "methods", ["None"], ["", "def", "get_traj_size", "(", "self", ",", "task_id", ")", ":", "\n", "        ", "return", "self", ".", "cells", "[", "task_id", "]", ".", "seq_cur_size", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.sac.SoftAC.__init__": [[7, 25], ["sac.SoftAC._build", "tf.train.Saver", "tf.Session", "sac.SoftAC.sess.run", "sac.SoftAC.sess.run", "state_embed.StateEmbed", "tf.global_variables_initializer", "enumerate"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed._build"], ["    ", "def", "__init__", "(", "self", ",", "state_dims", ",", "state_embed_dim", ",", "action_embed_dim", ",", "task_ids", ",", "state_embed_hiddens", ",", "hiddens", ",", "\n", "gamma", "=", "0.99", ",", "actor_lr", "=", "0.00025", ",", "critic_lr", "=", "0.001", ",", "tau", "=", "0.999", ",", "alpha", "=", "0.2", ")", ":", "\n", "        ", "self", ".", "state_dims", "=", "state_dims", "\n", "self", ".", "state_embed_dim", "=", "state_embed_dim", "\n", "self", ".", "action_dim", "=", "action_embed_dim", "\n", "self", ".", "task_ids", "=", "task_ids", "\n", "self", ".", "hiddens", "=", "hiddens", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "tau", "=", "tau", "\n", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "actor_lr", ",", "self", ".", "critic_lr", "=", "actor_lr", ",", "critic_lr", "\n", "\n", "self", ".", "states_embed", "=", "[", "StateEmbed", "(", "dim", ",", "state_embed_dim", ",", "state_embed_hiddens", ",", "task_ids", "[", "i", "]", ")", "for", "i", ",", "dim", "in", "enumerate", "(", "state_dims", ")", "]", "\n", "self", ".", "_build", "(", ")", "\n", "self", ".", "saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "\n", "self", ".", "sess", "=", "tf", ".", "Session", "(", ")", "\n", "self", ".", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "self", ".", "sess", ".", "run", "(", "self", ".", "init_target", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.sac.SoftAC._build": [[26, 137], ["tf.placeholder", "tf.placeholder", "tf.placeholder", "tf.placeholder", "tf.placeholder", "tf.case", "tf.case", "tf.minimum", "tf.stop_gradient", "tf.stop_gradient", "tf.reduce_mean", "tf.train.AdamOptimizer", "tf.train.AdamOptimizer", "tf.train.AdamOptimizer.minimize", "tf.train.AdamOptimizer.minimize", "summaries.append", "summaries.append", "summaries.append", "summaries.append", "tf.summary.merge", "tf.placeholder", "tf.placeholder", "tf.variable_scope", "tf.squeeze", "tf.variable_scope", "tf.squeeze", "tf.variable_scope", "utils.mlp", "tf.layers.dense", "tf.layers.dense", "tf.exp", "utils.gaussian_likelihood", "tf.tanh", "tf.tanh", "tf.cast", "tf.cast", "tf.reduce_sum", "tf.variable_scope", "tf.squeeze", "tf.variable_scope", "tf.squeeze", "tf.variable_scope", "tf.squeeze", "tf.variable_scope", "tf.squeeze", "tf.reduce_mean", "tf.reduce_mean", "tf.reduce_mean", "tf.assign", "tf.assign", "tf.summary.scalar", "tf.summary.scalar", "tf.summary.histogram", "tf.summary.histogram", "utils.mlp1", "utils.mlp1", "tf.square", "tf.log", "utils.mlp1", "utils.mlp1", "utils.mlp1", "utils.mlp1", "tf.square", "tf.square", "tf.square", "zip", "zip", "tf.equal", "tf.equal", "tf.equal", "tf.equal", "tf.equal", "tf.equal", "tf.equal", "tf.equal", "tf.random_normal", "tf.concat", "tf.concat", "tf.concat", "tf.concat", "utils.get_vars", "utils.get_vars", "utils.get_vars", "utils.get_vars", "tf.shape", "tf.stop_gradient"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.mlp", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.gaussian_likelihood", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.mlp1", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.mlp1", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.mlp1", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.mlp1", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.mlp1", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.mlp1", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.get_vars", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.get_vars", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.get_vars", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.get_vars"], ["", "def", "_build", "(", "self", ")", ":", "\n", "        ", "self", ".", "states_pl", "=", "[", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "(", "None", ",", "dim", ")", ")", "for", "dim", "in", "self", ".", "state_dims", "]", "\n", "self", ".", "next_states_pl", "=", "[", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "(", "None", ",", "dim", ")", ")", "for", "dim", "in", "self", ".", "state_dims", "]", "\n", "self", ".", "a_pl", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "shape", "=", "(", "None", ",", ")", ")", "\n", "self", ".", "a_embed_pl", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "(", "None", ",", "self", ".", "action_dim", ")", ")", "\n", "self", ".", "r_pl", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "(", "None", ",", ")", ")", "\n", "self", ".", "done_pl", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "(", "None", ",", ")", ")", "\n", "self", ".", "task_id", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ")", "\n", "\n", "# todo: seems tf.case has bugs, cannot use for to generate the cases.", "\n", "# todo: Use eager execution to rewrite", "\n", "# self.states = tf.case([(tf.equal(self.task_id, i), lambda:self.states_embed[i](self.states_pl[i])) for i in self.task_ids])", "\n", "# self.next_state = tf.case([(tf.equal(self.task_id, i), lambda:self.states_embed[i](self.next_states_pl[i])) for i in self.task_ids])", "\n", "\n", "self", ".", "states", "=", "tf", ".", "case", "(", "[", "(", "tf", ".", "equal", "(", "self", ".", "task_id", ",", "0", ")", ",", "lambda", ":", "self", ".", "states_embed", "[", "0", "]", "(", "self", ".", "states_pl", "[", "0", "]", ")", ")", ",", "\n", "(", "tf", ".", "equal", "(", "self", ".", "task_id", ",", "1", ")", ",", "lambda", ":", "self", ".", "states_embed", "[", "1", "]", "(", "self", ".", "states_pl", "[", "1", "]", ")", ")", ",", "\n", "(", "tf", ".", "equal", "(", "self", ".", "task_id", ",", "2", ")", ",", "lambda", ":", "self", ".", "states_embed", "[", "2", "]", "(", "self", ".", "states_pl", "[", "2", "]", ")", ")", ",", "\n", "(", "tf", ".", "equal", "(", "self", ".", "task_id", ",", "3", ")", ",", "lambda", ":", "self", ".", "states_embed", "[", "3", "]", "(", "self", ".", "states_pl", "[", "3", "]", ")", ")", "\n", "]", ")", "\n", "next_state", "=", "tf", ".", "case", "(", "[", "(", "tf", ".", "equal", "(", "self", ".", "task_id", ",", "0", ")", ",", "lambda", ":", "self", ".", "states_embed", "[", "0", "]", "(", "self", ".", "next_states_pl", "[", "0", "]", ")", ")", ",", "\n", "(", "tf", ".", "equal", "(", "self", ".", "task_id", ",", "1", ")", ",", "lambda", ":", "self", ".", "states_embed", "[", "1", "]", "(", "self", ".", "next_states_pl", "[", "1", "]", ")", ")", ",", "\n", "(", "tf", ".", "equal", "(", "self", ".", "task_id", ",", "2", ")", ",", "lambda", ":", "self", ".", "states_embed", "[", "2", "]", "(", "self", ".", "next_states_pl", "[", "2", "]", ")", ")", ",", "\n", "(", "tf", ".", "equal", "(", "self", ".", "task_id", ",", "3", ")", ",", "lambda", ":", "self", ".", "states_embed", "[", "3", "]", "(", "self", ".", "next_states_pl", "[", "3", "]", ")", ")", "\n", "]", ")", "\n", "\n", "# if no state embedding", "\n", "# self.states = tf.case([(tf.equal(self.task_id, 0), lambda: self.states_pl[0]),", "\n", "#                        (tf.equal(self.task_id, 1), lambda: self.states_pl[1]),", "\n", "#                        (tf.equal(self.task_id, 2), lambda: self.states_pl[2])])", "\n", "# next_state = tf.case([(tf.equal(self.task_id, 0), lambda: self.next_states_pl[0]),", "\n", "#                       (tf.equal(self.task_id, 1), lambda: self.next_states_pl[1]),", "\n", "#                       (tf.equal(self.task_id, 2), lambda: self.next_states_pl[2])])", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'main_v'", ")", ":", "\n", "            ", "self", ".", "v", "=", "tf", ".", "squeeze", "(", "mlp1", "(", "self", ".", "states", ",", "self", ".", "hiddens", "+", "[", "1", "]", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", ",", "axis", "=", "1", ")", "\n", "", "with", "tf", ".", "variable_scope", "(", "'target_v'", ")", ":", "\n", "            ", "target_v", "=", "tf", ".", "squeeze", "(", "mlp1", "(", "next_state", ",", "self", ".", "hiddens", "+", "[", "1", "]", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", ",", "axis", "=", "1", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'policy'", ")", ":", "\n", "            ", "LOG_STDMIN", ",", "LOG_STDMAX", "=", "-", "20", ",", "2", "\n", "temp", "=", "mlp", "(", "self", ".", "states", ",", "self", ".", "hiddens", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "\n", "mu", "=", "tf", ".", "layers", ".", "dense", "(", "temp", ",", "self", ".", "action_dim", ",", "activation", "=", "None", ")", "\n", "log_std", "=", "tf", ".", "layers", ".", "dense", "(", "temp", ",", "self", ".", "action_dim", ",", "activation", "=", "tf", ".", "tanh", ")", "\n", "log_std", "=", "LOG_STDMIN", "+", "0.5", "*", "(", "LOG_STDMAX", "-", "LOG_STDMIN", ")", "*", "(", "log_std", "+", "1", ")", "\n", "\n", "std", "=", "tf", ".", "exp", "(", "log_std", ")", "\n", "pi", "=", "mu", "+", "tf", ".", "random_normal", "(", "tf", ".", "shape", "(", "mu", ")", ")", "*", "std", "\n", "logp_pi", "=", "gaussian_likelihood", "(", "pi", ",", "mu", ",", "log_std", ")", "\n", "\n", "mu", "=", "tf", ".", "tanh", "(", "mu", ")", "\n", "pi", "=", "tf", ".", "tanh", "(", "pi", ")", "\n", "\n", "temp", "=", "1", "-", "tf", ".", "square", "(", "pi", ")", "\n", "clip_up", "=", "tf", ".", "cast", "(", "temp", ">", "1", ",", "tf", ".", "float32", ")", "\n", "clip_down", "=", "tf", ".", "cast", "(", "temp", "<", "0", ",", "tf", ".", "float32", ")", "\n", "logp_pi", "-=", "tf", ".", "reduce_sum", "(", "tf", ".", "log", "(", "temp", "+", "tf", ".", "stop_gradient", "(", "clip_up", "*", "(", "1", "-", "temp", ")", "+", "clip_down", "*", "(", "0", "-", "temp", ")", ")", "+", "1e-6", ")", ",", "axis", "=", "1", ")", "\n", "\n", "self", ".", "mu", "=", "mu", "\n", "self", ".", "action", "=", "pi", "\n", "\n", "# if train alpha", "\n", "# with tf.variable_scope('alpha'):", "\n", "#     log_alpha = tf.Variable(0., trainable=True, name='log_alpha')", "\n", "#     self.alpha = tf.exp(log_alpha)", "\n", "#     alpha_loss = tf.reduce_mean(-log_alpha * tf.stop_gradient(logp_pi + 0.2))", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'q1'", ")", ":", "\n", "            ", "q1", "=", "tf", ".", "squeeze", "(", "mlp1", "(", "tf", ".", "concat", "(", "[", "self", ".", "states", ",", "self", ".", "a_embed_pl", "]", ",", "axis", "=", "-", "1", ")", ",", "self", ".", "hiddens", "+", "[", "1", "]", ",", "\n", "activation", "=", "tf", ".", "nn", ".", "relu", ")", ",", "axis", "=", "1", ")", "\n", "", "with", "tf", ".", "variable_scope", "(", "'q1'", ",", "reuse", "=", "True", ")", ":", "\n", "            ", "q1_pi", "=", "tf", ".", "squeeze", "(", "mlp1", "(", "tf", ".", "concat", "(", "[", "self", ".", "states", ",", "pi", "]", ",", "axis", "=", "-", "1", ")", ",", "self", ".", "hiddens", "+", "[", "1", "]", ",", "\n", "activation", "=", "tf", ".", "nn", ".", "relu", ")", ",", "axis", "=", "1", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'q2'", ")", ":", "\n", "            ", "q2", "=", "tf", ".", "squeeze", "(", "mlp1", "(", "tf", ".", "concat", "(", "[", "self", ".", "states", ",", "self", ".", "a_embed_pl", "]", ",", "axis", "=", "-", "1", ")", ",", "self", ".", "hiddens", "+", "[", "1", "]", ",", "\n", "activation", "=", "tf", ".", "nn", ".", "relu", ")", ",", "axis", "=", "1", ")", "\n", "", "with", "tf", ".", "variable_scope", "(", "'q2'", ",", "reuse", "=", "True", ")", ":", "\n", "            ", "q2_pi", "=", "tf", ".", "squeeze", "(", "mlp1", "(", "tf", ".", "concat", "(", "[", "self", ".", "states", ",", "pi", "]", ",", "axis", "=", "-", "1", ")", ",", "self", ".", "hiddens", "+", "[", "1", "]", ",", "\n", "activation", "=", "tf", ".", "nn", ".", "relu", ")", ",", "axis", "=", "1", ")", "\n", "\n", "", "q_", "=", "tf", ".", "minimum", "(", "q1_pi", ",", "q2_pi", ")", "\n", "v_", "=", "tf", ".", "stop_gradient", "(", "q_", "-", "self", ".", "alpha", "*", "logp_pi", ")", "\n", "target_q", "=", "tf", ".", "stop_gradient", "(", "self", ".", "r_pl", "+", "self", ".", "gamma", "*", "(", "1", "-", "self", ".", "done_pl", ")", "*", "target_v", ")", "\n", "\n", "v_loss", "=", "0.5", "*", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "v_", "-", "self", ".", "v", ")", ")", "\n", "self", ".", "actor_loss", "=", "tf", ".", "reduce_mean", "(", "self", ".", "alpha", "*", "logp_pi", "-", "q1_pi", ")", "\n", "q1_loss", "=", "0.5", "*", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "q1", "-", "target_q", ")", ")", "\n", "q2_loss", "=", "0.5", "*", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "q2", "-", "target_q", ")", ")", "\n", "self", ".", "critic_loss", "=", "v_loss", "+", "q1_loss", "+", "q2_loss", "\n", "\n", "actor_opt", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "self", ".", "actor_lr", ")", "\n", "critic_opt", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "self", ".", "critic_lr", ")", "\n", "# alpha_opt = tf.train.AdamOptimizer(self.critic_lr)", "\n", "\n", "# self.alpha_train_op = alpha_opt.minimize(alpha_loss)", "\n", "self", ".", "actor_train_op", "=", "actor_opt", ".", "minimize", "(", "self", ".", "actor_loss", ")", "# , var_list=get_vars('policy') + get_vars('state_embed')", "\n", "\n", "self", ".", "critic_train_op", "=", "critic_opt", ".", "minimize", "(", "self", ".", "critic_loss", ")", "# , var_list=get_vars('main_v') + get_vars('q') + get_vars('state_embed')", "\n", "\n", "self", ".", "soft_update", "=", "[", "tf", ".", "assign", "(", "v_targ", ",", "self", ".", "tau", "*", "v_targ", "+", "(", "1", "-", "self", ".", "tau", ")", "*", "v", ")", "\n", "for", "v", ",", "v_targ", "in", "zip", "(", "get_vars", "(", "'main_v'", ")", ",", "get_vars", "(", "'target_v'", ")", ")", "]", "\n", "\n", "self", ".", "init_target", "=", "[", "tf", ".", "assign", "(", "v_targ", ",", "v", ")", "for", "v", ",", "v_targ", "in", "zip", "(", "get_vars", "(", "'main_v'", ")", ",", "get_vars", "(", "'target_v'", ")", ")", "]", "\n", "\n", "summaries", "=", "[", "]", "\n", "summaries", ".", "append", "(", "tf", ".", "summary", ".", "scalar", "(", "'critic_loss'", ",", "self", ".", "critic_loss", ")", ")", "\n", "summaries", ".", "append", "(", "tf", ".", "summary", ".", "scalar", "(", "'actor_loss'", ",", "self", ".", "actor_loss", ")", ")", "\n", "summaries", ".", "append", "(", "tf", ".", "summary", ".", "histogram", "(", "'actions'", ",", "self", ".", "action", ")", ")", "\n", "summaries", ".", "append", "(", "tf", ".", "summary", ".", "histogram", "(", "'state_embedding'", ",", "self", ".", "states", ")", ")", "\n", "self", ".", "summary_op", "=", "tf", ".", "summary", ".", "merge", "(", "summaries", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.sac.SoftAC.train": [[138, 153], ["enumerate", "sac.SoftAC.sess.run", "sac.SoftAC.sess.run", "sac.SoftAC.sess.run", "np.zeros", "np.zeros"], "methods", ["None"], ["", "def", "train", "(", "self", ",", "step", ",", "state", ",", "action", ",", "action_embed", ",", "next_state", ",", "reward", ",", "done", ",", "task_id", ")", ":", "\n", "        ", "feeds", "=", "{", "self", ".", "states_pl", "[", "task_id", "]", ":", "state", ",", "self", ".", "a_pl", ":", "action", ",", "self", ".", "a_embed_pl", ":", "action_embed", ",", "\n", "self", ".", "next_states_pl", "[", "task_id", "]", ":", "next_state", ",", "self", ".", "r_pl", ":", "reward", ",", "self", ".", "done_pl", ":", "done", ",", "self", ".", "task_id", ":", "task_id", "}", "\n", "\n", "# add data for not used placeholder in states, or it will report error", "\n", "for", "i", ",", "dim", "in", "enumerate", "(", "self", ".", "state_dims", ")", ":", "\n", "            ", "if", "i", "!=", "task_id", ":", "\n", "                ", "feeds", "[", "self", ".", "states_pl", "[", "i", "]", "]", "=", "np", ".", "zeros", "(", "(", "0", ",", "dim", ")", ")", "\n", "feeds", "[", "self", ".", "next_states_pl", "[", "i", "]", "]", "=", "np", ".", "zeros", "(", "(", "0", ",", "dim", ")", ")", "\n", "\n", "# self.sess.run(self.alpha_train_op, feed_dict=feeds)", "\n", "", "", "actor_loss", ",", "_", "=", "self", ".", "sess", ".", "run", "(", "[", "self", ".", "actor_loss", ",", "self", ".", "actor_train_op", "]", ",", "feed_dict", "=", "feeds", ")", "\n", "summary", ",", "critic_loss", ",", "_", "=", "self", ".", "sess", ".", "run", "(", "[", "self", ".", "summary_op", ",", "self", ".", "critic_loss", ",", "self", ".", "critic_train_op", "]", ",", "feed_dict", "=", "feeds", ")", "\n", "self", ".", "sess", ".", "run", "(", "self", ".", "soft_update", ")", "\n", "return", "actor_loss", ",", "critic_loss", ",", "summary", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.sac.SoftAC.get_state_embedding": [[154, 162], ["enumerate", "sac.SoftAC.sess.run", "np.zeros"], "methods", ["None"], ["", "def", "get_state_embedding", "(", "self", ",", "state", ",", "task_id", ")", ":", "\n", "# process state", "\n", "        ", "feeds", "=", "{", "self", ".", "states_pl", "[", "task_id", "]", ":", "state", ",", "self", ".", "task_id", ":", "task_id", "}", "\n", "for", "i", ",", "dim", "in", "enumerate", "(", "self", ".", "state_dims", ")", ":", "\n", "            ", "if", "i", "!=", "task_id", ":", "\n", "                ", "feeds", "[", "self", ".", "states_pl", "[", "i", "]", "]", "=", "np", ".", "zeros", "(", "(", "0", ",", "dim", ")", ")", "\n", "", "", "embed", "=", "self", ".", "sess", ".", "run", "(", "self", ".", "states", ",", "feed_dict", "=", "feeds", ")", "\n", "return", "embed", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.sac.SoftAC.act": [[163, 169], ["enumerate", "sac.SoftAC.sess.run", "np.zeros"], "methods", ["None"], ["", "def", "act", "(", "self", ",", "state", ",", "task_id", ")", ":", "\n", "        ", "feeds", "=", "{", "self", ".", "states_pl", "[", "task_id", "]", ":", "state", ",", "self", ".", "task_id", ":", "task_id", "}", "\n", "for", "i", ",", "dim", "in", "enumerate", "(", "self", ".", "state_dims", ")", ":", "\n", "            ", "if", "i", "!=", "task_id", ":", "\n", "                ", "feeds", "[", "self", ".", "states_pl", "[", "i", "]", "]", "=", "np", ".", "zeros", "(", "(", "0", ",", "dim", ")", ")", "\n", "", "", "return", "self", ".", "sess", ".", "run", "(", "self", ".", "action", ",", "feed_dict", "=", "feeds", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.sac.SoftAC.save": [[170, 172], ["sac.SoftAC.saver.save"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.save"], ["", "def", "save", "(", "self", ",", "epoch", ",", "path", "=", "\"saved_models/sac\"", ")", ":", "\n", "        ", "self", ".", "saver", ".", "save", "(", "self", ".", "sess", ",", "path", ",", "global_step", "=", "epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.sac.SoftAC.restore": [[173, 175], ["sac.SoftAC.saver.restore"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.restore"], ["", "def", "restore", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "saver", ".", "restore", "(", "self", ".", "sess", ",", "path", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.__init__": [[6, 21], ["action_embed.ActionEmbed._build", "tensorflow.train.Saver", "tensorflow.Session", "action_embed.ActionEmbed.sess.run", "tensorflow.global_variables_initializer"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed._build"], ["    ", "def", "__init__", "(", "self", ",", "state_embed_dim", ",", "act_embed_dim", ",", "action_nums", ",", "task_ids", ",", "seq_len", ",", "hiddens", ",", "cell_num", ",", "\n", "lr", "=", "0.001", ")", ":", "\n", "        ", "self", ".", "state_dim", "=", "state_embed_dim", "\n", "self", ".", "act_dim", "=", "act_embed_dim", "\n", "self", ".", "action_nums", "=", "action_nums", "\n", "self", ".", "task_ids", "=", "task_ids", "\n", "self", ".", "seq_len", "=", "seq_len", "\n", "self", ".", "hiddens", "=", "hiddens", "\n", "self", ".", "cell_num", "=", "cell_num", "\n", "self", ".", "lr", "=", "lr", "\n", "\n", "self", ".", "_build", "(", ")", "\n", "self", ".", "saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "\n", "self", ".", "sess", "=", "tf", ".", "Session", "(", ")", "\n", "self", ".", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed._build": [[22, 65], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.case", "tensorflow.nn.embedding_lookup", "tensorflow.concat", "tensorflow.nn.rnn_cell.BasicLSTMCell", "tensorflow.nn.dynamic_rnn", "tensorflow.reshape", "utils.mlp", "tensorflow.layers.dense", "tensorflow.reshape", "tensorflow.reduce_mean", "tensorflow.train.AdamOptimizer", "tensorflow.train.AdamOptimizer.minimize", "tensorflow.train.AdamOptimizer.minimize", "summaries.append", "tensorflow.summary.merge", "tensorflow.variable_scope", "tensorflow.reduce_sum", "tensorflow.summary.histogram", "tensorflow.summary.scalar", "tensorflow.get_variable", "tensorflow.square", "utils.get_vars", "range", "enumerate", "tensorflow.equal", "tensorflow.equal", "tensorflow.equal", "tensorflow.equal", "len"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.mlp", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.get_vars"], ["", "def", "_build", "(", "self", ")", ":", "\n", "        ", "self", ".", "state_pl", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "(", "None", ",", "self", ".", "seq_len", "+", "1", ",", "self", ".", "state_dim", ")", ")", "\n", "self", ".", "a_pl", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "shape", "=", "(", "None", ",", "self", ".", "seq_len", ")", ")", "\n", "self", ".", "length_pl", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "shape", "=", "(", "None", ",", ")", ")", "\n", "self", ".", "task_id", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ")", "\n", "self", ".", "all_pl", "=", "[", "self", ".", "state_pl", ",", "self", ".", "a_pl", ",", "self", ".", "length_pl", ",", "self", ".", "task_id", "]", "\n", "with", "tf", ".", "variable_scope", "(", "'action_embedding'", ")", ":", "\n", "            ", "self", ".", "embeddings", "=", "[", "tf", ".", "get_variable", "(", "'embedding_{}'", ".", "format", "(", "self", ".", "task_ids", "[", "i", "]", ")", ",", "shape", "=", "(", "n", ",", "self", ".", "act_dim", ")", ",", "\n", "initializer", "=", "tf", ".", "orthogonal_initializer", ")", "for", "i", ",", "n", "in", "enumerate", "(", "self", ".", "action_nums", ")", "]", "\n", "\n", "", "states", "=", "self", ".", "state_pl", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "targets", "=", "self", ".", "state_pl", "[", ":", ",", "1", ":", ",", ":", "]", "\n", "\n", "# todo: seems tf.case has bugs, cannot use for to generate the cases.", "\n", "# todo: Use eager execution to rewrite", "\n", "# embedding_mat = tf.case([(tf.equal(self.task_id, i), lambda:self.embeddings[i]) for i in self.task_ids])", "\n", "embedding_mat", "=", "tf", ".", "case", "(", "[", "(", "tf", ".", "equal", "(", "self", ".", "task_id", ",", "0", ")", ",", "lambda", ":", "self", ".", "embeddings", "[", "0", "]", ")", ",", "\n", "(", "tf", ".", "equal", "(", "self", ".", "task_id", ",", "1", ")", ",", "lambda", ":", "self", ".", "embeddings", "[", "1", "]", ")", ",", "\n", "(", "tf", ".", "equal", "(", "self", ".", "task_id", ",", "2", ")", ",", "lambda", ":", "self", ".", "embeddings", "[", "2", "]", ")", ",", "\n", "(", "tf", ".", "equal", "(", "self", ".", "task_id", ",", "3", ")", ",", "lambda", ":", "self", ".", "embeddings", "[", "3", "]", ")", "\n", "]", ")", "\n", "\n", "act_embed", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embedding_mat", ",", "self", ".", "a_pl", ")", "\n", "\n", "inputs", "=", "tf", ".", "concat", "(", "[", "states", ",", "act_embed", "]", ",", "axis", "=", "2", ")", "\n", "\n", "rnncell", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "BasicLSTMCell", "(", "self", ".", "cell_num", ")", "\n", "# rnn = tf.nn.rnn_cell.DropoutWrapper(rnncell, output_keep_prob=0.9)", "\n", "outputs", ",", "s", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "rnncell", ",", "inputs", ",", "self", ".", "length_pl", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "x", "=", "tf", ".", "reshape", "(", "outputs", ",", "(", "-", "1", ",", "self", ".", "cell_num", ")", ")", "\n", "x", "=", "mlp", "(", "x", ",", "self", ".", "hiddens", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "x", "=", "tf", ".", "layers", ".", "dense", "(", "x", ",", "self", ".", "state_dim", ",", "activation", "=", "None", ")", "\n", "\n", "targets", "=", "tf", ".", "reshape", "(", "targets", ",", "(", "-", "1", ",", "self", ".", "state_dim", ")", ")", "\n", "self", ".", "loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "reduce_sum", "(", "tf", ".", "square", "(", "x", "-", "targets", ")", ",", "axis", "=", "1", ")", ")", "\n", "optimizers", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "self", ".", "lr", ")", "\n", "self", ".", "train_all_op", "=", "optimizers", ".", "minimize", "(", "self", ".", "loss", ")", "\n", "self", ".", "train_embed_op", "=", "optimizers", ".", "minimize", "(", "self", ".", "loss", ",", "var_list", "=", "get_vars", "(", "'action_embedding'", ")", ")", "\n", "\n", "summaries", "=", "[", "tf", ".", "summary", ".", "histogram", "(", "'embedding_{}'", ".", "format", "(", "i", ")", ",", "self", ".", "embeddings", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "self", ".", "action_nums", ")", ")", "]", "\n", "summaries", ".", "append", "(", "tf", ".", "summary", ".", "scalar", "(", "'embedding_loss'", ",", "self", ".", "loss", ")", ")", "\n", "self", ".", "summary_op", "=", "tf", ".", "summary", ".", "merge", "(", "summaries", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.train": [[66, 70], ["action_embed.ActionEmbed.sess.run", "zip"], "methods", ["None"], ["", "def", "train", "(", "self", ",", "step", ",", "states", ",", "actions", ",", "length", ",", "task_id", ")", ":", "\n", "        ", "feeds", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "zip", "(", "self", ".", "all_pl", ",", "[", "states", ",", "actions", ",", "length", ",", "task_id", "]", ")", "}", "\n", "loss", ",", "summary", ",", "_", "=", "self", ".", "sess", ".", "run", "(", "[", "self", ".", "loss", ",", "self", ".", "summary_op", ",", "self", ".", "train_all_op", "]", ",", "feed_dict", "=", "feeds", ")", "\n", "return", "loss", ",", "summary", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.train_embedding": [[71, 74], ["action_embed.ActionEmbed.sess.run", "zip"], "methods", ["None"], ["", "def", "train_embedding", "(", "self", ",", "step", ",", "states", ",", "actions", ",", "length", ",", "task_id", ")", ":", "\n", "        ", "feeds", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "zip", "(", "self", ".", "all_pl", ",", "[", "states", ",", "actions", ",", "length", ",", "task_id", "]", ")", "}", "\n", "self", ".", "sess", ".", "run", "(", "self", ".", "train_embed_op", ",", "feed_dict", "=", "feeds", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.get_embedding": [[75, 77], ["action_embed.ActionEmbed.sess.run"], "methods", ["None"], ["", "def", "get_embedding", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "sess", ".", "run", "(", "self", ".", "embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.save": [[78, 80], ["action_embed.ActionEmbed.saver.save"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.save"], ["", "def", "save", "(", "self", ",", "epoch", ",", "path", "=", "\"saved_models/action_embedding\"", ")", ":", "\n", "        ", "self", ".", "saver", ".", "save", "(", "self", ".", "sess", ",", "path", ",", "global_step", "=", "epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.restore": [[81, 83], ["action_embed.ActionEmbed.saver.restore"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.restore"], ["", "def", "restore", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "saver", ".", "restore", "(", "self", ".", "sess", ",", "path", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.state_embed.StateEmbed.__init__": [[5, 10], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "state_dim", ",", "embed_dim", ",", "hiddens", ",", "task_id", ")", ":", "\n", "        ", "self", ".", "state_dim", "=", "state_dim", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "hiddens", "=", "hiddens", "\n", "self", ".", "task_id", "=", "task_id", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.state_embed.StateEmbed.forward": [[11, 18], ["tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.layers.dense"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "'state_embed_{}'", ".", "format", "(", "self", ".", "task_id", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "            ", "for", "h", "in", "self", ".", "hiddens", ":", "\n", "                ", "x", "=", "tf", ".", "layers", ".", "dense", "(", "x", ",", "h", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "", "x", "=", "tf", ".", "layers", ".", "dense", "(", "x", ",", "self", ".", "embed_dim", ")", "\n", "# x = tf.contrib.layers.layer_norm(x)", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.state_embed.StateEmbed.__call__": [[19, 21], ["state_embed.StateEmbed.forward"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.state_embed.StateEmbed.forward"], ["", "def", "__call__", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "forward", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.discrete_agent.DiscreteAgent.__init__": [[14, 25], ["sac_discrete.DiscreteSoftAC", "experience.ExperienceCell", "tensorflow.summary.FileWriter"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "state_dim", ",", "act_dim", ")", ":", "\n", "        ", "self", ".", "env", "=", "env", "\n", "self", ".", "state_dim", "=", "state_dim", "\n", "self", ".", "act_dim", "=", "act_dim", "\n", "self", ".", "policy_net", "=", "DiscreteSoftAC", "(", "state_dim", ",", "act_dim", ",", "Config", ".", "ac_hiddens", ",", "Config", ".", "state_embed_hiddens", ",", "Config", ".", "gamma", ",", "Config", ".", "actor_lr", ",", "\n", "Config", ".", "critic_lr", ",", "Config", ".", "tau", ",", "Config", ".", "alpha", ")", "\n", "\n", "self", ".", "experience", "=", "ExperienceCell", "(", "state_dim", ",", "1", ",", "Config", ".", "seq_len", ",", "Config", ".", "memory_size", ")", "\n", "self", ".", "writer", "=", "None", "\n", "if", "args", ".", "summary", ":", "\n", "            ", "self", ".", "writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "Config", ".", "summary_folder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.discrete_agent.DiscreteAgent.choose_action": [[26, 32], ["numpy.random.choice", "discrete_agent.DiscreteAgent.policy_net.act", "state.reshape().astype", "state.reshape"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.sac.SoftAC.act"], ["", "", "def", "choose_action", "(", "self", ",", "state", ",", "determinitic", "=", "False", ",", "random", "=", "False", ")", ":", "\n", "        ", "if", "random", ":", "\n", "            ", "a", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "env", ".", "actions", ")", "\n", "", "else", ":", "\n", "            ", "a", "=", "self", ".", "policy_net", ".", "act", "(", "state", ".", "reshape", "(", "1", ",", "-", "1", ")", ".", "astype", "(", "np", ".", "float32", ")", ")", "[", "0", "]", "\n", "", "return", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.discrete_agent.DiscreteAgent.train_policy": [[33, 39], ["discrete_agent.DiscreteAgent.experience.sample", "discrete_agent.DiscreteAgent.policy_net.train", "discrete_agent.DiscreteAgent.writer.add_summary"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.sample", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.discrete_agent.DiscreteAgent.train"], ["", "def", "train_policy", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "s", ",", "a", ",", "a_embed", ",", "n_s", ",", "r", ",", "d", "=", "self", ".", "experience", ".", "sample", "(", "Config", ".", "batch_size", ")", "\n", "loss_act", ",", "loss_crt", ",", "summary", "=", "self", ".", "policy_net", ".", "train", "(", "epoch", ",", "s", ",", "a", ",", "n_s", ",", "r", ",", "d", ")", "\n", "if", "self", ".", "writer", ":", "\n", "            ", "self", ".", "writer", ".", "add_summary", "(", "summary", ",", "global_step", "=", "epoch", ")", "\n", "", "return", "loss_act", ",", "loss_crt", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.discrete_agent.DiscreteAgent.train": [[40, 71], ["range", "env.reset", "discrete_agent.DiscreteAgent.experience.finish", "print", "rewards.append", "numpy.mean", "env.get_state", "discrete_agent.DiscreteAgent.choose_action", "env.step", "discrete_agent.DiscreteAgent.experience.store", "len", "rewards.pop", "tensorflow.Summary", "discrete_agent.DiscreteAgent.writer.add_summary", "numpy.zeros", "discrete_agent.DiscreteAgent.train_policy", "discrete_agent.DiscreteAgent.experience.finish", "tensorflow.Summary.Value"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.reset", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.finish", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.get_state", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.discrete_agent.DiscreteAgent.choose_action", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.step", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.store", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.discrete_agent.DiscreteAgent.train_policy", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.experience.Expericence.finish"], ["", "def", "train", "(", "self", ")", ":", "\n", "        ", "global_step", "=", "0", "\n", "rewards", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "Config", ".", "epoches", ")", ":", "\n", "            ", "env", "=", "self", ".", "env", "\n", "env", ".", "reset", "(", ")", "\n", "total_r", ",", "done", ",", "step", "=", "0", ",", "False", ",", "0", "\n", "while", "not", "done", "and", "step", "<", "Config", ".", "max_step", ":", "#  // (task_id + 1)", "\n", "                ", "obs", "=", "env", ".", "get_state", "(", ")", "\n", "a", "=", "self", ".", "choose_action", "(", "obs", ")", "\n", "n_obs", ",", "r", ",", "done", ",", "_", "=", "env", ".", "step", "(", "a", ")", "\n", "total_r", "+=", "r", "\n", "step", "+=", "1", "\n", "self", ".", "experience", ".", "store", "(", "obs", ",", "a", ",", "np", ".", "zeros", "(", "1", ")", ",", "n_obs", ",", "r", ",", "done", ")", "\n", "# self.envs[task_id].render()", "\n", "\n", "if", "self", ".", "experience", ".", "cur_size", ">", "Config", ".", "batch_size", "*", "2", ":", "\n", "                    ", "self", ".", "train_policy", "(", "global_step", ")", "\n", "", "if", "step", "%", "(", "Config", ".", "seq_len", "+", "1", ")", "==", "0", ":", "\n", "                    ", "self", ".", "experience", ".", "finish", "(", ")", "\n", "", "global_step", "+=", "1", "\n", "", "self", ".", "experience", ".", "finish", "(", ")", "\n", "print", "(", "'epoch: {}, steps: {} total reward: {}'", ".", "format", "(", "i", ",", "step", ",", "total_r", ")", ")", "\n", "rewards", ".", "append", "(", "total_r", ")", "\n", "if", "len", "(", "rewards", ")", ">", "100", ":", "\n", "                ", "rewards", ".", "pop", "(", "0", ")", "\n", "", "avg_reward", "=", "np", ".", "mean", "(", "rewards", ")", "\n", "\n", "if", "self", ".", "writer", ":", "\n", "                ", "summary", "=", "tf", ".", "Summary", "(", "value", "=", "[", "tf", ".", "Summary", ".", "Value", "(", "tag", "=", "\"reward1\"", ",", "simple_value", "=", "avg_reward", ")", "]", ")", "\n", "self", ".", "writer", ".", "add_summary", "(", "summary", ",", "global_step", "=", "global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.discrete_agent.parse": [[72, 79], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["", "", "", "", "def", "parse", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'-i'", ",", "default", "=", "'0'", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "'-t'", ",", "default", "=", "'0'", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'-seed'", ",", "default", "=", "0", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'-summary'", ",", "default", "=", "False", ",", "type", "=", "bool", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.discrete_agent.save_rewards": [[81, 85], ["numpy.save", "os.path.exists", "os.makedirs", "str"], "function", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.action_embed.ActionEmbed.save"], ["", "def", "save_rewards", "(", "path", ",", "rewards", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "path", ")", "\n", "", "np", ".", "save", "(", "path", "+", "\"rewards_\"", "+", "str", "(", "args", ".", "seed", ")", ",", "rewards", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.mlp": [[8, 12], ["tensorflow.layers.dense"], "function", ["None"], ["def", "mlp", "(", "x", ",", "hiddens", ",", "activation", "=", "None", ",", "reguliazer", "=", "None", ")", ":", "\n", "    ", "for", "h", "in", "hiddens", ":", "\n", "        ", "x", "=", "tf", ".", "layers", ".", "dense", "(", "x", ",", "h", ",", "activation", "=", "activation", ",", "kernel_regularizer", "=", "reguliazer", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.mlp1": [[14, 19], ["tensorflow.layers.dense", "tensorflow.layers.dense"], "function", ["None"], ["", "def", "mlp1", "(", "x", ",", "hiddens", ",", "activation", "=", "None", ",", "output_activation", "=", "None", ",", "reguliazer", "=", "None", ")", ":", "\n", "    ", "for", "h", "in", "hiddens", "[", ":", "-", "1", "]", ":", "\n", "        ", "x", "=", "tf", ".", "layers", ".", "dense", "(", "x", ",", "h", ",", "activation", "=", "activation", ",", "kernel_regularizer", "=", "reguliazer", ")", "\n", "", "x", "=", "tf", ".", "layers", ".", "dense", "(", "x", ",", "hiddens", "[", "-", "1", "]", ",", "activation", "=", "output_activation", ",", "kernel_regularizer", "=", "reguliazer", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.gaussian_likelihood": [[21, 24], ["tensorflow.reduce_sum", "numpy.log", "tensorflow.exp"], "function", ["None"], ["", "def", "gaussian_likelihood", "(", "x", ",", "mu", ",", "log_std", ")", ":", "\n", "    ", "pre_sum", "=", "-", "0.5", "*", "(", "(", "(", "x", "-", "mu", ")", "/", "(", "tf", ".", "exp", "(", "log_std", ")", "+", "EPS", ")", ")", "**", "2", "+", "2", "*", "log_std", "+", "np", ".", "log", "(", "2", "*", "np", ".", "pi", ")", ")", "\n", "return", "tf", ".", "reduce_sum", "(", "pre_sum", ",", "axis", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.None.utils.get_vars": [[26, 28], ["tensorflow.global_variables"], "function", ["None"], ["", "def", "get_vars", "(", "scope", ")", ":", "\n", "    ", "return", "[", "x", "for", "x", "in", "tf", ".", "global_variables", "(", ")", "if", "scope", "in", "x", ".", "name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.grid_world.GridWorld.__init__": [[20, 39], ["numpy.arange", "list", "enumerate", "numpy.zeros", "numpy.zeros", "numpy.zeros", "itertools.product", "numpy.zeros", "len", "range"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "seq_len", "=", "2", ",", "size", "=", "5", ")", ":", "\n", "        ", "self", ".", "seq_len", "=", "seq_len", "\n", "self", ".", "actions", "=", "np", ".", "arange", "(", "len", "(", "self", ".", "basic_actions", ")", "**", "seq_len", ")", "\n", "self", ".", "all_actions", "=", "list", "(", "product", "(", "*", "[", "self", ".", "basic_actions", "for", "_", "in", "range", "(", "seq_len", ")", "]", ")", ")", "\n", "self", ".", "action_reps", "=", "{", "}", "\n", "self", ".", "action_effects", "=", "{", "}", "\n", "for", "i", ",", "v", "in", "enumerate", "(", "self", ".", "all_actions", ")", ":", "\n", "            ", "rep", "=", "\"\"", "\n", "effect", "=", "np", ".", "zeros", "(", "2", ")", "\n", "for", "d", "in", "v", ":", "\n", "                ", "rep", "+=", "self", ".", "basic_action_reps", "[", "d", "]", "\n", "effect", "+=", "self", ".", "basic_effects", "[", "d", "]", "\n", "", "self", ".", "action_reps", "[", "i", "]", "=", "rep", "\n", "self", ".", "action_effects", "[", "i", "]", "=", "effect", "\n", "\n", "", "self", ".", "size", "=", "size", "\n", "self", ".", "grid", "=", "np", ".", "zeros", "(", "(", "size", ",", "size", ")", ",", "dtype", "=", "np", ".", "float", ")", "\n", "self", ".", "position", "=", "np", ".", "zeros", "(", "2", ",", "dtype", "=", "np", ".", "float", ")", "\n", "self", ".", "goal", "=", "np", ".", "zeros", "(", "2", ",", "dtype", "=", "np", ".", "float", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.grid_world.GridWorld.reset": [[40, 45], ["numpy.random.random_integers().astype", "numpy.random.random_integers().astype", "all", "numpy.random.random_integers().astype", "numpy.random.random_integers", "numpy.random.random_integers", "numpy.random.random_integers"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "position", "=", "np", ".", "random", ".", "random_integers", "(", "-", "self", ".", "size", "//", "2", ",", "self", ".", "size", "//", "2", ",", "2", ")", ".", "astype", "(", "np", ".", "float", ")", "\n", "self", ".", "goal", "=", "np", ".", "random", ".", "random_integers", "(", "-", "self", ".", "size", ",", "self", ".", "size", ",", "2", ")", ".", "astype", "(", "np", ".", "float", ")", "\n", "while", "all", "(", "self", ".", "goal", "==", "self", ".", "position", ")", ":", "\n", "            ", "self", ".", "goal", "=", "np", ".", "random", ".", "random_integers", "(", "-", "self", ".", "size", ",", "self", ".", "size", ",", "2", ")", ".", "astype", "(", "np", ".", "float", ")", "\n", "# self.goal = np.array([self.size, self.size])", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.grid_world.GridWorld.step_one": [[47, 58], ["None"], "methods", ["None"], ["", "", "def", "step_one", "(", "self", ",", "a", ")", ":", "\n", "        ", "self", ".", "position", "+=", "self", ".", "basic_effects", "[", "a", "]", "\n", "self", ".", "position", "[", "0", "]", "=", "self", ".", "size", "if", "self", ".", "position", "[", "0", "]", ">", "self", ".", "size", "else", "self", ".", "position", "[", "0", "]", "\n", "self", ".", "position", "[", "0", "]", "=", "-", "self", ".", "size", "if", "self", ".", "position", "[", "0", "]", "<", "-", "self", ".", "size", "else", "self", ".", "position", "[", "0", "]", "\n", "self", ".", "position", "[", "1", "]", "=", "self", ".", "size", "if", "self", ".", "position", "[", "1", "]", ">", "self", ".", "size", "else", "self", ".", "position", "[", "1", "]", "\n", "self", ".", "position", "[", "1", "]", "=", "-", "self", ".", "size", "if", "self", ".", "position", "[", "1", "]", "<", "-", "self", ".", "size", "else", "self", ".", "position", "[", "1", "]", "\n", "\n", "if", "self", ".", "is_terminal", ":", "\n", "            ", "return", "10", ",", "True", "\n", "", "else", ":", "\n", "            ", "return", "-", "0.05", ",", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.grid_world.GridWorld.step": [[59, 69], ["enumerate", "grid_world.GridWorld.step_one", "grid_world.GridWorld.get_state", "grid_world.GridWorld.get_state"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.grid_world.GridWorld.step_one", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.get_state", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.get_state"], ["", "", "def", "step", "(", "self", ",", "a", ",", "rand", "=", "False", ")", ":", "\n", "        ", "actions", "=", "self", ".", "all_actions", "[", "a", "]", "\n", "reward", "=", "0", "\n", "d", "=", "False", "\n", "for", "i", ",", "action", "in", "enumerate", "(", "actions", ")", ":", "\n", "            ", "r", ",", "d", "=", "self", ".", "step_one", "(", "action", ")", "\n", "reward", "+=", "r", "\n", "if", "d", ":", "\n", "                ", "return", "self", ".", "get_state", "(", ")", ",", "reward", ",", "d", ",", "None", "\n", "", "", "return", "self", ".", "get_state", "(", ")", ",", "reward", ",", "d", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.grid_world.GridWorld.get_state": [[70, 72], ["numpy.concatenate"], "methods", ["None"], ["", "def", "get_state", "(", "self", ")", ":", "\n", "        ", "return", "np", ".", "concatenate", "(", "[", "self", ".", "position", ",", "self", ".", "goal", "]", ")", "/", "self", ".", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.grid_world.GridWorld.is_terminal": [[73, 78], ["all"], "methods", ["None"], ["", "@", "property", "\n", "def", "is_terminal", "(", "self", ")", ":", "\n", "        ", "if", "all", "(", "self", ".", "position", "==", "self", ".", "goal", ")", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.__init__": [[7, 20], ["gym.make", "itertools.product", "numpy.array", "len", "numpy.arange", "mujoco_wrapper.MujocoWrapper.env.reset", "list", "numpy.linspace", "range", "len"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.reset"], ["    ", "def", "__init__", "(", "self", ",", "env_name", ",", "action_num", "=", "101", ")", ":", "\n", "        ", "self", ".", "env", "=", "gym", ".", "make", "(", "env_name", ")", "\n", "self", ".", "min_action", "=", "self", ".", "action_space", ".", "low", "\n", "self", ".", "max_action", "=", "self", ".", "action_space", ".", "high", "\n", "\n", "# self.real_actions = np.linspace(self.min_action, self.max_action, action_num) #.reshape(action_num, -1)", "\n", "real_actions", "=", "product", "(", "*", "[", "np", ".", "linspace", "(", "self", ".", "min_action", "[", "i", "]", ",", "self", ".", "max_action", "[", "i", "]", ",", "action_num", ")", "for", "i", "in", "range", "(", "len", "(", "self", ".", "min_action", ")", ")", "]", ")", "\n", "self", ".", "real_actions", "=", "np", ".", "array", "(", "list", "(", "real_actions", ")", ")", "\n", "self", ".", "action_num", "=", "len", "(", "self", ".", "real_actions", ")", "\n", "self", ".", "actions", "=", "np", ".", "arange", "(", "self", ".", "action_num", ")", "\n", "self", ".", "obs", "=", "None", "\n", "self", ".", "reward", "=", "0", "\n", "self", ".", "env", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.get_state": [[21, 23], ["mujoco_wrapper.MujocoWrapper.obs.copy"], "methods", ["None"], ["", "def", "get_state", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "obs", ".", "copy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.reset": [[24, 27], ["mujoco_wrapper.MujocoWrapper.env.reset", "mujoco_wrapper.MujocoWrapper.get_state"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.reset", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.get_state"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "obs", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "return", "self", ".", "get_state", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.observation_space": [[28, 31], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "observation_space", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "observation_space", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.action_space": [[32, 35], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "action_space", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "action_space", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.step": [[36, 40], ["mujoco_wrapper.MujocoWrapper.env.step", "mujoco_wrapper.MujocoWrapper.get_state"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.step", "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.get_state"], ["", "def", "step", "(", "self", ",", "a", ")", ":", "\n", "        ", "a", "=", "self", ".", "real_actions", "[", "a", "]", "\n", "self", ".", "obs", ",", "r", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "a", ")", "\n", "return", "self", ".", "get_state", "(", ")", ",", "r", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.render": [[41, 43], ["mujoco_wrapper.MujocoWrapper.env.render"], "methods", ["home.repos.pwc.inspect_result.ActionEmbedding_ActionEmbedding.envs.mujoco_wrapper.MujocoWrapper.render"], ["", "def", "render", "(", "self", ")", ":", "\n", "        ", "self", ".", "env", ".", "render", "(", ")", "\n", "\n"]]}