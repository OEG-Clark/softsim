{"home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.config.ppo_argsparser": [[26, 44], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "ppo_argsparser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\"PPO\"", ")", "\n", "parser", ".", "add_argument", "(", "'--env_id'", ",", "help", "=", "'environment ID'", ",", "default", "=", "'Hopper-v2'", ")", "\n", "parser", ".", "add_argument", "(", "'--algo_id'", ",", "help", "=", "'algorithm ID'", ",", "default", "=", "'PPO'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "help", "=", "'batch size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--nenv'", ",", "help", "=", "'num env'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_epoch'", ",", "help", "=", "'ppo epoch num'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--hidden_size'", ",", "help", "=", "'hidden size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--nsteps'", ",", "help", "=", "'nsteps'", ",", "type", "=", "int", ",", "default", "=", "2048", ")", "\n", "parser", ".", "add_argument", "(", "'--total_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--learning_rate'", ",", "help", "=", "'learning rate'", ",", "type", "=", "int", ",", "default", "=", "3e-4", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluate_every'", ",", "help", "=", "'evaluate every'", ",", "type", "=", "int", ",", "default", "=", "20480", ")", "\n", "parser", ".", "add_argument", "(", "'--save_condition'", ",", "help", "=", "'save_condition'", ",", "type", "=", "int", ",", "default", "=", "1000", ")", "\n", "parser", ".", "add_argument", "(", "'--num_model'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--use_device'", ",", "help", "=", "'use_device'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.build.build_env": [[7, 23], ["common.vec_env.subproc_vec_env.SubprocVecEnv", "build.build_env.make_env"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.make_env"], ["def", "build_env", "(", "args", ",", "nenv", ",", "norm_env", "=", "True", ")", ":", "\n", "\n", "    ", "def", "make_env", "(", ")", ":", "\n", "        ", "def", "_thunk", "(", ")", ":", "\n", "            ", "env", "=", "gym", ".", "make", "(", "args", ".", "env_id", ")", "\n", "env", ".", "seed", "(", "args", ".", "seed", ")", "# to make the result more reproducibility", "\n", "env", "=", "Monitor", "(", "env", ",", "logger", ".", "get_dir", "(", ")", ",", "allow_early_resets", "=", "True", ")", "\n", "return", "env", "\n", "", "return", "_thunk", "\n", "\n", "", "envs", "=", "[", "make_env", "(", ")", "for", "i", "in", "range", "(", "nenv", ")", "]", "\n", "envs", "=", "SubprocVecEnv", "(", "envs", ")", "\n", "if", "norm_env", ":", "\n", "        ", "envs", "=", "VecNormalize", "(", "envs", ")", "\n", "\n", "", "return", "envs", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.build.build_env4gail": [[24, 39], ["common.vec_env.subproc_vec_env.SubprocVecEnv", "common.vec_env.vec_normalize.VecNormalize", "build.build_env.make_env"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.make_env"], ["", "def", "build_env4gail", "(", "args", ",", "nenv", ")", ":", "\n", "\n", "    ", "def", "make_env", "(", ")", ":", "\n", "        ", "def", "_thunk", "(", ")", ":", "\n", "            ", "env", "=", "gym", ".", "make", "(", "args", ".", "env_id", ")", "\n", "env", ".", "seed", "(", "args", ".", "seed", ")", "# to make the result more reproducibility", "\n", "env", "=", "Monitor", "(", "env", ",", "logger", ".", "get_dir", "(", ")", ",", "allow_early_resets", "=", "True", ")", "\n", "return", "env", "\n", "", "return", "_thunk", "\n", "\n", "", "envs", "=", "[", "make_env", "(", ")", "for", "i", "in", "range", "(", "nenv", ")", "]", "\n", "envs", "=", "SubprocVecEnv", "(", "envs", ")", "\n", "envs", "=", "VecNormalize", "(", "envs", ")", "\n", "\n", "return", "envs", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.build.build_env4wdail": [[40, 56], ["common.vec_env.subproc_vec_env.SubprocVecEnv", "build.build_env.make_env"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.make_env"], ["", "def", "build_env4wdail", "(", "args", ",", "nenv", ")", ":", "\n", "\n", "    ", "def", "make_env", "(", ")", ":", "\n", "        ", "def", "_thunk", "(", ")", ":", "\n", "            ", "env", "=", "gym", ".", "make", "(", "args", ".", "env_id", ")", "\n", "env", ".", "seed", "(", "args", ".", "seed", ")", "# to make the result more reproducibility", "\n", "env", "=", "Monitor", "(", "env", ",", "logger", ".", "get_dir", "(", ")", ",", "allow_early_resets", "=", "True", ")", "\n", "return", "env", "\n", "", "return", "_thunk", "\n", "\n", "", "envs", "=", "[", "make_env", "(", ")", "for", "i", "in", "range", "(", "nenv", ")", "]", "\n", "envs", "=", "SubprocVecEnv", "(", "envs", ")", "\n", "if", "args", ".", "env_norm", ":", "\n", "        ", "envs", "=", "VecNormalize", "(", "envs", ")", "\n", "\n", "", "return", "envs", "", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.KVWriter.writekvs": [[20, 22], ["None"], "methods", ["None"], ["    ", "def", "writekvs", "(", "self", ",", "kvs", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.SeqWriter.writeseq": [[24, 26], ["None"], "methods", ["None"], ["    ", "def", "writeseq", "(", "self", ",", "seq", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.HumanOutputFormat.__init__": [[28, 36], ["isinstance", "open", "hasattr"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "filename_or_file", ")", ":", "\n", "        ", "if", "isinstance", "(", "filename_or_file", ",", "str", ")", ":", "\n", "            ", "self", ".", "file", "=", "open", "(", "filename_or_file", ",", "'wt'", ")", "\n", "self", ".", "own_file", "=", "True", "\n", "", "else", ":", "\n", "            ", "assert", "hasattr", "(", "filename_or_file", ",", "'read'", ")", ",", "'expected file or str, got %s'", "%", "filename_or_file", "\n", "self", ".", "file", "=", "filename_or_file", "\n", "self", ".", "own_file", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.HumanOutputFormat.writekvs": [[37, 70], ["sorted", "sorted", "lines.append", "logger.HumanOutputFormat.file.write", "logger.HumanOutputFormat.file.flush", "kvs.items", "isinstance", "logger.HumanOutputFormat._truncate", "len", "print", "max", "max", "key2str.items", "lines.append", "str", "map", "map", "logger.HumanOutputFormat._truncate", "key2str.keys", "key2str.values", "kv[].lower", "len", "len"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.HumanOutputFormat._truncate", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.HumanOutputFormat._truncate"], ["", "", "def", "writekvs", "(", "self", ",", "kvs", ")", ":", "\n", "# Create strings for printing", "\n", "        ", "key2str", "=", "{", "}", "\n", "for", "(", "key", ",", "val", ")", "in", "sorted", "(", "kvs", ".", "items", "(", ")", ")", ":", "\n", "            ", "if", "isinstance", "(", "val", ",", "float", ")", ":", "\n", "                ", "valstr", "=", "'%-8.3g'", "%", "(", "val", ",", ")", "\n", "", "else", ":", "\n", "                ", "valstr", "=", "str", "(", "val", ")", "\n", "", "key2str", "[", "self", ".", "_truncate", "(", "key", ")", "]", "=", "self", ".", "_truncate", "(", "valstr", ")", "\n", "\n", "# Find max widths", "\n", "", "if", "len", "(", "key2str", ")", "==", "0", ":", "\n", "            ", "print", "(", "'WARNING: tried to write empty key-value dict'", ")", "\n", "return", "\n", "", "else", ":", "\n", "            ", "keywidth", "=", "max", "(", "map", "(", "len", ",", "key2str", ".", "keys", "(", ")", ")", ")", "\n", "valwidth", "=", "max", "(", "map", "(", "len", ",", "key2str", ".", "values", "(", ")", ")", ")", "\n", "\n", "# Write out the data", "\n", "", "dashes", "=", "'-'", "*", "(", "keywidth", "+", "valwidth", "+", "7", ")", "\n", "lines", "=", "[", "dashes", "]", "\n", "for", "(", "key", ",", "val", ")", "in", "sorted", "(", "key2str", ".", "items", "(", ")", ",", "key", "=", "lambda", "kv", ":", "kv", "[", "0", "]", ".", "lower", "(", ")", ")", ":", "\n", "            ", "lines", ".", "append", "(", "'| %s%s | %s%s |'", "%", "(", "\n", "key", ",", "\n", "' '", "*", "(", "keywidth", "-", "len", "(", "key", ")", ")", ",", "\n", "val", ",", "\n", "' '", "*", "(", "valwidth", "-", "len", "(", "val", ")", ")", ",", "\n", ")", ")", "\n", "", "lines", ".", "append", "(", "dashes", ")", "\n", "self", ".", "file", ".", "write", "(", "'\\n'", ".", "join", "(", "lines", ")", "+", "'\\n'", ")", "\n", "\n", "# Flush the output to the file", "\n", "self", ".", "file", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.HumanOutputFormat._truncate": [[71, 74], ["len"], "methods", ["None"], ["", "def", "_truncate", "(", "self", ",", "s", ")", ":", "\n", "        ", "maxlen", "=", "30", "\n", "return", "s", "[", ":", "maxlen", "-", "3", "]", "+", "'...'", "if", "len", "(", "s", ")", ">", "maxlen", "else", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.HumanOutputFormat.writeseq": [[75, 83], ["list", "enumerate", "logger.HumanOutputFormat.file.write", "logger.HumanOutputFormat.file.flush", "logger.HumanOutputFormat.file.write", "logger.HumanOutputFormat.file.write", "len"], "methods", ["None"], ["", "def", "writeseq", "(", "self", ",", "seq", ")", ":", "\n", "        ", "seq", "=", "list", "(", "seq", ")", "\n", "for", "(", "i", ",", "elem", ")", "in", "enumerate", "(", "seq", ")", ":", "\n", "            ", "self", ".", "file", ".", "write", "(", "elem", ")", "\n", "if", "i", "<", "len", "(", "seq", ")", "-", "1", ":", "# add space unless this is the last one", "\n", "                ", "self", ".", "file", ".", "write", "(", "' '", ")", "\n", "", "", "self", ".", "file", ".", "write", "(", "'\\n'", ")", "\n", "self", ".", "file", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.HumanOutputFormat.close": [[84, 87], ["logger.HumanOutputFormat.file.close"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "own_file", ":", "\n", "            ", "self", ".", "file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.JSONOutputFormat.__init__": [[89, 91], ["open"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "filename", ")", ":", "\n", "        ", "self", ".", "file", "=", "open", "(", "filename", ",", "'wt'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.JSONOutputFormat.writekvs": [[92, 99], ["sorted", "logger.JSONOutputFormat.file.write", "logger.JSONOutputFormat.file.flush", "kvs.items", "hasattr", "v.tolist.tolist.tolist", "float", "json.dumps"], "methods", ["None"], ["", "def", "writekvs", "(", "self", ",", "kvs", ")", ":", "\n", "        ", "for", "k", ",", "v", "in", "sorted", "(", "kvs", ".", "items", "(", ")", ")", ":", "\n", "            ", "if", "hasattr", "(", "v", ",", "'dtype'", ")", ":", "\n", "                ", "v", "=", "v", ".", "tolist", "(", ")", "\n", "kvs", "[", "k", "]", "=", "float", "(", "v", ")", "\n", "", "", "self", ".", "file", ".", "write", "(", "json", ".", "dumps", "(", "kvs", ")", "+", "'\\n'", ")", "\n", "self", ".", "file", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.JSONOutputFormat.close": [[100, 102], ["logger.JSONOutputFormat.file.close"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.CSVOutputFormat.__init__": [[104, 108], ["open"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "filename", ")", ":", "\n", "        ", "self", ".", "file", "=", "open", "(", "filename", ",", "'w+t'", ")", "\n", "self", ".", "keys", "=", "[", "]", "\n", "self", ".", "sep", "=", "','", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.CSVOutputFormat.writekvs": [[109, 135], ["list", "list.sort", "enumerate", "logger.CSVOutputFormat.file.write", "logger.CSVOutputFormat.file.flush", "logger.CSVOutputFormat.keys.extend", "logger.CSVOutputFormat.file.seek", "logger.CSVOutputFormat.file.readlines", "logger.CSVOutputFormat.file.seek", "enumerate", "logger.CSVOutputFormat.file.write", "kvs.get", "kvs.keys", "logger.CSVOutputFormat.file.write", "logger.CSVOutputFormat.file.write", "logger.CSVOutputFormat.file.write", "logger.CSVOutputFormat.file.write", "logger.CSVOutputFormat.file.write", "logger.CSVOutputFormat.file.write", "logger.CSVOutputFormat.file.write", "str", "len"], "methods", ["None"], ["", "def", "writekvs", "(", "self", ",", "kvs", ")", ":", "\n", "# Add our current row to the history", "\n", "        ", "extra_keys", "=", "list", "(", "kvs", ".", "keys", "(", ")", "-", "self", ".", "keys", ")", "\n", "extra_keys", ".", "sort", "(", ")", "\n", "if", "extra_keys", ":", "\n", "            ", "self", ".", "keys", ".", "extend", "(", "extra_keys", ")", "\n", "self", ".", "file", ".", "seek", "(", "0", ")", "\n", "lines", "=", "self", ".", "file", ".", "readlines", "(", ")", "\n", "self", ".", "file", ".", "seek", "(", "0", ")", "\n", "for", "(", "i", ",", "k", ")", "in", "enumerate", "(", "self", ".", "keys", ")", ":", "\n", "                ", "if", "i", ">", "0", ":", "\n", "                    ", "self", ".", "file", ".", "write", "(", "','", ")", "\n", "", "self", ".", "file", ".", "write", "(", "k", ")", "\n", "", "self", ".", "file", ".", "write", "(", "'\\n'", ")", "\n", "for", "line", "in", "lines", "[", "1", ":", "]", ":", "\n", "                ", "self", ".", "file", ".", "write", "(", "line", "[", ":", "-", "1", "]", ")", "\n", "self", ".", "file", ".", "write", "(", "self", ".", "sep", "*", "len", "(", "extra_keys", ")", ")", "\n", "self", ".", "file", ".", "write", "(", "'\\n'", ")", "\n", "", "", "for", "(", "i", ",", "k", ")", "in", "enumerate", "(", "self", ".", "keys", ")", ":", "\n", "            ", "if", "i", ">", "0", ":", "\n", "                ", "self", ".", "file", ".", "write", "(", "','", ")", "\n", "", "v", "=", "kvs", ".", "get", "(", "k", ")", "\n", "if", "v", "is", "not", "None", ":", "\n", "                ", "self", ".", "file", ".", "write", "(", "str", "(", "v", ")", ")", "\n", "", "", "self", ".", "file", ".", "write", "(", "'\\n'", ")", "\n", "self", ".", "file", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.CSVOutputFormat.close": [[136, 138], ["logger.CSVOutputFormat.file.close"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.TensorBoardOutputFormat.__init__": [[144, 158], ["os.makedirs", "os.makedirs", "os.makedirs", "os.makedirs", "os.join", "os.join", "pywrap_tensorflow.EventsWriter", "os.abspath", "os.abspath", "compat.as_bytes"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "dir", ",", "exist_ok", "=", "True", ")", "\n", "self", ".", "dir", "=", "dir", "\n", "self", ".", "step", "=", "1", "\n", "prefix", "=", "'events'", "\n", "path", "=", "osp", ".", "join", "(", "osp", ".", "abspath", "(", "dir", ")", ",", "prefix", ")", "\n", "import", "tensorflow", "as", "tf", "\n", "from", "tensorflow", ".", "python", "import", "pywrap_tensorflow", "\n", "from", "tensorflow", ".", "core", ".", "util", "import", "event_pb2", "\n", "from", "tensorflow", ".", "python", ".", "util", "import", "compat", "\n", "self", ".", "tf", "=", "tf", "\n", "self", ".", "event_pb2", "=", "event_pb2", "\n", "self", ".", "pywrap_tensorflow", "=", "pywrap_tensorflow", "\n", "self", ".", "writer", "=", "pywrap_tensorflow", ".", "EventsWriter", "(", "compat", ".", "as_bytes", "(", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.TensorBoardOutputFormat.writekvs": [[159, 169], ["logger.TensorBoardOutputFormat.tf.Summary", "logger.TensorBoardOutputFormat.event_pb2.Event", "logger.TensorBoardOutputFormat.writer.WriteEvent", "logger.TensorBoardOutputFormat.writer.Flush", "logger.TensorBoardOutputFormat.tf.Summary.Value", "float", "time.time", "logger.TensorBoardOutputFormat.writekvs.summary_val"], "methods", ["None"], ["", "def", "writekvs", "(", "self", ",", "kvs", ")", ":", "\n", "        ", "def", "summary_val", "(", "k", ",", "v", ")", ":", "\n", "            ", "kwargs", "=", "{", "'tag'", ":", "k", ",", "'simple_value'", ":", "float", "(", "v", ")", "}", "\n", "return", "self", ".", "tf", ".", "Summary", ".", "Value", "(", "**", "kwargs", ")", "\n", "", "summary", "=", "self", ".", "tf", ".", "Summary", "(", "value", "=", "[", "summary_val", "(", "k", ",", "v", ")", "for", "k", ",", "v", "in", "kvs", ".", "items", "(", ")", "]", ")", "\n", "event", "=", "self", ".", "event_pb2", ".", "Event", "(", "wall_time", "=", "time", ".", "time", "(", ")", ",", "summary", "=", "summary", ")", "\n", "event", ".", "step", "=", "self", ".", "step", "# is there any reason why you'd want to specify the step?", "\n", "self", ".", "writer", ".", "WriteEvent", "(", "event", ")", "\n", "self", ".", "writer", ".", "Flush", "(", ")", "\n", "self", ".", "step", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.TensorBoardOutputFormat.close": [[170, 174], ["logger.TensorBoardOutputFormat.writer.Close"], "methods", ["None"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "writer", ":", "\n", "            ", "self", ".", "writer", ".", "Close", "(", ")", "\n", "self", ".", "writer", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.Logger.__init__": [[302, 309], ["collections.defaultdict", "collections.defaultdict"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dir", ",", "output_formats", ",", "comm", "=", "None", ")", ":", "\n", "        ", "self", ".", "name2val", "=", "defaultdict", "(", "float", ")", "# values this iteration", "\n", "self", ".", "name2cnt", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "level", "=", "INFO", "\n", "self", ".", "dir", "=", "dir", "\n", "self", ".", "output_formats", "=", "output_formats", "\n", "self", ".", "comm", "=", "comm", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.Logger.logkv": [[312, 314], ["None"], "methods", ["None"], ["", "def", "logkv", "(", "self", ",", "key", ",", "val", ")", ":", "\n", "        ", "self", ".", "name2val", "[", "key", "]", "=", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.Logger.logkv_mean": [[315, 319], ["None"], "methods", ["None"], ["", "def", "logkv_mean", "(", "self", ",", "key", ",", "val", ")", ":", "\n", "        ", "oldval", ",", "cnt", "=", "self", ".", "name2val", "[", "key", "]", ",", "self", ".", "name2cnt", "[", "key", "]", "\n", "self", ".", "name2val", "[", "key", "]", "=", "oldval", "*", "cnt", "/", "(", "cnt", "+", "1", ")", "+", "val", "/", "(", "cnt", "+", "1", ")", "\n", "self", ".", "name2cnt", "[", "key", "]", "=", "cnt", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.Logger.dumpkvs": [[320, 337], ["mpi_util.mpi_weighted_mean.copy", "logger.Logger.name2val.clear", "logger.Logger.name2cnt.clear", "mpi_util.mpi_weighted_mean", "isinstance", "fmt.writekvs", "logger.Logger.name2cnt.get", "logger.Logger.name2val.items"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.TensorBoardOutputFormat.writekvs"], ["", "def", "dumpkvs", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "comm", "is", "None", ":", "\n", "            ", "d", "=", "self", ".", "name2val", "\n", "", "else", ":", "\n", "            ", "from", "baselines", ".", "common", "import", "mpi_util", "\n", "d", "=", "mpi_util", ".", "mpi_weighted_mean", "(", "self", ".", "comm", ",", "\n", "{", "name", ":", "(", "val", ",", "self", ".", "name2cnt", ".", "get", "(", "name", ",", "1", ")", ")", "\n", "for", "(", "name", ",", "val", ")", "in", "self", ".", "name2val", ".", "items", "(", ")", "}", ")", "\n", "if", "self", ".", "comm", ".", "rank", "!=", "0", ":", "\n", "                ", "d", "[", "'dummy'", "]", "=", "1", "# so we don't get a warning about empty dict", "\n", "", "", "out", "=", "d", ".", "copy", "(", ")", "# Return the dict for unit testing purposes", "\n", "for", "fmt", "in", "self", ".", "output_formats", ":", "\n", "            ", "if", "isinstance", "(", "fmt", ",", "KVWriter", ")", ":", "\n", "                ", "fmt", ".", "writekvs", "(", "d", ")", "\n", "", "", "self", ".", "name2val", ".", "clear", "(", ")", "\n", "self", ".", "name2cnt", ".", "clear", "(", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.Logger.log": [[338, 341], ["logger.Logger._do_log"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.Logger._do_log"], ["", "def", "log", "(", "self", ",", "*", "args", ",", "level", "=", "INFO", ")", ":", "\n", "        ", "if", "self", ".", "level", "<=", "level", ":", "\n", "            ", "self", ".", "_do_log", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.Logger.set_level": [[344, 346], ["None"], "methods", ["None"], ["", "", "def", "set_level", "(", "self", ",", "level", ")", ":", "\n", "        ", "self", ".", "level", "=", "level", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.Logger.set_comm": [[347, 349], ["None"], "methods", ["None"], ["", "def", "set_comm", "(", "self", ",", "comm", ")", ":", "\n", "        ", "self", ".", "comm", "=", "comm", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.Logger.get_dir": [[350, 352], ["None"], "methods", ["None"], ["", "def", "get_dir", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "dir", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.Logger.close": [[353, 356], ["fmt.close"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "for", "fmt", "in", "self", ".", "output_formats", ":", "\n", "            ", "fmt", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.Logger._do_log": [[359, 363], ["isinstance", "fmt.writeseq", "map"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.HumanOutputFormat.writeseq"], ["", "", "def", "_do_log", "(", "self", ",", "args", ")", ":", "\n", "        ", "for", "fmt", "in", "self", ".", "output_formats", ":", "\n", "            ", "if", "isinstance", "(", "fmt", ",", "SeqWriter", ")", ":", "\n", "                ", "fmt", ".", "writeseq", "(", "map", "(", "str", ",", "args", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.make_output_format": [[175, 189], ["os.makedirs", "os.makedirs", "logger.HumanOutputFormat", "logger.HumanOutputFormat", "os.join", "logger.JSONOutputFormat", "os.join", "logger.CSVOutputFormat", "os.join", "logger.TensorBoardOutputFormat", "ValueError", "os.join"], "function", ["None"], ["", "", "", "def", "make_output_format", "(", "format", ",", "ev_dir", ",", "log_suffix", "=", "''", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "ev_dir", ",", "exist_ok", "=", "True", ")", "\n", "if", "format", "==", "'stdout'", ":", "\n", "        ", "return", "HumanOutputFormat", "(", "sys", ".", "stdout", ")", "\n", "", "elif", "format", "==", "'log'", ":", "\n", "        ", "return", "HumanOutputFormat", "(", "osp", ".", "join", "(", "ev_dir", ",", "'log%s.txt'", "%", "log_suffix", ")", ")", "\n", "", "elif", "format", "==", "'json'", ":", "\n", "        ", "return", "JSONOutputFormat", "(", "osp", ".", "join", "(", "ev_dir", ",", "'progress%s.json'", "%", "log_suffix", ")", ")", "\n", "", "elif", "format", "==", "'csv'", ":", "\n", "        ", "return", "CSVOutputFormat", "(", "osp", ".", "join", "(", "ev_dir", ",", "'progress%s.csv'", "%", "log_suffix", ")", ")", "\n", "", "elif", "format", "==", "'tensorboard'", ":", "\n", "        ", "return", "TensorBoardOutputFormat", "(", "osp", ".", "join", "(", "ev_dir", ",", "'tb%s'", "%", "log_suffix", ")", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Unknown format specified: %s'", "%", "(", "format", ",", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.logkv": [[194, 201], ["get_current().logkv", "logger.get_current"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.logkv", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.get_current"], ["", "", "def", "logkv", "(", "key", ",", "val", ")", ":", "\n", "    ", "\"\"\"\n    Log a value of some diagnostic\n    Call this once for each diagnostic quantity, each iteration\n    If called many times, last value will be used.\n    \"\"\"", "\n", "get_current", "(", ")", ".", "logkv", "(", "key", ",", "val", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.logkv_mean": [[202, 207], ["get_current().logkv_mean", "logger.get_current"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.logkv_mean", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.get_current"], ["", "def", "logkv_mean", "(", "key", ",", "val", ")", ":", "\n", "    ", "\"\"\"\n    The same as logkv(), but if called many times, values averaged.\n    \"\"\"", "\n", "get_current", "(", ")", ".", "logkv_mean", "(", "key", ",", "val", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.logkvs": [[208, 214], ["d.items", "logger.logkv"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.logkv"], ["", "def", "logkvs", "(", "d", ")", ":", "\n", "    ", "\"\"\"\n    Log a dictionary of key-value pairs\n    \"\"\"", "\n", "for", "(", "k", ",", "v", ")", "in", "d", ".", "items", "(", ")", ":", "\n", "        ", "logkv", "(", "k", ",", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.dumpkvs": [[215, 220], ["get_current().dumpkvs", "logger.get_current"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.dumpkvs", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.get_current"], ["", "", "def", "dumpkvs", "(", ")", ":", "\n", "    ", "\"\"\"\n    Write all of the diagnostics from the current iteration\n    \"\"\"", "\n", "return", "get_current", "(", ")", ".", "dumpkvs", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.getkvs": [[221, 223], ["logger.get_current"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.get_current"], ["", "def", "getkvs", "(", ")", ":", "\n", "    ", "return", "get_current", "(", ")", ".", "name2val", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.log": [[225, 230], ["get_current().log", "logger.get_current"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.log", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.get_current"], ["", "def", "log", "(", "*", "args", ",", "level", "=", "INFO", ")", ":", "\n", "    ", "\"\"\"\n    Write the sequence of args, with no separators, to the console and output files (if you've configured an output file).\n    \"\"\"", "\n", "get_current", "(", ")", ".", "log", "(", "*", "args", ",", "level", "=", "level", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.debug": [[231, 233], ["logger.log"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.log"], ["", "def", "debug", "(", "*", "args", ")", ":", "\n", "    ", "log", "(", "*", "args", ",", "level", "=", "DEBUG", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.info": [[234, 236], ["logger.log"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.log"], ["", "def", "info", "(", "*", "args", ")", ":", "\n", "    ", "log", "(", "*", "args", ",", "level", "=", "INFO", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.warn": [[237, 239], ["logger.log"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.log"], ["", "def", "warn", "(", "*", "args", ")", ":", "\n", "    ", "log", "(", "*", "args", ",", "level", "=", "WARN", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.error": [[240, 242], ["logger.log"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.log"], ["", "def", "error", "(", "*", "args", ")", ":", "\n", "    ", "log", "(", "*", "args", ",", "level", "=", "ERROR", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.set_level": [[244, 249], ["get_current().set_level", "logger.get_current"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.set_level", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.get_current"], ["", "def", "set_level", "(", "level", ")", ":", "\n", "    ", "\"\"\"\n    Set logging threshold on current logger.\n    \"\"\"", "\n", "get_current", "(", ")", ".", "set_level", "(", "level", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.set_comm": [[250, 252], ["get_current().set_comm", "logger.get_current"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.set_comm", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.get_current"], ["", "def", "set_comm", "(", "comm", ")", ":", "\n", "    ", "get_current", "(", ")", ".", "set_comm", "(", "comm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.get_dir": [[253, 259], ["get_current().get_dir", "logger.get_current"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.get_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.get_current"], ["", "def", "get_dir", "(", ")", ":", "\n", "    ", "\"\"\"\n    Get directory that log files are being written to.\n    will be None if there is no output directory (i.e., if you didn't call start)\n    \"\"\"", "\n", "return", "get_current", "(", ")", ".", "get_dir", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.profile_kv": [[263, 271], ["time.time", "time.time", "logger.get_current"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.get_current"], ["@", "contextmanager", "\n", "def", "profile_kv", "(", "scopename", ")", ":", "\n", "    ", "logkey", "=", "'wait_'", "+", "scopename", "\n", "tstart", "=", "time", ".", "time", "(", ")", "\n", "try", ":", "\n", "        ", "yield", "\n", "", "finally", ":", "\n", "        ", "get_current", "(", ")", ".", "name2val", "[", "logkey", "]", "+=", "time", ".", "time", "(", ")", "-", "tstart", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.profile": [[272, 284], ["logger.profile_kv", "func"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.profile_kv"], ["", "", "def", "profile", "(", "n", ")", ":", "\n", "    ", "\"\"\"\n    Usage:\n    @profile(\"my_func\")\n    def my_func(): code\n    \"\"\"", "\n", "def", "decorator_with_name", "(", "func", ")", ":", "\n", "        ", "def", "func_wrapper", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "            ", "with", "profile_kv", "(", "n", ")", ":", "\n", "                ", "return", "func", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "", "", "return", "func_wrapper", "\n", "", "return", "decorator_with_name", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.get_current": [[290, 295], ["logger._configure_default_logger"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger._configure_default_logger"], ["", "def", "get_current", "(", ")", ":", "\n", "    ", "if", "Logger", ".", "CURRENT", "is", "None", ":", "\n", "        ", "_configure_default_logger", "(", ")", "\n", "\n", "", "return", "Logger", ".", "CURRENT", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.configure": [[364, 396], ["isinstance", "os.makedirs", "os.makedirs", "filter", "logger.Logger", "logger.log", "os.getenv", "os.getenv", "os.join", "logger.make_output_format", "tempfile.gettempdir", "datetime.datetime.now().strftime", "int", "os.getenv().split", "os.getenv().split", "os.getenv().split", "os.getenv().split", "datetime.datetime.now", "os.getenv", "os.getenv", "os.getenv", "os.getenv"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.log", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.make_output_format"], ["", "", "", "", "def", "configure", "(", "dir", "=", "None", ",", "format_strs", "=", "None", ",", "comm", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    If comm is provided, average all numerical stats across that comm\n    \"\"\"", "\n", "if", "dir", "is", "None", ":", "\n", "        ", "dir", "=", "os", ".", "getenv", "(", "'OPENAI_LOGDIR'", ")", "\n", "", "if", "dir", "is", "None", ":", "\n", "        ", "dir", "=", "osp", ".", "join", "(", "tempfile", ".", "gettempdir", "(", ")", ",", "\n", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"openai-%Y-%m-%d-%H-%M-%S-%f\"", ")", ")", "\n", "", "assert", "isinstance", "(", "dir", ",", "str", ")", "\n", "os", ".", "makedirs", "(", "dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "log_suffix", "=", "''", "\n", "rank", "=", "0", "\n", "# check environment variables here instead of importing mpi4py", "\n", "# to avoid calling MPI_Init() when this module is imported", "\n", "for", "varname", "in", "[", "'PMI_RANK'", ",", "'OMPI_COMM_WORLD_RANK'", "]", ":", "\n", "        ", "if", "varname", "in", "os", ".", "environ", ":", "\n", "            ", "rank", "=", "int", "(", "os", ".", "environ", "[", "varname", "]", ")", "\n", "", "", "if", "rank", ">", "0", ":", "\n", "        ", "log_suffix", "=", "\"-rank%03i\"", "%", "rank", "\n", "\n", "", "if", "format_strs", "is", "None", ":", "\n", "        ", "if", "rank", "==", "0", ":", "\n", "            ", "format_strs", "=", "os", ".", "getenv", "(", "'OPENAI_LOG_FORMAT'", ",", "'stdout,log,csv'", ")", ".", "split", "(", "','", ")", "\n", "", "else", ":", "\n", "            ", "format_strs", "=", "os", ".", "getenv", "(", "'OPENAI_LOG_FORMAT_MPI'", ",", "'log'", ")", ".", "split", "(", "','", ")", "\n", "", "", "format_strs", "=", "filter", "(", "None", ",", "format_strs", ")", "\n", "output_formats", "=", "[", "make_output_format", "(", "f", ",", "dir", ",", "log_suffix", ")", "for", "f", "in", "format_strs", "]", "\n", "\n", "Logger", ".", "CURRENT", "=", "Logger", "(", "dir", "=", "dir", ",", "output_formats", "=", "output_formats", ",", "comm", "=", "comm", ")", "\n", "log", "(", "'Logging to %s'", "%", "dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger._configure_default_logger": [[397, 400], ["logger.configure"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.configure"], ["", "def", "_configure_default_logger", "(", ")", ":", "\n", "    ", "configure", "(", ")", "\n", "Logger", ".", "DEFAULT", "=", "Logger", ".", "CURRENT", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.reset": [[401, 406], ["Logger.CURRENT.close", "logger.log"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.log"], ["", "def", "reset", "(", ")", ":", "\n", "    ", "if", "Logger", ".", "CURRENT", "is", "not", "Logger", ".", "DEFAULT", ":", "\n", "        ", "Logger", ".", "CURRENT", ".", "close", "(", ")", "\n", "Logger", ".", "CURRENT", "=", "Logger", ".", "DEFAULT", "\n", "log", "(", "'Reset logger'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.scoped_configure": [[407, 416], ["logger.configure", "Logger.CURRENT.close"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.configure", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close"], ["", "", "@", "contextmanager", "\n", "def", "scoped_configure", "(", "dir", "=", "None", ",", "format_strs", "=", "None", ",", "comm", "=", "None", ")", ":", "\n", "    ", "prevlogger", "=", "Logger", ".", "CURRENT", "\n", "configure", "(", "dir", "=", "dir", ",", "format_strs", "=", "format_strs", ",", "comm", "=", "comm", ")", "\n", "try", ":", "\n", "        ", "yield", "\n", "", "finally", ":", "\n", "        ", "Logger", ".", "CURRENT", ".", "close", "(", ")", "\n", "Logger", ".", "CURRENT", "=", "prevlogger", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger._demo": [[419, 446], ["logger.info", "logger.debug", "logger.set_level", "logger.debug", "os.path.exists", "os.path.exists", "logger.configure", "logger.logkv", "logger.logkv", "logger.dumpkvs", "logger.logkv", "logger.logkv", "logger.dumpkvs", "logger.info", "logger.logkv_mean", "logger.logkv_mean", "logger.logkv", "logger.dumpkvs", "logger.info", "logger.logkv", "logger.dumpkvs", "logger.logkv", "logger.dumpkvs", "shutil.rmtree"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.info", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.debug", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.set_level", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.debug", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.configure", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.logkv", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.logkv", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.dumpkvs", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.logkv", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.logkv", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.dumpkvs", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.info", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.logkv_mean", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.logkv_mean", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.logkv", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.dumpkvs", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.info", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.logkv", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.dumpkvs", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.logkv", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.dumpkvs"], ["", "", "def", "_demo", "(", ")", ":", "\n", "    ", "info", "(", "\"hi\"", ")", "\n", "debug", "(", "\"shouldn't appear\"", ")", "\n", "set_level", "(", "DEBUG", ")", "\n", "debug", "(", "\"should appear\"", ")", "\n", "dir", "=", "\"/tmp/testlogging\"", "\n", "if", "os", ".", "path", ".", "exists", "(", "dir", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "dir", ")", "\n", "", "configure", "(", "dir", "=", "dir", ")", "\n", "logkv", "(", "\"a\"", ",", "3", ")", "\n", "logkv", "(", "\"b\"", ",", "2.5", ")", "\n", "dumpkvs", "(", ")", "\n", "logkv", "(", "\"b\"", ",", "-", "2.5", ")", "\n", "logkv", "(", "\"a\"", ",", "5.5", ")", "\n", "dumpkvs", "(", ")", "\n", "info", "(", "\"^^^ should see a = 5.5\"", ")", "\n", "logkv_mean", "(", "\"b\"", ",", "-", "22.5", ")", "\n", "logkv_mean", "(", "\"b\"", ",", "-", "44.4", ")", "\n", "logkv", "(", "\"a\"", ",", "5.5", ")", "\n", "dumpkvs", "(", ")", "\n", "info", "(", "\"^^^ should see b = -33.3\"", ")", "\n", "\n", "logkv", "(", "\"b\"", ",", "-", "2.5", ")", "\n", "dumpkvs", "(", ")", "\n", "\n", "logkv", "(", "\"a\"", ",", "\"longasslongasslongasslongasslongasslongassvalue\"", ")", "\n", "dumpkvs", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.read_json": [[452, 459], ["pandas.DataFrame", "open", "ds.append", "json.loads"], "function", ["None"], ["", "def", "read_json", "(", "fname", ")", ":", "\n", "    ", "import", "pandas", "\n", "ds", "=", "[", "]", "\n", "with", "open", "(", "fname", ",", "'rt'", ")", "as", "fh", ":", "\n", "        ", "for", "line", "in", "fh", ":", "\n", "            ", "ds", ".", "append", "(", "json", ".", "loads", "(", "line", ")", ")", "\n", "", "", "return", "pandas", ".", "DataFrame", "(", "ds", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.read_csv": [[460, 463], ["pandas.read_csv"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.read_csv"], ["", "def", "read_csv", "(", "fname", ")", ":", "\n", "    ", "import", "pandas", "\n", "return", "pandas", ".", "read_csv", "(", "fname", ",", "index_col", "=", "None", ",", "comment", "=", "'#'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.read_tb": [[464, 496], ["os.isdir", "collections.defaultdict", "np.empty", "sorted", "enumerate", "pandas.DataFrame", "glob", "os.basename().startswith", "tf.train.summary_iterator", "collections.defaultdict.keys", "os.join", "NotImplementedError", "len", "os.basename", "max", "tag2pairs[].append"], "function", ["None"], ["", "def", "read_tb", "(", "path", ")", ":", "\n", "    ", "\"\"\"\n    path : a tensorboard file OR a directory, where we will find all TB files\n           of the form events.*\n    \"\"\"", "\n", "import", "pandas", "\n", "import", "numpy", "as", "np", "\n", "from", "glob", "import", "glob", "\n", "import", "tensorflow", "as", "tf", "\n", "if", "osp", ".", "isdir", "(", "path", ")", ":", "\n", "        ", "fnames", "=", "glob", "(", "osp", ".", "join", "(", "path", ",", "\"events.*\"", ")", ")", "\n", "", "elif", "osp", ".", "basename", "(", "path", ")", ".", "startswith", "(", "\"events.\"", ")", ":", "\n", "        ", "fnames", "=", "[", "path", "]", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Expected tensorboard file or directory containing them. Got %s\"", "%", "path", ")", "\n", "", "tag2pairs", "=", "defaultdict", "(", "list", ")", "\n", "maxstep", "=", "0", "\n", "for", "fname", "in", "fnames", ":", "\n", "        ", "for", "summary", "in", "tf", ".", "train", ".", "summary_iterator", "(", "fname", ")", ":", "\n", "            ", "if", "summary", ".", "step", ">", "0", ":", "\n", "                ", "for", "v", "in", "summary", ".", "summary", ".", "value", ":", "\n", "                    ", "pair", "=", "(", "summary", ".", "step", ",", "v", ".", "simple_value", ")", "\n", "tag2pairs", "[", "v", ".", "tag", "]", ".", "append", "(", "pair", ")", "\n", "", "maxstep", "=", "max", "(", "summary", ".", "step", ",", "maxstep", ")", "\n", "", "", "", "data", "=", "np", ".", "empty", "(", "(", "maxstep", ",", "len", "(", "tag2pairs", ")", ")", ")", "\n", "data", "[", ":", "]", "=", "np", ".", "nan", "\n", "tags", "=", "sorted", "(", "tag2pairs", ".", "keys", "(", ")", ")", "\n", "for", "(", "colidx", ",", "tag", ")", "in", "enumerate", "(", "tags", ")", ":", "\n", "        ", "pairs", "=", "tag2pairs", "[", "tag", "]", "\n", "for", "(", "step", ",", "value", ")", "in", "pairs", ":", "\n", "            ", "data", "[", "step", "-", "1", ",", "colidx", "]", "=", "value", "\n", "", "", "return", "pandas", ".", "DataFrame", "(", "data", ",", "columns", "=", "tags", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail.wdail_hopper.argsparser": [[17, 73], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argsparser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\"WDAIL\"", ")", "\n", "parser", ".", "add_argument", "(", "'--env_name'", ",", "help", "=", "'environment ID'", ",", "default", "=", "'Hopper-v1'", ")", "\n", "parser", ".", "add_argument", "(", "'--algo'", ",", "help", "=", "'algorithm ID'", ",", "default", "=", "'WDAIL'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-dir'", ",", "default", "=", "'/tmp/gym/'", ",", "help", "=", "'directory to save agent logs (default: /tmp/gym)'", ")", "\n", "# general", "\n", "parser", ".", "add_argument", "(", "'--total_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--num_env_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluate_every'", ",", "help", "=", "'evaluate every'", ",", "type", "=", "int", ",", "default", "=", "2e10", ")", "\n", "parser", ".", "add_argument", "(", "'--save_condition'", ",", "help", "=", "'save_condition'", ",", "type", "=", "int", ",", "default", "=", "1000", ")", "\n", "parser", ".", "add_argument", "(", "'--num_model'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--use_device'", ",", "help", "=", "'use_device'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "help", "=", "'seed'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--use_linear_lr_decay'", ",", "help", "=", "'use linear lr decay'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--recurrent-policy'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'use a recurrent policy'", ")", "\n", "\n", "#ppo", "\n", "parser", ".", "add_argument", "(", "'--num_processes'", ",", "help", "=", "'num_processes'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--num-steps'", ",", "help", "=", "'num-steps'", ",", "type", "=", "int", ",", "default", "=", "2048", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "help", "=", "'learning rate'", ",", "type", "=", "float", ",", "default", "=", "3e-4", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "help", "=", "'batch size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_epoch'", ",", "help", "=", "'ppo epoch num'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--hidden_size'", ",", "help", "=", "'hidden size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_entcoeff'", ",", "help", "=", "'entropy coefficiency of policy'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ")", "#default=1e-3", "\n", "parser", ".", "add_argument", "(", "'--ppo_obs_norm'", ",", "help", "=", "'ppo_vec_norm'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--num-mini-batch'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "'number of batches for ppo (default: 32)'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-param'", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "help", "=", "'ppo clip parameter (default: 0.2)'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps'", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "help", "=", "'RMSprop optimizer epsilon (default: 1e-5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "'RMSprop optimizer apha (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "'discount factor for rewards (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "'--use-gae'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'use generalized advantage estimation'", ")", "\n", "parser", ".", "add_argument", "(", "'--gae-lambda'", ",", "type", "=", "float", ",", "default", "=", "0.95", ",", "help", "=", "'gae lambda parameter (default: 0.95)'", ")", "\n", "parser", ".", "add_argument", "(", "'--entropy-coef'", ",", "type", "=", "float", ",", "default", "=", "0.00", ",", "help", "=", "'entropy term coefficient (default: 0.01)'", ")", "\n", "parser", ".", "add_argument", "(", "'--value-loss-coef'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'value loss coefficient (default: 0.5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-grad-norm'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'max norm of gradients (default: 0.5)'", ")", "\n", "\n", "# #gail", "\n", "parser", ".", "add_argument", "(", "'--gail'", ",", "help", "=", "'if gail'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "# parser.add_argument('--expert_path', help='trajs path', type=str, default='../data/baseline/deterministic.trpo.HalfCheetah.0.00.npz')", "\n", "parser", ".", "add_argument", "(", "'--expert_path'", ",", "help", "=", "'trajs path'", ",", "type", "=", "str", ",", "default", "=", "'../data/ikostirkov/trajs_hopper.h5'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail-experts-dir'", ",", "default", "=", "'./gail_experts'", ",", "help", "=", "'directory that contains expert demonstrations for gail'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_batch_size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'gail batch size (default: 128)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_thre'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_pre_epoch'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "100", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_epoch'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "'--num_trajs'", ",", "help", "=", "'num trajs'", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "'--subsample_frequency'", ",", "help", "=", "'num trajs'", ",", "type", "=", "int", ",", "default", "=", "20", ")", "\n", "parser", ".", "add_argument", "(", "'--adversary_entcoeff'", ",", "help", "=", "'entropy coefficiency of discriminator'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ")", "\n", "parser", ".", "add_argument", "(", "'--use-proper-time-limits'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'compute returns taking into account time limits'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-interval'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'log interval, one log per n updates (default: 10)'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--reward_type'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'0,1,2,3,4'", ")", "\n", "parser", ".", "add_argument", "(", "'--update_rms'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "'False or True'", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail.wdail_hopper.train": [[74, 175], ["np.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "os.path.expanduser", "utils.cleanup_log_dir", "utils.cleanup_log_dir", "torch.set_num_threads", "torch.device", "make_vec_envs", "Policy", "Policy.to", "PPO", "Discriminator", "os.path.join", "torch.utils.data.DataLoader", "RolloutStorage", "gailLearning_mujoco_origin", "ExpertDataset", "torch.cuda.is_available", "[].lower", "str", "args.env_name.split"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.make_vec_envs", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.gailLearning_mujoco_origin"], ["", "def", "train", "(", "args", ")", ":", "\n", "\n", "# from ppo_gail_iko.algo.ppo4multienvs import PPO, ReplayBuffer", "\n", "    ", "from", "ppo_wdail", ".", "algo", ".", "ppo", "import", "PPO", "\n", "from", "ppo_wdail", ".", "tools", ".", "storage", "import", "RolloutStorage", "\n", "from", "ppo_wdail", ".", "tools", ".", "model", "import", "Policy", "\n", "\n", "from", "ppo_wdail", ".", "algo", ".", "wdgail", "import", "Discriminator", ",", "ExpertDataset", "\n", "from", "ppo_wdail", ".", "algo", ".", "mujoco_dset_zm_iko", "import", "Mujoco_Dset", "\n", "\n", "from", "ppo_wdail", ".", "tools", ".", "learn", "import", "gailLearning_mujoco", ",", "Learning_process", ",", "Learning_process_record", ",", "gailLearning_mujoco_test", ",", "gailLearning_mujoco_origin", "\n", "from", "ppo_wdail", ".", "tools", ".", "envs", "import", "make_vec_envs", "\n", "\n", "from", "ppo_wdail", ".", "tools", "import", "utli", "\n", "from", "ppo_wdail", ".", "tools", "import", "utils", "\n", "\n", "from", "collections", "import", "deque", "\n", "import", "time", "\n", "import", "numpy", "as", "np", "\n", "\n", "\n", "# from nets.network import ActorCritic_mujoco as ActorCritic", "\n", "cl_args", "=", "args", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "# if args.cuda and torch.cuda.is_available() and args.cuda_deterministic:", "\n", "#     torch.backends.cudnn.benchmark = False", "\n", "#     torch.backends.cudnn.deterministic = True", "\n", "\n", "log_dir", "=", "os", ".", "path", ".", "expanduser", "(", "args", ".", "log_dir", ")", "\n", "eval_log_dir", "=", "log_dir", "+", "\"_eval\"", "\n", "utils", ".", "cleanup_log_dir", "(", "log_dir", ")", "\n", "utils", ".", "cleanup_log_dir", "(", "eval_log_dir", ")", "\n", "\n", "torch", ".", "set_num_threads", "(", "1", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "'cuda:'", "+", "str", "(", "cl_args", ".", "cuda", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "# device = torch.device('cpu')", "\n", "\n", "envs", "=", "make_vec_envs", "(", "args", ".", "env_name", ",", "args", ".", "seed", ",", "args", ".", "num_processes", ",", "\n", "args", ".", "gamma", ",", "args", ".", "log_dir", ",", "device", ",", "False", ")", "\n", "\n", "# envs_eval = make_vec_envs(args.env_name, args.seed, args.num_processes,", "\n", "#                      args.gamma, args.log_dir, device, False)", "\n", "envs_eval", "=", "[", "]", "\n", "\n", "# network", "\n", "actor_critic", "=", "Policy", "(", "\n", "envs", ".", "observation_space", ".", "shape", ",", "\n", "envs", ".", "action_space", ",", "\n", "base_kwargs", "=", "{", "'recurrent'", ":", "args", ".", "recurrent_policy", "}", ")", "\n", "actor_critic", ".", "to", "(", "device", ")", "\n", "\n", "agent", "=", "PPO", "(", "\n", "actor_critic", ",", "\n", "args", ".", "clip_param", ",", "\n", "args", ".", "ppo_epoch", ",", "\n", "args", ".", "num_mini_batch", ",", "\n", "args", ".", "value_loss_coef", ",", "\n", "args", ".", "entropy_coef", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "eps", "=", "args", ".", "eps", ",", "\n", "max_grad_norm", "=", "args", ".", "max_grad_norm", ")", "\n", "\n", "# discriminator", "\n", "discr", "=", "Discriminator", "(", "envs", ".", "observation_space", ".", "shape", "[", "0", "]", "+", "envs", ".", "action_space", ".", "shape", "[", "0", "]", ",", "100", ",", "device", ",", "args", ".", "reward_type", ",", "args", ".", "update_rms", ")", "\n", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "gail_experts_dir", ",", "\"trajs_{}.pt\"", ".", "format", "(", "\n", "args", ".", "env_name", ".", "split", "(", "'-'", ")", "[", "0", "]", ".", "lower", "(", ")", ")", ")", "\n", "\n", "gail_train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "ExpertDataset", "(", "\n", "file_name", ",", "num_trajectories", "=", "args", ".", "num_trajs", ",", "subsample_frequency", "=", "args", ".", "subsample_frequency", ")", ",", "\n", "batch_size", "=", "args", ".", "gail_batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "drop_last", "=", "True", ")", "\n", "\n", "# The buffer", "\n", "rollouts", "=", "RolloutStorage", "(", "args", ".", "num_steps", ",", "args", ".", "num_processes", ",", "\n", "envs", ".", "observation_space", ".", "shape", ",", "envs", ".", "action_space", ",", "\n", "actor_critic", ".", "recurrent_hidden_state_size", ")", "\n", "\n", "# The buffer for the expert -> refer to dataset/mujoco_dset.py", "\n", "# expert_path = cl_args.expert_path+cl_args.env_id+\".h5\"", "\n", "# expert_buffer = Mujoco_Dset(cl_args.expert_path, traj_limitation=cl_args.num_trajs, subsample_frequency=args.subsample_frequency)", "\n", "\n", "model", "=", "gailLearning_mujoco_origin", "(", "cl_args", "=", "cl_args", ",", "\n", "envs", "=", "envs", ",", "\n", "envs_eval", "=", "envs_eval", ",", "\n", "actor_critic", "=", "actor_critic", ",", "\n", "agent", "=", "agent", ",", "\n", "discriminator", "=", "discr", ",", "\n", "rollouts", "=", "rollouts", ",", "\n", "gail_train_loader", "=", "gail_train_loader", ",", "\n", "device", "=", "device", ",", "\n", "utli", "=", "utli", ")", "\n", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail.wdail_hopper.main": [[177, 207], ["wdail_hopper.train"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.train"], ["", "def", "main", "(", "args", ")", ":", "\n", "\n", "    ", "model", ",", "env", "=", "train", "(", "args", ")", "\n", "\n", "# if args.play:", "\n", "#", "\n", "#     obs = env.reset()", "\n", "#", "\n", "#     state = model.initial_state if hasattr(model, 'initial_state') else None", "\n", "#     dones = np.zeros((1,))", "\n", "#", "\n", "#     episode_rew = 0", "\n", "#     while True:", "\n", "#         if state is not None:", "\n", "#             actions, _, state, _ = model.step(obs,S=state, M=dones)", "\n", "#         else:", "\n", "#             actions, _, _, _ = model.step(obs)", "\n", "#", "\n", "#         obs, rew, done, _ = env.step(actions)", "\n", "#         episode_rew += rew[0] if isinstance(env, VecEnv) else rew", "\n", "#         env.render()", "\n", "#         done = done.any() if isinstance(done, np.ndarray) else done", "\n", "#         if done:", "\n", "#             print(f'episode_rew={episode_rew}')", "\n", "#             episode_rew = 0", "\n", "#             obs = env.reset()", "\n", "#", "\n", "#     env.close()", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail.wdail_reacher.argsparser": [[17, 73], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argsparser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\"WDAIL\"", ")", "\n", "parser", ".", "add_argument", "(", "'--env_name'", ",", "help", "=", "'environment ID'", ",", "default", "=", "'Reacher-v1'", ")", "\n", "parser", ".", "add_argument", "(", "'--algo'", ",", "help", "=", "'algorithm ID'", ",", "default", "=", "'WDAIL'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-dir'", ",", "default", "=", "'/tmp/gym/'", ",", "help", "=", "'directory to save agent logs (default: /tmp/gym)'", ")", "\n", "# general", "\n", "parser", ".", "add_argument", "(", "'--total_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--num_env_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluate_every'", ",", "help", "=", "'evaluate every'", ",", "type", "=", "int", ",", "default", "=", "2e10", ")", "\n", "parser", ".", "add_argument", "(", "'--save_condition'", ",", "help", "=", "'save_condition'", ",", "type", "=", "int", ",", "default", "=", "1000", ")", "\n", "parser", ".", "add_argument", "(", "'--num_model'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--use_device'", ",", "help", "=", "'use_device'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "help", "=", "'seed'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--use_linear_lr_decay'", ",", "help", "=", "'use linear lr decay'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--recurrent-policy'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'use a recurrent policy'", ")", "\n", "\n", "#ppo", "\n", "parser", ".", "add_argument", "(", "'--num_processes'", ",", "help", "=", "'num_processes'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--num_steps'", ",", "help", "=", "'num-steps'", ",", "type", "=", "int", ",", "default", "=", "2048", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "help", "=", "'learning rate'", ",", "type", "=", "float", ",", "default", "=", "3e-4", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "help", "=", "'batch size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_epoch'", ",", "help", "=", "'ppo epoch num'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--hidden_size'", ",", "help", "=", "'hidden size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_entcoeff'", ",", "help", "=", "'entropy coefficiency of policy'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ")", "#default=1e-3", "\n", "parser", ".", "add_argument", "(", "'--ppo_obs_norm'", ",", "help", "=", "'ppo_vec_norm'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--num-mini-batch'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "'number of batches for ppo (default: 32)'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-param'", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "help", "=", "'ppo clip parameter (default: 0.2)'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps'", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "help", "=", "'RMSprop optimizer epsilon (default: 1e-5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "'RMSprop optimizer apha (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "'discount factor for rewards (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "'--use-gae'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'use generalized advantage estimation'", ")", "\n", "parser", ".", "add_argument", "(", "'--gae-lambda'", ",", "type", "=", "float", ",", "default", "=", "0.95", ",", "help", "=", "'gae lambda parameter (default: 0.95)'", ")", "\n", "parser", ".", "add_argument", "(", "'--entropy-coef'", ",", "type", "=", "float", ",", "default", "=", "0.00", ",", "help", "=", "'entropy term coefficient (default: 0.01)'", ")", "\n", "parser", ".", "add_argument", "(", "'--value-loss-coef'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'value loss coefficient (default: 0.5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-grad-norm'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'max norm of gradients (default: 0.5)'", ")", "\n", "\n", "# #gail", "\n", "parser", ".", "add_argument", "(", "'--gail'", ",", "help", "=", "'if gail'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "# parser.add_argument('--expert_path', help='trajs path', type=str, default='../data/baseline/deterministic.trpo.HalfCheetah.0.00.npz')", "\n", "parser", ".", "add_argument", "(", "'--expert_path'", ",", "help", "=", "'trajs path'", ",", "type", "=", "str", ",", "default", "=", "'../data/ikostirkov/trajs_reacher.h5'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail-experts-dir'", ",", "default", "=", "'./gail_experts'", ",", "help", "=", "'directory that contains expert demonstrations for gail'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_batch_size'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "'gail batch size (default: 128)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_thre'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_pre_epoch'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "100", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_epoch'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "'--num_trajs'", ",", "help", "=", "'num trajs'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--subsample_frequency'", ",", "help", "=", "'num trajs'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--adversary_entcoeff'", ",", "help", "=", "'entropy coefficiency of discriminator'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ")", "\n", "parser", ".", "add_argument", "(", "'--use-proper-time-limits'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'compute returns taking into account time limits'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-interval'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'log interval, one log per n updates (default: 10)'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--reward_type'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'0,1,2,3,4'", ")", "\n", "parser", ".", "add_argument", "(", "'--update_rms'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "'False or True'", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail.wdail_reacher.train": [[74, 216], ["np.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "os.path.expanduser", "utils.cleanup_log_dir", "utils.cleanup_log_dir", "torch.set_num_threads", "torch.device", "make_vec_envs", "Policy", "Policy.to", "PPO", "Discriminator", "os.path.join", "torch.utils.data.DataLoader", "RolloutStorage", "Mujoco_Dset", "gailLearning_mujoco_origin", "ExpertDataset", "torch.cuda.is_available", "[].lower", "str", "args.env_name.split"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.make_vec_envs", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.gailLearning_mujoco_origin"], ["", "def", "train", "(", "args", ")", ":", "\n", "\n", "# from ppo_gail_iko.algo.ppo4multienvs import PPO, ReplayBuffer", "\n", "    ", "from", "ppo_wdail", ".", "algo", ".", "ppo", "import", "PPO", "\n", "from", "ppo_wdail", ".", "tools", ".", "storage", "import", "RolloutStorage", "\n", "from", "ppo_wdail", ".", "tools", ".", "model", "import", "Policy", "\n", "\n", "from", "ppo_wdail", ".", "algo", ".", "wdgail", "import", "Discriminator", ",", "ExpertDataset", "\n", "from", "ppo_wdail", ".", "algo", ".", "mujoco_dset_zm_iko", "import", "Mujoco_Dset", "\n", "\n", "from", "ppo_wdail", ".", "tools", ".", "learn", "import", "gailLearning_mujoco", ",", "Learning_process", ",", "Learning_process_record", ",", "gailLearning_mujoco_test", ",", "gailLearning_mujoco_origin", "\n", "from", "ppo_wdail", ".", "tools", ".", "envs", "import", "make_vec_envs", "\n", "\n", "from", "ppo_wdail", ".", "tools", "import", "utli", "\n", "from", "ppo_wdail", ".", "tools", "import", "utils", "\n", "\n", "from", "collections", "import", "deque", "\n", "import", "time", "\n", "import", "numpy", "as", "np", "\n", "\n", "\n", "# from nets.network import ActorCritic_mujoco as ActorCritic", "\n", "cl_args", "=", "args", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "# if args.cuda and torch.cuda.is_available() and args.cuda_deterministic:", "\n", "#     torch.backends.cudnn.benchmark = False", "\n", "#     torch.backends.cudnn.deterministic = True", "\n", "\n", "log_dir", "=", "os", ".", "path", ".", "expanduser", "(", "args", ".", "log_dir", ")", "\n", "eval_log_dir", "=", "log_dir", "+", "\"_eval\"", "\n", "utils", ".", "cleanup_log_dir", "(", "log_dir", ")", "\n", "utils", ".", "cleanup_log_dir", "(", "eval_log_dir", ")", "\n", "\n", "torch", ".", "set_num_threads", "(", "1", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "'cuda:'", "+", "str", "(", "cl_args", ".", "cuda", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "# device = torch.device('cpu')", "\n", "\n", "envs", "=", "make_vec_envs", "(", "args", ".", "env_name", ",", "args", ".", "seed", ",", "args", ".", "num_processes", ",", "\n", "args", ".", "gamma", ",", "args", ".", "log_dir", ",", "device", ",", "False", ")", "\n", "\n", "# envs_eval = make_vec_envs(args.env_name, args.seed, args.num_processes,", "\n", "#                      args.gamma, args.log_dir, device, False)", "\n", "envs_eval", "=", "[", "]", "\n", "\n", "# network", "\n", "actor_critic", "=", "Policy", "(", "\n", "envs", ".", "observation_space", ".", "shape", ",", "\n", "envs", ".", "action_space", ",", "\n", "base_kwargs", "=", "{", "'recurrent'", ":", "args", ".", "recurrent_policy", "}", ")", "\n", "actor_critic", ".", "to", "(", "device", ")", "\n", "\n", "agent", "=", "PPO", "(", "\n", "actor_critic", ",", "\n", "args", ".", "clip_param", ",", "\n", "args", ".", "ppo_epoch", ",", "\n", "args", ".", "num_mini_batch", ",", "\n", "args", ".", "value_loss_coef", ",", "\n", "args", ".", "entropy_coef", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "eps", "=", "args", ".", "eps", ",", "\n", "max_grad_norm", "=", "args", ".", "max_grad_norm", ")", "\n", "\n", "# discriminator", "\n", "discr", "=", "Discriminator", "(", "envs", ".", "observation_space", ".", "shape", "[", "0", "]", "+", "envs", ".", "action_space", ".", "shape", "[", "0", "]", ",", "100", ",", "device", ",", "args", ".", "reward_type", ",", "args", ".", "update_rms", ")", "\n", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "gail_experts_dir", ",", "\"trajs_{}.pt\"", ".", "format", "(", "\n", "args", ".", "env_name", ".", "split", "(", "'-'", ")", "[", "0", "]", ".", "lower", "(", ")", ")", ")", "\n", "\n", "gail_train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "ExpertDataset", "(", "\n", "file_name", ",", "num_trajectories", "=", "args", ".", "num_trajs", ",", "subsample_frequency", "=", "args", ".", "subsample_frequency", ")", ",", "\n", "batch_size", "=", "args", ".", "gail_batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "drop_last", "=", "True", ")", "\n", "\n", "# The buffer", "\n", "rollouts", "=", "RolloutStorage", "(", "args", ".", "num_steps", ",", "args", ".", "num_processes", ",", "\n", "envs", ".", "observation_space", ".", "shape", ",", "envs", ".", "action_space", ",", "\n", "actor_critic", ".", "recurrent_hidden_state_size", ")", "\n", "\n", "# The buffer for the expert -> refer to dataset/mujoco_dset.py", "\n", "# expert_path = cl_args.expert_path+cl_args.env_id+\".h5\"", "\n", "expert_buffer", "=", "Mujoco_Dset", "(", "cl_args", ".", "expert_path", ",", "traj_limitation", "=", "cl_args", ".", "num_trajs", ",", "subsample_frequency", "=", "args", ".", "subsample_frequency", ")", "\n", "\n", "# model = gailLearning_mujoco_test(cl_args=cl_args,", "\n", "#                                  envs=envs,", "\n", "#                                  envs_eval=envs_eval,", "\n", "#                                  actor_critic=actor_critic,", "\n", "#                                  agent=agent,", "\n", "#                                  discriminator=discr,", "\n", "#                                  rollouts=rollouts,", "\n", "#                                  expert_buffer=expert_buffer,", "\n", "#                                  device=device,", "\n", "#                                  utli=utli)", "\n", "\n", "model", "=", "gailLearning_mujoco_origin", "(", "cl_args", "=", "cl_args", ",", "\n", "envs", "=", "envs", ",", "\n", "envs_eval", "=", "envs_eval", ",", "\n", "actor_critic", "=", "actor_critic", ",", "\n", "agent", "=", "agent", ",", "\n", "discriminator", "=", "discr", ",", "\n", "rollouts", "=", "rollouts", ",", "\n", "gail_train_loader", "=", "gail_train_loader", ",", "\n", "device", "=", "device", ",", "\n", "utli", "=", "utli", ")", "\n", "\n", "# model = gailLearning_mujoco(cl_args=cl_args,", "\n", "#                             envs=envs,", "\n", "#                             envs_eval=envs_eval,", "\n", "#                             actor_critic=actor_critic,", "\n", "#                             agent=agent,", "\n", "#                             discriminator=discr,", "\n", "#                             rollouts=rollouts,", "\n", "#                             gail_train_loader=gail_train_loader,", "\n", "#                             device=device,", "\n", "#                             utli=utli)", "\n", "\n", "# sign = Learning_process_record(args=args,", "\n", "#                                envs=envs,", "\n", "#                                rollouts=rollouts,", "\n", "#                                actor_critic=actor_critic,", "\n", "#                                agent=agent,", "\n", "#                                discr=discr,", "\n", "#                                gail_train_loader=gail_train_loader,", "\n", "#                                device=device)", "\n", "\n", "# sign = Learning_process(args=args,", "\n", "#                         envs=envs,", "\n", "#                         rollouts=rollouts,", "\n", "#                         actor_critic=actor_critic,", "\n", "#                         agent=agent,", "\n", "#                         discr=discr,", "\n", "#                         gail_train_loader=gail_train_loader,", "\n", "#                         device=device)", "\n", "\n", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail.wdail_reacher.main": [[218, 248], ["wdail_reacher.train"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.train"], ["", "def", "main", "(", "args", ")", ":", "\n", "\n", "    ", "model", ",", "env", "=", "train", "(", "args", ")", "\n", "\n", "# if args.play:", "\n", "#", "\n", "#     obs = env.reset()", "\n", "#", "\n", "#     state = model.initial_state if hasattr(model, 'initial_state') else None", "\n", "#     dones = np.zeros((1,))", "\n", "#", "\n", "#     episode_rew = 0", "\n", "#     while True:", "\n", "#         if state is not None:", "\n", "#             actions, _, state, _ = model.step(obs,S=state, M=dones)", "\n", "#         else:", "\n", "#             actions, _, _, _ = model.step(obs)", "\n", "#", "\n", "#         obs, rew, done, _ = env.step(actions)", "\n", "#         episode_rew += rew[0] if isinstance(env, VecEnv) else rew", "\n", "#         env.render()", "\n", "#         done = done.any() if isinstance(done, np.ndarray) else done", "\n", "#         if done:", "\n", "#             print(f'episode_rew={episode_rew}')", "\n", "#             episode_rew = 0", "\n", "#             obs = env.reset()", "\n", "#", "\n", "#     env.close()", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail.wdail_halfcheetah.argsparser": [[17, 73], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argsparser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\"WDAIL\"", ")", "\n", "parser", ".", "add_argument", "(", "'--env_name'", ",", "help", "=", "'environment ID'", ",", "default", "=", "'HalfCheetah-v2'", ")", "\n", "parser", ".", "add_argument", "(", "'--algo'", ",", "help", "=", "'algorithm ID'", ",", "default", "=", "'WDAIL'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-dir'", ",", "default", "=", "'/tmp/gym/'", ",", "help", "=", "'directory to save agent logs (default: /tmp/gym)'", ")", "\n", "# general", "\n", "parser", ".", "add_argument", "(", "'--total_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--num_env_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluate_every'", ",", "help", "=", "'evaluate every'", ",", "type", "=", "int", ",", "default", "=", "2e10", ")", "\n", "parser", ".", "add_argument", "(", "'--save_condition'", ",", "help", "=", "'save_condition'", ",", "type", "=", "int", ",", "default", "=", "1000", ")", "\n", "parser", ".", "add_argument", "(", "'--num_model'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--use_device'", ",", "help", "=", "'use_device'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "help", "=", "'seed'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--use_linear_lr_decay'", ",", "help", "=", "'use linear lr decay'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--recurrent-policy'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'use a recurrent policy'", ")", "\n", "\n", "#ppo", "\n", "parser", ".", "add_argument", "(", "'--num_processes'", ",", "help", "=", "'num_processes'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--num-steps'", ",", "help", "=", "'num-steps'", ",", "type", "=", "int", ",", "default", "=", "2048", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "help", "=", "'learning rate'", ",", "type", "=", "float", ",", "default", "=", "3e-4", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "help", "=", "'batch size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_epoch'", ",", "help", "=", "'ppo epoch num'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--hidden_size'", ",", "help", "=", "'hidden size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_entcoeff'", ",", "help", "=", "'entropy coefficiency of policy'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ")", "#default=1e-3", "\n", "parser", ".", "add_argument", "(", "'--ppo_obs_norm'", ",", "help", "=", "'ppo_vec_norm'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--num-mini-batch'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "'number of batches for ppo (default: 32)'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-param'", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "help", "=", "'ppo clip parameter (default: 0.2)'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps'", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "help", "=", "'RMSprop optimizer epsilon (default: 1e-5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "'RMSprop optimizer apha (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "'discount factor for rewards (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "'--use-gae'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'use generalized advantage estimation'", ")", "\n", "parser", ".", "add_argument", "(", "'--gae-lambda'", ",", "type", "=", "float", ",", "default", "=", "0.95", ",", "help", "=", "'gae lambda parameter (default: 0.95)'", ")", "\n", "parser", ".", "add_argument", "(", "'--entropy-coef'", ",", "type", "=", "float", ",", "default", "=", "0.00", ",", "help", "=", "'entropy term coefficient (default: 0.01)'", ")", "\n", "parser", ".", "add_argument", "(", "'--value-loss-coef'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'value loss coefficient (default: 0.5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-grad-norm'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'max norm of gradients (default: 0.5)'", ")", "\n", "\n", "# #gail", "\n", "parser", ".", "add_argument", "(", "'--gail'", ",", "help", "=", "'if gail'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "# parser.add_argument('--expert_path', help='trajs path', type=str, default='../data/baseline/deterministic.trpo.HalfCheetah.0.00.npz')", "\n", "parser", ".", "add_argument", "(", "'--expert_path'", ",", "help", "=", "'trajs path'", ",", "type", "=", "str", ",", "default", "=", "'../data/ikostirkov/trajs_halfcheetah.h5'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail-experts-dir'", ",", "default", "=", "'./gail_experts'", ",", "help", "=", "'directory that contains expert demonstrations for gail'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_batch_size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'gail batch size (default: 128)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_thre'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_pre_epoch'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "100", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_epoch'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "'--num_trajs'", ",", "help", "=", "'num trajs'", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "'--subsample_frequency'", ",", "help", "=", "'num trajs'", ",", "type", "=", "int", ",", "default", "=", "20", ")", "\n", "parser", ".", "add_argument", "(", "'--adversary_entcoeff'", ",", "help", "=", "'entropy coefficiency of discriminator'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ")", "\n", "parser", ".", "add_argument", "(", "'--use-proper-time-limits'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'compute returns taking into account time limits'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-interval'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'log interval, one log per n updates (default: 10)'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--reward_type'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'0,1,2,3,4'", ")", "\n", "parser", ".", "add_argument", "(", "'--update_rms'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "'False or True'", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail.wdail_halfcheetah.train": [[74, 216], ["np.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "os.path.expanduser", "utils.cleanup_log_dir", "utils.cleanup_log_dir", "torch.set_num_threads", "torch.device", "make_vec_envs", "Policy", "Policy.to", "PPO", "Discriminator", "os.path.join", "torch.utils.data.DataLoader", "RolloutStorage", "gailLearning_mujoco_origin", "ExpertDataset", "torch.cuda.is_available", "[].lower", "str", "args.env_name.split"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.make_vec_envs", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.gailLearning_mujoco_origin"], ["", "def", "train", "(", "args", ")", ":", "\n", "\n", "# from ppo_gail_iko.algo.ppo4multienvs import PPO, ReplayBuffer", "\n", "    ", "from", "ppo_wdail", ".", "algo", ".", "ppo", "import", "PPO", "\n", "from", "ppo_wdail", ".", "tools", ".", "storage", "import", "RolloutStorage", "\n", "from", "ppo_wdail", ".", "tools", ".", "model", "import", "Policy", "\n", "\n", "from", "ppo_wdail", ".", "algo", ".", "wdgail", "import", "Discriminator", ",", "ExpertDataset", "\n", "from", "ppo_wdail", ".", "algo", ".", "mujoco_dset_zm_iko", "import", "Mujoco_Dset", "\n", "\n", "from", "ppo_wdail", ".", "tools", ".", "learn", "import", "gailLearning_mujoco", ",", "Learning_process", ",", "Learning_process_record", ",", "gailLearning_mujoco_test", ",", "gailLearning_mujoco_origin", "\n", "from", "ppo_wdail", ".", "tools", ".", "envs", "import", "make_vec_envs", "\n", "\n", "from", "ppo_wdail", ".", "tools", "import", "utli", "\n", "from", "ppo_wdail", ".", "tools", "import", "utils", "\n", "\n", "from", "collections", "import", "deque", "\n", "import", "time", "\n", "import", "numpy", "as", "np", "\n", "\n", "\n", "# from nets.network import ActorCritic_mujoco as ActorCritic", "\n", "cl_args", "=", "args", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "# if args.cuda and torch.cuda.is_available() and args.cuda_deterministic:", "\n", "#     torch.backends.cudnn.benchmark = False", "\n", "#     torch.backends.cudnn.deterministic = True", "\n", "\n", "log_dir", "=", "os", ".", "path", ".", "expanduser", "(", "args", ".", "log_dir", ")", "\n", "eval_log_dir", "=", "log_dir", "+", "\"_eval\"", "\n", "utils", ".", "cleanup_log_dir", "(", "log_dir", ")", "\n", "utils", ".", "cleanup_log_dir", "(", "eval_log_dir", ")", "\n", "\n", "torch", ".", "set_num_threads", "(", "1", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "'cuda:'", "+", "str", "(", "cl_args", ".", "cuda", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "# device = torch.device('cpu')", "\n", "\n", "envs", "=", "make_vec_envs", "(", "args", ".", "env_name", ",", "args", ".", "seed", ",", "args", ".", "num_processes", ",", "\n", "args", ".", "gamma", ",", "args", ".", "log_dir", ",", "device", ",", "False", ")", "\n", "\n", "# envs_eval = make_vec_envs(args.env_name, args.seed, args.num_processes,", "\n", "#                      args.gamma, args.log_dir, device, False)", "\n", "envs_eval", "=", "[", "]", "\n", "\n", "# network", "\n", "actor_critic", "=", "Policy", "(", "\n", "envs", ".", "observation_space", ".", "shape", ",", "\n", "envs", ".", "action_space", ",", "\n", "base_kwargs", "=", "{", "'recurrent'", ":", "args", ".", "recurrent_policy", "}", ")", "\n", "actor_critic", ".", "to", "(", "device", ")", "\n", "\n", "agent", "=", "PPO", "(", "\n", "actor_critic", ",", "\n", "args", ".", "clip_param", ",", "\n", "args", ".", "ppo_epoch", ",", "\n", "args", ".", "num_mini_batch", ",", "\n", "args", ".", "value_loss_coef", ",", "\n", "args", ".", "entropy_coef", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "eps", "=", "args", ".", "eps", ",", "\n", "max_grad_norm", "=", "args", ".", "max_grad_norm", ")", "\n", "\n", "# discriminator", "\n", "discr", "=", "Discriminator", "(", "envs", ".", "observation_space", ".", "shape", "[", "0", "]", "+", "envs", ".", "action_space", ".", "shape", "[", "0", "]", ",", "100", ",", "device", ",", "args", ".", "reward_type", ",", "args", ".", "update_rms", ")", "\n", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "gail_experts_dir", ",", "\"trajs_{}.pt\"", ".", "format", "(", "\n", "args", ".", "env_name", ".", "split", "(", "'-'", ")", "[", "0", "]", ".", "lower", "(", ")", ")", ")", "\n", "\n", "gail_train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "ExpertDataset", "(", "\n", "file_name", ",", "num_trajectories", "=", "args", ".", "num_trajs", ",", "subsample_frequency", "=", "args", ".", "subsample_frequency", ")", ",", "\n", "batch_size", "=", "args", ".", "gail_batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "drop_last", "=", "True", ")", "\n", "\n", "# The buffer", "\n", "rollouts", "=", "RolloutStorage", "(", "args", ".", "num_steps", ",", "args", ".", "num_processes", ",", "\n", "envs", ".", "observation_space", ".", "shape", ",", "envs", ".", "action_space", ",", "\n", "actor_critic", ".", "recurrent_hidden_state_size", ")", "\n", "\n", "# The buffer for the expert -> refer to dataset/mujoco_dset.py", "\n", "# expert_path = cl_args.expert_path+cl_args.env_id+\".h5\"", "\n", "# expert_buffer = Mujoco_Dset(cl_args.expert_path, traj_limitation=cl_args.num_trajs, subsample_frequency=20)", "\n", "\n", "# model = gailLearning_mujoco_test(cl_args=cl_args,", "\n", "#                                  envs=envs,", "\n", "#                                  envs_eval=envs_eval,", "\n", "#                                  actor_critic=actor_critic,", "\n", "#                                  agent=agent,", "\n", "#                                  discriminator=discr,", "\n", "#                                  rollouts=rollouts,", "\n", "#                                  expert_buffer=expert_buffer,", "\n", "#                                  device=device,", "\n", "#                                  utli=utli)", "\n", "\n", "model", "=", "gailLearning_mujoco_origin", "(", "cl_args", "=", "cl_args", ",", "\n", "envs", "=", "envs", ",", "\n", "envs_eval", "=", "envs_eval", ",", "\n", "actor_critic", "=", "actor_critic", ",", "\n", "agent", "=", "agent", ",", "\n", "discriminator", "=", "discr", ",", "\n", "rollouts", "=", "rollouts", ",", "\n", "gail_train_loader", "=", "gail_train_loader", ",", "\n", "device", "=", "device", ",", "\n", "utli", "=", "utli", ")", "\n", "\n", "# model = gailLearning_mujoco(cl_args=cl_args,", "\n", "#                             envs=envs,", "\n", "#                             envs_eval=envs_eval,", "\n", "#                             actor_critic=actor_critic,", "\n", "#                             agent=agent,", "\n", "#                             discriminator=discr,", "\n", "#                             rollouts=rollouts,", "\n", "#                             gail_train_loader=gail_train_loader,", "\n", "#                             device=device,", "\n", "#                             utli=utli)", "\n", "\n", "# sign = Learning_process_record(args=args,", "\n", "#                                envs=envs,", "\n", "#                                rollouts=rollouts,", "\n", "#                                actor_critic=actor_critic,", "\n", "#                                agent=agent,", "\n", "#                                discr=discr,", "\n", "#                                gail_train_loader=gail_train_loader,", "\n", "#                                device=device)", "\n", "\n", "# sign = Learning_process(args=args,", "\n", "#                         envs=envs,", "\n", "#                         rollouts=rollouts,", "\n", "#                         actor_critic=actor_critic,", "\n", "#                         agent=agent,", "\n", "#                         discr=discr,", "\n", "#                         gail_train_loader=gail_train_loader,", "\n", "#                         device=device)", "\n", "\n", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail.wdail_halfcheetah.main": [[218, 248], ["wdail_halfcheetah.train"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.train"], ["", "def", "main", "(", "args", ")", ":", "\n", "\n", "    ", "model", ",", "env", "=", "train", "(", "args", ")", "\n", "\n", "# if args.play:", "\n", "#", "\n", "#     obs = env.reset()", "\n", "#", "\n", "#     state = model.initial_state if hasattr(model, 'initial_state') else None", "\n", "#     dones = np.zeros((1,))", "\n", "#", "\n", "#     episode_rew = 0", "\n", "#     while True:", "\n", "#         if state is not None:", "\n", "#             actions, _, state, _ = model.step(obs,S=state, M=dones)", "\n", "#         else:", "\n", "#             actions, _, _, _ = model.step(obs)", "\n", "#", "\n", "#         obs, rew, done, _ = env.step(actions)", "\n", "#         episode_rew += rew[0] if isinstance(env, VecEnv) else rew", "\n", "#         env.render()", "\n", "#         done = done.any() if isinstance(done, np.ndarray) else done", "\n", "#         if done:", "\n", "#             print(f'episode_rew={episode_rew}')", "\n", "#             episode_rew = 0", "\n", "#             obs = env.reset()", "\n", "#", "\n", "#     env.close()", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail.wdail_walker.argsparser": [[17, 73], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argsparser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\"WDAIL\"", ")", "\n", "parser", ".", "add_argument", "(", "'--env_name'", ",", "help", "=", "'environment ID'", ",", "default", "=", "'Walker2d-v2'", ")", "\n", "parser", ".", "add_argument", "(", "'--algo'", ",", "help", "=", "'algorithm ID'", ",", "default", "=", "'WDAIL'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-dir'", ",", "default", "=", "'/tmp/gym/'", ",", "help", "=", "'directory to save agent logs (default: /tmp/gym)'", ")", "\n", "# general", "\n", "parser", ".", "add_argument", "(", "'--total_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--num_env_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluate_every'", ",", "help", "=", "'evaluate every'", ",", "type", "=", "int", ",", "default", "=", "2e10", ")", "\n", "parser", ".", "add_argument", "(", "'--save_condition'", ",", "help", "=", "'save_condition'", ",", "type", "=", "int", ",", "default", "=", "1000", ")", "\n", "parser", ".", "add_argument", "(", "'--num_model'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--use_device'", ",", "help", "=", "'use_device'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "help", "=", "'seed'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--use_linear_lr_decay'", ",", "help", "=", "'use linear lr decay'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--recurrent-policy'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'use a recurrent policy'", ")", "\n", "\n", "#ppo", "\n", "parser", ".", "add_argument", "(", "'--num_processes'", ",", "help", "=", "'num_processes'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--nsteps'", ",", "help", "=", "'nsteps'", ",", "type", "=", "int", ",", "default", "=", "2048", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "help", "=", "'learning rate'", ",", "type", "=", "float", ",", "default", "=", "3e-4", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "help", "=", "'batch size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_epoch'", ",", "help", "=", "'ppo epoch num'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--hidden_size'", ",", "help", "=", "'hidden size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_entcoeff'", ",", "help", "=", "'entropy coefficiency of policy'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ")", "#default=1e-3", "\n", "parser", ".", "add_argument", "(", "'--ppo_obs_norm'", ",", "help", "=", "'ppo_vec_norm'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--num-mini-batch'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "'number of batches for ppo (default: 32)'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-param'", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "help", "=", "'ppo clip parameter (default: 0.2)'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps'", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "help", "=", "'RMSprop optimizer epsilon (default: 1e-5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "'RMSprop optimizer apha (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "'discount factor for rewards (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "'--use-gae'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'use generalized advantage estimation'", ")", "\n", "parser", ".", "add_argument", "(", "'--gae-lambda'", ",", "type", "=", "float", ",", "default", "=", "0.95", ",", "help", "=", "'gae lambda parameter (default: 0.95)'", ")", "\n", "parser", ".", "add_argument", "(", "'--entropy-coef'", ",", "type", "=", "float", ",", "default", "=", "0.00", ",", "help", "=", "'entropy term coefficient (default: 0.01)'", ")", "\n", "parser", ".", "add_argument", "(", "'--value-loss-coef'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'value loss coefficient (default: 0.5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-grad-norm'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'max norm of gradients (default: 0.5)'", ")", "\n", "\n", "# #gail", "\n", "parser", ".", "add_argument", "(", "'--gail'", ",", "help", "=", "'if gail'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "# parser.add_argument('--expert_path', help='trajs path', type=str, default='../data/baseline/deterministic.trpo.HalfCheetah.0.00.npz')", "\n", "parser", ".", "add_argument", "(", "'--expert_path'", ",", "help", "=", "'trajs path'", ",", "type", "=", "str", ",", "default", "=", "'../data/ikostirkov/trajs_walker.h5'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail-experts-dir'", ",", "default", "=", "'./gail_experts'", ",", "help", "=", "'directory that contains expert demonstrations for gail'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_batch_size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'gail batch size (default: 128)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_thre'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_pre_epoch'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "100", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_epoch'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "'--num_trajs'", ",", "help", "=", "'num trajs'", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "'--subsample_frequency'", ",", "help", "=", "'num trajs'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--adversary_entcoeff'", ",", "help", "=", "'entropy coefficiency of discriminator'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ")", "\n", "parser", ".", "add_argument", "(", "'--use-proper-time-limits'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'compute returns taking into account time limits'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-interval'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'log interval, one log per n updates (default: 10)'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--reward_type'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'0,1,2,3,4'", ")", "\n", "parser", ".", "add_argument", "(", "'--update_rms'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "'False or True'", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail.wdail_walker.train": [[74, 176], ["np.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "os.path.expanduser", "utils.cleanup_log_dir", "utils.cleanup_log_dir", "torch.set_num_threads", "torch.device", "make_vec_envs", "Policy", "Policy.to", "PPO", "Discriminator", "os.path.join", "torch.utils.data.DataLoader", "RolloutStorage", "gailLearning_mujoco_origin", "ExpertDataset", "torch.cuda.is_available", "[].lower", "str", "args.env_name.split"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.make_vec_envs", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.gailLearning_mujoco_origin"], ["", "def", "train", "(", "args", ")", ":", "\n", "\n", "# from ppo_gail_iko.algo.ppo4multienvs import PPO, ReplayBuffer", "\n", "    ", "from", "ppo_wdail", ".", "algo", ".", "ppo", "import", "PPO", "\n", "from", "ppo_wdail", ".", "tools", ".", "storage", "import", "RolloutStorage", "\n", "from", "ppo_wdail", ".", "tools", ".", "model", "import", "Policy", "\n", "\n", "from", "ppo_wdail", ".", "algo", ".", "wdgail", "import", "Discriminator", ",", "ExpertDataset", "\n", "from", "ppo_wdail", ".", "algo", ".", "mujoco_dset_zm_iko", "import", "Mujoco_Dset", "\n", "\n", "from", "ppo_wdail", ".", "tools", ".", "learn", "import", "gailLearning_mujoco", ",", "Learning_process", ",", "Learning_process_record", ",", "gailLearning_mujoco_test", ",", "gailLearning_mujoco_origin", "\n", "from", "ppo_wdail", ".", "tools", ".", "envs", "import", "make_vec_envs", "\n", "\n", "from", "ppo_wdail", ".", "tools", "import", "utli", "\n", "from", "ppo_wdail", ".", "tools", "import", "utils", "\n", "\n", "from", "collections", "import", "deque", "\n", "import", "time", "\n", "import", "numpy", "as", "np", "\n", "\n", "\n", "# from nets.network import ActorCritic_mujoco as ActorCritic", "\n", "cl_args", "=", "args", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "# if args.cuda and torch.cuda.is_available() and args.cuda_deterministic:", "\n", "#     torch.backends.cudnn.benchmark = False", "\n", "#     torch.backends.cudnn.deterministic = True", "\n", "\n", "log_dir", "=", "os", ".", "path", ".", "expanduser", "(", "args", ".", "log_dir", ")", "\n", "eval_log_dir", "=", "log_dir", "+", "\"_eval\"", "\n", "utils", ".", "cleanup_log_dir", "(", "log_dir", ")", "\n", "utils", ".", "cleanup_log_dir", "(", "eval_log_dir", ")", "\n", "\n", "torch", ".", "set_num_threads", "(", "1", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "'cuda:'", "+", "str", "(", "cl_args", ".", "cuda", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "# device = torch.device('cpu')", "\n", "\n", "envs", "=", "make_vec_envs", "(", "args", ".", "env_name", ",", "args", ".", "seed", ",", "args", ".", "num_processes", ",", "\n", "args", ".", "gamma", ",", "args", ".", "log_dir", ",", "device", ",", "False", ")", "\n", "\n", "# envs_eval = make_vec_envs(args.env_name, args.seed, args.num_processes,", "\n", "#                      args.gamma, args.log_dir, device, False)", "\n", "envs_eval", "=", "[", "]", "\n", "\n", "# network", "\n", "actor_critic", "=", "Policy", "(", "\n", "envs", ".", "observation_space", ".", "shape", ",", "\n", "envs", ".", "action_space", ",", "\n", "base_kwargs", "=", "{", "'recurrent'", ":", "args", ".", "recurrent_policy", "}", ")", "\n", "actor_critic", ".", "to", "(", "device", ")", "\n", "\n", "agent", "=", "PPO", "(", "\n", "actor_critic", ",", "\n", "args", ".", "clip_param", ",", "\n", "args", ".", "ppo_epoch", ",", "\n", "args", ".", "num_mini_batch", ",", "\n", "args", ".", "value_loss_coef", ",", "\n", "args", ".", "entropy_coef", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "eps", "=", "args", ".", "eps", ",", "\n", "max_grad_norm", "=", "args", ".", "max_grad_norm", ")", "\n", "\n", "# discriminator", "\n", "# discr = Discriminator(envs.observation_space.shape[0] + envs.action_space.shape[0], 100, device)", "\n", "discr", "=", "Discriminator", "(", "envs", ".", "observation_space", ".", "shape", "[", "0", "]", "+", "envs", ".", "action_space", ".", "shape", "[", "0", "]", ",", "100", ",", "device", ",", "args", ".", "reward_type", ",", "args", ".", "update_rms", ")", "\n", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "gail_experts_dir", ",", "\"trajs_{}.pt\"", ".", "format", "(", "\n", "args", ".", "env_name", ".", "split", "(", "'-'", ")", "[", "0", "]", ".", "lower", "(", ")", ")", ")", "\n", "\n", "gail_train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "ExpertDataset", "(", "\n", "file_name", ",", "num_trajectories", "=", "args", ".", "num_trajs", ",", "subsample_frequency", "=", "args", ".", "subsample_frequency", ")", ",", "\n", "batch_size", "=", "args", ".", "gail_batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "drop_last", "=", "True", ")", "\n", "\n", "# The buffer", "\n", "rollouts", "=", "RolloutStorage", "(", "args", ".", "nsteps", ",", "args", ".", "num_processes", ",", "\n", "envs", ".", "observation_space", ".", "shape", ",", "envs", ".", "action_space", ",", "\n", "actor_critic", ".", "recurrent_hidden_state_size", ")", "\n", "\n", "# The buffer for the expert -> refer to dataset/mujoco_dset.py", "\n", "# expert_path = cl_args.expert_path+cl_args.env_id+\".h5\"", "\n", "# expert_buffer = Mujoco_Dset(cl_args.expert_path, traj_limitation=cl_args.num_trajs, subsample_frequency=args.subsample_frequency)", "\n", "\n", "model", "=", "gailLearning_mujoco_origin", "(", "cl_args", "=", "cl_args", ",", "\n", "envs", "=", "envs", ",", "\n", "envs_eval", "=", "envs_eval", ",", "\n", "actor_critic", "=", "actor_critic", ",", "\n", "agent", "=", "agent", ",", "\n", "discriminator", "=", "discr", ",", "\n", "rollouts", "=", "rollouts", ",", "\n", "gail_train_loader", "=", "gail_train_loader", ",", "\n", "device", "=", "device", ",", "\n", "utli", "=", "utli", ")", "\n", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail.wdail_walker.main": [[178, 208], ["wdail_walker.train"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.train"], ["", "def", "main", "(", "args", ")", ":", "\n", "\n", "    ", "model", ",", "env", "=", "train", "(", "args", ")", "\n", "\n", "# if args.play:", "\n", "#", "\n", "#     obs = env.reset()", "\n", "#", "\n", "#     state = model.initial_state if hasattr(model, 'initial_state') else None", "\n", "#     dones = np.zeros((1,))", "\n", "#", "\n", "#     episode_rew = 0", "\n", "#     while True:", "\n", "#         if state is not None:", "\n", "#             actions, _, state, _ = model.step(obs,S=state, M=dones)", "\n", "#         else:", "\n", "#             actions, _, _, _ = model.step(obs)", "\n", "#", "\n", "#         obs, rew, done, _ = env.step(actions)", "\n", "#         episode_rew += rew[0] if isinstance(env, VecEnv) else rew", "\n", "#         env.render()", "\n", "#         done = done.any() if isinstance(done, np.ndarray) else done", "\n", "#         if done:", "\n", "#             print(f'episode_rew={episode_rew}')", "\n", "#             episode_rew = 0", "\n", "#             obs = env.reset()", "\n", "#", "\n", "#     env.close()", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail.wdail_ant.argsparser": [[17, 74], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argsparser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\"WDAIL\"", ")", "\n", "parser", ".", "add_argument", "(", "'--env_name'", ",", "help", "=", "'environment ID'", ",", "default", "=", "'Ant-v1'", ")", "\n", "parser", ".", "add_argument", "(", "'--algo'", ",", "help", "=", "'algorithm ID'", ",", "default", "=", "'WDAIL'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-dir'", ",", "default", "=", "'/tmp/gym/'", ",", "help", "=", "'directory to save agent logs (default: /tmp/gym)'", ")", "\n", "# general", "\n", "parser", ".", "add_argument", "(", "'--total_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--num_env_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluate_every'", ",", "help", "=", "'evaluate every'", ",", "type", "=", "int", ",", "default", "=", "2e10", ")", "\n", "parser", ".", "add_argument", "(", "'--save_condition'", ",", "help", "=", "'save_condition'", ",", "type", "=", "int", ",", "default", "=", "1000", ")", "\n", "parser", ".", "add_argument", "(", "'--num_model'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--use_device'", ",", "help", "=", "'use_device'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "help", "=", "'seed'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--use_linear_lr_decay'", ",", "help", "=", "'use linear lr decay'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--recurrent-policy'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'use a recurrent policy'", ")", "\n", "\n", "#ppo", "\n", "parser", ".", "add_argument", "(", "'--num_processes'", ",", "help", "=", "'num_processes'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--num-steps'", ",", "help", "=", "'num-steps'", ",", "type", "=", "int", ",", "default", "=", "2048", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "help", "=", "'learning rate'", ",", "type", "=", "float", ",", "default", "=", "3e-4", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "help", "=", "'batch size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_epoch'", ",", "help", "=", "'ppo epoch num'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--hidden_size'", ",", "help", "=", "'hidden size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_entcoeff'", ",", "help", "=", "'entropy coefficiency of policy'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ")", "#default=1e-3", "\n", "parser", ".", "add_argument", "(", "'--ppo_obs_norm'", ",", "help", "=", "'ppo_vec_norm'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--num-mini-batch'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "'number of batches for ppo (default: 32)'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-param'", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "help", "=", "'ppo clip parameter (default: 0.2)'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps'", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "help", "=", "'RMSprop optimizer epsilon (default: 1e-5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "'RMSprop optimizer apha (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "'discount factor for rewards (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "'--use-gae'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'use generalized advantage estimation'", ")", "\n", "parser", ".", "add_argument", "(", "'--gae-lambda'", ",", "type", "=", "float", ",", "default", "=", "0.95", ",", "help", "=", "'gae lambda parameter (default: 0.95)'", ")", "\n", "parser", ".", "add_argument", "(", "'--entropy-coef'", ",", "type", "=", "float", ",", "default", "=", "0.00", ",", "help", "=", "'entropy term coefficient (default: 0.01)'", ")", "\n", "parser", ".", "add_argument", "(", "'--value-loss-coef'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'value loss coefficient (default: 0.5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-grad-norm'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'max norm of gradients (default: 0.5)'", ")", "\n", "\n", "# #gail", "\n", "parser", ".", "add_argument", "(", "'--gail'", ",", "help", "=", "'if gail'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "# parser.add_argument('--expert_path', help='trajs path', type=str, default='../data/baseline/deterministic.trpo.HalfCheetah.0.00.npz')", "\n", "parser", ".", "add_argument", "(", "'--expert_path'", ",", "help", "=", "'trajs path'", ",", "type", "=", "str", ",", "default", "=", "'../data/ikostirkov/trajs_ant.h5'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail-experts-dir'", ",", "default", "=", "'./gail_experts'", ",", "help", "=", "'directory that contains expert demonstrations for gail'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_batch_size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'gail batch size (default: 128)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_thre'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_pre_epoch'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "100", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_epoch'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "'--num_trajs'", ",", "help", "=", "'num trajs'", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "'--subsample_frequency'", ",", "help", "=", "'num trajs'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--adversary_entcoeff'", ",", "help", "=", "'entropy coefficiency of discriminator'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ")", "\n", "parser", ".", "add_argument", "(", "'--use-proper-time-limits'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'compute returns taking into account time limits'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-interval'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'log interval, one log per n updates (default: 10)'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--reward_type'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'0,1,2,3,4'", ")", "\n", "parser", ".", "add_argument", "(", "'--update_rms'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "'False or True'", ")", "\n", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail.wdail_ant.train": [[75, 177], ["np.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "os.path.expanduser", "utils.cleanup_log_dir", "utils.cleanup_log_dir", "torch.set_num_threads", "torch.device", "make_vec_envs", "Policy", "Policy.to", "PPO", "Discriminator", "os.path.join", "torch.utils.data.DataLoader", "RolloutStorage", "gailLearning_mujoco_origin", "ExpertDataset", "torch.cuda.is_available", "[].lower", "str", "args.env_name.split"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.make_vec_envs", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.gailLearning_mujoco_origin"], ["", "def", "train", "(", "args", ")", ":", "\n", "\n", "# from ppo_gail_iko.algo.ppo4multienvs import PPO, ReplayBuffer", "\n", "    ", "from", "ppo_wdail", ".", "algo", ".", "ppo", "import", "PPO", "\n", "from", "ppo_wdail", ".", "tools", ".", "storage", "import", "RolloutStorage", "\n", "from", "ppo_wdail", ".", "tools", ".", "model", "import", "Policy", "\n", "\n", "from", "ppo_wdail", ".", "algo", ".", "wdgail", "import", "Discriminator", ",", "ExpertDataset", "\n", "from", "ppo_wdail", ".", "algo", ".", "mujoco_dset_zm_iko", "import", "Mujoco_Dset", "\n", "\n", "from", "ppo_wdail", ".", "tools", ".", "learn", "import", "gailLearning_mujoco_origin", "\n", "from", "ppo_wdail", ".", "tools", ".", "envs", "import", "make_vec_envs", "\n", "\n", "from", "ppo_wdail", ".", "tools", "import", "utli", "\n", "from", "ppo_wdail", ".", "tools", "import", "utils", "\n", "\n", "from", "collections", "import", "deque", "\n", "import", "time", "\n", "import", "numpy", "as", "np", "\n", "\n", "\n", "# from nets.network import ActorCritic_mujoco as ActorCritic", "\n", "cl_args", "=", "args", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "# if args.cuda and torch.cuda.is_available() and args.cuda_deterministic:", "\n", "#     torch.backends.cudnn.benchmark = False", "\n", "#     torch.backends.cudnn.deterministic = True", "\n", "\n", "log_dir", "=", "os", ".", "path", ".", "expanduser", "(", "args", ".", "log_dir", ")", "\n", "eval_log_dir", "=", "log_dir", "+", "\"_eval\"", "\n", "utils", ".", "cleanup_log_dir", "(", "log_dir", ")", "\n", "utils", ".", "cleanup_log_dir", "(", "eval_log_dir", ")", "\n", "\n", "torch", ".", "set_num_threads", "(", "1", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "'cuda:'", "+", "str", "(", "cl_args", ".", "cuda", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "# device = torch.device('cpu')", "\n", "\n", "envs", "=", "make_vec_envs", "(", "args", ".", "env_name", ",", "args", ".", "seed", ",", "args", ".", "num_processes", ",", "\n", "args", ".", "gamma", ",", "args", ".", "log_dir", ",", "device", ",", "False", ")", "\n", "\n", "# envs_eval = make_vec_envs(args.env_name, args.seed, args.num_processes,", "\n", "#                      args.gamma, args.log_dir, device, False)", "\n", "envs_eval", "=", "[", "]", "\n", "\n", "# network", "\n", "actor_critic", "=", "Policy", "(", "\n", "envs", ".", "observation_space", ".", "shape", ",", "\n", "envs", ".", "action_space", ",", "\n", "base_kwargs", "=", "{", "'recurrent'", ":", "args", ".", "recurrent_policy", "}", ")", "\n", "actor_critic", ".", "to", "(", "device", ")", "\n", "\n", "agent", "=", "PPO", "(", "\n", "actor_critic", ",", "\n", "args", ".", "clip_param", ",", "\n", "args", ".", "ppo_epoch", ",", "\n", "args", ".", "num_mini_batch", ",", "\n", "args", ".", "value_loss_coef", ",", "\n", "args", ".", "entropy_coef", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "eps", "=", "args", ".", "eps", ",", "\n", "max_grad_norm", "=", "args", ".", "max_grad_norm", ")", "\n", "\n", "# discriminator", "\n", "discr", "=", "Discriminator", "(", "envs", ".", "observation_space", ".", "shape", "[", "0", "]", "+", "envs", ".", "action_space", ".", "shape", "[", "0", "]", ",", "100", ",", "device", ",", "args", ".", "reward_type", ",", "args", ".", "update_rms", ")", "\n", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "gail_experts_dir", ",", "\"trajs_{}.pt\"", ".", "format", "(", "\n", "args", ".", "env_name", ".", "split", "(", "'-'", ")", "[", "0", "]", ".", "lower", "(", ")", ")", ")", "\n", "\n", "gail_train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "ExpertDataset", "(", "\n", "file_name", ",", "num_trajectories", "=", "args", ".", "num_trajs", ",", "subsample_frequency", "=", "args", ".", "subsample_frequency", ")", ",", "\n", "batch_size", "=", "args", ".", "gail_batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "drop_last", "=", "True", ")", "\n", "\n", "# The buffer", "\n", "rollouts", "=", "RolloutStorage", "(", "args", ".", "num_steps", ",", "args", ".", "num_processes", ",", "\n", "envs", ".", "observation_space", ".", "shape", ",", "envs", ".", "action_space", ",", "\n", "actor_critic", ".", "recurrent_hidden_state_size", ")", "\n", "\n", "# The buffer for the expert -> refer to dataset/mujoco_dset.py", "\n", "# expert_path = cl_args.expert_path+cl_args.env_id+\".h5\"", "\n", "# expert_buffer = Mujoco_Dset(cl_args.expert_path, traj_limitation=cl_args.num_trajs, subsample_frequency=20)", "\n", "\n", "\n", "model", "=", "gailLearning_mujoco_origin", "(", "cl_args", "=", "cl_args", ",", "\n", "envs", "=", "envs", ",", "\n", "envs_eval", "=", "envs_eval", ",", "\n", "actor_critic", "=", "actor_critic", ",", "\n", "agent", "=", "agent", ",", "\n", "discriminator", "=", "discr", ",", "\n", "rollouts", "=", "rollouts", ",", "\n", "gail_train_loader", "=", "gail_train_loader", ",", "\n", "device", "=", "device", ",", "\n", "utli", "=", "utli", ")", "\n", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail.wdail_ant.main": [[179, 209], ["wdail_ant.train"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.train"], ["", "def", "main", "(", "args", ")", ":", "\n", "\n", "    ", "model", ",", "env", "=", "train", "(", "args", ")", "\n", "\n", "# if args.play:", "\n", "#", "\n", "#     obs = env.reset()", "\n", "#", "\n", "#     state = model.initial_state if hasattr(model, 'initial_state') else None", "\n", "#     dones = np.zeros((1,))", "\n", "#", "\n", "#     episode_rew = 0", "\n", "#     while True:", "\n", "#         if state is not None:", "\n", "#             actions, _, state, _ = model.step(obs,S=state, M=dones)", "\n", "#         else:", "\n", "#             actions, _, _, _ = model.step(obs)", "\n", "#", "\n", "#         obs, rew, done, _ = env.step(actions)", "\n", "#         episode_rew += rew[0] if isinstance(env, VecEnv) else rew", "\n", "#         env.render()", "\n", "#         done = done.any() if isinstance(done, np.ndarray) else done", "\n", "#         if done:", "\n", "#             print(f'episode_rew={episode_rew}')", "\n", "#             episode_rew = 0", "\n", "#             obs = env.reset()", "\n", "#", "\n", "#     env.close()", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.safemean": [[11, 13], ["numpy.mean", "len"], "function", ["None"], ["def", "safemean", "(", "xs", ")", ":", "\n", "    ", "return", "np", ".", "nan", "if", "len", "(", "xs", ")", "==", "0", "else", "np", ".", "mean", "(", "xs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.gailLearning_mujoco": [[14, 198], ["tools.utli.Log_save_name4gail", "os.path.join", "os.path.exists", "tensorboardX.SummaryWriter", "collections.deque", "time.time", "numpy.floor", "nbatch.astype.astype", "numpy.floor", "nupdates.astype.astype", "collections.deque", "collections.deque", "envs.reset", "rollouts.obs[].copy_", "rollouts.to", "time.time", "time.time", "tools.utli.store_results", "shutil.rmtree", "range", "rollouts.compute_returns", "agent.update", "tools.utli.recordLossResults", "rollouts.after_update", "collections.deque.extend", "learn.safemean", "learn.safemean", "tools.utli.recordTrainResults", "print", "tools.utli.update_linear_schedule", "envs.step", "torch.FloatTensor", "torch.FloatTensor", "rollouts.insert", "torch.no_grad", "actor_critic.get_value().detach", "range", "tools.utli.recordDisLossResults", "range", "len", "time.time", "print", "torch.no_grad", "actor_critic.act", "info.get", "envs.venv.eval", "discriminator.update", "dis_losses.append", "dis_gps.append", "dis_entropys.append", "dis_total_losses.append", "discriminator.predict_reward", "len", "epinfos.append", "info.keys", "collections.deque.append", "actor_critic.get_value", "int", "len", "numpy.mean", "numpy.median", "numpy.min", "numpy.max", "tools.utils.get_vec_normalize", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "info.keys", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Log_save_name4gail", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.store_results", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.compute_returns", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordLossResults", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.after_update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.safemean", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.safemean", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordTrainResults", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.update_linear_schedule", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.step", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.insert", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordDisLossResults", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Policy.act", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecNormalize.eval", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.predict_reward", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Policy.get_value", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.get_vec_normalize"], ["", "def", "gailLearning_mujoco", "(", "cl_args", ",", "envs", ",", "envs_eval", ",", "actor_critic", ",", "agent", ",", "discriminator", ",", "rollouts", ",", "gail_train_loader", ",", "device", ",", "utli", ")", ":", "\n", "\n", "    ", "log_save_name", "=", "utli", ".", "Log_save_name4gail", "(", "cl_args", ")", "\n", "log_save_path", "=", "os", ".", "path", ".", "join", "(", "\"./runs\"", ",", "log_save_name", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "log_save_path", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "log_save_path", ")", "\n", "", "utli", ".", "writer", "=", "SummaryWriter", "(", "log_save_path", ")", "\n", "\n", "# model_dir = utli.Save_model_dir(cl_args.algo_id, cl_args.env_name)", "\n", "\n", "\n", "\n", "# Evaluate the initial network", "\n", "evaluations", "=", "[", "]", "\n", "# begin optimize", "\n", "\n", "reward_window4Evaluate", "=", "deque", "(", "maxlen", "=", "10", ")", "\n", "time_step", "=", "0", "\n", "episode_t", "=", "0", "\n", "episode_timesteps", "=", "0", "\n", "count", "=", "0", "\n", "\n", "# begin optimize", "\n", "\n", "nsteps", "=", "cl_args", ".", "nsteps", "\n", "S_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "nenv", "=", "1", "\n", "\n", "nbatch", "=", "np", ".", "floor", "(", "nsteps", "/", "nenv", ")", "\n", "nbatch", "=", "nbatch", ".", "astype", "(", "np", ".", "int16", ")", "\n", "nupdates", "=", "np", ".", "floor", "(", "cl_args", ".", "total_steps", "/", "nsteps", ")", "\n", "nupdates", "=", "nupdates", ".", "astype", "(", "np", ".", "int16", ")", "\n", "\n", "epinfobuf", "=", "deque", "(", "maxlen", "=", "10", ")", "\n", "\n", "# epgailbuf = deque(maxlen=10)", "\n", "\n", "episode_rewards", "=", "deque", "(", "maxlen", "=", "10", ")", "\n", "\n", "cum_gailrewards", "=", ".0", "\n", "\n", "i_update", "=", "0", "\n", "\n", "obs", "=", "envs", ".", "reset", "(", ")", "\n", "rollouts", ".", "obs", "[", "0", "]", ".", "copy_", "(", "obs", ")", "\n", "rollouts", ".", "to", "(", "device", ")", "\n", "\n", "# write_result = utli.Write_Result(cl_args=cl_args)", "\n", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "# num_updates = int(", "\n", "#     args.num_env_steps) // args.num_steps // args.num_processes", "\n", "\n", "while", "i_update", "<", "nupdates", ":", "\n", "\n", "        ", "episode_t", "+=", "1", "\n", "i_update", "+=", "1", "\n", "epinfos", "=", "[", "]", "\n", "\n", "if", "cl_args", ".", "use_linear_lr_decay", ":", "\n", "# decrease learning rate linearly", "\n", "            ", "utli", ".", "update_linear_schedule", "(", "\n", "agent", ".", "optimizer", ",", "i_update", ",", "nupdates", ",", "\n", "cl_args", ".", "lr", ")", "\n", "\n", "\n", "", "for", "step", "in", "range", "(", "nbatch", ")", ":", "\n", "            ", "time_step", "+=", "1", "\n", "\n", "# Sample actions", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "value", ",", "action", ",", "action_log_prob", ",", "recurrent_hidden_states", "=", "actor_critic", ".", "act", "(", "\n", "rollouts", ".", "obs", "[", "step", "]", ",", "rollouts", ".", "recurrent_hidden_states", "[", "step", "]", ",", "\n", "rollouts", ".", "masks", "[", "step", "]", ")", "\n", "\n", "# action, log_prob, value = model_step(cl_args=cl_args, model=model, state=cur_state, device=device)", "\n", "# time.sleep(.002)", "\n", "# next_state, reward, done, infos = envs.step(action)   # error 01", "\n", "", "obs", ",", "reward", ",", "done", ",", "infos", "=", "envs", ".", "step", "(", "action", ")", "\n", "\n", "for", "info", "in", "infos", ":", "\n", "                ", "maybeepinfo", "=", "info", ".", "get", "(", "'episode'", ")", "\n", "if", "maybeepinfo", ":", "\n", "                    ", "epinfos", ".", "append", "(", "maybeepinfo", ")", "\n", "", "", "for", "info", "in", "infos", ":", "\n", "                ", "if", "'episode'", "in", "info", ".", "keys", "(", ")", ":", "\n", "                    ", "episode_rewards", ".", "append", "(", "info", "[", "'episode'", "]", "[", "'r'", "]", ")", "\n", "\n", "# If done then clean the history of observations.", "\n", "", "", "masks", "=", "torch", ".", "FloatTensor", "(", "\n", "[", "[", "0.0", "]", "if", "done_", "else", "[", "1.0", "]", "for", "done_", "in", "done", "]", ")", "\n", "bad_masks", "=", "torch", ".", "FloatTensor", "(", "\n", "[", "[", "0.0", "]", "if", "'bad_transition'", "in", "info", ".", "keys", "(", ")", "else", "[", "1.0", "]", "\n", "for", "info", "in", "infos", "]", ")", "\n", "rollouts", ".", "insert", "(", "obs", ",", "recurrent_hidden_states", ",", "action", ",", "\n", "action_log_prob", ",", "value", ",", "reward", ",", "masks", ",", "bad_masks", ")", "\n", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "next_value", "=", "actor_critic", ".", "get_value", "(", "\n", "rollouts", ".", "obs", "[", "-", "1", "]", ",", "rollouts", ".", "recurrent_hidden_states", "[", "-", "1", "]", ",", "\n", "rollouts", ".", "masks", "[", "-", "1", "]", ")", ".", "detach", "(", ")", "\n", "\n", "# gail", "\n", "", "if", "cl_args", ".", "gail", ":", "\n", "            ", "if", "i_update", ">=", "10", ":", "\n", "                ", "envs", ".", "venv", ".", "eval", "(", ")", "\n", "# gail_epoch = args.gail_epoch", "\n", "", "gail_epoch", "=", "cl_args", ".", "gail_epoch", "\n", "if", "i_update", "<", "10", ":", "\n", "                ", "gail_epoch", "=", "100", "# Warm up", "\n", "\n", "", "dis_losses", ",", "dis_gps", ",", "dis_entropys", ",", "dis_total_losses", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "_", "in", "range", "(", "gail_epoch", ")", ":", "\n", "# dis_loss, dis_gp, dis_entropy, dis_total_loss = \\", "\n", "#     discriminator.update(replay_buf=rollouts, expert_buf=expert_buffer,", "\n", "#                          obsfilt=utils.get_vec_normalize(envs)._obfilt, batch_size=batch_size)", "\n", "                ", "dis_loss", ",", "dis_gp", ",", "dis_entropy", ",", "dis_total_loss", "=", "discriminator", ".", "update", "(", "gail_train_loader", ",", "rollouts", ",", "utils", ".", "get_vec_normalize", "(", "envs", ")", ".", "_obfilt", ")", "\n", "dis_losses", ".", "append", "(", "dis_loss", ")", "\n", "dis_gps", ".", "append", "(", "dis_gp", ")", "\n", "dis_entropys", ".", "append", "(", "dis_entropy", ")", "\n", "dis_total_losses", ".", "append", "(", "dis_total_loss", ")", "\n", "\n", "", "utli", ".", "recordDisLossResults", "(", "results", "=", "(", "np", ".", "mean", "(", "np", ".", "array", "(", "dis_losses", ")", ")", ",", "\n", "np", ".", "mean", "(", "np", ".", "array", "(", "dis_gps", ")", ")", ",", "\n", "np", ".", "mean", "(", "np", ".", "array", "(", "dis_entropys", ")", ")", ",", "\n", "np", ".", "mean", "(", "np", ".", "array", "(", "dis_total_losses", ")", ")", ")", ",", "\n", "time_step", "=", "time_step", ")", "\n", "# write_result.step_train(time_step)", "\n", "\n", "for", "step", "in", "range", "(", "nbatch", ")", ":", "\n", "                ", "rollouts", ".", "rewards", "[", "step", "]", "=", "discriminator", ".", "predict_reward", "(", "\n", "rollouts", ".", "obs", "[", "step", "]", ",", "rollouts", ".", "actions", "[", "step", "]", ",", "cl_args", ".", "gamma", ",", "\n", "rollouts", ".", "masks", "[", "step", "]", ")", "\n", "\n", "# compute returns", "\n", "", "", "rollouts", ".", "compute_returns", "(", "next_value", ",", "cl_args", ".", "use_gae", ",", "cl_args", ".", "gamma", ",", "\n", "cl_args", ".", "gae_lambda", ",", "cl_args", ".", "use_proper_time_limits", ")", "\n", "\n", "# training PPO policy", "\n", "\n", "value_loss", ",", "action_loss", ",", "dist_entropy", ",", "total_loss", "=", "agent", ".", "update", "(", "rollouts", ")", "\n", "\n", "utli", ".", "recordLossResults", "(", "results", "=", "(", "value_loss", ",", "\n", "action_loss", ",", "\n", "dist_entropy", ",", "\n", "total_loss", ")", ",", "\n", "time_step", "=", "time_step", ")", "\n", "rollouts", ".", "after_update", "(", ")", "\n", "\n", "\n", "epinfobuf", ".", "extend", "(", "epinfos", ")", "\n", "if", "not", "len", "(", "epinfobuf", ")", ":", "\n", "            ", "continue", "\n", "", "eprewmean", "=", "safemean", "(", "[", "epinfo", "[", "'r'", "]", "for", "epinfo", "in", "epinfobuf", "]", ")", "\n", "eplenmean", "=", "safemean", "(", "[", "epinfo", "[", "'l'", "]", "for", "epinfo", "in", "epinfobuf", "]", ")", "\n", "\n", "utli", ".", "recordTrainResults", "(", "results", "=", "(", "eprewmean", ",", "\n", "eplenmean", ")", ",", "\n", "time_step", "=", "time_step", ")", "\n", "\n", "# write_result.step_train(time_step)", "\n", "\n", "print", "(", "\"Episode: %d,   Time steps: %d,   Mean length: %d    Mean Reward: %f  \"", "\n", "%", "(", "episode_t", ",", "time_step", ",", "eplenmean", ",", "eprewmean", ")", ")", "\n", "\n", "if", "i_update", "%", "cl_args", ".", "log_interval", "==", "0", "and", "len", "(", "episode_rewards", ")", ">", "1", ":", "\n", "            ", "total_num_steps", "=", "(", "i_update", "+", "1", ")", "*", "cl_args", ".", "num_processes", "*", "cl_args", ".", "nsteps", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "\n", "\"Updates {}, num timesteps {}, FPS {} \\n Last {} training episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\\n\"", "\n", ".", "format", "(", "i_update", ",", "total_num_steps", ",", "\n", "int", "(", "total_num_steps", "/", "(", "end", "-", "start", ")", ")", ",", "\n", "len", "(", "episode_rewards", ")", ",", "np", ".", "mean", "(", "episode_rewards", ")", ",", "\n", "np", ".", "median", "(", "episode_rewards", ")", ",", "np", ".", "min", "(", "episode_rewards", ")", ",", "\n", "np", ".", "max", "(", "episode_rewards", ")", ",", "dist_entropy", ",", "value_loss", ",", "\n", "action_loss", ")", ")", "\n", "\n", "", "", "E_time", "=", "time", ".", "time", "(", ")", "\n", "# store results", "\n", "utli", ".", "store_results", "(", "evaluations", ",", "time_step", ",", "cl_args", ",", "S_time", "=", "S_time", ",", "E_time", "=", "E_time", ")", "\n", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.gailLearning_mujoco_test": [[199, 404], ["tools.utli.Log_save_name4gail", "os.path.join", "os.path.exists", "tensorboardX.SummaryWriter", "collections.deque", "time.time", "numpy.floor", "nbatch.astype.astype", "numpy.floor", "nupdates.astype.astype", "collections.deque", "collections.deque", "collections.deque", "envs.reset", "rollouts.obs[].copy_", "rollouts.to", "time.time", "time.time", "tools.utli.store_results", "shutil.rmtree", "range", "rollouts.compute_returns", "agent.update", "tools.utli.recordLossResults", "rollouts.after_update", "collections.deque.extend", "learn.safemean", "learn.safemean", "tools.utli.recordTrainResults_gail", "print", "tools.utli.update_linear_schedule", "envs.step", "torch.FloatTensor", "torch.FloatTensor", "rollouts.insert", "torch.no_grad", "actor_critic.get_value().detach", "range", "tools.utli.recordDisLossResults", "range", "len", "time.time", "print", "torch.no_grad", "actor_critic.act", "info.get", "envs.venv.eval", "discriminator.update_zm", "dis_losses.append", "dis_gps.append", "dis_entropys.append", "dis_total_losses.append", "tools.utli.recordDisLossResults", "discriminator.predict_reward", "len", "epinfos.append", "info.keys", "collections.deque.append", "actor_critic.get_value", "rollouts.rewards[].item", "collections.deque.append", "numpy.mean", "numpy.mean", "int", "len", "numpy.mean", "numpy.median", "numpy.min", "numpy.max", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.array", "numpy.array", "info.keys", "tools.utils.get_vec_normalize", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Log_save_name4gail", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.store_results", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.compute_returns", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordLossResults", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.after_update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.safemean", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.safemean", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordTrainResults_gail", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.update_linear_schedule", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.step", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.insert", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordDisLossResults", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Policy.act", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecNormalize.eval", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.update_zm", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordDisLossResults", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.predict_reward", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Policy.get_value", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.get_vec_normalize"], ["", "def", "gailLearning_mujoco_test", "(", "cl_args", ",", "envs", ",", "envs_eval", ",", "actor_critic", ",", "agent", ",", "discriminator", ",", "rollouts", ",", "expert_buffer", ",", "device", ",", "utli", ")", ":", "\n", "\n", "    ", "log_save_name", "=", "utli", ".", "Log_save_name4gail", "(", "cl_args", ")", "\n", "log_save_path", "=", "os", ".", "path", ".", "join", "(", "\"./runs\"", ",", "log_save_name", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "log_save_path", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "log_save_path", ")", "\n", "", "utli", ".", "writer", "=", "SummaryWriter", "(", "log_save_path", ")", "\n", "\n", "# model_dir = utli.Save_model_dir(cl_args.algo_id, cl_args.env_name)", "\n", "\n", "\n", "\n", "# Evaluate the initial network", "\n", "evaluations", "=", "[", "]", "\n", "# begin optimize", "\n", "\n", "reward_window4Evaluate", "=", "deque", "(", "maxlen", "=", "10", ")", "\n", "time_step", "=", "0", "\n", "episode_t", "=", "0", "\n", "episode_timesteps", "=", "0", "\n", "count", "=", "0", "\n", "\n", "# begin optimize", "\n", "\n", "nsteps", "=", "cl_args", ".", "nsteps", "\n", "S_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "nenv", "=", "1", "\n", "\n", "nbatch", "=", "np", ".", "floor", "(", "nsteps", "/", "nenv", ")", "\n", "nbatch", "=", "nbatch", ".", "astype", "(", "np", ".", "int16", ")", "\n", "nupdates", "=", "np", ".", "floor", "(", "cl_args", ".", "total_steps", "/", "nsteps", ")", "\n", "nupdates", "=", "nupdates", ".", "astype", "(", "np", ".", "int16", ")", "\n", "\n", "epinfobuf", "=", "deque", "(", "maxlen", "=", "10", ")", "\n", "\n", "epgailbuf", "=", "deque", "(", "maxlen", "=", "10", ")", "\n", "\n", "episode_rewards", "=", "deque", "(", "maxlen", "=", "10", ")", "\n", "\n", "cum_gailrewards", "=", ".0", "\n", "\n", "i_update", "=", "0", "\n", "dis_init", "=", "True", "\n", "\n", "obs", "=", "envs", ".", "reset", "(", ")", "\n", "rollouts", ".", "obs", "[", "0", "]", ".", "copy_", "(", "obs", ")", "\n", "rollouts", ".", "to", "(", "device", ")", "\n", "\n", "# write_result = utli.Write_Result(cl_args=cl_args)", "\n", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "\n", "while", "i_update", "<", "nupdates", ":", "\n", "\n", "        ", "episode_t", "+=", "1", "\n", "i_update", "+=", "1", "\n", "epinfos", "=", "[", "]", "\n", "\n", "if", "cl_args", ".", "use_linear_lr_decay", ":", "\n", "# decrease learning rate linearly", "\n", "            ", "utli", ".", "update_linear_schedule", "(", "\n", "agent", ".", "optimizer", ",", "i_update", ",", "nupdates", ",", "\n", "cl_args", ".", "lr", ")", "\n", "\n", "\n", "", "for", "step", "in", "range", "(", "nbatch", ")", ":", "\n", "            ", "time_step", "+=", "1", "\n", "\n", "# Sample actions", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "value", ",", "action", ",", "action_log_prob", ",", "recurrent_hidden_states", "=", "actor_critic", ".", "act", "(", "\n", "rollouts", ".", "obs", "[", "step", "]", ",", "rollouts", ".", "recurrent_hidden_states", "[", "step", "]", ",", "\n", "rollouts", ".", "masks", "[", "step", "]", ")", "\n", "\n", "# action, log_prob, value = model_step(cl_args=cl_args, model=model, state=cur_state, device=device)", "\n", "# time.sleep(.002)", "\n", "# next_state, reward, done, infos = envs.step(action)   # error 01", "\n", "", "obs", ",", "reward", ",", "done", ",", "infos", "=", "envs", ".", "step", "(", "action", ")", "\n", "\n", "for", "info", "in", "infos", ":", "\n", "                ", "maybeepinfo", "=", "info", ".", "get", "(", "'episode'", ")", "\n", "if", "maybeepinfo", ":", "\n", "                    ", "epinfos", ".", "append", "(", "maybeepinfo", ")", "\n", "", "", "for", "info", "in", "infos", ":", "\n", "                ", "if", "'episode'", "in", "info", ".", "keys", "(", ")", ":", "\n", "                    ", "episode_rewards", ".", "append", "(", "info", "[", "'episode'", "]", "[", "'r'", "]", ")", "\n", "\n", "# If done then clean the history of observations.", "\n", "", "", "masks", "=", "torch", ".", "FloatTensor", "(", "\n", "[", "[", "0.0", "]", "if", "done_", "else", "[", "1.0", "]", "for", "done_", "in", "done", "]", ")", "\n", "bad_masks", "=", "torch", ".", "FloatTensor", "(", "\n", "[", "[", "0.0", "]", "if", "'bad_transition'", "in", "info", ".", "keys", "(", ")", "else", "[", "1.0", "]", "\n", "for", "info", "in", "infos", "]", ")", "\n", "rollouts", ".", "insert", "(", "obs", ",", "recurrent_hidden_states", ",", "action", ",", "\n", "action_log_prob", ",", "value", ",", "reward", ",", "masks", ",", "bad_masks", ")", "\n", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "next_value", "=", "actor_critic", ".", "get_value", "(", "\n", "rollouts", ".", "obs", "[", "-", "1", "]", ",", "rollouts", ".", "recurrent_hidden_states", "[", "-", "1", "]", ",", "\n", "rollouts", ".", "masks", "[", "-", "1", "]", ")", ".", "detach", "(", ")", "\n", "\n", "# gail", "\n", "", "if", "cl_args", ".", "gail", ":", "\n", "            ", "if", "i_update", ">=", "cl_args", ".", "gail_thre", ":", "\n", "                ", "envs", ".", "venv", ".", "eval", "(", ")", "\n", "# gail_epoch = args.gail_epoch", "\n", "", "gail_epoch", "=", "cl_args", ".", "gail_epoch", "\n", "if", "i_update", "<", "cl_args", ".", "gail_thre", ":", "\n", "                ", "gail_epoch", "=", "cl_args", ".", "gail_pre_epoch", "# Warm up", "\n", "\n", "", "dis_losses", ",", "dis_gps", ",", "dis_entropys", ",", "dis_total_losses", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "_", "in", "range", "(", "gail_epoch", ")", ":", "\n", "                ", "dis_loss", ",", "dis_gp", ",", "dis_entropy", ",", "dis_total_loss", "=", "discriminator", ".", "update_zm", "(", "replay_buf", "=", "rollouts", ",", "expert_buf", "=", "expert_buffer", ",", "\n", "obsfilt", "=", "utils", ".", "get_vec_normalize", "(", "envs", ")", ".", "_obfilt", ",", "batch_size", "=", "cl_args", ".", "gail_batch_size", ")", "\n", "# dis_loss, dis_gp, dis_entropy, dis_total_loss = discriminator.update(gail_train_loader, rollouts, utils.get_vec_normalize(envs)._obfilt)", "\n", "dis_losses", ".", "append", "(", "dis_loss", ")", "\n", "dis_gps", ".", "append", "(", "dis_gp", ")", "\n", "dis_entropys", ".", "append", "(", "dis_entropy", ")", "\n", "dis_total_losses", ".", "append", "(", "dis_total_loss", ")", "\n", "\n", "", "if", "dis_init", ":", "\n", "                ", "utli", ".", "recordDisLossResults", "(", "results", "=", "(", "np", ".", "array", "(", "dis_losses", ")", "[", "0", "]", ",", "\n", "np", ".", "array", "(", "dis_gps", ")", "[", "0", "]", ",", "\n", "np", ".", "array", "(", "dis_entropys", ")", "[", "0", "]", ",", "\n", "np", ".", "array", "(", "dis_total_losses", ")", "[", "0", "]", ")", ",", "\n", "time_step", "=", "0", ")", "\n", "dis_init", "=", "False", "\n", "\n", "\n", "", "utli", ".", "recordDisLossResults", "(", "results", "=", "(", "np", ".", "mean", "(", "np", ".", "array", "(", "dis_losses", ")", ")", ",", "\n", "np", ".", "mean", "(", "np", ".", "array", "(", "dis_gps", ")", ")", ",", "\n", "np", ".", "mean", "(", "np", ".", "array", "(", "dis_entropys", ")", ")", ",", "\n", "np", ".", "mean", "(", "np", ".", "array", "(", "dis_total_losses", ")", ")", ")", ",", "\n", "time_step", "=", "time_step", ")", "\n", "# write_result.step_train(time_step)", "\n", "\n", "for", "step", "in", "range", "(", "nbatch", ")", ":", "\n", "                ", "rollouts", ".", "rewards", "[", "step", "]", "=", "discriminator", ".", "predict_reward", "(", "\n", "rollouts", ".", "obs", "[", "step", "]", ",", "rollouts", ".", "actions", "[", "step", "]", ",", "cl_args", ".", "gamma", ",", "\n", "rollouts", ".", "masks", "[", "step", "]", ")", "\n", "if", "rollouts", ".", "masks", "[", "step", "]", "==", "1", ":", "\n", "                    ", "cum_gailrewards", "+=", "rollouts", ".", "rewards", "[", "step", "]", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "                    ", "epgailbuf", ".", "append", "(", "cum_gailrewards", ")", "\n", "cum_gailrewards", "=", ".0", "\n", "\n", "# compute returns", "\n", "", "", "", "rollouts", ".", "compute_returns", "(", "next_value", ",", "cl_args", ".", "use_gae", ",", "cl_args", ".", "gamma", ",", "\n", "cl_args", ".", "gae_lambda", ",", "cl_args", ".", "use_proper_time_limits", ")", "\n", "\n", "# training PPO policy", "\n", "\n", "value_loss", ",", "action_loss", ",", "dist_entropy", ",", "total_loss", "=", "agent", ".", "update", "(", "rollouts", ")", "\n", "\n", "utli", ".", "recordLossResults", "(", "results", "=", "(", "value_loss", ",", "\n", "action_loss", ",", "\n", "dist_entropy", ",", "\n", "total_loss", ")", ",", "\n", "time_step", "=", "time_step", ")", "\n", "rollouts", ".", "after_update", "(", ")", "\n", "\n", "\n", "epinfobuf", ".", "extend", "(", "epinfos", ")", "\n", "if", "not", "len", "(", "epinfobuf", ")", ":", "\n", "            ", "continue", "\n", "", "eprewmean", "=", "safemean", "(", "[", "epinfo", "[", "'r'", "]", "for", "epinfo", "in", "epinfobuf", "]", ")", "\n", "eplenmean", "=", "safemean", "(", "[", "epinfo", "[", "'l'", "]", "for", "epinfo", "in", "epinfobuf", "]", ")", "\n", "\n", "# utli.recordTrainResults(results=(eprewmean,", "\n", "#                                  eplenmean),", "\n", "#                         time_step=time_step)", "\n", "\n", "utli", ".", "recordTrainResults_gail", "(", "results", "=", "(", "eprewmean", ",", "\n", "eplenmean", ",", "\n", "np", ".", "mean", "(", "np", ".", "array", "(", "epgailbuf", ")", ")", ")", ",", "\n", "time_step", "=", "time_step", ")", "\n", "\n", "# write_result.step_train(time_step)", "\n", "\n", "# print(\"Episode: %d,   Time steps: %d,   Mean length: %d    Mean Reward: %f  \"", "\n", "#     % (episode_t, time_step, eplenmean, eprewmean))", "\n", "\n", "print", "(", "\"Episode: %d,   Time steps: %d,   Mean length: %d    Mean Reward: %f    Mean Gail Reward:%f\"", "\n", "%", "(", "episode_t", ",", "time_step", ",", "eplenmean", ",", "eprewmean", ",", "np", ".", "mean", "(", "np", ".", "array", "(", "epgailbuf", ")", ")", ")", ")", "\n", "\n", "if", "i_update", "%", "cl_args", ".", "log_interval", "==", "0", "and", "len", "(", "episode_rewards", ")", ">", "1", ":", "\n", "            ", "total_num_steps", "=", "(", "i_update", "+", "1", ")", "*", "cl_args", ".", "num_processes", "*", "cl_args", ".", "nsteps", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "\n", "\"Updates {}, num timesteps {}, FPS {} \\n Last {} training episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\\n\"", "\n", ".", "format", "(", "i_update", ",", "total_num_steps", ",", "\n", "int", "(", "total_num_steps", "/", "(", "end", "-", "start", ")", ")", ",", "\n", "len", "(", "episode_rewards", ")", ",", "np", ".", "mean", "(", "episode_rewards", ")", ",", "\n", "np", ".", "median", "(", "episode_rewards", ")", ",", "np", ".", "min", "(", "episode_rewards", ")", ",", "\n", "np", ".", "max", "(", "episode_rewards", ")", ",", "dist_entropy", ",", "value_loss", ",", "\n", "action_loss", ")", ")", "\n", "\n", "", "", "E_time", "=", "time", ".", "time", "(", ")", "\n", "# store results", "\n", "utli", ".", "store_results", "(", "evaluations", ",", "time_step", ",", "cl_args", ",", "S_time", "=", "S_time", ",", "E_time", "=", "E_time", ")", "\n", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.gailLearning_mujoco_origin": [[405, 606], ["tools.utli.Log_save_name4gail", "os.path.join", "os.path.exists", "tensorboardX.SummaryWriter", "collections.deque", "time.time", "numpy.floor", "nbatch.astype.astype", "numpy.floor", "nupdates.astype.astype", "collections.deque", "collections.deque", "collections.deque", "envs.reset", "rollouts.obs[].copy_", "rollouts.to", "tools.utli.Write_Result", "time.time", "time.time", "tools.utli.store_results", "shutil.rmtree", "range", "rollouts.compute_returns", "agent.update", "tools.utli.recordLossResults", "rollouts.after_update", "collections.deque.extend", "learn.safemean", "learn.safemean", "tools.utli.recordTrainResults_gail", "utli.Write_Result.step_train", "print", "tools.utli.update_linear_schedule", "envs.step", "torch.FloatTensor", "torch.FloatTensor", "rollouts.insert", "torch.no_grad", "actor_critic.get_value().detach", "range", "tools.utli.recordDisLossResults", "range", "len", "time.time", "print", "torch.no_grad", "actor_critic.act", "info.get", "envs.venv.eval", "discriminator.update", "dis_losses.append", "dis_gps.append", "dis_entropys.append", "dis_total_losses.append", "tools.utli.recordDisLossResults", "discriminator.predict_reward", "len", "epinfos.append", "info.keys", "collections.deque.append", "actor_critic.get_value", "rollouts.rewards[].item", "collections.deque.append", "numpy.mean", "numpy.mean", "int", "len", "numpy.mean", "numpy.median", "numpy.min", "numpy.max", "tools.utils.get_vec_normalize", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.array", "numpy.array", "info.keys", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Log_save_name4gail", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.store_results", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.compute_returns", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordLossResults", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.after_update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.safemean", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.safemean", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordTrainResults_gail", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Write_Result.step_train", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.update_linear_schedule", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.step", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.insert", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordDisLossResults", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Policy.act", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecNormalize.eval", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordDisLossResults", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.predict_reward", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Policy.get_value", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.get_vec_normalize"], ["", "def", "gailLearning_mujoco_origin", "(", "cl_args", ",", "envs", ",", "envs_eval", ",", "actor_critic", ",", "agent", ",", "discriminator", ",", "rollouts", ",", "gail_train_loader", ",", "device", ",", "utli", ")", ":", "\n", "\n", "    ", "log_save_name", "=", "utli", ".", "Log_save_name4gail", "(", "cl_args", ")", "\n", "log_save_path", "=", "os", ".", "path", ".", "join", "(", "\"./runs\"", ",", "log_save_name", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "log_save_path", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "log_save_path", ")", "\n", "", "utli", ".", "writer", "=", "SummaryWriter", "(", "log_save_path", ")", "\n", "\n", "# model_dir = utli.Save_model_dir(cl_args.algo_id, cl_args.env_name)", "\n", "\n", "\n", "\n", "# Evaluate the initial network", "\n", "evaluations", "=", "[", "]", "\n", "# begin optimize", "\n", "\n", "reward_window4Evaluate", "=", "deque", "(", "maxlen", "=", "10", ")", "\n", "time_step", "=", "0", "\n", "episode_t", "=", "0", "\n", "episode_timesteps", "=", "0", "\n", "count", "=", "0", "\n", "\n", "# begin optimize", "\n", "\n", "nsteps", "=", "cl_args", ".", "num_steps", "\n", "S_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "nenv", "=", "1", "\n", "\n", "nbatch", "=", "np", ".", "floor", "(", "nsteps", "/", "nenv", ")", "\n", "nbatch", "=", "nbatch", ".", "astype", "(", "np", ".", "int16", ")", "\n", "nupdates", "=", "np", ".", "floor", "(", "cl_args", ".", "num_env_steps", "/", "nsteps", ")", "\n", "nupdates", "=", "nupdates", ".", "astype", "(", "np", ".", "int16", ")", "\n", "\n", "epinfobuf", "=", "deque", "(", "maxlen", "=", "10", ")", "\n", "\n", "epgailbuf", "=", "deque", "(", "maxlen", "=", "10", ")", "\n", "\n", "episode_rewards", "=", "deque", "(", "maxlen", "=", "10", ")", "\n", "\n", "cum_gailrewards", "=", ".0", "\n", "\n", "i_update", "=", "0", "\n", "dis_init", "=", "True", "\n", "\n", "obs", "=", "envs", ".", "reset", "(", ")", "\n", "rollouts", ".", "obs", "[", "0", "]", ".", "copy_", "(", "obs", ")", "\n", "rollouts", ".", "to", "(", "device", ")", "\n", "\n", "write_result", "=", "utli", ".", "Write_Result", "(", "cl_args", "=", "cl_args", ")", "\n", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "\n", "while", "i_update", "<", "nupdates", ":", "\n", "\n", "        ", "episode_t", "+=", "1", "\n", "i_update", "+=", "1", "\n", "epinfos", "=", "[", "]", "\n", "\n", "if", "cl_args", ".", "use_linear_lr_decay", ":", "\n", "# decrease learning rate linearly", "\n", "            ", "utli", ".", "update_linear_schedule", "(", "\n", "agent", ".", "optimizer", ",", "i_update", ",", "nupdates", ",", "\n", "cl_args", ".", "lr", ")", "\n", "\n", "", "for", "step", "in", "range", "(", "nbatch", ")", ":", "\n", "            ", "time_step", "+=", "1", "\n", "\n", "# Sample actions", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "value", ",", "action", ",", "action_log_prob", ",", "recurrent_hidden_states", "=", "actor_critic", ".", "act", "(", "\n", "rollouts", ".", "obs", "[", "step", "]", ",", "rollouts", ".", "recurrent_hidden_states", "[", "step", "]", ",", "\n", "rollouts", ".", "masks", "[", "step", "]", ")", "\n", "\n", "# action, log_prob, value = model_step(cl_args=cl_args, model=model, state=cur_state, device=device)", "\n", "# time.sleep(.002)", "\n", "# next_state, reward, done, infos = envs.step(action)   # error 01", "\n", "", "obs", ",", "reward", ",", "done", ",", "infos", "=", "envs", ".", "step", "(", "action", ")", "\n", "\n", "for", "info", "in", "infos", ":", "\n", "                ", "maybeepinfo", "=", "info", ".", "get", "(", "'episode'", ")", "\n", "if", "maybeepinfo", ":", "\n", "                    ", "epinfos", ".", "append", "(", "maybeepinfo", ")", "\n", "", "", "for", "info", "in", "infos", ":", "\n", "                ", "if", "'episode'", "in", "info", ".", "keys", "(", ")", ":", "\n", "                    ", "episode_rewards", ".", "append", "(", "info", "[", "'episode'", "]", "[", "'r'", "]", ")", "\n", "\n", "# If done then clean the history of observations.", "\n", "", "", "masks", "=", "torch", ".", "FloatTensor", "(", "\n", "[", "[", "0.0", "]", "if", "done_", "else", "[", "1.0", "]", "for", "done_", "in", "done", "]", ")", "\n", "bad_masks", "=", "torch", ".", "FloatTensor", "(", "\n", "[", "[", "0.0", "]", "if", "'bad_transition'", "in", "info", ".", "keys", "(", ")", "else", "[", "1.0", "]", "\n", "for", "info", "in", "infos", "]", ")", "\n", "rollouts", ".", "insert", "(", "obs", ",", "recurrent_hidden_states", ",", "action", ",", "\n", "action_log_prob", ",", "value", ",", "reward", ",", "masks", ",", "bad_masks", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "next_value", "=", "actor_critic", ".", "get_value", "(", "\n", "rollouts", ".", "obs", "[", "-", "1", "]", ",", "rollouts", ".", "recurrent_hidden_states", "[", "-", "1", "]", ",", "\n", "rollouts", ".", "masks", "[", "-", "1", "]", ")", ".", "detach", "(", ")", "\n", "\n", "# gail", "\n", "", "if", "cl_args", ".", "gail", ":", "\n", "            ", "if", "i_update", ">=", "cl_args", ".", "gail_thre", ":", "\n", "                ", "envs", ".", "venv", ".", "eval", "(", ")", "\n", "# gail_epoch = args.gail_epoch", "\n", "", "gail_epoch", "=", "cl_args", ".", "gail_epoch", "\n", "if", "i_update", "<", "cl_args", ".", "gail_thre", ":", "\n", "                ", "gail_epoch", "=", "cl_args", ".", "gail_pre_epoch", "# Warm up", "\n", "\n", "", "dis_losses", ",", "dis_gps", ",", "dis_entropys", ",", "dis_total_losses", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "_", "in", "range", "(", "gail_epoch", ")", ":", "\n", "# dis_loss, dis_gp, dis_entropy, dis_total_loss = \\", "\n", "#     discriminator.update_zm(replay_buf=rollouts, expert_buf=expert_buffer,", "\n", "#                          obsfilt=utils.get_vec_normalize(envs)._obfilt, batch_size=cl_args.gail_batch_size)", "\n", "                ", "dis_loss", ",", "dis_gp", ",", "dis_entropy", ",", "dis_total_loss", "=", "discriminator", ".", "update", "(", "gail_train_loader", ",", "rollouts", ",", "utils", ".", "get_vec_normalize", "(", "envs", ")", ".", "_obfilt", ")", "\n", "dis_losses", ".", "append", "(", "dis_loss", ")", "\n", "dis_gps", ".", "append", "(", "dis_gp", ")", "\n", "dis_entropys", ".", "append", "(", "dis_entropy", ")", "\n", "dis_total_losses", ".", "append", "(", "dis_total_loss", ")", "\n", "\n", "", "if", "dis_init", ":", "\n", "                ", "utli", ".", "recordDisLossResults", "(", "results", "=", "(", "np", ".", "array", "(", "dis_losses", ")", "[", "0", "]", ",", "\n", "np", ".", "array", "(", "dis_gps", ")", "[", "0", "]", ",", "\n", "np", ".", "array", "(", "dis_entropys", ")", "[", "0", "]", ",", "\n", "np", ".", "array", "(", "dis_total_losses", ")", "[", "0", "]", ")", ",", "\n", "time_step", "=", "0", ")", "\n", "dis_init", "=", "False", "\n", "\n", "\n", "", "utli", ".", "recordDisLossResults", "(", "results", "=", "(", "np", ".", "mean", "(", "np", ".", "array", "(", "dis_losses", ")", ")", ",", "\n", "np", ".", "mean", "(", "np", ".", "array", "(", "dis_gps", ")", ")", ",", "\n", "np", ".", "mean", "(", "np", ".", "array", "(", "dis_entropys", ")", ")", ",", "\n", "np", ".", "mean", "(", "np", ".", "array", "(", "dis_total_losses", ")", ")", ")", ",", "\n", "time_step", "=", "time_step", ")", "\n", "\n", "\n", "for", "step", "in", "range", "(", "nbatch", ")", ":", "\n", "                ", "rollouts", ".", "rewards", "[", "step", "]", "=", "discriminator", ".", "predict_reward", "(", "\n", "rollouts", ".", "obs", "[", "step", "]", ",", "rollouts", ".", "actions", "[", "step", "]", ",", "cl_args", ".", "gamma", ",", "\n", "rollouts", ".", "masks", "[", "step", "]", ")", "\n", "if", "rollouts", ".", "masks", "[", "step", "]", "==", "1", ":", "\n", "                    ", "cum_gailrewards", "+=", "rollouts", ".", "rewards", "[", "step", "]", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "                    ", "epgailbuf", ".", "append", "(", "cum_gailrewards", ")", "\n", "cum_gailrewards", "=", ".0", "\n", "\n", "# compute returns", "\n", "", "", "", "rollouts", ".", "compute_returns", "(", "next_value", ",", "cl_args", ".", "use_gae", ",", "cl_args", ".", "gamma", ",", "\n", "cl_args", ".", "gae_lambda", ",", "cl_args", ".", "use_proper_time_limits", ")", "\n", "\n", "# training PPO policy", "\n", "\n", "value_loss", ",", "action_loss", ",", "dist_entropy", ",", "total_loss", "=", "agent", ".", "update", "(", "rollouts", ")", "\n", "\n", "utli", ".", "recordLossResults", "(", "results", "=", "(", "value_loss", ",", "\n", "action_loss", ",", "\n", "dist_entropy", ",", "\n", "total_loss", ")", ",", "\n", "time_step", "=", "time_step", ")", "\n", "rollouts", ".", "after_update", "(", ")", "\n", "\n", "\n", "epinfobuf", ".", "extend", "(", "epinfos", ")", "\n", "if", "not", "len", "(", "epinfobuf", ")", ":", "\n", "            ", "continue", "\n", "", "eprewmean", "=", "safemean", "(", "[", "epinfo", "[", "'r'", "]", "for", "epinfo", "in", "epinfobuf", "]", ")", "\n", "eplenmean", "=", "safemean", "(", "[", "epinfo", "[", "'l'", "]", "for", "epinfo", "in", "epinfobuf", "]", ")", "\n", "\n", "# utli.recordTrainResults(results=(eprewmean,", "\n", "#                                  eplenmean),", "\n", "#                         time_step=time_step)", "\n", "\n", "utli", ".", "recordTrainResults_gail", "(", "results", "=", "(", "eprewmean", ",", "\n", "eplenmean", ",", "\n", "np", ".", "mean", "(", "np", ".", "array", "(", "epgailbuf", ")", ")", ")", ",", "\n", "time_step", "=", "time_step", ")", "\n", "\n", "write_result", ".", "step_train", "(", "time_step", ")", "\n", "\n", "\n", "print", "(", "\"Episode: %d,   Time steps: %d,   Mean length: %d    Mean Reward: %f    Mean Gail Reward:%f\"", "\n", "%", "(", "episode_t", ",", "time_step", ",", "eplenmean", ",", "eprewmean", ",", "np", ".", "mean", "(", "np", ".", "array", "(", "epgailbuf", ")", ")", ")", ")", "\n", "\n", "if", "i_update", "%", "cl_args", ".", "log_interval", "==", "0", "and", "len", "(", "episode_rewards", ")", ">", "1", ":", "\n", "            ", "total_num_steps", "=", "(", "i_update", "+", "1", ")", "*", "cl_args", ".", "num_processes", "*", "cl_args", ".", "num_steps", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "\n", "\"Updates {}, num timesteps {}, FPS {} \\n Last {} training episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\\n\"", "\n", ".", "format", "(", "i_update", ",", "total_num_steps", ",", "\n", "int", "(", "total_num_steps", "/", "(", "end", "-", "start", ")", ")", ",", "\n", "len", "(", "episode_rewards", ")", ",", "np", ".", "mean", "(", "episode_rewards", ")", ",", "\n", "np", ".", "median", "(", "episode_rewards", ")", ",", "np", ".", "min", "(", "episode_rewards", ")", ",", "\n", "np", ".", "max", "(", "episode_rewards", ")", ",", "dist_entropy", ",", "value_loss", ",", "\n", "action_loss", ")", ")", "\n", "\n", "", "", "E_time", "=", "time", ".", "time", "(", ")", "\n", "# store results", "\n", "utli", ".", "store_results", "(", "evaluations", ",", "time_step", ",", "cl_args", ",", "S_time", "=", "S_time", ",", "E_time", "=", "E_time", ")", "\n", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.gailLearning_mujoco_BL": [[608, 809], ["tools.utli.Log_save_name4gail", "os.path.join", "os.path.exists", "tensorboardX.SummaryWriter", "collections.deque", "time.time", "numpy.floor", "nbatch.astype.astype", "numpy.floor", "nupdates.astype.astype", "collections.deque", "collections.deque", "collections.deque", "envs.reset", "rollouts.obs[].copy_", "rollouts.to", "tools.utli.Write_Result", "time.time", "time.time", "tools.utli.store_results", "shutil.rmtree", "range", "rollouts.compute_returns", "agent.update", "tools.utli.recordLossResults", "rollouts.after_update", "collections.deque.extend", "learn.safemean", "learn.safemean", "tools.utli.recordTrainResults_gail", "utli.Write_Result.step_train", "print", "tools.utli.update_linear_schedule", "envs.step", "torch.FloatTensor", "torch.FloatTensor", "rollouts.insert", "torch.no_grad", "actor_critic.get_value().detach", "range", "tools.utli.recordDisLossResults", "range", "len", "time.time", "print", "torch.no_grad", "actor_critic.act", "info.get", "envs.venv.eval", "discriminator.update_zm", "dis_losses.append", "dis_gps.append", "dis_entropys.append", "dis_total_losses.append", "tools.utli.recordDisLossResults", "discriminator.predict_reward", "len", "epinfos.append", "info.keys", "collections.deque.append", "actor_critic.get_value", "rollouts.rewards[].item", "collections.deque.append", "numpy.mean", "numpy.mean", "int", "len", "numpy.mean", "numpy.median", "numpy.min", "numpy.max", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.array", "numpy.array", "info.keys", "tools.utils.get_vec_normalize", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Log_save_name4gail", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.store_results", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.compute_returns", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordLossResults", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.after_update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.safemean", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.safemean", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordTrainResults_gail", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Write_Result.step_train", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.update_linear_schedule", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.step", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.insert", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordDisLossResults", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Policy.act", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecNormalize.eval", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.update_zm", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordDisLossResults", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.predict_reward", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Policy.get_value", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.get_vec_normalize"], ["", "def", "gailLearning_mujoco_BL", "(", "cl_args", ",", "envs", ",", "envs_eval", ",", "actor_critic", ",", "agent", ",", "discriminator", ",", "rollouts", ",", "expert_buffer", ",", "device", ",", "utli", ")", ":", "\n", "\n", "    ", "log_save_name", "=", "utli", ".", "Log_save_name4gail", "(", "cl_args", ")", "\n", "log_save_path", "=", "os", ".", "path", ".", "join", "(", "\"./runs\"", ",", "log_save_name", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "log_save_path", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "log_save_path", ")", "\n", "", "utli", ".", "writer", "=", "SummaryWriter", "(", "log_save_path", ")", "\n", "\n", "# model_dir = utli.Save_model_dir(cl_args.algo_id, cl_args.env_name)", "\n", "\n", "\n", "\n", "# Evaluate the initial network", "\n", "evaluations", "=", "[", "]", "\n", "# begin optimize", "\n", "\n", "reward_window4Evaluate", "=", "deque", "(", "maxlen", "=", "10", ")", "\n", "time_step", "=", "0", "\n", "episode_t", "=", "0", "\n", "episode_timesteps", "=", "0", "\n", "count", "=", "0", "\n", "\n", "# begin optimize", "\n", "\n", "nsteps", "=", "cl_args", ".", "num_steps", "\n", "S_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "nenv", "=", "1", "\n", "\n", "nbatch", "=", "np", ".", "floor", "(", "nsteps", "/", "nenv", ")", "\n", "nbatch", "=", "nbatch", ".", "astype", "(", "np", ".", "int16", ")", "\n", "nupdates", "=", "np", ".", "floor", "(", "cl_args", ".", "num_env_steps", "/", "nsteps", ")", "\n", "nupdates", "=", "nupdates", ".", "astype", "(", "np", ".", "int16", ")", "\n", "\n", "epinfobuf", "=", "deque", "(", "maxlen", "=", "10", ")", "\n", "\n", "epgailbuf", "=", "deque", "(", "maxlen", "=", "10", ")", "\n", "\n", "episode_rewards", "=", "deque", "(", "maxlen", "=", "10", ")", "\n", "\n", "cum_gailrewards", "=", ".0", "\n", "\n", "i_update", "=", "0", "\n", "dis_init", "=", "True", "\n", "\n", "obs", "=", "envs", ".", "reset", "(", ")", "\n", "rollouts", ".", "obs", "[", "0", "]", ".", "copy_", "(", "obs", ")", "\n", "rollouts", ".", "to", "(", "device", ")", "\n", "\n", "write_result", "=", "utli", ".", "Write_Result", "(", "cl_args", "=", "cl_args", ")", "\n", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "\n", "while", "i_update", "<", "nupdates", ":", "\n", "\n", "        ", "episode_t", "+=", "1", "\n", "i_update", "+=", "1", "\n", "epinfos", "=", "[", "]", "\n", "\n", "if", "cl_args", ".", "use_linear_lr_decay", ":", "\n", "# decrease learning rate linearly", "\n", "            ", "utli", ".", "update_linear_schedule", "(", "\n", "agent", ".", "optimizer", ",", "i_update", ",", "nupdates", ",", "\n", "cl_args", ".", "lr", ")", "\n", "\n", "", "for", "step", "in", "range", "(", "nbatch", ")", ":", "\n", "            ", "time_step", "+=", "1", "\n", "\n", "# Sample actions", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "value", ",", "action", ",", "action_log_prob", ",", "recurrent_hidden_states", "=", "actor_critic", ".", "act", "(", "\n", "rollouts", ".", "obs", "[", "step", "]", ",", "rollouts", ".", "recurrent_hidden_states", "[", "step", "]", ",", "\n", "rollouts", ".", "masks", "[", "step", "]", ")", "\n", "\n", "# action, log_prob, value = model_step(cl_args=cl_args, model=model, state=cur_state, device=device)", "\n", "# time.sleep(.002)", "\n", "# next_state, reward, done, infos = envs.step(action)   # error 01", "\n", "", "obs", ",", "reward", ",", "done", ",", "infos", "=", "envs", ".", "step", "(", "action", ")", "\n", "\n", "for", "info", "in", "infos", ":", "\n", "                ", "maybeepinfo", "=", "info", ".", "get", "(", "'episode'", ")", "\n", "if", "maybeepinfo", ":", "\n", "                    ", "epinfos", ".", "append", "(", "maybeepinfo", ")", "\n", "", "", "for", "info", "in", "infos", ":", "\n", "                ", "if", "'episode'", "in", "info", ".", "keys", "(", ")", ":", "\n", "                    ", "episode_rewards", ".", "append", "(", "info", "[", "'episode'", "]", "[", "'r'", "]", ")", "\n", "\n", "# If done then clean the history of observations.", "\n", "", "", "masks", "=", "torch", ".", "FloatTensor", "(", "\n", "[", "[", "0.0", "]", "if", "done_", "else", "[", "1.0", "]", "for", "done_", "in", "done", "]", ")", "\n", "bad_masks", "=", "torch", ".", "FloatTensor", "(", "\n", "[", "[", "0.0", "]", "if", "'bad_transition'", "in", "info", ".", "keys", "(", ")", "else", "[", "1.0", "]", "\n", "for", "info", "in", "infos", "]", ")", "\n", "rollouts", ".", "insert", "(", "obs", ",", "recurrent_hidden_states", ",", "action", ",", "\n", "action_log_prob", ",", "value", ",", "reward", ",", "masks", ",", "bad_masks", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "next_value", "=", "actor_critic", ".", "get_value", "(", "\n", "rollouts", ".", "obs", "[", "-", "1", "]", ",", "rollouts", ".", "recurrent_hidden_states", "[", "-", "1", "]", ",", "\n", "rollouts", ".", "masks", "[", "-", "1", "]", ")", ".", "detach", "(", ")", "\n", "\n", "# gail", "\n", "", "if", "cl_args", ".", "gail", ":", "\n", "            ", "if", "i_update", ">=", "cl_args", ".", "gail_thre", ":", "\n", "                ", "envs", ".", "venv", ".", "eval", "(", ")", "\n", "# gail_epoch = args.gail_epoch", "\n", "", "gail_epoch", "=", "cl_args", ".", "gail_epoch", "\n", "if", "i_update", "<", "cl_args", ".", "gail_thre", ":", "\n", "                ", "gail_epoch", "=", "cl_args", ".", "gail_pre_epoch", "# Warm up", "\n", "\n", "", "dis_losses", ",", "dis_gps", ",", "dis_entropys", ",", "dis_total_losses", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "_", "in", "range", "(", "gail_epoch", ")", ":", "\n", "                ", "dis_loss", ",", "dis_gp", ",", "dis_entropy", ",", "dis_total_loss", "=", "discriminator", ".", "update_zm", "(", "replay_buf", "=", "rollouts", ",", "expert_buf", "=", "expert_buffer", ",", "\n", "obsfilt", "=", "utils", ".", "get_vec_normalize", "(", "envs", ")", ".", "_obfilt", ",", "batch_size", "=", "cl_args", ".", "gail_batch_size", ")", "\n", "# dis_loss, dis_gp, dis_entropy, dis_total_loss = discriminator.update(gail_train_loader, rollouts, utils.get_vec_normalize(envs)._obfilt)", "\n", "dis_losses", ".", "append", "(", "dis_loss", ")", "\n", "dis_gps", ".", "append", "(", "dis_gp", ")", "\n", "dis_entropys", ".", "append", "(", "dis_entropy", ")", "\n", "dis_total_losses", ".", "append", "(", "dis_total_loss", ")", "\n", "\n", "", "if", "dis_init", ":", "\n", "                ", "utli", ".", "recordDisLossResults", "(", "results", "=", "(", "np", ".", "array", "(", "dis_losses", ")", "[", "0", "]", ",", "\n", "np", ".", "array", "(", "dis_gps", ")", "[", "0", "]", ",", "\n", "np", ".", "array", "(", "dis_entropys", ")", "[", "0", "]", ",", "\n", "np", ".", "array", "(", "dis_total_losses", ")", "[", "0", "]", ")", ",", "\n", "time_step", "=", "0", ")", "\n", "dis_init", "=", "False", "\n", "\n", "\n", "", "utli", ".", "recordDisLossResults", "(", "results", "=", "(", "np", ".", "mean", "(", "np", ".", "array", "(", "dis_losses", ")", ")", ",", "\n", "np", ".", "mean", "(", "np", ".", "array", "(", "dis_gps", ")", ")", ",", "\n", "np", ".", "mean", "(", "np", ".", "array", "(", "dis_entropys", ")", ")", ",", "\n", "np", ".", "mean", "(", "np", ".", "array", "(", "dis_total_losses", ")", ")", ")", ",", "\n", "time_step", "=", "time_step", ")", "\n", "\n", "\n", "for", "step", "in", "range", "(", "nbatch", ")", ":", "\n", "                ", "rollouts", ".", "rewards", "[", "step", "]", "=", "discriminator", ".", "predict_reward", "(", "\n", "rollouts", ".", "obs", "[", "step", "]", ",", "rollouts", ".", "actions", "[", "step", "]", ",", "cl_args", ".", "gamma", ",", "\n", "rollouts", ".", "masks", "[", "step", "]", ")", "\n", "if", "rollouts", ".", "masks", "[", "step", "]", "==", "1", ":", "\n", "                    ", "cum_gailrewards", "+=", "rollouts", ".", "rewards", "[", "step", "]", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "                    ", "epgailbuf", ".", "append", "(", "cum_gailrewards", ")", "\n", "cum_gailrewards", "=", ".0", "\n", "\n", "# compute returns", "\n", "", "", "", "rollouts", ".", "compute_returns", "(", "next_value", ",", "cl_args", ".", "use_gae", ",", "cl_args", ".", "gamma", ",", "\n", "cl_args", ".", "gae_lambda", ",", "cl_args", ".", "use_proper_time_limits", ")", "\n", "\n", "# training PPO policy", "\n", "\n", "value_loss", ",", "action_loss", ",", "dist_entropy", ",", "total_loss", "=", "agent", ".", "update", "(", "rollouts", ")", "\n", "\n", "utli", ".", "recordLossResults", "(", "results", "=", "(", "value_loss", ",", "\n", "action_loss", ",", "\n", "dist_entropy", ",", "\n", "total_loss", ")", ",", "\n", "time_step", "=", "time_step", ")", "\n", "rollouts", ".", "after_update", "(", ")", "\n", "\n", "\n", "epinfobuf", ".", "extend", "(", "epinfos", ")", "\n", "if", "not", "len", "(", "epinfobuf", ")", ":", "\n", "            ", "continue", "\n", "", "eprewmean", "=", "safemean", "(", "[", "epinfo", "[", "'r'", "]", "for", "epinfo", "in", "epinfobuf", "]", ")", "\n", "eplenmean", "=", "safemean", "(", "[", "epinfo", "[", "'l'", "]", "for", "epinfo", "in", "epinfobuf", "]", ")", "\n", "\n", "# utli.recordTrainResults(results=(eprewmean,", "\n", "#                                  eplenmean),", "\n", "#                         time_step=time_step)", "\n", "\n", "utli", ".", "recordTrainResults_gail", "(", "results", "=", "(", "eprewmean", ",", "\n", "eplenmean", ",", "\n", "np", ".", "mean", "(", "np", ".", "array", "(", "epgailbuf", ")", ")", ")", ",", "\n", "time_step", "=", "time_step", ")", "\n", "\n", "write_result", ".", "step_train", "(", "time_step", ")", "\n", "\n", "\n", "print", "(", "\"Episode: %d,   Time steps: %d,   Mean length: %d    Mean Reward: %f    Mean Gail Reward:%f\"", "\n", "%", "(", "episode_t", ",", "time_step", ",", "eplenmean", ",", "eprewmean", ",", "np", ".", "mean", "(", "np", ".", "array", "(", "epgailbuf", ")", ")", ")", ")", "\n", "\n", "if", "i_update", "%", "cl_args", ".", "log_interval", "==", "0", "and", "len", "(", "episode_rewards", ")", ">", "1", ":", "\n", "            ", "total_num_steps", "=", "(", "i_update", "+", "1", ")", "*", "cl_args", ".", "num_processes", "*", "cl_args", ".", "num_steps", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "\n", "\"Updates {}, num timesteps {}, FPS {} \\n Last {} training episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\\n\"", "\n", ".", "format", "(", "i_update", ",", "total_num_steps", ",", "\n", "int", "(", "total_num_steps", "/", "(", "end", "-", "start", ")", ")", ",", "\n", "len", "(", "episode_rewards", ")", ",", "np", ".", "mean", "(", "episode_rewards", ")", ",", "\n", "np", ".", "median", "(", "episode_rewards", ")", ",", "np", ".", "min", "(", "episode_rewards", ")", ",", "\n", "np", ".", "max", "(", "episode_rewards", ")", ",", "dist_entropy", ",", "value_loss", ",", "\n", "action_loss", ")", ")", "\n", "\n", "", "", "E_time", "=", "time", ".", "time", "(", ")", "\n", "# store results", "\n", "utli", ".", "store_results", "(", "evaluations", ",", "time_step", ",", "cl_args", ",", "S_time", "=", "S_time", ",", "E_time", "=", "E_time", ")", "\n", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.Learning_process": [[810, 908], ["envs.reset", "rollouts.obs[].copy_", "rollouts.to", "collections.deque", "time.time", "range", "range", "rollouts.compute_returns", "agent.update", "rollouts.after_update", "int", "tools.utils.update_linear_schedule", "envs.step", "torch.FloatTensor", "torch.FloatTensor", "rollouts.insert", "torch.no_grad", "actor_critic.get_value().detach", "range", "range", "time.time", "print", "torch.no_grad", "actor_critic.act", "envs.venv.eval", "discr.update", "discr.predict_reward", "len", "info.keys", "collections.deque.append", "actor_critic.get_value", "int", "len", "numpy.mean", "numpy.median", "numpy.min", "numpy.max", "tools.utils.get_vec_normalize", "info.keys"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.compute_returns", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.after_update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.update_linear_schedule", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.step", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.insert", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Policy.act", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecNormalize.eval", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.predict_reward", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Policy.get_value", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.get_vec_normalize"], ["", "def", "Learning_process", "(", "args", ",", "envs", ",", "rollouts", ",", "actor_critic", ",", "agent", ",", "discr", ",", "gail_train_loader", ",", "device", ")", ":", "\n", "\n", "\n", "    ", "obs", "=", "envs", ".", "reset", "(", ")", "\n", "rollouts", ".", "obs", "[", "0", "]", ".", "copy_", "(", "obs", ")", "\n", "rollouts", ".", "to", "(", "device", ")", "\n", "\n", "episode_rewards", "=", "deque", "(", "maxlen", "=", "10", ")", "\n", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "num_updates", "=", "int", "(", "\n", "args", ".", "num_env_steps", ")", "//", "args", ".", "num_steps", "//", "args", ".", "num_processes", "\n", "for", "j", "in", "range", "(", "num_updates", ")", ":", "\n", "\n", "        ", "if", "args", ".", "use_linear_lr_decay", ":", "\n", "# decrease learning rate linearly", "\n", "            ", "utils", ".", "update_linear_schedule", "(", "\n", "agent", ".", "optimizer", ",", "j", ",", "num_updates", ",", "\n", "agent", ".", "optimizer", ".", "lr", "if", "args", ".", "algo", "==", "\"acktr\"", "else", "args", ".", "lr", ")", "\n", "\n", "", "for", "step", "in", "range", "(", "args", ".", "num_steps", ")", ":", "\n", "# Sample actions", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "value", ",", "action", ",", "action_log_prob", ",", "recurrent_hidden_states", "=", "actor_critic", ".", "act", "(", "\n", "rollouts", ".", "obs", "[", "step", "]", ",", "rollouts", ".", "recurrent_hidden_states", "[", "step", "]", ",", "\n", "rollouts", ".", "masks", "[", "step", "]", ")", "\n", "\n", "# Obser reward and next obs", "\n", "", "obs", ",", "reward", ",", "done", ",", "infos", "=", "envs", ".", "step", "(", "action", ")", "\n", "\n", "for", "info", "in", "infos", ":", "\n", "                ", "if", "'episode'", "in", "info", ".", "keys", "(", ")", ":", "\n", "                    ", "episode_rewards", ".", "append", "(", "info", "[", "'episode'", "]", "[", "'r'", "]", ")", "\n", "\n", "# If done then clean the history of observations.", "\n", "", "", "masks", "=", "torch", ".", "FloatTensor", "(", "\n", "[", "[", "0.0", "]", "if", "done_", "else", "[", "1.0", "]", "for", "done_", "in", "done", "]", ")", "\n", "bad_masks", "=", "torch", ".", "FloatTensor", "(", "\n", "[", "[", "0.0", "]", "if", "'bad_transition'", "in", "info", ".", "keys", "(", ")", "else", "[", "1.0", "]", "\n", "for", "info", "in", "infos", "]", ")", "\n", "rollouts", ".", "insert", "(", "obs", ",", "recurrent_hidden_states", ",", "action", ",", "\n", "action_log_prob", ",", "value", ",", "reward", ",", "masks", ",", "bad_masks", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "next_value", "=", "actor_critic", ".", "get_value", "(", "\n", "rollouts", ".", "obs", "[", "-", "1", "]", ",", "rollouts", ".", "recurrent_hidden_states", "[", "-", "1", "]", ",", "\n", "rollouts", ".", "masks", "[", "-", "1", "]", ")", ".", "detach", "(", ")", "\n", "\n", "", "if", "args", ".", "gail", ":", "\n", "            ", "if", "j", ">=", "10", ":", "\n", "                ", "envs", ".", "venv", ".", "eval", "(", ")", "\n", "\n", "", "gail_epoch", "=", "args", ".", "gail_epoch", "\n", "if", "j", "<", "10", ":", "\n", "                ", "gail_epoch", "=", "100", "# Warm up", "\n", "", "for", "_", "in", "range", "(", "gail_epoch", ")", ":", "\n", "                ", "discr", ".", "update", "(", "gail_train_loader", ",", "rollouts", ",", "\n", "utils", ".", "get_vec_normalize", "(", "envs", ")", ".", "_obfilt", ")", "\n", "\n", "", "for", "step", "in", "range", "(", "args", ".", "num_steps", ")", ":", "\n", "                ", "rollouts", ".", "rewards", "[", "step", "]", "=", "discr", ".", "predict_reward", "(", "\n", "rollouts", ".", "obs", "[", "step", "]", ",", "rollouts", ".", "actions", "[", "step", "]", ",", "args", ".", "gamma", ",", "\n", "rollouts", ".", "masks", "[", "step", "]", ")", "\n", "\n", "", "", "rollouts", ".", "compute_returns", "(", "next_value", ",", "args", ".", "use_gae", ",", "args", ".", "gamma", ",", "\n", "args", ".", "gae_lambda", ",", "args", ".", "use_proper_time_limits", ")", "\n", "\n", "value_loss", ",", "action_loss", ",", "dist_entropy", ",", "total_loss", "=", "agent", ".", "update", "(", "rollouts", ")", "\n", "\n", "rollouts", ".", "after_update", "(", ")", "\n", "\n", "# save for every interval-th episode or for the last epoch", "\n", "# if (j % args.save_interval == 0", "\n", "#     or j == num_updates - 1) and args.save_dir != \"\":", "\n", "#     save_path = os.path.join(args.save_dir, args.algo)", "\n", "#     try:", "\n", "#         os.makedirs(save_path)", "\n", "#     except OSError:", "\n", "#         pass", "\n", "#", "\n", "#     torch.save([", "\n", "#         actor_critic,", "\n", "#         getattr(utils.get_vec_normalize(envs), 'ob_rms', None)", "\n", "#     ], os.path.join(save_path, args.env_name + \".pt\"))", "\n", "\n", "if", "j", "%", "args", ".", "log_interval", "==", "0", "and", "len", "(", "episode_rewards", ")", ">", "1", ":", "\n", "            ", "total_num_steps", "=", "(", "j", "+", "1", ")", "*", "args", ".", "num_processes", "*", "args", ".", "num_steps", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "\n", "\"Updates {}, num timesteps {}, FPS {} \\n Last {} training episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\\n\"", "\n", ".", "format", "(", "j", ",", "total_num_steps", ",", "\n", "int", "(", "total_num_steps", "/", "(", "end", "-", "start", ")", ")", ",", "\n", "len", "(", "episode_rewards", ")", ",", "np", ".", "mean", "(", "episode_rewards", ")", ",", "\n", "np", ".", "median", "(", "episode_rewards", ")", ",", "np", ".", "min", "(", "episode_rewards", ")", ",", "\n", "np", ".", "max", "(", "episode_rewards", ")", ",", "dist_entropy", ",", "value_loss", ",", "\n", "action_loss", ")", ")", "\n", "\n", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.Learning_process_record": [[909, 1053], ["tools.utli.Log_save_name4gail", "os.path.join", "os.path.exists", "tensorboardX.SummaryWriter", "envs.reset", "rollouts.obs[].copy_", "rollouts.to", "collections.deque", "collections.deque", "time.time", "range", "shutil.rmtree", "range", "rollouts.compute_returns", "agent.update", "tools.utli.recordLossResults", "rollouts.after_update", "learn.safemean", "learn.safemean", "tools.utli.recordTrainResults", "print", "int", "tools.utils.update_linear_schedule", "envs.step", "torch.FloatTensor", "torch.FloatTensor", "rollouts.insert", "torch.no_grad", "actor_critic.get_value().detach", "range", "tools.utli.recordDisLossResults", "range", "len", "time.time", "print", "torch.no_grad", "actor_critic.act", "info.get", "envs.venv.eval", "discr.update", "dis_losses.append", "dis_gps.append", "dis_entropys.append", "dis_total_losses.append", "discr.predict_reward", "len", "info.keys", "collections.deque.append", "collections.deque.append", "actor_critic.get_value", "int", "len", "numpy.mean", "numpy.median", "numpy.min", "numpy.max", "tools.utils.get_vec_normalize", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "info.keys", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Log_save_name4gail", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.compute_returns", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordLossResults", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.after_update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.safemean", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.safemean", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordTrainResults", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.update_linear_schedule", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.step", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.insert", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordDisLossResults", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Policy.act", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecNormalize.eval", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.predict_reward", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Policy.get_value", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.get_vec_normalize"], ["", "def", "Learning_process_record", "(", "args", ",", "envs", ",", "rollouts", ",", "actor_critic", ",", "agent", ",", "discr", ",", "gail_train_loader", ",", "device", ")", ":", "\n", "\n", "\n", "    ", "log_save_name", "=", "utli", ".", "Log_save_name4gail", "(", "args", ")", "\n", "log_save_path", "=", "os", ".", "path", ".", "join", "(", "\"./runs\"", ",", "log_save_name", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "log_save_path", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "log_save_path", ")", "\n", "", "utli", ".", "writer", "=", "SummaryWriter", "(", "log_save_path", ")", "\n", "\n", "obs", "=", "envs", ".", "reset", "(", ")", "\n", "rollouts", ".", "obs", "[", "0", "]", ".", "copy_", "(", "obs", ")", "\n", "rollouts", ".", "to", "(", "device", ")", "\n", "\n", "episode_rewards", "=", "deque", "(", "maxlen", "=", "10", ")", "\n", "epinfobuf", "=", "deque", "(", "maxlen", "=", "10", ")", "\n", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "num_updates", "=", "int", "(", "\n", "args", ".", "num_env_steps", ")", "//", "args", ".", "num_steps", "//", "args", ".", "num_processes", "\n", "time_step", "=", "0", "\n", "for", "j", "in", "range", "(", "num_updates", ")", ":", "\n", "\n", "        ", "if", "args", ".", "use_linear_lr_decay", ":", "\n", "# decrease learning rate linearly", "\n", "            ", "utils", ".", "update_linear_schedule", "(", "\n", "agent", ".", "optimizer", ",", "j", ",", "num_updates", ",", "\n", "agent", ".", "optimizer", ".", "lr", "if", "args", ".", "algo", "==", "\"acktr\"", "else", "args", ".", "lr", ")", "\n", "\n", "", "for", "step", "in", "range", "(", "args", ".", "num_steps", ")", ":", "\n", "            ", "time_step", "+=", "1", "\n", "# Sample actions", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "value", ",", "action", ",", "action_log_prob", ",", "recurrent_hidden_states", "=", "actor_critic", ".", "act", "(", "\n", "rollouts", ".", "obs", "[", "step", "]", ",", "rollouts", ".", "recurrent_hidden_states", "[", "step", "]", ",", "\n", "rollouts", ".", "masks", "[", "step", "]", ")", "\n", "\n", "# Obser reward and next obs", "\n", "", "obs", ",", "reward", ",", "done", ",", "infos", "=", "envs", ".", "step", "(", "action", ")", "\n", "\n", "for", "info", "in", "infos", ":", "\n", "                ", "if", "'episode'", "in", "info", ".", "keys", "(", ")", ":", "\n", "                    ", "episode_rewards", ".", "append", "(", "info", "[", "'episode'", "]", "[", "'r'", "]", ")", "\n", "\n", "", "", "for", "info", "in", "infos", ":", "\n", "                ", "maybeepinfo", "=", "info", ".", "get", "(", "'episode'", ")", "\n", "if", "maybeepinfo", ":", "\n", "                    ", "epinfobuf", ".", "append", "(", "maybeepinfo", ")", "\n", "\n", "# If done then clean the history of observations.", "\n", "", "", "masks", "=", "torch", ".", "FloatTensor", "(", "\n", "[", "[", "0.0", "]", "if", "done_", "else", "[", "1.0", "]", "for", "done_", "in", "done", "]", ")", "\n", "bad_masks", "=", "torch", ".", "FloatTensor", "(", "\n", "[", "[", "0.0", "]", "if", "'bad_transition'", "in", "info", ".", "keys", "(", ")", "else", "[", "1.0", "]", "\n", "for", "info", "in", "infos", "]", ")", "\n", "rollouts", ".", "insert", "(", "obs", ",", "recurrent_hidden_states", ",", "action", ",", "\n", "action_log_prob", ",", "value", ",", "reward", ",", "masks", ",", "bad_masks", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "next_value", "=", "actor_critic", ".", "get_value", "(", "\n", "rollouts", ".", "obs", "[", "-", "1", "]", ",", "rollouts", ".", "recurrent_hidden_states", "[", "-", "1", "]", ",", "\n", "rollouts", ".", "masks", "[", "-", "1", "]", ")", ".", "detach", "(", ")", "\n", "\n", "", "if", "args", ".", "gail", ":", "\n", "            ", "if", "j", ">=", "10", ":", "\n", "                ", "envs", ".", "venv", ".", "eval", "(", ")", "\n", "\n", "", "gail_epoch", "=", "args", ".", "gail_epoch", "\n", "if", "j", "<", "10", ":", "\n", "                ", "gail_epoch", "=", "100", "# Warm up", "\n", "", "dis_losses", ",", "dis_gps", ",", "dis_entropys", ",", "dis_total_losses", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "_", "in", "range", "(", "gail_epoch", ")", ":", "\n", "                ", "dis_loss", ",", "dis_gp", ",", "dis_entropy", ",", "dis_total_loss", "=", "discr", ".", "update", "(", "gail_train_loader", ",", "rollouts", ",", "\n", "utils", ".", "get_vec_normalize", "(", "envs", ")", ".", "_obfilt", ")", "\n", "dis_losses", ".", "append", "(", "dis_loss", ")", "\n", "dis_gps", ".", "append", "(", "dis_gp", ")", "\n", "dis_entropys", ".", "append", "(", "dis_entropy", ")", "\n", "dis_total_losses", ".", "append", "(", "dis_total_loss", ")", "\n", "\n", "", "utli", ".", "recordDisLossResults", "(", "results", "=", "(", "np", ".", "mean", "(", "np", ".", "array", "(", "dis_losses", ")", ")", ",", "\n", "np", ".", "mean", "(", "np", ".", "array", "(", "dis_gps", ")", ")", ",", "\n", "np", ".", "mean", "(", "np", ".", "array", "(", "dis_entropys", ")", ")", ",", "\n", "np", ".", "mean", "(", "np", ".", "array", "(", "dis_total_losses", ")", ")", ")", ",", "\n", "time_step", "=", "time_step", ")", "\n", "\n", "for", "step", "in", "range", "(", "args", ".", "num_steps", ")", ":", "\n", "                ", "rollouts", ".", "rewards", "[", "step", "]", "=", "discr", ".", "predict_reward", "(", "\n", "rollouts", ".", "obs", "[", "step", "]", ",", "rollouts", ".", "actions", "[", "step", "]", ",", "args", ".", "gamma", ",", "\n", "rollouts", ".", "masks", "[", "step", "]", ")", "\n", "\n", "", "", "rollouts", ".", "compute_returns", "(", "next_value", ",", "args", ".", "use_gae", ",", "args", ".", "gamma", ",", "\n", "args", ".", "gae_lambda", ",", "args", ".", "use_proper_time_limits", ")", "\n", "\n", "value_loss", ",", "action_loss", ",", "dist_entropy", ",", "total_loss", "=", "agent", ".", "update", "(", "rollouts", ")", "\n", "\n", "utli", ".", "recordLossResults", "(", "results", "=", "(", "value_loss", ",", "\n", "action_loss", ",", "\n", "dist_entropy", ",", "\n", "total_loss", ")", ",", "\n", "time_step", "=", "time_step", ")", "\n", "\n", "rollouts", ".", "after_update", "(", ")", "\n", "\n", "# epinfobuf.extend(epinfos)", "\n", "if", "not", "len", "(", "epinfobuf", ")", ":", "\n", "            ", "continue", "\n", "", "eprewmean", "=", "safemean", "(", "[", "epinfo", "[", "'r'", "]", "for", "epinfo", "in", "epinfobuf", "]", ")", "\n", "eplenmean", "=", "safemean", "(", "[", "epinfo", "[", "'l'", "]", "for", "epinfo", "in", "epinfobuf", "]", ")", "\n", "\n", "utli", ".", "recordTrainResults", "(", "results", "=", "(", "eprewmean", ",", "\n", "eplenmean", ")", ",", "\n", "time_step", "=", "time_step", ")", "\n", "\n", "# write_result.step_train(time_step)", "\n", "\n", "print", "(", "\"Episode: %d,   Time steps: %d,   Mean length: %d    Mean Reward: %f  \"", "\n", "%", "(", "j", ",", "time_step", ",", "eplenmean", ",", "eprewmean", ")", ")", "\n", "\n", "# save for every interval-th episode or for the last epoch", "\n", "# if (j % args.save_interval == 0", "\n", "#     or j == num_updates - 1) and args.save_dir != \"\":", "\n", "#     save_path = os.path.join(args.save_dir, args.algo)", "\n", "#     try:", "\n", "#         os.makedirs(save_path)", "\n", "#     except OSError:", "\n", "#         pass", "\n", "#", "\n", "#     torch.save([", "\n", "#         actor_critic,", "\n", "#         getattr(utils.get_vec_normalize(envs), 'ob_rms', None)", "\n", "#     ], os.path.join(save_path, args.env_name + \".pt\"))", "\n", "\n", "if", "j", "%", "args", ".", "log_interval", "==", "0", "and", "len", "(", "episode_rewards", ")", ">", "1", ":", "\n", "            ", "total_num_steps", "=", "(", "j", "+", "1", ")", "*", "args", ".", "num_processes", "*", "args", ".", "num_steps", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "\n", "\"Updates {}, num timesteps {}, FPS {} \\n Last {} training episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\\n\"", "\n", ".", "format", "(", "j", ",", "total_num_steps", ",", "\n", "int", "(", "total_num_steps", "/", "(", "end", "-", "start", ")", ")", ",", "\n", "len", "(", "episode_rewards", ")", ",", "np", ".", "mean", "(", "episode_rewards", ")", ",", "\n", "np", ".", "median", "(", "episode_rewards", ")", ",", "np", ".", "min", "(", "episode_rewards", ")", ",", "\n", "np", ".", "max", "(", "episode_rewards", ")", ",", "dist_entropy", ",", "value_loss", ",", "\n", "action_loss", ")", ")", "\n", "\n", "", "", "return", "True", "", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.TimeLimitMask.step": [[116, 122], ["envs.TimeLimitMask.env.step"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.step"], ["    ", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "obs", ",", "rew", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "if", "done", "and", "self", ".", "env", ".", "_max_episode_steps", "==", "self", ".", "env", ".", "_elapsed_steps", ":", "\n", "            ", "info", "[", "'bad_transition'", "]", "=", "True", "\n", "\n", "", "return", "obs", ",", "rew", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.TimeLimitMask.reset": [[123, 125], ["envs.TimeLimitMask.env.reset"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.MaskGoal.observation": [[129, 133], ["None"], "methods", ["None"], ["    ", "def", "observation", "(", "self", ",", "observation", ")", ":", "\n", "        ", "if", "self", ".", "env", ".", "_elapsed_steps", ">", "0", ":", "\n", "            ", "observation", "[", "-", "2", ":", "]", "=", "0", "\n", "", "return", "observation", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.TransposeObs.__init__": [[136, 141], ["gym.ObservationWrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Transpose observation space (base class)\n        \"\"\"", "\n", "super", "(", "TransposeObs", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.TransposeImage.__init__": [[144, 159], ["envs.TransposeObs.__init__", "gym.spaces.box.Box", "len", "str"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", "=", "None", ",", "op", "=", "[", "2", ",", "0", ",", "1", "]", ")", ":", "\n", "        ", "\"\"\"\n        Transpose observation space for images\n        \"\"\"", "\n", "super", "(", "TransposeImage", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "assert", "len", "(", "op", ")", "==", "3", ",", "\"Error: Operation, \"", "+", "str", "(", "op", ")", "+", "\", must be dim3\"", "\n", "self", ".", "op", "=", "op", "\n", "obs_shape", "=", "self", ".", "observation_space", ".", "shape", "\n", "self", ".", "observation_space", "=", "Box", "(", "\n", "self", ".", "observation_space", ".", "low", "[", "0", ",", "0", ",", "0", "]", ",", "\n", "self", ".", "observation_space", ".", "high", "[", "0", ",", "0", ",", "0", "]", ",", "[", "\n", "obs_shape", "[", "self", ".", "op", "[", "0", "]", "]", ",", "obs_shape", "[", "self", ".", "op", "[", "1", "]", "]", ",", "\n", "obs_shape", "[", "self", ".", "op", "[", "2", "]", "]", "\n", "]", ",", "\n", "dtype", "=", "self", ".", "observation_space", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.TransposeImage.observation": [[160, 162], ["ob.transpose"], "methods", ["None"], ["", "def", "observation", "(", "self", ",", "ob", ")", ":", "\n", "        ", "return", "ob", ".", "transpose", "(", "self", ".", "op", "[", "0", "]", ",", "self", ".", "op", "[", "1", "]", ",", "self", ".", "op", "[", "2", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecPyTorch.__init__": [[165, 169], ["baselines.common.vec_env.VecEnvWrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "venv", ",", "device", ")", ":", "\n", "        ", "\"\"\"Return only every `skip`-th frame\"\"\"", "\n", "super", "(", "VecPyTorch", ",", "self", ")", ".", "__init__", "(", "venv", ")", "\n", "self", ".", "device", "=", "device", "\n", "# TODO: Fix data types", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecPyTorch.reset": [[171, 175], ["envs.VecPyTorch.venv.reset", "torch.from_numpy().float().to", "torch.from_numpy().float", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "obs", "=", "self", ".", "venv", ".", "reset", "(", ")", "\n", "obs", "=", "torch", ".", "from_numpy", "(", "obs", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecPyTorch.step_async": [[176, 182], ["isinstance", "actions.squeeze.squeeze.cpu().numpy", "envs.VecPyTorch.venv.step_async", "actions.squeeze.squeeze.squeeze", "actions.squeeze.squeeze.cpu"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.step_async"], ["", "def", "step_async", "(", "self", ",", "actions", ")", ":", "\n", "        ", "if", "isinstance", "(", "actions", ",", "torch", ".", "LongTensor", ")", ":", "\n", "# Squeeze the dimension for discrete actions", "\n", "            ", "actions", "=", "actions", ".", "squeeze", "(", "1", ")", "\n", "", "actions", "=", "actions", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "self", ".", "venv", ".", "step_async", "(", "actions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecPyTorch.step_wait": [[183, 188], ["envs.VecPyTorch.venv.step_wait", "torch.from_numpy().float().to", "torch.from_numpy().unsqueeze().float", "torch.from_numpy().float", "torch.from_numpy().unsqueeze", "torch.from_numpy", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.step_wait", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to"], ["", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "venv", ".", "step_wait", "(", ")", "\n", "obs", "=", "torch", ".", "from_numpy", "(", "obs", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "reward", "=", "torch", ".", "from_numpy", "(", "reward", ")", ".", "unsqueeze", "(", "dim", "=", "1", ")", ".", "float", "(", ")", "\n", "return", "obs", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecNormalize.__init__": [[191, 194], ["baselines.common.vec_env.vec_normalize.VecNormalize.__init__"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "VecNormalize", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "training", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecNormalize._obfilt": [[195, 205], ["numpy.clip", "envs.VecNormalize.ob_rms.update", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update"], ["", "def", "_obfilt", "(", "self", ",", "obs", ",", "update", "=", "True", ")", ":", "\n", "        ", "if", "self", ".", "ob_rms", ":", "\n", "            ", "if", "self", ".", "training", "and", "update", ":", "\n", "                ", "self", ".", "ob_rms", ".", "update", "(", "obs", ")", "\n", "", "obs", "=", "np", ".", "clip", "(", "(", "obs", "-", "self", ".", "ob_rms", ".", "mean", ")", "/", "\n", "np", ".", "sqrt", "(", "self", ".", "ob_rms", ".", "var", "+", "self", ".", "epsilon", ")", ",", "\n", "-", "self", ".", "clipob", ",", "self", ".", "clipob", ")", "\n", "return", "obs", "\n", "", "else", ":", "\n", "            ", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecNormalize.train": [[206, 208], ["None"], "methods", ["None"], ["", "", "def", "train", "(", "self", ")", ":", "\n", "        ", "self", ".", "training", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecNormalize.eval": [[209, 211], ["None"], "methods", ["None"], ["", "def", "eval", "(", "self", ")", ":", "\n", "        ", "self", ".", "training", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecPyTorchFrameStack.__init__": [[216, 234], ["numpy.repeat", "numpy.repeat", "torch.zeros().to", "gym.spaces.Box", "baselines.common.vec_env.VecEnvWrapper.__init__", "torch.device", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "venv", ",", "nstack", ",", "device", "=", "None", ")", ":", "\n", "        ", "self", ".", "venv", "=", "venv", "\n", "self", ".", "nstack", "=", "nstack", "\n", "\n", "wos", "=", "venv", ".", "observation_space", "# wrapped ob space", "\n", "self", ".", "shape_dim0", "=", "wos", ".", "shape", "[", "0", "]", "\n", "\n", "low", "=", "np", ".", "repeat", "(", "wos", ".", "low", ",", "self", ".", "nstack", ",", "axis", "=", "0", ")", "\n", "high", "=", "np", ".", "repeat", "(", "wos", ".", "high", ",", "self", ".", "nstack", ",", "axis", "=", "0", ")", "\n", "\n", "if", "device", "is", "None", ":", "\n", "            ", "device", "=", "torch", ".", "device", "(", "'cpu'", ")", "\n", "", "self", ".", "stacked_obs", "=", "torch", ".", "zeros", "(", "(", "venv", ".", "num_envs", ",", ")", "+", "\n", "low", ".", "shape", ")", ".", "to", "(", "device", ")", "\n", "\n", "observation_space", "=", "gym", ".", "spaces", ".", "Box", "(", "\n", "low", "=", "low", ",", "high", "=", "high", ",", "dtype", "=", "venv", ".", "observation_space", ".", "dtype", ")", "\n", "VecEnvWrapper", ".", "__init__", "(", "self", ",", "venv", ",", "observation_space", "=", "observation_space", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecPyTorchFrameStack.step_wait": [[235, 244], ["envs.VecPyTorchFrameStack.venv.step_wait", "enumerate"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.step_wait"], ["", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "obs", ",", "rews", ",", "news", ",", "infos", "=", "self", ".", "venv", ".", "step_wait", "(", ")", "\n", "self", ".", "stacked_obs", "[", ":", ",", ":", "-", "self", ".", "shape_dim0", "]", "=", "self", ".", "stacked_obs", "[", ":", ",", "self", ".", "shape_dim0", ":", "]", "\n", "for", "(", "i", ",", "new", ")", "in", "enumerate", "(", "news", ")", ":", "\n", "            ", "if", "new", ":", "\n", "                ", "self", ".", "stacked_obs", "[", "i", "]", "=", "0", "\n", "", "", "self", ".", "stacked_obs", "[", ":", ",", "-", "self", ".", "shape_dim0", ":", "]", "=", "obs", "\n", "return", "self", ".", "stacked_obs", ",", "rews", ",", "news", ",", "infos", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecPyTorchFrameStack.reset": [[245, 253], ["envs.VecPyTorchFrameStack.venv.reset", "torch.zeros", "envs.VecPyTorchFrameStack.stacked_obs.zero_"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "obs", "=", "self", ".", "venv", ".", "reset", "(", ")", "\n", "if", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", ":", "\n", "            ", "self", ".", "stacked_obs", "=", "torch", ".", "zeros", "(", "self", ".", "stacked_obs", ".", "shape", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "stacked_obs", ".", "zero_", "(", ")", "\n", "", "self", ".", "stacked_obs", "[", ":", ",", "-", "self", ".", "shape_dim0", ":", "]", "=", "obs", "\n", "return", "self", ".", "stacked_obs", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecPyTorchFrameStack.close": [[254, 256], ["envs.VecPyTorchFrameStack.venv.close"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "venv", ".", "close", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.make_env": [[33, 76], ["env_id.startswith", "baselines.common.atari_wrappers.wrap_deepmind.seed", "env_id.split", "dm_control2gym.make", "gym.make", "hasattr", "isinstance", "baselines.common.atari_wrappers.make_atari", "str().find", "envs.TimeLimitMask", "baselines.bench.Monitor", "envs.TransposeImage", "os.path.join", "len", "baselines.common.atari_wrappers.wrap_deepmind", "len", "NotImplementedError", "len", "str", "str"], "function", ["None"], ["", "def", "make_env", "(", "env_id", ",", "seed", ",", "rank", ",", "log_dir", ",", "allow_early_resets", ")", ":", "\n", "    ", "def", "_thunk", "(", ")", ":", "\n", "        ", "if", "env_id", ".", "startswith", "(", "\"dm\"", ")", ":", "\n", "            ", "_", ",", "domain", ",", "task", "=", "env_id", ".", "split", "(", "'.'", ")", "\n", "env", "=", "dm_control2gym", ".", "make", "(", "domain_name", "=", "domain", ",", "task_name", "=", "task", ")", "\n", "", "else", ":", "\n", "            ", "env", "=", "gym", ".", "make", "(", "env_id", ")", "\n", "\n", "", "is_atari", "=", "hasattr", "(", "gym", ".", "envs", ",", "'atari'", ")", "and", "isinstance", "(", "\n", "env", ".", "unwrapped", ",", "gym", ".", "envs", ".", "atari", ".", "atari_env", ".", "AtariEnv", ")", "\n", "if", "is_atari", ":", "\n", "            ", "env", "=", "make_atari", "(", "env_id", ")", "\n", "\n", "", "env", ".", "seed", "(", "seed", "+", "rank", ")", "\n", "\n", "obs_shape", "=", "env", ".", "observation_space", ".", "shape", "\n", "\n", "if", "str", "(", "env", ".", "__class__", ".", "__name__", ")", ".", "find", "(", "'TimeLimit'", ")", ">=", "0", ":", "\n", "            ", "env", "=", "TimeLimitMask", "(", "env", ")", "\n", "\n", "", "if", "log_dir", "is", "not", "None", ":", "\n", "            ", "env", "=", "bench", ".", "Monitor", "(", "\n", "env", ",", "\n", "os", ".", "path", ".", "join", "(", "log_dir", ",", "str", "(", "rank", ")", ")", ",", "\n", "allow_early_resets", "=", "allow_early_resets", ")", "\n", "\n", "", "if", "is_atari", ":", "\n", "            ", "if", "len", "(", "env", ".", "observation_space", ".", "shape", ")", "==", "3", ":", "\n", "                ", "env", "=", "wrap_deepmind", "(", "env", ")", "\n", "", "", "elif", "len", "(", "env", ".", "observation_space", ".", "shape", ")", "==", "3", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"CNN models work only for atari,\\n\"", "\n", "\"please use a custom wrapper for a custom pixel input env.\\n\"", "\n", "\"See wrap_deepmind for an example.\"", ")", "\n", "\n", "# If the input has shape (W,H,3), wrap for PyTorch convolutions", "\n", "", "obs_shape", "=", "env", ".", "observation_space", ".", "shape", "\n", "if", "len", "(", "obs_shape", ")", "==", "3", "and", "obs_shape", "[", "2", "]", "in", "[", "1", ",", "3", "]", ":", "\n", "            ", "env", "=", "TransposeImage", "(", "env", ",", "op", "=", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "\n", "", "return", "env", "\n", "\n", "", "return", "_thunk", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.make_vec_envs": [[78, 112], ["envs.VecPyTorch", "envs.make_env", "len", "baselines.common.vec_env.subproc_vec_env.SubprocVecEnv", "baselines.common.vec_env.subproc_vec_env.SubprocVecEnv", "len", "envs.VecPyTorchFrameStack", "range", "envs.VecNormalize", "envs.VecNormalize", "len", "envs.VecPyTorchFrameStack"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.make_env"], ["", "def", "make_vec_envs", "(", "env_name", ",", "\n", "seed", ",", "\n", "num_processes", ",", "\n", "gamma", ",", "\n", "log_dir", ",", "\n", "device", ",", "\n", "allow_early_resets", ",", "\n", "num_frame_stack", "=", "None", ")", ":", "\n", "    ", "envs", "=", "[", "\n", "make_env", "(", "env_name", ",", "seed", ",", "i", ",", "log_dir", ",", "allow_early_resets", ")", "\n", "for", "i", "in", "range", "(", "num_processes", ")", "\n", "]", "\n", "\n", "if", "len", "(", "envs", ")", ">", "1", ":", "\n", "# envs = ShmemVecEnv(envs, context='fork')", "\n", "        ", "envs", "=", "SubprocVecEnv", "(", "envs", ")", "\n", "", "else", ":", "\n", "# envs = DummyVecEnv(envs)", "\n", "        ", "envs", "=", "SubprocVecEnv", "(", "envs", ")", "\n", "\n", "", "if", "len", "(", "envs", ".", "observation_space", ".", "shape", ")", "==", "1", ":", "\n", "        ", "if", "gamma", "is", "None", ":", "\n", "            ", "envs", "=", "VecNormalize", "(", "envs", ",", "ret", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "envs", "=", "VecNormalize", "(", "envs", ",", "gamma", "=", "gamma", ")", "\n", "\n", "", "", "envs", "=", "VecPyTorch", "(", "envs", ",", "device", ")", "\n", "\n", "if", "num_frame_stack", "is", "not", "None", ":", "\n", "        ", "envs", "=", "VecPyTorchFrameStack", "(", "envs", ",", "num_frame_stack", ",", "device", ")", "\n", "", "elif", "len", "(", "envs", ".", "observation_space", ".", "shape", ")", "==", "3", ":", "\n", "        ", "envs", "=", "VecPyTorchFrameStack", "(", "envs", ",", "4", ",", "device", ")", "\n", "\n", "", "return", "envs", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.build.build_env": [[7, 14], ["gym.make", "bench.monitor.Monitor.seed", "bench.monitor.Monitor", "logger.get_dir"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.get_dir"], ["def", "build_env", "(", "args", ",", "nenv", ",", "norm_env", "=", "True", ")", ":", "\n", "\n", "    ", "def", "make_env", "(", ")", ":", "\n", "        ", "def", "_thunk", "(", ")", ":", "\n", "            ", "env", "=", "gym", ".", "make", "(", "args", ".", "env_id", ")", "\n", "env", ".", "seed", "(", "args", ".", "seed", ")", "# to make the result more reproducibility", "\n", "env", "=", "Monitor", "(", "env", ",", "logger", ".", "get_dir", "(", ")", ",", "allow_early_resets", "=", "True", ")", "\n", "return", "env", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.build.build_envs": [[16, 32], ["common.vec_env.subproc_vec_env.SubprocVecEnv", "build.build_envs.make_env"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.make_env"], ["\n", "", "envs", "=", "[", "make_env", "(", ")", "for", "i", "in", "range", "(", "nenv", ")", "]", "\n", "envs", "=", "SubprocVecEnv", "(", "envs", ")", "\n", "if", "norm_env", ":", "\n", "        ", "envs", "=", "VecNormalize", "(", "envs", ")", "\n", "\n", "", "return", "envs", "\n", "\n", "", "def", "build_env4gail", "(", "args", ",", "nenv", ")", ":", "\n", "\n", "    ", "def", "make_env", "(", ")", ":", "\n", "        ", "def", "_thunk", "(", ")", ":", "\n", "            ", "env", "=", "gym", ".", "make", "(", "args", ".", "env_id", ")", "\n", "env", ".", "seed", "(", "args", ".", "seed", ")", "# to make the result more reproducibility", "\n", "env", "=", "Monitor", "(", "env", ",", "logger", ".", "get_dir", "(", ")", ",", "allow_early_resets", "=", "True", ")", "\n", "return", "env", "\n", "", "return", "_thunk", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.build.build_env4gail": [[33, 48], ["common.vec_env.subproc_vec_env.SubprocVecEnv", "common.vec_env.vec_normalize.VecNormalize", "build.build_envs.make_env"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.make_env"], ["\n", "", "envs", "=", "[", "make_env", "(", ")", "for", "i", "in", "range", "(", "nenv", ")", "]", "\n", "envs", "=", "SubprocVecEnv", "(", "envs", ")", "\n", "envs", "=", "VecNormalize", "(", "envs", ")", "\n", "\n", "return", "envs", "\n", "\n", "", "def", "build_env4wdail", "(", "args", ",", "nenv", ")", ":", "\n", "\n", "    ", "def", "make_env", "(", ")", ":", "\n", "        ", "def", "_thunk", "(", ")", ":", "\n", "            ", "env", "=", "gym", ".", "make", "(", "args", ".", "env_id", ")", "\n", "env", ".", "seed", "(", "args", ".", "seed", ")", "# to make the result more reproducibility", "\n", "env", "=", "Monitor", "(", "env", ",", "logger", ".", "get_dir", "(", ")", ",", "allow_early_resets", "=", "True", ")", "\n", "return", "env", "\n", "", "return", "_thunk", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.build.build_env4wdail": [[49, 65], ["common.vec_env.subproc_vec_env.SubprocVecEnv", "build.build_envs.make_env"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.make_env"], ["\n", "", "envs", "=", "[", "make_env", "(", ")", "for", "i", "in", "range", "(", "nenv", ")", "]", "\n", "envs", "=", "SubprocVecEnv", "(", "envs", ")", "\n", "if", "args", ".", "env_norm", ":", "\n", "        ", "envs", "=", "VecNormalize", "(", "envs", ")", "\n", "\n", "", "return", "envs", "", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learning_rate.LearningRate.__init__": [[10, 18], ["Exception"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "if", "LearningRate", ".", "__instance", "is", "not", "None", ":", "\n", "            ", "raise", "Exception", "(", "\"Singleton instantiation called twice\"", ")", "\n", "", "else", ":", "\n", "            ", "LearningRate", ".", "__instance", "=", "self", "\n", "self", ".", "lr", "=", "None", "\n", "self", ".", "decay_factor", "=", "None", "\n", "self", ".", "training_step", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learning_rate.LearningRate.get_instance": [[19, 29], ["learning_rate.LearningRate"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "get_instance", "(", ")", ":", "\n", "        ", "\"\"\"Get the singleton instance.\n\n        Returns:\n            (LearningRate)\n        \"\"\"", "\n", "if", "LearningRate", ".", "__instance", "is", "None", ":", "\n", "            ", "LearningRate", "(", ")", "\n", "", "return", "LearningRate", ".", "__instance", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learning_rate.LearningRate.set_learning_rate": [[30, 32], ["None"], "methods", ["None"], ["", "def", "set_learning_rate", "(", "self", ",", "lr", ")", ":", "\n", "        ", "self", ".", "lr", "=", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learning_rate.LearningRate.get_learning_rate": [[33, 35], ["None"], "methods", ["None"], ["", "def", "get_learning_rate", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learning_rate.LearningRate.increment_step": [[36, 38], ["None"], "methods", ["None"], ["", "def", "increment_step", "(", "self", ")", ":", "\n", "        ", "self", ".", "training_step", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learning_rate.LearningRate.get_step": [[39, 41], ["None"], "methods", ["None"], ["", "def", "get_step", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "training_step", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learning_rate.LearningRate.set_decay": [[42, 44], ["None"], "methods", ["None"], ["", "def", "set_decay", "(", "self", ",", "d", ")", ":", "\n", "        ", "self", ".", "decay_factor", "=", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learning_rate.LearningRate.decay": [[45, 47], ["None"], "methods", ["None"], ["", "def", "decay", "(", "self", ")", ":", "\n", "        ", "self", ".", "lr", "=", "self", ".", "lr", "*", "self", ".", "decay_factor", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Flatten.forward": [[12, 14], ["x.view", "x.size"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Policy.__init__": [[17, 42], ["torch.Module.__init__", "base", "tools.distributions.Categorical", "len", "tools.distributions.DiagGaussian", "len", "tools.distributions.Bernoulli"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "obs_shape", ",", "action_space", ",", "base", "=", "None", ",", "base_kwargs", "=", "None", ")", ":", "\n", "        ", "super", "(", "Policy", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "base_kwargs", "is", "None", ":", "\n", "            ", "base_kwargs", "=", "{", "}", "\n", "", "if", "base", "is", "None", ":", "\n", "            ", "if", "len", "(", "obs_shape", ")", "==", "3", ":", "\n", "                ", "base", "=", "CNNBase", "\n", "", "elif", "len", "(", "obs_shape", ")", "==", "1", ":", "\n", "                ", "base", "=", "MLPBase", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "\n", "", "", "self", ".", "base", "=", "base", "(", "obs_shape", "[", "0", "]", ",", "**", "base_kwargs", ")", "\n", "\n", "if", "action_space", ".", "__class__", ".", "__name__", "==", "\"Discrete\"", ":", "\n", "            ", "num_outputs", "=", "action_space", ".", "n", "\n", "self", ".", "dist", "=", "Categorical", "(", "self", ".", "base", ".", "output_size", ",", "num_outputs", ")", "\n", "", "elif", "action_space", ".", "__class__", ".", "__name__", "==", "\"Box\"", ":", "\n", "            ", "num_outputs", "=", "action_space", ".", "shape", "[", "0", "]", "\n", "self", ".", "dist", "=", "DiagGaussian", "(", "self", ".", "base", ".", "output_size", ",", "num_outputs", ")", "\n", "", "elif", "action_space", ".", "__class__", ".", "__name__", "==", "\"MultiBinary\"", ":", "\n", "            ", "num_outputs", "=", "action_space", ".", "shape", "[", "0", "]", "\n", "self", ".", "dist", "=", "Bernoulli", "(", "self", ".", "base", ".", "output_size", ",", "num_outputs", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Policy.is_recurrent": [[43, 46], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "is_recurrent", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "base", ".", "is_recurrent", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Policy.recurrent_hidden_state_size": [[47, 51], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "recurrent_hidden_state_size", "(", "self", ")", ":", "\n", "        ", "\"\"\"Size of rnn_hx.\"\"\"", "\n", "return", "self", ".", "base", ".", "recurrent_hidden_state_size", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Policy.forward": [[52, 54], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "rnn_hxs", ",", "masks", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Policy.act": [[55, 68], ["model.Policy.base", "model.Policy.dist", "model.Policy.log_probs", "model.Policy.entropy().mean", "model.Policy.mode", "model.Policy.sample", "model.Policy.entropy"], "methods", ["None"], ["", "def", "act", "(", "self", ",", "inputs", ",", "rnn_hxs", ",", "masks", ",", "deterministic", "=", "False", ")", ":", "\n", "        ", "value", ",", "actor_features", ",", "rnn_hxs", "=", "self", ".", "base", "(", "inputs", ",", "rnn_hxs", ",", "masks", ")", "\n", "dist", "=", "self", ".", "dist", "(", "actor_features", ")", "\n", "\n", "if", "deterministic", ":", "\n", "            ", "action", "=", "dist", ".", "mode", "(", ")", "\n", "", "else", ":", "\n", "            ", "action", "=", "dist", ".", "sample", "(", ")", "\n", "\n", "", "action_log_probs", "=", "dist", ".", "log_probs", "(", "action", ")", "\n", "dist_entropy", "=", "dist", ".", "entropy", "(", ")", ".", "mean", "(", ")", "\n", "\n", "return", "value", ",", "action", ",", "action_log_probs", ",", "rnn_hxs", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Policy.get_value": [[69, 72], ["model.Policy.base"], "methods", ["None"], ["", "def", "get_value", "(", "self", ",", "inputs", ",", "rnn_hxs", ",", "masks", ")", ":", "\n", "        ", "value", ",", "_", ",", "_", "=", "self", ".", "base", "(", "inputs", ",", "rnn_hxs", ",", "masks", ")", "\n", "return", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Policy.evaluate_actions": [[73, 81], ["model.Policy.base", "model.Policy.dist", "model.Policy.log_probs", "model.Policy.entropy().mean", "model.Policy.entropy"], "methods", ["None"], ["", "def", "evaluate_actions", "(", "self", ",", "inputs", ",", "rnn_hxs", ",", "masks", ",", "action", ")", ":", "\n", "        ", "value", ",", "actor_features", ",", "rnn_hxs", "=", "self", ".", "base", "(", "inputs", ",", "rnn_hxs", ",", "masks", ")", "\n", "dist", "=", "self", ".", "dist", "(", "actor_features", ")", "\n", "\n", "action_log_probs", "=", "dist", ".", "log_probs", "(", "action", ")", "\n", "dist_entropy", "=", "dist", ".", "entropy", "(", ")", ".", "mean", "(", ")", "\n", "\n", "return", "value", ",", "action_log_probs", ",", "dist_entropy", ",", "rnn_hxs", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.NNBase.__init__": [[84, 97], ["torch.Module.__init__", "torch.GRU", "torch.GRU", "torch.GRU", "model.NNBase.gru.named_parameters", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.orthogonal_", "torch.init.orthogonal_", "torch.init.orthogonal_"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "recurrent", ",", "recurrent_input_size", ",", "hidden_size", ")", ":", "\n", "        ", "super", "(", "NNBase", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "_hidden_size", "=", "hidden_size", "\n", "self", ".", "_recurrent", "=", "recurrent", "\n", "\n", "if", "recurrent", ":", "\n", "            ", "self", ".", "gru", "=", "nn", ".", "GRU", "(", "recurrent_input_size", ",", "hidden_size", ")", "\n", "for", "name", ",", "param", "in", "self", ".", "gru", ".", "named_parameters", "(", ")", ":", "\n", "                ", "if", "'bias'", "in", "name", ":", "\n", "                    ", "nn", ".", "init", ".", "constant_", "(", "param", ",", "0", ")", "\n", "", "elif", "'weight'", "in", "name", ":", "\n", "                    ", "nn", ".", "init", ".", "orthogonal_", "(", "param", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.NNBase.is_recurrent": [[98, 101], ["None"], "methods", ["None"], ["", "", "", "", "@", "property", "\n", "def", "is_recurrent", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_recurrent", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.NNBase.recurrent_hidden_state_size": [[102, 107], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "recurrent_hidden_state_size", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_recurrent", ":", "\n", "            ", "return", "self", ".", "_hidden_size", "\n", "", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.NNBase.output_size": [[108, 111], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "output_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.NNBase._forward_gru": [[112, 168], ["x.view.view.size", "hxs.squeeze.squeeze.size", "model.NNBase.gru", "x.view.view.squeeze", "hxs.squeeze.squeeze.squeeze", "hxs.squeeze.squeeze.size", "int", "x.view.view.view", "masks.view.view.view", "hxs.squeeze.squeeze.unsqueeze", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "x.view.view.view", "hxs.squeeze.squeeze.squeeze", "x.view.view.unsqueeze", "x.view.view.size", "has_zeros.dim", "model.NNBase.gru", "outputs.append", "x.view.view.size", "len", "has_zeros.item", "masks[].view"], "methods", ["None"], ["", "def", "_forward_gru", "(", "self", ",", "x", ",", "hxs", ",", "masks", ")", ":", "\n", "        ", "if", "x", ".", "size", "(", "0", ")", "==", "hxs", ".", "size", "(", "0", ")", ":", "\n", "            ", "x", ",", "hxs", "=", "self", ".", "gru", "(", "x", ".", "unsqueeze", "(", "0", ")", ",", "(", "hxs", "*", "masks", ")", ".", "unsqueeze", "(", "0", ")", ")", "\n", "x", "=", "x", ".", "squeeze", "(", "0", ")", "\n", "hxs", "=", "hxs", ".", "squeeze", "(", "0", ")", "\n", "", "else", ":", "\n", "# x is a (T, N, -1) tensor that has been flatten to (T * N, -1)", "\n", "            ", "N", "=", "hxs", ".", "size", "(", "0", ")", "\n", "T", "=", "int", "(", "x", ".", "size", "(", "0", ")", "/", "N", ")", "\n", "\n", "# unflatten", "\n", "x", "=", "x", ".", "view", "(", "T", ",", "N", ",", "x", ".", "size", "(", "1", ")", ")", "\n", "\n", "# Same deal with masks", "\n", "masks", "=", "masks", ".", "view", "(", "T", ",", "N", ")", "\n", "\n", "# Let's figure out which steps in the sequence have a zero for any agent", "\n", "# We will always assume t=0 has a zero in it as that makes the logic cleaner", "\n", "has_zeros", "=", "(", "(", "masks", "[", "1", ":", "]", "==", "0.0", ")", ".", "any", "(", "dim", "=", "-", "1", ")", "\n", ".", "nonzero", "(", ")", "\n", ".", "squeeze", "(", ")", "\n", ".", "cpu", "(", ")", ")", "\n", "\n", "# +1 to correct the masks[1:]", "\n", "if", "has_zeros", ".", "dim", "(", ")", "==", "0", ":", "\n", "# Deal with scalar", "\n", "                ", "has_zeros", "=", "[", "has_zeros", ".", "item", "(", ")", "+", "1", "]", "\n", "", "else", ":", "\n", "                ", "has_zeros", "=", "(", "has_zeros", "+", "1", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "# add t=0 and t=T to the list", "\n", "", "has_zeros", "=", "[", "0", "]", "+", "has_zeros", "+", "[", "T", "]", "\n", "\n", "hxs", "=", "hxs", ".", "unsqueeze", "(", "0", ")", "\n", "outputs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "has_zeros", ")", "-", "1", ")", ":", "\n", "# We can now process steps that don't have any zeros in masks together!", "\n", "# This is much faster", "\n", "                ", "start_idx", "=", "has_zeros", "[", "i", "]", "\n", "end_idx", "=", "has_zeros", "[", "i", "+", "1", "]", "\n", "\n", "rnn_scores", ",", "hxs", "=", "self", ".", "gru", "(", "\n", "x", "[", "start_idx", ":", "end_idx", "]", ",", "\n", "hxs", "*", "masks", "[", "start_idx", "]", ".", "view", "(", "1", ",", "-", "1", ",", "1", ")", ")", "\n", "\n", "outputs", ".", "append", "(", "rnn_scores", ")", "\n", "\n", "# assert len(outputs) == T", "\n", "# x is a (T, N, -1) tensor", "\n", "", "x", "=", "torch", ".", "cat", "(", "outputs", ",", "dim", "=", "0", ")", "\n", "# flatten", "\n", "x", "=", "x", ".", "view", "(", "T", "*", "N", ",", "-", "1", ")", "\n", "hxs", "=", "hxs", ".", "squeeze", "(", "0", ")", "\n", "\n", "", "return", "x", ",", "hxs", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.CNNBase.__init__": [[171, 189], ["model.NNBase.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "init_", "model.CNNBase.train", "tools.utils.init", "init_", "torch.ReLU", "torch.ReLU", "torch.ReLU", "init_", "torch.ReLU", "torch.ReLU", "torch.ReLU", "init_", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.Flatten", "init_", "torch.ReLU", "torch.ReLU", "torch.ReLU", "tools.utils.init", "torch.Linear", "torch.Linear", "torch.Linear", "torch.init.calculate_gain", "torch.init.calculate_gain", "torch.init.calculate_gain", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Linear", "torch.Linear", "torch.Linear", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.train", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.init", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.init"], ["    ", "def", "__init__", "(", "self", ",", "num_inputs", ",", "recurrent", "=", "False", ",", "hidden_size", "=", "512", ")", ":", "\n", "        ", "super", "(", "CNNBase", ",", "self", ")", ".", "__init__", "(", "recurrent", ",", "hidden_size", ",", "hidden_size", ")", "\n", "\n", "init_", "=", "lambda", "m", ":", "init", "(", "m", ",", "nn", ".", "init", ".", "orthogonal_", ",", "lambda", "x", ":", "nn", ".", "init", ".", "\n", "constant_", "(", "x", ",", "0", ")", ",", "nn", ".", "init", ".", "calculate_gain", "(", "'relu'", ")", ")", "\n", "\n", "self", ".", "main", "=", "nn", ".", "Sequential", "(", "\n", "init_", "(", "nn", ".", "Conv2d", "(", "num_inputs", ",", "32", ",", "8", ",", "stride", "=", "4", ")", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "\n", "init_", "(", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "4", ",", "stride", "=", "2", ")", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "\n", "init_", "(", "nn", ".", "Conv2d", "(", "64", ",", "32", ",", "3", ",", "stride", "=", "1", ")", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "Flatten", "(", ")", ",", "\n", "init_", "(", "nn", ".", "Linear", "(", "32", "*", "7", "*", "7", ",", "hidden_size", ")", ")", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "\n", "init_", "=", "lambda", "m", ":", "init", "(", "m", ",", "nn", ".", "init", ".", "orthogonal_", ",", "lambda", "x", ":", "nn", ".", "init", ".", "\n", "constant_", "(", "x", ",", "0", ")", ")", "\n", "\n", "self", ".", "critic_linear", "=", "init_", "(", "nn", ".", "Linear", "(", "hidden_size", ",", "1", ")", ")", "\n", "\n", "self", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.CNNBase.forward": [[190, 197], ["model.CNNBase.main", "model.CNNBase._forward_gru", "model.CNNBase.critic_linear"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.main", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.NNBase._forward_gru"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "rnn_hxs", ",", "masks", ")", ":", "\n", "        ", "x", "=", "self", ".", "main", "(", "inputs", "/", "255.0", ")", "\n", "\n", "if", "self", ".", "is_recurrent", ":", "\n", "            ", "x", ",", "rnn_hxs", "=", "self", ".", "_forward_gru", "(", "x", ",", "rnn_hxs", ",", "masks", ")", "\n", "\n", "", "return", "self", ".", "critic_linear", "(", "x", ")", ",", "x", ",", "rnn_hxs", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.MLPBase.__init__": [[200, 220], ["model.NNBase.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "init_", "model.MLPBase.train", "tools.utils.init", "init_", "torch.Tanh", "torch.Tanh", "torch.Tanh", "init_", "torch.Tanh", "torch.Tanh", "torch.Tanh", "init_", "torch.Tanh", "torch.Tanh", "torch.Tanh", "init_", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Linear", "torch.Linear", "torch.Linear", "numpy.sqrt", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.train", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.init"], ["    ", "def", "__init__", "(", "self", ",", "num_inputs", ",", "recurrent", "=", "False", ",", "hidden_size", "=", "64", ")", ":", "\n", "        ", "super", "(", "MLPBase", ",", "self", ")", ".", "__init__", "(", "recurrent", ",", "num_inputs", ",", "hidden_size", ")", "\n", "\n", "if", "recurrent", ":", "\n", "            ", "num_inputs", "=", "hidden_size", "\n", "\n", "", "init_", "=", "lambda", "m", ":", "init", "(", "m", ",", "nn", ".", "init", ".", "orthogonal_", ",", "lambda", "x", ":", "nn", ".", "init", ".", "\n", "constant_", "(", "x", ",", "0", ")", ",", "np", ".", "sqrt", "(", "2", ")", ")", "\n", "\n", "self", ".", "actor", "=", "nn", ".", "Sequential", "(", "\n", "init_", "(", "nn", ".", "Linear", "(", "num_inputs", ",", "hidden_size", ")", ")", ",", "nn", ".", "Tanh", "(", ")", ",", "\n", "init_", "(", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", ")", ",", "nn", ".", "Tanh", "(", ")", ")", "\n", "\n", "self", ".", "critic", "=", "nn", ".", "Sequential", "(", "\n", "init_", "(", "nn", ".", "Linear", "(", "num_inputs", ",", "hidden_size", ")", ")", ",", "nn", ".", "Tanh", "(", ")", ",", "\n", "init_", "(", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", ")", ",", "nn", ".", "Tanh", "(", ")", ")", "\n", "\n", "self", ".", "critic_linear", "=", "init_", "(", "nn", ".", "Linear", "(", "hidden_size", ",", "1", ")", ")", "\n", "\n", "self", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.MLPBase.forward": [[221, 231], ["model.MLPBase.critic", "model.MLPBase.actor", "model.MLPBase._forward_gru", "model.MLPBase.critic_linear"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.NNBase._forward_gru"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "rnn_hxs", ",", "masks", ")", ":", "\n", "        ", "x", "=", "inputs", "\n", "\n", "if", "self", ".", "is_recurrent", ":", "\n", "            ", "x", ",", "rnn_hxs", "=", "self", ".", "_forward_gru", "(", "x", ",", "rnn_hxs", ",", "masks", ")", "\n", "\n", "", "hidden_critic", "=", "self", ".", "critic", "(", "x", ")", "\n", "hidden_actor", "=", "self", ".", "actor", "(", "x", ")", "\n", "\n", "return", "self", ".", "critic_linear", "(", "hidden_critic", ")", ",", "hidden_actor", ",", "rnn_hxs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.distributions.Categorical.__init__": [[55, 65], ["torch.Module.__init__", "init_", "tools.utils.init", "torch.Linear", "torch.Linear", "torch.Linear", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.init"], ["    ", "def", "__init__", "(", "self", ",", "num_inputs", ",", "num_outputs", ")", ":", "\n", "        ", "super", "(", "Categorical", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "init_", "=", "lambda", "m", ":", "init", "(", "\n", "m", ",", "\n", "nn", ".", "init", ".", "orthogonal_", ",", "\n", "lambda", "x", ":", "nn", ".", "init", ".", "constant_", "(", "x", ",", "0", ")", ",", "\n", "gain", "=", "0.01", ")", "\n", "\n", "self", ".", "linear", "=", "init_", "(", "nn", ".", "Linear", "(", "num_inputs", ",", "num_outputs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.distributions.Categorical.forward": [[66, 69], ["distributions.Categorical.linear", "FixedCategorical"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "linear", "(", "x", ")", "\n", "return", "FixedCategorical", "(", "logits", "=", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.distributions.DiagGaussian.__init__": [[72, 80], ["torch.Module.__init__", "init_", "tools.utils.AddBias", "tools.utils.init", "torch.Linear", "torch.Linear", "torch.Linear", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.init"], ["    ", "def", "__init__", "(", "self", ",", "num_inputs", ",", "num_outputs", ")", ":", "\n", "        ", "super", "(", "DiagGaussian", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "init_", "=", "lambda", "m", ":", "init", "(", "m", ",", "nn", ".", "init", ".", "orthogonal_", ",", "lambda", "x", ":", "nn", ".", "init", ".", "\n", "constant_", "(", "x", ",", "0", ")", ")", "\n", "\n", "self", ".", "fc_mean", "=", "init_", "(", "nn", ".", "Linear", "(", "num_inputs", ",", "num_outputs", ")", ")", "\n", "self", ".", "logstd", "=", "AddBias", "(", "torch", ".", "zeros", "(", "num_outputs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.distributions.DiagGaussian.forward": [[81, 91], ["distributions.DiagGaussian.fc_mean", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "distributions.DiagGaussian.logstd", "FixedNormal", "distributions.DiagGaussian.size", "zeros.cuda.cuda.cuda", "distributions.DiagGaussian.exp"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "action_mean", "=", "self", ".", "fc_mean", "(", "x", ")", "\n", "\n", "#  An ugly hack for my KFAC implementation.", "\n", "zeros", "=", "torch", ".", "zeros", "(", "action_mean", ".", "size", "(", ")", ")", "\n", "if", "x", ".", "is_cuda", ":", "\n", "            ", "zeros", "=", "zeros", ".", "cuda", "(", ")", "\n", "\n", "", "action_logstd", "=", "self", ".", "logstd", "(", "zeros", ")", "\n", "return", "FixedNormal", "(", "action_mean", ",", "action_logstd", ".", "exp", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.distributions.Bernoulli.__init__": [[94, 101], ["torch.Module.__init__", "init_", "tools.utils.init", "torch.Linear", "torch.Linear", "torch.Linear", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.init"], ["    ", "def", "__init__", "(", "self", ",", "num_inputs", ",", "num_outputs", ")", ":", "\n", "        ", "super", "(", "Bernoulli", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "init_", "=", "lambda", "m", ":", "init", "(", "m", ",", "nn", ".", "init", ".", "orthogonal_", ",", "lambda", "x", ":", "nn", ".", "init", ".", "\n", "constant_", "(", "x", ",", "0", ")", ")", "\n", "\n", "self", ".", "linear", "=", "init_", "(", "nn", ".", "Linear", "(", "num_inputs", ",", "num_outputs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.distributions.Bernoulli.forward": [[102, 105], ["distributions.Bernoulli.linear", "FixedBernoulli"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "linear", "(", "x", ")", "\n", "return", "FixedBernoulli", "(", "logits", "=", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Write_Result.__init__": [[214, 223], ["utli.Log_save_name4gail", "utli.Write_Result.initTrain", "utli.Write_Result.initEval"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Log_save_name4gail", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Write_Result.initTrain", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Write_Result.initEval"], ["    ", "def", "__init__", "(", "self", ",", "cl_args", ")", ":", "\n", "        ", "save_name", "=", "Log_save_name4gail", "(", "cl_args", ")", "\n", "self", ".", "train_name", "=", "'Train_'", "+", "save_name", "+", "'.txt'", "\n", "self", ".", "eval_name", "=", "'Eval_'", "+", "save_name", "+", "'.txt'", "\n", "self", ".", "path", "=", "config", ".", "results_dir", "\n", "self", ".", "results4train", "=", "results4train_gail", "\n", "self", ".", "results4evaluate", "=", "results4evaluate", "\n", "self", ".", "initTrain", "(", ")", "\n", "self", ".", "initEval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Write_Result.initTrain": [[225, 232], ["utli.Write_Result.results4train.keys", "open", "Record_f.write", "Record_f.write", "os.path.join", "Record_f.write"], "methods", ["None"], ["", "def", "initTrain", "(", "self", ")", ":", "\n", "        ", "names", "=", "self", ".", "results4train", ".", "keys", "(", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "path", ",", "self", ".", "train_name", ")", ",", "'w'", ")", "as", "Record_f", ":", "\n", "            ", "Record_f", ".", "write", "(", "'Time_step'", "+", "'\\t'", ")", "\n", "for", "name", "in", "names", ":", "\n", "                ", "Record_f", ".", "write", "(", "name", "+", "'\\t'", ")", "\n", "", "Record_f", ".", "write", "(", "'Other'", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Write_Result.initEval": [[234, 241], ["utli.Write_Result.results4evaluate.keys", "open", "Record_f.write", "Record_f.write", "os.path.join", "Record_f.write"], "methods", ["None"], ["", "", "def", "initEval", "(", "self", ")", ":", "\n", "        ", "names", "=", "self", ".", "results4evaluate", ".", "keys", "(", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "path", ",", "self", ".", "eval_name", ")", ",", "'w'", ")", "as", "Record_f", ":", "\n", "            ", "Record_f", ".", "write", "(", "'Time_step'", "+", "'\\t'", ")", "\n", "for", "name", "in", "names", ":", "\n", "                ", "Record_f", ".", "write", "(", "name", "+", "'\\t'", ")", "\n", "", "Record_f", ".", "write", "(", "'Other'", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Write_Result.step_train": [[242, 250], ["results.keys", "open", "Record_f.write", "Record_f.write", "os.path.join", "Record_f.write", "str", "str", "str", "numpy.around"], "methods", ["None"], ["", "", "def", "step_train", "(", "self", ",", "time_step", ")", ":", "\n", "        ", "results", "=", "self", ".", "results4train", "\n", "names", "=", "results", ".", "keys", "(", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "path", ",", "self", ".", "train_name", ")", ",", "'a'", ")", "as", "Record_f", ":", "\n", "            ", "Record_f", ".", "write", "(", "str", "(", "time_step", ")", "+", "'\\t'", ")", "\n", "for", "name", "in", "names", ":", "\n", "                ", "Record_f", ".", "write", "(", "str", "(", "np", ".", "around", "(", "results", "[", "name", "]", ",", "decimals", "=", "4", ")", ")", "+", "'\\t'", ")", "\n", "", "Record_f", ".", "write", "(", "str", "(", "0", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Write_Result.step_eval": [[251, 259], ["results.keys", "open", "Record_f.write", "Record_f.write", "os.path.join", "Record_f.write", "str", "str", "str"], "methods", ["None"], ["", "", "def", "step_eval", "(", "self", ",", "time_step", ")", ":", "\n", "        ", "results", "=", "self", ".", "results4evaluate", "\n", "names", "=", "results", ".", "keys", "(", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "path", ",", "self", ".", "train_name", ")", ",", "'a'", ")", "as", "Record_f", ":", "\n", "            ", "Record_f", ".", "write", "(", "str", "(", "time_step", ")", "+", "'\\t'", ")", "\n", "for", "name", "in", "names", ":", "\n", "                ", "Record_f", ".", "write", "(", "str", "(", "results", "[", "name", "]", ")", "+", "'\\t'", ")", "\n", "", "Record_f", ".", "write", "(", "str", "(", "0", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordTrainResults_test": [[26, 34], ["utli.write2tensorboard"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.write2tensorboard"], ["def", "recordTrainResults_test", "(", "results", ",", "time_step", ")", ":", "\n", "    ", "results4train_test", "[", "'Train reward'", "]", "=", "results", "[", "0", "]", "\n", "results4train_test", "[", "'Train steps'", "]", "=", "results", "[", "1", "]", "\n", "results4train_test", "[", "'min reward in episodes'", "]", "=", "results", "[", "2", "]", "\n", "results4train_test", "[", "'max reward in episodes'", "]", "=", "results", "[", "3", "]", "\n", "results4train_test", "[", "'mean reward in episodes'", "]", "=", "results", "[", "4", "]", "\n", "\n", "write2tensorboard", "(", "results", "=", "results4train_test", ",", "time_step", "=", "time_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordLossResults": [[53, 60], ["utli.write2tensorboard"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.write2tensorboard"], ["def", "recordLossResults", "(", "results", ",", "time_step", ")", ":", "\n", "    ", "results4loss", "[", "'ppo_value'", "]", "=", "results", "[", "0", "]", "\n", "results4loss", "[", "'ppo_loss'", "]", "=", "results", "[", "1", "]", "\n", "results4loss", "[", "'ppo_entropy'", "]", "=", "results", "[", "2", "]", "\n", "results4loss", "[", "'ppo_total_loss'", "]", "=", "results", "[", "3", "]", "\n", "\n", "write2tensorboard", "(", "results", "=", "results4loss", ",", "time_step", "=", "time_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordTD3LossResults": [[67, 73], ["utli.write2tensorboard"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.write2tensorboard"], ["def", "recordTD3LossResults", "(", "results", ",", "time_step", ")", ":", "\n", "    ", "results4td3loss", "[", "'learning rate decay'", "]", "=", "results", "[", "0", "]", "\n", "results4td3loss", "[", "'critic_loss'", "]", "=", "results", "[", "1", "]", "\n", "results4td3loss", "[", "'actor_loss'", "]", "=", "results", "[", "2", "]", "\n", "\n", "write2tensorboard", "(", "results", "=", "results4td3loss", ",", "time_step", "=", "time_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordDisLossResults": [[80, 87], ["utli.write2tensorboard"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.write2tensorboard"], ["def", "recordDisLossResults", "(", "results", ",", "time_step", ")", ":", "\n", "    ", "results4Disloss", "[", "'dis_loss'", "]", "=", "results", "[", "0", "]", "\n", "results4Disloss", "[", "'dis_gp'", "]", "=", "results", "[", "1", "]", "\n", "results4Disloss", "[", "'dis_entropy'", "]", "=", "results", "[", "2", "]", "\n", "results4Disloss", "[", "'dis_total_loss'", "]", "=", "results", "[", "3", "]", "\n", "\n", "write2tensorboard", "(", "results", "=", "results4Disloss", ",", "time_step", "=", "time_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordTrainResults": [[88, 94], ["utli.write2tensorboard"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.write2tensorboard"], ["", "def", "recordTrainResults", "(", "results", ",", "time_step", ")", ":", "\n", "\n", "    ", "results4train", "[", "'Train reward'", "]", "=", "results", "[", "0", "]", "\n", "results4train", "[", "'Train steps'", "]", "=", "results", "[", "1", "]", "\n", "\n", "write2tensorboard", "(", "results", "=", "results4train", ",", "time_step", "=", "time_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordTrainResults_gail": [[101, 107], ["utli.write2tensorboard"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.write2tensorboard"], ["def", "recordTrainResults_gail", "(", "results", ",", "time_step", ")", ":", "\n", "    ", "results4train_gail", "[", "'Train reward'", "]", "=", "results", "[", "0", "]", "\n", "results4train_gail", "[", "'Train steps'", "]", "=", "results", "[", "1", "]", "\n", "results4train_gail", "[", "'Expert reward'", "]", "=", "results", "[", "2", "]", "\n", "\n", "write2tensorboard", "(", "results", "=", "results4train_gail", ",", "time_step", "=", "time_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.recordEvaluateResults": [[110, 116], ["utli.write2tensorboard"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.write2tensorboard"], ["", "def", "recordEvaluateResults", "(", "results", ",", "time_step", ")", ":", "\n", "    ", "results4evaluate", "[", "'Evaluate mean reward'", "]", "=", "results", "[", "0", "]", "\n", "results4evaluate", "[", "'Evaluate mean steps'", "]", "=", "results", "[", "1", "]", "\n", "results4evaluate", "[", "'Window Evaluate reward'", "]", "=", "results", "[", "2", "]", "\n", "\n", "write2tensorboard", "(", "results", "=", "results4evaluate", ",", "time_step", "=", "time_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.write2tensorboard": [[117, 121], ["results.keys", "writer.add_scalar"], "function", ["None"], ["", "def", "write2tensorboard", "(", "results", ",", "time_step", ")", ":", "\n", "    ", "titles", "=", "results", ".", "keys", "(", ")", "\n", "for", "title", "in", "titles", ":", "\n", "        ", "writer", ".", "add_scalar", "(", "title", ",", "results", "[", "title", "]", ",", "time_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.store_results": [[122, 147], ["pandas.DataFrame.from_records", "columns.append", "utli.Log_save_name", "numpy.around", "pd.DataFrame.from_records.to_csv", "len", "str", "range"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Log_save_name"], ["", "", "def", "store_results", "(", "evaluations", ",", "number_of_timesteps", ",", "cl_args", ",", "S_time", "=", "0", ",", "E_time", "=", "3600", ")", ":", "\n", "    ", "\"\"\"Store the results of a run.\n\n    Args:\n        evaluations:\n        number_of_timesteps (int):\n        loss_aggregate (str): The name of the loss aggregation used. (sum or mean)\n        loss_function (str): The name of the loss function used.\n\n    Returns:\n        None\n    \"\"\"", "\n", "\n", "df", "=", "pd", ".", "DataFrame", ".", "from_records", "(", "evaluations", ")", "\n", "number_of_trajectories", "=", "len", "(", "evaluations", "[", "0", "]", ")", "-", "1", "\n", "columns", "=", "[", "\"reward_{}\"", ".", "format", "(", "i", ")", "for", "i", "in", "range", "(", "number_of_trajectories", ")", "]", "\n", "columns", ".", "append", "(", "\"timestep\"", ")", "\n", "df", ".", "columns", "=", "columns", "\n", "log_save_name", "=", "Log_save_name", "(", "cl_args", ")", "\n", "# timestamp = time.time()", "\n", "timestamp", "=", "np", ".", "around", "(", "(", "E_time", "-", "S_time", ")", "/", "3600", ",", "2", ")", "\n", "results_fname", "=", "'FinalEvaluation_'", "+", "log_save_name", "+", "'_tsteps_{}_consume_time_{}_results.csv'", ".", "format", "(", "number_of_timesteps", ",", "\n", "timestamp", ")", "\n", "df", ".", "to_csv", "(", "str", "(", "config", ".", "results_dir", "/", "results_fname", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Save_model_dir": [[149, 158], ["dir_abs.is_dir", "dir_abs.mkdir"], "function", ["None"], ["", "def", "Save_model_dir", "(", "algo_id", ",", "env_id", ")", ":", "\n", "    ", "fname", "=", "'{}_{}'", ".", "format", "(", "algo_id", ",", "env_id", ")", "\n", "# dir_rela = os.path.join(config.trained_model_dir_rela,fname)", "\n", "dir_abs", "=", "config", ".", "trained_model_dir", "/", "fname", "\n", "\n", "if", "not", "dir_abs", ".", "is_dir", "(", ")", ":", "\n", "        ", "dir_abs", ".", "mkdir", "(", ")", "\n", "\n", "", "return", "dir_abs", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Save_trained_model": [[159, 176], ["os.path.join", "torch.save", "print", "print", "print", "print", "model.state_dict", "numpy.mean"], "function", ["None"], ["", "def", "Save_trained_model", "(", "count", ",", "num", ",", "model", ",", "model_dir", ",", "save_condition", ",", "eprewmean", ",", "reward_window4Evaluate", ")", ":", "\n", "\n", "# if eprewmean >= save_condition:", "\n", "    ", "if", "eprewmean", ">=", "save_condition", "and", "reward_window4Evaluate", "[", "-", "1", "]", ">=", "save_condition", ":", "\n", "        ", "count", "+=", "1", "\n", "model_fname", "=", "'Trained_model_{}.pt'", ".", "format", "(", "count", ")", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "model_fname", ")", "\n", "# torch.save(model.state_dict(), path)", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "path", ")", "\n", "print", "(", "'********************************************************'", ")", "\n", "print", "(", "'Save model Train mean reward {:.2f}'", ".", "format", "(", "eprewmean", ")", ")", "\n", "print", "(", "'Save model Evaluate mean reward {:.2f}'", ".", "format", "(", "np", ".", "mean", "(", "reward_window4Evaluate", ")", ")", ")", "\n", "print", "(", "'********************************************************'", ")", "\n", "if", "count", ">=", "num", ":", "\n", "            ", "count", "=", "0", "\n", "\n", "", "", "return", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Log_save_name": [[179, 191], ["int"], "function", ["None"], ["", "def", "Log_save_name", "(", "cl_args", ")", ":", "\n", "\n", "    ", "save_name", "=", "cl_args", ".", "algo_id", "+", "'_'", "+", "cl_args", ".", "env_name", "+", "'_hidden_{}_lr_{}_nstep_{}_batch_size_{}_ppo_epoch_{}_total_steps_{}'", ".", "format", "(", "cl_args", ".", "hidden_size", ",", "\n", "cl_args", ".", "lr", ",", "\n", "cl_args", ".", "nsteps", ",", "\n", "cl_args", ".", "batch_size", ",", "\n", "cl_args", ".", "ppo_epoch", ",", "\n", "int", "(", "cl_args", ".", "total_steps", ")", "\n", ")", "\n", "return", "save_name", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Log_save_name4gail": [[192, 207], ["int"], "function", ["None"], ["", "def", "Log_save_name4gail", "(", "cl_args", ")", ":", "\n", "\n", "    ", "save_name", "=", "cl_args", ".", "algo", "+", "'_'", "+", "cl_args", ".", "env_name", "+", "'_seed_{}_num_trajs_{}_subsample_frequency_{}_reward_type_{}_update_rms_{}_gail_{}_{}_{}_{}'", ".", "format", "(", "cl_args", ".", "seed", ",", "\n", "cl_args", ".", "num_trajs", ",", "\n", "cl_args", ".", "subsample_frequency", ",", "\n", "cl_args", ".", "reward_type", ",", "\n", "int", "(", "cl_args", ".", "update_rms", ")", ",", "\n", "cl_args", ".", "gail_batch_size", ",", "\n", "cl_args", ".", "gail_thre", ",", "\n", "cl_args", ".", "gail_pre_epoch", ",", "\n", "cl_args", ".", "gail_epoch", "\n", ")", "\n", "return", "save_name", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.get_train_dir": [[208, 212], ["utli.Log_save_name"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.Log_save_name"], ["", "def", "get_train_dir", "(", "cl_args", ")", ":", "\n", "    ", "save_name", "=", "Log_save_name", "(", "cl_args", ")", "\n", "train_dir", "=", "config", ".", "results_dir", "/", "save_name", "\n", "return", "train_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utli.update_linear_schedule": [[260, 265], ["float"], "function", ["None"], ["", "", "", "def", "update_linear_schedule", "(", "optimizer", ",", "epoch", ",", "total_num_epochs", ",", "initial_lr", ")", ":", "\n", "    ", "\"\"\"Decreases the learning rate linearly\"\"\"", "\n", "lr", "=", "initial_lr", "-", "(", "initial_lr", "*", "(", "epoch", "/", "float", "(", "total_num_epochs", ")", ")", ")", "\n", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "        ", "param_group", "[", "'lr'", "]", "=", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.__init__": [[10, 34], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones", "torch.ones", "storage.RolloutStorage.actions.long"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "num_steps", ",", "num_processes", ",", "obs_shape", ",", "action_space", ",", "\n", "recurrent_hidden_state_size", ")", ":", "\n", "        ", "self", ".", "obs", "=", "torch", ".", "zeros", "(", "num_steps", "+", "1", ",", "num_processes", ",", "*", "obs_shape", ")", "\n", "self", ".", "recurrent_hidden_states", "=", "torch", ".", "zeros", "(", "\n", "num_steps", "+", "1", ",", "num_processes", ",", "recurrent_hidden_state_size", ")", "\n", "self", ".", "rewards", "=", "torch", ".", "zeros", "(", "num_steps", ",", "num_processes", ",", "1", ")", "\n", "self", ".", "value_preds", "=", "torch", ".", "zeros", "(", "num_steps", "+", "1", ",", "num_processes", ",", "1", ")", "\n", "self", ".", "returns", "=", "torch", ".", "zeros", "(", "num_steps", "+", "1", ",", "num_processes", ",", "1", ")", "\n", "self", ".", "action_log_probs", "=", "torch", ".", "zeros", "(", "num_steps", ",", "num_processes", ",", "1", ")", "\n", "if", "action_space", ".", "__class__", ".", "__name__", "==", "'Discrete'", ":", "\n", "            ", "action_shape", "=", "1", "\n", "", "else", ":", "\n", "            ", "action_shape", "=", "action_space", ".", "shape", "[", "0", "]", "\n", "", "self", ".", "actions", "=", "torch", ".", "zeros", "(", "num_steps", ",", "num_processes", ",", "action_shape", ")", "\n", "if", "action_space", ".", "__class__", ".", "__name__", "==", "'Discrete'", ":", "\n", "            ", "self", ".", "actions", "=", "self", ".", "actions", ".", "long", "(", ")", "\n", "", "self", ".", "masks", "=", "torch", ".", "ones", "(", "num_steps", "+", "1", ",", "num_processes", ",", "1", ")", "\n", "\n", "# Masks that indicate whether it's a true terminal state", "\n", "# or time limit end state", "\n", "self", ".", "bad_masks", "=", "torch", ".", "ones", "(", "num_steps", "+", "1", ",", "num_processes", ",", "1", ")", "\n", "\n", "self", ".", "num_steps", "=", "num_steps", "\n", "self", ".", "step", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to": [[35, 45], ["storage.RolloutStorage.obs.to", "storage.RolloutStorage.recurrent_hidden_states.to", "storage.RolloutStorage.rewards.to", "storage.RolloutStorage.value_preds.to", "storage.RolloutStorage.returns.to", "storage.RolloutStorage.action_log_probs.to", "storage.RolloutStorage.actions.to", "storage.RolloutStorage.masks.to", "storage.RolloutStorage.bad_masks.to"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "        ", "self", ".", "obs", "=", "self", ".", "obs", ".", "to", "(", "device", ")", "\n", "self", ".", "recurrent_hidden_states", "=", "self", ".", "recurrent_hidden_states", ".", "to", "(", "device", ")", "\n", "self", ".", "rewards", "=", "self", ".", "rewards", ".", "to", "(", "device", ")", "\n", "self", ".", "value_preds", "=", "self", ".", "value_preds", ".", "to", "(", "device", ")", "\n", "self", ".", "returns", "=", "self", ".", "returns", ".", "to", "(", "device", ")", "\n", "self", ".", "action_log_probs", "=", "self", ".", "action_log_probs", ".", "to", "(", "device", ")", "\n", "self", ".", "actions", "=", "self", ".", "actions", ".", "to", "(", "device", ")", "\n", "self", ".", "masks", "=", "self", ".", "masks", ".", "to", "(", "device", ")", "\n", "self", ".", "bad_masks", "=", "self", ".", "bad_masks", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.insert": [[46, 59], ["storage.RolloutStorage.obs[].copy_", "storage.RolloutStorage.recurrent_hidden_states[].copy_", "storage.RolloutStorage.actions[].copy_", "storage.RolloutStorage.action_log_probs[].copy_", "storage.RolloutStorage.value_preds[].copy_", "storage.RolloutStorage.rewards[].copy_", "storage.RolloutStorage.masks[].copy_", "storage.RolloutStorage.bad_masks[].copy_"], "methods", ["None"], ["", "def", "insert", "(", "self", ",", "obs", ",", "recurrent_hidden_states", ",", "actions", ",", "action_log_probs", ",", "\n", "value_preds", ",", "rewards", ",", "masks", ",", "bad_masks", ")", ":", "\n", "        ", "self", ".", "obs", "[", "self", ".", "step", "+", "1", "]", ".", "copy_", "(", "obs", ")", "\n", "self", ".", "recurrent_hidden_states", "[", "self", ".", "step", "+", "\n", "1", "]", ".", "copy_", "(", "recurrent_hidden_states", ")", "\n", "self", ".", "actions", "[", "self", ".", "step", "]", ".", "copy_", "(", "actions", ")", "\n", "self", ".", "action_log_probs", "[", "self", ".", "step", "]", ".", "copy_", "(", "action_log_probs", ")", "\n", "self", ".", "value_preds", "[", "self", ".", "step", "]", ".", "copy_", "(", "value_preds", ")", "\n", "self", ".", "rewards", "[", "self", ".", "step", "]", ".", "copy_", "(", "rewards", ")", "\n", "self", ".", "masks", "[", "self", ".", "step", "+", "1", "]", ".", "copy_", "(", "masks", ")", "\n", "self", ".", "bad_masks", "[", "self", ".", "step", "+", "1", "]", ".", "copy_", "(", "bad_masks", ")", "\n", "\n", "self", ".", "step", "=", "(", "self", ".", "step", "+", "1", ")", "%", "self", ".", "num_steps", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.after_update": [[60, 65], ["storage.RolloutStorage.obs[].copy_", "storage.RolloutStorage.recurrent_hidden_states[].copy_", "storage.RolloutStorage.masks[].copy_", "storage.RolloutStorage.bad_masks[].copy_"], "methods", ["None"], ["", "def", "after_update", "(", "self", ")", ":", "\n", "        ", "self", ".", "obs", "[", "0", "]", ".", "copy_", "(", "self", ".", "obs", "[", "-", "1", "]", ")", "\n", "self", ".", "recurrent_hidden_states", "[", "0", "]", ".", "copy_", "(", "self", ".", "recurrent_hidden_states", "[", "-", "1", "]", ")", "\n", "self", ".", "masks", "[", "0", "]", ".", "copy_", "(", "self", ".", "masks", "[", "-", "1", "]", ")", "\n", "self", ".", "bad_masks", "[", "0", "]", ".", "copy_", "(", "self", ".", "bad_masks", "[", "-", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.compute_returns": [[66, 106], ["reversed", "reversed", "reversed", "reversed", "range", "range", "range", "range", "storage.RolloutStorage.rewards.size", "storage.RolloutStorage.rewards.size", "storage.RolloutStorage.rewards.size", "storage.RolloutStorage.rewards.size"], "methods", ["None"], ["", "def", "compute_returns", "(", "self", ",", "\n", "next_value", ",", "\n", "use_gae", ",", "\n", "gamma", ",", "\n", "gae_lambda", ",", "\n", "use_proper_time_limits", "=", "True", ")", ":", "\n", "        ", "if", "use_proper_time_limits", ":", "\n", "            ", "if", "use_gae", ":", "\n", "                ", "self", ".", "value_preds", "[", "-", "1", "]", "=", "next_value", "\n", "gae", "=", "0", "\n", "for", "step", "in", "reversed", "(", "range", "(", "self", ".", "rewards", ".", "size", "(", "0", ")", ")", ")", ":", "\n", "                    ", "delta", "=", "self", ".", "rewards", "[", "step", "]", "+", "gamma", "*", "self", ".", "value_preds", "[", "\n", "step", "+", "1", "]", "*", "self", ".", "masks", "[", "step", "+", "\n", "1", "]", "-", "self", ".", "value_preds", "[", "step", "]", "\n", "gae", "=", "delta", "+", "gamma", "*", "gae_lambda", "*", "self", ".", "masks", "[", "step", "+", "\n", "1", "]", "*", "gae", "\n", "gae", "=", "gae", "*", "self", ".", "bad_masks", "[", "step", "+", "1", "]", "\n", "self", ".", "returns", "[", "step", "]", "=", "gae", "+", "self", ".", "value_preds", "[", "step", "]", "\n", "", "", "else", ":", "\n", "                ", "self", ".", "returns", "[", "-", "1", "]", "=", "next_value", "\n", "for", "step", "in", "reversed", "(", "range", "(", "self", ".", "rewards", ".", "size", "(", "0", ")", ")", ")", ":", "\n", "                    ", "self", ".", "returns", "[", "step", "]", "=", "(", "self", ".", "returns", "[", "step", "+", "1", "]", "*", "gamma", "*", "self", ".", "masks", "[", "step", "+", "1", "]", "+", "self", ".", "rewards", "[", "step", "]", ")", "*", "self", ".", "bad_masks", "[", "step", "+", "1", "]", "+", "(", "1", "-", "self", ".", "bad_masks", "[", "step", "+", "1", "]", ")", "*", "self", ".", "value_preds", "[", "step", "]", "\n", "", "", "", "else", ":", "\n", "            ", "if", "use_gae", ":", "\n", "                ", "self", ".", "value_preds", "[", "-", "1", "]", "=", "next_value", "\n", "gae", "=", "0", "\n", "for", "step", "in", "reversed", "(", "range", "(", "self", ".", "rewards", ".", "size", "(", "0", ")", ")", ")", ":", "\n", "                    ", "delta", "=", "self", ".", "rewards", "[", "step", "]", "+", "gamma", "*", "self", ".", "value_preds", "[", "\n", "step", "+", "1", "]", "*", "self", ".", "masks", "[", "step", "+", "\n", "1", "]", "-", "self", ".", "value_preds", "[", "step", "]", "\n", "gae", "=", "delta", "+", "gamma", "*", "gae_lambda", "*", "self", ".", "masks", "[", "step", "+", "\n", "1", "]", "*", "gae", "\n", "self", ".", "returns", "[", "step", "]", "=", "gae", "+", "self", ".", "value_preds", "[", "step", "]", "\n", "", "", "else", ":", "\n", "                ", "self", ".", "returns", "[", "-", "1", "]", "=", "next_value", "\n", "for", "step", "in", "reversed", "(", "range", "(", "self", ".", "rewards", ".", "size", "(", "0", ")", ")", ")", ":", "\n", "                    ", "self", ".", "returns", "[", "step", "]", "=", "self", ".", "returns", "[", "step", "+", "1", "]", "*", "gamma", "*", "self", ".", "masks", "[", "step", "+", "1", "]", "+", "self", ".", "rewards", "[", "step", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.feed_forward_generator": [[107, 144], ["torch.utils.data.sampler.BatchSampler", "storage.RolloutStorage.rewards.size", "torch.utils.data.sampler.SubsetRandomSampler", "range", "storage.RolloutStorage.obs[].view", "storage.RolloutStorage.recurrent_hidden_states[].view", "storage.RolloutStorage.actions.view", "storage.RolloutStorage.value_preds[].view", "storage.RolloutStorage.returns[].view", "storage.RolloutStorage.masks[].view", "storage.RolloutStorage.action_log_probs.view", "storage.RolloutStorage.recurrent_hidden_states.size", "storage.RolloutStorage.actions.size", "advantages.view", "storage.RolloutStorage.obs.size"], "methods", ["None"], ["", "", "", "", "def", "feed_forward_generator", "(", "self", ",", "\n", "advantages", ",", "\n", "num_mini_batch", "=", "None", ",", "\n", "mini_batch_size", "=", "None", ")", ":", "\n", "        ", "num_steps", ",", "num_processes", "=", "self", ".", "rewards", ".", "size", "(", ")", "[", "0", ":", "2", "]", "\n", "batch_size", "=", "num_processes", "*", "num_steps", "\n", "\n", "if", "mini_batch_size", "is", "None", ":", "\n", "            ", "assert", "batch_size", ">=", "num_mini_batch", ",", "(", "\n", "\"PPO requires the number of processes ({}) \"", "\n", "\"* number of steps ({}) = {} \"", "\n", "\"to be greater than or equal to the number of PPO mini batches ({}).\"", "\n", "\"\"", ".", "format", "(", "num_processes", ",", "num_steps", ",", "num_processes", "*", "num_steps", ",", "\n", "num_mini_batch", ")", ")", "\n", "mini_batch_size", "=", "batch_size", "//", "num_mini_batch", "\n", "", "sampler", "=", "BatchSampler", "(", "\n", "SubsetRandomSampler", "(", "range", "(", "batch_size", ")", ")", ",", "\n", "mini_batch_size", ",", "\n", "drop_last", "=", "True", ")", "\n", "for", "indices", "in", "sampler", ":", "\n", "            ", "obs_batch", "=", "self", ".", "obs", "[", ":", "-", "1", "]", ".", "view", "(", "-", "1", ",", "*", "self", ".", "obs", ".", "size", "(", ")", "[", "2", ":", "]", ")", "[", "indices", "]", "\n", "recurrent_hidden_states_batch", "=", "self", ".", "recurrent_hidden_states", "[", ":", "-", "1", "]", ".", "view", "(", "\n", "-", "1", ",", "self", ".", "recurrent_hidden_states", ".", "size", "(", "-", "1", ")", ")", "[", "indices", "]", "\n", "actions_batch", "=", "self", ".", "actions", ".", "view", "(", "-", "1", ",", "\n", "self", ".", "actions", ".", "size", "(", "-", "1", ")", ")", "[", "indices", "]", "\n", "value_preds_batch", "=", "self", ".", "value_preds", "[", ":", "-", "1", "]", ".", "view", "(", "-", "1", ",", "1", ")", "[", "indices", "]", "\n", "return_batch", "=", "self", ".", "returns", "[", ":", "-", "1", "]", ".", "view", "(", "-", "1", ",", "1", ")", "[", "indices", "]", "\n", "masks_batch", "=", "self", ".", "masks", "[", ":", "-", "1", "]", ".", "view", "(", "-", "1", ",", "1", ")", "[", "indices", "]", "\n", "old_action_log_probs_batch", "=", "self", ".", "action_log_probs", ".", "view", "(", "-", "1", ",", "\n", "1", ")", "[", "indices", "]", "\n", "if", "advantages", "is", "None", ":", "\n", "                ", "adv_targ", "=", "None", "\n", "", "else", ":", "\n", "                ", "adv_targ", "=", "advantages", ".", "view", "(", "-", "1", ",", "1", ")", "[", "indices", "]", "\n", "\n", "", "yield", "obs_batch", ",", "recurrent_hidden_states_batch", ",", "actions_batch", ",", "value_preds_batch", ",", "return_batch", ",", "masks_batch", ",", "old_action_log_probs_batch", ",", "adv_targ", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.recurrent_generator": [[145, 203], ["storage.RolloutStorage.rewards.size", "torch.randperm", "range", "range", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack().view", "storage._flatten_helper", "storage._flatten_helper", "storage._flatten_helper", "storage._flatten_helper", "storage._flatten_helper", "storage._flatten_helper", "storage._flatten_helper", "_flatten_helper.append", "torch.stack().view.append", "_flatten_helper.append", "_flatten_helper.append", "_flatten_helper.append", "_flatten_helper.append", "_flatten_helper.append", "_flatten_helper.append", "torch.stack"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage._flatten_helper", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage._flatten_helper", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage._flatten_helper", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage._flatten_helper", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage._flatten_helper", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage._flatten_helper", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage._flatten_helper"], ["", "", "def", "recurrent_generator", "(", "self", ",", "advantages", ",", "num_mini_batch", ")", ":", "\n", "        ", "num_processes", "=", "self", ".", "rewards", ".", "size", "(", "1", ")", "\n", "assert", "num_processes", ">=", "num_mini_batch", ",", "(", "\n", "\"PPO requires the number of processes ({}) \"", "\n", "\"to be greater than or equal to the number of \"", "\n", "\"PPO mini batches ({}).\"", ".", "format", "(", "num_processes", ",", "num_mini_batch", ")", ")", "\n", "num_envs_per_batch", "=", "num_processes", "//", "num_mini_batch", "\n", "perm", "=", "torch", ".", "randperm", "(", "num_processes", ")", "\n", "for", "start_ind", "in", "range", "(", "0", ",", "num_processes", ",", "num_envs_per_batch", ")", ":", "\n", "            ", "obs_batch", "=", "[", "]", "\n", "recurrent_hidden_states_batch", "=", "[", "]", "\n", "actions_batch", "=", "[", "]", "\n", "value_preds_batch", "=", "[", "]", "\n", "return_batch", "=", "[", "]", "\n", "masks_batch", "=", "[", "]", "\n", "old_action_log_probs_batch", "=", "[", "]", "\n", "adv_targ", "=", "[", "]", "\n", "\n", "for", "offset", "in", "range", "(", "num_envs_per_batch", ")", ":", "\n", "                ", "ind", "=", "perm", "[", "start_ind", "+", "offset", "]", "\n", "obs_batch", ".", "append", "(", "self", ".", "obs", "[", ":", "-", "1", ",", "ind", "]", ")", "\n", "recurrent_hidden_states_batch", ".", "append", "(", "\n", "self", ".", "recurrent_hidden_states", "[", "0", ":", "1", ",", "ind", "]", ")", "\n", "actions_batch", ".", "append", "(", "self", ".", "actions", "[", ":", ",", "ind", "]", ")", "\n", "value_preds_batch", ".", "append", "(", "self", ".", "value_preds", "[", ":", "-", "1", ",", "ind", "]", ")", "\n", "return_batch", ".", "append", "(", "self", ".", "returns", "[", ":", "-", "1", ",", "ind", "]", ")", "\n", "masks_batch", ".", "append", "(", "self", ".", "masks", "[", ":", "-", "1", ",", "ind", "]", ")", "\n", "old_action_log_probs_batch", ".", "append", "(", "\n", "self", ".", "action_log_probs", "[", ":", ",", "ind", "]", ")", "\n", "adv_targ", ".", "append", "(", "advantages", "[", ":", ",", "ind", "]", ")", "\n", "\n", "", "T", ",", "N", "=", "self", ".", "num_steps", ",", "num_envs_per_batch", "\n", "# These are all tensors of size (T, N, -1)", "\n", "obs_batch", "=", "torch", ".", "stack", "(", "obs_batch", ",", "1", ")", "\n", "actions_batch", "=", "torch", ".", "stack", "(", "actions_batch", ",", "1", ")", "\n", "value_preds_batch", "=", "torch", ".", "stack", "(", "value_preds_batch", ",", "1", ")", "\n", "return_batch", "=", "torch", ".", "stack", "(", "return_batch", ",", "1", ")", "\n", "masks_batch", "=", "torch", ".", "stack", "(", "masks_batch", ",", "1", ")", "\n", "old_action_log_probs_batch", "=", "torch", ".", "stack", "(", "\n", "old_action_log_probs_batch", ",", "1", ")", "\n", "adv_targ", "=", "torch", ".", "stack", "(", "adv_targ", ",", "1", ")", "\n", "\n", "# States is just a (N, -1) tensor", "\n", "recurrent_hidden_states_batch", "=", "torch", ".", "stack", "(", "\n", "recurrent_hidden_states_batch", ",", "1", ")", ".", "view", "(", "N", ",", "-", "1", ")", "\n", "\n", "# Flatten the (T, N, ...) tensors to (T * N, ...)", "\n", "obs_batch", "=", "_flatten_helper", "(", "T", ",", "N", ",", "obs_batch", ")", "\n", "actions_batch", "=", "_flatten_helper", "(", "T", ",", "N", ",", "actions_batch", ")", "\n", "value_preds_batch", "=", "_flatten_helper", "(", "T", ",", "N", ",", "value_preds_batch", ")", "\n", "return_batch", "=", "_flatten_helper", "(", "T", ",", "N", ",", "return_batch", ")", "\n", "masks_batch", "=", "_flatten_helper", "(", "T", ",", "N", ",", "masks_batch", ")", "\n", "old_action_log_probs_batch", "=", "_flatten_helper", "(", "T", ",", "N", ",", "old_action_log_probs_batch", ")", "\n", "adv_targ", "=", "_flatten_helper", "(", "T", ",", "N", ",", "adv_targ", ")", "\n", "\n", "yield", "obs_batch", ",", "recurrent_hidden_states_batch", ",", "actions_batch", ",", "value_preds_batch", ",", "return_batch", ",", "masks_batch", ",", "old_action_log_probs_batch", ",", "adv_targ", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage._flatten_helper": [[5, 7], ["_tensor.view", "_tensor.size"], "function", ["None"], ["def", "_flatten_helper", "(", "T", ",", "N", ",", "_tensor", ")", ":", "\n", "    ", "return", "_tensor", ".", "view", "(", "T", "*", "N", ",", "*", "_tensor", ".", "size", "(", ")", "[", "2", ":", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.AddBias.__init__": [[33, 36], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "bias.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "bias", ")", ":", "\n", "        ", "super", "(", "AddBias", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_bias", "=", "nn", ".", "Parameter", "(", "bias", ".", "unsqueeze", "(", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.AddBias.forward": [[37, 44], ["x.dim", "utils.AddBias._bias.t().view", "utils.AddBias._bias.t().view", "utils.AddBias._bias.t", "utils.AddBias._bias.t"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "x", ".", "dim", "(", ")", "==", "2", ":", "\n", "            ", "bias", "=", "self", ".", "_bias", ".", "t", "(", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "bias", "=", "self", ".", "_bias", ".", "t", "(", ")", ".", "view", "(", "1", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "\n", "", "return", "x", "+", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.get_render_func": [[11, 20], ["hasattr", "hasattr", "utils.get_render_func", "hasattr", "utils.get_render_func"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.get_render_func", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.get_render_func"], ["def", "get_render_func", "(", "venv", ")", ":", "\n", "    ", "if", "hasattr", "(", "venv", ",", "'envs'", ")", ":", "\n", "        ", "return", "venv", ".", "envs", "[", "0", "]", ".", "render", "\n", "", "elif", "hasattr", "(", "venv", ",", "'venv'", ")", ":", "\n", "        ", "return", "get_render_func", "(", "venv", ".", "venv", ")", "\n", "", "elif", "hasattr", "(", "venv", ",", "'env'", ")", ":", "\n", "        ", "return", "get_render_func", "(", "venv", ".", "env", ")", "\n", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.get_vec_normalize": [[22, 29], ["isinstance", "hasattr", "utils.get_vec_normalize"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.get_vec_normalize"], ["", "def", "get_vec_normalize", "(", "venv", ")", ":", "\n", "    ", "if", "isinstance", "(", "venv", ",", "VecNormalize", ")", ":", "\n", "        ", "return", "venv", "\n", "", "elif", "hasattr", "(", "venv", ",", "'venv'", ")", ":", "\n", "        ", "return", "get_vec_normalize", "(", "venv", ".", "venv", ")", "\n", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.update_linear_schedule": [[46, 51], ["float"], "function", ["None"], ["", "", "def", "update_linear_schedule", "(", "optimizer", ",", "epoch", ",", "total_num_epochs", ",", "initial_lr", ")", ":", "\n", "    ", "\"\"\"Decreases the learning rate linearly\"\"\"", "\n", "lr", "=", "initial_lr", "-", "(", "initial_lr", "*", "(", "epoch", "/", "float", "(", "total_num_epochs", ")", ")", ")", "\n", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "        ", "param_group", "[", "'lr'", "]", "=", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.init": [[53, 57], ["weight_init", "bias_init"], "function", ["None"], ["", "", "def", "init", "(", "module", ",", "weight_init", ",", "bias_init", ",", "gain", "=", "1", ")", ":", "\n", "    ", "weight_init", "(", "module", ".", "weight", ".", "data", ",", "gain", "=", "gain", ")", "\n", "bias_init", "(", "module", ".", "bias", ".", "data", ")", "\n", "return", "module", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir": [[59, 66], ["os.makedirs", "glob.glob", "os.path.join", "os.remove"], "function", ["None"], ["", "def", "cleanup_log_dir", "(", "log_dir", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "os", ".", "makedirs", "(", "log_dir", ")", "\n", "", "except", "OSError", ":", "\n", "        ", "files", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "log_dir", ",", "'*.monitor.csv'", ")", ")", "\n", "for", "f", "in", "files", ":", "\n", "            ", "os", ".", "remove", "(", "f", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_iko.Dset__.__init__": [[47, 57], ["int", "len", "len", "len"], "methods", ["None"], ["", "class", "Dset__", "(", "object", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "obs", ",", "acs", ",", "num_traj", ",", "absorbing_state", ",", "absorbing_action", ")", ":", "\n", "        ", "self", ".", "obs", "=", "obs", "\n", "self", ".", "acs", "=", "acs", "\n", "self", ".", "num_traj", "=", "num_traj", "\n", "assert", "len", "(", "self", ".", "obs", ")", "==", "len", "(", "self", ".", "acs", ")", "\n", "assert", "self", ".", "num_traj", ">", "0", "\n", "self", ".", "steps_per_traj", "=", "int", "(", "len", "(", "self", ".", "obs", ")", "/", "num_traj", ")", "\n", "\n", "self", ".", "absorbing_state", "=", "absorbing_state", "\n", "self", ".", "absorbing_action", "=", "absorbing_action", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_iko.Dset__.get_next_batch": [[58, 89], ["int", "range", "len", "numpy.sort", "range", "numpy.random.choice", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "range"], "methods", ["None"], ["\n", "", "def", "get_next_batch", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "assert", "batch_size", "<=", "len", "(", "self", ".", "obs", ")", "\n", "num_samples_per_traj", "=", "int", "(", "batch_size", "/", "self", ".", "num_traj", ")", "\n", "assert", "num_samples_per_traj", "*", "self", ".", "num_traj", "==", "batch_size", "\n", "N", "=", "self", ".", "steps_per_traj", "/", "num_samples_per_traj", "# This is the importance weight for", "\n", "j", "=", "num_samples_per_traj", "\n", "num_samples_per_traj", "=", "num_samples_per_traj", "-", "1", "# make room for absorbing", "\n", "\n", "obs", "=", "None", "\n", "acs", "=", "None", "\n", "weights", "=", "[", "1", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "while", "j", "<=", "batch_size", ":", "\n", "            ", "weights", "[", "j", "-", "1", "]", "=", "1.0", "/", "N", "\n", "j", "=", "j", "+", "num_samples_per_traj", "+", "1", "\n", "\n", "", "for", "i", "in", "range", "(", "self", ".", "num_traj", ")", ":", "\n", "            ", "indicies", "=", "np", ".", "sort", "(", "\n", "np", ".", "random", ".", "choice", "(", "range", "(", "self", ".", "steps_per_traj", "*", "i", ",", "self", ".", "steps_per_traj", "*", "(", "i", "+", "1", ")", ")", ",", "num_samples_per_traj", ",", "\n", "replace", "=", "False", ")", ")", "\n", "\n", "if", "obs", "is", "None", ":", "\n", "                ", "obs", "=", "np", ".", "concatenate", "(", "(", "self", ".", "obs", "[", "indicies", ",", ":", "]", ",", "self", ".", "absorbing_state", ")", ",", "axis", "=", "0", ")", "\n", "", "else", ":", "\n", "                ", "obs", "=", "np", ".", "concatenate", "(", "(", "obs", ",", "self", ".", "obs", "[", "indicies", ",", ":", "]", ",", "self", ".", "absorbing_state", ")", ",", "axis", "=", "0", ")", "\n", "\n", "", "if", "acs", "is", "None", ":", "\n", "                ", "acs", "=", "np", ".", "concatenate", "(", "(", "self", ".", "acs", "[", "indicies", ",", ":", "]", ",", "self", ".", "absorbing_action", ")", ",", "axis", "=", "0", ")", "\n", "", "else", ":", "\n", "                ", "acs", "=", "np", ".", "concatenate", "(", "(", "acs", ",", "self", ".", "acs", "[", "indicies", ",", ":", "]", ",", "self", ".", "absorbing_action", ")", ",", "axis", "=", "0", ")", "\n", "\n", "", "", "return", "obs", ",", "acs", ",", "weights", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_iko.Dset.__init__": [[109, 116], ["len", "mujoco_dset_zm_iko.Dset.init_pointer", "len", "len"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Dset.init_pointer"], ["", "", "class", "Dset", "(", "object", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "inputs", ",", "labels", ",", "randomize", ")", ":", "\n", "        ", "self", ".", "inputs", "=", "inputs", "\n", "self", ".", "labels", "=", "labels", "\n", "assert", "len", "(", "self", ".", "inputs", ")", "==", "len", "(", "self", ".", "labels", ")", "\n", "self", ".", "randomize", "=", "randomize", "\n", "self", ".", "num_pairs", "=", "len", "(", "inputs", ")", "\n", "self", ".", "init_pointer", "(", ")", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_iko.Dset.init_pointer": [[117, 124], ["numpy.arange", "numpy.random.shuffle"], "methods", ["None"], ["\n", "", "def", "init_pointer", "(", "self", ")", ":", "\n", "        ", "self", ".", "pointer", "=", "0", "\n", "if", "self", ".", "randomize", ":", "\n", "            ", "idx", "=", "np", ".", "arange", "(", "self", ".", "num_pairs", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "idx", ")", "\n", "self", ".", "inputs", "=", "self", ".", "inputs", "[", "idx", ",", ":", "]", "\n", "self", ".", "labels", "=", "self", ".", "labels", "[", "idx", ",", ":", "]", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_iko.Dset.get_next_batch": [[125, 136], ["mujoco_dset_zm_iko.Dset.init_pointer"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Dset.init_pointer"], ["\n", "", "", "def", "get_next_batch", "(", "self", ",", "batch_size", ")", ":", "\n", "# if batch_size is negative -> return all", "\n", "        ", "if", "batch_size", "<", "0", ":", "\n", "            ", "return", "self", ".", "inputs", ",", "self", ".", "labels", "\n", "", "if", "self", ".", "pointer", "+", "batch_size", ">=", "self", ".", "num_pairs", ":", "\n", "            ", "self", ".", "init_pointer", "(", ")", "\n", "", "end", "=", "self", ".", "pointer", "+", "batch_size", "\n", "inputs", "=", "self", ".", "inputs", "[", "self", ".", "pointer", ":", "end", ",", ":", "]", "\n", "labels", "=", "self", ".", "labels", "[", "self", ".", "pointer", ":", "end", ",", ":", "]", "\n", "self", ".", "pointer", "=", "end", "\n", "return", "inputs", ",", "labels", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_iko.Mujoco_Dset.__init__": [[139, 165], ["mujoco_dset_zm_iko.Mujoco_Dset.subsample", "numpy.reshape", "numpy.reshape", "rets.sum", "numpy.std", "len", "len", "mujoco_dset_zm_iko.Dset", "mujoco_dset_zm_iko.Mujoco_Dset.log_info", "mujoco_dset_zm_iko.load_dataset_random", "mujoco_dset_zm_iko.load_dataset", "sum", "len", "numpy.array", "len", "numpy.squeeze", "len", "len", "numpy.prod", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Mujoco_Dset.subsample", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Mujoco_Dset.log_info", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_iko.load_dataset_random", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_iko.load_dataset"], ["", "", "class", "Mujoco_Dset", "(", "object", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "expert_path", ",", "traj_limitation", "=", "-", "1", ",", "subsample_frequency", "=", "20", ",", "randomize", "=", "True", ",", "initrandom", "=", "True", ")", ":", "\n", "\n", "        ", "if", "initrandom", ":", "\n", "            ", "obs", ",", "acs", ",", "rets", "=", "load_dataset_random", "(", "expert_path", ",", "traj_limitation", ")", "\n", "", "else", ":", "\n", "            ", "obs", ",", "acs", ",", "rets", "=", "load_dataset", "(", "expert_path", ",", "traj_limitation", ")", "\n", "\n", "", "obs", ",", "acs", "=", "self", ".", "subsample", "(", "obs", ",", "acs", ",", "subsample_frequency", "=", "subsample_frequency", ")", "\n", "\n", "# obs, acs: shape (N, L, ) + S where N = # episodes, L = episode length", "\n", "# and S is the environment observation/action space.", "\n", "# Flatten to (N * L, prod(S))", "\n", "self", ".", "obs", "=", "np", ".", "reshape", "(", "obs", ",", "[", "-", "1", ",", "np", ".", "prod", "(", "obs", ".", "shape", "[", "2", ":", "]", ")", "]", ")", "\n", "self", ".", "acs", "=", "np", ".", "reshape", "(", "acs", ",", "[", "-", "1", ",", "np", ".", "prod", "(", "acs", ".", "shape", "[", "2", ":", "]", ")", "]", ")", "\n", "\n", "self", ".", "rets", "=", "rets", ".", "sum", "(", "axis", "=", "1", ")", "\n", "self", ".", "avg_ret", "=", "sum", "(", "self", ".", "rets", ")", "/", "len", "(", "self", ".", "rets", ")", "\n", "self", ".", "std_ret", "=", "np", ".", "std", "(", "np", ".", "array", "(", "self", ".", "rets", ")", ")", "\n", "if", "len", "(", "self", ".", "acs", ")", ">", "2", ":", "\n", "            ", "self", ".", "acs", "=", "np", ".", "squeeze", "(", "self", ".", "acs", ")", "\n", "", "assert", "len", "(", "self", ".", "obs", ")", "==", "len", "(", "self", ".", "acs", ")", "\n", "self", ".", "num_traj", "=", "len", "(", "rets", ")", "\n", "self", ".", "num_transition", "=", "len", "(", "self", ".", "obs", ")", "\n", "\n", "self", ".", "dset", "=", "Dset", "(", "self", ".", "obs", ",", "self", ".", "acs", ",", "randomize", "=", "randomize", ")", "\n", "self", ".", "log_info", "(", ")", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_iko.Mujoco_Dset.log_info": [[166, 171], ["print", "print", "print", "print"], "methods", ["None"], ["\n", "", "def", "log_info", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"Total trajs: %d\"", "%", "self", ".", "num_traj", ")", "\n", "print", "(", "\"Total transitions: %d\"", "%", "self", ".", "num_transition", ")", "\n", "print", "(", "\"Average returns: %f\"", "%", "self", ".", "avg_ret", ")", "\n", "print", "(", "\"Std for returns: %f\"", "%", "self", ".", "std_ret", ")", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_iko.Mujoco_Dset.get_next_batch": [[172, 174], ["mujoco_dset_zm_iko.Mujoco_Dset.dset.get_next_batch"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Mujoco_Dset.get_next_batch"], ["\n", "", "def", "get_next_batch", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "return", "self", ".", "dset", ".", "get_next_batch", "(", "batch_size", ")", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_iko.Mujoco_Dset.subsample": [[175, 197], ["len", "numpy.random.randint", "range", "numpy.array", "numpy.array", "numpy.array.append", "numpy.array.append"], "methods", ["None"], ["\n", "", "def", "subsample", "(", "self", ",", "obs", ",", "acts", ",", "subsample_frequency", "=", "20", ")", ":", "\n", "\n", "        ", "num_trajectories", "=", "len", "(", "obs", ")", "\n", "start_idx", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "subsample_frequency", ",", "num_trajectories", ")", "\n", "t_obs", "=", "[", "]", "\n", "t_acts", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "num_trajectories", ")", ":", "\n", "            ", "obs_i", "=", "obs", "[", "i", "]", "\n", "acts_i", "=", "acts", "[", "i", "]", "\n", "\n", "t_obs", ".", "append", "(", "obs_i", "[", "start_idx", "[", "i", "]", ":", ":", "subsample_frequency", "]", ")", "\n", "t_acts", ".", "append", "(", "acts_i", "[", "start_idx", "[", "i", "]", ":", ":", "subsample_frequency", "]", ")", "\n", "\n", "# t_obs.append(obs[i, start_idx[i]::subsample_frequency])", "\n", "# t_acts.append(acts[i, start_idx[i]::subsample_frequency])", "\n", "\n", "", "t_obs", "=", "np", ".", "array", "(", "t_obs", ")", "\n", "t_acts", "=", "np", ".", "array", "(", "t_acts", ")", "\n", "\n", "\n", "return", "t_obs", ",", "t_acts", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_iko.load_dataset": [[10, 23], ["h5py.File", "min"], "function", ["None"], ["def", "load_dataset", "(", "filename", ",", "limit_trajs", ",", "data_subsamp_freq", "=", "1", ")", ":", "\n", "# Load expert data", "\n", "    ", "with", "h5py", ".", "File", "(", "filename", ",", "'r'", ")", "as", "f", ":", "\n", "# Read data as written by vis_mj.py", "\n", "        ", "full_dset_size", "=", "f", "[", "'obs_B_T_Do'", "]", ".", "shape", "[", "0", "]", "# full dataset size", "\n", "dset_size", "=", "min", "(", "full_dset_size", ",", "limit_trajs", ")", "if", "limit_trajs", "is", "not", "None", "else", "full_dset_size", "\n", "\n", "exobs_B_T_Do", "=", "f", "[", "'obs_B_T_Do'", "]", "[", ":", "dset_size", ",", "...", "]", "[", "...", "]", "\n", "exa_B_T_Da", "=", "f", "[", "'a_B_T_Da'", "]", "[", ":", "dset_size", ",", "...", "]", "[", "...", "]", "\n", "exr_B_T", "=", "f", "[", "'r_B_T'", "]", "[", ":", "dset_size", ",", "...", "]", "[", "...", "]", "\n", "exlen_B", "=", "f", "[", "'len_B'", "]", "[", ":", "dset_size", ",", "...", "]", "[", "...", "]", "\n", "\n", "", "return", "exobs_B_T_Do", ",", "exa_B_T_Da", ",", "exr_B_T", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_iko.load_dataset_random": [[24, 45], ["h5py.File", "numpy.random.choice", "numpy.sort", "min"], "function", ["None"], ["", "def", "load_dataset_random", "(", "filename", ",", "limit_trajs", ")", ":", "\n", "# Load expert data", "\n", "# filename = '../data/baseline/deterministic.trpo.Hopper.0.00.npz'", "\n", "    ", "with", "h5py", ".", "File", "(", "filename", ",", "'r'", ")", "as", "f", ":", "\n", "# Read data as written by vis_mj.py", "\n", "        ", "full_dset_size", "=", "f", "[", "'obs_B_T_Do'", "]", ".", "shape", "[", "0", "]", "# full dataset size", "\n", "dset_size", "=", "min", "(", "full_dset_size", ",", "limit_trajs", ")", "if", "limit_trajs", "is", "not", "None", "else", "full_dset_size", "\n", "\n", "index", "=", "np", ".", "random", ".", "choice", "(", "full_dset_size", ",", "limit_trajs", ")", "\n", "index", "=", "np", ".", "sort", "(", "index", ")", "\n", "\n", "exobs_B_T_Do", "=", "f", "[", "'obs_B_T_Do'", "]", "[", "index", ",", "...", "]", "[", "...", "]", "\n", "exa_B_T_Da", "=", "f", "[", "'a_B_T_Da'", "]", "[", "index", ",", "...", "]", "[", "...", "]", "\n", "exr_B_T", "=", "f", "[", "'r_B_T'", "]", "[", "index", ",", "...", "]", "[", "...", "]", "\n", "exlen_B", "=", "f", "[", "'len_B'", "]", "[", "index", ",", "...", "]", "[", "...", "]", "\n", "\n", "# exobs_B_T_Do = f['obs_B_T_Do'][:dset_size, ...][...]", "\n", "# exa_B_T_Da = f['a_B_T_Da'][:dset_size, ...][...]", "\n", "# exr_B_T = f['r_B_T'][:dset_size, ...][...]", "\n", "# exlen_B = f['len_B'][:dset_size, ...][...]", "\n", "\n", "", "return", "exobs_B_T_Do", ",", "exa_B_T_Da", ",", "exr_B_T", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Dset.__init__": [[12, 19], ["len", "wdgail.Dset.init_pointer", "len", "len"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Dset.init_pointer"], ["    ", "def", "__init__", "(", "self", ",", "inputs", ",", "labels", ",", "randomize", ")", ":", "\n", "        ", "self", ".", "inputs", "=", "inputs", "\n", "self", ".", "labels", "=", "labels", "\n", "assert", "len", "(", "self", ".", "inputs", ")", "==", "len", "(", "self", ".", "labels", ")", "\n", "self", ".", "randomize", "=", "randomize", "\n", "self", ".", "num_pairs", "=", "len", "(", "inputs", ")", "\n", "self", ".", "init_pointer", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Dset.init_pointer": [[20, 27], ["numpy.arange", "numpy.random.shuffle"], "methods", ["None"], ["", "def", "init_pointer", "(", "self", ")", ":", "\n", "        ", "self", ".", "pointer", "=", "0", "\n", "if", "self", ".", "randomize", ":", "\n", "            ", "idx", "=", "np", ".", "arange", "(", "self", ".", "num_pairs", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "idx", ")", "\n", "self", ".", "inputs", "=", "self", ".", "inputs", "[", "idx", ",", ":", "]", "\n", "self", ".", "labels", "=", "self", ".", "labels", "[", "idx", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Dset.get_next_batch": [[28, 39], ["wdgail.Dset.init_pointer"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Dset.init_pointer"], ["", "", "def", "get_next_batch", "(", "self", ",", "batch_size", ")", ":", "\n", "# if batch_size is negative -> return all", "\n", "        ", "if", "batch_size", "<", "0", ":", "\n", "            ", "return", "self", ".", "inputs", ",", "self", ".", "labels", "\n", "", "if", "self", ".", "pointer", "+", "batch_size", ">=", "self", ".", "num_pairs", ":", "\n", "            ", "self", ".", "init_pointer", "(", ")", "\n", "", "end", "=", "self", ".", "pointer", "+", "batch_size", "\n", "inputs", "=", "self", ".", "inputs", "[", "self", ".", "pointer", ":", "end", ",", ":", "]", "\n", "labels", "=", "self", ".", "labels", "[", "self", ".", "pointer", ":", "end", ",", ":", "]", "\n", "self", ".", "pointer", "=", "end", "\n", "return", "inputs", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.__init__": [[41, 65], ["torch.Module.__init__", "torch.Sequential().to", "torch.Sequential().to", "torch.Sequential().to", "torch.Sequential().to", "wdgail.Discriminator.trunk.train", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "baselines.common.running_mean_std.RunningMeanStd", "wdgail.Discriminator.trunk.parameters", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.train"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "hidden_dim", ",", "device", ",", "reward_type", ",", "update_rms", ",", "cliprew_down", "=", "-", "10.0", ",", "cliprew_up", "=", "10.0", ")", ":", "\n", "        ", "super", "(", "Discriminator", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "cliprew_down", "=", "cliprew_down", "\n", "self", ".", "cliprew_up", "=", "cliprew_up", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "reward_type", "=", "reward_type", "\n", "self", ".", "update_rms", "=", "update_rms", "\n", "\n", "# self.trunk = nn.Sequential(", "\n", "#     nn.Linear(input_dim, hidden_dim), nn.Tanh(),", "\n", "#     nn.Linear(hidden_dim, hidden_dim), nn.Tanh(),", "\n", "#     nn.Linear(hidden_dim, 1), nn.Tanh()).to(device)", "\n", "\n", "self", ".", "trunk", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "input_dim", ",", "hidden_dim", ")", ",", "nn", ".", "Tanh", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "hidden_dim", ",", "hidden_dim", ")", ",", "nn", ".", "Tanh", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "hidden_dim", ",", "1", ")", ")", ".", "to", "(", "device", ")", "\n", "\n", "self", ".", "trunk", ".", "train", "(", ")", "\n", "\n", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "trunk", ".", "parameters", "(", ")", ")", "\n", "\n", "self", ".", "returns", "=", "None", "\n", "self", ".", "ret_rms", "=", "RunningMeanStd", "(", "shape", "=", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.compute_grad_pen": [[66, 93], ["torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "alpha.expand_as().to.expand_as().to.expand_as().to", "wdgail.Discriminator.trunk", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "expert_state.size", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "alpha.expand_as().to.expand_as().to.expand_as", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "wdgail.Discriminator.size", "grad.norm"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to"], ["", "def", "compute_grad_pen", "(", "self", ",", "\n", "expert_state", ",", "\n", "expert_action", ",", "\n", "policy_state", ",", "\n", "policy_action", ",", "\n", "lambda_", "=", "10", ")", ":", "\n", "        ", "alpha", "=", "torch", ".", "rand", "(", "expert_state", ".", "size", "(", "0", ")", ",", "1", ")", "\n", "expert_data", "=", "torch", ".", "cat", "(", "[", "expert_state", ",", "expert_action", "]", ",", "dim", "=", "1", ")", "\n", "policy_data", "=", "torch", ".", "cat", "(", "[", "policy_state", ",", "policy_action", "]", ",", "dim", "=", "1", ")", "\n", "\n", "alpha", "=", "alpha", ".", "expand_as", "(", "expert_data", ")", ".", "to", "(", "expert_data", ".", "device", ")", "\n", "\n", "mixup_data", "=", "alpha", "*", "expert_data", "+", "(", "1", "-", "alpha", ")", "*", "policy_data", "\n", "mixup_data", ".", "requires_grad", "=", "True", "\n", "\n", "disc", "=", "self", ".", "trunk", "(", "mixup_data", ")", "\n", "ones", "=", "torch", ".", "ones", "(", "disc", ".", "size", "(", ")", ")", ".", "to", "(", "disc", ".", "device", ")", "\n", "grad", "=", "autograd", ".", "grad", "(", "\n", "outputs", "=", "disc", ",", "\n", "inputs", "=", "mixup_data", ",", "\n", "grad_outputs", "=", "ones", ",", "\n", "create_graph", "=", "True", ",", "\n", "retain_graph", "=", "True", ",", "\n", "only_inputs", "=", "True", ")", "[", "0", "]", "\n", "\n", "grad_pen", "=", "lambda_", "*", "(", "grad", ".", "norm", "(", "2", ",", "dim", "=", "1", ")", "-", "1", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "return", "grad_pen", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.update_zm": [[94, 159], ["wdgail.Discriminator.train", "obs[].view", "obs[].view.cpu().detach().numpy", "actions.view.cpu().detach().numpy.view", "actions_batch.cpu().detach().numpy.view.cpu().detach().numpy", "wdgail.Dset", "wdgail.Dset.get_next_batch", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "wdgail.Discriminator.trunk", "expert_buf.get_next_batch", "obsfilt", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "wdgail.Discriminator.trunk", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "wdgail.Discriminator.compute_grad_pen", "wd.item", "wdgail.Discriminator.item", "wdgail.Discriminator.optimizer.zero_grad", "wdgail.Discriminator.optimizer.step", "actions.view.cpu().detach().numpy.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "obs[].view.cpu().detach", "actions_batch.cpu().detach().numpy.view.cpu().detach", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "obs.size", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "obs[].view.cpu", "actions_batch.cpu().detach().numpy.view.cpu", "len"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.train", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Mujoco_Dset.get_next_batch", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Mujoco_Dset.get_next_batch", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.compute_grad_pen", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.step"], ["", "def", "update_zm", "(", "self", ",", "replay_buf", ",", "expert_buf", ",", "obsfilt", "=", "None", ",", "batch_size", "=", "128", ")", ":", "\n", "        ", "self", ".", "train", "(", ")", "\n", "obs", "=", "replay_buf", ".", "obs", "\n", "obs_batch", "=", "obs", "[", ":", "-", "1", "]", ".", "view", "(", "-", "1", ",", "*", "obs", ".", "size", "(", ")", "[", "2", ":", "]", ")", "\n", "states", "=", "obs_batch", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "# states = np.concatenate(states,axis=1)", "\n", "actions", "=", "replay_buf", ".", "actions", "\n", "actions_batch", "=", "actions", ".", "view", "(", "-", "1", ",", "actions", ".", "size", "(", "-", "1", ")", ")", "\n", "actions", "=", "actions_batch", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "policy_buf", "=", "Dset", "(", "inputs", "=", "states", "[", "0", ":", "len", "(", "actions", ")", "]", ",", "labels", "=", "actions", ",", "randomize", "=", "True", ")", "\n", "\n", "loss", "=", "0", "\n", "g_loss", "=", "0.0", "\n", "gp", "=", "0.0", "\n", "n", "=", "0", "\n", "\n", "# loss = 0", "\n", "\n", "# Sample replay buffer", "\n", "policy_state", ",", "policy_action", "=", "policy_buf", ".", "get_next_batch", "(", "batch_size", ")", "\n", "policy_state", "=", "torch", ".", "FloatTensor", "(", "policy_state", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "policy_action", "=", "torch", ".", "FloatTensor", "(", "policy_action", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "temp", "=", "[", "policy_state", ",", "policy_action", "]", "\n", "policy_d", "=", "self", ".", "trunk", "(", "torch", ".", "cat", "(", "[", "policy_state", ",", "policy_action", "]", ",", "dim", "=", "1", ")", ")", "\n", "\n", "# Sample expert buffer", "\n", "expert_state", ",", "expert_action", "=", "expert_buf", ".", "get_next_batch", "(", "batch_size", ")", "\n", "expert_state", "=", "obsfilt", "(", "expert_state", ",", "update", "=", "False", ")", "\n", "expert_state", "=", "torch", ".", "FloatTensor", "(", "expert_state", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "expert_action", "=", "torch", ".", "FloatTensor", "(", "expert_action", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "expert_d", "=", "self", ".", "trunk", "(", "torch", ".", "cat", "(", "[", "expert_state", ",", "expert_action", "]", ",", "dim", "=", "1", ")", ")", "\n", "\n", "# expert_loss = F.binary_cross_entropy_with_logits(", "\n", "#     expert_d,", "\n", "#     torch.ones(expert_d.size()).to(self.device))", "\n", "# policy_loss = F.binary_cross_entropy_with_logits(", "\n", "#     policy_d,", "\n", "#     torch.zeros(policy_d.size()).to(self.device))", "\n", "\n", "# expert_loss = torch.mean(expert_d).to(self.device)", "\n", "# policy_loss = torch.mean(policy_d).to(self.device)", "\n", "\n", "expert_loss", "=", "torch", ".", "mean", "(", "torch", ".", "tanh", "(", "expert_d", ")", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "policy_loss", "=", "torch", ".", "mean", "(", "torch", ".", "tanh", "(", "policy_d", ")", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# gail_loss = expert_loss + policy_loss", "\n", "wd", "=", "expert_loss", "-", "policy_loss", "\n", "grad_pen", "=", "self", ".", "compute_grad_pen", "(", "expert_state", ",", "expert_action", ",", "\n", "policy_state", ",", "policy_action", ")", "\n", "\n", "# loss += (gail_loss + grad_pen).item()", "\n", "loss", "+=", "(", "-", "wd", "+", "grad_pen", ")", ".", "item", "(", ")", "\n", "g_loss", "+=", "(", "wd", ")", ".", "item", "(", ")", "\n", "gp", "+=", "(", "grad_pen", ")", ".", "item", "(", ")", "\n", "n", "+=", "1", "\n", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "# (gail_loss + grad_pen).backward()", "\n", "(", "-", "wd", "+", "grad_pen", ")", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "\n", "return", "g_loss", "/", "n", ",", "gp", "/", "n", ",", "0.0", ",", "loss", "/", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.update": [[160, 213], ["wdgail.Discriminator.train", "rollouts.feed_forward_generator", "zip", "wdgail.Discriminator.trunk", "obsfilt", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "expert_action.to.to.to", "wdgail.Discriminator.trunk", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "wdgail.Discriminator.compute_grad_pen", "wd.item", "wdgail.Discriminator.item", "wdgail.Discriminator.optimizer.zero_grad", "wdgail.Discriminator.optimizer.step", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.FloatTensor().to.numpy", "torch.FloatTensor().to.numpy", "torch.FloatTensor().to.numpy", "torch.FloatTensor().to.numpy", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.train", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.feed_forward_generator", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.compute_grad_pen", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.step"], ["", "def", "update", "(", "self", ",", "expert_loader", ",", "rollouts", ",", "obsfilt", "=", "None", ")", ":", "\n", "        ", "self", ".", "train", "(", ")", "\n", "\n", "policy_data_generator", "=", "rollouts", ".", "feed_forward_generator", "(", "\n", "None", ",", "mini_batch_size", "=", "expert_loader", ".", "batch_size", ")", "\n", "\n", "loss", "=", "0", "\n", "g_loss", "=", "0.0", "\n", "gp", "=", "0.0", "\n", "n", "=", "0", "\n", "for", "expert_batch", ",", "policy_batch", "in", "zip", "(", "expert_loader", ",", "\n", "policy_data_generator", ")", ":", "\n", "            ", "policy_state", ",", "policy_action", "=", "policy_batch", "[", "0", "]", ",", "policy_batch", "[", "2", "]", "\n", "policy_d", "=", "self", ".", "trunk", "(", "\n", "torch", ".", "cat", "(", "[", "policy_state", ",", "policy_action", "]", ",", "dim", "=", "1", ")", ")", "\n", "\n", "expert_state", ",", "expert_action", "=", "expert_batch", "\n", "expert_state", "=", "obsfilt", "(", "expert_state", ".", "numpy", "(", ")", ",", "update", "=", "False", ")", "\n", "expert_state", "=", "torch", ".", "FloatTensor", "(", "expert_state", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "expert_action", "=", "expert_action", ".", "to", "(", "self", ".", "device", ")", "\n", "expert_d", "=", "self", ".", "trunk", "(", "\n", "torch", ".", "cat", "(", "[", "expert_state", ",", "expert_action", "]", ",", "dim", "=", "1", ")", ")", "\n", "\n", "# expert_loss = F.binary_cross_entropy_with_logits(", "\n", "#     expert_d,", "\n", "#     torch.ones(expert_d.size()).to(self.device))", "\n", "# policy_loss = F.binary_cross_entropy_with_logits(", "\n", "#     policy_d,", "\n", "#     torch.zeros(policy_d.size()).to(self.device))", "\n", "\n", "# expert_loss = torch.mean(expert_d).to(self.device)", "\n", "# policy_loss = torch.mean(policy_d).to(self.device)", "\n", "\n", "expert_loss", "=", "torch", ".", "mean", "(", "torch", ".", "tanh", "(", "expert_d", ")", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "policy_loss", "=", "torch", ".", "mean", "(", "torch", ".", "tanh", "(", "policy_d", ")", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# gail_loss = expert_loss + policy_loss", "\n", "wd", "=", "expert_loss", "-", "policy_loss", "\n", "grad_pen", "=", "self", ".", "compute_grad_pen", "(", "expert_state", ",", "expert_action", ",", "\n", "policy_state", ",", "policy_action", ")", "\n", "\n", "# loss += (gail_loss + grad_pen).item()", "\n", "loss", "+=", "(", "-", "wd", "+", "grad_pen", ")", ".", "item", "(", ")", "\n", "g_loss", "+=", "(", "wd", ")", ".", "item", "(", ")", "\n", "gp", "+=", "(", "grad_pen", ")", ".", "item", "(", ")", "\n", "n", "+=", "1", "\n", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "# (gail_loss + grad_pen).backward()", "\n", "(", "-", "wd", "+", "grad_pen", ")", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "return", "g_loss", "/", "n", ",", "gp", "/", "n", ",", "0.0", ",", "loss", "/", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.update_origin": [[214, 264], ["wdgail.Discriminator.train", "rollouts.feed_forward_generator", "zip", "wdgail.Discriminator.trunk", "obsfilt", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "expert_action.to.to.to", "wdgail.Discriminator.trunk", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "torch.mean().to", "wdgail.Discriminator.compute_grad_pen", "wd.item", "wdgail.Discriminator.item", "wdgail.Discriminator.optimizer.zero_grad", "wdgail.Discriminator.optimizer.step", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.FloatTensor().to.numpy", "torch.FloatTensor().to.numpy", "torch.FloatTensor().to.numpy", "torch.FloatTensor().to.numpy", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.train", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.feed_forward_generator", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.compute_grad_pen", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.step"], ["", "def", "update_origin", "(", "self", ",", "expert_loader", ",", "rollouts", ",", "obsfilt", "=", "None", ")", ":", "\n", "        ", "self", ".", "train", "(", ")", "\n", "\n", "policy_data_generator", "=", "rollouts", ".", "feed_forward_generator", "(", "\n", "None", ",", "mini_batch_size", "=", "expert_loader", ".", "batch_size", ")", "\n", "\n", "loss", "=", "0", "\n", "g_loss", "=", "0.0", "\n", "gp", "=", "0.0", "\n", "n", "=", "0", "\n", "for", "expert_batch", ",", "policy_batch", "in", "zip", "(", "expert_loader", ",", "\n", "policy_data_generator", ")", ":", "\n", "            ", "policy_state", ",", "policy_action", "=", "policy_batch", "[", "0", "]", ",", "policy_batch", "[", "2", "]", "\n", "policy_d", "=", "self", ".", "trunk", "(", "\n", "torch", ".", "cat", "(", "[", "policy_state", ",", "policy_action", "]", ",", "dim", "=", "1", ")", ")", "\n", "\n", "expert_state", ",", "expert_action", "=", "expert_batch", "\n", "expert_state", "=", "obsfilt", "(", "expert_state", ".", "numpy", "(", ")", ",", "update", "=", "False", ")", "\n", "expert_state", "=", "torch", ".", "FloatTensor", "(", "expert_state", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "expert_action", "=", "expert_action", ".", "to", "(", "self", ".", "device", ")", "\n", "expert_d", "=", "self", ".", "trunk", "(", "\n", "torch", ".", "cat", "(", "[", "expert_state", ",", "expert_action", "]", ",", "dim", "=", "1", ")", ")", "\n", "\n", "# expert_loss = F.binary_cross_entropy_with_logits(", "\n", "#     expert_d,", "\n", "#     torch.ones(expert_d.size()).to(self.device))", "\n", "# policy_loss = F.binary_cross_entropy_with_logits(", "\n", "#     policy_d,", "\n", "#     torch.zeros(policy_d.size()).to(self.device))", "\n", "\n", "expert_loss", "=", "torch", ".", "mean", "(", "expert_d", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "policy_loss", "=", "torch", ".", "mean", "(", "policy_d", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# gail_loss = expert_loss + policy_loss", "\n", "wd", "=", "expert_loss", "-", "policy_loss", "\n", "grad_pen", "=", "self", ".", "compute_grad_pen", "(", "expert_state", ",", "expert_action", ",", "\n", "policy_state", ",", "policy_action", ")", "\n", "\n", "# loss += (gail_loss + grad_pen).item()", "\n", "loss", "+=", "(", "-", "wd", "+", "grad_pen", ")", ".", "item", "(", ")", "\n", "g_loss", "+=", "(", "wd", ")", ".", "item", "(", ")", "\n", "gp", "+=", "(", "grad_pen", ")", ".", "item", "(", ")", "\n", "n", "+=", "1", "\n", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "# (gail_loss + grad_pen).backward()", "\n", "(", "-", "wd", "+", "grad_pen", ")", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "return", "g_loss", "/", "n", ",", "gp", "/", "n", ",", "0.0", ",", "loss", "/", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.update_zm_origin": [[265, 315], ["wdgail.Discriminator.train", "obs[].view", "obs[].view.cpu().detach().numpy", "actions.view.cpu().detach().numpy.view", "actions_batch.cpu().detach().numpy.view.cpu().detach().numpy", "wdgail.Dset", "wdgail.Dset.get_next_batch", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "wdgail.Discriminator.trunk", "expert_buf.get_next_batch", "obsfilt", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "wdgail.Discriminator.trunk", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "wdgail.Discriminator.compute_grad_pen", "wdgail.Discriminator.optimizer.zero_grad", "wdgail.Discriminator.optimizer.step", "actions.view.cpu().detach().numpy.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "gail_loss.item", "wdgail.Discriminator.item", "obs[].view.cpu().detach", "actions_batch.cpu().detach().numpy.view.cpu().detach", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "obs.size", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "obs[].view.cpu", "actions_batch.cpu().detach().numpy.view.cpu", "len", "wdgail.Discriminator.size", "wdgail.Discriminator.size"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.train", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Mujoco_Dset.get_next_batch", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Mujoco_Dset.get_next_batch", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.compute_grad_pen", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.step", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to"], ["", "def", "update_zm_origin", "(", "self", ",", "replay_buf", ",", "expert_buf", ",", "obsfilt", "=", "None", ",", "batch_size", "=", "128", ")", ":", "\n", "        ", "self", ".", "train", "(", ")", "\n", "obs", "=", "replay_buf", ".", "obs", "\n", "obs_batch", "=", "obs", "[", ":", "-", "1", "]", ".", "view", "(", "-", "1", ",", "*", "obs", ".", "size", "(", ")", "[", "2", ":", "]", ")", "\n", "states", "=", "obs_batch", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "# states = np.concatenate(states,axis=1)", "\n", "actions", "=", "replay_buf", ".", "actions", "\n", "actions_batch", "=", "actions", ".", "view", "(", "-", "1", ",", "actions", ".", "size", "(", "-", "1", ")", ")", "\n", "actions", "=", "actions_batch", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "policy_buf", "=", "Dset", "(", "inputs", "=", "states", "[", "0", ":", "len", "(", "actions", ")", "]", ",", "labels", "=", "actions", ",", "randomize", "=", "True", ")", "\n", "\n", "# loss = 0", "\n", "\n", "# Sample replay buffer", "\n", "policy_state", ",", "policy_action", "=", "policy_buf", ".", "get_next_batch", "(", "batch_size", ")", "\n", "policy_state", "=", "torch", ".", "FloatTensor", "(", "policy_state", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "policy_action", "=", "torch", ".", "FloatTensor", "(", "policy_action", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "temp", "=", "[", "policy_state", ",", "policy_action", "]", "\n", "policy_d", "=", "self", ".", "trunk", "(", "torch", ".", "cat", "(", "[", "policy_state", ",", "policy_action", "]", ",", "dim", "=", "1", ")", ")", "\n", "\n", "# Sample expert buffer", "\n", "expert_state", ",", "expert_action", "=", "expert_buf", ".", "get_next_batch", "(", "batch_size", ")", "\n", "expert_state", "=", "obsfilt", "(", "expert_state", ",", "update", "=", "False", ")", "\n", "expert_state", "=", "torch", ".", "FloatTensor", "(", "expert_state", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "expert_action", "=", "torch", ".", "FloatTensor", "(", "expert_action", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "expert_d", "=", "self", ".", "trunk", "(", "torch", ".", "cat", "(", "[", "expert_state", ",", "expert_action", "]", ",", "dim", "=", "1", ")", ")", "\n", "\n", "expert_loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "\n", "expert_d", ",", "\n", "torch", ".", "ones", "(", "expert_d", ".", "size", "(", ")", ")", ".", "to", "(", "self", ".", "device", ")", ")", "\n", "policy_loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "\n", "policy_d", ",", "\n", "torch", ".", "zeros", "(", "policy_d", ".", "size", "(", ")", ")", ".", "to", "(", "self", ".", "device", ")", ")", "\n", "\n", "gail_loss", "=", "expert_loss", "+", "policy_loss", "\n", "grad_pen", "=", "self", ".", "compute_grad_pen", "(", "expert_state", ",", "expert_action", ",", "\n", "policy_state", ",", "policy_action", ")", "\n", "# print(\"gail_loss = %s,    gp=%s\" % (gail_loss.item(), grad_pen.item()))", "\n", "\n", "loss", "=", "(", "gail_loss", "+", "grad_pen", ")", ".", "item", "(", ")", "\n", "# loss = (gail_loss).item()", "\n", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "(", "gail_loss", "+", "grad_pen", ")", ".", "backward", "(", ")", "\n", "# (gail_loss).backward()", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "return", "gail_loss", ".", "item", "(", ")", ",", "grad_pen", ".", "item", "(", ")", ",", "0.0", ",", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.predict_reward": [[316, 349], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "wdgail.Discriminator.eval", "wdgail.Discriminator.trunk", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.sigmoid.exp.clone", "wdgail.Discriminator.ret_rms.update", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "wdgail.Discriminator.returns.cpu().numpy", "numpy.sqrt", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid.exp", "torch.sigmoid.exp", "torch.sigmoid.exp", "torch.sigmoid.exp", "wdgail.Discriminator.returns.cpu"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecNormalize.eval", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update"], ["", "def", "predict_reward", "(", "self", ",", "state", ",", "action", ",", "gamma", ",", "masks", ",", "update_rms", "=", "True", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "d", "=", "self", ".", "trunk", "(", "torch", ".", "cat", "(", "[", "state", ",", "action", "]", ",", "dim", "=", "1", ")", ")", "\n", "if", "self", ".", "reward_type", "==", "0", ":", "\n", "                ", "s", "=", "torch", ".", "exp", "(", "d", ")", "\n", "reward", "=", "s", "\n", "", "elif", "self", ".", "reward_type", "==", "1", ":", "\n", "                ", "s", "=", "torch", ".", "sigmoid", "(", "d", ")", "\n", "reward", "=", "-", "(", "1", "-", "s", ")", ".", "log", "(", ")", "\n", "", "elif", "self", ".", "reward_type", "==", "2", ":", "\n", "                ", "s", "=", "torch", ".", "sigmoid", "(", "d", ")", "\n", "reward", "=", "s", "\n", "", "elif", "self", ".", "reward_type", "==", "3", ":", "\n", "                ", "s", "=", "torch", ".", "sigmoid", "(", "d", ")", "\n", "reward", "=", "s", ".", "exp", "(", ")", "\n", "", "elif", "self", ".", "reward_type", "==", "4", ":", "\n", "                ", "reward", "=", "d", "\n", "", "elif", "self", ".", "reward_type", "==", "5", ":", "\n", "                ", "s", "=", "torch", ".", "sigmoid", "(", "d", ")", "\n", "reward", "=", "s", ".", "log", "(", ")", "-", "(", "1", "-", "s", ")", ".", "log", "(", ")", "\n", "\n", "# s = torch.exp(d)", "\n", "# # reward = s.log() - (1 - s).log()", "\n", "# s = torch.sigmoid(d)", "\n", "# reward = s", "\n", "# # reward = d", "\n", "", "if", "self", ".", "returns", "is", "None", ":", "\n", "                ", "self", ".", "returns", "=", "reward", ".", "clone", "(", ")", "\n", "\n", "", "if", "self", ".", "update_rms", ":", "\n", "                ", "self", ".", "returns", "=", "self", ".", "returns", "*", "masks", "*", "gamma", "+", "reward", "\n", "self", ".", "ret_rms", ".", "update", "(", "self", ".", "returns", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "return", "reward", "/", "np", ".", "sqrt", "(", "self", ".", "ret_rms", ".", "var", "[", "0", "]", "+", "1e-8", ")", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.predict_reward_exp": [[357, 375], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "wdgail.Discriminator.eval", "wdgail.Discriminator.trunk", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "reward.clone", "wdgail.Discriminator.ret_rms.update", "numpy.sqrt", "wdgail.Discriminator.returns.cpu().numpy", "wdgail.Discriminator.returns.cpu"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecNormalize.eval", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update"], ["# return torch.clamp(reward,self.cliprew_down, self.cliprew_up)", "\n", "# return reward", "\n", "\n", "", "", "", "def", "predict_reward_exp", "(", "self", ",", "state", ",", "action", ",", "gamma", ",", "masks", ",", "update_rms", "=", "True", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "d", "=", "self", ".", "trunk", "(", "torch", ".", "cat", "(", "[", "state", ",", "action", "]", ",", "dim", "=", "1", ")", ")", "\n", "s", "=", "torch", ".", "exp", "(", "d", ")", "\n", "# s = torch.sigmoid(d)", "\n", "# reward = s.log() - (1 - s).log()", "\n", "reward", "=", "s", "\n", "# reward = d", "\n", "if", "self", ".", "returns", "is", "None", ":", "\n", "                ", "self", ".", "returns", "=", "reward", ".", "clone", "(", ")", "\n", "\n", "", "if", "update_rms", ":", "\n", "                ", "self", ".", "returns", "=", "self", ".", "returns", "*", "masks", "*", "gamma", "+", "reward", "\n", "self", ".", "ret_rms", ".", "update", "(", "self", ".", "returns", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "# ttt = torch.clamp(reward / np.sqrt(self.ret_rms.var[0] + 1e-8), self.cliprew_down, self.cliprew_up)", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.predict_reward_t1": [[378, 398], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "wdgail.Discriminator.eval", "wdgail.Discriminator.trunk", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "reward.clone", "wdgail.Discriminator.ret_rms.update", "wdgail.Discriminator.returns.cpu().numpy", "wdgail.Discriminator.returns.cpu"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecNormalize.eval", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update"], ["# return torch.clamp(reward,self.cliprew_down, self.cliprew_up)", "\n", "# return reward", "\n", "\n", "", "", "def", "predict_reward_t1", "(", "self", ",", "state", ",", "action", ",", "gamma", ",", "masks", ",", "update_rms", "=", "True", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "d", "=", "self", ".", "trunk", "(", "torch", ".", "cat", "(", "[", "state", ",", "action", "]", ",", "dim", "=", "1", ")", ")", "\n", "# s = torch.exp(d)", "\n", "s", "=", "torch", ".", "sigmoid", "(", "d", ")", "\n", "# reward = s.log() - (1 - s).log()", "\n", "reward", "=", "-", "(", "1", "-", "s", ")", ".", "log", "(", ")", "\n", "# reward = d", "\n", "if", "self", ".", "returns", "is", "None", ":", "\n", "                ", "self", ".", "returns", "=", "reward", ".", "clone", "(", ")", "\n", "\n", "", "if", "update_rms", ":", "\n", "                ", "self", ".", "returns", "=", "self", ".", "returns", "*", "masks", "*", "gamma", "+", "reward", "\n", "self", ".", "ret_rms", ".", "update", "(", "self", ".", "returns", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "# ttt = torch.clamp(reward / np.sqrt(self.ret_rms.var[0] + 1e-8), self.cliprew_down, self.cliprew_up)", "\n", "# return torch.clamp(reward / np.sqrt(self.ret_rms.var[0] + 1e-8), self.cliprew_down, self.cliprew_up)", "\n", "# return reward / np.sqrt(self.ret_rms.var[0] + 1e-8)", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.Discriminator.predict_reward_origin": [[399, 420], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "wdgail.Discriminator.eval", "wdgail.Discriminator.trunk", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "reward.clone", "wdgail.Discriminator.ret_rms.update", "wdgail.Discriminator.returns.cpu().numpy", "wdgail.Discriminator.returns.cpu"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.VecNormalize.eval", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update"], ["# return torch.clamp(reward,self.cliprew_down, self.cliprew_up)", "\n", "", "return", "reward", "\n", "\n", "", "", "def", "predict_reward_origin", "(", "self", ",", "state", ",", "action", ",", "gamma", ",", "masks", ",", "update_rms", "=", "True", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "d", "=", "self", ".", "trunk", "(", "torch", ".", "cat", "(", "[", "state", ",", "action", "]", ",", "dim", "=", "1", ")", ")", "\n", "s", "=", "torch", ".", "exp", "(", "d", ")", "\n", "# s = torch.sigmoid(d)", "\n", "# reward = s.log() - (1 - s).log()", "\n", "# reward = - (1 - s).log()", "\n", "reward", "=", "s", "\n", "# reward = d", "\n", "if", "self", ".", "returns", "is", "None", ":", "\n", "                ", "self", ".", "returns", "=", "reward", ".", "clone", "(", ")", "\n", "\n", "", "if", "update_rms", ":", "\n", "                ", "self", ".", "returns", "=", "self", ".", "returns", "*", "masks", "*", "gamma", "+", "reward", "\n", "self", ".", "ret_rms", ".", "update", "(", "self", ".", "returns", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "# ttt = torch.clamp(reward / np.sqrt(self.ret_rms.var[0] + 1e-8), self.cliprew_down, self.cliprew_up)", "\n", "# return torch.clamp(reward / np.sqrt(self.ret_rms.var[0] + 1e-8), self.cliprew_down, self.cliprew_up)", "\n", "# return reward / np.sqrt(self.ret_rms.var[0] + 1e-8)", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.ExpertDataset.__init__": [[422, 464], ["torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.randint().long", "torch.load.items", "torch.load.items", "torch.load.items", "torch.load.items", "wdgail.ExpertDataset.trajectories[].sum().item", "range", "all_trajectories[].size", "wdgail.ExpertDataset.get_idx.append", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "range", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "wdgail.ExpertDataset.trajectories[].sum", "[].item", "[].item", "samples.append"], "methods", ["None"], ["", "return", "reward", "\n", "\n", "", "", "", "class", "ExpertDataset", "(", "torch", ".", "utils", ".", "data", ".", "Dataset", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "file_name", ",", "num_trajectories", "=", "4", ",", "subsample_frequency", "=", "20", ")", ":", "\n", "        ", "all_trajectories", "=", "torch", ".", "load", "(", "file_name", ")", "\n", "\n", "perm", "=", "torch", ".", "randperm", "(", "all_trajectories", "[", "'states'", "]", ".", "size", "(", "0", ")", ")", "\n", "idx", "=", "perm", "[", ":", "num_trajectories", "]", "\n", "\n", "self", ".", "trajectories", "=", "{", "}", "\n", "\n", "# See https://github.com/pytorch/pytorch/issues/14886", "\n", "# .long() for fixing bug in torch v0.4.1", "\n", "start_idx", "=", "torch", ".", "randint", "(", "0", ",", "subsample_frequency", ",", "size", "=", "(", "num_trajectories", ",", ")", ")", ".", "long", "(", ")", "\n", "\n", "for", "k", ",", "v", "in", "all_trajectories", ".", "items", "(", ")", ":", "\n", "            ", "data", "=", "v", "[", "idx", "]", "\n", "\n", "if", "k", "!=", "'lengths'", ":", "\n", "                ", "samples", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_trajectories", ")", ":", "\n", "                    ", "samples", ".", "append", "(", "data", "[", "i", ",", "start_idx", "[", "i", "]", ":", ":", "subsample_frequency", "]", ")", "\n", "", "self", ".", "trajectories", "[", "k", "]", "=", "torch", ".", "stack", "(", "samples", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "trajectories", "[", "k", "]", "=", "data", "//", "subsample_frequency", "\n", "\n", "", "", "self", ".", "i2traj_idx", "=", "{", "}", "\n", "self", ".", "i2i", "=", "{", "}", "\n", "\n", "self", ".", "length", "=", "self", ".", "trajectories", "[", "'lengths'", "]", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "traj_idx", "=", "0", "\n", "i", "=", "0", "\n", "\n", "self", ".", "get_idx", "=", "[", "]", "\n", "\n", "for", "j", "in", "range", "(", "self", ".", "length", ")", ":", "\n", "\n", "            ", "while", "self", ".", "trajectories", "[", "'lengths'", "]", "[", "traj_idx", "]", ".", "item", "(", ")", "<=", "i", ":", "\n", "                ", "i", "-=", "self", ".", "trajectories", "[", "'lengths'", "]", "[", "traj_idx", "]", ".", "item", "(", ")", "\n", "traj_idx", "+=", "1", "\n", "\n", "", "self", ".", "get_idx", ".", "append", "(", "(", "traj_idx", ",", "i", ")", ")", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.ExpertDataset.__len__": [[465, 467], ["None"], "methods", ["None"], ["\n", "i", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.wdgail.ExpertDataset.__getitem__": [[468, 473], ["None"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "length", "\n", "\n", "", "def", "__getitem__", "(", "self", ",", "j", ")", ":", "\n", "        ", "traj_idx", ",", "i", "=", "self", ".", "get_idx", "[", "j", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.ppo.PPO.__init__": [[8, 33], ["torch.Adam", "torch.Adam", "torch.Adam", "torch.Adam", "actor_critic.parameters"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "\n", "actor_critic", ",", "\n", "clip_param", ",", "\n", "ppo_epoch", ",", "\n", "num_mini_batch", ",", "\n", "value_loss_coef", ",", "\n", "entropy_coef", ",", "\n", "lr", "=", "None", ",", "\n", "eps", "=", "None", ",", "\n", "max_grad_norm", "=", "None", ",", "\n", "use_clipped_value_loss", "=", "True", ")", ":", "\n", "\n", "        ", "self", ".", "actor_critic", "=", "actor_critic", "\n", "\n", "self", ".", "clip_param", "=", "clip_param", "\n", "self", ".", "ppo_epoch", "=", "ppo_epoch", "\n", "self", ".", "num_mini_batch", "=", "num_mini_batch", "\n", "\n", "self", ".", "value_loss_coef", "=", "value_loss_coef", "\n", "self", ".", "entropy_coef", "=", "entropy_coef", "\n", "\n", "self", ".", "max_grad_norm", "=", "max_grad_norm", "\n", "self", ".", "use_clipped_value_loss", "=", "use_clipped_value_loss", "\n", "\n", "self", ".", "optimizer", "=", "optim", ".", "Adam", "(", "actor_critic", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "eps", "=", "eps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.ppo.PPO.update": [[34, 98], ["range", "advantages.mean", "advantages.std", "rollouts.recurrent_generator", "rollouts.feed_forward_generator", "ppo.PPO.actor_critic.evaluate_actions", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "ppo.PPO.optimizer.zero_grad", "torch.utils.clip_grad_norm_", "torch.utils.clip_grad_norm_", "torch.utils.clip_grad_norm_", "torch.utils.clip_grad_norm_", "ppo.PPO.optimizer.step", "value_loss.item", "action_loss.item", "dist_entropy.item", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "ppo.PPO.actor_critic.parameters", "torch.max().mean", "torch.max().mean", "torch.max().mean", "torch.max().mean", "torch.max().mean", "torch.max().mean", "torch.max().mean", "torch.max().mean", "torch.max().mean", "torch.max().mean", "torch.max().mean", "torch.max().mean", "torch.max().mean", "torch.max().mean", "torch.max().mean", "torch.max().mean", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.recurrent_generator", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.feed_forward_generator", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.model.Policy.evaluate_actions", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.step"], ["", "def", "update", "(", "self", ",", "rollouts", ")", ":", "\n", "        ", "advantages", "=", "rollouts", ".", "returns", "[", ":", "-", "1", "]", "-", "rollouts", ".", "value_preds", "[", ":", "-", "1", "]", "\n", "advantages", "=", "(", "advantages", "-", "advantages", ".", "mean", "(", ")", ")", "/", "(", "\n", "advantages", ".", "std", "(", ")", "+", "1e-5", ")", "\n", "\n", "value_loss_epoch", "=", "0", "\n", "action_loss_epoch", "=", "0", "\n", "dist_entropy_epoch", "=", "0", "\n", "\n", "for", "e", "in", "range", "(", "self", ".", "ppo_epoch", ")", ":", "\n", "            ", "if", "self", ".", "actor_critic", ".", "is_recurrent", ":", "\n", "                ", "data_generator", "=", "rollouts", ".", "recurrent_generator", "(", "\n", "advantages", ",", "self", ".", "num_mini_batch", ")", "\n", "", "else", ":", "\n", "                ", "data_generator", "=", "rollouts", ".", "feed_forward_generator", "(", "\n", "advantages", ",", "self", ".", "num_mini_batch", ")", "\n", "\n", "", "for", "sample", "in", "data_generator", ":", "\n", "                ", "obs_batch", ",", "recurrent_hidden_states_batch", ",", "actions_batch", ",", "value_preds_batch", ",", "return_batch", ",", "masks_batch", ",", "old_action_log_probs_batch", ",", "adv_targ", "=", "sample", "\n", "\n", "# Reshape to do in a single forward pass for all steps", "\n", "values", ",", "action_log_probs", ",", "dist_entropy", ",", "_", "=", "self", ".", "actor_critic", ".", "evaluate_actions", "(", "\n", "obs_batch", ",", "recurrent_hidden_states_batch", ",", "masks_batch", ",", "\n", "actions_batch", ")", "\n", "\n", "ratio", "=", "torch", ".", "exp", "(", "action_log_probs", "-", "\n", "old_action_log_probs_batch", ")", "\n", "surr1", "=", "ratio", "*", "adv_targ", "\n", "surr2", "=", "torch", ".", "clamp", "(", "ratio", ",", "1.0", "-", "self", ".", "clip_param", ",", "\n", "1.0", "+", "self", ".", "clip_param", ")", "*", "adv_targ", "\n", "action_loss", "=", "-", "torch", ".", "min", "(", "surr1", ",", "surr2", ")", ".", "mean", "(", ")", "\n", "\n", "if", "self", ".", "use_clipped_value_loss", ":", "\n", "                    ", "value_pred_clipped", "=", "value_preds_batch", "+", "(", "values", "-", "value_preds_batch", ")", ".", "clamp", "(", "-", "self", ".", "clip_param", ",", "self", ".", "clip_param", ")", "\n", "value_losses", "=", "(", "values", "-", "return_batch", ")", ".", "pow", "(", "2", ")", "\n", "value_losses_clipped", "=", "(", "\n", "value_pred_clipped", "-", "return_batch", ")", ".", "pow", "(", "2", ")", "\n", "value_loss", "=", "0.5", "*", "torch", ".", "max", "(", "value_losses", ",", "\n", "value_losses_clipped", ")", ".", "mean", "(", ")", "\n", "", "else", ":", "\n", "                    ", "value_loss", "=", "0.5", "*", "(", "return_batch", "-", "values", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "\n", "", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "(", "value_loss", "*", "self", ".", "value_loss_coef", "+", "action_loss", "-", "\n", "dist_entropy", "*", "self", ".", "entropy_coef", ")", ".", "backward", "(", ")", "\n", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "actor_critic", ".", "parameters", "(", ")", ",", "\n", "self", ".", "max_grad_norm", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "value_loss_epoch", "+=", "value_loss", ".", "item", "(", ")", "\n", "action_loss_epoch", "+=", "action_loss", ".", "item", "(", ")", "\n", "dist_entropy_epoch", "+=", "dist_entropy", ".", "item", "(", ")", "\n", "\n", "", "", "num_updates", "=", "self", ".", "ppo_epoch", "*", "self", ".", "num_mini_batch", "\n", "\n", "value_loss_epoch", "/=", "num_updates", "\n", "action_loss_epoch", "/=", "num_updates", "\n", "dist_entropy_epoch", "/=", "num_updates", "\n", "total_loss", "=", "value_loss_epoch", "+", "action_loss_epoch", "+", "dist_entropy_epoch", "\n", "\n", "return", "value_loss_epoch", ",", "action_loss_epoch", ",", "dist_entropy_epoch", ",", "total_loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Dset.__init__": [[13, 20], ["len", "mujoco_dset_zm_base.Dset.init_pointer", "len", "len"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Dset.init_pointer"], ["    ", "def", "__init__", "(", "self", ",", "inputs", ",", "labels", ",", "randomize", ")", ":", "\n", "        ", "self", ".", "inputs", "=", "inputs", "\n", "self", ".", "labels", "=", "labels", "\n", "assert", "len", "(", "self", ".", "inputs", ")", "==", "len", "(", "self", ".", "labels", ")", "\n", "self", ".", "randomize", "=", "randomize", "\n", "self", ".", "num_pairs", "=", "len", "(", "inputs", ")", "\n", "self", ".", "init_pointer", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Dset.init_pointer": [[21, 28], ["numpy.arange", "numpy.random.shuffle"], "methods", ["None"], ["", "def", "init_pointer", "(", "self", ")", ":", "\n", "        ", "self", ".", "pointer", "=", "0", "\n", "if", "self", ".", "randomize", ":", "\n", "            ", "idx", "=", "np", ".", "arange", "(", "self", ".", "num_pairs", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "idx", ")", "\n", "self", ".", "inputs", "=", "self", ".", "inputs", "[", "idx", ",", ":", "]", "\n", "self", ".", "labels", "=", "self", ".", "labels", "[", "idx", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Dset.get_next_batch": [[29, 40], ["mujoco_dset_zm_base.Dset.init_pointer"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Dset.init_pointer"], ["", "", "def", "get_next_batch", "(", "self", ",", "batch_size", ")", ":", "\n", "# if batch_size is negative -> return all", "\n", "        ", "if", "batch_size", "<", "0", ":", "\n", "            ", "return", "self", ".", "inputs", ",", "self", ".", "labels", "\n", "", "if", "self", ".", "pointer", "+", "batch_size", ">=", "self", ".", "num_pairs", ":", "\n", "            ", "self", ".", "init_pointer", "(", ")", "\n", "", "end", "=", "self", ".", "pointer", "+", "batch_size", "\n", "inputs", "=", "self", ".", "inputs", "[", "self", ".", "pointer", ":", "end", ",", ":", "]", "\n", "labels", "=", "self", ".", "labels", "[", "self", ".", "pointer", ":", "end", ",", ":", "]", "\n", "self", ".", "pointer", "=", "end", "\n", "return", "inputs", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Mujoco_Dset.__init__": [[43, 93], ["numpy.load", "numpy.argsort", "mujoco_dset_zm_base.Mujoco_Dset.subsample", "numpy.array", "numpy.array", "numpy.std", "min", "len", "mujoco_dset_zm_base.Dset", "mujoco_dset_zm_base.Dset", "mujoco_dset_zm_base.Dset", "mujoco_dset_zm_base.Mujoco_Dset.log_info", "len", "numpy.zeros", "zip", "mujoco_dset_zm_base.Mujoco_Dset.__init__.flatten"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Mujoco_Dset.subsample", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Mujoco_Dset.log_info"], ["    ", "def", "__init__", "(", "self", ",", "expert_path", ",", "train_fraction", "=", "0.7", ",", "traj_limitation", "=", "-", "1", ",", "subsample_frequency", "=", "20", ",", "randomize", "=", "True", ")", ":", "\n", "        ", "traj_data", "=", "np", ".", "load", "(", "expert_path", ",", "allow_pickle", "=", "True", ")", "\n", "\n", "rets_", "=", "traj_data", "[", "'ep_rets'", "]", "\n", "index", "=", "np", ".", "argsort", "(", "rets_", ")", "\n", "# index_ = np.random.choice(len(rets_),traj_limitation)", "\n", "\n", "# aa = len(traj_data['obs'])", "\n", "if", "traj_limitation", "<", "0", ":", "\n", "            ", "traj_limitation", "=", "len", "(", "traj_data", "[", "'obs'", "]", ")", "\n", "# obs = traj_data['obs'][:traj_limitation]", "\n", "# acs = traj_data['acs'][:traj_limitation]", "\n", "", "select_bset", "=", "False", "\n", "if", "select_bset", ":", "\n", "            ", "sel_idx", "=", "index", "[", ":", ":", "-", "1", "]", "[", ":", "traj_limitation", "]", "\n", "", "else", ":", "\n", "            ", "sel_idx", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "rets_", ")", ",", "traj_limitation", ")", "\n", "# sel_idx = [0: traj_limitation]", "\n", "\n", "# obs = traj_data['obs'][sel_idx]", "\n", "# acs = traj_data['acs'][sel_idx]", "\n", "\n", "", "obs", "=", "traj_data", "[", "'obs'", "]", "[", ":", "traj_limitation", "]", "\n", "acs", "=", "traj_data", "[", "'acs'", "]", "[", ":", "traj_limitation", "]", "\n", "\n", "obs", ",", "acs", "=", "self", ".", "subsample", "(", "obs", ",", "acs", ",", "subsample_frequency", ")", "\n", "\n", "def", "flatten", "(", "x", ")", ":", "\n", "# x.shape = (E,), or (E, L, D)", "\n", "            ", "_", ",", "size", "=", "x", "[", "0", "]", ".", "shape", "\n", "episode_length", "=", "[", "len", "(", "i", ")", "for", "i", "in", "x", "]", "\n", "y", "=", "np", ".", "zeros", "(", "(", "sum", "(", "episode_length", ")", ",", "size", ")", ")", "\n", "start_idx", "=", "0", "\n", "for", "l", ",", "x_i", "in", "zip", "(", "episode_length", ",", "x", ")", ":", "\n", "                ", "y", "[", "start_idx", ":", "(", "start_idx", "+", "l", ")", "]", "=", "x_i", "\n", "start_idx", "+=", "l", "\n", "return", "y", "\n", "", "", "self", ".", "obs", "=", "np", ".", "array", "(", "flatten", "(", "obs", ")", ")", "\n", "self", ".", "acs", "=", "np", ".", "array", "(", "flatten", "(", "acs", ")", ")", "\n", "self", ".", "rets", "=", "traj_data", "[", "'ep_rets'", "]", "[", ":", "traj_limitation", "]", "\n", "# self.rets = traj_data['ep_rets'][sel_idx]", "\n", "\n", "self", ".", "avg_ret", "=", "sum", "(", "self", ".", "rets", ")", "/", "len", "(", "self", ".", "rets", ")", "\n", "self", ".", "std_ret", "=", "np", ".", "std", "(", "np", ".", "array", "(", "self", ".", "rets", ")", ")", "\n", "if", "len", "(", "self", ".", "acs", ")", ">", "2", ":", "\n", "            ", "self", ".", "acs", "=", "np", ".", "squeeze", "(", "self", ".", "acs", ")", "\n", "", "assert", "len", "(", "self", ".", "obs", ")", "==", "len", "(", "self", ".", "acs", ")", "\n", "self", ".", "num_traj", "=", "min", "(", "traj_limitation", ",", "len", "(", "traj_data", "[", "'obs'", "]", ")", ")", "\n", "self", ".", "num_transition", "=", "len", "(", "self", ".", "obs", ")", "\n", "self", ".", "randomize", "=", "randomize", "\n", "self", ".", "dset", "=", "Dset", "(", "self", ".", "obs", ",", "self", ".", "acs", ",", "self", ".", "randomize", ")", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Mujoco_Dset.subsample": [[94, 116], ["len", "numpy.random.randint", "range", "numpy.array", "numpy.array", "numpy.array.append", "numpy.array.append"], "methods", ["None"], ["# for behavior cloning", "\n", "self", ".", "train_set", "=", "Dset", "(", "self", ".", "obs", "[", ":", "int", "(", "self", ".", "num_transition", "*", "train_fraction", ")", ",", ":", "]", ",", "\n", "self", ".", "acs", "[", ":", "int", "(", "self", ".", "num_transition", "*", "train_fraction", ")", ",", ":", "]", ",", "\n", "self", ".", "randomize", ")", "\n", "self", ".", "val_set", "=", "Dset", "(", "self", ".", "obs", "[", "int", "(", "self", ".", "num_transition", "*", "train_fraction", ")", ":", ",", ":", "]", ",", "\n", "self", ".", "acs", "[", "int", "(", "self", ".", "num_transition", "*", "train_fraction", ")", ":", ",", ":", "]", ",", "\n", "self", ".", "randomize", ")", "\n", "self", ".", "log_info", "(", ")", "\n", "\n", "", "def", "subsample", "(", "self", ",", "obs", ",", "acts", ",", "subsample_frequency", "=", "20", ")", ":", "\n", "\n", "        ", "num_trajectories", "=", "len", "(", "obs", ")", "\n", "start_idx", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "subsample_frequency", ",", "num_trajectories", ")", "\n", "t_obs", "=", "[", "]", "\n", "t_acts", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "num_trajectories", ")", ":", "\n", "            ", "obs_i", "=", "obs", "[", "i", "]", "\n", "acts_i", "=", "acts", "[", "i", "]", "\n", "\n", "t_obs", ".", "append", "(", "obs_i", "[", "start_idx", "[", "i", "]", ":", ":", "subsample_frequency", "]", ")", "\n", "t_acts", ".", "append", "(", "acts_i", "[", "start_idx", "[", "i", "]", ":", ":", "subsample_frequency", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Mujoco_Dset.log_info": [[118, 123], ["baselines.logger.log", "baselines.logger.log", "baselines.logger.log", "baselines.logger.log"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.log", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.log", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.log", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.log"], ["# t_acts.append(acts[i, start_idx[i]::subsample_frequency])", "\n", "\n", "", "t_obs", "=", "np", ".", "array", "(", "t_obs", ")", "\n", "t_acts", "=", "np", ".", "array", "(", "t_acts", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Mujoco_Dset.get_next_batch": [[124, 133], ["mujoco_dset_zm_base.Mujoco_Dset.dset.get_next_batch", "mujoco_dset_zm_base.Mujoco_Dset.train_set.get_next_batch", "mujoco_dset_zm_base.Mujoco_Dset.val_set.get_next_batch"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Mujoco_Dset.get_next_batch", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Mujoco_Dset.get_next_batch", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Mujoco_Dset.get_next_batch"], ["return", "t_obs", ",", "t_acts", "\n", "\n", "\n", "", "def", "log_info", "(", "self", ")", ":", "\n", "        ", "logger", ".", "log", "(", "\"Total trajectorues: %d\"", "%", "self", ".", "num_traj", ")", "\n", "logger", ".", "log", "(", "\"Total transitions: %d\"", "%", "self", ".", "num_transition", ")", "\n", "logger", ".", "log", "(", "\"Average returns: %f\"", "%", "self", ".", "avg_ret", ")", "\n", "logger", ".", "log", "(", "\"Std for returns: %f\"", "%", "self", ".", "std_ret", ")", "\n", "\n", "", "def", "get_next_batch", "(", "self", ",", "batch_size", ",", "split", "=", "None", ")", ":", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Mujoco_Dset.plot": [[135, 140], ["plt.hist", "plt.savefig", "plt.close"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close"], ["            ", "return", "self", ".", "dset", ".", "get_next_batch", "(", "batch_size", ")", "\n", "", "elif", "split", "==", "'train'", ":", "\n", "            ", "return", "self", ".", "train_set", ".", "get_next_batch", "(", "batch_size", ")", "\n", "", "elif", "split", "==", "'val'", ":", "\n", "            ", "return", "self", ".", "val_set", ".", "get_next_batch", "(", "batch_size", ")", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.test": [[142, 146], ["mujoco_dset_zm_base.Mujoco_Dset", "mujoco_dset_zm_base.Mujoco_Dset.plot"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.algo.mujoco_dset_zm_base.Mujoco_Dset.plot"], ["\n", "\n", "", "", "def", "plot", "(", "self", ")", ":", "\n", "        ", "import", "matplotlib", ".", "pyplot", "as", "plt", "\n", "plt", ".", "hist", "(", "self", ".", "rets", ")", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.gail_experts.convert_to_pytorch.main": [[10, 54], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "torch.save", "h5py.File", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().long", "os.path.splitext", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "function", ["None"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "'Converts expert trajectories from h5 to pt format.'", ")", "\n", "# parser.add_argument(", "\n", "#     '--h5-file',", "\n", "#     default='trajs_halfcheetah.h5',", "\n", "#     help='input h5 file',", "\n", "#     type=str)", "\n", "parser", ".", "add_argument", "(", "\n", "'--h5-file'", ",", "\n", "default", "=", "'trajs_walker.h5'", ",", "\n", "help", "=", "'input h5 file'", ",", "\n", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--pt-file'", ",", "\n", "default", "=", "None", ",", "\n", "help", "=", "'output pt file, by default replaces file extension with pt'", ",", "\n", "type", "=", "str", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "pt_file", "is", "None", ":", "\n", "        ", "args", ".", "pt_file", "=", "os", ".", "path", ".", "splitext", "(", "args", ".", "h5_file", ")", "[", "0", "]", "+", "'.pt'", "\n", "\n", "", "with", "h5py", ".", "File", "(", "args", ".", "h5_file", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "dataset_size", "=", "f", "[", "'obs_B_T_Do'", "]", ".", "shape", "[", "0", "]", "# full dataset size", "\n", "\n", "states", "=", "f", "[", "'obs_B_T_Do'", "]", "[", ":", "dataset_size", ",", "...", "]", "[", "...", "]", "\n", "actions", "=", "f", "[", "'a_B_T_Da'", "]", "[", ":", "dataset_size", ",", "...", "]", "[", "...", "]", "\n", "rewards", "=", "f", "[", "'r_B_T'", "]", "[", ":", "dataset_size", ",", "...", "]", "[", "...", "]", "\n", "lens", "=", "f", "[", "'len_B'", "]", "[", ":", "dataset_size", ",", "...", "]", "[", "...", "]", "\n", "\n", "states", "=", "torch", ".", "from_numpy", "(", "states", ")", ".", "float", "(", ")", "\n", "actions", "=", "torch", ".", "from_numpy", "(", "actions", ")", ".", "float", "(", ")", "\n", "rewards", "=", "torch", ".", "from_numpy", "(", "rewards", ")", ".", "float", "(", ")", "\n", "lens", "=", "torch", ".", "from_numpy", "(", "lens", ")", ".", "long", "(", ")", "\n", "\n", "", "data", "=", "{", "\n", "'states'", ":", "states", ",", "\n", "'actions'", ":", "actions", ",", "\n", "'rewards'", ":", "rewards", ",", "\n", "'lengths'", ":", "lens", "\n", "}", "\n", "\n", "torch", ".", "save", "(", "data", ",", "args", ".", "pt_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoidstandup.argsparser": [[17, 74], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argsparser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\"WDAIL\"", ")", "\n", "parser", ".", "add_argument", "(", "'--env_name'", ",", "help", "=", "'environment ID'", ",", "default", "=", "'HumanoidStandup-v1'", ")", "\n", "parser", ".", "add_argument", "(", "'--algo'", ",", "help", "=", "'algorithm ID'", ",", "default", "=", "'WDAIL'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-dir'", ",", "default", "=", "'/tmp/gym/'", ",", "help", "=", "'directory to save agent logs (default: /tmp/gym)'", ")", "\n", "# general", "\n", "parser", ".", "add_argument", "(", "'--total_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--num_env_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluate_every'", ",", "help", "=", "'evaluate every'", ",", "type", "=", "int", ",", "default", "=", "2e10", ")", "\n", "parser", ".", "add_argument", "(", "'--save_condition'", ",", "help", "=", "'save_condition'", ",", "type", "=", "int", ",", "default", "=", "1000", ")", "\n", "parser", ".", "add_argument", "(", "'--num_model'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--use_device'", ",", "help", "=", "'use_device'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "help", "=", "'seed'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--use_linear_lr_decay'", ",", "help", "=", "'use linear lr decay'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--recurrent-policy'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'use a recurrent policy'", ")", "\n", "\n", "#ppo", "\n", "parser", ".", "add_argument", "(", "'--num_processes'", ",", "help", "=", "'num_processes'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--num-steps'", ",", "help", "=", "'num-steps'", ",", "type", "=", "int", ",", "default", "=", "2048", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "help", "=", "'learning rate'", ",", "type", "=", "float", ",", "default", "=", "3e-4", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "help", "=", "'batch size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_epoch'", ",", "help", "=", "'ppo epoch num'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--hidden_size'", ",", "help", "=", "'hidden size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_entcoeff'", ",", "help", "=", "'entropy coefficiency of policy'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ")", "#default=1e-3", "\n", "parser", ".", "add_argument", "(", "'--ppo_obs_norm'", ",", "help", "=", "'ppo_vec_norm'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--num-mini-batch'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "'number of batches for ppo (default: 32)'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-param'", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "help", "=", "'ppo clip parameter (default: 0.2)'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps'", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "help", "=", "'RMSprop optimizer epsilon (default: 1e-5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "'RMSprop optimizer apha (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "'discount factor for rewards (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "'--use-gae'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'use generalized advantage estimation'", ")", "\n", "parser", ".", "add_argument", "(", "'--gae-lambda'", ",", "type", "=", "float", ",", "default", "=", "0.95", ",", "help", "=", "'gae lambda parameter (default: 0.95)'", ")", "\n", "parser", ".", "add_argument", "(", "'--entropy-coef'", ",", "type", "=", "float", ",", "default", "=", "0.00", ",", "help", "=", "'entropy term coefficient (default: 0.01)'", ")", "\n", "parser", ".", "add_argument", "(", "'--value-loss-coef'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'value loss coefficient (default: 0.5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-grad-norm'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'max norm of gradients (default: 0.5)'", ")", "\n", "\n", "# #gail", "\n", "parser", ".", "add_argument", "(", "'--gail'", ",", "help", "=", "'if gail'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--expert_path'", ",", "help", "=", "'trajs path'", ",", "type", "=", "str", ",", "default", "=", "'../data/baseline/deterministic.trpo.HumanoidStandup.0.00.npz'", ")", "\n", "# parser.add_argument('--expert_path', help='trajs path', type=str, default='../data/ikostirkov/trajs_ant.h5')", "\n", "parser", ".", "add_argument", "(", "'--gail-experts-dir'", ",", "default", "=", "'./gail_experts'", ",", "help", "=", "'directory that contains expert demonstrations for gail'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_batch_size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'gail batch size (default: 128)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_thre'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_pre_epoch'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "1000", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_epoch'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "'--num_trajs'", ",", "help", "=", "'num trajs'", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "'--subsample_frequency'", ",", "help", "=", "'num trajs'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--adversary_entcoeff'", ",", "help", "=", "'entropy coefficiency of discriminator'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ")", "\n", "parser", ".", "add_argument", "(", "'--use-proper-time-limits'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'compute returns taking into account time limits'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-interval'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'log interval, one log per n updates (default: 10)'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--reward_type'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'0,1,2,3,4'", ")", "\n", "parser", ".", "add_argument", "(", "'--update_rms'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "'False or True'", ")", "\n", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoidstandup.train": [[75, 177], ["np.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "os.path.expanduser", "utils.cleanup_log_dir", "utils.cleanup_log_dir", "torch.set_num_threads", "torch.device", "make_vec_envs", "Policy", "Policy.to", "PPO", "Discriminator", "RolloutStorage", "Mujoco_Dset", "gailLearning_mujoco_BL", "torch.cuda.is_available", "str"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.make_vec_envs", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.gailLearning_mujoco_BL"], ["", "def", "train", "(", "args", ")", ":", "\n", "\n", "# from ppo_gail_iko.algo.ppo4multienvs import PPO, ReplayBuffer", "\n", "    ", "from", "ppo_wdail_BL", ".", "algo", ".", "ppo", "import", "PPO", "\n", "from", "ppo_wdail_BL", ".", "tools", ".", "storage", "import", "RolloutStorage", "\n", "from", "ppo_wdail_BL", ".", "tools", ".", "model", "import", "Policy", "\n", "\n", "from", "ppo_wdail_BL", ".", "algo", ".", "wdgail", "import", "Discriminator", ",", "ExpertDataset", "\n", "from", "ppo_wdail_BL", ".", "algo", ".", "mujoco_dset_zm_base", "import", "Mujoco_Dset", "\n", "\n", "from", "ppo_wdail_BL", ".", "tools", ".", "learn", "import", "gailLearning_mujoco_origin", ",", "gailLearning_mujoco_BL", "\n", "from", "ppo_wdail_BL", ".", "tools", ".", "envs", "import", "make_vec_envs", "\n", "\n", "from", "ppo_wdail_BL", ".", "tools", "import", "utli", "\n", "from", "ppo_wdail_BL", ".", "tools", "import", "utils", "\n", "\n", "from", "collections", "import", "deque", "\n", "import", "time", "\n", "import", "numpy", "as", "np", "\n", "\n", "\n", "# from nets.network import ActorCritic_mujoco as ActorCritic", "\n", "cl_args", "=", "args", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "# if args.cuda and torch.cuda.is_available() and args.cuda_deterministic:", "\n", "#     torch.backends.cudnn.benchmark = False", "\n", "#     torch.backends.cudnn.deterministic = True", "\n", "\n", "log_dir", "=", "os", ".", "path", ".", "expanduser", "(", "args", ".", "log_dir", ")", "\n", "eval_log_dir", "=", "log_dir", "+", "\"_eval\"", "\n", "utils", ".", "cleanup_log_dir", "(", "log_dir", ")", "\n", "utils", ".", "cleanup_log_dir", "(", "eval_log_dir", ")", "\n", "\n", "torch", ".", "set_num_threads", "(", "1", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "'cuda:'", "+", "str", "(", "cl_args", ".", "cuda", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "# device = torch.device('cpu')", "\n", "\n", "envs", "=", "make_vec_envs", "(", "args", ".", "env_name", ",", "args", ".", "seed", ",", "args", ".", "num_processes", ",", "\n", "args", ".", "gamma", ",", "args", ".", "log_dir", ",", "device", ",", "False", ")", "\n", "\n", "# envs_eval = make_vec_envs(args.env_name, args.seed, args.num_processes,", "\n", "#                      args.gamma, args.log_dir, device, False)", "\n", "envs_eval", "=", "[", "]", "\n", "\n", "# network", "\n", "actor_critic", "=", "Policy", "(", "\n", "envs", ".", "observation_space", ".", "shape", ",", "\n", "envs", ".", "action_space", ",", "\n", "base_kwargs", "=", "{", "'recurrent'", ":", "args", ".", "recurrent_policy", "}", ")", "\n", "actor_critic", ".", "to", "(", "device", ")", "\n", "\n", "agent", "=", "PPO", "(", "\n", "actor_critic", ",", "\n", "args", ".", "clip_param", ",", "\n", "args", ".", "ppo_epoch", ",", "\n", "args", ".", "num_mini_batch", ",", "\n", "args", ".", "value_loss_coef", ",", "\n", "args", ".", "entropy_coef", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "eps", "=", "args", ".", "eps", ",", "\n", "max_grad_norm", "=", "args", ".", "max_grad_norm", ")", "\n", "\n", "# discriminator", "\n", "discr", "=", "Discriminator", "(", "envs", ".", "observation_space", ".", "shape", "[", "0", "]", "+", "envs", ".", "action_space", ".", "shape", "[", "0", "]", ",", "100", ",", "device", ",", "args", ".", "reward_type", ",", "args", ".", "update_rms", ")", "\n", "\n", "# file_name = os.path.join(", "\n", "#     args.gail_experts_dir, \"trajs_{}.pt\".format(", "\n", "#         args.env_name.split('-')[0].lower()))", "\n", "#", "\n", "# gail_train_loader = torch.utils.data.DataLoader(", "\n", "#     ExpertDataset(", "\n", "#     file_name, num_trajectories=args.num_trajs, subsample_frequency=args.subsample_frequency),", "\n", "#     batch_size=args.gail_batch_size,", "\n", "#     shuffle=True,", "\n", "#     drop_last=True)", "\n", "\n", "# The buffer", "\n", "rollouts", "=", "RolloutStorage", "(", "args", ".", "num_steps", ",", "args", ".", "num_processes", ",", "\n", "envs", ".", "observation_space", ".", "shape", ",", "envs", ".", "action_space", ",", "\n", "actor_critic", ".", "recurrent_hidden_state_size", ")", "\n", "\n", "# The buffer for the expert -> refer to dataset/mujoco_dset.py", "\n", "# expert_path = cl_args.expert_path+cl_args.env_id+\".h5\"", "\n", "expert_buffer", "=", "Mujoco_Dset", "(", "cl_args", ".", "expert_path", ",", "traj_limitation", "=", "cl_args", ".", "num_trajs", ",", "subsample_frequency", "=", "args", ".", "subsample_frequency", ")", "\n", "\n", "\n", "model", "=", "gailLearning_mujoco_BL", "(", "cl_args", "=", "cl_args", ",", "\n", "envs", "=", "envs", ",", "\n", "envs_eval", "=", "envs_eval", ",", "\n", "actor_critic", "=", "actor_critic", ",", "\n", "agent", "=", "agent", ",", "\n", "discriminator", "=", "discr", ",", "\n", "rollouts", "=", "rollouts", ",", "\n", "expert_buffer", "=", "expert_buffer", ",", "\n", "device", "=", "device", ",", "\n", "utli", "=", "utli", ")", "\n", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoidstandup.main": [[179, 209], ["wdail_humanoidstandup.train"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.train"], ["", "def", "main", "(", "args", ")", ":", "\n", "\n", "    ", "model", ",", "env", "=", "train", "(", "args", ")", "\n", "\n", "# if args.play:", "\n", "#", "\n", "#     obs = env.reset()", "\n", "#", "\n", "#     state = model.initial_state if hasattr(model, 'initial_state') else None", "\n", "#     dones = np.zeros((1,))", "\n", "#", "\n", "#     episode_rew = 0", "\n", "#     while True:", "\n", "#         if state is not None:", "\n", "#             actions, _, state, _ = model.step(obs,S=state, M=dones)", "\n", "#         else:", "\n", "#             actions, _, _, _ = model.step(obs)", "\n", "#", "\n", "#         obs, rew, done, _ = env.step(actions)", "\n", "#         episode_rew += rew[0] if isinstance(env, VecEnv) else rew", "\n", "#         env.render()", "\n", "#         done = done.any() if isinstance(done, np.ndarray) else done", "\n", "#         if done:", "\n", "#             print(f'episode_rew={episode_rew}')", "\n", "#             episode_rew = 0", "\n", "#             obs = env.reset()", "\n", "#", "\n", "#     env.close()", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_hopper.argsparser": [[17, 74], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argsparser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\"WDAIL\"", ")", "\n", "parser", ".", "add_argument", "(", "'--env_name'", ",", "help", "=", "'environment ID'", ",", "default", "=", "'Hopper-v1'", ")", "\n", "parser", ".", "add_argument", "(", "'--algo'", ",", "help", "=", "'algorithm ID'", ",", "default", "=", "'WDAIL'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-dir'", ",", "default", "=", "'/tmp/gym/'", ",", "help", "=", "'directory to save agent logs (default: /tmp/gym)'", ")", "\n", "# general", "\n", "parser", ".", "add_argument", "(", "'--total_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--num_env_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluate_every'", ",", "help", "=", "'evaluate every'", ",", "type", "=", "int", ",", "default", "=", "2e10", ")", "\n", "parser", ".", "add_argument", "(", "'--save_condition'", ",", "help", "=", "'save_condition'", ",", "type", "=", "int", ",", "default", "=", "1000", ")", "\n", "parser", ".", "add_argument", "(", "'--num_model'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--use_device'", ",", "help", "=", "'use_device'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "help", "=", "'seed'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--use_linear_lr_decay'", ",", "help", "=", "'use linear lr decay'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--recurrent-policy'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'use a recurrent policy'", ")", "\n", "\n", "#ppo", "\n", "parser", ".", "add_argument", "(", "'--num_processes'", ",", "help", "=", "'num_processes'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--num-steps'", ",", "help", "=", "'num-steps'", ",", "type", "=", "int", ",", "default", "=", "2048", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "help", "=", "'learning rate'", ",", "type", "=", "float", ",", "default", "=", "3e-4", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "help", "=", "'batch size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_epoch'", ",", "help", "=", "'ppo epoch num'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--hidden_size'", ",", "help", "=", "'hidden size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_entcoeff'", ",", "help", "=", "'entropy coefficiency of policy'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ")", "#default=1e-3", "\n", "parser", ".", "add_argument", "(", "'--ppo_obs_norm'", ",", "help", "=", "'ppo_vec_norm'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--num-mini-batch'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "'number of batches for ppo (default: 32)'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-param'", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "help", "=", "'ppo clip parameter (default: 0.2)'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps'", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "help", "=", "'RMSprop optimizer epsilon (default: 1e-5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "'RMSprop optimizer apha (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "'discount factor for rewards (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "'--use-gae'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'use generalized advantage estimation'", ")", "\n", "parser", ".", "add_argument", "(", "'--gae-lambda'", ",", "type", "=", "float", ",", "default", "=", "0.95", ",", "help", "=", "'gae lambda parameter (default: 0.95)'", ")", "\n", "parser", ".", "add_argument", "(", "'--entropy-coef'", ",", "type", "=", "float", ",", "default", "=", "0.00", ",", "help", "=", "'entropy term coefficient (default: 0.01)'", ")", "\n", "parser", ".", "add_argument", "(", "'--value-loss-coef'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'value loss coefficient (default: 0.5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-grad-norm'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'max norm of gradients (default: 0.5)'", ")", "\n", "\n", "# #gail", "\n", "parser", ".", "add_argument", "(", "'--gail'", ",", "help", "=", "'if gail'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "# parser.add_argument('--expert_path', help='trajs path', type=str, default='../data/baseline/deterministic.trpo.HalfCheetah.0.00.npz')", "\n", "parser", ".", "add_argument", "(", "'--expert_path'", ",", "help", "=", "'trajs path'", ",", "type", "=", "str", ",", "default", "=", "'../data/ikostirkov/trajs_hopper.h5'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail-experts-dir'", ",", "default", "=", "'./gail_experts'", ",", "help", "=", "'directory that contains expert demonstrations for gail'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_batch_size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'gail batch size (default: 128)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_thre'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_pre_epoch'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "100", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_epoch'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "'--num_trajs'", ",", "help", "=", "'num trajs'", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "'--subsample_frequency'", ",", "help", "=", "'num trajs'", ",", "type", "=", "int", ",", "default", "=", "20", ")", "\n", "parser", ".", "add_argument", "(", "'--adversary_entcoeff'", ",", "help", "=", "'entropy coefficiency of discriminator'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ")", "\n", "parser", ".", "add_argument", "(", "'--use-proper-time-limits'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'compute returns taking into account time limits'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-interval'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'log interval, one log per n updates (default: 10)'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--reward_type'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'0,1,2,3,4'", ")", "\n", "parser", ".", "add_argument", "(", "'--update_rms'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "'False or True'", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n", "", "def", "train", "(", "args", ")", ":", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_hopper.train": [[75, 177], ["np.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "os.path.expanduser", "utils.cleanup_log_dir", "utils.cleanup_log_dir", "torch.set_num_threads", "torch.device", "make_vec_envs", "Policy", "Policy.to", "PPO", "Discriminator", "RolloutStorage", "Mujoco_Dset", "gailLearning_mujoco_BL", "torch.cuda.is_available", "str"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.make_vec_envs", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.gailLearning_mujoco_BL"], ["\n", "# from ppo_gail_iko.algo.ppo4multienvs import PPO, ReplayBuffer", "\n", "    ", "from", "ppo_wdail", ".", "algo", ".", "ppo", "import", "PPO", "\n", "from", "ppo_wdail", ".", "tools", ".", "storage", "import", "RolloutStorage", "\n", "from", "ppo_wdail", ".", "tools", ".", "model", "import", "Policy", "\n", "\n", "from", "ppo_wdail", ".", "algo", ".", "wdgail", "import", "Discriminator", ",", "ExpertDataset", "\n", "from", "ppo_wdail", ".", "algo", ".", "mujoco_dset_zm_iko", "import", "Mujoco_Dset", "\n", "\n", "from", "ppo_wdail", ".", "tools", ".", "learn", "import", "gailLearning_mujoco", ",", "Learning_process", ",", "Learning_process_record", ",", "gailLearning_mujoco_test", ",", "gailLearning_mujoco_origin", "\n", "from", "ppo_wdail", ".", "tools", ".", "envs", "import", "make_vec_envs", "\n", "\n", "from", "ppo_wdail", ".", "tools", "import", "utli", "\n", "from", "ppo_wdail", ".", "tools", "import", "utils", "\n", "\n", "from", "collections", "import", "deque", "\n", "import", "time", "\n", "import", "numpy", "as", "np", "\n", "\n", "\n", "# from nets.network import ActorCritic_mujoco as ActorCritic", "\n", "cl_args", "=", "args", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "# if args.cuda and torch.cuda.is_available() and args.cuda_deterministic:", "\n", "#     torch.backends.cudnn.benchmark = False", "\n", "#     torch.backends.cudnn.deterministic = True", "\n", "\n", "log_dir", "=", "os", ".", "path", ".", "expanduser", "(", "args", ".", "log_dir", ")", "\n", "eval_log_dir", "=", "log_dir", "+", "\"_eval\"", "\n", "utils", ".", "cleanup_log_dir", "(", "log_dir", ")", "\n", "utils", ".", "cleanup_log_dir", "(", "eval_log_dir", ")", "\n", "\n", "torch", ".", "set_num_threads", "(", "1", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "'cuda:'", "+", "str", "(", "cl_args", ".", "cuda", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "# device = torch.device('cpu')", "\n", "\n", "envs", "=", "make_vec_envs", "(", "args", ".", "env_name", ",", "args", ".", "seed", ",", "args", ".", "num_processes", ",", "\n", "args", ".", "gamma", ",", "args", ".", "log_dir", ",", "device", ",", "False", ")", "\n", "\n", "# envs_eval = make_vec_envs(args.env_name, args.seed, args.num_processes,", "\n", "#                      args.gamma, args.log_dir, device, False)", "\n", "envs_eval", "=", "[", "]", "\n", "\n", "# network", "\n", "actor_critic", "=", "Policy", "(", "\n", "envs", ".", "observation_space", ".", "shape", ",", "\n", "envs", ".", "action_space", ",", "\n", "base_kwargs", "=", "{", "'recurrent'", ":", "args", ".", "recurrent_policy", "}", ")", "\n", "actor_critic", ".", "to", "(", "device", ")", "\n", "\n", "agent", "=", "PPO", "(", "\n", "actor_critic", ",", "\n", "args", ".", "clip_param", ",", "\n", "args", ".", "ppo_epoch", ",", "\n", "args", ".", "num_mini_batch", ",", "\n", "args", ".", "value_loss_coef", ",", "\n", "args", ".", "entropy_coef", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "eps", "=", "args", ".", "eps", ",", "\n", "max_grad_norm", "=", "args", ".", "max_grad_norm", ")", "\n", "\n", "# discriminator", "\n", "discr", "=", "Discriminator", "(", "envs", ".", "observation_space", ".", "shape", "[", "0", "]", "+", "envs", ".", "action_space", ".", "shape", "[", "0", "]", ",", "100", ",", "device", ",", "args", ".", "reward_type", ",", "args", ".", "update_rms", ")", "\n", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "gail_experts_dir", ",", "\"trajs_{}.pt\"", ".", "format", "(", "\n", "args", ".", "env_name", ".", "split", "(", "'-'", ")", "[", "0", "]", ".", "lower", "(", ")", ")", ")", "\n", "\n", "gail_train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "ExpertDataset", "(", "\n", "file_name", ",", "num_trajectories", "=", "args", ".", "num_trajs", ",", "subsample_frequency", "=", "args", ".", "subsample_frequency", ")", ",", "\n", "batch_size", "=", "args", ".", "gail_batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "drop_last", "=", "True", ")", "\n", "\n", "# The buffer", "\n", "rollouts", "=", "RolloutStorage", "(", "args", ".", "num_steps", ",", "args", ".", "num_processes", ",", "\n", "envs", ".", "observation_space", ".", "shape", ",", "envs", ".", "action_space", ",", "\n", "actor_critic", ".", "recurrent_hidden_state_size", ")", "\n", "\n", "# The buffer for the expert -> refer to dataset/mujoco_dset.py", "\n", "# expert_path = cl_args.expert_path+cl_args.env_id+\".h5\"", "\n", "# expert_buffer = Mujoco_Dset(cl_args.expert_path, traj_limitation=cl_args.num_trajs, subsample_frequency=args.subsample_frequency)", "\n", "\n", "model", "=", "gailLearning_mujoco_origin", "(", "cl_args", "=", "cl_args", ",", "\n", "envs", "=", "envs", ",", "\n", "envs_eval", "=", "envs_eval", ",", "\n", "actor_critic", "=", "actor_critic", ",", "\n", "agent", "=", "agent", ",", "\n", "discriminator", "=", "discr", ",", "\n", "rollouts", "=", "rollouts", ",", "\n", "gail_train_loader", "=", "gail_train_loader", ",", "\n", "device", "=", "device", ",", "\n", "utli", "=", "utli", ")", "\n", "\n", "return", "0", "\n", "\n", "\n", "", "def", "main", "(", "args", ")", ":", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_hopper.main": [[179, 209], ["wdail_hopper.train"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.train"], ["    ", "model", ",", "env", "=", "train", "(", "args", ")", "\n", "\n", "# if args.play:", "\n", "#", "\n", "#     obs = env.reset()", "\n", "#", "\n", "#     state = model.initial_state if hasattr(model, 'initial_state') else None", "\n", "#     dones = np.zeros((1,))", "\n", "#", "\n", "#     episode_rew = 0", "\n", "#     while True:", "\n", "#         if state is not None:", "\n", "#             actions, _, state, _ = model.step(obs,S=state, M=dones)", "\n", "#         else:", "\n", "#             actions, _, _, _ = model.step(obs)", "\n", "#", "\n", "#         obs, rew, done, _ = env.step(actions)", "\n", "#         episode_rew += rew[0] if isinstance(env, VecEnv) else rew", "\n", "#         env.render()", "\n", "#         done = done.any() if isinstance(done, np.ndarray) else done", "\n", "#         if done:", "\n", "#             print(f'episode_rew={episode_rew}')", "\n", "#             episode_rew = 0", "\n", "#             obs = env.reset()", "\n", "#", "\n", "#     env.close()", "\n", "\n", "return", "model", "\n", "\n", "", "if", "__name__", "==", "'__main__'", ":", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_halfcheetah.argsparser": [[17, 75], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argsparser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\"WDAIL\"", ")", "\n", "parser", ".", "add_argument", "(", "'--env_name'", ",", "help", "=", "'environment ID'", ",", "default", "=", "'HalfCheetah-v2'", ")", "\n", "parser", ".", "add_argument", "(", "'--algo'", ",", "help", "=", "'algorithm ID'", ",", "default", "=", "'WDAIL'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-dir'", ",", "default", "=", "'/tmp/gym/'", ",", "help", "=", "'directory to save agent logs (default: /tmp/gym)'", ")", "\n", "# general", "\n", "parser", ".", "add_argument", "(", "'--total_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--num_env_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluate_every'", ",", "help", "=", "'evaluate every'", ",", "type", "=", "int", ",", "default", "=", "2e10", ")", "\n", "parser", ".", "add_argument", "(", "'--save_condition'", ",", "help", "=", "'save_condition'", ",", "type", "=", "int", ",", "default", "=", "1000", ")", "\n", "parser", ".", "add_argument", "(", "'--num_model'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--use_device'", ",", "help", "=", "'use_device'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "help", "=", "'seed'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--use_linear_lr_decay'", ",", "help", "=", "'use linear lr decay'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--recurrent-policy'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'use a recurrent policy'", ")", "\n", "\n", "#ppo", "\n", "parser", ".", "add_argument", "(", "'--num_processes'", ",", "help", "=", "'num_processes'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--num-steps'", ",", "help", "=", "'num-steps'", ",", "type", "=", "int", ",", "default", "=", "2048", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "help", "=", "'learning rate'", ",", "type", "=", "float", ",", "default", "=", "3e-4", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "help", "=", "'batch size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_epoch'", ",", "help", "=", "'ppo epoch num'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--hidden_size'", ",", "help", "=", "'hidden size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_entcoeff'", ",", "help", "=", "'entropy coefficiency of policy'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ")", "#default=1e-3", "\n", "parser", ".", "add_argument", "(", "'--ppo_obs_norm'", ",", "help", "=", "'ppo_vec_norm'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--num-mini-batch'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "'number of batches for ppo (default: 32)'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-param'", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "help", "=", "'ppo clip parameter (default: 0.2)'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps'", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "help", "=", "'RMSprop optimizer epsilon (default: 1e-5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "'RMSprop optimizer apha (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "'discount factor for rewards (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "'--use-gae'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'use generalized advantage estimation'", ")", "\n", "parser", ".", "add_argument", "(", "'--gae-lambda'", ",", "type", "=", "float", ",", "default", "=", "0.95", ",", "help", "=", "'gae lambda parameter (default: 0.95)'", ")", "\n", "parser", ".", "add_argument", "(", "'--entropy-coef'", ",", "type", "=", "float", ",", "default", "=", "0.00", ",", "help", "=", "'entropy term coefficient (default: 0.01)'", ")", "\n", "parser", ".", "add_argument", "(", "'--value-loss-coef'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'value loss coefficient (default: 0.5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-grad-norm'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'max norm of gradients (default: 0.5)'", ")", "\n", "\n", "# #gail", "\n", "parser", ".", "add_argument", "(", "'--gail'", ",", "help", "=", "'if gail'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "# parser.add_argument('--expert_path', help='trajs path', type=str, default='../data/baseline/deterministic.trpo.HalfCheetah.0.00.npz')", "\n", "parser", ".", "add_argument", "(", "'--expert_path'", ",", "help", "=", "'trajs path'", ",", "type", "=", "str", ",", "default", "=", "'../data/ikostirkov/trajs_halfcheetah.h5'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail-experts-dir'", ",", "default", "=", "'./gail_experts'", ",", "help", "=", "'directory that contains expert demonstrations for gail'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_batch_size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'gail batch size (default: 128)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_thre'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_pre_epoch'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "100", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_epoch'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "'--num_trajs'", ",", "help", "=", "'num trajs'", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "'--subsample_frequency'", ",", "help", "=", "'num trajs'", ",", "type", "=", "int", ",", "default", "=", "20", ")", "\n", "parser", ".", "add_argument", "(", "'--adversary_entcoeff'", ",", "help", "=", "'entropy coefficiency of discriminator'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ")", "\n", "parser", ".", "add_argument", "(", "'--use-proper-time-limits'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'compute returns taking into account time limits'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-interval'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'log interval, one log per n updates (default: 10)'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--reward_type'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'0,1,2,3,4'", ")", "\n", "parser", ".", "add_argument", "(", "'--update_rms'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "'False or True'", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n", "", "def", "train", "(", "args", ")", ":", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_halfcheetah.train": [[76, 178], ["np.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "os.path.expanduser", "utils.cleanup_log_dir", "utils.cleanup_log_dir", "torch.set_num_threads", "torch.device", "make_vec_envs", "Policy", "Policy.to", "PPO", "Discriminator", "RolloutStorage", "Mujoco_Dset", "gailLearning_mujoco_BL", "torch.cuda.is_available", "str"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.make_vec_envs", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.gailLearning_mujoco_BL"], ["# from ppo_gail_iko.algo.ppo4multienvs import PPO, ReplayBuffer", "\n", "    ", "from", "ppo_wdail", ".", "algo", ".", "ppo", "import", "PPO", "\n", "from", "ppo_wdail", ".", "tools", ".", "storage", "import", "RolloutStorage", "\n", "from", "ppo_wdail", ".", "tools", ".", "model", "import", "Policy", "\n", "\n", "from", "ppo_wdail", ".", "algo", ".", "wdgail", "import", "Discriminator", ",", "ExpertDataset", "\n", "from", "ppo_wdail", ".", "algo", ".", "mujoco_dset_zm_iko", "import", "Mujoco_Dset", "\n", "\n", "from", "ppo_wdail", ".", "tools", ".", "learn", "import", "gailLearning_mujoco", ",", "Learning_process", ",", "Learning_process_record", ",", "gailLearning_mujoco_test", ",", "gailLearning_mujoco_origin", "\n", "from", "ppo_wdail", ".", "tools", ".", "envs", "import", "make_vec_envs", "\n", "\n", "from", "ppo_wdail", ".", "tools", "import", "utli", "\n", "from", "ppo_wdail", ".", "tools", "import", "utils", "\n", "\n", "from", "collections", "import", "deque", "\n", "import", "time", "\n", "import", "numpy", "as", "np", "\n", "\n", "\n", "# from nets.network import ActorCritic_mujoco as ActorCritic", "\n", "cl_args", "=", "args", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "# if args.cuda and torch.cuda.is_available() and args.cuda_deterministic:", "\n", "#     torch.backends.cudnn.benchmark = False", "\n", "#     torch.backends.cudnn.deterministic = True", "\n", "\n", "log_dir", "=", "os", ".", "path", ".", "expanduser", "(", "args", ".", "log_dir", ")", "\n", "eval_log_dir", "=", "log_dir", "+", "\"_eval\"", "\n", "utils", ".", "cleanup_log_dir", "(", "log_dir", ")", "\n", "utils", ".", "cleanup_log_dir", "(", "eval_log_dir", ")", "\n", "\n", "torch", ".", "set_num_threads", "(", "1", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "'cuda:'", "+", "str", "(", "cl_args", ".", "cuda", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "# device = torch.device('cpu')", "\n", "\n", "envs", "=", "make_vec_envs", "(", "args", ".", "env_name", ",", "args", ".", "seed", ",", "args", ".", "num_processes", ",", "\n", "args", ".", "gamma", ",", "args", ".", "log_dir", ",", "device", ",", "False", ")", "\n", "\n", "# envs_eval = make_vec_envs(args.env_name, args.seed, args.num_processes,", "\n", "#                      args.gamma, args.log_dir, device, False)", "\n", "envs_eval", "=", "[", "]", "\n", "\n", "# network", "\n", "actor_critic", "=", "Policy", "(", "\n", "envs", ".", "observation_space", ".", "shape", ",", "\n", "envs", ".", "action_space", ",", "\n", "base_kwargs", "=", "{", "'recurrent'", ":", "args", ".", "recurrent_policy", "}", ")", "\n", "actor_critic", ".", "to", "(", "device", ")", "\n", "\n", "agent", "=", "PPO", "(", "\n", "actor_critic", ",", "\n", "args", ".", "clip_param", ",", "\n", "args", ".", "ppo_epoch", ",", "\n", "args", ".", "num_mini_batch", ",", "\n", "args", ".", "value_loss_coef", ",", "\n", "args", ".", "entropy_coef", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "eps", "=", "args", ".", "eps", ",", "\n", "max_grad_norm", "=", "args", ".", "max_grad_norm", ")", "\n", "\n", "# discriminator", "\n", "discr", "=", "Discriminator", "(", "envs", ".", "observation_space", ".", "shape", "[", "0", "]", "+", "envs", ".", "action_space", ".", "shape", "[", "0", "]", ",", "100", ",", "device", ",", "args", ".", "reward_type", ",", "args", ".", "update_rms", ")", "\n", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "gail_experts_dir", ",", "\"trajs_{}.pt\"", ".", "format", "(", "\n", "args", ".", "env_name", ".", "split", "(", "'-'", ")", "[", "0", "]", ".", "lower", "(", ")", ")", ")", "\n", "\n", "gail_train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "ExpertDataset", "(", "\n", "file_name", ",", "num_trajectories", "=", "args", ".", "num_trajs", ",", "subsample_frequency", "=", "args", ".", "subsample_frequency", ")", ",", "\n", "batch_size", "=", "args", ".", "gail_batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "drop_last", "=", "True", ")", "\n", "\n", "# The buffer", "\n", "rollouts", "=", "RolloutStorage", "(", "args", ".", "num_steps", ",", "args", ".", "num_processes", ",", "\n", "envs", ".", "observation_space", ".", "shape", ",", "envs", ".", "action_space", ",", "\n", "actor_critic", ".", "recurrent_hidden_state_size", ")", "\n", "\n", "# The buffer for the expert -> refer to dataset/mujoco_dset.py", "\n", "# expert_path = cl_args.expert_path+cl_args.env_id+\".h5\"", "\n", "# expert_buffer = Mujoco_Dset(cl_args.expert_path, traj_limitation=cl_args.num_trajs, subsample_frequency=20)", "\n", "\n", "# model = gailLearning_mujoco_test(cl_args=cl_args,", "\n", "#                                  envs=envs,", "\n", "#                                  envs_eval=envs_eval,", "\n", "#                                  actor_critic=actor_critic,", "\n", "#                                  agent=agent,", "\n", "#                                  discriminator=discr,", "\n", "#                                  rollouts=rollouts,", "\n", "#                                  expert_buffer=expert_buffer,", "\n", "#                                  device=device,", "\n", "#                                  utli=utli)", "\n", "\n", "model", "=", "gailLearning_mujoco_origin", "(", "cl_args", "=", "cl_args", ",", "\n", "envs", "=", "envs", ",", "\n", "envs_eval", "=", "envs_eval", ",", "\n", "actor_critic", "=", "actor_critic", ",", "\n", "agent", "=", "agent", ",", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_halfcheetah.main": [[180, 210], ["wdail_halfcheetah.train"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.train"], ["rollouts", "=", "rollouts", ",", "\n", "gail_train_loader", "=", "gail_train_loader", ",", "\n", "device", "=", "device", ",", "\n", "utli", "=", "utli", ")", "\n", "\n", "# model = gailLearning_mujoco(cl_args=cl_args,", "\n", "#                             envs=envs,", "\n", "#                             envs_eval=envs_eval,", "\n", "#                             actor_critic=actor_critic,", "\n", "#                             agent=agent,", "\n", "#                             discriminator=discr,", "\n", "#                             rollouts=rollouts,", "\n", "#                             gail_train_loader=gail_train_loader,", "\n", "#                             device=device,", "\n", "#                             utli=utli)", "\n", "\n", "# sign = Learning_process_record(args=args,", "\n", "#                                envs=envs,", "\n", "#                                rollouts=rollouts,", "\n", "#                                actor_critic=actor_critic,", "\n", "#                                agent=agent,", "\n", "#                                discr=discr,", "\n", "#                                gail_train_loader=gail_train_loader,", "\n", "#                                device=device)", "\n", "\n", "# sign = Learning_process(args=args,", "\n", "#                         envs=envs,", "\n", "#                         rollouts=rollouts,", "\n", "#                         actor_critic=actor_critic,", "\n", "#                         agent=agent,", "\n", "#                         discr=discr,", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_walker.argsparser": [[17, 74], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argsparser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\"WDAIL\"", ")", "\n", "parser", ".", "add_argument", "(", "'--env_name'", ",", "help", "=", "'environment ID'", ",", "default", "=", "'Walker2d-v2'", ")", "\n", "parser", ".", "add_argument", "(", "'--algo'", ",", "help", "=", "'algorithm ID'", ",", "default", "=", "'WDAIL'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-dir'", ",", "default", "=", "'/tmp/gym/'", ",", "help", "=", "'directory to save agent logs (default: /tmp/gym)'", ")", "\n", "# general", "\n", "parser", ".", "add_argument", "(", "'--total_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--num_env_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluate_every'", ",", "help", "=", "'evaluate every'", ",", "type", "=", "int", ",", "default", "=", "2e10", ")", "\n", "parser", ".", "add_argument", "(", "'--save_condition'", ",", "help", "=", "'save_condition'", ",", "type", "=", "int", ",", "default", "=", "1000", ")", "\n", "parser", ".", "add_argument", "(", "'--num_model'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--use_device'", ",", "help", "=", "'use_device'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "help", "=", "'seed'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--use_linear_lr_decay'", ",", "help", "=", "'use linear lr decay'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--recurrent-policy'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'use a recurrent policy'", ")", "\n", "\n", "#ppo", "\n", "parser", ".", "add_argument", "(", "'--num_processes'", ",", "help", "=", "'num_processes'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--nsteps'", ",", "help", "=", "'nsteps'", ",", "type", "=", "int", ",", "default", "=", "2048", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "help", "=", "'learning rate'", ",", "type", "=", "float", ",", "default", "=", "3e-4", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "help", "=", "'batch size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_epoch'", ",", "help", "=", "'ppo epoch num'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--hidden_size'", ",", "help", "=", "'hidden size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_entcoeff'", ",", "help", "=", "'entropy coefficiency of policy'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ")", "#default=1e-3", "\n", "parser", ".", "add_argument", "(", "'--ppo_obs_norm'", ",", "help", "=", "'ppo_vec_norm'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--num-mini-batch'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "'number of batches for ppo (default: 32)'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-param'", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "help", "=", "'ppo clip parameter (default: 0.2)'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps'", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "help", "=", "'RMSprop optimizer epsilon (default: 1e-5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "'RMSprop optimizer apha (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "'discount factor for rewards (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "'--use-gae'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'use generalized advantage estimation'", ")", "\n", "parser", ".", "add_argument", "(", "'--gae-lambda'", ",", "type", "=", "float", ",", "default", "=", "0.95", ",", "help", "=", "'gae lambda parameter (default: 0.95)'", ")", "\n", "parser", ".", "add_argument", "(", "'--entropy-coef'", ",", "type", "=", "float", ",", "default", "=", "0.00", ",", "help", "=", "'entropy term coefficient (default: 0.01)'", ")", "\n", "parser", ".", "add_argument", "(", "'--value-loss-coef'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'value loss coefficient (default: 0.5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-grad-norm'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'max norm of gradients (default: 0.5)'", ")", "\n", "\n", "# #gail", "\n", "parser", ".", "add_argument", "(", "'--gail'", ",", "help", "=", "'if gail'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "# parser.add_argument('--expert_path', help='trajs path', type=str, default='../data/baseline/deterministic.trpo.HalfCheetah.0.00.npz')", "\n", "parser", ".", "add_argument", "(", "'--expert_path'", ",", "help", "=", "'trajs path'", ",", "type", "=", "str", ",", "default", "=", "'../data/ikostirkov/trajs_walker.h5'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail-experts-dir'", ",", "default", "=", "'./gail_experts'", ",", "help", "=", "'directory that contains expert demonstrations for gail'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_batch_size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'gail batch size (default: 128)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_thre'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_pre_epoch'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "100", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_epoch'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "'--num_trajs'", ",", "help", "=", "'num trajs'", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "'--subsample_frequency'", ",", "help", "=", "'num trajs'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--adversary_entcoeff'", ",", "help", "=", "'entropy coefficiency of discriminator'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ")", "\n", "parser", ".", "add_argument", "(", "'--use-proper-time-limits'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'compute returns taking into account time limits'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-interval'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'log interval, one log per n updates (default: 10)'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--reward_type'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'0,1,2,3,4'", ")", "\n", "parser", ".", "add_argument", "(", "'--update_rms'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "'False or True'", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n", "", "def", "train", "(", "args", ")", ":", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_walker.train": [[75, 177], ["np.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "os.path.expanduser", "utils.cleanup_log_dir", "utils.cleanup_log_dir", "torch.set_num_threads", "torch.device", "make_vec_envs", "Policy", "Policy.to", "PPO", "Discriminator", "RolloutStorage", "Mujoco_Dset", "gailLearning_mujoco_BL", "torch.cuda.is_available", "str"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.make_vec_envs", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.gailLearning_mujoco_BL"], ["\n", "# from ppo_gail_iko.algo.ppo4multienvs import PPO, ReplayBuffer", "\n", "    ", "from", "ppo_wdail", ".", "algo", ".", "ppo", "import", "PPO", "\n", "from", "ppo_wdail", ".", "tools", ".", "storage", "import", "RolloutStorage", "\n", "from", "ppo_wdail", ".", "tools", ".", "model", "import", "Policy", "\n", "\n", "from", "ppo_wdail", ".", "algo", ".", "wdgail", "import", "Discriminator", ",", "ExpertDataset", "\n", "from", "ppo_wdail", ".", "algo", ".", "mujoco_dset_zm_iko", "import", "Mujoco_Dset", "\n", "\n", "from", "ppo_wdail", ".", "tools", ".", "learn", "import", "gailLearning_mujoco", ",", "Learning_process", ",", "Learning_process_record", ",", "gailLearning_mujoco_test", ",", "gailLearning_mujoco_origin", "\n", "from", "ppo_wdail", ".", "tools", ".", "envs", "import", "make_vec_envs", "\n", "\n", "from", "ppo_wdail", ".", "tools", "import", "utli", "\n", "from", "ppo_wdail", ".", "tools", "import", "utils", "\n", "\n", "from", "collections", "import", "deque", "\n", "import", "time", "\n", "import", "numpy", "as", "np", "\n", "\n", "\n", "# from nets.network import ActorCritic_mujoco as ActorCritic", "\n", "cl_args", "=", "args", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "# if args.cuda and torch.cuda.is_available() and args.cuda_deterministic:", "\n", "#     torch.backends.cudnn.benchmark = False", "\n", "#     torch.backends.cudnn.deterministic = True", "\n", "\n", "log_dir", "=", "os", ".", "path", ".", "expanduser", "(", "args", ".", "log_dir", ")", "\n", "eval_log_dir", "=", "log_dir", "+", "\"_eval\"", "\n", "utils", ".", "cleanup_log_dir", "(", "log_dir", ")", "\n", "utils", ".", "cleanup_log_dir", "(", "eval_log_dir", ")", "\n", "\n", "torch", ".", "set_num_threads", "(", "1", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "'cuda:'", "+", "str", "(", "cl_args", ".", "cuda", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "# device = torch.device('cpu')", "\n", "\n", "envs", "=", "make_vec_envs", "(", "args", ".", "env_name", ",", "args", ".", "seed", ",", "args", ".", "num_processes", ",", "\n", "args", ".", "gamma", ",", "args", ".", "log_dir", ",", "device", ",", "False", ")", "\n", "\n", "# envs_eval = make_vec_envs(args.env_name, args.seed, args.num_processes,", "\n", "#                      args.gamma, args.log_dir, device, False)", "\n", "envs_eval", "=", "[", "]", "\n", "\n", "# network", "\n", "actor_critic", "=", "Policy", "(", "\n", "envs", ".", "observation_space", ".", "shape", ",", "\n", "envs", ".", "action_space", ",", "\n", "base_kwargs", "=", "{", "'recurrent'", ":", "args", ".", "recurrent_policy", "}", ")", "\n", "actor_critic", ".", "to", "(", "device", ")", "\n", "\n", "agent", "=", "PPO", "(", "\n", "actor_critic", ",", "\n", "args", ".", "clip_param", ",", "\n", "args", ".", "ppo_epoch", ",", "\n", "args", ".", "num_mini_batch", ",", "\n", "args", ".", "value_loss_coef", ",", "\n", "args", ".", "entropy_coef", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "eps", "=", "args", ".", "eps", ",", "\n", "max_grad_norm", "=", "args", ".", "max_grad_norm", ")", "\n", "\n", "# discriminator", "\n", "# discr = Discriminator(envs.observation_space.shape[0] + envs.action_space.shape[0], 100, device)", "\n", "discr", "=", "Discriminator", "(", "envs", ".", "observation_space", ".", "shape", "[", "0", "]", "+", "envs", ".", "action_space", ".", "shape", "[", "0", "]", ",", "100", ",", "device", ",", "args", ".", "reward_type", ",", "args", ".", "update_rms", ")", "\n", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "gail_experts_dir", ",", "\"trajs_{}.pt\"", ".", "format", "(", "\n", "args", ".", "env_name", ".", "split", "(", "'-'", ")", "[", "0", "]", ".", "lower", "(", ")", ")", ")", "\n", "\n", "gail_train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "ExpertDataset", "(", "\n", "file_name", ",", "num_trajectories", "=", "args", ".", "num_trajs", ",", "subsample_frequency", "=", "args", ".", "subsample_frequency", ")", ",", "\n", "batch_size", "=", "args", ".", "gail_batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "drop_last", "=", "True", ")", "\n", "\n", "# The buffer", "\n", "rollouts", "=", "RolloutStorage", "(", "args", ".", "nsteps", ",", "args", ".", "num_processes", ",", "\n", "envs", ".", "observation_space", ".", "shape", ",", "envs", ".", "action_space", ",", "\n", "actor_critic", ".", "recurrent_hidden_state_size", ")", "\n", "\n", "# The buffer for the expert -> refer to dataset/mujoco_dset.py", "\n", "# expert_path = cl_args.expert_path+cl_args.env_id+\".h5\"", "\n", "# expert_buffer = Mujoco_Dset(cl_args.expert_path, traj_limitation=cl_args.num_trajs, subsample_frequency=args.subsample_frequency)", "\n", "\n", "model", "=", "gailLearning_mujoco_origin", "(", "cl_args", "=", "cl_args", ",", "\n", "envs", "=", "envs", ",", "\n", "envs_eval", "=", "envs_eval", ",", "\n", "actor_critic", "=", "actor_critic", ",", "\n", "agent", "=", "agent", ",", "\n", "discriminator", "=", "discr", ",", "\n", "rollouts", "=", "rollouts", ",", "\n", "gail_train_loader", "=", "gail_train_loader", ",", "\n", "device", "=", "device", ",", "\n", "utli", "=", "utli", ")", "\n", "\n", "return", "0", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_walker.main": [[179, 209], ["wdail_walker.train"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.train"], ["\n", "    ", "model", ",", "env", "=", "train", "(", "args", ")", "\n", "\n", "# if args.play:", "\n", "#", "\n", "#     obs = env.reset()", "\n", "#", "\n", "#     state = model.initial_state if hasattr(model, 'initial_state') else None", "\n", "#     dones = np.zeros((1,))", "\n", "#", "\n", "#     episode_rew = 0", "\n", "#     while True:", "\n", "#         if state is not None:", "\n", "#             actions, _, state, _ = model.step(obs,S=state, M=dones)", "\n", "#         else:", "\n", "#             actions, _, _, _ = model.step(obs)", "\n", "#", "\n", "#         obs, rew, done, _ = env.step(actions)", "\n", "#         episode_rew += rew[0] if isinstance(env, VecEnv) else rew", "\n", "#         env.render()", "\n", "#         done = done.any() if isinstance(done, np.ndarray) else done", "\n", "#         if done:", "\n", "#             print(f'episode_rew={episode_rew}')", "\n", "#             episode_rew = 0", "\n", "#             obs = env.reset()", "\n", "#", "\n", "#     env.close()", "\n", "\n", "return", "model", "\n", "\n", "", "if", "__name__", "==", "'__main__'", ":", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.argsparser": [[17, 74], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argsparser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\"WDAIL\"", ")", "\n", "parser", ".", "add_argument", "(", "'--env_name'", ",", "help", "=", "'environment ID'", ",", "default", "=", "'Humanoid-v1'", ")", "\n", "parser", ".", "add_argument", "(", "'--algo'", ",", "help", "=", "'algorithm ID'", ",", "default", "=", "'WDAIL'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-dir'", ",", "default", "=", "'/tmp/gym/'", ",", "help", "=", "'directory to save agent logs (default: /tmp/gym)'", ")", "\n", "# general", "\n", "parser", ".", "add_argument", "(", "'--total_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--num_env_steps'", ",", "help", "=", "'total steps'", ",", "type", "=", "int", ",", "default", "=", "10e6", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluate_every'", ",", "help", "=", "'evaluate every'", ",", "type", "=", "int", ",", "default", "=", "2e10", ")", "\n", "parser", ".", "add_argument", "(", "'--save_condition'", ",", "help", "=", "'save_condition'", ",", "type", "=", "int", ",", "default", "=", "1000", ")", "\n", "parser", ".", "add_argument", "(", "'--num_model'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--use_device'", ",", "help", "=", "'use_device'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda'", ",", "help", "=", "'num_model'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "help", "=", "'seed'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--use_linear_lr_decay'", ",", "help", "=", "'use linear lr decay'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--recurrent-policy'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'use a recurrent policy'", ")", "\n", "\n", "#ppo", "\n", "parser", ".", "add_argument", "(", "'--num_processes'", ",", "help", "=", "'num_processes'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--num-steps'", ",", "help", "=", "'num-steps'", ",", "type", "=", "int", ",", "default", "=", "2048", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "help", "=", "'learning rate'", ",", "type", "=", "float", ",", "default", "=", "3e-4", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "help", "=", "'batch size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_epoch'", ",", "help", "=", "'ppo epoch num'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--hidden_size'", ",", "help", "=", "'hidden size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--ppo_entcoeff'", ",", "help", "=", "'entropy coefficiency of policy'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ")", "#default=1e-3", "\n", "parser", ".", "add_argument", "(", "'--ppo_obs_norm'", ",", "help", "=", "'ppo_vec_norm'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--num-mini-batch'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "'number of batches for ppo (default: 32)'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-param'", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "help", "=", "'ppo clip parameter (default: 0.2)'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps'", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "help", "=", "'RMSprop optimizer epsilon (default: 1e-5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "'RMSprop optimizer apha (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "'discount factor for rewards (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "'--use-gae'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'use generalized advantage estimation'", ")", "\n", "parser", ".", "add_argument", "(", "'--gae-lambda'", ",", "type", "=", "float", ",", "default", "=", "0.95", ",", "help", "=", "'gae lambda parameter (default: 0.95)'", ")", "\n", "parser", ".", "add_argument", "(", "'--entropy-coef'", ",", "type", "=", "float", ",", "default", "=", "0.00", ",", "help", "=", "'entropy term coefficient (default: 0.01)'", ")", "\n", "parser", ".", "add_argument", "(", "'--value-loss-coef'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'value loss coefficient (default: 0.5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-grad-norm'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'max norm of gradients (default: 0.5)'", ")", "\n", "\n", "# #gail", "\n", "parser", ".", "add_argument", "(", "'--gail'", ",", "help", "=", "'if gail'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--expert_path'", ",", "help", "=", "'trajs path'", ",", "type", "=", "str", ",", "default", "=", "'../data/baseline/deterministic.trpo.Humanoid.0.00.npz'", ")", "\n", "# parser.add_argument('--expert_path', help='trajs path', type=str, default='../data/ikostirkov/trajs_ant.h5')", "\n", "parser", ".", "add_argument", "(", "'--gail-experts-dir'", ",", "default", "=", "'./gail_experts'", ",", "help", "=", "'directory that contains expert demonstrations for gail'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_batch_size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'gail batch size (default: 128)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_thre'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_pre_epoch'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "1000", ")", "\n", "parser", ".", "add_argument", "(", "'--gail_epoch'", ",", "help", "=", "'number of steps to train discriminator in each epoch'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "'--num_trajs'", ",", "help", "=", "'num trajs'", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "'--subsample_frequency'", ",", "help", "=", "'num trajs'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--adversary_entcoeff'", ",", "help", "=", "'entropy coefficiency of discriminator'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ")", "\n", "parser", ".", "add_argument", "(", "'--use-proper-time-limits'", ",", "action", "=", "'store_true'", ",", "default", "=", "True", ",", "help", "=", "'compute returns taking into account time limits'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-interval'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'log interval, one log per n updates (default: 10)'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--reward_type'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'0,1,2,3,4'", ")", "\n", "parser", ".", "add_argument", "(", "'--update_rms'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "'False or True'", ")", "\n", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.train": [[75, 177], ["np.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "os.path.expanduser", "utils.cleanup_log_dir", "utils.cleanup_log_dir", "torch.set_num_threads", "torch.device", "make_vec_envs", "Policy", "Policy.to", "PPO", "Discriminator", "RolloutStorage", "Mujoco_Dset", "gailLearning_mujoco_BL", "torch.cuda.is_available", "str"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.utils.cleanup_log_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.envs.make_vec_envs", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.storage.RolloutStorage.to", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.tools.learn.gailLearning_mujoco_BL"], ["", "def", "train", "(", "args", ")", ":", "\n", "\n", "# from ppo_gail_iko.algo.ppo4multienvs import PPO, ReplayBuffer", "\n", "    ", "from", "ppo_wdail_BL", ".", "algo", ".", "ppo", "import", "PPO", "\n", "from", "ppo_wdail_BL", ".", "tools", ".", "storage", "import", "RolloutStorage", "\n", "from", "ppo_wdail_BL", ".", "tools", ".", "model", "import", "Policy", "\n", "\n", "from", "ppo_wdail_BL", ".", "algo", ".", "wdgail", "import", "Discriminator", ",", "ExpertDataset", "\n", "from", "ppo_wdail_BL", ".", "algo", ".", "mujoco_dset_zm_base", "import", "Mujoco_Dset", "\n", "\n", "from", "ppo_wdail_BL", ".", "tools", ".", "learn", "import", "gailLearning_mujoco_origin", ",", "gailLearning_mujoco_BL", "\n", "from", "ppo_wdail_BL", ".", "tools", ".", "envs", "import", "make_vec_envs", "\n", "\n", "from", "ppo_wdail_BL", ".", "tools", "import", "utli", "\n", "from", "ppo_wdail_BL", ".", "tools", "import", "utils", "\n", "\n", "from", "collections", "import", "deque", "\n", "import", "time", "\n", "import", "numpy", "as", "np", "\n", "\n", "\n", "# from nets.network import ActorCritic_mujoco as ActorCritic", "\n", "cl_args", "=", "args", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "# if args.cuda and torch.cuda.is_available() and args.cuda_deterministic:", "\n", "#     torch.backends.cudnn.benchmark = False", "\n", "#     torch.backends.cudnn.deterministic = True", "\n", "\n", "log_dir", "=", "os", ".", "path", ".", "expanduser", "(", "args", ".", "log_dir", ")", "\n", "eval_log_dir", "=", "log_dir", "+", "\"_eval\"", "\n", "utils", ".", "cleanup_log_dir", "(", "log_dir", ")", "\n", "utils", ".", "cleanup_log_dir", "(", "eval_log_dir", ")", "\n", "\n", "torch", ".", "set_num_threads", "(", "1", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "'cuda:'", "+", "str", "(", "cl_args", ".", "cuda", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "# device = torch.device('cpu')", "\n", "\n", "envs", "=", "make_vec_envs", "(", "args", ".", "env_name", ",", "args", ".", "seed", ",", "args", ".", "num_processes", ",", "\n", "args", ".", "gamma", ",", "args", ".", "log_dir", ",", "device", ",", "False", ")", "\n", "\n", "# envs_eval = make_vec_envs(args.env_name, args.seed, args.num_processes,", "\n", "#                      args.gamma, args.log_dir, device, False)", "\n", "envs_eval", "=", "[", "]", "\n", "\n", "# network", "\n", "actor_critic", "=", "Policy", "(", "\n", "envs", ".", "observation_space", ".", "shape", ",", "\n", "envs", ".", "action_space", ",", "\n", "base_kwargs", "=", "{", "'recurrent'", ":", "args", ".", "recurrent_policy", "}", ")", "\n", "actor_critic", ".", "to", "(", "device", ")", "\n", "\n", "agent", "=", "PPO", "(", "\n", "actor_critic", ",", "\n", "args", ".", "clip_param", ",", "\n", "args", ".", "ppo_epoch", ",", "\n", "args", ".", "num_mini_batch", ",", "\n", "args", ".", "value_loss_coef", ",", "\n", "args", ".", "entropy_coef", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "eps", "=", "args", ".", "eps", ",", "\n", "max_grad_norm", "=", "args", ".", "max_grad_norm", ")", "\n", "\n", "# discriminator", "\n", "discr", "=", "Discriminator", "(", "envs", ".", "observation_space", ".", "shape", "[", "0", "]", "+", "envs", ".", "action_space", ".", "shape", "[", "0", "]", ",", "300", ",", "device", ",", "args", ".", "reward_type", ",", "args", ".", "update_rms", ")", "\n", "\n", "# file_name = os.path.join(", "\n", "#     args.gail_experts_dir, \"trajs_{}.pt\".format(", "\n", "#         args.env_name.split('-')[0].lower()))", "\n", "#", "\n", "# gail_train_loader = torch.utils.data.DataLoader(", "\n", "#     ExpertDataset(", "\n", "#     file_name, num_trajectories=args.num_trajs, subsample_frequency=args.subsample_frequency),", "\n", "#     batch_size=args.gail_batch_size,", "\n", "#     shuffle=True,", "\n", "#     drop_last=True)", "\n", "\n", "# The buffer", "\n", "rollouts", "=", "RolloutStorage", "(", "args", ".", "num_steps", ",", "args", ".", "num_processes", ",", "\n", "envs", ".", "observation_space", ".", "shape", ",", "envs", ".", "action_space", ",", "\n", "actor_critic", ".", "recurrent_hidden_state_size", ")", "\n", "\n", "# The buffer for the expert -> refer to dataset/mujoco_dset.py", "\n", "# expert_path = cl_args.expert_path+cl_args.env_id+\".h5\"", "\n", "expert_buffer", "=", "Mujoco_Dset", "(", "cl_args", ".", "expert_path", ",", "traj_limitation", "=", "cl_args", ".", "num_trajs", ",", "subsample_frequency", "=", "args", ".", "subsample_frequency", ")", "\n", "\n", "\n", "model", "=", "gailLearning_mujoco_BL", "(", "cl_args", "=", "cl_args", ",", "\n", "envs", "=", "envs", ",", "\n", "envs_eval", "=", "envs_eval", ",", "\n", "actor_critic", "=", "actor_critic", ",", "\n", "agent", "=", "agent", ",", "\n", "discriminator", "=", "discr", ",", "\n", "rollouts", "=", "rollouts", ",", "\n", "expert_buffer", "=", "expert_buffer", ",", "\n", "device", "=", "device", ",", "\n", "utli", "=", "utli", ")", "\n", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.main": [[179, 209], ["wdail_humanoid.train"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.ppo_wdail_BL.wdail_humanoid.train"], ["", "def", "main", "(", "args", ")", ":", "\n", "\n", "    ", "model", ",", "env", "=", "train", "(", "args", ")", "\n", "\n", "# if args.play:", "\n", "#", "\n", "#     obs = env.reset()", "\n", "#", "\n", "#     state = model.initial_state if hasattr(model, 'initial_state') else None", "\n", "#     dones = np.zeros((1,))", "\n", "#", "\n", "#     episode_rew = 0", "\n", "#     while True:", "\n", "#         if state is not None:", "\n", "#             actions, _, state, _ = model.step(obs,S=state, M=dones)", "\n", "#         else:", "\n", "#             actions, _, _, _ = model.step(obs)", "\n", "#", "\n", "#         obs, rew, done, _ = env.step(actions)", "\n", "#         episode_rew += rew[0] if isinstance(env, VecEnv) else rew", "\n", "#         env.render()", "\n", "#         done = done.any() if isinstance(done, np.ndarray) else done", "\n", "#         if done:", "\n", "#             print(f'episode_rew={episode_rew}')", "\n", "#             episode_rew = 0", "\n", "#             obs = env.reset()", "\n", "#", "\n", "#     env.close()", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.monitor.Monitor.__init__": [[16, 36], ["gym.core.Wrapper.__init__", "time.time", "monitor.ResultsWriter", "time.time"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__"], ["def", "__init__", "(", "self", ",", "env", ",", "filename", ",", "allow_early_resets", "=", "False", ",", "reset_keywords", "=", "(", ")", ",", "info_keywords", "=", "(", ")", ")", ":", "\n", "        ", "Wrapper", ".", "__init__", "(", "self", ",", "env", "=", "env", ")", "\n", "self", ".", "tstart", "=", "time", ".", "time", "(", ")", "\n", "if", "filename", ":", "\n", "            ", "self", ".", "results_writer", "=", "ResultsWriter", "(", "filename", ",", "\n", "header", "=", "{", "\"t_start\"", ":", "time", ".", "time", "(", ")", ",", "'env_id'", ":", "env", ".", "spec", "and", "env", ".", "spec", ".", "id", "}", ",", "\n", "extra_keys", "=", "reset_keywords", "+", "info_keywords", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "results_writer", "=", "None", "\n", "", "self", ".", "reset_keywords", "=", "reset_keywords", "\n", "self", ".", "info_keywords", "=", "info_keywords", "\n", "self", ".", "allow_early_resets", "=", "allow_early_resets", "\n", "self", ".", "rewards", "=", "None", "\n", "self", ".", "needs_reset", "=", "True", "\n", "self", ".", "episode_rewards", "=", "[", "]", "\n", "self", ".", "episode_lengths", "=", "[", "]", "\n", "self", ".", "episode_times", "=", "[", "]", "\n", "self", ".", "total_steps", "=", "0", "\n", "self", ".", "current_reset_info", "=", "{", "}", "# extra info about the current episode, that was passed in during reset()", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.monitor.Monitor.reset": [[37, 45], ["monitor.Monitor.reset_state", "monitor.Monitor.env.reset", "kwargs.get", "ValueError"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.reset_state", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "reset_state", "(", ")", "\n", "for", "k", "in", "self", ".", "reset_keywords", ":", "\n", "            ", "v", "=", "kwargs", ".", "get", "(", "k", ")", "\n", "if", "v", "is", "None", ":", "\n", "                ", "raise", "ValueError", "(", "'Expected you to pass kwarg %s into reset'", "%", "k", ")", "\n", "", "self", ".", "current_reset_info", "[", "k", "]", "=", "v", "\n", "", "return", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.monitor.Monitor.reset_state": [[46, 51], ["RuntimeError"], "methods", ["None"], ["", "def", "reset_state", "(", "self", ")", ":", "\n", "        ", "if", "not", "self", ".", "allow_early_resets", "and", "not", "self", ".", "needs_reset", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Tried to reset an environment before done. If you want to allow early resets, wrap your env with Monitor(env, path, allow_early_resets=True)\"", ")", "\n", "", "self", ".", "rewards", "=", "[", "]", "\n", "self", ".", "needs_reset", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.monitor.Monitor.step": [[53, 59], ["monitor.Monitor.env.step", "monitor.Monitor.update", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.step", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "if", "self", ".", "needs_reset", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Tried to step environment that needs reset\"", ")", "\n", "", "ob", ",", "rew", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "update", "(", "ob", ",", "rew", ",", "done", ",", "info", ")", "\n", "return", "(", "ob", ",", "rew", ",", "done", ",", "info", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.monitor.Monitor.update": [[60, 80], ["monitor.Monitor.rewards.append", "sum", "len", "monitor.Monitor.episode_rewards.append", "monitor.Monitor.episode_lengths.append", "monitor.Monitor.episode_times.append", "epinfo.update", "isinstance", "isinstance", "round", "round", "monitor.Monitor.results_writer.write_row", "time.time", "time.time"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.ResultsWriter.write_row"], ["", "def", "update", "(", "self", ",", "ob", ",", "rew", ",", "done", ",", "info", ")", ":", "\n", "        ", "self", ".", "rewards", ".", "append", "(", "rew", ")", "\n", "if", "done", ":", "\n", "            ", "self", ".", "needs_reset", "=", "True", "\n", "eprew", "=", "sum", "(", "self", ".", "rewards", ")", "\n", "eplen", "=", "len", "(", "self", ".", "rewards", ")", "\n", "epinfo", "=", "{", "\"r\"", ":", "round", "(", "eprew", ",", "6", ")", ",", "\"l\"", ":", "eplen", ",", "\"t\"", ":", "round", "(", "time", ".", "time", "(", ")", "-", "self", ".", "tstart", ",", "6", ")", "}", "\n", "for", "k", "in", "self", ".", "info_keywords", ":", "\n", "                ", "epinfo", "[", "k", "]", "=", "info", "[", "k", "]", "\n", "", "self", ".", "episode_rewards", ".", "append", "(", "eprew", ")", "\n", "self", ".", "episode_lengths", ".", "append", "(", "eplen", ")", "\n", "self", ".", "episode_times", ".", "append", "(", "time", ".", "time", "(", ")", "-", "self", ".", "tstart", ")", "\n", "epinfo", ".", "update", "(", "self", ".", "current_reset_info", ")", "\n", "if", "self", ".", "results_writer", ":", "\n", "                ", "self", ".", "results_writer", ".", "write_row", "(", "epinfo", ")", "\n", "", "assert", "isinstance", "(", "info", ",", "dict", ")", "\n", "if", "isinstance", "(", "info", ",", "dict", ")", ":", "\n", "                ", "info", "[", "'episode'", "]", "=", "epinfo", "\n", "\n", "", "", "self", ".", "total_steps", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.monitor.Monitor.close": [[81, 84], ["monitor.Monitor.f.close"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "f", "is", "not", "None", ":", "\n", "            ", "self", ".", "f", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.monitor.Monitor.get_total_steps": [[85, 87], ["None"], "methods", ["None"], ["", "", "def", "get_total_steps", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "total_steps", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.monitor.Monitor.get_episode_rewards": [[88, 90], ["None"], "methods", ["None"], ["", "def", "get_episode_rewards", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "episode_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.monitor.Monitor.get_episode_lengths": [[91, 93], ["None"], "methods", ["None"], ["", "def", "get_episode_lengths", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "episode_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.monitor.Monitor.get_episode_times": [[94, 96], ["None"], "methods", ["None"], ["", "def", "get_episode_times", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "episode_times", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.monitor.ResultsWriter.__init__": [[102, 117], ["open", "isinstance", "monitor.ResultsWriter.f.write", "csv.DictWriter", "monitor.ResultsWriter.logger.writeheader", "monitor.ResultsWriter.f.flush", "os.join.endswith", "os.isdir", "os.join", "json.dumps", "tuple"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "filename", ",", "header", "=", "''", ",", "extra_keys", "=", "(", ")", ")", ":", "\n", "        ", "self", ".", "extra_keys", "=", "extra_keys", "\n", "assert", "filename", "is", "not", "None", "\n", "if", "not", "filename", ".", "endswith", "(", "Monitor", ".", "EXT", ")", ":", "\n", "            ", "if", "osp", ".", "isdir", "(", "filename", ")", ":", "\n", "                ", "filename", "=", "osp", ".", "join", "(", "filename", ",", "Monitor", ".", "EXT", ")", "\n", "", "else", ":", "\n", "                ", "filename", "=", "filename", "+", "\".\"", "+", "Monitor", ".", "EXT", "\n", "", "", "self", ".", "f", "=", "open", "(", "filename", ",", "\"wt\"", ")", "\n", "if", "isinstance", "(", "header", ",", "dict", ")", ":", "\n", "            ", "header", "=", "'# {} \\n'", ".", "format", "(", "json", ".", "dumps", "(", "header", ")", ")", "\n", "", "self", ".", "f", ".", "write", "(", "header", ")", "\n", "self", ".", "logger", "=", "csv", ".", "DictWriter", "(", "self", ".", "f", ",", "fieldnames", "=", "(", "'r'", ",", "'l'", ",", "'t'", ")", "+", "tuple", "(", "extra_keys", ")", ")", "\n", "self", ".", "logger", ".", "writeheader", "(", ")", "\n", "self", ".", "f", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.monitor.ResultsWriter.write_row": [[118, 122], ["monitor.ResultsWriter.logger.writerow", "monitor.ResultsWriter.f.flush"], "methods", ["None"], ["", "def", "write_row", "(", "self", ",", "epinfo", ")", ":", "\n", "        ", "if", "self", ".", "logger", ":", "\n", "            ", "self", ".", "logger", ".", "writerow", "(", "epinfo", ")", "\n", "self", ".", "f", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.monitor.get_monitor_files": [[124, 126], ["glob.glob", "os.join"], "function", ["None"], ["", "", "", "def", "get_monitor_files", "(", "dir", ")", ":", "\n", "    ", "return", "glob", "(", "osp", ".", "join", "(", "dir", ",", "\"*\"", "+", "Monitor", ".", "EXT", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.monitor.load_results": [[127, 165], ["pandas.concat", "pandas.DataFrame.sort_values", "pandas.DataFrame.reset_index", "min", "glob.glob", "glob.glob", "monitor.LoadMonitorResultsError", "dfs.append", "os.join", "os.join", "open", "fname.endswith", "fh.readline", "json.loads", "pandas.read_csv", "headers.append", "fname.endswith", "fh.readlines", "json.loads", "headers.append", "pandas.DataFrame", "json.loads", "episodes.append"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.read_csv"], ["", "def", "load_results", "(", "dir", ")", ":", "\n", "    ", "import", "pandas", "\n", "monitor_files", "=", "(", "\n", "glob", "(", "osp", ".", "join", "(", "dir", ",", "\"*monitor.json\"", ")", ")", "+", "\n", "glob", "(", "osp", ".", "join", "(", "dir", ",", "\"*monitor.csv\"", ")", ")", ")", "# get both csv and (old) json files", "\n", "if", "not", "monitor_files", ":", "\n", "        ", "raise", "LoadMonitorResultsError", "(", "\"no monitor files of the form *%s found in %s\"", "%", "(", "Monitor", ".", "EXT", ",", "dir", ")", ")", "\n", "", "dfs", "=", "[", "]", "\n", "headers", "=", "[", "]", "\n", "for", "fname", "in", "monitor_files", ":", "\n", "        ", "with", "open", "(", "fname", ",", "'rt'", ")", "as", "fh", ":", "\n", "            ", "if", "fname", ".", "endswith", "(", "'csv'", ")", ":", "\n", "                ", "firstline", "=", "fh", ".", "readline", "(", ")", "\n", "if", "not", "firstline", ":", "\n", "                    ", "continue", "\n", "", "assert", "firstline", "[", "0", "]", "==", "'#'", "\n", "header", "=", "json", ".", "loads", "(", "firstline", "[", "1", ":", "]", ")", "\n", "df", "=", "pandas", ".", "read_csv", "(", "fh", ",", "index_col", "=", "None", ")", "\n", "headers", ".", "append", "(", "header", ")", "\n", "", "elif", "fname", ".", "endswith", "(", "'json'", ")", ":", "# Deprecated json format", "\n", "                ", "episodes", "=", "[", "]", "\n", "lines", "=", "fh", ".", "readlines", "(", ")", "\n", "header", "=", "json", ".", "loads", "(", "lines", "[", "0", "]", ")", "\n", "headers", ".", "append", "(", "header", ")", "\n", "for", "line", "in", "lines", "[", "1", ":", "]", ":", "\n", "                    ", "episode", "=", "json", ".", "loads", "(", "line", ")", "\n", "episodes", ".", "append", "(", "episode", ")", "\n", "", "df", "=", "pandas", ".", "DataFrame", "(", "episodes", ")", "\n", "", "else", ":", "\n", "                ", "assert", "0", ",", "'unreachable'", "\n", "", "df", "[", "'t'", "]", "+=", "header", "[", "'t_start'", "]", "\n", "", "dfs", ".", "append", "(", "df", ")", "\n", "", "df", "=", "pandas", ".", "concat", "(", "dfs", ")", "\n", "df", ".", "sort_values", "(", "'t'", ",", "inplace", "=", "True", ")", "\n", "df", ".", "reset_index", "(", "inplace", "=", "True", ")", "\n", "df", "[", "'t'", "]", "-=", "min", "(", "header", "[", "'t_start'", "]", "for", "header", "in", "headers", ")", "\n", "df", ".", "headers", "=", "headers", "# HACK to preserve backwards compatibility", "\n", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.monitor.test_monitor": [[166, 189], ["gym.make", "gym.make.seed", "monitor.Monitor", "monitor.Monitor.reset", "range", "open", "open.readline", "f.readline.startswith", "json.loads", "pandas.read_csv", "open.close", "os.remove", "uuid.uuid4", "monitor.Monitor.step", "set", "set", "monitor.Monitor.reset", "json.loads.keys", "pandas.read_csv.keys"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.read_csv", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.step", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset"], ["", "def", "test_monitor", "(", ")", ":", "\n", "    ", "env", "=", "gym", ".", "make", "(", "\"CartPole-v1\"", ")", "\n", "env", ".", "seed", "(", "0", ")", "\n", "mon_file", "=", "\"/tmp/baselines-test-%s.monitor.csv\"", "%", "uuid", ".", "uuid4", "(", ")", "\n", "menv", "=", "Monitor", "(", "env", ",", "mon_file", ")", "\n", "menv", ".", "reset", "(", ")", "\n", "for", "_", "in", "range", "(", "1000", ")", ":", "\n", "        ", "_", ",", "_", ",", "done", ",", "_", "=", "menv", ".", "step", "(", "0", ")", "\n", "if", "done", ":", "\n", "            ", "menv", ".", "reset", "(", ")", "\n", "\n", "", "", "f", "=", "open", "(", "mon_file", ",", "'rt'", ")", "\n", "\n", "firstline", "=", "f", ".", "readline", "(", ")", "\n", "assert", "firstline", ".", "startswith", "(", "'#'", ")", "\n", "metadata", "=", "json", ".", "loads", "(", "firstline", "[", "1", ":", "]", ")", "\n", "assert", "metadata", "[", "'env_id'", "]", "==", "\"CartPole-v1\"", "\n", "assert", "set", "(", "metadata", ".", "keys", "(", ")", ")", "==", "{", "'env_id'", ",", "'gym_version'", ",", "'t_start'", "}", ",", "\"Incorrect keys in monitor metadata\"", "\n", "\n", "last_logline", "=", "pandas", ".", "read_csv", "(", "f", ",", "index_col", "=", "None", ")", "\n", "assert", "set", "(", "last_logline", ".", "keys", "(", ")", ")", "==", "{", "'l'", ",", "'t'", ",", "'r'", "}", ",", "\"Incorrect keys in monitor logline\"", "\n", "f", ".", "close", "(", ")", "\n", "os", ".", "remove", "(", "mon_file", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.benchmarks.register_benchmark": [[14, 25], ["_BENCHMARKS.append", "ValueError", "remove_version_re.sub", "t.get", "t.get"], "function", ["None"], ["def", "register_benchmark", "(", "benchmark", ")", ":", "\n", "    ", "for", "b", "in", "_BENCHMARKS", ":", "\n", "        ", "if", "b", "[", "'name'", "]", "==", "benchmark", "[", "'name'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "'Benchmark with name %s already registered!'", "%", "b", "[", "'name'", "]", ")", "\n", "\n", "# automatically add a description if it is not present", "\n", "", "", "if", "'tasks'", "in", "benchmark", ":", "\n", "        ", "for", "t", "in", "benchmark", "[", "'tasks'", "]", ":", "\n", "            ", "if", "'desc'", "not", "in", "t", ":", "\n", "                ", "t", "[", "'desc'", "]", "=", "remove_version_re", ".", "sub", "(", "''", ",", "t", ".", "get", "(", "'env_id'", ",", "t", ".", "get", "(", "'id'", ")", ")", ")", "\n", "", "", "", "_BENCHMARKS", ".", "append", "(", "benchmark", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.benchmarks.list_benchmarks": [[27, 29], ["None"], "function", ["None"], ["", "def", "list_benchmarks", "(", ")", ":", "\n", "    ", "return", "[", "b", "[", "'name'", "]", "for", "b", "in", "_BENCHMARKS", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.benchmarks.get_benchmark": [[31, 36], ["ValueError", "benchmarks.list_benchmarks"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.benchmarks.list_benchmarks"], ["", "def", "get_benchmark", "(", "benchmark_name", ")", ":", "\n", "    ", "for", "b", "in", "_BENCHMARKS", ":", "\n", "        ", "if", "b", "[", "'name'", "]", "==", "benchmark_name", ":", "\n", "            ", "return", "b", "\n", "", "", "raise", "ValueError", "(", "'%s not found! Known benchmarks: %s'", "%", "(", "benchmark_name", ",", "list_benchmarks", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.benchmarks.get_task": [[38, 41], ["next", "filter"], "function", ["None"], ["", "def", "get_task", "(", "benchmark", ",", "env_id", ")", ":", "\n", "    ", "\"\"\"Get a task by env_id. Return None if the benchmark doesn't have the env\"\"\"", "\n", "return", "next", "(", "filter", "(", "lambda", "task", ":", "task", "[", "'env_id'", "]", "==", "env_id", ",", "benchmark", "[", "'tasks'", "]", ")", ",", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.bench.benchmarks.find_task_for_env_id_in_any_benchmark": [[43, 49], ["None"], "function", ["None"], ["", "def", "find_task_for_env_id_in_any_benchmark", "(", "env_id", ")", ":", "\n", "    ", "for", "bm", "in", "_BENCHMARKS", ":", "\n", "        ", "for", "task", "in", "bm", "[", "\"tasks\"", "]", ":", "\n", "            ", "if", "task", "[", "\"env_id\"", "]", "==", "env_id", ":", "\n", "                ", "return", "bm", ",", "task", "\n", "", "", "", "return", "None", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.make_vec_env": [[21, 53], ["baselines.logger.get_dir", "baselines.common.set_global_seeds", "MPI.COMM_WORLD.Get_rank", "baselines.common.vec_env.subproc_vec_env.SubprocVecEnv", "baselines.common.vec_env.dummy_vec_env.DummyVecEnv", "cmd_util.make_env", "cmd_util.make_vec_env.make_thunk"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.get_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.make_env"], ["def", "make_vec_env", "(", "env_id", ",", "env_type", ",", "num_env", ",", "seed", ",", "\n", "wrapper_kwargs", "=", "None", ",", "\n", "start_index", "=", "0", ",", "\n", "reward_scale", "=", "1.0", ",", "\n", "flatten_dict_observations", "=", "True", ",", "\n", "gamestate", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Create a wrapped, monitored SubprocVecEnv for Atari and MuJoCo.\n    \"\"\"", "\n", "wrapper_kwargs", "=", "wrapper_kwargs", "or", "{", "}", "\n", "mpi_rank", "=", "MPI", ".", "COMM_WORLD", ".", "Get_rank", "(", ")", "if", "MPI", "else", "0", "\n", "seed", "=", "seed", "+", "10000", "*", "mpi_rank", "if", "seed", "is", "not", "None", "else", "None", "\n", "logger_dir", "=", "logger", ".", "get_dir", "(", ")", "\n", "def", "make_thunk", "(", "rank", ")", ":", "\n", "        ", "return", "lambda", ":", "make_env", "(", "\n", "env_id", "=", "env_id", ",", "\n", "env_type", "=", "env_type", ",", "\n", "mpi_rank", "=", "mpi_rank", ",", "\n", "subrank", "=", "rank", ",", "\n", "seed", "=", "seed", ",", "\n", "reward_scale", "=", "reward_scale", ",", "\n", "gamestate", "=", "gamestate", ",", "\n", "flatten_dict_observations", "=", "flatten_dict_observations", ",", "\n", "wrapper_kwargs", "=", "wrapper_kwargs", ",", "\n", "logger_dir", "=", "logger_dir", "\n", ")", "\n", "\n", "", "set_global_seeds", "(", "seed", ")", "\n", "if", "num_env", ">", "1", ":", "\n", "        ", "return", "SubprocVecEnv", "(", "[", "make_thunk", "(", "i", "+", "start_index", ")", "for", "i", "in", "range", "(", "num_env", ")", "]", ")", "\n", "", "else", ":", "\n", "        ", "return", "DummyVecEnv", "(", "[", "make_thunk", "(", "start_index", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.make_env": [[55, 84], ["retro_wrappers.wrap_deepmind_retro.seed", "baselines.bench.Monitor", "baselines.common.atari_wrappers.make_atari", "isinstance", "retro_wrappers.wrap_deepmind_retro.observation_space.spaces.keys", "gym.wrappers.FlattenDictWrapper", "baselines.common.atari_wrappers.wrap_deepmind", "baselines.common.retro_wrappers.RewardScaler", "baselines.common.retro_wrappers.make_retro", "gym.make", "os.path.join", "baselines.common.retro_wrappers.wrap_deepmind_retro", "list", "str", "str"], "function", ["None"], ["", "", "def", "make_env", "(", "env_id", ",", "env_type", ",", "mpi_rank", "=", "0", ",", "subrank", "=", "0", ",", "seed", "=", "None", ",", "reward_scale", "=", "1.0", ",", "gamestate", "=", "None", ",", "flatten_dict_observations", "=", "True", ",", "wrapper_kwargs", "=", "None", ",", "logger_dir", "=", "None", ")", ":", "\n", "    ", "wrapper_kwargs", "=", "wrapper_kwargs", "or", "{", "}", "\n", "if", "env_type", "==", "'atari'", ":", "\n", "        ", "env", "=", "make_atari", "(", "env_id", ")", "\n", "", "elif", "env_type", "==", "'retro'", ":", "\n", "        ", "import", "retro", "\n", "gamestate", "=", "gamestate", "or", "retro", ".", "State", ".", "DEFAULT", "\n", "env", "=", "retro_wrappers", ".", "make_retro", "(", "game", "=", "env_id", ",", "max_episode_steps", "=", "10000", ",", "use_restricted_actions", "=", "retro", ".", "Actions", ".", "DISCRETE", ",", "state", "=", "gamestate", ")", "\n", "", "else", ":", "\n", "        ", "env", "=", "gym", ".", "make", "(", "env_id", ")", "\n", "\n", "", "if", "flatten_dict_observations", "and", "isinstance", "(", "env", ".", "observation_space", ",", "gym", ".", "spaces", ".", "Dict", ")", ":", "\n", "        ", "keys", "=", "env", ".", "observation_space", ".", "spaces", ".", "keys", "(", ")", "\n", "env", "=", "gym", ".", "wrappers", ".", "FlattenDictWrapper", "(", "env", ",", "dict_keys", "=", "list", "(", "keys", ")", ")", "\n", "\n", "", "env", ".", "seed", "(", "seed", "+", "subrank", "if", "seed", "is", "not", "None", "else", "None", ")", "\n", "env", "=", "Monitor", "(", "env", ",", "\n", "logger_dir", "and", "os", ".", "path", ".", "join", "(", "logger_dir", ",", "str", "(", "mpi_rank", ")", "+", "'.'", "+", "str", "(", "subrank", ")", ")", ",", "\n", "allow_early_resets", "=", "True", ")", "\n", "\n", "if", "env_type", "==", "'atari'", ":", "\n", "        ", "env", "=", "wrap_deepmind", "(", "env", ",", "**", "wrapper_kwargs", ")", "\n", "", "elif", "env_type", "==", "'retro'", ":", "\n", "        ", "env", "=", "retro_wrappers", ".", "wrap_deepmind_retro", "(", "env", ",", "**", "wrapper_kwargs", ")", "\n", "\n", "", "if", "reward_scale", "!=", "1", ":", "\n", "        ", "env", "=", "retro_wrappers", ".", "RewardScaler", "(", "env", ",", "reward_scale", ")", "\n", "\n", "", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.make_mujoco_env": [[86, 101], ["MPI.COMM_WORLD.Get_rank", "baselines.common.set_global_seeds", "gym.make", "baselines.bench.Monitor", "RewardScaler.seed", "os.path.join", "RewardScaler", "baselines.logger.get_dir", "baselines.logger.get_dir", "str"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.get_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.get_dir"], ["", "def", "make_mujoco_env", "(", "env_id", ",", "seed", ",", "reward_scale", "=", "1.0", ")", ":", "\n", "    ", "\"\"\"\n    Create a wrapped, monitored gym.Env for MuJoCo.\n    \"\"\"", "\n", "rank", "=", "MPI", ".", "COMM_WORLD", ".", "Get_rank", "(", ")", "\n", "myseed", "=", "seed", "+", "1000", "*", "rank", "if", "seed", "is", "not", "None", "else", "None", "\n", "set_global_seeds", "(", "myseed", ")", "\n", "env", "=", "gym", ".", "make", "(", "env_id", ")", "\n", "logger_path", "=", "None", "if", "logger", ".", "get_dir", "(", ")", "is", "None", "else", "os", ".", "path", ".", "join", "(", "logger", ".", "get_dir", "(", ")", ",", "str", "(", "rank", ")", ")", "\n", "env", "=", "Monitor", "(", "env", ",", "logger_path", ",", "allow_early_resets", "=", "True", ")", "\n", "env", ".", "seed", "(", "seed", ")", "\n", "if", "reward_scale", "!=", "1.0", ":", "\n", "        ", "from", "baselines", ".", "common", ".", "retro_wrappers", "import", "RewardScaler", "\n", "env", "=", "RewardScaler", "(", "env", ",", "reward_scale", ")", "\n", "", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.make_robotics_env": [[102, 114], ["baselines.common.set_global_seeds", "gym.make", "gym.wrappers.FlattenDictWrapper", "baselines.bench.Monitor", "baselines.bench.Monitor.seed", "baselines.logger.get_dir", "os.path.join", "baselines.logger.get_dir", "str"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.get_dir", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.get_dir"], ["", "def", "make_robotics_env", "(", "env_id", ",", "seed", ",", "rank", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n    Create a wrapped, monitored gym.Env for MuJoCo.\n    \"\"\"", "\n", "set_global_seeds", "(", "seed", ")", "\n", "env", "=", "gym", ".", "make", "(", "env_id", ")", "\n", "env", "=", "FlattenDictWrapper", "(", "env", ",", "[", "'observation'", ",", "'desired_goal'", "]", ")", "\n", "env", "=", "Monitor", "(", "\n", "env", ",", "logger", ".", "get_dir", "(", ")", "and", "os", ".", "path", ".", "join", "(", "logger", ".", "get_dir", "(", ")", ",", "str", "(", "rank", ")", ")", ",", "\n", "info_keywords", "=", "(", "'is_success'", ",", ")", ")", "\n", "env", ".", "seed", "(", "seed", ")", "\n", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.arg_parser": [[115, 121], ["argparse.ArgumentParser"], "function", ["None"], ["", "def", "arg_parser", "(", ")", ":", "\n", "    ", "\"\"\"\n    Create an empty argparse.ArgumentParser.\n    \"\"\"", "\n", "import", "argparse", "\n", "return", "argparse", ".", "ArgumentParser", "(", "formatter_class", "=", "argparse", ".", "ArgumentDefaultsHelpFormatter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.atari_arg_parser": [[122, 128], ["print", "cmd_util.common_arg_parser"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.common_arg_parser"], ["", "def", "atari_arg_parser", "(", ")", ":", "\n", "    ", "\"\"\"\n    Create an argparse.ArgumentParser for run_atari.py.\n    \"\"\"", "\n", "print", "(", "'Obsolete - use common_arg_parser instead'", ")", "\n", "return", "common_arg_parser", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.mujoco_arg_parser": [[129, 132], ["print", "cmd_util.common_arg_parser"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.common_arg_parser"], ["", "def", "mujoco_arg_parser", "(", ")", ":", "\n", "    ", "print", "(", "'Obsolete - use common_arg_parser instead'", ")", "\n", "return", "common_arg_parser", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.common_arg_parser": [[133, 153], ["cmd_util.arg_parser", "arg_parser.add_argument", "arg_parser.add_argument", "arg_parser.add_argument", "arg_parser.add_argument", "arg_parser.add_argument", "arg_parser.add_argument", "arg_parser.add_argument", "arg_parser.add_argument", "arg_parser.add_argument", "arg_parser.add_argument", "arg_parser.add_argument", "arg_parser.add_argument", "arg_parser.add_argument", "arg_parser.add_argument"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.arg_parser"], ["", "def", "common_arg_parser", "(", ")", ":", "\n", "    ", "\"\"\"\n    Create an argparse.ArgumentParser for run_mujoco.py.\n    \"\"\"", "\n", "parser", "=", "arg_parser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--env'", ",", "help", "=", "'environment ID'", ",", "type", "=", "str", ",", "default", "=", "'Reacher-v2'", ")", "\n", "parser", ".", "add_argument", "(", "'--env_type'", ",", "help", "=", "'type of environment, used when the environment type cannot be automatically determined'", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "help", "=", "'RNG seed'", ",", "type", "=", "int", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--alg'", ",", "help", "=", "'Algorithm'", ",", "type", "=", "str", ",", "default", "=", "'ppo2'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_timesteps'", ",", "type", "=", "float", ",", "default", "=", "1e6", ")", ",", "\n", "parser", ".", "add_argument", "(", "'--network'", ",", "help", "=", "'network type (mlp, cnn, lstm, cnn_lstm, conv_only)'", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--gamestate'", ",", "help", "=", "'game state to load (so far only used in retro games)'", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--num_env'", ",", "help", "=", "'Number of environment copies being run in parallel. When not specified, set to number of cpus for Atari, and to 1 for Mujoco'", ",", "default", "=", "None", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'--reward_scale'", ",", "help", "=", "'Reward scale factor. Default: 1.0'", ",", "default", "=", "1.0", ",", "type", "=", "float", ")", "\n", "parser", ".", "add_argument", "(", "'--save_path'", ",", "help", "=", "'Path to save trained model to'", ",", "default", "=", "None", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "'--save_video_interval'", ",", "help", "=", "'Save video every x steps (0 = disabled)'", ",", "default", "=", "0", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'--save_video_length'", ",", "help", "=", "'Length of recorded video. Default: 200'", ",", "default", "=", "200", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'--play'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--extra_import'", ",", "help", "=", "'Extra module to import to access external environments'", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.robotics_arg_parser": [[154, 163], ["cmd_util.arg_parser", "arg_parser.add_argument", "arg_parser.add_argument", "arg_parser.add_argument", "int"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.arg_parser"], ["", "def", "robotics_arg_parser", "(", ")", ":", "\n", "    ", "\"\"\"\n    Create an argparse.ArgumentParser for run_mujoco.py.\n    \"\"\"", "\n", "parser", "=", "arg_parser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--env'", ",", "help", "=", "'environment ID'", ",", "type", "=", "str", ",", "default", "=", "'FetchReach-v0'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "help", "=", "'RNG seed'", ",", "type", "=", "int", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--num-timesteps'", ",", "type", "=", "int", ",", "default", "=", "int", "(", "1e6", ")", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.cmd_util.parse_unknown_args": [[165, 185], ["arg.startswith", "arg.split", "arg.split"], "function", ["None"], ["", "def", "parse_unknown_args", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n    Parse arguments not consumed by arg parser into a dicitonary\n    \"\"\"", "\n", "retval", "=", "{", "}", "\n", "preceded_by_key", "=", "False", "\n", "for", "arg", "in", "args", ":", "\n", "        ", "if", "arg", ".", "startswith", "(", "'--'", ")", ":", "\n", "            ", "if", "'='", "in", "arg", ":", "\n", "                ", "key", "=", "arg", ".", "split", "(", "'='", ")", "[", "0", "]", "[", "2", ":", "]", "\n", "value", "=", "arg", ".", "split", "(", "'='", ")", "[", "1", "]", "\n", "retval", "[", "key", "]", "=", "value", "\n", "", "else", ":", "\n", "                ", "key", "=", "arg", "[", "2", ":", "]", "\n", "preceded_by_key", "=", "True", "\n", "", "", "elif", "preceded_by_key", ":", "\n", "            ", "retval", "[", "key", "]", "=", "arg", "\n", "preceded_by_key", "=", "False", "\n", "\n", "", "", "return", "retval", "\n", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.running_mean_std.RunningMeanStd.__init__": [[7, 11], ["numpy.zeros", "numpy.ones"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "epsilon", "=", "1e-4", ",", "shape", "=", "(", ")", ")", ":", "\n", "        ", "self", ".", "mean", "=", "np", ".", "zeros", "(", "shape", ",", "'float64'", ")", "\n", "self", ".", "var", "=", "np", ".", "ones", "(", "shape", ",", "'float64'", ")", "\n", "self", ".", "count", "=", "epsilon", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.running_mean_std.RunningMeanStd.update": [[12, 17], ["numpy.mean", "numpy.var", "running_mean_std.RunningMeanStd.update_from_moments"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.running_mean_std.RunningMeanStd.update_from_moments"], ["", "def", "update", "(", "self", ",", "x", ")", ":", "\n", "        ", "batch_mean", "=", "np", ".", "mean", "(", "x", ",", "axis", "=", "0", ")", "\n", "batch_var", "=", "np", ".", "var", "(", "x", ",", "axis", "=", "0", ")", "\n", "batch_count", "=", "x", ".", "shape", "[", "0", "]", "\n", "self", ".", "update_from_moments", "(", "batch_mean", ",", "batch_var", ",", "batch_count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.running_mean_std.RunningMeanStd.update_from_moments": [[18, 21], ["running_mean_std.update_mean_var_count_from_moments"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.running_mean_std.update_mean_var_count_from_moments"], ["", "def", "update_from_moments", "(", "self", ",", "batch_mean", ",", "batch_var", ",", "batch_count", ")", ":", "\n", "        ", "self", ".", "mean", ",", "self", ".", "var", ",", "self", ".", "count", "=", "update_mean_var_count_from_moments", "(", "\n", "self", ".", "mean", ",", "self", ".", "var", ",", "self", ".", "count", ",", "batch_mean", ",", "batch_var", ",", "batch_count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.running_mean_std.update_mean_var_count_from_moments": [[22, 34], ["numpy.square"], "function", ["None"], ["", "", "def", "update_mean_var_count_from_moments", "(", "mean", ",", "var", ",", "count", ",", "batch_mean", ",", "batch_var", ",", "batch_count", ")", ":", "\n", "    ", "delta", "=", "batch_mean", "-", "mean", "\n", "tot_count", "=", "count", "+", "batch_count", "\n", "\n", "new_mean", "=", "mean", "+", "delta", "*", "batch_count", "/", "tot_count", "\n", "m_a", "=", "var", "*", "count", "\n", "m_b", "=", "batch_var", "*", "batch_count", "\n", "M2", "=", "m_a", "+", "m_b", "+", "np", ".", "square", "(", "delta", ")", "*", "count", "*", "batch_count", "/", "tot_count", "\n", "new_var", "=", "M2", "/", "tot_count", "\n", "new_count", "=", "tot_count", "\n", "\n", "return", "new_mean", ",", "new_var", ",", "new_count", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.running_mean_std.test_runningmeanstd": [[85, 101], ["running_mean_std.RunningMeanStd", "numpy.concatenate", "running_mean_std.RunningMeanStd.update", "running_mean_std.RunningMeanStd.update", "running_mean_std.RunningMeanStd.update", "numpy.testing.assert_allclose", "numpy.random.randn", "numpy.random.randn", "numpy.random.randn", "numpy.random.randn", "numpy.random.randn", "numpy.random.randn", "np.concatenate.mean", "np.concatenate.var"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update"], ["", "def", "test_runningmeanstd", "(", ")", ":", "\n", "    ", "for", "(", "x1", ",", "x2", ",", "x3", ")", "in", "[", "\n", "(", "np", ".", "random", ".", "randn", "(", "3", ")", ",", "np", ".", "random", ".", "randn", "(", "4", ")", ",", "np", ".", "random", ".", "randn", "(", "5", ")", ")", ",", "\n", "(", "np", ".", "random", ".", "randn", "(", "3", ",", "2", ")", ",", "np", ".", "random", ".", "randn", "(", "4", ",", "2", ")", ",", "np", ".", "random", ".", "randn", "(", "5", ",", "2", ")", ")", ",", "\n", "]", ":", "\n", "\n", "        ", "rms", "=", "RunningMeanStd", "(", "epsilon", "=", "0.0", ",", "shape", "=", "x1", ".", "shape", "[", "1", ":", "]", ")", "\n", "\n", "x", "=", "np", ".", "concatenate", "(", "[", "x1", ",", "x2", ",", "x3", "]", ",", "axis", "=", "0", ")", "\n", "ms1", "=", "[", "x", ".", "mean", "(", "axis", "=", "0", ")", ",", "x", ".", "var", "(", "axis", "=", "0", ")", "]", "\n", "rms", ".", "update", "(", "x1", ")", "\n", "rms", ".", "update", "(", "x2", ")", "\n", "rms", ".", "update", "(", "x3", ")", "\n", "ms2", "=", "[", "rms", ".", "mean", ",", "rms", ".", "var", "]", "\n", "\n", "np", ".", "testing", ".", "assert_allclose", "(", "ms1", ",", "ms2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.running_mean_std.test_tf_runningmeanstd": [[102, 118], ["TfRunningMeanStd", "numpy.concatenate", "TfRunningMeanStd.update", "TfRunningMeanStd.update", "TfRunningMeanStd.update", "numpy.testing.assert_allclose", "numpy.random.randn", "numpy.random.randn", "numpy.random.randn", "numpy.random.randn", "numpy.random.randn", "numpy.random.randn", "np.concatenate.mean", "np.concatenate.var", "str", "numpy.random.randint"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update"], ["", "", "def", "test_tf_runningmeanstd", "(", ")", ":", "\n", "    ", "for", "(", "x1", ",", "x2", ",", "x3", ")", "in", "[", "\n", "(", "np", ".", "random", ".", "randn", "(", "3", ")", ",", "np", ".", "random", ".", "randn", "(", "4", ")", ",", "np", ".", "random", ".", "randn", "(", "5", ")", ")", ",", "\n", "(", "np", ".", "random", ".", "randn", "(", "3", ",", "2", ")", ",", "np", ".", "random", ".", "randn", "(", "4", ",", "2", ")", ",", "np", ".", "random", ".", "randn", "(", "5", ",", "2", ")", ")", ",", "\n", "]", ":", "\n", "\n", "        ", "rms", "=", "TfRunningMeanStd", "(", "epsilon", "=", "0.0", ",", "shape", "=", "x1", ".", "shape", "[", "1", ":", "]", ",", "scope", "=", "'running_mean_std'", "+", "str", "(", "np", ".", "random", ".", "randint", "(", "0", ",", "128", ")", ")", ")", "\n", "\n", "x", "=", "np", ".", "concatenate", "(", "[", "x1", ",", "x2", ",", "x3", "]", ",", "axis", "=", "0", ")", "\n", "ms1", "=", "[", "x", ".", "mean", "(", "axis", "=", "0", ")", ",", "x", ".", "var", "(", "axis", "=", "0", ")", "]", "\n", "rms", ".", "update", "(", "x1", ")", "\n", "rms", ".", "update", "(", "x2", ")", "\n", "rms", ".", "update", "(", "x3", ")", "\n", "ms2", "=", "[", "rms", ".", "mean", ",", "rms", ".", "var", "]", "\n", "\n", "np", ".", "testing", ".", "assert_allclose", "(", "ms1", ",", "ms2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.common.running_mean_std.profile_tf_runningmeanstd": [[120, 183], ["tf_util.get_session", "numpy.random.random", "running_mean_std.RunningMeanStd", "TfRunningMeanStd", "time.time", "range", "time.time", "range", "time.time", "print", "print", "time.time", "range", "time.time", "range", "time.time", "print", "print", "running_mean_std.RunningMeanStd.update", "TfRunningMeanStd.update", "tensorflow.ConfigProto"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update"], ["", "", "def", "profile_tf_runningmeanstd", "(", ")", ":", "\n", "    ", "import", "time", "\n", "from", "baselines", ".", "common", "import", "tf_util", "\n", "\n", "tf_util", ".", "get_session", "(", "config", "=", "tf", ".", "ConfigProto", "(", "\n", "inter_op_parallelism_threads", "=", "1", ",", "\n", "intra_op_parallelism_threads", "=", "1", ",", "\n", "allow_soft_placement", "=", "True", "\n", ")", ")", "\n", "\n", "x", "=", "np", ".", "random", ".", "random", "(", "(", "376", ",", ")", ")", "\n", "\n", "n_trials", "=", "10000", "\n", "rms", "=", "RunningMeanStd", "(", ")", "\n", "tfrms", "=", "TfRunningMeanStd", "(", ")", "\n", "\n", "tic1", "=", "time", ".", "time", "(", ")", "\n", "for", "_", "in", "range", "(", "n_trials", ")", ":", "\n", "        ", "rms", ".", "update", "(", "x", ")", "\n", "\n", "", "tic2", "=", "time", ".", "time", "(", ")", "\n", "for", "_", "in", "range", "(", "n_trials", ")", ":", "\n", "        ", "tfrms", ".", "update", "(", "x", ")", "\n", "\n", "", "tic3", "=", "time", ".", "time", "(", ")", "\n", "\n", "print", "(", "'rms update time ({} trials): {} s'", ".", "format", "(", "n_trials", ",", "tic2", "-", "tic1", ")", ")", "\n", "print", "(", "'tfrms update time ({} trials): {} s'", ".", "format", "(", "n_trials", ",", "tic3", "-", "tic2", ")", ")", "\n", "\n", "\n", "tic1", "=", "time", ".", "time", "(", ")", "\n", "for", "_", "in", "range", "(", "n_trials", ")", ":", "\n", "        ", "z1", "=", "rms", ".", "mean", "\n", "\n", "", "tic2", "=", "time", ".", "time", "(", ")", "\n", "for", "_", "in", "range", "(", "n_trials", ")", ":", "\n", "        ", "z2", "=", "tfrms", ".", "mean", "\n", "\n", "", "assert", "z1", "==", "z2", "\n", "\n", "tic3", "=", "time", ".", "time", "(", ")", "\n", "\n", "print", "(", "'rms get mean time ({} trials): {} s'", ".", "format", "(", "n_trials", ",", "tic2", "-", "tic1", ")", ")", "\n", "print", "(", "'tfrms get mean time ({} trials): {} s'", ".", "format", "(", "n_trials", ",", "tic3", "-", "tic2", ")", ")", "\n", "\n", "\n", "\n", "'''\n    options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE) #pylint: disable=E1101\n    run_metadata = tf.RunMetadata()\n    profile_opts = dict(options=options, run_metadata=run_metadata)\n\n\n\n    from tensorflow.python.client import timeline\n    fetched_timeline = timeline.Timeline(run_metadata.step_stats) #pylint: disable=E1101\n    chrome_trace = fetched_timeline.generate_chrome_trace_format()\n    outfile = '/tmp/timeline.json'\n    with open(outfile, 'wt') as f:\n        f.write(chrome_trace)\n    print(f'Successfully saved profile to {outfile}. Exiting.')\n    exit(0)\n    '''", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.test_vec_env.SimpleEnv.__init__": [[76, 88], ["numpy.random.seed", "numpy.array", "gym.spaces.Box", "numpy.random.randint"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "seed", ",", "shape", ",", "dtype", ")", ":", "\n", "        ", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "self", ".", "_dtype", "=", "dtype", "\n", "self", ".", "_start_obs", "=", "np", ".", "array", "(", "np", ".", "random", ".", "randint", "(", "0", ",", "0x100", ",", "size", "=", "shape", ")", ",", "\n", "dtype", "=", "dtype", ")", "\n", "self", ".", "_max_steps", "=", "seed", "+", "1", "\n", "self", ".", "_cur_obs", "=", "None", "\n", "self", ".", "_cur_step", "=", "0", "\n", "# this is 0xFF instead of 0x100 because the Box space includes", "\n", "# the high end, while randint does not", "\n", "self", ".", "action_space", "=", "gym", ".", "spaces", ".", "Box", "(", "low", "=", "0", ",", "high", "=", "0xFF", ",", "shape", "=", "shape", ",", "dtype", "=", "dtype", ")", "\n", "self", ".", "observation_space", "=", "self", ".", "action_space", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.test_vec_env.SimpleEnv.step": [[89, 95], ["numpy.array", "str"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "self", ".", "_cur_obs", "+=", "np", ".", "array", "(", "action", ",", "dtype", "=", "self", ".", "_dtype", ")", "\n", "self", ".", "_cur_step", "+=", "1", "\n", "done", "=", "self", ".", "_cur_step", ">=", "self", ".", "_max_steps", "\n", "reward", "=", "self", ".", "_cur_step", "/", "self", ".", "_max_steps", "\n", "return", "self", ".", "_cur_obs", ",", "reward", ",", "done", ",", "{", "'foo'", ":", "'bar'", "+", "str", "(", "reward", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.test_vec_env.SimpleEnv.reset": [[96, 100], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_cur_obs", "=", "self", ".", "_start_obs", "\n", "self", ".", "_cur_step", "=", "0", "\n", "return", "self", ".", "_cur_obs", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.test_vec_env.SimpleEnv.render": [[101, 103], ["None"], "methods", ["None"], ["", "def", "render", "(", "self", ",", "mode", "=", "None", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.test_vec_env.assert_venvs_equal": [[14, 45], ["numpy.allclose", "venv1.action_space.seed", "range", "venv1.close", "venv2.close", "venv1.reset", "venv2.reset", "numpy.array", "venv1.step_wait", "venv2.step_wait", "zip", "numpy.array", "numpy.array", "numpy.array", "venv.step_async", "numpy.allclose", "list", "list", "venv1.action_space.sample", "range", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.step_wait", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.step_wait", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.step_async"], ["def", "assert_venvs_equal", "(", "venv1", ",", "venv2", ",", "num_steps", ")", ":", "\n", "    ", "\"\"\"\n    Compare two environments over num_steps steps and make sure\n    that the observations produced by each are the same when given\n    the same actions.\n    \"\"\"", "\n", "assert", "venv1", ".", "num_envs", "==", "venv2", ".", "num_envs", "\n", "assert", "venv1", ".", "observation_space", ".", "shape", "==", "venv2", ".", "observation_space", ".", "shape", "\n", "assert", "venv1", ".", "observation_space", ".", "dtype", "==", "venv2", ".", "observation_space", ".", "dtype", "\n", "assert", "venv1", ".", "action_space", ".", "shape", "==", "venv2", ".", "action_space", ".", "shape", "\n", "assert", "venv1", ".", "action_space", ".", "dtype", "==", "venv2", ".", "action_space", ".", "dtype", "\n", "\n", "try", ":", "\n", "        ", "obs1", ",", "obs2", "=", "venv1", ".", "reset", "(", ")", ",", "venv2", ".", "reset", "(", ")", "\n", "assert", "np", ".", "array", "(", "obs1", ")", ".", "shape", "==", "np", ".", "array", "(", "obs2", ")", ".", "shape", "\n", "assert", "np", ".", "array", "(", "obs1", ")", ".", "shape", "==", "(", "venv1", ".", "num_envs", ",", ")", "+", "venv1", ".", "observation_space", ".", "shape", "\n", "assert", "np", ".", "allclose", "(", "obs1", ",", "obs2", ")", "\n", "venv1", ".", "action_space", ".", "seed", "(", "1337", ")", "\n", "for", "_", "in", "range", "(", "num_steps", ")", ":", "\n", "            ", "actions", "=", "np", ".", "array", "(", "[", "venv1", ".", "action_space", ".", "sample", "(", ")", "for", "_", "in", "range", "(", "venv1", ".", "num_envs", ")", "]", ")", "\n", "for", "venv", "in", "[", "venv1", ",", "venv2", "]", ":", "\n", "                ", "venv", ".", "step_async", "(", "actions", ")", "\n", "", "outs1", "=", "venv1", ".", "step_wait", "(", ")", "\n", "outs2", "=", "venv2", ".", "step_wait", "(", ")", "\n", "for", "out1", ",", "out2", "in", "zip", "(", "outs1", "[", ":", "3", "]", ",", "outs2", "[", ":", "3", "]", ")", ":", "\n", "                ", "assert", "np", ".", "array", "(", "out1", ")", ".", "shape", "==", "np", ".", "array", "(", "out2", ")", ".", "shape", "\n", "assert", "np", ".", "allclose", "(", "out1", ",", "out2", ")", "\n", "", "assert", "list", "(", "outs1", "[", "3", "]", ")", "==", "list", "(", "outs2", "[", "3", "]", ")", "\n", "", "", "finally", ":", "\n", "        ", "venv1", ".", "close", "(", ")", "\n", "venv2", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.test_vec_env.test_vec_env": [[47, 68], ["pytest.mark.parametrize", "pytest.mark.parametrize", "dummy_vec_env.DummyVecEnv", "klass", "test_vec_env.assert_venvs_equal", "test_vec_env.test_vec_env.make_fn"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.test_vec_env.assert_venvs_equal"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'klass'", ",", "(", "ShmemVecEnv", ",", "SubprocVecEnv", ")", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'dtype'", ",", "(", "'uint8'", ",", "'float32'", ")", ")", "\n", "def", "test_vec_env", "(", "klass", ",", "dtype", ")", ":", "# pylint: disable=R0914", "\n", "    ", "\"\"\"\n    Test that a vectorized environment is equivalent to\n    DummyVecEnv, since DummyVecEnv is less likely to be\n    error prone.\n    \"\"\"", "\n", "num_envs", "=", "3", "\n", "num_steps", "=", "100", "\n", "shape", "=", "(", "3", ",", "8", ")", "\n", "\n", "def", "make_fn", "(", "seed", ")", ":", "\n", "        ", "\"\"\"\n        Get an environment constructor with a seed.\n        \"\"\"", "\n", "return", "lambda", ":", "SimpleEnv", "(", "seed", ",", "shape", ",", "dtype", ")", "\n", "", "fns", "=", "[", "make_fn", "(", "i", ")", "for", "i", "in", "range", "(", "num_envs", ")", "]", "\n", "env1", "=", "DummyVecEnv", "(", "fns", ")", "\n", "env2", "=", "klass", "(", "fns", ")", "\n", "assert_venvs_equal", "(", "env1", ",", "env2", ",", "num_steps", "=", "num_steps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.test_vec_env.test_mpi_with_subprocvecenv": [[106, 114], ["baselines.common.tests.test_with_mpi.with_mpi", "subproc_vec_env.SubprocVecEnv", "subproc_vec_env.SubprocVecEnv.reset", "subproc_vec_env.SubprocVecEnv.close", "test_vec_env.SimpleEnv"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close"], ["", "", "@", "with_mpi", "(", ")", "\n", "def", "test_mpi_with_subprocvecenv", "(", ")", ":", "\n", "    ", "shape", "=", "(", "2", ",", "3", ",", "4", ")", "\n", "nenv", "=", "1", "\n", "venv", "=", "SubprocVecEnv", "(", "[", "lambda", ":", "SimpleEnv", "(", "0", ",", "shape", ",", "'float32'", ")", "]", "*", "nenv", ")", "\n", "ob", "=", "venv", ".", "reset", "(", ")", "\n", "venv", ".", "close", "(", ")", "\n", "assert", "ob", ".", "shape", "==", "(", "nenv", ",", ")", "+", "shape", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_frame_stack.VecFrameStack.__init__": [[7, 16], ["numpy.repeat", "numpy.repeat", "numpy.zeros", "gym.spaces.Box", "vec_env.VecEnvWrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "venv", ",", "nstack", ")", ":", "\n", "        ", "self", ".", "venv", "=", "venv", "\n", "self", ".", "nstack", "=", "nstack", "\n", "wos", "=", "venv", ".", "observation_space", "# wrapped ob space", "\n", "low", "=", "np", ".", "repeat", "(", "wos", ".", "low", ",", "self", ".", "nstack", ",", "axis", "=", "-", "1", ")", "\n", "high", "=", "np", ".", "repeat", "(", "wos", ".", "high", ",", "self", ".", "nstack", ",", "axis", "=", "-", "1", ")", "\n", "self", ".", "stackedobs", "=", "np", ".", "zeros", "(", "(", "venv", ".", "num_envs", ",", ")", "+", "low", ".", "shape", ",", "low", ".", "dtype", ")", "\n", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "low", ",", "high", "=", "high", ",", "dtype", "=", "venv", ".", "observation_space", ".", "dtype", ")", "\n", "VecEnvWrapper", ".", "__init__", "(", "self", ",", "venv", ",", "observation_space", "=", "observation_space", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_frame_stack.VecFrameStack.step_wait": [[17, 25], ["vec_frame_stack.VecFrameStack.venv.step_wait", "numpy.roll", "enumerate"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.step_wait"], ["", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "obs", ",", "rews", ",", "news", ",", "infos", "=", "self", ".", "venv", ".", "step_wait", "(", ")", "\n", "self", ".", "stackedobs", "=", "np", ".", "roll", "(", "self", ".", "stackedobs", ",", "shift", "=", "-", "1", ",", "axis", "=", "-", "1", ")", "\n", "for", "(", "i", ",", "new", ")", "in", "enumerate", "(", "news", ")", ":", "\n", "            ", "if", "new", ":", "\n", "                ", "self", ".", "stackedobs", "[", "i", "]", "=", "0", "\n", "", "", "self", ".", "stackedobs", "[", "...", ",", "-", "obs", ".", "shape", "[", "-", "1", "]", ":", "]", "=", "obs", "\n", "return", "self", ".", "stackedobs", ",", "rews", ",", "news", ",", "infos", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_frame_stack.VecFrameStack.reset": [[26, 31], ["vec_frame_stack.VecFrameStack.venv.reset"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "obs", "=", "self", ".", "venv", ".", "reset", "(", ")", "\n", "self", ".", "stackedobs", "[", "...", "]", "=", "0", "\n", "self", ".", "stackedobs", "[", "...", ",", "-", "obs", ".", "shape", "[", "-", "1", "]", ":", "]", "=", "obs", "\n", "return", "self", ".", "stackedobs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_remove_dict_obs.VecExtractDictObs.__init__": [[5, 9], ["vec_env.VecEnvObservationWrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "venv", ",", "key", ")", ":", "\n", "        ", "self", ".", "key", "=", "key", "\n", "super", "(", ")", ".", "__init__", "(", "venv", "=", "venv", ",", "\n", "observation_space", "=", "venv", ".", "observation_space", ".", "spaces", "[", "self", ".", "key", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_remove_dict_obs.VecExtractDictObs.process": [[10, 12], ["None"], "methods", ["None"], ["", "def", "process", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "obs", "[", "self", ".", "key", "]", "", "", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.AlreadySteppingError.__init__": [[13, 16], ["Exception.__init__"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "msg", "=", "'already running an async step'", "\n", "Exception", ".", "__init__", "(", "self", ",", "msg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.NotSteppingError.__init__": [[24, 27], ["Exception.__init__"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "msg", "=", "'not running an async step'", "\n", "Exception", ".", "__init__", "(", "self", ",", "msg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnv.__init__": [[43, 47], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "num_envs", ",", "observation_space", ",", "action_space", ")", ":", "\n", "        ", "self", ".", "num_envs", "=", "num_envs", "\n", "self", ".", "observation_space", "=", "observation_space", "\n", "self", ".", "action_space", "=", "action_space", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnv.reset": [[48, 59], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Reset all the environments and return an array of\n        observations, or a dict of observation arrays.\n\n        If step_async is still doing work, that work will\n        be cancelled and step_wait() should not be called\n        until step_async() is invoked again.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnv.step_async": [[60, 71], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "step_async", "(", "self", ",", "actions", ")", ":", "\n", "        ", "\"\"\"\n        Tell all the environments to start taking a step\n        with the given actions.\n        Call step_wait() to get the results of the step.\n\n        You should not call this if a step_async run is\n        already pending.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnv.step_wait": [[72, 85], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Wait for the step taken with step_async().\n\n        Returns (obs, rews, dones, infos):\n         - obs: an array of observations, or a dict of\n                arrays of observations.\n         - rews: an array of rewards\n         - dones: an array of \"episode done\" booleans\n         - infos: a sequence of info objects\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnv.close_extras": [[86, 92], ["None"], "methods", ["None"], ["", "def", "close_extras", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Clean up the  extra resources, beyond what's in this base class.\n        Only runs when not self.closed.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnv.close": [[93, 100], ["vec_env.VecEnv.close_extras", "vec_env.VecEnv.viewer.close"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.close_extras", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "closed", ":", "\n", "            ", "return", "\n", "", "if", "self", ".", "viewer", "is", "not", "None", ":", "\n", "            ", "self", ".", "viewer", ".", "close", "(", ")", "\n", "", "self", ".", "close_extras", "(", ")", "\n", "self", ".", "closed", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnv.step": [[101, 109], ["vec_env.VecEnv.step_async", "vec_env.VecEnv.step_wait"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.step_async", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.step_wait"], ["", "def", "step", "(", "self", ",", "actions", ")", ":", "\n", "        ", "\"\"\"\n        Step the environments synchronously.\n\n        This is available for backwards compatibility.\n        \"\"\"", "\n", "self", ".", "step_async", "(", "actions", ")", "\n", "return", "self", ".", "step_wait", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnv.render": [[110, 120], ["vec_env.VecEnv.get_images", "tile_images.tile_images.tile_images", "vec_env.VecEnv.get_viewer().imshow", "vec_env.VecEnv.get_viewer", "vec_env.VecEnv.get_viewer"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.get_images", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.tile_images.tile_images", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnv.get_viewer", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnv.get_viewer"], ["", "def", "render", "(", "self", ",", "mode", "=", "'human'", ")", ":", "\n", "        ", "imgs", "=", "self", ".", "get_images", "(", ")", "\n", "bigimg", "=", "tile_images", "(", "imgs", ")", "\n", "if", "mode", "==", "'human'", ":", "\n", "            ", "self", ".", "get_viewer", "(", ")", ".", "imshow", "(", "bigimg", ")", "\n", "return", "self", ".", "get_viewer", "(", ")", ".", "isopen", "\n", "", "elif", "mode", "==", "'rgb_array'", ":", "\n", "            ", "return", "bigimg", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnv.get_images": [[121, 126], ["None"], "methods", ["None"], ["", "", "def", "get_images", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return RGB images from each environment\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnv.unwrapped": [[127, 133], ["isinstance"], "methods", ["None"], ["", "@", "property", "\n", "def", "unwrapped", "(", "self", ")", ":", "\n", "        ", "if", "isinstance", "(", "self", ",", "VecEnvWrapper", ")", ":", "\n", "            ", "return", "self", ".", "venv", ".", "unwrapped", "\n", "", "else", ":", "\n", "            ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnv.get_viewer": [[134, 139], ["rendering.SimpleImageViewer"], "methods", ["None"], ["", "", "def", "get_viewer", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "viewer", "is", "None", ":", "\n", "            ", "from", "gym", ".", "envs", ".", "classic_control", "import", "rendering", "\n", "self", ".", "viewer", "=", "rendering", ".", "SimpleImageViewer", "(", ")", "\n", "", "return", "self", ".", "viewer", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnvWrapper.__init__": [[146, 152], ["vec_env.VecEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__"], ["def", "__init__", "(", "self", ",", "venv", ",", "observation_space", "=", "None", ",", "action_space", "=", "None", ")", ":", "\n", "        ", "self", ".", "venv", "=", "venv", "\n", "VecEnv", ".", "__init__", "(", "self", ",", "\n", "num_envs", "=", "venv", ".", "num_envs", ",", "\n", "observation_space", "=", "observation_space", "or", "venv", ".", "observation_space", ",", "\n", "action_space", "=", "action_space", "or", "venv", ".", "action_space", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnvWrapper.step_async": [[153, 155], ["vec_env.VecEnvWrapper.venv.step_async"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.step_async"], ["", "def", "step_async", "(", "self", ",", "actions", ")", ":", "\n", "        ", "self", ".", "venv", ".", "step_async", "(", "actions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnvWrapper.reset": [[156, 159], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnvWrapper.step_wait": [[160, 163], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnvWrapper.close": [[164, 166], ["vec_env.VecEnvWrapper.venv.close"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "venv", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnvWrapper.render": [[167, 169], ["vec_env.VecEnvWrapper.venv.render"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.dummy_vec_env.DummyVecEnv.render"], ["", "def", "render", "(", "self", ",", "mode", "=", "'human'", ")", ":", "\n", "        ", "return", "self", ".", "venv", ".", "render", "(", "mode", "=", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnvWrapper.get_images": [[170, 172], ["vec_env.VecEnvWrapper.venv.get_images"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.get_images"], ["", "def", "get_images", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "venv", ".", "get_images", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnvObservationWrapper.process": [[174, 177], ["None"], "methods", ["None"], ["    ", "@", "abstractmethod", "\n", "def", "process", "(", "self", ",", "obs", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnvObservationWrapper.reset": [[178, 181], ["vec_env.VecEnvObservationWrapper.venv.reset", "vec_env.VecEnvObservationWrapper.process"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnvObservationWrapper.process"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "obs", "=", "self", ".", "venv", ".", "reset", "(", ")", "\n", "return", "self", ".", "process", "(", "obs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnvObservationWrapper.step_wait": [[182, 185], ["vec_env.VecEnvObservationWrapper.venv.step_wait", "vec_env.VecEnvObservationWrapper.process"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.step_wait", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.VecEnvObservationWrapper.process"], ["", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "obs", ",", "rews", ",", "dones", ",", "infos", "=", "self", ".", "venv", ".", "step_wait", "(", ")", "\n", "return", "self", ".", "process", "(", "obs", ")", ",", "rews", ",", "dones", ",", "infos", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.CloudpickleWrapper.__init__": [[191, 193], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "x", ")", ":", "\n", "        ", "self", ".", "x", "=", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.CloudpickleWrapper.__getstate__": [[194, 197], ["cloudpickle.dumps"], "methods", ["None"], ["", "def", "__getstate__", "(", "self", ")", ":", "\n", "        ", "import", "cloudpickle", "\n", "return", "cloudpickle", ".", "dumps", "(", "self", ".", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.CloudpickleWrapper.__setstate__": [[198, 201], ["pickle.loads"], "methods", ["None"], ["", "def", "__setstate__", "(", "self", ",", "ob", ")", ":", "\n", "        ", "import", "pickle", "\n", "self", ".", "x", "=", "pickle", ".", "loads", "(", "ob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.clear_mpi_env_vars": [[203, 220], ["list", "os.environ.items", "os.environ.update", "k.startswith"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update"], ["", "", "@", "contextlib", ".", "contextmanager", "\n", "def", "clear_mpi_env_vars", "(", ")", ":", "\n", "    ", "\"\"\"\n    from mpi4py import MPI will call MPI_Init by default.  If the child process has MPI environment variables, MPI will think that the child process is an MPI process just like the parent and do bad things such as hang.\n    This context manager is a hacky way to clear those environment variables temporarily such as when we are starting multiprocessing\n    Processes.\n    \"\"\"", "\n", "removed_environment", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "list", "(", "os", ".", "environ", ".", "items", "(", ")", ")", ":", "\n", "        ", "for", "prefix", "in", "[", "'OMPI_'", ",", "'PMI_'", "]", ":", "\n", "            ", "if", "k", ".", "startswith", "(", "prefix", ")", ":", "\n", "                ", "removed_environment", "[", "k", "]", "=", "v", "\n", "del", "os", ".", "environ", "[", "k", "]", "\n", "", "", "", "try", ":", "\n", "        ", "yield", "\n", "", "finally", ":", "\n", "        ", "os", ".", "environ", ".", "update", "(", "removed_environment", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.subproc_vec_env.SubprocVecEnv.__init__": [[41, 65], ["len", "multiprocessing.get_context", "zip", "subproc_vec_env.SubprocVecEnv.remotes[].send", "subproc_vec_env.SubprocVecEnv.remotes[].recv", "vec_env.VecEnv.__init__", "multiprocessing.get_context.Process", "remote.close", "len", "zip", "vec_env.clear_mpi_env_vars", "p.start", "multiprocessing.get_context.Pipe", "range", "vec_env.CloudpickleWrapper"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.clear_mpi_env_vars"], ["def", "__init__", "(", "self", ",", "env_fns", ",", "spaces", "=", "None", ",", "context", "=", "'spawn'", ")", ":", "\n", "        ", "\"\"\"\n        Arguments:\n\n        env_fns: iterable of callables -  functions that create environments to run in subprocesses. Need to be cloud-pickleable\n        \"\"\"", "\n", "self", ".", "waiting", "=", "False", "\n", "self", ".", "closed", "=", "False", "\n", "nenvs", "=", "len", "(", "env_fns", ")", "\n", "ctx", "=", "mp", ".", "get_context", "(", "context", ")", "\n", "self", ".", "remotes", ",", "self", ".", "work_remotes", "=", "zip", "(", "*", "[", "ctx", ".", "Pipe", "(", ")", "for", "_", "in", "range", "(", "nenvs", ")", "]", ")", "\n", "self", ".", "ps", "=", "[", "ctx", ".", "Process", "(", "target", "=", "worker", ",", "args", "=", "(", "work_remote", ",", "remote", ",", "CloudpickleWrapper", "(", "env_fn", ")", ")", ")", "\n", "for", "(", "work_remote", ",", "remote", ",", "env_fn", ")", "in", "zip", "(", "self", ".", "work_remotes", ",", "self", ".", "remotes", ",", "env_fns", ")", "]", "\n", "for", "p", "in", "self", ".", "ps", ":", "\n", "            ", "p", ".", "daemon", "=", "True", "# if the main process crashes, we should not cause things to hang", "\n", "with", "clear_mpi_env_vars", "(", ")", ":", "\n", "                ", "p", ".", "start", "(", ")", "\n", "", "", "for", "remote", "in", "self", ".", "work_remotes", ":", "\n", "            ", "remote", ".", "close", "(", ")", "\n", "\n", "", "self", ".", "remotes", "[", "0", "]", ".", "send", "(", "(", "'get_spaces_spec'", ",", "None", ")", ")", "\n", "observation_space", ",", "action_space", ",", "self", ".", "spec", "=", "self", ".", "remotes", "[", "0", "]", ".", "recv", "(", ")", "\n", "self", ".", "viewer", "=", "None", "\n", "VecEnv", ".", "__init__", "(", "self", ",", "len", "(", "env_fns", ")", ",", "observation_space", ",", "action_space", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.subproc_vec_env.SubprocVecEnv.step_async": [[66, 71], ["subproc_vec_env.SubprocVecEnv._assert_not_closed", "zip", "remote.send"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.subproc_vec_env.SubprocVecEnv._assert_not_closed"], ["", "def", "step_async", "(", "self", ",", "actions", ")", ":", "\n", "        ", "self", ".", "_assert_not_closed", "(", ")", "\n", "for", "remote", ",", "action", "in", "zip", "(", "self", ".", "remotes", ",", "actions", ")", ":", "\n", "            ", "remote", ".", "send", "(", "(", "'step'", ",", "action", ")", ")", "\n", "", "self", ".", "waiting", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.subproc_vec_env.SubprocVecEnv.step_wait": [[72, 78], ["subproc_vec_env.SubprocVecEnv._assert_not_closed", "zip", "remote.recv", "subproc_vec_env._flatten_obs", "numpy.stack", "numpy.stack"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.subproc_vec_env.SubprocVecEnv._assert_not_closed", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.subproc_vec_env._flatten_obs"], ["", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "self", ".", "_assert_not_closed", "(", ")", "\n", "results", "=", "[", "remote", ".", "recv", "(", ")", "for", "remote", "in", "self", ".", "remotes", "]", "\n", "self", ".", "waiting", "=", "False", "\n", "obs", ",", "rews", ",", "dones", ",", "infos", "=", "zip", "(", "*", "results", ")", "\n", "return", "_flatten_obs", "(", "obs", ")", ",", "np", ".", "stack", "(", "rews", ")", ",", "np", ".", "stack", "(", "dones", ")", ",", "infos", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.subproc_vec_env.SubprocVecEnv.reset": [[79, 84], ["subproc_vec_env.SubprocVecEnv._assert_not_closed", "subproc_vec_env._flatten_obs", "remote.send", "remote.recv"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.subproc_vec_env.SubprocVecEnv._assert_not_closed", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.subproc_vec_env._flatten_obs"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_assert_not_closed", "(", ")", "\n", "for", "remote", "in", "self", ".", "remotes", ":", "\n", "            ", "remote", ".", "send", "(", "(", "'reset'", ",", "None", ")", ")", "\n", "", "return", "_flatten_obs", "(", "[", "remote", ".", "recv", "(", ")", "for", "remote", "in", "self", ".", "remotes", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.subproc_vec_env.SubprocVecEnv.close_extras": [[85, 94], ["remote.send", "p.join", "remote.recv"], "methods", ["None"], ["", "def", "close_extras", "(", "self", ")", ":", "\n", "        ", "self", ".", "closed", "=", "True", "\n", "if", "self", ".", "waiting", ":", "\n", "            ", "for", "remote", "in", "self", ".", "remotes", ":", "\n", "                ", "remote", ".", "recv", "(", ")", "\n", "", "", "for", "remote", "in", "self", ".", "remotes", ":", "\n", "            ", "remote", ".", "send", "(", "(", "'close'", ",", "None", ")", ")", "\n", "", "for", "p", "in", "self", ".", "ps", ":", "\n", "            ", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.subproc_vec_env.SubprocVecEnv.get_images": [[95, 101], ["subproc_vec_env.SubprocVecEnv._assert_not_closed", "pipe.send", "pipe.recv"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.subproc_vec_env.SubprocVecEnv._assert_not_closed"], ["", "", "def", "get_images", "(", "self", ")", ":", "\n", "        ", "self", ".", "_assert_not_closed", "(", ")", "\n", "for", "pipe", "in", "self", ".", "remotes", ":", "\n", "            ", "pipe", ".", "send", "(", "(", "'render'", ",", "None", ")", ")", "\n", "", "imgs", "=", "[", "pipe", ".", "recv", "(", ")", "for", "pipe", "in", "self", ".", "remotes", "]", "\n", "return", "imgs", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.subproc_vec_env.SubprocVecEnv._assert_not_closed": [[102, 104], ["None"], "methods", ["None"], ["", "def", "_assert_not_closed", "(", "self", ")", ":", "\n", "        ", "assert", "not", "self", ".", "closed", ",", "\"Trying to operate on a SubprocVecEnv after calling close()\"", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.subproc_vec_env.SubprocVecEnv.__del__": [[105, 108], ["subproc_vec_env.SubprocVecEnv.close"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "if", "not", "self", ".", "closed", ":", "\n", "            ", "self", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.subproc_vec_env.worker": [[7, 34], ["parent_remote.close", "env_fn_wrapper.x", "env_fn_wrapper.x.close", "remote.recv", "print", "env_fn_wrapper.x.step", "remote.send", "env_fn_wrapper.x.reset", "env_fn_wrapper.x.reset", "remote.send", "remote.send", "env_fn_wrapper.x.render", "remote.close", "remote.send"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.step", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.dummy_vec_env.DummyVecEnv.render", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close"], ["def", "worker", "(", "remote", ",", "parent_remote", ",", "env_fn_wrapper", ")", ":", "\n", "    ", "parent_remote", ".", "close", "(", ")", "\n", "env", "=", "env_fn_wrapper", ".", "x", "(", ")", "\n", "try", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "cmd", ",", "data", "=", "remote", ".", "recv", "(", ")", "\n", "if", "cmd", "==", "'step'", ":", "\n", "                ", "ob", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "data", ")", "\n", "if", "done", ":", "\n", "                    ", "ob", "=", "env", ".", "reset", "(", ")", "\n", "", "remote", ".", "send", "(", "(", "ob", ",", "reward", ",", "done", ",", "info", ")", ")", "\n", "", "elif", "cmd", "==", "'reset'", ":", "\n", "                ", "ob", "=", "env", ".", "reset", "(", ")", "\n", "remote", ".", "send", "(", "ob", ")", "\n", "", "elif", "cmd", "==", "'render'", ":", "\n", "                ", "remote", ".", "send", "(", "env", ".", "render", "(", "mode", "=", "'rgb_array'", ")", ")", "\n", "", "elif", "cmd", "==", "'close'", ":", "\n", "                ", "remote", ".", "close", "(", ")", "\n", "break", "\n", "", "elif", "cmd", "==", "'get_spaces_spec'", ":", "\n", "                ", "remote", ".", "send", "(", "(", "env", ".", "observation_space", ",", "env", ".", "action_space", ",", "env", ".", "spec", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "", "", "", "except", "KeyboardInterrupt", ":", "\n", "        ", "print", "(", "'SubprocVecEnv worker: got KeyboardInterrupt'", ")", "\n", "", "finally", ":", "\n", "        ", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.subproc_vec_env._flatten_obs": [[109, 118], ["isinstance", "isinstance", "len", "obs[].keys", "numpy.stack", "numpy.stack"], "function", ["None"], ["", "", "", "def", "_flatten_obs", "(", "obs", ")", ":", "\n", "    ", "assert", "isinstance", "(", "obs", ",", "(", "list", ",", "tuple", ")", ")", "\n", "assert", "len", "(", "obs", ")", ">", "0", "\n", "\n", "if", "isinstance", "(", "obs", "[", "0", "]", ",", "dict", ")", ":", "\n", "        ", "keys", "=", "obs", "[", "0", "]", ".", "keys", "(", ")", "\n", "return", "{", "k", ":", "np", ".", "stack", "(", "[", "o", "[", "k", "]", "for", "o", "in", "obs", "]", ")", "for", "k", "in", "keys", "}", "\n", "", "else", ":", "\n", "        ", "return", "np", ".", "stack", "(", "obs", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.test_video_recorder.test_video_recorder": [[16, 48], ["pytest.mark.parametrize", "pytest.mark.parametrize", "pytest.mark.parametrize", "pytest.mark.parametrize", "klass", "gym.make", "tempfile.TemporaryDirectory", "vec_video_recorder.VecVideoRecorder", "vec_video_recorder.VecVideoRecorder.reset", "range", "vec_video_recorder.VecVideoRecorder.close", "glob.glob", "all", "range", "vec_video_recorder.VecVideoRecorder.step", "os.path.join", "len", "os.stat"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.step"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "'klass'", ",", "(", "DummyVecEnv", ",", "ShmemVecEnv", ",", "SubprocVecEnv", ")", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'num_envs'", ",", "(", "1", ",", "4", ")", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'video_length'", ",", "(", "10", ",", "100", ")", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'video_interval'", ",", "(", "1", ",", "50", ")", ")", "\n", "def", "test_video_recorder", "(", "klass", ",", "num_envs", ",", "video_length", ",", "video_interval", ")", ":", "\n", "    ", "\"\"\"\n    Wrap an existing VecEnv with VevVideoRecorder,\n    Make (video_interval + video_length + 1) steps,\n    then check that the file is present\n    \"\"\"", "\n", "\n", "def", "make_fn", "(", ")", ":", "\n", "        ", "env", "=", "gym", ".", "make", "(", "'PongNoFrameskip-v4'", ")", "\n", "return", "env", "\n", "", "fns", "=", "[", "make_fn", "for", "_", "in", "range", "(", "num_envs", ")", "]", "\n", "env", "=", "klass", "(", "fns", ")", "\n", "\n", "with", "tempfile", ".", "TemporaryDirectory", "(", ")", "as", "video_path", ":", "\n", "        ", "env", "=", "VecVideoRecorder", "(", "env", ",", "video_path", ",", "record_video_trigger", "=", "lambda", "x", ":", "x", "%", "video_interval", "==", "0", ",", "video_length", "=", "video_length", ")", "\n", "\n", "env", ".", "reset", "(", ")", "\n", "for", "_", "in", "range", "(", "video_interval", "+", "video_length", "+", "1", ")", ":", "\n", "            ", "env", ".", "step", "(", "[", "0", "]", "*", "num_envs", ")", "\n", "", "env", ".", "close", "(", ")", "\n", "\n", "\n", "recorded_video", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "video_path", ",", "\"*.mp4\"", ")", ")", "\n", "\n", "# first and second step", "\n", "assert", "len", "(", "recorded_video", ")", "==", "2", "\n", "# Files are not empty", "\n", "assert", "all", "(", "os", ".", "stat", "(", "p", ")", ".", "st_size", "!=", "0", "for", "p", "in", "recorded_video", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.dummy_vec_env.DummyVecEnv.__init__": [[12, 30], ["vec_env.VecEnv.__init__", "util.obs_space_info", "numpy.zeros", "numpy.zeros", "fn", "len", "numpy.zeros", "range", "tuple"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.util.obs_space_info"], ["def", "__init__", "(", "self", ",", "env_fns", ")", ":", "\n", "        ", "\"\"\"\n        Arguments:\n\n        env_fns: iterable of callables      functions that build environments\n        \"\"\"", "\n", "self", ".", "envs", "=", "[", "fn", "(", ")", "for", "fn", "in", "env_fns", "]", "\n", "env", "=", "self", ".", "envs", "[", "0", "]", "\n", "VecEnv", ".", "__init__", "(", "self", ",", "len", "(", "env_fns", ")", ",", "env", ".", "observation_space", ",", "env", ".", "action_space", ")", "\n", "obs_space", "=", "env", ".", "observation_space", "\n", "self", ".", "keys", ",", "shapes", ",", "dtypes", "=", "obs_space_info", "(", "obs_space", ")", "\n", "\n", "self", ".", "buf_obs", "=", "{", "k", ":", "np", ".", "zeros", "(", "(", "self", ".", "num_envs", ",", ")", "+", "tuple", "(", "shapes", "[", "k", "]", ")", ",", "dtype", "=", "dtypes", "[", "k", "]", ")", "for", "k", "in", "self", ".", "keys", "}", "\n", "self", ".", "buf_dones", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_envs", ",", ")", ",", "dtype", "=", "np", ".", "bool", ")", "\n", "self", ".", "buf_rews", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_envs", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "buf_infos", "=", "[", "{", "}", "for", "_", "in", "range", "(", "self", ".", "num_envs", ")", "]", "\n", "self", ".", "actions", "=", "None", "\n", "self", ".", "spec", "=", "self", ".", "envs", "[", "0", "]", ".", "spec", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.dummy_vec_env.DummyVecEnv.step_async": [[31, 44], ["len"], "methods", ["None"], ["", "def", "step_async", "(", "self", ",", "actions", ")", ":", "\n", "        ", "listify", "=", "True", "\n", "try", ":", "\n", "            ", "if", "len", "(", "actions", ")", "==", "self", ".", "num_envs", ":", "\n", "                ", "listify", "=", "False", "\n", "", "", "except", "TypeError", ":", "\n", "            ", "pass", "\n", "\n", "", "if", "not", "listify", ":", "\n", "            ", "self", ".", "actions", "=", "actions", "\n", "", "else", ":", "\n", "            ", "assert", "self", ".", "num_envs", "==", "1", ",", "\"actions {} is either not a list or has a wrong size - cannot match to {} environments\"", ".", "format", "(", "actions", ",", "self", ".", "num_envs", ")", "\n", "self", ".", "actions", "=", "[", "actions", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.dummy_vec_env.DummyVecEnv.step_wait": [[45, 57], ["range", "dummy_vec_env.DummyVecEnv.envs[].step", "dummy_vec_env.DummyVecEnv._save_obs", "dummy_vec_env.DummyVecEnv._obs_from_buf", "numpy.copy", "numpy.copy", "dummy_vec_env.DummyVecEnv.buf_infos.copy", "dummy_vec_env.DummyVecEnv.envs[].reset"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.step", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.dummy_vec_env.DummyVecEnv._save_obs", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.dummy_vec_env.DummyVecEnv._obs_from_buf", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset"], ["", "", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "for", "e", "in", "range", "(", "self", ".", "num_envs", ")", ":", "\n", "            ", "action", "=", "self", ".", "actions", "[", "e", "]", "\n", "# if isinstance(self.envs[e].action_space, spaces.Discrete):", "\n", "#    action = int(action)", "\n", "\n", "obs", ",", "self", ".", "buf_rews", "[", "e", "]", ",", "self", ".", "buf_dones", "[", "e", "]", ",", "self", ".", "buf_infos", "[", "e", "]", "=", "self", ".", "envs", "[", "e", "]", ".", "step", "(", "action", ")", "\n", "if", "self", ".", "buf_dones", "[", "e", "]", ":", "\n", "                ", "obs", "=", "self", ".", "envs", "[", "e", "]", ".", "reset", "(", ")", "\n", "", "self", ".", "_save_obs", "(", "e", ",", "obs", ")", "\n", "", "return", "(", "self", ".", "_obs_from_buf", "(", ")", ",", "np", ".", "copy", "(", "self", ".", "buf_rews", ")", ",", "np", ".", "copy", "(", "self", ".", "buf_dones", ")", ",", "\n", "self", ".", "buf_infos", ".", "copy", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.dummy_vec_env.DummyVecEnv.reset": [[58, 63], ["range", "dummy_vec_env.DummyVecEnv._obs_from_buf", "dummy_vec_env.DummyVecEnv.envs[].reset", "dummy_vec_env.DummyVecEnv._save_obs"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.dummy_vec_env.DummyVecEnv._obs_from_buf", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.dummy_vec_env.DummyVecEnv._save_obs"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "for", "e", "in", "range", "(", "self", ".", "num_envs", ")", ":", "\n", "            ", "obs", "=", "self", ".", "envs", "[", "e", "]", ".", "reset", "(", ")", "\n", "self", ".", "_save_obs", "(", "e", ",", "obs", ")", "\n", "", "return", "self", ".", "_obs_from_buf", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.dummy_vec_env.DummyVecEnv._save_obs": [[64, 70], ["None"], "methods", ["None"], ["", "def", "_save_obs", "(", "self", ",", "e", ",", "obs", ")", ":", "\n", "        ", "for", "k", "in", "self", ".", "keys", ":", "\n", "            ", "if", "k", "is", "None", ":", "\n", "                ", "self", ".", "buf_obs", "[", "k", "]", "[", "e", "]", "=", "obs", "\n", "", "else", ":", "\n", "                ", "self", ".", "buf_obs", "[", "k", "]", "[", "e", "]", "=", "obs", "[", "k", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.dummy_vec_env.DummyVecEnv._obs_from_buf": [[71, 73], ["util.dict_to_obs", "util.copy_obs_dict"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.util.dict_to_obs", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.util.copy_obs_dict"], ["", "", "", "def", "_obs_from_buf", "(", "self", ")", ":", "\n", "        ", "return", "dict_to_obs", "(", "copy_obs_dict", "(", "self", ".", "buf_obs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.dummy_vec_env.DummyVecEnv.get_images": [[74, 76], ["env.render"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.dummy_vec_env.DummyVecEnv.render"], ["", "def", "get_images", "(", "self", ")", ":", "\n", "        ", "return", "[", "env", ".", "render", "(", "mode", "=", "'rgb_array'", ")", "for", "env", "in", "self", ".", "envs", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.dummy_vec_env.DummyVecEnv.render": [[77, 82], ["dummy_vec_env.DummyVecEnv.envs[].render", "super().render"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.dummy_vec_env.DummyVecEnv.render", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.dummy_vec_env.DummyVecEnv.render"], ["", "def", "render", "(", "self", ",", "mode", "=", "'human'", ")", ":", "\n", "        ", "if", "self", ".", "num_envs", "==", "1", ":", "\n", "            ", "return", "self", ".", "envs", "[", "0", "]", ".", "render", "(", "mode", "=", "mode", ")", "\n", "", "else", ":", "\n", "            ", "return", "super", "(", ")", ".", "render", "(", "mode", "=", "mode", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_video_recorder.VecVideoRecorder.__init__": [[12, 38], ["baselines.common.vec_env.VecEnvWrapper.__init__", "os.path.abspath", "os.path.exists", "os.mkdir", "os.getpid"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__"], ["def", "__init__", "(", "self", ",", "venv", ",", "directory", ",", "record_video_trigger", ",", "video_length", "=", "200", ")", ":", "\n", "        ", "\"\"\"\n        # Arguments\n            venv: VecEnv to wrap\n            directory: Where to save videos\n            record_video_trigger:\n                Function that defines when to start recording.\n                The function takes the current number of step,\n                and returns whether we should start recording or not.\n            video_length: Length of recorded video\n        \"\"\"", "\n", "\n", "VecEnvWrapper", ".", "__init__", "(", "self", ",", "venv", ")", "\n", "self", ".", "record_video_trigger", "=", "record_video_trigger", "\n", "self", ".", "video_recorder", "=", "None", "\n", "\n", "self", ".", "directory", "=", "os", ".", "path", ".", "abspath", "(", "directory", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "directory", ")", ":", "os", ".", "mkdir", "(", "self", ".", "directory", ")", "\n", "\n", "self", ".", "file_prefix", "=", "\"vecenv\"", "\n", "self", ".", "file_infix", "=", "'{}'", ".", "format", "(", "os", ".", "getpid", "(", ")", ")", "\n", "self", ".", "step_id", "=", "0", "\n", "self", ".", "video_length", "=", "video_length", "\n", "\n", "self", ".", "recording", "=", "False", "\n", "self", ".", "recorded_frames", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_video_recorder.VecVideoRecorder.reset": [[39, 45], ["vec_video_recorder.VecVideoRecorder.venv.reset", "vec_video_recorder.VecVideoRecorder.start_video_recorder"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_video_recorder.VecVideoRecorder.start_video_recorder"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "obs", "=", "self", ".", "venv", ".", "reset", "(", ")", "\n", "\n", "self", ".", "start_video_recorder", "(", ")", "\n", "\n", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_video_recorder.VecVideoRecorder.start_video_recorder": [[46, 59], ["vec_video_recorder.VecVideoRecorder.close_video_recorder", "os.path.join", "gym.wrappers.monitoring.video_recorder.VideoRecorder", "vec_video_recorder.VecVideoRecorder.video_recorder.capture_frame"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_video_recorder.VecVideoRecorder.close_video_recorder"], ["", "def", "start_video_recorder", "(", "self", ")", ":", "\n", "        ", "self", ".", "close_video_recorder", "(", ")", "\n", "\n", "base_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "directory", ",", "'{}.video.{}.video{:06}'", ".", "format", "(", "self", ".", "file_prefix", ",", "self", ".", "file_infix", ",", "self", ".", "step_id", ")", ")", "\n", "self", ".", "video_recorder", "=", "video_recorder", ".", "VideoRecorder", "(", "\n", "env", "=", "self", ".", "venv", ",", "\n", "base_path", "=", "base_path", ",", "\n", "metadata", "=", "{", "'step_id'", ":", "self", ".", "step_id", "}", "\n", ")", "\n", "\n", "self", ".", "video_recorder", ".", "capture_frame", "(", ")", "\n", "self", ".", "recorded_frames", "=", "1", "\n", "self", ".", "recording", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_video_recorder.VecVideoRecorder._video_enabled": [[60, 62], ["vec_video_recorder.VecVideoRecorder.record_video_trigger"], "methods", ["None"], ["", "def", "_video_enabled", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "record_video_trigger", "(", "self", ".", "step_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_video_recorder.VecVideoRecorder.step_wait": [[63, 77], ["vec_video_recorder.VecVideoRecorder.venv.step_wait", "vec_video_recorder.VecVideoRecorder.video_recorder.capture_frame", "vec_video_recorder.VecVideoRecorder._video_enabled", "baselines.logger.info", "vec_video_recorder.VecVideoRecorder.close_video_recorder", "vec_video_recorder.VecVideoRecorder.start_video_recorder"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.step_wait", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_video_recorder.VecVideoRecorder._video_enabled", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.info", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_video_recorder.VecVideoRecorder.close_video_recorder", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_video_recorder.VecVideoRecorder.start_video_recorder"], ["", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "obs", ",", "rews", ",", "dones", ",", "infos", "=", "self", ".", "venv", ".", "step_wait", "(", ")", "\n", "\n", "self", ".", "step_id", "+=", "1", "\n", "if", "self", ".", "recording", ":", "\n", "            ", "self", ".", "video_recorder", ".", "capture_frame", "(", ")", "\n", "self", ".", "recorded_frames", "+=", "1", "\n", "if", "self", ".", "recorded_frames", ">", "self", ".", "video_length", ":", "\n", "                ", "logger", ".", "info", "(", "\"Saving video to \"", ",", "self", ".", "video_recorder", ".", "path", ")", "\n", "self", ".", "close_video_recorder", "(", ")", "\n", "", "", "elif", "self", ".", "_video_enabled", "(", ")", ":", "\n", "                ", "self", ".", "start_video_recorder", "(", ")", "\n", "\n", "", "return", "obs", ",", "rews", ",", "dones", ",", "infos", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_video_recorder.VecVideoRecorder.close_video_recorder": [[78, 83], ["vec_video_recorder.VecVideoRecorder.video_recorder.close"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close"], ["", "def", "close_video_recorder", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "recording", ":", "\n", "            ", "self", ".", "video_recorder", ".", "close", "(", ")", "\n", "", "self", ".", "recording", "=", "False", "\n", "self", ".", "recorded_frames", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_video_recorder.VecVideoRecorder.close": [[84, 87], ["baselines.common.vec_env.VecEnvWrapper.close", "vec_video_recorder.VecVideoRecorder.close_video_recorder"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_video_recorder.VecVideoRecorder.close_video_recorder"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "VecEnvWrapper", ".", "close", "(", "self", ")", "\n", "self", ".", "close_video_recorder", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_video_recorder.VecVideoRecorder.__del__": [[88, 90], ["vec_video_recorder.VecVideoRecorder.close"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "self", ".", "close", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_normalize.VecNormalize.__init__": [[12, 21], ["VecEnvWrapper.__init__", "numpy.zeros", "baselines.common.running_mean_std.RunningMeanStd", "baselines.common.running_mean_std.RunningMeanStd"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__"], ["def", "__init__", "(", "self", ",", "venv", ",", "ob", "=", "True", ",", "ret", "=", "True", ",", "clipob", "=", "10.", ",", "cliprew", "=", "10.", ",", "gamma", "=", "0.99", ",", "epsilon", "=", "1e-8", ")", ":", "\n", "        ", "VecEnvWrapper", ".", "__init__", "(", "self", ",", "venv", ")", "\n", "self", ".", "ob_rms", "=", "RunningMeanStd", "(", "shape", "=", "self", ".", "observation_space", ".", "shape", ")", "if", "ob", "else", "None", "\n", "self", ".", "ret_rms", "=", "RunningMeanStd", "(", "shape", "=", "(", ")", ")", "if", "ret", "else", "None", "\n", "self", ".", "clipob", "=", "clipob", "\n", "self", ".", "cliprew", "=", "cliprew", "\n", "self", ".", "ret", "=", "np", ".", "zeros", "(", "self", ".", "num_envs", ")", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_normalize.VecNormalize.step_wait": [[22, 31], ["vec_normalize.VecNormalize.venv.step_wait", "vec_normalize.VecNormalize._obfilt", "vec_normalize.VecNormalize.ret_rms.update", "numpy.clip", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.step_wait", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_normalize.VecNormalize._obfilt", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update"], ["", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "obs", ",", "rews", ",", "news", ",", "infos", "=", "self", ".", "venv", ".", "step_wait", "(", ")", "\n", "self", ".", "ret", "=", "self", ".", "ret", "*", "self", ".", "gamma", "+", "rews", "\n", "obs", "=", "self", ".", "_obfilt", "(", "obs", ")", "\n", "if", "self", ".", "ret_rms", ":", "\n", "            ", "self", ".", "ret_rms", ".", "update", "(", "self", ".", "ret", ")", "\n", "rews", "=", "np", ".", "clip", "(", "rews", "/", "np", ".", "sqrt", "(", "self", ".", "ret_rms", ".", "var", "+", "self", ".", "epsilon", ")", ",", "-", "self", ".", "cliprew", ",", "self", ".", "cliprew", ")", "\n", "", "self", ".", "ret", "[", "news", "]", "=", "0.", "\n", "return", "obs", ",", "rews", ",", "news", ",", "infos", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_normalize.VecNormalize._obfilt": [[32, 39], ["vec_normalize.VecNormalize.ob_rms.update", "numpy.clip", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update"], ["", "def", "_obfilt", "(", "self", ",", "obs", ")", ":", "\n", "        ", "if", "self", ".", "ob_rms", ":", "\n", "            ", "self", ".", "ob_rms", ".", "update", "(", "obs", ")", "\n", "obs", "=", "np", ".", "clip", "(", "(", "obs", "-", "self", ".", "ob_rms", ".", "mean", ")", "/", "np", ".", "sqrt", "(", "self", ".", "ob_rms", ".", "var", "+", "self", ".", "epsilon", ")", ",", "-", "self", ".", "clipob", ",", "self", ".", "clipob", ")", "\n", "return", "obs", "\n", "", "else", ":", "\n", "            ", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_normalize.VecNormalize.reset": [[40, 44], ["numpy.zeros", "vec_normalize.VecNormalize.venv.reset", "vec_normalize.VecNormalize._obfilt"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_normalize.VecNormalize._obfilt"], ["", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "ret", "=", "np", ".", "zeros", "(", "self", ".", "num_envs", ")", "\n", "obs", "=", "self", ".", "venv", ".", "reset", "(", ")", "\n", "return", "self", ".", "_obfilt", "(", "obs", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_monitor.VecMonitor.__init__": [[10, 24], ["VecEnvWrapper.__init__", "time.time", "monitor.ResultsWriter", "collections.deque", "collections.deque"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "venv", ",", "filename", "=", "None", ",", "keep_buf", "=", "0", ")", ":", "\n", "        ", "VecEnvWrapper", ".", "__init__", "(", "self", ",", "venv", ")", "\n", "self", ".", "eprets", "=", "None", "\n", "self", ".", "eplens", "=", "None", "\n", "self", ".", "epcount", "=", "0", "\n", "self", ".", "tstart", "=", "time", ".", "time", "(", ")", "\n", "if", "filename", ":", "\n", "            ", "self", ".", "results_writer", "=", "ResultsWriter", "(", "filename", ",", "header", "=", "{", "'t_start'", ":", "self", ".", "tstart", "}", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "results_writer", "=", "None", "\n", "", "self", ".", "keep_buf", "=", "keep_buf", "\n", "if", "self", ".", "keep_buf", ":", "\n", "            ", "self", ".", "epret_buf", "=", "deque", "(", "[", "]", ",", "maxlen", "=", "keep_buf", ")", "\n", "self", ".", "eplen_buf", "=", "deque", "(", "[", "]", ",", "maxlen", "=", "keep_buf", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_monitor.VecMonitor.reset": [[25, 30], ["vec_monitor.VecMonitor.venv.reset", "numpy.zeros", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset"], ["", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "obs", "=", "self", ".", "venv", ".", "reset", "(", ")", "\n", "self", ".", "eprets", "=", "np", ".", "zeros", "(", "self", ".", "num_envs", ",", "'f'", ")", "\n", "self", ".", "eplens", "=", "np", ".", "zeros", "(", "self", ".", "num_envs", ",", "'i'", ")", "\n", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_monitor.VecMonitor.step_wait": [[31, 52], ["vec_monitor.VecMonitor.venv.step_wait", "enumerate", "zip", "info.copy.copy.copy", "newinfos.append", "round", "vec_monitor.VecMonitor.epret_buf.append", "vec_monitor.VecMonitor.eplen_buf.append", "vec_monitor.VecMonitor.results_writer.write_row", "time.time"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.step_wait", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.ResultsWriter.write_row"], ["", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "obs", ",", "rews", ",", "dones", ",", "infos", "=", "self", ".", "venv", ".", "step_wait", "(", ")", "\n", "self", ".", "eprets", "+=", "rews", "\n", "self", ".", "eplens", "+=", "1", "\n", "newinfos", "=", "[", "]", "\n", "for", "(", "i", ",", "(", "done", ",", "ret", ",", "eplen", ",", "info", ")", ")", "in", "enumerate", "(", "zip", "(", "dones", ",", "self", ".", "eprets", ",", "self", ".", "eplens", ",", "infos", ")", ")", ":", "\n", "            ", "info", "=", "info", ".", "copy", "(", ")", "\n", "if", "done", ":", "\n", "                ", "epinfo", "=", "{", "'r'", ":", "ret", ",", "'l'", ":", "eplen", ",", "'t'", ":", "round", "(", "time", ".", "time", "(", ")", "-", "self", ".", "tstart", ",", "6", ")", "}", "\n", "info", "[", "'episode'", "]", "=", "epinfo", "\n", "if", "self", ".", "keep_buf", ":", "\n", "                    ", "self", ".", "epret_buf", ".", "append", "(", "ret", ")", "\n", "self", ".", "eplen_buf", ".", "append", "(", "eplen", ")", "\n", "", "self", ".", "epcount", "+=", "1", "\n", "self", ".", "eprets", "[", "i", "]", "=", "0", "\n", "self", ".", "eplens", "[", "i", "]", "=", "0", "\n", "if", "self", ".", "results_writer", ":", "\n", "                    ", "self", ".", "results_writer", ".", "write_row", "(", "epinfo", ")", "\n", "", "", "newinfos", ".", "append", "(", "info", ")", "\n", "\n", "", "return", "obs", ",", "rews", ",", "dones", ",", "newinfos", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.util.copy_obs_dict": [[11, 16], ["numpy.copy", "obs.items"], "function", ["None"], ["def", "copy_obs_dict", "(", "obs", ")", ":", "\n", "    ", "\"\"\"\n    Deep-copy an observation dict.\n    \"\"\"", "\n", "return", "{", "k", ":", "np", ".", "copy", "(", "v", ")", "for", "k", ",", "v", "in", "obs", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.util.dict_to_obs": [[18, 26], ["set", "obs_dict.keys"], "function", ["None"], ["", "def", "dict_to_obs", "(", "obs_dict", ")", ":", "\n", "    ", "\"\"\"\n    Convert an observation dict into a raw array if the\n    original observation space was not a Dict space.\n    \"\"\"", "\n", "if", "set", "(", "obs_dict", ".", "keys", "(", ")", ")", "==", "{", "None", "}", ":", "\n", "        ", "return", "obs_dict", "[", "None", "]", "\n", "", "return", "obs_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.util.obs_space_info": [[28, 51], ["isinstance", "subspaces.items", "isinstance", "keys.append"], "function", ["None"], ["", "def", "obs_space_info", "(", "obs_space", ")", ":", "\n", "    ", "\"\"\"\n    Get dict-structured information about a gym.Space.\n\n    Returns:\n      A tuple (keys, shapes, dtypes):\n        keys: a list of dict keys.\n        shapes: a dict mapping keys to shapes.\n        dtypes: a dict mapping keys to dtypes.\n    \"\"\"", "\n", "if", "isinstance", "(", "obs_space", ",", "gym", ".", "spaces", ".", "Dict", ")", ":", "\n", "        ", "assert", "isinstance", "(", "obs_space", ".", "spaces", ",", "OrderedDict", ")", "\n", "subspaces", "=", "obs_space", ".", "spaces", "\n", "", "else", ":", "\n", "        ", "subspaces", "=", "{", "None", ":", "obs_space", "}", "\n", "", "keys", "=", "[", "]", "\n", "shapes", "=", "{", "}", "\n", "dtypes", "=", "{", "}", "\n", "for", "key", ",", "box", "in", "subspaces", ".", "items", "(", ")", ":", "\n", "        ", "keys", ".", "append", "(", "key", ")", "\n", "shapes", "[", "key", "]", "=", "box", ".", "shape", "\n", "dtypes", "[", "key", "]", "=", "box", ".", "dtype", "\n", "", "return", "keys", ",", "shapes", ",", "dtypes", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.util.obs_to_dict": [[53, 60], ["isinstance"], "function", ["None"], ["", "def", "obs_to_dict", "(", "obs", ")", ":", "\n", "    ", "\"\"\"\n    Convert an observation into a dict.\n    \"\"\"", "\n", "if", "isinstance", "(", "obs", ",", "dict", ")", ":", "\n", "        ", "return", "obs", "\n", "", "return", "{", "None", ":", "obs", "}", "\n", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.__init__": [[16, 36], ["gym.core.Wrapper.__init__", "time.time", "monitor.ResultsWriter", "time.time"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__"], ["def", "__init__", "(", "self", ",", "env", ",", "filename", ",", "allow_early_resets", "=", "False", ",", "reset_keywords", "=", "(", ")", ",", "info_keywords", "=", "(", ")", ")", ":", "\n", "        ", "Wrapper", ".", "__init__", "(", "self", ",", "env", "=", "env", ")", "\n", "self", ".", "tstart", "=", "time", ".", "time", "(", ")", "\n", "if", "filename", ":", "\n", "            ", "self", ".", "results_writer", "=", "ResultsWriter", "(", "filename", ",", "\n", "header", "=", "{", "\"t_start\"", ":", "time", ".", "time", "(", ")", ",", "'env_id'", ":", "env", ".", "spec", "and", "env", ".", "spec", ".", "id", "}", ",", "\n", "extra_keys", "=", "reset_keywords", "+", "info_keywords", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "results_writer", "=", "None", "\n", "", "self", ".", "reset_keywords", "=", "reset_keywords", "\n", "self", ".", "info_keywords", "=", "info_keywords", "\n", "self", ".", "allow_early_resets", "=", "allow_early_resets", "\n", "self", ".", "rewards", "=", "None", "\n", "self", ".", "needs_reset", "=", "True", "\n", "self", ".", "episode_rewards", "=", "[", "]", "\n", "self", ".", "episode_lengths", "=", "[", "]", "\n", "self", ".", "episode_times", "=", "[", "]", "\n", "self", ".", "total_steps", "=", "0", "\n", "self", ".", "current_reset_info", "=", "{", "}", "# extra info about the current episode, that was passed in during reset()", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.reset": [[37, 45], ["monitor.Monitor.reset_state", "monitor.Monitor.env.reset", "kwargs.get", "ValueError"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.reset_state", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "reset_state", "(", ")", "\n", "for", "k", "in", "self", ".", "reset_keywords", ":", "\n", "            ", "v", "=", "kwargs", ".", "get", "(", "k", ")", "\n", "if", "v", "is", "None", ":", "\n", "                ", "raise", "ValueError", "(", "'Expected you to pass kwarg %s into reset'", "%", "k", ")", "\n", "", "self", ".", "current_reset_info", "[", "k", "]", "=", "v", "\n", "", "return", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.reset_state": [[46, 51], ["RuntimeError"], "methods", ["None"], ["", "def", "reset_state", "(", "self", ")", ":", "\n", "        ", "if", "not", "self", ".", "allow_early_resets", "and", "not", "self", ".", "needs_reset", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Tried to reset an environment before done. If you want to allow early resets, wrap your env with Monitor(env, path, allow_early_resets=True)\"", ")", "\n", "", "self", ".", "rewards", "=", "[", "]", "\n", "self", ".", "needs_reset", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.step": [[53, 59], ["monitor.Monitor.env.step", "monitor.Monitor.update", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.step", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "if", "self", ".", "needs_reset", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Tried to step environment that needs reset\"", ")", "\n", "", "ob", ",", "rew", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "update", "(", "ob", ",", "rew", ",", "done", ",", "info", ")", "\n", "return", "(", "ob", ",", "rew", ",", "done", ",", "info", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update": [[60, 80], ["monitor.Monitor.rewards.append", "sum", "len", "monitor.Monitor.episode_rewards.append", "monitor.Monitor.episode_lengths.append", "monitor.Monitor.episode_times.append", "epinfo.update", "isinstance", "isinstance", "round", "round", "monitor.Monitor.results_writer.write_row", "time.time", "time.time"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.update", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.ResultsWriter.write_row"], ["", "def", "update", "(", "self", ",", "ob", ",", "rew", ",", "done", ",", "info", ")", ":", "\n", "        ", "self", ".", "rewards", ".", "append", "(", "rew", ")", "\n", "if", "done", ":", "\n", "            ", "self", ".", "needs_reset", "=", "True", "\n", "eprew", "=", "sum", "(", "self", ".", "rewards", ")", "\n", "eplen", "=", "len", "(", "self", ".", "rewards", ")", "\n", "epinfo", "=", "{", "\"r\"", ":", "round", "(", "eprew", ",", "6", ")", ",", "\"l\"", ":", "eplen", ",", "\"t\"", ":", "round", "(", "time", ".", "time", "(", ")", "-", "self", ".", "tstart", ",", "6", ")", "}", "\n", "for", "k", "in", "self", ".", "info_keywords", ":", "\n", "                ", "epinfo", "[", "k", "]", "=", "info", "[", "k", "]", "\n", "", "self", ".", "episode_rewards", ".", "append", "(", "eprew", ")", "\n", "self", ".", "episode_lengths", ".", "append", "(", "eplen", ")", "\n", "self", ".", "episode_times", ".", "append", "(", "time", ".", "time", "(", ")", "-", "self", ".", "tstart", ")", "\n", "epinfo", ".", "update", "(", "self", ".", "current_reset_info", ")", "\n", "if", "self", ".", "results_writer", ":", "\n", "                ", "self", ".", "results_writer", ".", "write_row", "(", "epinfo", ")", "\n", "", "assert", "isinstance", "(", "info", ",", "dict", ")", "\n", "if", "isinstance", "(", "info", ",", "dict", ")", ":", "\n", "                ", "info", "[", "'episode'", "]", "=", "epinfo", "\n", "\n", "", "", "self", ".", "total_steps", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close": [[81, 84], ["monitor.Monitor.f.close"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "f", "is", "not", "None", ":", "\n", "            ", "self", ".", "f", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.get_total_steps": [[85, 87], ["None"], "methods", ["None"], ["", "", "def", "get_total_steps", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "total_steps", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.get_episode_rewards": [[88, 90], ["None"], "methods", ["None"], ["", "def", "get_episode_rewards", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "episode_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.get_episode_lengths": [[91, 93], ["None"], "methods", ["None"], ["", "def", "get_episode_lengths", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "episode_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.get_episode_times": [[94, 96], ["None"], "methods", ["None"], ["", "def", "get_episode_times", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "episode_times", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.ResultsWriter.__init__": [[102, 117], ["open", "isinstance", "monitor.ResultsWriter.f.write", "csv.DictWriter", "monitor.ResultsWriter.logger.writeheader", "monitor.ResultsWriter.f.flush", "os.join.endswith", "os.isdir", "os.join", "json.dumps", "tuple"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "filename", ",", "header", "=", "''", ",", "extra_keys", "=", "(", ")", ")", ":", "\n", "        ", "self", ".", "extra_keys", "=", "extra_keys", "\n", "assert", "filename", "is", "not", "None", "\n", "if", "not", "filename", ".", "endswith", "(", "Monitor", ".", "EXT", ")", ":", "\n", "            ", "if", "osp", ".", "isdir", "(", "filename", ")", ":", "\n", "                ", "filename", "=", "osp", ".", "join", "(", "filename", ",", "Monitor", ".", "EXT", ")", "\n", "", "else", ":", "\n", "                ", "filename", "=", "filename", "+", "\".\"", "+", "Monitor", ".", "EXT", "\n", "", "", "self", ".", "f", "=", "open", "(", "filename", ",", "\"wt\"", ")", "\n", "if", "isinstance", "(", "header", ",", "dict", ")", ":", "\n", "            ", "header", "=", "'# {} \\n'", ".", "format", "(", "json", ".", "dumps", "(", "header", ")", ")", "\n", "", "self", ".", "f", ".", "write", "(", "header", ")", "\n", "self", ".", "logger", "=", "csv", ".", "DictWriter", "(", "self", ".", "f", ",", "fieldnames", "=", "(", "'r'", ",", "'l'", ",", "'t'", ")", "+", "tuple", "(", "extra_keys", ")", ")", "\n", "self", ".", "logger", ".", "writeheader", "(", ")", "\n", "self", ".", "f", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.ResultsWriter.write_row": [[118, 122], ["monitor.ResultsWriter.logger.writerow", "monitor.ResultsWriter.f.flush"], "methods", ["None"], ["", "def", "write_row", "(", "self", ",", "epinfo", ")", ":", "\n", "        ", "if", "self", ".", "logger", ":", "\n", "            ", "self", ".", "logger", ".", "writerow", "(", "epinfo", ")", "\n", "self", ".", "f", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.get_monitor_files": [[124, 126], ["glob.glob", "os.join"], "function", ["None"], ["", "", "", "def", "get_monitor_files", "(", "dir", ")", ":", "\n", "    ", "return", "glob", "(", "osp", ".", "join", "(", "dir", ",", "\"*\"", "+", "Monitor", ".", "EXT", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.load_results": [[127, 165], ["pandas.concat", "pandas.DataFrame.sort_values", "pandas.DataFrame.reset_index", "min", "glob.glob", "glob.glob", "monitor.LoadMonitorResultsError", "dfs.append", "os.join", "os.join", "open", "fname.endswith", "fh.readline", "json.loads", "pandas.read_csv", "headers.append", "fname.endswith", "fh.readlines", "json.loads", "headers.append", "pandas.DataFrame", "json.loads", "episodes.append"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.read_csv"], ["", "def", "load_results", "(", "dir", ")", ":", "\n", "    ", "import", "pandas", "\n", "monitor_files", "=", "(", "\n", "glob", "(", "osp", ".", "join", "(", "dir", ",", "\"*monitor.json\"", ")", ")", "+", "\n", "glob", "(", "osp", ".", "join", "(", "dir", ",", "\"*monitor.csv\"", ")", ")", ")", "# get both csv and (old) json files", "\n", "if", "not", "monitor_files", ":", "\n", "        ", "raise", "LoadMonitorResultsError", "(", "\"no monitor files of the form *%s found in %s\"", "%", "(", "Monitor", ".", "EXT", ",", "dir", ")", ")", "\n", "", "dfs", "=", "[", "]", "\n", "headers", "=", "[", "]", "\n", "for", "fname", "in", "monitor_files", ":", "\n", "        ", "with", "open", "(", "fname", ",", "'rt'", ")", "as", "fh", ":", "\n", "            ", "if", "fname", ".", "endswith", "(", "'csv'", ")", ":", "\n", "                ", "firstline", "=", "fh", ".", "readline", "(", ")", "\n", "if", "not", "firstline", ":", "\n", "                    ", "continue", "\n", "", "assert", "firstline", "[", "0", "]", "==", "'#'", "\n", "header", "=", "json", ".", "loads", "(", "firstline", "[", "1", ":", "]", ")", "\n", "df", "=", "pandas", ".", "read_csv", "(", "fh", ",", "index_col", "=", "None", ")", "\n", "headers", ".", "append", "(", "header", ")", "\n", "", "elif", "fname", ".", "endswith", "(", "'json'", ")", ":", "# Deprecated json format", "\n", "                ", "episodes", "=", "[", "]", "\n", "lines", "=", "fh", ".", "readlines", "(", ")", "\n", "header", "=", "json", ".", "loads", "(", "lines", "[", "0", "]", ")", "\n", "headers", ".", "append", "(", "header", ")", "\n", "for", "line", "in", "lines", "[", "1", ":", "]", ":", "\n", "                    ", "episode", "=", "json", ".", "loads", "(", "line", ")", "\n", "episodes", ".", "append", "(", "episode", ")", "\n", "", "df", "=", "pandas", ".", "DataFrame", "(", "episodes", ")", "\n", "", "else", ":", "\n", "                ", "assert", "0", ",", "'unreachable'", "\n", "", "df", "[", "'t'", "]", "+=", "header", "[", "'t_start'", "]", "\n", "", "dfs", ".", "append", "(", "df", ")", "\n", "", "df", "=", "pandas", ".", "concat", "(", "dfs", ")", "\n", "df", ".", "sort_values", "(", "'t'", ",", "inplace", "=", "True", ")", "\n", "df", ".", "reset_index", "(", "inplace", "=", "True", ")", "\n", "df", "[", "'t'", "]", "-=", "min", "(", "header", "[", "'t_start'", "]", "for", "header", "in", "headers", ")", "\n", "df", ".", "headers", "=", "headers", "# HACK to preserve backwards compatibility", "\n", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.test_monitor": [[166, 189], ["gym.make", "gym.make.seed", "monitor.Monitor", "monitor.Monitor.reset", "range", "open", "open.readline", "f.readline.startswith", "json.loads", "pandas.read_csv", "open.close", "os.remove", "uuid.uuid4", "monitor.Monitor.step", "set", "set", "monitor.Monitor.reset", "json.loads.keys", "pandas.read_csv.keys"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.read_csv", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.step", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset"], ["", "def", "test_monitor", "(", ")", ":", "\n", "    ", "env", "=", "gym", ".", "make", "(", "\"CartPole-v1\"", ")", "\n", "env", ".", "seed", "(", "0", ")", "\n", "mon_file", "=", "\"/tmp/baselines-test-%s.monitor.csv\"", "%", "uuid", ".", "uuid4", "(", ")", "\n", "menv", "=", "Monitor", "(", "env", ",", "mon_file", ")", "\n", "menv", ".", "reset", "(", ")", "\n", "for", "_", "in", "range", "(", "1000", ")", ":", "\n", "        ", "_", ",", "_", ",", "done", ",", "_", "=", "menv", ".", "step", "(", "0", ")", "\n", "if", "done", ":", "\n", "            ", "menv", ".", "reset", "(", ")", "\n", "\n", "", "", "f", "=", "open", "(", "mon_file", ",", "'rt'", ")", "\n", "\n", "firstline", "=", "f", ".", "readline", "(", ")", "\n", "assert", "firstline", ".", "startswith", "(", "'#'", ")", "\n", "metadata", "=", "json", ".", "loads", "(", "firstline", "[", "1", ":", "]", ")", "\n", "assert", "metadata", "[", "'env_id'", "]", "==", "\"CartPole-v1\"", "\n", "assert", "set", "(", "metadata", ".", "keys", "(", ")", ")", "==", "{", "'env_id'", ",", "'gym_version'", ",", "'t_start'", "}", ",", "\"Incorrect keys in monitor metadata\"", "\n", "\n", "last_logline", "=", "pandas", ".", "read_csv", "(", "f", ",", "index_col", "=", "None", ")", "\n", "assert", "set", "(", "last_logline", ".", "keys", "(", ")", ")", "==", "{", "'l'", ",", "'t'", ",", "'r'", "}", ",", "\"Incorrect keys in monitor logline\"", "\n", "f", ".", "close", "(", ")", "\n", "os", ".", "remove", "(", "mon_file", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.tile_images.tile_images": [[3, 23], ["numpy.asarray", "int", "int", "numpy.array", "np.array.reshape", "img_nhwc.reshape.transpose", "img_HWhwc.transpose.reshape", "numpy.ceil", "numpy.ceil", "numpy.sqrt", "list", "float", "range"], "function", ["None"], ["def", "tile_images", "(", "img_nhwc", ")", ":", "\n", "    ", "\"\"\"\n    Tile N images into one big PxQ image\n    (P,Q) are chosen to be as close as possible, and if N\n    is square, then P=Q.\n\n    input: img_nhwc, list or array of images, ndim=4 once turned into array\n        n = batch index, h = height, w = width, c = channel\n    returns:\n        bigim_HWc, ndarray with ndim=3\n    \"\"\"", "\n", "img_nhwc", "=", "np", ".", "asarray", "(", "img_nhwc", ")", "\n", "N", ",", "h", ",", "w", ",", "c", "=", "img_nhwc", ".", "shape", "\n", "H", "=", "int", "(", "np", ".", "ceil", "(", "np", ".", "sqrt", "(", "N", ")", ")", ")", "\n", "W", "=", "int", "(", "np", ".", "ceil", "(", "float", "(", "N", ")", "/", "H", ")", ")", "\n", "img_nhwc", "=", "np", ".", "array", "(", "list", "(", "img_nhwc", ")", "+", "[", "img_nhwc", "[", "0", "]", "*", "0", "for", "_", "in", "range", "(", "N", ",", "H", "*", "W", ")", "]", ")", "\n", "img_HWhwc", "=", "img_nhwc", ".", "reshape", "(", "H", ",", "W", ",", "h", ",", "w", ",", "c", ")", "\n", "img_HhWwc", "=", "img_HWhwc", ".", "transpose", "(", "0", ",", "2", ",", "1", ",", "3", ",", "4", ")", "\n", "img_Hh_Ww_c", "=", "img_HhWwc", ".", "reshape", "(", "H", "*", "h", ",", "W", "*", "w", ",", "c", ")", "\n", "return", "img_Hh_Ww_c", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__": [[25, 60], ["multiprocessing.get_context", "vec_env.VecEnv.__init__", "util.obs_space_info", "baselines.logger.log", "len", "vec_env.clear_mpi_env_vars", "zip", "baselines.logger.scoped_configure", "dummy.close", "multiprocessing.get_context.Array", "vec_env.CloudpickleWrapper", "multiprocessing.get_context.Pipe", "multiprocessing.get_context.Process", "shmem_vec_env.ShmemVecEnv.procs.append", "shmem_vec_env.ShmemVecEnv.parent_pipes.append", "mp.get_context.Process.start", "child_pipe.close", "int", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.__init__", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.util.obs_space_info", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.log", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.vec_env.clear_mpi_env_vars", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.scoped_configure", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close"], ["def", "__init__", "(", "self", ",", "env_fns", ",", "spaces", "=", "None", ",", "context", "=", "'spawn'", ")", ":", "\n", "        ", "\"\"\"\n        If you don't specify observation_space, we'll have to create a dummy\n        environment to get it.\n        \"\"\"", "\n", "ctx", "=", "mp", ".", "get_context", "(", "context", ")", "\n", "if", "spaces", ":", "\n", "            ", "observation_space", ",", "action_space", "=", "spaces", "\n", "", "else", ":", "\n", "            ", "logger", ".", "log", "(", "'Creating dummy env object to get spaces'", ")", "\n", "with", "logger", ".", "scoped_configure", "(", "format_strs", "=", "[", "]", ")", ":", "\n", "                ", "dummy", "=", "env_fns", "[", "0", "]", "(", ")", "\n", "observation_space", ",", "action_space", "=", "dummy", ".", "observation_space", ",", "dummy", ".", "action_space", "\n", "dummy", ".", "close", "(", ")", "\n", "del", "dummy", "\n", "", "", "VecEnv", ".", "__init__", "(", "self", ",", "len", "(", "env_fns", ")", ",", "observation_space", ",", "action_space", ")", "\n", "self", ".", "obs_keys", ",", "self", ".", "obs_shapes", ",", "self", ".", "obs_dtypes", "=", "obs_space_info", "(", "observation_space", ")", "\n", "self", ".", "obs_bufs", "=", "[", "\n", "{", "k", ":", "ctx", ".", "Array", "(", "_NP_TO_CT", "[", "self", ".", "obs_dtypes", "[", "k", "]", ".", "type", "]", ",", "int", "(", "np", ".", "prod", "(", "self", ".", "obs_shapes", "[", "k", "]", ")", ")", ")", "for", "k", "in", "self", ".", "obs_keys", "}", "\n", "for", "_", "in", "env_fns", "]", "\n", "self", ".", "parent_pipes", "=", "[", "]", "\n", "self", ".", "procs", "=", "[", "]", "\n", "with", "clear_mpi_env_vars", "(", ")", ":", "\n", "            ", "for", "env_fn", ",", "obs_buf", "in", "zip", "(", "env_fns", ",", "self", ".", "obs_bufs", ")", ":", "\n", "                ", "wrapped_fn", "=", "CloudpickleWrapper", "(", "env_fn", ")", "\n", "parent_pipe", ",", "child_pipe", "=", "ctx", ".", "Pipe", "(", ")", "\n", "proc", "=", "ctx", ".", "Process", "(", "target", "=", "_subproc_worker", ",", "\n", "args", "=", "(", "child_pipe", ",", "parent_pipe", ",", "wrapped_fn", ",", "obs_buf", ",", "self", ".", "obs_shapes", ",", "self", ".", "obs_dtypes", ",", "self", ".", "obs_keys", ")", ")", "\n", "proc", ".", "daemon", "=", "True", "\n", "self", ".", "procs", ".", "append", "(", "proc", ")", "\n", "self", ".", "parent_pipes", ".", "append", "(", "parent_pipe", ")", "\n", "proc", ".", "start", "(", ")", "\n", "child_pipe", ".", "close", "(", ")", "\n", "", "", "self", ".", "waiting_step", "=", "False", "\n", "self", ".", "viewer", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.reset": [[61, 68], ["shmem_vec_env.ShmemVecEnv._decode_obses", "baselines.logger.warn", "shmem_vec_env.ShmemVecEnv.step_wait", "pipe.send", "pipe.recv"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv._decode_obses", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.WDAIL.logger.warn", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.step_wait"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "waiting_step", ":", "\n", "            ", "logger", ".", "warn", "(", "'Called reset() while waiting for the step to complete'", ")", "\n", "self", ".", "step_wait", "(", ")", "\n", "", "for", "pipe", "in", "self", ".", "parent_pipes", ":", "\n", "            ", "pipe", ".", "send", "(", "(", "'reset'", ",", "None", ")", ")", "\n", "", "return", "self", ".", "_decode_obses", "(", "[", "pipe", ".", "recv", "(", ")", "for", "pipe", "in", "self", ".", "parent_pipes", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.step_async": [[69, 73], ["zip", "len", "len", "pipe.send"], "methods", ["None"], ["", "def", "step_async", "(", "self", ",", "actions", ")", ":", "\n", "        ", "assert", "len", "(", "actions", ")", "==", "len", "(", "self", ".", "parent_pipes", ")", "\n", "for", "pipe", ",", "act", "in", "zip", "(", "self", ".", "parent_pipes", ",", "actions", ")", ":", "\n", "            ", "pipe", ".", "send", "(", "(", "'step'", ",", "act", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.step_wait": [[74, 78], ["zip", "pipe.recv", "shmem_vec_env.ShmemVecEnv._decode_obses", "numpy.array", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv._decode_obses"], ["", "", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "outs", "=", "[", "pipe", ".", "recv", "(", ")", "for", "pipe", "in", "self", ".", "parent_pipes", "]", "\n", "obs", ",", "rews", ",", "dones", ",", "infos", "=", "zip", "(", "*", "outs", ")", "\n", "return", "self", ".", "_decode_obses", "(", "obs", ")", ",", "np", ".", "array", "(", "rews", ")", ",", "np", ".", "array", "(", "dones", ")", ",", "infos", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.close_extras": [[79, 89], ["shmem_vec_env.ShmemVecEnv.step_wait", "pipe.send", "pipe.recv", "pipe.close", "proc.join"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.step_wait", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close"], ["", "def", "close_extras", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "waiting_step", ":", "\n", "            ", "self", ".", "step_wait", "(", ")", "\n", "", "for", "pipe", "in", "self", ".", "parent_pipes", ":", "\n", "            ", "pipe", ".", "send", "(", "(", "'close'", ",", "None", ")", ")", "\n", "", "for", "pipe", "in", "self", ".", "parent_pipes", ":", "\n", "            ", "pipe", ".", "recv", "(", ")", "\n", "pipe", ".", "close", "(", ")", "\n", "", "for", "proc", "in", "self", ".", "procs", ":", "\n", "            ", "proc", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv.get_images": [[90, 94], ["pipe.send", "pipe.recv"], "methods", ["None"], ["", "", "def", "get_images", "(", "self", ",", "mode", "=", "'human'", ")", ":", "\n", "        ", "for", "pipe", "in", "self", ".", "parent_pipes", ":", "\n", "            ", "pipe", ".", "send", "(", "(", "'render'", ",", "None", ")", ")", "\n", "", "return", "[", "pipe", ".", "recv", "(", ")", "for", "pipe", "in", "self", ".", "parent_pipes", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env.ShmemVecEnv._decode_obses": [[95, 103], ["util.dict_to_obs", "numpy.array", "numpy.frombuffer().reshape", "numpy.frombuffer", "b.get_obj"], "methods", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.util.dict_to_obs"], ["", "def", "_decode_obses", "(", "self", ",", "obs", ")", ":", "\n", "        ", "result", "=", "{", "}", "\n", "for", "k", "in", "self", ".", "obs_keys", ":", "\n", "\n", "            ", "bufs", "=", "[", "b", "[", "k", "]", "for", "b", "in", "self", ".", "obs_bufs", "]", "\n", "o", "=", "[", "np", ".", "frombuffer", "(", "b", ".", "get_obj", "(", ")", ",", "dtype", "=", "self", ".", "obs_dtypes", "[", "k", "]", ")", ".", "reshape", "(", "self", ".", "obs_shapes", "[", "k", "]", ")", "for", "b", "in", "bufs", "]", "\n", "result", "[", "k", "]", "=", "np", ".", "array", "(", "o", ")", "\n", "", "return", "dict_to_obs", "(", "result", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.shmem_vec_env._subproc_worker": [[105, 140], ["env_fn_wrapper.x", "parent_pipe.close", "util.obs_to_dict", "env_fn_wrapper.x.close", "obs_bufs[].get_obj", "numpy.frombuffer().reshape", "numpy.copyto", "pipe.recv", "print", "pipe.send", "numpy.frombuffer", "shmem_vec_env._subproc_worker._write_obs"], "function", ["home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.util.obs_to_dict", "home.repos.pwc.inspect_result.mingzhangPHD_Adversarial-Imitation-Learning.vec_env.monitor.Monitor.close"], ["", "", "def", "_subproc_worker", "(", "pipe", ",", "parent_pipe", ",", "env_fn_wrapper", ",", "obs_bufs", ",", "obs_shapes", ",", "obs_dtypes", ",", "keys", ")", ":", "\n", "    ", "\"\"\"\n    Control a single environment instance using IPC and\n    shared memory.\n    \"\"\"", "\n", "def", "_write_obs", "(", "maybe_dict_obs", ")", ":", "\n", "        ", "flatdict", "=", "obs_to_dict", "(", "maybe_dict_obs", ")", "\n", "for", "k", "in", "keys", ":", "\n", "            ", "dst", "=", "obs_bufs", "[", "k", "]", ".", "get_obj", "(", ")", "\n", "dst_np", "=", "np", ".", "frombuffer", "(", "dst", ",", "dtype", "=", "obs_dtypes", "[", "k", "]", ")", ".", "reshape", "(", "obs_shapes", "[", "k", "]", ")", "# pylint: disable=W0212", "\n", "np", ".", "copyto", "(", "dst_np", ",", "flatdict", "[", "k", "]", ")", "\n", "\n", "", "", "env", "=", "env_fn_wrapper", ".", "x", "(", ")", "\n", "parent_pipe", ".", "close", "(", ")", "\n", "try", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "cmd", ",", "data", "=", "pipe", ".", "recv", "(", ")", "\n", "if", "cmd", "==", "'reset'", ":", "\n", "                ", "pipe", ".", "send", "(", "_write_obs", "(", "env", ".", "reset", "(", ")", ")", ")", "\n", "", "elif", "cmd", "==", "'step'", ":", "\n", "                ", "obs", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "data", ")", "\n", "if", "done", ":", "\n", "                    ", "obs", "=", "env", ".", "reset", "(", ")", "\n", "", "pipe", ".", "send", "(", "(", "_write_obs", "(", "obs", ")", ",", "reward", ",", "done", ",", "info", ")", ")", "\n", "", "elif", "cmd", "==", "'render'", ":", "\n", "                ", "pipe", ".", "send", "(", "env", ".", "render", "(", "mode", "=", "'rgb_array'", ")", ")", "\n", "", "elif", "cmd", "==", "'close'", ":", "\n", "                ", "pipe", ".", "send", "(", "None", ")", "\n", "break", "\n", "", "else", ":", "\n", "                ", "raise", "RuntimeError", "(", "'Got unrecognized cmd %s'", "%", "cmd", ")", "\n", "", "", "", "except", "KeyboardInterrupt", ":", "\n", "        ", "print", "(", "'ShmemVecEnv worker: got KeyboardInterrupt'", ")", "\n", "", "finally", ":", "\n", "        ", "env", ".", "close", "(", ")", "\n", "", "", ""]]}