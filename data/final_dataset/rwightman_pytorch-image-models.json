{"home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.validate.validate": [[116, 281], ["timm.models.create_model", "sum", "_logger.info", "timm.data.resolve_data_config", "torch.nn.DataParallel.cuda", "torch.CrossEntropyLoss().cuda", "timm.data.create_dataset", "timm.data.create_loader", "timm.utils.AverageMeter", "timm.utils.AverageMeter", "timm.utils.AverageMeter", "timm.utils.AverageMeter", "torch.nn.DataParallel.eval", "collections.OrderedDict", "_logger.info", "_logger.info", "timm.utils.set_jit_fuser", "hasattr", "timm.models.load_checkpoint", "vars", "timm.models.apply_test_time_pool", "torch.jit.optimized_execution", "torch.jit.optimized_execution", "torch.jit.optimized_execution", "torch.jit.script", "torch.jit.script", "torch.jit.script", "amp.initialize", "torch.nn.DataParallel.to", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "timm.data.RealLabelsImagenet", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.randn().cuda", "torch.randn().cuda", "torch.randn().cuda", "time.time", "enumerate", "_logger.info", "_logger.info", "m.numel", "torch.CrossEntropyLoss", "open", "timm.data.create_dataset.filenames", "input.contiguous.contiguous", "amp_autocast", "torch.nn.DataParallel.", "nn.CrossEntropyLoss().cuda.", "timm.utils.accuracy", "timm.utils.AverageMeter.update", "timm.utils.AverageMeter.update", "timm.utils.AverageMeter.update", "timm.utils.AverageMeter.update", "time.time", "timm.data.RealLabelsImagenet.get_accuracy", "timm.data.RealLabelsImagenet.get_accuracy", "round", "round", "round", "round", "round", "_logger.warning", "torch.nn.DataParallel.parameters", "list", "int", "torch.randn", "torch.randn", "torch.randn", "target.cuda.cuda", "input.contiguous.cuda", "input.contiguous.contiguous", "amp_autocast", "torch.nn.DataParallel.", "timm.data.RealLabelsImagenet.add_result", "model.detach", "criterion.item", "input.contiguous.size", "acc1.item", "input.contiguous.size", "acc5.item", "input.contiguous.size", "_logger.info", "range", "line.rstrip", "range", "time.time", "tuple", "len", "input.contiguous.size"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.create_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.config.resolve_data_config", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset_factory.create_dataset", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader.create_loader", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.jit.set_jit_fuser", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_checkpoint", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.test_time_pool.apply_test_time_pool", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser.Parser.filenames", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.accuracy", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.real_labels.RealLabelsImagenet.get_accuracy", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.real_labels.RealLabelsImagenet.get_accuracy", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.real_labels.RealLabelsImagenet.add_result"], ["def", "validate", "(", "args", ")", ":", "\n", "# might as well try to validate something", "\n", "    ", "args", ".", "pretrained", "=", "args", ".", "pretrained", "or", "not", "args", ".", "checkpoint", "\n", "args", ".", "prefetcher", "=", "not", "args", ".", "no_prefetcher", "\n", "amp_autocast", "=", "suppress", "# do nothing", "\n", "if", "args", ".", "amp", ":", "\n", "        ", "if", "has_native_amp", ":", "\n", "            ", "args", ".", "native_amp", "=", "True", "\n", "", "elif", "has_apex", ":", "\n", "            ", "args", ".", "apex_amp", "=", "True", "\n", "", "else", ":", "\n", "            ", "_logger", ".", "warning", "(", "\"Neither APEX or Native Torch AMP is available.\"", ")", "\n", "", "", "assert", "not", "args", ".", "apex_amp", "or", "not", "args", ".", "native_amp", ",", "\"Only one AMP mode should be set.\"", "\n", "if", "args", ".", "native_amp", ":", "\n", "        ", "amp_autocast", "=", "torch", ".", "cuda", ".", "amp", ".", "autocast", "\n", "_logger", ".", "info", "(", "'Validating in mixed precision with native PyTorch AMP.'", ")", "\n", "", "elif", "args", ".", "apex_amp", ":", "\n", "        ", "_logger", ".", "info", "(", "'Validating in mixed precision with NVIDIA APEX AMP.'", ")", "\n", "", "else", ":", "\n", "        ", "_logger", ".", "info", "(", "'Validating in float32. AMP not enabled.'", ")", "\n", "\n", "", "if", "args", ".", "fuser", ":", "\n", "        ", "set_jit_fuser", "(", "args", ".", "fuser", ")", "\n", "\n", "# create model", "\n", "", "model", "=", "create_model", "(", "\n", "args", ".", "model", ",", "\n", "pretrained", "=", "args", ".", "pretrained", ",", "\n", "num_classes", "=", "args", ".", "num_classes", ",", "\n", "in_chans", "=", "3", ",", "\n", "global_pool", "=", "args", ".", "gp", ",", "\n", "scriptable", "=", "args", ".", "torchscript", ")", "\n", "if", "args", ".", "num_classes", "is", "None", ":", "\n", "        ", "assert", "hasattr", "(", "model", ",", "'num_classes'", ")", ",", "'Model must have `num_classes` attr if not set on cmd line/config.'", "\n", "args", ".", "num_classes", "=", "model", ".", "num_classes", "\n", "\n", "", "if", "args", ".", "checkpoint", ":", "\n", "        ", "load_checkpoint", "(", "model", ",", "args", ".", "checkpoint", ",", "args", ".", "use_ema", ")", "\n", "\n", "", "param_count", "=", "sum", "(", "[", "m", ".", "numel", "(", ")", "for", "m", "in", "model", ".", "parameters", "(", ")", "]", ")", "\n", "_logger", ".", "info", "(", "'Model %s created, param count: %d'", "%", "(", "args", ".", "model", ",", "param_count", ")", ")", "\n", "\n", "data_config", "=", "resolve_data_config", "(", "vars", "(", "args", ")", ",", "model", "=", "model", ",", "use_test_size", "=", "True", ",", "verbose", "=", "True", ")", "\n", "test_time_pool", "=", "False", "\n", "if", "args", ".", "test_pool", ":", "\n", "        ", "model", ",", "test_time_pool", "=", "apply_test_time_pool", "(", "model", ",", "data_config", ",", "use_test_size", "=", "True", ")", "\n", "\n", "", "if", "args", ".", "torchscript", ":", "\n", "        ", "torch", ".", "jit", ".", "optimized_execution", "(", "True", ")", "\n", "model", "=", "torch", ".", "jit", ".", "script", "(", "model", ")", "\n", "\n", "", "model", "=", "model", ".", "cuda", "(", ")", "\n", "if", "args", ".", "apex_amp", ":", "\n", "        ", "model", "=", "amp", ".", "initialize", "(", "model", ",", "opt_level", "=", "'O1'", ")", "\n", "\n", "", "if", "args", ".", "channels_last", ":", "\n", "        ", "model", "=", "model", ".", "to", "(", "memory_format", "=", "torch", ".", "channels_last", ")", "\n", "\n", "", "if", "args", ".", "num_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ",", "device_ids", "=", "list", "(", "range", "(", "args", ".", "num_gpu", ")", ")", ")", "\n", "\n", "", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "dataset", "=", "create_dataset", "(", "\n", "root", "=", "args", ".", "data", ",", "name", "=", "args", ".", "dataset", ",", "split", "=", "args", ".", "split", ",", "\n", "download", "=", "args", ".", "dataset_download", ",", "load_bytes", "=", "args", ".", "tf_preprocessing", ",", "class_map", "=", "args", ".", "class_map", ")", "\n", "\n", "if", "args", ".", "valid_labels", ":", "\n", "        ", "with", "open", "(", "args", ".", "valid_labels", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "valid_labels", "=", "{", "int", "(", "line", ".", "rstrip", "(", ")", ")", "for", "line", "in", "f", "}", "\n", "valid_labels", "=", "[", "i", "in", "valid_labels", "for", "i", "in", "range", "(", "args", ".", "num_classes", ")", "]", "\n", "", "", "else", ":", "\n", "        ", "valid_labels", "=", "None", "\n", "\n", "", "if", "args", ".", "real_labels", ":", "\n", "        ", "real_labels", "=", "RealLabelsImagenet", "(", "dataset", ".", "filenames", "(", "basename", "=", "True", ")", ",", "real_json", "=", "args", ".", "real_labels", ")", "\n", "", "else", ":", "\n", "        ", "real_labels", "=", "None", "\n", "\n", "", "crop_pct", "=", "1.0", "if", "test_time_pool", "else", "data_config", "[", "'crop_pct'", "]", "\n", "loader", "=", "create_loader", "(", "\n", "dataset", ",", "\n", "input_size", "=", "data_config", "[", "'input_size'", "]", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "use_prefetcher", "=", "args", ".", "prefetcher", ",", "\n", "interpolation", "=", "data_config", "[", "'interpolation'", "]", ",", "\n", "mean", "=", "data_config", "[", "'mean'", "]", ",", "\n", "std", "=", "data_config", "[", "'std'", "]", ",", "\n", "num_workers", "=", "args", ".", "workers", ",", "\n", "crop_pct", "=", "crop_pct", ",", "\n", "pin_memory", "=", "args", ".", "pin_mem", ",", "\n", "tf_preprocessing", "=", "args", ".", "tf_preprocessing", ")", "\n", "\n", "batch_time", "=", "AverageMeter", "(", ")", "\n", "losses", "=", "AverageMeter", "(", ")", "\n", "top1", "=", "AverageMeter", "(", ")", "\n", "top5", "=", "AverageMeter", "(", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# warmup, reduce variability of first batch time, especially for comparing torchscript vs non", "\n", "        ", "input", "=", "torch", ".", "randn", "(", "(", "args", ".", "batch_size", ",", ")", "+", "tuple", "(", "data_config", "[", "'input_size'", "]", ")", ")", ".", "cuda", "(", ")", "\n", "if", "args", ".", "channels_last", ":", "\n", "            ", "input", "=", "input", ".", "contiguous", "(", "memory_format", "=", "torch", ".", "channels_last", ")", "\n", "", "with", "amp_autocast", "(", ")", ":", "\n", "            ", "model", "(", "input", ")", "\n", "\n", "", "end", "=", "time", ".", "time", "(", ")", "\n", "for", "batch_idx", ",", "(", "input", ",", "target", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "            ", "if", "args", ".", "no_prefetcher", ":", "\n", "                ", "target", "=", "target", ".", "cuda", "(", ")", "\n", "input", "=", "input", ".", "cuda", "(", ")", "\n", "", "if", "args", ".", "channels_last", ":", "\n", "                ", "input", "=", "input", ".", "contiguous", "(", "memory_format", "=", "torch", ".", "channels_last", ")", "\n", "\n", "# compute output", "\n", "", "with", "amp_autocast", "(", ")", ":", "\n", "                ", "output", "=", "model", "(", "input", ")", "\n", "\n", "", "if", "valid_labels", "is", "not", "None", ":", "\n", "                ", "output", "=", "output", "[", ":", ",", "valid_labels", "]", "\n", "", "loss", "=", "criterion", "(", "output", ",", "target", ")", "\n", "\n", "if", "real_labels", "is", "not", "None", ":", "\n", "                ", "real_labels", ".", "add_result", "(", "output", ")", "\n", "\n", "# measure accuracy and record loss", "\n", "", "acc1", ",", "acc5", "=", "accuracy", "(", "output", ".", "detach", "(", ")", ",", "target", ",", "topk", "=", "(", "1", ",", "5", ")", ")", "\n", "losses", ".", "update", "(", "loss", ".", "item", "(", ")", ",", "input", ".", "size", "(", "0", ")", ")", "\n", "top1", ".", "update", "(", "acc1", ".", "item", "(", ")", ",", "input", ".", "size", "(", "0", ")", ")", "\n", "top5", ".", "update", "(", "acc5", ".", "item", "(", ")", ",", "input", ".", "size", "(", "0", ")", ")", "\n", "\n", "# measure elapsed time", "\n", "batch_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "\n", "if", "batch_idx", "%", "args", ".", "log_freq", "==", "0", ":", "\n", "                ", "_logger", ".", "info", "(", "\n", "'Test: [{0:>4d}/{1}]  '", "\n", "'Time: {batch_time.val:.3f}s ({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '", "\n", "'Loss: {loss.val:>7.4f} ({loss.avg:>6.4f})  '", "\n", "'Acc@1: {top1.val:>7.3f} ({top1.avg:>7.3f})  '", "\n", "'Acc@5: {top5.val:>7.3f} ({top5.avg:>7.3f})'", ".", "format", "(", "\n", "batch_idx", ",", "len", "(", "loader", ")", ",", "batch_time", "=", "batch_time", ",", "\n", "rate_avg", "=", "input", ".", "size", "(", "0", ")", "/", "batch_time", ".", "avg", ",", "\n", "loss", "=", "losses", ",", "top1", "=", "top1", ",", "top5", "=", "top5", ")", ")", "\n", "\n", "", "", "", "if", "real_labels", "is", "not", "None", ":", "\n", "# real labels mode replaces topk values at the end", "\n", "        ", "top1a", ",", "top5a", "=", "real_labels", ".", "get_accuracy", "(", "k", "=", "1", ")", ",", "real_labels", ".", "get_accuracy", "(", "k", "=", "5", ")", "\n", "", "else", ":", "\n", "        ", "top1a", ",", "top5a", "=", "top1", ".", "avg", ",", "top5", ".", "avg", "\n", "", "results", "=", "OrderedDict", "(", "\n", "model", "=", "args", ".", "model", ",", "\n", "top1", "=", "round", "(", "top1a", ",", "4", ")", ",", "top1_err", "=", "round", "(", "100", "-", "top1a", ",", "4", ")", ",", "\n", "top5", "=", "round", "(", "top5a", ",", "4", ")", ",", "top5_err", "=", "round", "(", "100", "-", "top5a", ",", "4", ")", ",", "\n", "param_count", "=", "round", "(", "param_count", "/", "1e6", ",", "2", ")", ",", "\n", "img_size", "=", "data_config", "[", "'input_size'", "]", "[", "-", "1", "]", ",", "\n", "crop_pct", "=", "crop_pct", ",", "\n", "interpolation", "=", "data_config", "[", "'interpolation'", "]", ")", "\n", "\n", "_logger", ".", "info", "(", "' * Acc@1 {:.3f} ({:.3f}) Acc@5 {:.3f} ({:.3f})'", ".", "format", "(", "\n", "results", "[", "'top1'", "]", ",", "results", "[", "'top1_err'", "]", ",", "results", "[", "'top5'", "]", ",", "results", "[", "'top5_err'", "]", ")", ")", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.validate._try_run": [[283, 302], ["collections.OrderedDict", "_logger.error", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "validate.validate", "str", "_logger.warning"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.train.validate"], ["", "def", "_try_run", "(", "args", ",", "initial_batch_size", ")", ":", "\n", "    ", "batch_size", "=", "initial_batch_size", "\n", "results", "=", "OrderedDict", "(", ")", "\n", "error_str", "=", "'Unknown'", "\n", "while", "batch_size", ">=", "1", ":", "\n", "        ", "args", ".", "batch_size", "=", "batch_size", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "try", ":", "\n", "            ", "results", "=", "validate", "(", "args", ")", "\n", "return", "results", "\n", "", "except", "RuntimeError", "as", "e", ":", "\n", "            ", "error_str", "=", "str", "(", "e", ")", "\n", "if", "'channels_last'", "in", "error_str", ":", "\n", "                ", "break", "\n", "", "_logger", ".", "warning", "(", "f'\"{error_str}\" while running validation. Reducing batch size to {batch_size} for retry.'", ")", "\n", "", "batch_size", "=", "batch_size", "//", "2", "\n", "", "results", "[", "'error'", "]", "=", "error_str", "\n", "_logger", ".", "error", "(", "f'{args.model} failed to validate ({error_str}).'", ")", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.validate.main": [[304, 355], ["timm.utils.setup_default_logging", "parser.parse_args", "os.path.isdir", "len", "print", "glob.glob", "glob.glob", "timm.models.list_models", "_logger.info", "sorted", "len", "validate.validate", "timm.models.list_models", "os.path.isfile", "validate.write_results", "sorted", "timm.models.is_model", "timm.models.list_models", "open", "validate._try_run", "validate.append", "json.dumps", "line.rstrip"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.log.setup_default_logging", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.list_models", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.train.validate", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.list_models", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.write_results", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.is_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.list_models", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark._try_run"], ["", "def", "main", "(", ")", ":", "\n", "    ", "setup_default_logging", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "model_cfgs", "=", "[", "]", "\n", "model_names", "=", "[", "]", "\n", "if", "os", ".", "path", ".", "isdir", "(", "args", ".", "checkpoint", ")", ":", "\n", "# validate all checkpoints in a path with same model", "\n", "        ", "checkpoints", "=", "glob", ".", "glob", "(", "args", ".", "checkpoint", "+", "'/*.pth.tar'", ")", "\n", "checkpoints", "+=", "glob", ".", "glob", "(", "args", ".", "checkpoint", "+", "'/*.pth'", ")", "\n", "model_names", "=", "list_models", "(", "args", ".", "model", ")", "\n", "model_cfgs", "=", "[", "(", "args", ".", "model", ",", "c", ")", "for", "c", "in", "sorted", "(", "checkpoints", ",", "key", "=", "natural_key", ")", "]", "\n", "", "else", ":", "\n", "        ", "if", "args", ".", "model", "==", "'all'", ":", "\n", "# validate all models in a list of names with pretrained checkpoints", "\n", "            ", "args", ".", "pretrained", "=", "True", "\n", "model_names", "=", "list_models", "(", "pretrained", "=", "True", ",", "exclude_filters", "=", "[", "'*_in21k'", ",", "'*_in22k'", ",", "'*_dino'", "]", ")", "\n", "model_cfgs", "=", "[", "(", "n", ",", "''", ")", "for", "n", "in", "model_names", "]", "\n", "", "elif", "not", "is_model", "(", "args", ".", "model", ")", ":", "\n", "# model name doesn't exist, try as wildcard filter", "\n", "            ", "model_names", "=", "list_models", "(", "args", ".", "model", ")", "\n", "model_cfgs", "=", "[", "(", "n", ",", "''", ")", "for", "n", "in", "model_names", "]", "\n", "\n", "", "if", "not", "model_cfgs", "and", "os", ".", "path", ".", "isfile", "(", "args", ".", "model", ")", ":", "\n", "            ", "with", "open", "(", "args", ".", "model", ")", "as", "f", ":", "\n", "                ", "model_names", "=", "[", "line", ".", "rstrip", "(", ")", "for", "line", "in", "f", "]", "\n", "", "model_cfgs", "=", "[", "(", "n", ",", "None", ")", "for", "n", "in", "model_names", "if", "n", "]", "\n", "\n", "", "", "if", "len", "(", "model_cfgs", ")", ":", "\n", "        ", "results_file", "=", "args", ".", "results_file", "or", "'./results-all.csv'", "\n", "_logger", ".", "info", "(", "'Running bulk validation on these pretrained models: {}'", ".", "format", "(", "', '", ".", "join", "(", "model_names", ")", ")", ")", "\n", "results", "=", "[", "]", "\n", "try", ":", "\n", "            ", "initial_batch_size", "=", "args", ".", "batch_size", "\n", "for", "m", ",", "c", "in", "model_cfgs", ":", "\n", "                ", "args", ".", "model", "=", "m", "\n", "args", ".", "checkpoint", "=", "c", "\n", "r", "=", "_try_run", "(", "args", ",", "initial_batch_size", ")", "\n", "if", "'error'", "in", "r", ":", "\n", "                    ", "continue", "\n", "", "if", "args", ".", "checkpoint", ":", "\n", "                    ", "r", "[", "'checkpoint'", "]", "=", "args", ".", "checkpoint", "\n", "", "results", ".", "append", "(", "r", ")", "\n", "", "", "except", "KeyboardInterrupt", "as", "e", ":", "\n", "            ", "pass", "\n", "", "results", "=", "sorted", "(", "results", ",", "key", "=", "lambda", "x", ":", "x", "[", "'top1'", "]", ",", "reverse", "=", "True", ")", "\n", "if", "len", "(", "results", ")", ":", "\n", "            ", "write_results", "(", "results_file", ",", "results", ")", "\n", "", "", "else", ":", "\n", "        ", "results", "=", "validate", "(", "args", ")", "\n", "# output results in JSON to stdout w/ delimiter for runner script", "\n", "", "print", "(", "f'--result\\n{json.dumps(results, indent=4)}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.validate.write_results": [[357, 364], ["open", "csv.DictWriter", "csv.DictWriter.writeheader", "cf.flush", "csv.DictWriter.writerow", "results[].keys"], "function", ["None"], ["", "def", "write_results", "(", "results_file", ",", "results", ")", ":", "\n", "    ", "with", "open", "(", "results_file", ",", "mode", "=", "'w'", ")", "as", "cf", ":", "\n", "        ", "dw", "=", "csv", ".", "DictWriter", "(", "cf", ",", "fieldnames", "=", "results", "[", "0", "]", ".", "keys", "(", ")", ")", "\n", "dw", ".", "writeheader", "(", ")", "\n", "for", "r", "in", "results", ":", "\n", "            ", "dw", ".", "writerow", "(", "r", ")", "\n", "", "cf", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.inference.main": [[60, 124], ["timm.utils.setup_default_logging", "parser.parse_args", "timm.models.create_model", "_logger.info", "timm.data.resolve_data_config", "timm.data.create_loader", "model.cuda.eval", "min", "timm.utils.AverageMeter", "time.time", "numpy.concatenate", "vars", "timm.models.apply_test_time_pool", "torch.nn.DataParallel().cuda", "model.cuda.cuda", "timm.data.ImageDataset", "torch.no_grad", "enumerate", "open", "timm.data.create_loader.dataset.filenames", "zip", "input.cuda.cuda", "model.cuda.", "np.concatenate.append", "timm.utils.AverageMeter.update", "time.time", "os.path.join", "out_file.write", "sum", "torch.nn.DataParallel", "model.topk", "topk.cpu().numpy", "_logger.info", "time.time", "m.numel", "list", "topk.cpu", "len", "model.cuda.parameters", "range", "str"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.log.setup_default_logging", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.create_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.config.resolve_data_config", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader.create_loader", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.test_time_pool.apply_test_time_pool", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser.Parser.filenames", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update"], ["def", "main", "(", ")", ":", "\n", "    ", "setup_default_logging", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "# might as well try to do something useful...", "\n", "args", ".", "pretrained", "=", "args", ".", "pretrained", "or", "not", "args", ".", "checkpoint", "\n", "\n", "# create model", "\n", "model", "=", "create_model", "(", "\n", "args", ".", "model", ",", "\n", "num_classes", "=", "args", ".", "num_classes", ",", "\n", "in_chans", "=", "3", ",", "\n", "pretrained", "=", "args", ".", "pretrained", ",", "\n", "checkpoint_path", "=", "args", ".", "checkpoint", ")", "\n", "\n", "_logger", ".", "info", "(", "'Model %s created, param count: %d'", "%", "\n", "(", "args", ".", "model", ",", "sum", "(", "[", "m", ".", "numel", "(", ")", "for", "m", "in", "model", ".", "parameters", "(", ")", "]", ")", ")", ")", "\n", "\n", "config", "=", "resolve_data_config", "(", "vars", "(", "args", ")", ",", "model", "=", "model", ")", "\n", "model", ",", "test_time_pool", "=", "(", "model", ",", "False", ")", "if", "args", ".", "no_test_pool", "else", "apply_test_time_pool", "(", "model", ",", "config", ")", "\n", "\n", "if", "args", ".", "num_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ",", "device_ids", "=", "list", "(", "range", "(", "args", ".", "num_gpu", ")", ")", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "model", ".", "cuda", "(", ")", "\n", "\n", "", "loader", "=", "create_loader", "(", "\n", "ImageDataset", "(", "args", ".", "data", ")", ",", "\n", "input_size", "=", "config", "[", "'input_size'", "]", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "use_prefetcher", "=", "True", ",", "\n", "interpolation", "=", "config", "[", "'interpolation'", "]", ",", "\n", "mean", "=", "config", "[", "'mean'", "]", ",", "\n", "std", "=", "config", "[", "'std'", "]", ",", "\n", "num_workers", "=", "args", ".", "workers", ",", "\n", "crop_pct", "=", "1.0", "if", "test_time_pool", "else", "config", "[", "'crop_pct'", "]", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "k", "=", "min", "(", "args", ".", "topk", ",", "args", ".", "num_classes", ")", "\n", "batch_time", "=", "AverageMeter", "(", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "topk_ids", "=", "[", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "batch_idx", ",", "(", "input", ",", "_", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "            ", "input", "=", "input", ".", "cuda", "(", ")", "\n", "labels", "=", "model", "(", "input", ")", "\n", "topk", "=", "labels", ".", "topk", "(", "k", ")", "[", "1", "]", "\n", "topk_ids", ".", "append", "(", "topk", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "# measure elapsed time", "\n", "batch_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "\n", "if", "batch_idx", "%", "args", ".", "log_freq", "==", "0", ":", "\n", "                ", "_logger", ".", "info", "(", "'Predict: [{0}/{1}] Time {batch_time.val:.3f} ({batch_time.avg:.3f})'", ".", "format", "(", "\n", "batch_idx", ",", "len", "(", "loader", ")", ",", "batch_time", "=", "batch_time", ")", ")", "\n", "\n", "", "", "", "topk_ids", "=", "np", ".", "concatenate", "(", "topk_ids", ",", "axis", "=", "0", ")", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'./topk_ids.csv'", ")", ",", "'w'", ")", "as", "out_file", ":", "\n", "        ", "filenames", "=", "loader", ".", "dataset", ".", "filenames", "(", "basename", "=", "True", ")", "\n", "for", "filename", ",", "label", "in", "zip", "(", "filenames", ",", "topk_ids", ")", ":", "\n", "            ", "out_file", ".", "write", "(", "'{0},{1}\\n'", ".", "format", "(", "\n", "filename", ",", "','", ".", "join", "(", "[", "str", "(", "v", ")", "for", "v", "in", "label", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.train._parse_args": [[329, 344], ["config_parser.parse_known_args", "parser.parse_args", "yaml.safe_dump", "open", "yaml.safe_load", "parser.set_defaults"], "function", ["None"], ["def", "_parse_args", "(", ")", ":", "\n", "# Do we have a config file to parse?", "\n", "    ", "args_config", ",", "remaining", "=", "config_parser", ".", "parse_known_args", "(", ")", "\n", "if", "args_config", ".", "config", ":", "\n", "        ", "with", "open", "(", "args_config", ".", "config", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "cfg", "=", "yaml", ".", "safe_load", "(", "f", ")", "\n", "parser", ".", "set_defaults", "(", "**", "cfg", ")", "\n", "\n", "# The main arg parser parses the rest of the args, the usual", "\n", "# defaults will have been overridden if config file specified.", "\n", "", "", "args", "=", "parser", ".", "parse_args", "(", "remaining", ")", "\n", "\n", "# Cache the args as a text string to save them in the output dir later", "\n", "args_text", "=", "yaml", ".", "safe_dump", "(", "args", ".", "__dict__", ",", "default_flow_style", "=", "False", ")", "\n", "return", "args", ",", "args_text", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.train.main": [[346, 693], ["timm.utils.setup_default_logging", "train._parse_args", "timm.utils.random_seed", "timm.models.create_model", "timm.data.resolve_data_config", "torch.nn.parallel.DistributedDataParallel.cuda", "timm.optim.create_optimizer_v2", "timm.scheduler.create_scheduler", "timm.data.create_dataset", "timm.data.create_dataset", "timm.data.create_loader", "timm.data.create_loader", "timm.loss.LabelSmoothingCrossEntropy.cuda", "torch.CrossEntropyLoss().cuda", "torch.cuda.set_device", "torch.cuda.set_device", "torch.distributed.init_process_group", "torch.distributed.init_process_group", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_rank", "torch.distributed.get_rank", "_logger.info", "_logger.info", "timm.utils.set_jit_fuser", "hasattr", "torch.nn.parallel.DistributedDataParallel.set_grad_checkpointing", "_logger.info", "vars", "timm.models.convert_splitbn_model", "torch.nn.parallel.DistributedDataParallel.to", "torch.jit.script", "torch.jit.script", "memory_efficient_fusion", "amp.initialize", "timm.utils.ApexScaler", "timm.models.resume_checkpoint", "timm.utils.ModelEmaV2", "lr_scheduler.step", "_logger.info", "dict", "timm.data.AugMixDataset", "timm.loss.JsdCrossEntropy", "timm.utils.get_outdir", "timm.utils.CheckpointSaver", "range", "_logger.info", "int", "wandb.init", "_logger.warning", "max", "convert_syncbn_model", "timm.models.convert_sync_batchnorm", "_logger.info", "timm.optim.optimizer_kwargs", "_logger.info", "timm.utils.NativeScaler", "timm.models.load_checkpoint", "ApexDDP", "torch.nn.parallel.DistributedDataParallel", "timm.data.FastCollateMixup", "timm.data.Mixup", "torch.CrossEntropyLoss", "open", "f.write", "train.train_one_epoch", "train.validate", "_logger.warning", "_logger.info", "_logger.info", "_logger.info", "_logger.info", "timm.loss.BinaryCrossEntropy", "timm.loss.SoftTargetCrossEntropy", "torch.CrossEntropyLoss", "os.path.join", "hasattr", "timm.data.create_loader.sampler.set_epoch", "timm.utils.distribute_bn", "train.validate", "lr_scheduler.step", "timm.utils.update_summary", "utils.CheckpointSaver.save_checkpoint", "timm.models.safe_model_name", "sum", "timm.loss.BinaryCrossEntropy", "timm.loss.LabelSmoothingCrossEntropy", "datetime.datetime.now().strftime", "timm.models.safe_model_name", "str", "_logger.info", "timm.utils.distribute_bn", "os.path.join", "m.numel", "datetime.datetime.now", "torch.nn.parallel.DistributedDataParallel.parameters"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.log.setup_default_logging", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.train._parse_args", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.random.random_seed", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.create_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.config.resolve_data_config", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler_factory.create_scheduler", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset_factory.create_dataset", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset_factory.create_dataset", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader.create_loader", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader.create_loader", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.jit.set_jit_fuser", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.set_grad_checkpointing", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.split_batchnorm.convert_splitbn_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.resume_checkpoint", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.summary.get_outdir", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.norm_act.convert_sync_batchnorm", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.optimizer_kwargs", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_checkpoint", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.train.train_one_epoch", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.train.validate", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.distributed_sampler.RepeatAugSampler.set_epoch", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.distributed.distribute_bn", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.train.validate", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.summary.update_summary", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.checkpoint_saver.CheckpointSaver.save_checkpoint", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.safe_model_name", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.safe_model_name", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.distributed.distribute_bn"], ["", "def", "main", "(", ")", ":", "\n", "    ", "utils", ".", "setup_default_logging", "(", ")", "\n", "args", ",", "args_text", "=", "_parse_args", "(", ")", "\n", "\n", "args", ".", "prefetcher", "=", "not", "args", ".", "no_prefetcher", "\n", "args", ".", "distributed", "=", "False", "\n", "if", "'WORLD_SIZE'", "in", "os", ".", "environ", ":", "\n", "        ", "args", ".", "distributed", "=", "int", "(", "os", ".", "environ", "[", "'WORLD_SIZE'", "]", ")", ">", "1", "\n", "", "args", ".", "device", "=", "'cuda:0'", "\n", "args", ".", "world_size", "=", "1", "\n", "args", ".", "rank", "=", "0", "# global rank", "\n", "if", "args", ".", "distributed", ":", "\n", "        ", "args", ".", "device", "=", "'cuda:%d'", "%", "args", ".", "local_rank", "\n", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ",", "init_method", "=", "'env://'", ")", "\n", "args", ".", "world_size", "=", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "args", ".", "rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", ")", "\n", "_logger", ".", "info", "(", "'Training in distributed mode with multiple processes, 1 GPU per process. Process %d, total %d.'", "\n", "%", "(", "args", ".", "rank", ",", "args", ".", "world_size", ")", ")", "\n", "", "else", ":", "\n", "        ", "_logger", ".", "info", "(", "'Training with a single process on 1 GPUs.'", ")", "\n", "", "assert", "args", ".", "rank", ">=", "0", "\n", "\n", "if", "args", ".", "rank", "==", "0", "and", "args", ".", "log_wandb", ":", "\n", "        ", "if", "has_wandb", ":", "\n", "            ", "wandb", ".", "init", "(", "project", "=", "args", ".", "experiment", ",", "config", "=", "args", ")", "\n", "", "else", ":", "\n", "            ", "_logger", ".", "warning", "(", "\"You've requested to log metrics to wandb but package not found. \"", "\n", "\"Metrics not being logged to wandb, try `pip install wandb`\"", ")", "\n", "\n", "# resolve AMP arguments based on PyTorch / Apex availability", "\n", "", "", "use_amp", "=", "None", "\n", "if", "args", ".", "amp", ":", "\n", "# `--amp` chooses native amp before apex (APEX ver not actively maintained)", "\n", "        ", "if", "has_native_amp", ":", "\n", "            ", "args", ".", "native_amp", "=", "True", "\n", "", "elif", "has_apex", ":", "\n", "            ", "args", ".", "apex_amp", "=", "True", "\n", "", "", "if", "args", ".", "apex_amp", "and", "has_apex", ":", "\n", "        ", "use_amp", "=", "'apex'", "\n", "", "elif", "args", ".", "native_amp", "and", "has_native_amp", ":", "\n", "        ", "use_amp", "=", "'native'", "\n", "", "elif", "args", ".", "apex_amp", "or", "args", ".", "native_amp", ":", "\n", "        ", "_logger", ".", "warning", "(", "\"Neither APEX or native Torch AMP is available, using float32. \"", "\n", "\"Install NVIDA apex or upgrade to PyTorch 1.6\"", ")", "\n", "\n", "", "utils", ".", "random_seed", "(", "args", ".", "seed", ",", "args", ".", "rank", ")", "\n", "\n", "if", "args", ".", "fuser", ":", "\n", "        ", "utils", ".", "set_jit_fuser", "(", "args", ".", "fuser", ")", "\n", "\n", "", "model", "=", "create_model", "(", "\n", "args", ".", "model", ",", "\n", "pretrained", "=", "args", ".", "pretrained", ",", "\n", "num_classes", "=", "args", ".", "num_classes", ",", "\n", "drop_rate", "=", "args", ".", "drop", ",", "\n", "drop_connect_rate", "=", "args", ".", "drop_connect", ",", "# DEPRECATED, use drop_path", "\n", "drop_path_rate", "=", "args", ".", "drop_path", ",", "\n", "drop_block_rate", "=", "args", ".", "drop_block", ",", "\n", "global_pool", "=", "args", ".", "gp", ",", "\n", "bn_momentum", "=", "args", ".", "bn_momentum", ",", "\n", "bn_eps", "=", "args", ".", "bn_eps", ",", "\n", "scriptable", "=", "args", ".", "torchscript", ",", "\n", "checkpoint_path", "=", "args", ".", "initial_checkpoint", ")", "\n", "if", "args", ".", "num_classes", "is", "None", ":", "\n", "        ", "assert", "hasattr", "(", "model", ",", "'num_classes'", ")", ",", "'Model must have `num_classes` attr if not set on cmd line/config.'", "\n", "args", ".", "num_classes", "=", "model", ".", "num_classes", "# FIXME handle model default vs config num_classes more elegantly", "\n", "\n", "", "if", "args", ".", "grad_checkpointing", ":", "\n", "        ", "model", ".", "set_grad_checkpointing", "(", "enable", "=", "True", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "        ", "_logger", ".", "info", "(", "\n", "f'Model {safe_model_name(args.model)} created, param count:{sum([m.numel() for m in model.parameters()])}'", ")", "\n", "\n", "", "data_config", "=", "resolve_data_config", "(", "vars", "(", "args", ")", ",", "model", "=", "model", ",", "verbose", "=", "args", ".", "local_rank", "==", "0", ")", "\n", "\n", "# setup augmentation batch splits for contrastive loss or split bn", "\n", "num_aug_splits", "=", "0", "\n", "if", "args", ".", "aug_splits", ">", "0", ":", "\n", "        ", "assert", "args", ".", "aug_splits", ">", "1", ",", "'A split of 1 makes no sense'", "\n", "num_aug_splits", "=", "args", ".", "aug_splits", "\n", "\n", "# enable split bn (separate bn stats per batch-portion)", "\n", "", "if", "args", ".", "split_bn", ":", "\n", "        ", "assert", "num_aug_splits", ">", "1", "or", "args", ".", "resplit", "\n", "model", "=", "convert_splitbn_model", "(", "model", ",", "max", "(", "num_aug_splits", ",", "2", ")", ")", "\n", "\n", "# move model to GPU, enable channels last layout if set", "\n", "", "model", ".", "cuda", "(", ")", "\n", "if", "args", ".", "channels_last", ":", "\n", "        ", "model", "=", "model", ".", "to", "(", "memory_format", "=", "torch", ".", "channels_last", ")", "\n", "\n", "# setup synchronized BatchNorm for distributed training", "\n", "", "if", "args", ".", "distributed", "and", "args", ".", "sync_bn", ":", "\n", "        ", "args", ".", "dist_bn", "=", "''", "# disable dist_bn when sync BN active", "\n", "assert", "not", "args", ".", "split_bn", "\n", "if", "has_apex", "and", "use_amp", "==", "'apex'", ":", "\n", "# Apex SyncBN used with Apex AMP", "\n", "# WARNING this won't currently work with models using BatchNormAct2d", "\n", "            ", "model", "=", "convert_syncbn_model", "(", "model", ")", "\n", "", "else", ":", "\n", "            ", "model", "=", "convert_sync_batchnorm", "(", "model", ")", "\n", "", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "            ", "_logger", ".", "info", "(", "\n", "'Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using '", "\n", "'zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.'", ")", "\n", "\n", "", "", "if", "args", ".", "torchscript", ":", "\n", "        ", "assert", "not", "use_amp", "==", "'apex'", ",", "'Cannot use APEX AMP with torchscripted model'", "\n", "assert", "not", "args", ".", "sync_bn", ",", "'Cannot use SyncBatchNorm with torchscripted model'", "\n", "model", "=", "torch", ".", "jit", ".", "script", "(", "model", ")", "\n", "", "if", "args", ".", "aot_autograd", ":", "\n", "        ", "assert", "has_functorch", ",", "\"functorch is needed for --aot-autograd\"", "\n", "model", "=", "memory_efficient_fusion", "(", "model", ")", "\n", "\n", "", "optimizer", "=", "create_optimizer_v2", "(", "model", ",", "**", "optimizer_kwargs", "(", "cfg", "=", "args", ")", ")", "\n", "\n", "# setup automatic mixed-precision (AMP) loss scaling and op casting", "\n", "amp_autocast", "=", "suppress", "# do nothing", "\n", "loss_scaler", "=", "None", "\n", "if", "use_amp", "==", "'apex'", ":", "\n", "        ", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "optimizer", ",", "opt_level", "=", "'O1'", ")", "\n", "loss_scaler", "=", "ApexScaler", "(", ")", "\n", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "            ", "_logger", ".", "info", "(", "'Using NVIDIA APEX AMP. Training in mixed precision.'", ")", "\n", "", "", "elif", "use_amp", "==", "'native'", ":", "\n", "        ", "amp_autocast", "=", "torch", ".", "cuda", ".", "amp", ".", "autocast", "\n", "loss_scaler", "=", "NativeScaler", "(", ")", "\n", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "            ", "_logger", ".", "info", "(", "'Using native Torch AMP. Training in mixed precision.'", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "            ", "_logger", ".", "info", "(", "'AMP not enabled. Training in float32.'", ")", "\n", "\n", "\n", "# optionally resume from a checkpoint", "\n", "", "", "resume_epoch", "=", "None", "\n", "if", "args", ".", "resume", ":", "\n", "        ", "resume_epoch", "=", "resume_checkpoint", "(", "\n", "model", ",", "args", ".", "resume", ",", "\n", "optimizer", "=", "None", "if", "args", ".", "no_resume_opt", "else", "optimizer", ",", "\n", "loss_scaler", "=", "None", "if", "args", ".", "no_resume_opt", "else", "loss_scaler", ",", "\n", "log_info", "=", "args", ".", "local_rank", "==", "0", ")", "\n", "\n", "# setup exponential moving average of model weights, SWA could be used here too", "\n", "", "model_ema", "=", "None", "\n", "if", "args", ".", "model_ema", ":", "\n", "# Important to create EMA model after cuda(), DP wrapper, and AMP but before DDP wrapper", "\n", "        ", "model_ema", "=", "utils", ".", "ModelEmaV2", "(", "\n", "model", ",", "decay", "=", "args", ".", "model_ema_decay", ",", "device", "=", "'cpu'", "if", "args", ".", "model_ema_force_cpu", "else", "None", ")", "\n", "if", "args", ".", "resume", ":", "\n", "            ", "load_checkpoint", "(", "model_ema", ".", "module", ",", "args", ".", "resume", ",", "use_ema", "=", "True", ")", "\n", "\n", "# setup distributed training", "\n", "", "", "if", "args", ".", "distributed", ":", "\n", "        ", "if", "has_apex", "and", "use_amp", "==", "'apex'", ":", "\n", "# Apex DDP preferred unless native amp is activated", "\n", "            ", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "                ", "_logger", ".", "info", "(", "\"Using NVIDIA APEX DistributedDataParallel.\"", ")", "\n", "", "model", "=", "ApexDDP", "(", "model", ",", "delay_allreduce", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "                ", "_logger", ".", "info", "(", "\"Using native Torch DistributedDataParallel.\"", ")", "\n", "", "model", "=", "NativeDDP", "(", "model", ",", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "broadcast_buffers", "=", "not", "args", ".", "no_ddp_bb", ")", "\n", "# NOTE: EMA model does not need to be wrapped by DDP", "\n", "\n", "# setup learning rate schedule and starting epoch", "\n", "", "", "lr_scheduler", ",", "num_epochs", "=", "create_scheduler", "(", "args", ",", "optimizer", ")", "\n", "start_epoch", "=", "0", "\n", "if", "args", ".", "start_epoch", "is", "not", "None", ":", "\n", "# a specified start_epoch will always override the resume epoch", "\n", "        ", "start_epoch", "=", "args", ".", "start_epoch", "\n", "", "elif", "resume_epoch", "is", "not", "None", ":", "\n", "        ", "start_epoch", "=", "resume_epoch", "\n", "", "if", "lr_scheduler", "is", "not", "None", "and", "start_epoch", ">", "0", ":", "\n", "        ", "lr_scheduler", ".", "step", "(", "start_epoch", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "        ", "_logger", ".", "info", "(", "'Scheduled epochs: {}'", ".", "format", "(", "num_epochs", ")", ")", "\n", "\n", "# create the train and eval datasets", "\n", "", "dataset_train", "=", "create_dataset", "(", "\n", "args", ".", "dataset", ",", "root", "=", "args", ".", "data_dir", ",", "split", "=", "args", ".", "train_split", ",", "is_training", "=", "True", ",", "\n", "class_map", "=", "args", ".", "class_map", ",", "\n", "download", "=", "args", ".", "dataset_download", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "repeats", "=", "args", ".", "epoch_repeats", ")", "\n", "dataset_eval", "=", "create_dataset", "(", "\n", "args", ".", "dataset", ",", "root", "=", "args", ".", "data_dir", ",", "split", "=", "args", ".", "val_split", ",", "is_training", "=", "False", ",", "\n", "class_map", "=", "args", ".", "class_map", ",", "\n", "download", "=", "args", ".", "dataset_download", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "\n", "# setup mixup / cutmix", "\n", "collate_fn", "=", "None", "\n", "mixup_fn", "=", "None", "\n", "mixup_active", "=", "args", ".", "mixup", ">", "0", "or", "args", ".", "cutmix", ">", "0.", "or", "args", ".", "cutmix_minmax", "is", "not", "None", "\n", "if", "mixup_active", ":", "\n", "        ", "mixup_args", "=", "dict", "(", "\n", "mixup_alpha", "=", "args", ".", "mixup", ",", "cutmix_alpha", "=", "args", ".", "cutmix", ",", "cutmix_minmax", "=", "args", ".", "cutmix_minmax", ",", "\n", "prob", "=", "args", ".", "mixup_prob", ",", "switch_prob", "=", "args", ".", "mixup_switch_prob", ",", "mode", "=", "args", ".", "mixup_mode", ",", "\n", "label_smoothing", "=", "args", ".", "smoothing", ",", "num_classes", "=", "args", ".", "num_classes", ")", "\n", "if", "args", ".", "prefetcher", ":", "\n", "            ", "assert", "not", "num_aug_splits", "# collate conflict (need to support deinterleaving in collate mixup)", "\n", "collate_fn", "=", "FastCollateMixup", "(", "**", "mixup_args", ")", "\n", "", "else", ":", "\n", "            ", "mixup_fn", "=", "Mixup", "(", "**", "mixup_args", ")", "\n", "\n", "# wrap dataset in AugMix helper", "\n", "", "", "if", "num_aug_splits", ">", "1", ":", "\n", "        ", "dataset_train", "=", "AugMixDataset", "(", "dataset_train", ",", "num_splits", "=", "num_aug_splits", ")", "\n", "\n", "# create data loaders w/ augmentation pipeiine", "\n", "", "train_interpolation", "=", "args", ".", "train_interpolation", "\n", "if", "args", ".", "no_aug", "or", "not", "train_interpolation", ":", "\n", "        ", "train_interpolation", "=", "data_config", "[", "'interpolation'", "]", "\n", "", "loader_train", "=", "create_loader", "(", "\n", "dataset_train", ",", "\n", "input_size", "=", "data_config", "[", "'input_size'", "]", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "is_training", "=", "True", ",", "\n", "use_prefetcher", "=", "args", ".", "prefetcher", ",", "\n", "no_aug", "=", "args", ".", "no_aug", ",", "\n", "re_prob", "=", "args", ".", "reprob", ",", "\n", "re_mode", "=", "args", ".", "remode", ",", "\n", "re_count", "=", "args", ".", "recount", ",", "\n", "re_split", "=", "args", ".", "resplit", ",", "\n", "scale", "=", "args", ".", "scale", ",", "\n", "ratio", "=", "args", ".", "ratio", ",", "\n", "hflip", "=", "args", ".", "hflip", ",", "\n", "vflip", "=", "args", ".", "vflip", ",", "\n", "color_jitter", "=", "args", ".", "color_jitter", ",", "\n", "auto_augment", "=", "args", ".", "aa", ",", "\n", "num_aug_repeats", "=", "args", ".", "aug_repeats", ",", "\n", "num_aug_splits", "=", "num_aug_splits", ",", "\n", "interpolation", "=", "train_interpolation", ",", "\n", "mean", "=", "data_config", "[", "'mean'", "]", ",", "\n", "std", "=", "data_config", "[", "'std'", "]", ",", "\n", "num_workers", "=", "args", ".", "workers", ",", "\n", "distributed", "=", "args", ".", "distributed", ",", "\n", "collate_fn", "=", "collate_fn", ",", "\n", "pin_memory", "=", "args", ".", "pin_mem", ",", "\n", "use_multi_epochs_loader", "=", "args", ".", "use_multi_epochs_loader", ",", "\n", "worker_seeding", "=", "args", ".", "worker_seeding", ",", "\n", ")", "\n", "\n", "loader_eval", "=", "create_loader", "(", "\n", "dataset_eval", ",", "\n", "input_size", "=", "data_config", "[", "'input_size'", "]", ",", "\n", "batch_size", "=", "args", ".", "validation_batch_size", "or", "args", ".", "batch_size", ",", "\n", "is_training", "=", "False", ",", "\n", "use_prefetcher", "=", "args", ".", "prefetcher", ",", "\n", "interpolation", "=", "data_config", "[", "'interpolation'", "]", ",", "\n", "mean", "=", "data_config", "[", "'mean'", "]", ",", "\n", "std", "=", "data_config", "[", "'std'", "]", ",", "\n", "num_workers", "=", "args", ".", "workers", ",", "\n", "distributed", "=", "args", ".", "distributed", ",", "\n", "crop_pct", "=", "data_config", "[", "'crop_pct'", "]", ",", "\n", "pin_memory", "=", "args", ".", "pin_mem", ",", "\n", ")", "\n", "\n", "# setup loss function", "\n", "if", "args", ".", "jsd_loss", ":", "\n", "        ", "assert", "num_aug_splits", ">", "1", "# JSD only valid with aug splits set", "\n", "train_loss_fn", "=", "JsdCrossEntropy", "(", "num_splits", "=", "num_aug_splits", ",", "smoothing", "=", "args", ".", "smoothing", ")", "\n", "", "elif", "mixup_active", ":", "\n", "# smoothing is handled with mixup target transform which outputs sparse, soft targets", "\n", "        ", "if", "args", ".", "bce_loss", ":", "\n", "            ", "train_loss_fn", "=", "BinaryCrossEntropy", "(", "target_threshold", "=", "args", ".", "bce_target_thresh", ")", "\n", "", "else", ":", "\n", "            ", "train_loss_fn", "=", "SoftTargetCrossEntropy", "(", ")", "\n", "", "", "elif", "args", ".", "smoothing", ":", "\n", "        ", "if", "args", ".", "bce_loss", ":", "\n", "            ", "train_loss_fn", "=", "BinaryCrossEntropy", "(", "smoothing", "=", "args", ".", "smoothing", ",", "target_threshold", "=", "args", ".", "bce_target_thresh", ")", "\n", "", "else", ":", "\n", "            ", "train_loss_fn", "=", "LabelSmoothingCrossEntropy", "(", "smoothing", "=", "args", ".", "smoothing", ")", "\n", "", "", "else", ":", "\n", "        ", "train_loss_fn", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "", "train_loss_fn", "=", "train_loss_fn", ".", "cuda", "(", ")", "\n", "validate_loss_fn", "=", "nn", ".", "CrossEntropyLoss", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "# setup checkpoint saver and eval metric tracking", "\n", "eval_metric", "=", "args", ".", "eval_metric", "\n", "best_metric", "=", "None", "\n", "best_epoch", "=", "None", "\n", "saver", "=", "None", "\n", "output_dir", "=", "None", "\n", "if", "args", ".", "rank", "==", "0", ":", "\n", "        ", "if", "args", ".", "experiment", ":", "\n", "            ", "exp_name", "=", "args", ".", "experiment", "\n", "", "else", ":", "\n", "            ", "exp_name", "=", "'-'", ".", "join", "(", "[", "\n", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y%m%d-%H%M%S\"", ")", ",", "\n", "safe_model_name", "(", "args", ".", "model", ")", ",", "\n", "str", "(", "data_config", "[", "'input_size'", "]", "[", "-", "1", "]", ")", "\n", "]", ")", "\n", "", "output_dir", "=", "utils", ".", "get_outdir", "(", "args", ".", "output", "if", "args", ".", "output", "else", "'./output/train'", ",", "exp_name", ")", "\n", "decreasing", "=", "True", "if", "eval_metric", "==", "'loss'", "else", "False", "\n", "saver", "=", "utils", ".", "CheckpointSaver", "(", "\n", "model", "=", "model", ",", "optimizer", "=", "optimizer", ",", "args", "=", "args", ",", "model_ema", "=", "model_ema", ",", "amp_scaler", "=", "loss_scaler", ",", "\n", "checkpoint_dir", "=", "output_dir", ",", "recovery_dir", "=", "output_dir", ",", "decreasing", "=", "decreasing", ",", "max_history", "=", "args", ".", "checkpoint_hist", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'args.yaml'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "args_text", ")", "\n", "\n", "", "", "try", ":", "\n", "        ", "for", "epoch", "in", "range", "(", "start_epoch", ",", "num_epochs", ")", ":", "\n", "            ", "if", "args", ".", "distributed", "and", "hasattr", "(", "loader_train", ".", "sampler", ",", "'set_epoch'", ")", ":", "\n", "                ", "loader_train", ".", "sampler", ".", "set_epoch", "(", "epoch", ")", "\n", "\n", "", "train_metrics", "=", "train_one_epoch", "(", "\n", "epoch", ",", "model", ",", "loader_train", ",", "optimizer", ",", "train_loss_fn", ",", "args", ",", "\n", "lr_scheduler", "=", "lr_scheduler", ",", "saver", "=", "saver", ",", "output_dir", "=", "output_dir", ",", "\n", "amp_autocast", "=", "amp_autocast", ",", "loss_scaler", "=", "loss_scaler", ",", "model_ema", "=", "model_ema", ",", "mixup_fn", "=", "mixup_fn", ")", "\n", "\n", "if", "args", ".", "distributed", "and", "args", ".", "dist_bn", "in", "(", "'broadcast'", ",", "'reduce'", ")", ":", "\n", "                ", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "                    ", "_logger", ".", "info", "(", "\"Distributing BatchNorm running means and vars\"", ")", "\n", "", "utils", ".", "distribute_bn", "(", "model", ",", "args", ".", "world_size", ",", "args", ".", "dist_bn", "==", "'reduce'", ")", "\n", "\n", "", "eval_metrics", "=", "validate", "(", "model", ",", "loader_eval", ",", "validate_loss_fn", ",", "args", ",", "amp_autocast", "=", "amp_autocast", ")", "\n", "\n", "if", "model_ema", "is", "not", "None", "and", "not", "args", ".", "model_ema_force_cpu", ":", "\n", "                ", "if", "args", ".", "distributed", "and", "args", ".", "dist_bn", "in", "(", "'broadcast'", ",", "'reduce'", ")", ":", "\n", "                    ", "utils", ".", "distribute_bn", "(", "model_ema", ",", "args", ".", "world_size", ",", "args", ".", "dist_bn", "==", "'reduce'", ")", "\n", "", "ema_eval_metrics", "=", "validate", "(", "\n", "model_ema", ".", "module", ",", "loader_eval", ",", "validate_loss_fn", ",", "args", ",", "amp_autocast", "=", "amp_autocast", ",", "log_suffix", "=", "' (EMA)'", ")", "\n", "eval_metrics", "=", "ema_eval_metrics", "\n", "\n", "", "if", "lr_scheduler", "is", "not", "None", ":", "\n", "# step LR for next epoch", "\n", "                ", "lr_scheduler", ".", "step", "(", "epoch", "+", "1", ",", "eval_metrics", "[", "eval_metric", "]", ")", "\n", "\n", "", "if", "output_dir", "is", "not", "None", ":", "\n", "                ", "utils", ".", "update_summary", "(", "\n", "epoch", ",", "train_metrics", ",", "eval_metrics", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'summary.csv'", ")", ",", "\n", "write_header", "=", "best_metric", "is", "None", ",", "log_wandb", "=", "args", ".", "log_wandb", "and", "has_wandb", ")", "\n", "\n", "", "if", "saver", "is", "not", "None", ":", "\n", "# save proper checkpoint with eval metric", "\n", "                ", "save_metric", "=", "eval_metrics", "[", "eval_metric", "]", "\n", "best_metric", ",", "best_epoch", "=", "saver", ".", "save_checkpoint", "(", "epoch", ",", "metric", "=", "save_metric", ")", "\n", "\n", "", "", "", "except", "KeyboardInterrupt", ":", "\n", "        ", "pass", "\n", "", "if", "best_metric", "is", "not", "None", ":", "\n", "        ", "_logger", ".", "info", "(", "'*** Best metric: {0} (epoch {1})'", ".", "format", "(", "best_metric", ",", "best_epoch", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.train.train_one_epoch": [[695, 801], ["timm.utils.AverageMeter", "timm.utils.AverageMeter", "timm.utils.AverageMeter", "model.train", "time.time", "enumerate", "hasattr", "collections.OrderedDict", "hasattr", "len", "len", "utils.AverageMeter.update", "optimizer.zero_grad", "torch.cuda.synchronize", "torch.cuda.synchronize", "utils.AverageMeter.update", "time.time", "optimizer.sync_lookahead", "input.contiguous.contiguous", "amp_autocast", "model", "loss_fn", "utils.AverageMeter.update", "loss_scaler", "loss_fn.backward", "optimizer.step", "model_ema.update", "saver.save_recovery", "lr_scheduler.step_update", "time.time", "input.contiguous.cuda", "target.cuda", "mixup_fn", "loss_fn.item", "input.contiguous.size", "timm.utils.dispatch_clip_grad", "time.time", "sum", "len", "timm.utils.reduce_tensor", "utils.AverageMeter.update", "_logger.info", "timm.models.model_parameters", "timm.models.model_parameters", "utils.reduce_tensor.item", "input.contiguous.size", "torchvision.utils.save_image", "len", "os.path.join", "input.contiguous.size", "input.contiguous.size"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.AttentionSubsample.train", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.sync_lookahead", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardMishJitAutoFn.backward", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.checkpoint_saver.CheckpointSaver.save_recovery", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler.step_update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.clip_grad.dispatch_clip_grad", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.distributed.reduce_tensor", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.model_parameters", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.model_parameters"], ["", "", "def", "train_one_epoch", "(", "\n", "epoch", ",", "model", ",", "loader", ",", "optimizer", ",", "loss_fn", ",", "args", ",", "\n", "lr_scheduler", "=", "None", ",", "saver", "=", "None", ",", "output_dir", "=", "None", ",", "amp_autocast", "=", "suppress", ",", "\n", "loss_scaler", "=", "None", ",", "model_ema", "=", "None", ",", "mixup_fn", "=", "None", ")", ":", "\n", "\n", "    ", "if", "args", ".", "mixup_off_epoch", "and", "epoch", ">=", "args", ".", "mixup_off_epoch", ":", "\n", "        ", "if", "args", ".", "prefetcher", "and", "loader", ".", "mixup_enabled", ":", "\n", "            ", "loader", ".", "mixup_enabled", "=", "False", "\n", "", "elif", "mixup_fn", "is", "not", "None", ":", "\n", "            ", "mixup_fn", ".", "mixup_enabled", "=", "False", "\n", "\n", "", "", "second_order", "=", "hasattr", "(", "optimizer", ",", "'is_second_order'", ")", "and", "optimizer", ".", "is_second_order", "\n", "batch_time_m", "=", "utils", ".", "AverageMeter", "(", ")", "\n", "data_time_m", "=", "utils", ".", "AverageMeter", "(", ")", "\n", "losses_m", "=", "utils", ".", "AverageMeter", "(", ")", "\n", "\n", "model", ".", "train", "(", ")", "\n", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "last_idx", "=", "len", "(", "loader", ")", "-", "1", "\n", "num_updates", "=", "epoch", "*", "len", "(", "loader", ")", "\n", "for", "batch_idx", ",", "(", "input", ",", "target", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "        ", "last_batch", "=", "batch_idx", "==", "last_idx", "\n", "data_time_m", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "if", "not", "args", ".", "prefetcher", ":", "\n", "            ", "input", ",", "target", "=", "input", ".", "cuda", "(", ")", ",", "target", ".", "cuda", "(", ")", "\n", "if", "mixup_fn", "is", "not", "None", ":", "\n", "                ", "input", ",", "target", "=", "mixup_fn", "(", "input", ",", "target", ")", "\n", "", "", "if", "args", ".", "channels_last", ":", "\n", "            ", "input", "=", "input", ".", "contiguous", "(", "memory_format", "=", "torch", ".", "channels_last", ")", "\n", "\n", "", "with", "amp_autocast", "(", ")", ":", "\n", "            ", "output", "=", "model", "(", "input", ")", "\n", "loss", "=", "loss_fn", "(", "output", ",", "target", ")", "\n", "\n", "", "if", "not", "args", ".", "distributed", ":", "\n", "            ", "losses_m", ".", "update", "(", "loss", ".", "item", "(", ")", ",", "input", ".", "size", "(", "0", ")", ")", "\n", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "if", "loss_scaler", "is", "not", "None", ":", "\n", "            ", "loss_scaler", "(", "\n", "loss", ",", "optimizer", ",", "\n", "clip_grad", "=", "args", ".", "clip_grad", ",", "clip_mode", "=", "args", ".", "clip_mode", ",", "\n", "parameters", "=", "model_parameters", "(", "model", ",", "exclude_head", "=", "'agc'", "in", "args", ".", "clip_mode", ")", ",", "\n", "create_graph", "=", "second_order", ")", "\n", "", "else", ":", "\n", "            ", "loss", ".", "backward", "(", "create_graph", "=", "second_order", ")", "\n", "if", "args", ".", "clip_grad", "is", "not", "None", ":", "\n", "                ", "utils", ".", "dispatch_clip_grad", "(", "\n", "model_parameters", "(", "model", ",", "exclude_head", "=", "'agc'", "in", "args", ".", "clip_mode", ")", ",", "\n", "value", "=", "args", ".", "clip_grad", ",", "mode", "=", "args", ".", "clip_mode", ")", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "if", "model_ema", "is", "not", "None", ":", "\n", "            ", "model_ema", ".", "update", "(", "model", ")", "\n", "\n", "", "torch", ".", "cuda", ".", "synchronize", "(", ")", "\n", "num_updates", "+=", "1", "\n", "batch_time_m", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "if", "last_batch", "or", "batch_idx", "%", "args", ".", "log_interval", "==", "0", ":", "\n", "            ", "lrl", "=", "[", "param_group", "[", "'lr'", "]", "for", "param_group", "in", "optimizer", ".", "param_groups", "]", "\n", "lr", "=", "sum", "(", "lrl", ")", "/", "len", "(", "lrl", ")", "\n", "\n", "if", "args", ".", "distributed", ":", "\n", "                ", "reduced_loss", "=", "utils", ".", "reduce_tensor", "(", "loss", ".", "data", ",", "args", ".", "world_size", ")", "\n", "losses_m", ".", "update", "(", "reduced_loss", ".", "item", "(", ")", ",", "input", ".", "size", "(", "0", ")", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "                ", "_logger", ".", "info", "(", "\n", "'Train: {} [{:>4d}/{} ({:>3.0f}%)]  '", "\n", "'Loss: {loss.val:#.4g} ({loss.avg:#.3g})  '", "\n", "'Time: {batch_time.val:.3f}s, {rate:>7.2f}/s  '", "\n", "'({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '", "\n", "'LR: {lr:.3e}  '", "\n", "'Data: {data_time.val:.3f} ({data_time.avg:.3f})'", ".", "format", "(", "\n", "epoch", ",", "\n", "batch_idx", ",", "len", "(", "loader", ")", ",", "\n", "100.", "*", "batch_idx", "/", "last_idx", ",", "\n", "loss", "=", "losses_m", ",", "\n", "batch_time", "=", "batch_time_m", ",", "\n", "rate", "=", "input", ".", "size", "(", "0", ")", "*", "args", ".", "world_size", "/", "batch_time_m", ".", "val", ",", "\n", "rate_avg", "=", "input", ".", "size", "(", "0", ")", "*", "args", ".", "world_size", "/", "batch_time_m", ".", "avg", ",", "\n", "lr", "=", "lr", ",", "\n", "data_time", "=", "data_time_m", ")", ")", "\n", "\n", "if", "args", ".", "save_images", "and", "output_dir", ":", "\n", "                    ", "torchvision", ".", "utils", ".", "save_image", "(", "\n", "input", ",", "\n", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'train-batch-%d.jpg'", "%", "batch_idx", ")", ",", "\n", "padding", "=", "0", ",", "\n", "normalize", "=", "True", ")", "\n", "\n", "", "", "", "if", "saver", "is", "not", "None", "and", "args", ".", "recovery_interval", "and", "(", "\n", "last_batch", "or", "(", "batch_idx", "+", "1", ")", "%", "args", ".", "recovery_interval", "==", "0", ")", ":", "\n", "            ", "saver", ".", "save_recovery", "(", "epoch", ",", "batch_idx", "=", "batch_idx", ")", "\n", "\n", "", "if", "lr_scheduler", "is", "not", "None", ":", "\n", "            ", "lr_scheduler", ".", "step_update", "(", "num_updates", "=", "num_updates", ",", "metric", "=", "losses_m", ".", "avg", ")", "\n", "\n", "", "end", "=", "time", ".", "time", "(", ")", "\n", "# end for", "\n", "\n", "", "if", "hasattr", "(", "optimizer", ",", "'sync_lookahead'", ")", ":", "\n", "        ", "optimizer", ".", "sync_lookahead", "(", ")", "\n", "\n", "", "return", "OrderedDict", "(", "[", "(", "'loss'", ",", "losses_m", ".", "avg", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.train.validate": [[803, 865], ["timm.utils.AverageMeter", "timm.utils.AverageMeter", "timm.utils.AverageMeter", "timm.utils.AverageMeter", "model.eval", "time.time", "collections.OrderedDict", "len", "torch.no_grad", "torch.no_grad", "enumerate", "isinstance", "loss_fn", "timm.utils.accuracy", "torch.cuda.synchronize", "torch.cuda.synchronize", "utils.AverageMeter.update", "utils.AverageMeter.update", "utils.AverageMeter.update", "utils.AverageMeter.update", "time.time", "input.contiguous.cuda", "target.cuda.cuda", "input.contiguous.contiguous", "amp_autocast", "model", "output.unfold().mean.unfold().mean", "timm.utils.reduce_tensor", "timm.utils.reduce_tensor", "timm.utils.reduce_tensor", "utils.reduce_tensor.item", "input.contiguous.size", "utils.reduce_tensor.item", "output.unfold().mean.size", "utils.reduce_tensor.item", "output.unfold().mean.size", "_logger.info", "time.time", "output.unfold().mean.unfold", "target.cuda.size"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.accuracy", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.distributed.reduce_tensor", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.distributed.reduce_tensor", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.distributed.reduce_tensor"], ["", "def", "validate", "(", "model", ",", "loader", ",", "loss_fn", ",", "args", ",", "amp_autocast", "=", "suppress", ",", "log_suffix", "=", "''", ")", ":", "\n", "    ", "batch_time_m", "=", "utils", ".", "AverageMeter", "(", ")", "\n", "losses_m", "=", "utils", ".", "AverageMeter", "(", ")", "\n", "top1_m", "=", "utils", ".", "AverageMeter", "(", ")", "\n", "top5_m", "=", "utils", ".", "AverageMeter", "(", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "last_idx", "=", "len", "(", "loader", ")", "-", "1", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "batch_idx", ",", "(", "input", ",", "target", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "            ", "last_batch", "=", "batch_idx", "==", "last_idx", "\n", "if", "not", "args", ".", "prefetcher", ":", "\n", "                ", "input", "=", "input", ".", "cuda", "(", ")", "\n", "target", "=", "target", ".", "cuda", "(", ")", "\n", "", "if", "args", ".", "channels_last", ":", "\n", "                ", "input", "=", "input", ".", "contiguous", "(", "memory_format", "=", "torch", ".", "channels_last", ")", "\n", "\n", "", "with", "amp_autocast", "(", ")", ":", "\n", "                ", "output", "=", "model", "(", "input", ")", "\n", "", "if", "isinstance", "(", "output", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "                ", "output", "=", "output", "[", "0", "]", "\n", "\n", "# augmentation reduction", "\n", "", "reduce_factor", "=", "args", ".", "tta", "\n", "if", "reduce_factor", ">", "1", ":", "\n", "                ", "output", "=", "output", ".", "unfold", "(", "0", ",", "reduce_factor", ",", "reduce_factor", ")", ".", "mean", "(", "dim", "=", "2", ")", "\n", "target", "=", "target", "[", "0", ":", "target", ".", "size", "(", "0", ")", ":", "reduce_factor", "]", "\n", "\n", "", "loss", "=", "loss_fn", "(", "output", ",", "target", ")", "\n", "acc1", ",", "acc5", "=", "utils", ".", "accuracy", "(", "output", ",", "target", ",", "topk", "=", "(", "1", ",", "5", ")", ")", "\n", "\n", "if", "args", ".", "distributed", ":", "\n", "                ", "reduced_loss", "=", "utils", ".", "reduce_tensor", "(", "loss", ".", "data", ",", "args", ".", "world_size", ")", "\n", "acc1", "=", "utils", ".", "reduce_tensor", "(", "acc1", ",", "args", ".", "world_size", ")", "\n", "acc5", "=", "utils", ".", "reduce_tensor", "(", "acc5", ",", "args", ".", "world_size", ")", "\n", "", "else", ":", "\n", "                ", "reduced_loss", "=", "loss", ".", "data", "\n", "\n", "", "torch", ".", "cuda", ".", "synchronize", "(", ")", "\n", "\n", "losses_m", ".", "update", "(", "reduced_loss", ".", "item", "(", ")", ",", "input", ".", "size", "(", "0", ")", ")", "\n", "top1_m", ".", "update", "(", "acc1", ".", "item", "(", ")", ",", "output", ".", "size", "(", "0", ")", ")", "\n", "top5_m", ".", "update", "(", "acc5", ".", "item", "(", ")", ",", "output", ".", "size", "(", "0", ")", ")", "\n", "\n", "batch_time_m", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "if", "args", ".", "local_rank", "==", "0", "and", "(", "last_batch", "or", "batch_idx", "%", "args", ".", "log_interval", "==", "0", ")", ":", "\n", "                ", "log_name", "=", "'Test'", "+", "log_suffix", "\n", "_logger", ".", "info", "(", "\n", "'{0}: [{1:>4d}/{2}]  '", "\n", "'Time: {batch_time.val:.3f} ({batch_time.avg:.3f})  '", "\n", "'Loss: {loss.val:>7.4f} ({loss.avg:>6.4f})  '", "\n", "'Acc@1: {top1.val:>7.4f} ({top1.avg:>7.4f})  '", "\n", "'Acc@5: {top5.val:>7.4f} ({top5.avg:>7.4f})'", ".", "format", "(", "\n", "log_name", ",", "batch_idx", ",", "last_idx", ",", "batch_time", "=", "batch_time_m", ",", "\n", "loss", "=", "losses_m", ",", "top1", "=", "top1_m", ",", "top5", "=", "top5_m", ")", ")", "\n", "\n", "", "", "", "metrics", "=", "OrderedDict", "(", "[", "(", "'loss'", ",", "losses_m", ".", "avg", ")", ",", "(", "'top1'", ",", "top1_m", ".", "avg", ")", ",", "(", "'top5'", ",", "top5_m", ".", "avg", ")", "]", ")", "\n", "\n", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.clean_checkpoint.main": [[31, 39], ["parser.parse_args", "os.path.exists", "clean_checkpoint.clean_checkpoint", "print", "exit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.clean_checkpoint.clean_checkpoint"], ["def", "main", "(", ")", ":", "\n", "    ", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "output", ")", ":", "\n", "        ", "print", "(", "\"Error: Output filename ({}) already exists.\"", ".", "format", "(", "args", ".", "output", ")", ")", "\n", "exit", "(", "1", ")", "\n", "\n", "", "clean_checkpoint", "(", "args", ".", "checkpoint", ",", "args", ".", "output", ",", "not", "args", ".", "no_use_ema", ",", "args", ".", "clean_aux_bn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.clean_checkpoint.clean_checkpoint": [[41, 77], ["os.path.isfile", "print", "timm.models.helpers.load_state_dict", "timm.models.helpers.load_state_dict.items", "print", "shutil.move", "print", "print", "torch.save", "open", "hashlib.sha256().hexdigest", "os.path.split", "os.path.join", "k.startswith", "torch.save", "os.path.splitext", "os.path.splitext", "hashlib.sha256", "f.read"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_state_dict"], ["", "def", "clean_checkpoint", "(", "checkpoint", ",", "output", "=", "''", ",", "use_ema", "=", "True", ",", "clean_aux_bn", "=", "False", ")", ":", "\n", "# Load an existing checkpoint to CPU, strip everything but the state_dict and re-save", "\n", "    ", "if", "checkpoint", "and", "os", ".", "path", ".", "isfile", "(", "checkpoint", ")", ":", "\n", "        ", "print", "(", "\"=> Loading checkpoint '{}'\"", ".", "format", "(", "checkpoint", ")", ")", "\n", "state_dict", "=", "load_state_dict", "(", "checkpoint", ",", "use_ema", "=", "use_ema", ")", "\n", "new_state_dict", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "clean_aux_bn", "and", "'aux_bn'", "in", "k", ":", "\n", "# If all aux_bn keys are removed, the SplitBN layers will end up as normal and", "\n", "# load with the unmodified model using BatchNorm2d.", "\n", "                ", "continue", "\n", "", "name", "=", "k", "[", "7", ":", "]", "if", "k", ".", "startswith", "(", "'module.'", ")", "else", "k", "\n", "new_state_dict", "[", "name", "]", "=", "v", "\n", "", "print", "(", "\"=> Loaded state_dict from '{}'\"", ".", "format", "(", "checkpoint", ")", ")", "\n", "\n", "try", ":", "\n", "            ", "torch", ".", "save", "(", "new_state_dict", ",", "_TEMP_NAME", ",", "_use_new_zipfile_serialization", "=", "False", ")", "\n", "", "except", ":", "\n", "            ", "torch", ".", "save", "(", "new_state_dict", ",", "_TEMP_NAME", ")", "\n", "\n", "", "with", "open", "(", "_TEMP_NAME", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "sha_hash", "=", "hashlib", ".", "sha256", "(", "f", ".", "read", "(", ")", ")", ".", "hexdigest", "(", ")", "\n", "\n", "", "if", "output", ":", "\n", "            ", "checkpoint_root", ",", "checkpoint_base", "=", "os", ".", "path", ".", "split", "(", "output", ")", "\n", "checkpoint_base", "=", "os", ".", "path", ".", "splitext", "(", "checkpoint_base", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "checkpoint_root", "=", "''", "\n", "checkpoint_base", "=", "os", ".", "path", ".", "splitext", "(", "checkpoint", ")", "[", "0", "]", "\n", "", "final_filename", "=", "'-'", ".", "join", "(", "[", "checkpoint_base", ",", "sha_hash", "[", ":", "8", "]", "]", ")", "+", "'.pth'", "\n", "shutil", ".", "move", "(", "_TEMP_NAME", ",", "os", ".", "path", ".", "join", "(", "checkpoint_root", ",", "final_filename", ")", ")", "\n", "print", "(", "\"=> Saved state_dict to '{}, SHA256: {}'\"", ".", "format", "(", "final_filename", ",", "sha_hash", ")", ")", "\n", "return", "final_filename", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Error: Checkpoint ({}) doesn't exist\"", ".", "format", "(", "checkpoint", ")", ")", "\n", "return", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.BenchmarkRunner.__init__": [[199, 248], ["benchmark.resolve_precision", "kwargs.pop", "timm.models.create_model", "benchmark.BenchmarkRunner.model.to", "benchmark.count_params", "_logger.info", "timm.data.resolve_data_config", "kwargs.pop", "timm.utils.set_jit_fuser", "torch.jit.script", "torch.jit.script", "torch.jit.script", "torch.jit.script", "torch.jit.script", "torch.jit.script", "torch.jit.script", "torch.jit.script", "torch.jit.script", "memory_efficient_fusion", "functools.partial", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.resolve_precision", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.create_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.count_params", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.config.resolve_data_config", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.jit.set_jit_fuser"], ["    ", "def", "__init__", "(", "\n", "self", ",", "model_name", ",", "detail", "=", "False", ",", "device", "=", "'cuda'", ",", "torchscript", "=", "False", ",", "aot_autograd", "=", "False", ",", "precision", "=", "'float32'", ",", "\n", "fuser", "=", "''", ",", "num_warm_iter", "=", "10", ",", "num_bench_iter", "=", "50", ",", "use_train_size", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "model_name", "=", "model_name", "\n", "self", ".", "detail", "=", "detail", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "use_amp", ",", "self", ".", "model_dtype", ",", "self", ".", "data_dtype", "=", "resolve_precision", "(", "precision", ")", "\n", "self", ".", "channels_last", "=", "kwargs", ".", "pop", "(", "'channels_last'", ",", "False", ")", "\n", "self", ".", "amp_autocast", "=", "torch", ".", "cuda", ".", "amp", ".", "autocast", "if", "self", ".", "use_amp", "else", "suppress", "\n", "\n", "if", "fuser", ":", "\n", "            ", "set_jit_fuser", "(", "fuser", ")", "\n", "", "self", ".", "model", "=", "create_model", "(", "\n", "model_name", ",", "\n", "num_classes", "=", "kwargs", ".", "pop", "(", "'num_classes'", ",", "None", ")", ",", "\n", "in_chans", "=", "3", ",", "\n", "global_pool", "=", "kwargs", ".", "pop", "(", "'gp'", ",", "'fast'", ")", ",", "\n", "scriptable", "=", "torchscript", ",", "\n", "drop_rate", "=", "kwargs", ".", "pop", "(", "'drop'", ",", "0.", ")", ",", "\n", "drop_path_rate", "=", "kwargs", ".", "pop", "(", "'drop_path'", ",", "None", ")", ",", "\n", "drop_block_rate", "=", "kwargs", ".", "pop", "(", "'drop_block'", ",", "None", ")", ",", "\n", ")", "\n", "self", ".", "model", ".", "to", "(", "\n", "device", "=", "self", ".", "device", ",", "\n", "dtype", "=", "self", ".", "model_dtype", ",", "\n", "memory_format", "=", "torch", ".", "channels_last", "if", "self", ".", "channels_last", "else", "None", ")", "\n", "self", ".", "num_classes", "=", "self", ".", "model", ".", "num_classes", "\n", "self", ".", "param_count", "=", "count_params", "(", "self", ".", "model", ")", "\n", "_logger", ".", "info", "(", "'Model %s created, param count: %d'", "%", "(", "model_name", ",", "self", ".", "param_count", ")", ")", "\n", "self", ".", "scripted", "=", "False", "\n", "if", "torchscript", ":", "\n", "            ", "self", ".", "model", "=", "torch", ".", "jit", ".", "script", "(", "self", ".", "model", ")", "\n", "self", ".", "scripted", "=", "True", "\n", "", "data_config", "=", "resolve_data_config", "(", "kwargs", ",", "model", "=", "self", ".", "model", ",", "use_test_size", "=", "not", "use_train_size", ")", "\n", "self", ".", "input_size", "=", "data_config", "[", "'input_size'", "]", "\n", "self", ".", "batch_size", "=", "kwargs", ".", "pop", "(", "'batch_size'", ",", "256", ")", "\n", "\n", "if", "aot_autograd", ":", "\n", "            ", "assert", "has_functorch", ",", "\"functorch is needed for --aot-autograd\"", "\n", "self", ".", "model", "=", "memory_efficient_fusion", "(", "self", ".", "model", ")", "\n", "\n", "", "self", ".", "example_inputs", "=", "None", "\n", "self", ".", "num_warm_iter", "=", "num_warm_iter", "\n", "self", ".", "num_bench_iter", "=", "num_bench_iter", "\n", "self", ".", "log_freq", "=", "num_bench_iter", "//", "5", "\n", "if", "'cuda'", "in", "self", ".", "device", ":", "\n", "            ", "self", ".", "time_fn", "=", "partial", "(", "cuda_timestamp", ",", "device", "=", "self", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "time_fn", "=", "timestamp", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.BenchmarkRunner._init_input": [[249, 254], ["torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "benchmark.BenchmarkRunner.example_inputs.contiguous"], "methods", ["None"], ["", "", "def", "_init_input", "(", "self", ")", ":", "\n", "        ", "self", ".", "example_inputs", "=", "torch", ".", "randn", "(", "\n", "(", "self", ".", "batch_size", ",", ")", "+", "self", ".", "input_size", ",", "device", "=", "self", ".", "device", ",", "dtype", "=", "self", ".", "data_dtype", ")", "\n", "if", "self", ".", "channels_last", ":", "\n", "            ", "self", ".", "example_inputs", "=", "self", ".", "example_inputs", ".", "contiguous", "(", "memory_format", "=", "torch", ".", "channels_last", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.InferenceBenchmarkRunner.__init__": [[258, 261], ["benchmark.BenchmarkRunner.__init__", "benchmark.InferenceBenchmarkRunner.model.eval"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "model_name", ",", "device", "=", "'cuda'", ",", "torchscript", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model_name", "=", "model_name", ",", "device", "=", "device", ",", "torchscript", "=", "torchscript", ",", "**", "kwargs", ")", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.InferenceBenchmarkRunner.run": [[262, 323], ["_logger.info", "dict", "_logger.info", "benchmark.InferenceBenchmarkRunner.time_fn", "benchmark.InferenceBenchmarkRunner.time_fn", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "benchmark.InferenceBenchmarkRunner._init_input", "range", "benchmark.InferenceBenchmarkRunner.time_fn", "range", "benchmark.InferenceBenchmarkRunner.time_fn", "benchmark.InferenceBenchmarkRunner.amp_autocast", "benchmark.InferenceBenchmarkRunner.model", "benchmark.InferenceBenchmarkRunner.run._step"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.BenchmarkRunner._init_input"], ["", "def", "run", "(", "self", ")", ":", "\n", "        ", "def", "_step", "(", ")", ":", "\n", "            ", "t_step_start", "=", "self", ".", "time_fn", "(", ")", "\n", "with", "self", ".", "amp_autocast", "(", ")", ":", "\n", "                ", "output", "=", "self", ".", "model", "(", "self", ".", "example_inputs", ")", "\n", "", "t_step_end", "=", "self", ".", "time_fn", "(", "True", ")", "\n", "return", "t_step_end", "-", "t_step_start", "\n", "\n", "", "_logger", ".", "info", "(", "\n", "f'Running inference benchmark on {self.model_name} for {self.num_bench_iter} steps w/ '", "\n", "f'input size {self.input_size} and batch size {self.batch_size}.'", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "self", ".", "_init_input", "(", ")", "\n", "\n", "for", "_", "in", "range", "(", "self", ".", "num_warm_iter", ")", ":", "\n", "                ", "_step", "(", ")", "\n", "\n", "", "total_step", "=", "0.", "\n", "num_samples", "=", "0", "\n", "t_run_start", "=", "self", ".", "time_fn", "(", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "num_bench_iter", ")", ":", "\n", "                ", "delta_fwd", "=", "_step", "(", ")", "\n", "total_step", "+=", "delta_fwd", "\n", "num_samples", "+=", "self", ".", "batch_size", "\n", "num_steps", "=", "i", "+", "1", "\n", "if", "num_steps", "%", "self", ".", "log_freq", "==", "0", ":", "\n", "                    ", "_logger", ".", "info", "(", "\n", "f\"Infer [{num_steps}/{self.num_bench_iter}].\"", "\n", "f\" {num_samples / total_step:0.2f} samples/sec.\"", "\n", "f\" {1000 * total_step / num_steps:0.3f} ms/step.\"", ")", "\n", "", "", "t_run_end", "=", "self", ".", "time_fn", "(", "True", ")", "\n", "t_run_elapsed", "=", "t_run_end", "-", "t_run_start", "\n", "\n", "", "results", "=", "dict", "(", "\n", "samples_per_sec", "=", "round", "(", "num_samples", "/", "t_run_elapsed", ",", "2", ")", ",", "\n", "step_time", "=", "round", "(", "1000", "*", "total_step", "/", "self", ".", "num_bench_iter", ",", "3", ")", ",", "\n", "batch_size", "=", "self", ".", "batch_size", ",", "\n", "img_size", "=", "self", ".", "input_size", "[", "-", "1", "]", ",", "\n", "param_count", "=", "round", "(", "self", ".", "param_count", "/", "1e6", ",", "2", ")", ",", "\n", ")", "\n", "\n", "retries", "=", "0", "if", "self", ".", "scripted", "else", "2", "# skip profiling if model is scripted", "\n", "while", "retries", ":", "\n", "            ", "retries", "-=", "1", "\n", "try", ":", "\n", "                ", "if", "has_deepspeed_profiling", ":", "\n", "                    ", "macs", ",", "_", "=", "profile_deepspeed", "(", "self", ".", "model", ",", "self", ".", "input_size", ")", "\n", "results", "[", "'gmacs'", "]", "=", "round", "(", "macs", "/", "1e9", ",", "2", ")", "\n", "", "elif", "has_fvcore_profiling", ":", "\n", "                    ", "macs", ",", "activations", "=", "profile_fvcore", "(", "self", ".", "model", ",", "self", ".", "input_size", ",", "force_cpu", "=", "not", "retries", ")", "\n", "results", "[", "'gmacs'", "]", "=", "round", "(", "macs", "/", "1e9", ",", "2", ")", "\n", "results", "[", "'macts'", "]", "=", "round", "(", "activations", "/", "1e6", ",", "2", ")", "\n", "", "", "except", "RuntimeError", "as", "e", ":", "\n", "                ", "pass", "\n", "\n", "", "", "_logger", ".", "info", "(", "\n", "f\"Inference benchmark of {self.model_name} done. \"", "\n", "f\"{results['samples_per_sec']:.2f} samples/sec, {results['step_time']:.2f} ms/step\"", ")", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.TrainBenchmarkRunner.__init__": [[327, 341], ["benchmark.BenchmarkRunner.__init__", "benchmark.TrainBenchmarkRunner.model.train", "torch.CrossEntropyLoss().to", "torch.CrossEntropyLoss().to", "torch.CrossEntropyLoss().to", "tuple", "timm.optim.create_optimizer_v2", "kwargs.pop", "benchmark.TrainBenchmarkRunner.model.set_grad_checkpointing", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "kwargs.pop", "kwargs.pop"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.AttentionSubsample.train", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.set_grad_checkpointing"], ["    ", "def", "__init__", "(", "self", ",", "model_name", ",", "device", "=", "'cuda'", ",", "torchscript", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model_name", "=", "model_name", ",", "device", "=", "device", ",", "torchscript", "=", "torchscript", ",", "**", "kwargs", ")", "\n", "self", ".", "model", ".", "train", "(", ")", "\n", "\n", "self", ".", "loss", "=", "nn", ".", "CrossEntropyLoss", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "target_shape", "=", "tuple", "(", ")", "\n", "\n", "self", ".", "optimizer", "=", "create_optimizer_v2", "(", "\n", "self", ".", "model", ",", "\n", "opt", "=", "kwargs", ".", "pop", "(", "'opt'", ",", "'sgd'", ")", ",", "\n", "lr", "=", "kwargs", ".", "pop", "(", "'lr'", ",", "1e-4", ")", ")", "\n", "\n", "if", "kwargs", ".", "pop", "(", "'grad_checkpointing'", ",", "False", ")", ":", "\n", "            ", "self", ".", "model", ".", "set_grad_checkpointing", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.TrainBenchmarkRunner._gen_target": [[342, 345], ["torch.empty().random_", "torch.empty().random_", "torch.empty().random_", "torch.empty().random_", "torch.empty().random_", "torch.empty().random_", "torch.empty().random_", "torch.empty().random_", "torch.empty().random_", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty"], "methods", ["None"], ["", "", "def", "_gen_target", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "return", "torch", ".", "empty", "(", "\n", "(", "batch_size", ",", ")", "+", "self", ".", "target_shape", ",", "device", "=", "self", ".", "device", ",", "dtype", "=", "torch", ".", "long", ")", ".", "random_", "(", "self", ".", "num_classes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.TrainBenchmarkRunner.run": [[346, 443], ["_logger.info", "benchmark.TrainBenchmarkRunner._init_input", "range", "benchmark.TrainBenchmarkRunner.time_fn", "_logger.info", "benchmark.TrainBenchmarkRunner.optimizer.zero_grad", "benchmark.TrainBenchmarkRunner.time_fn", "benchmark.TrainBenchmarkRunner.optimizer.step", "benchmark.TrainBenchmarkRunner.time_fn", "benchmark.TrainBenchmarkRunner.run._step"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.BenchmarkRunner._init_input", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor.step"], ["", "def", "run", "(", "self", ")", ":", "\n", "        ", "def", "_step", "(", "detail", "=", "False", ")", ":", "\n", "            ", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "# can this be ignored?", "\n", "t_start", "=", "self", ".", "time_fn", "(", ")", "\n", "t_fwd_end", "=", "t_start", "\n", "t_bwd_end", "=", "t_start", "\n", "with", "self", ".", "amp_autocast", "(", ")", ":", "\n", "                ", "output", "=", "self", ".", "model", "(", "self", ".", "example_inputs", ")", "\n", "if", "isinstance", "(", "output", ",", "tuple", ")", ":", "\n", "                    ", "output", "=", "output", "[", "0", "]", "\n", "", "if", "detail", ":", "\n", "                    ", "t_fwd_end", "=", "self", ".", "time_fn", "(", "True", ")", "\n", "", "target", "=", "self", ".", "_gen_target", "(", "output", ".", "shape", "[", "0", "]", ")", "\n", "self", ".", "loss", "(", "output", ",", "target", ")", ".", "backward", "(", ")", "\n", "if", "detail", ":", "\n", "                    ", "t_bwd_end", "=", "self", ".", "time_fn", "(", "True", ")", "\n", "", "", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "t_end", "=", "self", ".", "time_fn", "(", "True", ")", "\n", "if", "detail", ":", "\n", "                ", "delta_fwd", "=", "t_fwd_end", "-", "t_start", "\n", "delta_bwd", "=", "t_bwd_end", "-", "t_fwd_end", "\n", "delta_opt", "=", "t_end", "-", "t_bwd_end", "\n", "return", "delta_fwd", ",", "delta_bwd", ",", "delta_opt", "\n", "", "else", ":", "\n", "                ", "delta_step", "=", "t_end", "-", "t_start", "\n", "return", "delta_step", "\n", "\n", "", "", "_logger", ".", "info", "(", "\n", "f'Running train benchmark on {self.model_name} for {self.num_bench_iter} steps w/ '", "\n", "f'input size {self.input_size} and batch size {self.batch_size}.'", ")", "\n", "\n", "self", ".", "_init_input", "(", ")", "\n", "\n", "for", "_", "in", "range", "(", "self", ".", "num_warm_iter", ")", ":", "\n", "            ", "_step", "(", ")", "\n", "\n", "", "t_run_start", "=", "self", ".", "time_fn", "(", ")", "\n", "if", "self", ".", "detail", ":", "\n", "            ", "total_fwd", "=", "0.", "\n", "total_bwd", "=", "0.", "\n", "total_opt", "=", "0.", "\n", "num_samples", "=", "0", "\n", "for", "i", "in", "range", "(", "self", ".", "num_bench_iter", ")", ":", "\n", "                ", "delta_fwd", ",", "delta_bwd", ",", "delta_opt", "=", "_step", "(", "True", ")", "\n", "num_samples", "+=", "self", ".", "batch_size", "\n", "total_fwd", "+=", "delta_fwd", "\n", "total_bwd", "+=", "delta_bwd", "\n", "total_opt", "+=", "delta_opt", "\n", "num_steps", "=", "(", "i", "+", "1", ")", "\n", "if", "num_steps", "%", "self", ".", "log_freq", "==", "0", ":", "\n", "                    ", "total_step", "=", "total_fwd", "+", "total_bwd", "+", "total_opt", "\n", "_logger", ".", "info", "(", "\n", "f\"Train [{num_steps}/{self.num_bench_iter}].\"", "\n", "f\" {num_samples / total_step:0.2f} samples/sec.\"", "\n", "f\" {1000 * total_fwd / num_steps:0.3f} ms/step fwd,\"", "\n", "f\" {1000 * total_bwd / num_steps:0.3f} ms/step bwd,\"", "\n", "f\" {1000 * total_opt / num_steps:0.3f} ms/step opt.\"", "\n", ")", "\n", "", "", "total_step", "=", "total_fwd", "+", "total_bwd", "+", "total_opt", "\n", "t_run_elapsed", "=", "self", ".", "time_fn", "(", ")", "-", "t_run_start", "\n", "results", "=", "dict", "(", "\n", "samples_per_sec", "=", "round", "(", "num_samples", "/", "t_run_elapsed", ",", "2", ")", ",", "\n", "step_time", "=", "round", "(", "1000", "*", "total_step", "/", "self", ".", "num_bench_iter", ",", "3", ")", ",", "\n", "fwd_time", "=", "round", "(", "1000", "*", "total_fwd", "/", "self", ".", "num_bench_iter", ",", "3", ")", ",", "\n", "bwd_time", "=", "round", "(", "1000", "*", "total_bwd", "/", "self", ".", "num_bench_iter", ",", "3", ")", ",", "\n", "opt_time", "=", "round", "(", "1000", "*", "total_opt", "/", "self", ".", "num_bench_iter", ",", "3", ")", ",", "\n", "batch_size", "=", "self", ".", "batch_size", ",", "\n", "img_size", "=", "self", ".", "input_size", "[", "-", "1", "]", ",", "\n", "param_count", "=", "round", "(", "self", ".", "param_count", "/", "1e6", ",", "2", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "total_step", "=", "0.", "\n", "num_samples", "=", "0", "\n", "for", "i", "in", "range", "(", "self", ".", "num_bench_iter", ")", ":", "\n", "                ", "delta_step", "=", "_step", "(", "False", ")", "\n", "num_samples", "+=", "self", ".", "batch_size", "\n", "total_step", "+=", "delta_step", "\n", "num_steps", "=", "(", "i", "+", "1", ")", "\n", "if", "num_steps", "%", "self", ".", "log_freq", "==", "0", ":", "\n", "                    ", "_logger", ".", "info", "(", "\n", "f\"Train [{num_steps}/{self.num_bench_iter}].\"", "\n", "f\" {num_samples / total_step:0.2f} samples/sec.\"", "\n", "f\" {1000 * total_step / num_steps:0.3f} ms/step.\"", ")", "\n", "", "", "t_run_elapsed", "=", "self", ".", "time_fn", "(", ")", "-", "t_run_start", "\n", "results", "=", "dict", "(", "\n", "samples_per_sec", "=", "round", "(", "num_samples", "/", "t_run_elapsed", ",", "2", ")", ",", "\n", "step_time", "=", "round", "(", "1000", "*", "total_step", "/", "self", ".", "num_bench_iter", ",", "3", ")", ",", "\n", "batch_size", "=", "self", ".", "batch_size", ",", "\n", "img_size", "=", "self", ".", "input_size", "[", "-", "1", "]", ",", "\n", "param_count", "=", "round", "(", "self", ".", "param_count", "/", "1e6", ",", "2", ")", ",", "\n", ")", "\n", "\n", "", "_logger", ".", "info", "(", "\n", "f\"Train benchmark of {self.model_name} done. \"", "\n", "f\"{results['samples_per_sec']:.2f} samples/sec, {results['step_time']:.2f} ms/sample\"", ")", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.ProfileRunner.__init__": [[447, 457], ["benchmark.BenchmarkRunner.__init__", "benchmark.ProfileRunner.model.eval"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "model_name", ",", "device", "=", "'cuda'", ",", "profiler", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model_name", "=", "model_name", ",", "device", "=", "device", ",", "**", "kwargs", ")", "\n", "if", "not", "profiler", ":", "\n", "            ", "if", "has_deepspeed_profiling", ":", "\n", "                ", "profiler", "=", "'deepspeed'", "\n", "", "elif", "has_fvcore_profiling", ":", "\n", "                ", "profiler", "=", "'fvcore'", "\n", "", "", "assert", "profiler", ",", "\"One of deepspeed or fvcore needs to be installed for profiling to work.\"", "\n", "self", ".", "profiler", "=", "profiler", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.ProfileRunner.run": [[458, 483], ["_logger.info", "dict", "_logger.info", "benchmark.profile_deepspeed", "benchmark.profile_fvcore", "round", "round", "round"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.profile_deepspeed", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.profile_fvcore"], ["", "def", "run", "(", "self", ")", ":", "\n", "        ", "_logger", ".", "info", "(", "\n", "f'Running profiler on {self.model_name} w/ '", "\n", "f'input size {self.input_size} and batch size {self.batch_size}.'", ")", "\n", "\n", "macs", "=", "0", "\n", "activations", "=", "0", "\n", "if", "self", ".", "profiler", "==", "'deepspeed'", ":", "\n", "            ", "macs", ",", "_", "=", "profile_deepspeed", "(", "self", ".", "model", ",", "self", ".", "input_size", ",", "batch_size", "=", "self", ".", "batch_size", ",", "detailed", "=", "True", ")", "\n", "", "elif", "self", ".", "profiler", "==", "'fvcore'", ":", "\n", "            ", "macs", ",", "activations", "=", "profile_fvcore", "(", "self", ".", "model", ",", "self", ".", "input_size", ",", "batch_size", "=", "self", ".", "batch_size", ",", "detailed", "=", "True", ")", "\n", "\n", "", "results", "=", "dict", "(", "\n", "gmacs", "=", "round", "(", "macs", "/", "1e9", ",", "2", ")", ",", "\n", "macts", "=", "round", "(", "activations", "/", "1e6", ",", "2", ")", ",", "\n", "batch_size", "=", "self", ".", "batch_size", ",", "\n", "img_size", "=", "self", ".", "input_size", "[", "-", "1", "]", ",", "\n", "param_count", "=", "round", "(", "self", ".", "param_count", "/", "1e6", ",", "2", ")", ",", "\n", ")", "\n", "\n", "_logger", ".", "info", "(", "\n", "f\"Profile of {self.model_name} done. \"", "\n", "f\"{results['gmacs']:.2f} GMACs, {results['param_count']:.2f} M params.\"", ")", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.timestamp": [[141, 143], ["time.perf_counter"], "function", ["None"], ["def", "timestamp", "(", "sync", "=", "False", ")", ":", "\n", "    ", "return", "time", ".", "perf_counter", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.cuda_timestamp": [[145, 149], ["time.perf_counter", "torch.cuda.synchronize", "torch.cuda.synchronize", "torch.cuda.synchronize"], "function", ["None"], ["", "def", "cuda_timestamp", "(", "sync", "=", "False", ",", "device", "=", "None", ")", ":", "\n", "    ", "if", "sync", ":", "\n", "        ", "torch", ".", "cuda", ".", "synchronize", "(", "device", "=", "device", ")", "\n", "", "return", "time", ".", "perf_counter", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.count_params": [[151, 153], ["sum", "m.numel", "model.parameters"], "function", ["None"], ["", "def", "count_params", "(", "model", ":", "nn", ".", "Module", ")", ":", "\n", "    ", "return", "sum", "(", "[", "m", ".", "numel", "(", ")", "for", "m", "in", "model", ".", "parameters", "(", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.resolve_precision": [[155, 169], ["None"], "function", ["None"], ["", "def", "resolve_precision", "(", "precision", ":", "str", ")", ":", "\n", "    ", "assert", "precision", "in", "(", "'amp'", ",", "'float16'", ",", "'bfloat16'", ",", "'float32'", ")", "\n", "use_amp", "=", "False", "\n", "model_dtype", "=", "torch", ".", "float32", "\n", "data_dtype", "=", "torch", ".", "float32", "\n", "if", "precision", "==", "'amp'", ":", "\n", "        ", "use_amp", "=", "True", "\n", "", "elif", "precision", "==", "'float16'", ":", "\n", "        ", "model_dtype", "=", "torch", ".", "float16", "\n", "data_dtype", "=", "torch", ".", "float16", "\n", "", "elif", "precision", "==", "'bfloat16'", ":", "\n", "        ", "model_dtype", "=", "torch", ".", "bfloat16", "\n", "data_dtype", "=", "torch", ".", "bfloat16", "\n", "", "return", "use_amp", ",", "model_dtype", ",", "data_dtype", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.profile_deepspeed": [[171, 183], ["get_model_profile"], "function", ["None"], ["", "def", "profile_deepspeed", "(", "model", ",", "input_size", "=", "(", "3", ",", "224", ",", "224", ")", ",", "batch_size", "=", "1", ",", "detailed", "=", "False", ")", ":", "\n", "    ", "macs", ",", "_", "=", "get_model_profile", "(", "\n", "model", "=", "model", ",", "\n", "input_res", "=", "(", "batch_size", ",", ")", "+", "input_size", ",", "# input shape or input to the input_constructor", "\n", "input_constructor", "=", "None", ",", "# if specified, a constructor taking input_res is used as input to the model", "\n", "print_profile", "=", "detailed", ",", "# prints the model graph with the measured profile attached to each module", "\n", "detailed", "=", "detailed", ",", "# print the detailed profile", "\n", "warm_up", "=", "10", ",", "# the number of warm-ups before measuring the time of each module", "\n", "as_string", "=", "False", ",", "# print raw numbers (e.g. 1000) or as human-readable strings (e.g. 1k)", "\n", "output_file", "=", "None", ",", "# path to the output file. If None, the profiler prints to stdout.", "\n", "ignore_modules", "=", "None", ")", "# the list of modules to ignore in the profiling", "\n", "return", "macs", ",", "0", "# no activation count in DS", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.profile_fvcore": [[185, 196], ["torch.ones", "torch.ones", "torch.ones", "FlopCountAnalysis", "ActivationCountAnalysis", "model.to.to", "flop_count_str", "print", "FlopCountAnalysis.total", "ActivationCountAnalysis.total", "next", "next", "model.to.parameters", "model.to.parameters"], "function", ["None"], ["", "def", "profile_fvcore", "(", "model", ",", "input_size", "=", "(", "3", ",", "224", ",", "224", ")", ",", "batch_size", "=", "1", ",", "detailed", "=", "False", ",", "force_cpu", "=", "False", ")", ":", "\n", "    ", "if", "force_cpu", ":", "\n", "        ", "model", "=", "model", ".", "to", "(", "'cpu'", ")", "\n", "", "device", ",", "dtype", "=", "next", "(", "model", ".", "parameters", "(", ")", ")", ".", "device", ",", "next", "(", "model", ".", "parameters", "(", ")", ")", ".", "dtype", "\n", "example_input", "=", "torch", ".", "ones", "(", "(", "batch_size", ",", ")", "+", "input_size", ",", "device", "=", "device", ",", "dtype", "=", "dtype", ")", "\n", "fca", "=", "FlopCountAnalysis", "(", "model", ",", "example_input", ")", "\n", "aca", "=", "ActivationCountAnalysis", "(", "model", ",", "example_input", ")", "\n", "if", "detailed", ":", "\n", "        ", "fcs", "=", "flop_count_str", "(", "fca", ")", "\n", "print", "(", "fcs", ")", "\n", "", "return", "fca", ".", "total", "(", ")", ",", "aca", ".", "total", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.decay_batch_exp": [[485, 492], ["max", "int"], "function", ["None"], ["", "", "def", "decay_batch_exp", "(", "batch_size", ",", "factor", "=", "0.5", ",", "divisor", "=", "16", ")", ":", "\n", "    ", "out_batch_size", "=", "batch_size", "*", "factor", "\n", "if", "out_batch_size", ">", "divisor", ":", "\n", "        ", "out_batch_size", "=", "(", "out_batch_size", "+", "1", ")", "//", "divisor", "*", "divisor", "\n", "", "else", ":", "\n", "        ", "out_batch_size", "=", "batch_size", "-", "1", "\n", "", "return", "max", "(", "0", ",", "int", "(", "out_batch_size", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark._try_run": [[494, 513], ["dict", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "benchmark.decay_batch_exp", "bench_fn", "bench_fn.run", "str", "_logger.warning", "_logger.error"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.decay_batch_exp", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.ProfileRunner.run"], ["", "def", "_try_run", "(", "model_name", ",", "bench_fn", ",", "initial_batch_size", ",", "bench_kwargs", ")", ":", "\n", "    ", "batch_size", "=", "initial_batch_size", "\n", "results", "=", "dict", "(", ")", "\n", "error_str", "=", "'Unknown'", "\n", "while", "batch_size", ">=", "1", ":", "\n", "        ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "try", ":", "\n", "            ", "bench", "=", "bench_fn", "(", "model_name", "=", "model_name", ",", "batch_size", "=", "batch_size", ",", "**", "bench_kwargs", ")", "\n", "results", "=", "bench", ".", "run", "(", ")", "\n", "return", "results", "\n", "", "except", "RuntimeError", "as", "e", ":", "\n", "            ", "error_str", "=", "str", "(", "e", ")", "\n", "if", "'channels_last'", "in", "error_str", ":", "\n", "                ", "_logger", ".", "error", "(", "f'{model_name} not supported in channels_last, skipping.'", ")", "\n", "break", "\n", "", "_logger", ".", "warning", "(", "f'\"{error_str}\" while running benchmark. Reducing batch size to {batch_size} for retry.'", ")", "\n", "", "batch_size", "=", "decay_batch_exp", "(", "batch_size", ")", "\n", "", "results", "[", "'error'", "]", "=", "error_str", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.benchmark": [[515, 561], ["_logger.info", "vars().copy", "vars().copy.pop", "vars().copy.pop", "vars().copy.pop", "collections.OrderedDict", "zip", "_logger.warning", "benchmark._try_run", "collections.OrderedDict.update", "collections.OrderedDict.pop", "collections.OrderedDict.setdefault", "collections.OrderedDict.pop", "vars", "args.bench.startswith", "collections.OrderedDict.pop", "_try_run.items"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark._try_run", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update"], ["", "def", "benchmark", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "amp", ":", "\n", "        ", "_logger", ".", "warning", "(", "\"Overriding precision to 'amp' since --amp flag set.\"", ")", "\n", "args", ".", "precision", "=", "'amp'", "\n", "", "_logger", ".", "info", "(", "f'Benchmarking in {args.precision} precision. '", "\n", "f'{\"NHWC\" if args.channels_last else \"NCHW\"} layout. '", "\n", "f'torchscript {\"enabled\" if args.torchscript else \"disabled\"}'", ")", "\n", "\n", "bench_kwargs", "=", "vars", "(", "args", ")", ".", "copy", "(", ")", "\n", "bench_kwargs", ".", "pop", "(", "'amp'", ")", "\n", "model", "=", "bench_kwargs", ".", "pop", "(", "'model'", ")", "\n", "batch_size", "=", "bench_kwargs", ".", "pop", "(", "'batch_size'", ")", "\n", "\n", "bench_fns", "=", "(", "InferenceBenchmarkRunner", ",", ")", "\n", "prefixes", "=", "(", "'infer'", ",", ")", "\n", "if", "args", ".", "bench", "==", "'both'", ":", "\n", "        ", "bench_fns", "=", "(", "\n", "InferenceBenchmarkRunner", ",", "\n", "TrainBenchmarkRunner", "\n", ")", "\n", "prefixes", "=", "(", "'infer'", ",", "'train'", ")", "\n", "", "elif", "args", ".", "bench", "==", "'train'", ":", "\n", "        ", "bench_fns", "=", "TrainBenchmarkRunner", ",", "\n", "prefixes", "=", "'train'", ",", "\n", "", "elif", "args", ".", "bench", ".", "startswith", "(", "'profile'", ")", ":", "\n", "# specific profiler used if included in bench mode string, otherwise default to deepspeed, fallback to fvcore", "\n", "        ", "if", "'deepspeed'", "in", "args", ".", "bench", ":", "\n", "            ", "assert", "has_deepspeed_profiling", ",", "\"deepspeed must be installed to use deepspeed flop counter\"", "\n", "bench_kwargs", "[", "'profiler'", "]", "=", "'deepspeed'", "\n", "", "elif", "'fvcore'", "in", "args", ".", "bench", ":", "\n", "            ", "assert", "has_fvcore_profiling", ",", "\"fvcore must be installed to use fvcore flop counter\"", "\n", "bench_kwargs", "[", "'profiler'", "]", "=", "'fvcore'", "\n", "", "bench_fns", "=", "ProfileRunner", ",", "\n", "batch_size", "=", "1", "\n", "\n", "", "model_results", "=", "OrderedDict", "(", "model", "=", "model", ")", "\n", "for", "prefix", ",", "bench_fn", "in", "zip", "(", "prefixes", ",", "bench_fns", ")", ":", "\n", "        ", "run_results", "=", "_try_run", "(", "model", ",", "bench_fn", ",", "initial_batch_size", "=", "batch_size", ",", "bench_kwargs", "=", "bench_kwargs", ")", "\n", "if", "prefix", "and", "'error'", "not", "in", "run_results", ":", "\n", "            ", "run_results", "=", "{", "'_'", ".", "join", "(", "[", "prefix", ",", "k", "]", ")", ":", "v", "for", "k", ",", "v", "in", "run_results", ".", "items", "(", ")", "}", "\n", "", "model_results", ".", "update", "(", "run_results", ")", "\n", "", "if", "'error'", "not", "in", "model_results", ":", "\n", "        ", "param_count", "=", "model_results", ".", "pop", "(", "'infer_param_count'", ",", "model_results", ".", "pop", "(", "'train_param_count'", ",", "0", ")", ")", "\n", "model_results", ".", "setdefault", "(", "'param_count'", ",", "param_count", ")", "\n", "model_results", ".", "pop", "(", "'train_param_count'", ",", "0", ")", "\n", "", "return", "model_results", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.main": [[563, 613], ["timm.utils.setup_default_logging", "parser.parse_args", "len", "print", "_logger.info", "filter", "sorted", "len", "benchmark.benchmark", "open", "timm.models.list_models", "benchmark.write_results", "line.rstrip", "timm.models.is_model", "timm.models.list_models", "benchmark.benchmark", "time.sleep", "json.dumps", "benchmark.append"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.log.setup_default_logging", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.benchmark", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.list_models", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.write_results", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.is_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.list_models", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.benchmark"], ["", "def", "main", "(", ")", ":", "\n", "    ", "setup_default_logging", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "model_cfgs", "=", "[", "]", "\n", "model_names", "=", "[", "]", "\n", "\n", "if", "args", ".", "model_list", ":", "\n", "        ", "args", ".", "model", "=", "''", "\n", "with", "open", "(", "args", ".", "model_list", ")", "as", "f", ":", "\n", "            ", "model_names", "=", "[", "line", ".", "rstrip", "(", ")", "for", "line", "in", "f", "]", "\n", "", "model_cfgs", "=", "[", "(", "n", ",", "None", ")", "for", "n", "in", "model_names", "]", "\n", "", "elif", "args", ".", "model", "==", "'all'", ":", "\n", "# validate all models in a list of names with pretrained checkpoints", "\n", "        ", "args", ".", "pretrained", "=", "True", "\n", "model_names", "=", "list_models", "(", "pretrained", "=", "True", ",", "exclude_filters", "=", "[", "'*in21k'", "]", ")", "\n", "model_cfgs", "=", "[", "(", "n", ",", "None", ")", "for", "n", "in", "model_names", "]", "\n", "", "elif", "not", "is_model", "(", "args", ".", "model", ")", ":", "\n", "# model name doesn't exist, try as wildcard filter", "\n", "        ", "model_names", "=", "list_models", "(", "args", ".", "model", ")", "\n", "model_cfgs", "=", "[", "(", "n", ",", "None", ")", "for", "n", "in", "model_names", "]", "\n", "\n", "", "if", "len", "(", "model_cfgs", ")", ":", "\n", "        ", "results_file", "=", "args", ".", "results_file", "or", "'./benchmark.csv'", "\n", "_logger", ".", "info", "(", "'Running bulk validation on these pretrained models: {}'", ".", "format", "(", "', '", ".", "join", "(", "model_names", ")", ")", ")", "\n", "results", "=", "[", "]", "\n", "try", ":", "\n", "            ", "for", "m", ",", "_", "in", "model_cfgs", ":", "\n", "                ", "if", "not", "m", ":", "\n", "                    ", "continue", "\n", "", "args", ".", "model", "=", "m", "\n", "r", "=", "benchmark", "(", "args", ")", "\n", "if", "r", ":", "\n", "                    ", "results", ".", "append", "(", "r", ")", "\n", "", "time", ".", "sleep", "(", "10", ")", "\n", "", "", "except", "KeyboardInterrupt", "as", "e", ":", "\n", "            ", "pass", "\n", "", "sort_key", "=", "'infer_samples_per_sec'", "\n", "if", "'train'", "in", "args", ".", "bench", ":", "\n", "            ", "sort_key", "=", "'train_samples_per_sec'", "\n", "", "elif", "'profile'", "in", "args", ".", "bench", ":", "\n", "            ", "sort_key", "=", "'infer_gmacs'", "\n", "", "results", "=", "filter", "(", "lambda", "x", ":", "sort_key", "in", "x", ",", "results", ")", "\n", "results", "=", "sorted", "(", "results", ",", "key", "=", "lambda", "x", ":", "x", "[", "sort_key", "]", ",", "reverse", "=", "True", ")", "\n", "if", "len", "(", "results", ")", ":", "\n", "            ", "write_results", "(", "results_file", ",", "results", ")", "\n", "", "", "else", ":", "\n", "        ", "results", "=", "benchmark", "(", "args", ")", "\n", "\n", "# output results in JSON to stdout w/ delimiter for runner script", "\n", "", "print", "(", "f'--result\\n{json.dumps(results, indent=4)}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.write_results": [[615, 622], ["open", "csv.DictWriter", "csv.DictWriter.writeheader", "cf.flush", "csv.DictWriter.writerow", "results[].keys"], "function", ["None"], ["", "def", "write_results", "(", "results_file", ",", "results", ")", ":", "\n", "    ", "with", "open", "(", "results_file", ",", "mode", "=", "'w'", ")", "as", "cf", ":", "\n", "        ", "dw", "=", "csv", ".", "DictWriter", "(", "cf", ",", "fieldnames", "=", "results", "[", "0", "]", ".", "keys", "(", ")", ")", "\n", "dw", ".", "writeheader", "(", ")", "\n", "for", "r", "in", "results", ":", "\n", "            ", "dw", ".", "writerow", "(", "r", ")", "\n", "", "cf", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.avg_checkpoints.checkpoint_metric": [[36, 49], ["print", "torch.load", "os.path.isfile", "print"], "function", ["None"], ["def", "checkpoint_metric", "(", "checkpoint_path", ")", ":", "\n", "    ", "if", "not", "checkpoint_path", "or", "not", "os", ".", "path", ".", "isfile", "(", "checkpoint_path", ")", ":", "\n", "        ", "return", "{", "}", "\n", "", "print", "(", "\"=> Extracting metric from checkpoint '{}'\"", ".", "format", "(", "checkpoint_path", ")", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "checkpoint_path", ",", "map_location", "=", "'cpu'", ")", "\n", "metric", "=", "None", "\n", "if", "'metric'", "in", "checkpoint", ":", "\n", "        ", "metric", "=", "checkpoint", "[", "'metric'", "]", "\n", "", "elif", "'metrics'", "in", "checkpoint", "and", "'metric_name'", "in", "checkpoint", ":", "\n", "        ", "metrics", "=", "checkpoint", "[", "'metrics'", "]", "\n", "print", "(", "metrics", ")", "\n", "metric", "=", "metrics", "[", "checkpoint", "[", "'metric_name'", "]", "]", "\n", "", "return", "metric", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.avg_checkpoints.main": [[51, 118], ["parser.parse_args", "os.path.exists", "glob.glob", "avg_state_dict.items", "torch.finfo", "avg_state_dict.items", "print", "print", "exit", "list", "print", "print", "timm.models.helpers.load_state_dict", "timm.models.helpers.load_state_dict.items", "v.clamp.div_", "v.clamp.clamp", "v.clamp.to", "torch.save", "open", "hashlib.sha256().hexdigest", "parser.parse_args.input.endswith", "parser.parse_args.filter.startswith", "avg_checkpoints.checkpoint_metric", "sorted", "print", "print", "print", "torch.save", "list.append", "v.clamp.clone().to", "v.clamp.to", "hashlib.sha256", "f.read", "v.clamp.clone"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.avg_checkpoints.checkpoint_metric"], ["", "def", "main", "(", ")", ":", "\n", "    ", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "# by default use the EMA weights (if present)", "\n", "args", ".", "use_ema", "=", "not", "args", ".", "no_use_ema", "\n", "# by default sort by checkpoint metric (if present) and avg top n checkpoints", "\n", "args", ".", "sort", "=", "not", "args", ".", "no_sort", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "output", ")", ":", "\n", "        ", "print", "(", "\"Error: Output filename ({}) already exists.\"", ".", "format", "(", "args", ".", "output", ")", ")", "\n", "exit", "(", "1", ")", "\n", "\n", "", "pattern", "=", "args", ".", "input", "\n", "if", "not", "args", ".", "input", ".", "endswith", "(", "os", ".", "path", ".", "sep", ")", "and", "not", "args", ".", "filter", ".", "startswith", "(", "os", ".", "path", ".", "sep", ")", ":", "\n", "        ", "pattern", "+=", "os", ".", "path", ".", "sep", "\n", "", "pattern", "+=", "args", ".", "filter", "\n", "checkpoints", "=", "glob", ".", "glob", "(", "pattern", ",", "recursive", "=", "True", ")", "\n", "\n", "if", "args", ".", "sort", ":", "\n", "        ", "checkpoint_metrics", "=", "[", "]", "\n", "for", "c", "in", "checkpoints", ":", "\n", "            ", "metric", "=", "checkpoint_metric", "(", "c", ")", "\n", "if", "metric", "is", "not", "None", ":", "\n", "                ", "checkpoint_metrics", ".", "append", "(", "(", "metric", ",", "c", ")", ")", "\n", "", "", "checkpoint_metrics", "=", "list", "(", "sorted", "(", "checkpoint_metrics", ")", ")", "\n", "checkpoint_metrics", "=", "checkpoint_metrics", "[", "-", "args", ".", "n", ":", "]", "\n", "print", "(", "\"Selected checkpoints:\"", ")", "\n", "[", "print", "(", "m", ",", "c", ")", "for", "m", ",", "c", "in", "checkpoint_metrics", "]", "\n", "avg_checkpoints", "=", "[", "c", "for", "m", ",", "c", "in", "checkpoint_metrics", "]", "\n", "", "else", ":", "\n", "        ", "avg_checkpoints", "=", "checkpoints", "\n", "print", "(", "\"Selected checkpoints:\"", ")", "\n", "[", "print", "(", "c", ")", "for", "c", "in", "checkpoints", "]", "\n", "\n", "", "avg_state_dict", "=", "{", "}", "\n", "avg_counts", "=", "{", "}", "\n", "for", "c", "in", "avg_checkpoints", ":", "\n", "        ", "new_state_dict", "=", "load_state_dict", "(", "c", ",", "args", ".", "use_ema", ")", "\n", "if", "not", "new_state_dict", ":", "\n", "            ", "print", "(", "\"Error: Checkpoint ({}) doesn't exist\"", ".", "format", "(", "args", ".", "checkpoint", ")", ")", "\n", "continue", "\n", "\n", "", "for", "k", ",", "v", "in", "new_state_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", "not", "in", "avg_state_dict", ":", "\n", "                ", "avg_state_dict", "[", "k", "]", "=", "v", ".", "clone", "(", ")", ".", "to", "(", "dtype", "=", "torch", ".", "float64", ")", "\n", "avg_counts", "[", "k", "]", "=", "1", "\n", "", "else", ":", "\n", "                ", "avg_state_dict", "[", "k", "]", "+=", "v", ".", "to", "(", "dtype", "=", "torch", ".", "float64", ")", "\n", "avg_counts", "[", "k", "]", "+=", "1", "\n", "\n", "", "", "", "for", "k", ",", "v", "in", "avg_state_dict", ".", "items", "(", ")", ":", "\n", "        ", "v", ".", "div_", "(", "avg_counts", "[", "k", "]", ")", "\n", "\n", "# float32 overflow seems unlikely based on weights seen to date, but who knows", "\n", "", "float32_info", "=", "torch", ".", "finfo", "(", "torch", ".", "float32", ")", "\n", "final_state_dict", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "avg_state_dict", ".", "items", "(", ")", ":", "\n", "        ", "v", "=", "v", ".", "clamp", "(", "float32_info", ".", "min", ",", "float32_info", ".", "max", ")", "\n", "final_state_dict", "[", "k", "]", "=", "v", ".", "to", "(", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n", "", "try", ":", "\n", "        ", "torch", ".", "save", "(", "final_state_dict", ",", "args", ".", "output", ",", "_use_new_zipfile_serialization", "=", "False", ")", "\n", "", "except", ":", "\n", "        ", "torch", ".", "save", "(", "final_state_dict", ",", "args", ".", "output", ")", "\n", "\n", "", "with", "open", "(", "args", ".", "output", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "sha_hash", "=", "hashlib", ".", "sha256", "(", "f", ".", "read", "(", ")", ")", ".", "hexdigest", "(", ")", "\n", "", "print", "(", "\"=> Saved state_dict to '{}, SHA256: {}'\"", ".", "format", "(", "args", ".", "output", ",", "sha_hash", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.results.generate_csv_results.diff": [[20, 67], ["pandas.read_csv", "numpy.zeros_like", "numpy.zeros_like", "numpy.zeros_like", "enumerate", "test_df[].map", "pd.read_csv.sort_values", "pd.read_csv.to_csv", "int", "numpy.where", "abs", "abs"], "function", ["None"], ["def", "diff", "(", "base_df", ",", "test_csv", ")", ":", "\n", "    ", "base_models", "=", "base_df", "[", "'model'", "]", ".", "values", "\n", "test_df", "=", "pd", ".", "read_csv", "(", "test_csv", ")", "\n", "test_models", "=", "test_df", "[", "'model'", "]", ".", "values", "\n", "\n", "rank_diff", "=", "np", ".", "zeros_like", "(", "test_models", ",", "dtype", "=", "'object'", ")", "\n", "top1_diff", "=", "np", ".", "zeros_like", "(", "test_models", ",", "dtype", "=", "'object'", ")", "\n", "top5_diff", "=", "np", ".", "zeros_like", "(", "test_models", ",", "dtype", "=", "'object'", ")", "\n", "\n", "for", "rank", ",", "model", "in", "enumerate", "(", "test_models", ")", ":", "\n", "        ", "if", "model", "in", "base_models", ":", "\n", "            ", "base_rank", "=", "int", "(", "np", ".", "where", "(", "base_models", "==", "model", ")", "[", "0", "]", ")", "\n", "top1_d", "=", "test_df", "[", "'top1'", "]", "[", "rank", "]", "-", "base_df", "[", "'top1'", "]", "[", "base_rank", "]", "\n", "top5_d", "=", "test_df", "[", "'top5'", "]", "[", "rank", "]", "-", "base_df", "[", "'top5'", "]", "[", "base_rank", "]", "\n", "\n", "# rank_diff", "\n", "if", "rank", "==", "base_rank", ":", "\n", "                ", "rank_diff", "[", "rank", "]", "=", "f'0'", "\n", "", "elif", "rank", ">", "base_rank", ":", "\n", "                ", "rank_diff", "[", "rank", "]", "=", "f'-{rank - base_rank}'", "\n", "", "else", ":", "\n", "                ", "rank_diff", "[", "rank", "]", "=", "f'+{base_rank - rank}'", "\n", "\n", "# top1_diff", "\n", "", "if", "top1_d", ">=", ".0", ":", "\n", "                ", "top1_diff", "[", "rank", "]", "=", "f'+{top1_d:.3f}'", "\n", "", "else", ":", "\n", "                ", "top1_diff", "[", "rank", "]", "=", "f'-{abs(top1_d):.3f}'", "\n", "\n", "# top5_diff", "\n", "", "if", "top5_d", ">=", ".0", ":", "\n", "                ", "top5_diff", "[", "rank", "]", "=", "f'+{top5_d:.3f}'", "\n", "", "else", ":", "\n", "                ", "top5_diff", "[", "rank", "]", "=", "f'-{abs(top5_d):.3f}'", "\n", "\n", "", "", "else", ":", "\n", "            ", "rank_diff", "[", "rank", "]", "=", "''", "\n", "top1_diff", "[", "rank", "]", "=", "''", "\n", "top5_diff", "[", "rank", "]", "=", "''", "\n", "\n", "", "", "test_df", "[", "'top1_diff'", "]", "=", "top1_diff", "\n", "test_df", "[", "'top5_diff'", "]", "=", "top5_diff", "\n", "test_df", "[", "'rank_diff'", "]", "=", "rank_diff", "\n", "\n", "test_df", "[", "'param_count'", "]", "=", "test_df", "[", "'param_count'", "]", ".", "map", "(", "'{:,.2f}'", ".", "format", ")", "\n", "test_df", ".", "sort_values", "(", "'top1'", ",", "ascending", "=", "False", ",", "inplace", "=", "True", ")", "\n", "test_df", ".", "to_csv", "(", "test_csv", ",", "index", "=", "False", ",", "float_format", "=", "'%.3f'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_models._get_input_size": [[58, 81], ["timm.get_pretrained_cfg_value", "timm.get_pretrained_cfg_value", "timm.get_pretrained_cfg_value", "default_cfg.get", "default_cfg.get", "tuple", "max", "max", "min"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.get_pretrained_cfg_value", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.get_pretrained_cfg_value", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.get_pretrained_cfg_value", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["def", "_get_input_size", "(", "model", "=", "None", ",", "model_name", "=", "''", ",", "target", "=", "None", ")", ":", "\n", "    ", "if", "model", "is", "None", ":", "\n", "        ", "assert", "model_name", ",", "\"One of model or model_name must be provided\"", "\n", "input_size", "=", "get_pretrained_cfg_value", "(", "model_name", ",", "'input_size'", ")", "\n", "fixed_input_size", "=", "get_pretrained_cfg_value", "(", "model_name", ",", "'fixed_input_size'", ")", "\n", "min_input_size", "=", "get_pretrained_cfg_value", "(", "model_name", ",", "'min_input_size'", ")", "\n", "", "else", ":", "\n", "        ", "default_cfg", "=", "model", ".", "default_cfg", "\n", "input_size", "=", "default_cfg", "[", "'input_size'", "]", "\n", "fixed_input_size", "=", "default_cfg", ".", "get", "(", "'fixed_input_size'", ",", "None", ")", "\n", "min_input_size", "=", "default_cfg", ".", "get", "(", "'min_input_size'", ",", "None", ")", "\n", "", "assert", "input_size", "is", "not", "None", "\n", "\n", "if", "fixed_input_size", ":", "\n", "        ", "return", "input_size", "\n", "\n", "", "if", "min_input_size", ":", "\n", "        ", "if", "target", "and", "max", "(", "input_size", ")", ">", "target", ":", "\n", "            ", "input_size", "=", "min_input_size", "\n", "", "", "else", ":", "\n", "        ", "if", "target", "and", "max", "(", "input_size", ")", ">", "target", ":", "\n", "            ", "input_size", "=", "tuple", "(", "[", "min", "(", "x", ",", "target", ")", "for", "x", "in", "input_size", "]", ")", "\n", "", "", "return", "input_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_models.test_model_forward": [[83, 99], ["pytest.mark.timeout", "pytest.mark.parametrize", "pytest.mark.parametrize", "timm.create_model", "timm.create_model.eval", "test_models._get_input_size", "torch.randn", "timm.create_model.", "timm.list_models", "max", "pytest.skip", "torch.isnan().any", "torch.isnan"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.create_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_models._get_input_size", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.list_models"], ["", "@", "pytest", ".", "mark", ".", "timeout", "(", "120", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'model_name'", ",", "list_models", "(", "exclude_filters", "=", "EXCLUDE_FILTERS", ")", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'batch_size'", ",", "[", "1", "]", ")", "\n", "def", "test_model_forward", "(", "model_name", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\"Run a single forward pass with each model\"\"\"", "\n", "model", "=", "create_model", "(", "model_name", ",", "pretrained", "=", "False", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "input_size", "=", "_get_input_size", "(", "model", "=", "model", ",", "target", "=", "TARGET_FWD_SIZE", ")", "\n", "if", "max", "(", "input_size", ")", ">", "MAX_FWD_SIZE", ":", "\n", "        ", "pytest", ".", "skip", "(", "\"Fixed input size model > limit.\"", ")", "\n", "", "inputs", "=", "torch", ".", "randn", "(", "(", "batch_size", ",", "*", "input_size", ")", ")", "\n", "outputs", "=", "model", "(", "inputs", ")", "\n", "\n", "assert", "outputs", ".", "shape", "[", "0", "]", "==", "batch_size", "\n", "assert", "not", "torch", ".", "isnan", "(", "outputs", ")", ".", "any", "(", ")", ",", "'Output included NaNs'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_models.test_model_backward": [[101, 126], ["pytest.mark.timeout", "pytest.mark.parametrize", "pytest.mark.parametrize", "test_models._get_input_size", "timm.create_model", "sum", "timm.create_model.train", "torch.randn", "timm.create_model.", "isinstance", "torch.cat.mean().backward", "timm.create_model.named_parameters", "sum", "timm.list_models", "max", "pytest.skip", "torch.cat", "torch.isnan().any", "x.numel", "torch.cat.mean", "x.grad.numel", "timm.create_model.parameters", "timm.create_model.parameters", "torch.isnan"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_models._get_input_size", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.create_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.AttentionSubsample.train", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardMishJitAutoFn.backward", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.list_models"], ["", "@", "pytest", ".", "mark", ".", "timeout", "(", "120", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'model_name'", ",", "list_models", "(", "exclude_filters", "=", "EXCLUDE_FILTERS", ",", "name_matches_cfg", "=", "True", ")", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'batch_size'", ",", "[", "2", "]", ")", "\n", "def", "test_model_backward", "(", "model_name", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\"Run a single forward pass with each model\"\"\"", "\n", "input_size", "=", "_get_input_size", "(", "model_name", "=", "model_name", ",", "target", "=", "TARGET_BWD_SIZE", ")", "\n", "if", "max", "(", "input_size", ")", ">", "MAX_BWD_SIZE", ":", "\n", "        ", "pytest", ".", "skip", "(", "\"Fixed input size model > limit.\"", ")", "\n", "\n", "", "model", "=", "create_model", "(", "model_name", ",", "pretrained", "=", "False", ",", "num_classes", "=", "42", ")", "\n", "num_params", "=", "sum", "(", "[", "x", ".", "numel", "(", ")", "for", "x", "in", "model", ".", "parameters", "(", ")", "]", ")", "\n", "model", ".", "train", "(", ")", "\n", "\n", "inputs", "=", "torch", ".", "randn", "(", "(", "batch_size", ",", "*", "input_size", ")", ")", "\n", "outputs", "=", "model", "(", "inputs", ")", "\n", "if", "isinstance", "(", "outputs", ",", "tuple", ")", ":", "\n", "        ", "outputs", "=", "torch", ".", "cat", "(", "outputs", ")", "\n", "", "outputs", ".", "mean", "(", ")", ".", "backward", "(", ")", "\n", "for", "n", ",", "x", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "        ", "assert", "x", ".", "grad", "is", "not", "None", ",", "f'No gradient for {n}'", "\n", "", "num_grad", "=", "sum", "(", "[", "x", ".", "grad", ".", "numel", "(", ")", "for", "x", "in", "model", ".", "parameters", "(", ")", "if", "x", ".", "grad", "is", "not", "None", "]", ")", "\n", "\n", "assert", "outputs", ".", "shape", "[", "-", "1", "]", "==", "42", "\n", "assert", "num_params", "==", "num_grad", ",", "'Some parameters are missing gradients'", "\n", "assert", "not", "torch", ".", "isnan", "(", "outputs", ")", ".", "any", "(", ")", ",", "'Output included NaNs'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_models.test_model_default_cfgs": [[128, 188], ["pytest.mark.timeout", "pytest.mark.parametrize", "pytest.mark.parametrize", "timm.create_model", "create_model().eval.eval", "create_model().eval.state_dict", "cfg.get", "isinstance", "isinstance", "timm.list_models", "all", "tuple", "torch.randn", "create_model().eval.forward_features", "create_model().eval.reset_classifier", "create_model().eval.forward", "create_model().eval.reset_classifier", "create_model().eval.forward", "any", "len", "len", "isinstance", "timm.create_model().eval", "create_model().eval.forward", "isinstance", "model.state_dict.keys", "min", "len", "isinstance", "model.state_dict.keys", "fnmatch.fnmatch", "timm.create_model"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.create_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.list_models", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.reset_classifier", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.forward", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.reset_classifier", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.forward", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.forward", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.create_model"], ["", "@", "pytest", ".", "mark", ".", "timeout", "(", "300", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'model_name'", ",", "list_models", "(", "exclude_filters", "=", "NON_STD_FILTERS", ")", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'batch_size'", ",", "[", "1", "]", ")", "\n", "def", "test_model_default_cfgs", "(", "model_name", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\"Run a single forward pass with each model\"\"\"", "\n", "model", "=", "create_model", "(", "model_name", ",", "pretrained", "=", "False", ")", "\n", "model", ".", "eval", "(", ")", "\n", "state_dict", "=", "model", ".", "state_dict", "(", ")", "\n", "cfg", "=", "model", ".", "default_cfg", "\n", "\n", "pool_size", "=", "cfg", "[", "'pool_size'", "]", "\n", "input_size", "=", "model", ".", "default_cfg", "[", "'input_size'", "]", "\n", "\n", "if", "all", "(", "[", "x", "<=", "MAX_FWD_OUT_SIZE", "for", "x", "in", "input_size", "]", ")", "and", "not", "any", "(", "[", "fnmatch", ".", "fnmatch", "(", "model_name", ",", "x", ")", "for", "x", "in", "EXCLUDE_FILTERS", "]", ")", ":", "\n", "# output sizes only checked if default res <= 448 * 448 to keep resource down", "\n", "        ", "input_size", "=", "tuple", "(", "[", "min", "(", "x", ",", "MAX_FWD_OUT_SIZE", ")", "for", "x", "in", "input_size", "]", ")", "\n", "input_tensor", "=", "torch", ".", "randn", "(", "(", "batch_size", ",", "*", "input_size", ")", ")", "\n", "\n", "# test forward_features (always unpooled)", "\n", "outputs", "=", "model", ".", "forward_features", "(", "input_tensor", ")", "\n", "assert", "outputs", ".", "shape", "[", "-", "1", "]", "==", "pool_size", "[", "-", "1", "]", "and", "outputs", ".", "shape", "[", "-", "2", "]", "==", "pool_size", "[", "-", "2", "]", ",", "'unpooled feature shape != config'", "\n", "\n", "# test forward after deleting the classifier, output should be poooled, size(-1) == model.num_features", "\n", "model", ".", "reset_classifier", "(", "0", ")", "\n", "outputs", "=", "model", ".", "forward", "(", "input_tensor", ")", "\n", "assert", "len", "(", "outputs", ".", "shape", ")", "==", "2", "\n", "assert", "outputs", ".", "shape", "[", "-", "1", "]", "==", "model", ".", "num_features", "\n", "\n", "# test model forward without pooling and classifier", "\n", "model", ".", "reset_classifier", "(", "0", ",", "''", ")", "# reset classifier and set global pooling to pass-through", "\n", "outputs", "=", "model", ".", "forward", "(", "input_tensor", ")", "\n", "assert", "len", "(", "outputs", ".", "shape", ")", "==", "4", "\n", "if", "not", "isinstance", "(", "model", ",", "(", "timm", ".", "models", ".", "MobileNetV3", ",", "timm", ".", "models", ".", "GhostNet", ",", "timm", ".", "models", ".", "VGG", ")", ")", ":", "\n", "# mobilenetv3/ghostnet/vgg forward_features vs removed pooling differ due to location or lack of GAP", "\n", "            ", "assert", "outputs", ".", "shape", "[", "-", "1", "]", "==", "pool_size", "[", "-", "1", "]", "and", "outputs", ".", "shape", "[", "-", "2", "]", "==", "pool_size", "[", "-", "2", "]", "\n", "\n", "", "if", "'pruned'", "not", "in", "model_name", ":", "# FIXME better pruned model handling", "\n", "# test classifier + global pool deletion via __init__", "\n", "            ", "model", "=", "create_model", "(", "model_name", ",", "pretrained", "=", "False", ",", "num_classes", "=", "0", ",", "global_pool", "=", "''", ")", ".", "eval", "(", ")", "\n", "outputs", "=", "model", ".", "forward", "(", "input_tensor", ")", "\n", "assert", "len", "(", "outputs", ".", "shape", ")", "==", "4", "\n", "if", "not", "isinstance", "(", "model", ",", "(", "timm", ".", "models", ".", "MobileNetV3", ",", "timm", ".", "models", ".", "GhostNet", ",", "timm", ".", "models", ".", "VGG", ")", ")", ":", "\n", "                ", "assert", "outputs", ".", "shape", "[", "-", "1", "]", "==", "pool_size", "[", "-", "1", "]", "and", "outputs", ".", "shape", "[", "-", "2", "]", "==", "pool_size", "[", "-", "2", "]", "\n", "\n", "# check classifier name matches default_cfg", "\n", "", "", "", "if", "cfg", ".", "get", "(", "'num_classes'", ",", "None", ")", ":", "\n", "        ", "classifier", "=", "cfg", "[", "'classifier'", "]", "\n", "if", "not", "isinstance", "(", "classifier", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "classifier", "=", "classifier", ",", "\n", "", "for", "c", "in", "classifier", ":", "\n", "            ", "assert", "c", "+", "\".weight\"", "in", "state_dict", ".", "keys", "(", ")", ",", "f'{c} not in model params'", "\n", "\n", "# check first conv(s) names match default_cfg", "\n", "", "", "first_conv", "=", "cfg", "[", "'first_conv'", "]", "\n", "if", "isinstance", "(", "first_conv", ",", "str", ")", ":", "\n", "        ", "first_conv", "=", "(", "first_conv", ",", ")", "\n", "", "assert", "isinstance", "(", "first_conv", ",", "(", "tuple", ",", "list", ")", ")", "\n", "for", "fc", "in", "first_conv", ":", "\n", "        ", "assert", "fc", "+", "\".weight\"", "in", "state_dict", ".", "keys", "(", ")", ",", "f'{fc} not in model params'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_models.test_model_default_cfgs_non_std": [[190, 248], ["pytest.mark.timeout", "pytest.mark.parametrize", "pytest.mark.parametrize", "timm.create_model", "create_model().eval.eval", "create_model().eval.state_dict", "test_models._get_input_size", "torch.randn", "getattr", "create_model().eval.forward_features", "isinstance", "create_model().eval.reset_classifier", "create_model().eval.forward", "isinstance", "timm.create_model().eval", "create_model().eval.forward", "isinstance", "cfg.get", "isinstance", "isinstance", "timm.list_models", "max", "pytest.skip", "timm.create_model", "isinstance", "model.state_dict.keys", "model.state_dict.keys"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.create_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_models._get_input_size", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.reset_classifier", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.forward", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.forward", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.list_models", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.create_model"], ["", "", "@", "pytest", ".", "mark", ".", "timeout", "(", "300", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'model_name'", ",", "list_models", "(", "filter", "=", "NON_STD_FILTERS", ",", "exclude_filters", "=", "NON_STD_EXCLUDE_FILTERS", ")", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'batch_size'", ",", "[", "1", "]", ")", "\n", "def", "test_model_default_cfgs_non_std", "(", "model_name", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\"Run a single forward pass with each model\"\"\"", "\n", "model", "=", "create_model", "(", "model_name", ",", "pretrained", "=", "False", ")", "\n", "model", ".", "eval", "(", ")", "\n", "state_dict", "=", "model", ".", "state_dict", "(", ")", "\n", "cfg", "=", "model", ".", "default_cfg", "\n", "\n", "input_size", "=", "_get_input_size", "(", "model", "=", "model", ")", "\n", "if", "max", "(", "input_size", ")", ">", "320", ":", "# FIXME const", "\n", "        ", "pytest", ".", "skip", "(", "\"Fixed input size model > limit.\"", ")", "\n", "\n", "", "input_tensor", "=", "torch", ".", "randn", "(", "(", "batch_size", ",", "*", "input_size", ")", ")", "\n", "feat_dim", "=", "getattr", "(", "model", ",", "'feature_dim'", ",", "None", ")", "\n", "\n", "outputs", "=", "model", ".", "forward_features", "(", "input_tensor", ")", "\n", "if", "isinstance", "(", "outputs", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "# cannot currently verify multi-tensor output.", "\n", "        ", "pass", "\n", "", "else", ":", "\n", "        ", "if", "feat_dim", "is", "None", ":", "\n", "            ", "feat_dim", "=", "-", "1", "if", "outputs", ".", "ndim", "==", "3", "else", "1", "\n", "", "assert", "outputs", ".", "shape", "[", "feat_dim", "]", "==", "model", ".", "num_features", "\n", "\n", "# test forward after deleting the classifier, output should be poooled, size(-1) == model.num_features", "\n", "", "model", ".", "reset_classifier", "(", "0", ")", "\n", "outputs", "=", "model", ".", "forward", "(", "input_tensor", ")", "\n", "if", "isinstance", "(", "outputs", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "outputs", "=", "outputs", "[", "0", "]", "\n", "", "if", "feat_dim", "is", "None", ":", "\n", "        ", "feat_dim", "=", "-", "1", "if", "outputs", ".", "ndim", "==", "3", "else", "1", "\n", "", "assert", "outputs", ".", "shape", "[", "feat_dim", "]", "==", "model", ".", "num_features", ",", "'pooled num_features != config'", "\n", "\n", "model", "=", "create_model", "(", "model_name", ",", "pretrained", "=", "False", ",", "num_classes", "=", "0", ")", ".", "eval", "(", ")", "\n", "outputs", "=", "model", ".", "forward", "(", "input_tensor", ")", "\n", "if", "isinstance", "(", "outputs", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "outputs", "=", "outputs", "[", "0", "]", "\n", "", "if", "feat_dim", "is", "None", ":", "\n", "        ", "feat_dim", "=", "-", "1", "if", "outputs", ".", "ndim", "==", "3", "else", "1", "\n", "", "assert", "outputs", ".", "shape", "[", "feat_dim", "]", "==", "model", ".", "num_features", "\n", "\n", "# check classifier name matches default_cfg", "\n", "if", "cfg", ".", "get", "(", "'num_classes'", ",", "None", ")", ":", "\n", "        ", "classifier", "=", "cfg", "[", "'classifier'", "]", "\n", "if", "not", "isinstance", "(", "classifier", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "classifier", "=", "classifier", ",", "\n", "", "for", "c", "in", "classifier", ":", "\n", "            ", "assert", "c", "+", "\".weight\"", "in", "state_dict", ".", "keys", "(", ")", ",", "f'{c} not in model params'", "\n", "\n", "# check first conv(s) names match default_cfg", "\n", "", "", "first_conv", "=", "cfg", "[", "'first_conv'", "]", "\n", "if", "isinstance", "(", "first_conv", ",", "str", ")", ":", "\n", "        ", "first_conv", "=", "(", "first_conv", ",", ")", "\n", "", "assert", "isinstance", "(", "first_conv", ",", "(", "tuple", ",", "list", ")", ")", "\n", "for", "fc", "in", "first_conv", ":", "\n", "        ", "assert", "fc", "+", "\".weight\"", "in", "state_dict", ".", "keys", "(", ")", ",", "f'{fc} not in model params'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_models.test_model_forward_torchscript": [[274, 293], ["pytest.mark.timeout", "pytest.mark.parametrize", "pytest.mark.parametrize", "test_models._get_input_size", "timm.create_model.eval", "torch.jit.script", "timm.create_model.", "timm.list_models", "max", "pytest.skip", "timm.set_scriptable", "timm.create_model", "torch.randn", "torch.isnan().any", "torch.isnan"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_models._get_input_size", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.list_models", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.create_model"], ["@", "pytest", ".", "mark", ".", "timeout", "(", "120", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\n", "'model_name'", ",", "list_models", "(", "exclude_filters", "=", "EXCLUDE_FILTERS", "+", "EXCLUDE_JIT_FILTERS", ",", "name_matches_cfg", "=", "True", ")", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'batch_size'", ",", "[", "1", "]", ")", "\n", "def", "test_model_forward_torchscript", "(", "model_name", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\"Run a single forward pass with each model\"\"\"", "\n", "input_size", "=", "_get_input_size", "(", "model_name", "=", "model_name", ",", "target", "=", "TARGET_JIT_SIZE", ")", "\n", "if", "max", "(", "input_size", ")", ">", "MAX_JIT_SIZE", ":", "\n", "        ", "pytest", ".", "skip", "(", "\"Fixed input size model > limit.\"", ")", "\n", "\n", "", "with", "set_scriptable", "(", "True", ")", ":", "\n", "        ", "model", "=", "create_model", "(", "model_name", ",", "pretrained", "=", "False", ")", "\n", "", "model", ".", "eval", "(", ")", "\n", "\n", "model", "=", "torch", ".", "jit", ".", "script", "(", "model", ")", "\n", "outputs", "=", "model", "(", "torch", ".", "randn", "(", "(", "batch_size", ",", "*", "input_size", ")", ")", ")", "\n", "\n", "assert", "outputs", ".", "shape", "[", "0", "]", "==", "batch_size", "\n", "assert", "not", "torch", ".", "isnan", "(", "outputs", ")", ".", "any", "(", ")", ",", "'Output included NaNs'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_models.test_model_forward_features": [[303, 323], ["pytest.mark.timeout", "pytest.mark.parametrize", "pytest.mark.parametrize", "timm.create_model", "timm.create_model.eval", "timm.create_model.feature_info.channels", "test_models._get_input_size", "timm.create_model.", "zip", "timm.list_models", "len", "max", "pytest.skip", "torch.randn", "len", "len", "torch.isnan().any", "torch.isnan"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.create_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.channels", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_models._get_input_size", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.list_models"], ["", "@", "pytest", ".", "mark", ".", "timeout", "(", "120", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'model_name'", ",", "list_models", "(", "exclude_filters", "=", "EXCLUDE_FILTERS", "+", "EXCLUDE_FEAT_FILTERS", ")", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'batch_size'", ",", "[", "1", "]", ")", "\n", "def", "test_model_forward_features", "(", "model_name", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\"Run a single forward pass with each model in feature extraction mode\"\"\"", "\n", "model", "=", "create_model", "(", "model_name", ",", "pretrained", "=", "False", ",", "features_only", "=", "True", ")", "\n", "model", ".", "eval", "(", ")", "\n", "expected_channels", "=", "model", ".", "feature_info", ".", "channels", "(", ")", "\n", "assert", "len", "(", "expected_channels", ")", ">=", "4", "# all models here should have at least 4 feature levels by default, some 5 or 6", "\n", "\n", "input_size", "=", "_get_input_size", "(", "model", "=", "model", ",", "target", "=", "TARGET_FFEAT_SIZE", ")", "\n", "if", "max", "(", "input_size", ")", ">", "MAX_FFEAT_SIZE", ":", "\n", "        ", "pytest", ".", "skip", "(", "\"Fixed input size model > limit.\"", ")", "\n", "\n", "", "outputs", "=", "model", "(", "torch", ".", "randn", "(", "(", "batch_size", ",", "*", "input_size", ")", ")", ")", "\n", "assert", "len", "(", "expected_channels", ")", "==", "len", "(", "outputs", ")", "\n", "for", "e", ",", "o", "in", "zip", "(", "expected_channels", ",", "outputs", ")", ":", "\n", "        ", "assert", "e", "==", "o", ".", "shape", "[", "1", "]", "\n", "assert", "o", ".", "shape", "[", "0", "]", "==", "batch_size", "\n", "assert", "not", "torch", ".", "isnan", "(", "o", ")", ".", "any", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_models._create_fx_model": [[325, 355], ["dict", "get_graph_node_names", "create_feature_extractor", "NodePathTracer", "NodePathTracer.trace", "list", "list", "list", "reversed", "graph_nodes[]._input_nodes.keys", "graph_node_names.index"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.fx_features.create_feature_extractor"], ["", "", "def", "_create_fx_model", "(", "model", ",", "train", "=", "False", ")", ":", "\n", "# This block of code does a bit of juggling to handle any case where there are multiple outputs in train mode", "\n", "# So we trace once and look at the graph, and get the indices of the nodes that lead into the original fx output", "\n", "# node. Then we use those indices to select from train_nodes returned by torchvision get_graph_node_names", "\n", "    ", "tracer_kwargs", "=", "dict", "(", "\n", "leaf_modules", "=", "list", "(", "_leaf_modules", ")", ",", "\n", "autowrap_functions", "=", "list", "(", "_autowrap_functions", ")", ",", "\n", "#enable_cpatching=True,", "\n", "param_shapes_constant", "=", "True", "\n", ")", "\n", "train_nodes", ",", "eval_nodes", "=", "get_graph_node_names", "(", "model", ",", "tracer_kwargs", "=", "tracer_kwargs", ")", "\n", "\n", "eval_return_nodes", "=", "[", "eval_nodes", "[", "-", "1", "]", "]", "\n", "train_return_nodes", "=", "[", "train_nodes", "[", "-", "1", "]", "]", "\n", "if", "train", ":", "\n", "        ", "tracer", "=", "NodePathTracer", "(", "**", "tracer_kwargs", ")", "\n", "graph", "=", "tracer", ".", "trace", "(", "model", ")", "\n", "graph_nodes", "=", "list", "(", "reversed", "(", "graph", ".", "nodes", ")", ")", "\n", "output_node_names", "=", "[", "n", ".", "name", "for", "n", "in", "graph_nodes", "[", "0", "]", ".", "_input_nodes", ".", "keys", "(", ")", "]", "\n", "graph_node_names", "=", "[", "n", ".", "name", "for", "n", "in", "graph_nodes", "]", "\n", "output_node_indices", "=", "[", "-", "graph_node_names", ".", "index", "(", "node_name", ")", "for", "node_name", "in", "output_node_names", "]", "\n", "train_return_nodes", "=", "[", "train_nodes", "[", "ix", "]", "for", "ix", "in", "output_node_indices", "]", "\n", "\n", "", "fx_model", "=", "create_feature_extractor", "(", "\n", "model", ",", "\n", "train_return_nodes", "=", "train_return_nodes", ",", "\n", "eval_return_nodes", "=", "eval_return_nodes", ",", "\n", "tracer_kwargs", "=", "tracer_kwargs", ",", "\n", ")", "\n", "return", "fx_model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_models.test_model_forward_fx": [[376, 407], ["pytest.mark.timeout", "pytest.mark.parametrize", "pytest.mark.parametrize", "timm.create_model", "_create_fx_model.eval", "test_models._get_input_size", "torch.all", "timm.list_models", "pytest.skip", "max", "pytest.skip", "torch.no_grad", "torch.randn", "_create_fx_model.", "isinstance", "test_models._create_fx_model", "tuple", "isinstance", "torch.isnan().any", "torch.cat", "_create_fx_model.values", "torch.cat", "torch.isnan", "_create_fx_model."], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.create_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_models._get_input_size", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.list_models", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_models._create_fx_model"], ["", "@", "pytest", ".", "mark", ".", "timeout", "(", "120", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'model_name'", ",", "list_models", "(", "exclude_filters", "=", "EXCLUDE_FILTERS", "+", "EXCLUDE_FX_FILTERS", ")", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'batch_size'", ",", "[", "1", "]", ")", "\n", "def", "test_model_forward_fx", "(", "model_name", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\"\n    Symbolically trace each model and run single forward pass through the resulting GraphModule\n    Also check that the output of a forward pass through the GraphModule is the same as that from the original Module\n    \"\"\"", "\n", "if", "not", "has_fx_feature_extraction", ":", "\n", "        ", "pytest", ".", "skip", "(", "\"Can't test FX. Torch >= 1.10 and Torchvision >= 0.11 are required.\"", ")", "\n", "\n", "", "model", "=", "create_model", "(", "model_name", ",", "pretrained", "=", "False", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "input_size", "=", "_get_input_size", "(", "model", "=", "model", ",", "target", "=", "TARGET_FWD_FX_SIZE", ")", "\n", "if", "max", "(", "input_size", ")", ">", "MAX_FWD_FX_SIZE", ":", "\n", "        ", "pytest", ".", "skip", "(", "\"Fixed input size model > limit.\"", ")", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "inputs", "=", "torch", ".", "randn", "(", "(", "batch_size", ",", "*", "input_size", ")", ")", "\n", "outputs", "=", "model", "(", "inputs", ")", "\n", "if", "isinstance", "(", "outputs", ",", "tuple", ")", ":", "\n", "            ", "outputs", "=", "torch", ".", "cat", "(", "outputs", ")", "\n", "\n", "", "model", "=", "_create_fx_model", "(", "model", ")", "\n", "fx_outputs", "=", "tuple", "(", "model", "(", "inputs", ")", ".", "values", "(", ")", ")", "\n", "if", "isinstance", "(", "fx_outputs", ",", "tuple", ")", ":", "\n", "            ", "fx_outputs", "=", "torch", ".", "cat", "(", "fx_outputs", ")", "\n", "\n", "", "", "assert", "torch", ".", "all", "(", "fx_outputs", "==", "outputs", ")", "\n", "assert", "outputs", ".", "shape", "[", "0", "]", "==", "batch_size", "\n", "assert", "not", "torch", ".", "isnan", "(", "outputs", ")", ".", "any", "(", ")", ",", "'Output included NaNs'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_models.test_model_backward_fx": [[409, 440], ["pytest.mark.timeout", "pytest.mark.parametrize", "pytest.mark.parametrize", "test_models._get_input_size", "timm.create_model", "_create_fx_model.train", "sum", "test_models._create_fx_model", "tuple", "isinstance", "torch.cat.mean().backward", "_create_fx_model.named_parameters", "sum", "timm.list_models", "pytest.skip", "max", "pytest.skip", "pytest.skip", "_create_fx_model.values", "torch.cat", "torch.isnan().any", "x.numel", "torch.cat.mean", "x.grad.numel", "_create_fx_model.parameters", "_create_fx_model.", "_create_fx_model.parameters", "torch.isnan", "torch.randn"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_models._get_input_size", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.create_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.AttentionSubsample.train", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_models._create_fx_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardMishJitAutoFn.backward", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.list_models"], ["", "@", "pytest", ".", "mark", ".", "timeout", "(", "120", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'model_name'", ",", "list_models", "(", "\n", "exclude_filters", "=", "EXCLUDE_FILTERS", "+", "EXCLUDE_FX_FILTERS", ",", "name_matches_cfg", "=", "True", ")", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'batch_size'", ",", "[", "2", "]", ")", "\n", "def", "test_model_backward_fx", "(", "model_name", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\"Symbolically trace each model and run single backward pass through the resulting GraphModule\"\"\"", "\n", "if", "not", "has_fx_feature_extraction", ":", "\n", "        ", "pytest", ".", "skip", "(", "\"Can't test FX. Torch >= 1.10 and Torchvision >= 0.11 are required.\"", ")", "\n", "\n", "", "input_size", "=", "_get_input_size", "(", "model_name", "=", "model_name", ",", "target", "=", "TARGET_BWD_FX_SIZE", ")", "\n", "if", "max", "(", "input_size", ")", ">", "MAX_BWD_FX_SIZE", ":", "\n", "        ", "pytest", ".", "skip", "(", "\"Fixed input size model > limit.\"", ")", "\n", "\n", "", "model", "=", "create_model", "(", "model_name", ",", "pretrained", "=", "False", ",", "num_classes", "=", "42", ")", "\n", "model", ".", "train", "(", ")", "\n", "num_params", "=", "sum", "(", "[", "x", ".", "numel", "(", ")", "for", "x", "in", "model", ".", "parameters", "(", ")", "]", ")", "\n", "if", "'GITHUB_ACTIONS'", "in", "os", ".", "environ", "and", "num_params", ">", "100e6", ":", "\n", "        ", "pytest", ".", "skip", "(", "\"Skipping FX backward test on model with more than 100M params.\"", ")", "\n", "\n", "", "model", "=", "_create_fx_model", "(", "model", ",", "train", "=", "True", ")", "\n", "outputs", "=", "tuple", "(", "model", "(", "torch", ".", "randn", "(", "(", "batch_size", ",", "*", "input_size", ")", ")", ")", ".", "values", "(", ")", ")", "\n", "if", "isinstance", "(", "outputs", ",", "tuple", ")", ":", "\n", "        ", "outputs", "=", "torch", ".", "cat", "(", "outputs", ")", "\n", "", "outputs", ".", "mean", "(", ")", ".", "backward", "(", ")", "\n", "for", "n", ",", "x", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "        ", "assert", "x", ".", "grad", "is", "not", "None", ",", "f'No gradient for {n}'", "\n", "", "num_grad", "=", "sum", "(", "[", "x", ".", "grad", ".", "numel", "(", ")", "for", "x", "in", "model", ".", "parameters", "(", ")", "if", "x", ".", "grad", "is", "not", "None", "]", ")", "\n", "\n", "assert", "outputs", ".", "shape", "[", "-", "1", "]", "==", "42", "\n", "assert", "num_params", "==", "num_grad", ",", "'Some parameters are missing gradients'", "\n", "assert", "not", "torch", ".", "isnan", "(", "outputs", ")", ".", "any", "(", ")", ",", "'Output included NaNs'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases_template": [[23, 55], ["torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "constructor", "constructor.__repr__", "fn().item", "range", "schedulers.append", "constructor.zero_grad", "torch.autograd.Variable.mv", "loss.backward", "constructor.step", "fn().item", "scheduler_constructor", "y.cuda.cuda", "test_optim._test_basic_cases_template.fn"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.AugMixAugment.__repr__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardMishJitAutoFn.backward", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor.step"], ["def", "_test_basic_cases_template", "(", "weight", ",", "bias", ",", "input", ",", "constructor", ",", "scheduler_constructors", ")", ":", "\n", "    ", "weight", "=", "Variable", "(", "weight", ",", "requires_grad", "=", "True", ")", "\n", "bias", "=", "Variable", "(", "bias", ",", "requires_grad", "=", "True", ")", "\n", "input", "=", "Variable", "(", "input", ")", "\n", "optimizer", "=", "constructor", "(", "weight", ",", "bias", ")", "\n", "schedulers", "=", "[", "]", "\n", "for", "scheduler_constructor", "in", "scheduler_constructors", ":", "\n", "        ", "schedulers", ".", "append", "(", "scheduler_constructor", "(", "optimizer", ")", ")", "\n", "\n", "# to check if the optimizer can be printed as a string", "\n", "", "optimizer", ".", "__repr__", "(", ")", "\n", "\n", "def", "fn", "(", ")", ":", "\n", "        ", "optimizer", ".", "zero_grad", "(", ")", "\n", "y", "=", "weight", ".", "mv", "(", "input", ")", "\n", "if", "y", ".", "is_cuda", "and", "bias", ".", "is_cuda", "and", "y", ".", "get_device", "(", ")", "!=", "bias", ".", "get_device", "(", ")", ":", "\n", "            ", "y", "=", "y", ".", "cuda", "(", "bias", ".", "get_device", "(", ")", ")", "\n", "", "loss", "=", "(", "y", "+", "bias", ")", ".", "pow", "(", "2", ")", ".", "sum", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "return", "loss", "\n", "\n", "", "initial_value", "=", "fn", "(", ")", ".", "item", "(", ")", "\n", "for", "_i", "in", "range", "(", "200", ")", ":", "\n", "        ", "for", "scheduler", "in", "schedulers", ":", "\n", "            ", "if", "isinstance", "(", "scheduler", ",", "PlateauLRScheduler", ")", ":", "\n", "                ", "val_loss", "=", "fn", "(", ")", "\n", "scheduler", ".", "step", "(", "val_loss", ")", "\n", "", "else", ":", "\n", "                ", "scheduler", ".", "step", "(", ")", "\n", "", "", "optimizer", ".", "step", "(", "fn", ")", "\n", "\n", "", "assert", "fn", "(", ")", ".", "item", "(", ")", "<", "initial_value", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_state_dict": [[57, 130], ["torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "constructor", "functools.partial", "range", "torch.autograd.Variable", "torch.autograd.Variable", "constructor", "functools.partial", "copy.deepcopy", "copy.deepcopy", "constructor.load_state_dict", "range", "torch_tc.assertEqual", "torch_tc.assertEqual", "constructor.param_groups.extend", "torch_tc.assertEqual", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "constructor", "functools.partial", "copy.deepcopy", "copy.deepcopy", "constructor.load_state_dict", "torch_tc.assertEqual", "range", "constructor.zero_grad", "loss.backward", "constructor.step", "torch.autograd.Variable.data.clone", "torch.autograd.Variable.data.clone", "constructor.state_dict", "constructor.state_dict", "constructor.step", "constructor.step", "torch_tc.assertEqual", "torch_tc.assertEqual", "constructor.state_dict", "constructor.state_dict", "torch.cuda.is_available", "torch.autograd.Variable.data.float().cuda", "torch.autograd.Variable.data.float().cuda", "torch.autograd.Variable.data.float().cuda", "constructor.state_dict", "constructor.state_dict", "constructor.step", "constructor.step", "torch_tc.assertEqual", "torch_tc.assertEqual", "set", "test_optim._test_state_dict.getPublicAttr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardMishJitAutoFn.backward", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEmaV2.set"], ["", "def", "_test_state_dict", "(", "weight", ",", "bias", ",", "input", ",", "constructor", ")", ":", "\n", "    ", "weight", "=", "Variable", "(", "weight", ",", "requires_grad", "=", "True", ")", "\n", "bias", "=", "Variable", "(", "bias", ",", "requires_grad", "=", "True", ")", "\n", "input", "=", "Variable", "(", "input", ")", "\n", "\n", "def", "fn_base", "(", "optimizer", ",", "weight", ",", "bias", ")", ":", "\n", "        ", "optimizer", ".", "zero_grad", "(", ")", "\n", "i", "=", "input_cuda", "if", "weight", ".", "is_cuda", "else", "input", "\n", "loss", "=", "(", "weight", ".", "mv", "(", "i", ")", "+", "bias", ")", ".", "pow", "(", "2", ")", ".", "sum", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "return", "loss", "\n", "\n", "", "optimizer", "=", "constructor", "(", "weight", ",", "bias", ")", "\n", "fn", "=", "functools", ".", "partial", "(", "fn_base", ",", "optimizer", ",", "weight", ",", "bias", ")", "\n", "\n", "# Prime the optimizer", "\n", "for", "_i", "in", "range", "(", "20", ")", ":", "\n", "        ", "optimizer", ".", "step", "(", "fn", ")", "\n", "# Clone the weights and construct new optimizer for them", "\n", "", "weight_c", "=", "Variable", "(", "weight", ".", "data", ".", "clone", "(", ")", ",", "requires_grad", "=", "True", ")", "\n", "bias_c", "=", "Variable", "(", "bias", ".", "data", ".", "clone", "(", ")", ",", "requires_grad", "=", "True", ")", "\n", "optimizer_c", "=", "constructor", "(", "weight_c", ",", "bias_c", ")", "\n", "fn_c", "=", "functools", ".", "partial", "(", "fn_base", ",", "optimizer_c", ",", "weight_c", ",", "bias_c", ")", "\n", "# Load state dict", "\n", "state_dict", "=", "deepcopy", "(", "optimizer", ".", "state_dict", "(", ")", ")", "\n", "state_dict_c", "=", "deepcopy", "(", "optimizer", ".", "state_dict", "(", ")", ")", "\n", "optimizer_c", ".", "load_state_dict", "(", "state_dict_c", ")", "\n", "\n", "# Run both optimizations in parallel", "\n", "for", "_i", "in", "range", "(", "20", ")", ":", "\n", "        ", "optimizer", ".", "step", "(", "fn", ")", "\n", "optimizer_c", ".", "step", "(", "fn_c", ")", "\n", "#assert torch.equal(weight, weight_c)", "\n", "#assert torch.equal(bias, bias_c)", "\n", "torch_tc", ".", "assertEqual", "(", "weight", ",", "weight_c", ")", "\n", "torch_tc", ".", "assertEqual", "(", "bias", ",", "bias_c", ")", "\n", "# Make sure state dict wasn't modified", "\n", "", "torch_tc", ".", "assertEqual", "(", "state_dict", ",", "state_dict_c", ")", "\n", "# Make sure state dict is deterministic with equal but not identical parameters", "\n", "torch_tc", ".", "assertEqual", "(", "optimizer", ".", "state_dict", "(", ")", ",", "optimizer_c", ".", "state_dict", "(", ")", ")", "\n", "# Make sure repeated parameters have identical representation in state dict", "\n", "optimizer_c", ".", "param_groups", ".", "extend", "(", "optimizer_c", ".", "param_groups", ")", "\n", "torch_tc", ".", "assertEqual", "(", "optimizer", ".", "state_dict", "(", ")", "[", "'param_groups'", "]", "[", "-", "1", "]", ",", "optimizer_c", ".", "state_dict", "(", ")", "[", "'param_groups'", "]", "[", "-", "1", "]", ")", "\n", "\n", "# Check that state dict can be loaded even when we cast parameters", "\n", "# to a different type and move to a different device.", "\n", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "return", "\n", "\n", "", "input_cuda", "=", "Variable", "(", "input", ".", "data", ".", "float", "(", ")", ".", "cuda", "(", ")", ")", "\n", "weight_cuda", "=", "Variable", "(", "weight", ".", "data", ".", "float", "(", ")", ".", "cuda", "(", ")", ",", "requires_grad", "=", "True", ")", "\n", "bias_cuda", "=", "Variable", "(", "bias", ".", "data", ".", "float", "(", ")", ".", "cuda", "(", ")", ",", "requires_grad", "=", "True", ")", "\n", "optimizer_cuda", "=", "constructor", "(", "weight_cuda", ",", "bias_cuda", ")", "\n", "fn_cuda", "=", "functools", ".", "partial", "(", "fn_base", ",", "optimizer_cuda", ",", "weight_cuda", ",", "bias_cuda", ")", "\n", "\n", "state_dict", "=", "deepcopy", "(", "optimizer", ".", "state_dict", "(", ")", ")", "\n", "state_dict_c", "=", "deepcopy", "(", "optimizer", ".", "state_dict", "(", ")", ")", "\n", "optimizer_cuda", ".", "load_state_dict", "(", "state_dict_c", ")", "\n", "\n", "# Make sure state dict wasn't modified", "\n", "torch_tc", ".", "assertEqual", "(", "state_dict", ",", "state_dict_c", ")", "\n", "\n", "for", "_i", "in", "range", "(", "20", ")", ":", "\n", "        ", "optimizer", ".", "step", "(", "fn", ")", "\n", "optimizer_cuda", ".", "step", "(", "fn_cuda", ")", "\n", "torch_tc", ".", "assertEqual", "(", "weight", ",", "weight_cuda", ")", "\n", "torch_tc", ".", "assertEqual", "(", "bias", ",", "bias_cuda", ")", "\n", "\n", "# validate deepcopy() copies all public attributes", "\n", "", "def", "getPublicAttr", "(", "obj", ")", ":", "\n", "        ", "return", "set", "(", "k", "for", "k", "in", "obj", ".", "__dict__", "if", "not", "k", ".", "startswith", "(", "'_'", ")", ")", "\n", "\n", "", "assert", "getPublicAttr", "(", "optimizer", ")", "==", "getPublicAttr", "(", "deepcopy", "(", "optimizer", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases": [[132, 165], ["test_optim._test_state_dict", "test_optim._test_basic_cases_template", "test_optim._test_basic_cases_template", "test_optim._test_basic_cases_template", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.cuda.is_available", "torch.randn().cuda", "torch.randn().cuda", "torch.randn().cuda", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases_template", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases_template", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases_template"], ["", "def", "_test_basic_cases", "(", "constructor", ",", "scheduler_constructors", "=", "None", ")", ":", "\n", "    ", "if", "scheduler_constructors", "is", "None", ":", "\n", "        ", "scheduler_constructors", "=", "[", "]", "\n", "", "_test_state_dict", "(", "\n", "torch", ".", "randn", "(", "10", ",", "5", ")", ",", "\n", "torch", ".", "randn", "(", "10", ")", ",", "\n", "torch", ".", "randn", "(", "5", ")", ",", "\n", "constructor", "\n", ")", "\n", "_test_basic_cases_template", "(", "\n", "torch", ".", "randn", "(", "10", ",", "5", ")", ",", "\n", "torch", ".", "randn", "(", "10", ")", ",", "\n", "torch", ".", "randn", "(", "5", ")", ",", "\n", "constructor", ",", "\n", "scheduler_constructors", "\n", ")", "\n", "# non-contiguous parameters", "\n", "_test_basic_cases_template", "(", "\n", "torch", ".", "randn", "(", "10", ",", "5", ",", "2", ")", "[", "...", ",", "0", "]", ",", "\n", "torch", ".", "randn", "(", "10", ",", "2", ")", "[", "...", ",", "0", "]", ",", "\n", "torch", ".", "randn", "(", "5", ")", ",", "\n", "constructor", ",", "\n", "scheduler_constructors", "\n", ")", "\n", "# CUDA", "\n", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "return", "\n", "", "_test_basic_cases_template", "(", "\n", "torch", ".", "randn", "(", "10", ",", "5", ")", ".", "cuda", "(", ")", ",", "\n", "torch", ".", "randn", "(", "10", ")", ".", "cuda", "(", ")", ",", "\n", "torch", ".", "randn", "(", "5", ")", ".", "cuda", "(", ")", ",", "\n", "constructor", ",", "\n", "scheduler_constructors", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_model": [[168, 202], ["torch.device", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor().reshape", "torch.nn.Sequential", "torch.nn.Sequential.to", "torch.nn.Sequential.state_dict", "torch.nn.Sequential.load_state_dict", "timm.optim.create_optimizer_v2", "float", "range", "torch.nn.Linear", "torch.nn.Sigmoid", "torch.nn.Linear", "torch.nn.Sigmoid", "timm.optim.create_optimizer_v2.zero_grad", "torch.nn.Sequential.", "model.sum", "loss.item.backward", "loss.item.item", "timm.optim.create_optimizer_v2.step", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardMishJitAutoFn.backward", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor.step"], ["", "def", "_test_model", "(", "optimizer", ",", "params", ",", "device", "=", "torch", ".", "device", "(", "'cpu'", ")", ")", ":", "\n", "    ", "weight", "=", "torch", ".", "tensor", "(", "\n", "[", "[", "-", "0.2109", ",", "-", "0.4976", "]", ",", "[", "-", "0.1413", ",", "-", "0.3420", "]", ",", "[", "-", "0.2524", ",", "0.6976", "]", "]", ",", "\n", "device", "=", "device", ",", "requires_grad", "=", "True", ")", "\n", "bias", "=", "torch", ".", "tensor", "(", "[", "-", "0.1085", ",", "-", "0.2979", ",", "0.6892", "]", ",", "device", "=", "device", ",", "requires_grad", "=", "True", ")", "\n", "weight2", "=", "torch", ".", "tensor", "(", "[", "[", "-", "0.0508", ",", "-", "0.3941", ",", "-", "0.2843", "]", "]", ",", "device", "=", "device", ",", "requires_grad", "=", "True", ")", "\n", "bias2", "=", "torch", ".", "tensor", "(", "[", "-", "0.0711", "]", ",", "device", "=", "device", ",", "requires_grad", "=", "True", ")", "\n", "input", "=", "torch", ".", "tensor", "(", "[", "0.1", ",", "0.2", ",", "0.3", ",", "0.4", ",", "0.5", ",", "0.6", "]", ",", "device", "=", "device", ")", ".", "reshape", "(", "3", ",", "2", ")", "\n", "\n", "model", "=", "torch", ".", "nn", ".", "Sequential", "(", "torch", ".", "nn", ".", "Linear", "(", "2", ",", "3", ")", ",", "\n", "torch", ".", "nn", ".", "Sigmoid", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "3", ",", "1", ")", ",", "\n", "torch", ".", "nn", ".", "Sigmoid", "(", ")", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "pretrained_dict", "=", "model", ".", "state_dict", "(", ")", "\n", "pretrained_dict", "[", "'0.weight'", "]", "=", "weight", "\n", "pretrained_dict", "[", "'0.bias'", "]", "=", "bias", "\n", "pretrained_dict", "[", "'2.weight'", "]", "=", "weight2", "\n", "pretrained_dict", "[", "'2.bias'", "]", "=", "bias2", "\n", "model", ".", "load_state_dict", "(", "pretrained_dict", ")", "\n", "\n", "optimizer", "=", "create_optimizer_v2", "(", "model", ",", "opt", "=", "optimizer", ",", "**", "params", ")", "\n", "\n", "prev_loss", "=", "float", "(", "'inf'", ")", "\n", "for", "i", "in", "range", "(", "20", ")", ":", "\n", "        ", "optimizer", ".", "zero_grad", "(", ")", "\n", "output", "=", "model", "(", "input", ")", "\n", "loss", "=", "output", ".", "sum", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "loss", "=", "loss", ".", "item", "(", ")", "\n", "assert", "loss", "<", "prev_loss", "\n", "prev_loss", "=", "loss", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.rosenbrock": [[204, 207], ["None"], "function", ["None"], ["", "", "def", "rosenbrock", "(", "tensor", ")", ":", "\n", "    ", "x", ",", "y", "=", "tensor", "\n", "return", "(", "1", "-", "x", ")", "**", "2", "+", "100", "*", "(", "y", "-", "x", "**", "2", ")", "**", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.drosenbrock": [[209, 212], ["torch.tensor"], "function", ["None"], ["", "def", "drosenbrock", "(", "tensor", ")", ":", "\n", "    ", "x", ",", "y", "=", "tensor", "\n", "return", "torch", ".", "tensor", "(", "(", "-", "400", "*", "x", "*", "(", "y", "-", "x", "**", "2", ")", "-", "2", "*", "(", "1", "-", "x", ")", ",", "200", "*", "(", "y", "-", "x", "**", "2", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_rosenbrock": [[214, 260], ["torch.tensor", "torch.autograd.Variable", "constructor", "torch.tensor", "torch.autograd.Variable.data.dist", "range", "torch_tc.assertLessEqual", "schedulers.append", "constructor.zero_grad", "test_optim.rosenbrock", "rosenbrock.backward", "test_optim.drosenbrock", "torch.sparse.DoubleTensor().to", "constructor.step", "torch.autograd.Variable.data.dist", "scheduler_constructor", "torch.LongTensor", "torch.tensor", "torch.LongTensor", "torch.tensor", "torch.no_grad", "torch.sparse.DoubleTensor().to.to_dense", "functools.partial", "isinstance", "torch.sparse.DoubleTensor", "scheduler.step", "scheduler.step", "torch.Size", "test_optim.rosenbrock"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.rosenbrock", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardMishJitAutoFn.backward", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.drosenbrock", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.rosenbrock"], ["", "def", "_test_rosenbrock", "(", "constructor", ",", "scheduler_constructors", "=", "None", ")", ":", "\n", "    ", "if", "scheduler_constructors", "is", "None", ":", "\n", "        ", "scheduler_constructors", "=", "[", "]", "\n", "", "params_t", "=", "torch", ".", "tensor", "(", "[", "1.5", ",", "1.5", "]", ")", "\n", "\n", "params", "=", "Variable", "(", "params_t", ",", "requires_grad", "=", "True", ")", "\n", "optimizer", "=", "constructor", "(", "[", "params", "]", ")", "\n", "schedulers", "=", "[", "]", "\n", "for", "scheduler_constructor", "in", "scheduler_constructors", ":", "\n", "        ", "schedulers", ".", "append", "(", "scheduler_constructor", "(", "optimizer", ")", ")", "\n", "\n", "", "solution", "=", "torch", ".", "tensor", "(", "[", "1", ",", "1", "]", ")", "\n", "initial_dist", "=", "params", ".", "data", ".", "dist", "(", "solution", ")", "\n", "\n", "def", "eval", "(", "params", ",", "w", ")", ":", "\n", "# Depending on w, provide only the x or y gradient", "\n", "        ", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", "=", "rosenbrock", "(", "params", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "grad", "=", "drosenbrock", "(", "params", ".", "data", ")", "\n", "# NB: We torture test the optimizer by returning an", "\n", "# uncoalesced sparse tensor", "\n", "if", "w", ":", "\n", "            ", "i", "=", "torch", ".", "LongTensor", "(", "[", "[", "0", ",", "0", "]", "]", ")", "\n", "x", "=", "grad", "[", "0", "]", "\n", "v", "=", "torch", ".", "tensor", "(", "[", "x", "/", "4.", ",", "x", "-", "x", "/", "4.", "]", ")", "\n", "", "else", ":", "\n", "            ", "i", "=", "torch", ".", "LongTensor", "(", "[", "[", "1", ",", "1", "]", "]", ")", "\n", "y", "=", "grad", "[", "1", "]", "\n", "v", "=", "torch", ".", "tensor", "(", "[", "y", "-", "y", "/", "4.", ",", "y", "/", "4.", "]", ")", "\n", "", "x", "=", "torch", ".", "sparse", ".", "DoubleTensor", "(", "i", ",", "v", ",", "torch", ".", "Size", "(", "[", "2", "]", ")", ")", ".", "to", "(", "dtype", "=", "v", ".", "dtype", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "params", ".", "grad", "=", "x", ".", "to_dense", "(", ")", "\n", "", "return", "loss", "\n", "\n", "", "for", "i", "in", "range", "(", "2000", ")", ":", "\n", "# Do cyclic coordinate descent", "\n", "        ", "w", "=", "i", "%", "2", "\n", "optimizer", ".", "step", "(", "functools", ".", "partial", "(", "eval", ",", "params", ",", "w", ")", ")", "\n", "for", "scheduler", "in", "schedulers", ":", "\n", "            ", "if", "isinstance", "(", "scheduler", ",", "PlateauLRScheduler", ")", ":", "\n", "                ", "scheduler", ".", "step", "(", "rosenbrock", "(", "params", ")", ")", "\n", "", "else", ":", "\n", "                ", "scheduler", ".", "step", "(", ")", "\n", "\n", "", "", "", "torch_tc", ".", "assertLessEqual", "(", "params", ".", "data", ".", "dist", "(", "solution", ")", ",", "initial_dist", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict": [[262, 264], ["dict"], "function", ["None"], ["", "def", "_build_params_dict", "(", "weight", ",", "bias", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "[", "{", "'params'", ":", "[", "weight", "]", "}", ",", "dict", "(", "params", "=", "[", "bias", "]", ",", "**", "kwargs", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single": [[266, 268], ["dict"], "function", ["None"], ["", "def", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "[", "dict", "(", "params", "=", "bias", ",", "**", "kwargs", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.test_sgd": [[272, 331], ["pytest.mark.parametrize", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_rosenbrock", "test_optim._test_model", "dict", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "test_optim._build_params_dict", "test_optim._build_params_dict_single", "test_optim._build_params_dict_single"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_rosenbrock", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'optimizer'", ",", "[", "'sgd'", "]", ")", "\n", "def", "test_sgd", "(", "optimizer", ")", ":", "\n", "    ", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict", "(", "weight", ",", "bias", ",", "lr", "=", "1e-2", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "1e-2", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "1e-2", ")", ",", "optimizer", ")", "\n", ")", "\n", "# _test_basic_cases(", "\n", "#     lambda weight, bias: create_optimizer_v2([weight, bias], optimizer, lr=1e-3),", "\n", "#     [lambda opt: StepLR(opt, gamma=0.9, step_size=10)]", "\n", "# )", "\n", "# _test_basic_cases(", "\n", "#     lambda weight, bias: create_optimizer_v2([weight, bias], optimizer, lr=1e-3),", "\n", "#     [lambda opt: WarmUpLR(opt, warmup_factor=0.4, warmup_iters=4, warmup_method=\"linear\")]", "\n", "# )", "\n", "# _test_basic_cases(", "\n", "#     lambda weight, bias: optimizer([weight, bias], lr=1e-3),", "\n", "#     [lambda opt: WarmUpLR(opt, warmup_factor=0.4, warmup_iters=4, warmup_method=\"constant\")]", "\n", "# )", "\n", "# _test_basic_cases(", "\n", "#     lambda weight, bias: optimizer([weight, bias], lr=1e-3),", "\n", "#     [lambda opt: StepLR(opt, gamma=0.9, step_size=10),", "\n", "#      lambda opt: WarmUpLR(opt, warmup_factor=0.4, warmup_iters=4)]", "\n", "# )", "\n", "# _test_basic_cases(", "\n", "#     lambda weight, bias: optimizer([weight, bias], lr=1e-3),", "\n", "#     [lambda opt: StepLR(opt, gamma=0.9, step_size=10),", "\n", "#      lambda opt: ReduceLROnPlateau(opt)]", "\n", "# )", "\n", "# _test_basic_cases(", "\n", "#     lambda weight, bias: optimizer([weight, bias], lr=1e-3),", "\n", "#     [lambda opt: StepLR(opt, gamma=0.99, step_size=10),", "\n", "#      lambda opt: ExponentialLR(opt, gamma=0.99),", "\n", "#      lambda opt: ReduceLROnPlateau(opt)]", "\n", "# )", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "3e-3", ",", "momentum", "=", "1", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "3e-3", ",", "momentum", "=", "1", ",", "weight_decay", "=", ".1", ")", "\n", ")", "\n", "_test_rosenbrock", "(", "\n", "lambda", "params", ":", "create_optimizer_v2", "(", "params", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_model", "(", "optimizer", ",", "dict", "(", "lr", "=", "1e-3", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.test_adam": [[333, 354], ["pytest.mark.parametrize", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_rosenbrock", "test_optim._test_model", "dict", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "test_optim._build_params_dict", "test_optim._build_params_dict_single"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_rosenbrock", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'optimizer'", ",", "[", "'adamw'", ",", "'adam'", ",", "'nadam'", ",", "'adamax'", "]", ")", "\n", "def", "test_adam", "(", "optimizer", ")", ":", "\n", "    ", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_rosenbrock", "(", "\n", "lambda", "params", ":", "create_optimizer_v2", "(", "params", ",", "optimizer", ",", "lr", "=", "5e-2", ")", "\n", ")", "\n", "_test_model", "(", "optimizer", ",", "dict", "(", "lr", "=", "5e-2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.test_adabelief": [[356, 384], ["pytest.mark.parametrize", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_rosenbrock", "test_optim._test_model", "dict", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "test_optim._build_params_dict", "test_optim._build_params_dict_single", "test_optim._build_params_dict_single"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_rosenbrock", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'optimizer'", ",", "[", "'adabelief'", "]", ")", "\n", "def", "test_adabelief", "(", "optimizer", ")", ":", "\n", "    ", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "optimizer", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "1e-3", ",", "weight_decay", "=", "1", ")", "\n", ")", "\n", "_test_rosenbrock", "(", "\n", "lambda", "params", ":", "create_optimizer_v2", "(", "params", ",", "optimizer", ",", "lr", "=", "5e-2", ")", "\n", ")", "\n", "_test_model", "(", "optimizer", ",", "dict", "(", "lr", "=", "5e-2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.test_rectified": [[386, 407], ["pytest.mark.parametrize", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_rosenbrock", "test_optim._test_model", "dict", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "test_optim._build_params_dict", "test_optim._build_params_dict_single"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_rosenbrock", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'optimizer'", ",", "[", "'radam'", ",", "'radabelief'", "]", ")", "\n", "def", "test_rectified", "(", "optimizer", ")", ":", "\n", "    ", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_rosenbrock", "(", "\n", "lambda", "params", ":", "create_optimizer_v2", "(", "params", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_model", "(", "optimizer", ",", "dict", "(", "lr", "=", "1e-3", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.test_adaother": [[409, 437], ["pytest.mark.parametrize", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_rosenbrock", "test_optim._test_model", "dict", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "test_optim._build_params_dict", "test_optim._build_params_dict_single", "test_optim._build_params_dict_single"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_rosenbrock", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'optimizer'", ",", "[", "'adadelta'", ",", "'adagrad'", "]", ")", "\n", "def", "test_adaother", "(", "optimizer", ")", ":", "\n", "    ", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "optimizer", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "1e-3", ",", "weight_decay", "=", "1", ")", "\n", ")", "\n", "_test_rosenbrock", "(", "\n", "lambda", "params", ":", "create_optimizer_v2", "(", "params", ",", "optimizer", ",", "lr", "=", "1e-1", ")", "\n", ")", "\n", "_test_model", "(", "optimizer", ",", "dict", "(", "lr", "=", "5e-2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.test_adafactor": [[439, 466], ["pytest.mark.parametrize", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_rosenbrock", "test_optim._test_model", "dict", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "test_optim._build_params_dict", "test_optim._build_params_dict_single", "test_optim._build_params_dict_single"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_rosenbrock", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'optimizer'", ",", "[", "'adafactor'", "]", ")", "\n", "def", "test_adafactor", "(", "optimizer", ")", ":", "\n", "    ", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "_build_params_dict_single", "(", "weight", ",", "bias", ")", ",", "optimizer", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "1e-3", ",", "weight_decay", "=", "1", ")", "\n", ")", "\n", "_test_rosenbrock", "(", "\n", "lambda", "params", ":", "create_optimizer_v2", "(", "params", ",", "optimizer", ",", "lr", "=", "5e-2", ")", "\n", ")", "\n", "_test_model", "(", "optimizer", ",", "dict", "(", "lr", "=", "5e-2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.test_lamb": [[468, 493], ["pytest.mark.parametrize", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_rosenbrock", "test_optim._test_model", "dict", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "test_optim._build_params_dict", "test_optim._build_params_dict_single", "test_optim._build_params_dict_single"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_rosenbrock", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'optimizer'", ",", "[", "'lamb'", ",", "'lambc'", "]", ")", "\n", "def", "test_lamb", "(", "optimizer", ")", ":", "\n", "    ", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict", "(", "weight", ",", "bias", ",", "lr", "=", "1e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "1e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "1e-3", ")", ",", "optimizer", ")", "\n", ")", "\n", "_test_rosenbrock", "(", "\n", "lambda", "params", ":", "create_optimizer_v2", "(", "params", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_model", "(", "optimizer", ",", "dict", "(", "lr", "=", "1e-3", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.test_lars": [[495, 520], ["pytest.mark.parametrize", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_rosenbrock", "test_optim._test_model", "dict", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "test_optim._build_params_dict", "test_optim._build_params_dict_single", "test_optim._build_params_dict_single"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_rosenbrock", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'optimizer'", ",", "[", "'lars'", ",", "'larc'", ",", "'nlars'", ",", "'nlarc'", "]", ")", "\n", "def", "test_lars", "(", "optimizer", ")", ":", "\n", "    ", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict", "(", "weight", ",", "bias", ",", "lr", "=", "1e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "1e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "1e-3", ")", ",", "optimizer", ")", "\n", ")", "\n", "_test_rosenbrock", "(", "\n", "lambda", "params", ":", "create_optimizer_v2", "(", "params", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_model", "(", "optimizer", ",", "dict", "(", "lr", "=", "1e-3", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.test_madgrad": [[522, 547], ["pytest.mark.parametrize", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_rosenbrock", "test_optim._test_model", "dict", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "test_optim._build_params_dict", "test_optim._build_params_dict_single", "test_optim._build_params_dict_single"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_rosenbrock", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'optimizer'", ",", "[", "'madgrad'", ",", "'madgradw'", "]", ")", "\n", "def", "test_madgrad", "(", "optimizer", ")", ":", "\n", "    ", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "optimizer", ")", "\n", ")", "\n", "_test_rosenbrock", "(", "\n", "lambda", "params", ":", "create_optimizer_v2", "(", "params", ",", "optimizer", ",", "lr", "=", "1e-2", ")", "\n", ")", "\n", "_test_model", "(", "optimizer", ",", "dict", "(", "lr", "=", "1e-2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.test_novograd": [[549, 574], ["pytest.mark.parametrize", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_rosenbrock", "test_optim._test_model", "dict", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "test_optim._build_params_dict", "test_optim._build_params_dict_single", "test_optim._build_params_dict_single"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_rosenbrock", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'optimizer'", ",", "[", "'novograd'", "]", ")", "\n", "def", "test_novograd", "(", "optimizer", ")", ":", "\n", "    ", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "optimizer", ")", "\n", ")", "\n", "_test_rosenbrock", "(", "\n", "lambda", "params", ":", "create_optimizer_v2", "(", "params", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_model", "(", "optimizer", ",", "dict", "(", "lr", "=", "1e-3", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.test_rmsprop": [[576, 601], ["pytest.mark.parametrize", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_rosenbrock", "test_optim._test_model", "dict", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "test_optim._build_params_dict", "test_optim._build_params_dict_single", "test_optim._build_params_dict_single"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_rosenbrock", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'optimizer'", ",", "[", "'rmsprop'", ",", "'rmsproptf'", "]", ")", "\n", "def", "test_rmsprop", "(", "optimizer", ")", ":", "\n", "    ", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "optimizer", ")", "\n", ")", "\n", "_test_rosenbrock", "(", "\n", "lambda", "params", ":", "create_optimizer_v2", "(", "params", ",", "optimizer", ",", "lr", "=", "1e-2", ")", "\n", ")", "\n", "_test_model", "(", "optimizer", ",", "dict", "(", "lr", "=", "1e-2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.test_adamp": [[603, 628], ["pytest.mark.parametrize", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_rosenbrock", "test_optim._test_model", "dict", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "test_optim._build_params_dict", "test_optim._build_params_dict_single", "test_optim._build_params_dict_single"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_rosenbrock", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'optimizer'", ",", "[", "'adamp'", "]", ")", "\n", "def", "test_adamp", "(", "optimizer", ")", ":", "\n", "    ", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "optimizer", ")", "\n", ")", "\n", "_test_rosenbrock", "(", "\n", "lambda", "params", ":", "create_optimizer_v2", "(", "params", ",", "optimizer", ",", "lr", "=", "5e-2", ")", "\n", ")", "\n", "_test_model", "(", "optimizer", ",", "dict", "(", "lr", "=", "5e-2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.test_sgdp": [[630, 655], ["pytest.mark.parametrize", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_rosenbrock", "test_optim._test_model", "dict", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "test_optim._build_params_dict", "test_optim._build_params_dict_single", "test_optim._build_params_dict_single"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_rosenbrock", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'optimizer'", ",", "[", "'sgdp'", "]", ")", "\n", "def", "test_sgdp", "(", "optimizer", ")", ":", "\n", "    ", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "optimizer", ")", "\n", ")", "\n", "_test_rosenbrock", "(", "\n", "lambda", "params", ":", "create_optimizer_v2", "(", "params", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_model", "(", "optimizer", ",", "dict", "(", "lr", "=", "1e-3", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.test_lookahead_sgd": [[657, 680], ["pytest.mark.parametrize", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_rosenbrock", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "test_optim._build_params_dict", "test_optim._build_params_dict_single", "test_optim._build_params_dict_single"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_rosenbrock", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'optimizer'", ",", "[", "'lookahead_sgd'", ",", "'lookahead_momentum'", "]", ")", "\n", "def", "test_lookahead_sgd", "(", "optimizer", ")", ":", "\n", "    ", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "optimizer", ")", "\n", ")", "\n", "_test_rosenbrock", "(", "\n", "lambda", "params", ":", "create_optimizer_v2", "(", "params", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.test_lookahead_adam": [[683, 706], ["pytest.mark.parametrize", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_rosenbrock", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "test_optim._build_params_dict", "test_optim._build_params_dict_single", "test_optim._build_params_dict_single"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_rosenbrock", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'optimizer'", ",", "[", "'lookahead_adamw'", ",", "'lookahead_adam'", "]", ")", "\n", "def", "test_lookahead_adam", "(", "optimizer", ")", ":", "\n", "    ", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "optimizer", ")", "\n", ")", "\n", "_test_rosenbrock", "(", "\n", "lambda", "params", ":", "create_optimizer_v2", "(", "params", ",", "optimizer", ",", "lr", "=", "5e-2", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim.test_lookahead_radam": [[709, 732], ["pytest.mark.parametrize", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_basic_cases", "test_optim._test_rosenbrock", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "timm.optim.create_optimizer_v2", "test_optim._build_params_dict", "test_optim._build_params_dict_single", "test_optim._build_params_dict_single"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_basic_cases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._test_rosenbrock", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_optim._build_params_dict_single"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'optimizer'", ",", "[", "'lookahead_radam'", "]", ")", "\n", "def", "test_lookahead_radam", "(", "optimizer", ")", ":", "\n", "    ", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "[", "weight", ",", "bias", "]", ",", "optimizer", ",", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "\n", "optimizer", ",", "\n", "lr", "=", "1e-3", ")", "\n", ")", "\n", "_test_basic_cases", "(", "\n", "lambda", "weight", ",", "bias", ":", "create_optimizer_v2", "(", "\n", "_build_params_dict_single", "(", "weight", ",", "bias", ",", "lr", "=", "3e-3", ")", ",", "optimizer", ")", "\n", ")", "\n", "_test_rosenbrock", "(", "\n", "lambda", "params", ":", "create_optimizer_v2", "(", "params", ",", "optimizer", ",", "lr", "=", "1e-4", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_utils.test_freeze_unfreeze": [[8, 58], ["timm.create_model", "timm.utils.model.freeze", "isinstance", "timm.utils.model.unfreeze", "isinstance", "timm.utils.model.freeze", "isinstance", "isinstance", "timm.utils.model.unfreeze", "isinstance", "timm.utils.model.freeze", "isinstance", "timm.utils.model.unfreeze", "isinstance", "timm.utils.model.freeze", "isinstance", "timm.utils.model.unfreeze", "isinstance"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.create_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.freeze", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.unfreeze", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.freeze", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.unfreeze", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.freeze", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.unfreeze", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.freeze", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.unfreeze"], ["def", "test_freeze_unfreeze", "(", ")", ":", "\n", "    ", "model", "=", "timm", ".", "create_model", "(", "'resnet18'", ")", "\n", "\n", "# Freeze all", "\n", "freeze", "(", "model", ")", "\n", "# Check top level module", "\n", "assert", "model", ".", "fc", ".", "weight", ".", "requires_grad", "==", "False", "\n", "# Check submodule", "\n", "assert", "model", ".", "layer1", "[", "0", "]", ".", "conv1", ".", "weight", ".", "requires_grad", "==", "False", "\n", "# Check BN", "\n", "assert", "isinstance", "(", "model", ".", "layer1", "[", "0", "]", ".", "bn1", ",", "FrozenBatchNorm2d", ")", "\n", "\n", "# Unfreeze all", "\n", "unfreeze", "(", "model", ")", "\n", "# Check top level module", "\n", "assert", "model", ".", "fc", ".", "weight", ".", "requires_grad", "==", "True", "\n", "# Check submodule", "\n", "assert", "model", ".", "layer1", "[", "0", "]", ".", "conv1", ".", "weight", ".", "requires_grad", "==", "True", "\n", "# Check BN", "\n", "assert", "isinstance", "(", "model", ".", "layer1", "[", "0", "]", ".", "bn1", ",", "BatchNorm2d", ")", "\n", "\n", "# Freeze some", "\n", "freeze", "(", "model", ",", "[", "'layer1'", ",", "'layer2.0'", "]", ")", "\n", "# Check frozen", "\n", "assert", "model", ".", "layer1", "[", "0", "]", ".", "conv1", ".", "weight", ".", "requires_grad", "==", "False", "\n", "assert", "isinstance", "(", "model", ".", "layer1", "[", "0", "]", ".", "bn1", ",", "FrozenBatchNorm2d", ")", "\n", "assert", "model", ".", "layer2", "[", "0", "]", ".", "conv1", ".", "weight", ".", "requires_grad", "==", "False", "\n", "# Check not frozen", "\n", "assert", "model", ".", "layer3", "[", "0", "]", ".", "conv1", ".", "weight", ".", "requires_grad", "==", "True", "\n", "assert", "isinstance", "(", "model", ".", "layer3", "[", "0", "]", ".", "bn1", ",", "BatchNorm2d", ")", "\n", "assert", "model", ".", "layer2", "[", "1", "]", ".", "conv1", ".", "weight", ".", "requires_grad", "==", "True", "\n", "\n", "# Unfreeze some", "\n", "unfreeze", "(", "model", ",", "[", "'layer1'", ",", "'layer2.0'", "]", ")", "\n", "# Check not frozen", "\n", "assert", "model", ".", "layer1", "[", "0", "]", ".", "conv1", ".", "weight", ".", "requires_grad", "==", "True", "\n", "assert", "isinstance", "(", "model", ".", "layer1", "[", "0", "]", ".", "bn1", ",", "BatchNorm2d", ")", "\n", "assert", "model", ".", "layer2", "[", "0", "]", ".", "conv1", ".", "weight", ".", "requires_grad", "==", "True", "\n", "\n", "# Freeze/unfreeze BN", "\n", "# From root", "\n", "freeze", "(", "model", ",", "[", "'layer1.0.bn1'", "]", ")", "\n", "assert", "isinstance", "(", "model", ".", "layer1", "[", "0", "]", ".", "bn1", ",", "FrozenBatchNorm2d", ")", "\n", "unfreeze", "(", "model", ",", "[", "'layer1.0.bn1'", "]", ")", "\n", "assert", "isinstance", "(", "model", ".", "layer1", "[", "0", "]", ".", "bn1", ",", "BatchNorm2d", ")", "\n", "# From direct parent", "\n", "freeze", "(", "model", ".", "layer1", "[", "0", "]", ",", "[", "'bn1'", "]", ")", "\n", "assert", "isinstance", "(", "model", ".", "layer1", "[", "0", "]", ".", "bn1", ",", "FrozenBatchNorm2d", ")", "\n", "unfreeze", "(", "model", ".", "layer1", "[", "0", "]", ",", "[", "'bn1'", "]", ")", "\n", "assert", "isinstance", "(", "model", ".", "layer1", "[", "0", "]", ".", "bn1", ",", "BatchNorm2d", ")", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_layers.MLP.__init__": [[11, 16], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "timm.models.layers.create_act_layer", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.create_act_layer"], ["    ", "def", "__init__", "(", "self", ",", "act_layer", "=", "\"relu\"", ",", "inplace", "=", "True", ")", ":", "\n", "        ", "super", "(", "MLP", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "1000", ",", "100", ")", "\n", "self", ".", "act", "=", "create_act_layer", "(", "act_layer", ",", "inplace", "=", "inplace", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "100", ",", "10", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_layers.MLP.forward": [[17, 22], ["test_layers.MLP.fc1", "test_layers.MLP.act", "test_layers.MLP.fc2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_layers._run_act_layer_grad": [[24, 47], ["test_layers.MLP", "test_layers._run_act_layer_grad._run"], "function", ["None"], ["", "", "def", "_run_act_layer_grad", "(", "act_type", ",", "inplace", "=", "True", ")", ":", "\n", "    ", "x", "=", "torch", ".", "rand", "(", "10", ",", "1000", ")", "*", "10", "\n", "m", "=", "MLP", "(", "act_layer", "=", "act_type", ",", "inplace", "=", "inplace", ")", "\n", "\n", "def", "_run", "(", "x", ",", "act_layer", "=", "''", ")", ":", "\n", "        ", "if", "act_layer", ":", "\n", "# replace act layer if set", "\n", "            ", "m", ".", "act", "=", "create_act_layer", "(", "act_layer", ",", "inplace", "=", "inplace", ")", "\n", "", "out", "=", "m", "(", "x", ")", "\n", "l", "=", "(", "out", "-", "0", ")", ".", "pow", "(", "2", ")", ".", "sum", "(", ")", "\n", "return", "l", "\n", "\n", "", "out_me", "=", "_run", "(", "x", ")", "\n", "\n", "with", "set_layer_config", "(", "scriptable", "=", "True", ")", ":", "\n", "        ", "out_jit", "=", "_run", "(", "x", ",", "act_type", ")", "\n", "\n", "", "assert", "torch", ".", "isclose", "(", "out_jit", ",", "out_me", ")", "\n", "\n", "with", "set_layer_config", "(", "no_jit", "=", "True", ")", ":", "\n", "        ", "out_basic", "=", "_run", "(", "x", ",", "act_type", ")", "\n", "\n", "", "assert", "torch", ".", "isclose", "(", "out_basic", ",", "out_jit", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_layers.test_swish_grad": [[49, 52], ["range", "test_layers._run_act_layer_grad"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_layers._run_act_layer_grad"], ["", "def", "test_swish_grad", "(", ")", ":", "\n", "    ", "for", "_", "in", "range", "(", "100", ")", ":", "\n", "        ", "_run_act_layer_grad", "(", "'swish'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_layers.test_mish_grad": [[54, 57], ["range", "test_layers._run_act_layer_grad"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_layers._run_act_layer_grad"], ["", "", "def", "test_mish_grad", "(", ")", ":", "\n", "    ", "for", "_", "in", "range", "(", "100", ")", ":", "\n", "        ", "_run_act_layer_grad", "(", "'mish'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_layers.test_hard_sigmoid_grad": [[59, 62], ["range", "test_layers._run_act_layer_grad"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_layers._run_act_layer_grad"], ["", "", "def", "test_hard_sigmoid_grad", "(", ")", ":", "\n", "    ", "for", "_", "in", "range", "(", "100", ")", ":", "\n", "        ", "_run_act_layer_grad", "(", "'hard_sigmoid'", ",", "inplace", "=", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_layers.test_hard_swish_grad": [[64, 67], ["range", "test_layers._run_act_layer_grad"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_layers._run_act_layer_grad"], ["", "", "def", "test_hard_swish_grad", "(", ")", ":", "\n", "    ", "for", "_", "in", "range", "(", "100", ")", ":", "\n", "        ", "_run_act_layer_grad", "(", "'hard_swish'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_layers.test_hard_mish_grad": [[69, 72], ["range", "test_layers._run_act_layer_grad"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.tests.test_layers._run_act_layer_grad"], ["", "", "def", "test_hard_mish_grad", "(", ")", ":", "\n", "    ", "for", "_", "in", "range", "(", "100", ")", ":", "\n", "        ", "_run_act_layer_grad", "(", "'hard_mish'", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.checkpoint_saver.CheckpointSaver.__init__": [[22, 62], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model", ",", "\n", "optimizer", ",", "\n", "args", "=", "None", ",", "\n", "model_ema", "=", "None", ",", "\n", "amp_scaler", "=", "None", ",", "\n", "checkpoint_prefix", "=", "'checkpoint'", ",", "\n", "recovery_prefix", "=", "'recovery'", ",", "\n", "checkpoint_dir", "=", "''", ",", "\n", "recovery_dir", "=", "''", ",", "\n", "decreasing", "=", "False", ",", "\n", "max_history", "=", "10", ",", "\n", "unwrap_fn", "=", "unwrap_model", ")", ":", "\n", "\n", "# objects to save state_dicts of", "\n", "        ", "self", ".", "model", "=", "model", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "model_ema", "=", "model_ema", "\n", "self", ".", "amp_scaler", "=", "amp_scaler", "\n", "\n", "# state", "\n", "self", ".", "checkpoint_files", "=", "[", "]", "# (filename, metric) tuples in order of decreasing betterness", "\n", "self", ".", "best_epoch", "=", "None", "\n", "self", ".", "best_metric", "=", "None", "\n", "self", ".", "curr_recovery_file", "=", "''", "\n", "self", ".", "last_recovery_file", "=", "''", "\n", "\n", "# config", "\n", "self", ".", "checkpoint_dir", "=", "checkpoint_dir", "\n", "self", ".", "recovery_dir", "=", "recovery_dir", "\n", "self", ".", "save_prefix", "=", "checkpoint_prefix", "\n", "self", ".", "recovery_prefix", "=", "recovery_prefix", "\n", "self", ".", "extension", "=", "'.pth.tar'", "\n", "self", ".", "decreasing", "=", "decreasing", "# a lower metric is better if True", "\n", "self", ".", "cmp", "=", "operator", ".", "lt", "if", "decreasing", "else", "operator", ".", "gt", "# True if lhs better than rhs", "\n", "self", ".", "max_history", "=", "max_history", "\n", "self", ".", "unwrap_fn", "=", "unwrap_fn", "\n", "assert", "self", ".", "max_history", ">=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.checkpoint_saver.CheckpointSaver.save_checkpoint": [[63, 98], ["os.path.join", "os.path.join", "checkpoint_saver.CheckpointSaver._save", "os.path.exists", "os.rename", "os.unlink", "checkpoint_saver.CheckpointSaver.cmp", "os.path.join", "os.link", "checkpoint_saver.CheckpointSaver.checkpoint_files.append", "sorted", "_logger.info", "len", "len", "checkpoint_saver.CheckpointSaver._cleanup_checkpoints", "os.path.join", "os.path.exists", "os.link", "checkpoint_saver.CheckpointSaver.cmp", "os.unlink", "str"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.checkpoint_saver.CheckpointSaver._save", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.checkpoint_saver.CheckpointSaver._cleanup_checkpoints"], ["", "def", "save_checkpoint", "(", "self", ",", "epoch", ",", "metric", "=", "None", ")", ":", "\n", "        ", "assert", "epoch", ">=", "0", "\n", "tmp_save_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "checkpoint_dir", ",", "'tmp'", "+", "self", ".", "extension", ")", "\n", "last_save_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "checkpoint_dir", ",", "'last'", "+", "self", ".", "extension", ")", "\n", "self", ".", "_save", "(", "tmp_save_path", ",", "epoch", ",", "metric", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "last_save_path", ")", ":", "\n", "            ", "os", ".", "unlink", "(", "last_save_path", ")", "# required for Windows support.", "\n", "", "os", ".", "rename", "(", "tmp_save_path", ",", "last_save_path", ")", "\n", "worst_file", "=", "self", ".", "checkpoint_files", "[", "-", "1", "]", "if", "self", ".", "checkpoint_files", "else", "None", "\n", "if", "(", "len", "(", "self", ".", "checkpoint_files", ")", "<", "self", ".", "max_history", "\n", "or", "metric", "is", "None", "or", "self", ".", "cmp", "(", "metric", ",", "worst_file", "[", "1", "]", ")", ")", ":", "\n", "            ", "if", "len", "(", "self", ".", "checkpoint_files", ")", ">=", "self", ".", "max_history", ":", "\n", "                ", "self", ".", "_cleanup_checkpoints", "(", "1", ")", "\n", "", "filename", "=", "'-'", ".", "join", "(", "[", "self", ".", "save_prefix", ",", "str", "(", "epoch", ")", "]", ")", "+", "self", ".", "extension", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "checkpoint_dir", ",", "filename", ")", "\n", "os", ".", "link", "(", "last_save_path", ",", "save_path", ")", "\n", "self", ".", "checkpoint_files", ".", "append", "(", "(", "save_path", ",", "metric", ")", ")", "\n", "self", ".", "checkpoint_files", "=", "sorted", "(", "\n", "self", ".", "checkpoint_files", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "\n", "reverse", "=", "not", "self", ".", "decreasing", ")", "# sort in descending order if a lower metric is not better", "\n", "\n", "checkpoints_str", "=", "\"Current checkpoints:\\n\"", "\n", "for", "c", "in", "self", ".", "checkpoint_files", ":", "\n", "                ", "checkpoints_str", "+=", "' {}\\n'", ".", "format", "(", "c", ")", "\n", "", "_logger", ".", "info", "(", "checkpoints_str", ")", "\n", "\n", "if", "metric", "is", "not", "None", "and", "(", "self", ".", "best_metric", "is", "None", "or", "self", ".", "cmp", "(", "metric", ",", "self", ".", "best_metric", ")", ")", ":", "\n", "                ", "self", ".", "best_epoch", "=", "epoch", "\n", "self", ".", "best_metric", "=", "metric", "\n", "best_save_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "checkpoint_dir", ",", "'model_best'", "+", "self", ".", "extension", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "best_save_path", ")", ":", "\n", "                    ", "os", ".", "unlink", "(", "best_save_path", ")", "\n", "", "os", ".", "link", "(", "last_save_path", ",", "best_save_path", ")", "\n", "\n", "", "", "return", "(", "None", ",", "None", ")", "if", "self", ".", "best_metric", "is", "None", "else", "(", "self", ".", "best_metric", ",", "self", ".", "best_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.checkpoint_saver.CheckpointSaver._save": [[99, 117], ["torch.save", "type().__name__.lower", "model.get_state_dict", "checkpoint_saver.CheckpointSaver.optimizer.state_dict", "checkpoint_saver.CheckpointSaver.amp_scaler.state_dict", "model.get_state_dict", "type"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.get_state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.get_state_dict"], ["", "def", "_save", "(", "self", ",", "save_path", ",", "epoch", ",", "metric", "=", "None", ")", ":", "\n", "        ", "save_state", "=", "{", "\n", "'epoch'", ":", "epoch", ",", "\n", "'arch'", ":", "type", "(", "self", ".", "model", ")", ".", "__name__", ".", "lower", "(", ")", ",", "\n", "'state_dict'", ":", "get_state_dict", "(", "self", ".", "model", ",", "self", ".", "unwrap_fn", ")", ",", "\n", "'optimizer'", ":", "self", ".", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "'version'", ":", "2", ",", "# version < 2 increments epoch before save", "\n", "}", "\n", "if", "self", ".", "args", "is", "not", "None", ":", "\n", "            ", "save_state", "[", "'arch'", "]", "=", "self", ".", "args", ".", "model", "\n", "save_state", "[", "'args'", "]", "=", "self", ".", "args", "\n", "", "if", "self", ".", "amp_scaler", "is", "not", "None", ":", "\n", "            ", "save_state", "[", "self", ".", "amp_scaler", ".", "state_dict_key", "]", "=", "self", ".", "amp_scaler", ".", "state_dict", "(", ")", "\n", "", "if", "self", ".", "model_ema", "is", "not", "None", ":", "\n", "            ", "save_state", "[", "'state_dict_ema'", "]", "=", "get_state_dict", "(", "self", ".", "model_ema", ",", "self", ".", "unwrap_fn", ")", "\n", "", "if", "metric", "is", "not", "None", ":", "\n", "            ", "save_state", "[", "'metric'", "]", "=", "metric", "\n", "", "torch", ".", "save", "(", "save_state", ",", "save_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.checkpoint_saver.CheckpointSaver._cleanup_checkpoints": [[118, 131], ["min", "len", "len", "_logger.debug", "os.remove", "_logger.error"], "methods", ["None"], ["", "def", "_cleanup_checkpoints", "(", "self", ",", "trim", "=", "0", ")", ":", "\n", "        ", "trim", "=", "min", "(", "len", "(", "self", ".", "checkpoint_files", ")", ",", "trim", ")", "\n", "delete_index", "=", "self", ".", "max_history", "-", "trim", "\n", "if", "delete_index", "<", "0", "or", "len", "(", "self", ".", "checkpoint_files", ")", "<=", "delete_index", ":", "\n", "            ", "return", "\n", "", "to_delete", "=", "self", ".", "checkpoint_files", "[", "delete_index", ":", "]", "\n", "for", "d", "in", "to_delete", ":", "\n", "            ", "try", ":", "\n", "                ", "_logger", ".", "debug", "(", "\"Cleaning checkpoint: {}\"", ".", "format", "(", "d", ")", ")", "\n", "os", ".", "remove", "(", "d", "[", "0", "]", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                ", "_logger", ".", "error", "(", "\"Exception '{}' while deleting checkpoint\"", ".", "format", "(", "e", ")", ")", "\n", "", "", "self", ".", "checkpoint_files", "=", "self", ".", "checkpoint_files", "[", ":", "delete_index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.checkpoint_saver.CheckpointSaver.save_recovery": [[132, 145], ["os.path.join", "checkpoint_saver.CheckpointSaver._save", "os.path.exists", "_logger.debug", "os.remove", "str", "str", "_logger.error"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.checkpoint_saver.CheckpointSaver._save"], ["", "def", "save_recovery", "(", "self", ",", "epoch", ",", "batch_idx", "=", "0", ")", ":", "\n", "        ", "assert", "epoch", ">=", "0", "\n", "filename", "=", "'-'", ".", "join", "(", "[", "self", ".", "recovery_prefix", ",", "str", "(", "epoch", ")", ",", "str", "(", "batch_idx", ")", "]", ")", "+", "self", ".", "extension", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "recovery_dir", ",", "filename", ")", "\n", "self", ".", "_save", "(", "save_path", ",", "epoch", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "self", ".", "last_recovery_file", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "_logger", ".", "debug", "(", "\"Cleaning recovery: {}\"", ".", "format", "(", "self", ".", "last_recovery_file", ")", ")", "\n", "os", ".", "remove", "(", "self", ".", "last_recovery_file", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                ", "_logger", ".", "error", "(", "\"Exception '{}' while removing {}\"", ".", "format", "(", "e", ",", "self", ".", "last_recovery_file", ")", ")", "\n", "", "", "self", ".", "last_recovery_file", "=", "self", ".", "curr_recovery_file", "\n", "self", ".", "curr_recovery_file", "=", "save_path", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.checkpoint_saver.CheckpointSaver.find_recovery": [[146, 151], ["os.path.join", "glob.glob", "sorted", "len"], "methods", ["None"], ["", "def", "find_recovery", "(", "self", ")", ":", "\n", "        ", "recovery_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "recovery_dir", ",", "self", ".", "recovery_prefix", ")", "\n", "files", "=", "glob", ".", "glob", "(", "recovery_path", "+", "'*'", "+", "self", ".", "extension", ")", "\n", "files", "=", "sorted", "(", "files", ")", "\n", "return", "files", "[", "0", "]", "if", "len", "(", "files", ")", "else", "''", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEma.__init__": [[37, 50], ["copy.deepcopy", "model_ema.ModelEma.ema.eval", "hasattr", "model_ema.ModelEma.ema.parameters", "model_ema.ModelEma.ema.to", "model_ema.ModelEma._load_checkpoint", "p.requires_grad_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEma._load_checkpoint"], ["def", "__init__", "(", "self", ",", "model", ",", "decay", "=", "0.9999", ",", "device", "=", "''", ",", "resume", "=", "''", ")", ":", "\n", "# make a copy of the model for accumulating moving average of weights", "\n", "        ", "self", ".", "ema", "=", "deepcopy", "(", "model", ")", "\n", "self", ".", "ema", ".", "eval", "(", ")", "\n", "self", ".", "decay", "=", "decay", "\n", "self", ".", "device", "=", "device", "# perform ema on different device from model if set", "\n", "if", "device", ":", "\n", "            ", "self", ".", "ema", ".", "to", "(", "device", "=", "device", ")", "\n", "", "self", ".", "ema_has_module", "=", "hasattr", "(", "self", ".", "ema", ",", "'module'", ")", "\n", "if", "resume", ":", "\n", "            ", "self", ".", "_load_checkpoint", "(", "resume", ")", "\n", "", "for", "p", "in", "self", ".", "ema", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad_", "(", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEma._load_checkpoint": [[51, 67], ["torch.load", "torch.load", "torch.load", "torch.load", "isinstance", "collections.OrderedDict", "checkpoint[].items", "model_ema.ModelEma.ema.load_state_dict", "_logger.info", "_logger.warning", "k.startswith"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_state_dict"], ["", "", "def", "_load_checkpoint", "(", "self", ",", "checkpoint_path", ")", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "checkpoint_path", ",", "map_location", "=", "'cpu'", ")", "\n", "assert", "isinstance", "(", "checkpoint", ",", "dict", ")", "\n", "if", "'state_dict_ema'", "in", "checkpoint", ":", "\n", "            ", "new_state_dict", "=", "OrderedDict", "(", ")", "\n", "for", "k", ",", "v", "in", "checkpoint", "[", "'state_dict_ema'", "]", ".", "items", "(", ")", ":", "\n", "# ema model may have been wrapped by DataParallel, and need module prefix", "\n", "                ", "if", "self", ".", "ema_has_module", ":", "\n", "                    ", "name", "=", "'module.'", "+", "k", "if", "not", "k", ".", "startswith", "(", "'module'", ")", "else", "k", "\n", "", "else", ":", "\n", "                    ", "name", "=", "k", "\n", "", "new_state_dict", "[", "name", "]", "=", "v", "\n", "", "self", ".", "ema", ".", "load_state_dict", "(", "new_state_dict", ")", "\n", "_logger", ".", "info", "(", "\"Loaded state_dict_ema\"", ")", "\n", "", "else", ":", "\n", "            ", "_logger", ".", "warning", "(", "\"Failed to find state_dict_ema, starting from loaded model weights\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEma.update": [[68, 80], ["hasattr", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.state_dict", "model_ema.ModelEma.ema.state_dict().items", "msd[].detach", "ema_v.copy_", "model_ema.ModelEma.ema.state_dict", "model_v.to.to.to"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict"], ["", "", "def", "update", "(", "self", ",", "model", ")", ":", "\n", "# correct a mismatch in state dict keys", "\n", "        ", "needs_module", "=", "hasattr", "(", "model", ",", "'module'", ")", "and", "not", "self", ".", "ema_has_module", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "msd", "=", "model", ".", "state_dict", "(", ")", "\n", "for", "k", ",", "ema_v", "in", "self", ".", "ema", ".", "state_dict", "(", ")", ".", "items", "(", ")", ":", "\n", "                ", "if", "needs_module", ":", "\n", "                    ", "k", "=", "'module.'", "+", "k", "\n", "", "model_v", "=", "msd", "[", "k", "]", ".", "detach", "(", ")", "\n", "if", "self", ".", "device", ":", "\n", "                    ", "model_v", "=", "model_v", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "", "ema_v", ".", "copy_", "(", "ema_v", "*", "self", ".", "decay", "+", "(", "1.", "-", "self", ".", "decay", ")", "*", "model_v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEmaV2.__init__": [[105, 114], ["torch.Module.__init__", "copy.deepcopy", "model_ema.ModelEmaV2.module.eval", "model_ema.ModelEmaV2.module.to"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "model", ",", "decay", "=", "0.9999", ",", "device", "=", "None", ")", ":", "\n", "        ", "super", "(", "ModelEmaV2", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# make a copy of the model for accumulating moving average of weights", "\n", "self", ".", "module", "=", "deepcopy", "(", "model", ")", "\n", "self", ".", "module", ".", "eval", "(", ")", "\n", "self", ".", "decay", "=", "decay", "\n", "self", ".", "device", "=", "device", "# perform ema on different device from model if set", "\n", "if", "self", ".", "device", "is", "not", "None", ":", "\n", "            ", "self", ".", "module", ".", "to", "(", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEmaV2._update": [[115, 121], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "zip", "model_ema.ModelEmaV2.module.state_dict().values", "model.state_dict().values", "ema_v.copy_", "model_v.to.to.to", "update_fn", "model_ema.ModelEmaV2.module.state_dict", "model.state_dict"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict"], ["", "", "def", "_update", "(", "self", ",", "model", ",", "update_fn", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "ema_v", ",", "model_v", "in", "zip", "(", "self", ".", "module", ".", "state_dict", "(", ")", ".", "values", "(", ")", ",", "model", ".", "state_dict", "(", ")", ".", "values", "(", ")", ")", ":", "\n", "                ", "if", "self", ".", "device", "is", "not", "None", ":", "\n", "                    ", "model_v", "=", "model_v", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "", "ema_v", ".", "copy_", "(", "update_fn", "(", "ema_v", ",", "model_v", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEmaV2.update": [[122, 124], ["model_ema.ModelEmaV2._update"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEmaV2._update"], ["", "", "", "def", "update", "(", "self", ",", "model", ")", ":", "\n", "        ", "self", ".", "_update", "(", "model", ",", "update_fn", "=", "lambda", "e", ",", "m", ":", "self", ".", "decay", "*", "e", "+", "(", "1.", "-", "self", ".", "decay", ")", "*", "m", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEmaV2.set": [[125, 127], ["model_ema.ModelEmaV2._update"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEmaV2._update"], ["", "def", "set", "(", "self", ",", "model", ")", ":", "\n", "        ", "self", ".", "_update", "(", "model", ",", "update_fn", "=", "lambda", "e", ",", "m", ":", "m", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.summary.get_outdir": [[13, 27], ["os.path.join", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "str", "str"], "function", ["None"], ["", "def", "get_outdir", "(", "path", ",", "*", "paths", ",", "inc", "=", "False", ")", ":", "\n", "    ", "outdir", "=", "os", ".", "path", ".", "join", "(", "path", ",", "*", "paths", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "outdir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "outdir", ")", "\n", "", "elif", "inc", ":", "\n", "        ", "count", "=", "1", "\n", "outdir_inc", "=", "outdir", "+", "'-'", "+", "str", "(", "count", ")", "\n", "while", "os", ".", "path", ".", "exists", "(", "outdir_inc", ")", ":", "\n", "            ", "count", "=", "count", "+", "1", "\n", "outdir_inc", "=", "outdir", "+", "'-'", "+", "str", "(", "count", ")", "\n", "assert", "count", "<", "100", "\n", "", "outdir", "=", "outdir_inc", "\n", "os", ".", "makedirs", "(", "outdir", ")", "\n", "", "return", "outdir", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.summary.update_summary": [[29, 40], ["collections.OrderedDict", "collections.OrderedDict.update", "collections.OrderedDict.update", "wandb.log", "open", "csv.DictWriter", "csv.DictWriter.writerow", "csv.DictWriter.writeheader", "train_metrics.items", "eval_metrics.items", "collections.OrderedDict.keys"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update"], ["", "def", "update_summary", "(", "epoch", ",", "train_metrics", ",", "eval_metrics", ",", "filename", ",", "write_header", "=", "False", ",", "log_wandb", "=", "False", ")", ":", "\n", "    ", "rowd", "=", "OrderedDict", "(", "epoch", "=", "epoch", ")", "\n", "rowd", ".", "update", "(", "[", "(", "'train_'", "+", "k", ",", "v", ")", "for", "k", ",", "v", "in", "train_metrics", ".", "items", "(", ")", "]", ")", "\n", "rowd", ".", "update", "(", "[", "(", "'eval_'", "+", "k", ",", "v", ")", "for", "k", ",", "v", "in", "eval_metrics", ".", "items", "(", ")", "]", ")", "\n", "if", "log_wandb", ":", "\n", "        ", "wandb", ".", "log", "(", "rowd", ")", "\n", "", "with", "open", "(", "filename", ",", "mode", "=", "'a'", ")", "as", "cf", ":", "\n", "        ", "dw", "=", "csv", ".", "DictWriter", "(", "cf", ",", "fieldnames", "=", "rowd", ".", "keys", "(", ")", ")", "\n", "if", "write_header", ":", "# first iteration (epoch == 1 can't be used)", "\n", "            ", "dw", ".", "writeheader", "(", ")", "\n", "", "dw", ".", "writerow", "(", "rowd", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.agc.unitwise_norm": [[21, 28], ["x.norm", "x.norm", "tuple", "range"], "function", ["None"], ["def", "unitwise_norm", "(", "x", ",", "norm_type", "=", "2.0", ")", ":", "\n", "    ", "if", "x", ".", "ndim", "<=", "1", ":", "\n", "        ", "return", "x", ".", "norm", "(", "norm_type", ")", "\n", "", "else", ":", "\n", "# works for nn.ConvNd and nn,Linear where output dim is first in the kernel/weight tensor", "\n", "# might need special cases for other weights (possibly MHA) where this may not be true", "\n", "        ", "return", "x", ".", "norm", "(", "norm_type", ",", "dim", "=", "tuple", "(", "range", "(", "1", ",", "x", ".", "ndim", ")", ")", ",", "keepdim", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.agc.adaptive_clip_grad": [[30, 43], ["isinstance", "p.detach", "p.grad.detach", "unitwise_norm().clamp_().mul_", "agc.unitwise_norm", "torch.where", "p.grad.detach().copy_", "unitwise_norm().clamp_", "unitwise_norm.clamp", "p.grad.detach", "agc.unitwise_norm"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.agc.unitwise_norm", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.agc.unitwise_norm"], ["", "", "def", "adaptive_clip_grad", "(", "parameters", ",", "clip_factor", "=", "0.01", ",", "eps", "=", "1e-3", ",", "norm_type", "=", "2.0", ")", ":", "\n", "    ", "if", "isinstance", "(", "parameters", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "parameters", "=", "[", "parameters", "]", "\n", "", "for", "p", "in", "parameters", ":", "\n", "        ", "if", "p", ".", "grad", "is", "None", ":", "\n", "            ", "continue", "\n", "", "p_data", "=", "p", ".", "detach", "(", ")", "\n", "g_data", "=", "p", ".", "grad", ".", "detach", "(", ")", "\n", "max_norm", "=", "unitwise_norm", "(", "p_data", ",", "norm_type", "=", "norm_type", ")", ".", "clamp_", "(", "min", "=", "eps", ")", ".", "mul_", "(", "clip_factor", ")", "\n", "grad_norm", "=", "unitwise_norm", "(", "g_data", ",", "norm_type", "=", "norm_type", ")", "\n", "clipped_grad", "=", "g_data", "*", "(", "max_norm", "/", "grad_norm", ".", "clamp", "(", "min", "=", "1e-6", ")", ")", "\n", "new_grads", "=", "torch", ".", "where", "(", "grad_norm", "<", "max_norm", ",", "g_data", ",", "clipped_grad", ")", "\n", "p", ".", "grad", ".", "detach", "(", ")", ".", "copy_", "(", "new_grads", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.distributed.reduce_tensor": [[11, 16], ["tensor.clone", "torch.distributed.all_reduce"], "function", ["None"], ["def", "reduce_tensor", "(", "tensor", ",", "n", ")", ":", "\n", "    ", "rt", "=", "tensor", ".", "clone", "(", ")", "\n", "dist", ".", "all_reduce", "(", "rt", ",", "op", "=", "dist", ".", "ReduceOp", ".", "SUM", ")", "\n", "rt", "/=", "n", "\n", "return", "rt", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.distributed.distribute_bn": [[18, 29], ["model.unwrap_model().named_buffers", "model.unwrap_model", "torch.distributed.all_reduce", "float", "torch.distributed.broadcast"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.unwrap_model"], ["", "def", "distribute_bn", "(", "model", ",", "world_size", ",", "reduce", "=", "False", ")", ":", "\n", "# ensure every node has the same running bn stats", "\n", "    ", "for", "bn_name", ",", "bn_buf", "in", "unwrap_model", "(", "model", ")", ".", "named_buffers", "(", "recurse", "=", "True", ")", ":", "\n", "        ", "if", "(", "'running_mean'", "in", "bn_name", ")", "or", "(", "'running_var'", "in", "bn_name", ")", ":", "\n", "            ", "if", "reduce", ":", "\n", "# average bn stats across whole group", "\n", "                ", "torch", ".", "distributed", ".", "all_reduce", "(", "bn_buf", ",", "op", "=", "dist", ".", "ReduceOp", ".", "SUM", ")", "\n", "bn_buf", "/=", "float", "(", "world_size", ")", "\n", "", "else", ":", "\n", "# broadcast bn stats from rank 0 to whole group", "\n", "                ", "torch", ".", "distributed", ".", "broadcast", "(", "bn_buf", ",", "0", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.__init__": [[9, 11], ["metrics.AverageMeter.reset"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_in_tar.TarState.reset"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.reset": [[12, 17], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "val", "=", "0", "\n", "self", ".", "avg", "=", "0", "\n", "self", ".", "sum", "=", "0", "\n", "self", ".", "count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update": [[18, 23], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "val", ",", "n", "=", "1", ")", ":", "\n", "        ", "self", ".", "val", "=", "val", "\n", "self", ".", "sum", "+=", "val", "*", "n", "\n", "self", ".", "count", "+=", "n", "\n", "self", ".", "avg", "=", "self", ".", "sum", "/", "self", ".", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.accuracy": [[25, 33], ["min", "target.size", "output.topk", "pred.t.t", "pred.t.eq", "max", "target.reshape().expand_as", "output.size", "target.reshape", "correct[].reshape().float().sum", "correct[].reshape().float", "correct[].reshape", "min"], "function", ["None"], ["", "", "def", "accuracy", "(", "output", ",", "target", ",", "topk", "=", "(", "1", ",", ")", ")", ":", "\n", "    ", "\"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"", "\n", "maxk", "=", "min", "(", "max", "(", "topk", ")", ",", "output", ".", "size", "(", ")", "[", "1", "]", ")", "\n", "batch_size", "=", "target", ".", "size", "(", "0", ")", "\n", "_", ",", "pred", "=", "output", ".", "topk", "(", "maxk", ",", "1", ",", "True", ",", "True", ")", "\n", "pred", "=", "pred", ".", "t", "(", ")", "\n", "correct", "=", "pred", ".", "eq", "(", "target", ".", "reshape", "(", "1", ",", "-", "1", ")", ".", "expand_as", "(", "pred", ")", ")", "\n", "return", "[", "correct", "[", ":", "min", "(", "k", ",", "maxk", ")", "]", ".", "reshape", "(", "-", "1", ")", ".", "float", "(", ")", ".", "sum", "(", "0", ")", "*", "100.", "/", "batch_size", "for", "k", "in", "topk", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.clip_grad.dispatch_clip_grad": [[6, 23], ["torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_value_", "timm.utils.agc.adaptive_clip_grad"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.agc.adaptive_clip_grad"], ["def", "dispatch_clip_grad", "(", "parameters", ",", "value", ":", "float", ",", "mode", ":", "str", "=", "'norm'", ",", "norm_type", ":", "float", "=", "2.0", ")", ":", "\n", "    ", "\"\"\" Dispatch to gradient clipping method\n\n    Args:\n        parameters (Iterable): model parameters to clip\n        value (float): clipping value/factor/norm, mode dependant\n        mode (str): clipping mode, one of 'norm', 'value', 'agc'\n        norm_type (float): p-norm, default 2.0\n    \"\"\"", "\n", "if", "mode", "==", "'norm'", ":", "\n", "        ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "parameters", ",", "value", ",", "norm_type", "=", "norm_type", ")", "\n", "", "elif", "mode", "==", "'value'", ":", "\n", "        ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_value_", "(", "parameters", ",", "value", ")", "\n", "", "elif", "mode", "==", "'agc'", ":", "\n", "        ", "adaptive_clip_grad", "(", "parameters", ",", "value", ",", "norm_type", "=", "norm_type", ")", "\n", "", "else", ":", "\n", "        ", "assert", "False", ",", "f\"Unknown clip mode ({mode}).\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.random.random_seed": [[6, 10], ["torch.manual_seed", "numpy.random.seed", "random.seed"], "function", ["None"], ["def", "random_seed", "(", "seed", "=", "42", ",", "rank", "=", "0", ")", ":", "\n", "    ", "torch", ".", "manual_seed", "(", "seed", "+", "rank", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", "+", "rank", ")", "\n", "random", ".", "seed", "(", "seed", "+", "rank", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.ActivationStatsHook.__init__": [[60, 70], ["dict", "zip", "len", "len", "ValueError", "model.ActivationStatsHook.register_hook"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.ActivationStatsHook.register_hook"], ["def", "__init__", "(", "self", ",", "model", ",", "hook_fn_locs", ",", "hook_fns", ")", ":", "\n", "        ", "self", ".", "model", "=", "model", "\n", "self", ".", "hook_fn_locs", "=", "hook_fn_locs", "\n", "self", ".", "hook_fns", "=", "hook_fns", "\n", "if", "len", "(", "hook_fn_locs", ")", "!=", "len", "(", "hook_fns", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Please provide `hook_fns` for each `hook_fn_locs`, \\\n                their lengths are different.\"", ")", "\n", "", "self", ".", "stats", "=", "dict", "(", "(", "hook_fn", ".", "__name__", ",", "[", "]", ")", "for", "hook_fn", "in", "hook_fns", ")", "\n", "for", "hook_fn_loc", ",", "hook_fn", "in", "zip", "(", "hook_fn_locs", ",", "hook_fns", ")", ":", "\n", "            ", "self", ".", "register_hook", "(", "hook_fn_loc", ",", "hook_fn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.ActivationStatsHook._create_hook": [[71, 77], ["hook_fn", "model.ActivationStatsHook.stats[].append"], "methods", ["None"], ["", "", "def", "_create_hook", "(", "self", ",", "hook_fn", ")", ":", "\n", "        ", "def", "append_activation_stats", "(", "module", ",", "input", ",", "output", ")", ":", "\n", "            ", "out", "=", "hook_fn", "(", "module", ",", "input", ",", "output", ")", "\n", "self", ".", "stats", "[", "hook_fn", ".", "__name__", "]", ".", "append", "(", "out", ")", "\n", "\n", "", "return", "append_activation_stats", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.ActivationStatsHook.register_hook": [[78, 83], ["model.ActivationStatsHook.model.named_modules", "module.register_forward_hook", "fnmatch.fnmatch", "model.ActivationStatsHook._create_hook"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_modules", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.ActivationStatsHook._create_hook"], ["", "def", "register_hook", "(", "self", ",", "hook_fn_loc", ",", "hook_fn", ")", ":", "\n", "        ", "for", "name", ",", "module", "in", "self", ".", "model", ".", "named_modules", "(", ")", ":", "\n", "            ", "if", "not", "fnmatch", ".", "fnmatch", "(", "name", ",", "hook_fn_loc", ")", ":", "\n", "                ", "continue", "\n", "", "module", ".", "register_forward_hook", "(", "self", ".", "_create_hook", "(", "hook_fn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.unwrap_model": [[13, 18], ["isinstance", "model.unwrap_model", "hasattr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.unwrap_model"], ["def", "unwrap_model", "(", "model", ")", ":", "\n", "    ", "if", "isinstance", "(", "model", ",", "ModelEma", ")", ":", "\n", "        ", "return", "unwrap_model", "(", "model", ".", "ema", ")", "\n", "", "else", ":", "\n", "        ", "return", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.get_state_dict": [[20, 22], ["unwrap_fn().state_dict", "unwrap_fn"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict"], ["", "", "def", "get_state_dict", "(", "model", ",", "unwrap_fn", "=", "unwrap_model", ")", ":", "\n", "    ", "return", "unwrap_fn", "(", "model", ")", ".", "state_dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.avg_sq_ch_mean": [[24, 28], ["torch.mean().item", "torch.mean", "output.mean"], "function", ["None"], ["", "def", "avg_sq_ch_mean", "(", "model", ",", "input", ",", "output", ")", ":", "\n", "    ", "\"\"\" calculate average channel square mean of output activations\n    \"\"\"", "\n", "return", "torch", ".", "mean", "(", "output", ".", "mean", "(", "axis", "=", "[", "0", ",", "2", ",", "3", "]", ")", "**", "2", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.avg_ch_var": [[30, 34], ["torch.mean().item", "torch.mean", "output.var"], "function", ["None"], ["", "def", "avg_ch_var", "(", "model", ",", "input", ",", "output", ")", ":", "\n", "    ", "\"\"\" calculate average channel variance of output activations\n    \"\"\"", "\n", "return", "torch", ".", "mean", "(", "output", ".", "var", "(", "axis", "=", "[", "0", ",", "2", ",", "3", "]", ")", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.avg_ch_var_residual": [[36, 40], ["torch.mean().item", "torch.mean", "output.var"], "function", ["None"], ["", "def", "avg_ch_var_residual", "(", "model", ",", "input", ",", "output", ")", ":", "\n", "    ", "\"\"\" calculate average channel variance of output activations\n    \"\"\"", "\n", "return", "torch", ".", "mean", "(", "output", ".", "var", "(", "axis", "=", "[", "0", ",", "2", ",", "3", "]", ")", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.extract_spp_stats": [[85, 101], ["torch.normal", "model.ActivationStatsHook", "model"], "function", ["None"], ["", "", "", "def", "extract_spp_stats", "(", "\n", "model", ",", "\n", "hook_fn_locs", ",", "\n", "hook_fns", ",", "\n", "input_shape", "=", "[", "8", ",", "3", ",", "224", ",", "224", "]", ")", ":", "\n", "    ", "\"\"\"Extract average square channel mean and variance of activations during \n    forward pass to plot Signal Propogation Plots (SPP).\n    \n    Paper: https://arxiv.org/abs/2101.08692\n\n    Example Usage: https://gist.github.com/amaarora/6e56942fcb46e67ba203f3009b30d950\n    \"\"\"", "\n", "x", "=", "torch", ".", "normal", "(", "0.", ",", "1.", ",", "input_shape", ")", "\n", "hook", "=", "ActivationStatsHook", "(", "model", ",", "hook_fn_locs", "=", "hook_fn_locs", ",", "hook_fns", "=", "hook_fns", ")", "\n", "_", "=", "model", "(", "x", ")", "\n", "return", "hook", ".", "stats", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.freeze_batch_norm_2d": [[103, 134], ["isinstance", "torchvision.ops.misc.FrozenBatchNorm2d", "module.named_children", "module.weight.data.clone().detach", "module.bias.data.clone().detach", "model.freeze_batch_norm_2d", "torchvision.ops.misc.FrozenBatchNorm2d.add_module", "module.weight.data.clone", "module.bias.data.clone"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.freeze_batch_norm_2d"], ["", "def", "freeze_batch_norm_2d", "(", "module", ")", ":", "\n", "    ", "\"\"\"\n    Converts all `BatchNorm2d` and `SyncBatchNorm` layers of provided module into `FrozenBatchNorm2d`. If `module` is\n    itself an instance of either `BatchNorm2d` or `SyncBatchNorm`, it is converted into `FrozenBatchNorm2d` and\n    returned. Otherwise, the module is walked recursively and submodules are converted in place.\n\n    Args:\n        module (torch.nn.Module): Any PyTorch module.\n\n    Returns:\n        torch.nn.Module: Resulting module\n\n    Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762\n    \"\"\"", "\n", "res", "=", "module", "\n", "if", "isinstance", "(", "module", ",", "(", "torch", ".", "nn", ".", "modules", ".", "batchnorm", ".", "BatchNorm2d", ",", "torch", ".", "nn", ".", "modules", ".", "batchnorm", ".", "SyncBatchNorm", ")", ")", ":", "\n", "        ", "res", "=", "FrozenBatchNorm2d", "(", "module", ".", "num_features", ")", "\n", "res", ".", "num_features", "=", "module", ".", "num_features", "\n", "res", ".", "affine", "=", "module", ".", "affine", "\n", "if", "module", ".", "affine", ":", "\n", "            ", "res", ".", "weight", ".", "data", "=", "module", ".", "weight", ".", "data", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "res", ".", "bias", ".", "data", "=", "module", ".", "bias", ".", "data", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "res", ".", "running_mean", ".", "data", "=", "module", ".", "running_mean", ".", "data", "\n", "res", ".", "running_var", ".", "data", "=", "module", ".", "running_var", ".", "data", "\n", "res", ".", "eps", "=", "module", ".", "eps", "\n", "", "else", ":", "\n", "        ", "for", "name", ",", "child", "in", "module", ".", "named_children", "(", ")", ":", "\n", "            ", "new_child", "=", "freeze_batch_norm_2d", "(", "child", ")", "\n", "if", "new_child", "is", "not", "child", ":", "\n", "                ", "res", ".", "add_module", "(", "name", ",", "new_child", ")", "\n", "", "", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.unfreeze_batch_norm_2d": [[136, 165], ["isinstance", "torch.nn.BatchNorm2d", "module.named_children", "module.weight.data.clone().detach", "module.bias.data.clone().detach", "model.unfreeze_batch_norm_2d", "torch.nn.BatchNorm2d.add_module", "module.weight.data.clone", "module.bias.data.clone"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.unfreeze_batch_norm_2d"], ["", "def", "unfreeze_batch_norm_2d", "(", "module", ")", ":", "\n", "    ", "\"\"\"\n    Converts all `FrozenBatchNorm2d` layers of provided module into `BatchNorm2d`. If `module` is itself and instance\n    of `FrozenBatchNorm2d`, it is converted into `BatchNorm2d` and returned. Otherwise, the module is walked\n    recursively and submodules are converted in place.\n\n    Args:\n        module (torch.nn.Module): Any PyTorch module.\n\n    Returns:\n        torch.nn.Module: Resulting module\n\n    Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762\n    \"\"\"", "\n", "res", "=", "module", "\n", "if", "isinstance", "(", "module", ",", "FrozenBatchNorm2d", ")", ":", "\n", "        ", "res", "=", "torch", ".", "nn", ".", "BatchNorm2d", "(", "module", ".", "num_features", ")", "\n", "if", "module", ".", "affine", ":", "\n", "            ", "res", ".", "weight", ".", "data", "=", "module", ".", "weight", ".", "data", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "res", ".", "bias", ".", "data", "=", "module", ".", "bias", ".", "data", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "res", ".", "running_mean", ".", "data", "=", "module", ".", "running_mean", ".", "data", "\n", "res", ".", "running_var", ".", "data", "=", "module", ".", "running_var", ".", "data", "\n", "res", ".", "eps", "=", "module", ".", "eps", "\n", "", "else", ":", "\n", "        ", "for", "name", ",", "child", "in", "module", ".", "named_children", "(", ")", ":", "\n", "            ", "new_child", "=", "unfreeze_batch_norm_2d", "(", "child", ")", "\n", "if", "new_child", "is", "not", "child", ":", "\n", "                ", "res", ".", "add_module", "(", "name", ",", "new_child", ")", "\n", "", "", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model._freeze_unfreeze": [[167, 224], ["isinstance", "isinstance", "zip", "AssertionError", "root_module.get_submodule", "len", "list", "m.parameters", "zip", "name.rsplit", "model.freeze_batch_norm_2d", "isinstance", "model.unfreeze_batch_norm_2d", "isinstance", "root_module.named_children", "len", "module.get_submodule().add_module", "module.add_module", "model._freeze_unfreeze._add_submodule"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.freeze_batch_norm_2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.unfreeze_batch_norm_2d"], ["", "def", "_freeze_unfreeze", "(", "root_module", ",", "submodules", "=", "[", "]", ",", "include_bn_running_stats", "=", "True", ",", "mode", "=", "'freeze'", ")", ":", "\n", "    ", "\"\"\"\n    Freeze or unfreeze parameters of the specified modules and those of all their hierarchical descendants. This is\n    done in place.\n    Args:\n        root_module (nn.Module, optional): Root module relative to which the `submodules` are referenced.\n        submodules (list[str]): List of modules for which the parameters will be (un)frozen. They are to be provided as\n            named modules relative to the root module (accessible via `root_module.named_modules()`). An empty list\n            means that the whole root module will be (un)frozen. Defaults to []\n        include_bn_running_stats (bool): Whether to also (un)freeze the running statistics of batch norm 2d layers.\n            Defaults to `True`.\n        mode (bool): Whether to freeze (\"freeze\") or unfreeze (\"unfreeze\"). Defaults to `\"freeze\"`.\n    \"\"\"", "\n", "assert", "mode", "in", "[", "\"freeze\"", ",", "\"unfreeze\"", "]", ",", "'`mode` must be one of \"freeze\" or \"unfreeze\"'", "\n", "\n", "if", "isinstance", "(", "root_module", ",", "(", "torch", ".", "nn", ".", "modules", ".", "batchnorm", ".", "BatchNorm2d", ",", "torch", ".", "nn", ".", "modules", ".", "batchnorm", ".", "SyncBatchNorm", ")", ")", ":", "\n", "# Raise assertion here because we can't convert it in place", "\n", "        ", "raise", "AssertionError", "(", "\n", "\"You have provided a batch norm layer as the `root module`. Please use \"", "\n", "\"`timm.utils.model.freeze_batch_norm_2d` or `timm.utils.model.unfreeze_batch_norm_2d` instead.\"", ")", "\n", "\n", "", "if", "isinstance", "(", "submodules", ",", "str", ")", ":", "\n", "        ", "submodules", "=", "[", "submodules", "]", "\n", "\n", "", "named_modules", "=", "submodules", "\n", "submodules", "=", "[", "root_module", ".", "get_submodule", "(", "m", ")", "for", "m", "in", "submodules", "]", "\n", "\n", "if", "not", "len", "(", "submodules", ")", ":", "\n", "        ", "named_modules", ",", "submodules", "=", "list", "(", "zip", "(", "*", "root_module", ".", "named_children", "(", ")", ")", ")", "\n", "\n", "", "for", "n", ",", "m", "in", "zip", "(", "named_modules", ",", "submodules", ")", ":", "\n", "# (Un)freeze parameters", "\n", "        ", "for", "p", "in", "m", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "False", "if", "mode", "==", "'freeze'", "else", "True", "\n", "", "if", "include_bn_running_stats", ":", "\n", "# Helper to add submodule specified as a named_module", "\n", "            ", "def", "_add_submodule", "(", "module", ",", "name", ",", "submodule", ")", ":", "\n", "                ", "split", "=", "name", ".", "rsplit", "(", "'.'", ",", "1", ")", "\n", "if", "len", "(", "split", ")", ">", "1", ":", "\n", "                    ", "module", ".", "get_submodule", "(", "split", "[", "0", "]", ")", ".", "add_module", "(", "split", "[", "1", "]", ",", "submodule", ")", "\n", "", "else", ":", "\n", "                    ", "module", ".", "add_module", "(", "name", ",", "submodule", ")", "\n", "\n", "# Freeze batch norm", "\n", "", "", "if", "mode", "==", "'freeze'", ":", "\n", "                ", "res", "=", "freeze_batch_norm_2d", "(", "m", ")", "\n", "# It's possible that `m` is a type of BatchNorm in itself, in which case `unfreeze_batch_norm_2d` won't", "\n", "# convert it in place, but will return the converted result. In this case `res` holds the converted", "\n", "# result and we may try to re-assign the named module", "\n", "if", "isinstance", "(", "m", ",", "(", "torch", ".", "nn", ".", "modules", ".", "batchnorm", ".", "BatchNorm2d", ",", "torch", ".", "nn", ".", "modules", ".", "batchnorm", ".", "SyncBatchNorm", ")", ")", ":", "\n", "                    ", "_add_submodule", "(", "root_module", ",", "n", ",", "res", ")", "\n", "# Unfreeze batch norm", "\n", "", "", "else", ":", "\n", "                ", "res", "=", "unfreeze_batch_norm_2d", "(", "m", ")", "\n", "# Ditto. See note above in mode == 'freeze' branch", "\n", "if", "isinstance", "(", "m", ",", "FrozenBatchNorm2d", ")", ":", "\n", "                    ", "_add_submodule", "(", "root_module", ",", "n", ",", "res", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.freeze": [[226, 258], ["model._freeze_unfreeze"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model._freeze_unfreeze"], ["", "", "", "", "", "def", "freeze", "(", "root_module", ",", "submodules", "=", "[", "]", ",", "include_bn_running_stats", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Freeze parameters of the specified modules and those of all their hierarchical descendants. This is done in place.\n    Args:\n        root_module (nn.Module): Root module relative to which `submodules` are referenced.\n        submodules (list[str]): List of modules for which the parameters will be frozen. They are to be provided as\n            named modules relative to the root module (accessible via `root_module.named_modules()`). An empty list\n            means that the whole root module will be frozen. Defaults to `[]`.\n        include_bn_running_stats (bool): Whether to also freeze the running statistics of `BatchNorm2d` and\n            `SyncBatchNorm` layers. These will be converted to `FrozenBatchNorm2d` in place. Hint: During fine tuning,\n            it's good practice to freeze batch norm stats. And note that these are different to the affine parameters\n            which are just normal PyTorch parameters. Defaults to `True`.\n\n    Hint: If you want to freeze batch norm ONLY, use `timm.utils.model.freeze_batch_norm_2d`.\n\n    Examples::\n\n        >>> model = timm.create_model('resnet18')\n        >>> # Freeze up to and including layer2\n        >>> submodules = [n for n, _ in model.named_children()]\n        >>> print(submodules)\n        ['conv1', 'bn1', 'act1', 'maxpool', 'layer1', 'layer2', 'layer3', 'layer4', 'global_pool', 'fc']\n        >>> freeze(model, submodules[:submodules.index('layer2') + 1])\n        >>> # Check for yourself that it works as expected\n        >>> print(model.layer2[0].conv1.weight.requires_grad)\n        False\n        >>> print(model.layer3[0].conv1.weight.requires_grad)\n        True\n        >>> # Unfreeze\n        >>> unfreeze(model)\n    \"\"\"", "\n", "_freeze_unfreeze", "(", "root_module", ",", "submodules", ",", "include_bn_running_stats", "=", "include_bn_running_stats", ",", "mode", "=", "\"freeze\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model.unfreeze": [[260, 274], ["model._freeze_unfreeze"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model._freeze_unfreeze"], ["", "def", "unfreeze", "(", "root_module", ",", "submodules", "=", "[", "]", ",", "include_bn_running_stats", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Unfreeze parameters of the specified modules and those of all their hierarchical descendants. This is done in place.\n    Args:\n        root_module (nn.Module): Root module relative to which `submodules` are referenced.\n        submodules (list[str]): List of submodules for which the parameters will be (un)frozen. They are to be provided\n            as named modules relative to the root module (accessible via `root_module.named_modules()`). An empty\n            list means that the whole root module will be unfrozen. Defaults to `[]`.\n        include_bn_running_stats (bool): Whether to also unfreeze the running statistics of `FrozenBatchNorm2d` layers.\n            These will be converted to `BatchNorm2d` in place. Defaults to `True`.\n\n    See example in docstring for `freeze`.\n    \"\"\"", "\n", "_freeze_unfreeze", "(", "root_module", ",", "submodules", ",", "include_bn_running_stats", "=", "include_bn_running_stats", ",", "mode", "=", "\"unfreeze\"", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.misc.natural_key": [[8, 11], ["s.isdigit", "int", "re.split", "string_.lower"], "function", ["None"], ["def", "natural_key", "(", "string_", ")", ":", "\n", "    ", "\"\"\"See http://www.codinghorror.com/blog/archives/001018.html\"\"\"", "\n", "return", "[", "int", "(", "s", ")", "if", "s", ".", "isdigit", "(", ")", "else", "s", "for", "s", "in", "re", ".", "split", "(", "r'(\\d+)'", ",", "string_", ".", "lower", "(", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.misc.add_bool_arg": [[13, 19], ["name.replace", "parser.add_mutually_exclusive_group", "parser.add_mutually_exclusive_group.add_argument", "parser.add_mutually_exclusive_group.add_argument", "parser.set_defaults"], "function", ["None"], ["", "def", "add_bool_arg", "(", "parser", ",", "name", ",", "default", "=", "False", ",", "help", "=", "''", ")", ":", "\n", "    ", "dest_name", "=", "name", ".", "replace", "(", "'-'", ",", "'_'", ")", "\n", "group", "=", "parser", ".", "add_mutually_exclusive_group", "(", "required", "=", "False", ")", "\n", "group", ".", "add_argument", "(", "'--'", "+", "name", ",", "dest", "=", "dest_name", ",", "action", "=", "'store_true'", ",", "help", "=", "help", ")", "\n", "group", ".", "add_argument", "(", "'--no-'", "+", "name", ",", "dest", "=", "dest_name", ",", "action", "=", "'store_false'", ",", "help", "=", "help", ")", "\n", "parser", ".", "set_defaults", "(", "**", "{", "dest_name", ":", "default", "}", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.log.FormatterNoInfo.__init__": [[10, 12], ["logging.Formatter.__init__", "logging.Formatter.__init__", "logging.Formatter.__init__", "logging.Formatter.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "fmt", "=", "'%(levelname)s: %(message)s'", ")", ":", "\n", "        ", "logging", ".", "Formatter", ".", "__init__", "(", "self", ",", "fmt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.log.FormatterNoInfo.format": [[13, 17], ["logging.Formatter.format", "logging.Formatter.format", "logging.Formatter.format", "logging.Formatter.format", "str", "record.getMessage"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.log.FormatterNoInfo.format", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.log.FormatterNoInfo.format", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.log.FormatterNoInfo.format", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.log.FormatterNoInfo.format"], ["", "def", "format", "(", "self", ",", "record", ")", ":", "\n", "        ", "if", "record", ".", "levelno", "==", "logging", ".", "INFO", ":", "\n", "            ", "return", "str", "(", "record", ".", "getMessage", "(", ")", ")", "\n", "", "return", "logging", ".", "Formatter", ".", "format", "(", "self", ",", "record", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.log.setup_default_logging": [[19, 29], ["logging.StreamHandler", "logging.StreamHandler", "logging.StreamHandler.setFormatter", "logging.root.addHandler", "logging.root.addHandler", "logging.root.setLevel", "logging.root.setLevel", "log.FormatterNoInfo", "logging.handlers.RotatingFileHandler", "logging.handlers.RotatingFileHandler", "logging.Formatter", "logging.Formatter", "logging.handlers.RotatingFileHandler.setFormatter", "logging.root.addHandler", "logging.root.addHandler"], "function", ["None"], ["", "", "def", "setup_default_logging", "(", "default_level", "=", "logging", ".", "INFO", ",", "log_path", "=", "''", ")", ":", "\n", "    ", "console_handler", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "console_handler", ".", "setFormatter", "(", "FormatterNoInfo", "(", ")", ")", "\n", "logging", ".", "root", ".", "addHandler", "(", "console_handler", ")", "\n", "logging", ".", "root", ".", "setLevel", "(", "default_level", ")", "\n", "if", "log_path", ":", "\n", "        ", "file_handler", "=", "logging", ".", "handlers", ".", "RotatingFileHandler", "(", "log_path", ",", "maxBytes", "=", "(", "1024", "**", "2", "*", "2", ")", ",", "backupCount", "=", "3", ")", "\n", "file_formatter", "=", "logging", ".", "Formatter", "(", "\"%(asctime)s - %(name)20s: [%(levelname)8s] - %(message)s\"", ")", "\n", "file_handler", ".", "setFormatter", "(", "file_formatter", ")", "\n", "logging", ".", "root", ".", "addHandler", "(", "file_handler", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.jit.set_jit_legacy": [[10, 20], ["hasattr", "torch._C._jit_set_profiling_executor", "torch._C._jit_set_profiling_mode", "torch._C._jit_override_can_fuse_on_gpu"], "function", ["None"], ["def", "set_jit_legacy", "(", ")", ":", "\n", "    ", "\"\"\" Set JIT executor to legacy w/ support for op fusion\n    This is hopefully a temporary need in 1.5/1.5.1/1.6 to restore performance due to changes\n    in the JIT exectutor. These API are not supported so could change.\n    \"\"\"", "\n", "#", "\n", "assert", "hasattr", "(", "torch", ".", "_C", ",", "'_jit_set_profiling_executor'", ")", ",", "\"Old JIT behavior doesn't exist!\"", "\n", "torch", ".", "_C", ".", "_jit_set_profiling_executor", "(", "False", ")", "\n", "torch", ".", "_C", ".", "_jit_set_profiling_mode", "(", "False", ")", "\n", "torch", ".", "_C", ".", "_jit_override_can_fuse_on_gpu", "(", "True", ")", "\n", "#torch._C._jit_set_texpr_fuser_enabled(True)", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.jit.set_jit_fuser": [[23, 51], ["torch._C._jit_set_profiling_executor", "torch._C._jit_set_profiling_mode", "torch._C._jit_override_can_fuse_on_cpu", "torch._C._jit_override_can_fuse_on_gpu", "torch._C._jit_set_texpr_fuser_enabled", "torch._C._jit_set_profiling_executor", "torch._C._jit_set_profiling_mode", "torch._C._jit_override_can_fuse_on_gpu", "torch._C._jit_set_texpr_fuser_enabled", "torch._C._jit_set_texpr_fuser_enabled", "torch._C._jit_set_profiling_executor", "torch._C._jit_set_profiling_mode", "torch._C._jit_can_fuse_on_cpu", "torch._C._jit_can_fuse_on_gpu", "torch._C._jit_override_can_fuse_on_cpu", "torch._C._jit_override_can_fuse_on_gpu", "torch._C._jit_set_nvfuser_guard_mode", "torch._C._jit_set_nvfuser_enabled"], "function", ["None"], ["", "def", "set_jit_fuser", "(", "fuser", ")", ":", "\n", "    ", "if", "fuser", "==", "\"te\"", ":", "\n", "# default fuser should be == 'te'", "\n", "        ", "torch", ".", "_C", ".", "_jit_set_profiling_executor", "(", "True", ")", "\n", "torch", ".", "_C", ".", "_jit_set_profiling_mode", "(", "True", ")", "\n", "torch", ".", "_C", ".", "_jit_override_can_fuse_on_cpu", "(", "False", ")", "\n", "torch", ".", "_C", ".", "_jit_override_can_fuse_on_gpu", "(", "True", ")", "\n", "torch", ".", "_C", ".", "_jit_set_texpr_fuser_enabled", "(", "True", ")", "\n", "", "elif", "fuser", "==", "\"old\"", "or", "fuser", "==", "\"legacy\"", ":", "\n", "        ", "torch", ".", "_C", ".", "_jit_set_profiling_executor", "(", "False", ")", "\n", "torch", ".", "_C", ".", "_jit_set_profiling_mode", "(", "False", ")", "\n", "torch", ".", "_C", ".", "_jit_override_can_fuse_on_gpu", "(", "True", ")", "\n", "torch", ".", "_C", ".", "_jit_set_texpr_fuser_enabled", "(", "False", ")", "\n", "", "elif", "fuser", "==", "\"nvfuser\"", "or", "fuser", "==", "\"nvf\"", ":", "\n", "        ", "os", ".", "environ", "[", "'PYTORCH_NVFUSER_DISABLE_FALLBACK'", "]", "=", "'1'", "\n", "#os.environ['PYTORCH_NVFUSER_DISABLE_FMA'] = '1'", "\n", "#os.environ['PYTORCH_NVFUSER_JIT_OPT_LEVEL'] = '0'", "\n", "torch", ".", "_C", ".", "_jit_set_texpr_fuser_enabled", "(", "False", ")", "\n", "torch", ".", "_C", ".", "_jit_set_profiling_executor", "(", "True", ")", "\n", "torch", ".", "_C", ".", "_jit_set_profiling_mode", "(", "True", ")", "\n", "torch", ".", "_C", ".", "_jit_can_fuse_on_cpu", "(", ")", "\n", "torch", ".", "_C", ".", "_jit_can_fuse_on_gpu", "(", ")", "\n", "torch", ".", "_C", ".", "_jit_override_can_fuse_on_cpu", "(", "False", ")", "\n", "torch", ".", "_C", ".", "_jit_override_can_fuse_on_gpu", "(", "False", ")", "\n", "torch", ".", "_C", ".", "_jit_set_nvfuser_guard_mode", "(", "True", ")", "\n", "torch", ".", "_C", ".", "_jit_set_nvfuser_enabled", "(", "True", ")", "\n", "", "else", ":", "\n", "        ", "assert", "False", ",", "f\"Invalid jit fuser ({fuser})\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.cuda.ApexScaler.__call__": [[20, 26], ["optimizer.step", "amp.scale_loss", "scaled_loss.backward", "clip_grad.dispatch_clip_grad", "amp.master_params"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardMishJitAutoFn.backward", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.clip_grad.dispatch_clip_grad"], ["def", "__call__", "(", "self", ",", "loss", ",", "optimizer", ",", "clip_grad", "=", "None", ",", "clip_mode", "=", "'norm'", ",", "parameters", "=", "None", ",", "create_graph", "=", "False", ")", ":", "\n", "        ", "with", "amp", ".", "scale_loss", "(", "loss", ",", "optimizer", ")", "as", "scaled_loss", ":", "\n", "            ", "scaled_loss", ".", "backward", "(", "create_graph", "=", "create_graph", ")", "\n", "", "if", "clip_grad", "is", "not", "None", ":", "\n", "            ", "dispatch_clip_grad", "(", "amp", ".", "master_params", "(", "optimizer", ")", ",", "clip_grad", ",", "mode", "=", "clip_mode", ")", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.cuda.ApexScaler.state_dict": [[27, 30], ["amp.state_dict"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "if", "'state_dict'", "in", "amp", ".", "__dict__", ":", "\n", "            ", "return", "amp", ".", "state_dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.cuda.ApexScaler.load_state_dict": [[31, 34], ["amp.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_state_dict"], ["", "", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "if", "'load_state_dict'", "in", "amp", ".", "__dict__", ":", "\n", "            ", "amp", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.cuda.NativeScaler.__init__": [[39, 41], ["torch.cuda.amp.GradScaler"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "_scaler", "=", "torch", ".", "cuda", ".", "amp", ".", "GradScaler", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.cuda.NativeScaler.__call__": [[42, 50], ["cuda.NativeScaler._scaler.scale().backward", "cuda.NativeScaler._scaler.step", "cuda.NativeScaler._scaler.update", "cuda.NativeScaler._scaler.unscale_", "clip_grad.dispatch_clip_grad", "cuda.NativeScaler._scaler.scale"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardMishJitAutoFn.backward", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.clip_grad.dispatch_clip_grad"], ["", "def", "__call__", "(", "self", ",", "loss", ",", "optimizer", ",", "clip_grad", "=", "None", ",", "clip_mode", "=", "'norm'", ",", "parameters", "=", "None", ",", "create_graph", "=", "False", ")", ":", "\n", "        ", "self", ".", "_scaler", ".", "scale", "(", "loss", ")", ".", "backward", "(", "create_graph", "=", "create_graph", ")", "\n", "if", "clip_grad", "is", "not", "None", ":", "\n", "            ", "assert", "parameters", "is", "not", "None", "\n", "self", ".", "_scaler", ".", "unscale_", "(", "optimizer", ")", "# unscale the gradients of optimizer's assigned params in-place", "\n", "dispatch_clip_grad", "(", "parameters", ",", "clip_grad", ",", "mode", "=", "clip_mode", ")", "\n", "", "self", ".", "_scaler", ".", "step", "(", "optimizer", ")", "\n", "self", ".", "_scaler", ".", "update", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.cuda.NativeScaler.state_dict": [[51, 53], ["cuda.NativeScaler._scaler.state_dict"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_scaler", ".", "state_dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.cuda.NativeScaler.load_state_dict": [[54, 56], ["cuda.NativeScaler._scaler.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "self", ".", "_scaler", ".", "load_state_dict", "(", "state_dict", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.poly_lr.PolyLRScheduler.__init__": [[24, 68], ["scheduler.Scheduler.__init__", "_logger.warning", "super().update_groups"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler.update_groups"], ["def", "__init__", "(", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "t_initial", ":", "int", ",", "\n", "power", ":", "float", "=", "0.5", ",", "\n", "lr_min", ":", "float", "=", "0.", ",", "\n", "cycle_mul", ":", "float", "=", "1.", ",", "\n", "cycle_decay", ":", "float", "=", "1.", ",", "\n", "cycle_limit", ":", "int", "=", "1", ",", "\n", "warmup_t", "=", "0", ",", "\n", "warmup_lr_init", "=", "0", ",", "\n", "warmup_prefix", "=", "False", ",", "\n", "t_in_epochs", "=", "True", ",", "\n", "noise_range_t", "=", "None", ",", "\n", "noise_pct", "=", "0.67", ",", "\n", "noise_std", "=", "1.0", ",", "\n", "noise_seed", "=", "42", ",", "\n", "k_decay", "=", "1.0", ",", "\n", "initialize", "=", "True", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "optimizer", ",", "param_group_field", "=", "\"lr\"", ",", "\n", "noise_range_t", "=", "noise_range_t", ",", "noise_pct", "=", "noise_pct", ",", "noise_std", "=", "noise_std", ",", "noise_seed", "=", "noise_seed", ",", "\n", "initialize", "=", "initialize", ")", "\n", "\n", "assert", "t_initial", ">", "0", "\n", "assert", "lr_min", ">=", "0", "\n", "if", "t_initial", "==", "1", "and", "cycle_mul", "==", "1", "and", "cycle_decay", "==", "1", ":", "\n", "            ", "_logger", ".", "warning", "(", "\"Cosine annealing scheduler will have no effect on the learning \"", "\n", "\"rate since t_initial = t_mul = eta_mul = 1.\"", ")", "\n", "", "self", ".", "t_initial", "=", "t_initial", "\n", "self", ".", "power", "=", "power", "\n", "self", ".", "lr_min", "=", "lr_min", "\n", "self", ".", "cycle_mul", "=", "cycle_mul", "\n", "self", ".", "cycle_decay", "=", "cycle_decay", "\n", "self", ".", "cycle_limit", "=", "cycle_limit", "\n", "self", ".", "warmup_t", "=", "warmup_t", "\n", "self", ".", "warmup_lr_init", "=", "warmup_lr_init", "\n", "self", ".", "warmup_prefix", "=", "warmup_prefix", "\n", "self", ".", "t_in_epochs", "=", "t_in_epochs", "\n", "self", ".", "k_decay", "=", "k_decay", "\n", "if", "self", ".", "warmup_t", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "(", "v", "-", "warmup_lr_init", ")", "/", "self", ".", "warmup_t", "for", "v", "in", "self", ".", "base_values", "]", "\n", "super", "(", ")", ".", "update_groups", "(", "self", ".", "warmup_lr_init", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "1", "for", "_", "in", "self", ".", "base_values", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.poly_lr.PolyLRScheduler._get_lr": [[69, 98], ["math.floor", "math.log"], "methods", ["None"], ["", "", "def", "_get_lr", "(", "self", ",", "t", ")", ":", "\n", "        ", "if", "t", "<", "self", ".", "warmup_t", ":", "\n", "            ", "lrs", "=", "[", "self", ".", "warmup_lr_init", "+", "t", "*", "s", "for", "s", "in", "self", ".", "warmup_steps", "]", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "warmup_prefix", ":", "\n", "                ", "t", "=", "t", "-", "self", ".", "warmup_t", "\n", "\n", "", "if", "self", ".", "cycle_mul", "!=", "1", ":", "\n", "                ", "i", "=", "math", ".", "floor", "(", "math", ".", "log", "(", "1", "-", "t", "/", "self", ".", "t_initial", "*", "(", "1", "-", "self", ".", "cycle_mul", ")", ",", "self", ".", "cycle_mul", ")", ")", "\n", "t_i", "=", "self", ".", "cycle_mul", "**", "i", "*", "self", ".", "t_initial", "\n", "t_curr", "=", "t", "-", "(", "1", "-", "self", ".", "cycle_mul", "**", "i", ")", "/", "(", "1", "-", "self", ".", "cycle_mul", ")", "*", "self", ".", "t_initial", "\n", "", "else", ":", "\n", "                ", "i", "=", "t", "//", "self", ".", "t_initial", "\n", "t_i", "=", "self", ".", "t_initial", "\n", "t_curr", "=", "t", "-", "(", "self", ".", "t_initial", "*", "i", ")", "\n", "\n", "", "gamma", "=", "self", ".", "cycle_decay", "**", "i", "\n", "lr_max_values", "=", "[", "v", "*", "gamma", "for", "v", "in", "self", ".", "base_values", "]", "\n", "k", "=", "self", ".", "k_decay", "\n", "\n", "if", "i", "<", "self", ".", "cycle_limit", ":", "\n", "                ", "lrs", "=", "[", "\n", "self", ".", "lr_min", "+", "(", "lr_max", "-", "self", ".", "lr_min", ")", "*", "(", "1", "-", "t_curr", "**", "k", "/", "t_i", "**", "k", ")", "**", "self", ".", "power", "\n", "for", "lr_max", "in", "lr_max_values", "\n", "]", "\n", "", "else", ":", "\n", "                ", "lrs", "=", "[", "self", ".", "lr_min", "for", "_", "in", "self", ".", "base_values", "]", "\n", "\n", "", "", "return", "lrs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.poly_lr.PolyLRScheduler.get_epoch_values": [[99, 104], ["poly_lr.PolyLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor._get_lr"], ["", "def", "get_epoch_values", "(", "self", ",", "epoch", ":", "int", ")", ":", "\n", "        ", "if", "self", ".", "t_in_epochs", ":", "\n", "            ", "return", "self", ".", "_get_lr", "(", "epoch", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.poly_lr.PolyLRScheduler.get_update_values": [[105, 110], ["poly_lr.PolyLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor._get_lr"], ["", "", "def", "get_update_values", "(", "self", ",", "num_updates", ":", "int", ")", ":", "\n", "        ", "if", "not", "self", ".", "t_in_epochs", ":", "\n", "            ", "return", "self", ".", "_get_lr", "(", "num_updates", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.poly_lr.PolyLRScheduler.get_cycle_length": [[111, 117], ["max", "int", "math.floor"], "methods", ["None"], ["", "", "def", "get_cycle_length", "(", "self", ",", "cycles", "=", "0", ")", ":", "\n", "        ", "cycles", "=", "max", "(", "1", ",", "cycles", "or", "self", ".", "cycle_limit", ")", "\n", "if", "self", ".", "cycle_mul", "==", "1.0", ":", "\n", "            ", "return", "self", ".", "t_initial", "*", "cycles", "\n", "", "else", ":", "\n", "            ", "return", "int", "(", "math", ".", "floor", "(", "-", "self", ".", "t_initial", "*", "(", "self", ".", "cycle_mul", "**", "cycles", "-", "1", ")", "/", "(", "1", "-", "self", ".", "cycle_mul", ")", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.plateau_lr.PlateauLRScheduler.__init__": [[15, 63], ["scheduler.Scheduler.__init__", "torch.optim.lr_scheduler.ReduceLROnPlateau", "super().update_groups"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler.update_groups"], ["def", "__init__", "(", "self", ",", "\n", "optimizer", ",", "\n", "decay_rate", "=", "0.1", ",", "\n", "patience_t", "=", "10", ",", "\n", "verbose", "=", "True", ",", "\n", "threshold", "=", "1e-4", ",", "\n", "cooldown_t", "=", "0", ",", "\n", "warmup_t", "=", "0", ",", "\n", "warmup_lr_init", "=", "0", ",", "\n", "lr_min", "=", "0", ",", "\n", "mode", "=", "'max'", ",", "\n", "noise_range_t", "=", "None", ",", "\n", "noise_type", "=", "'normal'", ",", "\n", "noise_pct", "=", "0.67", ",", "\n", "noise_std", "=", "1.0", ",", "\n", "noise_seed", "=", "None", ",", "\n", "initialize", "=", "True", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "optimizer", ",", "\n", "'lr'", ",", "\n", "noise_range_t", "=", "noise_range_t", ",", "\n", "noise_type", "=", "noise_type", ",", "\n", "noise_pct", "=", "noise_pct", ",", "\n", "noise_std", "=", "noise_std", ",", "\n", "noise_seed", "=", "noise_seed", ",", "\n", "initialize", "=", "initialize", ",", "\n", ")", "\n", "\n", "self", ".", "lr_scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "ReduceLROnPlateau", "(", "\n", "self", ".", "optimizer", ",", "\n", "patience", "=", "patience_t", ",", "\n", "factor", "=", "decay_rate", ",", "\n", "verbose", "=", "verbose", ",", "\n", "threshold", "=", "threshold", ",", "\n", "cooldown", "=", "cooldown_t", ",", "\n", "mode", "=", "mode", ",", "\n", "min_lr", "=", "lr_min", "\n", ")", "\n", "\n", "self", ".", "warmup_t", "=", "warmup_t", "\n", "self", ".", "warmup_lr_init", "=", "warmup_lr_init", "\n", "if", "self", ".", "warmup_t", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "(", "v", "-", "warmup_lr_init", ")", "/", "self", ".", "warmup_t", "for", "v", "in", "self", ".", "base_values", "]", "\n", "super", "(", ")", ".", "update_groups", "(", "self", ".", "warmup_lr_init", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "1", "for", "_", "in", "self", ".", "base_values", "]", "\n", "", "self", ".", "restore_lr", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.plateau_lr.PlateauLRScheduler.state_dict": [[64, 68], ["None"], "methods", ["None"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "'best'", ":", "self", ".", "lr_scheduler", ".", "best", ",", "\n", "'last_epoch'", ":", "self", ".", "lr_scheduler", ".", "last_epoch", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.plateau_lr.PlateauLRScheduler.load_state_dict": [[70, 74], ["None"], "methods", ["None"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "self", ".", "lr_scheduler", ".", "best", "=", "state_dict", "[", "'best'", "]", "\n", "if", "'last_epoch'", "in", "state_dict", ":", "\n", "            ", "self", ".", "lr_scheduler", ".", "last_epoch", "=", "state_dict", "[", "'last_epoch'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.plateau_lr.PlateauLRScheduler.step": [[76, 91], ["super().update_groups", "plateau_lr.PlateauLRScheduler.lr_scheduler.step", "plateau_lr.PlateauLRScheduler._is_apply_noise", "enumerate", "plateau_lr.PlateauLRScheduler._apply_noise"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler.update_groups", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler._is_apply_noise", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.plateau_lr.PlateauLRScheduler._apply_noise"], ["", "", "def", "step", "(", "self", ",", "epoch", ",", "metric", "=", "None", ")", ":", "\n", "        ", "if", "epoch", "<=", "self", ".", "warmup_t", ":", "\n", "            ", "lrs", "=", "[", "self", ".", "warmup_lr_init", "+", "epoch", "*", "s", "for", "s", "in", "self", ".", "warmup_steps", "]", "\n", "super", "(", ")", ".", "update_groups", "(", "lrs", ")", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "restore_lr", "is", "not", "None", ":", "\n", "# restore actual LR from before our last noise perturbation before stepping base", "\n", "                ", "for", "i", ",", "param_group", "in", "enumerate", "(", "self", ".", "optimizer", ".", "param_groups", ")", ":", "\n", "                    ", "param_group", "[", "'lr'", "]", "=", "self", ".", "restore_lr", "[", "i", "]", "\n", "", "self", ".", "restore_lr", "=", "None", "\n", "\n", "", "self", ".", "lr_scheduler", ".", "step", "(", "metric", ",", "epoch", ")", "# step the base scheduler", "\n", "\n", "if", "self", ".", "_is_apply_noise", "(", "epoch", ")", ":", "\n", "                ", "self", ".", "_apply_noise", "(", "epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.plateau_lr.PlateauLRScheduler._apply_noise": [[92, 104], ["plateau_lr.PlateauLRScheduler._calculate_noise", "enumerate", "float", "restore_lr.append"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler._calculate_noise"], ["", "", "", "def", "_apply_noise", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "noise", "=", "self", ".", "_calculate_noise", "(", "epoch", ")", "\n", "\n", "# apply the noise on top of previous LR, cache the old value so we can restore for normal", "\n", "# stepping of base scheduler", "\n", "restore_lr", "=", "[", "]", "\n", "for", "i", ",", "param_group", "in", "enumerate", "(", "self", ".", "optimizer", ".", "param_groups", ")", ":", "\n", "            ", "old_lr", "=", "float", "(", "param_group", "[", "'lr'", "]", ")", "\n", "restore_lr", ".", "append", "(", "old_lr", ")", "\n", "new_lr", "=", "old_lr", "+", "old_lr", "*", "noise", "\n", "param_group", "[", "'lr'", "]", "=", "new_lr", "\n", "", "self", ".", "restore_lr", "=", "restore_lr", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler_factory.create_scheduler": [[12, 108], ["dict", "dict", "getattr", "getattr", "isinstance", "cosine_lr.CosineLRScheduler", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "poly_lr.PolyLRScheduler.get_cycle_length", "tanh_lr.TanhLRScheduler", "len", "getattr", "poly_lr.PolyLRScheduler.get_cycle_length", "step_lr.StepLRScheduler", "multistep_lr.MultiStepLRScheduler", "plateau_lr.PlateauLRScheduler", "poly_lr.PolyLRScheduler", "getattr", "poly_lr.PolyLRScheduler.get_cycle_length", "getattr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.tanh_lr.TanhLRScheduler.get_cycle_length", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.tanh_lr.TanhLRScheduler.get_cycle_length", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.tanh_lr.TanhLRScheduler.get_cycle_length"], ["def", "create_scheduler", "(", "args", ",", "optimizer", ")", ":", "\n", "    ", "num_epochs", "=", "args", ".", "epochs", "\n", "\n", "if", "getattr", "(", "args", ",", "'lr_noise'", ",", "None", ")", "is", "not", "None", ":", "\n", "        ", "lr_noise", "=", "getattr", "(", "args", ",", "'lr_noise'", ")", "\n", "if", "isinstance", "(", "lr_noise", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "noise_range", "=", "[", "n", "*", "num_epochs", "for", "n", "in", "lr_noise", "]", "\n", "if", "len", "(", "noise_range", ")", "==", "1", ":", "\n", "                ", "noise_range", "=", "noise_range", "[", "0", "]", "\n", "", "", "else", ":", "\n", "            ", "noise_range", "=", "lr_noise", "*", "num_epochs", "\n", "", "", "else", ":", "\n", "        ", "noise_range", "=", "None", "\n", "", "noise_args", "=", "dict", "(", "\n", "noise_range_t", "=", "noise_range", ",", "\n", "noise_pct", "=", "getattr", "(", "args", ",", "'lr_noise_pct'", ",", "0.67", ")", ",", "\n", "noise_std", "=", "getattr", "(", "args", ",", "'lr_noise_std'", ",", "1.", ")", ",", "\n", "noise_seed", "=", "getattr", "(", "args", ",", "'seed'", ",", "42", ")", ",", "\n", ")", "\n", "cycle_args", "=", "dict", "(", "\n", "cycle_mul", "=", "getattr", "(", "args", ",", "'lr_cycle_mul'", ",", "1.", ")", ",", "\n", "cycle_decay", "=", "getattr", "(", "args", ",", "'lr_cycle_decay'", ",", "0.1", ")", ",", "\n", "cycle_limit", "=", "getattr", "(", "args", ",", "'lr_cycle_limit'", ",", "1", ")", ",", "\n", ")", "\n", "\n", "lr_scheduler", "=", "None", "\n", "if", "args", ".", "sched", "==", "'cosine'", ":", "\n", "        ", "lr_scheduler", "=", "CosineLRScheduler", "(", "\n", "optimizer", ",", "\n", "t_initial", "=", "num_epochs", ",", "\n", "lr_min", "=", "args", ".", "min_lr", ",", "\n", "warmup_lr_init", "=", "args", ".", "warmup_lr", ",", "\n", "warmup_t", "=", "args", ".", "warmup_epochs", ",", "\n", "k_decay", "=", "getattr", "(", "args", ",", "'lr_k_decay'", ",", "1.0", ")", ",", "\n", "**", "cycle_args", ",", "\n", "**", "noise_args", ",", "\n", ")", "\n", "num_epochs", "=", "lr_scheduler", ".", "get_cycle_length", "(", ")", "+", "args", ".", "cooldown_epochs", "\n", "", "elif", "args", ".", "sched", "==", "'tanh'", ":", "\n", "        ", "lr_scheduler", "=", "TanhLRScheduler", "(", "\n", "optimizer", ",", "\n", "t_initial", "=", "num_epochs", ",", "\n", "lr_min", "=", "args", ".", "min_lr", ",", "\n", "warmup_lr_init", "=", "args", ".", "warmup_lr", ",", "\n", "warmup_t", "=", "args", ".", "warmup_epochs", ",", "\n", "t_in_epochs", "=", "True", ",", "\n", "**", "cycle_args", ",", "\n", "**", "noise_args", ",", "\n", ")", "\n", "num_epochs", "=", "lr_scheduler", ".", "get_cycle_length", "(", ")", "+", "args", ".", "cooldown_epochs", "\n", "", "elif", "args", ".", "sched", "==", "'step'", ":", "\n", "        ", "lr_scheduler", "=", "StepLRScheduler", "(", "\n", "optimizer", ",", "\n", "decay_t", "=", "args", ".", "decay_epochs", ",", "\n", "decay_rate", "=", "args", ".", "decay_rate", ",", "\n", "warmup_lr_init", "=", "args", ".", "warmup_lr", ",", "\n", "warmup_t", "=", "args", ".", "warmup_epochs", ",", "\n", "**", "noise_args", ",", "\n", ")", "\n", "", "elif", "args", ".", "sched", "==", "'multistep'", ":", "\n", "        ", "lr_scheduler", "=", "MultiStepLRScheduler", "(", "\n", "optimizer", ",", "\n", "decay_t", "=", "args", ".", "decay_milestones", ",", "\n", "decay_rate", "=", "args", ".", "decay_rate", ",", "\n", "warmup_lr_init", "=", "args", ".", "warmup_lr", ",", "\n", "warmup_t", "=", "args", ".", "warmup_epochs", ",", "\n", "**", "noise_args", ",", "\n", ")", "\n", "", "elif", "args", ".", "sched", "==", "'plateau'", ":", "\n", "        ", "mode", "=", "'min'", "if", "'loss'", "in", "getattr", "(", "args", ",", "'eval_metric'", ",", "''", ")", "else", "'max'", "\n", "lr_scheduler", "=", "PlateauLRScheduler", "(", "\n", "optimizer", ",", "\n", "decay_rate", "=", "args", ".", "decay_rate", ",", "\n", "patience_t", "=", "args", ".", "patience_epochs", ",", "\n", "lr_min", "=", "args", ".", "min_lr", ",", "\n", "mode", "=", "mode", ",", "\n", "warmup_lr_init", "=", "args", ".", "warmup_lr", ",", "\n", "warmup_t", "=", "args", ".", "warmup_epochs", ",", "\n", "cooldown_t", "=", "0", ",", "\n", "**", "noise_args", ",", "\n", ")", "\n", "", "elif", "args", ".", "sched", "==", "'poly'", ":", "\n", "        ", "lr_scheduler", "=", "PolyLRScheduler", "(", "\n", "optimizer", ",", "\n", "power", "=", "args", ".", "decay_rate", ",", "# overloading 'decay_rate' as polynomial power", "\n", "t_initial", "=", "num_epochs", ",", "\n", "lr_min", "=", "args", ".", "min_lr", ",", "\n", "warmup_lr_init", "=", "args", ".", "warmup_lr", ",", "\n", "warmup_t", "=", "args", ".", "warmup_epochs", ",", "\n", "k_decay", "=", "getattr", "(", "args", ",", "'lr_k_decay'", ",", "1.0", ")", ",", "\n", "**", "cycle_args", ",", "\n", "**", "noise_args", ",", "\n", ")", "\n", "num_epochs", "=", "lr_scheduler", ".", "get_cycle_length", "(", ")", "+", "args", ".", "cooldown_epochs", "\n", "\n", "", "return", "lr_scheduler", ",", "num_epochs", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.cosine_lr.CosineLRScheduler.__init__": [[29, 71], ["scheduler.Scheduler.__init__", "_logger.warning", "super().update_groups"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler.update_groups"], ["def", "__init__", "(", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "t_initial", ":", "int", ",", "\n", "lr_min", ":", "float", "=", "0.", ",", "\n", "cycle_mul", ":", "float", "=", "1.", ",", "\n", "cycle_decay", ":", "float", "=", "1.", ",", "\n", "cycle_limit", ":", "int", "=", "1", ",", "\n", "warmup_t", "=", "0", ",", "\n", "warmup_lr_init", "=", "0", ",", "\n", "warmup_prefix", "=", "False", ",", "\n", "t_in_epochs", "=", "True", ",", "\n", "noise_range_t", "=", "None", ",", "\n", "noise_pct", "=", "0.67", ",", "\n", "noise_std", "=", "1.0", ",", "\n", "noise_seed", "=", "42", ",", "\n", "k_decay", "=", "1.0", ",", "\n", "initialize", "=", "True", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "optimizer", ",", "param_group_field", "=", "\"lr\"", ",", "\n", "noise_range_t", "=", "noise_range_t", ",", "noise_pct", "=", "noise_pct", ",", "noise_std", "=", "noise_std", ",", "noise_seed", "=", "noise_seed", ",", "\n", "initialize", "=", "initialize", ")", "\n", "\n", "assert", "t_initial", ">", "0", "\n", "assert", "lr_min", ">=", "0", "\n", "if", "t_initial", "==", "1", "and", "cycle_mul", "==", "1", "and", "cycle_decay", "==", "1", ":", "\n", "            ", "_logger", ".", "warning", "(", "\"Cosine annealing scheduler will have no effect on the learning \"", "\n", "\"rate since t_initial = t_mul = eta_mul = 1.\"", ")", "\n", "", "self", ".", "t_initial", "=", "t_initial", "\n", "self", ".", "lr_min", "=", "lr_min", "\n", "self", ".", "cycle_mul", "=", "cycle_mul", "\n", "self", ".", "cycle_decay", "=", "cycle_decay", "\n", "self", ".", "cycle_limit", "=", "cycle_limit", "\n", "self", ".", "warmup_t", "=", "warmup_t", "\n", "self", ".", "warmup_lr_init", "=", "warmup_lr_init", "\n", "self", ".", "warmup_prefix", "=", "warmup_prefix", "\n", "self", ".", "t_in_epochs", "=", "t_in_epochs", "\n", "self", ".", "k_decay", "=", "k_decay", "\n", "if", "self", ".", "warmup_t", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "(", "v", "-", "warmup_lr_init", ")", "/", "self", ".", "warmup_t", "for", "v", "in", "self", ".", "base_values", "]", "\n", "super", "(", ")", ".", "update_groups", "(", "self", ".", "warmup_lr_init", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "1", "for", "_", "in", "self", ".", "base_values", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.cosine_lr.CosineLRScheduler._get_lr": [[72, 101], ["math.floor", "math.log", "math.cos"], "methods", ["None"], ["", "", "def", "_get_lr", "(", "self", ",", "t", ")", ":", "\n", "        ", "if", "t", "<", "self", ".", "warmup_t", ":", "\n", "            ", "lrs", "=", "[", "self", ".", "warmup_lr_init", "+", "t", "*", "s", "for", "s", "in", "self", ".", "warmup_steps", "]", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "warmup_prefix", ":", "\n", "                ", "t", "=", "t", "-", "self", ".", "warmup_t", "\n", "\n", "", "if", "self", ".", "cycle_mul", "!=", "1", ":", "\n", "                ", "i", "=", "math", ".", "floor", "(", "math", ".", "log", "(", "1", "-", "t", "/", "self", ".", "t_initial", "*", "(", "1", "-", "self", ".", "cycle_mul", ")", ",", "self", ".", "cycle_mul", ")", ")", "\n", "t_i", "=", "self", ".", "cycle_mul", "**", "i", "*", "self", ".", "t_initial", "\n", "t_curr", "=", "t", "-", "(", "1", "-", "self", ".", "cycle_mul", "**", "i", ")", "/", "(", "1", "-", "self", ".", "cycle_mul", ")", "*", "self", ".", "t_initial", "\n", "", "else", ":", "\n", "                ", "i", "=", "t", "//", "self", ".", "t_initial", "\n", "t_i", "=", "self", ".", "t_initial", "\n", "t_curr", "=", "t", "-", "(", "self", ".", "t_initial", "*", "i", ")", "\n", "\n", "", "gamma", "=", "self", ".", "cycle_decay", "**", "i", "\n", "lr_max_values", "=", "[", "v", "*", "gamma", "for", "v", "in", "self", ".", "base_values", "]", "\n", "k", "=", "self", ".", "k_decay", "\n", "\n", "if", "i", "<", "self", ".", "cycle_limit", ":", "\n", "                ", "lrs", "=", "[", "\n", "self", ".", "lr_min", "+", "0.5", "*", "(", "lr_max", "-", "self", ".", "lr_min", ")", "*", "(", "1", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "t_curr", "**", "k", "/", "t_i", "**", "k", ")", ")", "\n", "for", "lr_max", "in", "lr_max_values", "\n", "]", "\n", "", "else", ":", "\n", "                ", "lrs", "=", "[", "self", ".", "lr_min", "for", "_", "in", "self", ".", "base_values", "]", "\n", "\n", "", "", "return", "lrs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.cosine_lr.CosineLRScheduler.get_epoch_values": [[102, 107], ["cosine_lr.CosineLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor._get_lr"], ["", "def", "get_epoch_values", "(", "self", ",", "epoch", ":", "int", ")", ":", "\n", "        ", "if", "self", ".", "t_in_epochs", ":", "\n", "            ", "return", "self", ".", "_get_lr", "(", "epoch", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.cosine_lr.CosineLRScheduler.get_update_values": [[108, 113], ["cosine_lr.CosineLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor._get_lr"], ["", "", "def", "get_update_values", "(", "self", ",", "num_updates", ":", "int", ")", ":", "\n", "        ", "if", "not", "self", ".", "t_in_epochs", ":", "\n", "            ", "return", "self", ".", "_get_lr", "(", "num_updates", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.cosine_lr.CosineLRScheduler.get_cycle_length": [[114, 120], ["max", "int", "math.floor"], "methods", ["None"], ["", "", "def", "get_cycle_length", "(", "self", ",", "cycles", "=", "0", ")", ":", "\n", "        ", "cycles", "=", "max", "(", "1", ",", "cycles", "or", "self", ".", "cycle_limit", ")", "\n", "if", "self", ".", "cycle_mul", "==", "1.0", ":", "\n", "            ", "return", "self", ".", "t_initial", "*", "cycles", "\n", "", "else", ":", "\n", "            ", "return", "int", "(", "math", ".", "floor", "(", "-", "self", ".", "t_initial", "*", "(", "self", ".", "cycle_mul", "**", "cycles", "-", "1", ")", "/", "(", "1", "-", "self", ".", "cycle_mul", ")", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.step_lr.StepLRScheduler.__init__": [[17, 45], ["scheduler.Scheduler.__init__", "super().update_groups"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler.update_groups"], ["def", "__init__", "(", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "decay_t", ":", "float", ",", "\n", "decay_rate", ":", "float", "=", "1.", ",", "\n", "warmup_t", "=", "0", ",", "\n", "warmup_lr_init", "=", "0", ",", "\n", "t_in_epochs", "=", "True", ",", "\n", "noise_range_t", "=", "None", ",", "\n", "noise_pct", "=", "0.67", ",", "\n", "noise_std", "=", "1.0", ",", "\n", "noise_seed", "=", "42", ",", "\n", "initialize", "=", "True", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "optimizer", ",", "param_group_field", "=", "\"lr\"", ",", "\n", "noise_range_t", "=", "noise_range_t", ",", "noise_pct", "=", "noise_pct", ",", "noise_std", "=", "noise_std", ",", "noise_seed", "=", "noise_seed", ",", "\n", "initialize", "=", "initialize", ")", "\n", "\n", "self", ".", "decay_t", "=", "decay_t", "\n", "self", ".", "decay_rate", "=", "decay_rate", "\n", "self", ".", "warmup_t", "=", "warmup_t", "\n", "self", ".", "warmup_lr_init", "=", "warmup_lr_init", "\n", "self", ".", "t_in_epochs", "=", "t_in_epochs", "\n", "if", "self", ".", "warmup_t", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "(", "v", "-", "warmup_lr_init", ")", "/", "self", ".", "warmup_t", "for", "v", "in", "self", ".", "base_values", "]", "\n", "super", "(", ")", ".", "update_groups", "(", "self", ".", "warmup_lr_init", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "1", "for", "_", "in", "self", ".", "base_values", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.step_lr.StepLRScheduler._get_lr": [[46, 52], ["None"], "methods", ["None"], ["", "", "def", "_get_lr", "(", "self", ",", "t", ")", ":", "\n", "        ", "if", "t", "<", "self", ".", "warmup_t", ":", "\n", "            ", "lrs", "=", "[", "self", ".", "warmup_lr_init", "+", "t", "*", "s", "for", "s", "in", "self", ".", "warmup_steps", "]", "\n", "", "else", ":", "\n", "            ", "lrs", "=", "[", "v", "*", "(", "self", ".", "decay_rate", "**", "(", "t", "//", "self", ".", "decay_t", ")", ")", "for", "v", "in", "self", ".", "base_values", "]", "\n", "", "return", "lrs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.step_lr.StepLRScheduler.get_epoch_values": [[53, 58], ["step_lr.StepLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor._get_lr"], ["", "def", "get_epoch_values", "(", "self", ",", "epoch", ":", "int", ")", ":", "\n", "        ", "if", "self", ".", "t_in_epochs", ":", "\n", "            ", "return", "self", ".", "_get_lr", "(", "epoch", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.step_lr.StepLRScheduler.get_update_values": [[59, 64], ["step_lr.StepLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor._get_lr"], ["", "", "def", "get_update_values", "(", "self", ",", "num_updates", ":", "int", ")", ":", "\n", "        ", "if", "not", "self", ".", "t_in_epochs", ":", "\n", "            ", "return", "self", ".", "_get_lr", "(", "num_updates", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler.__init__": [[25, 54], ["scheduler.Scheduler.update_groups", "enumerate", "enumerate", "group.setdefault", "KeyError", "KeyError"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler.update_groups"], ["def", "__init__", "(", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "param_group_field", ":", "str", ",", "\n", "noise_range_t", "=", "None", ",", "\n", "noise_type", "=", "'normal'", ",", "\n", "noise_pct", "=", "0.67", ",", "\n", "noise_std", "=", "1.0", ",", "\n", "noise_seed", "=", "None", ",", "\n", "initialize", ":", "bool", "=", "True", ")", "->", "None", ":", "\n", "        ", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "param_group_field", "=", "param_group_field", "\n", "self", ".", "_initial_param_group_field", "=", "f\"initial_{param_group_field}\"", "\n", "if", "initialize", ":", "\n", "            ", "for", "i", ",", "group", "in", "enumerate", "(", "self", ".", "optimizer", ".", "param_groups", ")", ":", "\n", "                ", "if", "param_group_field", "not", "in", "group", ":", "\n", "                    ", "raise", "KeyError", "(", "f\"{param_group_field} missing from param_groups[{i}]\"", ")", "\n", "", "group", ".", "setdefault", "(", "self", ".", "_initial_param_group_field", ",", "group", "[", "param_group_field", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "i", ",", "group", "in", "enumerate", "(", "self", ".", "optimizer", ".", "param_groups", ")", ":", "\n", "                ", "if", "self", ".", "_initial_param_group_field", "not", "in", "group", ":", "\n", "                    ", "raise", "KeyError", "(", "f\"{self._initial_param_group_field} missing from param_groups[{i}]\"", ")", "\n", "", "", "", "self", ".", "base_values", "=", "[", "group", "[", "self", ".", "_initial_param_group_field", "]", "for", "group", "in", "self", ".", "optimizer", ".", "param_groups", "]", "\n", "self", ".", "metric", "=", "None", "# any point to having this for all?", "\n", "self", ".", "noise_range_t", "=", "noise_range_t", "\n", "self", ".", "noise_pct", "=", "noise_pct", "\n", "self", ".", "noise_type", "=", "noise_type", "\n", "self", ".", "noise_std", "=", "noise_std", "\n", "self", ".", "noise_seed", "=", "noise_seed", "if", "noise_seed", "is", "not", "None", "else", "42", "\n", "self", ".", "update_groups", "(", "self", ".", "base_values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler.state_dict": [[55, 57], ["scheduler.Scheduler.__dict__.items"], "methods", ["None"], ["", "def", "state_dict", "(", "self", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "        ", "return", "{", "key", ":", "value", "for", "key", ",", "value", "in", "self", ".", "__dict__", ".", "items", "(", ")", "if", "key", "!=", "'optimizer'", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler.load_state_dict": [[58, 60], ["scheduler.Scheduler.__dict__.update"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ":", "Dict", "[", "str", ",", "Any", "]", ")", "->", "None", ":", "\n", "        ", "self", ".", "__dict__", ".", "update", "(", "state_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler.get_epoch_values": [[61, 63], ["None"], "methods", ["None"], ["", "def", "get_epoch_values", "(", "self", ",", "epoch", ":", "int", ")", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler.get_update_values": [[64, 66], ["None"], "methods", ["None"], ["", "def", "get_update_values", "(", "self", ",", "num_updates", ":", "int", ")", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler.step": [[67, 73], ["scheduler.Scheduler.get_epoch_values", "scheduler.Scheduler._add_noise", "scheduler.Scheduler.update_groups"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.tanh_lr.TanhLRScheduler.get_epoch_values", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler._add_noise", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler.update_groups"], ["", "def", "step", "(", "self", ",", "epoch", ":", "int", ",", "metric", ":", "float", "=", "None", ")", "->", "None", ":", "\n", "        ", "self", ".", "metric", "=", "metric", "\n", "values", "=", "self", ".", "get_epoch_values", "(", "epoch", ")", "\n", "if", "values", "is", "not", "None", ":", "\n", "            ", "values", "=", "self", ".", "_add_noise", "(", "values", ",", "epoch", ")", "\n", "self", ".", "update_groups", "(", "values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler.step_update": [[74, 80], ["scheduler.Scheduler.get_update_values", "scheduler.Scheduler._add_noise", "scheduler.Scheduler.update_groups"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.tanh_lr.TanhLRScheduler.get_update_values", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler._add_noise", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler.update_groups"], ["", "", "def", "step_update", "(", "self", ",", "num_updates", ":", "int", ",", "metric", ":", "float", "=", "None", ")", ":", "\n", "        ", "self", ".", "metric", "=", "metric", "\n", "values", "=", "self", ".", "get_update_values", "(", "num_updates", ")", "\n", "if", "values", "is", "not", "None", ":", "\n", "            ", "values", "=", "self", ".", "_add_noise", "(", "values", ",", "num_updates", ")", "\n", "self", ".", "update_groups", "(", "values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler.update_groups": [[81, 89], ["zip", "isinstance", "len"], "methods", ["None"], ["", "", "def", "update_groups", "(", "self", ",", "values", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "values", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "values", "=", "[", "values", "]", "*", "len", "(", "self", ".", "optimizer", ".", "param_groups", ")", "\n", "", "for", "param_group", ",", "value", "in", "zip", "(", "self", ".", "optimizer", ".", "param_groups", ",", "values", ")", ":", "\n", "            ", "if", "'lr_scale'", "in", "param_group", ":", "\n", "                ", "param_group", "[", "self", ".", "param_group_field", "]", "=", "value", "*", "param_group", "[", "'lr_scale'", "]", "\n", "", "else", ":", "\n", "                ", "param_group", "[", "self", ".", "param_group_field", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler._add_noise": [[90, 95], ["scheduler.Scheduler._is_apply_noise", "scheduler.Scheduler._calculate_noise"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler._is_apply_noise", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler._calculate_noise"], ["", "", "", "def", "_add_noise", "(", "self", ",", "lrs", ",", "t", ")", ":", "\n", "        ", "if", "self", ".", "_is_apply_noise", "(", "t", ")", ":", "\n", "            ", "noise", "=", "self", ".", "_calculate_noise", "(", "t", ")", "\n", "lrs", "=", "[", "v", "+", "v", "*", "noise", "for", "v", "in", "lrs", "]", "\n", "", "return", "lrs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler._is_apply_noise": [[96, 105], ["isinstance"], "methods", ["None"], ["", "def", "_is_apply_noise", "(", "self", ",", "t", ")", "->", "bool", ":", "\n", "        ", "\"\"\"Return True if scheduler in noise range.\"\"\"", "\n", "apply_noise", "=", "False", "\n", "if", "self", ".", "noise_range_t", "is", "not", "None", ":", "\n", "            ", "if", "isinstance", "(", "self", ".", "noise_range_t", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "                ", "apply_noise", "=", "self", ".", "noise_range_t", "[", "0", "]", "<=", "t", "<", "self", ".", "noise_range_t", "[", "1", "]", "\n", "", "else", ":", "\n", "                ", "apply_noise", "=", "t", ">=", "self", ".", "noise_range_t", "\n", "", "", "return", "apply_noise", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler._calculate_noise": [[106, 118], ["torch.Generator", "torch.Generator.manual_seed", "torch.randn().item", "abs", "torch.randn", "torch.rand().item", "torch.rand"], "methods", ["None"], ["", "def", "_calculate_noise", "(", "self", ",", "t", ")", "->", "float", ":", "\n", "        ", "g", "=", "torch", ".", "Generator", "(", ")", "\n", "g", ".", "manual_seed", "(", "self", ".", "noise_seed", "+", "t", ")", "\n", "if", "self", ".", "noise_type", "==", "'normal'", ":", "\n", "            ", "while", "True", ":", "\n", "# resample if noise out of percent limit, brute force but shouldn't spin much", "\n", "                ", "noise", "=", "torch", ".", "randn", "(", "1", ",", "generator", "=", "g", ")", ".", "item", "(", ")", "\n", "if", "abs", "(", "noise", ")", "<", "self", ".", "noise_pct", ":", "\n", "                    ", "return", "noise", "\n", "", "", "", "else", ":", "\n", "            ", "noise", "=", "2", "*", "(", "torch", ".", "rand", "(", "1", ",", "generator", "=", "g", ")", ".", "item", "(", ")", "-", "0.5", ")", "*", "self", ".", "noise_pct", "\n", "", "return", "noise", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.multistep_lr.MultiStepLRScheduler.__init__": [[14, 42], ["timm.scheduler.scheduler.Scheduler.__init__", "super().update_groups"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler.update_groups"], ["def", "__init__", "(", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "decay_t", ":", "List", "[", "int", "]", ",", "\n", "decay_rate", ":", "float", "=", "1.", ",", "\n", "warmup_t", "=", "0", ",", "\n", "warmup_lr_init", "=", "0", ",", "\n", "t_in_epochs", "=", "True", ",", "\n", "noise_range_t", "=", "None", ",", "\n", "noise_pct", "=", "0.67", ",", "\n", "noise_std", "=", "1.0", ",", "\n", "noise_seed", "=", "42", ",", "\n", "initialize", "=", "True", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "optimizer", ",", "param_group_field", "=", "\"lr\"", ",", "\n", "noise_range_t", "=", "noise_range_t", ",", "noise_pct", "=", "noise_pct", ",", "noise_std", "=", "noise_std", ",", "noise_seed", "=", "noise_seed", ",", "\n", "initialize", "=", "initialize", ")", "\n", "\n", "self", ".", "decay_t", "=", "decay_t", "\n", "self", ".", "decay_rate", "=", "decay_rate", "\n", "self", ".", "warmup_t", "=", "warmup_t", "\n", "self", ".", "warmup_lr_init", "=", "warmup_lr_init", "\n", "self", ".", "t_in_epochs", "=", "t_in_epochs", "\n", "if", "self", ".", "warmup_t", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "(", "v", "-", "warmup_lr_init", ")", "/", "self", ".", "warmup_t", "for", "v", "in", "self", ".", "base_values", "]", "\n", "super", "(", ")", ".", "update_groups", "(", "self", ".", "warmup_lr_init", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "1", "for", "_", "in", "self", ".", "base_values", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.multistep_lr.MultiStepLRScheduler.get_curr_decay_steps": [[43, 47], ["bisect.bisect_right"], "methods", ["None"], ["", "", "def", "get_curr_decay_steps", "(", "self", ",", "t", ")", ":", "\n", "# find where in the array t goes,", "\n", "# assumes self.decay_t is sorted", "\n", "        ", "return", "bisect", ".", "bisect_right", "(", "self", ".", "decay_t", ",", "t", "+", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.multistep_lr.MultiStepLRScheduler._get_lr": [[48, 54], ["multistep_lr.MultiStepLRScheduler.get_curr_decay_steps"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.multistep_lr.MultiStepLRScheduler.get_curr_decay_steps"], ["", "def", "_get_lr", "(", "self", ",", "t", ")", ":", "\n", "        ", "if", "t", "<", "self", ".", "warmup_t", ":", "\n", "            ", "lrs", "=", "[", "self", ".", "warmup_lr_init", "+", "t", "*", "s", "for", "s", "in", "self", ".", "warmup_steps", "]", "\n", "", "else", ":", "\n", "            ", "lrs", "=", "[", "v", "*", "(", "self", ".", "decay_rate", "**", "self", ".", "get_curr_decay_steps", "(", "t", ")", ")", "for", "v", "in", "self", ".", "base_values", "]", "\n", "", "return", "lrs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.multistep_lr.MultiStepLRScheduler.get_epoch_values": [[55, 60], ["multistep_lr.MultiStepLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor._get_lr"], ["", "def", "get_epoch_values", "(", "self", ",", "epoch", ":", "int", ")", ":", "\n", "        ", "if", "self", ".", "t_in_epochs", ":", "\n", "            ", "return", "self", ".", "_get_lr", "(", "epoch", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.multistep_lr.MultiStepLRScheduler.get_update_values": [[61, 66], ["multistep_lr.MultiStepLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor._get_lr"], ["", "", "def", "get_update_values", "(", "self", ",", "num_updates", ":", "int", ")", ":", "\n", "        ", "if", "not", "self", ".", "t_in_epochs", ":", "\n", "            ", "return", "self", ".", "_get_lr", "(", "num_updates", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.tanh_lr.TanhLRScheduler.__init__": [[24, 70], ["scheduler.Scheduler.__init__", "super().update_groups", "tanh_lr.TanhLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.scheduler.Scheduler.update_groups", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor._get_lr"], ["def", "__init__", "(", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "t_initial", ":", "int", ",", "\n", "lb", ":", "float", "=", "-", "7.", ",", "\n", "ub", ":", "float", "=", "3.", ",", "\n", "lr_min", ":", "float", "=", "0.", ",", "\n", "cycle_mul", ":", "float", "=", "1.", ",", "\n", "cycle_decay", ":", "float", "=", "1.", ",", "\n", "cycle_limit", ":", "int", "=", "1", ",", "\n", "warmup_t", "=", "0", ",", "\n", "warmup_lr_init", "=", "0", ",", "\n", "warmup_prefix", "=", "False", ",", "\n", "t_in_epochs", "=", "True", ",", "\n", "noise_range_t", "=", "None", ",", "\n", "noise_pct", "=", "0.67", ",", "\n", "noise_std", "=", "1.0", ",", "\n", "noise_seed", "=", "42", ",", "\n", "initialize", "=", "True", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "optimizer", ",", "param_group_field", "=", "\"lr\"", ",", "\n", "noise_range_t", "=", "noise_range_t", ",", "noise_pct", "=", "noise_pct", ",", "noise_std", "=", "noise_std", ",", "noise_seed", "=", "noise_seed", ",", "\n", "initialize", "=", "initialize", ")", "\n", "\n", "assert", "t_initial", ">", "0", "\n", "assert", "lr_min", ">=", "0", "\n", "assert", "lb", "<", "ub", "\n", "assert", "cycle_limit", ">=", "0", "\n", "assert", "warmup_t", ">=", "0", "\n", "assert", "warmup_lr_init", ">=", "0", "\n", "self", ".", "lb", "=", "lb", "\n", "self", ".", "ub", "=", "ub", "\n", "self", ".", "t_initial", "=", "t_initial", "\n", "self", ".", "lr_min", "=", "lr_min", "\n", "self", ".", "cycle_mul", "=", "cycle_mul", "\n", "self", ".", "cycle_decay", "=", "cycle_decay", "\n", "self", ".", "cycle_limit", "=", "cycle_limit", "\n", "self", ".", "warmup_t", "=", "warmup_t", "\n", "self", ".", "warmup_lr_init", "=", "warmup_lr_init", "\n", "self", ".", "warmup_prefix", "=", "warmup_prefix", "\n", "self", ".", "t_in_epochs", "=", "t_in_epochs", "\n", "if", "self", ".", "warmup_t", ":", "\n", "            ", "t_v", "=", "self", ".", "base_values", "if", "self", ".", "warmup_prefix", "else", "self", ".", "_get_lr", "(", "self", ".", "warmup_t", ")", "\n", "self", ".", "warmup_steps", "=", "[", "(", "v", "-", "warmup_lr_init", ")", "/", "self", ".", "warmup_t", "for", "v", "in", "t_v", "]", "\n", "super", "(", ")", ".", "update_groups", "(", "self", ".", "warmup_lr_init", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "1", "for", "_", "in", "self", ".", "base_values", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.tanh_lr.TanhLRScheduler._get_lr": [[71, 99], ["math.floor", "math.log", "math.tanh"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.tanh"], ["", "", "def", "_get_lr", "(", "self", ",", "t", ")", ":", "\n", "        ", "if", "t", "<", "self", ".", "warmup_t", ":", "\n", "            ", "lrs", "=", "[", "self", ".", "warmup_lr_init", "+", "t", "*", "s", "for", "s", "in", "self", ".", "warmup_steps", "]", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "warmup_prefix", ":", "\n", "                ", "t", "=", "t", "-", "self", ".", "warmup_t", "\n", "\n", "", "if", "self", ".", "cycle_mul", "!=", "1", ":", "\n", "                ", "i", "=", "math", ".", "floor", "(", "math", ".", "log", "(", "1", "-", "t", "/", "self", ".", "t_initial", "*", "(", "1", "-", "self", ".", "cycle_mul", ")", ",", "self", ".", "cycle_mul", ")", ")", "\n", "t_i", "=", "self", ".", "cycle_mul", "**", "i", "*", "self", ".", "t_initial", "\n", "t_curr", "=", "t", "-", "(", "1", "-", "self", ".", "cycle_mul", "**", "i", ")", "/", "(", "1", "-", "self", ".", "cycle_mul", ")", "*", "self", ".", "t_initial", "\n", "", "else", ":", "\n", "                ", "i", "=", "t", "//", "self", ".", "t_initial", "\n", "t_i", "=", "self", ".", "t_initial", "\n", "t_curr", "=", "t", "-", "(", "self", ".", "t_initial", "*", "i", ")", "\n", "\n", "", "if", "i", "<", "self", ".", "cycle_limit", ":", "\n", "                ", "gamma", "=", "self", ".", "cycle_decay", "**", "i", "\n", "lr_max_values", "=", "[", "v", "*", "gamma", "for", "v", "in", "self", ".", "base_values", "]", "\n", "\n", "tr", "=", "t_curr", "/", "t_i", "\n", "lrs", "=", "[", "\n", "self", ".", "lr_min", "+", "0.5", "*", "(", "lr_max", "-", "self", ".", "lr_min", ")", "*", "(", "1", "-", "math", ".", "tanh", "(", "self", ".", "lb", "*", "(", "1.", "-", "tr", ")", "+", "self", ".", "ub", "*", "tr", ")", ")", "\n", "for", "lr_max", "in", "lr_max_values", "\n", "]", "\n", "", "else", ":", "\n", "                ", "lrs", "=", "[", "self", ".", "lr_min", "for", "_", "in", "self", ".", "base_values", "]", "\n", "", "", "return", "lrs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.tanh_lr.TanhLRScheduler.get_epoch_values": [[100, 105], ["tanh_lr.TanhLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor._get_lr"], ["", "def", "get_epoch_values", "(", "self", ",", "epoch", ":", "int", ")", ":", "\n", "        ", "if", "self", ".", "t_in_epochs", ":", "\n", "            ", "return", "self", ".", "_get_lr", "(", "epoch", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.tanh_lr.TanhLRScheduler.get_update_values": [[106, 111], ["tanh_lr.TanhLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor._get_lr"], ["", "", "def", "get_update_values", "(", "self", ",", "num_updates", ":", "int", ")", ":", "\n", "        ", "if", "not", "self", ".", "t_in_epochs", ":", "\n", "            ", "return", "self", ".", "_get_lr", "(", "num_updates", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.scheduler.tanh_lr.TanhLRScheduler.get_cycle_length": [[112, 118], ["max", "int", "math.floor"], "methods", ["None"], ["", "", "def", "get_cycle_length", "(", "self", ",", "cycles", "=", "0", ")", ":", "\n", "        ", "cycles", "=", "max", "(", "1", ",", "cycles", "or", "self", ".", "cycle_limit", ")", "\n", "if", "self", ".", "cycle_mul", "==", "1.0", ":", "\n", "            ", "return", "self", ".", "t_initial", "*", "cycles", "\n", "", "else", ":", "\n", "            ", "return", "int", "(", "math", ".", "floor", "(", "-", "self", ".", "t_initial", "*", "(", "self", ".", "cycle_mul", "**", "cycles", "-", "1", ")", "/", "(", "1", "-", "self", ".", "cycle_mul", ")", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lars.Lars.__init__": [[35, 69], ["dict", "torch.optim.optimizer.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "params", ",", "\n", "lr", "=", "1.0", ",", "\n", "momentum", "=", "0", ",", "\n", "dampening", "=", "0", ",", "\n", "weight_decay", "=", "0", ",", "\n", "nesterov", "=", "False", ",", "\n", "trust_coeff", "=", "0.001", ",", "\n", "eps", "=", "1e-8", ",", "\n", "trust_clip", "=", "False", ",", "\n", "always_adapt", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "lr", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Invalid learning rate: {lr}\"", ")", "\n", "", "if", "momentum", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Invalid momentum value: {momentum}\"", ")", "\n", "", "if", "weight_decay", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Invalid weight_decay value: {weight_decay}\"", ")", "\n", "", "if", "nesterov", "and", "(", "momentum", "<=", "0", "or", "dampening", "!=", "0", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Nesterov momentum requires a momentum and zero dampening\"", ")", "\n", "\n", "", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "\n", "momentum", "=", "momentum", ",", "\n", "dampening", "=", "dampening", ",", "\n", "weight_decay", "=", "weight_decay", ",", "\n", "nesterov", "=", "nesterov", ",", "\n", "trust_coeff", "=", "trust_coeff", ",", "\n", "eps", "=", "eps", ",", "\n", "trust_clip", "=", "trust_clip", ",", "\n", "always_adapt", "=", "always_adapt", ",", "\n", ")", "\n", "super", "(", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lars.Lars.__setstate__": [[70, 74], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.ml_decoder.TransformerDecoderLayerOptimal.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "\"nesterov\"", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lars.Lars.step": [[75, 136], ["torch.no_grad", "torch.tensor", "torch.enable_grad", "closure", "p.add_", "p.norm", "grad.add.add.norm", "torch.where", "grad.add.add.add_", "grad.add.add.mul_", "torch.where", "torch.minimum", "torch.clone().detach", "torch.clone().detach.mul_().add_", "grad.add.add.add", "torch.clone", "torch.clone().detach.mul_"], "methods", ["None"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Args:\n            closure (callable, optional): A closure that reevaluates the model and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "device", "=", "self", ".", "param_groups", "[", "0", "]", "[", "'params'", "]", "[", "0", "]", ".", "device", "\n", "one_tensor", "=", "torch", ".", "tensor", "(", "1.0", ",", "device", "=", "device", ")", "# because torch.where doesn't handle scalars correctly", "\n", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "weight_decay", "=", "group", "[", "'weight_decay'", "]", "\n", "momentum", "=", "group", "[", "'momentum'", "]", "\n", "dampening", "=", "group", "[", "'dampening'", "]", "\n", "nesterov", "=", "group", "[", "'nesterov'", "]", "\n", "trust_coeff", "=", "group", "[", "'trust_coeff'", "]", "\n", "eps", "=", "group", "[", "'eps'", "]", "\n", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", "\n", "\n", "# apply LARS LR adaptation, LARC clipping, weight decay", "\n", "# ref: https://github.com/NVIDIA/apex/blob/master/apex/parallel/LARC.py", "\n", "if", "weight_decay", "!=", "0", "or", "group", "[", "'always_adapt'", "]", ":", "\n", "                    ", "w_norm", "=", "p", ".", "norm", "(", "2.0", ")", "\n", "g_norm", "=", "grad", ".", "norm", "(", "2.0", ")", "\n", "trust_ratio", "=", "trust_coeff", "*", "w_norm", "/", "(", "g_norm", "+", "w_norm", "*", "weight_decay", "+", "eps", ")", "\n", "# FIXME nested where required since logical and/or not working in PT XLA", "\n", "trust_ratio", "=", "torch", ".", "where", "(", "\n", "w_norm", ">", "0", ",", "\n", "torch", ".", "where", "(", "g_norm", ">", "0", ",", "trust_ratio", ",", "one_tensor", ")", ",", "\n", "one_tensor", ",", "\n", ")", "\n", "if", "group", "[", "'trust_clip'", "]", ":", "\n", "                        ", "trust_ratio", "=", "torch", ".", "minimum", "(", "trust_ratio", "/", "group", "[", "'lr'", "]", ",", "one_tensor", ")", "\n", "", "grad", ".", "add_", "(", "p", ",", "alpha", "=", "weight_decay", ")", "\n", "grad", ".", "mul_", "(", "trust_ratio", ")", "\n", "\n", "# apply SGD update https://github.com/pytorch/pytorch/blob/1.7/torch/optim/sgd.py#L100", "\n", "", "if", "momentum", "!=", "0", ":", "\n", "                    ", "param_state", "=", "self", ".", "state", "[", "p", "]", "\n", "if", "'momentum_buffer'", "not", "in", "param_state", ":", "\n", "                        ", "buf", "=", "param_state", "[", "'momentum_buffer'", "]", "=", "torch", ".", "clone", "(", "grad", ")", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "                        ", "buf", "=", "param_state", "[", "'momentum_buffer'", "]", "\n", "buf", ".", "mul_", "(", "momentum", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1.", "-", "dampening", ")", "\n", "", "if", "nesterov", ":", "\n", "                        ", "grad", "=", "grad", ".", "add", "(", "buf", ",", "alpha", "=", "momentum", ")", "\n", "", "else", ":", "\n", "                        ", "grad", "=", "buf", "\n", "\n", "", "", "p", ".", "add_", "(", "grad", ",", "alpha", "=", "-", "group", "[", "'lr'", "]", ")", "\n", "\n", "", "", "return", "loss", "", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.madgrad.MADGRAD.__init__": [[55, 76], ["dict", "super().__init__", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "params", ":", "_params_t", ",", "\n", "lr", ":", "float", "=", "1e-2", ",", "\n", "momentum", ":", "float", "=", "0.9", ",", "\n", "weight_decay", ":", "float", "=", "0", ",", "\n", "eps", ":", "float", "=", "1e-6", ",", "\n", "decoupled_decay", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "momentum", "<", "0", "or", "momentum", ">=", "1", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Momentum {momentum} must be in the range [0,1]\"", ")", "\n", "", "if", "lr", "<=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Learning rate {lr} must be positive\"", ")", "\n", "", "if", "weight_decay", "<", "0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Weight decay {weight_decay} must be non-negative\"", ")", "\n", "", "if", "eps", "<", "0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Eps must be non-negative\"", ")", "\n", "\n", "", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "eps", "=", "eps", ",", "momentum", "=", "momentum", ",", "weight_decay", "=", "weight_decay", ",", "decoupled_decay", "=", "decoupled_decay", ")", "\n", "super", "(", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.madgrad.MADGRAD.supports_memory_efficient_fp16": [[77, 80], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_memory_efficient_fp16", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.madgrad.MADGRAD.supports_flat_params": [[81, 84], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_flat_params", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.madgrad.MADGRAD.step": [[85, 185], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "closure", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "math.sqrt", "grad.coalesce.coalesce.coalesce", "grad.coalesce.coalesce._values", "p.sparse_mask", "grad_sum_sq.sparse_mask", "s.sparse_mask", "grad_sum_sq.sparse_mask._values().pow().add_", "p.sparse_mask._values().addcdiv", "grad_sum_sq.add_", "grad_sum_sq.sparse_mask.add_", "grad_sum_sq.sparse_mask._values().pow_().add_", "s.add_", "s.sparse_mask._values().add_", "p.sparse_mask._values().addcdiv.addcdiv", "p.sparse_mask._values().add_", "p.add_", "grad_sum_sq.addcmul_", "grad_sum_sq.pow().add_", "s.add_", "torch.clone().detach", "torch.clone().detach", "torch.clone().detach", "torch.clone().detach", "p.mul_", "grad.coalesce.coalesce.add_", "s.sparse_mask._values", "s.sparse_mask._values", "grad_sum_sq.pow().add_", "p.addcdiv", "p.copy_", "p.addcdiv.addcdiv", "p.mul_().add_", "RuntimeError", "grad_sum_sq.sparse_mask._values().pow", "p.sparse_mask._values", "grad_sum_sq.sparse_mask._values().pow_", "s.sparse_mask._values", "p.sparse_mask._values", "grad_sum_sq.pow", "p.addcdiv.addcdiv", "torch.clone", "torch.clone", "torch.clone", "torch.clone", "grad_sum_sq.pow", "p.mul_", "grad_sum_sq.sparse_mask._values", "grad_sum_sq.sparse_mask._values"], "methods", ["None"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", ":", "Optional", "[", "Callable", "[", "[", "]", ",", "float", "]", "]", "=", "None", ")", "->", "Optional", "[", "float", "]", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "eps", "=", "group", "[", "'eps'", "]", "\n", "lr", "=", "group", "[", "'lr'", "]", "+", "eps", "\n", "weight_decay", "=", "group", "[", "'weight_decay'", "]", "\n", "momentum", "=", "group", "[", "'momentum'", "]", "\n", "ck", "=", "1", "-", "momentum", "\n", "\n", "for", "p", "in", "group", "[", "\"params\"", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", "\n", "if", "momentum", "!=", "0.0", "and", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\"momentum != 0 is not compatible with sparse gradients\"", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "state", "[", "'grad_sum_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "state", "[", "'s'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "if", "momentum", "!=", "0", ":", "\n", "                        ", "state", "[", "'x0'", "]", "=", "torch", ".", "clone", "(", "p", ")", ".", "detach", "(", ")", "\n", "\n", "", "", "state", "[", "'step'", "]", "+=", "1", "\n", "grad_sum_sq", "=", "state", "[", "'grad_sum_sq'", "]", "\n", "s", "=", "state", "[", "'s'", "]", "\n", "lamb", "=", "lr", "*", "math", ".", "sqrt", "(", "state", "[", "'step'", "]", ")", "\n", "\n", "# Apply weight decay", "\n", "if", "weight_decay", "!=", "0", ":", "\n", "                    ", "if", "group", "[", "'decoupled_decay'", "]", ":", "\n", "                        ", "p", ".", "mul_", "(", "1.0", "-", "group", "[", "'lr'", "]", "*", "weight_decay", ")", "\n", "", "else", ":", "\n", "                        ", "if", "grad", ".", "is_sparse", ":", "\n", "                            ", "raise", "RuntimeError", "(", "\"weight_decay option is not compatible with sparse gradients\"", ")", "\n", "", "grad", ".", "add_", "(", "p", ",", "alpha", "=", "weight_decay", ")", "\n", "\n", "", "", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "grad", "=", "grad", ".", "coalesce", "(", ")", "\n", "grad_val", "=", "grad", ".", "_values", "(", ")", "\n", "\n", "p_masked", "=", "p", ".", "sparse_mask", "(", "grad", ")", "\n", "grad_sum_sq_masked", "=", "grad_sum_sq", ".", "sparse_mask", "(", "grad", ")", "\n", "s_masked", "=", "s", ".", "sparse_mask", "(", "grad", ")", "\n", "\n", "# Compute x_0 from other known quantities", "\n", "rms_masked_vals", "=", "grad_sum_sq_masked", ".", "_values", "(", ")", ".", "pow", "(", "1", "/", "3", ")", ".", "add_", "(", "eps", ")", "\n", "x0_masked_vals", "=", "p_masked", ".", "_values", "(", ")", ".", "addcdiv", "(", "s_masked", ".", "_values", "(", ")", ",", "rms_masked_vals", ",", "value", "=", "1", ")", "\n", "\n", "# Dense + sparse op", "\n", "grad_sq", "=", "grad", "*", "grad", "\n", "grad_sum_sq", ".", "add_", "(", "grad_sq", ",", "alpha", "=", "lamb", ")", "\n", "grad_sum_sq_masked", ".", "add_", "(", "grad_sq", ",", "alpha", "=", "lamb", ")", "\n", "\n", "rms_masked_vals", "=", "grad_sum_sq_masked", ".", "_values", "(", ")", ".", "pow_", "(", "1", "/", "3", ")", ".", "add_", "(", "eps", ")", "\n", "\n", "s", ".", "add_", "(", "grad", ",", "alpha", "=", "lamb", ")", "\n", "s_masked", ".", "_values", "(", ")", ".", "add_", "(", "grad_val", ",", "alpha", "=", "lamb", ")", "\n", "\n", "# update masked copy of p", "\n", "p_kp1_masked_vals", "=", "x0_masked_vals", ".", "addcdiv", "(", "s_masked", ".", "_values", "(", ")", ",", "rms_masked_vals", ",", "value", "=", "-", "1", ")", "\n", "# Copy updated masked p to dense p using an add operation", "\n", "p_masked", ".", "_values", "(", ")", ".", "add_", "(", "p_kp1_masked_vals", ",", "alpha", "=", "-", "1", ")", "\n", "p", ".", "add_", "(", "p_masked", ",", "alpha", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "                    ", "if", "momentum", "==", "0", ":", "\n", "# Compute x_0 from other known quantities", "\n", "                        ", "rms", "=", "grad_sum_sq", ".", "pow", "(", "1", "/", "3", ")", ".", "add_", "(", "eps", ")", "\n", "x0", "=", "p", ".", "addcdiv", "(", "s", ",", "rms", ",", "value", "=", "1", ")", "\n", "", "else", ":", "\n", "                        ", "x0", "=", "state", "[", "'x0'", "]", "\n", "\n", "# Accumulate second moments", "\n", "", "grad_sum_sq", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "lamb", ")", "\n", "rms", "=", "grad_sum_sq", ".", "pow", "(", "1", "/", "3", ")", ".", "add_", "(", "eps", ")", "\n", "\n", "# Update s", "\n", "s", ".", "add_", "(", "grad", ",", "alpha", "=", "lamb", ")", "\n", "\n", "# Step", "\n", "if", "momentum", "==", "0", ":", "\n", "                        ", "p", ".", "copy_", "(", "x0", ".", "addcdiv", "(", "s", ",", "rms", ",", "value", "=", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                        ", "z", "=", "x0", ".", "addcdiv", "(", "s", ",", "rms", ",", "value", "=", "-", "1", ")", "\n", "\n", "# p is a moving average of z", "\n", "p", ".", "mul_", "(", "1", "-", "ck", ")", ".", "add_", "(", "z", ",", "alpha", "=", "ck", ")", "\n", "\n", "", "", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adahessian.Adahessian.__init__": [[26, 53], ["torch.Generator().manual_seed", "dict", "super().__init__", "adahessian.Adahessian.get_params", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "torch.Generator"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms.RandomResizedCropAndInterpolation.get_params"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "0.1", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "weight_decay", "=", "0.0", ",", "\n", "hessian_power", "=", "1.0", ",", "update_each", "=", "1", ",", "n_samples", "=", "1", ",", "avg_conv_kernel", "=", "False", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Invalid learning rate: {lr}\"", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Invalid epsilon value: {eps}\"", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Invalid beta parameter at index 0: {betas[0]}\"", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Invalid beta parameter at index 1: {betas[1]}\"", ")", "\n", "", "if", "not", "0.0", "<=", "hessian_power", "<=", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Invalid Hessian power value: {hessian_power}\"", ")", "\n", "\n", "", "self", ".", "n_samples", "=", "n_samples", "\n", "self", ".", "update_each", "=", "update_each", "\n", "self", ".", "avg_conv_kernel", "=", "avg_conv_kernel", "\n", "\n", "# use a separate generator that deterministically generates the same `z`s across all GPUs in case of distributed training", "\n", "self", ".", "seed", "=", "2147483647", "\n", "self", ".", "generator", "=", "torch", ".", "Generator", "(", ")", ".", "manual_seed", "(", "self", ".", "seed", ")", "\n", "\n", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ",", "hessian_power", "=", "hessian_power", ")", "\n", "super", "(", "Adahessian", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n", "for", "p", "in", "self", ".", "get_params", "(", ")", ":", "\n", "            ", "p", ".", "hess", "=", "0.0", "\n", "self", ".", "state", "[", "p", "]", "[", "\"hessian step\"", "]", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adahessian.Adahessian.is_second_order": [[54, 57], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "is_second_order", "(", "self", ")", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adahessian.Adahessian.get_params": [[58, 64], ["None"], "methods", ["None"], ["", "def", "get_params", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Gets all parameters in all param_groups with gradients\n        \"\"\"", "\n", "\n", "return", "(", "p", "for", "group", "in", "self", ".", "param_groups", "for", "p", "in", "group", "[", "'params'", "]", "if", "p", ".", "requires_grad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adahessian.Adahessian.zero_hessian": [[65, 73], ["adahessian.Adahessian.get_params", "p.hess.zero_", "isinstance"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms.RandomResizedCropAndInterpolation.get_params"], ["", "def", "zero_hessian", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Zeros out the accumalated hessian traces.\n        \"\"\"", "\n", "\n", "for", "p", "in", "self", ".", "get_params", "(", ")", ":", "\n", "            ", "if", "not", "isinstance", "(", "p", ".", "hess", ",", "float", ")", "and", "self", ".", "state", "[", "p", "]", "[", "\"hessian step\"", "]", "%", "self", ".", "update_each", "==", "0", ":", "\n", "                ", "p", ".", "hess", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adahessian.Adahessian.set_hessian": [[74, 101], ["torch.no_grad", "filter", "range", "adahessian.Adahessian.get_params", "len", "torch.Generator().manual_seed", "torch.autograd.grad", "zip", "params.append", "torch.Generator", "torch.randint", "p.size"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms.RandomResizedCropAndInterpolation.get_params"], ["", "", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "set_hessian", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Computes the Hutchinson approximation of the hessian trace and accumulates it for each trainable parameter.\n        \"\"\"", "\n", "\n", "params", "=", "[", "]", "\n", "for", "p", "in", "filter", "(", "lambda", "p", ":", "p", ".", "grad", "is", "not", "None", ",", "self", ".", "get_params", "(", ")", ")", ":", "\n", "            ", "if", "self", ".", "state", "[", "p", "]", "[", "\"hessian step\"", "]", "%", "self", ".", "update_each", "==", "0", ":", "# compute the trace only each `update_each` step", "\n", "                ", "params", ".", "append", "(", "p", ")", "\n", "", "self", ".", "state", "[", "p", "]", "[", "\"hessian step\"", "]", "+=", "1", "\n", "\n", "", "if", "len", "(", "params", ")", "==", "0", ":", "\n", "            ", "return", "\n", "\n", "", "if", "self", ".", "generator", ".", "device", "!=", "params", "[", "0", "]", ".", "device", ":", "# hackish way of casting the generator to the right device", "\n", "            ", "self", ".", "generator", "=", "torch", ".", "Generator", "(", "params", "[", "0", "]", ".", "device", ")", ".", "manual_seed", "(", "self", ".", "seed", ")", "\n", "\n", "", "grads", "=", "[", "p", ".", "grad", "for", "p", "in", "params", "]", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "n_samples", ")", ":", "\n", "# Rademacher distribution {-1.0, 1.0}", "\n", "            ", "zs", "=", "[", "torch", ".", "randint", "(", "0", ",", "2", ",", "p", ".", "size", "(", ")", ",", "generator", "=", "self", ".", "generator", ",", "device", "=", "p", ".", "device", ")", "*", "2.0", "-", "1.0", "for", "p", "in", "params", "]", "\n", "h_zs", "=", "torch", ".", "autograd", ".", "grad", "(", "\n", "grads", ",", "params", ",", "grad_outputs", "=", "zs", ",", "only_inputs", "=", "True", ",", "retain_graph", "=", "i", "<", "self", ".", "n_samples", "-", "1", ")", "\n", "for", "h_z", ",", "z", ",", "p", "in", "zip", "(", "h_zs", ",", "zs", ",", "params", ")", ":", "\n", "                ", "p", ".", "hess", "+=", "h_z", "*", "z", "/", "self", ".", "n_samples", "# approximate the expected values of z*(H@z)", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adahessian.Adahessian.step": [[102, 157], ["torch.no_grad", "adahessian.Adahessian.zero_hessian", "adahessian.Adahessian.set_hessian", "closure", "p.mul_", "exp_avg.mul_().add_", "exp_hessian_diag_sq.mul_().addcmul_", "p.addcdiv_", "torch.abs().mean().expand_as().clone", "len", "torch.zeros_like", "torch.zeros_like", "p.dim", "exp_avg.mul_", "exp_hessian_diag_sq.mul_", "torch.abs().mean().expand_as", "torch.abs().mean", "torch.abs"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adahessian.Adahessian.zero_hessian", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adahessian.Adahessian.set_hessian"], ["", "", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Performs a single optimization step.\n        Arguments:\n            closure (callable, optional) -- a closure that reevaluates the model and returns the loss (default: None)\n        \"\"\"", "\n", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "self", ".", "zero_hessian", "(", ")", "\n", "self", ".", "set_hessian", "(", ")", "\n", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", "or", "p", ".", "hess", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "self", ".", "avg_conv_kernel", "and", "p", ".", "dim", "(", ")", "==", "4", ":", "\n", "                    ", "p", ".", "hess", "=", "torch", ".", "abs", "(", "p", ".", "hess", ")", ".", "mean", "(", "dim", "=", "[", "2", ",", "3", "]", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "p", ".", "hess", ")", ".", "clone", "(", ")", "\n", "\n", "# Perform correct stepweight decay as in AdamW", "\n", "", "p", ".", "mul_", "(", "1", "-", "group", "[", "'lr'", "]", "*", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "1", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "# Exponential moving average of Hessian diagonal square values", "\n", "state", "[", "'exp_hessian_diag_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "\n", "", "exp_avg", ",", "exp_hessian_diag_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_hessian_diag_sq'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "p", ".", "grad", ",", "alpha", "=", "1", "-", "beta1", ")", "\n", "exp_hessian_diag_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "p", ".", "hess", ",", "p", ".", "hess", ",", "value", "=", "1", "-", "beta2", ")", "\n", "\n", "bias_correction1", "=", "1", "-", "beta1", "**", "state", "[", "'step'", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "state", "[", "'step'", "]", "\n", "\n", "k", "=", "group", "[", "'hessian_power'", "]", "\n", "denom", "=", "(", "exp_hessian_diag_sq", "/", "bias_correction2", ")", ".", "pow_", "(", "k", "/", "2", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "\n", "# make update", "\n", "step_size", "=", "group", "[", "'lr'", "]", "/", "bias_correction1", "\n", "p", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "step_size", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adabelief.AdaBelief.__init__": [[42, 65], ["dict", "torch.optim.optimizer.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "isinstance", "isinstance", "len", "range", "range"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-16", ",", "weight_decay", "=", "0", ",", "amsgrad", "=", "False", ",", "\n", "decoupled_decay", "=", "True", ",", "fixed_decay", "=", "False", ",", "rectify", "=", "True", ",", "degenerated_to_sgd", "=", "True", ")", ":", "\n", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "\n", "", "if", "isinstance", "(", "params", ",", "(", "list", ",", "tuple", ")", ")", "and", "len", "(", "params", ")", ">", "0", "and", "isinstance", "(", "params", "[", "0", "]", ",", "dict", ")", ":", "\n", "            ", "for", "param", "in", "params", ":", "\n", "                ", "if", "'betas'", "in", "param", "and", "(", "param", "[", "'betas'", "]", "[", "0", "]", "!=", "betas", "[", "0", "]", "or", "param", "[", "'betas'", "]", "[", "1", "]", "!=", "betas", "[", "1", "]", ")", ":", "\n", "                    ", "param", "[", "'buffer'", "]", "=", "[", "[", "None", ",", "None", ",", "None", "]", "for", "_", "in", "range", "(", "10", ")", "]", "\n", "\n", "", "", "", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ",", "amsgrad", "=", "amsgrad", ",", "\n", "degenerated_to_sgd", "=", "degenerated_to_sgd", ",", "decoupled_decay", "=", "decoupled_decay", ",", "rectify", "=", "rectify", ",", "\n", "fixed_decay", "=", "fixed_decay", ",", "buffer", "=", "[", "[", "None", ",", "None", ",", "None", "]", "for", "_", "in", "range", "(", "10", ")", "]", ")", "\n", "super", "(", "AdaBelief", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adabelief.AdaBelief.__setstate__": [[66, 70], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.ml_decoder.TransformerDecoderLayerOptimal.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "AdaBelief", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'amsgrad'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adabelief.AdaBelief.reset": [[71, 88], ["torch.no_grad", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "methods", ["None"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "amsgrad", "=", "group", "[", "'amsgrad'", "]", "\n", "\n", "# State initialization", "\n", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_var'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                    ", "state", "[", "'max_exp_avg_var'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adabelief.AdaBelief.step": [[89, 202], ["torch.no_grad", "torch.enable_grad", "closure", "exp_avg.mul_().add_", "exp_avg_var.mul_().addcmul_", "grad.float.float.float", "RuntimeError", "p_fp32.float.float.float", "len", "torch.zeros_like", "torch.zeros_like", "torch.max", "p_fp32.float.float.addcdiv_", "p.copy_", "torch.zeros_like", "p_fp32.float.float.mul_", "p_fp32.float.float.mul_", "grad.float.float.add_", "exp_avg.mul_", "exp_avg_var.mul_", "exp_avg_var.add_", "exp_avg_var.sqrt().add_", "p_fp32.float.float.addcdiv_", "int", "p_fp32.float.float.add_", "max_exp_avg_var.sqrt", "math.sqrt", "exp_avg_var.add_().sqrt", "math.sqrt", "math.sqrt", "exp_avg_var.sqrt", "exp_avg_var.add_"], "methods", ["None"], ["", "", "", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", "\n", "if", "grad", ".", "dtype", "in", "{", "torch", ".", "float16", ",", "torch", ".", "bfloat16", "}", ":", "\n", "                    ", "grad", "=", "grad", ".", "float", "(", ")", "\n", "", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\n", "'AdaBelief does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "p_fp32", "=", "p", "\n", "if", "p", ".", "dtype", "in", "{", "torch", ".", "float16", ",", "torch", ".", "bfloat16", "}", ":", "\n", "                    ", "p_fp32", "=", "p_fp32", ".", "float", "(", ")", "\n", "\n", "", "amsgrad", "=", "group", "[", "'amsgrad'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p_fp32", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_var'", "]", "=", "torch", ".", "zeros_like", "(", "p_fp32", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_var'", "]", "=", "torch", ".", "zeros_like", "(", "p_fp32", ")", "\n", "\n", "# perform weight decay, check if decoupled weight decay", "\n", "", "", "if", "group", "[", "'decoupled_decay'", "]", ":", "\n", "                    ", "if", "not", "group", "[", "'fixed_decay'", "]", ":", "\n", "                        ", "p_fp32", ".", "mul_", "(", "1.0", "-", "group", "[", "'lr'", "]", "*", "group", "[", "'weight_decay'", "]", ")", "\n", "", "else", ":", "\n", "                        ", "p_fp32", ".", "mul_", "(", "1.0", "-", "group", "[", "'weight_decay'", "]", ")", "\n", "", "", "else", ":", "\n", "                    ", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                        ", "grad", ".", "add_", "(", "p_fp32", ",", "alpha", "=", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "# get current state variable", "\n", "", "", "exp_avg", ",", "exp_avg_var", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_var'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "bias_correction1", "=", "1", "-", "beta1", "**", "state", "[", "'step'", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "state", "[", "'step'", "]", "\n", "\n", "# Update first and second moment running average", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1", "-", "beta1", ")", "\n", "grad_residual", "=", "grad", "-", "exp_avg", "\n", "exp_avg_var", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "grad_residual", ",", "grad_residual", ",", "value", "=", "1", "-", "beta2", ")", "\n", "\n", "if", "amsgrad", ":", "\n", "                    ", "max_exp_avg_var", "=", "state", "[", "'max_exp_avg_var'", "]", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "torch", ".", "max", "(", "max_exp_avg_var", ",", "exp_avg_var", ".", "add_", "(", "group", "[", "'eps'", "]", ")", ",", "out", "=", "max_exp_avg_var", ")", "\n", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "(", "max_exp_avg_var", ".", "sqrt", "(", ")", "/", "math", ".", "sqrt", "(", "bias_correction2", ")", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "denom", "=", "(", "exp_avg_var", ".", "add_", "(", "group", "[", "'eps'", "]", ")", ".", "sqrt", "(", ")", "/", "math", ".", "sqrt", "(", "bias_correction2", ")", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "\n", "# update", "\n", "", "if", "not", "group", "[", "'rectify'", "]", ":", "\n", "# Default update", "\n", "                    ", "step_size", "=", "group", "[", "'lr'", "]", "/", "bias_correction1", "\n", "p_fp32", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "step_size", ")", "\n", "", "else", ":", "\n", "# Rectified update, forked from RAdam", "\n", "                    ", "buffered", "=", "group", "[", "'buffer'", "]", "[", "int", "(", "state", "[", "'step'", "]", "%", "10", ")", "]", "\n", "if", "state", "[", "'step'", "]", "==", "buffered", "[", "0", "]", ":", "\n", "                        ", "num_sma", ",", "step_size", "=", "buffered", "[", "1", "]", ",", "buffered", "[", "2", "]", "\n", "", "else", ":", "\n", "                        ", "buffered", "[", "0", "]", "=", "state", "[", "'step'", "]", "\n", "beta2_t", "=", "beta2", "**", "state", "[", "'step'", "]", "\n", "num_sma_max", "=", "2", "/", "(", "1", "-", "beta2", ")", "-", "1", "\n", "num_sma", "=", "num_sma_max", "-", "2", "*", "state", "[", "'step'", "]", "*", "beta2_t", "/", "(", "1", "-", "beta2_t", ")", "\n", "buffered", "[", "1", "]", "=", "num_sma", "\n", "\n", "# more conservative since it's an approximated value", "\n", "if", "num_sma", ">=", "5", ":", "\n", "                            ", "step_size", "=", "math", ".", "sqrt", "(", "\n", "(", "1", "-", "beta2_t", ")", "*", "\n", "(", "num_sma", "-", "4", ")", "/", "(", "num_sma_max", "-", "4", ")", "*", "\n", "(", "num_sma", "-", "2", ")", "/", "num_sma", "*", "\n", "num_sma_max", "/", "(", "num_sma_max", "-", "2", ")", ")", "/", "(", "1", "-", "beta1", "**", "state", "[", "'step'", "]", ")", "\n", "", "elif", "group", "[", "'degenerated_to_sgd'", "]", ":", "\n", "                            ", "step_size", "=", "1.0", "/", "(", "1", "-", "beta1", "**", "state", "[", "'step'", "]", ")", "\n", "", "else", ":", "\n", "                            ", "step_size", "=", "-", "1", "\n", "", "buffered", "[", "2", "]", "=", "step_size", "\n", "\n", "", "if", "num_sma", ">=", "5", ":", "\n", "                        ", "denom", "=", "exp_avg_var", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "p_fp32", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "step_size", "*", "group", "[", "'lr'", "]", ")", "\n", "", "elif", "step_size", ">", "0", ":", "\n", "                        ", "p_fp32", ".", "add_", "(", "exp_avg", ",", "alpha", "=", "-", "step_size", "*", "group", "[", "'lr'", "]", ")", "\n", "\n", "", "", "if", "p", ".", "dtype", "in", "{", "torch", ".", "float16", ",", "torch", ".", "bfloat16", "}", ":", "\n", "                    ", "p", ".", "copy_", "(", "p_fp32", ")", "\n", "\n", "", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adamw.AdamW.__init__": [[39, 52], ["dict", "torch.optim.optimizer.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "1e-2", ",", "amsgrad", "=", "False", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "amsgrad", "=", "amsgrad", ")", "\n", "super", "(", "AdamW", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adamw.AdamW.__setstate__": [[53, 57], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.ml_decoder.TransformerDecoderLayerOptimal.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "AdamW", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'amsgrad'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adamw.AdamW.step": [[58, 123], ["torch.no_grad", "torch.enable_grad", "closure", "p.data.mul_", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "p.addcdiv_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.max", "torch.zeros_like", "exp_avg.mul_", "exp_avg_sq.mul_", "max_exp_avg_sq.sqrt", "math.sqrt", "exp_avg_sq.sqrt", "math.sqrt"], "methods", ["None"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "# Perform stepweight decay", "\n", "", "p", ".", "data", ".", "mul_", "(", "1", "-", "group", "[", "'lr'", "]", "*", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "# Perform optimization step", "\n", "grad", "=", "p", ".", "grad", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "", "amsgrad", "=", "group", "[", "'amsgrad'", "]", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "\n", "", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "if", "amsgrad", ":", "\n", "                    ", "max_exp_avg_sq", "=", "state", "[", "'max_exp_avg_sq'", "]", "\n", "", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "bias_correction1", "=", "1", "-", "beta1", "**", "state", "[", "'step'", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "state", "[", "'step'", "]", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1", "-", "beta1", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "1", "-", "beta2", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "                    ", "torch", ".", "max", "(", "max_exp_avg_sq", ",", "exp_avg_sq", ",", "out", "=", "max_exp_avg_sq", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "(", "max_exp_avg_sq", ".", "sqrt", "(", ")", "/", "math", ".", "sqrt", "(", "bias_correction2", ")", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "denom", "=", "(", "exp_avg_sq", ".", "sqrt", "(", ")", "/", "math", ".", "sqrt", "(", "bias_correction2", ")", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "\n", "", "step_size", "=", "group", "[", "'lr'", "]", "/", "bias_correction1", "\n", "\n", "p", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "step_size", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.sgdp.SGDP.__init__": [[20, 26], ["dict", "torch.optim.optimizer.Optimizer.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "required", ",", "momentum", "=", "0", ",", "dampening", "=", "0", ",", "\n", "weight_decay", "=", "0", ",", "nesterov", "=", "False", ",", "eps", "=", "1e-8", ",", "delta", "=", "0.1", ",", "wd_ratio", "=", "0.1", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "momentum", "=", "momentum", ",", "dampening", "=", "dampening", ",", "weight_decay", "=", "weight_decay", ",", "\n", "nesterov", "=", "nesterov", ",", "eps", "=", "eps", ",", "delta", "=", "delta", ",", "wd_ratio", "=", "wd_ratio", ")", "\n", "super", "(", "SGDP", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.sgdp.SGDP.step": [[27, 71], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "closure", "buf.mul_().add_", "p.add_", "len", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "len", "adamp.projection", "p.mul_", "buf.mul_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adamp.projection"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "weight_decay", "=", "group", "[", "'weight_decay'", "]", "\n", "momentum", "=", "group", "[", "'momentum'", "]", "\n", "dampening", "=", "group", "[", "'dampening'", "]", "\n", "nesterov", "=", "group", "[", "'nesterov'", "]", "\n", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'momentum'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "\n", "# SGD", "\n", "", "buf", "=", "state", "[", "'momentum'", "]", "\n", "buf", ".", "mul_", "(", "momentum", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1.", "-", "dampening", ")", "\n", "if", "nesterov", ":", "\n", "                    ", "d_p", "=", "grad", "+", "momentum", "*", "buf", "\n", "", "else", ":", "\n", "                    ", "d_p", "=", "buf", "\n", "\n", "# Projection", "\n", "", "wd_ratio", "=", "1.", "\n", "if", "len", "(", "p", ".", "shape", ")", ">", "1", ":", "\n", "                    ", "d_p", ",", "wd_ratio", "=", "projection", "(", "p", ",", "grad", ",", "d_p", ",", "group", "[", "'delta'", "]", ",", "group", "[", "'wd_ratio'", "]", ",", "group", "[", "'eps'", "]", ")", "\n", "\n", "# Weight decay", "\n", "", "if", "weight_decay", "!=", "0", ":", "\n", "                    ", "p", ".", "mul_", "(", "1.", "-", "group", "[", "'lr'", "]", "*", "group", "[", "'weight_decay'", "]", "*", "wd_ratio", "/", "(", "1", "-", "momentum", ")", ")", "\n", "\n", "# Step", "\n", "", "p", ".", "add_", "(", "d_p", ",", "alpha", "=", "-", "group", "[", "'lr'", "]", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.nvnovograd.NvNovoGrad.__init__": [[32, 48], ["dict", "torch.optim.optimizer.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.95", ",", "0.98", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "grad_averaging", "=", "False", ",", "amsgrad", "=", "False", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "\n", "grad_averaging", "=", "grad_averaging", ",", "\n", "amsgrad", "=", "amsgrad", ")", "\n", "\n", "super", "(", "NvNovoGrad", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.nvnovograd.NvNovoGrad.__setstate__": [[49, 53], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.ml_decoder.TransformerDecoderLayerOptimal.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "NvNovoGrad", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'amsgrad'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.nvnovograd.NvNovoGrad.step": [[54, 121], ["torch.no_grad", "torch.enable_grad", "closure", "torch.sum", "grad.div_", "exp_avg.mul_().add_", "p.add_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros().to", "torch.pow", "exp_avg_sq.copy_", "exp_avg_sq.mul_().add_", "torch.max", "max_exp_avg_sq.sqrt().add_", "exp_avg_sq.sqrt().add_", "grad.add_", "grad.mul_", "torch.zeros().to", "exp_avg.mul_", "torch.zeros", "exp_avg_sq.mul_", "max_exp_avg_sq.sqrt", "exp_avg_sq.sqrt", "torch.zeros"], "methods", ["None"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n            and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Sparse gradients are not supported.'", ")", "\n", "", "amsgrad", "=", "group", "[", "'amsgrad'", "]", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros", "(", "[", "]", ")", ".", "to", "(", "state", "[", "'exp_avg'", "]", ".", "device", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_sq'", "]", "=", "torch", ".", "zeros", "(", "[", "]", ")", ".", "to", "(", "state", "[", "'exp_avg'", "]", ".", "device", ")", "\n", "\n", "", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "if", "amsgrad", ":", "\n", "                    ", "max_exp_avg_sq", "=", "state", "[", "'max_exp_avg_sq'", "]", "\n", "", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "norm", "=", "torch", ".", "sum", "(", "torch", ".", "pow", "(", "grad", ",", "2", ")", ")", "\n", "\n", "if", "exp_avg_sq", "==", "0", ":", "\n", "                    ", "exp_avg_sq", ".", "copy_", "(", "norm", ")", "\n", "", "else", ":", "\n", "                    ", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "add_", "(", "norm", ",", "alpha", "=", "1", "-", "beta2", ")", "\n", "\n", "", "if", "amsgrad", ":", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "                    ", "torch", ".", "max", "(", "max_exp_avg_sq", ",", "exp_avg_sq", ",", "out", "=", "max_exp_avg_sq", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "\n", "", "grad", ".", "div_", "(", "denom", ")", "\n", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "grad", ".", "add_", "(", "p", ",", "alpha", "=", "group", "[", "'weight_decay'", "]", ")", "\n", "", "if", "group", "[", "'grad_averaging'", "]", ":", "\n", "                    ", "grad", ".", "mul_", "(", "1", "-", "beta1", ")", "\n", "", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "grad", ")", "\n", "\n", "p", ".", "add_", "(", "exp_avg", ",", "alpha", "=", "-", "group", "[", "'lr'", "]", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.nadam.Nadam.__init__": [[30, 37], ["dict", "torch.optim.optimizer.Optimizer.__init__", "ValueError"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "2e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "schedule_decay", "=", "4e-3", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ",", "schedule_decay", "=", "schedule_decay", ")", "\n", "super", "(", "Nadam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.nadam.Nadam.step": [[38, 93], ["torch.no_grad", "torch.enable_grad", "closure", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "p.addcdiv_", "p.addcdiv_", "len", "torch.zeros_like", "torch.zeros_like", "grad.add.add.add", "exp_avg.mul_", "exp_avg_sq.mul_", "exp_avg_sq.sqrt", "math.sqrt"], "methods", ["None"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "state", "[", "'m_schedule'", "]", "=", "1.", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "\n", "# Warming momentum schedule", "\n", "", "m_schedule", "=", "state", "[", "'m_schedule'", "]", "\n", "schedule_decay", "=", "group", "[", "'schedule_decay'", "]", "\n", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "eps", "=", "group", "[", "'eps'", "]", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "t", "=", "state", "[", "'step'", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "t", "\n", "\n", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "grad", "=", "grad", ".", "add", "(", "p", ",", "alpha", "=", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "", "momentum_cache_t", "=", "beta1", "*", "(", "1.", "-", "0.5", "*", "(", "0.96", "**", "(", "t", "*", "schedule_decay", ")", ")", ")", "\n", "momentum_cache_t_1", "=", "beta1", "*", "(", "1.", "-", "0.5", "*", "(", "0.96", "**", "(", "(", "t", "+", "1", ")", "*", "schedule_decay", ")", ")", ")", "\n", "m_schedule_new", "=", "m_schedule", "*", "momentum_cache_t", "\n", "m_schedule_next", "=", "m_schedule", "*", "momentum_cache_t", "*", "momentum_cache_t_1", "\n", "state", "[", "'m_schedule'", "]", "=", "m_schedule_new", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1.", "-", "beta1", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "1.", "-", "beta2", ")", "\n", "\n", "denom", "=", "(", "exp_avg_sq", ".", "sqrt", "(", ")", "/", "math", ".", "sqrt", "(", "bias_correction2", ")", ")", ".", "add_", "(", "eps", ")", "\n", "p", ".", "addcdiv_", "(", "grad", ",", "denom", ",", "value", "=", "-", "group", "[", "'lr'", "]", "*", "(", "1.", "-", "momentum_cache_t", ")", "/", "(", "1.", "-", "m_schedule_new", ")", ")", "\n", "p", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "group", "[", "'lr'", "]", "*", "momentum_cache_t_1", "/", "(", "1.", "-", "m_schedule_next", ")", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.param_groups_weight_decay": [[35, 55], ["set", "model.named_parameters", "name.endswith", "no_decay.append", "decay.append"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEmaV2.set"], ["", "def", "param_groups_weight_decay", "(", "\n", "model", ":", "nn", ".", "Module", ",", "\n", "weight_decay", "=", "1e-5", ",", "\n", "no_weight_decay_list", "=", "(", ")", "\n", ")", ":", "\n", "    ", "no_weight_decay_list", "=", "set", "(", "no_weight_decay_list", ")", "\n", "decay", "=", "[", "]", "\n", "no_decay", "=", "[", "]", "\n", "for", "name", ",", "param", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "        ", "if", "not", "param", ".", "requires_grad", ":", "\n", "            ", "continue", "\n", "\n", "", "if", "param", ".", "ndim", "<=", "1", "or", "name", ".", "endswith", "(", "\".bias\"", ")", "or", "name", "in", "no_weight_decay_list", ":", "\n", "            ", "no_decay", ".", "append", "(", "param", ")", "\n", "", "else", ":", "\n", "            ", "decay", ".", "append", "(", "param", ")", "\n", "\n", "", "", "return", "[", "\n", "{", "'params'", ":", "no_decay", ",", "'weight_decay'", ":", "0.", "}", ",", "\n", "{", "'params'", ":", "decay", ",", "'weight_decay'", ":", "weight_decay", "}", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory._group": [[57, 60], ["iter", "iter", "tuple", "itertools.islice"], "function", ["None"], ["", "def", "_group", "(", "it", ",", "size", ")", ":", "\n", "    ", "it", "=", "iter", "(", "it", ")", "\n", "return", "iter", "(", "lambda", ":", "tuple", "(", "islice", "(", "it", ",", "size", ")", ")", ",", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory._layer_map": [[62, 87], ["getattr().get", "model.named_parameters", "len", "list", "len", "layer_map.update", "optim_factory._group", "isinstance", "getattr", "optim_factory._layer_map._in_head"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory._group"], ["", "def", "_layer_map", "(", "model", ",", "layers_per_group", "=", "12", ",", "num_groups", "=", "None", ")", ":", "\n", "    ", "def", "_in_head", "(", "n", ",", "hp", ")", ":", "\n", "        ", "if", "not", "hp", ":", "\n", "            ", "return", "True", "\n", "", "elif", "isinstance", "(", "hp", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "return", "any", "(", "[", "n", ".", "startswith", "(", "hpi", ")", "for", "hpi", "in", "hp", "]", ")", "\n", "", "else", ":", "\n", "            ", "return", "n", ".", "startswith", "(", "hp", ")", "\n", "\n", "", "", "head_prefix", "=", "getattr", "(", "model", ",", "'pretrained_cfg'", ",", "{", "}", ")", ".", "get", "(", "'classifier'", ",", "None", ")", "\n", "names_trunk", "=", "[", "]", "\n", "names_head", "=", "[", "]", "\n", "for", "n", ",", "_", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "        ", "names_head", ".", "append", "(", "n", ")", "if", "_in_head", "(", "n", ",", "head_prefix", ")", "else", "names_trunk", ".", "append", "(", "n", ")", "\n", "\n", "# group non-head layers", "\n", "", "num_trunk_layers", "=", "len", "(", "names_trunk", ")", "\n", "if", "num_groups", "is", "not", "None", ":", "\n", "        ", "layers_per_group", "=", "-", "(", "num_trunk_layers", "//", "-", "num_groups", ")", "\n", "", "names_trunk", "=", "list", "(", "_group", "(", "names_trunk", ",", "layers_per_group", ")", ")", "\n", "\n", "num_trunk_groups", "=", "len", "(", "names_trunk", ")", "\n", "layer_map", "=", "{", "n", ":", "i", "for", "i", ",", "l", "in", "enumerate", "(", "names_trunk", ")", "for", "n", "in", "l", "}", "\n", "layer_map", ".", "update", "(", "{", "n", ":", "num_trunk_groups", "for", "n", "in", "names_head", "}", ")", "\n", "return", "layer_map", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.param_groups_layer_decay": [[89, 149], ["set", "hasattr", "list", "model.named_parameters", "print", "list", "timm.models.helpers.group_parameters", "optim_factory._layer_map", "max", "_layer_map.get", "[].append", "[].append", "param_groups.values", "model.group_matcher", "_layer_map.values", "json.dumps", "range"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.group_parameters", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory._layer_map", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.group_matcher"], ["", "def", "param_groups_layer_decay", "(", "\n", "model", ":", "nn", ".", "Module", ",", "\n", "weight_decay", ":", "float", "=", "0.05", ",", "\n", "no_weight_decay_list", ":", "Tuple", "[", "str", "]", "=", "(", ")", ",", "\n", "layer_decay", ":", "float", "=", ".75", ",", "\n", "end_layer_decay", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Parameter groups for layer-wise lr decay & weight decay\n    Based on BEiT: https://github.com/microsoft/unilm/blob/master/beit/optim_factory.py#L58\n    \"\"\"", "\n", "no_weight_decay_list", "=", "set", "(", "no_weight_decay_list", ")", "\n", "param_group_names", "=", "{", "}", "# NOTE for debugging", "\n", "param_groups", "=", "{", "}", "\n", "\n", "if", "hasattr", "(", "model", ",", "'group_matcher'", ")", ":", "\n", "# FIXME interface needs more work", "\n", "        ", "layer_map", "=", "group_parameters", "(", "model", ",", "model", ".", "group_matcher", "(", "coarse", "=", "False", ")", ",", "reverse", "=", "True", ")", "\n", "", "else", ":", "\n", "# fallback", "\n", "        ", "layer_map", "=", "_layer_map", "(", "model", ")", "\n", "", "num_layers", "=", "max", "(", "layer_map", ".", "values", "(", ")", ")", "+", "1", "\n", "layer_max", "=", "num_layers", "-", "1", "\n", "layer_scales", "=", "list", "(", "layer_decay", "**", "(", "layer_max", "-", "i", ")", "for", "i", "in", "range", "(", "num_layers", ")", ")", "\n", "\n", "for", "name", ",", "param", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "        ", "if", "not", "param", ".", "requires_grad", ":", "\n", "            ", "continue", "\n", "\n", "# no decay: all 1D parameters and model specific ones", "\n", "", "if", "param", ".", "ndim", "==", "1", "or", "name", "in", "no_weight_decay_list", ":", "\n", "            ", "g_decay", "=", "\"no_decay\"", "\n", "this_decay", "=", "0.", "\n", "", "else", ":", "\n", "            ", "g_decay", "=", "\"decay\"", "\n", "this_decay", "=", "weight_decay", "\n", "\n", "", "layer_id", "=", "layer_map", ".", "get", "(", "name", ",", "layer_max", ")", "\n", "group_name", "=", "\"layer_%d_%s\"", "%", "(", "layer_id", ",", "g_decay", ")", "\n", "\n", "if", "group_name", "not", "in", "param_groups", ":", "\n", "            ", "this_scale", "=", "layer_scales", "[", "layer_id", "]", "\n", "param_group_names", "[", "group_name", "]", "=", "{", "\n", "\"lr_scale\"", ":", "this_scale", ",", "\n", "\"weight_decay\"", ":", "this_decay", ",", "\n", "\"param_names\"", ":", "[", "]", ",", "\n", "}", "\n", "param_groups", "[", "group_name", "]", "=", "{", "\n", "\"lr_scale\"", ":", "this_scale", ",", "\n", "\"weight_decay\"", ":", "this_decay", ",", "\n", "\"params\"", ":", "[", "]", ",", "\n", "}", "\n", "\n", "", "param_group_names", "[", "group_name", "]", "[", "\"param_names\"", "]", ".", "append", "(", "name", ")", "\n", "param_groups", "[", "group_name", "]", "[", "\"params\"", "]", ".", "append", "(", "param", ")", "\n", "\n", "# FIXME temporary output to debug new feature", "\n", "", "print", "(", "\"parameter groups: \\n%s\"", "%", "json", ".", "dumps", "(", "param_group_names", ",", "indent", "=", "2", ")", ")", "\n", "\n", "return", "list", "(", "param_groups", ".", "values", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.optimizer_kwargs": [[151, 169], ["dict", "getattr", "getattr", "getattr", "getattr", "dict.update"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update"], ["", "def", "optimizer_kwargs", "(", "cfg", ")", ":", "\n", "    ", "\"\"\" cfg/argparse to kwargs helper\n    Convert optimizer args in argparse args or cfg like object to keyword args for updated create fn.\n    \"\"\"", "\n", "kwargs", "=", "dict", "(", "\n", "opt", "=", "cfg", ".", "opt", ",", "\n", "lr", "=", "cfg", ".", "lr", ",", "\n", "weight_decay", "=", "cfg", ".", "weight_decay", ",", "\n", "momentum", "=", "cfg", ".", "momentum", ")", "\n", "if", "getattr", "(", "cfg", ",", "'opt_eps'", ",", "None", ")", "is", "not", "None", ":", "\n", "        ", "kwargs", "[", "'eps'", "]", "=", "cfg", ".", "opt_eps", "\n", "", "if", "getattr", "(", "cfg", ",", "'opt_betas'", ",", "None", ")", "is", "not", "None", ":", "\n", "        ", "kwargs", "[", "'betas'", "]", "=", "cfg", ".", "opt_betas", "\n", "", "if", "getattr", "(", "cfg", ",", "'layer_decay'", ",", "None", ")", "is", "not", "None", ":", "\n", "        ", "kwargs", "[", "'layer_decay'", "]", "=", "cfg", ".", "layer_decay", "\n", "", "if", "getattr", "(", "cfg", ",", "'opt_args'", ",", "None", ")", "is", "not", "None", ":", "\n", "        ", "kwargs", ".", "update", "(", "cfg", ".", "opt_args", ")", "\n", "", "return", "kwargs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer": [[171, 179], ["optim_factory.create_optimizer_v2", "optim_factory.optimizer_kwargs"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.optimizer_kwargs"], ["", "def", "create_optimizer", "(", "args", ",", "model", ",", "filter_bias_and_bn", "=", "True", ")", ":", "\n", "    ", "\"\"\" Legacy optimizer factory for backwards compatibility.\n    NOTE: Use create_optimizer_v2 for new code.\n    \"\"\"", "\n", "return", "create_optimizer_v2", "(", "\n", "model", ",", "\n", "**", "optimizer_kwargs", "(", "cfg", "=", "args", ")", ",", "\n", "filter_bias_and_bn", "=", "filter_bias_and_bn", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.create_optimizer_v2": [[182, 337], ["isinstance", "opt.lower", "opt.lower.split", "dict", "hasattr", "dict.setdefault", "dict.pop", "torch.SGD", "len", "model_or_params.no_weight_decay", "param_group_fn", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "dict.pop", "torch.SGD", "lookahead.Lookahead", "optim_factory.param_groups_layer_decay", "sgdp.SGDP", "optim_factory.param_groups_weight_decay", "model_or_params.parameters", "torch.Adam", "torch.AdamW", "adamp.AdamP", "torch.Nadam", "radam.RAdam", "nadam.Nadam", "torch.Adamax", "adabelief.AdaBelief", "adabelief.AdaBelief", "torch.Adadelta", "dict.setdefault", "torch.Adagrad", "adafactor.Adafactor", "lamb.Lamb", "lamb.Lamb", "lars.Lars", "lars.Lars", "lars.Lars", "lars.Lars", "madgrad.MADGRAD", "madgrad.MADGRAD", "nvnovograd.NvNovoGrad", "torch.RMSprop", "rmsprop_tf.RMSpropTF", "adahessian.Adahessian", "dict.pop", "FusedSGD", "dict.pop", "FusedSGD", "FusedAdam", "FusedAdam", "FusedLAMB", "dict.setdefault", "FusedNovoGrad"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.no_weight_decay", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.param_groups_layer_decay", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.optim_factory.param_groups_weight_decay"], ["", "def", "create_optimizer_v2", "(", "\n", "model_or_params", ",", "\n", "opt", ":", "str", "=", "'sgd'", ",", "\n", "lr", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "weight_decay", ":", "float", "=", "0.", ",", "\n", "momentum", ":", "float", "=", "0.9", ",", "\n", "filter_bias_and_bn", ":", "bool", "=", "True", ",", "\n", "layer_decay", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "param_group_fn", ":", "Optional", "[", "Callable", "]", "=", "None", ",", "\n", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Create an optimizer.\n\n    TODO currently the model is passed in and all parameters are selected for optimization.\n    For more general use an interface that allows selection of parameters to optimize and lr groups, one of:\n      * a filter fn interface that further breaks params into groups in a weight_decay compatible fashion\n      * expose the parameters interface and leave it up to caller\n\n    Args:\n        model_or_params (nn.Module): model containing parameters to optimize\n        opt: name of optimizer to create\n        lr: initial learning rate\n        weight_decay: weight decay to apply in optimizer\n        momentum:  momentum for momentum based optimizers (others may use betas via kwargs)\n        filter_bias_and_bn:  filter out bias, bn and other 1d params from weight decay\n        **kwargs: extra optimizer specific kwargs to pass through\n\n    Returns:\n        Optimizer\n    \"\"\"", "\n", "if", "isinstance", "(", "model_or_params", ",", "nn", ".", "Module", ")", ":", "\n", "# a model was passed in, extract parameters and add weight decays to appropriate layers", "\n", "        ", "no_weight_decay", "=", "{", "}", "\n", "if", "hasattr", "(", "model_or_params", ",", "'no_weight_decay'", ")", ":", "\n", "            ", "no_weight_decay", "=", "model_or_params", ".", "no_weight_decay", "(", ")", "\n", "\n", "", "if", "param_group_fn", ":", "\n", "            ", "parameters", "=", "param_group_fn", "(", "model_or_params", ")", "\n", "", "elif", "layer_decay", "is", "not", "None", ":", "\n", "            ", "parameters", "=", "param_groups_layer_decay", "(", "\n", "model_or_params", ",", "\n", "weight_decay", "=", "weight_decay", ",", "\n", "layer_decay", "=", "layer_decay", ",", "\n", "no_weight_decay_list", "=", "no_weight_decay", ")", "\n", "weight_decay", "=", "0.", "\n", "", "elif", "weight_decay", "and", "filter_bias_and_bn", ":", "\n", "            ", "parameters", "=", "param_groups_weight_decay", "(", "model_or_params", ",", "weight_decay", ",", "no_weight_decay", ")", "\n", "weight_decay", "=", "0.", "\n", "", "else", ":", "\n", "            ", "parameters", "=", "model_or_params", ".", "parameters", "(", ")", "\n", "", "", "else", ":", "\n", "# iterable of parameters or param groups passed in", "\n", "        ", "parameters", "=", "model_or_params", "\n", "\n", "", "opt_lower", "=", "opt", ".", "lower", "(", ")", "\n", "opt_split", "=", "opt_lower", ".", "split", "(", "'_'", ")", "\n", "opt_lower", "=", "opt_split", "[", "-", "1", "]", "\n", "if", "'fused'", "in", "opt_lower", ":", "\n", "        ", "assert", "has_apex", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "'APEX and CUDA required for fused optimizers'", "\n", "\n", "", "opt_args", "=", "dict", "(", "weight_decay", "=", "weight_decay", ",", "**", "kwargs", ")", "\n", "if", "lr", "is", "not", "None", ":", "\n", "        ", "opt_args", ".", "setdefault", "(", "'lr'", ",", "lr", ")", "\n", "\n", "# basic SGD & related", "\n", "", "if", "opt_lower", "==", "'sgd'", "or", "opt_lower", "==", "'nesterov'", ":", "\n", "# NOTE 'sgd' refers to SGD + nesterov momentum for legacy / backwards compat reasons", "\n", "        ", "opt_args", ".", "pop", "(", "'eps'", ",", "None", ")", "\n", "optimizer", "=", "optim", ".", "SGD", "(", "parameters", ",", "momentum", "=", "momentum", ",", "nesterov", "=", "True", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'momentum'", ":", "\n", "        ", "opt_args", ".", "pop", "(", "'eps'", ",", "None", ")", "\n", "optimizer", "=", "optim", ".", "SGD", "(", "parameters", ",", "momentum", "=", "momentum", ",", "nesterov", "=", "False", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'sgdp'", ":", "\n", "        ", "optimizer", "=", "SGDP", "(", "parameters", ",", "momentum", "=", "momentum", ",", "nesterov", "=", "True", ",", "**", "opt_args", ")", "\n", "\n", "# adaptive", "\n", "", "elif", "opt_lower", "==", "'adam'", ":", "\n", "        ", "optimizer", "=", "optim", ".", "Adam", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'adamw'", ":", "\n", "        ", "optimizer", "=", "optim", ".", "AdamW", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'adamp'", ":", "\n", "        ", "optimizer", "=", "AdamP", "(", "parameters", ",", "wd_ratio", "=", "0.01", ",", "nesterov", "=", "True", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'nadam'", ":", "\n", "        ", "try", ":", "\n", "# NOTE PyTorch >= 1.10 should have native NAdam", "\n", "            ", "optimizer", "=", "optim", ".", "Nadam", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "except", "AttributeError", ":", "\n", "            ", "optimizer", "=", "Nadam", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "", "elif", "opt_lower", "==", "'radam'", ":", "\n", "        ", "optimizer", "=", "RAdam", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'adamax'", ":", "\n", "        ", "optimizer", "=", "optim", ".", "Adamax", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'adabelief'", ":", "\n", "        ", "optimizer", "=", "AdaBelief", "(", "parameters", ",", "rectify", "=", "False", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'radabelief'", ":", "\n", "        ", "optimizer", "=", "AdaBelief", "(", "parameters", ",", "rectify", "=", "True", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'adadelta'", ":", "\n", "        ", "optimizer", "=", "optim", ".", "Adadelta", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'adagrad'", ":", "\n", "        ", "opt_args", ".", "setdefault", "(", "'eps'", ",", "1e-8", ")", "\n", "optimizer", "=", "optim", ".", "Adagrad", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'adafactor'", ":", "\n", "        ", "optimizer", "=", "Adafactor", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'lamb'", ":", "\n", "        ", "optimizer", "=", "Lamb", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'lambc'", ":", "\n", "        ", "optimizer", "=", "Lamb", "(", "parameters", ",", "trust_clip", "=", "True", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'larc'", ":", "\n", "        ", "optimizer", "=", "Lars", "(", "parameters", ",", "momentum", "=", "momentum", ",", "trust_clip", "=", "True", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'lars'", ":", "\n", "        ", "optimizer", "=", "Lars", "(", "parameters", ",", "momentum", "=", "momentum", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'nlarc'", ":", "\n", "        ", "optimizer", "=", "Lars", "(", "parameters", ",", "momentum", "=", "momentum", ",", "trust_clip", "=", "True", ",", "nesterov", "=", "True", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'nlars'", ":", "\n", "        ", "optimizer", "=", "Lars", "(", "parameters", ",", "momentum", "=", "momentum", ",", "nesterov", "=", "True", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'madgrad'", ":", "\n", "        ", "optimizer", "=", "MADGRAD", "(", "parameters", ",", "momentum", "=", "momentum", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'madgradw'", ":", "\n", "        ", "optimizer", "=", "MADGRAD", "(", "parameters", ",", "momentum", "=", "momentum", ",", "decoupled_decay", "=", "True", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'novograd'", "or", "opt_lower", "==", "'nvnovograd'", ":", "\n", "        ", "optimizer", "=", "NvNovoGrad", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'rmsprop'", ":", "\n", "        ", "optimizer", "=", "optim", ".", "RMSprop", "(", "parameters", ",", "alpha", "=", "0.9", ",", "momentum", "=", "momentum", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'rmsproptf'", ":", "\n", "        ", "optimizer", "=", "RMSpropTF", "(", "parameters", ",", "alpha", "=", "0.9", ",", "momentum", "=", "momentum", ",", "**", "opt_args", ")", "\n", "\n", "# second order", "\n", "", "elif", "opt_lower", "==", "'adahessian'", ":", "\n", "        ", "optimizer", "=", "Adahessian", "(", "parameters", ",", "**", "opt_args", ")", "\n", "\n", "# NVIDIA fused optimizers, require APEX to be installed", "\n", "", "elif", "opt_lower", "==", "'fusedsgd'", ":", "\n", "        ", "opt_args", ".", "pop", "(", "'eps'", ",", "None", ")", "\n", "optimizer", "=", "FusedSGD", "(", "parameters", ",", "momentum", "=", "momentum", ",", "nesterov", "=", "True", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'fusedmomentum'", ":", "\n", "        ", "opt_args", ".", "pop", "(", "'eps'", ",", "None", ")", "\n", "optimizer", "=", "FusedSGD", "(", "parameters", ",", "momentum", "=", "momentum", ",", "nesterov", "=", "False", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'fusedadam'", ":", "\n", "        ", "optimizer", "=", "FusedAdam", "(", "parameters", ",", "adam_w_mode", "=", "False", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'fusedadamw'", ":", "\n", "        ", "optimizer", "=", "FusedAdam", "(", "parameters", ",", "adam_w_mode", "=", "True", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'fusedlamb'", ":", "\n", "        ", "optimizer", "=", "FusedLAMB", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'fusednovograd'", ":", "\n", "        ", "opt_args", ".", "setdefault", "(", "'betas'", ",", "(", "0.95", ",", "0.98", ")", ")", "\n", "optimizer", "=", "FusedNovoGrad", "(", "parameters", ",", "**", "opt_args", ")", "\n", "\n", "", "else", ":", "\n", "        ", "assert", "False", "and", "\"Invalid optimizer\"", "\n", "raise", "ValueError", "\n", "\n", "", "if", "len", "(", "opt_split", ")", ">", "1", ":", "\n", "        ", "if", "opt_split", "[", "0", "]", "==", "'lookahead'", ":", "\n", "            ", "optimizer", "=", "Lookahead", "(", "optimizer", ")", "\n", "\n", "", "", "return", "optimizer", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lamb.Lamb.__init__": [[87, 95], ["dict", "torch.optim.Optimizer.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "bias_correction", "=", "True", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-6", ",", "\n", "weight_decay", "=", "0.01", ",", "grad_averaging", "=", "True", ",", "max_grad_norm", "=", "1.0", ",", "trust_clip", "=", "False", ",", "always_adapt", "=", "False", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "bias_correction", "=", "bias_correction", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ",", "\n", "grad_averaging", "=", "grad_averaging", ",", "max_grad_norm", "=", "max_grad_norm", ",", "\n", "trust_clip", "=", "trust_clip", ",", "always_adapt", "=", "always_adapt", ")", "\n", "super", "(", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lamb.Lamb.step": [[96, 193], ["torch.no_grad", "torch.tensor", "torch.zeros", "torch.sqrt", "torch.tensor", "torch.where", "torch.enable_grad", "closure", "torch.sqrt.add_", "p.grad.div_", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "p.add_", "RuntimeError", "p.grad.div_.pow().sum", "len", "torch.zeros_like", "torch.zeros_like", "update.add_", "p.norm", "update.norm", "torch.where", "update.mul_", "exp_avg.mul_", "exp_avg_sq.mul_", "torch.where", "torch.minimum", "p.grad.div_.pow", "exp_avg_sq.sqrt", "math.sqrt"], "methods", ["None"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "device", "=", "self", ".", "param_groups", "[", "0", "]", "[", "'params'", "]", "[", "0", "]", ".", "device", "\n", "one_tensor", "=", "torch", ".", "tensor", "(", "1.0", ",", "device", "=", "device", ")", "# because torch.where doesn't handle scalars correctly", "\n", "global_grad_norm", "=", "torch", ".", "zeros", "(", "1", ",", "device", "=", "device", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Lamb does not support sparse gradients, consider SparseAdam instad.'", ")", "\n", "", "global_grad_norm", ".", "add_", "(", "grad", ".", "pow", "(", "2", ")", ".", "sum", "(", ")", ")", "\n", "\n", "", "", "global_grad_norm", "=", "torch", ".", "sqrt", "(", "global_grad_norm", ")", "\n", "# FIXME it'd be nice to remove explicit tensor conversion of scalars when torch.where promotes", "\n", "# scalar types properly https://github.com/pytorch/pytorch/issues/9190", "\n", "max_grad_norm", "=", "torch", ".", "tensor", "(", "self", ".", "defaults", "[", "'max_grad_norm'", "]", ",", "device", "=", "device", ")", "\n", "clip_global_grad_norm", "=", "torch", ".", "where", "(", "\n", "global_grad_norm", ">", "max_grad_norm", ",", "\n", "global_grad_norm", "/", "max_grad_norm", ",", "\n", "one_tensor", ")", "\n", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "bias_correction", "=", "1", "if", "group", "[", "'bias_correction'", "]", "else", "0", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "grad_averaging", "=", "1", "if", "group", "[", "'grad_averaging'", "]", "else", "0", "\n", "beta3", "=", "1", "-", "beta1", "if", "grad_averaging", "else", "1.0", "\n", "\n", "# assume same step across group now to simplify things", "\n", "# per parameter step can be easily support by making it tensor, or pass list into kernel", "\n", "if", "'step'", "in", "group", ":", "\n", "                ", "group", "[", "'step'", "]", "+=", "1", "\n", "", "else", ":", "\n", "                ", "group", "[", "'step'", "]", "=", "1", "\n", "\n", "", "if", "bias_correction", ":", "\n", "                ", "bias_correction1", "=", "1", "-", "beta1", "**", "group", "[", "'step'", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "group", "[", "'step'", "]", "\n", "", "else", ":", "\n", "                ", "bias_correction1", ",", "bias_correction2", "=", "1.0", ",", "1.0", "\n", "\n", "", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "div_", "(", "clip_global_grad_norm", ")", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "# Exponential moving average of gradient valuesa", "\n", "                    ", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "beta3", ")", "# m_t", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "1", "-", "beta2", ")", "# v_t", "\n", "\n", "denom", "=", "(", "exp_avg_sq", ".", "sqrt", "(", ")", "/", "math", ".", "sqrt", "(", "bias_correction2", ")", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "update", "=", "(", "exp_avg", "/", "bias_correction1", ")", ".", "div_", "(", "denom", ")", "\n", "\n", "weight_decay", "=", "group", "[", "'weight_decay'", "]", "\n", "if", "weight_decay", "!=", "0", ":", "\n", "                    ", "update", ".", "add_", "(", "p", ",", "alpha", "=", "weight_decay", ")", "\n", "\n", "", "if", "weight_decay", "!=", "0", "or", "group", "[", "'always_adapt'", "]", ":", "\n", "# Layer-wise LR adaptation. By default, skip adaptation on parameters that are", "\n", "# excluded from weight decay, unless always_adapt == True, then always enabled.", "\n", "                    ", "w_norm", "=", "p", ".", "norm", "(", "2.0", ")", "\n", "g_norm", "=", "update", ".", "norm", "(", "2.0", ")", "\n", "# FIXME nested where required since logical and/or not working in PT XLA", "\n", "trust_ratio", "=", "torch", ".", "where", "(", "\n", "w_norm", ">", "0", ",", "\n", "torch", ".", "where", "(", "g_norm", ">", "0", ",", "w_norm", "/", "g_norm", ",", "one_tensor", ")", ",", "\n", "one_tensor", ",", "\n", ")", "\n", "if", "group", "[", "'trust_clip'", "]", ":", "\n", "# LAMBC trust clipping, upper bound fixed at one", "\n", "                        ", "trust_ratio", "=", "torch", ".", "minimum", "(", "trust_ratio", ",", "one_tensor", ")", "\n", "", "update", ".", "mul_", "(", "trust_ratio", ")", "\n", "\n", "", "p", ".", "add_", "(", "update", ",", "alpha", "=", "-", "group", "[", "'lr'", "]", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.radam.RAdam.__init__": [[12, 17], ["dict", "torch.optim.optimizer.Optimizer.__init__", "range"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "weight_decay", "=", "0", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ",", "\n", "buffer", "=", "[", "[", "None", ",", "None", ",", "None", "]", "for", "_", "in", "range", "(", "10", ")", "]", ")", "\n", "super", "(", "RAdam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.radam.RAdam.__setstate__": [[18, 20], ["super().__setstate__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.ml_decoder.TransformerDecoderLayerOptimal.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "RAdam", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.radam.RAdam.step": [[21, 90], ["torch.no_grad", "torch.enable_grad", "closure", "p.grad.float", "p.float", "exp_avg_sq.mul_().addcmul_", "exp_avg.mul_().add_", "p.copy_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "state[].type_as", "state[].type_as", "p.float.add_", "exp_avg_sq.sqrt().add_", "p.float.addcdiv_", "p.float.add_", "exp_avg_sq.mul_", "exp_avg.mul_", "int", "exp_avg_sq.sqrt", "math.sqrt"], "methods", ["None"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "float", "(", ")", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'RAdam does not support sparse gradients'", ")", "\n", "\n", "", "p_fp32", "=", "p", ".", "float", "(", ")", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p_fp32", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p_fp32", ")", "\n", "", "else", ":", "\n", "                    ", "state", "[", "'exp_avg'", "]", "=", "state", "[", "'exp_avg'", "]", ".", "type_as", "(", "p_fp32", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "state", "[", "'exp_avg_sq'", "]", ".", "type_as", "(", "p_fp32", ")", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "1", "-", "beta2", ")", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1", "-", "beta1", ")", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "buffered", "=", "group", "[", "'buffer'", "]", "[", "int", "(", "state", "[", "'step'", "]", "%", "10", ")", "]", "\n", "if", "state", "[", "'step'", "]", "==", "buffered", "[", "0", "]", ":", "\n", "                    ", "num_sma", ",", "step_size", "=", "buffered", "[", "1", "]", ",", "buffered", "[", "2", "]", "\n", "", "else", ":", "\n", "                    ", "buffered", "[", "0", "]", "=", "state", "[", "'step'", "]", "\n", "beta2_t", "=", "beta2", "**", "state", "[", "'step'", "]", "\n", "num_sma_max", "=", "2", "/", "(", "1", "-", "beta2", ")", "-", "1", "\n", "num_sma", "=", "num_sma_max", "-", "2", "*", "state", "[", "'step'", "]", "*", "beta2_t", "/", "(", "1", "-", "beta2_t", ")", "\n", "buffered", "[", "1", "]", "=", "num_sma", "\n", "\n", "# more conservative since it's an approximated value", "\n", "if", "num_sma", ">=", "5", ":", "\n", "                        ", "step_size", "=", "group", "[", "'lr'", "]", "*", "math", ".", "sqrt", "(", "\n", "(", "1", "-", "beta2_t", ")", "*", "\n", "(", "num_sma", "-", "4", ")", "/", "(", "num_sma_max", "-", "4", ")", "*", "\n", "(", "num_sma", "-", "2", ")", "/", "num_sma", "*", "\n", "num_sma_max", "/", "(", "num_sma_max", "-", "2", ")", ")", "/", "(", "1", "-", "beta1", "**", "state", "[", "'step'", "]", ")", "\n", "", "else", ":", "\n", "                        ", "step_size", "=", "group", "[", "'lr'", "]", "/", "(", "1", "-", "beta1", "**", "state", "[", "'step'", "]", ")", "\n", "", "buffered", "[", "2", "]", "=", "step_size", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p_fp32", ".", "add_", "(", "p_fp32", ",", "alpha", "=", "-", "group", "[", "'weight_decay'", "]", "*", "group", "[", "'lr'", "]", ")", "\n", "\n", "# more conservative since it's an approximated value", "\n", "", "if", "num_sma", ">=", "5", ":", "\n", "                    ", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "p_fp32", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "step_size", ")", "\n", "", "else", ":", "\n", "                    ", "p_fp32", ".", "add_", "(", "exp_avg", ",", "alpha", "=", "-", "step_size", ")", "\n", "\n", "", "p", ".", "copy_", "(", "p_fp32", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.__init__": [[13, 29], ["dict", "lookahead.Lookahead.defaults.update", "collections.defaultdict", "dict.items", "ValueError", "ValueError", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update"], ["    ", "def", "__init__", "(", "self", ",", "base_optimizer", ",", "alpha", "=", "0.5", ",", "k", "=", "6", ")", ":", "\n", "# NOTE super().__init__() not called on purpose", "\n", "        ", "if", "not", "0.0", "<=", "alpha", "<=", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "f'Invalid slow update rate: {alpha}'", ")", "\n", "", "if", "not", "1", "<=", "k", ":", "\n", "            ", "raise", "ValueError", "(", "f'Invalid lookahead steps: {k}'", ")", "\n", "", "defaults", "=", "dict", "(", "lookahead_alpha", "=", "alpha", ",", "lookahead_k", "=", "k", ",", "lookahead_step", "=", "0", ")", "\n", "self", ".", "_base_optimizer", "=", "base_optimizer", "\n", "self", ".", "param_groups", "=", "base_optimizer", ".", "param_groups", "\n", "self", ".", "defaults", "=", "base_optimizer", ".", "defaults", "\n", "self", ".", "defaults", ".", "update", "(", "defaults", ")", "\n", "self", ".", "state", "=", "defaultdict", "(", "dict", ")", "\n", "# manually add our defaults to the param groups", "\n", "for", "name", ",", "default", "in", "defaults", ".", "items", "(", ")", ":", "\n", "            ", "for", "group", "in", "self", ".", "_base_optimizer", ".", "param_groups", ":", "\n", "                ", "group", ".", "setdefault", "(", "name", ",", "default", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.update_slow": [[30, 42], ["torch.no_grad", "slow.add_", "fast_p.copy_", "torch.empty_like", "param_state[].copy_"], "methods", ["None"], ["", "", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "update_slow", "(", "self", ",", "group", ")", ":", "\n", "        ", "for", "fast_p", "in", "group", "[", "\"params\"", "]", ":", "\n", "            ", "if", "fast_p", ".", "grad", "is", "None", ":", "\n", "                ", "continue", "\n", "", "param_state", "=", "self", ".", "_base_optimizer", ".", "state", "[", "fast_p", "]", "\n", "if", "'lookahead_slow_buff'", "not", "in", "param_state", ":", "\n", "                ", "param_state", "[", "'lookahead_slow_buff'", "]", "=", "torch", ".", "empty_like", "(", "fast_p", ")", "\n", "param_state", "[", "'lookahead_slow_buff'", "]", ".", "copy_", "(", "fast_p", ")", "\n", "", "slow", "=", "param_state", "[", "'lookahead_slow_buff'", "]", "\n", "slow", ".", "add_", "(", "fast_p", "-", "slow", ",", "alpha", "=", "group", "[", "'lookahead_alpha'", "]", ")", "\n", "fast_p", ".", "copy_", "(", "slow", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.sync_lookahead": [[43, 46], ["lookahead.Lookahead.update_slow"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.update_slow"], ["", "", "def", "sync_lookahead", "(", "self", ")", ":", "\n", "        ", "for", "group", "in", "self", ".", "_base_optimizer", ".", "param_groups", ":", "\n", "            ", "self", ".", "update_slow", "(", "group", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.step": [[47, 55], ["torch.no_grad", "lookahead.Lookahead._base_optimizer.step", "lookahead.Lookahead.update_slow"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.update_slow"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "loss", "=", "self", ".", "_base_optimizer", ".", "step", "(", "closure", ")", "\n", "for", "group", "in", "self", ".", "_base_optimizer", ".", "param_groups", ":", "\n", "            ", "group", "[", "'lookahead_step'", "]", "+=", "1", "\n", "if", "group", "[", "'lookahead_step'", "]", "%", "group", "[", "'lookahead_k'", "]", "==", "0", ":", "\n", "                ", "self", ".", "update_slow", "(", "group", ")", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict": [[56, 58], ["lookahead.Lookahead._base_optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_base_optimizer", ".", "state_dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.load_state_dict": [[59, 62], ["lookahead.Lookahead._base_optimizer.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "self", ".", "_base_optimizer", ".", "load_state_dict", "(", "state_dict", ")", "\n", "self", ".", "param_groups", "=", "self", ".", "_base_optimizer", ".", "param_groups", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.rmsprop_tf.RMSpropTF.__init__": [[48, 65], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-2", ",", "alpha", "=", "0.9", ",", "eps", "=", "1e-10", ",", "weight_decay", "=", "0", ",", "momentum", "=", "0.", ",", "centered", "=", "False", ",", "\n", "decoupled_decay", "=", "False", ",", "lr_in_momentum", "=", "True", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "momentum", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid momentum value: {}\"", ".", "format", "(", "momentum", ")", ")", "\n", "", "if", "not", "0.0", "<=", "weight_decay", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid weight_decay value: {}\"", ".", "format", "(", "weight_decay", ")", ")", "\n", "", "if", "not", "0.0", "<=", "alpha", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid alpha value: {}\"", ".", "format", "(", "alpha", ")", ")", "\n", "\n", "", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "momentum", "=", "momentum", ",", "alpha", "=", "alpha", ",", "eps", "=", "eps", ",", "centered", "=", "centered", ",", "weight_decay", "=", "weight_decay", ",", "\n", "decoupled_decay", "=", "decoupled_decay", ",", "lr_in_momentum", "=", "lr_in_momentum", ")", "\n", "super", "(", "RMSpropTF", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.rmsprop_tf.RMSpropTF.__setstate__": [[66, 71], ["super().__setstate__", "group.setdefault", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.ml_decoder.TransformerDecoderLayerOptimal.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "RMSpropTF", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'momentum'", ",", "0", ")", "\n", "group", ".", "setdefault", "(", "'centered'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.rmsprop_tf.RMSpropTF.step": [[72, 140], ["torch.no_grad", "torch.enable_grad", "closure", "square_avg.add_", "RuntimeError", "len", "torch.ones_like", "grad_avg.add_", "square_avg.addcmul().add().sqrt_", "square_avg.add().sqrt_", "p.addcdiv_", "torch.zeros_like", "torch.zeros_like", "p.mul_", "grad.add.add.add", "grad.add.add.pow", "buf.mul_().addcdiv_", "p.add_", "buf.mul_().addcdiv_", "p.add_", "square_avg.addcmul().add", "square_avg.add", "buf.mul_", "buf.mul_", "square_avg.addcmul"], "methods", ["None"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'RMSprop does not support sparse gradients'", ")", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "state", "[", "'square_avg'", "]", "=", "torch", ".", "ones_like", "(", "p", ")", "# PyTorch inits to zero", "\n", "if", "group", "[", "'momentum'", "]", ">", "0", ":", "\n", "                        ", "state", "[", "'momentum_buffer'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "", "if", "group", "[", "'centered'", "]", ":", "\n", "                        ", "state", "[", "'grad_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "\n", "", "", "square_avg", "=", "state", "[", "'square_avg'", "]", "\n", "one_minus_alpha", "=", "1.", "-", "group", "[", "'alpha'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "if", "group", "[", "'decoupled_decay'", "]", ":", "\n", "                        ", "p", ".", "mul_", "(", "1.", "-", "group", "[", "'lr'", "]", "*", "group", "[", "'weight_decay'", "]", ")", "\n", "", "else", ":", "\n", "                        ", "grad", "=", "grad", ".", "add", "(", "p", ",", "alpha", "=", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "# Tensorflow order of ops for updating squared avg", "\n", "", "", "square_avg", ".", "add_", "(", "grad", ".", "pow", "(", "2", ")", "-", "square_avg", ",", "alpha", "=", "one_minus_alpha", ")", "\n", "# square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)  # PyTorch original", "\n", "\n", "if", "group", "[", "'centered'", "]", ":", "\n", "                    ", "grad_avg", "=", "state", "[", "'grad_avg'", "]", "\n", "grad_avg", ".", "add_", "(", "grad", "-", "grad_avg", ",", "alpha", "=", "one_minus_alpha", ")", "\n", "avg", "=", "square_avg", ".", "addcmul", "(", "grad_avg", ",", "grad_avg", ",", "value", "=", "-", "1", ")", ".", "add", "(", "group", "[", "'eps'", "]", ")", ".", "sqrt_", "(", ")", "# eps in sqrt", "\n", "# grad_avg.mul_(alpha).add_(grad, alpha=1 - alpha)  # PyTorch original", "\n", "", "else", ":", "\n", "                    ", "avg", "=", "square_avg", ".", "add", "(", "group", "[", "'eps'", "]", ")", ".", "sqrt_", "(", ")", "# eps moved in sqrt", "\n", "\n", "", "if", "group", "[", "'momentum'", "]", ">", "0", ":", "\n", "                    ", "buf", "=", "state", "[", "'momentum_buffer'", "]", "\n", "# Tensorflow accumulates the LR scaling in the momentum buffer", "\n", "if", "group", "[", "'lr_in_momentum'", "]", ":", "\n", "                        ", "buf", ".", "mul_", "(", "group", "[", "'momentum'", "]", ")", ".", "addcdiv_", "(", "grad", ",", "avg", ",", "value", "=", "group", "[", "'lr'", "]", ")", "\n", "p", ".", "add_", "(", "-", "buf", ")", "\n", "", "else", ":", "\n", "# PyTorch scales the param update by LR", "\n", "                        ", "buf", ".", "mul_", "(", "group", "[", "'momentum'", "]", ")", ".", "addcdiv_", "(", "grad", ",", "avg", ")", "\n", "p", ".", "add_", "(", "buf", ",", "alpha", "=", "-", "group", "[", "'lr'", "]", ")", "\n", "", "", "else", ":", "\n", "                    ", "p", ".", "addcdiv_", "(", "grad", ",", "avg", ",", "value", "=", "-", "group", "[", "'lr'", "]", ")", "\n", "\n", "", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adamp.AdamP.__init__": [[44, 50], ["dict", "torch.optim.optimizer.Optimizer.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "delta", "=", "0.1", ",", "wd_ratio", "=", "0.1", ",", "nesterov", "=", "False", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ",", "\n", "delta", "=", "delta", ",", "wd_ratio", "=", "wd_ratio", ",", "nesterov", "=", "nesterov", ")", "\n", "super", "(", "AdamP", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adamp.AdamP.step": [[51, 106], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "closure", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "p.add_", "len", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "len", "adamp.projection", "p.mul_", "exp_avg.mul_", "exp_avg_sq.mul_", "exp_avg_sq.sqrt", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adamp.projection"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "grad", "=", "p", ".", "grad", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "nesterov", "=", "group", "[", "'nesterov'", "]", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "\n", "# Adam", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "bias_correction1", "=", "1", "-", "beta1", "**", "state", "[", "'step'", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "state", "[", "'step'", "]", "\n", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1", "-", "beta1", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "1", "-", "beta2", ")", "\n", "\n", "denom", "=", "(", "exp_avg_sq", ".", "sqrt", "(", ")", "/", "math", ".", "sqrt", "(", "bias_correction2", ")", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "step_size", "=", "group", "[", "'lr'", "]", "/", "bias_correction1", "\n", "\n", "if", "nesterov", ":", "\n", "                    ", "perturb", "=", "(", "beta1", "*", "exp_avg", "+", "(", "1", "-", "beta1", ")", "*", "grad", ")", "/", "denom", "\n", "", "else", ":", "\n", "                    ", "perturb", "=", "exp_avg", "/", "denom", "\n", "\n", "# Projection", "\n", "", "wd_ratio", "=", "1.", "\n", "if", "len", "(", "p", ".", "shape", ")", ">", "1", ":", "\n", "                    ", "perturb", ",", "wd_ratio", "=", "projection", "(", "p", ",", "grad", ",", "perturb", ",", "group", "[", "'delta'", "]", ",", "group", "[", "'wd_ratio'", "]", ",", "group", "[", "'eps'", "]", ")", "\n", "\n", "# Weight decay", "\n", "", "if", "group", "[", "'weight_decay'", "]", ">", "0", ":", "\n", "                    ", "p", ".", "mul_", "(", "1.", "-", "group", "[", "'lr'", "]", "*", "group", "[", "'weight_decay'", "]", "*", "wd_ratio", ")", "\n", "\n", "# Step", "\n", "", "p", ".", "add_", "(", "perturb", ",", "alpha", "=", "-", "step_size", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adamp._channel_view": [[17, 19], ["x.reshape", "x.size"], "function", ["None"], ["def", "_channel_view", "(", "x", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "return", "x", ".", "reshape", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adamp._layer_view": [[21, 23], ["x.reshape"], "function", ["None"], ["", "def", "_layer_view", "(", "x", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "return", "x", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adamp.projection": [[25, 41], ["view_func", "view_func", "torch.cosine_similarity().abs_", "F.cosine_similarity().abs_.max", "len", "torch.cosine_similarity", "math.sqrt", "view_func.norm().add_().reshape", "view_func().sum().reshape", "view_func.size", "view_func.norm().add_", "view_func().sum", "view_func.norm", "view_func"], "function", ["None"], ["", "def", "projection", "(", "p", ",", "grad", ",", "perturb", ",", "delta", ":", "float", ",", "wd_ratio", ":", "float", ",", "eps", ":", "float", ")", ":", "\n", "    ", "wd", "=", "1.", "\n", "expand_size", "=", "(", "-", "1", ",", ")", "+", "(", "1", ",", ")", "*", "(", "len", "(", "p", ".", "shape", ")", "-", "1", ")", "\n", "for", "view_func", "in", "[", "_channel_view", ",", "_layer_view", "]", ":", "\n", "        ", "param_view", "=", "view_func", "(", "p", ")", "\n", "grad_view", "=", "view_func", "(", "grad", ")", "\n", "cosine_sim", "=", "F", ".", "cosine_similarity", "(", "grad_view", ",", "param_view", ",", "dim", "=", "1", ",", "eps", "=", "eps", ")", ".", "abs_", "(", ")", "\n", "\n", "# FIXME this is a problem for PyTorch XLA", "\n", "if", "cosine_sim", ".", "max", "(", ")", "<", "delta", "/", "math", ".", "sqrt", "(", "param_view", ".", "size", "(", "1", ")", ")", ":", "\n", "            ", "p_n", "=", "p", "/", "param_view", ".", "norm", "(", "p", "=", "2", ",", "dim", "=", "1", ")", ".", "add_", "(", "eps", ")", ".", "reshape", "(", "expand_size", ")", "\n", "perturb", "-=", "p_n", "*", "view_func", "(", "p_n", "*", "perturb", ")", ".", "sum", "(", "dim", "=", "1", ")", ".", "reshape", "(", "expand_size", ")", "\n", "wd", "=", "wd_ratio", "\n", "return", "perturb", ",", "wd", "\n", "\n", "", "", "return", "perturb", ",", "wd", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor.__init__": [[41, 52], ["dict", "super().__init__", "ValueError"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "None", ",", "eps", "=", "1e-30", ",", "eps_scale", "=", "1e-3", ",", "clip_threshold", "=", "1.0", ",", "\n", "decay_rate", "=", "-", "0.8", ",", "betas", "=", "None", ",", "weight_decay", "=", "0.0", ",", "scale_parameter", "=", "True", ",", "warmup_init", "=", "False", ")", ":", "\n", "        ", "relative_step", "=", "not", "lr", "\n", "if", "warmup_init", "and", "not", "relative_step", ":", "\n", "            ", "raise", "ValueError", "(", "'warmup_init requires relative_step=True'", ")", "\n", "\n", "", "beta1", "=", "None", "if", "betas", "is", "None", "else", "betas", "[", "0", "]", "# make it compat with standard betas arg", "\n", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "eps", "=", "eps", ",", "eps_scale", "=", "eps_scale", ",", "clip_threshold", "=", "clip_threshold", ",", "decay_rate", "=", "decay_rate", ",", "\n", "beta1", "=", "beta1", ",", "weight_decay", "=", "weight_decay", ",", "scale_parameter", "=", "scale_parameter", ",", "\n", "relative_step", "=", "relative_step", ",", "warmup_init", "=", "warmup_init", ")", "\n", "super", "(", "Adafactor", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor._get_lr": [[53, 63], ["min", "max", "math.sqrt"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_lr", "(", "param_group", ",", "param_state", ")", ":", "\n", "        ", "if", "param_group", "[", "'relative_step'", "]", ":", "\n", "            ", "min_step", "=", "1e-6", "*", "param_state", "[", "'step'", "]", "if", "param_group", "[", "'warmup_init'", "]", "else", "1e-2", "\n", "lr_t", "=", "min", "(", "min_step", ",", "1.0", "/", "math", ".", "sqrt", "(", "param_state", "[", "'step'", "]", ")", ")", "\n", "param_scale", "=", "1.0", "\n", "if", "param_group", "[", "'scale_parameter'", "]", ":", "\n", "                ", "param_scale", "=", "max", "(", "param_group", "[", "'eps_scale'", "]", ",", "param_state", "[", "'RMS'", "]", ")", "\n", "", "param_group", "[", "'lr'", "]", "=", "lr_t", "*", "param_scale", "\n", "", "return", "param_group", "[", "'lr'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor._get_options": [[64, 69], ["len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_options", "(", "param_group", ",", "param_shape", ")", ":", "\n", "        ", "factored", "=", "len", "(", "param_shape", ")", ">=", "2", "\n", "use_first_moment", "=", "param_group", "[", "'beta1'", "]", "is", "not", "None", "\n", "return", "factored", ",", "use_first_moment", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor._rms": [[70, 73], ["tensor.norm", "tensor.numel"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_rms", "(", "tensor", ")", ":", "\n", "        ", "return", "tensor", ".", "norm", "(", "2", ")", "/", "(", "tensor", ".", "numel", "(", ")", "**", "0.5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor._approx_sq_grad": [[74, 78], ["exp_avg_sq_col.unsqueeze().rsqrt", "torch.mul", "exp_avg_sq_col.unsqueeze", "exp_avg_sq_row.mean"], "methods", ["None"], ["", "def", "_approx_sq_grad", "(", "self", ",", "exp_avg_sq_row", ",", "exp_avg_sq_col", ")", ":", "\n", "        ", "r_factor", "=", "(", "exp_avg_sq_row", "/", "exp_avg_sq_row", ".", "mean", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", ".", "rsqrt_", "(", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "c_factor", "=", "exp_avg_sq_col", ".", "unsqueeze", "(", "-", "2", ")", ".", "rsqrt", "(", ")", "\n", "return", "torch", ".", "mul", "(", "r_factor", ",", "c_factor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor.step": [[79, 168], ["torch.no_grad", "torch.enable_grad", "closure", "adafactor.Adafactor._get_options", "adafactor.Adafactor._rms", "adafactor.Adafactor._get_lr", "exp_avg_sq.rsqrt().mul_.div_", "exp_avg_sq.rsqrt().mul_.mul_", "p_fp32.float.float.add_", "grad.float.float.float", "RuntimeError", "len", "p_fp32.float.float.float", "math.pow", "exp_avg_sq_row.mul_().add_", "exp_avg_sq_col.mul_().add_", "adafactor.Adafactor._approx_sq_grad", "exp_avg_sq.rsqrt().mul_.mul_", "exp_avg_sq.mul_().add_", "exp_avg_sq.rsqrt().mul_", "exp_avg.mul_().add_", "p_fp32.float.float.add_", "p.copy_", "torch.zeros_like", "torch.zeros().to", "torch.zeros().to", "torch.zeros_like", "state[].to", "state[].to", "state[].to", "state[].to", "exp_avg_sq.rsqrt().mul_.mean", "exp_avg_sq.rsqrt().mul_.mean", "exp_avg_sq_row.mul_", "exp_avg_sq_col.mul_", "exp_avg_sq.mul_", "exp_avg_sq.rsqrt", "exp_avg.mul_", "torch.zeros", "torch.zeros", "adafactor.Adafactor._rms"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor._get_options", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor._rms", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor._get_lr", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor._approx_sq_grad", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.adafactor.Adafactor._rms"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", "\n", "if", "grad", ".", "dtype", "in", "{", "torch", ".", "float16", ",", "torch", ".", "bfloat16", "}", ":", "\n", "                    ", "grad", "=", "grad", ".", "float", "(", ")", "\n", "", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adafactor does not support sparse gradients.'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "factored", ",", "use_first_moment", "=", "self", ".", "_get_options", "(", "group", ",", "grad", ".", "shape", ")", "\n", "# State Initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "\n", "if", "use_first_moment", ":", "\n", "# Exponential moving average of gradient values", "\n", "                        ", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "grad", ")", "\n", "", "if", "factored", ":", "\n", "                        ", "state", "[", "'exp_avg_sq_row'", "]", "=", "torch", ".", "zeros", "(", "grad", ".", "shape", "[", ":", "-", "1", "]", ")", ".", "to", "(", "grad", ")", "\n", "state", "[", "'exp_avg_sq_col'", "]", "=", "torch", ".", "zeros", "(", "grad", ".", "shape", "[", ":", "-", "2", "]", "+", "grad", ".", "shape", "[", "-", "1", ":", "]", ")", ".", "to", "(", "grad", ")", "\n", "", "else", ":", "\n", "                        ", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "grad", ")", "\n", "\n", "", "state", "[", "'RMS'", "]", "=", "0", "\n", "", "else", ":", "\n", "                    ", "if", "use_first_moment", ":", "\n", "                        ", "state", "[", "'exp_avg'", "]", "=", "state", "[", "'exp_avg'", "]", ".", "to", "(", "grad", ")", "\n", "", "if", "factored", ":", "\n", "                        ", "state", "[", "'exp_avg_sq_row'", "]", "=", "state", "[", "'exp_avg_sq_row'", "]", ".", "to", "(", "grad", ")", "\n", "state", "[", "'exp_avg_sq_col'", "]", "=", "state", "[", "'exp_avg_sq_col'", "]", ".", "to", "(", "grad", ")", "\n", "", "else", ":", "\n", "                        ", "state", "[", "'exp_avg_sq'", "]", "=", "state", "[", "'exp_avg_sq'", "]", ".", "to", "(", "grad", ")", "\n", "\n", "", "", "p_fp32", "=", "p", "\n", "if", "p", ".", "dtype", "in", "{", "torch", ".", "float16", ",", "torch", ".", "bfloat16", "}", ":", "\n", "                    ", "p_fp32", "=", "p_fp32", ".", "float", "(", ")", "\n", "\n", "", "state", "[", "'step'", "]", "+=", "1", "\n", "state", "[", "'RMS'", "]", "=", "self", ".", "_rms", "(", "p_fp32", ")", "\n", "lr_t", "=", "self", ".", "_get_lr", "(", "group", ",", "state", ")", "\n", "\n", "beta2t", "=", "1.0", "-", "math", ".", "pow", "(", "state", "[", "'step'", "]", ",", "group", "[", "'decay_rate'", "]", ")", "\n", "update", "=", "grad", "**", "2", "+", "group", "[", "'eps'", "]", "\n", "if", "factored", ":", "\n", "                    ", "exp_avg_sq_row", "=", "state", "[", "'exp_avg_sq_row'", "]", "\n", "exp_avg_sq_col", "=", "state", "[", "'exp_avg_sq_col'", "]", "\n", "\n", "exp_avg_sq_row", ".", "mul_", "(", "beta2t", ")", ".", "add_", "(", "update", ".", "mean", "(", "dim", "=", "-", "1", ")", ",", "alpha", "=", "1.0", "-", "beta2t", ")", "\n", "exp_avg_sq_col", ".", "mul_", "(", "beta2t", ")", ".", "add_", "(", "update", ".", "mean", "(", "dim", "=", "-", "2", ")", ",", "alpha", "=", "1.0", "-", "beta2t", ")", "\n", "\n", "# Approximation of exponential moving average of square of gradient", "\n", "update", "=", "self", ".", "_approx_sq_grad", "(", "exp_avg_sq_row", ",", "exp_avg_sq_col", ")", "\n", "update", ".", "mul_", "(", "grad", ")", "\n", "", "else", ":", "\n", "                    ", "exp_avg_sq", "=", "state", "[", "'exp_avg_sq'", "]", "\n", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2t", ")", ".", "add_", "(", "update", ",", "alpha", "=", "1.0", "-", "beta2t", ")", "\n", "update", "=", "exp_avg_sq", ".", "rsqrt", "(", ")", ".", "mul_", "(", "grad", ")", "\n", "\n", "", "update", ".", "div_", "(", "(", "self", ".", "_rms", "(", "update", ")", "/", "group", "[", "'clip_threshold'", "]", ")", ".", "clamp_", "(", "min", "=", "1.0", ")", ")", "\n", "update", ".", "mul_", "(", "lr_t", ")", "\n", "\n", "if", "use_first_moment", ":", "\n", "                    ", "exp_avg", "=", "state", "[", "'exp_avg'", "]", "\n", "exp_avg", ".", "mul_", "(", "group", "[", "'beta1'", "]", ")", ".", "add_", "(", "update", ",", "alpha", "=", "1", "-", "group", "[", "'beta1'", "]", ")", "\n", "update", "=", "exp_avg", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p_fp32", ".", "add_", "(", "p_fp32", ",", "alpha", "=", "-", "group", "[", "'weight_decay'", "]", "*", "lr_t", ")", "\n", "\n", "", "p_fp32", ".", "add_", "(", "-", "update", ")", "\n", "if", "p", ".", "dtype", "in", "{", "torch", ".", "float16", ",", "torch", ".", "bfloat16", "}", ":", "\n", "                    ", "p", ".", "copy_", "(", "p_fp32", ")", "\n", "\n", "", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.GPSA.__init__": [[64, 82], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "num_heads", "=", "8", ",", "qkv_bias", "=", "False", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ",", "locality_strength", "=", "1.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "dim", "=", "dim", "\n", "head_dim", "=", "dim", "//", "num_heads", "\n", "self", ".", "scale", "=", "head_dim", "**", "-", "0.5", "\n", "self", ".", "locality_strength", "=", "locality_strength", "\n", "\n", "self", ".", "qk", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", "*", "2", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "v", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ",", "bias", "=", "qkv_bias", ")", "\n", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "pos_proj", "=", "nn", ".", "Linear", "(", "3", ",", "num_heads", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "self", ".", "gating_param", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "self", ".", "num_heads", ")", ")", "\n", "self", ".", "rel_indices", ":", "torch", ".", "Tensor", "=", "torch", ".", "zeros", "(", "1", ",", "1", ",", "1", ",", "3", ")", "# silly torchscript hack, won't work with None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.GPSA.forward": [[83, 93], ["convit.GPSA.get_attention", "convit.GPSA.v().reshape().permute", "convit.GPSA.proj", "convit.GPSA.proj_drop", "convit.GPSA.get_rel_indices", "convit.GPSA.v().reshape", "convit.GPSA.v"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.GPSA.get_attention", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.GPSA.get_rel_indices"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "if", "self", ".", "rel_indices", "is", "None", "or", "self", ".", "rel_indices", ".", "shape", "[", "1", "]", "!=", "N", ":", "\n", "            ", "self", ".", "rel_indices", "=", "self", ".", "get_rel_indices", "(", "N", ")", "\n", "", "attn", "=", "self", ".", "get_attention", "(", "x", ")", "\n", "v", "=", "self", ".", "v", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "N", ",", "C", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.GPSA.get_attention": [[94, 109], ["convit.GPSA.qk().reshape().permute", "convit.GPSA.rel_indices.expand", "convit.GPSA.pos_proj().permute", "patch_score.softmax.softmax.softmax", "pos_score.softmax.softmax.softmax", "convit.GPSA.gating_param.view", "convit.GPSA.sum().unsqueeze", "convit.GPSA.attn_drop", "convit.GPSA.qk().reshape", "convit.GPSA.pos_proj", "k.transpose", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "convit.GPSA.sum", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "convit.GPSA.qk"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid"], ["", "def", "get_attention", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "qk", "=", "self", ".", "qk", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "2", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "q", ",", "k", "=", "qk", "[", "0", "]", ",", "qk", "[", "1", "]", "\n", "pos_score", "=", "self", ".", "rel_indices", ".", "expand", "(", "B", ",", "-", "1", ",", "-", "1", ",", "-", "1", ")", "\n", "pos_score", "=", "self", ".", "pos_proj", "(", "pos_score", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "patch_score", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "scale", "\n", "patch_score", "=", "patch_score", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "pos_score", "=", "pos_score", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "gating", "=", "self", ".", "gating_param", ".", "view", "(", "1", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "attn", "=", "(", "1.", "-", "torch", ".", "sigmoid", "(", "gating", ")", ")", "*", "patch_score", "+", "torch", ".", "sigmoid", "(", "gating", ")", "*", "pos_score", "\n", "attn", "/=", "attn", ".", "sum", "(", "dim", "=", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "return", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.GPSA.get_attention_map": [[110, 118], ["convit.GPSA.get_attention().mean", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "distances.size", "convit.GPSA.get_attention", "convit.GPSA.rel_indices.squeeze"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.GPSA.get_attention"], ["", "def", "get_attention_map", "(", "self", ",", "x", ",", "return_map", "=", "False", ")", ":", "\n", "        ", "attn_map", "=", "self", ".", "get_attention", "(", "x", ")", ".", "mean", "(", "0", ")", "# average over batch", "\n", "distances", "=", "self", ".", "rel_indices", ".", "squeeze", "(", ")", "[", ":", ",", ":", ",", "-", "1", "]", "**", ".5", "\n", "dist", "=", "torch", ".", "einsum", "(", "'nm,hnm->h'", ",", "(", "distances", ",", "attn_map", ")", ")", "/", "distances", ".", "size", "(", "0", ")", "\n", "if", "return_map", ":", "\n", "            ", "return", "dist", ",", "attn_map", "\n", "", "else", ":", "\n", "            ", "return", "dist", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.GPSA.local_init": [[119, 132], ["convit.GPSA.v.weight.data.copy_", "int", "range", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "range"], "methods", ["None"], ["", "", "def", "local_init", "(", "self", ")", ":", "\n", "        ", "self", ".", "v", ".", "weight", ".", "data", ".", "copy_", "(", "torch", ".", "eye", "(", "self", ".", "dim", ")", ")", "\n", "locality_distance", "=", "1", "# max(1,1/locality_strength**.5)", "\n", "\n", "kernel_size", "=", "int", "(", "self", ".", "num_heads", "**", ".5", ")", "\n", "center", "=", "(", "kernel_size", "-", "1", ")", "/", "2", "if", "kernel_size", "%", "2", "==", "0", "else", "kernel_size", "//", "2", "\n", "for", "h1", "in", "range", "(", "kernel_size", ")", ":", "\n", "            ", "for", "h2", "in", "range", "(", "kernel_size", ")", ":", "\n", "                ", "position", "=", "h1", "+", "kernel_size", "*", "h2", "\n", "self", ".", "pos_proj", ".", "weight", ".", "data", "[", "position", ",", "2", "]", "=", "-", "1", "\n", "self", ".", "pos_proj", ".", "weight", ".", "data", "[", "position", ",", "1", "]", "=", "2", "*", "(", "h1", "-", "center", ")", "*", "locality_distance", "\n", "self", ".", "pos_proj", ".", "weight", ".", "data", "[", "position", ",", "0", "]", "=", "2", "*", "(", "h2", "-", "center", ")", "*", "locality_distance", "\n", "", "", "self", ".", "pos_proj", ".", "weight", ".", "data", "*=", "self", ".", "locality_strength", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.GPSA.get_rel_indices": [[133, 145], ["int", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "ind.repeat", "ind.repeat_interleave().repeat_interleave", "indd.unsqueeze", "ind.repeat_interleave().repeat_interleave.unsqueeze", "ind.repeat.unsqueeze", "torch.zeros.to", "torch.zeros.to", "torch.zeros.to", "torch.zeros.to", "torch.zeros.to", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "ind.repeat_interleave", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "def", "get_rel_indices", "(", "self", ",", "num_patches", ":", "int", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "img_size", "=", "int", "(", "num_patches", "**", ".5", ")", "\n", "rel_indices", "=", "torch", ".", "zeros", "(", "1", ",", "num_patches", ",", "num_patches", ",", "3", ")", "\n", "ind", "=", "torch", ".", "arange", "(", "img_size", ")", ".", "view", "(", "1", ",", "-", "1", ")", "-", "torch", ".", "arange", "(", "img_size", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "indx", "=", "ind", ".", "repeat", "(", "img_size", ",", "img_size", ")", "\n", "indy", "=", "ind", ".", "repeat_interleave", "(", "img_size", ",", "dim", "=", "0", ")", ".", "repeat_interleave", "(", "img_size", ",", "dim", "=", "1", ")", "\n", "indd", "=", "indx", "**", "2", "+", "indy", "**", "2", "\n", "rel_indices", "[", ":", ",", ":", ",", ":", ",", "2", "]", "=", "indd", ".", "unsqueeze", "(", "0", ")", "\n", "rel_indices", "[", ":", ",", ":", ",", ":", ",", "1", "]", "=", "indy", ".", "unsqueeze", "(", "0", ")", "\n", "rel_indices", "[", ":", ",", ":", ",", ":", ",", "0", "]", "=", "indx", ".", "unsqueeze", "(", "0", ")", "\n", "device", "=", "self", ".", "qk", ".", "weight", ".", "device", "\n", "return", "rel_indices", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.MHSA.__init__": [[148, 158], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ",", "num_heads", "=", "8", ",", "qkv_bias", "=", "False", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "head_dim", "=", "dim", "//", "num_heads", "\n", "self", ".", "scale", "=", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", "*", "3", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.MHSA.get_attention_map": [[159, 179], ["convit.MHSA.qkv().reshape().permute", "attn_map.softmax().mean.softmax().mean.softmax().mean", "int", "ind.repeat", "ind.repeat_interleave().repeat_interleave", "distances.to.to.to", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "convit.MHSA.qkv().reshape", "k.transpose", "attn_map.softmax().mean.softmax().mean.softmax", "ind.repeat_interleave", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "convit.MHSA.qkv"], "methods", ["None"], ["", "def", "get_attention_map", "(", "self", ",", "x", ",", "return_map", "=", "False", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "qkv", "=", "self", ".", "qkv", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "3", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "q", ",", "k", ",", "v", "=", "qkv", "[", "0", "]", ",", "qkv", "[", "1", "]", ",", "qkv", "[", "2", "]", "\n", "attn_map", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "scale", "\n", "attn_map", "=", "attn_map", ".", "softmax", "(", "dim", "=", "-", "1", ")", ".", "mean", "(", "0", ")", "\n", "\n", "img_size", "=", "int", "(", "N", "**", ".5", ")", "\n", "ind", "=", "torch", ".", "arange", "(", "img_size", ")", ".", "view", "(", "1", ",", "-", "1", ")", "-", "torch", ".", "arange", "(", "img_size", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "indx", "=", "ind", ".", "repeat", "(", "img_size", ",", "img_size", ")", "\n", "indy", "=", "ind", ".", "repeat_interleave", "(", "img_size", ",", "dim", "=", "0", ")", ".", "repeat_interleave", "(", "img_size", ",", "dim", "=", "1", ")", "\n", "indd", "=", "indx", "**", "2", "+", "indy", "**", "2", "\n", "distances", "=", "indd", "**", ".5", "\n", "distances", "=", "distances", ".", "to", "(", "x", ".", "device", ")", "\n", "\n", "dist", "=", "torch", ".", "einsum", "(", "'nm,hnm->h'", ",", "(", "distances", ",", "attn_map", ")", ")", "/", "N", "\n", "if", "return_map", ":", "\n", "            ", "return", "dist", ",", "attn_map", "\n", "", "else", ":", "\n", "            ", "return", "dist", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.MHSA.forward": [[180, 193], ["convit.MHSA.qkv().reshape().permute", "convit.MHSA.unbind", "convit.MHSA.softmax", "convit.MHSA.attn_drop", "convit.MHSA.proj", "convit.MHSA.proj_drop", "convit.MHSA.qkv().reshape", "k.transpose", "convit.MHSA.qkv"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "qkv", "=", "self", ".", "qkv", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "3", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "q", ",", "k", ",", "v", "=", "qkv", ".", "unbind", "(", "0", ")", "\n", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "scale", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "N", ",", "C", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.Block.__init__": [[197, 212], ["torch.Module.__init__", "norm_layer", "norm_layer", "int", "layers.Mlp", "convit.GPSA", "convit.MHSA", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "num_heads", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "\n", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ",", "use_gpsa", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "use_gpsa", "=", "use_gpsa", "\n", "if", "self", ".", "use_gpsa", ":", "\n", "            ", "self", ".", "attn", "=", "GPSA", "(", "\n", "dim", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "attn", "=", "MHSA", "(", "dim", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ")", "\n", "", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "mlp_hidden_dim", "=", "int", "(", "dim", "*", "mlp_ratio", ")", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "mlp_hidden_dim", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.Block.forward": [[213, 217], ["convit.Block.drop_path", "convit.Block.drop_path", "convit.Block.attn", "convit.Block.mlp", "convit.Block.norm1", "convit.Block.norm2"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "attn", "(", "self", ".", "norm1", "(", "x", ")", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.ConViT.__init__": [[223, 278], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "norm_layer", "layers.trunc_normal_", "convit.ConViT.apply", "convit.ConViT.named_modules", "vision_transformer_hybrid.HybridEmbed", "layers.PatchEmbed", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "layers.trunc_normal_", "x.item", "dict", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "hasattr", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "m.local_init", "convit.Block", "convit.Block", "range"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_modules", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.GPSA.local_init"], ["def", "__init__", "(", "\n", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "global_pool", "=", "'token'", ",", "\n", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "drop_rate", "=", "0.", ",", "attn_drop_rate", "=", "0.", ",", "\n", "drop_path_rate", "=", "0.", ",", "hybrid_backbone", "=", "None", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ",", "\n", "local_up_to_layer", "=", "3", ",", "locality_strength", "=", "1.", ",", "use_pos_embed", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "global_pool", "in", "(", "''", ",", "'avg'", ",", "'token'", ")", "\n", "embed_dim", "*=", "num_heads", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "self", ".", "local_up_to_layer", "=", "local_up_to_layer", "\n", "self", ".", "num_features", "=", "self", ".", "embed_dim", "=", "embed_dim", "# num_features for consistency with other models", "\n", "self", ".", "locality_strength", "=", "locality_strength", "\n", "self", ".", "use_pos_embed", "=", "use_pos_embed", "\n", "\n", "if", "hybrid_backbone", "is", "not", "None", ":", "\n", "            ", "self", ".", "patch_embed", "=", "HybridEmbed", "(", "\n", "hybrid_backbone", ",", "img_size", "=", "img_size", ",", "in_chans", "=", "in_chans", ",", "embed_dim", "=", "embed_dim", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "patch_embed", "=", "PatchEmbed", "(", "\n", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", ",", "in_chans", "=", "in_chans", ",", "embed_dim", "=", "embed_dim", ")", "\n", "", "num_patches", "=", "self", ".", "patch_embed", ".", "num_patches", "\n", "self", ".", "num_patches", "=", "num_patches", "\n", "\n", "self", ".", "cls_token", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "self", ".", "pos_drop", "=", "nn", ".", "Dropout", "(", "p", "=", "drop_rate", ")", "\n", "\n", "if", "self", ".", "use_pos_embed", ":", "\n", "            ", "self", ".", "pos_embed", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "num_patches", ",", "embed_dim", ")", ")", "\n", "trunc_normal_", "(", "self", ".", "pos_embed", ",", "std", "=", ".02", ")", "\n", "\n", "", "dpr", "=", "[", "x", ".", "item", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "depth", ")", "]", "# stochastic depth decay rule", "\n", "self", ".", "blocks", "=", "nn", ".", "ModuleList", "(", "[", "\n", "Block", "(", "\n", "dim", "=", "embed_dim", ",", "num_heads", "=", "num_heads", ",", "mlp_ratio", "=", "mlp_ratio", ",", "qkv_bias", "=", "qkv_bias", ",", "\n", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "dpr", "[", "i", "]", ",", "norm_layer", "=", "norm_layer", ",", "\n", "use_gpsa", "=", "True", ",", "\n", "locality_strength", "=", "locality_strength", ")", "\n", "if", "i", "<", "local_up_to_layer", "else", "\n", "Block", "(", "\n", "dim", "=", "embed_dim", ",", "num_heads", "=", "num_heads", ",", "mlp_ratio", "=", "mlp_ratio", ",", "qkv_bias", "=", "qkv_bias", ",", "\n", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "dpr", "[", "i", "]", ",", "norm_layer", "=", "norm_layer", ",", "\n", "use_gpsa", "=", "False", ")", "\n", "for", "i", "in", "range", "(", "depth", ")", "]", ")", "\n", "self", ".", "norm", "=", "norm_layer", "(", "embed_dim", ")", "\n", "\n", "# Classifier head", "\n", "self", ".", "feature_info", "=", "[", "dict", "(", "num_chs", "=", "embed_dim", ",", "reduction", "=", "0", ",", "module", "=", "'head'", ")", "]", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "trunc_normal_", "(", "self", ".", "cls_token", ",", "std", "=", ".02", ")", "\n", "self", ".", "apply", "(", "self", ".", "_init_weights", ")", "\n", "for", "n", ",", "m", "in", "self", ".", "named_modules", "(", ")", ":", "\n", "            ", "if", "hasattr", "(", "m", ",", "'local_init'", ")", ":", "\n", "                ", "m", ".", "local_init", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.ConViT._init_weights": [[279, 287], ["isinstance", "layers.trunc_normal_", "isinstance", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["", "", "", "def", "_init_weights", "(", "self", ",", "m", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "            ", "trunc_normal_", "(", "m", ".", "weight", ",", "std", "=", ".02", ")", "\n", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.ConViT.no_weight_decay": [[288, 291], ["None"], "methods", ["None"], ["", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "no_weight_decay", "(", "self", ")", ":", "\n", "        ", "return", "{", "'pos_embed'", ",", "'cls_token'", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.ConViT.group_matcher": [[292, 297], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^cls_token|pos_embed|patch_embed'", ",", "# stem and embed", "\n", "blocks", "=", "[", "(", "r'^blocks\\.(\\d+)'", ",", "None", ")", ",", "(", "r'^norm'", ",", "(", "99999", ",", ")", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.ConViT.set_grad_checkpointing": [[299, 302], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "'gradient checkpointing not supported'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.ConViT.get_classifier": [[303, 306], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.ConViT.reset_classifier": [[307, 313], ["torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "assert", "global_pool", "in", "(", "''", ",", "'token'", ",", "'avg'", ")", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.ConViT.forward_features": [[314, 326], ["convit.ConViT.patch_embed", "convit.ConViT.pos_drop", "convit.ConViT.cls_token.expand", "enumerate", "convit.ConViT.norm", "blk", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "patch_embed", "(", "x", ")", "\n", "if", "self", ".", "use_pos_embed", ":", "\n", "            ", "x", "=", "x", "+", "self", ".", "pos_embed", "\n", "", "x", "=", "self", ".", "pos_drop", "(", "x", ")", "\n", "cls_tokens", "=", "self", ".", "cls_token", ".", "expand", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ",", "-", "1", ")", "\n", "for", "u", ",", "blk", "in", "enumerate", "(", "self", ".", "blocks", ")", ":", "\n", "            ", "if", "u", "==", "self", ".", "local_up_to_layer", ":", "\n", "                ", "x", "=", "torch", ".", "cat", "(", "(", "cls_tokens", ",", "x", ")", ",", "dim", "=", "1", ")", "\n", "", "x", "=", "blk", "(", "x", ")", "\n", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.ConViT.forward_head": [[327, 331], ["convit.ConViT.head", "x[].mean"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "global_pool", ":", "\n", "            ", "x", "=", "x", "[", ":", ",", "1", ":", "]", ".", "mean", "(", "dim", "=", "1", ")", "if", "self", ".", "global_pool", "==", "'avg'", "else", "x", "[", ":", ",", "0", "]", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.ConViT.forward": [[332, 336], ["convit.ConViT.forward_features", "convit.ConViT.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit._cfg": [[41, 48], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'first_conv'", ":", "'patch_embed.proj'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit._create_convit": [[338, 343], ["kwargs.get", "helpers.build_model_with_cfg", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_convit", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "kwargs", ".", "get", "(", "'features_only'", ",", "None", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'features_only not implemented for Vision Transformer models.'", ")", "\n", "\n", "", "return", "build_model_with_cfg", "(", "ConViT", ",", "variant", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.convit_tiny": [[345, 352], ["dict", "convit._create_convit", "functools.partial"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit._create_convit"], ["", "@", "register_model", "\n", "def", "convit_tiny", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "local_up_to_layer", "=", "10", ",", "locality_strength", "=", "1.0", ",", "embed_dim", "=", "48", ",", "\n", "num_heads", "=", "4", ",", "norm_layer", "=", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convit", "(", "variant", "=", "'convit_tiny'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.convit_small": [[354, 361], ["dict", "convit._create_convit", "functools.partial"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit._create_convit"], ["", "@", "register_model", "\n", "def", "convit_small", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "local_up_to_layer", "=", "10", ",", "locality_strength", "=", "1.0", ",", "embed_dim", "=", "48", ",", "\n", "num_heads", "=", "9", ",", "norm_layer", "=", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convit", "(", "variant", "=", "'convit_small'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit.convit_base": [[363, 370], ["dict", "convit._create_convit", "functools.partial"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convit._create_convit"], ["", "@", "register_model", "\n", "def", "convit_base", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "local_up_to_layer", "=", "10", ",", "locality_strength", "=", "1.0", ",", "embed_dim", "=", "48", ",", "\n", "num_heads", "=", "16", ",", "norm_layer", "=", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convit", "(", "variant", "=", "'convit_base'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.LocallyGroupedAttn.__init__": [[70, 85], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "dim", ",", "num_heads", "=", "8", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ",", "ws", "=", "1", ")", ":", "\n", "        ", "assert", "ws", "!=", "1", "\n", "super", "(", "LocallyGroupedAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "dim", "%", "num_heads", "==", "0", ",", "f\"dim {dim} should be divided by num_heads {num_heads}.\"", "\n", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "head_dim", "=", "dim", "//", "num_heads", "\n", "self", ".", "scale", "=", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", "*", "3", ",", "bias", "=", "True", ")", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "self", ".", "ws", "=", "ws", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.LocallyGroupedAttn.forward": [[86, 114], ["x[].contiguous.view", "torch.pad", "torch.pad", "torch.pad", "x[].contiguous.reshape().transpose", "twins.LocallyGroupedAttn.qkv().reshape().permute", "twins.LocallyGroupedAttn.softmax", "twins.LocallyGroupedAttn.attn_drop", "twins.LocallyGroupedAttn.transpose().reshape", "x[].contiguous.reshape", "twins.LocallyGroupedAttn.proj", "twins.LocallyGroupedAttn.proj_drop", "x[].contiguous", "x[].contiguous.reshape", "twins.LocallyGroupedAttn.qkv().reshape", "k.transpose", "twins.LocallyGroupedAttn.transpose", "twins.LocallyGroupedAttn.qkv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "size", ":", "Size_", ")", ":", "\n", "# There are two implementations for this function, zero padding or mask. We don't observe obvious difference for", "\n", "# both. You can choose any one, we recommend forward_padding because it's neat. However,", "\n", "# the masking implementation is more reasonable and accurate.", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "H", ",", "W", "=", "size", "\n", "x", "=", "x", ".", "view", "(", "B", ",", "H", ",", "W", ",", "C", ")", "\n", "pad_l", "=", "pad_t", "=", "0", "\n", "pad_r", "=", "(", "self", ".", "ws", "-", "W", "%", "self", ".", "ws", ")", "%", "self", ".", "ws", "\n", "pad_b", "=", "(", "self", ".", "ws", "-", "H", "%", "self", ".", "ws", ")", "%", "self", ".", "ws", "\n", "x", "=", "F", ".", "pad", "(", "x", ",", "(", "0", ",", "0", ",", "pad_l", ",", "pad_r", ",", "pad_t", ",", "pad_b", ")", ")", "\n", "_", ",", "Hp", ",", "Wp", ",", "_", "=", "x", ".", "shape", "\n", "_h", ",", "_w", "=", "Hp", "//", "self", ".", "ws", ",", "Wp", "//", "self", ".", "ws", "\n", "x", "=", "x", ".", "reshape", "(", "B", ",", "_h", ",", "self", ".", "ws", ",", "_w", ",", "self", ".", "ws", ",", "C", ")", ".", "transpose", "(", "2", ",", "3", ")", "\n", "qkv", "=", "self", ".", "qkv", "(", "x", ")", ".", "reshape", "(", "\n", "B", ",", "_h", "*", "_w", ",", "self", ".", "ws", "*", "self", ".", "ws", ",", "3", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "3", ",", "0", ",", "1", ",", "4", ",", "2", ",", "5", ")", "\n", "q", ",", "k", ",", "v", "=", "qkv", "[", "0", "]", ",", "qkv", "[", "1", "]", ",", "qkv", "[", "2", "]", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "scale", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "attn", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "2", ",", "3", ")", ".", "reshape", "(", "B", ",", "_h", ",", "_w", ",", "self", ".", "ws", ",", "self", ".", "ws", ",", "C", ")", "\n", "x", "=", "attn", ".", "transpose", "(", "2", ",", "3", ")", ".", "reshape", "(", "B", ",", "_h", "*", "self", ".", "ws", ",", "_w", "*", "self", ".", "ws", ",", "C", ")", "\n", "if", "pad_r", ">", "0", "or", "pad_b", ">", "0", ":", "\n", "            ", "x", "=", "x", "[", ":", ",", ":", "H", ",", ":", "W", ",", ":", "]", ".", "contiguous", "(", ")", "\n", "", "x", "=", "x", ".", "reshape", "(", "B", ",", "N", ",", "C", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.GlobalSubSampleAttn.__init__": [[154, 176], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "dim", ",", "num_heads", "=", "8", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ",", "sr_ratio", "=", "1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "dim", "%", "num_heads", "==", "0", ",", "f\"dim {dim} should be divided by num_heads {num_heads}.\"", "\n", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "head_dim", "=", "dim", "//", "num_heads", "\n", "self", ".", "scale", "=", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "q", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ",", "bias", "=", "True", ")", "\n", "self", ".", "kv", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", "*", "2", ",", "bias", "=", "True", ")", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "\n", "self", ".", "sr_ratio", "=", "sr_ratio", "\n", "if", "sr_ratio", ">", "1", ":", "\n", "            ", "self", ".", "sr", "=", "nn", ".", "Conv2d", "(", "dim", ",", "dim", ",", "kernel_size", "=", "sr_ratio", ",", "stride", "=", "sr_ratio", ")", "\n", "self", ".", "norm", "=", "nn", ".", "LayerNorm", "(", "dim", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "sr", "=", "None", "\n", "self", ".", "norm", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.GlobalSubSampleAttn.forward": [[177, 197], ["twins.GlobalSubSampleAttn.q().reshape().permute", "twins.GlobalSubSampleAttn.kv().reshape().permute", "twins.GlobalSubSampleAttn.softmax", "twins.GlobalSubSampleAttn.attn_drop", "twins.GlobalSubSampleAttn.proj", "twins.GlobalSubSampleAttn.proj_drop", "twins.GlobalSubSampleAttn.permute().reshape", "twins.GlobalSubSampleAttn.sr().reshape().permute", "twins.GlobalSubSampleAttn.norm", "twins.GlobalSubSampleAttn.q().reshape", "twins.GlobalSubSampleAttn.kv().reshape", "k.transpose", "twins.GlobalSubSampleAttn.permute", "twins.GlobalSubSampleAttn.sr().reshape", "twins.GlobalSubSampleAttn.q", "twins.GlobalSubSampleAttn.kv", "twins.GlobalSubSampleAttn.sr"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "size", ":", "Size_", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "q", "=", "self", ".", "q", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n", "if", "self", ".", "sr", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ".", "reshape", "(", "B", ",", "C", ",", "*", "size", ")", "\n", "x", "=", "self", ".", "sr", "(", "x", ")", ".", "reshape", "(", "B", ",", "C", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "", "kv", "=", "self", ".", "kv", "(", "x", ")", ".", "reshape", "(", "B", ",", "-", "1", ",", "2", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "k", ",", "v", "=", "kv", "[", "0", "]", ",", "kv", "[", "1", "]", "\n", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "scale", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "N", ",", "C", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.Block.__init__": [[201, 216], ["torch.Module.__init__", "norm_layer", "norm_layer", "int", "layers.Mlp", "vision_transformer.Attention", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "twins.GlobalSubSampleAttn", "twins.LocallyGroupedAttn"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "num_heads", ",", "mlp_ratio", "=", "4.", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "drop_path", "=", "0.", ",", "\n", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ",", "sr_ratio", "=", "1", ",", "ws", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "if", "ws", "is", "None", ":", "\n", "            ", "self", ".", "attn", "=", "Attention", "(", "dim", ",", "num_heads", ",", "False", ",", "None", ",", "attn_drop", ",", "drop", ")", "\n", "", "elif", "ws", "==", "1", ":", "\n", "            ", "self", ".", "attn", "=", "GlobalSubSampleAttn", "(", "dim", ",", "num_heads", ",", "attn_drop", ",", "drop", ",", "sr_ratio", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "attn", "=", "LocallyGroupedAttn", "(", "dim", ",", "num_heads", ",", "attn_drop", ",", "drop", ",", "ws", ")", "\n", "", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "mlp_hidden_dim", "=", "int", "(", "dim", "*", "mlp_ratio", ")", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "mlp_hidden_dim", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.Block.forward": [[217, 221], ["twins.Block.drop_path", "twins.Block.drop_path", "twins.Block.attn", "twins.Block.mlp", "twins.Block.norm1", "twins.Block.norm2"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ",", "size", ":", "Size_", ")", ":", "\n", "        ", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "attn", "(", "self", ".", "norm1", "(", "x", ")", ",", "size", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.PosConv.__init__": [[225, 229], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_chans", ",", "embed_dim", "=", "768", ",", "stride", "=", "1", ")", ":", "\n", "        ", "super", "(", "PosConv", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Sequential", "(", "nn", ".", "Conv2d", "(", "in_chans", ",", "embed_dim", ",", "3", ",", "stride", ",", "1", ",", "bias", "=", "True", ",", "groups", "=", "embed_dim", ")", ",", ")", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.PosConv.forward": [[230, 238], ["x.flatten().transpose.flatten().transpose.transpose().view", "twins.PosConv.proj", "x.flatten().transpose.flatten().transpose.flatten().transpose", "x.flatten().transpose.flatten().transpose.transpose", "x.flatten().transpose.flatten().transpose.flatten"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "size", ":", "Size_", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "cnn_feat_token", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "view", "(", "B", ",", "C", ",", "*", "size", ")", "\n", "x", "=", "self", ".", "proj", "(", "cnn_feat_token", ")", "\n", "if", "self", ".", "stride", "==", "1", ":", "\n", "            ", "x", "+=", "cnn_feat_token", "\n", "", "x", "=", "x", ".", "flatten", "(", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.PosConv.no_weight_decay": [[239, 241], ["range"], "methods", ["None"], ["", "def", "no_weight_decay", "(", "self", ")", ":", "\n", "        ", "return", "[", "'proj.%d.weight'", "%", "i", "for", "i", "in", "range", "(", "4", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.PatchEmbed.__init__": [[247, 260], ["torch.Module.__init__", "layers.to_2tuple", "layers.to_2tuple", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "embed_dim", "=", "768", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "img_size", "=", "to_2tuple", "(", "img_size", ")", "\n", "patch_size", "=", "to_2tuple", "(", "patch_size", ")", "\n", "\n", "self", ".", "img_size", "=", "img_size", "\n", "self", ".", "patch_size", "=", "patch_size", "\n", "assert", "img_size", "[", "0", "]", "%", "patch_size", "[", "0", "]", "==", "0", "and", "img_size", "[", "1", "]", "%", "patch_size", "[", "1", "]", "==", "0", ",", "f\"img_size {img_size} should be divided by patch_size {patch_size}.\"", "\n", "self", ".", "H", ",", "self", ".", "W", "=", "img_size", "[", "0", "]", "//", "patch_size", "[", "0", "]", ",", "img_size", "[", "1", "]", "//", "patch_size", "[", "1", "]", "\n", "self", ".", "num_patches", "=", "self", ".", "H", "*", "self", ".", "W", "\n", "self", ".", "proj", "=", "nn", ".", "Conv2d", "(", "in_chans", ",", "embed_dim", ",", "kernel_size", "=", "patch_size", ",", "stride", "=", "patch_size", ")", "\n", "self", ".", "norm", "=", "nn", ".", "LayerNorm", "(", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.PatchEmbed.forward": [[261, 269], ["twins.PatchEmbed.proj().flatten().transpose", "twins.PatchEmbed.norm", "twins.PatchEmbed.proj().flatten", "twins.PatchEmbed.proj"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "Size_", "]", ":", "\n", "        ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", ".", "flatten", "(", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "out_size", "=", "(", "H", "//", "self", ".", "patch_size", "[", "0", "]", ",", "W", "//", "self", ".", "patch_size", "[", "1", "]", ")", "\n", "\n", "return", "x", ",", "out_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.Twins.__init__": [[276, 320], ["functools.partial", "torch.Module.__init__", "layers.to_2tuple", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "norm_layer", "twins.Twins.apply", "len", "twins.Twins.patch_embeds.append", "twins.Twins.pos_drops.append", "tuple", "x.item", "len", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "twins.Twins.blocks.append", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "twins.PatchEmbed", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "twins.PosConv", "sum", "block_cls", "range"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "4", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "global_pool", "=", "'avg'", ",", "\n", "embed_dims", "=", "(", "64", ",", "128", ",", "256", ",", "512", ")", ",", "num_heads", "=", "(", "1", ",", "2", ",", "4", ",", "8", ")", ",", "mlp_ratios", "=", "(", "4", ",", "4", ",", "4", ",", "4", ")", ",", "depths", "=", "(", "3", ",", "4", ",", "6", ",", "3", ")", ",", "\n", "sr_ratios", "=", "(", "8", ",", "4", ",", "2", ",", "1", ")", ",", "wss", "=", "None", ",", "drop_rate", "=", "0.", ",", "attn_drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ",", "\n", "norm_layer", "=", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", ",", "block_cls", "=", "Block", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "self", ".", "depths", "=", "depths", "\n", "self", ".", "embed_dims", "=", "embed_dims", "\n", "self", ".", "num_features", "=", "embed_dims", "[", "-", "1", "]", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "img_size", "=", "to_2tuple", "(", "img_size", ")", "\n", "prev_chs", "=", "in_chans", "\n", "self", ".", "patch_embeds", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "pos_drops", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "depths", ")", ")", ":", "\n", "            ", "self", ".", "patch_embeds", ".", "append", "(", "PatchEmbed", "(", "img_size", ",", "patch_size", ",", "prev_chs", ",", "embed_dims", "[", "i", "]", ")", ")", "\n", "self", ".", "pos_drops", ".", "append", "(", "nn", ".", "Dropout", "(", "p", "=", "drop_rate", ")", ")", "\n", "prev_chs", "=", "embed_dims", "[", "i", "]", "\n", "img_size", "=", "tuple", "(", "t", "//", "patch_size", "for", "t", "in", "img_size", ")", "\n", "patch_size", "=", "2", "\n", "\n", "", "self", ".", "blocks", "=", "nn", ".", "ModuleList", "(", ")", "\n", "dpr", "=", "[", "x", ".", "item", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "sum", "(", "depths", ")", ")", "]", "# stochastic depth decay rule", "\n", "cur", "=", "0", "\n", "for", "k", "in", "range", "(", "len", "(", "depths", ")", ")", ":", "\n", "            ", "_block", "=", "nn", ".", "ModuleList", "(", "[", "block_cls", "(", "\n", "dim", "=", "embed_dims", "[", "k", "]", ",", "num_heads", "=", "num_heads", "[", "k", "]", ",", "mlp_ratio", "=", "mlp_ratios", "[", "k", "]", ",", "drop", "=", "drop_rate", ",", "\n", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "dpr", "[", "cur", "+", "i", "]", ",", "norm_layer", "=", "norm_layer", ",", "sr_ratio", "=", "sr_ratios", "[", "k", "]", ",", "\n", "ws", "=", "1", "if", "wss", "is", "None", "or", "i", "%", "2", "==", "1", "else", "wss", "[", "k", "]", ")", "for", "i", "in", "range", "(", "depths", "[", "k", "]", ")", "]", ")", "\n", "self", ".", "blocks", ".", "append", "(", "_block", ")", "\n", "cur", "+=", "depths", "[", "k", "]", "\n", "\n", "", "self", ".", "pos_block", "=", "nn", ".", "ModuleList", "(", "[", "PosConv", "(", "embed_dim", ",", "embed_dim", ")", "for", "embed_dim", "in", "embed_dims", "]", ")", "\n", "\n", "self", ".", "norm", "=", "norm_layer", "(", "self", ".", "num_features", ")", "\n", "\n", "# classification head", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "# init weights", "\n", "self", ".", "apply", "(", "self", ".", "_init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.Twins.no_weight_decay": [[321, 324], ["set", "twins.Twins.pos_block.named_parameters"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEmaV2.set"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "no_weight_decay", "(", "self", ")", ":", "\n", "        ", "return", "set", "(", "[", "'pos_block.'", "+", "n", "for", "n", ",", "p", "in", "self", ".", "pos_block", ".", "named_parameters", "(", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.Twins.group_matcher": [[325, 339], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "\n", "stem", "=", "r'^patch_embeds.0'", ",", "# stem and embed", "\n", "blocks", "=", "[", "\n", "(", "r'^(?:blocks|patch_embeds|pos_block)\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "'^norm'", ",", "(", "99999", ",", ")", ")", "\n", "]", "if", "coarse", "else", "[", "\n", "(", "r'^blocks\\.(\\d+)\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^(?:patch_embeds|pos_block)\\.(\\d+)'", ",", "(", "0", ",", ")", ")", ",", "\n", "(", "r'^norm'", ",", "(", "99999", ",", ")", ")", "\n", "]", "\n", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.Twins.set_grad_checkpointing": [[340, 343], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "'gradient checkpointing not supported'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.Twins.get_classifier": [[344, 347], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.Twins.reset_classifier": [[348, 354], ["torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "assert", "global_pool", "in", "(", "''", ",", "'avg'", ")", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.Twins._init_weights": [[355, 369], ["isinstance", "layers.trunc_normal_", "isinstance", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "isinstance", "m.weight.data.normal_", "math.sqrt", "m.bias.data.zero_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["", "def", "_init_weights", "(", "self", ",", "m", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "            ", "trunc_normal_", "(", "m", ".", "weight", ",", "std", "=", ".02", ")", "\n", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1.0", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "            ", "fan_out", "=", "m", ".", "kernel_size", "[", "0", "]", "*", "m", ".", "kernel_size", "[", "1", "]", "*", "m", ".", "out_channels", "\n", "fan_out", "//=", "m", ".", "groups", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "math", ".", "sqrt", "(", "2.0", "/", "fan_out", ")", ")", "\n", "if", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.Twins.forward_features": [[370, 384], ["enumerate", "twins.Twins.norm", "zip", "embed", "drop", "enumerate", "blk", "pos_blk.reshape().permute().contiguous", "pos_blk", "len", "pos_blk.reshape().permute", "pos_blk.reshape"], "methods", ["None"], ["", "", "", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "for", "i", ",", "(", "embed", ",", "drop", ",", "blocks", ",", "pos_blk", ")", "in", "enumerate", "(", "\n", "zip", "(", "self", ".", "patch_embeds", ",", "self", ".", "pos_drops", ",", "self", ".", "blocks", ",", "self", ".", "pos_block", ")", ")", ":", "\n", "            ", "x", ",", "size", "=", "embed", "(", "x", ")", "\n", "x", "=", "drop", "(", "x", ")", "\n", "for", "j", ",", "blk", "in", "enumerate", "(", "blocks", ")", ":", "\n", "                ", "x", "=", "blk", "(", "x", ",", "size", ")", "\n", "if", "j", "==", "0", ":", "\n", "                    ", "x", "=", "pos_blk", "(", "x", ",", "size", ")", "# PEG here", "\n", "", "", "if", "i", "<", "len", "(", "self", ".", "depths", ")", "-", "1", ":", "\n", "                ", "x", "=", "x", ".", "reshape", "(", "B", ",", "*", "size", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.Twins.forward_head": [[385, 389], ["x.mean.mean.mean", "twins.Twins.head"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "global_pool", "==", "'avg'", ":", "\n", "            ", "x", "=", "x", ".", "mean", "(", "dim", "=", "1", ")", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.Twins.forward": [[390, 394], ["twins.Twins.forward_features", "twins.Twins.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins._cfg": [[31, 39], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", ".9", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'patch_embeds.0.proj'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins._create_twins": [[396, 402], ["kwargs.get", "helpers.build_model_with_cfg", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_twins", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "kwargs", ".", "get", "(", "'features_only'", ",", "None", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'features_only not implemented for Vision Transformer models.'", ")", "\n", "\n", "", "model", "=", "build_model_with_cfg", "(", "Twins", ",", "variant", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.twins_pcpvt_small": [[404, 410], ["dict", "twins._create_twins"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins._create_twins"], ["", "@", "register_model", "\n", "def", "twins_pcpvt_small", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "embed_dims", "=", "[", "64", ",", "128", ",", "320", ",", "512", "]", ",", "num_heads", "=", "[", "1", ",", "2", ",", "5", ",", "8", "]", ",", "mlp_ratios", "=", "[", "8", ",", "8", ",", "4", ",", "4", "]", ",", "\n", "depths", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "sr_ratios", "=", "[", "8", ",", "4", ",", "2", ",", "1", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_twins", "(", "'twins_pcpvt_small'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.twins_pcpvt_base": [[412, 418], ["dict", "twins._create_twins"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins._create_twins"], ["", "@", "register_model", "\n", "def", "twins_pcpvt_base", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "embed_dims", "=", "[", "64", ",", "128", ",", "320", ",", "512", "]", ",", "num_heads", "=", "[", "1", ",", "2", ",", "5", ",", "8", "]", ",", "mlp_ratios", "=", "[", "8", ",", "8", ",", "4", ",", "4", "]", ",", "\n", "depths", "=", "[", "3", ",", "4", ",", "18", ",", "3", "]", ",", "sr_ratios", "=", "[", "8", ",", "4", ",", "2", ",", "1", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_twins", "(", "'twins_pcpvt_base'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.twins_pcpvt_large": [[420, 426], ["dict", "twins._create_twins"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins._create_twins"], ["", "@", "register_model", "\n", "def", "twins_pcpvt_large", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "embed_dims", "=", "[", "64", ",", "128", ",", "320", ",", "512", "]", ",", "num_heads", "=", "[", "1", ",", "2", ",", "5", ",", "8", "]", ",", "mlp_ratios", "=", "[", "8", ",", "8", ",", "4", ",", "4", "]", ",", "\n", "depths", "=", "[", "3", ",", "8", ",", "27", ",", "3", "]", ",", "sr_ratios", "=", "[", "8", ",", "4", ",", "2", ",", "1", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_twins", "(", "'twins_pcpvt_large'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.twins_svt_small": [[428, 434], ["dict", "twins._create_twins"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins._create_twins"], ["", "@", "register_model", "\n", "def", "twins_svt_small", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "embed_dims", "=", "[", "64", ",", "128", ",", "256", ",", "512", "]", ",", "num_heads", "=", "[", "2", ",", "4", ",", "8", ",", "16", "]", ",", "mlp_ratios", "=", "[", "4", ",", "4", ",", "4", ",", "4", "]", ",", "\n", "depths", "=", "[", "2", ",", "2", ",", "10", ",", "4", "]", ",", "wss", "=", "[", "7", ",", "7", ",", "7", ",", "7", "]", ",", "sr_ratios", "=", "[", "8", ",", "4", ",", "2", ",", "1", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_twins", "(", "'twins_svt_small'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.twins_svt_base": [[436, 442], ["dict", "twins._create_twins"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins._create_twins"], ["", "@", "register_model", "\n", "def", "twins_svt_base", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "embed_dims", "=", "[", "96", ",", "192", ",", "384", ",", "768", "]", ",", "num_heads", "=", "[", "3", ",", "6", ",", "12", ",", "24", "]", ",", "mlp_ratios", "=", "[", "4", ",", "4", ",", "4", ",", "4", "]", ",", "\n", "depths", "=", "[", "2", ",", "2", ",", "18", ",", "2", "]", ",", "wss", "=", "[", "7", ",", "7", ",", "7", ",", "7", "]", ",", "sr_ratios", "=", "[", "8", ",", "4", ",", "2", ",", "1", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_twins", "(", "'twins_svt_base'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins.twins_svt_large": [[444, 450], ["dict", "twins._create_twins"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.twins._create_twins"], ["", "@", "register_model", "\n", "def", "twins_svt_large", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "embed_dims", "=", "[", "128", ",", "256", ",", "512", ",", "1024", "]", ",", "num_heads", "=", "[", "4", ",", "8", ",", "16", ",", "32", "]", ",", "mlp_ratios", "=", "[", "4", ",", "4", ",", "4", ",", "4", "]", ",", "\n", "depths", "=", "[", "2", ",", "2", ",", "18", ",", "2", "]", ",", "wss", "=", "[", "7", ",", "7", ",", "7", ",", "7", "]", ",", "sr_ratios", "=", "[", "8", ",", "4", ",", "2", ",", "1", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_twins", "(", "'twins_svt_large'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_blocks.SqueezeExcite.__init__": [[38, 50], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "layers.create_act_layer", "torch.Conv2d", "torch.Conv2d", "layers.create_act_layer", "rd_round_fn"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.create_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.create_act_layer"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "rd_ratio", "=", "0.25", ",", "rd_channels", "=", "None", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "\n", "gate_layer", "=", "nn", ".", "Sigmoid", ",", "force_act_layer", "=", "None", ",", "rd_round_fn", "=", "None", ")", ":", "\n", "        ", "super", "(", "SqueezeExcite", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "rd_channels", "is", "None", ":", "\n", "            ", "rd_round_fn", "=", "rd_round_fn", "or", "round", "\n", "rd_channels", "=", "rd_round_fn", "(", "in_chs", "*", "rd_ratio", ")", "\n", "", "act_layer", "=", "force_act_layer", "or", "act_layer", "\n", "self", ".", "conv_reduce", "=", "nn", ".", "Conv2d", "(", "in_chs", ",", "rd_channels", ",", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "act1", "=", "create_act_layer", "(", "act_layer", ",", "inplace", "=", "True", ")", "\n", "self", ".", "conv_expand", "=", "nn", ".", "Conv2d", "(", "rd_channels", ",", "in_chs", ",", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "gate", "=", "create_act_layer", "(", "gate_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_blocks.SqueezeExcite.forward": [[51, 57], ["x.mean", "efficientnet_blocks.SqueezeExcite.conv_reduce", "efficientnet_blocks.SqueezeExcite.act1", "efficientnet_blocks.SqueezeExcite.conv_expand", "efficientnet_blocks.SqueezeExcite.gate"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_se", "=", "x", ".", "mean", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "x_se", "=", "self", ".", "conv_reduce", "(", "x_se", ")", "\n", "x_se", "=", "self", ".", "act1", "(", "x_se", ")", "\n", "x_se", "=", "self", ".", "conv_expand", "(", "x_se", ")", "\n", "return", "x", "*", "self", ".", "gate", "(", "x_se", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_blocks.ConvBnAct.__init__": [[62, 74], ["torch.Module.__init__", "layers.get_norm_act_layer", "efficientnet_blocks.num_groups", "layers.create_conv2d", "layers.get_norm_act_layer.", "layers.DropPath", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.get_norm_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.num_groups", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "kernel_size", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "group_size", "=", "0", ",", "pad_type", "=", "''", ",", "\n", "skip", "=", "False", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "        ", "super", "(", "ConvBnAct", ",", "self", ")", ".", "__init__", "(", ")", "\n", "norm_act_layer", "=", "get_norm_act_layer", "(", "norm_layer", ",", "act_layer", ")", "\n", "groups", "=", "num_groups", "(", "group_size", ",", "in_chs", ")", "\n", "self", ".", "has_skip", "=", "skip", "and", "stride", "==", "1", "and", "in_chs", "==", "out_chs", "\n", "\n", "self", ".", "conv", "=", "create_conv2d", "(", "\n", "in_chs", ",", "out_chs", ",", "kernel_size", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "groups", "=", "groups", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "bn1", "=", "norm_act_layer", "(", "out_chs", ",", "inplace", "=", "True", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path_rate", ")", "if", "drop_path_rate", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_blocks.ConvBnAct.feature_info": [[75, 80], ["dict", "dict"], "methods", ["None"], ["", "def", "feature_info", "(", "self", ",", "location", ")", ":", "\n", "        ", "if", "location", "==", "'expansion'", ":", "# output of conv after act, same as block coutput", "\n", "            ", "return", "dict", "(", "module", "=", "'bn1'", ",", "hook_type", "=", "'forward'", ",", "num_chs", "=", "self", ".", "conv", ".", "out_channels", ")", "\n", "", "else", ":", "# location == 'bottleneck', block output", "\n", "            ", "return", "dict", "(", "module", "=", "''", ",", "hook_type", "=", "''", ",", "num_chs", "=", "self", ".", "conv", ".", "out_channels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_blocks.ConvBnAct.forward": [[81, 88], ["efficientnet_blocks.ConvBnAct.conv", "efficientnet_blocks.ConvBnAct.bn1", "efficientnet_blocks.ConvBnAct.drop_path"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "if", "self", ".", "has_skip", ":", "\n", "            ", "x", "=", "self", ".", "drop_path", "(", "x", ")", "+", "shortcut", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_blocks.DepthwiseSeparableConv.__init__": [[95, 115], ["torch.Module.__init__", "layers.get_norm_act_layer", "efficientnet_blocks.num_groups", "layers.create_conv2d", "layers.get_norm_act_layer.", "layers.create_conv2d", "layers.get_norm_act_layer.", "se_layer", "torch.Identity", "torch.Identity", "layers.DropPath", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.get_norm_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.num_groups", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "dw_kernel_size", "=", "3", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "group_size", "=", "1", ",", "pad_type", "=", "''", ",", "\n", "noskip", "=", "False", ",", "pw_kernel_size", "=", "1", ",", "pw_act", "=", "False", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "\n", "se_layer", "=", "None", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "        ", "super", "(", "DepthwiseSeparableConv", ",", "self", ")", ".", "__init__", "(", ")", "\n", "norm_act_layer", "=", "get_norm_act_layer", "(", "norm_layer", ",", "act_layer", ")", "\n", "groups", "=", "num_groups", "(", "group_size", ",", "in_chs", ")", "\n", "self", ".", "has_skip", "=", "(", "stride", "==", "1", "and", "in_chs", "==", "out_chs", ")", "and", "not", "noskip", "\n", "self", ".", "has_pw_act", "=", "pw_act", "# activation after point-wise conv", "\n", "\n", "self", ".", "conv_dw", "=", "create_conv2d", "(", "\n", "in_chs", ",", "in_chs", ",", "dw_kernel_size", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "padding", "=", "pad_type", ",", "groups", "=", "groups", ")", "\n", "self", ".", "bn1", "=", "norm_act_layer", "(", "in_chs", ",", "inplace", "=", "True", ")", "\n", "\n", "# Squeeze-and-excitation", "\n", "self", ".", "se", "=", "se_layer", "(", "in_chs", ",", "act_layer", "=", "act_layer", ")", "if", "se_layer", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "conv_pw", "=", "create_conv2d", "(", "in_chs", ",", "out_chs", ",", "pw_kernel_size", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "bn2", "=", "norm_act_layer", "(", "out_chs", ",", "inplace", "=", "True", ",", "apply_act", "=", "self", ".", "has_pw_act", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path_rate", ")", "if", "drop_path_rate", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_blocks.DepthwiseSeparableConv.feature_info": [[116, 121], ["dict", "dict"], "methods", ["None"], ["", "def", "feature_info", "(", "self", ",", "location", ")", ":", "\n", "        ", "if", "location", "==", "'expansion'", ":", "# after SE, input to PW", "\n", "            ", "return", "dict", "(", "module", "=", "'conv_pw'", ",", "hook_type", "=", "'forward_pre'", ",", "num_chs", "=", "self", ".", "conv_pw", ".", "in_channels", ")", "\n", "", "else", ":", "# location == 'bottleneck', block output", "\n", "            ", "return", "dict", "(", "module", "=", "''", ",", "hook_type", "=", "''", ",", "num_chs", "=", "self", ".", "conv_pw", ".", "out_channels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_blocks.DepthwiseSeparableConv.forward": [[122, 132], ["efficientnet_blocks.DepthwiseSeparableConv.conv_dw", "efficientnet_blocks.DepthwiseSeparableConv.bn1", "efficientnet_blocks.DepthwiseSeparableConv.se", "efficientnet_blocks.DepthwiseSeparableConv.conv_pw", "efficientnet_blocks.DepthwiseSeparableConv.bn2", "efficientnet_blocks.DepthwiseSeparableConv.drop_path"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "x", "=", "self", ".", "conv_dw", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "x", "=", "self", ".", "se", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_pw", "(", "x", ")", "\n", "x", "=", "self", ".", "bn2", "(", "x", ")", "\n", "if", "self", ".", "has_skip", ":", "\n", "            ", "x", "=", "self", ".", "drop_path", "(", "x", ")", "+", "shortcut", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_blocks.InvertedResidual.__init__": [[144, 172], ["torch.Module.__init__", "layers.get_norm_act_layer", "layers.make_divisible", "efficientnet_blocks.num_groups", "layers.create_conv2d", "layers.get_norm_act_layer.", "layers.create_conv2d", "layers.get_norm_act_layer.", "layers.create_conv2d", "layers.get_norm_act_layer.", "se_layer", "torch.Identity", "torch.Identity", "layers.DropPath", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.get_norm_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.num_groups", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "dw_kernel_size", "=", "3", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "group_size", "=", "1", ",", "pad_type", "=", "''", ",", "\n", "noskip", "=", "False", ",", "exp_ratio", "=", "1.0", ",", "exp_kernel_size", "=", "1", ",", "pw_kernel_size", "=", "1", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "\n", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "se_layer", "=", "None", ",", "conv_kwargs", "=", "None", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "        ", "super", "(", "InvertedResidual", ",", "self", ")", ".", "__init__", "(", ")", "\n", "norm_act_layer", "=", "get_norm_act_layer", "(", "norm_layer", ",", "act_layer", ")", "\n", "conv_kwargs", "=", "conv_kwargs", "or", "{", "}", "\n", "mid_chs", "=", "make_divisible", "(", "in_chs", "*", "exp_ratio", ")", "\n", "groups", "=", "num_groups", "(", "group_size", ",", "mid_chs", ")", "\n", "self", ".", "has_skip", "=", "(", "in_chs", "==", "out_chs", "and", "stride", "==", "1", ")", "and", "not", "noskip", "\n", "\n", "# Point-wise expansion", "\n", "self", ".", "conv_pw", "=", "create_conv2d", "(", "in_chs", ",", "mid_chs", ",", "exp_kernel_size", ",", "padding", "=", "pad_type", ",", "**", "conv_kwargs", ")", "\n", "self", ".", "bn1", "=", "norm_act_layer", "(", "mid_chs", ",", "inplace", "=", "True", ")", "\n", "\n", "# Depth-wise convolution", "\n", "self", ".", "conv_dw", "=", "create_conv2d", "(", "\n", "mid_chs", ",", "mid_chs", ",", "dw_kernel_size", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "\n", "groups", "=", "groups", ",", "padding", "=", "pad_type", ",", "**", "conv_kwargs", ")", "\n", "self", ".", "bn2", "=", "norm_act_layer", "(", "mid_chs", ",", "inplace", "=", "True", ")", "\n", "\n", "# Squeeze-and-excitation", "\n", "self", ".", "se", "=", "se_layer", "(", "mid_chs", ",", "act_layer", "=", "act_layer", ")", "if", "se_layer", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "# Point-wise linear projection", "\n", "self", ".", "conv_pwl", "=", "create_conv2d", "(", "mid_chs", ",", "out_chs", ",", "pw_kernel_size", ",", "padding", "=", "pad_type", ",", "**", "conv_kwargs", ")", "\n", "self", ".", "bn3", "=", "norm_act_layer", "(", "out_chs", ",", "apply_act", "=", "False", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path_rate", ")", "if", "drop_path_rate", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_blocks.InvertedResidual.feature_info": [[173, 178], ["dict", "dict"], "methods", ["None"], ["", "def", "feature_info", "(", "self", ",", "location", ")", ":", "\n", "        ", "if", "location", "==", "'expansion'", ":", "# after SE, input to PWL", "\n", "            ", "return", "dict", "(", "module", "=", "'conv_pwl'", ",", "hook_type", "=", "'forward_pre'", ",", "num_chs", "=", "self", ".", "conv_pwl", ".", "in_channels", ")", "\n", "", "else", ":", "# location == 'bottleneck', block output", "\n", "            ", "return", "dict", "(", "module", "=", "''", ",", "hook_type", "=", "''", ",", "num_chs", "=", "self", ".", "conv_pwl", ".", "out_channels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_blocks.InvertedResidual.forward": [[179, 191], ["efficientnet_blocks.InvertedResidual.conv_pw", "efficientnet_blocks.InvertedResidual.bn1", "efficientnet_blocks.InvertedResidual.conv_dw", "efficientnet_blocks.InvertedResidual.bn2", "efficientnet_blocks.InvertedResidual.se", "efficientnet_blocks.InvertedResidual.conv_pwl", "efficientnet_blocks.InvertedResidual.bn3", "efficientnet_blocks.InvertedResidual.drop_path"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "x", "=", "self", ".", "conv_pw", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_dw", "(", "x", ")", "\n", "x", "=", "self", ".", "bn2", "(", "x", ")", "\n", "x", "=", "self", ".", "se", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_pwl", "(", "x", ")", "\n", "x", "=", "self", ".", "bn3", "(", "x", ")", "\n", "if", "self", ".", "has_skip", ":", "\n", "            ", "x", "=", "self", ".", "drop_path", "(", "x", ")", "+", "shortcut", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_blocks.CondConvResidual.__init__": [[196, 211], ["dict", "efficientnet_blocks.InvertedResidual.__init__", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "dw_kernel_size", "=", "3", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "group_size", "=", "1", ",", "pad_type", "=", "''", ",", "\n", "noskip", "=", "False", ",", "exp_ratio", "=", "1.0", ",", "exp_kernel_size", "=", "1", ",", "pw_kernel_size", "=", "1", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "\n", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "se_layer", "=", "None", ",", "num_experts", "=", "0", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "\n", "        ", "self", ".", "num_experts", "=", "num_experts", "\n", "conv_kwargs", "=", "dict", "(", "num_experts", "=", "self", ".", "num_experts", ")", "\n", "\n", "super", "(", "CondConvResidual", ",", "self", ")", ".", "__init__", "(", "\n", "in_chs", ",", "out_chs", ",", "dw_kernel_size", "=", "dw_kernel_size", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "group_size", "=", "group_size", ",", "\n", "pad_type", "=", "pad_type", ",", "act_layer", "=", "act_layer", ",", "noskip", "=", "noskip", ",", "exp_ratio", "=", "exp_ratio", ",", "exp_kernel_size", "=", "exp_kernel_size", ",", "\n", "pw_kernel_size", "=", "pw_kernel_size", ",", "se_layer", "=", "se_layer", ",", "norm_layer", "=", "norm_layer", ",", "conv_kwargs", "=", "conv_kwargs", ",", "\n", "drop_path_rate", "=", "drop_path_rate", ")", "\n", "\n", "self", ".", "routing_fn", "=", "nn", ".", "Linear", "(", "in_chs", ",", "self", ".", "num_experts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_blocks.CondConvResidual.forward": [[212, 226], ["torch.nn.functional.adaptive_avg_pool2d().flatten", "torch.nn.functional.adaptive_avg_pool2d().flatten", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "efficientnet_blocks.CondConvResidual.conv_pw", "efficientnet_blocks.CondConvResidual.bn1", "efficientnet_blocks.CondConvResidual.conv_dw", "efficientnet_blocks.CondConvResidual.bn2", "efficientnet_blocks.CondConvResidual.se", "efficientnet_blocks.CondConvResidual.conv_pwl", "efficientnet_blocks.CondConvResidual.bn3", "efficientnet_blocks.CondConvResidual.routing_fn", "torch.nn.functional.adaptive_avg_pool2d", "torch.nn.functional.adaptive_avg_pool2d", "efficientnet_blocks.CondConvResidual.drop_path"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "pooled_inputs", "=", "F", ".", "adaptive_avg_pool2d", "(", "x", ",", "1", ")", ".", "flatten", "(", "1", ")", "# CondConv routing", "\n", "routing_weights", "=", "torch", ".", "sigmoid", "(", "self", ".", "routing_fn", "(", "pooled_inputs", ")", ")", "\n", "x", "=", "self", ".", "conv_pw", "(", "x", ",", "routing_weights", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_dw", "(", "x", ",", "routing_weights", ")", "\n", "x", "=", "self", ".", "bn2", "(", "x", ")", "\n", "x", "=", "self", ".", "se", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_pwl", "(", "x", ",", "routing_weights", ")", "\n", "x", "=", "self", ".", "bn3", "(", "x", ")", "\n", "if", "self", ".", "has_skip", ":", "\n", "            ", "x", "=", "self", ".", "drop_path", "(", "x", ")", "+", "shortcut", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_blocks.EdgeResidual.__init__": [[240, 265], ["torch.Module.__init__", "layers.get_norm_act_layer", "efficientnet_blocks.num_groups", "layers.create_conv2d", "layers.get_norm_act_layer.", "layers.create_conv2d", "layers.get_norm_act_layer.", "layers.make_divisible", "layers.make_divisible", "se_layer", "torch.Identity", "torch.Identity", "layers.DropPath", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.get_norm_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.num_groups", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "exp_kernel_size", "=", "3", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "group_size", "=", "0", ",", "pad_type", "=", "''", ",", "\n", "force_in_chs", "=", "0", ",", "noskip", "=", "False", ",", "exp_ratio", "=", "1.0", ",", "pw_kernel_size", "=", "1", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "\n", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "se_layer", "=", "None", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "        ", "super", "(", "EdgeResidual", ",", "self", ")", ".", "__init__", "(", ")", "\n", "norm_act_layer", "=", "get_norm_act_layer", "(", "norm_layer", ",", "act_layer", ")", "\n", "if", "force_in_chs", ">", "0", ":", "\n", "            ", "mid_chs", "=", "make_divisible", "(", "force_in_chs", "*", "exp_ratio", ")", "\n", "", "else", ":", "\n", "            ", "mid_chs", "=", "make_divisible", "(", "in_chs", "*", "exp_ratio", ")", "\n", "", "groups", "=", "num_groups", "(", "group_size", ",", "in_chs", ")", "\n", "self", ".", "has_skip", "=", "(", "in_chs", "==", "out_chs", "and", "stride", "==", "1", ")", "and", "not", "noskip", "\n", "\n", "# Expansion convolution", "\n", "self", ".", "conv_exp", "=", "create_conv2d", "(", "\n", "in_chs", ",", "mid_chs", ",", "exp_kernel_size", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "groups", "=", "groups", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "bn1", "=", "norm_act_layer", "(", "mid_chs", ",", "inplace", "=", "True", ")", "\n", "\n", "# Squeeze-and-excitation", "\n", "self", ".", "se", "=", "se_layer", "(", "mid_chs", ",", "act_layer", "=", "act_layer", ")", "if", "se_layer", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "# Point-wise linear projection", "\n", "self", ".", "conv_pwl", "=", "create_conv2d", "(", "mid_chs", ",", "out_chs", ",", "pw_kernel_size", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "bn2", "=", "norm_act_layer", "(", "out_chs", ",", "apply_act", "=", "False", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path_rate", ")", "if", "drop_path_rate", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_blocks.EdgeResidual.feature_info": [[266, 271], ["dict", "dict"], "methods", ["None"], ["", "def", "feature_info", "(", "self", ",", "location", ")", ":", "\n", "        ", "if", "location", "==", "'expansion'", ":", "# after SE, before PWL", "\n", "            ", "return", "dict", "(", "module", "=", "'conv_pwl'", ",", "hook_type", "=", "'forward_pre'", ",", "num_chs", "=", "self", ".", "conv_pwl", ".", "in_channels", ")", "\n", "", "else", ":", "# location == 'bottleneck', block output", "\n", "            ", "return", "dict", "(", "module", "=", "''", ",", "hook_type", "=", "''", ",", "num_chs", "=", "self", ".", "conv_pwl", ".", "out_channels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_blocks.EdgeResidual.forward": [[272, 282], ["efficientnet_blocks.EdgeResidual.conv_exp", "efficientnet_blocks.EdgeResidual.bn1", "efficientnet_blocks.EdgeResidual.se", "efficientnet_blocks.EdgeResidual.conv_pwl", "efficientnet_blocks.EdgeResidual.bn2", "efficientnet_blocks.EdgeResidual.drop_path"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "x", "=", "self", ".", "conv_exp", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "x", "=", "self", ".", "se", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_pwl", "(", "x", ")", "\n", "x", "=", "self", ".", "bn2", "(", "x", ")", "\n", "if", "self", ".", "has_skip", ":", "\n", "            ", "x", "=", "self", ".", "drop_path", "(", "x", ")", "+", "shortcut", "\n", "", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_blocks.num_groups": [[17, 24], ["None"], "function", ["None"], ["def", "num_groups", "(", "group_size", ",", "channels", ")", ":", "\n", "    ", "if", "not", "group_size", ":", "# 0 or None", "\n", "        ", "return", "1", "# normal conv with 1 group", "\n", "", "else", ":", "\n", "# NOTE group_size == 1 -> depthwise conv", "\n", "        ", "assert", "channels", "%", "group_size", "==", "0", "\n", "return", "channels", "//", "group_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.BasicConv2d.__init__": [[29, 35], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_planes", ",", "out_planes", ",", "kernel_size", ",", "stride", ",", "padding", "=", "0", ")", ":", "\n", "        ", "super", "(", "BasicConv2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv", "=", "nn", ".", "Conv2d", "(", "\n", "in_planes", ",", "out_planes", ",", "kernel_size", "=", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn", "=", "nn", ".", "BatchNorm2d", "(", "out_planes", ",", "eps", "=", "0.001", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.BasicConv2d.forward": [[36, 41], ["inception_v4.BasicConv2d.conv", "inception_v4.BasicConv2d.bn", "inception_v4.BasicConv2d.relu"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "x", "=", "self", ".", "bn", "(", "x", ")", "\n", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.Mixed3a.__init__": [[44, 48], ["torch.Module.__init__", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "inception_v4.BasicConv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "Mixed3a", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "maxpool", "=", "nn", ".", "MaxPool2d", "(", "3", ",", "stride", "=", "2", ")", "\n", "self", ".", "conv", "=", "BasicConv2d", "(", "64", ",", "96", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.Mixed3a.forward": [[49, 54], ["inception_v4.Mixed3a.maxpool", "inception_v4.Mixed3a.conv", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x0", "=", "self", ".", "maxpool", "(", "x", ")", "\n", "x1", "=", "self", ".", "conv", "(", "x", ")", "\n", "out", "=", "torch", ".", "cat", "(", "(", "x0", ",", "x1", ")", ",", "1", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.Mixed4a.__init__": [[57, 70], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "Mixed4a", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "branch0", "=", "nn", ".", "Sequential", "(", "\n", "BasicConv2d", "(", "160", ",", "64", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "64", ",", "96", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", "\n", ")", "\n", "\n", "self", ".", "branch1", "=", "nn", ".", "Sequential", "(", "\n", "BasicConv2d", "(", "160", ",", "64", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "64", ",", "64", ",", "kernel_size", "=", "(", "1", ",", "7", ")", ",", "stride", "=", "1", ",", "padding", "=", "(", "0", ",", "3", ")", ")", ",", "\n", "BasicConv2d", "(", "64", ",", "64", ",", "kernel_size", "=", "(", "7", ",", "1", ")", ",", "stride", "=", "1", ",", "padding", "=", "(", "3", ",", "0", ")", ")", ",", "\n", "BasicConv2d", "(", "64", ",", "96", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "stride", "=", "1", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.Mixed4a.forward": [[72, 77], ["inception_v4.Mixed4a.branch0", "inception_v4.Mixed4a.branch1", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x0", "=", "self", ".", "branch0", "(", "x", ")", "\n", "x1", "=", "self", ".", "branch1", "(", "x", ")", "\n", "out", "=", "torch", ".", "cat", "(", "(", "x0", ",", "x1", ")", ",", "1", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.Mixed5a.__init__": [[80, 84], ["torch.Module.__init__", "inception_v4.BasicConv2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "Mixed5a", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv", "=", "BasicConv2d", "(", "192", ",", "192", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", "self", ".", "maxpool", "=", "nn", ".", "MaxPool2d", "(", "3", ",", "stride", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.Mixed5a.forward": [[85, 90], ["inception_v4.Mixed5a.conv", "inception_v4.Mixed5a.maxpool", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x0", "=", "self", ".", "conv", "(", "x", ")", "\n", "x1", "=", "self", ".", "maxpool", "(", "x", ")", "\n", "out", "=", "torch", ".", "cat", "(", "(", "x0", ",", "x1", ")", ",", "1", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.InceptionA.__init__": [[93, 111], ["torch.Module.__init__", "inception_v4.BasicConv2d", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "inception_v4.BasicConv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "InceptionA", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "branch0", "=", "BasicConv2d", "(", "384", ",", "96", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "\n", "self", ".", "branch1", "=", "nn", ".", "Sequential", "(", "\n", "BasicConv2d", "(", "384", ",", "64", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "64", ",", "96", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", ")", "\n", "\n", "self", ".", "branch2", "=", "nn", ".", "Sequential", "(", "\n", "BasicConv2d", "(", "384", ",", "64", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "64", ",", "96", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "96", ",", "96", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", ")", "\n", "\n", "self", ".", "branch3", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "AvgPool2d", "(", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ",", "count_include_pad", "=", "False", ")", ",", "\n", "BasicConv2d", "(", "384", ",", "96", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.InceptionA.forward": [[113, 120], ["inception_v4.InceptionA.branch0", "inception_v4.InceptionA.branch1", "inception_v4.InceptionA.branch2", "inception_v4.InceptionA.branch3", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x0", "=", "self", ".", "branch0", "(", "x", ")", "\n", "x1", "=", "self", ".", "branch1", "(", "x", ")", "\n", "x2", "=", "self", ".", "branch2", "(", "x", ")", "\n", "x3", "=", "self", ".", "branch3", "(", "x", ")", "\n", "out", "=", "torch", ".", "cat", "(", "(", "x0", ",", "x1", ",", "x2", ",", "x3", ")", ",", "1", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.ReductionA.__init__": [[123, 134], ["torch.Module.__init__", "inception_v4.BasicConv2d", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "ReductionA", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "branch0", "=", "BasicConv2d", "(", "384", ",", "384", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", "\n", "self", ".", "branch1", "=", "nn", ".", "Sequential", "(", "\n", "BasicConv2d", "(", "384", ",", "192", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "192", ",", "224", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "224", ",", "256", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", ")", "\n", "\n", "self", ".", "branch2", "=", "nn", ".", "MaxPool2d", "(", "3", ",", "stride", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.ReductionA.forward": [[135, 141], ["inception_v4.ReductionA.branch0", "inception_v4.ReductionA.branch1", "inception_v4.ReductionA.branch2", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x0", "=", "self", ".", "branch0", "(", "x", ")", "\n", "x1", "=", "self", ".", "branch1", "(", "x", ")", "\n", "x2", "=", "self", ".", "branch2", "(", "x", ")", "\n", "out", "=", "torch", ".", "cat", "(", "(", "x0", ",", "x1", ",", "x2", ")", ",", "1", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.InceptionB.__init__": [[144, 165], ["torch.Module.__init__", "inception_v4.BasicConv2d", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "inception_v4.BasicConv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "InceptionB", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "branch0", "=", "BasicConv2d", "(", "1024", ",", "384", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "\n", "self", ".", "branch1", "=", "nn", ".", "Sequential", "(", "\n", "BasicConv2d", "(", "1024", ",", "192", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "192", ",", "224", ",", "kernel_size", "=", "(", "1", ",", "7", ")", ",", "stride", "=", "1", ",", "padding", "=", "(", "0", ",", "3", ")", ")", ",", "\n", "BasicConv2d", "(", "224", ",", "256", ",", "kernel_size", "=", "(", "7", ",", "1", ")", ",", "stride", "=", "1", ",", "padding", "=", "(", "3", ",", "0", ")", ")", "\n", ")", "\n", "\n", "self", ".", "branch2", "=", "nn", ".", "Sequential", "(", "\n", "BasicConv2d", "(", "1024", ",", "192", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "192", ",", "192", ",", "kernel_size", "=", "(", "7", ",", "1", ")", ",", "stride", "=", "1", ",", "padding", "=", "(", "3", ",", "0", ")", ")", ",", "\n", "BasicConv2d", "(", "192", ",", "224", ",", "kernel_size", "=", "(", "1", ",", "7", ")", ",", "stride", "=", "1", ",", "padding", "=", "(", "0", ",", "3", ")", ")", ",", "\n", "BasicConv2d", "(", "224", ",", "224", ",", "kernel_size", "=", "(", "7", ",", "1", ")", ",", "stride", "=", "1", ",", "padding", "=", "(", "3", ",", "0", ")", ")", ",", "\n", "BasicConv2d", "(", "224", ",", "256", ",", "kernel_size", "=", "(", "1", ",", "7", ")", ",", "stride", "=", "1", ",", "padding", "=", "(", "0", ",", "3", ")", ")", "\n", ")", "\n", "\n", "self", ".", "branch3", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "AvgPool2d", "(", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ",", "count_include_pad", "=", "False", ")", ",", "\n", "BasicConv2d", "(", "1024", ",", "128", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.InceptionB.forward": [[167, 174], ["inception_v4.InceptionB.branch0", "inception_v4.InceptionB.branch1", "inception_v4.InceptionB.branch2", "inception_v4.InceptionB.branch3", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x0", "=", "self", ".", "branch0", "(", "x", ")", "\n", "x1", "=", "self", ".", "branch1", "(", "x", ")", "\n", "x2", "=", "self", ".", "branch2", "(", "x", ")", "\n", "x3", "=", "self", ".", "branch3", "(", "x", ")", "\n", "out", "=", "torch", ".", "cat", "(", "(", "x0", ",", "x1", ",", "x2", ",", "x3", ")", ",", "1", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.ReductionB.__init__": [[177, 193], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "ReductionB", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "branch0", "=", "nn", ".", "Sequential", "(", "\n", "BasicConv2d", "(", "1024", ",", "192", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "192", ",", "192", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", ")", "\n", "\n", "self", ".", "branch1", "=", "nn", ".", "Sequential", "(", "\n", "BasicConv2d", "(", "1024", ",", "256", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "256", ",", "256", ",", "kernel_size", "=", "(", "1", ",", "7", ")", ",", "stride", "=", "1", ",", "padding", "=", "(", "0", ",", "3", ")", ")", ",", "\n", "BasicConv2d", "(", "256", ",", "320", ",", "kernel_size", "=", "(", "7", ",", "1", ")", ",", "stride", "=", "1", ",", "padding", "=", "(", "3", ",", "0", ")", ")", ",", "\n", "BasicConv2d", "(", "320", ",", "320", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", ")", "\n", "\n", "self", ".", "branch2", "=", "nn", ".", "MaxPool2d", "(", "3", ",", "stride", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.ReductionB.forward": [[194, 200], ["inception_v4.ReductionB.branch0", "inception_v4.ReductionB.branch1", "inception_v4.ReductionB.branch2", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x0", "=", "self", ".", "branch0", "(", "x", ")", "\n", "x1", "=", "self", ".", "branch1", "(", "x", ")", "\n", "x2", "=", "self", ".", "branch2", "(", "x", ")", "\n", "out", "=", "torch", ".", "cat", "(", "(", "x0", ",", "x1", ",", "x2", ")", ",", "1", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.InceptionC.__init__": [[203, 221], ["torch.Module.__init__", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "inception_v4.BasicConv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "InceptionC", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "branch0", "=", "BasicConv2d", "(", "1536", ",", "256", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "\n", "self", ".", "branch1_0", "=", "BasicConv2d", "(", "1536", ",", "384", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "self", ".", "branch1_1a", "=", "BasicConv2d", "(", "384", ",", "256", ",", "kernel_size", "=", "(", "1", ",", "3", ")", ",", "stride", "=", "1", ",", "padding", "=", "(", "0", ",", "1", ")", ")", "\n", "self", ".", "branch1_1b", "=", "BasicConv2d", "(", "384", ",", "256", ",", "kernel_size", "=", "(", "3", ",", "1", ")", ",", "stride", "=", "1", ",", "padding", "=", "(", "1", ",", "0", ")", ")", "\n", "\n", "self", ".", "branch2_0", "=", "BasicConv2d", "(", "1536", ",", "384", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "self", ".", "branch2_1", "=", "BasicConv2d", "(", "384", ",", "448", ",", "kernel_size", "=", "(", "3", ",", "1", ")", ",", "stride", "=", "1", ",", "padding", "=", "(", "1", ",", "0", ")", ")", "\n", "self", ".", "branch2_2", "=", "BasicConv2d", "(", "448", ",", "512", ",", "kernel_size", "=", "(", "1", ",", "3", ")", ",", "stride", "=", "1", ",", "padding", "=", "(", "0", ",", "1", ")", ")", "\n", "self", ".", "branch2_3a", "=", "BasicConv2d", "(", "512", ",", "256", ",", "kernel_size", "=", "(", "1", ",", "3", ")", ",", "stride", "=", "1", ",", "padding", "=", "(", "0", ",", "1", ")", ")", "\n", "self", ".", "branch2_3b", "=", "BasicConv2d", "(", "512", ",", "256", ",", "kernel_size", "=", "(", "3", ",", "1", ")", ",", "stride", "=", "1", ",", "padding", "=", "(", "1", ",", "0", ")", ")", "\n", "\n", "self", ".", "branch3", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "AvgPool2d", "(", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ",", "count_include_pad", "=", "False", ")", ",", "\n", "BasicConv2d", "(", "1536", ",", "256", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.InceptionC.forward": [[223, 242], ["inception_v4.InceptionC.branch0", "inception_v4.InceptionC.branch1_0", "inception_v4.InceptionC.branch1_1a", "inception_v4.InceptionC.branch1_1b", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "inception_v4.InceptionC.branch2_0", "inception_v4.InceptionC.branch2_1", "inception_v4.InceptionC.branch2_2", "inception_v4.InceptionC.branch2_3a", "inception_v4.InceptionC.branch2_3b", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "inception_v4.InceptionC.branch3", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x0", "=", "self", ".", "branch0", "(", "x", ")", "\n", "\n", "x1_0", "=", "self", ".", "branch1_0", "(", "x", ")", "\n", "x1_1a", "=", "self", ".", "branch1_1a", "(", "x1_0", ")", "\n", "x1_1b", "=", "self", ".", "branch1_1b", "(", "x1_0", ")", "\n", "x1", "=", "torch", ".", "cat", "(", "(", "x1_1a", ",", "x1_1b", ")", ",", "1", ")", "\n", "\n", "x2_0", "=", "self", ".", "branch2_0", "(", "x", ")", "\n", "x2_1", "=", "self", ".", "branch2_1", "(", "x2_0", ")", "\n", "x2_2", "=", "self", ".", "branch2_2", "(", "x2_1", ")", "\n", "x2_3a", "=", "self", ".", "branch2_3a", "(", "x2_2", ")", "\n", "x2_3b", "=", "self", ".", "branch2_3b", "(", "x2_2", ")", "\n", "x2", "=", "torch", ".", "cat", "(", "(", "x2_3a", ",", "x2_3b", ")", ",", "1", ")", "\n", "\n", "x3", "=", "self", ".", "branch3", "(", "x", ")", "\n", "\n", "out", "=", "torch", ".", "cat", "(", "(", "x0", ",", "x1", ",", "x2", ",", "x3", ")", ",", "1", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.InceptionV4.__init__": [[245, 285], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "layers.create_classifier", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.BasicConv2d", "inception_v4.Mixed3a", "inception_v4.Mixed4a", "inception_v4.Mixed5a", "inception_v4.InceptionA", "inception_v4.InceptionA", "inception_v4.InceptionA", "inception_v4.InceptionA", "inception_v4.ReductionA", "inception_v4.InceptionB", "inception_v4.InceptionB", "inception_v4.InceptionB", "inception_v4.InceptionB", "inception_v4.InceptionB", "inception_v4.InceptionB", "inception_v4.InceptionB", "inception_v4.ReductionB", "inception_v4.InceptionC", "inception_v4.InceptionC", "inception_v4.InceptionC", "dict", "dict", "dict", "dict", "dict"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["    ", "def", "__init__", "(", "self", ",", "num_classes", "=", "1000", ",", "in_chans", "=", "3", ",", "output_stride", "=", "32", ",", "drop_rate", "=", "0.", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "super", "(", "InceptionV4", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "output_stride", "==", "32", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "num_features", "=", "1536", "\n", "\n", "self", ".", "features", "=", "nn", ".", "Sequential", "(", "\n", "BasicConv2d", "(", "in_chans", ",", "32", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", ",", "\n", "BasicConv2d", "(", "32", ",", "32", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "32", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ",", "\n", "Mixed3a", "(", ")", ",", "\n", "Mixed4a", "(", ")", ",", "\n", "Mixed5a", "(", ")", ",", "\n", "InceptionA", "(", ")", ",", "\n", "InceptionA", "(", ")", ",", "\n", "InceptionA", "(", ")", ",", "\n", "InceptionA", "(", ")", ",", "\n", "ReductionA", "(", ")", ",", "# Mixed6a", "\n", "InceptionB", "(", ")", ",", "\n", "InceptionB", "(", ")", ",", "\n", "InceptionB", "(", ")", ",", "\n", "InceptionB", "(", ")", ",", "\n", "InceptionB", "(", ")", ",", "\n", "InceptionB", "(", ")", ",", "\n", "InceptionB", "(", ")", ",", "\n", "ReductionB", "(", ")", ",", "# Mixed7a", "\n", "InceptionC", "(", ")", ",", "\n", "InceptionC", "(", ")", ",", "\n", "InceptionC", "(", ")", ",", "\n", ")", "\n", "self", ".", "feature_info", "=", "[", "\n", "dict", "(", "num_chs", "=", "64", ",", "reduction", "=", "2", ",", "module", "=", "'features.2'", ")", ",", "\n", "dict", "(", "num_chs", "=", "160", ",", "reduction", "=", "4", ",", "module", "=", "'features.3'", ")", ",", "\n", "dict", "(", "num_chs", "=", "384", ",", "reduction", "=", "8", ",", "module", "=", "'features.9'", ")", ",", "\n", "dict", "(", "num_chs", "=", "1024", ",", "reduction", "=", "16", ",", "module", "=", "'features.17'", ")", ",", "\n", "dict", "(", "num_chs", "=", "1536", ",", "reduction", "=", "32", ",", "module", "=", "'features.21'", ")", ",", "\n", "]", "\n", "self", ".", "global_pool", ",", "self", ".", "last_linear", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.InceptionV4.group_matcher": [[286, 291], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^features\\.[012]\\.'", ",", "\n", "blocks", "=", "r'^features\\.(\\d+)'", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.InceptionV4.set_grad_checkpointing": [[293, 296], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "'gradient checkpointing not supported'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.InceptionV4.get_classifier": [[297, 300], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "last_linear", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.InceptionV4.reset_classifier": [[301, 305], ["layers.create_classifier"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", ",", "self", ".", "last_linear", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.InceptionV4.forward_features": [[306, 308], ["inception_v4.InceptionV4.features"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "features", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.InceptionV4.forward_head": [[309, 314], ["inception_v4.InceptionV4.global_pool", "torch.dropout", "torch.dropout", "torch.dropout", "inception_v4.InceptionV4.last_linear"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "if", "self", ".", "drop_rate", ">", "0", ":", "\n", "            ", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "drop_rate", ",", "training", "=", "self", ".", "training", ")", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "last_linear", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.InceptionV4.forward": [[315, 319], ["inception_v4.InceptionV4.forward_features", "inception_v4.InceptionV4.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4._create_inception_v4": [[321, 326], ["helpers.build_model_with_cfg", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_inception_v4", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "\n", "InceptionV4", ",", "variant", ",", "pretrained", ",", "\n", "feature_cfg", "=", "dict", "(", "flatten_sequential", "=", "True", ")", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4.inception_v4": [[328, 331], ["inception_v4._create_inception_v4"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v4._create_inception_v4"], ["", "@", "register_model", "\n", "def", "inception_v4", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_inception_v4", "(", "'inception_v4'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.PatchEmbed.__init__": [[69, 76], ["torch.Module.__init__", "layers.to_2tuple", "layers.to_2tuple", "layers.to_2tuple", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "in_chs", "=", "3", ",", "embed_dim", "=", "768", ",", "patch_size", "=", "16", ",", "stride", "=", "16", ",", "padding", "=", "0", ",", "norm_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "patch_size", "=", "to_2tuple", "(", "patch_size", ")", "\n", "stride", "=", "to_2tuple", "(", "stride", ")", "\n", "padding", "=", "to_2tuple", "(", "padding", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Conv2d", "(", "in_chs", ",", "embed_dim", ",", "kernel_size", "=", "patch_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ")", "\n", "self", ".", "norm", "=", "norm_layer", "(", "embed_dim", ")", "if", "norm_layer", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.PatchEmbed.forward": [[77, 81], ["poolformer.PatchEmbed.proj", "poolformer.PatchEmbed.norm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.GroupNorm1.__init__": [[88, 90], ["torch.GroupNorm.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "num_channels", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "1", ",", "num_channels", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.Pooling.__init__": [[93, 96], ["torch.Module.__init__", "torch.AvgPool2d", "torch.AvgPool2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pool_size", "=", "3", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "pool", "=", "nn", ".", "AvgPool2d", "(", "pool_size", ",", "stride", "=", "1", ",", "padding", "=", "pool_size", "//", "2", ",", "count_include_pad", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.Pooling.forward": [[97, 99], ["poolformer.Pooling.pool"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "pool", "(", "x", ")", "-", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.PoolFormerBlock.__init__": [[114, 134], ["torch.Module.__init__", "norm_layer", "poolformer.Pooling", "norm_layer", "layers.ConvMlp", "layers.DropPath", "torch.Identity", "torch.Identity", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "int", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "dim", ",", "pool_size", "=", "3", ",", "mlp_ratio", "=", "4.", ",", "\n", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "GroupNorm1", ",", "\n", "drop", "=", "0.", ",", "drop_path", "=", "0.", ",", "layer_scale_init_value", "=", "1e-5", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "token_mixer", "=", "Pooling", "(", "pool_size", "=", "pool_size", ")", "\n", "self", ".", "drop_path1", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "mlp", "=", "ConvMlp", "(", "dim", ",", "hidden_features", "=", "int", "(", "dim", "*", "mlp_ratio", ")", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "self", ".", "drop_path2", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "if", "layer_scale_init_value", ":", "\n", "            ", "self", ".", "layer_scale_1", "=", "nn", ".", "Parameter", "(", "layer_scale_init_value", "*", "torch", ".", "ones", "(", "dim", ")", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "layer_scale_2", "=", "nn", ".", "Parameter", "(", "layer_scale_init_value", "*", "torch", ".", "ones", "(", "dim", ")", ",", "requires_grad", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "layer_scale_1", "=", "None", "\n", "self", ".", "layer_scale_2", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.PoolFormerBlock.forward": [[135, 143], ["poolformer.PoolFormerBlock.drop_path1", "poolformer.PoolFormerBlock.drop_path2", "poolformer.PoolFormerBlock.drop_path1", "poolformer.PoolFormerBlock.drop_path2", "poolformer.PoolFormerBlock.token_mixer", "poolformer.PoolFormerBlock.mlp", "poolformer.PoolFormerBlock.layer_scale_1.unsqueeze().unsqueeze", "poolformer.PoolFormerBlock.token_mixer", "poolformer.PoolFormerBlock.layer_scale_2.unsqueeze().unsqueeze", "poolformer.PoolFormerBlock.mlp", "poolformer.PoolFormerBlock.norm1", "poolformer.PoolFormerBlock.norm2", "poolformer.PoolFormerBlock.norm1", "poolformer.PoolFormerBlock.norm2", "poolformer.PoolFormerBlock.layer_scale_1.unsqueeze", "poolformer.PoolFormerBlock.layer_scale_2.unsqueeze"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "layer_scale_1", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", "+", "self", ".", "drop_path1", "(", "self", ".", "layer_scale_1", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "*", "self", ".", "token_mixer", "(", "self", ".", "norm1", "(", "x", ")", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path2", "(", "self", ".", "layer_scale_2", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "*", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", "+", "self", ".", "drop_path1", "(", "self", ".", "token_mixer", "(", "self", ".", "norm1", "(", "x", ")", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path2", "(", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.PoolFormer.__init__": [[170, 224], ["torch.Module.__init__", "poolformer.PatchEmbed", "range", "torch.Sequential", "torch.Sequential", "norm_layer", "poolformer.PoolFormer.apply", "len", "network.append", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "poolformer.basic_blocks", "network.append", "poolformer.PatchEmbed", "len"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.basic_blocks"], ["def", "__init__", "(", "\n", "self", ",", "\n", "layers", ",", "\n", "embed_dims", "=", "(", "64", ",", "128", ",", "320", ",", "512", ")", ",", "\n", "mlp_ratios", "=", "(", "4", ",", "4", ",", "4", ",", "4", ")", ",", "\n", "downsamples", "=", "(", "True", ",", "True", ",", "True", ",", "True", ")", ",", "\n", "pool_size", "=", "3", ",", "\n", "in_chans", "=", "3", ",", "\n", "num_classes", "=", "1000", ",", "\n", "global_pool", "=", "'avg'", ",", "\n", "norm_layer", "=", "GroupNorm1", ",", "\n", "act_layer", "=", "nn", ".", "GELU", ",", "\n", "in_patch_size", "=", "7", ",", "\n", "in_stride", "=", "4", ",", "\n", "in_pad", "=", "2", ",", "\n", "down_patch_size", "=", "3", ",", "\n", "down_stride", "=", "2", ",", "\n", "down_pad", "=", "1", ",", "\n", "drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ",", "\n", "layer_scale_init_value", "=", "1e-5", ",", "\n", "**", "kwargs", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "self", ".", "num_features", "=", "embed_dims", "[", "-", "1", "]", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "self", ".", "patch_embed", "=", "PatchEmbed", "(", "\n", "patch_size", "=", "in_patch_size", ",", "stride", "=", "in_stride", ",", "padding", "=", "in_pad", ",", "\n", "in_chs", "=", "in_chans", ",", "embed_dim", "=", "embed_dims", "[", "0", "]", ")", "\n", "\n", "# set the main block in network", "\n", "network", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "layers", ")", ")", ":", "\n", "            ", "network", ".", "append", "(", "basic_blocks", "(", "\n", "embed_dims", "[", "i", "]", ",", "i", ",", "layers", ",", "\n", "pool_size", "=", "pool_size", ",", "mlp_ratio", "=", "mlp_ratios", "[", "i", "]", ",", "\n", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ",", "\n", "drop_rate", "=", "drop_rate", ",", "drop_path_rate", "=", "drop_path_rate", ",", "\n", "layer_scale_init_value", "=", "layer_scale_init_value", ")", "\n", ")", "\n", "if", "i", "<", "len", "(", "layers", ")", "-", "1", "and", "(", "downsamples", "[", "i", "]", "or", "embed_dims", "[", "i", "]", "!=", "embed_dims", "[", "i", "+", "1", "]", ")", ":", "\n", "# downsampling between stages", "\n", "                ", "network", ".", "append", "(", "PatchEmbed", "(", "\n", "in_chs", "=", "embed_dims", "[", "i", "]", ",", "embed_dim", "=", "embed_dims", "[", "i", "+", "1", "]", ",", "\n", "patch_size", "=", "down_patch_size", ",", "stride", "=", "down_stride", ",", "padding", "=", "down_pad", ")", "\n", ")", "\n", "\n", "", "", "self", ".", "network", "=", "nn", ".", "Sequential", "(", "*", "network", ")", "\n", "self", ".", "norm", "=", "norm_layer", "(", "self", ".", "num_features", ")", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "apply", "(", "self", ".", "_init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.PoolFormer._init_weights": [[226, 231], ["isinstance", "layers.trunc_normal_", "isinstance", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["", "def", "_init_weights", "(", "self", ",", "m", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "            ", "trunc_normal_", "(", "m", ".", "weight", ",", "std", "=", ".02", ")", "\n", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.PoolFormer.group_matcher": [[232, 240], ["dict"], "methods", ["None"], ["", "", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^patch_embed'", ",", "# stem and embed", "\n", "blocks", "=", "[", "\n", "(", "r'^network\\.(\\d+).*\\.proj'", ",", "(", "99999", ",", ")", ")", ",", "\n", "(", "r'^network\\.(\\d+)'", ",", "None", ")", "if", "coarse", "else", "(", "r'^network\\.(\\d+)\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^norm'", ",", "(", "99999", ",", ")", ")", "\n", "]", ",", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.PoolFormer.set_grad_checkpointing": [[243, 246], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.PoolFormer.get_classifier": [[247, 250], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.PoolFormer.reset_classifier": [[251, 256], ["torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "self", ".", "global_pool", "=", "global_pool", "\n", "", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.PoolFormer.forward_features": [[257, 262], ["poolformer.PoolFormer.patch_embed", "poolformer.PoolFormer.network", "poolformer.PoolFormer.norm"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "patch_embed", "(", "x", ")", "\n", "x", "=", "self", ".", "network", "(", "x", ")", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.PoolFormer.forward_head": [[263, 267], ["x.mean.mean.mean", "poolformer.PoolFormer.head"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "global_pool", "==", "'avg'", ":", "\n", "            ", "x", "=", "x", ".", "mean", "(", "[", "-", "2", ",", "-", "1", "]", ")", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.PoolFormer.forward": [[268, 272], ["poolformer.PoolFormer.forward_features", "poolformer.PoolFormer.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer._cfg": [[33, 41], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", ".95", ",", "'interpolation'", ":", "'bicubic'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'patch_embed.proj'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.basic_blocks": [[145, 164], ["range", "torch.Sequential", "nn.Sequential.append", "poolformer.PoolFormerBlock", "sum", "sum"], "function", ["None"], ["", "", "def", "basic_blocks", "(", "\n", "dim", ",", "index", ",", "layers", ",", "\n", "pool_size", "=", "3", ",", "mlp_ratio", "=", "4.", ",", "\n", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "GroupNorm1", ",", "\n", "drop_rate", "=", ".0", ",", "drop_path_rate", "=", "0.", ",", "\n", "layer_scale_init_value", "=", "1e-5", ",", "\n", ")", ":", "\n", "    ", "\"\"\" generate PoolFormer blocks for a stage \"\"\"", "\n", "blocks", "=", "[", "]", "\n", "for", "block_idx", "in", "range", "(", "layers", "[", "index", "]", ")", ":", "\n", "        ", "block_dpr", "=", "drop_path_rate", "*", "(", "block_idx", "+", "sum", "(", "layers", "[", ":", "index", "]", ")", ")", "/", "(", "sum", "(", "layers", ")", "-", "1", ")", "\n", "blocks", ".", "append", "(", "PoolFormerBlock", "(", "\n", "dim", ",", "pool_size", "=", "pool_size", ",", "mlp_ratio", "=", "mlp_ratio", ",", "\n", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ",", "\n", "drop", "=", "drop_rate", ",", "drop_path", "=", "block_dpr", ",", "\n", "layer_scale_init_value", "=", "layer_scale_init_value", ",", "\n", ")", ")", "\n", "", "blocks", "=", "nn", ".", "Sequential", "(", "*", "blocks", ")", "\n", "return", "blocks", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer._create_poolformer": [[274, 279], ["kwargs.get", "helpers.build_model_with_cfg", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_poolformer", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "kwargs", ".", "get", "(", "'features_only'", ",", "None", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'features_only not implemented for Vision Transformer models.'", ")", "\n", "", "model", "=", "build_model_with_cfg", "(", "PoolFormer", ",", "variant", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.poolformer_s12": [[281, 286], ["poolformer._create_poolformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer._create_poolformer"], ["", "@", "register_model", "\n", "def", "poolformer_s12", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" PoolFormer-S12 model, Params: 12M \"\"\"", "\n", "model", "=", "_create_poolformer", "(", "'poolformer_s12'", ",", "pretrained", "=", "pretrained", ",", "layers", "=", "(", "2", ",", "2", ",", "6", ",", "2", ")", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.poolformer_s24": [[288, 293], ["poolformer._create_poolformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer._create_poolformer"], ["", "@", "register_model", "\n", "def", "poolformer_s24", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" PoolFormer-S24 model, Params: 21M \"\"\"", "\n", "model", "=", "_create_poolformer", "(", "'poolformer_s24'", ",", "pretrained", "=", "pretrained", ",", "layers", "=", "(", "4", ",", "4", ",", "12", ",", "4", ")", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.poolformer_s36": [[295, 301], ["poolformer._create_poolformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer._create_poolformer"], ["", "@", "register_model", "\n", "def", "poolformer_s36", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" PoolFormer-S36 model, Params: 31M \"\"\"", "\n", "model", "=", "_create_poolformer", "(", "\n", "'poolformer_s36'", ",", "pretrained", "=", "pretrained", ",", "layers", "=", "(", "6", ",", "6", ",", "18", ",", "6", ")", ",", "layer_scale_init_value", "=", "1e-6", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.poolformer_m36": [[303, 312], ["poolformer._create_poolformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer._create_poolformer"], ["", "@", "register_model", "\n", "def", "poolformer_m36", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" PoolFormer-M36 model, Params: 56M \"\"\"", "\n", "layers", "=", "(", "6", ",", "6", ",", "18", ",", "6", ")", "\n", "embed_dims", "=", "(", "96", ",", "192", ",", "384", ",", "768", ")", "\n", "model", "=", "_create_poolformer", "(", "\n", "'poolformer_m36'", ",", "pretrained", "=", "pretrained", ",", "layers", "=", "layers", ",", "embed_dims", "=", "embed_dims", ",", "\n", "layer_scale_init_value", "=", "1e-6", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer.poolformer_m48": [[314, 323], ["poolformer._create_poolformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.poolformer._create_poolformer"], ["", "@", "register_model", "\n", "def", "poolformer_m48", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" PoolFormer-M48 model, Params: 73M \"\"\"", "\n", "layers", "=", "(", "8", ",", "8", ",", "24", ",", "8", ")", "\n", "embed_dims", "=", "(", "96", ",", "192", ",", "384", ",", "768", ")", "\n", "model", "=", "_create_poolformer", "(", "\n", "'poolformer_m48'", ",", "pretrained", "=", "pretrained", ",", "layers", "=", "layers", ",", "embed_dims", "=", "embed_dims", ",", "\n", "layer_scale_init_value", "=", "1e-6", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.RNNIdentity.__init__": [[96, 98], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "RNNIdentity", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.RNNIdentity.forward": [[99, 101], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "None", "]", ":", "\n", "        ", "return", "x", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.RNN2DBase.__init__": [[105, 155], ["torch.Module.__init__", "sequencer.RNNIdentity", "sequencer.RNNIdentity", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "ValueError", "torch.Linear", "torch.Linear", "ValueError", "torch.Linear", "torch.Linear", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "input_size", ":", "int", ",", "hidden_size", ":", "int", ",", "\n", "num_layers", ":", "int", "=", "1", ",", "bias", ":", "bool", "=", "True", ",", "bidirectional", ":", "bool", "=", "True", ",", "\n", "union", "=", "\"cat\"", ",", "with_fc", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "output_size", "=", "2", "*", "hidden_size", "if", "bidirectional", "else", "hidden_size", "\n", "self", ".", "union", "=", "union", "\n", "\n", "self", ".", "with_vertical", "=", "True", "\n", "self", ".", "with_horizontal", "=", "True", "\n", "self", ".", "with_fc", "=", "with_fc", "\n", "\n", "self", ".", "fc", "=", "None", "\n", "if", "with_fc", ":", "\n", "            ", "if", "union", "==", "\"cat\"", ":", "\n", "                ", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "2", "*", "self", ".", "output_size", ",", "input_size", ")", "\n", "", "elif", "union", "==", "\"add\"", ":", "\n", "                ", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "self", ".", "output_size", ",", "input_size", ")", "\n", "", "elif", "union", "==", "\"vertical\"", ":", "\n", "                ", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "self", ".", "output_size", ",", "input_size", ")", "\n", "self", ".", "with_horizontal", "=", "False", "\n", "", "elif", "union", "==", "\"horizontal\"", ":", "\n", "                ", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "self", ".", "output_size", ",", "input_size", ")", "\n", "self", ".", "with_vertical", "=", "False", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Unrecognized union: \"", "+", "union", ")", "\n", "", "", "elif", "union", "==", "\"cat\"", ":", "\n", "            ", "pass", "\n", "if", "2", "*", "self", ".", "output_size", "!=", "input_size", ":", "\n", "                ", "raise", "ValueError", "(", "f\"The output channel {2 * self.output_size} is different from the input channel {input_size}.\"", ")", "\n", "", "", "elif", "union", "==", "\"add\"", ":", "\n", "            ", "pass", "\n", "if", "self", ".", "output_size", "!=", "input_size", ":", "\n", "                ", "raise", "ValueError", "(", "f\"The output channel {self.output_size} is different from the input channel {input_size}.\"", ")", "\n", "", "", "elif", "union", "==", "\"vertical\"", ":", "\n", "            ", "if", "self", ".", "output_size", "!=", "input_size", ":", "\n", "                ", "raise", "ValueError", "(", "f\"The output channel {self.output_size} is different from the input channel {input_size}.\"", ")", "\n", "", "self", ".", "with_horizontal", "=", "False", "\n", "", "elif", "union", "==", "\"horizontal\"", ":", "\n", "            ", "if", "self", ".", "output_size", "!=", "input_size", ":", "\n", "                ", "raise", "ValueError", "(", "f\"The output channel {self.output_size} is different from the input channel {input_size}.\"", ")", "\n", "", "self", ".", "with_vertical", "=", "False", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unrecognized union: \"", "+", "union", ")", "\n", "\n", "", "self", ".", "rnn_v", "=", "RNNIdentity", "(", ")", "\n", "self", ".", "rnn_h", "=", "RNNIdentity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.RNN2DBase.forward": [[156, 189], ["torch.cat.permute", "torch.cat.permute", "v.permute.permute.reshape", "sequencer.RNN2DBase.rnn_v", "v.permute.permute.reshape", "v.permute.permute.permute", "torch.cat.reshape", "torch.cat.reshape", "sequencer.RNN2DBase.rnn_h", "h.reshape.reshape.reshape", "sequencer.RNN2DBase.fc", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "H", ",", "W", ",", "C", "=", "x", ".", "shape", "\n", "\n", "if", "self", ".", "with_vertical", ":", "\n", "            ", "v", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "v", "=", "v", ".", "reshape", "(", "-", "1", ",", "H", ",", "C", ")", "\n", "v", ",", "_", "=", "self", ".", "rnn_v", "(", "v", ")", "\n", "v", "=", "v", ".", "reshape", "(", "B", ",", "W", ",", "H", ",", "-", "1", ")", "\n", "v", "=", "v", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "", "else", ":", "\n", "            ", "v", "=", "None", "\n", "\n", "", "if", "self", ".", "with_horizontal", ":", "\n", "            ", "h", "=", "x", ".", "reshape", "(", "-", "1", ",", "W", ",", "C", ")", "\n", "h", ",", "_", "=", "self", ".", "rnn_h", "(", "h", ")", "\n", "h", "=", "h", ".", "reshape", "(", "B", ",", "H", ",", "W", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "h", "=", "None", "\n", "\n", "", "if", "v", "is", "not", "None", "and", "h", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "union", "==", "\"cat\"", ":", "\n", "                ", "x", "=", "torch", ".", "cat", "(", "[", "v", ",", "h", "]", ",", "dim", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "v", "+", "h", "\n", "", "", "elif", "v", "is", "not", "None", ":", "\n", "            ", "x", "=", "v", "\n", "", "elif", "h", "is", "not", "None", ":", "\n", "            ", "x", "=", "h", "\n", "\n", "", "if", "self", ".", "fc", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.LSTM2D.__init__": [[193, 202], ["sequencer.RNN2DBase.__init__", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "input_size", ":", "int", ",", "hidden_size", ":", "int", ",", "\n", "num_layers", ":", "int", "=", "1", ",", "bias", ":", "bool", "=", "True", ",", "bidirectional", ":", "bool", "=", "True", ",", "\n", "union", "=", "\"cat\"", ",", "with_fc", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "input_size", ",", "hidden_size", ",", "num_layers", ",", "bias", ",", "bidirectional", ",", "union", ",", "with_fc", ")", "\n", "if", "self", ".", "with_vertical", ":", "\n", "            ", "self", ".", "rnn_v", "=", "nn", ".", "LSTM", "(", "input_size", ",", "hidden_size", ",", "num_layers", ",", "batch_first", "=", "True", ",", "bias", "=", "bias", ",", "bidirectional", "=", "bidirectional", ")", "\n", "", "if", "self", ".", "with_horizontal", ":", "\n", "            ", "self", ".", "rnn_h", "=", "nn", ".", "LSTM", "(", "input_size", ",", "hidden_size", ",", "num_layers", ",", "batch_first", "=", "True", ",", "bias", "=", "bias", ",", "bidirectional", "=", "bidirectional", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.Sequencer2DBlock.__init__": [[205, 217], ["functools.partial", "torch.Module.__init__", "int", "norm_layer", "rnn_layer", "norm_layer", "mlp_layer", "layers.DropPath", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "hidden_size", ",", "mlp_ratio", "=", "3.0", ",", "rnn_layer", "=", "LSTM2D", ",", "mlp_layer", "=", "Mlp", ",", "\n", "norm_layer", "=", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", ",", "act_layer", "=", "nn", ".", "GELU", ",", "\n", "num_layers", "=", "1", ",", "bidirectional", "=", "True", ",", "union", "=", "\"cat\"", ",", "with_fc", "=", "True", ",", "drop", "=", "0.", ",", "drop_path", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "channels_dim", "=", "int", "(", "mlp_ratio", "*", "dim", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "rnn_tokens", "=", "rnn_layer", "(", "dim", ",", "hidden_size", ",", "num_layers", "=", "num_layers", ",", "bidirectional", "=", "bidirectional", ",", "\n", "union", "=", "union", ",", "with_fc", "=", "with_fc", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "mlp_channels", "=", "mlp_layer", "(", "dim", ",", "channels_dim", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.Sequencer2DBlock.forward": [[218, 222], ["sequencer.Sequencer2DBlock.drop_path", "sequencer.Sequencer2DBlock.drop_path", "sequencer.Sequencer2DBlock.rnn_tokens", "sequencer.Sequencer2DBlock.mlp_channels", "sequencer.Sequencer2DBlock.norm1", "sequencer.Sequencer2DBlock.norm2"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "rnn_tokens", "(", "self", ".", "norm1", "(", "x", ")", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "mlp_channels", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.PatchEmbed.forward": [[225, 233], ["sequencer.PatchEmbed.proj", "sequencer.PatchEmbed.norm", "x.permute.permute.flatten().transpose", "x.permute.permute.permute", "x.permute.permute.flatten"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "if", "self", ".", "flatten", ":", "\n", "            ", "x", "=", "x", ".", "flatten", "(", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "# BCHW -> BNC", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "# BCHW -> BHWC", "\n", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.Shuffle.__init__": [[236, 238], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.Shuffle.forward": [[239, 246], ["torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "x[].reshape.reshape", "x[].reshape"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "training", ":", "\n", "            ", "B", ",", "H", ",", "W", ",", "C", "=", "x", ".", "shape", "\n", "r", "=", "torch", ".", "randperm", "(", "H", "*", "W", ")", "\n", "x", "=", "x", ".", "reshape", "(", "B", ",", "-", "1", ",", "C", ")", "\n", "x", "=", "x", "[", ":", ",", "r", ",", ":", "]", ".", "reshape", "(", "B", ",", "H", ",", "W", ",", "-", "1", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.Downsample2D.__init__": [[249, 252], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "output_dim", ",", "patch_size", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "down", "=", "nn", ".", "Conv2d", "(", "input_dim", ",", "output_dim", ",", "kernel_size", "=", "patch_size", ",", "stride", "=", "patch_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.Downsample2D.forward": [[253, 258], ["x.permute.permute.permute", "sequencer.Downsample2D.down", "x.permute.permute.permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "x", "=", "self", ".", "down", "(", "x", ")", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.Sequencer2D.__init__": [[261, 311], ["functools.partial", "torch.Module.__init__", "sequencer.PatchEmbed", "torch.Sequential", "torch.Sequential", "norm_layer", "sequencer.Sequencer2D.init_weights", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "sequencer.get_stage", "enumerate"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.get_stage"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "num_classes", "=", "1000", ",", "\n", "img_size", "=", "224", ",", "\n", "in_chans", "=", "3", ",", "\n", "global_pool", "=", "'avg'", ",", "\n", "layers", "=", "[", "4", ",", "3", ",", "8", ",", "3", "]", ",", "\n", "patch_sizes", "=", "[", "7", ",", "2", ",", "1", ",", "1", "]", ",", "\n", "embed_dims", "=", "[", "192", ",", "384", ",", "384", ",", "384", "]", ",", "\n", "hidden_sizes", "=", "[", "48", ",", "96", ",", "96", ",", "96", "]", ",", "\n", "mlp_ratios", "=", "[", "3.0", ",", "3.0", ",", "3.0", ",", "3.0", "]", ",", "\n", "block_layer", "=", "Sequencer2DBlock", ",", "\n", "rnn_layer", "=", "LSTM2D", ",", "\n", "mlp_layer", "=", "Mlp", ",", "\n", "norm_layer", "=", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", ",", "\n", "act_layer", "=", "nn", ".", "GELU", ",", "\n", "num_rnn_layers", "=", "1", ",", "\n", "bidirectional", "=", "True", ",", "\n", "union", "=", "\"cat\"", ",", "\n", "with_fc", "=", "True", ",", "\n", "drop_rate", "=", "0.", ",", "\n", "drop_path_rate", "=", "0.", ",", "\n", "nlhb", "=", "False", ",", "\n", "stem_norm", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "global_pool", "in", "(", "''", ",", "'avg'", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "self", ".", "num_features", "=", "embed_dims", "[", "-", "1", "]", "# num_features for consistency with other models", "\n", "self", ".", "feature_dim", "=", "-", "1", "# channel dim index for feature outputs (rank 4, NHWC)", "\n", "self", ".", "embed_dims", "=", "embed_dims", "\n", "self", ".", "stem", "=", "PatchEmbed", "(", "\n", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_sizes", "[", "0", "]", ",", "in_chans", "=", "in_chans", ",", "\n", "embed_dim", "=", "embed_dims", "[", "0", "]", ",", "norm_layer", "=", "norm_layer", "if", "stem_norm", "else", "None", ",", "\n", "flatten", "=", "False", ")", "\n", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "get_stage", "(", "\n", "i", ",", "layers", ",", "patch_sizes", ",", "embed_dims", ",", "hidden_sizes", ",", "mlp_ratios", ",", "block_layer", "=", "block_layer", ",", "\n", "rnn_layer", "=", "rnn_layer", ",", "mlp_layer", "=", "mlp_layer", ",", "norm_layer", "=", "norm_layer", ",", "act_layer", "=", "act_layer", ",", "\n", "num_layers", "=", "num_rnn_layers", ",", "bidirectional", "=", "bidirectional", ",", "\n", "union", "=", "union", ",", "with_fc", "=", "with_fc", ",", "drop", "=", "drop_rate", ",", "drop_path_rate", "=", "drop_path_rate", ",", "\n", ")", "\n", "for", "i", ",", "_", "in", "enumerate", "(", "embed_dims", ")", "]", ")", "\n", "\n", "self", ".", "norm", "=", "norm_layer", "(", "embed_dims", "[", "-", "1", "]", ")", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "embed_dims", "[", "-", "1", "]", ",", "self", ".", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "init_weights", "(", "nlhb", "=", "nlhb", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.Sequencer2D.init_weights": [[312, 315], ["helpers.named_apply", "functools.partial", "math.log"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_apply"], ["", "def", "init_weights", "(", "self", ",", "nlhb", "=", "False", ")", ":", "\n", "        ", "head_bias", "=", "-", "math", ".", "log", "(", "self", ".", "num_classes", ")", "if", "nlhb", "else", "0.", "\n", "named_apply", "(", "partial", "(", "_init_weights", ",", "head_bias", "=", "head_bias", ")", ",", "module", "=", "self", ")", "# depth-first", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.Sequencer2D.group_matcher": [[316, 324], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^stem'", ",", "\n", "blocks", "=", "[", "\n", "(", "r'^blocks\\.(\\d+)\\..*\\.down'", ",", "(", "99999", ",", ")", ")", ",", "\n", "(", "r'^blocks\\.(\\d+)'", ",", "None", ")", "if", "coarse", "else", "(", "r'^blocks\\.(\\d+)\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^norm'", ",", "(", "99999", ",", ")", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.Sequencer2D.set_grad_checkpointing": [[327, 330], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "'gradient checkpointing not supported'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.Sequencer2D.get_classifier": [[331, 334], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.Sequencer2D.reset_classifier": [[335, 341], ["torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "assert", "global_pool", "in", "(", "''", ",", "'avg'", ")", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.Sequencer2D.forward_features": [[342, 347], ["sequencer.Sequencer2D.stem", "sequencer.Sequencer2D.blocks", "sequencer.Sequencer2D.norm"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "stem", "(", "x", ")", "\n", "x", "=", "self", ".", "blocks", "(", "x", ")", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.Sequencer2D.forward_head": [[348, 352], ["x.mean.mean.mean", "sequencer.Sequencer2D.head"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "global_pool", "==", "'avg'", ":", "\n", "            ", "x", "=", "x", ".", "mean", "(", "dim", "=", "(", "1", ",", "2", ")", ")", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.Sequencer2D.forward": [[353, 357], ["sequencer.Sequencer2D.forward_features", "sequencer.Sequencer2D.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer._cfg": [[23, 31], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", "DEFAULT_CROP_PCT", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'stem.proj'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer._init_weights": [[41, 72], ["isinstance", "name.startswith", "isinstance", "torch.init.zeros_", "torch.init.constant_", "layers.lecun_normal_", "isinstance", "layers.lecun_normal_", "torch.init.xavier_uniform_", "torch.init.zeros_", "torch.init.ones_", "torch.init.zeros_", "isinstance", "torch.init.zeros_", "module.parameters", "hasattr", "torch.init.normal_", "torch.init.zeros_", "math.sqrt", "torch.init.uniform_", "module.init_weights"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.lecun_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.lecun_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights"], ["def", "_init_weights", "(", "module", ":", "nn", ".", "Module", ",", "name", ":", "str", ",", "head_bias", ":", "float", "=", "0.", ",", "flax", "=", "False", ")", ":", "\n", "    ", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "        ", "if", "name", ".", "startswith", "(", "'head'", ")", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "weight", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "module", ".", "bias", ",", "head_bias", ")", "\n", "", "else", ":", "\n", "            ", "if", "flax", ":", "\n", "# Flax defaults", "\n", "                ", "lecun_normal_", "(", "module", ".", "weight", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                    ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "", "else", ":", "\n", "                ", "nn", ".", "init", ".", "xavier_uniform_", "(", "module", ".", "weight", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                    ", "if", "'mlp'", "in", "name", ":", "\n", "                        ", "nn", ".", "init", ".", "normal_", "(", "module", ".", "bias", ",", "std", "=", "1e-6", ")", "\n", "", "else", ":", "\n", "                        ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "", "", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", ":", "\n", "        ", "lecun_normal_", "(", "module", ".", "weight", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "(", "nn", ".", "LayerNorm", ",", "nn", ".", "BatchNorm2d", ",", "nn", ".", "GroupNorm", ")", ")", ":", "\n", "        ", "nn", ".", "init", ".", "ones_", "(", "module", ".", "weight", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "(", "nn", ".", "RNN", ",", "nn", ".", "GRU", ",", "nn", ".", "LSTM", ")", ")", ":", "\n", "        ", "stdv", "=", "1.0", "/", "math", ".", "sqrt", "(", "module", ".", "hidden_size", ")", "\n", "for", "weight", "in", "module", ".", "parameters", "(", ")", ":", "\n", "            ", "nn", ".", "init", ".", "uniform_", "(", "weight", ",", "-", "stdv", ",", "stdv", ")", "\n", "", "", "elif", "hasattr", "(", "module", ",", "'init_weights'", ")", ":", "\n", "        ", "module", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.get_stage": [[74, 93], ["range", "torch.Sequential", "len", "len", "len", "len", "len", "nn.Sequential.append", "nn.Sequential.append", "block_layer", "len", "sequencer.Downsample2D", "sum", "sum"], "function", ["None"], ["", "", "def", "get_stage", "(", "\n", "index", ",", "layers", ",", "patch_sizes", ",", "embed_dims", ",", "hidden_sizes", ",", "mlp_ratios", ",", "block_layer", ",", "rnn_layer", ",", "mlp_layer", ",", "\n", "norm_layer", ",", "act_layer", ",", "num_layers", ",", "bidirectional", ",", "union", ",", "\n", "with_fc", ",", "drop", "=", "0.", ",", "drop_path_rate", "=", "0.", ",", "**", "kwargs", ")", ":", "\n", "    ", "assert", "len", "(", "layers", ")", "==", "len", "(", "patch_sizes", ")", "==", "len", "(", "embed_dims", ")", "==", "len", "(", "hidden_sizes", ")", "==", "len", "(", "mlp_ratios", ")", "\n", "blocks", "=", "[", "]", "\n", "for", "block_idx", "in", "range", "(", "layers", "[", "index", "]", ")", ":", "\n", "        ", "drop_path", "=", "drop_path_rate", "*", "(", "block_idx", "+", "sum", "(", "layers", "[", ":", "index", "]", ")", ")", "/", "(", "sum", "(", "layers", ")", "-", "1", ")", "\n", "blocks", ".", "append", "(", "block_layer", "(", "\n", "embed_dims", "[", "index", "]", ",", "hidden_sizes", "[", "index", "]", ",", "mlp_ratio", "=", "mlp_ratios", "[", "index", "]", ",", "\n", "rnn_layer", "=", "rnn_layer", ",", "mlp_layer", "=", "mlp_layer", ",", "norm_layer", "=", "norm_layer", ",", "act_layer", "=", "act_layer", ",", "\n", "num_layers", "=", "num_layers", ",", "bidirectional", "=", "bidirectional", ",", "union", "=", "union", ",", "with_fc", "=", "with_fc", ",", "\n", "drop", "=", "drop", ",", "drop_path", "=", "drop_path", ")", ")", "\n", "\n", "", "if", "index", "<", "len", "(", "embed_dims", ")", "-", "1", ":", "\n", "        ", "blocks", ".", "append", "(", "Downsample2D", "(", "embed_dims", "[", "index", "]", ",", "embed_dims", "[", "index", "+", "1", "]", ",", "patch_sizes", "[", "index", "+", "1", "]", ")", ")", "\n", "\n", "", "blocks", "=", "nn", ".", "Sequential", "(", "*", "blocks", ")", "\n", "return", "blocks", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer._create_sequencer2d": [[359, 365], ["kwargs.get", "helpers.build_model_with_cfg", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_sequencer2d", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "kwargs", ".", "get", "(", "'features_only'", ",", "None", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'features_only not implemented for Sequencer2D models.'", ")", "\n", "\n", "", "model", "=", "build_model_with_cfg", "(", "Sequencer2D", ",", "variant", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.sequencer2d_s": [[369, 384], ["dict", "sequencer._create_sequencer2d"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer._create_sequencer2d"], ["", "@", "register_model", "\n", "def", "sequencer2d_s", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "layers", "=", "[", "4", ",", "3", ",", "8", ",", "3", "]", ",", "\n", "patch_sizes", "=", "[", "7", ",", "2", ",", "1", ",", "1", "]", ",", "\n", "embed_dims", "=", "[", "192", ",", "384", ",", "384", ",", "384", "]", ",", "\n", "hidden_sizes", "=", "[", "48", ",", "96", ",", "96", ",", "96", "]", ",", "\n", "mlp_ratios", "=", "[", "3.0", ",", "3.0", ",", "3.0", ",", "3.0", "]", ",", "\n", "rnn_layer", "=", "LSTM2D", ",", "\n", "bidirectional", "=", "True", ",", "\n", "union", "=", "\"cat\"", ",", "\n", "with_fc", "=", "True", ",", "\n", "**", "kwargs", ")", "\n", "model", "=", "_create_sequencer2d", "(", "'sequencer2d_s'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.sequencer2d_m": [[386, 401], ["dict", "sequencer._create_sequencer2d"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer._create_sequencer2d"], ["", "@", "register_model", "\n", "def", "sequencer2d_m", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "layers", "=", "[", "4", ",", "3", ",", "14", ",", "3", "]", ",", "\n", "patch_sizes", "=", "[", "7", ",", "2", ",", "1", ",", "1", "]", ",", "\n", "embed_dims", "=", "[", "192", ",", "384", ",", "384", ",", "384", "]", ",", "\n", "hidden_sizes", "=", "[", "48", ",", "96", ",", "96", ",", "96", "]", ",", "\n", "mlp_ratios", "=", "[", "3.0", ",", "3.0", ",", "3.0", ",", "3.0", "]", ",", "\n", "rnn_layer", "=", "LSTM2D", ",", "\n", "bidirectional", "=", "True", ",", "\n", "union", "=", "\"cat\"", ",", "\n", "with_fc", "=", "True", ",", "\n", "**", "kwargs", ")", "\n", "model", "=", "_create_sequencer2d", "(", "'sequencer2d_m'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer.sequencer2d_l": [[403, 418], ["dict", "sequencer._create_sequencer2d"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sequencer._create_sequencer2d"], ["", "@", "register_model", "\n", "def", "sequencer2d_l", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "layers", "=", "[", "8", ",", "8", ",", "16", ",", "4", "]", ",", "\n", "patch_sizes", "=", "[", "7", ",", "2", ",", "1", ",", "1", "]", ",", "\n", "embed_dims", "=", "[", "192", ",", "384", ",", "384", ",", "384", "]", ",", "\n", "hidden_sizes", "=", "[", "48", ",", "96", ",", "96", ",", "96", "]", ",", "\n", "mlp_ratios", "=", "[", "3.0", ",", "3.0", ",", "3.0", ",", "3.0", "]", ",", "\n", "rnn_layer", "=", "LSTM2D", ",", "\n", "bidirectional", "=", "True", ",", "\n", "union", "=", "\"cat\"", ",", "\n", "with_fc", "=", "True", ",", "\n", "**", "kwargs", ")", "\n", "model", "=", "_create_sequencer2d", "(", "'sequencer2d_l'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionA.__init__": [[54, 68], ["torch.Module.__init__", "conv_block", "conv_block", "conv_block", "conv_block", "conv_block", "conv_block", "conv_block"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "pool_features", ",", "conv_block", "=", "None", ")", ":", "\n", "        ", "super", "(", "InceptionA", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "conv_block", "is", "None", ":", "\n", "            ", "conv_block", "=", "BasicConv2d", "\n", "", "self", ".", "branch1x1", "=", "conv_block", "(", "in_channels", ",", "64", ",", "kernel_size", "=", "1", ")", "\n", "\n", "self", ".", "branch5x5_1", "=", "conv_block", "(", "in_channels", ",", "48", ",", "kernel_size", "=", "1", ")", "\n", "self", ".", "branch5x5_2", "=", "conv_block", "(", "48", ",", "64", ",", "kernel_size", "=", "5", ",", "padding", "=", "2", ")", "\n", "\n", "self", ".", "branch3x3dbl_1", "=", "conv_block", "(", "in_channels", ",", "64", ",", "kernel_size", "=", "1", ")", "\n", "self", ".", "branch3x3dbl_2", "=", "conv_block", "(", "64", ",", "96", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ")", "\n", "self", ".", "branch3x3dbl_3", "=", "conv_block", "(", "96", ",", "96", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ")", "\n", "\n", "self", ".", "branch_pool", "=", "conv_block", "(", "in_channels", ",", "pool_features", ",", "kernel_size", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionA._forward": [[69, 84], ["inception_v3.InceptionA.branch1x1", "inception_v3.InceptionA.branch5x5_1", "inception_v3.InceptionA.branch5x5_2", "inception_v3.InceptionA.branch3x3dbl_1", "inception_v3.InceptionA.branch3x3dbl_2", "inception_v3.InceptionA.branch3x3dbl_3", "torch.avg_pool2d", "torch.avg_pool2d", "torch.avg_pool2d", "inception_v3.InceptionA.branch_pool"], "methods", ["None"], ["", "def", "_forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "branch1x1", "=", "self", ".", "branch1x1", "(", "x", ")", "\n", "\n", "branch5x5", "=", "self", ".", "branch5x5_1", "(", "x", ")", "\n", "branch5x5", "=", "self", ".", "branch5x5_2", "(", "branch5x5", ")", "\n", "\n", "branch3x3dbl", "=", "self", ".", "branch3x3dbl_1", "(", "x", ")", "\n", "branch3x3dbl", "=", "self", ".", "branch3x3dbl_2", "(", "branch3x3dbl", ")", "\n", "branch3x3dbl", "=", "self", ".", "branch3x3dbl_3", "(", "branch3x3dbl", ")", "\n", "\n", "branch_pool", "=", "F", ".", "avg_pool2d", "(", "x", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "branch_pool", "=", "self", ".", "branch_pool", "(", "branch_pool", ")", "\n", "\n", "outputs", "=", "[", "branch1x1", ",", "branch5x5", ",", "branch3x3dbl", ",", "branch_pool", "]", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionA.forward": [[85, 88], ["inception_v3.InceptionA._forward", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.ParallelBlock._forward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "outputs", "=", "self", ".", "_forward", "(", "x", ")", "\n", "return", "torch", ".", "cat", "(", "outputs", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionB.__init__": [[92, 101], ["torch.Module.__init__", "conv_block", "conv_block", "conv_block", "conv_block"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "conv_block", "=", "None", ")", ":", "\n", "        ", "super", "(", "InceptionB", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "conv_block", "is", "None", ":", "\n", "            ", "conv_block", "=", "BasicConv2d", "\n", "", "self", ".", "branch3x3", "=", "conv_block", "(", "in_channels", ",", "384", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", "\n", "self", ".", "branch3x3dbl_1", "=", "conv_block", "(", "in_channels", ",", "64", ",", "kernel_size", "=", "1", ")", "\n", "self", ".", "branch3x3dbl_2", "=", "conv_block", "(", "64", ",", "96", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ")", "\n", "self", ".", "branch3x3dbl_3", "=", "conv_block", "(", "96", ",", "96", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionB._forward": [[102, 113], ["inception_v3.InceptionB.branch3x3", "inception_v3.InceptionB.branch3x3dbl_1", "inception_v3.InceptionB.branch3x3dbl_2", "inception_v3.InceptionB.branch3x3dbl_3", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d"], "methods", ["None"], ["", "def", "_forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "branch3x3", "=", "self", ".", "branch3x3", "(", "x", ")", "\n", "\n", "branch3x3dbl", "=", "self", ".", "branch3x3dbl_1", "(", "x", ")", "\n", "branch3x3dbl", "=", "self", ".", "branch3x3dbl_2", "(", "branch3x3dbl", ")", "\n", "branch3x3dbl", "=", "self", ".", "branch3x3dbl_3", "(", "branch3x3dbl", ")", "\n", "\n", "branch_pool", "=", "F", ".", "max_pool2d", "(", "x", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", "\n", "outputs", "=", "[", "branch3x3", ",", "branch3x3dbl", ",", "branch_pool", "]", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionB.forward": [[114, 117], ["inception_v3.InceptionB._forward", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.ParallelBlock._forward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "outputs", "=", "self", ".", "_forward", "(", "x", ")", "\n", "return", "torch", ".", "cat", "(", "outputs", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionC.__init__": [[121, 139], ["torch.Module.__init__", "conv_block", "conv_block", "conv_block", "conv_block", "conv_block", "conv_block", "conv_block", "conv_block", "conv_block", "conv_block"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "channels_7x7", ",", "conv_block", "=", "None", ")", ":", "\n", "        ", "super", "(", "InceptionC", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "conv_block", "is", "None", ":", "\n", "            ", "conv_block", "=", "BasicConv2d", "\n", "", "self", ".", "branch1x1", "=", "conv_block", "(", "in_channels", ",", "192", ",", "kernel_size", "=", "1", ")", "\n", "\n", "c7", "=", "channels_7x7", "\n", "self", ".", "branch7x7_1", "=", "conv_block", "(", "in_channels", ",", "c7", ",", "kernel_size", "=", "1", ")", "\n", "self", ".", "branch7x7_2", "=", "conv_block", "(", "c7", ",", "c7", ",", "kernel_size", "=", "(", "1", ",", "7", ")", ",", "padding", "=", "(", "0", ",", "3", ")", ")", "\n", "self", ".", "branch7x7_3", "=", "conv_block", "(", "c7", ",", "192", ",", "kernel_size", "=", "(", "7", ",", "1", ")", ",", "padding", "=", "(", "3", ",", "0", ")", ")", "\n", "\n", "self", ".", "branch7x7dbl_1", "=", "conv_block", "(", "in_channels", ",", "c7", ",", "kernel_size", "=", "1", ")", "\n", "self", ".", "branch7x7dbl_2", "=", "conv_block", "(", "c7", ",", "c7", ",", "kernel_size", "=", "(", "7", ",", "1", ")", ",", "padding", "=", "(", "3", ",", "0", ")", ")", "\n", "self", ".", "branch7x7dbl_3", "=", "conv_block", "(", "c7", ",", "c7", ",", "kernel_size", "=", "(", "1", ",", "7", ")", ",", "padding", "=", "(", "0", ",", "3", ")", ")", "\n", "self", ".", "branch7x7dbl_4", "=", "conv_block", "(", "c7", ",", "c7", ",", "kernel_size", "=", "(", "7", ",", "1", ")", ",", "padding", "=", "(", "3", ",", "0", ")", ")", "\n", "self", ".", "branch7x7dbl_5", "=", "conv_block", "(", "c7", ",", "192", ",", "kernel_size", "=", "(", "1", ",", "7", ")", ",", "padding", "=", "(", "0", ",", "3", ")", ")", "\n", "\n", "self", ".", "branch_pool", "=", "conv_block", "(", "in_channels", ",", "192", ",", "kernel_size", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionC._forward": [[140, 158], ["inception_v3.InceptionC.branch1x1", "inception_v3.InceptionC.branch7x7_1", "inception_v3.InceptionC.branch7x7_2", "inception_v3.InceptionC.branch7x7_3", "inception_v3.InceptionC.branch7x7dbl_1", "inception_v3.InceptionC.branch7x7dbl_2", "inception_v3.InceptionC.branch7x7dbl_3", "inception_v3.InceptionC.branch7x7dbl_4", "inception_v3.InceptionC.branch7x7dbl_5", "torch.avg_pool2d", "torch.avg_pool2d", "torch.avg_pool2d", "inception_v3.InceptionC.branch_pool"], "methods", ["None"], ["", "def", "_forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "branch1x1", "=", "self", ".", "branch1x1", "(", "x", ")", "\n", "\n", "branch7x7", "=", "self", ".", "branch7x7_1", "(", "x", ")", "\n", "branch7x7", "=", "self", ".", "branch7x7_2", "(", "branch7x7", ")", "\n", "branch7x7", "=", "self", ".", "branch7x7_3", "(", "branch7x7", ")", "\n", "\n", "branch7x7dbl", "=", "self", ".", "branch7x7dbl_1", "(", "x", ")", "\n", "branch7x7dbl", "=", "self", ".", "branch7x7dbl_2", "(", "branch7x7dbl", ")", "\n", "branch7x7dbl", "=", "self", ".", "branch7x7dbl_3", "(", "branch7x7dbl", ")", "\n", "branch7x7dbl", "=", "self", ".", "branch7x7dbl_4", "(", "branch7x7dbl", ")", "\n", "branch7x7dbl", "=", "self", ".", "branch7x7dbl_5", "(", "branch7x7dbl", ")", "\n", "\n", "branch_pool", "=", "F", ".", "avg_pool2d", "(", "x", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "branch_pool", "=", "self", ".", "branch_pool", "(", "branch_pool", ")", "\n", "\n", "outputs", "=", "[", "branch1x1", ",", "branch7x7", ",", "branch7x7dbl", ",", "branch_pool", "]", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionC.forward": [[159, 162], ["inception_v3.InceptionC._forward", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.ParallelBlock._forward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "outputs", "=", "self", ".", "_forward", "(", "x", ")", "\n", "return", "torch", ".", "cat", "(", "outputs", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionD.__init__": [[166, 177], ["torch.Module.__init__", "conv_block", "conv_block", "conv_block", "conv_block", "conv_block", "conv_block"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "conv_block", "=", "None", ")", ":", "\n", "        ", "super", "(", "InceptionD", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "conv_block", "is", "None", ":", "\n", "            ", "conv_block", "=", "BasicConv2d", "\n", "", "self", ".", "branch3x3_1", "=", "conv_block", "(", "in_channels", ",", "192", ",", "kernel_size", "=", "1", ")", "\n", "self", ".", "branch3x3_2", "=", "conv_block", "(", "192", ",", "320", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", "\n", "self", ".", "branch7x7x3_1", "=", "conv_block", "(", "in_channels", ",", "192", ",", "kernel_size", "=", "1", ")", "\n", "self", ".", "branch7x7x3_2", "=", "conv_block", "(", "192", ",", "192", ",", "kernel_size", "=", "(", "1", ",", "7", ")", ",", "padding", "=", "(", "0", ",", "3", ")", ")", "\n", "self", ".", "branch7x7x3_3", "=", "conv_block", "(", "192", ",", "192", ",", "kernel_size", "=", "(", "7", ",", "1", ")", ",", "padding", "=", "(", "3", ",", "0", ")", ")", "\n", "self", ".", "branch7x7x3_4", "=", "conv_block", "(", "192", ",", "192", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionD._forward": [[178, 190], ["inception_v3.InceptionD.branch3x3_1", "inception_v3.InceptionD.branch3x3_2", "inception_v3.InceptionD.branch7x7x3_1", "inception_v3.InceptionD.branch7x7x3_2", "inception_v3.InceptionD.branch7x7x3_3", "inception_v3.InceptionD.branch7x7x3_4", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d"], "methods", ["None"], ["", "def", "_forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "branch3x3", "=", "self", ".", "branch3x3_1", "(", "x", ")", "\n", "branch3x3", "=", "self", ".", "branch3x3_2", "(", "branch3x3", ")", "\n", "\n", "branch7x7x3", "=", "self", ".", "branch7x7x3_1", "(", "x", ")", "\n", "branch7x7x3", "=", "self", ".", "branch7x7x3_2", "(", "branch7x7x3", ")", "\n", "branch7x7x3", "=", "self", ".", "branch7x7x3_3", "(", "branch7x7x3", ")", "\n", "branch7x7x3", "=", "self", ".", "branch7x7x3_4", "(", "branch7x7x3", ")", "\n", "\n", "branch_pool", "=", "F", ".", "max_pool2d", "(", "x", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", "outputs", "=", "[", "branch3x3", ",", "branch7x7x3", ",", "branch_pool", "]", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionD.forward": [[191, 194], ["inception_v3.InceptionD._forward", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.ParallelBlock._forward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "outputs", "=", "self", ".", "_forward", "(", "x", ")", "\n", "return", "torch", ".", "cat", "(", "outputs", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionE.__init__": [[198, 214], ["torch.Module.__init__", "conv_block", "conv_block", "conv_block", "conv_block", "conv_block", "conv_block", "conv_block", "conv_block", "conv_block"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "conv_block", "=", "None", ")", ":", "\n", "        ", "super", "(", "InceptionE", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "conv_block", "is", "None", ":", "\n", "            ", "conv_block", "=", "BasicConv2d", "\n", "", "self", ".", "branch1x1", "=", "conv_block", "(", "in_channels", ",", "320", ",", "kernel_size", "=", "1", ")", "\n", "\n", "self", ".", "branch3x3_1", "=", "conv_block", "(", "in_channels", ",", "384", ",", "kernel_size", "=", "1", ")", "\n", "self", ".", "branch3x3_2a", "=", "conv_block", "(", "384", ",", "384", ",", "kernel_size", "=", "(", "1", ",", "3", ")", ",", "padding", "=", "(", "0", ",", "1", ")", ")", "\n", "self", ".", "branch3x3_2b", "=", "conv_block", "(", "384", ",", "384", ",", "kernel_size", "=", "(", "3", ",", "1", ")", ",", "padding", "=", "(", "1", ",", "0", ")", ")", "\n", "\n", "self", ".", "branch3x3dbl_1", "=", "conv_block", "(", "in_channels", ",", "448", ",", "kernel_size", "=", "1", ")", "\n", "self", ".", "branch3x3dbl_2", "=", "conv_block", "(", "448", ",", "384", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ")", "\n", "self", ".", "branch3x3dbl_3a", "=", "conv_block", "(", "384", ",", "384", ",", "kernel_size", "=", "(", "1", ",", "3", ")", ",", "padding", "=", "(", "0", ",", "1", ")", ")", "\n", "self", ".", "branch3x3dbl_3b", "=", "conv_block", "(", "384", ",", "384", ",", "kernel_size", "=", "(", "3", ",", "1", ")", ",", "padding", "=", "(", "1", ",", "0", ")", ")", "\n", "\n", "self", ".", "branch_pool", "=", "conv_block", "(", "in_channels", ",", "192", ",", "kernel_size", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionE._forward": [[215, 238], ["inception_v3.InceptionE.branch1x1", "inception_v3.InceptionE.branch3x3_1", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "inception_v3.InceptionE.branch3x3dbl_1", "inception_v3.InceptionE.branch3x3dbl_2", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.avg_pool2d", "torch.avg_pool2d", "torch.avg_pool2d", "inception_v3.InceptionE.branch_pool", "inception_v3.InceptionE.branch3x3_2a", "inception_v3.InceptionE.branch3x3_2b", "inception_v3.InceptionE.branch3x3dbl_3a", "inception_v3.InceptionE.branch3x3dbl_3b"], "methods", ["None"], ["", "def", "_forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "branch1x1", "=", "self", ".", "branch1x1", "(", "x", ")", "\n", "\n", "branch3x3", "=", "self", ".", "branch3x3_1", "(", "x", ")", "\n", "branch3x3", "=", "[", "\n", "self", ".", "branch3x3_2a", "(", "branch3x3", ")", ",", "\n", "self", ".", "branch3x3_2b", "(", "branch3x3", ")", ",", "\n", "]", "\n", "branch3x3", "=", "torch", ".", "cat", "(", "branch3x3", ",", "1", ")", "\n", "\n", "branch3x3dbl", "=", "self", ".", "branch3x3dbl_1", "(", "x", ")", "\n", "branch3x3dbl", "=", "self", ".", "branch3x3dbl_2", "(", "branch3x3dbl", ")", "\n", "branch3x3dbl", "=", "[", "\n", "self", ".", "branch3x3dbl_3a", "(", "branch3x3dbl", ")", ",", "\n", "self", ".", "branch3x3dbl_3b", "(", "branch3x3dbl", ")", ",", "\n", "]", "\n", "branch3x3dbl", "=", "torch", ".", "cat", "(", "branch3x3dbl", ",", "1", ")", "\n", "\n", "branch_pool", "=", "F", ".", "avg_pool2d", "(", "x", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "branch_pool", "=", "self", ".", "branch_pool", "(", "branch_pool", ")", "\n", "\n", "outputs", "=", "[", "branch1x1", ",", "branch3x3", ",", "branch3x3dbl", ",", "branch_pool", "]", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionE.forward": [[239, 242], ["inception_v3.InceptionE._forward", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.ParallelBlock._forward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "outputs", "=", "self", ".", "_forward", "(", "x", ")", "\n", "return", "torch", ".", "cat", "(", "outputs", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionAux.__init__": [[246, 255], ["torch.Module.__init__", "conv_block", "conv_block", "layers.Linear"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "num_classes", ",", "conv_block", "=", "None", ")", ":", "\n", "        ", "super", "(", "InceptionAux", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "conv_block", "is", "None", ":", "\n", "            ", "conv_block", "=", "BasicConv2d", "\n", "", "self", ".", "conv0", "=", "conv_block", "(", "in_channels", ",", "128", ",", "kernel_size", "=", "1", ")", "\n", "self", ".", "conv1", "=", "conv_block", "(", "128", ",", "768", ",", "kernel_size", "=", "5", ")", "\n", "self", ".", "conv1", ".", "stddev", "=", "0.01", "\n", "self", ".", "fc", "=", "Linear", "(", "768", ",", "num_classes", ")", "\n", "self", ".", "fc", ".", "stddev", "=", "0.001", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionAux.forward": [[256, 272], ["torch.avg_pool2d", "torch.avg_pool2d", "torch.avg_pool2d", "inception_v3.InceptionAux.conv0", "inception_v3.InceptionAux.conv1", "torch.adaptive_avg_pool2d", "torch.adaptive_avg_pool2d", "torch.adaptive_avg_pool2d", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "inception_v3.InceptionAux.fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# N x 768 x 17 x 17", "\n", "        ", "x", "=", "F", ".", "avg_pool2d", "(", "x", ",", "kernel_size", "=", "5", ",", "stride", "=", "3", ")", "\n", "# N x 768 x 5 x 5", "\n", "x", "=", "self", ".", "conv0", "(", "x", ")", "\n", "# N x 128 x 5 x 5", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "# N x 768 x 1 x 1", "\n", "# Adaptive average pooling", "\n", "x", "=", "F", ".", "adaptive_avg_pool2d", "(", "x", ",", "(", "1", ",", "1", ")", ")", "\n", "# N x 768 x 1 x 1", "\n", "x", "=", "torch", ".", "flatten", "(", "x", ",", "1", ")", "\n", "# N x 768", "\n", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "# N x 1000", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.BasicConv2d.__init__": [[276, 280], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "BasicConv2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "bias", "=", "False", ",", "**", "kwargs", ")", "\n", "self", ".", "bn", "=", "nn", ".", "BatchNorm2d", "(", "out_channels", ",", "eps", "=", "0.001", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.BasicConv2d.forward": [[281, 285], ["inception_v3.BasicConv2d.conv", "inception_v3.BasicConv2d.bn", "torch.relu", "torch.relu", "torch.relu"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "x", "=", "self", ".", "bn", "(", "x", ")", "\n", "return", "F", ".", "relu", "(", "x", ",", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionV3.__init__": [[292, 338], ["torch.Module.__init__", "inception_v3.BasicConv2d", "inception_v3.BasicConv2d", "inception_v3.BasicConv2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "inception_v3.BasicConv2d", "inception_v3.BasicConv2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "inception_v3.InceptionA", "inception_v3.InceptionA", "inception_v3.InceptionA", "inception_v3.InceptionB", "inception_v3.InceptionC", "inception_v3.InceptionC", "inception_v3.InceptionC", "inception_v3.InceptionC", "inception_v3.InceptionD", "inception_v3.InceptionE", "inception_v3.InceptionE", "layers.create_classifier", "inception_v3.InceptionV3.modules", "inception_v3.InceptionAux", "dict", "dict", "dict", "dict", "dict", "isinstance", "isinstance", "layers.trunc_normal_", "isinstance", "hasattr", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["def", "__init__", "(", "self", ",", "num_classes", "=", "1000", ",", "in_chans", "=", "3", ",", "drop_rate", "=", "0.", ",", "global_pool", "=", "'avg'", ",", "aux_logits", "=", "False", ")", ":", "\n", "        ", "super", "(", "InceptionV3", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "self", ".", "aux_logits", "=", "aux_logits", "\n", "\n", "self", ".", "Conv2d_1a_3x3", "=", "BasicConv2d", "(", "in_chans", ",", "32", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", "self", ".", "Conv2d_2a_3x3", "=", "BasicConv2d", "(", "32", ",", "32", ",", "kernel_size", "=", "3", ")", "\n", "self", ".", "Conv2d_2b_3x3", "=", "BasicConv2d", "(", "32", ",", "64", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ")", "\n", "self", ".", "Pool1", "=", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", "self", ".", "Conv2d_3b_1x1", "=", "BasicConv2d", "(", "64", ",", "80", ",", "kernel_size", "=", "1", ")", "\n", "self", ".", "Conv2d_4a_3x3", "=", "BasicConv2d", "(", "80", ",", "192", ",", "kernel_size", "=", "3", ")", "\n", "self", ".", "Pool2", "=", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", "self", ".", "Mixed_5b", "=", "InceptionA", "(", "192", ",", "pool_features", "=", "32", ")", "\n", "self", ".", "Mixed_5c", "=", "InceptionA", "(", "256", ",", "pool_features", "=", "64", ")", "\n", "self", ".", "Mixed_5d", "=", "InceptionA", "(", "288", ",", "pool_features", "=", "64", ")", "\n", "self", ".", "Mixed_6a", "=", "InceptionB", "(", "288", ")", "\n", "self", ".", "Mixed_6b", "=", "InceptionC", "(", "768", ",", "channels_7x7", "=", "128", ")", "\n", "self", ".", "Mixed_6c", "=", "InceptionC", "(", "768", ",", "channels_7x7", "=", "160", ")", "\n", "self", ".", "Mixed_6d", "=", "InceptionC", "(", "768", ",", "channels_7x7", "=", "160", ")", "\n", "self", ".", "Mixed_6e", "=", "InceptionC", "(", "768", ",", "channels_7x7", "=", "192", ")", "\n", "if", "aux_logits", ":", "\n", "            ", "self", ".", "AuxLogits", "=", "InceptionAux", "(", "768", ",", "num_classes", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "AuxLogits", "=", "None", "\n", "", "self", ".", "Mixed_7a", "=", "InceptionD", "(", "768", ")", "\n", "self", ".", "Mixed_7b", "=", "InceptionE", "(", "1280", ")", "\n", "self", ".", "Mixed_7c", "=", "InceptionE", "(", "2048", ")", "\n", "self", ".", "feature_info", "=", "[", "\n", "dict", "(", "num_chs", "=", "64", ",", "reduction", "=", "2", ",", "module", "=", "'Conv2d_2b_3x3'", ")", ",", "\n", "dict", "(", "num_chs", "=", "192", ",", "reduction", "=", "4", ",", "module", "=", "'Conv2d_4a_3x3'", ")", ",", "\n", "dict", "(", "num_chs", "=", "288", ",", "reduction", "=", "8", ",", "module", "=", "'Mixed_5d'", ")", ",", "\n", "dict", "(", "num_chs", "=", "768", ",", "reduction", "=", "16", ",", "module", "=", "'Mixed_6e'", ")", ",", "\n", "dict", "(", "num_chs", "=", "2048", ",", "reduction", "=", "32", ",", "module", "=", "'Mixed_7c'", ")", ",", "\n", "]", "\n", "\n", "self", ".", "num_features", "=", "2048", "\n", "self", ".", "global_pool", ",", "self", ".", "fc", "=", "create_classifier", "(", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", "or", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "stddev", "=", "m", ".", "stddev", "if", "hasattr", "(", "m", ",", "'stddev'", ")", "else", "0.1", "\n", "trunc_normal_", "(", "m", ".", "weight", ",", "std", "=", "stddev", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionV3.group_matcher": [[339, 355], ["module_map.pop", "any", "enumerate", "any", "helpers.flatten_modules", "name.startswith", "module_map.keys", "float", "inception_v3.InceptionV3.named_children", "name.startswith", "tuple", "name.split", "len"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.flatten_modules"], ["", "", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "module_map", "=", "{", "k", ":", "i", "for", "i", ",", "(", "k", ",", "_", ")", "in", "enumerate", "(", "flatten_modules", "(", "self", ".", "named_children", "(", ")", ",", "prefix", "=", "(", ")", ")", ")", "}", "\n", "module_map", ".", "pop", "(", "(", "'fc'", ",", ")", ")", "\n", "\n", "def", "_matcher", "(", "name", ")", ":", "\n", "            ", "if", "any", "(", "[", "name", ".", "startswith", "(", "n", ")", "for", "n", "in", "(", "'Conv2d_1'", ",", "'Conv2d_2'", ")", "]", ")", ":", "\n", "                ", "return", "0", "\n", "", "elif", "any", "(", "[", "name", ".", "startswith", "(", "n", ")", "for", "n", "in", "(", "'Conv2d_3'", ",", "'Conv2d_4'", ")", "]", ")", ":", "\n", "                ", "return", "1", "\n", "", "else", ":", "\n", "                ", "for", "k", "in", "module_map", ".", "keys", "(", ")", ":", "\n", "                    ", "if", "k", "==", "tuple", "(", "name", ".", "split", "(", "'.'", ")", "[", ":", "len", "(", "k", ")", "]", ")", ":", "\n", "                        ", "return", "module_map", "[", "k", "]", "\n", "", "", "return", "float", "(", "'inf'", ")", "\n", "", "", "return", "_matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionV3.set_grad_checkpointing": [[356, 359], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "'gradient checkpointing not supported'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionV3.get_classifier": [[360, 363], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionV3.reset_classifier": [[364, 367], ["layers.create_classifier"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", ",", "self", ".", "fc", "=", "create_classifier", "(", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionV3.forward_preaux": [[368, 385], ["inception_v3.InceptionV3.Conv2d_1a_3x3", "inception_v3.InceptionV3.Conv2d_2a_3x3", "inception_v3.InceptionV3.Conv2d_2b_3x3", "inception_v3.InceptionV3.Pool1", "inception_v3.InceptionV3.Conv2d_3b_1x1", "inception_v3.InceptionV3.Conv2d_4a_3x3", "inception_v3.InceptionV3.Pool2", "inception_v3.InceptionV3.Mixed_5b", "inception_v3.InceptionV3.Mixed_5c", "inception_v3.InceptionV3.Mixed_5d", "inception_v3.InceptionV3.Mixed_6a", "inception_v3.InceptionV3.Mixed_6b", "inception_v3.InceptionV3.Mixed_6c", "inception_v3.InceptionV3.Mixed_6d", "inception_v3.InceptionV3.Mixed_6e"], "methods", ["None"], ["", "def", "forward_preaux", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "Conv2d_1a_3x3", "(", "x", ")", "# N x 32 x 149 x 149", "\n", "x", "=", "self", ".", "Conv2d_2a_3x3", "(", "x", ")", "# N x 32 x 147 x 147", "\n", "x", "=", "self", ".", "Conv2d_2b_3x3", "(", "x", ")", "# N x 64 x 147 x 147", "\n", "x", "=", "self", ".", "Pool1", "(", "x", ")", "# N x 64 x 73 x 73", "\n", "x", "=", "self", ".", "Conv2d_3b_1x1", "(", "x", ")", "# N x 80 x 73 x 73", "\n", "x", "=", "self", ".", "Conv2d_4a_3x3", "(", "x", ")", "# N x 192 x 71 x 71", "\n", "x", "=", "self", ".", "Pool2", "(", "x", ")", "# N x 192 x 35 x 35", "\n", "x", "=", "self", ".", "Mixed_5b", "(", "x", ")", "# N x 256 x 35 x 35", "\n", "x", "=", "self", ".", "Mixed_5c", "(", "x", ")", "# N x 288 x 35 x 35", "\n", "x", "=", "self", ".", "Mixed_5d", "(", "x", ")", "# N x 288 x 35 x 35", "\n", "x", "=", "self", ".", "Mixed_6a", "(", "x", ")", "# N x 768 x 17 x 17", "\n", "x", "=", "self", ".", "Mixed_6b", "(", "x", ")", "# N x 768 x 17 x 17", "\n", "x", "=", "self", ".", "Mixed_6c", "(", "x", ")", "# N x 768 x 17 x 17", "\n", "x", "=", "self", ".", "Mixed_6d", "(", "x", ")", "# N x 768 x 17 x 17", "\n", "x", "=", "self", ".", "Mixed_6e", "(", "x", ")", "# N x 768 x 17 x 17", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionV3.forward_postaux": [[386, 391], ["inception_v3.InceptionV3.Mixed_7a", "inception_v3.InceptionV3.Mixed_7b", "inception_v3.InceptionV3.Mixed_7c"], "methods", ["None"], ["", "def", "forward_postaux", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "Mixed_7a", "(", "x", ")", "# N x 1280 x 8 x 8", "\n", "x", "=", "self", ".", "Mixed_7b", "(", "x", ")", "# N x 2048 x 8 x 8", "\n", "x", "=", "self", ".", "Mixed_7c", "(", "x", ")", "# N x 2048 x 8 x 8", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionV3.forward_features": [[392, 396], ["inception_v3.InceptionV3.forward_preaux", "inception_v3.InceptionV3.forward_postaux"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionV3.forward_preaux", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionV3.forward_postaux"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_preaux", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_postaux", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionV3.forward_head": [[397, 403], ["inception_v3.InceptionV3.global_pool", "inception_v3.InceptionV3.fc", "torch.dropout", "torch.dropout", "torch.dropout"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "if", "self", ".", "drop_rate", ">", "0", ":", "\n", "            ", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "drop_rate", ",", "training", "=", "self", ".", "training", ")", "\n", "", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionV3.forward": [[404, 408], ["inception_v3.InceptionV3.forward_features", "inception_v3.InceptionV3.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionV3Aux.__init__": [[414, 417], ["inception_v3.InceptionV3.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "num_classes", "=", "1000", ",", "in_chans", "=", "3", ",", "drop_rate", "=", "0.", ",", "global_pool", "=", "'avg'", ",", "aux_logits", "=", "True", ")", ":", "\n", "        ", "super", "(", "InceptionV3Aux", ",", "self", ")", ".", "__init__", "(", "\n", "num_classes", ",", "in_chans", ",", "drop_rate", ",", "global_pool", ",", "aux_logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionV3Aux.forward_features": [[418, 423], ["inception_v3.InceptionV3Aux.forward_preaux", "inception_v3.InceptionV3Aux.forward_postaux", "inception_v3.InceptionV3Aux.AuxLogits"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionV3.forward_preaux", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionV3.forward_postaux"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_preaux", "(", "x", ")", "\n", "aux", "=", "self", ".", "AuxLogits", "(", "x", ")", "if", "self", ".", "training", "else", "None", "\n", "x", "=", "self", ".", "forward_postaux", "(", "x", ")", "\n", "return", "x", ",", "aux", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.InceptionV3Aux.forward": [[424, 428], ["inception_v3.InceptionV3Aux.forward_features", "inception_v3.InceptionV3Aux.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", ",", "aux", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", ",", "aux", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3._cfg": [[16, 24], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "299", ",", "299", ")", ",", "'pool_size'", ":", "(", "8", ",", "8", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bicubic'", ",", "\n", "'mean'", ":", "IMAGENET_INCEPTION_MEAN", ",", "'std'", ":", "IMAGENET_INCEPTION_STD", ",", "\n", "'first_conv'", ":", "'Conv2d_1a_3x3.conv'", ",", "'classifier'", ":", "'fc'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3._create_inception_v3": [[430, 446], ["helpers.resolve_pretrained_cfg", "kwargs.pop", "helpers.build_model_with_cfg", "kwargs.pop", "kwargs.pop"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.resolve_pretrained_cfg", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_inception_v3", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "pretrained_cfg", "=", "resolve_pretrained_cfg", "(", "variant", ",", "pretrained_cfg", "=", "kwargs", ".", "pop", "(", "'pretrained_cfg'", ",", "None", ")", ")", "\n", "aux_logits", "=", "kwargs", ".", "pop", "(", "'aux_logits'", ",", "False", ")", "\n", "if", "aux_logits", ":", "\n", "        ", "assert", "not", "kwargs", ".", "pop", "(", "'features_only'", ",", "False", ")", "\n", "model_cls", "=", "InceptionV3Aux", "\n", "load_strict", "=", "pretrained_cfg", "[", "'has_aux'", "]", "\n", "", "else", ":", "\n", "        ", "model_cls", "=", "InceptionV3", "\n", "load_strict", "=", "not", "pretrained_cfg", "[", "'has_aux'", "]", "\n", "\n", "", "return", "build_model_with_cfg", "(", "\n", "model_cls", ",", "variant", ",", "pretrained", ",", "\n", "pretrained_cfg", "=", "pretrained_cfg", ",", "\n", "pretrained_strict", "=", "load_strict", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.inception_v3": [[448, 453], ["inception_v3._create_inception_v3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3._create_inception_v3"], ["", "@", "register_model", "\n", "def", "inception_v3", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "# original PyTorch weights, ported from Tensorflow but modified", "\n", "    ", "model", "=", "_create_inception_v3", "(", "'inception_v3'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.tf_inception_v3": [[455, 460], ["inception_v3._create_inception_v3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3._create_inception_v3"], ["", "@", "register_model", "\n", "def", "tf_inception_v3", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "# my port of Tensorflow SLIM weights (http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz)", "\n", "    ", "model", "=", "_create_inception_v3", "(", "'tf_inception_v3'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.adv_inception_v3": [[462, 468], ["inception_v3._create_inception_v3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3._create_inception_v3"], ["", "@", "register_model", "\n", "def", "adv_inception_v3", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "# my port of Tensorflow adversarially trained Inception V3 from", "\n", "# http://download.tensorflow.org/models/adv_inception_v3_2017_08_18.tar.gz", "\n", "    ", "model", "=", "_create_inception_v3", "(", "'adv_inception_v3'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3.gluon_inception_v3": [[470, 476], ["inception_v3._create_inception_v3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_v3._create_inception_v3"], ["", "@", "register_model", "\n", "def", "gluon_inception_v3", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "# from gluon pretrained models, best performing in terms of accuracy/loss metrics", "\n", "# https://gluon-cv.mxnet.io/model_zoo/classification.html", "\n", "    ", "model", "=", "_create_inception_v3", "(", "'gluon_inception_v3'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.EfficientNetBuilder.__init__": [[285, 311], ["layers.get_attn", "efficientnet_builder.EfficientNetBuilder.se_layer", "_logger.warning"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.get_attn"], ["def", "__init__", "(", "self", ",", "output_stride", "=", "32", ",", "pad_type", "=", "''", ",", "round_chs_fn", "=", "round_channels", ",", "se_from_exp", "=", "False", ",", "\n", "act_layer", "=", "None", ",", "norm_layer", "=", "None", ",", "se_layer", "=", "None", ",", "drop_path_rate", "=", "0.", ",", "feature_location", "=", "''", ")", ":", "\n", "        ", "self", ".", "output_stride", "=", "output_stride", "\n", "self", ".", "pad_type", "=", "pad_type", "\n", "self", ".", "round_chs_fn", "=", "round_chs_fn", "\n", "self", ".", "se_from_exp", "=", "se_from_exp", "# calculate se channel reduction from expanded (mid) chs", "\n", "self", ".", "act_layer", "=", "act_layer", "\n", "self", ".", "norm_layer", "=", "norm_layer", "\n", "self", ".", "se_layer", "=", "get_attn", "(", "se_layer", ")", "\n", "try", ":", "\n", "            ", "self", ".", "se_layer", "(", "8", ",", "rd_ratio", "=", "1.0", ")", "# test if attn layer accepts rd_ratio arg", "\n", "self", ".", "se_has_ratio", "=", "True", "\n", "", "except", "TypeError", ":", "\n", "            ", "self", ".", "se_has_ratio", "=", "False", "\n", "", "self", ".", "drop_path_rate", "=", "drop_path_rate", "\n", "if", "feature_location", "==", "'depthwise'", ":", "\n", "# old 'depthwise' mode renamed 'expansion' to match TF impl, old expansion mode didn't make sense", "\n", "            ", "_logger", ".", "warning", "(", "\"feature_location=='depthwise' is deprecated, using 'expansion'\"", ")", "\n", "feature_location", "=", "'expansion'", "\n", "", "self", ".", "feature_location", "=", "feature_location", "\n", "assert", "feature_location", "in", "(", "'bottleneck'", ",", "'expansion'", ",", "''", ")", "\n", "self", ".", "verbose", "=", "_DEBUG_BUILDER", "\n", "\n", "# state updated during build, consumed by model", "\n", "self", ".", "in_chs", "=", "None", "\n", "self", ".", "features", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.EfficientNetBuilder._make_block": [[312, 354], ["ba.pop", "efficientnet_builder.EfficientNetBuilder.round_chs_fn", "efficientnet_builder.EfficientNetBuilder.round_chs_fn", "ba.pop", "efficientnet_builder._log_info_if", "ba.get", "efficientnet_blocks.CondConvResidual", "efficientnet_blocks.InvertedResidual", "efficientnet_builder._log_info_if", "efficientnet_blocks.DepthwiseSeparableConv", "ba.get", "functools.partial", "str", "efficientnet_builder._log_info_if", "efficientnet_blocks.EdgeResidual", "str", "efficientnet_builder._log_info_if", "efficientnet_blocks.ConvBnAct", "str", "str"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._log_info_if", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._log_info_if", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._log_info_if", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._log_info_if"], ["", "def", "_make_block", "(", "self", ",", "ba", ",", "block_idx", ",", "block_count", ")", ":", "\n", "        ", "drop_path_rate", "=", "self", ".", "drop_path_rate", "*", "block_idx", "/", "block_count", "\n", "bt", "=", "ba", ".", "pop", "(", "'block_type'", ")", "\n", "ba", "[", "'in_chs'", "]", "=", "self", ".", "in_chs", "\n", "ba", "[", "'out_chs'", "]", "=", "self", ".", "round_chs_fn", "(", "ba", "[", "'out_chs'", "]", ")", "\n", "if", "'force_in_chs'", "in", "ba", "and", "ba", "[", "'force_in_chs'", "]", ":", "\n", "# NOTE this is a hack to work around mismatch in TF EdgeEffNet impl", "\n", "            ", "ba", "[", "'force_in_chs'", "]", "=", "self", ".", "round_chs_fn", "(", "ba", "[", "'force_in_chs'", "]", ")", "\n", "", "ba", "[", "'pad_type'", "]", "=", "self", ".", "pad_type", "\n", "# block act fn overrides the model default", "\n", "ba", "[", "'act_layer'", "]", "=", "ba", "[", "'act_layer'", "]", "if", "ba", "[", "'act_layer'", "]", "is", "not", "None", "else", "self", ".", "act_layer", "\n", "assert", "ba", "[", "'act_layer'", "]", "is", "not", "None", "\n", "ba", "[", "'norm_layer'", "]", "=", "self", ".", "norm_layer", "\n", "ba", "[", "'drop_path_rate'", "]", "=", "drop_path_rate", "\n", "if", "bt", "!=", "'cn'", ":", "\n", "            ", "se_ratio", "=", "ba", ".", "pop", "(", "'se_ratio'", ")", "\n", "if", "se_ratio", "and", "self", ".", "se_layer", "is", "not", "None", ":", "\n", "                ", "if", "not", "self", ".", "se_from_exp", ":", "\n", "# adjust se_ratio by expansion ratio if calculating se channels from block input", "\n", "                    ", "se_ratio", "/=", "ba", ".", "get", "(", "'exp_ratio'", ",", "1.0", ")", "\n", "", "if", "self", ".", "se_has_ratio", ":", "\n", "                    ", "ba", "[", "'se_layer'", "]", "=", "partial", "(", "self", ".", "se_layer", ",", "rd_ratio", "=", "se_ratio", ")", "\n", "", "else", ":", "\n", "                    ", "ba", "[", "'se_layer'", "]", "=", "self", ".", "se_layer", "\n", "\n", "", "", "", "if", "bt", "==", "'ir'", ":", "\n", "            ", "_log_info_if", "(", "'  InvertedResidual {}, Args: {}'", ".", "format", "(", "block_idx", ",", "str", "(", "ba", ")", ")", ",", "self", ".", "verbose", ")", "\n", "block", "=", "CondConvResidual", "(", "**", "ba", ")", "if", "ba", ".", "get", "(", "'num_experts'", ",", "0", ")", "else", "InvertedResidual", "(", "**", "ba", ")", "\n", "", "elif", "bt", "==", "'ds'", "or", "bt", "==", "'dsa'", ":", "\n", "            ", "_log_info_if", "(", "'  DepthwiseSeparable {}, Args: {}'", ".", "format", "(", "block_idx", ",", "str", "(", "ba", ")", ")", ",", "self", ".", "verbose", ")", "\n", "block", "=", "DepthwiseSeparableConv", "(", "**", "ba", ")", "\n", "", "elif", "bt", "==", "'er'", ":", "\n", "            ", "_log_info_if", "(", "'  EdgeResidual {}, Args: {}'", ".", "format", "(", "block_idx", ",", "str", "(", "ba", ")", ")", ",", "self", ".", "verbose", ")", "\n", "block", "=", "EdgeResidual", "(", "**", "ba", ")", "\n", "", "elif", "bt", "==", "'cn'", ":", "\n", "            ", "_log_info_if", "(", "'  ConvBnAct {}, Args: {}'", ".", "format", "(", "block_idx", ",", "str", "(", "ba", ")", ")", ",", "self", ".", "verbose", ")", "\n", "block", "=", "ConvBnAct", "(", "**", "ba", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "'Uknkown block type (%s) while building model.'", "%", "bt", "\n", "\n", "", "self", ".", "in_chs", "=", "ba", "[", "'out_chs'", "]", "# update in_chs for arg of next block", "\n", "return", "block", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.EfficientNetBuilder.__call__": [[355, 430], ["efficientnet_builder._log_info_if", "sum", "enumerate", "dict", "efficientnet_builder.EfficientNetBuilder.features.append", "efficientnet_builder._log_info_if", "isinstance", "enumerate", "stages.append", "len", "len", "len", "efficientnet_builder._log_info_if", "efficientnet_builder.EfficientNetBuilder._make_block", "blocks.append", "torch.Sequential", "len", "dict", "dict.get", "efficientnet_builder.EfficientNetBuilder.features.append", "efficientnet_builder._log_info_if", "len", "efficientnet_builder.EfficientNetBuilder.feature_info"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._log_info_if", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._log_info_if", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._log_info_if", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.EfficientNetBuilder._make_block", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._log_info_if", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_blocks.EdgeResidual.feature_info"], ["", "def", "__call__", "(", "self", ",", "in_chs", ",", "model_block_args", ")", ":", "\n", "        ", "\"\"\" Build the blocks\n        Args:\n            in_chs: Number of input-channels passed to first block\n            model_block_args: A list of lists, outer list defines stages, inner\n                list contains strings defining block configuration(s)\n        Return:\n             List of block stacks (each stack wrapped in nn.Sequential)\n        \"\"\"", "\n", "_log_info_if", "(", "'Building model trunk with %d stages...'", "%", "len", "(", "model_block_args", ")", ",", "self", ".", "verbose", ")", "\n", "self", ".", "in_chs", "=", "in_chs", "\n", "total_block_count", "=", "sum", "(", "[", "len", "(", "x", ")", "for", "x", "in", "model_block_args", "]", ")", "\n", "total_block_idx", "=", "0", "\n", "current_stride", "=", "2", "\n", "current_dilation", "=", "1", "\n", "stages", "=", "[", "]", "\n", "if", "model_block_args", "[", "0", "]", "[", "0", "]", "[", "'stride'", "]", ">", "1", ":", "\n", "# if the first block starts with a stride, we need to extract first level feat from stem", "\n", "            ", "feature_info", "=", "dict", "(", "\n", "module", "=", "'act1'", ",", "num_chs", "=", "in_chs", ",", "stage", "=", "0", ",", "reduction", "=", "current_stride", ",", "\n", "hook_type", "=", "'forward'", "if", "self", ".", "feature_location", "!=", "'bottleneck'", "else", "''", ")", "\n", "self", ".", "features", ".", "append", "(", "feature_info", ")", "\n", "\n", "# outer list of block_args defines the stacks", "\n", "", "for", "stack_idx", ",", "stack_args", "in", "enumerate", "(", "model_block_args", ")", ":", "\n", "            ", "last_stack", "=", "stack_idx", "+", "1", "==", "len", "(", "model_block_args", ")", "\n", "_log_info_if", "(", "'Stack: {}'", ".", "format", "(", "stack_idx", ")", ",", "self", ".", "verbose", ")", "\n", "assert", "isinstance", "(", "stack_args", ",", "list", ")", "\n", "\n", "blocks", "=", "[", "]", "\n", "# each stack (stage of blocks) contains a list of block arguments", "\n", "for", "block_idx", ",", "block_args", "in", "enumerate", "(", "stack_args", ")", ":", "\n", "                ", "last_block", "=", "block_idx", "+", "1", "==", "len", "(", "stack_args", ")", "\n", "_log_info_if", "(", "' Block: {}'", ".", "format", "(", "block_idx", ")", ",", "self", ".", "verbose", ")", "\n", "\n", "assert", "block_args", "[", "'stride'", "]", "in", "(", "1", ",", "2", ")", "\n", "if", "block_idx", ">=", "1", ":", "# only the first block in any stack can have a stride > 1", "\n", "                    ", "block_args", "[", "'stride'", "]", "=", "1", "\n", "\n", "", "extract_features", "=", "False", "\n", "if", "last_block", ":", "\n", "                    ", "next_stack_idx", "=", "stack_idx", "+", "1", "\n", "extract_features", "=", "next_stack_idx", ">=", "len", "(", "model_block_args", ")", "or", "model_block_args", "[", "next_stack_idx", "]", "[", "0", "]", "[", "'stride'", "]", ">", "1", "\n", "\n", "", "next_dilation", "=", "current_dilation", "\n", "if", "block_args", "[", "'stride'", "]", ">", "1", ":", "\n", "                    ", "next_output_stride", "=", "current_stride", "*", "block_args", "[", "'stride'", "]", "\n", "if", "next_output_stride", ">", "self", ".", "output_stride", ":", "\n", "                        ", "next_dilation", "=", "current_dilation", "*", "block_args", "[", "'stride'", "]", "\n", "block_args", "[", "'stride'", "]", "=", "1", "\n", "_log_info_if", "(", "'  Converting stride to dilation to maintain output_stride=={}'", ".", "format", "(", "\n", "self", ".", "output_stride", ")", ",", "self", ".", "verbose", ")", "\n", "", "else", ":", "\n", "                        ", "current_stride", "=", "next_output_stride", "\n", "", "", "block_args", "[", "'dilation'", "]", "=", "current_dilation", "\n", "if", "next_dilation", "!=", "current_dilation", ":", "\n", "                    ", "current_dilation", "=", "next_dilation", "\n", "\n", "# create the block", "\n", "", "block", "=", "self", ".", "_make_block", "(", "block_args", ",", "total_block_idx", ",", "total_block_count", ")", "\n", "blocks", ".", "append", "(", "block", ")", "\n", "\n", "# stash feature module name and channel info for model feature extraction", "\n", "if", "extract_features", ":", "\n", "                    ", "feature_info", "=", "dict", "(", "\n", "stage", "=", "stack_idx", "+", "1", ",", "reduction", "=", "current_stride", ",", "**", "block", ".", "feature_info", "(", "self", ".", "feature_location", ")", ")", "\n", "module_name", "=", "f'blocks.{stack_idx}.{block_idx}'", "\n", "leaf_name", "=", "feature_info", ".", "get", "(", "'module'", ",", "''", ")", "\n", "feature_info", "[", "'module'", "]", "=", "'.'", ".", "join", "(", "[", "module_name", ",", "leaf_name", "]", ")", "if", "leaf_name", "else", "module_name", "\n", "self", ".", "features", ".", "append", "(", "feature_info", ")", "\n", "\n", "", "total_block_idx", "+=", "1", "# incr global block idx (across all stacks)", "\n", "", "stages", ".", "append", "(", "nn", ".", "Sequential", "(", "*", "blocks", ")", ")", "\n", "", "return", "stages", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.get_bn_args_tf": [[38, 40], ["_BN_ARGS_TF.copy"], "function", ["None"], ["def", "get_bn_args_tf", "(", ")", ":", "\n", "    ", "return", "_BN_ARGS_TF", ".", "copy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args": [[42, 51], ["kwargs.pop", "kwargs.pop"], "function", ["None"], ["", "def", "resolve_bn_args", "(", "kwargs", ")", ":", "\n", "    ", "bn_args", "=", "{", "}", "\n", "bn_momentum", "=", "kwargs", ".", "pop", "(", "'bn_momentum'", ",", "None", ")", "\n", "if", "bn_momentum", "is", "not", "None", ":", "\n", "        ", "bn_args", "[", "'momentum'", "]", "=", "bn_momentum", "\n", "", "bn_eps", "=", "kwargs", ".", "pop", "(", "'bn_eps'", ",", "None", ")", "\n", "if", "bn_eps", "is", "not", "None", ":", "\n", "        ", "bn_args", "[", "'eps'", "]", "=", "bn_eps", "\n", "", "return", "bn_args", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_act_layer": [[53, 55], ["layers.get_act_layer", "kwargs.pop"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_layer"], ["", "def", "resolve_act_layer", "(", "kwargs", ",", "default", "=", "'relu'", ")", ":", "\n", "    ", "return", "get_act_layer", "(", "kwargs", ".", "pop", "(", "'act_layer'", ",", "default", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.round_channels": [[57, 62], ["layers.make_divisible"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible"], ["", "def", "round_channels", "(", "channels", ",", "multiplier", "=", "1.0", ",", "divisor", "=", "8", ",", "channel_min", "=", "None", ",", "round_limit", "=", "0.9", ")", ":", "\n", "    ", "\"\"\"Round number of filters based on depth multiplier.\"\"\"", "\n", "if", "not", "multiplier", ":", "\n", "        ", "return", "channels", "\n", "", "return", "make_divisible", "(", "channels", "*", "multiplier", ",", "divisor", ",", "channel_min", ",", "round_limit", "=", "round_limit", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._log_info_if": [[64, 67], ["_logger.info"], "function", ["None"], ["", "def", "_log_info_if", "(", "msg", ",", "condition", ")", ":", "\n", "    ", "if", "condition", ":", "\n", "        ", "_logger", ".", "info", "(", "msg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._parse_ksize": [[69, 74], ["ss.isdigit", "int", "int", "ss.split"], "function", ["None"], ["", "", "def", "_parse_ksize", "(", "ss", ")", ":", "\n", "    ", "if", "ss", ".", "isdigit", "(", ")", ":", "\n", "        ", "return", "int", "(", "ss", ")", "\n", "", "else", ":", "\n", "        ", "return", "[", "int", "(", "k", ")", "for", "k", "in", "ss", ".", "split", "(", "'.'", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._decode_block_str": [[76, 190], ["isinstance", "block_str.split", "int", "dict", "efficientnet_builder._parse_ksize", "efficientnet_builder._parse_ksize", "int", "dict.update", "int", "int", "dict", "int", "dict.update", "op.startswith", "dict", "dict.update", "re.split", "efficientnet_builder._parse_ksize", "float", "dict", "dict.update", "layers.get_act_layer", "len", "float", "efficientnet_builder._parse_ksize", "dict", "layers.get_act_layer", "float", "efficientnet_builder._parse_ksize", "float", "layers.get_act_layer", "float", "int", "layers.get_act_layer", "layers.get_act_layer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._parse_ksize", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._parse_ksize", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._parse_ksize", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._parse_ksize", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._parse_ksize", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_layer"], ["", "", "def", "_decode_block_str", "(", "block_str", ")", ":", "\n", "    ", "\"\"\" Decode block definition string\n\n    Gets a list of block arg (dicts) through a string notation of arguments.\n    E.g. ir_r2_k3_s2_e1_i32_o16_se0.25_noskip\n\n    All args can exist in any order with the exception of the leading string which\n    is assumed to indicate the block type.\n\n    leading string - block type (\n      ir = InvertedResidual, ds = DepthwiseSep, dsa = DeptwhiseSep with pw act, cn = ConvBnAct)\n    r - number of repeat blocks,\n    k - kernel size,\n    s - strides (1-9),\n    e - expansion ratio,\n    c - output channels,\n    se - squeeze/excitation ratio\n    n - activation fn ('re', 'r6', 'hs', or 'sw')\n    Args:\n        block_str: a string representation of block arguments.\n    Returns:\n        A list of block args (dicts)\n    Raises:\n        ValueError: if the string def not properly specified (TODO)\n    \"\"\"", "\n", "assert", "isinstance", "(", "block_str", ",", "str", ")", "\n", "ops", "=", "block_str", ".", "split", "(", "'_'", ")", "\n", "block_type", "=", "ops", "[", "0", "]", "# take the block type off the front", "\n", "ops", "=", "ops", "[", "1", ":", "]", "\n", "options", "=", "{", "}", "\n", "skip", "=", "None", "\n", "for", "op", "in", "ops", ":", "\n", "# string options being checked on individual basis, combine if they grow", "\n", "        ", "if", "op", "==", "'noskip'", ":", "\n", "            ", "skip", "=", "False", "# force no skip connection", "\n", "", "elif", "op", "==", "'skip'", ":", "\n", "            ", "skip", "=", "True", "# force a skip connection", "\n", "", "elif", "op", ".", "startswith", "(", "'n'", ")", ":", "\n", "# activation fn", "\n", "            ", "key", "=", "op", "[", "0", "]", "\n", "v", "=", "op", "[", "1", ":", "]", "\n", "if", "v", "==", "'re'", ":", "\n", "                ", "value", "=", "get_act_layer", "(", "'relu'", ")", "\n", "", "elif", "v", "==", "'r6'", ":", "\n", "                ", "value", "=", "get_act_layer", "(", "'relu6'", ")", "\n", "", "elif", "v", "==", "'hs'", ":", "\n", "                ", "value", "=", "get_act_layer", "(", "'hard_swish'", ")", "\n", "", "elif", "v", "==", "'sw'", ":", "\n", "                ", "value", "=", "get_act_layer", "(", "'swish'", ")", "# aka SiLU", "\n", "", "elif", "v", "==", "'mi'", ":", "\n", "                ", "value", "=", "get_act_layer", "(", "'mish'", ")", "\n", "", "else", ":", "\n", "                ", "continue", "\n", "", "options", "[", "key", "]", "=", "value", "\n", "", "else", ":", "\n", "# all numeric options", "\n", "            ", "splits", "=", "re", ".", "split", "(", "r'(\\d.*)'", ",", "op", ")", "\n", "if", "len", "(", "splits", ")", ">=", "2", ":", "\n", "                ", "key", ",", "value", "=", "splits", "[", ":", "2", "]", "\n", "options", "[", "key", "]", "=", "value", "\n", "\n", "# if act_layer is None, the model default (passed to model init) will be used", "\n", "", "", "", "act_layer", "=", "options", "[", "'n'", "]", "if", "'n'", "in", "options", "else", "None", "\n", "exp_kernel_size", "=", "_parse_ksize", "(", "options", "[", "'a'", "]", ")", "if", "'a'", "in", "options", "else", "1", "\n", "pw_kernel_size", "=", "_parse_ksize", "(", "options", "[", "'p'", "]", ")", "if", "'p'", "in", "options", "else", "1", "\n", "force_in_chs", "=", "int", "(", "options", "[", "'fc'", "]", ")", "if", "'fc'", "in", "options", "else", "0", "# FIXME hack to deal with in_chs issue in TPU def", "\n", "num_repeat", "=", "int", "(", "options", "[", "'r'", "]", ")", "\n", "\n", "# each type of block has different valid arguments, fill accordingly", "\n", "block_args", "=", "dict", "(", "\n", "block_type", "=", "block_type", ",", "\n", "out_chs", "=", "int", "(", "options", "[", "'c'", "]", ")", ",", "\n", "stride", "=", "int", "(", "options", "[", "'s'", "]", ")", ",", "\n", "act_layer", "=", "act_layer", ",", "\n", ")", "\n", "if", "block_type", "==", "'ir'", ":", "\n", "        ", "block_args", ".", "update", "(", "dict", "(", "\n", "dw_kernel_size", "=", "_parse_ksize", "(", "options", "[", "'k'", "]", ")", ",", "\n", "exp_kernel_size", "=", "exp_kernel_size", ",", "\n", "pw_kernel_size", "=", "pw_kernel_size", ",", "\n", "exp_ratio", "=", "float", "(", "options", "[", "'e'", "]", ")", ",", "\n", "se_ratio", "=", "float", "(", "options", "[", "'se'", "]", ")", "if", "'se'", "in", "options", "else", "0.", ",", "\n", "noskip", "=", "skip", "is", "False", ",", "\n", ")", ")", "\n", "if", "'cc'", "in", "options", ":", "\n", "            ", "block_args", "[", "'num_experts'", "]", "=", "int", "(", "options", "[", "'cc'", "]", ")", "\n", "", "", "elif", "block_type", "==", "'ds'", "or", "block_type", "==", "'dsa'", ":", "\n", "        ", "block_args", ".", "update", "(", "dict", "(", "\n", "dw_kernel_size", "=", "_parse_ksize", "(", "options", "[", "'k'", "]", ")", ",", "\n", "pw_kernel_size", "=", "pw_kernel_size", ",", "\n", "se_ratio", "=", "float", "(", "options", "[", "'se'", "]", ")", "if", "'se'", "in", "options", "else", "0.", ",", "\n", "pw_act", "=", "block_type", "==", "'dsa'", ",", "\n", "noskip", "=", "block_type", "==", "'dsa'", "or", "skip", "is", "False", ",", "\n", ")", ")", "\n", "", "elif", "block_type", "==", "'er'", ":", "\n", "        ", "block_args", ".", "update", "(", "dict", "(", "\n", "exp_kernel_size", "=", "_parse_ksize", "(", "options", "[", "'k'", "]", ")", ",", "\n", "pw_kernel_size", "=", "pw_kernel_size", ",", "\n", "exp_ratio", "=", "float", "(", "options", "[", "'e'", "]", ")", ",", "\n", "force_in_chs", "=", "force_in_chs", ",", "\n", "se_ratio", "=", "float", "(", "options", "[", "'se'", "]", ")", "if", "'se'", "in", "options", "else", "0.", ",", "\n", "noskip", "=", "skip", "is", "False", ",", "\n", ")", ")", "\n", "", "elif", "block_type", "==", "'cn'", ":", "\n", "        ", "block_args", ".", "update", "(", "dict", "(", "\n", "kernel_size", "=", "int", "(", "options", "[", "'k'", "]", ")", ",", "\n", "skip", "=", "skip", "is", "True", ",", "\n", ")", ")", "\n", "", "else", ":", "\n", "        ", "assert", "False", ",", "'Unknown block type (%s)'", "%", "block_type", "\n", "", "if", "'gs'", "in", "options", ":", "\n", "        ", "block_args", "[", "'group_size'", "]", "=", "options", "[", "'gs'", "]", "\n", "\n", "", "return", "block_args", ",", "num_repeat", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._scale_stage_depth": [[192, 228], ["sum", "zip", "max", "int", "max", "repeats_scaled.append", "sa_scaled.extend", "round", "math.ceil", "round", "copy.deepcopy", "range"], "function", ["None"], ["", "def", "_scale_stage_depth", "(", "stack_args", ",", "repeats", ",", "depth_multiplier", "=", "1.0", ",", "depth_trunc", "=", "'ceil'", ")", ":", "\n", "    ", "\"\"\" Per-stage depth scaling\n    Scales the block repeats in each stage. This depth scaling impl maintains\n    compatibility with the EfficientNet scaling method, while allowing sensible\n    scaling for other models that may have multiple block arg definitions in each stage.\n    \"\"\"", "\n", "\n", "# We scale the total repeat count for each stage, there may be multiple", "\n", "# block arg defs per stage so we need to sum.", "\n", "num_repeat", "=", "sum", "(", "repeats", ")", "\n", "if", "depth_trunc", "==", "'round'", ":", "\n", "# Truncating to int by rounding allows stages with few repeats to remain", "\n", "# proportionally smaller for longer. This is a good choice when stage definitions", "\n", "# include single repeat stages that we'd prefer to keep that way as long as possible", "\n", "        ", "num_repeat_scaled", "=", "max", "(", "1", ",", "round", "(", "num_repeat", "*", "depth_multiplier", ")", ")", "\n", "", "else", ":", "\n", "# The default for EfficientNet truncates repeats to int via 'ceil'.", "\n", "# Any multiplier > 1.0 will result in an increased depth for every stage.", "\n", "        ", "num_repeat_scaled", "=", "int", "(", "math", ".", "ceil", "(", "num_repeat", "*", "depth_multiplier", ")", ")", "\n", "\n", "# Proportionally distribute repeat count scaling to each block definition in the stage.", "\n", "# Allocation is done in reverse as it results in the first block being less likely to be scaled.", "\n", "# The first block makes less sense to repeat in most of the arch definitions.", "\n", "", "repeats_scaled", "=", "[", "]", "\n", "for", "r", "in", "repeats", "[", ":", ":", "-", "1", "]", ":", "\n", "        ", "rs", "=", "max", "(", "1", ",", "round", "(", "(", "r", "/", "num_repeat", "*", "num_repeat_scaled", ")", ")", ")", "\n", "repeats_scaled", ".", "append", "(", "rs", ")", "\n", "num_repeat", "-=", "r", "\n", "num_repeat_scaled", "-=", "rs", "\n", "", "repeats_scaled", "=", "repeats_scaled", "[", ":", ":", "-", "1", "]", "\n", "\n", "# Apply the calculated scaling to each block arg in the stage", "\n", "sa_scaled", "=", "[", "]", "\n", "for", "ba", ",", "rep", "in", "zip", "(", "stack_args", ",", "repeats_scaled", ")", ":", "\n", "        ", "sa_scaled", ".", "extend", "(", "[", "deepcopy", "(", "ba", ")", "for", "_", "in", "range", "(", "rep", ")", "]", ")", "\n", "", "return", "sa_scaled", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def": [[230, 274], ["isinstance", "enumerate", "zip", "isinstance", "len", "len", "len", "isinstance", "efficientnet_builder._decode_block_str", "stack_args.append", "repeats.append", "arch_args.append", "arch_args.append", "ba.setdefault", "efficientnet_builder._scale_stage_depth", "efficientnet_builder._scale_stage_depth", "ba.get", "len"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._decode_block_str", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._scale_stage_depth", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._scale_stage_depth", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["", "def", "decode_arch_def", "(", "\n", "arch_def", ",", "\n", "depth_multiplier", "=", "1.0", ",", "\n", "depth_trunc", "=", "'ceil'", ",", "\n", "experts_multiplier", "=", "1", ",", "\n", "fix_first_last", "=", "False", ",", "\n", "group_size", "=", "None", ",", "\n", ")", ":", "\n", "    ", "\"\"\" Decode block architecture definition strings -> block kwargs\n\n    Args:\n        arch_def: architecture definition strings, list of list of strings\n        depth_multiplier: network depth multiplier\n        depth_trunc: networ depth truncation mode when applying multiplier\n        experts_multiplier: CondConv experts multiplier\n        fix_first_last: fix first and last block depths when multiplier is applied\n        group_size: group size override for all blocks that weren't explicitly set in arch string\n\n    Returns:\n        list of list of block kwargs\n    \"\"\"", "\n", "arch_args", "=", "[", "]", "\n", "if", "isinstance", "(", "depth_multiplier", ",", "tuple", ")", ":", "\n", "        ", "assert", "len", "(", "depth_multiplier", ")", "==", "len", "(", "arch_def", ")", "\n", "", "else", ":", "\n", "        ", "depth_multiplier", "=", "(", "depth_multiplier", ",", ")", "*", "len", "(", "arch_def", ")", "\n", "", "for", "stack_idx", ",", "(", "block_strings", ",", "multiplier", ")", "in", "enumerate", "(", "zip", "(", "arch_def", ",", "depth_multiplier", ")", ")", ":", "\n", "        ", "assert", "isinstance", "(", "block_strings", ",", "list", ")", "\n", "stack_args", "=", "[", "]", "\n", "repeats", "=", "[", "]", "\n", "for", "block_str", "in", "block_strings", ":", "\n", "            ", "assert", "isinstance", "(", "block_str", ",", "str", ")", "\n", "ba", ",", "rep", "=", "_decode_block_str", "(", "block_str", ")", "\n", "if", "ba", ".", "get", "(", "'num_experts'", ",", "0", ")", ">", "0", "and", "experts_multiplier", ">", "1", ":", "\n", "                ", "ba", "[", "'num_experts'", "]", "*=", "experts_multiplier", "\n", "", "if", "group_size", "is", "not", "None", ":", "\n", "                ", "ba", ".", "setdefault", "(", "'group_size'", ",", "group_size", ")", "\n", "", "stack_args", ".", "append", "(", "ba", ")", "\n", "repeats", ".", "append", "(", "rep", ")", "\n", "", "if", "fix_first_last", "and", "(", "stack_idx", "==", "0", "or", "stack_idx", "==", "len", "(", "arch_def", ")", "-", "1", ")", ":", "\n", "            ", "arch_args", ".", "append", "(", "_scale_stage_depth", "(", "stack_args", ",", "repeats", ",", "1.0", ",", "depth_trunc", ")", ")", "\n", "", "else", ":", "\n", "            ", "arch_args", ".", "append", "(", "_scale_stage_depth", "(", "stack_args", ",", "repeats", ",", "multiplier", ",", "depth_trunc", ")", ")", "\n", "", "", "return", "arch_args", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder._init_weight_goog": [[432, 471], ["isinstance", "layers.get_condconv_initializer", "layers.get_condconv_initializer.", "isinstance", "torch.init.zeros_", "torch.init.normal_", "isinstance", "torch.init.normal_", "math.sqrt", "torch.init.zeros_", "torch.init.ones_", "torch.init.zeros_", "isinstance", "math.sqrt", "m.weight.size", "torch.init.uniform_", "torch.init.zeros_", "m.weight.size", "math.sqrt"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.cond_conv2d.get_condconv_initializer"], ["", "", "def", "_init_weight_goog", "(", "m", ",", "n", "=", "''", ",", "fix_group_fanout", "=", "True", ")", ":", "\n", "    ", "\"\"\" Weight initialization as per Tensorflow official implementations.\n\n    Args:\n        m (nn.Module): module to init\n        n (str): module name\n        fix_group_fanout (bool): enable correct (matching Tensorflow TPU impl) fanout calculation w/ group convs\n\n    Handles layers in EfficientNet, EfficientNet-CondConv, MixNet, MnasNet, MobileNetV3, etc:\n    * https://github.com/tensorflow/tpu/blob/master/models/official/mnasnet/mnasnet_model.py\n    * https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_model.py\n    \"\"\"", "\n", "if", "isinstance", "(", "m", ",", "CondConv2d", ")", ":", "\n", "        ", "fan_out", "=", "m", ".", "kernel_size", "[", "0", "]", "*", "m", ".", "kernel_size", "[", "1", "]", "*", "m", ".", "out_channels", "\n", "if", "fix_group_fanout", ":", "\n", "            ", "fan_out", "//=", "m", ".", "groups", "\n", "", "init_weight_fn", "=", "get_condconv_initializer", "(", "\n", "lambda", "w", ":", "nn", ".", "init", ".", "normal_", "(", "w", ",", "0", ",", "math", ".", "sqrt", "(", "2.0", "/", "fan_out", ")", ")", ",", "m", ".", "num_experts", ",", "m", ".", "weight_shape", ")", "\n", "init_weight_fn", "(", "m", ".", "weight", ")", "\n", "if", "m", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "m", ".", "bias", ")", "\n", "", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "        ", "fan_out", "=", "m", ".", "kernel_size", "[", "0", "]", "*", "m", ".", "kernel_size", "[", "1", "]", "*", "m", ".", "out_channels", "\n", "if", "fix_group_fanout", ":", "\n", "            ", "fan_out", "//=", "m", ".", "groups", "\n", "", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "0", ",", "math", ".", "sqrt", "(", "2.0", "/", "fan_out", ")", ")", "\n", "if", "m", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "m", ".", "bias", ")", "\n", "", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "        ", "nn", ".", "init", ".", "ones_", "(", "m", ".", "weight", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "m", ".", "bias", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "        ", "fan_out", "=", "m", ".", "weight", ".", "size", "(", "0", ")", "# fan-out", "\n", "fan_in", "=", "0", "\n", "if", "'routing_fn'", "in", "n", ":", "\n", "            ", "fan_in", "=", "m", ".", "weight", ".", "size", "(", "1", ")", "\n", "", "init_range", "=", "1.0", "/", "math", ".", "sqrt", "(", "fan_in", "+", "fan_out", ")", "\n", "nn", ".", "init", ".", "uniform_", "(", "m", ".", "weight", ",", "-", "init_range", ",", "init_range", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "m", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.efficientnet_init_weights": [[473, 477], ["model.named_modules", "init_fn"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_modules"], ["", "", "def", "efficientnet_init_weights", "(", "model", ":", "nn", ".", "Module", ",", "init_fn", "=", "None", ")", ":", "\n", "    ", "init_fn", "=", "init_fn", "or", "_init_weight_goog", "\n", "for", "n", ",", "m", "in", "model", ".", "named_modules", "(", ")", ":", "\n", "        ", "init_fn", "(", "m", ",", "n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.HybridEmbed.__init__": [[104, 134], ["torch.Module.__init__", "isinstance", "layers.to_2tuple", "layers.to_2tuple", "torch.Conv2d", "torch.Conv2d", "layers.to_2tuple", "hasattr", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "vision_transformer_hybrid.HybridEmbed.backbone", "isinstance", "backbone.train", "backbone.eval", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "vision_transformer_hybrid.HybridEmbed.backbone.feature_info.channels"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.AttentionSubsample.train", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.channels"], ["def", "__init__", "(", "self", ",", "backbone", ",", "img_size", "=", "224", ",", "patch_size", "=", "1", ",", "feature_size", "=", "None", ",", "in_chans", "=", "3", ",", "embed_dim", "=", "768", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "isinstance", "(", "backbone", ",", "nn", ".", "Module", ")", "\n", "img_size", "=", "to_2tuple", "(", "img_size", ")", "\n", "patch_size", "=", "to_2tuple", "(", "patch_size", ")", "\n", "self", ".", "img_size", "=", "img_size", "\n", "self", ".", "patch_size", "=", "patch_size", "\n", "self", ".", "backbone", "=", "backbone", "\n", "if", "feature_size", "is", "None", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# NOTE Most reliable way of determining output dims is to run forward pass", "\n", "                ", "training", "=", "backbone", ".", "training", "\n", "if", "training", ":", "\n", "                    ", "backbone", ".", "eval", "(", ")", "\n", "", "o", "=", "self", ".", "backbone", "(", "torch", ".", "zeros", "(", "1", ",", "in_chans", ",", "img_size", "[", "0", "]", ",", "img_size", "[", "1", "]", ")", ")", "\n", "if", "isinstance", "(", "o", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "                    ", "o", "=", "o", "[", "-", "1", "]", "# last feature if backbone outputs list/tuple of features", "\n", "", "feature_size", "=", "o", ".", "shape", "[", "-", "2", ":", "]", "\n", "feature_dim", "=", "o", ".", "shape", "[", "1", "]", "\n", "backbone", ".", "train", "(", "training", ")", "\n", "", "", "else", ":", "\n", "            ", "feature_size", "=", "to_2tuple", "(", "feature_size", ")", "\n", "if", "hasattr", "(", "self", ".", "backbone", ",", "'feature_info'", ")", ":", "\n", "                ", "feature_dim", "=", "self", ".", "backbone", ".", "feature_info", ".", "channels", "(", ")", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "feature_dim", "=", "self", ".", "backbone", ".", "num_features", "\n", "", "", "assert", "feature_size", "[", "0", "]", "%", "patch_size", "[", "0", "]", "==", "0", "and", "feature_size", "[", "1", "]", "%", "patch_size", "[", "1", "]", "==", "0", "\n", "self", ".", "grid_size", "=", "(", "feature_size", "[", "0", "]", "//", "patch_size", "[", "0", "]", ",", "feature_size", "[", "1", "]", "//", "patch_size", "[", "1", "]", ")", "\n", "self", ".", "num_patches", "=", "self", ".", "grid_size", "[", "0", "]", "*", "self", ".", "grid_size", "[", "1", "]", "\n", "self", ".", "proj", "=", "nn", ".", "Conv2d", "(", "feature_dim", ",", "embed_dim", ",", "kernel_size", "=", "patch_size", ",", "stride", "=", "patch_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.HybridEmbed.forward": [[135, 141], ["vision_transformer_hybrid.HybridEmbed.backbone", "isinstance", "vision_transformer_hybrid.HybridEmbed.proj().flatten().transpose", "vision_transformer_hybrid.HybridEmbed.proj().flatten", "vision_transformer_hybrid.HybridEmbed.proj"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "backbone", "(", "x", ")", "\n", "if", "isinstance", "(", "x", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "x", "=", "x", "[", "-", "1", "]", "# last feature if backbone outputs list/tuple of features", "\n", "", "x", "=", "self", ".", "proj", "(", "x", ")", ".", "flatten", "(", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._cfg": [[30, 38], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", ".9", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "(", "0.5", ",", "0.5", ",", "0.5", ")", ",", "'std'", ":", "(", "0.5", ",", "0.5", ",", "0.5", ")", ",", "\n", "'first_conv'", ":", "'patch_embed.backbone.stem.conv'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._create_vision_transformer_hybrid": [[143, 147], ["functools.partial", "kwargs.setdefault", "timm.models.vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "", "def", "_create_vision_transformer_hybrid", "(", "variant", ",", "backbone", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "embed_layer", "=", "partial", "(", "HybridEmbed", ",", "backbone", "=", "backbone", ")", "\n", "kwargs", ".", "setdefault", "(", "'patch_size'", ",", "1", ")", "# default patch size for hybrid models if not set", "\n", "return", "_create_vision_transformer", "(", "variant", ",", "pretrained", "=", "pretrained", ",", "embed_layer", "=", "embed_layer", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._resnetv2": [[149, 162], ["kwargs.get", "len", "functools.partial", "functools.partial", "resnetv2.ResNetV2", "resnetv2.create_resnetv2_stem", "kwargs.get", "kwargs.get"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.create_resnetv2_stem", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["", "def", "_resnetv2", "(", "layers", "=", "(", "3", ",", "4", ",", "9", ")", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResNet-V2 backbone helper\"\"\"", "\n", "padding_same", "=", "kwargs", ".", "get", "(", "'padding_same'", ",", "True", ")", "\n", "stem_type", "=", "'same'", "if", "padding_same", "else", "''", "\n", "conv_layer", "=", "partial", "(", "StdConv2dSame", ",", "eps", "=", "1e-8", ")", "if", "padding_same", "else", "partial", "(", "StdConv2d", ",", "eps", "=", "1e-8", ")", "\n", "if", "len", "(", "layers", ")", ":", "\n", "        ", "backbone", "=", "ResNetV2", "(", "\n", "layers", "=", "layers", ",", "num_classes", "=", "0", ",", "global_pool", "=", "''", ",", "in_chans", "=", "kwargs", ".", "get", "(", "'in_chans'", ",", "3", ")", ",", "\n", "preact", "=", "False", ",", "stem_type", "=", "stem_type", ",", "conv_layer", "=", "conv_layer", ")", "\n", "", "else", ":", "\n", "        ", "backbone", "=", "create_resnetv2_stem", "(", "\n", "kwargs", ".", "get", "(", "'in_chans'", ",", "3", ")", ",", "stem_type", "=", "stem_type", ",", "preact", "=", "False", ",", "conv_layer", "=", "conv_layer", ")", "\n", "", "return", "backbone", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_tiny_r_s16_p8_224": [[164, 173], ["vision_transformer_hybrid._resnetv2", "dict", "vision_transformer_hybrid._create_vision_transformer_hybrid"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._resnetv2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._create_vision_transformer_hybrid"], ["", "@", "register_model", "\n", "def", "vit_tiny_r_s16_p8_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" R+ViT-Ti/S16 w/ 8x8 patch hybrid @ 224 x 224.\n    \"\"\"", "\n", "backbone", "=", "_resnetv2", "(", "layers", "=", "(", ")", ",", "**", "kwargs", ")", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "8", ",", "embed_dim", "=", "192", ",", "depth", "=", "12", ",", "num_heads", "=", "3", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_hybrid", "(", "\n", "'vit_tiny_r_s16_p8_224'", ",", "backbone", "=", "backbone", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_tiny_r_s16_p8_384": [[175, 184], ["vision_transformer_hybrid._resnetv2", "dict", "vision_transformer_hybrid._create_vision_transformer_hybrid"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._resnetv2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._create_vision_transformer_hybrid"], ["", "@", "register_model", "\n", "def", "vit_tiny_r_s16_p8_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" R+ViT-Ti/S16 w/ 8x8 patch hybrid @ 384 x 384.\n    \"\"\"", "\n", "backbone", "=", "_resnetv2", "(", "layers", "=", "(", ")", ",", "**", "kwargs", ")", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "8", ",", "embed_dim", "=", "192", ",", "depth", "=", "12", ",", "num_heads", "=", "3", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_hybrid", "(", "\n", "'vit_tiny_r_s16_p8_384'", ",", "backbone", "=", "backbone", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_small_r26_s32_224": [[186, 195], ["vision_transformer_hybrid._resnetv2", "dict", "vision_transformer_hybrid._create_vision_transformer_hybrid"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._resnetv2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._create_vision_transformer_hybrid"], ["", "@", "register_model", "\n", "def", "vit_small_r26_s32_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" R26+ViT-S/S32 hybrid.\n    \"\"\"", "\n", "backbone", "=", "_resnetv2", "(", "(", "2", ",", "2", ",", "2", ",", "2", ")", ",", "**", "kwargs", ")", "\n", "model_kwargs", "=", "dict", "(", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_hybrid", "(", "\n", "'vit_small_r26_s32_224'", ",", "backbone", "=", "backbone", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_small_r26_s32_384": [[197, 206], ["vision_transformer_hybrid._resnetv2", "dict", "vision_transformer_hybrid._create_vision_transformer_hybrid"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._resnetv2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._create_vision_transformer_hybrid"], ["", "@", "register_model", "\n", "def", "vit_small_r26_s32_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" R26+ViT-S/S32 hybrid.\n    \"\"\"", "\n", "backbone", "=", "_resnetv2", "(", "(", "2", ",", "2", ",", "2", ",", "2", ")", ",", "**", "kwargs", ")", "\n", "model_kwargs", "=", "dict", "(", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_hybrid", "(", "\n", "'vit_small_r26_s32_384'", ",", "backbone", "=", "backbone", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_base_r26_s32_224": [[208, 217], ["vision_transformer_hybrid._resnetv2", "dict", "vision_transformer_hybrid._create_vision_transformer_hybrid"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._resnetv2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._create_vision_transformer_hybrid"], ["", "@", "register_model", "\n", "def", "vit_base_r26_s32_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" R26+ViT-B/S32 hybrid.\n    \"\"\"", "\n", "backbone", "=", "_resnetv2", "(", "(", "2", ",", "2", ",", "2", ",", "2", ")", ",", "**", "kwargs", ")", "\n", "model_kwargs", "=", "dict", "(", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_hybrid", "(", "\n", "'vit_base_r26_s32_224'", ",", "backbone", "=", "backbone", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_base_r50_s16_224": [[219, 228], ["vision_transformer_hybrid._resnetv2", "dict", "vision_transformer_hybrid._create_vision_transformer_hybrid"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._resnetv2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._create_vision_transformer_hybrid"], ["", "@", "register_model", "\n", "def", "vit_base_r50_s16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" R50+ViT-B/S16 hybrid from original paper (https://arxiv.org/abs/2010.11929).\n    \"\"\"", "\n", "backbone", "=", "_resnetv2", "(", "(", "3", ",", "4", ",", "9", ")", ",", "**", "kwargs", ")", "\n", "model_kwargs", "=", "dict", "(", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_hybrid", "(", "\n", "'vit_base_r50_s16_224'", ",", "backbone", "=", "backbone", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_base_r50_s16_384": [[230, 240], ["vision_transformer_hybrid._resnetv2", "dict", "vision_transformer_hybrid._create_vision_transformer_hybrid"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._resnetv2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._create_vision_transformer_hybrid"], ["", "@", "register_model", "\n", "def", "vit_base_r50_s16_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" R50+ViT-B/16 hybrid from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.\n    \"\"\"", "\n", "backbone", "=", "_resnetv2", "(", "(", "3", ",", "4", ",", "9", ")", ",", "**", "kwargs", ")", "\n", "model_kwargs", "=", "dict", "(", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_hybrid", "(", "\n", "'vit_base_r50_s16_384'", ",", "backbone", "=", "backbone", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_base_resnet50_384": [[242, 246], ["vision_transformer_hybrid.vit_base_r50_s16_384"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_base_r50_s16_384"], ["", "@", "register_model", "\n", "def", "vit_base_resnet50_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "# DEPRECATED this is forwarding to model def above for backwards compatibility", "\n", "    ", "return", "vit_base_r50_s16_384", "(", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_large_r50_s32_224": [[248, 257], ["vision_transformer_hybrid._resnetv2", "dict", "vision_transformer_hybrid._create_vision_transformer_hybrid"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._resnetv2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._create_vision_transformer_hybrid"], ["", "@", "register_model", "\n", "def", "vit_large_r50_s32_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" R50+ViT-L/S32 hybrid.\n    \"\"\"", "\n", "backbone", "=", "_resnetv2", "(", "(", "3", ",", "4", ",", "6", ",", "3", ")", ",", "**", "kwargs", ")", "\n", "model_kwargs", "=", "dict", "(", "embed_dim", "=", "1024", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_hybrid", "(", "\n", "'vit_large_r50_s32_224'", ",", "backbone", "=", "backbone", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_large_r50_s32_384": [[259, 268], ["vision_transformer_hybrid._resnetv2", "dict", "vision_transformer_hybrid._create_vision_transformer_hybrid"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._resnetv2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._create_vision_transformer_hybrid"], ["", "@", "register_model", "\n", "def", "vit_large_r50_s32_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" R50+ViT-L/S32 hybrid.\n    \"\"\"", "\n", "backbone", "=", "_resnetv2", "(", "(", "3", ",", "4", ",", "6", ",", "3", ")", ",", "**", "kwargs", ")", "\n", "model_kwargs", "=", "dict", "(", "embed_dim", "=", "1024", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_hybrid", "(", "\n", "'vit_large_r50_s32_384'", ",", "backbone", "=", "backbone", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_tiny_r_s16_p8_224_in21k": [[270, 279], ["vision_transformer_hybrid._resnetv2", "dict", "vision_transformer_hybrid._create_vision_transformer_hybrid"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._resnetv2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._create_vision_transformer_hybrid"], ["", "@", "register_model", "\n", "def", "vit_tiny_r_s16_p8_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" R+ViT-Ti/S16 w/ 8x8 patch hybrid.  ImageNet-21k.\n    \"\"\"", "\n", "backbone", "=", "_resnetv2", "(", "layers", "=", "(", ")", ",", "**", "kwargs", ")", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "8", ",", "embed_dim", "=", "192", ",", "depth", "=", "12", ",", "num_heads", "=", "3", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_hybrid", "(", "\n", "'vit_tiny_r_s16_p8_224_in21k'", ",", "backbone", "=", "backbone", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_small_r26_s32_224_in21k": [[281, 290], ["vision_transformer_hybrid._resnetv2", "dict", "vision_transformer_hybrid._create_vision_transformer_hybrid"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._resnetv2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._create_vision_transformer_hybrid"], ["", "@", "register_model", "\n", "def", "vit_small_r26_s32_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" R26+ViT-S/S32 hybrid. ImageNet-21k.\n    \"\"\"", "\n", "backbone", "=", "_resnetv2", "(", "(", "2", ",", "2", ",", "2", ",", "2", ")", ",", "**", "kwargs", ")", "\n", "model_kwargs", "=", "dict", "(", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_hybrid", "(", "\n", "'vit_small_r26_s32_224_in21k'", ",", "backbone", "=", "backbone", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_base_r50_s16_224_in21k": [[292, 302], ["vision_transformer_hybrid._resnetv2", "dict", "vision_transformer_hybrid._create_vision_transformer_hybrid"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._resnetv2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._create_vision_transformer_hybrid"], ["", "@", "register_model", "\n", "def", "vit_base_r50_s16_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" R50+ViT-B/16 hybrid model from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n    \"\"\"", "\n", "backbone", "=", "_resnetv2", "(", "layers", "=", "(", "3", ",", "4", ",", "9", ")", ",", "**", "kwargs", ")", "\n", "model_kwargs", "=", "dict", "(", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_hybrid", "(", "\n", "'vit_base_r50_s16_224_in21k'", ",", "backbone", "=", "backbone", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_base_resnet50_224_in21k": [[304, 308], ["vision_transformer_hybrid.vit_base_r50_s16_224_in21k"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_base_r50_s16_224_in21k"], ["", "@", "register_model", "\n", "def", "vit_base_resnet50_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "# DEPRECATED this is forwarding to model def above for backwards compatibility", "\n", "    ", "return", "vit_base_r50_s16_224_in21k", "(", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_large_r50_s32_224_in21k": [[310, 319], ["vision_transformer_hybrid._resnetv2", "dict", "vision_transformer_hybrid._create_vision_transformer_hybrid"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._resnetv2", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._create_vision_transformer_hybrid"], ["", "@", "register_model", "\n", "def", "vit_large_r50_s32_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" R50+ViT-L/S32 hybrid. ImageNet-21k.\n    \"\"\"", "\n", "backbone", "=", "_resnetv2", "(", "(", "3", ",", "4", ",", "6", ",", "3", ")", ",", "**", "kwargs", ")", "\n", "model_kwargs", "=", "dict", "(", "embed_dim", "=", "1024", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_hybrid", "(", "\n", "'vit_large_r50_s32_224_in21k'", ",", "backbone", "=", "backbone", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_small_resnet26d_224": [[321, 330], ["resnet.resnet26d", "dict", "vision_transformer_hybrid._create_vision_transformer_hybrid", "kwargs.get"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet26d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._create_vision_transformer_hybrid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["", "@", "register_model", "\n", "def", "vit_small_resnet26d_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Custom ViT small hybrid w/ ResNet26D stride 32. No pretrained weights.\n    \"\"\"", "\n", "backbone", "=", "resnet26d", "(", "pretrained", "=", "pretrained", ",", "in_chans", "=", "kwargs", ".", "get", "(", "'in_chans'", ",", "3", ")", ",", "features_only", "=", "True", ",", "out_indices", "=", "[", "4", "]", ")", "\n", "model_kwargs", "=", "dict", "(", "embed_dim", "=", "768", ",", "depth", "=", "8", ",", "num_heads", "=", "8", ",", "mlp_ratio", "=", "3", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_hybrid", "(", "\n", "'vit_small_resnet26d_224'", ",", "backbone", "=", "backbone", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_small_resnet50d_s16_224": [[332, 341], ["resnet.resnet50d", "dict", "vision_transformer_hybrid._create_vision_transformer_hybrid", "kwargs.get"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet50d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._create_vision_transformer_hybrid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["", "@", "register_model", "\n", "def", "vit_small_resnet50d_s16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Custom ViT small hybrid w/ ResNet50D 3-stages, stride 16. No pretrained weights.\n    \"\"\"", "\n", "backbone", "=", "resnet50d", "(", "pretrained", "=", "pretrained", ",", "in_chans", "=", "kwargs", ".", "get", "(", "'in_chans'", ",", "3", ")", ",", "features_only", "=", "True", ",", "out_indices", "=", "[", "3", "]", ")", "\n", "model_kwargs", "=", "dict", "(", "embed_dim", "=", "768", ",", "depth", "=", "8", ",", "num_heads", "=", "8", ",", "mlp_ratio", "=", "3", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_hybrid", "(", "\n", "'vit_small_resnet50d_s16_224'", ",", "backbone", "=", "backbone", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_base_resnet26d_224": [[343, 352], ["resnet.resnet26d", "dict", "vision_transformer_hybrid._create_vision_transformer_hybrid", "kwargs.get"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet26d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._create_vision_transformer_hybrid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["", "@", "register_model", "\n", "def", "vit_base_resnet26d_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Custom ViT base hybrid w/ ResNet26D stride 32. No pretrained weights.\n    \"\"\"", "\n", "backbone", "=", "resnet26d", "(", "pretrained", "=", "pretrained", ",", "in_chans", "=", "kwargs", ".", "get", "(", "'in_chans'", ",", "3", ")", ",", "features_only", "=", "True", ",", "out_indices", "=", "[", "4", "]", ")", "\n", "model_kwargs", "=", "dict", "(", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_hybrid", "(", "\n", "'vit_base_resnet26d_224'", ",", "backbone", "=", "backbone", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid.vit_base_resnet50d_224": [[354, 363], ["resnet.resnet50d", "dict", "vision_transformer_hybrid._create_vision_transformer_hybrid", "kwargs.get"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet50d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_hybrid._create_vision_transformer_hybrid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["", "@", "register_model", "\n", "def", "vit_base_resnet50d_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Custom ViT base hybrid w/ ResNet50D stride 32. No pretrained weights.\n    \"\"\"", "\n", "backbone", "=", "resnet50d", "(", "pretrained", "=", "pretrained", ",", "in_chans", "=", "kwargs", ".", "get", "(", "'in_chans'", ",", "3", ")", ",", "features_only", "=", "True", ",", "out_indices", "=", "[", "4", "]", ")", "\n", "model_kwargs", "=", "dict", "(", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_hybrid", "(", "\n", "'vit_base_resnet50d_224'", ",", "backbone", "=", "backbone", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg.ConvMlp.__init__": [[59, 70], ["torch.Module.__init__", "int", "conv_layer", "act_layer", "torch.Dropout", "torch.Dropout", "torch.Dropout", "conv_layer", "act_layer"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_features", "=", "512", ",", "out_features", "=", "4096", ",", "kernel_size", "=", "7", ",", "mlp_ratio", "=", "1.0", ",", "\n", "drop_rate", ":", "float", "=", "0.2", ",", "act_layer", ":", "nn", ".", "Module", "=", "None", ",", "conv_layer", ":", "nn", ".", "Module", "=", "None", ")", ":", "\n", "        ", "super", "(", "ConvMlp", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_kernel_size", "=", "kernel_size", "\n", "mid_features", "=", "int", "(", "out_features", "*", "mlp_ratio", ")", "\n", "self", ".", "fc1", "=", "conv_layer", "(", "in_features", ",", "mid_features", ",", "kernel_size", ",", "bias", "=", "True", ")", "\n", "self", ".", "act1", "=", "act_layer", "(", "True", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "drop_rate", ")", "\n", "self", ".", "fc2", "=", "conv_layer", "(", "mid_features", ",", "out_features", ",", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "act2", "=", "act_layer", "(", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg.ConvMlp.forward": [[71, 82], ["vgg.ConvMlp.fc1", "vgg.ConvMlp.act1", "vgg.ConvMlp.drop", "vgg.ConvMlp.fc2", "vgg.ConvMlp.act2", "torch.adaptive_avg_pool2d", "torch.adaptive_avg_pool2d", "torch.adaptive_avg_pool2d", "max", "max"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "x", ".", "shape", "[", "-", "2", "]", "<", "self", ".", "input_kernel_size", "or", "x", ".", "shape", "[", "-", "1", "]", "<", "self", ".", "input_kernel_size", ":", "\n", "# keep the input size >= 7x7", "\n", "            ", "output_size", "=", "(", "max", "(", "self", ".", "input_kernel_size", ",", "x", ".", "shape", "[", "-", "2", "]", ")", ",", "max", "(", "self", ".", "input_kernel_size", ",", "x", ".", "shape", "[", "-", "1", "]", ")", ")", "\n", "x", "=", "F", ".", "adaptive_avg_pool2d", "(", "x", ",", "output_size", ")", "\n", "", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "x", "=", "self", ".", "act1", "(", "x", ")", "\n", "x", "=", "self", ".", "drop", "(", "x", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "self", ".", "act2", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg.VGG.__init__": [[86, 135], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "vgg.VGG.feature_info.append", "vgg.ConvMlp", "layers.ClassifierHead", "vgg.VGG._initialize_weights", "dict", "len", "vgg.VGG.feature_info.append", "typing.cast", "conv_layer", "dict", "pool_layer", "norm_layer", "act_layer", "act_layer", "len"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg.VGG._initialize_weights"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "cfg", ":", "List", "[", "Any", "]", ",", "\n", "num_classes", ":", "int", "=", "1000", ",", "\n", "in_chans", ":", "int", "=", "3", ",", "\n", "output_stride", ":", "int", "=", "32", ",", "\n", "mlp_ratio", ":", "float", "=", "1.0", ",", "\n", "act_layer", ":", "nn", ".", "Module", "=", "nn", ".", "ReLU", ",", "\n", "conv_layer", ":", "nn", ".", "Module", "=", "nn", ".", "Conv2d", ",", "\n", "norm_layer", ":", "nn", ".", "Module", "=", "None", ",", "\n", "global_pool", ":", "str", "=", "'avg'", ",", "\n", "drop_rate", ":", "float", "=", "0.", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", "VGG", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "output_stride", "==", "32", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "num_features", "=", "4096", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "self", ".", "use_norm", "=", "norm_layer", "is", "not", "None", "\n", "self", ".", "feature_info", "=", "[", "]", "\n", "prev_chs", "=", "in_chans", "\n", "net_stride", "=", "1", "\n", "pool_layer", "=", "nn", ".", "MaxPool2d", "\n", "layers", ":", "List", "[", "nn", ".", "Module", "]", "=", "[", "]", "\n", "for", "v", "in", "cfg", ":", "\n", "            ", "last_idx", "=", "len", "(", "layers", ")", "-", "1", "\n", "if", "v", "==", "'M'", ":", "\n", "                ", "self", ".", "feature_info", ".", "append", "(", "dict", "(", "num_chs", "=", "prev_chs", ",", "reduction", "=", "net_stride", ",", "module", "=", "f'features.{last_idx}'", ")", ")", "\n", "layers", "+=", "[", "pool_layer", "(", "kernel_size", "=", "2", ",", "stride", "=", "2", ")", "]", "\n", "net_stride", "*=", "2", "\n", "", "else", ":", "\n", "                ", "v", "=", "cast", "(", "int", ",", "v", ")", "\n", "conv2d", "=", "conv_layer", "(", "prev_chs", ",", "v", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ")", "\n", "if", "norm_layer", "is", "not", "None", ":", "\n", "                    ", "layers", "+=", "[", "conv2d", ",", "norm_layer", "(", "v", ")", ",", "act_layer", "(", "inplace", "=", "True", ")", "]", "\n", "", "else", ":", "\n", "                    ", "layers", "+=", "[", "conv2d", ",", "act_layer", "(", "inplace", "=", "True", ")", "]", "\n", "", "prev_chs", "=", "v", "\n", "", "", "self", ".", "features", "=", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "self", ".", "feature_info", ".", "append", "(", "dict", "(", "num_chs", "=", "prev_chs", ",", "reduction", "=", "net_stride", ",", "module", "=", "f'features.{len(layers) - 1}'", ")", ")", "\n", "\n", "self", ".", "pre_logits", "=", "ConvMlp", "(", "\n", "prev_chs", ",", "self", ".", "num_features", ",", "7", ",", "mlp_ratio", "=", "mlp_ratio", ",", "\n", "drop_rate", "=", "drop_rate", ",", "act_layer", "=", "act_layer", ",", "conv_layer", "=", "conv_layer", ")", "\n", "self", ".", "head", "=", "ClassifierHead", "(", "\n", "self", ".", "num_features", ",", "num_classes", ",", "pool_type", "=", "global_pool", ",", "drop_rate", "=", "drop_rate", ")", "\n", "\n", "self", ".", "_initialize_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg.VGG.group_matcher": [[136, 140], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "# this treats BN layers as separate groups for bn variants, a lot of effort to fix that", "\n", "        ", "return", "dict", "(", "stem", "=", "r'^features\\.0'", ",", "blocks", "=", "r'^features\\.(\\d+)'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg.VGG.set_grad_checkpointing": [[141, 144], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "'gradient checkpointing not supported'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg.VGG.get_classifier": [[145, 148], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", ".", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg.VGG.reset_classifier": [[149, 153], ["layers.ClassifierHead"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "head", "=", "ClassifierHead", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ",", "drop_rate", "=", "self", ".", "drop_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg.VGG.forward_features": [[154, 157], ["vgg.VGG.features"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "x", "=", "self", ".", "features", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg.VGG.forward_head": [[158, 161], ["vgg.VGG.pre_logits", "vgg.VGG.head"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "x", "=", "self", ".", "pre_logits", "(", "x", ")", "\n", "return", "x", "if", "pre_logits", "else", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg.VGG.forward": [[162, 166], ["vgg.VGG.forward_features", "vgg.VGG.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg.VGG._initialize_weights": [[167, 179], ["vgg.VGG.modules", "isinstance", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "isinstance", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["None"], ["", "def", "_initialize_weights", "(", "self", ")", "->", "None", ":", "\n", "        ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "if", "m", ".", "bias", "is", "not", "None", ":", "\n", "                    ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "0", ",", "0.01", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg._cfg": [[25, 33], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bilinear'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'features.0'", ",", "'classifier'", ":", "'head.fc'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg._filter_fn": [[181, 195], ["state_dict.items", "k_r.replace.replace", "k_r.replace.replace", "k_r.replace.replace", "v.reshape.reshape", "v.reshape.reshape"], "function", ["None"], ["", "", "", "", "def", "_filter_fn", "(", "state_dict", ")", ":", "\n", "    ", "\"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"", "\n", "out_dict", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "        ", "k_r", "=", "k", "\n", "k_r", "=", "k_r", ".", "replace", "(", "'classifier.0'", ",", "'pre_logits.fc1'", ")", "\n", "k_r", "=", "k_r", ".", "replace", "(", "'classifier.3'", ",", "'pre_logits.fc2'", ")", "\n", "k_r", "=", "k_r", ".", "replace", "(", "'classifier.6'", ",", "'head.fc'", ")", "\n", "if", "'classifier.0.weight'", "in", "k", ":", "\n", "            ", "v", "=", "v", ".", "reshape", "(", "-", "1", ",", "512", ",", "7", ",", "7", ")", "\n", "", "if", "'classifier.3.weight'", "in", "k", ":", "\n", "            ", "v", "=", "v", ".", "reshape", "(", "-", "1", ",", "4096", ",", "1", ",", "1", ")", "\n", "", "out_dict", "[", "k_r", "]", "=", "v", "\n", "", "return", "out_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg._create_vgg": [[197, 208], ["kwargs.pop", "helpers.build_model_with_cfg", "variant.split", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "def", "_create_vgg", "(", "variant", ":", "str", ",", "pretrained", ":", "bool", ",", "**", "kwargs", ":", "Any", ")", "->", "VGG", ":", "\n", "    ", "cfg", "=", "variant", ".", "split", "(", "'_'", ")", "[", "0", "]", "\n", "# NOTE: VGG is one of few models with stride==1 features w/ 6 out_indices [0..5]", "\n", "out_indices", "=", "kwargs", ".", "pop", "(", "'out_indices'", ",", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ",", "5", ")", ")", "\n", "model", "=", "build_model_with_cfg", "(", "\n", "VGG", ",", "variant", ",", "pretrained", ",", "\n", "model_cfg", "=", "cfgs", "[", "cfg", "]", ",", "\n", "feature_cfg", "=", "dict", "(", "flatten_sequential", "=", "True", ",", "out_indices", "=", "out_indices", ")", ",", "\n", "pretrained_filter_fn", "=", "_filter_fn", ",", "\n", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg.vgg11": [[210, 217], ["dict", "vgg._create_vgg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg._create_vgg"], ["", "@", "register_model", "\n", "def", "vgg11", "(", "pretrained", ":", "bool", "=", "False", ",", "**", "kwargs", ":", "Any", ")", "->", "VGG", ":", "\n", "    ", "r\"\"\"VGG 11-layer model (configuration \"A\") from\n    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "**", "kwargs", ")", "\n", "return", "_create_vgg", "(", "'vgg11'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg.vgg11_bn": [[219, 226], ["dict", "vgg._create_vgg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg._create_vgg"], ["", "@", "register_model", "\n", "def", "vgg11_bn", "(", "pretrained", ":", "bool", "=", "False", ",", "**", "kwargs", ":", "Any", ")", "->", "VGG", ":", "\n", "    ", "r\"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "**", "kwargs", ")", "\n", "return", "_create_vgg", "(", "'vgg11_bn'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg.vgg13": [[228, 235], ["dict", "vgg._create_vgg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg._create_vgg"], ["", "@", "register_model", "\n", "def", "vgg13", "(", "pretrained", ":", "bool", "=", "False", ",", "**", "kwargs", ":", "Any", ")", "->", "VGG", ":", "\n", "    ", "r\"\"\"VGG 13-layer model (configuration \"B\")\n    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "**", "kwargs", ")", "\n", "return", "_create_vgg", "(", "'vgg13'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg.vgg13_bn": [[237, 244], ["dict", "vgg._create_vgg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg._create_vgg"], ["", "@", "register_model", "\n", "def", "vgg13_bn", "(", "pretrained", ":", "bool", "=", "False", ",", "**", "kwargs", ":", "Any", ")", "->", "VGG", ":", "\n", "    ", "r\"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "**", "kwargs", ")", "\n", "return", "_create_vgg", "(", "'vgg13_bn'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg.vgg16": [[246, 253], ["dict", "vgg._create_vgg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg._create_vgg"], ["", "@", "register_model", "\n", "def", "vgg16", "(", "pretrained", ":", "bool", "=", "False", ",", "**", "kwargs", ":", "Any", ")", "->", "VGG", ":", "\n", "    ", "r\"\"\"VGG 16-layer model (configuration \"D\")\n    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "**", "kwargs", ")", "\n", "return", "_create_vgg", "(", "'vgg16'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg.vgg16_bn": [[255, 262], ["dict", "vgg._create_vgg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg._create_vgg"], ["", "@", "register_model", "\n", "def", "vgg16_bn", "(", "pretrained", ":", "bool", "=", "False", ",", "**", "kwargs", ":", "Any", ")", "->", "VGG", ":", "\n", "    ", "r\"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "**", "kwargs", ")", "\n", "return", "_create_vgg", "(", "'vgg16_bn'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg.vgg19": [[264, 271], ["dict", "vgg._create_vgg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg._create_vgg"], ["", "@", "register_model", "\n", "def", "vgg19", "(", "pretrained", ":", "bool", "=", "False", ",", "**", "kwargs", ":", "Any", ")", "->", "VGG", ":", "\n", "    ", "r\"\"\"VGG 19-layer model (configuration \"E\")\n    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "**", "kwargs", ")", "\n", "return", "_create_vgg", "(", "'vgg19'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg.vgg19_bn": [[273, 280], ["dict", "vgg._create_vgg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vgg._create_vgg"], ["", "@", "register_model", "\n", "def", "vgg19_bn", "(", "pretrained", ":", "bool", "=", "False", ",", "**", "kwargs", ":", "Any", ")", "->", "VGG", ":", "\n", "    ", "r\"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "**", "kwargs", ")", "\n", "return", "_create_vgg", "(", "'vgg19_bn'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.WindowMultiHeadAttention.__init__": [[160, 193], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "layers.Mlp", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "swin_transformer_v2_cr.WindowMultiHeadAttention._make_pair_wise_relative_positions", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.WindowMultiHeadAttention._make_pair_wise_relative_positions"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dim", ":", "int", ",", "\n", "num_heads", ":", "int", ",", "\n", "window_size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "\n", "drop_attn", ":", "float", "=", "0.0", ",", "\n", "drop_proj", ":", "float", "=", "0.0", ",", "\n", "meta_hidden_dim", ":", "int", "=", "384", ",", "# FIXME what's the optimal value?", "\n", "sequential_attn", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", "WindowMultiHeadAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "dim", "%", "num_heads", "==", "0", ",", "\"The number of input features (in_features) are not divisible by the number of heads (num_heads).\"", "\n", "self", ".", "in_features", ":", "int", "=", "dim", "\n", "self", ".", "window_size", ":", "Tuple", "[", "int", ",", "int", "]", "=", "window_size", "\n", "self", ".", "num_heads", ":", "int", "=", "num_heads", "\n", "self", ".", "sequential_attn", ":", "bool", "=", "sequential_attn", "\n", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "in_features", "=", "dim", ",", "out_features", "=", "dim", "*", "3", ",", "bias", "=", "True", ")", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "drop_attn", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "in_features", "=", "dim", ",", "out_features", "=", "dim", ",", "bias", "=", "True", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "drop_proj", ")", "\n", "# meta network for positional encodings", "\n", "self", ".", "meta_mlp", "=", "Mlp", "(", "\n", "2", ",", "# x, y", "\n", "hidden_features", "=", "meta_hidden_dim", ",", "\n", "out_features", "=", "num_heads", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "\n", "drop", "=", "(", "0.125", ",", "0.", ")", "# FIXME should there be stochasticity, appears to 'overfit' without?", "\n", ")", "\n", "# NOTE old checkpoints used inverse of logit_scale ('tau') following the paper, see conversion fn", "\n", "self", ".", "logit_scale", "=", "nn", ".", "Parameter", "(", "torch", ".", "log", "(", "10", "*", "torch", ".", "ones", "(", "num_heads", ")", ")", ")", "\n", "self", ".", "_make_pair_wise_relative_positions", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.WindowMultiHeadAttention._make_pair_wise_relative_positions": [[194, 205], ["torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "relative_coordinates.permute().reshape().float.permute().reshape().float.permute().reshape().float", "swin_transformer_v2_cr.WindowMultiHeadAttention.register_buffer", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "relative_coordinates.permute().reshape().float.permute().reshape().float.permute().reshape", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "relative_coordinates.permute().reshape().float.permute().reshape().float.abs", "relative_coordinates.permute().reshape().float.permute().reshape().float.permute", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "def", "_make_pair_wise_relative_positions", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Method initializes the pair-wise relative positions to compute the positional biases.\"\"\"", "\n", "device", "=", "self", ".", "logit_scale", ".", "device", "\n", "coordinates", "=", "torch", ".", "stack", "(", "torch", ".", "meshgrid", "(", "[", "\n", "torch", ".", "arange", "(", "self", ".", "window_size", "[", "0", "]", ",", "device", "=", "device", ")", ",", "\n", "torch", ".", "arange", "(", "self", ".", "window_size", "[", "1", "]", ",", "device", "=", "device", ")", "]", ")", ",", "dim", "=", "0", ")", ".", "flatten", "(", "1", ")", "\n", "relative_coordinates", "=", "coordinates", "[", ":", ",", ":", ",", "None", "]", "-", "coordinates", "[", ":", ",", "None", ",", ":", "]", "\n", "relative_coordinates", "=", "relative_coordinates", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ".", "reshape", "(", "-", "1", ",", "2", ")", ".", "float", "(", ")", "\n", "relative_coordinates_log", "=", "torch", ".", "sign", "(", "relative_coordinates", ")", "*", "torch", ".", "log", "(", "\n", "1.0", "+", "relative_coordinates", ".", "abs", "(", ")", ")", "\n", "self", ".", "register_buffer", "(", "\"relative_coordinates_log\"", ",", "relative_coordinates_log", ",", "persistent", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.WindowMultiHeadAttention.update_input_size": [[206, 216], ["swin_transformer_v2_cr.WindowMultiHeadAttention._make_pair_wise_relative_positions"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.WindowMultiHeadAttention._make_pair_wise_relative_positions"], ["", "def", "update_input_size", "(", "self", ",", "new_window_size", ":", "int", ",", "**", "kwargs", ":", "Any", ")", "->", "None", ":", "\n", "        ", "\"\"\"Method updates the window size and so the pair-wise relative positions\n\n        Args:\n            new_window_size (int): New window size\n            kwargs (Any): Unused\n        \"\"\"", "\n", "# Set new window size and new pair-wise relative positions", "\n", "self", ".", "window_size", ":", "int", "=", "new_window_size", "\n", "self", ".", "_make_pair_wise_relative_positions", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.WindowMultiHeadAttention._relative_positional_encodings": [[217, 231], ["swin_transformer_v2_cr.WindowMultiHeadAttention.meta_mlp", "relative_position_bias.unsqueeze.unsqueeze.transpose().reshape", "relative_position_bias.unsqueeze.unsqueeze.unsqueeze", "relative_position_bias.unsqueeze.unsqueeze.transpose"], "methods", ["None"], ["", "def", "_relative_positional_encodings", "(", "self", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"Method computes the relative positional encodings\n\n        Returns:\n            relative_position_bias (torch.Tensor): Relative positional encodings\n            (1, number of heads, window size ** 2, window size ** 2)\n        \"\"\"", "\n", "window_area", "=", "self", ".", "window_size", "[", "0", "]", "*", "self", ".", "window_size", "[", "1", "]", "\n", "relative_position_bias", "=", "self", ".", "meta_mlp", "(", "self", ".", "relative_coordinates_log", ")", "\n", "relative_position_bias", "=", "relative_position_bias", ".", "transpose", "(", "1", ",", "0", ")", ".", "reshape", "(", "\n", "self", ".", "num_heads", ",", "window_area", ",", "window_area", "\n", ")", "\n", "relative_position_bias", "=", "relative_position_bias", ".", "unsqueeze", "(", "0", ")", "\n", "return", "relative_position_bias", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.WindowMultiHeadAttention._forward_sequential": [[232, 241], ["None"], "methods", ["None"], ["", "def", "_forward_sequential", "(", "\n", "self", ",", "\n", "x", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        \"\"\"", "\n", "# FIXME TODO figure out 'sequential' attention mentioned in paper (should reduce GPU memory)", "\n", "assert", "False", ",", "\"not implemented\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.WindowMultiHeadAttention._forward_batch": [[242, 273], ["swin_transformer_v2_cr.WindowMultiHeadAttention.qkv().view().permute", "swin_transformer_v2_cr.WindowMultiHeadAttention.unbind", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "attn.view.view.softmax", "swin_transformer_v2_cr.WindowMultiHeadAttention.attn_drop", "swin_transformer_v2_cr.WindowMultiHeadAttention.proj", "swin_transformer_v2_cr.WindowMultiHeadAttention.proj_drop", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize().transpose", "torch.normalize().transpose", "torch.normalize().transpose", "torch.normalize().transpose", "swin_transformer_v2_cr.WindowMultiHeadAttention._relative_positional_encodings", "attn.view.view.view", "attn.view.view.view", "swin_transformer_v2_cr.WindowMultiHeadAttention.qkv().view", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "mask.unsqueeze().unsqueeze", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "swin_transformer_v2_cr.WindowMultiHeadAttention.logit_scale.reshape", "swin_transformer_v2_cr.WindowMultiHeadAttention.qkv", "math.log", "mask.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.WindowMultiHeadAttention._relative_positional_encodings"], ["", "def", "_forward_batch", "(", "\n", "self", ",", "\n", "x", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"This function performs standard (non-sequential) scaled cosine self-attention.\n        \"\"\"", "\n", "Bw", ",", "L", ",", "C", "=", "x", ".", "shape", "\n", "\n", "qkv", "=", "self", ".", "qkv", "(", "x", ")", ".", "view", "(", "Bw", ",", "L", ",", "3", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "query", ",", "key", ",", "value", "=", "qkv", ".", "unbind", "(", "0", ")", "\n", "\n", "# compute attention map with scaled cosine attention", "\n", "attn", "=", "(", "F", ".", "normalize", "(", "query", ",", "dim", "=", "-", "1", ")", "@", "F", ".", "normalize", "(", "key", ",", "dim", "=", "-", "1", ")", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "\n", "logit_scale", "=", "torch", ".", "clamp", "(", "self", ".", "logit_scale", ".", "reshape", "(", "1", ",", "self", ".", "num_heads", ",", "1", ",", "1", ")", ",", "max", "=", "math", ".", "log", "(", "1.", "/", "0.01", ")", ")", ".", "exp", "(", ")", "\n", "attn", "=", "attn", "*", "logit_scale", "\n", "attn", "=", "attn", "+", "self", ".", "_relative_positional_encodings", "(", ")", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "# Apply mask if utilized", "\n", "            ", "num_win", ":", "int", "=", "mask", ".", "shape", "[", "0", "]", "\n", "attn", "=", "attn", ".", "view", "(", "Bw", "//", "num_win", ",", "num_win", ",", "self", ".", "num_heads", ",", "L", ",", "L", ")", "\n", "attn", "=", "attn", "+", "mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "0", ")", "\n", "attn", "=", "attn", ".", "view", "(", "-", "1", ",", "self", ".", "num_heads", ",", "L", ",", "L", ")", "\n", "", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "x", "=", "(", "attn", "@", "value", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "Bw", ",", "L", ",", "-", "1", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.WindowMultiHeadAttention.forward": [[274, 287], ["swin_transformer_v2_cr.WindowMultiHeadAttention._forward_sequential", "swin_transformer_v2_cr.WindowMultiHeadAttention._forward_batch"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.WindowMultiHeadAttention._forward_sequential", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.WindowMultiHeadAttention._forward_batch"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ",", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\" Forward pass.\n        Args:\n            x (torch.Tensor): Input tensor of the shape (B * windows, N, C)\n            mask (Optional[torch.Tensor]): Attention mask for the shift case\n\n        Returns:\n            Output tensor of the shape [B * windows, N, C]\n        \"\"\"", "\n", "if", "self", ".", "sequential_attn", ":", "\n", "            ", "return", "self", ".", "_forward_sequential", "(", "x", ",", "mask", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_forward_batch", "(", "x", ",", "mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerBlock.__init__": [[307, 359], ["torch.Module.__init__", "layers.to_2tuple", "swin_transformer_v2_cr.SwinTransformerBlock._calc_window_shift", "swin_transformer_v2_cr.WindowMultiHeadAttention", "norm_layer", "layers.Mlp", "norm_layer", "swin_transformer_v2_cr.SwinTransformerBlock._make_attention_mask", "swin_transformer_v2_cr.SwinTransformerBlock.init_weights", "layers.to_2tuple", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "norm_layer", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "int"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerBlock._calc_window_shift", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerBlock._make_attention_mask", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dim", ":", "int", ",", "\n", "num_heads", ":", "int", ",", "\n", "feat_size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "\n", "window_size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "\n", "shift_size", ":", "Tuple", "[", "int", ",", "int", "]", "=", "(", "0", ",", "0", ")", ",", "\n", "mlp_ratio", ":", "float", "=", "4.0", ",", "\n", "init_values", ":", "Optional", "[", "float", "]", "=", "0", ",", "\n", "drop", ":", "float", "=", "0.0", ",", "\n", "drop_attn", ":", "float", "=", "0.0", ",", "\n", "drop_path", ":", "float", "=", "0.0", ",", "\n", "extra_norm", ":", "bool", "=", "False", ",", "\n", "sequential_attn", ":", "bool", "=", "False", ",", "\n", "norm_layer", ":", "Type", "[", "nn", ".", "Module", "]", "=", "nn", ".", "LayerNorm", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", "SwinTransformerBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dim", ":", "int", "=", "dim", "\n", "self", ".", "feat_size", ":", "Tuple", "[", "int", ",", "int", "]", "=", "feat_size", "\n", "self", ".", "target_shift_size", ":", "Tuple", "[", "int", ",", "int", "]", "=", "to_2tuple", "(", "shift_size", ")", "\n", "self", ".", "window_size", ",", "self", ".", "shift_size", "=", "self", ".", "_calc_window_shift", "(", "to_2tuple", "(", "window_size", ")", ")", "\n", "self", ".", "window_area", "=", "self", ".", "window_size", "[", "0", "]", "*", "self", ".", "window_size", "[", "1", "]", "\n", "self", ".", "init_values", ":", "Optional", "[", "float", "]", "=", "init_values", "\n", "\n", "# attn branch", "\n", "self", ".", "attn", "=", "WindowMultiHeadAttention", "(", "\n", "dim", "=", "dim", ",", "\n", "num_heads", "=", "num_heads", ",", "\n", "window_size", "=", "self", ".", "window_size", ",", "\n", "drop_attn", "=", "drop_attn", ",", "\n", "drop_proj", "=", "drop", ",", "\n", "sequential_attn", "=", "sequential_attn", ",", "\n", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "drop_path1", "=", "DropPath", "(", "drop_prob", "=", "drop_path", ")", "if", "drop_path", ">", "0.0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "# mlp branch", "\n", "self", ".", "mlp", "=", "Mlp", "(", "\n", "in_features", "=", "dim", ",", "\n", "hidden_features", "=", "int", "(", "dim", "*", "mlp_ratio", ")", ",", "\n", "drop", "=", "drop", ",", "\n", "out_features", "=", "dim", ",", "\n", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "drop_path2", "=", "DropPath", "(", "drop_prob", "=", "drop_path", ")", "if", "drop_path", ">", "0.0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "# Extra main branch norm layer mentioned for Huge/Giant models in V2 paper.", "\n", "# Also being used as final network norm and optional stage ending norm while still in a C-last format.", "\n", "self", ".", "norm3", "=", "norm_layer", "(", "dim", ")", "if", "extra_norm", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "_make_attention_mask", "(", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerBlock._calc_window_shift": [[360, 364], ["tuple", "tuple", "zip", "zip"], "methods", ["None"], ["", "def", "_calc_window_shift", "(", "self", ",", "target_window_size", ")", ":", "\n", "        ", "window_size", "=", "[", "f", "if", "f", "<=", "w", "else", "w", "for", "f", ",", "w", "in", "zip", "(", "self", ".", "feat_size", ",", "target_window_size", ")", "]", "\n", "shift_size", "=", "[", "0", "if", "f", "<=", "w", "else", "s", "for", "f", ",", "w", ",", "s", "in", "zip", "(", "self", ".", "feat_size", ",", "window_size", ",", "self", ".", "target_shift_size", ")", "]", "\n", "return", "tuple", "(", "window_size", ")", ",", "tuple", "(", "shift_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerBlock._make_attention_mask": [[365, 390], ["any", "swin_transformer_v2_cr.SwinTransformerBlock.register_buffer", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "swin_transformer_v2_cr.window_partition", "mask_windows.view.view.view", "attn_mask.masked_fill().masked_fill.masked_fill().masked_fill.masked_fill().masked_fill", "slice", "slice", "slice", "mask_windows.view.view.unsqueeze", "mask_windows.view.view.unsqueeze", "float", "slice", "slice", "slice", "attn_mask.masked_fill().masked_fill.masked_fill().masked_fill.masked_fill", "float"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.window_partition"], ["", "def", "_make_attention_mask", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"Method generates the attention mask used in shift case.\"\"\"", "\n", "# Make masks for shift case", "\n", "if", "any", "(", "self", ".", "shift_size", ")", ":", "\n", "# calculate attention mask for SW-MSA", "\n", "            ", "H", ",", "W", "=", "self", ".", "feat_size", "\n", "img_mask", "=", "torch", ".", "zeros", "(", "(", "1", ",", "H", ",", "W", ",", "1", ")", ")", "# 1 H W 1", "\n", "cnt", "=", "0", "\n", "for", "h", "in", "(", "\n", "slice", "(", "0", ",", "-", "self", ".", "window_size", "[", "0", "]", ")", ",", "\n", "slice", "(", "-", "self", ".", "window_size", "[", "0", "]", ",", "-", "self", ".", "shift_size", "[", "0", "]", ")", ",", "\n", "slice", "(", "-", "self", ".", "shift_size", "[", "0", "]", ",", "None", ")", ")", ":", "\n", "                ", "for", "w", "in", "(", "\n", "slice", "(", "0", ",", "-", "self", ".", "window_size", "[", "1", "]", ")", ",", "\n", "slice", "(", "-", "self", ".", "window_size", "[", "1", "]", ",", "-", "self", ".", "shift_size", "[", "1", "]", ")", ",", "\n", "slice", "(", "-", "self", ".", "shift_size", "[", "1", "]", ",", "None", ")", ")", ":", "\n", "                    ", "img_mask", "[", ":", ",", "h", ",", "w", ",", ":", "]", "=", "cnt", "\n", "cnt", "+=", "1", "\n", "", "", "mask_windows", "=", "window_partition", "(", "img_mask", ",", "self", ".", "window_size", ")", "# num_windows, window_size, window_size, 1", "\n", "mask_windows", "=", "mask_windows", ".", "view", "(", "-", "1", ",", "self", ".", "window_area", ")", "\n", "attn_mask", "=", "mask_windows", ".", "unsqueeze", "(", "1", ")", "-", "mask_windows", ".", "unsqueeze", "(", "2", ")", "\n", "attn_mask", "=", "attn_mask", ".", "masked_fill", "(", "attn_mask", "!=", "0", ",", "float", "(", "-", "100.0", ")", ")", ".", "masked_fill", "(", "attn_mask", "==", "0", ",", "float", "(", "0.0", ")", ")", "\n", "", "else", ":", "\n", "            ", "attn_mask", "=", "None", "\n", "", "self", ".", "register_buffer", "(", "\"attn_mask\"", ",", "attn_mask", ",", "persistent", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerBlock.init_weights": [[391, 396], ["torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "# extra, module specific weight init", "\n", "        ", "if", "self", ".", "init_values", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "norm1", ".", "weight", ",", "self", ".", "init_values", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "norm2", ".", "weight", ",", "self", ".", "init_values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerBlock.update_input_size": [[397, 410], ["swin_transformer_v2_cr.SwinTransformerBlock._calc_window_shift", "swin_transformer_v2_cr.SwinTransformerBlock.attn.update_input_size", "swin_transformer_v2_cr.SwinTransformerBlock._make_attention_mask", "layers.to_2tuple"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerBlock._calc_window_shift", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerV2Cr.update_input_size", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerBlock._make_attention_mask"], ["", "", "def", "update_input_size", "(", "self", ",", "new_window_size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "new_feat_size", ":", "Tuple", "[", "int", ",", "int", "]", ")", "->", "None", ":", "\n", "        ", "\"\"\"Method updates the image resolution to be processed and window size and so the pair-wise relative positions.\n\n        Args:\n            new_window_size (int): New window size\n            new_feat_size (Tuple[int, int]): New input resolution\n        \"\"\"", "\n", "# Update input resolution", "\n", "self", ".", "feat_size", ":", "Tuple", "[", "int", ",", "int", "]", "=", "new_feat_size", "\n", "self", ".", "window_size", ",", "self", ".", "shift_size", "=", "self", ".", "_calc_window_shift", "(", "to_2tuple", "(", "new_window_size", ")", ")", "\n", "self", ".", "window_area", "=", "self", ".", "window_size", "[", "0", "]", "*", "self", ".", "window_size", "[", "1", "]", "\n", "self", ".", "attn", ".", "update_input_size", "(", "new_window_size", "=", "self", ".", "window_size", ")", "\n", "self", ".", "_make_attention_mask", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerBlock._shifted_window_attn": [[411, 445], ["torch.roll.view", "torch.roll.view", "torch.roll.view", "torch.roll.view", "any", "swin_transformer_v2_cr.window_partition", "x_windows.view.view.view", "swin_transformer_v2_cr.SwinTransformerBlock.attn", "attn_windows.view.view.view", "swin_transformer_v2_cr.window_reverse", "torch.roll.view", "torch.roll.view", "torch.roll.view", "torch.roll.view", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.window_partition", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.window_reverse"], ["", "def", "_shifted_window_attn", "(", "self", ",", "x", ")", ":", "\n", "        ", "H", ",", "W", "=", "self", ".", "feat_size", "\n", "B", ",", "L", ",", "C", "=", "x", ".", "shape", "\n", "x", "=", "x", ".", "view", "(", "B", ",", "H", ",", "W", ",", "C", ")", "\n", "\n", "# cyclic shift", "\n", "sh", ",", "sw", "=", "self", ".", "shift_size", "\n", "do_shift", ":", "bool", "=", "any", "(", "self", ".", "shift_size", ")", "\n", "if", "do_shift", ":", "\n", "# FIXME PyTorch XLA needs cat impl, roll not lowered", "\n", "# x = torch.cat([x[:, sh:], x[:, :sh]], dim=1)", "\n", "# x = torch.cat([x[:, :, sw:], x[:, :, :sw]], dim=2)", "\n", "            ", "x", "=", "torch", ".", "roll", "(", "x", ",", "shifts", "=", "(", "-", "sh", ",", "-", "sw", ")", ",", "dims", "=", "(", "1", ",", "2", ")", ")", "\n", "\n", "# partition windows", "\n", "", "x_windows", "=", "window_partition", "(", "x", ",", "self", ".", "window_size", ")", "# num_windows * B, window_size, window_size, C", "\n", "x_windows", "=", "x_windows", ".", "view", "(", "-", "1", ",", "self", ".", "window_size", "[", "0", "]", "*", "self", ".", "window_size", "[", "1", "]", ",", "C", ")", "\n", "\n", "# W-MSA/SW-MSA", "\n", "attn_windows", "=", "self", ".", "attn", "(", "x_windows", ",", "mask", "=", "self", ".", "attn_mask", ")", "# num_windows * B, window_size * window_size, C", "\n", "\n", "# merge windows", "\n", "attn_windows", "=", "attn_windows", ".", "view", "(", "-", "1", ",", "self", ".", "window_size", "[", "0", "]", ",", "self", ".", "window_size", "[", "1", "]", ",", "C", ")", "\n", "x", "=", "window_reverse", "(", "attn_windows", ",", "self", ".", "window_size", ",", "self", ".", "feat_size", ")", "# B H' W' C", "\n", "\n", "# reverse cyclic shift", "\n", "if", "do_shift", ":", "\n", "# FIXME PyTorch XLA needs cat impl, roll not lowered", "\n", "# x = torch.cat([x[:, -sh:], x[:, :-sh]], dim=1)", "\n", "# x = torch.cat([x[:, :, -sw:], x[:, :, :-sw]], dim=2)", "\n", "            ", "x", "=", "torch", ".", "roll", "(", "x", ",", "shifts", "=", "(", "sh", ",", "sw", ")", ",", "dims", "=", "(", "1", ",", "2", ")", ")", "\n", "\n", "", "x", "=", "x", ".", "view", "(", "B", ",", "L", ",", "C", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerBlock.forward": [[446, 460], ["swin_transformer_v2_cr.SwinTransformerBlock.norm3", "swin_transformer_v2_cr.SwinTransformerBlock.drop_path1", "swin_transformer_v2_cr.SwinTransformerBlock.drop_path2", "swin_transformer_v2_cr.SwinTransformerBlock.norm1", "swin_transformer_v2_cr.SwinTransformerBlock.norm2", "swin_transformer_v2_cr.SwinTransformerBlock._shifted_window_attn", "swin_transformer_v2_cr.SwinTransformerBlock.mlp"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerBlock._shifted_window_attn"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"Forward pass.\n\n        Args:\n            x (torch.Tensor): Input tensor of the shape [B, C, H, W]\n\n        Returns:\n            output (torch.Tensor): Output tensor of the shape [B, C, H, W]\n        \"\"\"", "\n", "# post-norm branches (op -> norm -> drop)", "\n", "x", "=", "x", "+", "self", ".", "drop_path1", "(", "self", ".", "norm1", "(", "self", ".", "_shifted_window_attn", "(", "x", ")", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path2", "(", "self", ".", "norm2", "(", "self", ".", "mlp", "(", "x", ")", ")", ")", "\n", "x", "=", "self", ".", "norm3", "(", "x", ")", "# main-branch norm enabled for some blocks / stages (every 6 for Huge/Giant)", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.PatchMerging.__init__": [[469, 473], ["torch.Module.__init__", "norm_layer", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "dim", ":", "int", ",", "norm_layer", ":", "Type", "[", "nn", ".", "Module", "]", "=", "nn", ".", "LayerNorm", ")", "->", "None", ":", "\n", "        ", "super", "(", "PatchMerging", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm", "=", "norm_layer", "(", "4", "*", "dim", ")", "\n", "self", ".", "reduction", "=", "nn", ".", "Linear", "(", "in_features", "=", "4", "*", "dim", ",", "out_features", "=", "2", "*", "dim", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.PatchMerging.forward": [[474, 488], ["bhwc_to_bchw.reshape().permute().flatten", "swin_transformer_v2_cr.PatchMerging.norm", "swin_transformer_v2_cr.bhwc_to_bchw", "swin_transformer_v2_cr.PatchMerging.reduction", "bhwc_to_bchw.reshape().permute", "bhwc_to_bchw.reshape"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.bhwc_to_bchw", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.reduction"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\" Forward pass.\n        Args:\n            x (torch.Tensor): Input tensor of the shape [B, C, H, W]\n        Returns:\n            output (torch.Tensor): Output tensor of the shape [B, 2 * C, H // 2, W // 2]\n        \"\"\"", "\n", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "# unfold + BCHW -> BHWC together", "\n", "# ordering, 5, 3, 1 instead of 3, 5, 1 maintains compat with original swin v1 merge", "\n", "x", "=", "x", ".", "reshape", "(", "B", ",", "C", ",", "H", "//", "2", ",", "2", ",", "W", "//", "2", ",", "2", ")", ".", "permute", "(", "0", ",", "2", ",", "4", ",", "5", ",", "3", ",", "1", ")", ".", "flatten", "(", "3", ")", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "x", "=", "bhwc_to_bchw", "(", "self", ".", "reduction", "(", "x", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.PatchEmbed.__init__": [[492, 503], ["torch.Module.__init__", "layers.to_2tuple", "layers.to_2tuple", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "embed_dim", "=", "768", ",", "norm_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "img_size", "=", "to_2tuple", "(", "img_size", ")", "\n", "patch_size", "=", "to_2tuple", "(", "patch_size", ")", "\n", "self", ".", "img_size", "=", "img_size", "\n", "self", ".", "patch_size", "=", "patch_size", "\n", "self", ".", "grid_size", "=", "(", "img_size", "[", "0", "]", "//", "patch_size", "[", "0", "]", ",", "img_size", "[", "1", "]", "//", "patch_size", "[", "1", "]", ")", "\n", "self", ".", "num_patches", "=", "self", ".", "grid_size", "[", "0", "]", "*", "self", ".", "grid_size", "[", "1", "]", "\n", "\n", "self", ".", "proj", "=", "nn", ".", "Conv2d", "(", "in_chans", ",", "embed_dim", ",", "kernel_size", "=", "patch_size", ",", "stride", "=", "patch_size", ")", "\n", "self", ".", "norm", "=", "norm_layer", "(", "embed_dim", ")", "if", "norm_layer", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.PatchEmbed.forward": [[504, 511], ["layers._assert", "layers._assert", "swin_transformer_v2_cr.PatchEmbed.proj", "swin_transformer_v2_cr.PatchEmbed.norm().permute", "swin_transformer_v2_cr.PatchEmbed.norm", "swin_transformer_v2_cr.PatchEmbed.permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "_assert", "(", "H", "==", "self", ".", "img_size", "[", "0", "]", ",", "f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\"", ")", "\n", "_assert", "(", "W", "==", "self", ".", "img_size", "[", "1", "]", ",", "f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\"", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "norm", "(", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerStage.__init__": [[533, 582], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "swin_transformer_v2_cr.PatchMerging", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "swin_transformer_v2_cr.SwinTransformerBlock", "range", "tuple", "swin_transformer_v2_cr.SwinTransformerStage.__init__._extra_norm"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "embed_dim", ":", "int", ",", "\n", "depth", ":", "int", ",", "\n", "downscale", ":", "bool", ",", "\n", "num_heads", ":", "int", ",", "\n", "feat_size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "\n", "window_size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "\n", "mlp_ratio", ":", "float", "=", "4.0", ",", "\n", "init_values", ":", "Optional", "[", "float", "]", "=", "0.0", ",", "\n", "drop", ":", "float", "=", "0.0", ",", "\n", "drop_attn", ":", "float", "=", "0.0", ",", "\n", "drop_path", ":", "Union", "[", "List", "[", "float", "]", ",", "float", "]", "=", "0.0", ",", "\n", "norm_layer", ":", "Type", "[", "nn", ".", "Module", "]", "=", "nn", ".", "LayerNorm", ",", "\n", "extra_norm_period", ":", "int", "=", "0", ",", "\n", "extra_norm_stage", ":", "bool", "=", "False", ",", "\n", "sequential_attn", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", "SwinTransformerStage", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "downscale", ":", "bool", "=", "downscale", "\n", "self", ".", "grad_checkpointing", ":", "bool", "=", "False", "\n", "self", ".", "feat_size", ":", "Tuple", "[", "int", ",", "int", "]", "=", "(", "feat_size", "[", "0", "]", "//", "2", ",", "feat_size", "[", "1", "]", "//", "2", ")", "if", "downscale", "else", "feat_size", "\n", "\n", "self", ".", "downsample", "=", "PatchMerging", "(", "embed_dim", ",", "norm_layer", "=", "norm_layer", ")", "if", "downscale", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "def", "_extra_norm", "(", "index", ")", ":", "\n", "            ", "i", "=", "index", "+", "1", "\n", "if", "extra_norm_period", "and", "i", "%", "extra_norm_period", "==", "0", ":", "\n", "                ", "return", "True", "\n", "", "return", "i", "==", "depth", "if", "extra_norm_stage", "else", "False", "\n", "\n", "", "embed_dim", "=", "embed_dim", "*", "2", "if", "downscale", "else", "embed_dim", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "SwinTransformerBlock", "(", "\n", "dim", "=", "embed_dim", ",", "\n", "num_heads", "=", "num_heads", ",", "\n", "feat_size", "=", "self", ".", "feat_size", ",", "\n", "window_size", "=", "window_size", ",", "\n", "shift_size", "=", "tuple", "(", "[", "0", "if", "(", "(", "index", "%", "2", ")", "==", "0", ")", "else", "w", "//", "2", "for", "w", "in", "window_size", "]", ")", ",", "\n", "mlp_ratio", "=", "mlp_ratio", ",", "\n", "init_values", "=", "init_values", ",", "\n", "drop", "=", "drop", ",", "\n", "drop_attn", "=", "drop_attn", ",", "\n", "drop_path", "=", "drop_path", "[", "index", "]", "if", "isinstance", "(", "drop_path", ",", "list", ")", "else", "drop_path", ",", "\n", "extra_norm", "=", "_extra_norm", "(", "index", ")", ",", "\n", "sequential_attn", "=", "sequential_attn", ",", "\n", "norm_layer", "=", "norm_layer", ",", "\n", ")", "\n", "for", "index", "in", "range", "(", "depth", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerStage.update_input_size": [[584, 596], ["block.update_input_size"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerV2Cr.update_input_size"], ["", "def", "update_input_size", "(", "self", ",", "new_window_size", ":", "int", ",", "new_feat_size", ":", "Tuple", "[", "int", ",", "int", "]", ")", "->", "None", ":", "\n", "        ", "\"\"\"Method updates the resolution to utilize and the window size and so the pair-wise relative positions.\n\n        Args:\n            new_window_size (int): New window size\n            new_feat_size (Tuple[int, int]): New input resolution\n        \"\"\"", "\n", "self", ".", "feat_size", ":", "Tuple", "[", "int", ",", "int", "]", "=", "(", "\n", "(", "new_feat_size", "[", "0", "]", "//", "2", ",", "new_feat_size", "[", "1", "]", "//", "2", ")", "if", "self", ".", "downscale", "else", "new_feat_size", "\n", ")", "\n", "for", "block", "in", "self", ".", "blocks", ":", "\n", "            ", "block", ".", "update_input_size", "(", "new_window_size", "=", "new_window_size", ",", "new_feat_size", "=", "self", ".", "feat_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerStage.forward": [[597, 617], ["swin_transformer_v2_cr.SwinTransformerStage.downsample", "bchw_to_bhwc().reshape", "swin_transformer_v2_cr.bhwc_to_bchw", "block.reshape", "swin_transformer_v2_cr.bchw_to_bhwc", "torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "block", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.bhwc_to_bchw", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.bchw_to_bhwc"], ["", "", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"Forward pass.\n        Args:\n            x (torch.Tensor): Input tensor of the shape [B, C, H, W] or [B, L, C]\n        Returns:\n            output (torch.Tensor): Output tensor of the shape [B, 2 * C, H // 2, W // 2]\n        \"\"\"", "\n", "x", "=", "self", ".", "downsample", "(", "x", ")", "\n", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "L", "=", "H", "*", "W", "\n", "\n", "x", "=", "bchw_to_bhwc", "(", "x", ")", ".", "reshape", "(", "B", ",", "L", ",", "C", ")", "\n", "for", "block", "in", "self", ".", "blocks", ":", "\n", "# Perform checkpointing if utilized", "\n", "            ", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "                ", "x", "=", "checkpoint", ".", "checkpoint", "(", "block", ",", "x", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "block", "(", "x", ")", "\n", "", "", "x", "=", "bhwc_to_bchw", "(", "x", ".", "reshape", "(", "B", ",", "H", ",", "W", ",", "-", "1", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerV2Cr.__init__": [[644, 716], ["torch.Module.__init__", "layers.to_2tuple", "int", "swin_transformer_v2_cr.PatchEmbed", "torch.linspace().tolist", "torch.linspace().tolist", "torch.linspace().tolist", "torch.linspace().tolist", "torch.linspace().tolist", "torch.linspace().tolist", "torch.linspace().tolist", "torch.linspace().tolist", "torch.linspace().tolist", "torch.linspace().tolist", "torch.linspace().tolist", "torch.linspace().tolist", "torch.linspace().tolist", "torch.linspace().tolist", "torch.linspace().tolist", "torch.linspace().tolist", "enumerate", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "tuple", "layers.to_2tuple", "zip", "stages.append", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "helpers.named_apply", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "max", "swin_transformer_v2_cr.SwinTransformerStage", "sum", "len", "sum", "sum", "len"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_apply"], ["def", "__init__", "(", "\n", "self", ",", "\n", "img_size", ":", "Tuple", "[", "int", ",", "int", "]", "=", "(", "224", ",", "224", ")", ",", "\n", "patch_size", ":", "int", "=", "4", ",", "\n", "window_size", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "img_window_ratio", ":", "int", "=", "32", ",", "\n", "in_chans", ":", "int", "=", "3", ",", "\n", "num_classes", ":", "int", "=", "1000", ",", "\n", "embed_dim", ":", "int", "=", "96", ",", "\n", "depths", ":", "Tuple", "[", "int", ",", "...", "]", "=", "(", "2", ",", "2", ",", "6", ",", "2", ")", ",", "\n", "num_heads", ":", "Tuple", "[", "int", ",", "...", "]", "=", "(", "3", ",", "6", ",", "12", ",", "24", ")", ",", "\n", "mlp_ratio", ":", "float", "=", "4.0", ",", "\n", "init_values", ":", "Optional", "[", "float", "]", "=", "0.", ",", "\n", "drop_rate", ":", "float", "=", "0.0", ",", "\n", "attn_drop_rate", ":", "float", "=", "0.0", ",", "\n", "drop_path_rate", ":", "float", "=", "0.0", ",", "\n", "norm_layer", ":", "Type", "[", "nn", ".", "Module", "]", "=", "nn", ".", "LayerNorm", ",", "\n", "extra_norm_period", ":", "int", "=", "0", ",", "\n", "extra_norm_stage", ":", "bool", "=", "False", ",", "\n", "sequential_attn", ":", "bool", "=", "False", ",", "\n", "global_pool", ":", "str", "=", "'avg'", ",", "\n", "weight_init", "=", "'skip'", ",", "\n", "**", "kwargs", ":", "Any", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", "SwinTransformerV2Cr", ",", "self", ")", ".", "__init__", "(", ")", "\n", "img_size", "=", "to_2tuple", "(", "img_size", ")", "\n", "window_size", "=", "tuple", "(", "[", "\n", "s", "//", "img_window_ratio", "for", "s", "in", "img_size", "]", ")", "if", "window_size", "is", "None", "else", "to_2tuple", "(", "window_size", ")", "\n", "\n", "self", ".", "num_classes", ":", "int", "=", "num_classes", "\n", "self", ".", "patch_size", ":", "int", "=", "patch_size", "\n", "self", ".", "img_size", ":", "Tuple", "[", "int", ",", "int", "]", "=", "img_size", "\n", "self", ".", "window_size", ":", "int", "=", "window_size", "\n", "self", ".", "num_features", ":", "int", "=", "int", "(", "embed_dim", "*", "2", "**", "(", "len", "(", "depths", ")", "-", "1", ")", ")", "\n", "\n", "self", ".", "patch_embed", "=", "PatchEmbed", "(", "\n", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", ",", "in_chans", "=", "in_chans", ",", "\n", "embed_dim", "=", "embed_dim", ",", "norm_layer", "=", "norm_layer", ")", "\n", "patch_grid_size", ":", "Tuple", "[", "int", ",", "int", "]", "=", "self", ".", "patch_embed", ".", "grid_size", "\n", "\n", "drop_path_rate", "=", "torch", ".", "linspace", "(", "0.0", ",", "drop_path_rate", ",", "sum", "(", "depths", ")", ")", ".", "tolist", "(", ")", "\n", "stages", "=", "[", "]", "\n", "for", "index", ",", "(", "depth", ",", "num_heads", ")", "in", "enumerate", "(", "zip", "(", "depths", ",", "num_heads", ")", ")", ":", "\n", "            ", "stage_scale", "=", "2", "**", "max", "(", "index", "-", "1", ",", "0", ")", "\n", "stages", ".", "append", "(", "\n", "SwinTransformerStage", "(", "\n", "embed_dim", "=", "embed_dim", "*", "stage_scale", ",", "\n", "depth", "=", "depth", ",", "\n", "downscale", "=", "index", "!=", "0", ",", "\n", "feat_size", "=", "(", "patch_grid_size", "[", "0", "]", "//", "stage_scale", ",", "patch_grid_size", "[", "1", "]", "//", "stage_scale", ")", ",", "\n", "num_heads", "=", "num_heads", ",", "\n", "window_size", "=", "window_size", ",", "\n", "mlp_ratio", "=", "mlp_ratio", ",", "\n", "init_values", "=", "init_values", ",", "\n", "drop", "=", "drop_rate", ",", "\n", "drop_attn", "=", "attn_drop_rate", ",", "\n", "drop_path", "=", "drop_path_rate", "[", "sum", "(", "depths", "[", ":", "index", "]", ")", ":", "sum", "(", "depths", "[", ":", "index", "+", "1", "]", ")", "]", ",", "\n", "extra_norm_period", "=", "extra_norm_period", ",", "\n", "extra_norm_stage", "=", "extra_norm_stage", "or", "(", "index", "+", "1", ")", "==", "len", "(", "depths", ")", ",", "# last stage ends w/ norm", "\n", "sequential_attn", "=", "sequential_attn", ",", "\n", "norm_layer", "=", "norm_layer", ",", "\n", ")", "\n", ")", "\n", "", "self", ".", "stages", "=", "nn", ".", "Sequential", "(", "*", "stages", ")", "\n", "\n", "self", ".", "global_pool", ":", "str", "=", "global_pool", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "# current weight init skips custom init and uses pytorch layer defaults, seems to work well", "\n", "# FIXME more experiments needed", "\n", "if", "weight_init", "!=", "'skip'", ":", "\n", "            ", "named_apply", "(", "init_weights", ",", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerV2Cr.update_input_size": [[717, 744], ["enumerate", "layers.to_2tuple", "tuple", "stage.update_input_size", "max"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerV2Cr.update_input_size"], ["", "", "def", "update_input_size", "(", "\n", "self", ",", "\n", "new_img_size", ":", "Optional", "[", "Tuple", "[", "int", ",", "int", "]", "]", "=", "None", ",", "\n", "new_window_size", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "img_window_ratio", ":", "int", "=", "32", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"Method updates the image resolution to be processed and window size and so the pair-wise relative positions.\n\n        Args:\n            new_window_size (Optional[int]): New window size, if None based on new_img_size // window_div\n            new_img_size (Optional[Tuple[int, int]]): New input resolution, if None current resolution is used\n            img_window_ratio (int): divisor for calculating window size from image size\n        \"\"\"", "\n", "# Check parameters", "\n", "if", "new_img_size", "is", "None", ":", "\n", "            ", "new_img_size", "=", "self", ".", "img_size", "\n", "", "else", ":", "\n", "            ", "new_img_size", "=", "to_2tuple", "(", "new_img_size", ")", "\n", "", "if", "new_window_size", "is", "None", ":", "\n", "            ", "new_window_size", "=", "tuple", "(", "[", "s", "//", "img_window_ratio", "for", "s", "in", "new_img_size", "]", ")", "\n", "# Compute new patch resolution & update resolution of each stage", "\n", "", "new_patch_grid_size", "=", "(", "new_img_size", "[", "0", "]", "//", "self", ".", "patch_size", ",", "new_img_size", "[", "1", "]", "//", "self", ".", "patch_size", ")", "\n", "for", "index", ",", "stage", "in", "enumerate", "(", "self", ".", "stages", ")", ":", "\n", "            ", "stage_scale", "=", "2", "**", "max", "(", "index", "-", "1", ",", "0", ")", "\n", "stage", ".", "update_input_size", "(", "\n", "new_window_size", "=", "new_window_size", ",", "\n", "new_img_size", "=", "(", "new_patch_grid_size", "[", "0", "]", "//", "stage_scale", ",", "new_patch_grid_size", "[", "1", "]", "//", "stage_scale", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerV2Cr.group_matcher": [[746, 753], ["dict"], "methods", ["None"], ["", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^patch_embed'", ",", "# stem and embed", "\n", "blocks", "=", "r'^stages\\.(\\d+)'", "if", "coarse", "else", "[", "\n", "(", "r'^stages\\.(\\d+).downsample'", ",", "(", "0", ",", ")", ")", ",", "\n", "(", "r'^stages\\.(\\d+)\\.\\w+\\.(\\d+)'", ",", "None", ")", ",", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerV2Cr.set_grad_checkpointing": [[756, 760], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "for", "s", "in", "self", ".", "stages", ":", "\n", "            ", "s", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerV2Cr.get_classifier": [[761, 768], ["torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore"], "methods", ["None"], ["", "", "@", "torch", ".", "jit", ".", "ignore", "(", ")", "\n", "def", "get_classifier", "(", "self", ")", "->", "nn", ".", "Module", ":", "\n", "        ", "\"\"\"Method returns the classification head of the model.\n        Returns:\n            head (nn.Module): Current classification head\n        \"\"\"", "\n", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerV2Cr.reset_classifier": [[769, 780], ["torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ":", "int", ",", "global_pool", ":", "Optional", "[", "str", "]", "=", "None", ")", "->", "None", ":", "\n", "        ", "\"\"\"Method results the classification head\n\n        Args:\n            num_classes (int): Number of classes to be predicted\n            global_pool (str): Unused\n        \"\"\"", "\n", "self", ".", "num_classes", ":", "int", "=", "num_classes", "\n", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "self", ".", "global_pool", "=", "global_pool", "\n", "", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerV2Cr.forward_features": [[781, 785], ["swin_transformer_v2_cr.SwinTransformerV2Cr.patch_embed", "swin_transformer_v2_cr.SwinTransformerV2Cr.stages"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet.stages"], ["", "def", "forward_features", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "x", "=", "self", ".", "patch_embed", "(", "x", ")", "\n", "x", "=", "self", ".", "stages", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerV2Cr.forward_head": [[786, 790], ["x.mean.mean.mean", "swin_transformer_v2_cr.SwinTransformerV2Cr.head"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "global_pool", "==", "'avg'", ":", "\n", "            ", "x", "=", "x", ".", "mean", "(", "dim", "=", "(", "2", ",", "3", ")", ")", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.SwinTransformerV2Cr.forward": [[791, 795], ["swin_transformer_v2_cr.SwinTransformerV2Cr.forward_features", "swin_transformer_v2_cr.SwinTransformerV2Cr.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr._cfg": [[50, 64], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "\n", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "\n", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.9", ",", "\n", "'interpolation'", ":", "'bicubic'", ",", "\n", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "\n", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'patch_embed.proj'", ",", "\n", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.bchw_to_bhwc": [[104, 107], ["x.permute"], "function", ["None"], ["def", "bchw_to_bhwc", "(", "x", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"Permutes a tensor from the shape (B, C, H, W) to (B, H, W, C). \"\"\"", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.bhwc_to_bchw": [[109, 112], ["x.permute"], "function", ["None"], ["", "def", "bhwc_to_bchw", "(", "x", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"Permutes a tensor from the shape (B, H, W, C) to (B, C, H, W). \"\"\"", "\n", "return", "x", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.window_partition": [[114, 127], ["x.view.view", "x.view.permute().contiguous().view", "x.view.permute().contiguous", "x.view.permute"], "function", ["None"], ["", "def", "window_partition", "(", "x", ",", "window_size", ":", "Tuple", "[", "int", ",", "int", "]", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"", "\n", "B", ",", "H", ",", "W", ",", "C", "=", "x", ".", "shape", "\n", "x", "=", "x", ".", "view", "(", "B", ",", "H", "//", "window_size", "[", "0", "]", ",", "window_size", "[", "0", "]", ",", "W", "//", "window_size", "[", "1", "]", ",", "window_size", "[", "1", "]", ",", "C", ")", "\n", "windows", "=", "x", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ",", "4", ",", "5", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "window_size", "[", "0", "]", ",", "window_size", "[", "1", "]", ",", "C", ")", "\n", "return", "windows", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.window_reverse": [[129, 145], ["int", "windows.view", "x.permute().contiguous().view.permute().contiguous().view", "x.permute().contiguous().view.permute().contiguous", "x.permute().contiguous().view.permute"], "function", ["None"], ["", "@", "register_notrace_function", "# reason: int argument is a Proxy", "\n", "def", "window_reverse", "(", "windows", ",", "window_size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "img_size", ":", "Tuple", "[", "int", ",", "int", "]", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        windows: (num_windows * B, window_size[0], window_size[1], C)\n        window_size (Tuple[int, int]): Window size\n        img_size (Tuple[int, int]): Image size\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"", "\n", "H", ",", "W", "=", "img_size", "\n", "B", "=", "int", "(", "windows", ".", "shape", "[", "0", "]", "/", "(", "H", "*", "W", "/", "window_size", "[", "0", "]", "/", "window_size", "[", "1", "]", ")", ")", "\n", "x", "=", "windows", ".", "view", "(", "B", ",", "H", "//", "window_size", "[", "0", "]", ",", "W", "//", "window_size", "[", "1", "]", ",", "window_size", "[", "0", "]", ",", "window_size", "[", "1", "]", ",", "-", "1", ")", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ",", "4", ",", "5", ")", ".", "contiguous", "(", ")", ".", "view", "(", "B", ",", "H", ",", "W", ",", "-", "1", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.init_weights": [[797, 812], ["isinstance", "hasattr", "math.sqrt", "torch.init.uniform_", "torch.init.zeros_", "module.init_weights", "torch.init.zeros_", "torch.init.xavier_uniform_", "float"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights"], ["", "", "def", "init_weights", "(", "module", ":", "nn", ".", "Module", ",", "name", ":", "str", "=", "''", ")", ":", "\n", "# FIXME WIP determining if there's a better weight init", "\n", "    ", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "        ", "if", "'qkv'", "in", "name", ":", "\n", "# treat the weights of Q, K, V separately", "\n", "            ", "val", "=", "math", ".", "sqrt", "(", "6.", "/", "float", "(", "module", ".", "weight", ".", "shape", "[", "0", "]", "//", "3", "+", "module", ".", "weight", ".", "shape", "[", "1", "]", ")", ")", "\n", "nn", ".", "init", ".", "uniform_", "(", "module", ".", "weight", ",", "-", "val", ",", "val", ")", "\n", "", "elif", "'head'", "in", "name", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "weight", ")", "\n", "", "else", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_uniform_", "(", "module", ".", "weight", ")", "\n", "", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "", "elif", "hasattr", "(", "module", ",", "'init_weights'", ")", ":", "\n", "        ", "module", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.checkpoint_filter_fn": [[814, 827], ["state_dict.items", "torch.log", "torch.log", "torch.log", "torch.log", "k.replace.replace"], "function", ["None"], ["", "", "def", "checkpoint_filter_fn", "(", "state_dict", ",", "model", ")", ":", "\n", "    ", "\"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"", "\n", "out_dict", "=", "{", "}", "\n", "if", "'model'", "in", "state_dict", ":", "\n", "# For deit models", "\n", "        ", "state_dict", "=", "state_dict", "[", "'model'", "]", "\n", "", "for", "k", ",", "v", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "        ", "if", "'tau'", "in", "k", ":", "\n", "# convert old tau based checkpoints -> logit_scale (inverse)", "\n", "            ", "v", "=", "torch", ".", "log", "(", "1", "/", "v", ")", "\n", "k", "=", "k", ".", "replace", "(", "'tau'", ",", "'logit_scale'", ")", "\n", "", "out_dict", "[", "k", "]", "=", "v", "\n", "", "return", "out_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr._create_swin_transformer_v2_cr": [[829, 838], ["kwargs.get", "helpers.build_model_with_cfg", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "def", "_create_swin_transformer_v2_cr", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "kwargs", ".", "get", "(", "'features_only'", ",", "None", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'features_only not implemented for Vision Transformer models.'", ")", "\n", "", "model", "=", "build_model_with_cfg", "(", "\n", "SwinTransformerV2Cr", ",", "variant", ",", "pretrained", ",", "\n", "pretrained_filter_fn", "=", "checkpoint_filter_fn", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.swinv2_cr_tiny_384": [[840, 850], ["dict", "swin_transformer_v2_cr._create_swin_transformer_v2_cr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr._create_swin_transformer_v2_cr"], ["", "@", "register_model", "\n", "def", "swinv2_cr_tiny_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Swin-T V2 CR @ 384x384, trained ImageNet-1k\"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "embed_dim", "=", "96", ",", "\n", "depths", "=", "(", "2", ",", "2", ",", "6", ",", "2", ")", ",", "\n", "num_heads", "=", "(", "3", ",", "6", ",", "12", ",", "24", ")", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_swin_transformer_v2_cr", "(", "'swinv2_cr_tiny_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.swinv2_cr_tiny_224": [[852, 862], ["dict", "swin_transformer_v2_cr._create_swin_transformer_v2_cr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr._create_swin_transformer_v2_cr"], ["", "@", "register_model", "\n", "def", "swinv2_cr_tiny_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Swin-T V2 CR @ 224x224, trained ImageNet-1k\"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "embed_dim", "=", "96", ",", "\n", "depths", "=", "(", "2", ",", "2", ",", "6", ",", "2", ")", ",", "\n", "num_heads", "=", "(", "3", ",", "6", ",", "12", ",", "24", ")", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_swin_transformer_v2_cr", "(", "'swinv2_cr_tiny_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.swinv2_cr_tiny_ns_224": [[864, 877], ["dict", "swin_transformer_v2_cr._create_swin_transformer_v2_cr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr._create_swin_transformer_v2_cr"], ["", "@", "register_model", "\n", "def", "swinv2_cr_tiny_ns_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Swin-T V2 CR @ 224x224, trained ImageNet-1k w/ extra stage norms.\n    ** Experimental, may make default if results are improved. **\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "embed_dim", "=", "96", ",", "\n", "depths", "=", "(", "2", ",", "2", ",", "6", ",", "2", ")", ",", "\n", "num_heads", "=", "(", "3", ",", "6", ",", "12", ",", "24", ")", ",", "\n", "extra_norm_stage", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_swin_transformer_v2_cr", "(", "'swinv2_cr_tiny_ns_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.swinv2_cr_small_384": [[879, 889], ["dict", "swin_transformer_v2_cr._create_swin_transformer_v2_cr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr._create_swin_transformer_v2_cr"], ["", "@", "register_model", "\n", "def", "swinv2_cr_small_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Swin-S V2 CR @ 384x384, trained ImageNet-1k\"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "embed_dim", "=", "96", ",", "\n", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "\n", "num_heads", "=", "(", "3", ",", "6", ",", "12", ",", "24", ")", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_swin_transformer_v2_cr", "(", "'swinv2_cr_small_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.swinv2_cr_small_224": [[892, 902], ["dict", "swin_transformer_v2_cr._create_swin_transformer_v2_cr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr._create_swin_transformer_v2_cr"], ["", "@", "register_model", "\n", "def", "swinv2_cr_small_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Swin-S V2 CR @ 224x224, trained ImageNet-1k\"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "embed_dim", "=", "96", ",", "\n", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "\n", "num_heads", "=", "(", "3", ",", "6", ",", "12", ",", "24", ")", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_swin_transformer_v2_cr", "(", "'swinv2_cr_small_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.swinv2_cr_small_ns_224": [[904, 915], ["dict", "swin_transformer_v2_cr._create_swin_transformer_v2_cr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr._create_swin_transformer_v2_cr"], ["", "@", "register_model", "\n", "def", "swinv2_cr_small_ns_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Swin-S V2 CR @ 224x224, trained ImageNet-1k\"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "embed_dim", "=", "96", ",", "\n", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "\n", "num_heads", "=", "(", "3", ",", "6", ",", "12", ",", "24", ")", ",", "\n", "extra_norm_stage", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_swin_transformer_v2_cr", "(", "'swinv2_cr_small_ns_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.swinv2_cr_base_384": [[917, 927], ["dict", "swin_transformer_v2_cr._create_swin_transformer_v2_cr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr._create_swin_transformer_v2_cr"], ["", "@", "register_model", "\n", "def", "swinv2_cr_base_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Swin-B V2 CR @ 384x384, trained ImageNet-1k\"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "embed_dim", "=", "128", ",", "\n", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "\n", "num_heads", "=", "(", "4", ",", "8", ",", "16", ",", "32", ")", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_swin_transformer_v2_cr", "(", "'swinv2_cr_base_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.swinv2_cr_base_224": [[929, 939], ["dict", "swin_transformer_v2_cr._create_swin_transformer_v2_cr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr._create_swin_transformer_v2_cr"], ["", "@", "register_model", "\n", "def", "swinv2_cr_base_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Swin-B V2 CR @ 224x224, trained ImageNet-1k\"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "embed_dim", "=", "128", ",", "\n", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "\n", "num_heads", "=", "(", "4", ",", "8", ",", "16", ",", "32", ")", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_swin_transformer_v2_cr", "(", "'swinv2_cr_base_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.swinv2_cr_base_ns_224": [[941, 952], ["dict", "swin_transformer_v2_cr._create_swin_transformer_v2_cr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr._create_swin_transformer_v2_cr"], ["", "@", "register_model", "\n", "def", "swinv2_cr_base_ns_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Swin-B V2 CR @ 224x224, trained ImageNet-1k\"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "embed_dim", "=", "128", ",", "\n", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "\n", "num_heads", "=", "(", "4", ",", "8", ",", "16", ",", "32", ")", ",", "\n", "extra_norm_stage", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_swin_transformer_v2_cr", "(", "'swinv2_cr_base_ns_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.swinv2_cr_large_384": [[954, 964], ["dict", "swin_transformer_v2_cr._create_swin_transformer_v2_cr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr._create_swin_transformer_v2_cr"], ["", "@", "register_model", "\n", "def", "swinv2_cr_large_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Swin-L V2 CR @ 384x384, trained ImageNet-1k\"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "embed_dim", "=", "192", ",", "\n", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "\n", "num_heads", "=", "(", "6", ",", "12", ",", "24", ",", "48", ")", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_swin_transformer_v2_cr", "(", "'swinv2_cr_large_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.swinv2_cr_large_224": [[967, 977], ["dict", "swin_transformer_v2_cr._create_swin_transformer_v2_cr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr._create_swin_transformer_v2_cr"], ["", "@", "register_model", "\n", "def", "swinv2_cr_large_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Swin-L V2 CR @ 224x224, trained ImageNet-1k\"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "embed_dim", "=", "192", ",", "\n", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "\n", "num_heads", "=", "(", "6", ",", "12", ",", "24", ",", "48", ")", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_swin_transformer_v2_cr", "(", "'swinv2_cr_large_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.swinv2_cr_huge_384": [[979, 990], ["dict", "swin_transformer_v2_cr._create_swin_transformer_v2_cr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr._create_swin_transformer_v2_cr"], ["", "@", "register_model", "\n", "def", "swinv2_cr_huge_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Swin-H V2 CR @ 384x384, trained ImageNet-1k\"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "embed_dim", "=", "352", ",", "\n", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "\n", "num_heads", "=", "(", "11", ",", "22", ",", "44", ",", "88", ")", ",", "# head count not certain for Huge, 384 & 224 trying diff values", "\n", "extra_norm_period", "=", "6", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_swin_transformer_v2_cr", "(", "'swinv2_cr_huge_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.swinv2_cr_huge_224": [[992, 1003], ["dict", "swin_transformer_v2_cr._create_swin_transformer_v2_cr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr._create_swin_transformer_v2_cr"], ["", "@", "register_model", "\n", "def", "swinv2_cr_huge_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Swin-H V2 CR @ 224x224, trained ImageNet-1k\"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "embed_dim", "=", "352", ",", "\n", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "\n", "num_heads", "=", "(", "8", ",", "16", ",", "32", ",", "64", ")", ",", "# head count not certain for Huge, 384 & 224 trying diff values", "\n", "extra_norm_period", "=", "6", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_swin_transformer_v2_cr", "(", "'swinv2_cr_huge_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.swinv2_cr_giant_384": [[1005, 1016], ["dict", "swin_transformer_v2_cr._create_swin_transformer_v2_cr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr._create_swin_transformer_v2_cr"], ["", "@", "register_model", "\n", "def", "swinv2_cr_giant_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Swin-G V2 CR @ 384x384, trained ImageNet-1k\"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "embed_dim", "=", "512", ",", "\n", "depths", "=", "(", "2", ",", "2", ",", "42", ",", "2", ")", ",", "\n", "num_heads", "=", "(", "16", ",", "32", ",", "64", ",", "128", ")", ",", "\n", "extra_norm_period", "=", "6", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_swin_transformer_v2_cr", "(", "'swinv2_cr_giant_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr.swinv2_cr_giant_224": [[1019, 1030], ["dict", "swin_transformer_v2_cr._create_swin_transformer_v2_cr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2_cr._create_swin_transformer_v2_cr"], ["", "@", "register_model", "\n", "def", "swinv2_cr_giant_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Swin-G V2 CR @ 224x224, trained ImageNet-1k\"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "embed_dim", "=", "512", ",", "\n", "depths", "=", "(", "2", ",", "2", ",", "42", ",", "2", ")", ",", "\n", "num_heads", "=", "(", "16", ",", "32", ",", "64", ",", "128", ")", ",", "\n", "extra_norm_period", "=", "6", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_swin_transformer_v2_cr", "(", "'swinv2_cr_giant_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_xception.SeparableConv2d.__init__": [[46, 59], ["torch.Module.__init__", "layers.get_padding", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.get_padding"], ["    ", "def", "__init__", "(", "self", ",", "inplanes", ",", "planes", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "bias", "=", "False", ",", "norm_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "SeparableConv2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "dilation", "=", "dilation", "\n", "\n", "# depthwise convolution", "\n", "padding", "=", "get_padding", "(", "kernel_size", ",", "stride", ",", "dilation", ")", "\n", "self", ".", "conv_dw", "=", "nn", ".", "Conv2d", "(", "\n", "inplanes", ",", "inplanes", ",", "kernel_size", ",", "stride", "=", "stride", ",", "\n", "padding", "=", "padding", ",", "dilation", "=", "dilation", ",", "groups", "=", "inplanes", ",", "bias", "=", "bias", ")", "\n", "self", ".", "bn", "=", "norm_layer", "(", "num_features", "=", "inplanes", ")", "\n", "# pointwise convolution", "\n", "self", ".", "conv_pw", "=", "nn", ".", "Conv2d", "(", "inplanes", ",", "planes", ",", "kernel_size", "=", "1", ",", "bias", "=", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_xception.SeparableConv2d.forward": [[60, 65], ["gluon_xception.SeparableConv2d.conv_dw", "gluon_xception.SeparableConv2d.bn", "gluon_xception.SeparableConv2d.conv_pw"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv_dw", "(", "x", ")", "\n", "x", "=", "self", ".", "bn", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_pw", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_xception.Block.__init__": [[68, 97], ["torch.Module.__init__", "isinstance", "collections.OrderedDict", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "gluon_xception.Block.skip.add_module", "torch.ReLU", "torch.ReLU", "torch.ReLU", "gluon_xception.SeparableConv2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "len", "gluon_xception.Block.skip.add_module", "norm_layer", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "start_with_relu", "=", "True", ",", "norm_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "Block", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "isinstance", "(", "planes", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "assert", "len", "(", "planes", ")", "==", "3", "\n", "", "else", ":", "\n", "            ", "planes", "=", "(", "planes", ",", ")", "*", "3", "\n", "", "outplanes", "=", "planes", "[", "-", "1", "]", "\n", "\n", "if", "outplanes", "!=", "inplanes", "or", "stride", "!=", "1", ":", "\n", "            ", "self", ".", "skip", "=", "nn", ".", "Sequential", "(", ")", "\n", "self", ".", "skip", ".", "add_module", "(", "'conv1'", ",", "nn", ".", "Conv2d", "(", "\n", "inplanes", ",", "outplanes", ",", "1", ",", "stride", "=", "stride", ",", "bias", "=", "False", ")", ")", ",", "\n", "self", ".", "skip", ".", "add_module", "(", "'bn1'", ",", "norm_layer", "(", "num_features", "=", "outplanes", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "skip", "=", "None", "\n", "\n", "", "rep", "=", "OrderedDict", "(", ")", "\n", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "            ", "rep", "[", "'act%d'", "%", "(", "i", "+", "1", ")", "]", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "rep", "[", "'conv%d'", "%", "(", "i", "+", "1", ")", "]", "=", "SeparableConv2d", "(", "\n", "inplanes", ",", "planes", "[", "i", "]", ",", "3", ",", "stride", "=", "stride", "if", "i", "==", "2", "else", "1", ",", "dilation", "=", "dilation", ",", "norm_layer", "=", "norm_layer", ")", "\n", "rep", "[", "'bn%d'", "%", "(", "i", "+", "1", ")", "]", "=", "norm_layer", "(", "planes", "[", "i", "]", ")", "\n", "inplanes", "=", "planes", "[", "i", "]", "\n", "\n", "", "if", "not", "start_with_relu", ":", "\n", "            ", "del", "rep", "[", "'act1'", "]", "\n", "", "else", ":", "\n", "            ", "rep", "[", "'act1'", "]", "=", "nn", ".", "ReLU", "(", "inplace", "=", "False", ")", "\n", "", "self", ".", "rep", "=", "nn", ".", "Sequential", "(", "rep", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_xception.Block.forward": [[98, 104], ["gluon_xception.Block.skip", "gluon_xception.Block.rep"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "skip", "=", "x", "\n", "if", "self", ".", "skip", "is", "not", "None", ":", "\n", "            ", "skip", "=", "self", ".", "skip", "(", "skip", ")", "\n", "", "x", "=", "self", ".", "rep", "(", "x", ")", "+", "skip", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_xception.Xception65.__init__": [[113, 181], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "gluon_xception.Block", "torch.ReLU", "torch.ReLU", "torch.ReLU", "gluon_xception.Block", "gluon_xception.Block", "torch.Sequential", "torch.Sequential", "torch.Sequential", "gluon_xception.Block", "torch.ReLU", "torch.ReLU", "torch.ReLU", "gluon_xception.SeparableConv2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "gluon_xception.SeparableConv2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "gluon_xception.SeparableConv2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "layers.create_classifier", "collections.OrderedDict", "dict", "dict", "dict", "dict", "dict", "gluon_xception.Block", "range"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["def", "__init__", "(", "self", ",", "num_classes", "=", "1000", ",", "in_chans", "=", "3", ",", "output_stride", "=", "32", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "\n", "drop_rate", "=", "0.", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "super", "(", "Xception65", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "if", "output_stride", "==", "32", ":", "\n", "            ", "entry_block3_stride", "=", "2", "\n", "exit_block20_stride", "=", "2", "\n", "middle_dilation", "=", "1", "\n", "exit_dilation", "=", "(", "1", ",", "1", ")", "\n", "", "elif", "output_stride", "==", "16", ":", "\n", "            ", "entry_block3_stride", "=", "2", "\n", "exit_block20_stride", "=", "1", "\n", "middle_dilation", "=", "1", "\n", "exit_dilation", "=", "(", "1", ",", "2", ")", "\n", "", "elif", "output_stride", "==", "8", ":", "\n", "            ", "entry_block3_stride", "=", "1", "\n", "exit_block20_stride", "=", "1", "\n", "middle_dilation", "=", "2", "\n", "exit_dilation", "=", "(", "2", ",", "4", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "# Entry flow", "\n", "", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "in_chans", ",", "32", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "num_features", "=", "32", ")", "\n", "self", ".", "act1", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn2", "=", "norm_layer", "(", "num_features", "=", "64", ")", "\n", "self", ".", "act2", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n", "self", ".", "block1", "=", "Block", "(", "64", ",", "128", ",", "stride", "=", "2", ",", "start_with_relu", "=", "False", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "block1_act", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "block2", "=", "Block", "(", "128", ",", "256", ",", "stride", "=", "2", ",", "start_with_relu", "=", "False", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "block3", "=", "Block", "(", "256", ",", "728", ",", "stride", "=", "entry_block3_stride", ",", "norm_layer", "=", "norm_layer", ")", "\n", "\n", "# Middle flow", "\n", "self", ".", "mid", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "(", "'block%d'", "%", "i", ",", "Block", "(", "\n", "728", ",", "728", ",", "stride", "=", "1", ",", "dilation", "=", "middle_dilation", ",", "norm_layer", "=", "norm_layer", ")", ")", "for", "i", "in", "range", "(", "4", ",", "20", ")", "]", ")", ")", "\n", "\n", "# Exit flow", "\n", "self", ".", "block20", "=", "Block", "(", "\n", "728", ",", "(", "728", ",", "1024", ",", "1024", ")", ",", "stride", "=", "exit_block20_stride", ",", "dilation", "=", "exit_dilation", "[", "0", "]", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "block20_act", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n", "self", ".", "conv3", "=", "SeparableConv2d", "(", "1024", ",", "1536", ",", "3", ",", "stride", "=", "1", ",", "dilation", "=", "exit_dilation", "[", "1", "]", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "bn3", "=", "norm_layer", "(", "num_features", "=", "1536", ")", "\n", "self", ".", "act3", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n", "self", ".", "conv4", "=", "SeparableConv2d", "(", "1536", ",", "1536", ",", "3", ",", "stride", "=", "1", ",", "dilation", "=", "exit_dilation", "[", "1", "]", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "bn4", "=", "norm_layer", "(", "num_features", "=", "1536", ")", "\n", "self", ".", "act4", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n", "self", ".", "num_features", "=", "2048", "\n", "self", ".", "conv5", "=", "SeparableConv2d", "(", "\n", "1536", ",", "self", ".", "num_features", ",", "3", ",", "stride", "=", "1", ",", "dilation", "=", "exit_dilation", "[", "1", "]", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "bn5", "=", "norm_layer", "(", "num_features", "=", "self", ".", "num_features", ")", "\n", "self", ".", "act5", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "feature_info", "=", "[", "\n", "dict", "(", "num_chs", "=", "64", ",", "reduction", "=", "2", ",", "module", "=", "'act2'", ")", ",", "\n", "dict", "(", "num_chs", "=", "128", ",", "reduction", "=", "4", ",", "module", "=", "'block1_act'", ")", ",", "\n", "dict", "(", "num_chs", "=", "256", ",", "reduction", "=", "8", ",", "module", "=", "'block3.rep.act1'", ")", ",", "\n", "dict", "(", "num_chs", "=", "728", ",", "reduction", "=", "16", ",", "module", "=", "'block20.rep.act1'", ")", ",", "\n", "dict", "(", "num_chs", "=", "2048", ",", "reduction", "=", "32", ",", "module", "=", "'act5'", ")", ",", "\n", "]", "\n", "\n", "self", ".", "global_pool", ",", "self", ".", "fc", "=", "create_classifier", "(", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_xception.Xception65.group_matcher": [[182, 193], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "\n", "stem", "=", "r'^conv[12]|bn[12]'", ",", "\n", "blocks", "=", "[", "\n", "(", "r'^mid\\.block(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^block(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^conv[345]|bn[345]'", ",", "(", "99", ",", ")", ")", ",", "\n", "]", ",", "\n", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_xception.Xception65.set_grad_checkpointing": [[194, 197], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "\"gradient checkpointing not supported\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_xception.Xception65.get_classifier": [[198, 201], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_xception.Xception65.reset_classifier": [[202, 205], ["layers.create_classifier"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", ",", "self", ".", "fc", "=", "create_classifier", "(", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_xception.Xception65.forward_features": [[206, 242], ["gluon_xception.Xception65.conv1", "gluon_xception.Xception65.bn1", "gluon_xception.Xception65.act1", "gluon_xception.Xception65.conv2", "gluon_xception.Xception65.bn2", "gluon_xception.Xception65.act2", "gluon_xception.Xception65.block1", "gluon_xception.Xception65.block1_act", "gluon_xception.Xception65.block2", "gluon_xception.Xception65.block3", "gluon_xception.Xception65.mid", "gluon_xception.Xception65.block20", "gluon_xception.Xception65.block20_act", "gluon_xception.Xception65.conv3", "gluon_xception.Xception65.bn3", "gluon_xception.Xception65.act3", "gluon_xception.Xception65.conv4", "gluon_xception.Xception65.bn4", "gluon_xception.Xception65.act4", "gluon_xception.Xception65.conv5", "gluon_xception.Xception65.bn5", "gluon_xception.Xception65.act5"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "# Entry flow", "\n", "        ", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "x", "=", "self", ".", "act1", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "x", "=", "self", ".", "bn2", "(", "x", ")", "\n", "x", "=", "self", ".", "act2", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "block1", "(", "x", ")", "\n", "x", "=", "self", ".", "block1_act", "(", "x", ")", "\n", "# c1 = x", "\n", "x", "=", "self", ".", "block2", "(", "x", ")", "\n", "# c2 = x", "\n", "x", "=", "self", ".", "block3", "(", "x", ")", "\n", "\n", "# Middle flow", "\n", "x", "=", "self", ".", "mid", "(", "x", ")", "\n", "# c3 = x", "\n", "\n", "# Exit flow", "\n", "x", "=", "self", ".", "block20", "(", "x", ")", "\n", "x", "=", "self", ".", "block20_act", "(", "x", ")", "\n", "x", "=", "self", ".", "conv3", "(", "x", ")", "\n", "x", "=", "self", ".", "bn3", "(", "x", ")", "\n", "x", "=", "self", ".", "act3", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "conv4", "(", "x", ")", "\n", "x", "=", "self", ".", "bn4", "(", "x", ")", "\n", "x", "=", "self", ".", "act4", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "conv5", "(", "x", ")", "\n", "x", "=", "self", ".", "bn5", "(", "x", ")", "\n", "x", "=", "self", ".", "act5", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_xception.Xception65.forward_head": [[243, 249], ["gluon_xception.Xception65.global_pool", "gluon_xception.Xception65.fc", "torch.dropout", "torch.dropout", "torch.dropout"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "if", "self", ".", "drop_rate", ":", "\n", "            ", "F", ".", "dropout", "(", "x", ",", "self", ".", "drop_rate", ",", "training", "=", "self", ".", "training", ")", "\n", "", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_xception.Xception65.forward": [[250, 254], ["gluon_xception.Xception65.forward_features", "gluon_xception.Xception65.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_xception._create_gluon_xception": [[256, 261], ["helpers.build_model_with_cfg", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_gluon_xception", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "\n", "Xception65", ",", "variant", ",", "pretrained", ",", "\n", "feature_cfg", "=", "dict", "(", "feature_cls", "=", "'hook'", ")", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_xception.gluon_xception65": [[263, 268], ["gluon_xception._create_gluon_xception"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_xception._create_gluon_xception"], ["", "@", "register_model", "\n", "def", "gluon_xception65", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Modified Aligned Xception-65\n    \"\"\"", "\n", "return", "_create_gluon_xception", "(", "'gluon_xception65'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.EfficientNet.__init__": [[473, 508], ["torch.Module.__init__", "layers.get_norm_act_layer", "layers.create_conv2d", "layers.get_norm_act_layer.", "efficientnet_builder.EfficientNetBuilder", "torch.Sequential", "torch.Sequential", "torch.Sequential", "layers.create_conv2d", "layers.get_norm_act_layer.", "layers.create_classifier", "efficientnet_builder.efficientnet_init_weights", "round_chs_fn", "efficientnet_builder.EfficientNetBuilder."], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.get_norm_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.efficientnet_init_weights"], ["def", "__init__", "(", "\n", "self", ",", "block_args", ",", "num_classes", "=", "1000", ",", "num_features", "=", "1280", ",", "in_chans", "=", "3", ",", "stem_size", "=", "32", ",", "fix_stem", "=", "False", ",", "\n", "output_stride", "=", "32", ",", "pad_type", "=", "''", ",", "round_chs_fn", "=", "round_channels", ",", "act_layer", "=", "None", ",", "norm_layer", "=", "None", ",", "\n", "se_layer", "=", "None", ",", "drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "super", "(", "EfficientNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "act_layer", "=", "act_layer", "or", "nn", ".", "ReLU", "\n", "norm_layer", "=", "norm_layer", "or", "nn", ".", "BatchNorm2d", "\n", "norm_act_layer", "=", "get_norm_act_layer", "(", "norm_layer", ",", "act_layer", ")", "\n", "se_layer", "=", "se_layer", "or", "SqueezeExcite", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "num_features", "=", "num_features", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "# Stem", "\n", "if", "not", "fix_stem", ":", "\n", "            ", "stem_size", "=", "round_chs_fn", "(", "stem_size", ")", "\n", "", "self", ".", "conv_stem", "=", "create_conv2d", "(", "in_chans", ",", "stem_size", ",", "3", ",", "stride", "=", "2", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "bn1", "=", "norm_act_layer", "(", "stem_size", ",", "inplace", "=", "True", ")", "\n", "\n", "# Middle stages (IR/ER/DS Blocks)", "\n", "builder", "=", "EfficientNetBuilder", "(", "\n", "output_stride", "=", "output_stride", ",", "pad_type", "=", "pad_type", ",", "round_chs_fn", "=", "round_chs_fn", ",", "\n", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ",", "se_layer", "=", "se_layer", ",", "drop_path_rate", "=", "drop_path_rate", ")", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "*", "builder", "(", "stem_size", ",", "block_args", ")", ")", "\n", "self", ".", "feature_info", "=", "builder", ".", "features", "\n", "head_chs", "=", "builder", ".", "in_chs", "\n", "\n", "# Head + Pooling", "\n", "self", ".", "conv_head", "=", "create_conv2d", "(", "head_chs", ",", "self", ".", "num_features", ",", "1", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "bn2", "=", "norm_act_layer", "(", "self", ".", "num_features", ",", "inplace", "=", "True", ")", "\n", "self", ".", "global_pool", ",", "self", ".", "classifier", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n", "efficientnet_init_weights", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.EfficientNet.as_sequential": [[509, 515], ["layers.extend", "layers.extend", "layers.extend", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["None"], ["", "def", "as_sequential", "(", "self", ")", ":", "\n", "        ", "layers", "=", "[", "self", ".", "conv_stem", ",", "self", ".", "bn1", "]", "\n", "layers", ".", "extend", "(", "self", ".", "blocks", ")", "\n", "layers", ".", "extend", "(", "[", "self", ".", "conv_head", ",", "self", ".", "bn2", ",", "self", ".", "global_pool", "]", ")", "\n", "layers", ".", "extend", "(", "[", "nn", ".", "Dropout", "(", "self", ".", "drop_rate", ")", ",", "self", ".", "classifier", "]", ")", "\n", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.EfficientNet.group_matcher": [[516, 523], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^conv_stem|bn1'", ",", "\n", "blocks", "=", "[", "\n", "(", "r'^blocks\\.(\\d+)'", "if", "coarse", "else", "r'^blocks\\.(\\d+)\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'conv_head|bn2'", ",", "(", "99999", ",", ")", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.EfficientNet.set_grad_checkpointing": [[526, 529], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.EfficientNet.get_classifier": [[530, 533], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "classifier", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.EfficientNet.reset_classifier": [[534, 538], ["layers.create_classifier"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", ",", "self", ".", "classifier", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.EfficientNet.forward_features": [[539, 549], ["efficientnet.EfficientNet.conv_stem", "efficientnet.EfficientNet.bn1", "efficientnet.EfficientNet.conv_head", "efficientnet.EfficientNet.bn2", "helpers.checkpoint_seq", "efficientnet.EfficientNet.blocks", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv_stem", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "blocks", ",", "x", ",", "flatten", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "blocks", "(", "x", ")", "\n", "", "x", "=", "self", ".", "conv_head", "(", "x", ")", "\n", "x", "=", "self", ".", "bn2", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.EfficientNet.forward_head": [[550, 555], ["efficientnet.EfficientNet.global_pool", "torch.dropout", "torch.dropout", "torch.dropout", "efficientnet.EfficientNet.classifier"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "if", "self", ".", "drop_rate", ">", "0.", ":", "\n", "            ", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "drop_rate", ",", "training", "=", "self", ".", "training", ")", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "classifier", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.EfficientNet.forward": [[556, 560], ["efficientnet.EfficientNet.forward_features", "efficientnet.EfficientNet.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.EfficientNetFeatures.__init__": [[569, 602], ["torch.Module.__init__", "layers.get_norm_act_layer", "layers.create_conv2d", "layers.get_norm_act_layer.", "efficientnet_builder.EfficientNetBuilder", "torch.Sequential", "torch.Sequential", "torch.Sequential", "features.FeatureInfo", "efficientnet_builder.efficientnet_init_weights", "round_chs_fn", "efficientnet.EfficientNetFeatures.feature_info.get_dicts", "features.FeatureHooks", "efficientnet_builder.EfficientNetBuilder.", "enumerate", "efficientnet.EfficientNetFeatures.named_modules"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.get_norm_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.efficientnet_init_weights", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get_dicts", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_modules"], ["def", "__init__", "(", "\n", "self", ",", "block_args", ",", "out_indices", "=", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ",", "feature_location", "=", "'bottleneck'", ",", "in_chans", "=", "3", ",", "\n", "stem_size", "=", "32", ",", "fix_stem", "=", "False", ",", "output_stride", "=", "32", ",", "pad_type", "=", "''", ",", "round_chs_fn", "=", "round_channels", ",", "\n", "act_layer", "=", "None", ",", "norm_layer", "=", "None", ",", "se_layer", "=", "None", ",", "drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "        ", "super", "(", "EfficientNetFeatures", ",", "self", ")", ".", "__init__", "(", ")", "\n", "act_layer", "=", "act_layer", "or", "nn", ".", "ReLU", "\n", "norm_layer", "=", "norm_layer", "or", "nn", ".", "BatchNorm2d", "\n", "norm_act_layer", "=", "get_norm_act_layer", "(", "norm_layer", ",", "act_layer", ")", "\n", "se_layer", "=", "se_layer", "or", "SqueezeExcite", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "\n", "# Stem", "\n", "if", "not", "fix_stem", ":", "\n", "            ", "stem_size", "=", "round_chs_fn", "(", "stem_size", ")", "\n", "", "self", ".", "conv_stem", "=", "create_conv2d", "(", "in_chans", ",", "stem_size", ",", "3", ",", "stride", "=", "2", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "bn1", "=", "norm_act_layer", "(", "stem_size", ",", "inplace", "=", "True", ")", "\n", "\n", "# Middle stages (IR/ER/DS Blocks)", "\n", "builder", "=", "EfficientNetBuilder", "(", "\n", "output_stride", "=", "output_stride", ",", "pad_type", "=", "pad_type", ",", "round_chs_fn", "=", "round_chs_fn", ",", "\n", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ",", "se_layer", "=", "se_layer", ",", "drop_path_rate", "=", "drop_path_rate", ",", "\n", "feature_location", "=", "feature_location", ")", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "*", "builder", "(", "stem_size", ",", "block_args", ")", ")", "\n", "self", ".", "feature_info", "=", "FeatureInfo", "(", "builder", ".", "features", ",", "out_indices", ")", "\n", "self", ".", "_stage_out_idx", "=", "{", "v", "[", "'stage'", "]", ":", "i", "for", "i", ",", "v", "in", "enumerate", "(", "self", ".", "feature_info", ")", "if", "i", "in", "out_indices", "}", "\n", "\n", "efficientnet_init_weights", "(", "self", ")", "\n", "\n", "# Register feature extraction hooks with FeatureHooks helper", "\n", "self", ".", "feature_hooks", "=", "None", "\n", "if", "feature_location", "!=", "'bottleneck'", ":", "\n", "            ", "hooks", "=", "self", ".", "feature_info", ".", "get_dicts", "(", "keys", "=", "(", "'module'", ",", "'hook_type'", ")", ")", "\n", "self", ".", "feature_hooks", "=", "FeatureHooks", "(", "hooks", ",", "self", ".", "named_modules", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.EfficientNetFeatures.forward": [[603, 619], ["efficientnet.EfficientNetFeatures.conv_stem", "efficientnet.EfficientNetFeatures.bn1", "enumerate", "efficientnet.EfficientNetFeatures.blocks", "efficientnet.EfficientNetFeatures.feature_hooks.get_output", "list", "features.append", "b", "efficientnet.EfficientNetFeatures.values", "features.append"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureHooks.get_output"], ["", "", "def", "forward", "(", "self", ",", "x", ")", "->", "List", "[", "torch", ".", "Tensor", "]", ":", "\n", "        ", "x", "=", "self", ".", "conv_stem", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "if", "self", ".", "feature_hooks", "is", "None", ":", "\n", "            ", "features", "=", "[", "]", "\n", "if", "0", "in", "self", ".", "_stage_out_idx", ":", "\n", "                ", "features", ".", "append", "(", "x", ")", "# add stem out", "\n", "", "for", "i", ",", "b", "in", "enumerate", "(", "self", ".", "blocks", ")", ":", "\n", "                ", "x", "=", "b", "(", "x", ")", "\n", "if", "i", "+", "1", "in", "self", ".", "_stage_out_idx", ":", "\n", "                    ", "features", ".", "append", "(", "x", ")", "\n", "", "", "return", "features", "\n", "", "else", ":", "\n", "            ", "self", ".", "blocks", "(", "x", ")", "\n", "out", "=", "self", ".", "feature_hooks", ".", "get_output", "(", "x", ".", "device", ")", "\n", "return", "list", "(", "out", ".", "values", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._cfg": [[58, 65], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bicubic'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'conv_stem'", ",", "'classifier'", ":", "'classifier'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._create_effnet": [[621, 637], ["kwargs.pop", "helpers.build_model_with_cfg", "helpers.pretrained_cfg_for_features"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.pretrained_cfg_for_features"], ["", "", "", "def", "_create_effnet", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "features_only", "=", "False", "\n", "model_cls", "=", "EfficientNet", "\n", "kwargs_filter", "=", "None", "\n", "if", "kwargs", ".", "pop", "(", "'features_only'", ",", "False", ")", ":", "\n", "        ", "features_only", "=", "True", "\n", "kwargs_filter", "=", "(", "'num_classes'", ",", "'num_features'", ",", "'head_conv'", ",", "'global_pool'", ")", "\n", "model_cls", "=", "EfficientNetFeatures", "\n", "", "model", "=", "build_model_with_cfg", "(", "\n", "model_cls", ",", "variant", ",", "pretrained", ",", "\n", "pretrained_strict", "=", "not", "features_only", ",", "\n", "kwargs_filter", "=", "kwargs_filter", ",", "\n", "**", "kwargs", ")", "\n", "if", "features_only", ":", "\n", "        ", "model", ".", "default_cfg", "=", "pretrained_cfg_for_features", "(", "model", ".", "default_cfg", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mnasnet_a1": [[639, 673], ["dict", "efficientnet._create_effnet", "efficientnet_builder.decode_arch_def", "functools.partial", "kwargs.pop", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._create_effnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_mnasnet_a1", "(", "variant", ",", "channel_multiplier", "=", "1.0", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Creates a mnasnet-a1 model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet\n    Paper: https://arxiv.org/pdf/1807.11626.pdf.\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    \"\"\"", "\n", "arch_def", "=", "[", "\n", "# stage 0, 112x112 in", "\n", "[", "'ds_r1_k3_s1_e1_c16_noskip'", "]", ",", "\n", "# stage 1, 112x112 in", "\n", "[", "'ir_r2_k3_s2_e6_c24'", "]", ",", "\n", "# stage 2, 56x56 in", "\n", "[", "'ir_r3_k5_s2_e3_c40_se0.25'", "]", ",", "\n", "# stage 3, 28x28 in", "\n", "[", "'ir_r4_k3_s2_e6_c80'", "]", ",", "\n", "# stage 4, 14x14in", "\n", "[", "'ir_r2_k3_s1_e6_c112_se0.25'", "]", ",", "\n", "# stage 5, 14x14in", "\n", "[", "'ir_r3_k5_s2_e6_c160_se0.25'", "]", ",", "\n", "# stage 6, 7x7 in", "\n", "[", "'ir_r1_k3_s1_e6_c320'", "]", ",", "\n", "]", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ")", ",", "\n", "stem_size", "=", "32", ",", "\n", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ")", ",", "\n", "norm_layer", "=", "kwargs", ".", "pop", "(", "'norm_layer'", ",", "None", ")", "or", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "**", "kwargs", "\n", ")", "\n", "model", "=", "_create_effnet", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mnasnet_b1": [[675, 709], ["dict", "efficientnet._create_effnet", "efficientnet_builder.decode_arch_def", "functools.partial", "kwargs.pop", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._create_effnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_mnasnet_b1", "(", "variant", ",", "channel_multiplier", "=", "1.0", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Creates a mnasnet-b1 model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet\n    Paper: https://arxiv.org/pdf/1807.11626.pdf.\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    \"\"\"", "\n", "arch_def", "=", "[", "\n", "# stage 0, 112x112 in", "\n", "[", "'ds_r1_k3_s1_c16_noskip'", "]", ",", "\n", "# stage 1, 112x112 in", "\n", "[", "'ir_r3_k3_s2_e3_c24'", "]", ",", "\n", "# stage 2, 56x56 in", "\n", "[", "'ir_r3_k5_s2_e3_c40'", "]", ",", "\n", "# stage 3, 28x28 in", "\n", "[", "'ir_r3_k5_s2_e6_c80'", "]", ",", "\n", "# stage 4, 14x14in", "\n", "[", "'ir_r2_k3_s1_e6_c96'", "]", ",", "\n", "# stage 5, 14x14in", "\n", "[", "'ir_r4_k5_s2_e6_c192'", "]", ",", "\n", "# stage 6, 7x7 in", "\n", "[", "'ir_r1_k3_s1_e6_c320_noskip'", "]", "\n", "]", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ")", ",", "\n", "stem_size", "=", "32", ",", "\n", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ")", ",", "\n", "norm_layer", "=", "kwargs", ".", "pop", "(", "'norm_layer'", ",", "None", ")", "or", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "**", "kwargs", "\n", ")", "\n", "model", "=", "_create_effnet", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mnasnet_small": [[711, 738], ["dict", "efficientnet._create_effnet", "efficientnet_builder.decode_arch_def", "functools.partial", "kwargs.pop", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._create_effnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_mnasnet_small", "(", "variant", ",", "channel_multiplier", "=", "1.0", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Creates a mnasnet-b1 model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet\n    Paper: https://arxiv.org/pdf/1807.11626.pdf.\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    \"\"\"", "\n", "arch_def", "=", "[", "\n", "[", "'ds_r1_k3_s1_c8'", "]", ",", "\n", "[", "'ir_r1_k3_s2_e3_c16'", "]", ",", "\n", "[", "'ir_r2_k3_s2_e6_c16'", "]", ",", "\n", "[", "'ir_r4_k5_s2_e6_c32_se0.25'", "]", ",", "\n", "[", "'ir_r3_k3_s1_e6_c32_se0.25'", "]", ",", "\n", "[", "'ir_r3_k5_s2_e6_c88_se0.25'", "]", ",", "\n", "[", "'ir_r1_k3_s1_e6_c144'", "]", "\n", "]", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ")", ",", "\n", "stem_size", "=", "8", ",", "\n", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ")", ",", "\n", "norm_layer", "=", "kwargs", ".", "pop", "(", "'norm_layer'", ",", "None", ")", "or", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "**", "kwargs", "\n", ")", "\n", "model", "=", "_create_effnet", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mobilenet_v2": [[740, 768], ["functools.partial", "dict", "efficientnet._create_effnet", "efficientnet_builder.decode_arch_def", "efficientnet_builder.resolve_act_layer", "max", "kwargs.pop", "functools.partial", "functools.partial.", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._create_effnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_mobilenet_v2", "(", "\n", "variant", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "fix_stem_head", "=", "False", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Generate MobileNet-V2 network\n    Ref impl: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v2.py\n    Paper: https://arxiv.org/abs/1801.04381\n    \"\"\"", "\n", "arch_def", "=", "[", "\n", "[", "'ds_r1_k3_s1_c16'", "]", ",", "\n", "[", "'ir_r2_k3_s2_e6_c24'", "]", ",", "\n", "[", "'ir_r3_k3_s2_e6_c32'", "]", ",", "\n", "[", "'ir_r4_k3_s2_e6_c64'", "]", ",", "\n", "[", "'ir_r3_k3_s1_e6_c96'", "]", ",", "\n", "[", "'ir_r3_k3_s2_e6_c160'", "]", ",", "\n", "[", "'ir_r1_k3_s1_e6_c320'", "]", ",", "\n", "]", "\n", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ")", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ",", "depth_multiplier", "=", "depth_multiplier", ",", "fix_first_last", "=", "fix_stem_head", ")", ",", "\n", "num_features", "=", "1280", "if", "fix_stem_head", "else", "max", "(", "1280", ",", "round_chs_fn", "(", "1280", ")", ")", ",", "\n", "stem_size", "=", "32", ",", "\n", "fix_stem", "=", "fix_stem_head", ",", "\n", "round_chs_fn", "=", "round_chs_fn", ",", "\n", "norm_layer", "=", "kwargs", ".", "pop", "(", "'norm_layer'", ",", "None", ")", "or", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "act_layer", "=", "resolve_act_layer", "(", "kwargs", ",", "'relu6'", ")", ",", "\n", "**", "kwargs", "\n", ")", "\n", "model", "=", "_create_effnet", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_fbnetc": [[770, 798], ["dict", "efficientnet._create_effnet", "efficientnet_builder.decode_arch_def", "functools.partial", "kwargs.pop", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._create_effnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_fbnetc", "(", "variant", ",", "channel_multiplier", "=", "1.0", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" FBNet-C\n\n        Paper: https://arxiv.org/abs/1812.03443\n        Ref Impl: https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/modeling/backbone/fbnet_modeldef.py\n\n        NOTE: the impl above does not relate to the 'C' variant here, that was derived from paper,\n        it was used to confirm some building block details\n    \"\"\"", "\n", "arch_def", "=", "[", "\n", "[", "'ir_r1_k3_s1_e1_c16'", "]", ",", "\n", "[", "'ir_r1_k3_s2_e6_c24'", ",", "'ir_r2_k3_s1_e1_c24'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e6_c32'", ",", "'ir_r1_k5_s1_e3_c32'", ",", "'ir_r1_k5_s1_e6_c32'", ",", "'ir_r1_k3_s1_e6_c32'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e6_c64'", ",", "'ir_r1_k5_s1_e3_c64'", ",", "'ir_r2_k5_s1_e6_c64'", "]", ",", "\n", "[", "'ir_r3_k5_s1_e6_c112'", ",", "'ir_r1_k5_s1_e3_c112'", "]", ",", "\n", "[", "'ir_r4_k5_s2_e6_c184'", "]", ",", "\n", "[", "'ir_r1_k3_s1_e6_c352'", "]", ",", "\n", "]", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ")", ",", "\n", "stem_size", "=", "16", ",", "\n", "num_features", "=", "1984", ",", "# paper suggests this, but is not 100% clear", "\n", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ")", ",", "\n", "norm_layer", "=", "kwargs", ".", "pop", "(", "'norm_layer'", ",", "None", ")", "or", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "**", "kwargs", "\n", ")", "\n", "model", "=", "_create_effnet", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_spnasnet": [[800, 833], ["dict", "efficientnet._create_effnet", "efficientnet_builder.decode_arch_def", "functools.partial", "kwargs.pop", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._create_effnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_spnasnet", "(", "variant", ",", "channel_multiplier", "=", "1.0", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Creates the Single-Path NAS model from search targeted for Pixel1 phone.\n\n    Paper: https://arxiv.org/abs/1904.02877\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    \"\"\"", "\n", "arch_def", "=", "[", "\n", "# stage 0, 112x112 in", "\n", "[", "'ds_r1_k3_s1_c16_noskip'", "]", ",", "\n", "# stage 1, 112x112 in", "\n", "[", "'ir_r3_k3_s2_e3_c24'", "]", ",", "\n", "# stage 2, 56x56 in", "\n", "[", "'ir_r1_k5_s2_e6_c40'", ",", "'ir_r3_k3_s1_e3_c40'", "]", ",", "\n", "# stage 3, 28x28 in", "\n", "[", "'ir_r1_k5_s2_e6_c80'", ",", "'ir_r3_k3_s1_e3_c80'", "]", ",", "\n", "# stage 4, 14x14in", "\n", "[", "'ir_r1_k5_s1_e6_c96'", ",", "'ir_r3_k5_s1_e3_c96'", "]", ",", "\n", "# stage 5, 14x14in", "\n", "[", "'ir_r4_k5_s2_e6_c192'", "]", ",", "\n", "# stage 6, 7x7 in", "\n", "[", "'ir_r1_k3_s1_e6_c320_noskip'", "]", "\n", "]", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ")", ",", "\n", "stem_size", "=", "32", ",", "\n", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ")", ",", "\n", "norm_layer", "=", "kwargs", ".", "pop", "(", "'norm_layer'", ",", "None", ")", "or", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "**", "kwargs", "\n", ")", "\n", "model", "=", "_create_effnet", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet": [[835, 882], ["functools.partial", "dict", "efficientnet._create_effnet", "efficientnet_builder.decode_arch_def", "functools.partial.", "efficientnet_builder.resolve_act_layer", "kwargs.pop", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._create_effnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_efficientnet", "(", "\n", "variant", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "channel_divisor", "=", "8", ",", "\n", "group_size", "=", "None", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Creates an EfficientNet model.\n\n    Ref impl: https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_model.py\n    Paper: https://arxiv.org/abs/1905.11946\n\n    EfficientNet params\n    name: (channel_multiplier, depth_multiplier, resolution, dropout_rate)\n    'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n    'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n    'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n    'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n    'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n    'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n    'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n    'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n    'efficientnet-b8': (2.2, 3.6, 672, 0.5),\n    'efficientnet-l2': (4.3, 5.3, 800, 0.5),\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer\n      depth_multiplier: multiplier to number of repeats per stage\n\n    \"\"\"", "\n", "arch_def", "=", "[", "\n", "[", "'ds_r1_k3_s1_e1_c16_se0.25'", "]", ",", "\n", "[", "'ir_r2_k3_s2_e6_c24_se0.25'", "]", ",", "\n", "[", "'ir_r2_k5_s2_e6_c40_se0.25'", "]", ",", "\n", "[", "'ir_r3_k3_s2_e6_c80_se0.25'", "]", ",", "\n", "[", "'ir_r3_k5_s1_e6_c112_se0.25'", "]", ",", "\n", "[", "'ir_r4_k5_s2_e6_c192_se0.25'", "]", ",", "\n", "[", "'ir_r1_k3_s1_e6_c320_se0.25'", "]", ",", "\n", "]", "\n", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ",", "divisor", "=", "channel_divisor", ")", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ",", "depth_multiplier", ",", "group_size", "=", "group_size", ")", ",", "\n", "num_features", "=", "round_chs_fn", "(", "1280", ")", ",", "\n", "stem_size", "=", "32", ",", "\n", "round_chs_fn", "=", "round_chs_fn", ",", "\n", "act_layer", "=", "resolve_act_layer", "(", "kwargs", ",", "'swish'", ")", ",", "\n", "norm_layer", "=", "kwargs", ".", "pop", "(", "'norm_layer'", ",", "None", ")", "or", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "model", "=", "_create_effnet", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_edge": [[884, 913], ["functools.partial", "dict", "efficientnet._create_effnet", "efficientnet_builder.decode_arch_def", "functools.partial.", "efficientnet_builder.resolve_act_layer", "kwargs.pop", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._create_effnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_efficientnet_edge", "(", "\n", "variant", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "group_size", "=", "None", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Creates an EfficientNet-EdgeTPU model\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/edgetpu\n    \"\"\"", "\n", "\n", "arch_def", "=", "[", "\n", "# NOTE `fc` is present to override a mismatch between stem channels and in chs not", "\n", "# present in other models", "\n", "[", "'er_r1_k3_s1_e4_c24_fc24_noskip'", "]", ",", "\n", "[", "'er_r2_k3_s2_e8_c32'", "]", ",", "\n", "[", "'er_r4_k3_s2_e8_c48'", "]", ",", "\n", "[", "'ir_r5_k5_s2_e8_c96'", "]", ",", "\n", "[", "'ir_r4_k5_s1_e8_c144'", "]", ",", "\n", "[", "'ir_r2_k5_s2_e8_c192'", "]", ",", "\n", "]", "\n", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ")", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ",", "depth_multiplier", ",", "group_size", "=", "group_size", ")", ",", "\n", "num_features", "=", "round_chs_fn", "(", "1280", ")", ",", "\n", "stem_size", "=", "32", ",", "\n", "round_chs_fn", "=", "round_chs_fn", ",", "\n", "norm_layer", "=", "kwargs", ".", "pop", "(", "'norm_layer'", ",", "None", ")", "or", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "act_layer", "=", "resolve_act_layer", "(", "kwargs", ",", "'relu'", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "model", "=", "_create_effnet", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_condconv": [[915, 944], ["functools.partial", "dict", "efficientnet._create_effnet", "efficientnet_builder.decode_arch_def", "functools.partial.", "efficientnet_builder.resolve_act_layer", "kwargs.pop", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._create_effnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_efficientnet_condconv", "(", "\n", "variant", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "experts_multiplier", "=", "1", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Creates an EfficientNet-CondConv model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/condconv\n    \"\"\"", "\n", "arch_def", "=", "[", "\n", "[", "'ds_r1_k3_s1_e1_c16_se0.25'", "]", ",", "\n", "[", "'ir_r2_k3_s2_e6_c24_se0.25'", "]", ",", "\n", "[", "'ir_r2_k5_s2_e6_c40_se0.25'", "]", ",", "\n", "[", "'ir_r3_k3_s2_e6_c80_se0.25'", "]", ",", "\n", "[", "'ir_r3_k5_s1_e6_c112_se0.25_cc4'", "]", ",", "\n", "[", "'ir_r4_k5_s2_e6_c192_se0.25_cc4'", "]", ",", "\n", "[", "'ir_r1_k3_s1_e6_c320_se0.25_cc4'", "]", ",", "\n", "]", "\n", "# NOTE unlike official impl, this one uses `cc<x>` option where x is the base number of experts for each stage and", "\n", "# the expert_multiplier increases that on a per-model basis as with depth/channel multipliers", "\n", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ")", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ",", "depth_multiplier", ",", "experts_multiplier", "=", "experts_multiplier", ")", ",", "\n", "num_features", "=", "round_chs_fn", "(", "1280", ")", ",", "\n", "stem_size", "=", "32", ",", "\n", "round_chs_fn", "=", "round_chs_fn", ",", "\n", "norm_layer", "=", "kwargs", ".", "pop", "(", "'norm_layer'", ",", "None", ")", "or", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "act_layer", "=", "resolve_act_layer", "(", "kwargs", ",", "'swish'", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "model", "=", "_create_effnet", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_lite": [[946, 985], ["dict", "efficientnet._create_effnet", "efficientnet_builder.decode_arch_def", "functools.partial", "efficientnet_builder.resolve_act_layer", "kwargs.pop", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._create_effnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_efficientnet_lite", "(", "variant", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Creates an EfficientNet-Lite model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite\n    Paper: https://arxiv.org/abs/1905.11946\n\n    EfficientNet params\n    name: (channel_multiplier, depth_multiplier, resolution, dropout_rate)\n      'efficientnet-lite0': (1.0, 1.0, 224, 0.2),\n      'efficientnet-lite1': (1.0, 1.1, 240, 0.2),\n      'efficientnet-lite2': (1.1, 1.2, 260, 0.3),\n      'efficientnet-lite3': (1.2, 1.4, 280, 0.3),\n      'efficientnet-lite4': (1.4, 1.8, 300, 0.3),\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer\n      depth_multiplier: multiplier to number of repeats per stage\n    \"\"\"", "\n", "arch_def", "=", "[", "\n", "[", "'ds_r1_k3_s1_e1_c16'", "]", ",", "\n", "[", "'ir_r2_k3_s2_e6_c24'", "]", ",", "\n", "[", "'ir_r2_k5_s2_e6_c40'", "]", ",", "\n", "[", "'ir_r3_k3_s2_e6_c80'", "]", ",", "\n", "[", "'ir_r3_k5_s1_e6_c112'", "]", ",", "\n", "[", "'ir_r4_k5_s2_e6_c192'", "]", ",", "\n", "[", "'ir_r1_k3_s1_e6_c320'", "]", ",", "\n", "]", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ",", "depth_multiplier", ",", "fix_first_last", "=", "True", ")", ",", "\n", "num_features", "=", "1280", ",", "\n", "stem_size", "=", "32", ",", "\n", "fix_stem", "=", "True", ",", "\n", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ")", ",", "\n", "act_layer", "=", "resolve_act_layer", "(", "kwargs", ",", "'relu6'", ")", ",", "\n", "norm_layer", "=", "kwargs", ".", "pop", "(", "'norm_layer'", ",", "None", ")", "or", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "model", "=", "_create_effnet", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_base": [[987, 1014], ["functools.partial", "dict", "efficientnet._create_effnet", "efficientnet_builder.decode_arch_def", "functools.partial.", "efficientnet_builder.resolve_act_layer", "kwargs.pop", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._create_effnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_efficientnetv2_base", "(", "\n", "variant", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Creates an EfficientNet-V2 base model\n\n    Ref impl: https://github.com/google/automl/tree/master/efficientnetv2\n    Paper: `EfficientNetV2: Smaller Models and Faster Training` - https://arxiv.org/abs/2104.00298\n    \"\"\"", "\n", "arch_def", "=", "[", "\n", "[", "'cn_r1_k3_s1_e1_c16_skip'", "]", ",", "\n", "[", "'er_r2_k3_s2_e4_c32'", "]", ",", "\n", "[", "'er_r2_k3_s2_e4_c48'", "]", ",", "\n", "[", "'ir_r3_k3_s2_e4_c96_se0.25'", "]", ",", "\n", "[", "'ir_r5_k3_s1_e6_c112_se0.25'", "]", ",", "\n", "[", "'ir_r8_k3_s2_e6_c192_se0.25'", "]", ",", "\n", "]", "\n", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ",", "round_limit", "=", "0.", ")", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ",", "depth_multiplier", ")", ",", "\n", "num_features", "=", "round_chs_fn", "(", "1280", ")", ",", "\n", "stem_size", "=", "32", ",", "\n", "round_chs_fn", "=", "round_chs_fn", ",", "\n", "norm_layer", "=", "kwargs", ".", "pop", "(", "'norm_layer'", ",", "None", ")", "or", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "act_layer", "=", "resolve_act_layer", "(", "kwargs", ",", "'silu'", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "model", "=", "_create_effnet", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_s": [[1016, 1053], ["functools.partial", "dict", "efficientnet._create_effnet", "efficientnet_builder.decode_arch_def", "functools.partial.", "efficientnet_builder.resolve_act_layer", "kwargs.pop", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._create_effnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_efficientnetv2_s", "(", "\n", "variant", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "group_size", "=", "None", ",", "rw", "=", "False", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Creates an EfficientNet-V2 Small model\n\n    Ref impl: https://github.com/google/automl/tree/master/efficientnetv2\n    Paper: `EfficientNetV2: Smaller Models and Faster Training` - https://arxiv.org/abs/2104.00298\n\n    NOTE: `rw` flag sets up 'small' variant to behave like my initial v2 small model,\n        before ref the impl was released.\n    \"\"\"", "\n", "arch_def", "=", "[", "\n", "[", "'cn_r2_k3_s1_e1_c24_skip'", "]", ",", "\n", "[", "'er_r4_k3_s2_e4_c48'", "]", ",", "\n", "[", "'er_r4_k3_s2_e4_c64'", "]", ",", "\n", "[", "'ir_r6_k3_s2_e4_c128_se0.25'", "]", ",", "\n", "[", "'ir_r9_k3_s1_e6_c160_se0.25'", "]", ",", "\n", "[", "'ir_r15_k3_s2_e6_c256_se0.25'", "]", ",", "\n", "]", "\n", "num_features", "=", "1280", "\n", "if", "rw", ":", "\n", "# my original variant, based on paper figure differs from the official release", "\n", "        ", "arch_def", "[", "0", "]", "=", "[", "'er_r2_k3_s1_e1_c24'", "]", "\n", "arch_def", "[", "-", "1", "]", "=", "[", "'ir_r15_k3_s2_e6_c272_se0.25'", "]", "\n", "num_features", "=", "1792", "\n", "\n", "", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ")", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ",", "depth_multiplier", ",", "group_size", "=", "group_size", ")", ",", "\n", "num_features", "=", "round_chs_fn", "(", "num_features", ")", ",", "\n", "stem_size", "=", "24", ",", "\n", "round_chs_fn", "=", "round_chs_fn", ",", "\n", "norm_layer", "=", "kwargs", ".", "pop", "(", "'norm_layer'", ",", "None", ")", "or", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "act_layer", "=", "resolve_act_layer", "(", "kwargs", ",", "'silu'", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "model", "=", "_create_effnet", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_m": [[1055, 1083], ["dict", "efficientnet._create_effnet", "efficientnet_builder.decode_arch_def", "functools.partial", "efficientnet_builder.resolve_act_layer", "kwargs.pop", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._create_effnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_efficientnetv2_m", "(", "variant", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Creates an EfficientNet-V2 Medium model\n\n    Ref impl: https://github.com/google/automl/tree/master/efficientnetv2\n    Paper: `EfficientNetV2: Smaller Models and Faster Training` - https://arxiv.org/abs/2104.00298\n    \"\"\"", "\n", "\n", "arch_def", "=", "[", "\n", "[", "'cn_r3_k3_s1_e1_c24_skip'", "]", ",", "\n", "[", "'er_r5_k3_s2_e4_c48'", "]", ",", "\n", "[", "'er_r5_k3_s2_e4_c80'", "]", ",", "\n", "[", "'ir_r7_k3_s2_e4_c160_se0.25'", "]", ",", "\n", "[", "'ir_r14_k3_s1_e6_c176_se0.25'", "]", ",", "\n", "[", "'ir_r18_k3_s2_e6_c304_se0.25'", "]", ",", "\n", "[", "'ir_r5_k3_s1_e6_c512_se0.25'", "]", ",", "\n", "]", "\n", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ",", "depth_multiplier", ")", ",", "\n", "num_features", "=", "1280", ",", "\n", "stem_size", "=", "24", ",", "\n", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ")", ",", "\n", "norm_layer", "=", "kwargs", ".", "pop", "(", "'norm_layer'", ",", "None", ")", "or", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "act_layer", "=", "resolve_act_layer", "(", "kwargs", ",", "'silu'", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "model", "=", "_create_effnet", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_l": [[1085, 1113], ["dict", "efficientnet._create_effnet", "efficientnet_builder.decode_arch_def", "functools.partial", "efficientnet_builder.resolve_act_layer", "kwargs.pop", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._create_effnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_efficientnetv2_l", "(", "variant", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Creates an EfficientNet-V2 Large model\n\n    Ref impl: https://github.com/google/automl/tree/master/efficientnetv2\n    Paper: `EfficientNetV2: Smaller Models and Faster Training` - https://arxiv.org/abs/2104.00298\n    \"\"\"", "\n", "\n", "arch_def", "=", "[", "\n", "[", "'cn_r4_k3_s1_e1_c32_skip'", "]", ",", "\n", "[", "'er_r7_k3_s2_e4_c64'", "]", ",", "\n", "[", "'er_r7_k3_s2_e4_c96'", "]", ",", "\n", "[", "'ir_r10_k3_s2_e4_c192_se0.25'", "]", ",", "\n", "[", "'ir_r19_k3_s1_e6_c224_se0.25'", "]", ",", "\n", "[", "'ir_r25_k3_s2_e6_c384_se0.25'", "]", ",", "\n", "[", "'ir_r7_k3_s1_e6_c640_se0.25'", "]", ",", "\n", "]", "\n", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ",", "depth_multiplier", ")", ",", "\n", "num_features", "=", "1280", ",", "\n", "stem_size", "=", "32", ",", "\n", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ")", ",", "\n", "norm_layer", "=", "kwargs", ".", "pop", "(", "'norm_layer'", ",", "None", ")", "or", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "act_layer", "=", "resolve_act_layer", "(", "kwargs", ",", "'silu'", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "model", "=", "_create_effnet", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_xl": [[1115, 1143], ["dict", "efficientnet._create_effnet", "efficientnet_builder.decode_arch_def", "functools.partial", "efficientnet_builder.resolve_act_layer", "kwargs.pop", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._create_effnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_efficientnetv2_xl", "(", "variant", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Creates an EfficientNet-V2 Xtra-Large model\n\n    Ref impl: https://github.com/google/automl/tree/master/efficientnetv2\n    Paper: `EfficientNetV2: Smaller Models and Faster Training` - https://arxiv.org/abs/2104.00298\n    \"\"\"", "\n", "\n", "arch_def", "=", "[", "\n", "[", "'cn_r4_k3_s1_e1_c32_skip'", "]", ",", "\n", "[", "'er_r8_k3_s2_e4_c64'", "]", ",", "\n", "[", "'er_r8_k3_s2_e4_c96'", "]", ",", "\n", "[", "'ir_r16_k3_s2_e4_c192_se0.25'", "]", ",", "\n", "[", "'ir_r24_k3_s1_e6_c256_se0.25'", "]", ",", "\n", "[", "'ir_r32_k3_s2_e6_c512_se0.25'", "]", ",", "\n", "[", "'ir_r8_k3_s1_e6_c640_se0.25'", "]", ",", "\n", "]", "\n", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ",", "depth_multiplier", ")", ",", "\n", "num_features", "=", "1280", ",", "\n", "stem_size", "=", "32", ",", "\n", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ")", ",", "\n", "norm_layer", "=", "kwargs", ".", "pop", "(", "'norm_layer'", ",", "None", ")", "or", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "act_layer", "=", "resolve_act_layer", "(", "kwargs", ",", "'silu'", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "model", "=", "_create_effnet", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mixnet_s": [[1145, 1176], ["dict", "efficientnet._create_effnet", "efficientnet_builder.decode_arch_def", "functools.partial", "kwargs.pop", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._create_effnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_mixnet_s", "(", "variant", ",", "channel_multiplier", "=", "1.0", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Creates a MixNet Small model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet\n    Paper: https://arxiv.org/abs/1907.09595\n    \"\"\"", "\n", "arch_def", "=", "[", "\n", "# stage 0, 112x112 in", "\n", "[", "'ds_r1_k3_s1_e1_c16'", "]", ",", "# relu", "\n", "# stage 1, 112x112 in", "\n", "[", "'ir_r1_k3_a1.1_p1.1_s2_e6_c24'", ",", "'ir_r1_k3_a1.1_p1.1_s1_e3_c24'", "]", ",", "# relu", "\n", "# stage 2, 56x56 in", "\n", "[", "'ir_r1_k3.5.7_s2_e6_c40_se0.5_nsw'", ",", "'ir_r3_k3.5_a1.1_p1.1_s1_e6_c40_se0.5_nsw'", "]", ",", "# swish", "\n", "# stage 3, 28x28 in", "\n", "[", "'ir_r1_k3.5.7_p1.1_s2_e6_c80_se0.25_nsw'", ",", "'ir_r2_k3.5_p1.1_s1_e6_c80_se0.25_nsw'", "]", ",", "# swish", "\n", "# stage 4, 14x14in", "\n", "[", "'ir_r1_k3.5.7_a1.1_p1.1_s1_e6_c120_se0.5_nsw'", ",", "'ir_r2_k3.5.7.9_a1.1_p1.1_s1_e3_c120_se0.5_nsw'", "]", ",", "# swish", "\n", "# stage 5, 14x14in", "\n", "[", "'ir_r1_k3.5.7.9.11_s2_e6_c200_se0.5_nsw'", ",", "'ir_r2_k3.5.7.9_p1.1_s1_e6_c200_se0.5_nsw'", "]", ",", "# swish", "\n", "# 7x7", "\n", "]", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ")", ",", "\n", "num_features", "=", "1536", ",", "\n", "stem_size", "=", "16", ",", "\n", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ")", ",", "\n", "norm_layer", "=", "kwargs", ".", "pop", "(", "'norm_layer'", ",", "None", ")", "or", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "**", "kwargs", "\n", ")", "\n", "model", "=", "_create_effnet", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mixnet_m": [[1178, 1209], ["dict", "efficientnet._create_effnet", "efficientnet_builder.decode_arch_def", "functools.partial", "kwargs.pop", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._create_effnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_mixnet_m", "(", "variant", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Creates a MixNet Medium-Large model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet\n    Paper: https://arxiv.org/abs/1907.09595\n    \"\"\"", "\n", "arch_def", "=", "[", "\n", "# stage 0, 112x112 in", "\n", "[", "'ds_r1_k3_s1_e1_c24'", "]", ",", "# relu", "\n", "# stage 1, 112x112 in", "\n", "[", "'ir_r1_k3.5.7_a1.1_p1.1_s2_e6_c32'", ",", "'ir_r1_k3_a1.1_p1.1_s1_e3_c32'", "]", ",", "# relu", "\n", "# stage 2, 56x56 in", "\n", "[", "'ir_r1_k3.5.7.9_s2_e6_c40_se0.5_nsw'", ",", "'ir_r3_k3.5_a1.1_p1.1_s1_e6_c40_se0.5_nsw'", "]", ",", "# swish", "\n", "# stage 3, 28x28 in", "\n", "[", "'ir_r1_k3.5.7_s2_e6_c80_se0.25_nsw'", ",", "'ir_r3_k3.5.7.9_a1.1_p1.1_s1_e6_c80_se0.25_nsw'", "]", ",", "# swish", "\n", "# stage 4, 14x14in", "\n", "[", "'ir_r1_k3_s1_e6_c120_se0.5_nsw'", ",", "'ir_r3_k3.5.7.9_a1.1_p1.1_s1_e3_c120_se0.5_nsw'", "]", ",", "# swish", "\n", "# stage 5, 14x14in", "\n", "[", "'ir_r1_k3.5.7.9_s2_e6_c200_se0.5_nsw'", ",", "'ir_r3_k3.5.7.9_p1.1_s1_e6_c200_se0.5_nsw'", "]", ",", "# swish", "\n", "# 7x7", "\n", "]", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ",", "depth_multiplier", ",", "depth_trunc", "=", "'round'", ")", ",", "\n", "num_features", "=", "1536", ",", "\n", "stem_size", "=", "24", ",", "\n", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ")", ",", "\n", "norm_layer", "=", "kwargs", ".", "pop", "(", "'norm_layer'", ",", "None", ")", "or", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "**", "kwargs", "\n", ")", "\n", "model", "=", "_create_effnet", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_tinynet": [[1211, 1234], ["dict", "efficientnet._create_effnet", "efficientnet_builder.decode_arch_def", "max", "functools.partial", "efficientnet_builder.resolve_act_layer", "efficientnet_builder.round_channels", "kwargs.pop", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._create_effnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.round_channels", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_tinynet", "(", "\n", "variant", ",", "model_width", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "pretrained", "=", "False", ",", "**", "kwargs", "\n", ")", ":", "\n", "    ", "\"\"\"Creates a TinyNet model.\n    \"\"\"", "\n", "arch_def", "=", "[", "\n", "[", "'ds_r1_k3_s1_e1_c16_se0.25'", "]", ",", "[", "'ir_r2_k3_s2_e6_c24_se0.25'", "]", ",", "\n", "[", "'ir_r2_k5_s2_e6_c40_se0.25'", "]", ",", "[", "'ir_r3_k3_s2_e6_c80_se0.25'", "]", ",", "\n", "[", "'ir_r3_k5_s1_e6_c112_se0.25'", "]", ",", "[", "'ir_r4_k5_s2_e6_c192_se0.25'", "]", ",", "\n", "[", "'ir_r1_k3_s1_e6_c320_se0.25'", "]", ",", "\n", "]", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ",", "depth_multiplier", ",", "depth_trunc", "=", "'round'", ")", ",", "\n", "num_features", "=", "max", "(", "1280", ",", "round_channels", "(", "1280", ",", "model_width", ",", "8", ",", "None", ")", ")", ",", "\n", "stem_size", "=", "32", ",", "\n", "fix_stem", "=", "True", ",", "\n", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "model_width", ")", ",", "\n", "act_layer", "=", "resolve_act_layer", "(", "kwargs", ",", "'swish'", ")", ",", "\n", "norm_layer", "=", "kwargs", ".", "pop", "(", "'norm_layer'", ",", "None", ")", "or", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "model", "=", "_create_effnet", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.mnasnet_050": [[1236, 1241], ["efficientnet._gen_mnasnet_b1"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mnasnet_b1"], ["", "@", "register_model", "\n", "def", "mnasnet_050", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MNASNet B1, depth multiplier of 0.5. \"\"\"", "\n", "model", "=", "_gen_mnasnet_b1", "(", "'mnasnet_050'", ",", "0.5", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.mnasnet_075": [[1243, 1248], ["efficientnet._gen_mnasnet_b1"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mnasnet_b1"], ["", "@", "register_model", "\n", "def", "mnasnet_075", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MNASNet B1, depth multiplier of 0.75. \"\"\"", "\n", "model", "=", "_gen_mnasnet_b1", "(", "'mnasnet_075'", ",", "0.75", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.mnasnet_100": [[1250, 1255], ["efficientnet._gen_mnasnet_b1"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mnasnet_b1"], ["", "@", "register_model", "\n", "def", "mnasnet_100", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MNASNet B1, depth multiplier of 1.0. \"\"\"", "\n", "model", "=", "_gen_mnasnet_b1", "(", "'mnasnet_100'", ",", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.mnasnet_b1": [[1257, 1261], ["efficientnet.mnasnet_100"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.mnasnet_100"], ["", "@", "register_model", "\n", "def", "mnasnet_b1", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MNASNet B1, depth multiplier of 1.0. \"\"\"", "\n", "return", "mnasnet_100", "(", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.mnasnet_140": [[1263, 1268], ["efficientnet._gen_mnasnet_b1"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mnasnet_b1"], ["", "@", "register_model", "\n", "def", "mnasnet_140", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MNASNet B1,  depth multiplier of 1.4 \"\"\"", "\n", "model", "=", "_gen_mnasnet_b1", "(", "'mnasnet_140'", ",", "1.4", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.semnasnet_050": [[1270, 1275], ["efficientnet._gen_mnasnet_a1"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mnasnet_a1"], ["", "@", "register_model", "\n", "def", "semnasnet_050", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MNASNet A1 (w/ SE), depth multiplier of 0.5 \"\"\"", "\n", "model", "=", "_gen_mnasnet_a1", "(", "'semnasnet_050'", ",", "0.5", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.semnasnet_075": [[1277, 1282], ["efficientnet._gen_mnasnet_a1"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mnasnet_a1"], ["", "@", "register_model", "\n", "def", "semnasnet_075", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MNASNet A1 (w/ SE),  depth multiplier of 0.75. \"\"\"", "\n", "model", "=", "_gen_mnasnet_a1", "(", "'semnasnet_075'", ",", "0.75", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.semnasnet_100": [[1284, 1289], ["efficientnet._gen_mnasnet_a1"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mnasnet_a1"], ["", "@", "register_model", "\n", "def", "semnasnet_100", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MNASNet A1 (w/ SE), depth multiplier of 1.0. \"\"\"", "\n", "model", "=", "_gen_mnasnet_a1", "(", "'semnasnet_100'", ",", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.mnasnet_a1": [[1291, 1295], ["efficientnet.semnasnet_100"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.semnasnet_100"], ["", "@", "register_model", "\n", "def", "mnasnet_a1", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MNASNet A1 (w/ SE), depth multiplier of 1.0. \"\"\"", "\n", "return", "semnasnet_100", "(", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.semnasnet_140": [[1297, 1302], ["efficientnet._gen_mnasnet_a1"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mnasnet_a1"], ["", "@", "register_model", "\n", "def", "semnasnet_140", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MNASNet A1 (w/ SE), depth multiplier of 1.4. \"\"\"", "\n", "model", "=", "_gen_mnasnet_a1", "(", "'semnasnet_140'", ",", "1.4", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.mnasnet_small": [[1304, 1309], ["efficientnet._gen_mnasnet_small"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mnasnet_small"], ["", "@", "register_model", "\n", "def", "mnasnet_small", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MNASNet Small,  depth multiplier of 1.0. \"\"\"", "\n", "model", "=", "_gen_mnasnet_small", "(", "'mnasnet_small'", ",", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.mobilenetv2_035": [[1311, 1316], ["efficientnet._gen_mobilenet_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mobilenet_v2"], ["", "@", "register_model", "\n", "def", "mobilenetv2_035", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V2 w/ 0.35 channel multiplier \"\"\"", "\n", "model", "=", "_gen_mobilenet_v2", "(", "'mobilenetv2_035'", ",", "0.35", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.mobilenetv2_050": [[1318, 1323], ["efficientnet._gen_mobilenet_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mobilenet_v2"], ["", "@", "register_model", "\n", "def", "mobilenetv2_050", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V2 w/ 0.5 channel multiplier \"\"\"", "\n", "model", "=", "_gen_mobilenet_v2", "(", "'mobilenetv2_050'", ",", "0.5", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.mobilenetv2_075": [[1325, 1330], ["efficientnet._gen_mobilenet_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mobilenet_v2"], ["", "@", "register_model", "\n", "def", "mobilenetv2_075", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V2 w/ 0.75 channel multiplier \"\"\"", "\n", "model", "=", "_gen_mobilenet_v2", "(", "'mobilenetv2_075'", ",", "0.75", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.mobilenetv2_100": [[1332, 1337], ["efficientnet._gen_mobilenet_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mobilenet_v2"], ["", "@", "register_model", "\n", "def", "mobilenetv2_100", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V2 w/ 1.0 channel multiplier \"\"\"", "\n", "model", "=", "_gen_mobilenet_v2", "(", "'mobilenetv2_100'", ",", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.mobilenetv2_140": [[1339, 1344], ["efficientnet._gen_mobilenet_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mobilenet_v2"], ["", "@", "register_model", "\n", "def", "mobilenetv2_140", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V2 w/ 1.4 channel multiplier \"\"\"", "\n", "model", "=", "_gen_mobilenet_v2", "(", "'mobilenetv2_140'", ",", "1.4", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.mobilenetv2_110d": [[1346, 1352], ["efficientnet._gen_mobilenet_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mobilenet_v2"], ["", "@", "register_model", "\n", "def", "mobilenetv2_110d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V2 w/ 1.1 channel, 1.2 depth multipliers\"\"\"", "\n", "model", "=", "_gen_mobilenet_v2", "(", "\n", "'mobilenetv2_110d'", ",", "1.1", ",", "depth_multiplier", "=", "1.2", ",", "fix_stem_head", "=", "True", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.mobilenetv2_120d": [[1354, 1360], ["efficientnet._gen_mobilenet_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mobilenet_v2"], ["", "@", "register_model", "\n", "def", "mobilenetv2_120d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V2 w/ 1.2 channel, 1.4 depth multipliers \"\"\"", "\n", "model", "=", "_gen_mobilenet_v2", "(", "\n", "'mobilenetv2_120d'", ",", "1.2", ",", "depth_multiplier", "=", "1.4", ",", "fix_stem_head", "=", "True", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.fbnetc_100": [[1362, 1370], ["efficientnet._gen_fbnetc"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_fbnetc"], ["", "@", "register_model", "\n", "def", "fbnetc_100", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" FBNet-C \"\"\"", "\n", "if", "pretrained", ":", "\n", "# pretrained model trained with non-default BN epsilon", "\n", "        ", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "", "model", "=", "_gen_fbnetc", "(", "'fbnetc_100'", ",", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.spnasnet_100": [[1372, 1377], ["efficientnet._gen_spnasnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_spnasnet"], ["", "@", "register_model", "\n", "def", "spnasnet_100", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Single-Path NAS Pixel1\"\"\"", "\n", "model", "=", "_gen_spnasnet", "(", "'spnasnet_100'", ",", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b0": [[1379, 1386], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "efficientnet_b0", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B0 \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'efficientnet_b0'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b1": [[1388, 1395], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "efficientnet_b1", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B1 \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'efficientnet_b1'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.1", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b2": [[1397, 1404], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "efficientnet_b2", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B2 \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'efficientnet_b2'", ",", "channel_multiplier", "=", "1.1", ",", "depth_multiplier", "=", "1.2", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b2a": [[1406, 1411], ["efficientnet.efficientnet_b2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b2"], ["", "@", "register_model", "\n", "def", "efficientnet_b2a", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B2 @ 288x288 w/ 1.0 test crop\"\"\"", "\n", "# WARN this model def is deprecated, different train/test res + test crop handled by default_cfg now", "\n", "return", "efficientnet_b2", "(", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b3": [[1413, 1420], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "efficientnet_b3", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B3 \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'efficientnet_b3'", ",", "channel_multiplier", "=", "1.2", ",", "depth_multiplier", "=", "1.4", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b3a": [[1422, 1427], ["efficientnet.efficientnet_b3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b3"], ["", "@", "register_model", "\n", "def", "efficientnet_b3a", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B3 @ 320x320 w/ 1.0 test crop-pct \"\"\"", "\n", "# WARN this model def is deprecated, different train/test res + test crop handled by default_cfg now", "\n", "return", "efficientnet_b3", "(", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b4": [[1429, 1436], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "efficientnet_b4", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B4 \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.4, drop_path_rate should be 0.2", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'efficientnet_b4'", ",", "channel_multiplier", "=", "1.4", ",", "depth_multiplier", "=", "1.8", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b5": [[1438, 1445], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "efficientnet_b5", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B5 \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.4, drop_path_rate should be 0.2", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'efficientnet_b5'", ",", "channel_multiplier", "=", "1.6", ",", "depth_multiplier", "=", "2.2", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b6": [[1447, 1454], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "efficientnet_b6", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B6 \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.5, drop_path_rate should be 0.2", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'efficientnet_b6'", ",", "channel_multiplier", "=", "1.8", ",", "depth_multiplier", "=", "2.6", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b7": [[1456, 1463], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "efficientnet_b7", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B7 \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.5, drop_path_rate should be 0.2", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'efficientnet_b7'", ",", "channel_multiplier", "=", "2.0", ",", "depth_multiplier", "=", "3.1", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b8": [[1465, 1472], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "efficientnet_b8", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B8 \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.5, drop_path_rate should be 0.2", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'efficientnet_b8'", ",", "channel_multiplier", "=", "2.2", ",", "depth_multiplier", "=", "3.6", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_l2": [[1474, 1481], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "efficientnet_l2", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-L2.\"\"\"", "\n", "# NOTE for train, drop_rate should be 0.5, drop_path_rate should be 0.2", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'efficientnet_l2'", ",", "channel_multiplier", "=", "4.3", ",", "depth_multiplier", "=", "5.3", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b0_gn": [[1484, 1490], ["efficientnet._gen_efficientnet", "functools.partial"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "efficientnet_b0_gn", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B0 + GroupNorm\"\"\"", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'efficientnet_b0_gn'", ",", "norm_layer", "=", "partial", "(", "GroupNormAct", ",", "group_size", "=", "8", ")", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b0_g8_gn": [[1492, 1499], ["efficientnet._gen_efficientnet", "functools.partial"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "efficientnet_b0_g8_gn", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B0 w/ group conv + GroupNorm\"\"\"", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'efficientnet_b0_g8_gn'", ",", "group_size", "=", "8", ",", "norm_layer", "=", "partial", "(", "GroupNormAct", ",", "group_size", "=", "8", ")", ",", "\n", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b0_g16_evos": [[1501, 1508], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "efficientnet_b0_g16_evos", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B0 w/ group 16 conv + EvoNorm\"\"\"", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'efficientnet_b0_g16_evos'", ",", "group_size", "=", "16", ",", "channel_divisor", "=", "16", ",", "\n", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "#norm_layer=partial(EvoNorm2dS0, group_size=16),", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b3_gn": [[1510, 1518], ["efficientnet._gen_efficientnet", "functools.partial"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "efficientnet_b3_gn", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B3 w/ GroupNorm \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'efficientnet_b3_gn'", ",", "channel_multiplier", "=", "1.2", ",", "depth_multiplier", "=", "1.4", ",", "channel_divisor", "=", "16", ",", "\n", "norm_layer", "=", "partial", "(", "GroupNormAct", ",", "group_size", "=", "16", ")", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b3_g8_gn": [[1520, 1528], ["efficientnet._gen_efficientnet", "functools.partial"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "efficientnet_b3_g8_gn", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B3 w/ grouped conv + BN\"\"\"", "\n", "# NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'efficientnet_b3_g8_gn'", ",", "channel_multiplier", "=", "1.2", ",", "depth_multiplier", "=", "1.4", ",", "group_size", "=", "8", ",", "channel_divisor", "=", "16", ",", "\n", "norm_layer", "=", "partial", "(", "GroupNormAct", ",", "group_size", "=", "16", ")", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_es": [[1530, 1536], ["efficientnet._gen_efficientnet_edge"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_edge"], ["", "@", "register_model", "\n", "def", "efficientnet_es", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-Edge Small. \"\"\"", "\n", "model", "=", "_gen_efficientnet_edge", "(", "\n", "'efficientnet_es'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_es_pruned": [[1538, 1544], ["efficientnet._gen_efficientnet_edge"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_edge"], ["", "@", "register_model", "\n", "def", "efficientnet_es_pruned", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-Edge Small Pruned. For more info: https://github.com/DeGirum/pruned-models/releases/tag/efficientnet_v1.0\"\"\"", "\n", "model", "=", "_gen_efficientnet_edge", "(", "\n", "'efficientnet_es_pruned'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_em": [[1545, 1551], ["efficientnet._gen_efficientnet_edge"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_edge"], ["", "@", "register_model", "\n", "def", "efficientnet_em", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-Edge-Medium. \"\"\"", "\n", "model", "=", "_gen_efficientnet_edge", "(", "\n", "'efficientnet_em'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.1", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_el": [[1553, 1559], ["efficientnet._gen_efficientnet_edge"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_edge"], ["", "@", "register_model", "\n", "def", "efficientnet_el", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-Edge-Large. \"\"\"", "\n", "model", "=", "_gen_efficientnet_edge", "(", "\n", "'efficientnet_el'", ",", "channel_multiplier", "=", "1.2", ",", "depth_multiplier", "=", "1.4", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_el_pruned": [[1560, 1566], ["efficientnet._gen_efficientnet_edge"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_edge"], ["", "@", "register_model", "\n", "def", "efficientnet_el_pruned", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-Edge-Large pruned. For more info: https://github.com/DeGirum/pruned-models/releases/tag/efficientnet_v1.0\"\"\"", "\n", "model", "=", "_gen_efficientnet_edge", "(", "\n", "'efficientnet_el_pruned'", ",", "channel_multiplier", "=", "1.2", ",", "depth_multiplier", "=", "1.4", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_cc_b0_4e": [[1567, 1574], ["efficientnet._gen_efficientnet_condconv"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_condconv"], ["", "@", "register_model", "\n", "def", "efficientnet_cc_b0_4e", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-CondConv-B0 w/ 8 Experts \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2", "\n", "model", "=", "_gen_efficientnet_condconv", "(", "\n", "'efficientnet_cc_b0_4e'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_cc_b0_8e": [[1576, 1584], ["efficientnet._gen_efficientnet_condconv"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_condconv"], ["", "@", "register_model", "\n", "def", "efficientnet_cc_b0_8e", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-CondConv-B0 w/ 8 Experts \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2", "\n", "model", "=", "_gen_efficientnet_condconv", "(", "\n", "'efficientnet_cc_b0_8e'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "experts_multiplier", "=", "2", ",", "\n", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_cc_b1_8e": [[1586, 1594], ["efficientnet._gen_efficientnet_condconv"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_condconv"], ["", "@", "register_model", "\n", "def", "efficientnet_cc_b1_8e", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-CondConv-B1 w/ 8 Experts \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2", "\n", "model", "=", "_gen_efficientnet_condconv", "(", "\n", "'efficientnet_cc_b1_8e'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.1", ",", "experts_multiplier", "=", "2", ",", "\n", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_lite0": [[1596, 1603], ["efficientnet._gen_efficientnet_lite"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_lite"], ["", "@", "register_model", "\n", "def", "efficientnet_lite0", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-Lite0 \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2", "\n", "model", "=", "_gen_efficientnet_lite", "(", "\n", "'efficientnet_lite0'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_lite1": [[1605, 1612], ["efficientnet._gen_efficientnet_lite"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_lite"], ["", "@", "register_model", "\n", "def", "efficientnet_lite1", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-Lite1 \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2", "\n", "model", "=", "_gen_efficientnet_lite", "(", "\n", "'efficientnet_lite1'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.1", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_lite2": [[1614, 1621], ["efficientnet._gen_efficientnet_lite"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_lite"], ["", "@", "register_model", "\n", "def", "efficientnet_lite2", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-Lite2 \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2", "\n", "model", "=", "_gen_efficientnet_lite", "(", "\n", "'efficientnet_lite2'", ",", "channel_multiplier", "=", "1.1", ",", "depth_multiplier", "=", "1.2", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_lite3": [[1623, 1630], ["efficientnet._gen_efficientnet_lite"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_lite"], ["", "@", "register_model", "\n", "def", "efficientnet_lite3", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-Lite3 \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2", "\n", "model", "=", "_gen_efficientnet_lite", "(", "\n", "'efficientnet_lite3'", ",", "channel_multiplier", "=", "1.2", ",", "depth_multiplier", "=", "1.4", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_lite4": [[1632, 1639], ["efficientnet._gen_efficientnet_lite"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_lite"], ["", "@", "register_model", "\n", "def", "efficientnet_lite4", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-Lite4 \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.4, drop_path_rate should be 0.2", "\n", "model", "=", "_gen_efficientnet_lite", "(", "\n", "'efficientnet_lite4'", ",", "channel_multiplier", "=", "1.4", ",", "depth_multiplier", "=", "1.8", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b1_pruned": [[1641, 1650], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "efficientnet_b1_pruned", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B1 Pruned. The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf  \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "variant", "=", "'efficientnet_b1_pruned'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "variant", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.1", ",", "pruned", "=", "True", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b2_pruned": [[1652, 1661], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "efficientnet_b2_pruned", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B2 Pruned. The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'efficientnet_b2_pruned'", ",", "channel_multiplier", "=", "1.1", ",", "depth_multiplier", "=", "1.2", ",", "pruned", "=", "True", ",", "\n", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnet_b3_pruned": [[1663, 1672], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "efficientnet_b3_pruned", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B3 Pruned. The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'efficientnet_b3_pruned'", ",", "channel_multiplier", "=", "1.2", ",", "depth_multiplier", "=", "1.4", ",", "pruned", "=", "True", ",", "\n", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnetv2_rw_t": [[1674, 1680], ["efficientnet._gen_efficientnetv2_s"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_s"], ["", "@", "register_model", "\n", "def", "efficientnetv2_rw_t", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2 Tiny (Custom variant, tiny not in paper). \"\"\"", "\n", "model", "=", "_gen_efficientnetv2_s", "(", "\n", "'efficientnetv2_rw_t'", ",", "channel_multiplier", "=", "0.8", ",", "depth_multiplier", "=", "0.9", ",", "rw", "=", "False", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.gc_efficientnetv2_rw_t": [[1682, 1689], ["efficientnet._gen_efficientnetv2_s"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_s"], ["", "@", "register_model", "\n", "def", "gc_efficientnetv2_rw_t", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2 Tiny w/ Global Context Attn (Custom variant, tiny not in paper). \"\"\"", "\n", "model", "=", "_gen_efficientnetv2_s", "(", "\n", "'gc_efficientnetv2_rw_t'", ",", "channel_multiplier", "=", "0.8", ",", "depth_multiplier", "=", "0.9", ",", "\n", "rw", "=", "False", ",", "se_layer", "=", "'gc'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnetv2_rw_s": [[1691, 1699], ["efficientnet._gen_efficientnetv2_s"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_s"], ["", "@", "register_model", "\n", "def", "efficientnetv2_rw_s", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2 Small (RW variant).\n    NOTE: This is my initial (pre official code release) w/ some differences.\n    See efficientnetv2_s and tf_efficientnetv2_s for versions that match the official w/ PyTorch vs TF padding\n    \"\"\"", "\n", "model", "=", "_gen_efficientnetv2_s", "(", "'efficientnetv2_rw_s'", ",", "rw", "=", "True", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnetv2_rw_m": [[1701, 1709], ["efficientnet._gen_efficientnetv2_s"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_s"], ["", "@", "register_model", "\n", "def", "efficientnetv2_rw_m", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2 Medium (RW variant).\n    \"\"\"", "\n", "model", "=", "_gen_efficientnetv2_s", "(", "\n", "'efficientnetv2_rw_m'", ",", "channel_multiplier", "=", "1.2", ",", "depth_multiplier", "=", "(", "1.2", ",", ")", "*", "4", "+", "(", "1.6", ",", ")", "*", "2", ",", "rw", "=", "True", ",", "\n", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnetv2_s": [[1711, 1716], ["efficientnet._gen_efficientnetv2_s"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_s"], ["", "@", "register_model", "\n", "def", "efficientnetv2_s", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2 Small. \"\"\"", "\n", "model", "=", "_gen_efficientnetv2_s", "(", "'efficientnetv2_s'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnetv2_m": [[1718, 1723], ["efficientnet._gen_efficientnetv2_m"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_m"], ["", "@", "register_model", "\n", "def", "efficientnetv2_m", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2 Medium. \"\"\"", "\n", "model", "=", "_gen_efficientnetv2_m", "(", "'efficientnetv2_m'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnetv2_l": [[1725, 1730], ["efficientnet._gen_efficientnetv2_l"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_l"], ["", "@", "register_model", "\n", "def", "efficientnetv2_l", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2 Large. \"\"\"", "\n", "model", "=", "_gen_efficientnetv2_l", "(", "'efficientnetv2_l'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.efficientnetv2_xl": [[1732, 1737], ["efficientnet._gen_efficientnetv2_xl"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_xl"], ["", "@", "register_model", "\n", "def", "efficientnetv2_xl", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2 Xtra-Large. \"\"\"", "\n", "model", "=", "_gen_efficientnetv2_xl", "(", "'efficientnetv2_xl'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b0": [[1739, 1747], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b0", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B0. Tensorflow compatible variant  \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b0'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b1": [[1749, 1757], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b1", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B1. Tensorflow compatible variant  \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b1'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.1", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b2": [[1759, 1767], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b2", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B2. Tensorflow compatible variant  \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b2'", ",", "channel_multiplier", "=", "1.1", ",", "depth_multiplier", "=", "1.2", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b3": [[1769, 1777], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b3", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B3. Tensorflow compatible variant \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b3'", ",", "channel_multiplier", "=", "1.2", ",", "depth_multiplier", "=", "1.4", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b4": [[1779, 1787], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b4", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B4. Tensorflow compatible variant \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b4'", ",", "channel_multiplier", "=", "1.4", ",", "depth_multiplier", "=", "1.8", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b5": [[1789, 1797], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b5", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B5. Tensorflow compatible variant \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b5'", ",", "channel_multiplier", "=", "1.6", ",", "depth_multiplier", "=", "2.2", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b6": [[1799, 1808], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b6", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B6. Tensorflow compatible variant \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.5", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b6'", ",", "channel_multiplier", "=", "1.8", ",", "depth_multiplier", "=", "2.6", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b7": [[1810, 1819], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b7", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B7. Tensorflow compatible variant \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.5", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b7'", ",", "channel_multiplier", "=", "2.0", ",", "depth_multiplier", "=", "3.1", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b8": [[1821, 1830], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b8", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B8. Tensorflow compatible variant \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.5", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b8'", ",", "channel_multiplier", "=", "2.2", ",", "depth_multiplier", "=", "3.6", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b0_ap": [[1832, 1840], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b0_ap", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B0 AdvProp. Tensorflow compatible variant  \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b0_ap'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b1_ap": [[1842, 1850], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b1_ap", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B1 AdvProp. Tensorflow compatible variant  \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b1_ap'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.1", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b2_ap": [[1852, 1860], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b2_ap", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B2 AdvProp. Tensorflow compatible variant  \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b2_ap'", ",", "channel_multiplier", "=", "1.1", ",", "depth_multiplier", "=", "1.2", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b3_ap": [[1862, 1870], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b3_ap", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B3 AdvProp. Tensorflow compatible variant \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b3_ap'", ",", "channel_multiplier", "=", "1.2", ",", "depth_multiplier", "=", "1.4", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b4_ap": [[1872, 1880], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b4_ap", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B4 AdvProp. Tensorflow compatible variant \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b4_ap'", ",", "channel_multiplier", "=", "1.4", ",", "depth_multiplier", "=", "1.8", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b5_ap": [[1882, 1890], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b5_ap", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B5 AdvProp. Tensorflow compatible variant \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b5_ap'", ",", "channel_multiplier", "=", "1.6", ",", "depth_multiplier", "=", "2.2", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b6_ap": [[1892, 1901], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b6_ap", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B6 AdvProp. Tensorflow compatible variant \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.5", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b6_ap'", ",", "channel_multiplier", "=", "1.8", ",", "depth_multiplier", "=", "2.6", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b7_ap": [[1903, 1912], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b7_ap", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B7 AdvProp. Tensorflow compatible variant \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.5", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b7_ap'", ",", "channel_multiplier", "=", "2.0", ",", "depth_multiplier", "=", "3.1", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b8_ap": [[1914, 1923], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b8_ap", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B8 AdvProp. Tensorflow compatible variant \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.5", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b8_ap'", ",", "channel_multiplier", "=", "2.2", ",", "depth_multiplier", "=", "3.6", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b0_ns": [[1925, 1933], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b0_ns", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B0 NoisyStudent. Tensorflow compatible variant  \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b0_ns'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b1_ns": [[1935, 1943], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b1_ns", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B1 NoisyStudent. Tensorflow compatible variant  \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b1_ns'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.1", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b2_ns": [[1945, 1953], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b2_ns", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B2 NoisyStudent. Tensorflow compatible variant  \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b2_ns'", ",", "channel_multiplier", "=", "1.1", ",", "depth_multiplier", "=", "1.2", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b3_ns": [[1955, 1963], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b3_ns", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B3 NoisyStudent. Tensorflow compatible variant \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b3_ns'", ",", "channel_multiplier", "=", "1.2", ",", "depth_multiplier", "=", "1.4", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b4_ns": [[1965, 1973], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b4_ns", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B4 NoisyStudent. Tensorflow compatible variant \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b4_ns'", ",", "channel_multiplier", "=", "1.4", ",", "depth_multiplier", "=", "1.8", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b5_ns": [[1975, 1983], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b5_ns", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B5 NoisyStudent. Tensorflow compatible variant \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b5_ns'", ",", "channel_multiplier", "=", "1.6", ",", "depth_multiplier", "=", "2.2", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b6_ns": [[1985, 1994], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b6_ns", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B6 NoisyStudent. Tensorflow compatible variant \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.5", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b6_ns'", ",", "channel_multiplier", "=", "1.8", ",", "depth_multiplier", "=", "2.6", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_b7_ns": [[1996, 2005], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_b7_ns", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-B7 NoisyStudent. Tensorflow compatible variant \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.5", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_b7_ns'", ",", "channel_multiplier", "=", "2.0", ",", "depth_multiplier", "=", "3.1", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_l2_ns_475": [[2007, 2016], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_l2_ns_475", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-L2 NoisyStudent @ 475x475. Tensorflow compatible variant \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.5", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_l2_ns_475'", ",", "channel_multiplier", "=", "4.3", ",", "depth_multiplier", "=", "5.3", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_l2_ns": [[2018, 2027], ["efficientnet._gen_efficientnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_l2_ns", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-L2 NoisyStudent. Tensorflow compatible variant \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.5", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet", "(", "\n", "'tf_efficientnet_l2_ns'", ",", "channel_multiplier", "=", "4.3", ",", "depth_multiplier", "=", "5.3", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_es": [[2029, 2037], ["efficientnet._gen_efficientnet_edge"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_edge"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_es", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-Edge Small. Tensorflow compatible variant  \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet_edge", "(", "\n", "'tf_efficientnet_es'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_em": [[2039, 2047], ["efficientnet._gen_efficientnet_edge"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_edge"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_em", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-Edge-Medium. Tensorflow compatible variant  \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet_edge", "(", "\n", "'tf_efficientnet_em'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.1", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_el": [[2049, 2057], ["efficientnet._gen_efficientnet_edge"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_edge"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_el", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-Edge-Large. Tensorflow compatible variant  \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet_edge", "(", "\n", "'tf_efficientnet_el'", ",", "channel_multiplier", "=", "1.2", ",", "depth_multiplier", "=", "1.4", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_cc_b0_4e": [[2059, 2068], ["efficientnet._gen_efficientnet_condconv"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_condconv"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_cc_b0_4e", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-CondConv-B0 w/ 4 Experts. Tensorflow compatible variant \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet_condconv", "(", "\n", "'tf_efficientnet_cc_b0_4e'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_cc_b0_8e": [[2070, 2080], ["efficientnet._gen_efficientnet_condconv"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_condconv"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_cc_b0_8e", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-CondConv-B0 w/ 8 Experts. Tensorflow compatible variant \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet_condconv", "(", "\n", "'tf_efficientnet_cc_b0_8e'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "experts_multiplier", "=", "2", ",", "\n", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_cc_b1_8e": [[2082, 2092], ["efficientnet._gen_efficientnet_condconv"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_condconv"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_cc_b1_8e", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-CondConv-B1 w/ 8 Experts. Tensorflow compatible variant \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet_condconv", "(", "\n", "'tf_efficientnet_cc_b1_8e'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.1", ",", "experts_multiplier", "=", "2", ",", "\n", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_lite0": [[2094, 2103], ["efficientnet._gen_efficientnet_lite"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_lite"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_lite0", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-Lite0 \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet_lite", "(", "\n", "'tf_efficientnet_lite0'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_lite1": [[2105, 2114], ["efficientnet._gen_efficientnet_lite"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_lite"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_lite1", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-Lite1 \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet_lite", "(", "\n", "'tf_efficientnet_lite1'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.1", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_lite2": [[2116, 2125], ["efficientnet._gen_efficientnet_lite"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_lite"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_lite2", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-Lite2 \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet_lite", "(", "\n", "'tf_efficientnet_lite2'", ",", "channel_multiplier", "=", "1.1", ",", "depth_multiplier", "=", "1.2", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_lite3": [[2127, 2136], ["efficientnet._gen_efficientnet_lite"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_lite"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_lite3", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-Lite3 \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet_lite", "(", "\n", "'tf_efficientnet_lite3'", ",", "channel_multiplier", "=", "1.2", ",", "depth_multiplier", "=", "1.4", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnet_lite4": [[2138, 2147], ["efficientnet._gen_efficientnet_lite"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnet_lite"], ["", "@", "register_model", "\n", "def", "tf_efficientnet_lite4", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-Lite4 \"\"\"", "\n", "# NOTE for train, drop_rate should be 0.4, drop_path_rate should be 0.2", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnet_lite", "(", "\n", "'tf_efficientnet_lite4'", ",", "channel_multiplier", "=", "1.4", ",", "depth_multiplier", "=", "1.8", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnetv2_s": [[2150, 2157], ["efficientnet._gen_efficientnetv2_s"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_s"], ["", "@", "register_model", "\n", "def", "tf_efficientnetv2_s", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2 Small. Tensorflow compatible variant  \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnetv2_s", "(", "'tf_efficientnetv2_s'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnetv2_m": [[2159, 2166], ["efficientnet._gen_efficientnetv2_m"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_m"], ["", "@", "register_model", "\n", "def", "tf_efficientnetv2_m", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2 Medium. Tensorflow compatible variant  \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnetv2_m", "(", "'tf_efficientnetv2_m'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnetv2_l": [[2168, 2175], ["efficientnet._gen_efficientnetv2_l"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_l"], ["", "@", "register_model", "\n", "def", "tf_efficientnetv2_l", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2 Large. Tensorflow compatible variant  \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnetv2_l", "(", "'tf_efficientnetv2_l'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnetv2_s_in21ft1k": [[2177, 2185], ["efficientnet._gen_efficientnetv2_s"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_s"], ["", "@", "register_model", "\n", "def", "tf_efficientnetv2_s_in21ft1k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2 Small. Pretrained on ImageNet-21k, fine-tuned on 1k. Tensorflow compatible variant\n    \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnetv2_s", "(", "'tf_efficientnetv2_s_in21ft1k'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnetv2_m_in21ft1k": [[2187, 2195], ["efficientnet._gen_efficientnetv2_m"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_m"], ["", "@", "register_model", "\n", "def", "tf_efficientnetv2_m_in21ft1k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2 Medium. Pretrained on ImageNet-21k, fine-tuned on 1k. Tensorflow compatible variant\n    \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnetv2_m", "(", "'tf_efficientnetv2_m_in21ft1k'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnetv2_l_in21ft1k": [[2197, 2205], ["efficientnet._gen_efficientnetv2_l"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_l"], ["", "@", "register_model", "\n", "def", "tf_efficientnetv2_l_in21ft1k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2 Large. Pretrained on ImageNet-21k, fine-tuned on 1k. Tensorflow compatible variant\n    \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnetv2_l", "(", "'tf_efficientnetv2_l_in21ft1k'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnetv2_xl_in21ft1k": [[2207, 2215], ["efficientnet._gen_efficientnetv2_xl"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_xl"], ["", "@", "register_model", "\n", "def", "tf_efficientnetv2_xl_in21ft1k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2 Xtra-Large. Pretrained on ImageNet-21k, fine-tuned on 1k. Tensorflow compatible variant\n    \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnetv2_xl", "(", "'tf_efficientnetv2_xl_in21ft1k'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnetv2_s_in21k": [[2217, 2225], ["efficientnet._gen_efficientnetv2_s"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_s"], ["", "@", "register_model", "\n", "def", "tf_efficientnetv2_s_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2 Small w/ ImageNet-21k pretrained weights. Tensorflow compatible variant\n    \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnetv2_s", "(", "'tf_efficientnetv2_s_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnetv2_m_in21k": [[2227, 2235], ["efficientnet._gen_efficientnetv2_m"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_m"], ["", "@", "register_model", "\n", "def", "tf_efficientnetv2_m_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2 Medium w/ ImageNet-21k pretrained weights. Tensorflow compatible variant\n    \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnetv2_m", "(", "'tf_efficientnetv2_m_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnetv2_l_in21k": [[2237, 2245], ["efficientnet._gen_efficientnetv2_l"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_l"], ["", "@", "register_model", "\n", "def", "tf_efficientnetv2_l_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2 Large w/ ImageNet-21k pretrained weights. Tensorflow compatible variant\n    \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnetv2_l", "(", "'tf_efficientnetv2_l_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnetv2_xl_in21k": [[2247, 2255], ["efficientnet._gen_efficientnetv2_xl"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_xl"], ["", "@", "register_model", "\n", "def", "tf_efficientnetv2_xl_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2 Xtra-Large w/ ImageNet-21k pretrained weights. Tensorflow compatible variant\n    \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnetv2_xl", "(", "'tf_efficientnetv2_xl_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnetv2_b0": [[2257, 2264], ["efficientnet._gen_efficientnetv2_base"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_base"], ["", "@", "register_model", "\n", "def", "tf_efficientnetv2_b0", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2-B0. Tensorflow compatible variant  \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnetv2_base", "(", "'tf_efficientnetv2_b0'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnetv2_b1": [[2266, 2274], ["efficientnet._gen_efficientnetv2_base"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_base"], ["", "@", "register_model", "\n", "def", "tf_efficientnetv2_b1", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2-B1. Tensorflow compatible variant  \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnetv2_base", "(", "\n", "'tf_efficientnetv2_b1'", ",", "channel_multiplier", "=", "1.0", ",", "depth_multiplier", "=", "1.1", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnetv2_b2": [[2276, 2284], ["efficientnet._gen_efficientnetv2_base"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_base"], ["", "@", "register_model", "\n", "def", "tf_efficientnetv2_b2", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2-B2. Tensorflow compatible variant  \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnetv2_base", "(", "\n", "'tf_efficientnetv2_b2'", ",", "channel_multiplier", "=", "1.1", ",", "depth_multiplier", "=", "1.2", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_efficientnetv2_b3": [[2286, 2294], ["efficientnet._gen_efficientnetv2_base"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_efficientnetv2_base"], ["", "@", "register_model", "\n", "def", "tf_efficientnetv2_b3", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EfficientNet-V2-B3. Tensorflow compatible variant \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_efficientnetv2_base", "(", "\n", "'tf_efficientnetv2_b3'", ",", "channel_multiplier", "=", "1.2", ",", "depth_multiplier", "=", "1.4", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.mixnet_s": [[2296, 2303], ["efficientnet._gen_mixnet_s"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mixnet_s"], ["", "@", "register_model", "\n", "def", "mixnet_s", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Creates a MixNet Small model.\n    \"\"\"", "\n", "model", "=", "_gen_mixnet_s", "(", "\n", "'mixnet_s'", ",", "channel_multiplier", "=", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.mixnet_m": [[2305, 2312], ["efficientnet._gen_mixnet_m"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mixnet_m"], ["", "@", "register_model", "\n", "def", "mixnet_m", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Creates a MixNet Medium model.\n    \"\"\"", "\n", "model", "=", "_gen_mixnet_m", "(", "\n", "'mixnet_m'", ",", "channel_multiplier", "=", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.mixnet_l": [[2314, 2321], ["efficientnet._gen_mixnet_m"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mixnet_m"], ["", "@", "register_model", "\n", "def", "mixnet_l", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Creates a MixNet Large model.\n    \"\"\"", "\n", "model", "=", "_gen_mixnet_m", "(", "\n", "'mixnet_l'", ",", "channel_multiplier", "=", "1.3", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.mixnet_xl": [[2323, 2331], ["efficientnet._gen_mixnet_m"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mixnet_m"], ["", "@", "register_model", "\n", "def", "mixnet_xl", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Creates a MixNet Extra-Large model.\n    Not a paper spec, experimental def by RW w/ depth scaling.\n    \"\"\"", "\n", "model", "=", "_gen_mixnet_m", "(", "\n", "'mixnet_xl'", ",", "channel_multiplier", "=", "1.6", ",", "depth_multiplier", "=", "1.2", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.mixnet_xxl": [[2333, 2341], ["efficientnet._gen_mixnet_m"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mixnet_m"], ["", "@", "register_model", "\n", "def", "mixnet_xxl", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Creates a MixNet Double Extra Large model.\n    Not a paper spec, experimental def by RW w/ depth scaling.\n    \"\"\"", "\n", "model", "=", "_gen_mixnet_m", "(", "\n", "'mixnet_xxl'", ",", "channel_multiplier", "=", "2.4", ",", "depth_multiplier", "=", "1.3", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_mixnet_s": [[2343, 2352], ["efficientnet._gen_mixnet_s"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mixnet_s"], ["", "@", "register_model", "\n", "def", "tf_mixnet_s", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Creates a MixNet Small model. Tensorflow compatible variant\n    \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_mixnet_s", "(", "\n", "'tf_mixnet_s'", ",", "channel_multiplier", "=", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_mixnet_m": [[2354, 2363], ["efficientnet._gen_mixnet_m"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mixnet_m"], ["", "@", "register_model", "\n", "def", "tf_mixnet_m", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Creates a MixNet Medium model. Tensorflow compatible variant\n    \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_mixnet_m", "(", "\n", "'tf_mixnet_m'", ",", "channel_multiplier", "=", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tf_mixnet_l": [[2365, 2374], ["efficientnet._gen_mixnet_m"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_mixnet_m"], ["", "@", "register_model", "\n", "def", "tf_mixnet_l", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Creates a MixNet Large model. Tensorflow compatible variant\n    \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_mixnet_m", "(", "\n", "'tf_mixnet_l'", ",", "channel_multiplier", "=", "1.3", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tinynet_a": [[2376, 2380], ["efficientnet._gen_tinynet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_tinynet"], ["", "@", "register_model", "\n", "def", "tinynet_a", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model", "=", "_gen_tinynet", "(", "'tinynet_a'", ",", "1.0", ",", "1.2", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tinynet_b": [[2382, 2386], ["efficientnet._gen_tinynet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_tinynet"], ["", "@", "register_model", "\n", "def", "tinynet_b", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model", "=", "_gen_tinynet", "(", "'tinynet_b'", ",", "0.75", ",", "1.1", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tinynet_c": [[2388, 2392], ["efficientnet._gen_tinynet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_tinynet"], ["", "@", "register_model", "\n", "def", "tinynet_c", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model", "=", "_gen_tinynet", "(", "'tinynet_c'", ",", "0.54", ",", "0.85", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tinynet_d": [[2394, 2398], ["efficientnet._gen_tinynet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_tinynet"], ["", "@", "register_model", "\n", "def", "tinynet_d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model", "=", "_gen_tinynet", "(", "'tinynet_d'", ",", "0.54", ",", "0.695", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet.tinynet_e": [[2400, 2404], ["efficientnet._gen_tinynet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet._gen_tinynet"], ["", "@", "register_model", "\n", "def", "tinynet_e", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model", "=", "_gen_tinynet", "(", "'tinynet_e'", ",", "0.51", ",", "0.6", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.LayerNorm2d.__init__": [[106, 108], ["torch.LayerNorm.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "normalized_shape", ",", "eps", "=", "1e-6", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "normalized_shape", ",", "eps", "=", "eps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.LayerNorm2d.forward": [[109, 118], ["convnext._is_contiguous", "torch.layer_norm().permute", "torch.layer_norm().permute", "torch.layer_norm().permute", "torch.var_mean", "torch.var_mean", "torch.var_mean", "torch.var_mean", "torch.var_mean", "torch.var_mean", "torch.var_mean", "torch.var_mean", "torch.var_mean", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "torch.layer_norm", "torch.layer_norm", "torch.layer_norm", "x.permute"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._is_contiguous"], ["", "def", "forward", "(", "self", ",", "x", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "if", "_is_contiguous", "(", "x", ")", ":", "\n", "            ", "return", "F", ".", "layer_norm", "(", "\n", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ",", "self", ".", "normalized_shape", ",", "self", ".", "weight", ",", "self", ".", "bias", ",", "self", ".", "eps", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "", "else", ":", "\n", "            ", "s", ",", "u", "=", "torch", ".", "var_mean", "(", "x", ",", "dim", "=", "1", ",", "unbiased", "=", "False", ",", "keepdim", "=", "True", ")", "\n", "x", "=", "(", "x", "-", "u", ")", "*", "torch", ".", "rsqrt", "(", "s", "+", "self", ".", "eps", ")", "\n", "x", "=", "x", "*", "self", ".", "weight", "[", ":", ",", "None", ",", "None", "]", "+", "self", ".", "bias", "[", ":", ",", "None", ",", "None", "]", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.ConvNeXtBlock.__init__": [[136, 147], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "mlp_layer", "int", "torch.Parameter", "torch.Parameter", "torch.Parameter", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "functools.partial", "functools.partial", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "dim", ",", "drop_path", "=", "0.", ",", "ls_init_value", "=", "1e-6", ",", "conv_mlp", "=", "False", ",", "mlp_ratio", "=", "4", ",", "norm_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "not", "norm_layer", ":", "\n", "            ", "norm_layer", "=", "partial", "(", "LayerNorm2d", ",", "eps", "=", "1e-6", ")", "if", "conv_mlp", "else", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", "\n", "", "mlp_layer", "=", "ConvMlp", "if", "conv_mlp", "else", "Mlp", "\n", "self", ".", "use_conv_mlp", "=", "conv_mlp", "\n", "self", ".", "conv_dw", "=", "nn", ".", "Conv2d", "(", "dim", ",", "dim", ",", "kernel_size", "=", "7", ",", "padding", "=", "3", ",", "groups", "=", "dim", ")", "# depthwise conv", "\n", "self", ".", "norm", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "mlp", "=", "mlp_layer", "(", "dim", ",", "int", "(", "mlp_ratio", "*", "dim", ")", ",", "act_layer", "=", "nn", ".", "GELU", ")", "\n", "self", ".", "gamma", "=", "nn", ".", "Parameter", "(", "ls_init_value", "*", "torch", ".", "ones", "(", "dim", ")", ")", "if", "ls_init_value", ">", "0", "else", "None", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.ConvNeXtBlock.forward": [[148, 163], ["convnext.ConvNeXtBlock.conv_dw", "convnext.ConvNeXtBlock.norm", "convnext.ConvNeXtBlock.mlp", "x.mul.mul.permute", "convnext.ConvNeXtBlock.norm", "convnext.ConvNeXtBlock.mlp", "x.mul.mul.permute", "x.mul.mul.mul", "convnext.ConvNeXtBlock.drop_path", "convnext.ConvNeXtBlock.gamma.reshape"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "x", "=", "self", ".", "conv_dw", "(", "x", ")", "\n", "if", "self", ".", "use_conv_mlp", ":", "\n", "            ", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "x", "=", "self", ".", "mlp", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "x", "=", "self", ".", "mlp", "(", "x", ")", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "", "if", "self", ".", "gamma", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", ".", "mul", "(", "self", ".", "gamma", ".", "reshape", "(", "1", ",", "-", "1", ",", "1", ",", "1", ")", ")", "\n", "", "x", "=", "self", ".", "drop_path", "(", "x", ")", "+", "shortcut", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.ConvNeXtStage.__init__": [[167, 186], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Identity", "torch.Identity", "torch.Identity", "norm_layer", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "convnext.ConvNeXtBlock", "range"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "stride", "=", "2", ",", "depth", "=", "2", ",", "dp_rates", "=", "None", ",", "ls_init_value", "=", "1.0", ",", "conv_mlp", "=", "False", ",", "\n", "norm_layer", "=", "None", ",", "cl_norm_layer", "=", "None", ",", "cross_stage", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "if", "in_chs", "!=", "out_chs", "or", "stride", ">", "1", ":", "\n", "            ", "self", ".", "downsample", "=", "nn", ".", "Sequential", "(", "\n", "norm_layer", "(", "in_chs", ")", ",", "\n", "nn", ".", "Conv2d", "(", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "stride", ",", "stride", "=", "stride", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "downsample", "=", "nn", ".", "Identity", "(", ")", "\n", "\n", "", "dp_rates", "=", "dp_rates", "or", "[", "0.", "]", "*", "depth", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "*", "[", "ConvNeXtBlock", "(", "\n", "dim", "=", "out_chs", ",", "drop_path", "=", "dp_rates", "[", "j", "]", ",", "ls_init_value", "=", "ls_init_value", ",", "conv_mlp", "=", "conv_mlp", ",", "\n", "norm_layer", "=", "norm_layer", "if", "conv_mlp", "else", "cl_norm_layer", ")", "\n", "for", "j", "in", "range", "(", "depth", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.ConvNeXtStage.forward": [[188, 195], ["convnext.ConvNeXtStage.downsample", "helpers.checkpoint_seq", "convnext.ConvNeXtStage.blocks", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "downsample", "(", "x", ")", "\n", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "blocks", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "blocks", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.ConvNeXt.__init__": [[212, 280], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "helpers.named_apply", "functools.partial", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "x.tolist", "stages.append", "functools.partial.", "torch.Identity", "torch.Identity", "torch.Identity", "collections.OrderedDict", "functools.partial", "functools.partial", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "functools.partial.", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "functools.partial.", "torch.GELU", "torch.GELU", "torch.GELU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.linspace().split", "torch.linspace().split", "torch.linspace().split", "torch.linspace().split", "torch.linspace().split", "torch.linspace().split", "torch.linspace().split", "torch.linspace().split", "torch.linspace().split", "convnext.ConvNeXtStage", "dict", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "layers.SelectAdaptivePool2d", "torch.Dropout", "torch.Dropout", "torch.Dropout", "sum", "torch.Identity", "torch.Identity", "torch.Identity", "functools.partial.", "torch.Flatten", "torch.Flatten", "torch.Flatten", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_apply"], ["def", "__init__", "(", "\n", "self", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "global_pool", "=", "'avg'", ",", "output_stride", "=", "32", ",", "patch_size", "=", "4", ",", "\n", "depths", "=", "(", "3", ",", "3", ",", "9", ",", "3", ")", ",", "dims", "=", "(", "96", ",", "192", ",", "384", ",", "768", ")", ",", "ls_init_value", "=", "1e-6", ",", "conv_mlp", "=", "False", ",", "stem_type", "=", "'patch'", ",", "\n", "head_init_scale", "=", "1.", ",", "head_norm_first", "=", "False", ",", "norm_layer", "=", "None", ",", "drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "output_stride", "==", "32", "\n", "if", "norm_layer", "is", "None", ":", "\n", "            ", "norm_layer", "=", "partial", "(", "LayerNorm2d", ",", "eps", "=", "1e-6", ")", "\n", "cl_norm_layer", "=", "norm_layer", "if", "conv_mlp", "else", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", "\n", "", "else", ":", "\n", "            ", "assert", "conv_mlp", ",", "'If a norm_layer is specified, conv MLP must be used so all norm expect rank-4, channels-first input'", "\n", "cl_norm_layer", "=", "norm_layer", "\n", "\n", "", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "self", ".", "feature_info", "=", "[", "]", "\n", "\n", "# NOTE: this stem is a minimal form of ViT PatchEmbed, as used in SwinTransformer w/ patch_size = 4", "\n", "if", "stem_type", "==", "'patch'", ":", "\n", "            ", "self", ".", "stem", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "in_chans", ",", "dims", "[", "0", "]", ",", "kernel_size", "=", "patch_size", ",", "stride", "=", "patch_size", ")", ",", "\n", "norm_layer", "(", "dims", "[", "0", "]", ")", "\n", ")", "\n", "curr_stride", "=", "patch_size", "\n", "prev_chs", "=", "dims", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "stem", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "in_chans", ",", "32", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "\n", "norm_layer", "(", "32", ")", ",", "\n", "nn", ".", "GELU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ")", ",", "\n", ")", "\n", "curr_stride", "=", "2", "\n", "prev_chs", "=", "64", "\n", "\n", "", "self", ".", "stages", "=", "nn", ".", "Sequential", "(", ")", "\n", "dp_rates", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "sum", "(", "depths", ")", ")", ".", "split", "(", "depths", ")", "]", "\n", "stages", "=", "[", "]", "\n", "# 4 feature resolution stages, each consisting of multiple residual blocks", "\n", "for", "i", "in", "range", "(", "4", ")", ":", "\n", "            ", "stride", "=", "2", "if", "curr_stride", "==", "2", "or", "i", ">", "0", "else", "1", "\n", "# FIXME support dilation / output_stride", "\n", "curr_stride", "*=", "stride", "\n", "out_chs", "=", "dims", "[", "i", "]", "\n", "stages", ".", "append", "(", "ConvNeXtStage", "(", "\n", "prev_chs", ",", "out_chs", ",", "stride", "=", "stride", ",", "\n", "depth", "=", "depths", "[", "i", "]", ",", "dp_rates", "=", "dp_rates", "[", "i", "]", ",", "ls_init_value", "=", "ls_init_value", ",", "conv_mlp", "=", "conv_mlp", ",", "\n", "norm_layer", "=", "norm_layer", ",", "cl_norm_layer", "=", "cl_norm_layer", ")", "\n", ")", "\n", "prev_chs", "=", "out_chs", "\n", "# NOTE feature_info use currently assumes stage 0 == stride 1, rest are stride 2", "\n", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "prev_chs", ",", "reduction", "=", "curr_stride", ",", "module", "=", "f'stages.{i}'", ")", "]", "\n", "", "self", ".", "stages", "=", "nn", ".", "Sequential", "(", "*", "stages", ")", "\n", "\n", "self", ".", "num_features", "=", "prev_chs", "\n", "# if head_norm_first == true, norm -> global pool -> fc ordering, like most other nets", "\n", "# otherwise pool -> norm -> fc, the default ConvNeXt ordering (pretrained FB weights)", "\n", "self", ".", "norm_pre", "=", "norm_layer", "(", "self", ".", "num_features", ")", "if", "head_norm_first", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "head", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "'global_pool'", ",", "SelectAdaptivePool2d", "(", "pool_type", "=", "global_pool", ")", ")", ",", "\n", "(", "'norm'", ",", "nn", ".", "Identity", "(", ")", "if", "head_norm_first", "else", "norm_layer", "(", "self", ".", "num_features", ")", ")", ",", "\n", "(", "'flatten'", ",", "nn", ".", "Flatten", "(", "1", ")", "if", "global_pool", "else", "nn", ".", "Identity", "(", ")", ")", ",", "\n", "(", "'drop'", ",", "nn", ".", "Dropout", "(", "self", ".", "drop_rate", ")", ")", ",", "\n", "(", "'fc'", ",", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", ")", "]", ")", ")", "\n", "\n", "named_apply", "(", "partial", "(", "_init_weights", ",", "head_init_scale", "=", "head_init_scale", ")", ",", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.ConvNeXt.group_matcher": [[281, 289], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^stem'", ",", "\n", "blocks", "=", "r'^stages\\.(\\d+)'", "if", "coarse", "else", "[", "\n", "(", "r'^stages\\.(\\d+)\\.downsample'", ",", "(", "0", ",", ")", ")", ",", "# blocks", "\n", "(", "r'^stages\\.(\\d+)\\.blocks\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^norm_pre'", ",", "(", "99999", ",", ")", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.ConvNeXt.set_grad_checkpointing": [[292, 296], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "for", "s", "in", "self", ".", "stages", ":", "\n", "            ", "s", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.ConvNeXt.get_classifier": [[297, 300], ["None"], "methods", ["None"], ["", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", ".", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.ConvNeXt.reset_classifier": [[301, 306], ["layers.SelectAdaptivePool2d", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Flatten", "torch.Flatten", "torch.Flatten", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", "=", "0", ",", "global_pool", "=", "None", ")", ":", "\n", "        ", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "self", ".", "head", ".", "global_pool", "=", "SelectAdaptivePool2d", "(", "pool_type", "=", "global_pool", ")", "\n", "self", ".", "head", ".", "flatten", "=", "nn", ".", "Flatten", "(", "1", ")", "if", "global_pool", "else", "nn", ".", "Identity", "(", ")", "\n", "", "self", ".", "head", ".", "fc", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.ConvNeXt.forward_features": [[307, 312], ["convnext.ConvNeXt.stem", "convnext.ConvNeXt.stages", "convnext.ConvNeXt.norm_pre"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet.stages"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "stem", "(", "x", ")", "\n", "x", "=", "self", ".", "stages", "(", "x", ")", "\n", "x", "=", "self", ".", "norm_pre", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.ConvNeXt.forward_head": [[313, 320], ["convnext.ConvNeXt.head.global_pool", "convnext.ConvNeXt.head.norm", "convnext.ConvNeXt.head.flatten", "convnext.ConvNeXt.head.drop", "convnext.ConvNeXt.head.fc"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "# NOTE nn.Sequential in head broken down since can't call head[:-1](x) in torchscript :(", "\n", "        ", "x", "=", "self", ".", "head", ".", "global_pool", "(", "x", ")", "\n", "x", "=", "self", ".", "head", ".", "norm", "(", "x", ")", "\n", "x", "=", "self", ".", "head", ".", "flatten", "(", "x", ")", "\n", "x", "=", "self", ".", "head", ".", "drop", "(", "x", ")", "\n", "return", "x", "if", "pre_logits", "else", "self", ".", "head", ".", "fc", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.ConvNeXt.forward": [[321, 325], ["convnext.ConvNeXt.forward_features", "convnext.ConvNeXt.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._cfg": [[29, 37], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bicubic'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'stem.0'", ",", "'classifier'", ":", "'head.fc'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._is_contiguous": [[91, 99], ["torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "tensor.is_contiguous", "tensor.is_contiguous"], "function", ["None"], ["def", "_is_contiguous", "(", "tensor", ":", "torch", ".", "Tensor", ")", "->", "bool", ":", "\n", "# jit is oh so lovely :/", "\n", "# if torch.jit.is_tracing():", "\n", "#     return True", "\n", "    ", "if", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "        ", "return", "tensor", ".", "is_contiguous", "(", ")", "\n", "", "else", ":", "\n", "        ", "return", "tensor", ".", "is_contiguous", "(", "memory_format", "=", "torch", ".", "contiguous_format", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._init_weights": [[327, 337], ["isinstance", "layers.trunc_normal_", "torch.init.constant_", "isinstance", "layers.trunc_normal_", "torch.init.constant_", "module.weight.data.mul_", "module.bias.data.mul_"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["", "", "def", "_init_weights", "(", "module", ",", "name", "=", "None", ",", "head_init_scale", "=", "1.0", ")", ":", "\n", "    ", "if", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", ":", "\n", "        ", "trunc_normal_", "(", "module", ".", "weight", ",", "std", "=", ".02", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "module", ".", "bias", ",", "0", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "        ", "trunc_normal_", "(", "module", ".", "weight", ",", "std", "=", ".02", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "module", ".", "bias", ",", "0", ")", "\n", "if", "name", "and", "'head.'", "in", "name", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "mul_", "(", "head_init_scale", ")", "\n", "module", ".", "bias", ".", "data", ".", "mul_", "(", "head_init_scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.checkpoint_filter_fn": [[339, 361], ["state_dict.items", "k.replace.replace", "re.sub", "re.sub", "k.replace.replace", "k.replace.replace", "k.replace.replace", "k.replace.startswith", "k.replace.replace", "v.reshape.reshape", "model.state_dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict"], ["", "", "", "def", "checkpoint_filter_fn", "(", "state_dict", ",", "model", ")", ":", "\n", "    ", "\"\"\" Remap FB checkpoints -> timm \"\"\"", "\n", "if", "'head.norm.weight'", "in", "state_dict", "or", "'norm_pre.weight'", "in", "state_dict", ":", "\n", "        ", "return", "state_dict", "# non-FB checkpoint", "\n", "", "if", "'model'", "in", "state_dict", ":", "\n", "        ", "state_dict", "=", "state_dict", "[", "'model'", "]", "\n", "", "out_dict", "=", "{", "}", "\n", "import", "re", "\n", "for", "k", ",", "v", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "        ", "k", "=", "k", ".", "replace", "(", "'downsample_layers.0.'", ",", "'stem.'", ")", "\n", "k", "=", "re", ".", "sub", "(", "r'stages.([0-9]+).([0-9]+)'", ",", "r'stages.\\1.blocks.\\2'", ",", "k", ")", "\n", "k", "=", "re", ".", "sub", "(", "r'downsample_layers.([0-9]+).([0-9]+)'", ",", "r'stages.\\1.downsample.\\2'", ",", "k", ")", "\n", "k", "=", "k", ".", "replace", "(", "'dwconv'", ",", "'conv_dw'", ")", "\n", "k", "=", "k", ".", "replace", "(", "'pwconv'", ",", "'mlp.fc'", ")", "\n", "k", "=", "k", ".", "replace", "(", "'head.'", ",", "'head.fc.'", ")", "\n", "if", "k", ".", "startswith", "(", "'norm.'", ")", ":", "\n", "            ", "k", "=", "k", ".", "replace", "(", "'norm'", ",", "'head.norm'", ")", "\n", "", "if", "v", ".", "ndim", "==", "2", "and", "'head'", "not", "in", "k", ":", "\n", "            ", "model_shape", "=", "model", ".", "state_dict", "(", ")", "[", "k", "]", ".", "shape", "\n", "v", "=", "v", ".", "reshape", "(", "model_shape", ")", "\n", "", "out_dict", "[", "k", "]", "=", "v", "\n", "", "return", "out_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext": [[363, 370], ["helpers.build_model_with_cfg", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "def", "_create_convnext", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model", "=", "build_model_with_cfg", "(", "\n", "ConvNeXt", ",", "variant", ",", "pretrained", ",", "\n", "pretrained_filter_fn", "=", "checkpoint_filter_fn", ",", "\n", "feature_cfg", "=", "dict", "(", "out_indices", "=", "(", "0", ",", "1", ",", "2", ",", "3", ")", ",", "flatten_sequential", "=", "True", ")", ",", "\n", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_nano_hnf": [[372, 377], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_nano_hnf", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "(", "2", ",", "2", ",", "8", ",", "2", ")", ",", "dims", "=", "(", "80", ",", "160", ",", "320", ",", "640", ")", ",", "head_norm_first", "=", "True", ",", "conv_mlp", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_nano_hnf'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_tiny_hnf": [[379, 384], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_tiny_hnf", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "(", "3", ",", "3", ",", "9", ",", "3", ")", ",", "dims", "=", "(", "96", ",", "192", ",", "384", ",", "768", ")", ",", "head_norm_first", "=", "True", ",", "conv_mlp", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_tiny_hnf'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_tiny_hnfd": [[386, 392], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_tiny_hnfd", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "depths", "=", "(", "3", ",", "3", ",", "9", ",", "3", ")", ",", "dims", "=", "(", "96", ",", "192", ",", "384", ",", "768", ")", ",", "head_norm_first", "=", "True", ",", "conv_mlp", "=", "True", ",", "stem_type", "=", "'dual'", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_tiny_hnf'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_tiny": [[394, 399], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_tiny", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "(", "3", ",", "3", ",", "9", ",", "3", ")", ",", "dims", "=", "(", "96", ",", "192", ",", "384", ",", "768", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_tiny'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_small": [[401, 406], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_small", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "[", "3", ",", "3", ",", "27", ",", "3", "]", ",", "dims", "=", "[", "96", ",", "192", ",", "384", ",", "768", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_small'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_base": [[408, 413], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_base", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "[", "3", ",", "3", ",", "27", ",", "3", "]", ",", "dims", "=", "[", "128", ",", "256", ",", "512", ",", "1024", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_base'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_large": [[415, 420], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_large", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "[", "3", ",", "3", ",", "27", ",", "3", "]", ",", "dims", "=", "[", "192", ",", "384", ",", "768", ",", "1536", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_large'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_tiny_in22ft1k": [[422, 427], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_tiny_in22ft1k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "(", "3", ",", "3", ",", "9", ",", "3", ")", ",", "dims", "=", "(", "96", ",", "192", ",", "384", ",", "768", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_tiny_in22ft1k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_small_in22ft1k": [[429, 434], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_small_in22ft1k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "[", "3", ",", "3", ",", "27", ",", "3", "]", ",", "dims", "=", "[", "96", ",", "192", ",", "384", ",", "768", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_small_in22ft1k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_base_in22ft1k": [[436, 441], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_base_in22ft1k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "[", "3", ",", "3", ",", "27", ",", "3", "]", ",", "dims", "=", "[", "128", ",", "256", ",", "512", ",", "1024", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_base_in22ft1k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_large_in22ft1k": [[443, 448], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_large_in22ft1k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "[", "3", ",", "3", ",", "27", ",", "3", "]", ",", "dims", "=", "[", "192", ",", "384", ",", "768", ",", "1536", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_large_in22ft1k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_xlarge_in22ft1k": [[450, 455], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_xlarge_in22ft1k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "[", "3", ",", "3", ",", "27", ",", "3", "]", ",", "dims", "=", "[", "256", ",", "512", ",", "1024", ",", "2048", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_xlarge_in22ft1k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_tiny_384_in22ft1k": [[457, 462], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_tiny_384_in22ft1k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "(", "3", ",", "3", ",", "9", ",", "3", ")", ",", "dims", "=", "(", "96", ",", "192", ",", "384", ",", "768", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_tiny_384_in22ft1k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_small_384_in22ft1k": [[464, 469], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_small_384_in22ft1k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "[", "3", ",", "3", ",", "27", ",", "3", "]", ",", "dims", "=", "[", "96", ",", "192", ",", "384", ",", "768", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_small_384_in22ft1k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_base_384_in22ft1k": [[471, 476], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_base_384_in22ft1k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "[", "3", ",", "3", ",", "27", ",", "3", "]", ",", "dims", "=", "[", "128", ",", "256", ",", "512", ",", "1024", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_base_384_in22ft1k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_large_384_in22ft1k": [[478, 483], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_large_384_in22ft1k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "[", "3", ",", "3", ",", "27", ",", "3", "]", ",", "dims", "=", "[", "192", ",", "384", ",", "768", ",", "1536", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_large_384_in22ft1k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_xlarge_384_in22ft1k": [[485, 490], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_xlarge_384_in22ft1k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "[", "3", ",", "3", ",", "27", ",", "3", "]", ",", "dims", "=", "[", "256", ",", "512", ",", "1024", ",", "2048", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_xlarge_384_in22ft1k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_tiny_in22k": [[492, 497], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_tiny_in22k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "(", "3", ",", "3", ",", "9", ",", "3", ")", ",", "dims", "=", "(", "96", ",", "192", ",", "384", ",", "768", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_tiny_in22k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_small_in22k": [[499, 504], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_small_in22k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "[", "3", ",", "3", ",", "27", ",", "3", "]", ",", "dims", "=", "[", "96", ",", "192", ",", "384", ",", "768", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_small_in22k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_base_in22k": [[506, 511], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_base_in22k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "[", "3", ",", "3", ",", "27", ",", "3", "]", ",", "dims", "=", "[", "128", ",", "256", ",", "512", ",", "1024", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_base_in22k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_large_in22k": [[513, 518], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_large_in22k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "[", "3", ",", "3", ",", "27", ",", "3", "]", ",", "dims", "=", "[", "192", ",", "384", ",", "768", ",", "1536", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_large_in22k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext.convnext_xlarge_in22k": [[520, 525], ["dict", "convnext._create_convnext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convnext._create_convnext"], ["", "@", "register_model", "\n", "def", "convnext_xlarge_in22k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "depths", "=", "[", "3", ",", "3", ",", "27", ",", "3", "]", ",", "dims", "=", "[", "256", ",", "512", ",", "1024", ",", "2048", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_convnext", "(", "'convnext_xlarge_in22k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.RelPosMlp.__init__": [[113, 150], ["torch.Module.__init__", "layers.Mlp", "vision_transformer_relpos.RelPosMlp.register_buffer", "vision_transformer_relpos.RelPosMlp.register_buffer", "vision_transformer_relpos.gen_relative_position_index", "vision_transformer_relpos.gen_relative_log_coords"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.gen_relative_position_index", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.gen_relative_log_coords"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "window_size", ",", "\n", "num_heads", "=", "8", ",", "\n", "hidden_dim", "=", "128", ",", "\n", "class_token", "=", "False", ",", "\n", "mode", "=", "'cr'", ",", "\n", "pretrained_window_size", "=", "(", "0", ",", "0", ")", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "window_size", "=", "window_size", "\n", "self", ".", "window_area", "=", "self", ".", "window_size", "[", "0", "]", "*", "self", ".", "window_size", "[", "1", "]", "\n", "self", ".", "class_token", "=", "1", "if", "class_token", "else", "0", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "bias_shape", "=", "(", "self", ".", "window_area", ",", ")", "*", "2", "+", "(", "num_heads", ",", ")", "\n", "self", ".", "apply_sigmoid", "=", "mode", "==", "'swin'", "\n", "\n", "mlp_bias", "=", "(", "True", ",", "False", ")", "if", "mode", "==", "'swin'", "else", "True", "\n", "self", ".", "mlp", "=", "Mlp", "(", "\n", "2", ",", "# x, y", "\n", "hidden_features", "=", "hidden_dim", ",", "\n", "out_features", "=", "num_heads", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "\n", "bias", "=", "mlp_bias", ",", "\n", "drop", "=", "(", "0.125", ",", "0.", ")", "\n", ")", "\n", "\n", "self", ".", "register_buffer", "(", "\n", "\"relative_position_index\"", ",", "\n", "gen_relative_position_index", "(", "window_size", ")", ",", "\n", "persistent", "=", "False", ")", "\n", "\n", "# get relative_coords_table", "\n", "self", ".", "register_buffer", "(", "\n", "\"rel_coords_log\"", ",", "\n", "gen_relative_log_coords", "(", "window_size", ",", "pretrained_window_size", ",", "mode", "=", "mode", ")", ",", "\n", "persistent", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.RelPosMlp.get_bias": [[151, 163], ["vision_transformer_relpos.RelPosMlp.mlp", "torch.pad.permute", "torch.pad.unsqueeze().contiguous", "torch.pad.view", "torch.pad", "torch.pad", "torch.pad", "torch.pad.view", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.pad.unsqueeze", "vision_transformer_relpos.RelPosMlp.relative_position_index.view"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid"], ["", "def", "get_bias", "(", "self", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "relative_position_bias", "=", "self", ".", "mlp", "(", "self", ".", "rel_coords_log", ")", "\n", "if", "self", ".", "relative_position_index", "is", "not", "None", ":", "\n", "            ", "relative_position_bias", "=", "relative_position_bias", ".", "view", "(", "-", "1", ",", "self", ".", "num_heads", ")", "[", "\n", "self", ".", "relative_position_index", ".", "view", "(", "-", "1", ")", "]", "# Wh*Ww,Wh*Ww,nH", "\n", "relative_position_bias", "=", "relative_position_bias", ".", "view", "(", "self", ".", "bias_shape", ")", "\n", "", "relative_position_bias", "=", "relative_position_bias", ".", "permute", "(", "2", ",", "0", ",", "1", ")", "\n", "if", "self", ".", "apply_sigmoid", ":", "\n", "            ", "relative_position_bias", "=", "16", "*", "torch", ".", "sigmoid", "(", "relative_position_bias", ")", "\n", "", "if", "self", ".", "class_token", ":", "\n", "            ", "relative_position_bias", "=", "F", ".", "pad", "(", "relative_position_bias", ",", "[", "self", ".", "class_token", ",", "0", ",", "self", ".", "class_token", ",", "0", "]", ")", "\n", "", "return", "relative_position_bias", ".", "unsqueeze", "(", "0", ")", ".", "contiguous", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.RelPosMlp.forward": [[164, 166], ["vision_transformer_relpos.RelPosMlp.get_bias"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.RelPosBias.get_bias"], ["", "def", "forward", "(", "self", ",", "attn", ",", "shared_rel_pos", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ")", ":", "\n", "        ", "return", "attn", "+", "self", ".", "get_bias", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.RelPosBias.__init__": [[170, 186], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "vision_transformer_relpos.RelPosBias.register_buffer", "vision_transformer_relpos.RelPosBias.init_weights", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "vision_transformer_relpos.gen_relative_position_index"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.gen_relative_position_index"], ["    ", "def", "__init__", "(", "self", ",", "window_size", ",", "num_heads", ",", "class_token", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "window_size", "=", "window_size", "\n", "self", ".", "window_area", "=", "window_size", "[", "0", "]", "*", "window_size", "[", "1", "]", "\n", "self", ".", "class_token", "=", "1", "if", "class_token", "else", "0", "\n", "self", ".", "bias_shape", "=", "(", "self", ".", "window_area", "+", "self", ".", "class_token", ",", ")", "*", "2", "+", "(", "num_heads", ",", ")", "\n", "\n", "num_relative_distance", "=", "(", "2", "*", "window_size", "[", "0", "]", "-", "1", ")", "*", "(", "2", "*", "window_size", "[", "1", "]", "-", "1", ")", "+", "3", "*", "self", ".", "class_token", "\n", "self", ".", "relative_position_bias_table", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "num_relative_distance", ",", "num_heads", ")", ")", "\n", "self", ".", "register_buffer", "(", "\n", "\"relative_position_index\"", ",", "\n", "gen_relative_position_index", "(", "self", ".", "window_size", ",", "class_token", "=", "self", ".", "class_token", ")", ",", "\n", "persistent", "=", "False", ",", "\n", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.RelPosBias.init_weights": [[187, 189], ["layers.trunc_normal_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "trunc_normal_", "(", "self", ".", "relative_position_bias_table", ",", "std", "=", ".02", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.RelPosBias.get_bias": [[190, 195], ["relative_position_bias.view().permute.view().permute.view().permute", "relative_position_bias.view().permute.view().permute.unsqueeze().contiguous", "vision_transformer_relpos.RelPosBias.relative_position_index.view", "relative_position_bias.view().permute.view().permute.view", "relative_position_bias.view().permute.view().permute.unsqueeze"], "methods", ["None"], ["", "def", "get_bias", "(", "self", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "relative_position_bias", "=", "self", ".", "relative_position_bias_table", "[", "self", ".", "relative_position_index", ".", "view", "(", "-", "1", ")", "]", "\n", "# win_h * win_w, win_h * win_w, num_heads", "\n", "relative_position_bias", "=", "relative_position_bias", ".", "view", "(", "self", ".", "bias_shape", ")", ".", "permute", "(", "2", ",", "0", ",", "1", ")", "\n", "return", "relative_position_bias", ".", "unsqueeze", "(", "0", ")", ".", "contiguous", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.RelPosBias.forward": [[196, 198], ["vision_transformer_relpos.RelPosBias.get_bias"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.RelPosBias.get_bias"], ["", "def", "forward", "(", "self", ",", "attn", ",", "shared_rel_pos", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ")", ":", "\n", "        ", "return", "attn", "+", "self", ".", "get_bias", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.RelPosAttention.__init__": [[201, 213], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "rel_pos_cls"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ",", "num_heads", "=", "8", ",", "qkv_bias", "=", "False", ",", "rel_pos_cls", "=", "None", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "dim", "%", "num_heads", "==", "0", ",", "'dim should be divisible by num_heads'", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "head_dim", "=", "dim", "//", "num_heads", "\n", "self", ".", "scale", "=", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", "*", "3", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "rel_pos", "=", "rel_pos_cls", "(", "num_heads", "=", "num_heads", ")", "if", "rel_pos_cls", "else", "None", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.RelPosAttention.forward": [[214, 231], ["vision_transformer_relpos.RelPosAttention.qkv().reshape().permute", "vision_transformer_relpos.RelPosAttention.unbind", "vision_transformer_relpos.RelPosAttention.softmax", "vision_transformer_relpos.RelPosAttention.attn_drop", "vision_transformer_relpos.RelPosAttention.proj", "vision_transformer_relpos.RelPosAttention.proj_drop", "vision_transformer_relpos.RelPosAttention.rel_pos", "vision_transformer_relpos.RelPosAttention.qkv().reshape", "k.transpose", "vision_transformer_relpos.RelPosAttention.qkv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "shared_rel_pos", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "qkv", "=", "self", ".", "qkv", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "3", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "q", ",", "k", ",", "v", "=", "qkv", ".", "unbind", "(", "0", ")", "# make torchscript happy (cannot use tensor as tuple)", "\n", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "scale", "\n", "if", "self", ".", "rel_pos", "is", "not", "None", ":", "\n", "            ", "attn", "=", "self", ".", "rel_pos", "(", "attn", ",", "shared_rel_pos", "=", "shared_rel_pos", ")", "\n", "", "elif", "shared_rel_pos", "is", "not", "None", ":", "\n", "            ", "attn", "=", "attn", "+", "shared_rel_pos", "\n", "", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "N", ",", "C", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.LayerScale.__init__": [[234, 238], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ",", "init_values", "=", "1e-5", ",", "inplace", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inplace", "=", "inplace", "\n", "self", ".", "gamma", "=", "nn", ".", "Parameter", "(", "init_values", "*", "torch", ".", "ones", "(", "dim", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.LayerScale.forward": [[239, 241], ["x.mul_"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", ".", "mul_", "(", "self", ".", "gamma", ")", "if", "self", ".", "inplace", "else", "x", "*", "self", ".", "gamma", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.RelPosBlock.__init__": [[245, 260], ["torch.Module.__init__", "norm_layer", "vision_transformer_relpos.RelPosAttention", "norm_layer", "layers.Mlp", "vision_transformer_relpos.LayerScale", "torch.Identity", "torch.Identity", "torch.Identity", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "vision_transformer_relpos.LayerScale", "torch.Identity", "torch.Identity", "torch.Identity", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "int"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "num_heads", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "rel_pos_cls", "=", "None", ",", "init_values", "=", "None", ",", "\n", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "attn", "=", "RelPosAttention", "(", "\n", "dim", ",", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "rel_pos_cls", "=", "rel_pos_cls", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ")", "\n", "self", ".", "ls1", "=", "LayerScale", "(", "dim", ",", "init_values", "=", "init_values", ")", "if", "init_values", "else", "nn", ".", "Identity", "(", ")", "\n", "# NOTE: drop path for stochastic depth, we shall see if this is better than dropout here", "\n", "self", ".", "drop_path1", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "int", "(", "dim", "*", "mlp_ratio", ")", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "self", ".", "ls2", "=", "LayerScale", "(", "dim", ",", "init_values", "=", "init_values", ")", "if", "init_values", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "drop_path2", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.RelPosBlock.forward": [[261, 265], ["vision_transformer_relpos.RelPosBlock.drop_path1", "vision_transformer_relpos.RelPosBlock.drop_path2", "vision_transformer_relpos.RelPosBlock.ls1", "vision_transformer_relpos.RelPosBlock.ls2", "vision_transformer_relpos.RelPosBlock.attn", "vision_transformer_relpos.RelPosBlock.mlp", "vision_transformer_relpos.RelPosBlock.norm1", "vision_transformer_relpos.RelPosBlock.norm2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "shared_rel_pos", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ")", ":", "\n", "        ", "x", "=", "x", "+", "self", ".", "drop_path1", "(", "self", ".", "ls1", "(", "self", ".", "attn", "(", "self", ".", "norm1", "(", "x", ")", ",", "shared_rel_pos", "=", "shared_rel_pos", ")", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path2", "(", "self", ".", "ls2", "(", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.ResPostRelPosBlock.__init__": [[269, 285], ["torch.Module.__init__", "vision_transformer_relpos.RelPosAttention", "norm_layer", "layers.Mlp", "norm_layer", "vision_transformer_relpos.ResPostRelPosBlock.init_weights", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "int"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "num_heads", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "rel_pos_cls", "=", "None", ",", "init_values", "=", "None", ",", "\n", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "init_values", "=", "init_values", "\n", "\n", "self", ".", "attn", "=", "RelPosAttention", "(", "\n", "dim", ",", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "rel_pos_cls", "=", "rel_pos_cls", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "drop_path1", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "int", "(", "dim", "*", "mlp_ratio", ")", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "drop_path2", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.ResPostRelPosBlock.init_weights": [[286, 291], ["torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "# NOTE this init overrides that base model init with specific changes for the block type", "\n", "        ", "if", "self", ".", "init_values", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "norm1", ".", "weight", ",", "self", ".", "init_values", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "norm2", ".", "weight", ",", "self", ".", "init_values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.ResPostRelPosBlock.forward": [[292, 296], ["vision_transformer_relpos.ResPostRelPosBlock.drop_path1", "vision_transformer_relpos.ResPostRelPosBlock.drop_path2", "vision_transformer_relpos.ResPostRelPosBlock.norm1", "vision_transformer_relpos.ResPostRelPosBlock.norm2", "vision_transformer_relpos.ResPostRelPosBlock.attn", "vision_transformer_relpos.ResPostRelPosBlock.mlp"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "shared_rel_pos", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ")", ":", "\n", "        ", "x", "=", "x", "+", "self", ".", "drop_path1", "(", "self", ".", "norm1", "(", "self", ".", "attn", "(", "x", ",", "shared_rel_pos", "=", "shared_rel_pos", ")", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path2", "(", "self", ".", "norm2", "(", "self", ".", "mlp", "(", "x", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.VisionTransformerRelPos.__init__": [[308, 387], ["torch.Module.__init__", "embed_layer", "dict", "rel_pos_type.startswith", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "functools.partial", "functools.partial", "functools.partial", "functools.partial.", "torch.Parameter", "torch.Parameter", "torch.Parameter", "x.item", "norm_layer", "torch.Identity", "torch.Identity", "torch.Identity", "norm_layer", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "vision_transformer_relpos.VisionTransformerRelPos.init_weights", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "block_fn", "range"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights"], ["def", "__init__", "(", "\n", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "global_pool", "=", "'avg'", ",", "\n", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "True", ",", "init_values", "=", "1e-6", ",", "\n", "class_token", "=", "False", ",", "fc_norm", "=", "False", ",", "rel_pos_type", "=", "'mlp'", ",", "shared_rel_pos", "=", "False", ",", "rel_pos_dim", "=", "None", ",", "\n", "drop_rate", "=", "0.", ",", "attn_drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ",", "weight_init", "=", "'skip'", ",", "\n", "embed_layer", "=", "PatchEmbed", ",", "norm_layer", "=", "None", ",", "act_layer", "=", "None", ",", "block_fn", "=", "RelPosBlock", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img_size (int, tuple): input image size\n            patch_size (int, tuple): patch size\n            in_chans (int): number of input channels\n            num_classes (int): number of classes for classification head\n            global_pool (str): type of global pooling for final sequence (default: 'avg')\n            embed_dim (int): embedding dimension\n            depth (int): depth of transformer\n            num_heads (int): number of attention heads\n            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n            qkv_bias (bool): enable bias for qkv if True\n            init_values: (float): layer-scale init values\n            class_token (bool): use class token (default: False)\n            fc_norm (bool): use pre classifier norm instead of pre-pool\n            rel_pos_ty pe (str): type of relative position\n            shared_rel_pos (bool): share relative pos across all blocks\n            drop_rate (float): dropout rate\n            attn_drop_rate (float): attention dropout rate\n            drop_path_rate (float): stochastic depth rate\n            weight_init (str): weight init scheme\n            embed_layer (nn.Module): patch embedding layer\n            norm_layer: (nn.Module): normalization layer\n            act_layer: (nn.Module): MLP activation layer\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "global_pool", "in", "(", "''", ",", "'avg'", ",", "'token'", ")", "\n", "assert", "class_token", "or", "global_pool", "!=", "'token'", "\n", "norm_layer", "=", "norm_layer", "or", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", "\n", "act_layer", "=", "act_layer", "or", "nn", ".", "GELU", "\n", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "self", ".", "num_features", "=", "self", ".", "embed_dim", "=", "embed_dim", "# num_features for consistency with other models", "\n", "self", ".", "num_tokens", "=", "1", "if", "class_token", "else", "0", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "self", ".", "patch_embed", "=", "embed_layer", "(", "\n", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", ",", "in_chans", "=", "in_chans", ",", "embed_dim", "=", "embed_dim", ")", "\n", "feat_size", "=", "self", ".", "patch_embed", ".", "grid_size", "\n", "\n", "rel_pos_args", "=", "dict", "(", "window_size", "=", "feat_size", ",", "class_token", "=", "class_token", ")", "\n", "if", "rel_pos_type", ".", "startswith", "(", "'mlp'", ")", ":", "\n", "            ", "if", "rel_pos_dim", ":", "\n", "                ", "rel_pos_args", "[", "'hidden_dim'", "]", "=", "rel_pos_dim", "\n", "", "if", "'swin'", "in", "rel_pos_type", ":", "\n", "                ", "rel_pos_args", "[", "'mode'", "]", "=", "'swin'", "\n", "", "rel_pos_cls", "=", "partial", "(", "RelPosMlp", ",", "**", "rel_pos_args", ")", "\n", "", "else", ":", "\n", "            ", "rel_pos_cls", "=", "partial", "(", "RelPosBias", ",", "**", "rel_pos_args", ")", "\n", "", "self", ".", "shared_rel_pos", "=", "None", "\n", "if", "shared_rel_pos", ":", "\n", "            ", "self", ".", "shared_rel_pos", "=", "rel_pos_cls", "(", "num_heads", "=", "num_heads", ")", "\n", "# NOTE shared rel pos currently mutually exclusive w/ per-block, but could support both...", "\n", "rel_pos_cls", "=", "None", "\n", "\n", "", "self", ".", "cls_token", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "self", ".", "num_tokens", ",", "embed_dim", ")", ")", "if", "self", ".", "num_tokens", "else", "None", "\n", "\n", "dpr", "=", "[", "x", ".", "item", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "depth", ")", "]", "# stochastic depth decay rule", "\n", "self", ".", "blocks", "=", "nn", ".", "ModuleList", "(", "[", "\n", "block_fn", "(", "\n", "dim", "=", "embed_dim", ",", "num_heads", "=", "num_heads", ",", "mlp_ratio", "=", "mlp_ratio", ",", "qkv_bias", "=", "qkv_bias", ",", "rel_pos_cls", "=", "rel_pos_cls", ",", "\n", "init_values", "=", "init_values", ",", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "dpr", "[", "i", "]", ",", "\n", "norm_layer", "=", "norm_layer", ",", "act_layer", "=", "act_layer", ")", "\n", "for", "i", "in", "range", "(", "depth", ")", "]", ")", "\n", "self", ".", "norm", "=", "norm_layer", "(", "embed_dim", ")", "if", "not", "fc_norm", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "# Classifier Head", "\n", "self", ".", "fc_norm", "=", "norm_layer", "(", "embed_dim", ")", "if", "fc_norm", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "if", "weight_init", "!=", "'skip'", ":", "\n", "            ", "self", ".", "init_weights", "(", "weight_init", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.VisionTransformerRelPos.init_weights": [[388, 392], ["torch.init.normal_", "torch.init.normal_", "torch.init.normal_"], "methods", ["None"], ["", "", "def", "init_weights", "(", "self", ",", "mode", "=", "''", ")", ":", "\n", "        ", "assert", "mode", "in", "(", "'jax'", ",", "'moco'", ",", "''", ")", "\n", "if", "self", ".", "cls_token", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "normal_", "(", "self", ".", "cls_token", ",", "std", "=", "1e-6", ")", "\n", "# FIXME weight init scheme using PyTorch defaults curently", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.VisionTransformerRelPos.no_weight_decay": [[395, 398], ["None"], "methods", ["None"], ["", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "no_weight_decay", "(", "self", ")", ":", "\n", "        ", "return", "{", "'cls_token'", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.VisionTransformerRelPos.group_matcher": [[399, 404], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^cls_token|patch_embed'", ",", "# stem and embed", "\n", "blocks", "=", "[", "(", "r'^blocks\\.(\\d+)'", ",", "None", ")", ",", "(", "r'^norm'", ",", "(", "99999", ",", ")", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.VisionTransformerRelPos.set_grad_checkpointing": [[406, 409], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.VisionTransformerRelPos.get_classifier": [[410, 413], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.VisionTransformerRelPos.reset_classifier": [[414, 420], ["torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ":", "int", ",", "global_pool", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "assert", "global_pool", "in", "(", "''", ",", "'avg'", ",", "'token'", ")", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.VisionTransformerRelPos.forward_features": [[421, 434], ["vision_transformer_relpos.VisionTransformerRelPos.patch_embed", "vision_transformer_relpos.VisionTransformerRelPos.norm", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "vision_transformer_relpos.VisionTransformerRelPos.shared_rel_pos.get_bias", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "blk", "vision_transformer_relpos.VisionTransformerRelPos.cls_token.expand", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.RelPosBias.get_bias"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "patch_embed", "(", "x", ")", "\n", "if", "self", ".", "cls_token", "is", "not", "None", ":", "\n", "            ", "x", "=", "torch", ".", "cat", "(", "(", "self", ".", "cls_token", ".", "expand", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ",", "-", "1", ")", ",", "x", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "shared_rel_pos", "=", "self", ".", "shared_rel_pos", ".", "get_bias", "(", ")", "if", "self", ".", "shared_rel_pos", "is", "not", "None", "else", "None", "\n", "for", "blk", "in", "self", ".", "blocks", ":", "\n", "            ", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "                ", "x", "=", "checkpoint", "(", "blk", ",", "x", ",", "shared_rel_pos", "=", "shared_rel_pos", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "blk", "(", "x", ",", "shared_rel_pos", "=", "shared_rel_pos", ")", "\n", "", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.VisionTransformerRelPos.forward_head": [[435, 440], ["vision_transformer_relpos.VisionTransformerRelPos.fc_norm", "vision_transformer_relpos.VisionTransformerRelPos.head", "x[].mean"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "global_pool", ":", "\n", "            ", "x", "=", "x", "[", ":", ",", "self", ".", "num_tokens", ":", "]", ".", "mean", "(", "dim", "=", "1", ")", "if", "self", ".", "global_pool", "==", "'avg'", "else", "x", "[", ":", ",", "0", "]", "\n", "", "x", "=", "self", ".", "fc_norm", "(", "x", ")", "\n", "return", "x", "if", "pre_logits", "else", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.VisionTransformerRelPos.forward": [[441, 445], ["vision_transformer_relpos.VisionTransformerRelPos.forward_features", "vision_transformer_relpos.VisionTransformerRelPos.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos._cfg": [[26, 34], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", ".9", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "IMAGENET_INCEPTION_MEAN", ",", "'std'", ":", "IMAGENET_INCEPTION_STD", ",", "\n", "'first_conv'", ":", "'patch_embed.proj'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.gen_relative_position_index": [[62, 83], ["torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "relative_coords.permute().contiguous.permute().contiguous", "torch.zeros", "torch.zeros", "torch.zeros", "relative_coords.permute().contiguous.sum", "relative_coords.permute().contiguous.sum", "torch.stack", "torch.stack", "torch.stack", "relative_coords.permute().contiguous.permute", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "function", ["None"], ["def", "gen_relative_position_index", "(", "win_size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "class_token", ":", "int", "=", "0", ")", "->", "torch", ".", "Tensor", ":", "\n", "# cut and paste w/ modifications from swin / beit codebase", "\n", "# cls to token & token 2 cls & cls to cls", "\n", "# get pair-wise relative position index for each token inside the window", "\n", "    ", "window_area", "=", "win_size", "[", "0", "]", "*", "win_size", "[", "1", "]", "\n", "coords", "=", "torch", ".", "stack", "(", "torch", ".", "meshgrid", "(", "[", "torch", ".", "arange", "(", "win_size", "[", "0", "]", ")", ",", "torch", ".", "arange", "(", "win_size", "[", "1", "]", ")", "]", ")", ")", ".", "flatten", "(", "1", ")", "# 2, Wh, Ww", "\n", "relative_coords", "=", "coords", "[", ":", ",", ":", ",", "None", "]", "-", "coords", "[", ":", ",", "None", ",", ":", "]", "# 2, Wh*Ww, Wh*Ww", "\n", "relative_coords", "=", "relative_coords", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ".", "contiguous", "(", ")", "# Wh*Ww, Wh*Ww, 2", "\n", "relative_coords", "[", ":", ",", ":", ",", "0", "]", "+=", "win_size", "[", "0", "]", "-", "1", "# shift to start from 0", "\n", "relative_coords", "[", ":", ",", ":", ",", "1", "]", "+=", "win_size", "[", "1", "]", "-", "1", "\n", "relative_coords", "[", ":", ",", ":", ",", "0", "]", "*=", "2", "*", "win_size", "[", "1", "]", "-", "1", "\n", "if", "class_token", ":", "\n", "        ", "num_relative_distance", "=", "(", "2", "*", "win_size", "[", "0", "]", "-", "1", ")", "*", "(", "2", "*", "win_size", "[", "1", "]", "-", "1", ")", "+", "3", "\n", "relative_position_index", "=", "torch", ".", "zeros", "(", "size", "=", "(", "window_area", "+", "1", ",", ")", "*", "2", ",", "dtype", "=", "relative_coords", ".", "dtype", ")", "\n", "relative_position_index", "[", "1", ":", ",", "1", ":", "]", "=", "relative_coords", ".", "sum", "(", "-", "1", ")", "# Wh*Ww, Wh*Ww", "\n", "relative_position_index", "[", "0", ",", "0", ":", "]", "=", "num_relative_distance", "-", "3", "\n", "relative_position_index", "[", "0", ":", ",", "0", "]", "=", "num_relative_distance", "-", "2", "\n", "relative_position_index", "[", "0", ",", "0", "]", "=", "num_relative_distance", "-", "1", "\n", "", "else", ":", "\n", "        ", "relative_position_index", "=", "relative_coords", ".", "sum", "(", "-", "1", ")", "# Wh*Ww, Wh*Ww", "\n", "", "return", "relative_position_index", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.gen_relative_log_coords": [[85, 110], ["torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.stack", "torch.stack", "torch.stack", "relative_coords_table.permute().contiguous.permute().contiguous", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "math.log2", "math.log2", "relative_coords_table.permute().contiguous.permute", "torch.sign", "torch.sign", "torch.sign", "torch.log2", "torch.log2", "torch.log2", "relative_coords_table.permute().contiguous.abs"], "function", ["None"], ["", "def", "gen_relative_log_coords", "(", "\n", "win_size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "\n", "pretrained_win_size", ":", "Tuple", "[", "int", ",", "int", "]", "=", "(", "0", ",", "0", ")", ",", "\n", "mode", "=", "'swin'", "\n", ")", ":", "\n", "# as per official swin-v2 impl, supporting timm swin-v2-cr coords as well", "\n", "    ", "relative_coords_h", "=", "torch", ".", "arange", "(", "-", "(", "win_size", "[", "0", "]", "-", "1", ")", ",", "win_size", "[", "0", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "relative_coords_w", "=", "torch", ".", "arange", "(", "-", "(", "win_size", "[", "1", "]", "-", "1", ")", ",", "win_size", "[", "1", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "relative_coords_table", "=", "torch", ".", "stack", "(", "torch", ".", "meshgrid", "(", "[", "relative_coords_h", ",", "relative_coords_w", "]", ")", ")", "\n", "relative_coords_table", "=", "relative_coords_table", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ".", "contiguous", "(", ")", "# 2*Wh-1, 2*Ww-1, 2", "\n", "if", "mode", "==", "'swin'", ":", "\n", "        ", "if", "pretrained_win_size", "[", "0", "]", ">", "0", ":", "\n", "            ", "relative_coords_table", "[", ":", ",", ":", ",", "0", "]", "/=", "(", "pretrained_win_size", "[", "0", "]", "-", "1", ")", "\n", "relative_coords_table", "[", ":", ",", ":", ",", "1", "]", "/=", "(", "pretrained_win_size", "[", "1", "]", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "relative_coords_table", "[", ":", ",", ":", ",", "0", "]", "/=", "(", "win_size", "[", "0", "]", "-", "1", ")", "\n", "relative_coords_table", "[", ":", ",", ":", ",", "1", "]", "/=", "(", "win_size", "[", "1", "]", "-", "1", ")", "\n", "", "relative_coords_table", "*=", "8", "# normalize to -8, 8", "\n", "scale", "=", "math", ".", "log2", "(", "8", ")", "\n", "", "else", ":", "\n", "# FIXME we should support a form of normalization (to -1/1) for this mode?", "\n", "        ", "scale", "=", "math", ".", "log2", "(", "math", ".", "e", ")", "\n", "", "relative_coords_table", "=", "torch", ".", "sign", "(", "relative_coords_table", ")", "*", "torch", ".", "log2", "(", "\n", "1.0", "+", "relative_coords_table", ".", "abs", "(", ")", ")", "/", "scale", "\n", "return", "relative_coords_table", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos._create_vision_transformer_relpos": [[447, 453], ["kwargs.get", "helpers.build_model_with_cfg", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_vision_transformer_relpos", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "kwargs", ".", "get", "(", "'features_only'", ",", "None", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'features_only not implemented for Vision Transformer models.'", ")", "\n", "\n", "", "model", "=", "build_model_with_cfg", "(", "VisionTransformerRelPos", ",", "variant", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.vit_relpos_base_patch32_plus_rpn_256": [[455, 464], ["dict", "vision_transformer_relpos._create_vision_transformer_relpos"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos._create_vision_transformer_relpos"], ["", "@", "register_model", "\n", "def", "vit_relpos_base_patch32_plus_rpn_256", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/32+) w/ relative log-coord position and residual post-norm, no class token\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "32", ",", "embed_dim", "=", "896", ",", "depth", "=", "12", ",", "num_heads", "=", "14", ",", "block_fn", "=", "ResPostRelPosBlock", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_relpos", "(", "\n", "'vit_relpos_base_patch32_plus_rpn_256'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.vit_relpos_base_patch16_plus_240": [[466, 473], ["dict", "vision_transformer_relpos._create_vision_transformer_relpos"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos._create_vision_transformer_relpos"], ["", "@", "register_model", "\n", "def", "vit_relpos_base_patch16_plus_240", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/16+) w/ relative log-coord position, no class token\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "896", ",", "depth", "=", "12", ",", "num_heads", "=", "14", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_relpos", "(", "'vit_relpos_base_patch16_plus_240'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.vit_relpos_small_patch16_224": [[475, 483], ["dict", "vision_transformer_relpos._create_vision_transformer_relpos"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos._create_vision_transformer_relpos"], ["", "@", "register_model", "\n", "def", "vit_relpos_small_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/16) w/ relative log-coord position, no class token\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "qkv_bias", "=", "False", ",", "fc_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_relpos", "(", "'vit_relpos_small_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.vit_relpos_medium_patch16_224": [[485, 493], ["dict", "vision_transformer_relpos._create_vision_transformer_relpos"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos._create_vision_transformer_relpos"], ["", "@", "register_model", "\n", "def", "vit_relpos_medium_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/16) w/ relative log-coord position, no class token\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "512", ",", "depth", "=", "12", ",", "num_heads", "=", "8", ",", "qkv_bias", "=", "False", ",", "fc_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_relpos", "(", "'vit_relpos_medium_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.vit_relpos_base_patch16_224": [[495, 503], ["dict", "vision_transformer_relpos._create_vision_transformer_relpos"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos._create_vision_transformer_relpos"], ["", "@", "register_model", "\n", "def", "vit_relpos_base_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/16) w/ relative log-coord position, no class token\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "qkv_bias", "=", "False", ",", "fc_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_relpos", "(", "'vit_relpos_base_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.vit_relpos_base_patch16_cls_224": [[505, 514], ["dict", "vision_transformer_relpos._create_vision_transformer_relpos"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos._create_vision_transformer_relpos"], ["", "@", "register_model", "\n", "def", "vit_relpos_base_patch16_cls_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/16) w/ relative log-coord position, class token present\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "qkv_bias", "=", "False", ",", "\n", "class_token", "=", "True", ",", "global_pool", "=", "'token'", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_relpos", "(", "'vit_relpos_base_patch16_cls_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.vit_relpos_base_patch16_gapcls_224": [[516, 526], ["dict", "vision_transformer_relpos._create_vision_transformer_relpos"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos._create_vision_transformer_relpos"], ["", "@", "register_model", "\n", "def", "vit_relpos_base_patch16_gapcls_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/16) w/ relative log-coord position, class token present\n    NOTE this config is a bit of a mistake, class token was enabled but global avg-pool w/ fc-norm was not disabled\n    Leaving here for comparisons w/ a future re-train as it performs quite well.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "qkv_bias", "=", "False", ",", "fc_norm", "=", "True", ",", "class_token", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_relpos", "(", "'vit_relpos_base_patch16_gapcls_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.vit_relpos_small_patch16_rpn_224": [[528, 537], ["dict", "vision_transformer_relpos._create_vision_transformer_relpos"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos._create_vision_transformer_relpos"], ["", "@", "register_model", "\n", "def", "vit_relpos_small_patch16_rpn_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/16) w/ relative log-coord position and residual post-norm, no class token\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "qkv_bias", "=", "False", ",", "block_fn", "=", "ResPostRelPosBlock", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_relpos", "(", "\n", "'vit_relpos_small_patch16_rpn_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.vit_relpos_medium_patch16_rpn_224": [[539, 548], ["dict", "vision_transformer_relpos._create_vision_transformer_relpos"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos._create_vision_transformer_relpos"], ["", "@", "register_model", "\n", "def", "vit_relpos_medium_patch16_rpn_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/16) w/ relative log-coord position and residual post-norm, no class token\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "512", ",", "depth", "=", "12", ",", "num_heads", "=", "8", ",", "qkv_bias", "=", "False", ",", "block_fn", "=", "ResPostRelPosBlock", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_relpos", "(", "\n", "'vit_relpos_medium_patch16_rpn_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos.vit_relpos_base_patch16_rpn_224": [[550, 559], ["dict", "vision_transformer_relpos._create_vision_transformer_relpos"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer_relpos._create_vision_transformer_relpos"], ["", "@", "register_model", "\n", "def", "vit_relpos_base_patch16_rpn_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/16) w/ relative log-coord position and residual post-norm, no class token\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "qkv_bias", "=", "False", ",", "block_fn", "=", "ResPostRelPosBlock", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer_relpos", "(", "\n", "'vit_relpos_base_patch16_rpn_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.res2net.Bottle2neck.__init__": [[53, 90], ["torch.Module.__init__", "max", "torch.Conv2d", "torch.Conv2d", "norm_layer", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Conv2d", "torch.Conv2d", "norm_layer", "act_layer", "int", "convs.append", "bns.append", "torch.AvgPool2d", "torch.AvgPool2d", "attn_layer", "math.floor", "torch.Conv2d", "torch.Conv2d", "norm_layer"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "downsample", "=", "None", ",", "\n", "cardinality", "=", "1", ",", "base_width", "=", "26", ",", "scale", "=", "4", ",", "dilation", "=", "1", ",", "first_dilation", "=", "None", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "None", ",", "attn_layer", "=", "None", ",", "**", "_", ")", ":", "\n", "        ", "super", "(", "Bottle2neck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "scale", "=", "scale", "\n", "self", ".", "is_first", "=", "stride", ">", "1", "or", "downsample", "is", "not", "None", "\n", "self", ".", "num_scales", "=", "max", "(", "1", ",", "scale", "-", "1", ")", "\n", "width", "=", "int", "(", "math", ".", "floor", "(", "planes", "*", "(", "base_width", "/", "64.0", ")", ")", ")", "*", "cardinality", "\n", "self", ".", "width", "=", "width", "\n", "outplanes", "=", "planes", "*", "self", ".", "expansion", "\n", "first_dilation", "=", "first_dilation", "or", "dilation", "\n", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "inplanes", ",", "width", "*", "scale", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "width", "*", "scale", ")", "\n", "\n", "convs", "=", "[", "]", "\n", "bns", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "num_scales", ")", ":", "\n", "            ", "convs", ".", "append", "(", "nn", ".", "Conv2d", "(", "\n", "width", ",", "width", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "padding", "=", "first_dilation", ",", "\n", "dilation", "=", "first_dilation", ",", "groups", "=", "cardinality", ",", "bias", "=", "False", ")", ")", "\n", "bns", ".", "append", "(", "norm_layer", "(", "width", ")", ")", "\n", "", "self", ".", "convs", "=", "nn", ".", "ModuleList", "(", "convs", ")", "\n", "self", ".", "bns", "=", "nn", ".", "ModuleList", "(", "bns", ")", "\n", "if", "self", ".", "is_first", ":", "\n", "# FIXME this should probably have count_include_pad=False, but hurts original weights", "\n", "            ", "self", ".", "pool", "=", "nn", ".", "AvgPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "padding", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "pool", "=", "None", "\n", "\n", "", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "width", "*", "scale", ",", "outplanes", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn3", "=", "norm_layer", "(", "outplanes", ")", "\n", "self", ".", "se", "=", "attn_layer", "(", "outplanes", ")", "if", "attn_layer", "is", "not", "None", "else", "None", "\n", "\n", "self", ".", "relu", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.res2net.Bottle2neck.zero_init_last": [[91, 93], ["torch.init.zeros_", "torch.init.zeros_"], "methods", ["None"], ["", "def", "zero_init_last", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "bn3", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.res2net.Bottle2neck.forward": [[94, 133], ["res2net.Bottle2neck.conv1", "res2net.Bottle2neck.bn1", "res2net.Bottle2neck.relu", "torch.split", "torch.split", "torch.split", "torch.split", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "res2net.Bottle2neck.conv3", "res2net.Bottle2neck.bn3", "res2net.Bottle2neck.relu", "zip", "conv", "bn", "res2net.Bottle2neck.relu", "spo.append", "res2net.Bottle2neck.se", "res2net.Bottle2neck.downsample", "spo.append", "spo.append", "res2net.Bottle2neck.pool"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "spx", "=", "torch", ".", "split", "(", "out", ",", "self", ".", "width", ",", "1", ")", "\n", "spo", "=", "[", "]", "\n", "sp", "=", "spx", "[", "0", "]", "# redundant, for torchscript", "\n", "for", "i", ",", "(", "conv", ",", "bn", ")", "in", "enumerate", "(", "zip", "(", "self", ".", "convs", ",", "self", ".", "bns", ")", ")", ":", "\n", "            ", "if", "i", "==", "0", "or", "self", ".", "is_first", ":", "\n", "                ", "sp", "=", "spx", "[", "i", "]", "\n", "", "else", ":", "\n", "                ", "sp", "=", "sp", "+", "spx", "[", "i", "]", "\n", "", "sp", "=", "conv", "(", "sp", ")", "\n", "sp", "=", "bn", "(", "sp", ")", "\n", "sp", "=", "self", ".", "relu", "(", "sp", ")", "\n", "spo", ".", "append", "(", "sp", ")", "\n", "", "if", "self", ".", "scale", ">", "1", ":", "\n", "            ", "if", "self", ".", "pool", "is", "not", "None", ":", "# self.is_first == True, None check for torchscript", "\n", "                ", "spo", ".", "append", "(", "self", ".", "pool", "(", "spx", "[", "-", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "spo", ".", "append", "(", "spx", "[", "-", "1", "]", ")", "\n", "", "", "out", "=", "torch", ".", "cat", "(", "spo", ",", "1", ")", "\n", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "out", "=", "self", ".", "bn3", "(", "out", ")", "\n", "\n", "if", "self", ".", "se", "is", "not", "None", ":", "\n", "            ", "out", "=", "self", ".", "se", "(", "out", ")", "\n", "\n", "", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "shortcut", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "+=", "shortcut", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.res2net._cfg": [[18, 26], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bilinear'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'conv1'", ",", "'classifier'", ":", "'fc'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.res2net._create_res2net": [[135, 137], ["helpers.build_model_with_cfg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_res2net", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "ResNet", ",", "variant", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.res2net.res2net50_26w_4s": [[139, 148], ["dict", "res2net._create_res2net", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.res2net._create_res2net"], ["", "@", "register_model", "\n", "def", "res2net50_26w_4s", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a Res2Net-50 26w4s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottle2neck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "base_width", "=", "26", ",", "block_args", "=", "dict", "(", "scale", "=", "4", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_res2net", "(", "'res2net50_26w_4s'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.res2net.res2net101_26w_4s": [[150, 159], ["dict", "res2net._create_res2net", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.res2net._create_res2net"], ["", "@", "register_model", "\n", "def", "res2net101_26w_4s", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a Res2Net-101 26w4s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottle2neck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "base_width", "=", "26", ",", "block_args", "=", "dict", "(", "scale", "=", "4", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_res2net", "(", "'res2net101_26w_4s'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.res2net.res2net50_26w_6s": [[161, 170], ["dict", "res2net._create_res2net", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.res2net._create_res2net"], ["", "@", "register_model", "\n", "def", "res2net50_26w_6s", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a Res2Net-50 26w6s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottle2neck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "base_width", "=", "26", ",", "block_args", "=", "dict", "(", "scale", "=", "6", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_res2net", "(", "'res2net50_26w_6s'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.res2net.res2net50_26w_8s": [[172, 181], ["dict", "res2net._create_res2net", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.res2net._create_res2net"], ["", "@", "register_model", "\n", "def", "res2net50_26w_8s", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a Res2Net-50 26w8s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottle2neck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "base_width", "=", "26", ",", "block_args", "=", "dict", "(", "scale", "=", "8", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_res2net", "(", "'res2net50_26w_8s'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.res2net.res2net50_48w_2s": [[183, 192], ["dict", "res2net._create_res2net", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.res2net._create_res2net"], ["", "@", "register_model", "\n", "def", "res2net50_48w_2s", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a Res2Net-50 48w2s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottle2neck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "base_width", "=", "48", ",", "block_args", "=", "dict", "(", "scale", "=", "2", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_res2net", "(", "'res2net50_48w_2s'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.res2net.res2net50_14w_8s": [[194, 203], ["dict", "res2net._create_res2net", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.res2net._create_res2net"], ["", "@", "register_model", "\n", "def", "res2net50_14w_8s", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a Res2Net-50 14w8s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottle2neck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "base_width", "=", "14", ",", "block_args", "=", "dict", "(", "scale", "=", "8", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_res2net", "(", "'res2net50_14w_8s'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.res2net.res2next50": [[205, 214], ["dict", "res2net._create_res2net", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.res2net._create_res2net"], ["", "@", "register_model", "\n", "def", "res2next50", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Construct Res2NeXt-50 4s\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottle2neck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "base_width", "=", "4", ",", "cardinality", "=", "8", ",", "block_args", "=", "dict", "(", "scale", "=", "4", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_res2net", "(", "'res2next50'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.SeparableConv2d.__init__": [[55, 73], ["torch.Module.__init__", "layers.create_conv2d", "norm_layer", "layers.create_conv2d", "norm_layer", "act_layer", "torch.Identity", "torch.Identity", "act_layer", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "padding", "=", "''", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ")", ":", "\n", "        ", "super", "(", "SeparableConv2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "dilation", "=", "dilation", "\n", "\n", "# depthwise convolution", "\n", "self", ".", "conv_dw", "=", "create_conv2d", "(", "\n", "in_chs", ",", "in_chs", ",", "kernel_size", ",", "stride", "=", "stride", ",", "\n", "padding", "=", "padding", ",", "dilation", "=", "dilation", ",", "depthwise", "=", "True", ")", "\n", "self", ".", "bn_dw", "=", "norm_layer", "(", "in_chs", ")", "\n", "self", ".", "act_dw", "=", "act_layer", "(", "inplace", "=", "True", ")", "if", "act_layer", "is", "not", "None", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "# pointwise convolution", "\n", "self", ".", "conv_pw", "=", "create_conv2d", "(", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "1", ")", "\n", "self", ".", "bn_pw", "=", "norm_layer", "(", "out_chs", ")", "\n", "self", ".", "act_pw", "=", "act_layer", "(", "inplace", "=", "True", ")", "if", "act_layer", "is", "not", "None", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.SeparableConv2d.forward": [[74, 82], ["xception_aligned.SeparableConv2d.conv_dw", "xception_aligned.SeparableConv2d.bn_dw", "xception_aligned.SeparableConv2d.act_dw", "xception_aligned.SeparableConv2d.conv_pw", "xception_aligned.SeparableConv2d.bn_pw", "xception_aligned.SeparableConv2d.act_pw"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv_dw", "(", "x", ")", "\n", "x", "=", "self", ".", "bn_dw", "(", "x", ")", "\n", "x", "=", "self", ".", "act_dw", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_pw", "(", "x", ")", "\n", "x", "=", "self", ".", "bn_pw", "(", "x", ")", "\n", "x", "=", "self", ".", "act_pw", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.PreSeparableConv2d.__init__": [[85, 101], ["torch.Module.__init__", "layers.get_norm_act_layer", "layers.create_conv2d", "layers.create_conv2d", "layers.get_norm_act_layer.", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.get_norm_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "padding", "=", "''", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "first_act", "=", "True", ")", ":", "\n", "        ", "super", "(", "PreSeparableConv2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "norm_act_layer", "=", "get_norm_act_layer", "(", "norm_layer", ",", "act_layer", "=", "act_layer", ")", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "dilation", "=", "dilation", "\n", "\n", "self", ".", "norm", "=", "norm_act_layer", "(", "in_chs", ",", "inplace", "=", "True", ")", "if", "first_act", "else", "nn", ".", "Identity", "(", ")", "\n", "# depthwise convolution", "\n", "self", ".", "conv_dw", "=", "create_conv2d", "(", "\n", "in_chs", ",", "in_chs", ",", "kernel_size", ",", "stride", "=", "stride", ",", "\n", "padding", "=", "padding", ",", "dilation", "=", "dilation", ",", "depthwise", "=", "True", ")", "\n", "\n", "# pointwise convolution", "\n", "self", ".", "conv_pw", "=", "create_conv2d", "(", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.PreSeparableConv2d.forward": [[102, 107], ["xception_aligned.PreSeparableConv2d.norm", "xception_aligned.PreSeparableConv2d.conv_dw", "xception_aligned.PreSeparableConv2d.conv_pw"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_dw", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_pw", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.XceptionModule.__init__": [[110, 133], ["torch.Module.__init__", "layers.helpers.to_3tuple", "torch.Sequential", "torch.Sequential", "range", "layers.ConvNormAct", "xception_aligned.XceptionModule.stack.add_module", "xception_aligned.XceptionModule.stack.add_module", "xception_aligned.SeparableConv2d", "act_layer"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "pad_type", "=", "''", ",", "\n", "start_with_relu", "=", "True", ",", "no_skip", "=", "False", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "XceptionModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "out_chs", "=", "to_3tuple", "(", "out_chs", ")", "\n", "self", ".", "in_channels", "=", "in_chs", "\n", "self", ".", "out_channels", "=", "out_chs", "[", "-", "1", "]", "\n", "self", ".", "no_skip", "=", "no_skip", "\n", "if", "not", "no_skip", "and", "(", "self", ".", "out_channels", "!=", "self", ".", "in_channels", "or", "stride", "!=", "1", ")", ":", "\n", "            ", "self", ".", "shortcut", "=", "ConvNormAct", "(", "\n", "in_chs", ",", "self", ".", "out_channels", ",", "1", ",", "stride", "=", "stride", ",", "norm_layer", "=", "norm_layer", ",", "apply_act", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "shortcut", "=", "None", "\n", "\n", "", "separable_act_layer", "=", "None", "if", "start_with_relu", "else", "act_layer", "\n", "self", ".", "stack", "=", "nn", ".", "Sequential", "(", ")", "\n", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "            ", "if", "start_with_relu", ":", "\n", "                ", "self", ".", "stack", ".", "add_module", "(", "f'act{i + 1}'", ",", "act_layer", "(", "inplace", "=", "i", ">", "0", ")", ")", "\n", "", "self", ".", "stack", ".", "add_module", "(", "f'conv{i + 1}'", ",", "SeparableConv2d", "(", "\n", "in_chs", ",", "out_chs", "[", "i", "]", ",", "3", ",", "stride", "=", "stride", "if", "i", "==", "2", "else", "1", ",", "dilation", "=", "dilation", ",", "padding", "=", "pad_type", ",", "\n", "act_layer", "=", "separable_act_layer", ",", "norm_layer", "=", "norm_layer", ")", ")", "\n", "in_chs", "=", "out_chs", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.XceptionModule.forward": [[134, 142], ["xception_aligned.XceptionModule.stack", "xception_aligned.XceptionModule.shortcut"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "skip", "=", "x", "\n", "x", "=", "self", ".", "stack", "(", "x", ")", "\n", "if", "self", ".", "shortcut", "is", "not", "None", ":", "\n", "            ", "skip", "=", "self", ".", "shortcut", "(", "skip", ")", "\n", "", "if", "not", "self", ".", "no_skip", ":", "\n", "            ", "x", "=", "x", "+", "skip", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.PreXceptionModule.__init__": [[145, 165], ["torch.Module.__init__", "layers.helpers.to_3tuple", "torch.Sequential", "torch.Sequential", "range", "layers.create_conv2d", "torch.Identity", "torch.Identity", "layers.get_norm_act_layer", "xception_aligned.PreXceptionModule.stack.add_module", "xception_aligned.PreSeparableConv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.get_norm_act_layer"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "pad_type", "=", "''", ",", "\n", "no_skip", "=", "False", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "PreXceptionModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "out_chs", "=", "to_3tuple", "(", "out_chs", ")", "\n", "self", ".", "in_channels", "=", "in_chs", "\n", "self", ".", "out_channels", "=", "out_chs", "[", "-", "1", "]", "\n", "self", ".", "no_skip", "=", "no_skip", "\n", "if", "not", "no_skip", "and", "(", "self", ".", "out_channels", "!=", "self", ".", "in_channels", "or", "stride", "!=", "1", ")", ":", "\n", "            ", "self", ".", "shortcut", "=", "create_conv2d", "(", "in_chs", ",", "self", ".", "out_channels", ",", "1", ",", "stride", "=", "stride", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "shortcut", "=", "nn", ".", "Identity", "(", ")", "\n", "\n", "", "self", ".", "norm", "=", "get_norm_act_layer", "(", "norm_layer", ",", "act_layer", "=", "act_layer", ")", "(", "in_chs", ",", "inplace", "=", "True", ")", "\n", "self", ".", "stack", "=", "nn", ".", "Sequential", "(", ")", "\n", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "            ", "self", ".", "stack", ".", "add_module", "(", "f'conv{i + 1}'", ",", "PreSeparableConv2d", "(", "\n", "in_chs", ",", "out_chs", "[", "i", "]", ",", "3", ",", "stride", "=", "stride", "if", "i", "==", "2", "else", "1", ",", "dilation", "=", "dilation", ",", "padding", "=", "pad_type", ",", "\n", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ",", "first_act", "=", "i", ">", "0", ")", ")", "\n", "in_chs", "=", "out_chs", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.PreXceptionModule.forward": [[166, 173], ["xception_aligned.PreXceptionModule.norm", "xception_aligned.PreXceptionModule.stack", "xception_aligned.PreXceptionModule.shortcut"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "skip", "=", "x", "\n", "x", "=", "self", ".", "stack", "(", "x", ")", "\n", "if", "not", "self", ".", "no_skip", ":", "\n", "            ", "x", "=", "x", "+", "self", ".", "shortcut", "(", "skip", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.XceptionAligned.__init__": [[179, 219], ["torch.Module.__init__", "dict", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "enumerate", "layers.ClassifierHead", "xception_aligned.XceptionAligned.blocks.add_module", "dict", "act_layer", "torch.Identity", "torch.Identity", "str", "module_fn", "layers.ConvNormAct", "dict", "layers.create_conv2d", "layers.ConvNormAct", "str", "layers.helpers.to_3tuple", "len"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d"], ["def", "__init__", "(", "\n", "self", ",", "block_cfg", ",", "num_classes", "=", "1000", ",", "in_chans", "=", "3", ",", "output_stride", "=", "32", ",", "preact", "=", "False", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "drop_rate", "=", "0.", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "super", "(", "XceptionAligned", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "output_stride", "in", "(", "8", ",", "16", ",", "32", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "layer_args", "=", "dict", "(", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "stem", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "ConvNormAct", "(", "in_chans", ",", "32", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "**", "layer_args", ")", ",", "\n", "create_conv2d", "(", "32", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", "if", "preact", "else", "\n", "ConvNormAct", "(", "32", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "**", "layer_args", ")", "\n", "]", ")", "\n", "\n", "curr_dilation", "=", "1", "\n", "curr_stride", "=", "2", "\n", "self", ".", "feature_info", "=", "[", "]", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", ")", "\n", "module_fn", "=", "PreXceptionModule", "if", "preact", "else", "XceptionModule", "\n", "for", "i", ",", "b", "in", "enumerate", "(", "block_cfg", ")", ":", "\n", "            ", "b", "[", "'dilation'", "]", "=", "curr_dilation", "\n", "if", "b", "[", "'stride'", "]", ">", "1", ":", "\n", "                ", "name", "=", "f'blocks.{i}.stack.conv2'", "if", "preact", "else", "f'blocks.{i}.stack.act3'", "\n", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "to_3tuple", "(", "b", "[", "'out_chs'", "]", ")", "[", "-", "2", "]", ",", "reduction", "=", "curr_stride", ",", "module", "=", "name", ")", "]", "\n", "next_stride", "=", "curr_stride", "*", "b", "[", "'stride'", "]", "\n", "if", "next_stride", ">", "output_stride", ":", "\n", "                    ", "curr_dilation", "*=", "b", "[", "'stride'", "]", "\n", "b", "[", "'stride'", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "curr_stride", "=", "next_stride", "\n", "", "", "self", ".", "blocks", ".", "add_module", "(", "str", "(", "i", ")", ",", "module_fn", "(", "**", "b", ",", "**", "layer_args", ")", ")", "\n", "self", ".", "num_features", "=", "self", ".", "blocks", "[", "-", "1", "]", ".", "out_channels", "\n", "\n", "", "self", ".", "feature_info", "+=", "[", "dict", "(", "\n", "num_chs", "=", "self", ".", "num_features", ",", "reduction", "=", "curr_stride", ",", "module", "=", "'blocks.'", "+", "str", "(", "len", "(", "self", ".", "blocks", ")", "-", "1", ")", ")", "]", "\n", "self", ".", "act", "=", "act_layer", "(", "inplace", "=", "True", ")", "if", "preact", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "head", "=", "ClassifierHead", "(", "\n", "in_chs", "=", "self", ".", "num_features", ",", "num_classes", "=", "num_classes", ",", "pool_type", "=", "global_pool", ",", "drop_rate", "=", "drop_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.XceptionAligned.group_matcher": [[220, 225], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^stem'", ",", "\n", "blocks", "=", "r'^blocks\\.(\\d+)'", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.XceptionAligned.set_grad_checkpointing": [[227, 230], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.XceptionAligned.get_classifier": [[231, 234], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", ".", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.XceptionAligned.reset_classifier": [[235, 237], ["layers.ClassifierHead"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "head", "=", "ClassifierHead", "(", "self", ".", "num_features", ",", "num_classes", ",", "pool_type", "=", "global_pool", ",", "drop_rate", "=", "self", ".", "drop_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.XceptionAligned.forward_features": [[238, 246], ["xception_aligned.XceptionAligned.stem", "xception_aligned.XceptionAligned.act", "helpers.checkpoint_seq", "xception_aligned.XceptionAligned.blocks", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "stem", "(", "x", ")", "\n", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "blocks", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "blocks", "(", "x", ")", "\n", "", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.XceptionAligned.forward_head": [[247, 249], ["xception_aligned.XceptionAligned.head"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "head", "(", "x", ",", "pre_logits", "=", "pre_logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.XceptionAligned.forward": [[250, 254], ["xception_aligned.XceptionAligned.forward_features", "xception_aligned.XceptionAligned.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned._cfg": [[22, 30], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "299", ",", "299", ")", ",", "'pool_size'", ":", "(", "10", ",", "10", ")", ",", "\n", "'crop_pct'", ":", "0.903", ",", "'interpolation'", ":", "'bicubic'", ",", "\n", "'mean'", ":", "IMAGENET_INCEPTION_MEAN", ",", "'std'", ":", "IMAGENET_INCEPTION_STD", ",", "\n", "'first_conv'", ":", "'stem.0.conv'", ",", "'classifier'", ":", "'head.fc'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned._xception": [[256, 261], ["helpers.build_model_with_cfg", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_xception", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "\n", "XceptionAligned", ",", "variant", ",", "pretrained", ",", "\n", "feature_cfg", "=", "dict", "(", "flatten_sequential", "=", "True", ",", "feature_cls", "=", "'hook'", ")", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.xception41": [[263, 280], ["dict", "xception_aligned._xception", "dict", "dict", "dict", "dict", "dict", "functools.partial", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception._xception"], ["", "@", "register_model", "\n", "def", "xception41", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Modified Aligned Xception-41\n    \"\"\"", "\n", "block_cfg", "=", "[", "\n", "# entry flow", "\n", "dict", "(", "in_chs", "=", "64", ",", "out_chs", "=", "128", ",", "stride", "=", "2", ")", ",", "\n", "dict", "(", "in_chs", "=", "128", ",", "out_chs", "=", "256", ",", "stride", "=", "2", ")", ",", "\n", "dict", "(", "in_chs", "=", "256", ",", "out_chs", "=", "728", ",", "stride", "=", "2", ")", ",", "\n", "# middle flow", "\n", "*", "(", "[", "dict", "(", "in_chs", "=", "728", ",", "out_chs", "=", "728", ",", "stride", "=", "1", ")", "]", "*", "8", ")", ",", "\n", "# exit flow", "\n", "dict", "(", "in_chs", "=", "728", ",", "out_chs", "=", "(", "728", ",", "1024", ",", "1024", ")", ",", "stride", "=", "2", ")", ",", "\n", "dict", "(", "in_chs", "=", "1024", ",", "out_chs", "=", "(", "1536", ",", "1536", ",", "2048", ")", ",", "stride", "=", "1", ",", "no_skip", "=", "True", ",", "start_with_relu", "=", "False", ")", ",", "\n", "]", "\n", "model_args", "=", "dict", "(", "block_cfg", "=", "block_cfg", ",", "norm_layer", "=", "partial", "(", "nn", ".", "BatchNorm2d", ",", "eps", "=", ".001", ",", "momentum", "=", ".1", ")", ",", "**", "kwargs", ")", "\n", "return", "_xception", "(", "'xception41'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.xception65": [[282, 299], ["dict", "xception_aligned._xception", "dict", "dict", "dict", "dict", "dict", "functools.partial", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception._xception"], ["", "@", "register_model", "\n", "def", "xception65", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Modified Aligned Xception-65\n    \"\"\"", "\n", "block_cfg", "=", "[", "\n", "# entry flow", "\n", "dict", "(", "in_chs", "=", "64", ",", "out_chs", "=", "128", ",", "stride", "=", "2", ")", ",", "\n", "dict", "(", "in_chs", "=", "128", ",", "out_chs", "=", "256", ",", "stride", "=", "2", ")", ",", "\n", "dict", "(", "in_chs", "=", "256", ",", "out_chs", "=", "728", ",", "stride", "=", "2", ")", ",", "\n", "# middle flow", "\n", "*", "(", "[", "dict", "(", "in_chs", "=", "728", ",", "out_chs", "=", "728", ",", "stride", "=", "1", ")", "]", "*", "16", ")", ",", "\n", "# exit flow", "\n", "dict", "(", "in_chs", "=", "728", ",", "out_chs", "=", "(", "728", ",", "1024", ",", "1024", ")", ",", "stride", "=", "2", ")", ",", "\n", "dict", "(", "in_chs", "=", "1024", ",", "out_chs", "=", "(", "1536", ",", "1536", ",", "2048", ")", ",", "stride", "=", "1", ",", "no_skip", "=", "True", ",", "start_with_relu", "=", "False", ")", ",", "\n", "]", "\n", "model_args", "=", "dict", "(", "block_cfg", "=", "block_cfg", ",", "norm_layer", "=", "partial", "(", "nn", ".", "BatchNorm2d", ",", "eps", "=", ".001", ",", "momentum", "=", ".1", ")", ",", "**", "kwargs", ")", "\n", "return", "_xception", "(", "'xception65'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.xception71": [[301, 320], ["dict", "xception_aligned._xception", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "functools.partial", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception._xception"], ["", "@", "register_model", "\n", "def", "xception71", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Modified Aligned Xception-71\n    \"\"\"", "\n", "block_cfg", "=", "[", "\n", "# entry flow", "\n", "dict", "(", "in_chs", "=", "64", ",", "out_chs", "=", "128", ",", "stride", "=", "2", ")", ",", "\n", "dict", "(", "in_chs", "=", "128", ",", "out_chs", "=", "256", ",", "stride", "=", "1", ")", ",", "\n", "dict", "(", "in_chs", "=", "256", ",", "out_chs", "=", "256", ",", "stride", "=", "2", ")", ",", "\n", "dict", "(", "in_chs", "=", "256", ",", "out_chs", "=", "728", ",", "stride", "=", "1", ")", ",", "\n", "dict", "(", "in_chs", "=", "728", ",", "out_chs", "=", "728", ",", "stride", "=", "2", ")", ",", "\n", "# middle flow", "\n", "*", "(", "[", "dict", "(", "in_chs", "=", "728", ",", "out_chs", "=", "728", ",", "stride", "=", "1", ")", "]", "*", "16", ")", ",", "\n", "# exit flow", "\n", "dict", "(", "in_chs", "=", "728", ",", "out_chs", "=", "(", "728", ",", "1024", ",", "1024", ")", ",", "stride", "=", "2", ")", ",", "\n", "dict", "(", "in_chs", "=", "1024", ",", "out_chs", "=", "(", "1536", ",", "1536", ",", "2048", ")", ",", "stride", "=", "1", ",", "no_skip", "=", "True", ",", "start_with_relu", "=", "False", ")", ",", "\n", "]", "\n", "model_args", "=", "dict", "(", "block_cfg", "=", "block_cfg", ",", "norm_layer", "=", "partial", "(", "nn", ".", "BatchNorm2d", ",", "eps", "=", ".001", ",", "momentum", "=", ".1", ")", ",", "**", "kwargs", ")", "\n", "return", "_xception", "(", "'xception71'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.xception41p": [[322, 339], ["dict", "xception_aligned._xception", "dict", "dict", "dict", "dict", "dict", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception._xception"], ["", "@", "register_model", "\n", "def", "xception41p", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Modified Aligned Xception-41 w/ Pre-Act\n    \"\"\"", "\n", "block_cfg", "=", "[", "\n", "# entry flow", "\n", "dict", "(", "in_chs", "=", "64", ",", "out_chs", "=", "128", ",", "stride", "=", "2", ")", ",", "\n", "dict", "(", "in_chs", "=", "128", ",", "out_chs", "=", "256", ",", "stride", "=", "2", ")", ",", "\n", "dict", "(", "in_chs", "=", "256", ",", "out_chs", "=", "728", ",", "stride", "=", "2", ")", ",", "\n", "# middle flow", "\n", "*", "(", "[", "dict", "(", "in_chs", "=", "728", ",", "out_chs", "=", "728", ",", "stride", "=", "1", ")", "]", "*", "8", ")", ",", "\n", "# exit flow", "\n", "dict", "(", "in_chs", "=", "728", ",", "out_chs", "=", "(", "728", ",", "1024", ",", "1024", ")", ",", "stride", "=", "2", ")", ",", "\n", "dict", "(", "in_chs", "=", "1024", ",", "out_chs", "=", "(", "1536", ",", "1536", ",", "2048", ")", ",", "no_skip", "=", "True", ",", "stride", "=", "1", ")", ",", "\n", "]", "\n", "model_args", "=", "dict", "(", "block_cfg", "=", "block_cfg", ",", "preact", "=", "True", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "**", "kwargs", ")", "\n", "return", "_xception", "(", "'xception41p'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception_aligned.xception65p": [[341, 359], ["dict", "xception_aligned._xception", "dict", "dict", "dict", "dict", "dict", "functools.partial", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception._xception"], ["", "@", "register_model", "\n", "def", "xception65p", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Modified Aligned Xception-65 w/ Pre-Act\n    \"\"\"", "\n", "block_cfg", "=", "[", "\n", "# entry flow", "\n", "dict", "(", "in_chs", "=", "64", ",", "out_chs", "=", "128", ",", "stride", "=", "2", ")", ",", "\n", "dict", "(", "in_chs", "=", "128", ",", "out_chs", "=", "256", ",", "stride", "=", "2", ")", ",", "\n", "dict", "(", "in_chs", "=", "256", ",", "out_chs", "=", "728", ",", "stride", "=", "2", ")", ",", "\n", "# middle flow", "\n", "*", "(", "[", "dict", "(", "in_chs", "=", "728", ",", "out_chs", "=", "728", ",", "stride", "=", "1", ")", "]", "*", "16", ")", ",", "\n", "# exit flow", "\n", "dict", "(", "in_chs", "=", "728", ",", "out_chs", "=", "(", "728", ",", "1024", ",", "1024", ")", ",", "stride", "=", "2", ")", ",", "\n", "dict", "(", "in_chs", "=", "1024", ",", "out_chs", "=", "(", "1536", ",", "1536", ",", "2048", ")", ",", "stride", "=", "1", ",", "no_skip", "=", "True", ")", ",", "\n", "]", "\n", "model_args", "=", "dict", "(", "\n", "block_cfg", "=", "block_cfg", ",", "preact", "=", "True", ",", "norm_layer", "=", "partial", "(", "nn", ".", "BatchNorm2d", ",", "eps", "=", ".001", ",", "momentum", "=", ".1", ")", ",", "**", "kwargs", ")", "\n", "return", "_xception", "(", "'xception65p'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.get_cache_dir": [[27, 40], ["os.getenv", "get_dir", "os.path.join", "os.makedirs", "_logger.warning"], "function", ["None"], ["def", "get_cache_dir", "(", "child_dir", "=", "''", ")", ":", "\n", "    ", "\"\"\"\n    Returns the location of the directory where models are cached (and creates it if necessary).\n    \"\"\"", "\n", "# Issue warning to move data if old env is set", "\n", "if", "os", ".", "getenv", "(", "'TORCH_MODEL_ZOO'", ")", ":", "\n", "        ", "_logger", ".", "warning", "(", "'TORCH_MODEL_ZOO is deprecated, please use env TORCH_HOME instead'", ")", "\n", "\n", "", "hub_dir", "=", "get_dir", "(", ")", "\n", "child_dir", "=", "(", ")", "if", "not", "child_dir", "else", "(", "child_dir", ",", ")", "\n", "model_dir", "=", "os", ".", "path", ".", "join", "(", "hub_dir", ",", "'checkpoints'", ",", "*", "child_dir", ")", "\n", "os", ".", "makedirs", "(", "model_dir", ",", "exist_ok", "=", "True", ")", "\n", "return", "model_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.download_cached_file": [[42, 54], ["torch.hub.urlparse", "os.path.basename", "os.path.join", "hub.get_cache_dir", "os.path.exists", "_logger.info", "torch.hub.download_url_to_file", "torch.hub.HASH_REGEX.search", "HASH_REGEX.search.group"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.get_cache_dir"], ["", "def", "download_cached_file", "(", "url", ",", "check_hash", "=", "True", ",", "progress", "=", "False", ")", ":", "\n", "    ", "parts", "=", "urlparse", "(", "url", ")", "\n", "filename", "=", "os", ".", "path", ".", "basename", "(", "parts", ".", "path", ")", "\n", "cached_file", "=", "os", ".", "path", ".", "join", "(", "get_cache_dir", "(", ")", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cached_file", ")", ":", "\n", "        ", "_logger", ".", "info", "(", "'Downloading: \"{}\" to {}\\n'", ".", "format", "(", "url", ",", "cached_file", ")", ")", "\n", "hash_prefix", "=", "None", "\n", "if", "check_hash", ":", "\n", "            ", "r", "=", "HASH_REGEX", ".", "search", "(", "filename", ")", "# r is Optional[Match[str]]", "\n", "hash_prefix", "=", "r", ".", "group", "(", "1", ")", "if", "r", "else", "None", "\n", "", "download_url_to_file", "(", "url", ",", "cached_file", ",", "hash_prefix", ",", "progress", "=", "progress", ")", "\n", "", "return", "cached_file", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.has_hf_hub": [[56, 62], ["RuntimeError"], "function", ["None"], ["", "def", "has_hf_hub", "(", "necessary", "=", "False", ")", ":", "\n", "    ", "if", "not", "_has_hf_hub", "and", "necessary", ":", "\n", "# if no HF Hub module installed and it is necessary to continue, raise error", "\n", "        ", "raise", "RuntimeError", "(", "\n", "'Hugging Face hub model specified but package not installed. Run `pip install huggingface_hub`.'", ")", "\n", "", "return", "_has_hf_hub", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.hf_split": [[64, 71], ["hf_id.split", "len", "len"], "function", ["None"], ["", "def", "hf_split", "(", "hf_id", ")", ":", "\n", "# FIXME I may change @ -> # and be parsed as fragment in a URI model name scheme", "\n", "    ", "rev_split", "=", "hf_id", ".", "split", "(", "'@'", ")", "\n", "assert", "0", "<", "len", "(", "rev_split", ")", "<=", "2", ",", "'hf_hub id should only contain one @ character to identify revision.'", "\n", "hf_model_id", "=", "rev_split", "[", "0", "]", "\n", "hf_revision", "=", "rev_split", "[", "-", "1", "]", "if", "len", "(", "rev_split", ")", ">", "1", "else", "None", "\n", "return", "hf_model_id", ",", "hf_revision", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.load_cfg_from_json": [[73, 77], ["json.loads", "open", "reader.read"], "function", ["None"], ["", "def", "load_cfg_from_json", "(", "json_file", ":", "Union", "[", "str", ",", "os", ".", "PathLike", "]", ")", ":", "\n", "    ", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "        ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "json", ".", "loads", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub._download_from_hf": [[79, 83], ["hub.hf_split", "hf_hub_url", "cached_download", "hub.get_cache_dir"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.hf_split", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.get_cache_dir"], ["", "def", "_download_from_hf", "(", "model_id", ":", "str", ",", "filename", ":", "str", ")", ":", "\n", "    ", "hf_model_id", ",", "hf_revision", "=", "hf_split", "(", "model_id", ")", "\n", "url", "=", "hf_hub_url", "(", "hf_model_id", ",", "filename", ",", "revision", "=", "hf_revision", ")", "\n", "return", "cached_download", "(", "url", ",", "cache_dir", "=", "get_cache_dir", "(", "'hf'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.load_model_config_from_hf": [[85, 93], ["hub.has_hf_hub", "hub._download_from_hf", "hub.load_cfg_from_json", "load_cfg_from_json.get"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.has_hf_hub", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub._download_from_hf", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.load_cfg_from_json", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["", "def", "load_model_config_from_hf", "(", "model_id", ":", "str", ")", ":", "\n", "    ", "assert", "has_hf_hub", "(", "True", ")", "\n", "cached_file", "=", "_download_from_hf", "(", "model_id", ",", "'config.json'", ")", "\n", "pretrained_cfg", "=", "load_cfg_from_json", "(", "cached_file", ")", "\n", "pretrained_cfg", "[", "'hf_hub_id'", "]", "=", "model_id", "# insert hf_hub id for pretrained weight load during model creation", "\n", "pretrained_cfg", "[", "'source'", "]", "=", "'hf-hub'", "\n", "model_name", "=", "pretrained_cfg", ".", "get", "(", "'architecture'", ")", "\n", "return", "pretrained_cfg", ",", "model_name", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.load_state_dict_from_hf": [[95, 100], ["hub.has_hf_hub", "hub._download_from_hf", "torch.load"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.has_hf_hub", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub._download_from_hf"], ["", "def", "load_state_dict_from_hf", "(", "model_id", ":", "str", ")", ":", "\n", "    ", "assert", "has_hf_hub", "(", "True", ")", "\n", "cached_file", "=", "_download_from_hf", "(", "model_id", ",", "'pytorch_model.bin'", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "cached_file", ",", "map_location", "=", "'cpu'", ")", "\n", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.save_for_hf": [[102, 120], ["hub.has_hf_hub", "pathlib.Path", "pathlib.Path.mkdir", "torch.save", "model_config.pop", "model_config.pop", "model_config.pop", "hf_config.update", "model.state_dict", "config_path.open", "json.dump", "range"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.has_hf_hub", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict"], ["", "def", "save_for_hf", "(", "model", ",", "save_directory", ",", "model_config", "=", "None", ")", ":", "\n", "    ", "assert", "has_hf_hub", "(", "True", ")", "\n", "model_config", "=", "model_config", "or", "{", "}", "\n", "save_directory", "=", "Path", "(", "save_directory", ")", "\n", "save_directory", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "\n", "weights_path", "=", "save_directory", "/", "'pytorch_model.bin'", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "weights_path", ")", "\n", "\n", "config_path", "=", "save_directory", "/", "'config.json'", "\n", "hf_config", "=", "model", ".", "pretrained_cfg", "\n", "hf_config", "[", "'num_classes'", "]", "=", "model_config", ".", "pop", "(", "'num_classes'", ",", "model", ".", "num_classes", ")", "\n", "hf_config", "[", "'num_features'", "]", "=", "model_config", ".", "pop", "(", "'num_features'", ",", "model", ".", "num_features", ")", "\n", "hf_config", "[", "'labels'", "]", "=", "model_config", ".", "pop", "(", "'labels'", ",", "[", "f\"LABEL_{i}\"", "for", "i", "in", "range", "(", "hf_config", "[", "'num_classes'", "]", ")", "]", ")", "\n", "hf_config", ".", "update", "(", "model_config", ")", "\n", "\n", "with", "config_path", ".", "open", "(", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "hf_config", ",", "f", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.push_to_hf_hub": [[122, 174], ["Repository", "Repository.git_remote_url", "isinstance", "Repository.commit", "hub.save_for_hf", "repo_namespace_or_url.rstrip().split", "HfFolder.get_token", "ValueError", "HfApi().whoami", "pathlib.Path", "pathlib.Path", "readme_path.exists", "readme_path.write_text", "repo_namespace_or_url.rstrip", "HfApi"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.save_for_hf"], ["", "", "def", "push_to_hf_hub", "(", "\n", "model", ",", "\n", "local_dir", ",", "\n", "repo_namespace_or_url", "=", "None", ",", "\n", "commit_message", "=", "'Add model'", ",", "\n", "use_auth_token", "=", "True", ",", "\n", "git_email", "=", "None", ",", "\n", "git_user", "=", "None", ",", "\n", "revision", "=", "None", ",", "\n", "model_config", "=", "None", ",", "\n", ")", ":", "\n", "    ", "if", "repo_namespace_or_url", ":", "\n", "        ", "repo_owner", ",", "repo_name", "=", "repo_namespace_or_url", ".", "rstrip", "(", "'/'", ")", ".", "split", "(", "'/'", ")", "[", "-", "2", ":", "]", "\n", "", "else", ":", "\n", "        ", "if", "isinstance", "(", "use_auth_token", ",", "str", ")", ":", "\n", "            ", "token", "=", "use_auth_token", "\n", "", "else", ":", "\n", "            ", "token", "=", "HfFolder", ".", "get_token", "(", ")", "\n", "\n", "", "if", "token", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"You must login to the Hugging Face hub on this computer by typing `transformers-cli login` and \"", "\n", "\"entering your credentials to use `use_auth_token=True`. Alternatively, you can pass your own \"", "\n", "\"token as the `use_auth_token` argument.\"", "\n", ")", "\n", "\n", "", "repo_owner", "=", "HfApi", "(", ")", ".", "whoami", "(", "token", ")", "[", "'name'", "]", "\n", "repo_name", "=", "Path", "(", "local_dir", ")", ".", "name", "\n", "\n", "", "repo_url", "=", "f'https://huggingface.co/{repo_owner}/{repo_name}'", "\n", "\n", "repo", "=", "Repository", "(", "\n", "local_dir", ",", "\n", "clone_from", "=", "repo_url", ",", "\n", "use_auth_token", "=", "use_auth_token", ",", "\n", "git_user", "=", "git_user", ",", "\n", "git_email", "=", "git_email", ",", "\n", "revision", "=", "revision", ",", "\n", ")", "\n", "\n", "# Prepare a default model card that includes the necessary tags to enable inference.", "\n", "readme_text", "=", "f'---\\ntags:\\n- image-classification\\n- timm\\nlibrary_tag: timm\\n---\\n# Model card for {repo_name}'", "\n", "with", "repo", ".", "commit", "(", "commit_message", ")", ":", "\n", "# Save model weights and config.", "\n", "        ", "save_for_hf", "(", "model", ",", "repo", ".", "local_dir", ",", "model_config", "=", "model_config", ")", "\n", "\n", "# Save a model card if it doesn't exist.", "\n", "readme_path", "=", "Path", "(", "repo", ".", "local_dir", ")", "/", "'README.md'", "\n", "if", "not", "readme_path", ".", "exists", "(", ")", ":", "\n", "            ", "readme_path", ".", "write_text", "(", "readme_text", ")", "\n", "\n", "", "", "return", "repo", ".", "git_remote_url", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer.SpatialMlp.__init__": [[44, 74], ["torch.Module.__init__", "layers.to_2tuple", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "act_layer", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "act_layer"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_features", ",", "hidden_features", "=", "None", ",", "out_features", "=", "None", ",", "\n", "act_layer", "=", "nn", ".", "GELU", ",", "drop", "=", "0.", ",", "group", "=", "8", ",", "spatial_conv", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "out_features", "=", "out_features", "or", "in_features", "\n", "hidden_features", "=", "hidden_features", "or", "in_features", "\n", "drop_probs", "=", "to_2tuple", "(", "drop", ")", "\n", "\n", "self", ".", "in_features", "=", "in_features", "\n", "self", ".", "out_features", "=", "out_features", "\n", "self", ".", "spatial_conv", "=", "spatial_conv", "\n", "if", "self", ".", "spatial_conv", ":", "\n", "            ", "if", "group", "<", "2", ":", "# net setting", "\n", "                ", "hidden_features", "=", "in_features", "*", "5", "//", "6", "\n", "", "else", ":", "\n", "                ", "hidden_features", "=", "in_features", "*", "2", "\n", "", "", "self", ".", "hidden_features", "=", "hidden_features", "\n", "self", ".", "group", "=", "group", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "in_features", ",", "hidden_features", ",", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "False", ")", "\n", "self", ".", "act1", "=", "act_layer", "(", ")", "\n", "self", ".", "drop1", "=", "nn", ".", "Dropout", "(", "drop_probs", "[", "0", "]", ")", "\n", "if", "self", ".", "spatial_conv", ":", "\n", "            ", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "\n", "hidden_features", ",", "hidden_features", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ",", "groups", "=", "self", ".", "group", ",", "bias", "=", "False", ")", "\n", "self", ".", "act2", "=", "act_layer", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv2", "=", "None", "\n", "self", ".", "act2", "=", "None", "\n", "", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "hidden_features", ",", "out_features", ",", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "False", ")", "\n", "self", ".", "drop3", "=", "nn", ".", "Dropout", "(", "drop_probs", "[", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer.SpatialMlp.forward": [[75, 85], ["visformer.SpatialMlp.conv1", "visformer.SpatialMlp.act1", "visformer.SpatialMlp.drop1", "visformer.SpatialMlp.conv3", "visformer.SpatialMlp.drop3", "visformer.SpatialMlp.conv2", "visformer.SpatialMlp.act2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "act1", "(", "x", ")", "\n", "x", "=", "self", ".", "drop1", "(", "x", ")", "\n", "if", "self", ".", "conv2", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "x", "=", "self", ".", "act2", "(", "x", ")", "\n", "", "x", "=", "self", ".", "conv3", "(", "x", ")", "\n", "x", "=", "self", ".", "drop3", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer.Attention.__init__": [[88, 99], ["torch.Module.__init__", "round", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ",", "num_heads", "=", "8", ",", "head_dim_ratio", "=", "1.", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "head_dim", "=", "round", "(", "dim", "//", "num_heads", "*", "head_dim_ratio", ")", "\n", "self", ".", "head_dim", "=", "head_dim", "\n", "self", ".", "scale", "=", "head_dim", "**", "-", "0.5", "\n", "self", ".", "qkv", "=", "nn", ".", "Conv2d", "(", "dim", ",", "head_dim", "*", "num_heads", "*", "3", ",", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "False", ")", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Conv2d", "(", "self", ".", "head_dim", "*", "self", ".", "num_heads", ",", "dim", ",", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "False", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer.Attention.forward": [[100, 114], ["visformer.Attention.qkv().reshape().permute", "visformer.Attention.unbind", "visformer.Attention.softmax", "visformer.Attention.attn_drop", "visformer.Attention.permute().reshape", "visformer.Attention.proj", "visformer.Attention.proj_drop", "visformer.Attention.qkv().reshape", "k.transpose", "visformer.Attention.permute", "visformer.Attention.qkv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "x", "=", "self", ".", "qkv", "(", "x", ")", ".", "reshape", "(", "B", ",", "3", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", ",", "-", "1", ")", ".", "permute", "(", "1", ",", "0", ",", "2", ",", "4", ",", "3", ")", "\n", "q", ",", "k", ",", "v", "=", "x", ".", "unbind", "(", "0", ")", "\n", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "scale", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "x", "=", "attn", "@", "v", "\n", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ")", ".", "reshape", "(", "B", ",", "-", "1", ",", "H", ",", "W", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer.Block.__init__": [[117, 136], ["torch.Module.__init__", "norm_layer", "visformer.SpatialMlp", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "norm_layer", "visformer.Attention", "int"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "num_heads", ",", "head_dim_ratio", "=", "1.", ",", "mlp_ratio", "=", "4.", ",", "\n", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "LayerNorm2d", ",", "\n", "group", "=", "8", ",", "attn_disabled", "=", "False", ",", "spatial_conv", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "spatial_conv", "=", "spatial_conv", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "if", "attn_disabled", ":", "\n", "            ", "self", ".", "norm1", "=", "None", "\n", "self", ".", "attn", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "attn", "=", "Attention", "(", "\n", "dim", ",", "num_heads", "=", "num_heads", ",", "head_dim_ratio", "=", "head_dim_ratio", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ")", "\n", "\n", "", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "mlp", "=", "SpatialMlp", "(", "\n", "in_features", "=", "dim", ",", "hidden_features", "=", "int", "(", "dim", "*", "mlp_ratio", ")", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ",", "\n", "group", "=", "group", ",", "spatial_conv", "=", "spatial_conv", ")", "# new setting", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer.Block.forward": [[137, 142], ["visformer.Block.drop_path", "visformer.Block.drop_path", "visformer.Block.mlp", "visformer.Block.attn", "visformer.Block.norm2", "visformer.Block.norm1"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "attn", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "attn", "(", "self", ".", "norm1", "(", "x", ")", ")", ")", "\n", "", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer.Visformer.__init__": [[145, 255], ["torch.Module.__init__", "layers.to_2tuple", "isinstance", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "norm_layer", "layers.create_classifier", "visformer.Visformer.apply", "sum", "x.item", "layers.PatchEmbed", "torch.Dropout", "torch.Dropout", "torch.Dropout", "layers.PatchEmbed", "layers.PatchEmbed", "layers.trunc_normal_", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "layers.PatchEmbed", "torch.Sequential", "torch.Sequential", "torch.Sequential", "layers.PatchEmbed", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "layers.trunc_normal_", "layers.trunc_normal_", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "visformer.Block", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "visformer.Block", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "visformer.Block", "range", "range", "range"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["    ", "def", "__init__", "(", "\n", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "init_channels", "=", "32", ",", "embed_dim", "=", "384", ",", "\n", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "mlp_ratio", "=", "4.", ",", "drop_rate", "=", "0.", ",", "attn_drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ",", "\n", "norm_layer", "=", "LayerNorm2d", ",", "attn_stage", "=", "'111'", ",", "pos_embed", "=", "True", ",", "spatial_conv", "=", "'111'", ",", "\n", "vit_stem", "=", "False", ",", "group", "=", "8", ",", "global_pool", "=", "'avg'", ",", "conv_init", "=", "False", ",", "embed_norm", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "img_size", "=", "to_2tuple", "(", "img_size", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "init_channels", "=", "init_channels", "\n", "self", ".", "img_size", "=", "img_size", "\n", "self", ".", "vit_stem", "=", "vit_stem", "\n", "self", ".", "conv_init", "=", "conv_init", "\n", "if", "isinstance", "(", "depth", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "self", ".", "stage_num1", ",", "self", ".", "stage_num2", ",", "self", ".", "stage_num3", "=", "depth", "\n", "depth", "=", "sum", "(", "depth", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "stage_num1", "=", "self", ".", "stage_num3", "=", "depth", "//", "3", "\n", "self", ".", "stage_num2", "=", "depth", "-", "self", ".", "stage_num1", "-", "self", ".", "stage_num3", "\n", "", "self", ".", "pos_embed", "=", "pos_embed", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "dpr", "=", "[", "x", ".", "item", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "depth", ")", "]", "\n", "# stage 1", "\n", "if", "self", ".", "vit_stem", ":", "\n", "            ", "self", ".", "stem", "=", "None", "\n", "self", ".", "patch_embed1", "=", "PatchEmbed", "(", "\n", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", ",", "in_chans", "=", "in_chans", ",", "\n", "embed_dim", "=", "embed_dim", ",", "norm_layer", "=", "embed_norm", ",", "flatten", "=", "False", ")", "\n", "img_size", "=", "[", "x", "//", "patch_size", "for", "x", "in", "img_size", "]", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "init_channels", "is", "None", ":", "\n", "                ", "self", ".", "stem", "=", "None", "\n", "self", ".", "patch_embed1", "=", "PatchEmbed", "(", "\n", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", "//", "2", ",", "in_chans", "=", "in_chans", ",", "\n", "embed_dim", "=", "embed_dim", "//", "2", ",", "norm_layer", "=", "embed_norm", ",", "flatten", "=", "False", ")", "\n", "img_size", "=", "[", "x", "//", "(", "patch_size", "//", "2", ")", "for", "x", "in", "img_size", "]", "\n", "", "else", ":", "\n", "                ", "self", ".", "stem", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "in_chans", ",", "self", ".", "init_channels", ",", "7", ",", "stride", "=", "2", ",", "padding", "=", "3", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "self", ".", "init_channels", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", ")", "\n", "img_size", "=", "[", "x", "//", "2", "for", "x", "in", "img_size", "]", "\n", "self", ".", "patch_embed1", "=", "PatchEmbed", "(", "\n", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", "//", "4", ",", "in_chans", "=", "self", ".", "init_channels", ",", "\n", "embed_dim", "=", "embed_dim", "//", "2", ",", "norm_layer", "=", "embed_norm", ",", "flatten", "=", "False", ")", "\n", "img_size", "=", "[", "x", "//", "(", "patch_size", "//", "4", ")", "for", "x", "in", "img_size", "]", "\n", "\n", "", "", "if", "self", ".", "pos_embed", ":", "\n", "            ", "if", "self", ".", "vit_stem", ":", "\n", "                ", "self", ".", "pos_embed1", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "embed_dim", ",", "*", "img_size", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "pos_embed1", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "embed_dim", "//", "2", ",", "*", "img_size", ")", ")", "\n", "", "self", ".", "pos_drop", "=", "nn", ".", "Dropout", "(", "p", "=", "drop_rate", ")", "\n", "", "self", ".", "stage1", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "Block", "(", "\n", "dim", "=", "embed_dim", "//", "2", ",", "num_heads", "=", "num_heads", ",", "head_dim_ratio", "=", "0.5", ",", "mlp_ratio", "=", "mlp_ratio", ",", "\n", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "dpr", "[", "i", "]", ",", "norm_layer", "=", "norm_layer", ",", "\n", "group", "=", "group", ",", "attn_disabled", "=", "(", "attn_stage", "[", "0", "]", "==", "'0'", ")", ",", "spatial_conv", "=", "(", "spatial_conv", "[", "0", "]", "==", "'1'", ")", "\n", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "stage_num1", ")", "\n", "]", ")", "\n", "\n", "# stage2", "\n", "if", "not", "self", ".", "vit_stem", ":", "\n", "            ", "self", ".", "patch_embed2", "=", "PatchEmbed", "(", "\n", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", "//", "8", ",", "in_chans", "=", "embed_dim", "//", "2", ",", "\n", "embed_dim", "=", "embed_dim", ",", "norm_layer", "=", "embed_norm", ",", "flatten", "=", "False", ")", "\n", "img_size", "=", "[", "x", "//", "(", "patch_size", "//", "8", ")", "for", "x", "in", "img_size", "]", "\n", "if", "self", ".", "pos_embed", ":", "\n", "                ", "self", ".", "pos_embed2", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "embed_dim", ",", "*", "img_size", ")", ")", "\n", "", "", "self", ".", "stage2", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "Block", "(", "\n", "dim", "=", "embed_dim", ",", "num_heads", "=", "num_heads", ",", "head_dim_ratio", "=", "1.0", ",", "mlp_ratio", "=", "mlp_ratio", ",", "\n", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "dpr", "[", "i", "]", ",", "norm_layer", "=", "norm_layer", ",", "\n", "group", "=", "group", ",", "attn_disabled", "=", "(", "attn_stage", "[", "1", "]", "==", "'0'", ")", ",", "spatial_conv", "=", "(", "spatial_conv", "[", "1", "]", "==", "'1'", ")", "\n", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "stage_num1", ",", "self", ".", "stage_num1", "+", "self", ".", "stage_num2", ")", "\n", "]", ")", "\n", "\n", "# stage 3", "\n", "if", "not", "self", ".", "vit_stem", ":", "\n", "            ", "self", ".", "patch_embed3", "=", "PatchEmbed", "(", "\n", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", "//", "8", ",", "in_chans", "=", "embed_dim", ",", "\n", "embed_dim", "=", "embed_dim", "*", "2", ",", "norm_layer", "=", "embed_norm", ",", "flatten", "=", "False", ")", "\n", "img_size", "=", "[", "x", "//", "(", "patch_size", "//", "8", ")", "for", "x", "in", "img_size", "]", "\n", "if", "self", ".", "pos_embed", ":", "\n", "                ", "self", ".", "pos_embed3", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "embed_dim", "*", "2", ",", "*", "img_size", ")", ")", "\n", "", "", "self", ".", "stage3", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "Block", "(", "\n", "dim", "=", "embed_dim", "*", "2", ",", "num_heads", "=", "num_heads", ",", "head_dim_ratio", "=", "1.0", ",", "mlp_ratio", "=", "mlp_ratio", ",", "\n", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "dpr", "[", "i", "]", ",", "norm_layer", "=", "norm_layer", ",", "\n", "group", "=", "group", ",", "attn_disabled", "=", "(", "attn_stage", "[", "2", "]", "==", "'0'", ")", ",", "spatial_conv", "=", "(", "spatial_conv", "[", "2", "]", "==", "'1'", ")", "\n", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "stage_num1", "+", "self", ".", "stage_num2", ",", "depth", ")", "\n", "]", ")", "\n", "\n", "# head", "\n", "self", ".", "num_features", "=", "embed_dim", "if", "self", ".", "vit_stem", "else", "embed_dim", "*", "2", "\n", "self", ".", "norm", "=", "norm_layer", "(", "self", ".", "num_features", ")", "\n", "self", ".", "global_pool", ",", "self", ".", "head", "=", "create_classifier", "(", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n", "# weights init", "\n", "if", "self", ".", "pos_embed", ":", "\n", "            ", "trunc_normal_", "(", "self", ".", "pos_embed1", ",", "std", "=", "0.02", ")", "\n", "if", "not", "self", ".", "vit_stem", ":", "\n", "                ", "trunc_normal_", "(", "self", ".", "pos_embed2", ",", "std", "=", "0.02", ")", "\n", "trunc_normal_", "(", "self", ".", "pos_embed3", ",", "std", "=", "0.02", ")", "\n", "", "", "self", ".", "apply", "(", "self", ".", "_init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer.Visformer._init_weights": [[256, 268], ["isinstance", "layers.trunc_normal_", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "layers.trunc_normal_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["", "def", "_init_weights", "(", "self", ",", "m", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "            ", "trunc_normal_", "(", "m", ".", "weight", ",", "std", "=", "0.02", ")", "\n", "if", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "            ", "if", "self", ".", "conv_init", ":", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "", "else", ":", "\n", "                ", "trunc_normal_", "(", "m", ".", "weight", ",", "std", "=", "0.02", ")", "\n", "", "if", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0.", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer.Visformer.group_matcher": [[269, 277], ["dict"], "methods", ["None"], ["", "", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^patch_embed1|pos_embed1|stem'", ",", "# stem and embed", "\n", "blocks", "=", "[", "\n", "(", "r'^stage(\\d+)\\.(\\d+)'", "if", "coarse", "else", "r'^stage(\\d+)\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^(?:patch_embed|pos_embed)(\\d+)'", ",", "(", "0", ",", ")", ")", ",", "\n", "(", "r'^norm'", ",", "(", "99999", ",", ")", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer.Visformer.set_grad_checkpointing": [[280, 283], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer.Visformer.get_classifier": [[284, 287], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer.Visformer.reset_classifier": [[288, 291], ["layers.create_classifier"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", ",", "self", ".", "head", "=", "create_classifier", "(", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer.Visformer.forward_features": [[292, 327], ["visformer.Visformer.patch_embed1", "visformer.Visformer.norm", "visformer.Visformer.stem", "visformer.Visformer.pos_drop", "helpers.checkpoint_seq", "visformer.Visformer.stage1", "visformer.Visformer.patch_embed2", "helpers.checkpoint_seq", "visformer.Visformer.stage2", "visformer.Visformer.patch_embed3", "helpers.checkpoint_seq", "visformer.Visformer.stage3", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "visformer.Visformer.pos_drop", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "visformer.Visformer.pos_drop", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "stem", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "stem", "(", "x", ")", "\n", "\n", "# stage 1", "\n", "", "x", "=", "self", ".", "patch_embed1", "(", "x", ")", "\n", "if", "self", ".", "pos_embed", ":", "\n", "            ", "x", "=", "self", ".", "pos_drop", "(", "x", "+", "self", ".", "pos_embed1", ")", "\n", "", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "stage1", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "stage1", "(", "x", ")", "\n", "\n", "# stage 2", "\n", "", "if", "not", "self", ".", "vit_stem", ":", "\n", "            ", "x", "=", "self", ".", "patch_embed2", "(", "x", ")", "\n", "if", "self", ".", "pos_embed", ":", "\n", "                ", "x", "=", "self", ".", "pos_drop", "(", "x", "+", "self", ".", "pos_embed2", ")", "\n", "", "", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "stage2", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "stage2", "(", "x", ")", "\n", "\n", "# stage3", "\n", "", "if", "not", "self", ".", "vit_stem", ":", "\n", "            ", "x", "=", "self", ".", "patch_embed3", "(", "x", ")", "\n", "if", "self", ".", "pos_embed", ":", "\n", "                ", "x", "=", "self", ".", "pos_drop", "(", "x", "+", "self", ".", "pos_embed3", ")", "\n", "", "", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "stage3", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "stage3", "(", "x", ")", "\n", "\n", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer.Visformer.forward_head": [[328, 331], ["visformer.Visformer.global_pool", "visformer.Visformer.head"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "return", "x", "if", "pre_logits", "else", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer.Visformer.forward": [[332, 336], ["visformer.Visformer.forward_features", "visformer.Visformer.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer._cfg": [[24, 32], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", ".9", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'stem.0'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer._create_visformer": [[338, 343], ["kwargs.get", "helpers.build_model_with_cfg", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_visformer", "(", "variant", ",", "pretrained", "=", "False", ",", "default_cfg", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "kwargs", ".", "get", "(", "'features_only'", ",", "None", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'features_only not implemented for Vision Transformer models.'", ")", "\n", "", "model", "=", "build_model_with_cfg", "(", "Visformer", ",", "variant", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer.visformer_tiny": [[345, 353], ["dict", "visformer._create_visformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer._create_visformer"], ["", "@", "register_model", "\n", "def", "visformer_tiny", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_cfg", "=", "dict", "(", "\n", "init_channels", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "(", "7", ",", "4", ",", "4", ")", ",", "num_heads", "=", "3", ",", "mlp_ratio", "=", "4.", ",", "group", "=", "8", ",", "\n", "attn_stage", "=", "'011'", ",", "spatial_conv", "=", "'100'", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "conv_init", "=", "True", ",", "\n", "embed_norm", "=", "nn", ".", "BatchNorm2d", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_visformer", "(", "'visformer_tiny'", ",", "pretrained", "=", "pretrained", ",", "**", "model_cfg", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer.visformer_small": [[355, 363], ["dict", "visformer._create_visformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.visformer._create_visformer"], ["", "@", "register_model", "\n", "def", "visformer_small", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_cfg", "=", "dict", "(", "\n", "init_channels", "=", "32", ",", "embed_dim", "=", "384", ",", "depth", "=", "(", "7", ",", "4", ",", "4", ")", ",", "num_heads", "=", "6", ",", "mlp_ratio", "=", "4.", ",", "group", "=", "8", ",", "\n", "attn_stage", "=", "'011'", ",", "spatial_conv", "=", "'100'", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "conv_init", "=", "True", ",", "\n", "embed_norm", "=", "nn", ".", "BatchNorm2d", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_visformer", "(", "'visformer_small'", ",", "pretrained", "=", "pretrained", ",", "**", "model_cfg", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.SeparableConv2d.__init__": [[40, 47], ["torch.Module.__init__", "layers.create_conv2d", "layers.create_conv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", ",", "padding", "=", "''", ")", ":", "\n", "        ", "super", "(", "SeparableConv2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "depthwise_conv2d", "=", "create_conv2d", "(", "\n", "in_channels", ",", "in_channels", ",", "kernel_size", "=", "kernel_size", ",", "\n", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "groups", "=", "in_channels", ")", "\n", "self", ".", "pointwise_conv2d", "=", "create_conv2d", "(", "\n", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "1", ",", "padding", "=", "padding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.SeparableConv2d.forward": [[48, 52], ["pnasnet.SeparableConv2d.depthwise_conv2d", "pnasnet.SeparableConv2d.pointwise_conv2d"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "depthwise_conv2d", "(", "x", ")", "\n", "x", "=", "self", ".", "pointwise_conv2d", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.BranchSeparables.__init__": [[56, 67], ["torch.Module.__init__", "torch.ReLU", "torch.ReLU", "torch.ReLU", "pnasnet.SeparableConv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "pnasnet.SeparableConv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "stem_cell", "=", "False", ",", "padding", "=", "''", ")", ":", "\n", "        ", "super", "(", "BranchSeparables", ",", "self", ")", ".", "__init__", "(", ")", "\n", "middle_channels", "=", "out_channels", "if", "stem_cell", "else", "in_channels", "\n", "self", ".", "act_1", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "separable_1", "=", "SeparableConv2d", "(", "\n", "in_channels", ",", "middle_channels", ",", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ")", "\n", "self", ".", "bn_sep_1", "=", "nn", ".", "BatchNorm2d", "(", "middle_channels", ",", "eps", "=", "0.001", ")", "\n", "self", ".", "act_2", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "separable_2", "=", "SeparableConv2d", "(", "\n", "middle_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "padding", "=", "padding", ")", "\n", "self", ".", "bn_sep_2", "=", "nn", ".", "BatchNorm2d", "(", "out_channels", ",", "eps", "=", "0.001", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.BranchSeparables.forward": [[68, 76], ["pnasnet.BranchSeparables.act_1", "pnasnet.BranchSeparables.separable_1", "pnasnet.BranchSeparables.bn_sep_1", "pnasnet.BranchSeparables.act_2", "pnasnet.BranchSeparables.separable_2", "pnasnet.BranchSeparables.bn_sep_2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "act_1", "(", "x", ")", "\n", "x", "=", "self", ".", "separable_1", "(", "x", ")", "\n", "x", "=", "self", ".", "bn_sep_1", "(", "x", ")", "\n", "x", "=", "self", ".", "act_2", "(", "x", ")", "\n", "x", "=", "self", ".", "separable_2", "(", "x", ")", "\n", "x", "=", "self", ".", "bn_sep_2", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.ActConvBn.__init__": [[80, 86], ["torch.Module.__init__", "torch.ReLU", "torch.ReLU", "torch.ReLU", "layers.create_conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "padding", "=", "''", ")", ":", "\n", "        ", "super", "(", "ActConvBn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "act", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "conv", "=", "create_conv2d", "(", "\n", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ")", "\n", "self", ".", "bn", "=", "nn", ".", "BatchNorm2d", "(", "out_channels", ",", "eps", "=", "0.001", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.ActConvBn.forward": [[87, 92], ["pnasnet.ActConvBn.act", "pnasnet.ActConvBn.conv", "pnasnet.ActConvBn.bn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "x", "=", "self", ".", "bn", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.FactorizedReduction.__init__": [[96, 109], ["torch.Module.__init__", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "collections.OrderedDict", "collections.OrderedDict", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "layers.create_conv2d", "torch.ZeroPad2d", "torch.ZeroPad2d", "torch.ZeroPad2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "layers.create_conv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "padding", "=", "''", ")", ":", "\n", "        ", "super", "(", "FactorizedReduction", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "act", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "path_1", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "'avgpool'", ",", "nn", ".", "AvgPool2d", "(", "1", ",", "stride", "=", "2", ",", "count_include_pad", "=", "False", ")", ")", ",", "\n", "(", "'conv'", ",", "create_conv2d", "(", "in_channels", ",", "out_channels", "//", "2", ",", "kernel_size", "=", "1", ",", "padding", "=", "padding", ")", ")", ",", "\n", "]", ")", ")", "\n", "self", ".", "path_2", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "'pad'", ",", "nn", ".", "ZeroPad2d", "(", "(", "-", "1", ",", "1", ",", "-", "1", ",", "1", ")", ")", ")", ",", "# shift", "\n", "(", "'avgpool'", ",", "nn", ".", "AvgPool2d", "(", "1", ",", "stride", "=", "2", ",", "count_include_pad", "=", "False", ")", ")", ",", "\n", "(", "'conv'", ",", "create_conv2d", "(", "in_channels", ",", "out_channels", "//", "2", ",", "kernel_size", "=", "1", ",", "padding", "=", "padding", ")", ")", ",", "\n", "]", ")", ")", "\n", "self", ".", "final_path_bn", "=", "nn", ".", "BatchNorm2d", "(", "out_channels", ",", "eps", "=", "0.001", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.FactorizedReduction.forward": [[110, 116], ["pnasnet.FactorizedReduction.act", "pnasnet.FactorizedReduction.path_1", "pnasnet.FactorizedReduction.path_2", "pnasnet.FactorizedReduction.final_path_bn", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "x_path1", "=", "self", ".", "path_1", "(", "x", ")", "\n", "x_path2", "=", "self", ".", "path_2", "(", "x", ")", "\n", "out", "=", "self", ".", "final_path_bn", "(", "torch", ".", "cat", "(", "[", "x_path1", ",", "x_path2", "]", ",", "1", ")", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.CellBase.cell_forward": [[120, 146], ["pnasnet.CellBase.comb_iter_0_left", "pnasnet.CellBase.comb_iter_0_right", "pnasnet.CellBase.comb_iter_1_left", "pnasnet.CellBase.comb_iter_1_right", "pnasnet.CellBase.comb_iter_2_left", "pnasnet.CellBase.comb_iter_2_right", "pnasnet.CellBase.comb_iter_3_left", "pnasnet.CellBase.comb_iter_3_right", "pnasnet.CellBase.comb_iter_4_left", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "pnasnet.CellBase.comb_iter_4_right"], "methods", ["None"], ["    ", "def", "cell_forward", "(", "self", ",", "x_left", ",", "x_right", ")", ":", "\n", "        ", "x_comb_iter_0_left", "=", "self", ".", "comb_iter_0_left", "(", "x_left", ")", "\n", "x_comb_iter_0_right", "=", "self", ".", "comb_iter_0_right", "(", "x_left", ")", "\n", "x_comb_iter_0", "=", "x_comb_iter_0_left", "+", "x_comb_iter_0_right", "\n", "\n", "x_comb_iter_1_left", "=", "self", ".", "comb_iter_1_left", "(", "x_right", ")", "\n", "x_comb_iter_1_right", "=", "self", ".", "comb_iter_1_right", "(", "x_right", ")", "\n", "x_comb_iter_1", "=", "x_comb_iter_1_left", "+", "x_comb_iter_1_right", "\n", "\n", "x_comb_iter_2_left", "=", "self", ".", "comb_iter_2_left", "(", "x_right", ")", "\n", "x_comb_iter_2_right", "=", "self", ".", "comb_iter_2_right", "(", "x_right", ")", "\n", "x_comb_iter_2", "=", "x_comb_iter_2_left", "+", "x_comb_iter_2_right", "\n", "\n", "x_comb_iter_3_left", "=", "self", ".", "comb_iter_3_left", "(", "x_comb_iter_2", ")", "\n", "x_comb_iter_3_right", "=", "self", ".", "comb_iter_3_right", "(", "x_right", ")", "\n", "x_comb_iter_3", "=", "x_comb_iter_3_left", "+", "x_comb_iter_3_right", "\n", "\n", "x_comb_iter_4_left", "=", "self", ".", "comb_iter_4_left", "(", "x_left", ")", "\n", "if", "self", ".", "comb_iter_4_right", "is", "not", "None", ":", "\n", "            ", "x_comb_iter_4_right", "=", "self", ".", "comb_iter_4_right", "(", "x_right", ")", "\n", "", "else", ":", "\n", "            ", "x_comb_iter_4_right", "=", "x_right", "\n", "", "x_comb_iter_4", "=", "x_comb_iter_4_left", "+", "x_comb_iter_4_right", "\n", "\n", "x_out", "=", "torch", ".", "cat", "(", "[", "x_comb_iter_0", ",", "x_comb_iter_1", ",", "x_comb_iter_2", ",", "x_comb_iter_3", ",", "x_comb_iter_4", "]", ",", "1", ")", "\n", "return", "x_out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.CellStem0.__init__": [[150, 179], ["torch.Module.__init__", "pnasnet.ActConvBn", "pnasnet.BranchSeparables", "torch.Sequential", "torch.Sequential", "torch.Sequential", "pnasnet.BranchSeparables", "layers.create_pool2d", "pnasnet.BranchSeparables", "pnasnet.BranchSeparables", "pnasnet.BranchSeparables", "layers.create_pool2d", "pnasnet.BranchSeparables", "pnasnet.ActConvBn", "collections.OrderedDict", "layers.create_pool2d", "layers.create_conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d"], ["    ", "def", "__init__", "(", "self", ",", "in_chs_left", ",", "out_chs_left", ",", "in_chs_right", ",", "out_chs_right", ",", "pad_type", "=", "''", ")", ":", "\n", "        ", "super", "(", "CellStem0", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv_1x1", "=", "ActConvBn", "(", "in_chs_right", ",", "out_chs_right", ",", "kernel_size", "=", "1", ",", "padding", "=", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_0_left", "=", "BranchSeparables", "(", "\n", "in_chs_left", ",", "out_chs_left", ",", "kernel_size", "=", "5", ",", "stride", "=", "2", ",", "stem_cell", "=", "True", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "comb_iter_0_right", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "'max_pool'", ",", "create_pool2d", "(", "'max'", ",", "3", ",", "stride", "=", "2", ",", "padding", "=", "pad_type", ")", ")", ",", "\n", "(", "'conv'", ",", "create_conv2d", "(", "in_chs_left", ",", "out_chs_left", ",", "kernel_size", "=", "1", ",", "padding", "=", "pad_type", ")", ")", ",", "\n", "(", "'bn'", ",", "nn", ".", "BatchNorm2d", "(", "out_chs_left", ",", "eps", "=", "0.001", ")", ")", ",", "\n", "]", ")", ")", "\n", "\n", "self", ".", "comb_iter_1_left", "=", "BranchSeparables", "(", "\n", "out_chs_right", ",", "out_chs_right", ",", "kernel_size", "=", "7", ",", "stride", "=", "2", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "comb_iter_1_right", "=", "create_pool2d", "(", "'max'", ",", "3", ",", "stride", "=", "2", ",", "padding", "=", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_2_left", "=", "BranchSeparables", "(", "\n", "out_chs_right", ",", "out_chs_right", ",", "kernel_size", "=", "5", ",", "stride", "=", "2", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "comb_iter_2_right", "=", "BranchSeparables", "(", "\n", "out_chs_right", ",", "out_chs_right", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_3_left", "=", "BranchSeparables", "(", "\n", "out_chs_right", ",", "out_chs_right", ",", "kernel_size", "=", "3", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "comb_iter_3_right", "=", "create_pool2d", "(", "'max'", ",", "3", ",", "stride", "=", "2", ",", "padding", "=", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_4_left", "=", "BranchSeparables", "(", "\n", "in_chs_right", ",", "out_chs_right", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "stem_cell", "=", "True", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "comb_iter_4_right", "=", "ActConvBn", "(", "\n", "out_chs_right", ",", "out_chs_right", ",", "kernel_size", "=", "1", ",", "stride", "=", "2", ",", "padding", "=", "pad_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.CellStem0.forward": [[180, 184], ["pnasnet.CellStem0.conv_1x1", "pnasnet.CellStem0.cell_forward"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.CellBase.cell_forward"], ["", "def", "forward", "(", "self", ",", "x_left", ")", ":", "\n", "        ", "x_right", "=", "self", ".", "conv_1x1", "(", "x_left", ")", "\n", "x_out", "=", "self", ".", "cell_forward", "(", "x_left", ",", "x_right", ")", "\n", "return", "x_out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.Cell.__init__": [[188, 230], ["torch.Module.__init__", "pnasnet.ActConvBn", "pnasnet.BranchSeparables", "layers.create_pool2d", "pnasnet.BranchSeparables", "layers.create_pool2d", "pnasnet.BranchSeparables", "pnasnet.BranchSeparables", "pnasnet.BranchSeparables", "layers.create_pool2d", "pnasnet.BranchSeparables", "pnasnet.FactorizedReduction", "pnasnet.ActConvBn", "pnasnet.ActConvBn"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d"], ["    ", "def", "__init__", "(", "self", ",", "in_chs_left", ",", "out_chs_left", ",", "in_chs_right", ",", "out_chs_right", ",", "pad_type", "=", "''", ",", "\n", "is_reduction", "=", "False", ",", "match_prev_layer_dims", "=", "False", ")", ":", "\n", "        ", "super", "(", "Cell", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# If `is_reduction` is set to `True` stride 2 is used for", "\n", "# convolution and pooling layers to reduce the spatial size of", "\n", "# the output of a cell approximately by a factor of 2.", "\n", "stride", "=", "2", "if", "is_reduction", "else", "1", "\n", "\n", "# If `match_prev_layer_dimensions` is set to `True`", "\n", "# `FactorizedReduction` is used to reduce the spatial size", "\n", "# of the left input of a cell approximately by a factor of 2.", "\n", "self", ".", "match_prev_layer_dimensions", "=", "match_prev_layer_dims", "\n", "if", "match_prev_layer_dims", ":", "\n", "            ", "self", ".", "conv_prev_1x1", "=", "FactorizedReduction", "(", "in_chs_left", ",", "out_chs_left", ",", "padding", "=", "pad_type", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv_prev_1x1", "=", "ActConvBn", "(", "in_chs_left", ",", "out_chs_left", ",", "kernel_size", "=", "1", ",", "padding", "=", "pad_type", ")", "\n", "", "self", ".", "conv_1x1", "=", "ActConvBn", "(", "in_chs_right", ",", "out_chs_right", ",", "kernel_size", "=", "1", ",", "padding", "=", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_0_left", "=", "BranchSeparables", "(", "\n", "out_chs_left", ",", "out_chs_left", ",", "kernel_size", "=", "5", ",", "stride", "=", "stride", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "comb_iter_0_right", "=", "create_pool2d", "(", "'max'", ",", "3", ",", "stride", "=", "stride", ",", "padding", "=", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_1_left", "=", "BranchSeparables", "(", "\n", "out_chs_right", ",", "out_chs_right", ",", "kernel_size", "=", "7", ",", "stride", "=", "stride", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "comb_iter_1_right", "=", "create_pool2d", "(", "'max'", ",", "3", ",", "stride", "=", "stride", ",", "padding", "=", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_2_left", "=", "BranchSeparables", "(", "\n", "out_chs_right", ",", "out_chs_right", ",", "kernel_size", "=", "5", ",", "stride", "=", "stride", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "comb_iter_2_right", "=", "BranchSeparables", "(", "\n", "out_chs_right", ",", "out_chs_right", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "padding", "=", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_3_left", "=", "BranchSeparables", "(", "out_chs_right", ",", "out_chs_right", ",", "kernel_size", "=", "3", ")", "\n", "self", ".", "comb_iter_3_right", "=", "create_pool2d", "(", "'max'", ",", "3", ",", "stride", "=", "stride", ",", "padding", "=", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_4_left", "=", "BranchSeparables", "(", "\n", "out_chs_left", ",", "out_chs_left", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "padding", "=", "pad_type", ")", "\n", "if", "is_reduction", ":", "\n", "            ", "self", ".", "comb_iter_4_right", "=", "ActConvBn", "(", "\n", "out_chs_right", ",", "out_chs_right", ",", "kernel_size", "=", "1", ",", "stride", "=", "stride", ",", "padding", "=", "pad_type", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "comb_iter_4_right", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.Cell.forward": [[231, 236], ["pnasnet.Cell.conv_prev_1x1", "pnasnet.Cell.conv_1x1", "pnasnet.Cell.cell_forward"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.CellBase.cell_forward"], ["", "", "def", "forward", "(", "self", ",", "x_left", ",", "x_right", ")", ":", "\n", "        ", "x_left", "=", "self", ".", "conv_prev_1x1", "(", "x_left", ")", "\n", "x_right", "=", "self", ".", "conv_1x1", "(", "x_right", ")", "\n", "x_out", "=", "self", ".", "cell_forward", "(", "x_left", ",", "x_right", ")", "\n", "return", "x_out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.PNASNet5Large.__init__": [[239, 298], ["torch.Module.__init__", "layers.ConvNormAct", "pnasnet.CellStem0", "pnasnet.Cell", "pnasnet.Cell", "pnasnet.Cell", "pnasnet.Cell", "pnasnet.Cell", "pnasnet.Cell", "pnasnet.Cell", "pnasnet.Cell", "pnasnet.Cell", "pnasnet.Cell", "pnasnet.Cell", "pnasnet.Cell", "pnasnet.Cell", "torch.ReLU", "torch.ReLU", "torch.ReLU", "layers.create_classifier", "dict", "dict", "dict", "dict", "dict", "functools.partial"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["    ", "def", "__init__", "(", "self", ",", "num_classes", "=", "1000", ",", "in_chans", "=", "3", ",", "output_stride", "=", "32", ",", "drop_rate", "=", "0.", ",", "global_pool", "=", "'avg'", ",", "pad_type", "=", "''", ")", ":", "\n", "        ", "super", "(", "PNASNet5Large", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "self", ".", "num_features", "=", "4320", "\n", "assert", "output_stride", "==", "32", "\n", "\n", "self", ".", "conv_0", "=", "ConvNormAct", "(", "\n", "in_chans", ",", "96", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "0", ",", "\n", "norm_layer", "=", "partial", "(", "nn", ".", "BatchNorm2d", ",", "eps", "=", "0.001", ",", "momentum", "=", "0.1", ")", ",", "apply_act", "=", "False", ")", "\n", "\n", "self", ".", "cell_stem_0", "=", "CellStem0", "(", "\n", "in_chs_left", "=", "96", ",", "out_chs_left", "=", "54", ",", "in_chs_right", "=", "96", ",", "out_chs_right", "=", "54", ",", "pad_type", "=", "pad_type", ")", "\n", "\n", "self", ".", "cell_stem_1", "=", "Cell", "(", "\n", "in_chs_left", "=", "96", ",", "out_chs_left", "=", "108", ",", "in_chs_right", "=", "270", ",", "out_chs_right", "=", "108", ",", "pad_type", "=", "pad_type", ",", "\n", "match_prev_layer_dims", "=", "True", ",", "is_reduction", "=", "True", ")", "\n", "self", ".", "cell_0", "=", "Cell", "(", "\n", "in_chs_left", "=", "270", ",", "out_chs_left", "=", "216", ",", "in_chs_right", "=", "540", ",", "out_chs_right", "=", "216", ",", "pad_type", "=", "pad_type", ",", "\n", "match_prev_layer_dims", "=", "True", ")", "\n", "self", ".", "cell_1", "=", "Cell", "(", "\n", "in_chs_left", "=", "540", ",", "out_chs_left", "=", "216", ",", "in_chs_right", "=", "1080", ",", "out_chs_right", "=", "216", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_2", "=", "Cell", "(", "\n", "in_chs_left", "=", "1080", ",", "out_chs_left", "=", "216", ",", "in_chs_right", "=", "1080", ",", "out_chs_right", "=", "216", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_3", "=", "Cell", "(", "\n", "in_chs_left", "=", "1080", ",", "out_chs_left", "=", "216", ",", "in_chs_right", "=", "1080", ",", "out_chs_right", "=", "216", ",", "pad_type", "=", "pad_type", ")", "\n", "\n", "self", ".", "cell_4", "=", "Cell", "(", "\n", "in_chs_left", "=", "1080", ",", "out_chs_left", "=", "432", ",", "in_chs_right", "=", "1080", ",", "out_chs_right", "=", "432", ",", "pad_type", "=", "pad_type", ",", "\n", "is_reduction", "=", "True", ")", "\n", "self", ".", "cell_5", "=", "Cell", "(", "\n", "in_chs_left", "=", "1080", ",", "out_chs_left", "=", "432", ",", "in_chs_right", "=", "2160", ",", "out_chs_right", "=", "432", ",", "pad_type", "=", "pad_type", ",", "\n", "match_prev_layer_dims", "=", "True", ")", "\n", "self", ".", "cell_6", "=", "Cell", "(", "\n", "in_chs_left", "=", "2160", ",", "out_chs_left", "=", "432", ",", "in_chs_right", "=", "2160", ",", "out_chs_right", "=", "432", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_7", "=", "Cell", "(", "\n", "in_chs_left", "=", "2160", ",", "out_chs_left", "=", "432", ",", "in_chs_right", "=", "2160", ",", "out_chs_right", "=", "432", ",", "pad_type", "=", "pad_type", ")", "\n", "\n", "self", ".", "cell_8", "=", "Cell", "(", "\n", "in_chs_left", "=", "2160", ",", "out_chs_left", "=", "864", ",", "in_chs_right", "=", "2160", ",", "out_chs_right", "=", "864", ",", "pad_type", "=", "pad_type", ",", "\n", "is_reduction", "=", "True", ")", "\n", "self", ".", "cell_9", "=", "Cell", "(", "\n", "in_chs_left", "=", "2160", ",", "out_chs_left", "=", "864", ",", "in_chs_right", "=", "4320", ",", "out_chs_right", "=", "864", ",", "pad_type", "=", "pad_type", ",", "\n", "match_prev_layer_dims", "=", "True", ")", "\n", "self", ".", "cell_10", "=", "Cell", "(", "\n", "in_chs_left", "=", "4320", ",", "out_chs_left", "=", "864", ",", "in_chs_right", "=", "4320", ",", "out_chs_right", "=", "864", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_11", "=", "Cell", "(", "\n", "in_chs_left", "=", "4320", ",", "out_chs_left", "=", "864", ",", "in_chs_right", "=", "4320", ",", "out_chs_right", "=", "864", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "act", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "feature_info", "=", "[", "\n", "dict", "(", "num_chs", "=", "96", ",", "reduction", "=", "2", ",", "module", "=", "'conv_0'", ")", ",", "\n", "dict", "(", "num_chs", "=", "270", ",", "reduction", "=", "4", ",", "module", "=", "'cell_stem_1.conv_1x1.act'", ")", ",", "\n", "dict", "(", "num_chs", "=", "1080", ",", "reduction", "=", "8", ",", "module", "=", "'cell_4.conv_1x1.act'", ")", ",", "\n", "dict", "(", "num_chs", "=", "2160", ",", "reduction", "=", "16", ",", "module", "=", "'cell_8.conv_1x1.act'", ")", ",", "\n", "dict", "(", "num_chs", "=", "4320", ",", "reduction", "=", "32", ",", "module", "=", "'act'", ")", ",", "\n", "]", "\n", "\n", "self", ".", "global_pool", ",", "self", ".", "last_linear", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.PNASNet5Large.group_matcher": [[299, 302], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "stem", "=", "r'^conv_0|cell_stem_[01]'", ",", "blocks", "=", "r'^cell_(\\d+)'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.PNASNet5Large.set_grad_checkpointing": [[303, 306], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "'gradient checkpointing not supported'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.PNASNet5Large.get_classifier": [[307, 310], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "last_linear", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.PNASNet5Large.reset_classifier": [[311, 315], ["layers.create_classifier"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", ",", "self", ".", "last_linear", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.PNASNet5Large.forward_features": [[316, 334], ["pnasnet.PNASNet5Large.conv_0", "pnasnet.PNASNet5Large.cell_stem_0", "pnasnet.PNASNet5Large.cell_stem_1", "pnasnet.PNASNet5Large.cell_0", "pnasnet.PNASNet5Large.cell_1", "pnasnet.PNASNet5Large.cell_2", "pnasnet.PNASNet5Large.cell_3", "pnasnet.PNASNet5Large.cell_4", "pnasnet.PNASNet5Large.cell_5", "pnasnet.PNASNet5Large.cell_6", "pnasnet.PNASNet5Large.cell_7", "pnasnet.PNASNet5Large.cell_8", "pnasnet.PNASNet5Large.cell_9", "pnasnet.PNASNet5Large.cell_10", "pnasnet.PNASNet5Large.cell_11", "pnasnet.PNASNet5Large.act"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_conv_0", "=", "self", ".", "conv_0", "(", "x", ")", "\n", "x_stem_0", "=", "self", ".", "cell_stem_0", "(", "x_conv_0", ")", "\n", "x_stem_1", "=", "self", ".", "cell_stem_1", "(", "x_conv_0", ",", "x_stem_0", ")", "\n", "x_cell_0", "=", "self", ".", "cell_0", "(", "x_stem_0", ",", "x_stem_1", ")", "\n", "x_cell_1", "=", "self", ".", "cell_1", "(", "x_stem_1", ",", "x_cell_0", ")", "\n", "x_cell_2", "=", "self", ".", "cell_2", "(", "x_cell_0", ",", "x_cell_1", ")", "\n", "x_cell_3", "=", "self", ".", "cell_3", "(", "x_cell_1", ",", "x_cell_2", ")", "\n", "x_cell_4", "=", "self", ".", "cell_4", "(", "x_cell_2", ",", "x_cell_3", ")", "\n", "x_cell_5", "=", "self", ".", "cell_5", "(", "x_cell_3", ",", "x_cell_4", ")", "\n", "x_cell_6", "=", "self", ".", "cell_6", "(", "x_cell_4", ",", "x_cell_5", ")", "\n", "x_cell_7", "=", "self", ".", "cell_7", "(", "x_cell_5", ",", "x_cell_6", ")", "\n", "x_cell_8", "=", "self", ".", "cell_8", "(", "x_cell_6", ",", "x_cell_7", ")", "\n", "x_cell_9", "=", "self", ".", "cell_9", "(", "x_cell_7", ",", "x_cell_8", ")", "\n", "x_cell_10", "=", "self", ".", "cell_10", "(", "x_cell_8", ",", "x_cell_9", ")", "\n", "x_cell_11", "=", "self", ".", "cell_11", "(", "x_cell_9", ",", "x_cell_10", ")", "\n", "x", "=", "self", ".", "act", "(", "x_cell_11", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.PNASNet5Large.forward_head": [[335, 340], ["pnasnet.PNASNet5Large.global_pool", "torch.dropout", "torch.dropout", "torch.dropout", "pnasnet.PNASNet5Large.last_linear"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "if", "self", ".", "drop_rate", ">", "0", ":", "\n", "            ", "x", "=", "F", ".", "dropout", "(", "x", ",", "self", ".", "drop_rate", ",", "training", "=", "self", ".", "training", ")", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "last_linear", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.PNASNet5Large.forward": [[341, 345], ["pnasnet.PNASNet5Large.forward_features", "pnasnet.PNASNet5Large.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet._create_pnasnet": [[347, 352], ["helpers.build_model_with_cfg", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_pnasnet", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "\n", "PNASNet5Large", ",", "variant", ",", "pretrained", ",", "\n", "feature_cfg", "=", "dict", "(", "feature_cls", "=", "'hook'", ",", "no_rewrite", "=", "True", ")", ",", "# not possible to re-write this model", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet.pnasnet5large": [[354, 362], ["dict", "pnasnet._create_pnasnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pnasnet._create_pnasnet"], ["", "@", "register_model", "\n", "def", "pnasnet5large", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"PNASNet-5 model architecture from the\n    `\"Progressive Neural Architecture Search\"\n    <https://arxiv.org/abs/1712.00559>`_ paper.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "pad_type", "=", "'same'", ",", "**", "kwargs", ")", "\n", "return", "_create_pnasnet", "(", "'pnasnet5large'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.MixerBlock.__init__": [[150, 160], ["functools.partial", "torch.Module.__init__", "norm_layer", "mlp_layer", "norm_layer", "mlp_layer", "int", "layers.DropPath", "torch.Identity", "torch.Identity", "layers.to_2tuple"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "dim", ",", "seq_len", ",", "mlp_ratio", "=", "(", "0.5", ",", "4.0", ")", ",", "mlp_layer", "=", "Mlp", ",", "\n", "norm_layer", "=", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", ",", "act_layer", "=", "nn", ".", "GELU", ",", "drop", "=", "0.", ",", "drop_path", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "tokens_dim", ",", "channels_dim", "=", "[", "int", "(", "x", "*", "dim", ")", "for", "x", "in", "to_2tuple", "(", "mlp_ratio", ")", "]", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "mlp_tokens", "=", "mlp_layer", "(", "seq_len", ",", "tokens_dim", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "mlp_channels", "=", "mlp_layer", "(", "dim", ",", "channels_dim", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.MixerBlock.forward": [[161, 165], ["mlp_mixer.MixerBlock.drop_path", "mlp_mixer.MixerBlock.drop_path", "mlp_mixer.MixerBlock.mlp_tokens().transpose", "mlp_mixer.MixerBlock.mlp_channels", "mlp_mixer.MixerBlock.norm2", "mlp_mixer.MixerBlock.mlp_tokens", "mlp_mixer.MixerBlock.norm1().transpose", "mlp_mixer.MixerBlock.norm1"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "mlp_tokens", "(", "self", ".", "norm1", "(", "x", ")", ".", "transpose", "(", "1", ",", "2", ")", ")", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "mlp_channels", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.Affine.__init__": [[168, 172], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "alpha", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "(", "1", ",", "1", ",", "dim", ")", ")", ")", "\n", "self", ".", "beta", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "(", "1", ",", "1", ",", "dim", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.Affine.forward": [[173, 175], ["torch.addcmul", "torch.addcmul", "torch.addcmul", "torch.addcmul"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "torch", ".", "addcmul", "(", "self", ".", "beta", ",", "self", ".", "alpha", ",", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.ResBlock.__init__": [[182, 194], ["torch.Module.__init__", "int", "norm_layer", "torch.Linear", "torch.Linear", "norm_layer", "mlp_layer", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "dim", ",", "seq_len", ",", "mlp_ratio", "=", "4", ",", "mlp_layer", "=", "Mlp", ",", "norm_layer", "=", "Affine", ",", "\n", "act_layer", "=", "nn", ".", "GELU", ",", "init_values", "=", "1e-4", ",", "drop", "=", "0.", ",", "drop_path", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "channel_dim", "=", "int", "(", "dim", "*", "mlp_ratio", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "linear_tokens", "=", "nn", ".", "Linear", "(", "seq_len", ",", "seq_len", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "mlp_channels", "=", "mlp_layer", "(", "dim", ",", "channel_dim", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "self", ".", "ls1", "=", "nn", ".", "Parameter", "(", "init_values", "*", "torch", ".", "ones", "(", "dim", ")", ")", "\n", "self", ".", "ls2", "=", "nn", ".", "Parameter", "(", "init_values", "*", "torch", ".", "ones", "(", "dim", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.ResBlock.forward": [[195, 199], ["mlp_mixer.ResBlock.drop_path", "mlp_mixer.ResBlock.drop_path", "mlp_mixer.ResBlock.linear_tokens().transpose", "mlp_mixer.ResBlock.mlp_channels", "mlp_mixer.ResBlock.norm2", "mlp_mixer.ResBlock.linear_tokens", "mlp_mixer.ResBlock.norm1().transpose", "mlp_mixer.ResBlock.norm1"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "ls1", "*", "self", ".", "linear_tokens", "(", "self", ".", "norm1", "(", "x", ")", ".", "transpose", "(", "1", ",", "2", ")", ")", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "ls2", "*", "self", ".", "mlp_channels", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.SpatialGatingUnit.__init__": [[206, 211], ["torch.Module.__init__", "norm_layer", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "dim", ",", "seq_len", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "gate_dim", "=", "dim", "//", "2", "\n", "self", ".", "norm", "=", "norm_layer", "(", "gate_dim", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "seq_len", ",", "seq_len", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.SpatialGatingUnit.init_weights": [[212, 216], ["torch.init.normal_", "torch.init.normal_", "torch.init.ones_", "torch.init.ones_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "# special init for the projection gate, called as override by base model init", "\n", "        ", "nn", ".", "init", ".", "normal_", "(", "self", ".", "proj", ".", "weight", ",", "std", "=", "1e-6", ")", "\n", "nn", ".", "init", ".", "ones_", "(", "self", ".", "proj", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.SpatialGatingUnit.forward": [[217, 222], ["x.chunk", "mlp_mixer.SpatialGatingUnit.norm", "mlp_mixer.SpatialGatingUnit.proj", "mlp_mixer.SpatialGatingUnit.transpose", "mlp_mixer.SpatialGatingUnit.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "u", ",", "v", "=", "x", ".", "chunk", "(", "2", ",", "dim", "=", "-", "1", ")", "\n", "v", "=", "self", ".", "norm", "(", "v", ")", "\n", "v", "=", "self", ".", "proj", "(", "v", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "return", "u", "*", "v", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.SpatialGatingBlock.__init__": [[229, 238], ["functools.partial", "torch.Module.__init__", "int", "norm_layer", "functools.partial", "mlp_layer", "layers.DropPath", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "dim", ",", "seq_len", ",", "mlp_ratio", "=", "4", ",", "mlp_layer", "=", "GatedMlp", ",", "\n", "norm_layer", "=", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", ",", "act_layer", "=", "nn", ".", "GELU", ",", "drop", "=", "0.", ",", "drop_path", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "channel_dim", "=", "int", "(", "dim", "*", "mlp_ratio", ")", "\n", "self", ".", "norm", "=", "norm_layer", "(", "dim", ")", "\n", "sgu", "=", "partial", "(", "SpatialGatingUnit", ",", "seq_len", "=", "seq_len", ")", "\n", "self", ".", "mlp_channels", "=", "mlp_layer", "(", "dim", ",", "channel_dim", ",", "act_layer", "=", "act_layer", ",", "gate_layer", "=", "sgu", ",", "drop", "=", "drop", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.SpatialGatingBlock.forward": [[239, 242], ["mlp_mixer.SpatialGatingBlock.drop_path", "mlp_mixer.SpatialGatingBlock.mlp_channels", "mlp_mixer.SpatialGatingBlock.norm"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "mlp_channels", "(", "self", ".", "norm", "(", "x", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.MlpMixer.__init__": [[246, 284], ["functools.partial", "torch.Module.__init__", "layers.PatchEmbed", "torch.Sequential", "torch.Sequential", "norm_layer", "mlp_mixer.MlpMixer.init_weights", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "block_layer", "range"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "num_classes", "=", "1000", ",", "\n", "img_size", "=", "224", ",", "\n", "in_chans", "=", "3", ",", "\n", "patch_size", "=", "16", ",", "\n", "num_blocks", "=", "8", ",", "\n", "embed_dim", "=", "512", ",", "\n", "mlp_ratio", "=", "(", "0.5", ",", "4.0", ")", ",", "\n", "block_layer", "=", "MixerBlock", ",", "\n", "mlp_layer", "=", "Mlp", ",", "\n", "norm_layer", "=", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", ",", "\n", "act_layer", "=", "nn", ".", "GELU", ",", "\n", "drop_rate", "=", "0.", ",", "\n", "drop_path_rate", "=", "0.", ",", "\n", "nlhb", "=", "False", ",", "\n", "stem_norm", "=", "False", ",", "\n", "global_pool", "=", "'avg'", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "self", ".", "num_features", "=", "self", ".", "embed_dim", "=", "embed_dim", "# num_features for consistency with other models", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "self", ".", "stem", "=", "PatchEmbed", "(", "\n", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", ",", "in_chans", "=", "in_chans", ",", "\n", "embed_dim", "=", "embed_dim", ",", "norm_layer", "=", "norm_layer", "if", "stem_norm", "else", "None", ")", "\n", "# FIXME drop_path (stochastic depth scaling rule or all the same?)", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "block_layer", "(", "\n", "embed_dim", ",", "self", ".", "stem", ".", "num_patches", ",", "mlp_ratio", ",", "mlp_layer", "=", "mlp_layer", ",", "norm_layer", "=", "norm_layer", ",", "\n", "act_layer", "=", "act_layer", ",", "drop", "=", "drop_rate", ",", "drop_path", "=", "drop_path_rate", ")", "\n", "for", "_", "in", "range", "(", "num_blocks", ")", "]", ")", "\n", "self", ".", "norm", "=", "norm_layer", "(", "embed_dim", ")", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "self", ".", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "init_weights", "(", "nlhb", "=", "nlhb", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.MlpMixer.init_weights": [[285, 289], ["helpers.named_apply", "functools.partial", "math.log"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_apply"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "init_weights", "(", "self", ",", "nlhb", "=", "False", ")", ":", "\n", "        ", "head_bias", "=", "-", "math", ".", "log", "(", "self", ".", "num_classes", ")", "if", "nlhb", "else", "0.", "\n", "named_apply", "(", "partial", "(", "_init_weights", ",", "head_bias", "=", "head_bias", ")", ",", "module", "=", "self", ")", "# depth-first", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.MlpMixer.group_matcher": [[290, 295], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^stem'", ",", "# stem and embed", "\n", "blocks", "=", "[", "(", "r'^blocks\\.(\\d+)'", ",", "None", ")", ",", "(", "r'^norm'", ",", "(", "99999", ",", ")", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.MlpMixer.set_grad_checkpointing": [[297, 300], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.MlpMixer.get_classifier": [[301, 304], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.MlpMixer.reset_classifier": [[305, 311], ["torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "assert", "global_pool", "in", "(", "''", ",", "'avg'", ")", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.MlpMixer.forward_features": [[312, 320], ["mlp_mixer.MlpMixer.stem", "mlp_mixer.MlpMixer.norm", "helpers.checkpoint_seq", "mlp_mixer.MlpMixer.blocks", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "stem", "(", "x", ")", "\n", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "blocks", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "blocks", "(", "x", ")", "\n", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.MlpMixer.forward": [[321, 327], ["mlp_mixer.MlpMixer.forward_features", "mlp_mixer.MlpMixer.head", "x.mean.mean.mean"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "if", "self", ".", "global_pool", "==", "'avg'", ":", "\n", "            ", "x", "=", "x", ".", "mean", "(", "dim", "=", "1", ")", "\n", "", "x", "=", "self", ".", "head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._cfg": [[54, 62], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "(", "0.5", ",", "0.5", ",", "0.5", ")", ",", "'std'", ":", "(", "0.5", ",", "0.5", ",", "0.5", ")", ",", "\n", "'first_conv'", ":", "'stem.proj'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._init_weights": [[329, 361], ["isinstance", "name.startswith", "isinstance", "torch.init.zeros_", "torch.init.constant_", "layers.lecun_normal_", "isinstance", "layers.lecun_normal_", "torch.init.xavier_uniform_", "torch.init.zeros_", "torch.init.ones_", "torch.init.zeros_", "hasattr", "torch.init.zeros_", "module.init_weights", "torch.init.normal_", "torch.init.zeros_"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.lecun_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.lecun_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights"], ["", "", "def", "_init_weights", "(", "module", ":", "nn", ".", "Module", ",", "name", ":", "str", ",", "head_bias", ":", "float", "=", "0.", ",", "flax", "=", "False", ")", ":", "\n", "    ", "\"\"\" Mixer weight initialization (trying to match Flax defaults)\n    \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "        ", "if", "name", ".", "startswith", "(", "'head'", ")", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "weight", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "module", ".", "bias", ",", "head_bias", ")", "\n", "", "else", ":", "\n", "            ", "if", "flax", ":", "\n", "# Flax defaults", "\n", "                ", "lecun_normal_", "(", "module", ".", "weight", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                    ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "", "else", ":", "\n", "# like MLP init in vit (my original init)", "\n", "                ", "nn", ".", "init", ".", "xavier_uniform_", "(", "module", ".", "weight", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                    ", "if", "'mlp'", "in", "name", ":", "\n", "                        ", "nn", ".", "init", ".", "normal_", "(", "module", ".", "bias", ",", "std", "=", "1e-6", ")", "\n", "", "else", ":", "\n", "                        ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "", "", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", ":", "\n", "        ", "lecun_normal_", "(", "module", ".", "weight", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "(", "nn", ".", "LayerNorm", ",", "nn", ".", "BatchNorm2d", ",", "nn", ".", "GroupNorm", ")", ")", ":", "\n", "        ", "nn", ".", "init", ".", "ones_", "(", "module", ".", "weight", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "elif", "hasattr", "(", "module", ",", "'init_weights'", ")", ":", "\n", "# NOTE if a parent module contains init_weights method, it can override the init of the", "\n", "# child modules as this will be called in depth-first order.", "\n", "        ", "module", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.checkpoint_filter_fn": [[363, 378], ["state_dict.items", "k.replace.replace", "k.replace.replace", "k.replace.replace", "k.replace.replace", "k.replace.endswith", "k.replace.endswith", "v.reshape.reshape"], "function", ["None"], ["", "", "def", "checkpoint_filter_fn", "(", "state_dict", ",", "model", ")", ":", "\n", "    ", "\"\"\" Remap checkpoints if needed \"\"\"", "\n", "if", "'patch_embed.proj.weight'", "in", "state_dict", ":", "\n", "# Remap FB ResMlp models -> timm", "\n", "        ", "out_dict", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "            ", "k", "=", "k", ".", "replace", "(", "'patch_embed.'", ",", "'stem.'", ")", "\n", "k", "=", "k", ".", "replace", "(", "'attn.'", ",", "'linear_tokens.'", ")", "\n", "k", "=", "k", ".", "replace", "(", "'mlp.'", ",", "'mlp_channels.'", ")", "\n", "k", "=", "k", ".", "replace", "(", "'gamma_'", ",", "'ls'", ")", "\n", "if", "k", ".", "endswith", "(", "'.alpha'", ")", "or", "k", ".", "endswith", "(", "'.beta'", ")", ":", "\n", "                ", "v", "=", "v", ".", "reshape", "(", "1", ",", "1", ",", "-", "1", ")", "\n", "", "out_dict", "[", "k", "]", "=", "v", "\n", "", "return", "out_dict", "\n", "", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer": [[380, 389], ["kwargs.get", "helpers.build_model_with_cfg", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "def", "_create_mixer", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "kwargs", ".", "get", "(", "'features_only'", ",", "None", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'features_only not implemented for MLP-Mixer models.'", ")", "\n", "\n", "", "model", "=", "build_model_with_cfg", "(", "\n", "MlpMixer", ",", "variant", ",", "pretrained", ",", "\n", "pretrained_filter_fn", "=", "checkpoint_filter_fn", ",", "\n", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.mixer_s32_224": [[391, 399], ["dict", "mlp_mixer._create_mixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "mixer_s32_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Mixer-S/32 224x224\n    Paper: 'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "patch_size", "=", "32", ",", "num_blocks", "=", "8", ",", "embed_dim", "=", "512", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'mixer_s32_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.mixer_s16_224": [[401, 409], ["dict", "mlp_mixer._create_mixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "mixer_s16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Mixer-S/16 224x224\n    Paper:  'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "patch_size", "=", "16", ",", "num_blocks", "=", "8", ",", "embed_dim", "=", "512", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'mixer_s16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.mixer_b32_224": [[411, 419], ["dict", "mlp_mixer._create_mixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "mixer_b32_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Mixer-B/32 224x224\n    Paper:  'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "patch_size", "=", "32", ",", "num_blocks", "=", "12", ",", "embed_dim", "=", "768", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'mixer_b32_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.mixer_b16_224": [[421, 429], ["dict", "mlp_mixer._create_mixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "mixer_b16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Mixer-B/16 224x224. ImageNet-1k pretrained weights.\n    Paper:  'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "patch_size", "=", "16", ",", "num_blocks", "=", "12", ",", "embed_dim", "=", "768", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'mixer_b16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.mixer_b16_224_in21k": [[431, 439], ["dict", "mlp_mixer._create_mixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "mixer_b16_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Mixer-B/16 224x224. ImageNet-21k pretrained weights.\n    Paper:  'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "patch_size", "=", "16", ",", "num_blocks", "=", "12", ",", "embed_dim", "=", "768", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'mixer_b16_224_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.mixer_l32_224": [[441, 449], ["dict", "mlp_mixer._create_mixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "mixer_l32_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Mixer-L/32 224x224.\n    Paper:  'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "patch_size", "=", "32", ",", "num_blocks", "=", "24", ",", "embed_dim", "=", "1024", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'mixer_l32_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.mixer_l16_224": [[451, 459], ["dict", "mlp_mixer._create_mixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "mixer_l16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Mixer-L/16 224x224. ImageNet-1k pretrained weights.\n    Paper:  'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "patch_size", "=", "16", ",", "num_blocks", "=", "24", ",", "embed_dim", "=", "1024", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'mixer_l16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.mixer_l16_224_in21k": [[461, 469], ["dict", "mlp_mixer._create_mixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "mixer_l16_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Mixer-L/16 224x224. ImageNet-21k pretrained weights.\n    Paper:  'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "patch_size", "=", "16", ",", "num_blocks", "=", "24", ",", "embed_dim", "=", "1024", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'mixer_l16_224_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.mixer_b16_224_miil": [[471, 479], ["dict", "mlp_mixer._create_mixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "mixer_b16_224_miil", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Mixer-B/16 224x224. ImageNet-21k pretrained weights.\n    Weights taken from: https://github.com/Alibaba-MIIL/ImageNet21K\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "patch_size", "=", "16", ",", "num_blocks", "=", "12", ",", "embed_dim", "=", "768", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'mixer_b16_224_miil'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.mixer_b16_224_miil_in21k": [[481, 489], ["dict", "mlp_mixer._create_mixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "mixer_b16_224_miil_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Mixer-B/16 224x224. ImageNet-1k pretrained weights.\n    Weights taken from: https://github.com/Alibaba-MIIL/ImageNet21K\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "patch_size", "=", "16", ",", "num_blocks", "=", "12", ",", "embed_dim", "=", "768", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'mixer_b16_224_miil_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.gmixer_12_224": [[491, 501], ["dict", "mlp_mixer._create_mixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "gmixer_12_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Glu-Mixer-12 224x224\n    Experiment by Ross Wightman, adding (Si)GLU to MLP-Mixer\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "num_blocks", "=", "12", ",", "embed_dim", "=", "384", ",", "mlp_ratio", "=", "(", "1.0", ",", "4.0", ")", ",", "\n", "mlp_layer", "=", "GluMlp", ",", "act_layer", "=", "nn", ".", "SiLU", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'gmixer_12_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.gmixer_24_224": [[503, 513], ["dict", "mlp_mixer._create_mixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "gmixer_24_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Glu-Mixer-24 224x224\n    Experiment by Ross Wightman, adding (Si)GLU to MLP-Mixer\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "num_blocks", "=", "24", ",", "embed_dim", "=", "384", ",", "mlp_ratio", "=", "(", "1.0", ",", "4.0", ")", ",", "\n", "mlp_layer", "=", "GluMlp", ",", "act_layer", "=", "nn", ".", "SiLU", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'gmixer_24_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.resmlp_12_224": [[515, 524], ["dict", "mlp_mixer._create_mixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "resmlp_12_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResMLP-12\n    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "num_blocks", "=", "12", ",", "embed_dim", "=", "384", ",", "mlp_ratio", "=", "4", ",", "block_layer", "=", "ResBlock", ",", "norm_layer", "=", "Affine", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'resmlp_12_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.resmlp_24_224": [[526, 536], ["dict", "mlp_mixer._create_mixer", "functools.partial"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "resmlp_24_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResMLP-24\n    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "num_blocks", "=", "24", ",", "embed_dim", "=", "384", ",", "mlp_ratio", "=", "4", ",", "\n", "block_layer", "=", "partial", "(", "ResBlock", ",", "init_values", "=", "1e-5", ")", ",", "norm_layer", "=", "Affine", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'resmlp_24_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.resmlp_36_224": [[538, 548], ["dict", "mlp_mixer._create_mixer", "functools.partial"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "resmlp_36_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResMLP-36\n    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "num_blocks", "=", "36", ",", "embed_dim", "=", "384", ",", "mlp_ratio", "=", "4", ",", "\n", "block_layer", "=", "partial", "(", "ResBlock", ",", "init_values", "=", "1e-6", ")", ",", "norm_layer", "=", "Affine", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'resmlp_36_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.resmlp_big_24_224": [[550, 560], ["dict", "mlp_mixer._create_mixer", "functools.partial"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "resmlp_big_24_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResMLP-B-24\n    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "num_blocks", "=", "24", ",", "embed_dim", "=", "768", ",", "mlp_ratio", "=", "4", ",", "\n", "block_layer", "=", "partial", "(", "ResBlock", ",", "init_values", "=", "1e-6", ")", ",", "norm_layer", "=", "Affine", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'resmlp_big_24_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.resmlp_12_distilled_224": [[562, 571], ["dict", "mlp_mixer._create_mixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "resmlp_12_distilled_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResMLP-12\n    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "num_blocks", "=", "12", ",", "embed_dim", "=", "384", ",", "mlp_ratio", "=", "4", ",", "block_layer", "=", "ResBlock", ",", "norm_layer", "=", "Affine", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'resmlp_12_distilled_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.resmlp_24_distilled_224": [[573, 583], ["dict", "mlp_mixer._create_mixer", "functools.partial"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "resmlp_24_distilled_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResMLP-24\n    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "num_blocks", "=", "24", ",", "embed_dim", "=", "384", ",", "mlp_ratio", "=", "4", ",", "\n", "block_layer", "=", "partial", "(", "ResBlock", ",", "init_values", "=", "1e-5", ")", ",", "norm_layer", "=", "Affine", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'resmlp_24_distilled_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.resmlp_36_distilled_224": [[585, 595], ["dict", "mlp_mixer._create_mixer", "functools.partial"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "resmlp_36_distilled_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResMLP-36\n    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "num_blocks", "=", "36", ",", "embed_dim", "=", "384", ",", "mlp_ratio", "=", "4", ",", "\n", "block_layer", "=", "partial", "(", "ResBlock", ",", "init_values", "=", "1e-6", ")", ",", "norm_layer", "=", "Affine", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'resmlp_36_distilled_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.resmlp_big_24_distilled_224": [[597, 607], ["dict", "mlp_mixer._create_mixer", "functools.partial"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "resmlp_big_24_distilled_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResMLP-B-24\n    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "num_blocks", "=", "24", ",", "embed_dim", "=", "768", ",", "mlp_ratio", "=", "4", ",", "\n", "block_layer", "=", "partial", "(", "ResBlock", ",", "init_values", "=", "1e-6", ")", ",", "norm_layer", "=", "Affine", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'resmlp_big_24_distilled_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.resmlp_big_24_224_in22ft1k": [[609, 619], ["dict", "mlp_mixer._create_mixer", "functools.partial"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "resmlp_big_24_224_in22ft1k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResMLP-B-24\n    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "num_blocks", "=", "24", ",", "embed_dim", "=", "768", ",", "mlp_ratio", "=", "4", ",", "\n", "block_layer", "=", "partial", "(", "ResBlock", ",", "init_values", "=", "1e-6", ")", ",", "norm_layer", "=", "Affine", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'resmlp_big_24_224_in22ft1k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.resmlp_12_224_dino": [[621, 632], ["dict", "mlp_mixer._create_mixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "resmlp_12_224_dino", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResMLP-12\n    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404\n\n    Model pretrained via DINO (self-supervised) - https://arxiv.org/abs/2104.14294\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "num_blocks", "=", "12", ",", "embed_dim", "=", "384", ",", "mlp_ratio", "=", "4", ",", "block_layer", "=", "ResBlock", ",", "norm_layer", "=", "Affine", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'resmlp_12_224_dino'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.resmlp_24_224_dino": [[634, 646], ["dict", "mlp_mixer._create_mixer", "functools.partial"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "resmlp_24_224_dino", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResMLP-24\n    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404\n\n    Model pretrained via DINO (self-supervised) - https://arxiv.org/abs/2104.14294\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "num_blocks", "=", "24", ",", "embed_dim", "=", "384", ",", "mlp_ratio", "=", "4", ",", "\n", "block_layer", "=", "partial", "(", "ResBlock", ",", "init_values", "=", "1e-5", ")", ",", "norm_layer", "=", "Affine", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'resmlp_24_224_dino'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.gmlp_ti16_224": [[648, 658], ["dict", "mlp_mixer._create_mixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "gmlp_ti16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" gMLP-Tiny\n    Paper: `Pay Attention to MLPs` - https://arxiv.org/abs/2105.08050\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "num_blocks", "=", "30", ",", "embed_dim", "=", "128", ",", "mlp_ratio", "=", "6", ",", "block_layer", "=", "SpatialGatingBlock", ",", "\n", "mlp_layer", "=", "GatedMlp", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'gmlp_ti16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.gmlp_s16_224": [[660, 670], ["dict", "mlp_mixer._create_mixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "gmlp_s16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" gMLP-Small\n    Paper: `Pay Attention to MLPs` - https://arxiv.org/abs/2105.08050\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "num_blocks", "=", "30", ",", "embed_dim", "=", "256", ",", "mlp_ratio", "=", "6", ",", "block_layer", "=", "SpatialGatingBlock", ",", "\n", "mlp_layer", "=", "GatedMlp", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'gmlp_s16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer.gmlp_b16_224": [[672, 682], ["dict", "mlp_mixer._create_mixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mlp_mixer._create_mixer"], ["", "@", "register_model", "\n", "def", "gmlp_b16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" gMLP-Base\n    Paper: `Pay Attention to MLPs` - https://arxiv.org/abs/2105.08050\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "num_blocks", "=", "30", ",", "embed_dim", "=", "512", ",", "mlp_ratio", "=", "6", ",", "block_layer", "=", "SpatialGatingBlock", ",", "\n", "mlp_layer", "=", "GatedMlp", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_mixer", "(", "'gmlp_b16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.ClassAttn.__init__": [[78, 90], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ",", "num_heads", "=", "8", ",", "qkv_bias", "=", "False", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "head_dim", "=", "dim", "//", "num_heads", "\n", "self", ".", "scale", "=", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "q", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "k", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "v", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.ClassAttn.forward": [[91, 108], ["cait.ClassAttn.q().unsqueeze().reshape().permute", "cait.ClassAttn.k().reshape().permute", "cait.ClassAttn.v().reshape().permute", "cait.ClassAttn.softmax", "cait.ClassAttn.attn_drop", "cait.ClassAttn.proj", "cait.ClassAttn.proj_drop", "cait.ClassAttn.transpose", "cait.ClassAttn.q().unsqueeze().reshape", "cait.ClassAttn.k().reshape", "cait.ClassAttn.v().reshape", "cait.ClassAttn.q().unsqueeze", "cait.ClassAttn.k", "cait.ClassAttn.v", "cait.ClassAttn.q"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "q", "=", "self", ".", "q", "(", "x", "[", ":", ",", "0", "]", ")", ".", "unsqueeze", "(", "1", ")", ".", "reshape", "(", "B", ",", "1", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "k", "=", "self", ".", "k", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n", "q", "=", "q", "*", "self", ".", "scale", "\n", "v", "=", "self", ".", "v", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "x_cls", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "1", ",", "C", ")", "\n", "x_cls", "=", "self", ".", "proj", "(", "x_cls", ")", "\n", "x_cls", "=", "self", ".", "proj_drop", "(", "x_cls", ")", "\n", "\n", "return", "x_cls", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.LayerScaleBlockClassAttn.__init__": [[113, 127], ["torch.Module.__init__", "norm_layer", "attn_block", "norm_layer", "int", "mlp_block", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "num_heads", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "\n", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ",", "attn_block", "=", "ClassAttn", ",", "\n", "mlp_block", "=", "Mlp", ",", "init_values", "=", "1e-4", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "attn", "=", "attn_block", "(", "\n", "dim", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "mlp_hidden_dim", "=", "int", "(", "dim", "*", "mlp_ratio", ")", "\n", "self", ".", "mlp", "=", "mlp_block", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "mlp_hidden_dim", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "self", ".", "gamma_1", "=", "nn", ".", "Parameter", "(", "init_values", "*", "torch", ".", "ones", "(", "(", "dim", ")", ")", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "gamma_2", "=", "nn", ".", "Parameter", "(", "init_values", "*", "torch", ".", "ones", "(", "(", "dim", ")", ")", ",", "requires_grad", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.LayerScaleBlockClassAttn.forward": [[128, 133], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "cait.LayerScaleBlockClassAttn.drop_path", "cait.LayerScaleBlockClassAttn.drop_path", "cait.LayerScaleBlockClassAttn.attn", "cait.LayerScaleBlockClassAttn.mlp", "cait.LayerScaleBlockClassAttn.norm1", "cait.LayerScaleBlockClassAttn.norm2"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ",", "x_cls", ")", ":", "\n", "        ", "u", "=", "torch", ".", "cat", "(", "(", "x_cls", ",", "x", ")", ",", "dim", "=", "1", ")", "\n", "x_cls", "=", "x_cls", "+", "self", ".", "drop_path", "(", "self", ".", "gamma_1", "*", "self", ".", "attn", "(", "self", ".", "norm1", "(", "u", ")", ")", ")", "\n", "x_cls", "=", "x_cls", "+", "self", ".", "drop_path", "(", "self", ".", "gamma_2", "*", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x_cls", ")", ")", ")", "\n", "return", "x_cls", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.TalkingHeadAttn.__init__": [[138, 156], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ",", "num_heads", "=", "8", ",", "qkv_bias", "=", "False", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "\n", "head_dim", "=", "dim", "//", "num_heads", "\n", "\n", "self", ".", "scale", "=", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", "*", "3", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "\n", "self", ".", "proj_l", "=", "nn", ".", "Linear", "(", "num_heads", ",", "num_heads", ")", "\n", "self", ".", "proj_w", "=", "nn", ".", "Linear", "(", "num_heads", ",", "num_heads", ")", "\n", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.TalkingHeadAttn.forward": [[157, 175], ["cait.TalkingHeadAttn.qkv().reshape().permute", "cait.TalkingHeadAttn.proj_l().permute", "cait.TalkingHeadAttn.softmax", "cait.TalkingHeadAttn.proj_w().permute", "cait.TalkingHeadAttn.attn_drop", "cait.TalkingHeadAttn.proj", "cait.TalkingHeadAttn.proj_drop", "k.transpose", "cait.TalkingHeadAttn.qkv().reshape", "cait.TalkingHeadAttn.proj_l", "cait.TalkingHeadAttn.proj_w", "cait.TalkingHeadAttn.permute", "cait.TalkingHeadAttn.permute", "cait.TalkingHeadAttn.qkv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "qkv", "=", "self", ".", "qkv", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "3", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "q", ",", "k", ",", "v", "=", "qkv", "[", "0", "]", "*", "self", ".", "scale", ",", "qkv", "[", "1", "]", ",", "qkv", "[", "2", "]", "\n", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "\n", "\n", "attn", "=", "self", ".", "proj_l", "(", "attn", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "attn", "=", "self", ".", "proj_w", "(", "attn", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "N", ",", "C", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.LayerScaleBlock.__init__": [[180, 194], ["torch.Module.__init__", "norm_layer", "attn_block", "norm_layer", "int", "mlp_block", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "num_heads", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "\n", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ",", "attn_block", "=", "TalkingHeadAttn", ",", "\n", "mlp_block", "=", "Mlp", ",", "init_values", "=", "1e-4", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "attn", "=", "attn_block", "(", "\n", "dim", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "mlp_hidden_dim", "=", "int", "(", "dim", "*", "mlp_ratio", ")", "\n", "self", ".", "mlp", "=", "mlp_block", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "mlp_hidden_dim", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "self", ".", "gamma_1", "=", "nn", ".", "Parameter", "(", "init_values", "*", "torch", ".", "ones", "(", "(", "dim", ")", ")", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "gamma_2", "=", "nn", ".", "Parameter", "(", "init_values", "*", "torch", ".", "ones", "(", "(", "dim", ")", ")", ",", "requires_grad", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.LayerScaleBlock.forward": [[195, 199], ["cait.LayerScaleBlock.drop_path", "cait.LayerScaleBlock.drop_path", "cait.LayerScaleBlock.attn", "cait.LayerScaleBlock.mlp", "cait.LayerScaleBlock.norm1", "cait.LayerScaleBlock.norm2"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "gamma_1", "*", "self", ".", "attn", "(", "self", ".", "norm1", "(", "x", ")", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "gamma_2", "*", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.Cait.__init__": [[204, 262], ["functools.partial", "torch.Module.__init__", "patch_layer", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Dropout", "torch.Dropout", "torch.Sequential", "torch.Sequential", "torch.ModuleList", "torch.ModuleList", "norm_layer", "layers.trunc_normal_", "layers.trunc_normal_", "cait.Cait.apply", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "dict", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "range", "block_layers_token", "block_layers", "range", "range"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["    ", "def", "__init__", "(", "\n", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "global_pool", "=", "'token'", ",", "\n", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "True", ",", "\n", "drop_rate", "=", "0.", ",", "attn_drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ",", "\n", "block_layers", "=", "LayerScaleBlock", ",", "\n", "block_layers_token", "=", "LayerScaleBlockClassAttn", ",", "\n", "patch_layer", "=", "PatchEmbed", ",", "\n", "norm_layer", "=", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", ",", "\n", "act_layer", "=", "nn", ".", "GELU", ",", "\n", "attn_block", "=", "TalkingHeadAttn", ",", "\n", "mlp_block", "=", "Mlp", ",", "\n", "init_values", "=", "1e-4", ",", "\n", "attn_block_token_only", "=", "ClassAttn", ",", "\n", "mlp_block_token_only", "=", "Mlp", ",", "\n", "depth_token_only", "=", "2", ",", "\n", "mlp_ratio_token_only", "=", "4.0", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "global_pool", "in", "(", "''", ",", "'token'", ",", "'avg'", ")", "\n", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "self", ".", "num_features", "=", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "self", ".", "patch_embed", "=", "patch_layer", "(", "\n", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", ",", "in_chans", "=", "in_chans", ",", "embed_dim", "=", "embed_dim", ")", "\n", "\n", "num_patches", "=", "self", ".", "patch_embed", ".", "num_patches", "\n", "\n", "self", ".", "cls_token", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "self", ".", "pos_embed", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "num_patches", ",", "embed_dim", ")", ")", "\n", "self", ".", "pos_drop", "=", "nn", ".", "Dropout", "(", "p", "=", "drop_rate", ")", "\n", "\n", "dpr", "=", "[", "drop_path_rate", "for", "i", "in", "range", "(", "depth", ")", "]", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "block_layers", "(", "\n", "dim", "=", "embed_dim", ",", "num_heads", "=", "num_heads", ",", "mlp_ratio", "=", "mlp_ratio", ",", "qkv_bias", "=", "qkv_bias", ",", "\n", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "dpr", "[", "i", "]", ",", "norm_layer", "=", "norm_layer", ",", "\n", "act_layer", "=", "act_layer", ",", "attn_block", "=", "attn_block", ",", "mlp_block", "=", "mlp_block", ",", "init_values", "=", "init_values", ")", "\n", "for", "i", "in", "range", "(", "depth", ")", "]", ")", "\n", "\n", "self", ".", "blocks_token_only", "=", "nn", ".", "ModuleList", "(", "[", "\n", "block_layers_token", "(", "\n", "dim", "=", "embed_dim", ",", "num_heads", "=", "num_heads", ",", "mlp_ratio", "=", "mlp_ratio_token_only", ",", "qkv_bias", "=", "qkv_bias", ",", "\n", "drop", "=", "0.0", ",", "attn_drop", "=", "0.0", ",", "drop_path", "=", "0.0", ",", "norm_layer", "=", "norm_layer", ",", "\n", "act_layer", "=", "act_layer", ",", "attn_block", "=", "attn_block_token_only", ",", "\n", "mlp_block", "=", "mlp_block_token_only", ",", "init_values", "=", "init_values", ")", "\n", "for", "i", "in", "range", "(", "depth_token_only", ")", "]", ")", "\n", "\n", "self", ".", "norm", "=", "norm_layer", "(", "embed_dim", ")", "\n", "\n", "self", ".", "feature_info", "=", "[", "dict", "(", "num_chs", "=", "embed_dim", ",", "reduction", "=", "0", ",", "module", "=", "'head'", ")", "]", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "trunc_normal_", "(", "self", ".", "pos_embed", ",", "std", "=", ".02", ")", "\n", "trunc_normal_", "(", "self", ".", "cls_token", ",", "std", "=", ".02", ")", "\n", "self", ".", "apply", "(", "self", ".", "_init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.Cait._init_weights": [[263, 271], ["isinstance", "layers.trunc_normal_", "isinstance", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["", "def", "_init_weights", "(", "self", ",", "m", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "            ", "trunc_normal_", "(", "m", ".", "weight", ",", "std", "=", ".02", ")", "\n", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.Cait.no_weight_decay": [[272, 275], ["None"], "methods", ["None"], ["", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "no_weight_decay", "(", "self", ")", ":", "\n", "        ", "return", "{", "'pos_embed'", ",", "'cls_token'", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.Cait.set_grad_checkpointing": [[276, 279], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.Cait.group_matcher": [[280, 296], ["any", "name.startswith", "name.startswith", "name.startswith", "int", "name.startswith", "int", "len", "float", "name.split", "len", "len", "name.split"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "def", "_matcher", "(", "name", ")", ":", "\n", "            ", "if", "any", "(", "[", "name", ".", "startswith", "(", "n", ")", "for", "n", "in", "(", "'cls_token'", ",", "'pos_embed'", ",", "'patch_embed'", ")", "]", ")", ":", "\n", "                ", "return", "0", "\n", "", "elif", "name", ".", "startswith", "(", "'blocks.'", ")", ":", "\n", "                ", "return", "int", "(", "name", ".", "split", "(", "'.'", ")", "[", "1", "]", ")", "+", "1", "\n", "", "elif", "name", ".", "startswith", "(", "'blocks_token_only.'", ")", ":", "\n", "# overlap token only blocks with last blocks", "\n", "                ", "to_offset", "=", "len", "(", "self", ".", "blocks", ")", "-", "len", "(", "self", ".", "blocks_token_only", ")", "+", "1", "\n", "return", "int", "(", "name", ".", "split", "(", "'.'", ")", "[", "1", "]", ")", "+", "to_offset", "\n", "", "elif", "name", ".", "startswith", "(", "'norm.'", ")", ":", "\n", "                ", "return", "len", "(", "self", ".", "blocks", ")", "\n", "", "else", ":", "\n", "                ", "return", "float", "(", "'inf'", ")", "\n", "", "", "return", "_matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.Cait.get_classifier": [[297, 300], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.Cait.reset_classifier": [[301, 307], ["torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "assert", "global_pool", "in", "(", "''", ",", "'token'", ",", "'avg'", ")", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.Cait.forward_features": [[308, 322], ["cait.Cait.patch_embed", "cait.Cait.pos_drop", "cait.Cait.cls_token.expand", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "cait.Cait.norm", "helpers.checkpoint_seq", "cait.Cait.blocks", "blk", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "patch_embed", "(", "x", ")", "\n", "x", "=", "x", "+", "self", ".", "pos_embed", "\n", "x", "=", "self", ".", "pos_drop", "(", "x", ")", "\n", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "blocks", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "blocks", "(", "x", ")", "\n", "", "cls_tokens", "=", "self", ".", "cls_token", ".", "expand", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ",", "-", "1", ")", "\n", "for", "i", ",", "blk", "in", "enumerate", "(", "self", ".", "blocks_token_only", ")", ":", "\n", "            ", "cls_tokens", "=", "blk", "(", "x", ",", "cls_tokens", ")", "\n", "", "x", "=", "torch", ".", "cat", "(", "(", "cls_tokens", ",", "x", ")", ",", "dim", "=", "1", ")", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.Cait.forward_head": [[323, 327], ["cait.Cait.head", "x[].mean"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "global_pool", ":", "\n", "            ", "x", "=", "x", "[", ":", ",", "1", ":", "]", ".", "mean", "(", "dim", "=", "1", ")", "if", "self", ".", "global_pool", "==", "'avg'", "else", "x", "[", ":", ",", "0", "]", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.Cait.forward": [[328, 332], ["cait.Cait.forward_features", "cait.Cait.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait._cfg": [[26, 34], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "384", ",", "384", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", "1.0", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'patch_embed.proj'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.checkpoint_filter_fn": [[334, 341], ["state_dict.items", "k.replace"], "function", ["None"], ["", "", "def", "checkpoint_filter_fn", "(", "state_dict", ",", "model", "=", "None", ")", ":", "\n", "    ", "if", "'model'", "in", "state_dict", ":", "\n", "        ", "state_dict", "=", "state_dict", "[", "'model'", "]", "\n", "", "checkpoint_no_module", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "        ", "checkpoint_no_module", "[", "k", ".", "replace", "(", "'module.'", ",", "''", ")", "]", "=", "v", "\n", "", "return", "checkpoint_no_module", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait._create_cait": [[343, 352], ["kwargs.get", "helpers.build_model_with_cfg", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "def", "_create_cait", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "kwargs", ".", "get", "(", "'features_only'", ",", "None", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'features_only not implemented for Vision Transformer models.'", ")", "\n", "\n", "", "model", "=", "build_model_with_cfg", "(", "\n", "Cait", ",", "variant", ",", "pretrained", ",", "\n", "pretrained_filter_fn", "=", "checkpoint_filter_fn", ",", "\n", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.cait_xxs24_224": [[354, 359], ["dict", "cait._create_cait"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait._create_cait"], ["", "@", "register_model", "\n", "def", "cait_xxs24_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "24", ",", "num_heads", "=", "4", ",", "init_values", "=", "1e-5", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_cait", "(", "'cait_xxs24_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.cait_xxs24_384": [[361, 366], ["dict", "cait._create_cait"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait._create_cait"], ["", "@", "register_model", "\n", "def", "cait_xxs24_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "24", ",", "num_heads", "=", "4", ",", "init_values", "=", "1e-5", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_cait", "(", "'cait_xxs24_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.cait_xxs36_224": [[368, 373], ["dict", "cait._create_cait"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait._create_cait"], ["", "@", "register_model", "\n", "def", "cait_xxs36_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "36", ",", "num_heads", "=", "4", ",", "init_values", "=", "1e-5", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_cait", "(", "'cait_xxs36_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.cait_xxs36_384": [[375, 380], ["dict", "cait._create_cait"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait._create_cait"], ["", "@", "register_model", "\n", "def", "cait_xxs36_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "36", ",", "num_heads", "=", "4", ",", "init_values", "=", "1e-5", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_cait", "(", "'cait_xxs36_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.cait_xs24_384": [[382, 387], ["dict", "cait._create_cait"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait._create_cait"], ["", "@", "register_model", "\n", "def", "cait_xs24_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "288", ",", "depth", "=", "24", ",", "num_heads", "=", "6", ",", "init_values", "=", "1e-5", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_cait", "(", "'cait_xs24_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.cait_s24_224": [[389, 394], ["dict", "cait._create_cait"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait._create_cait"], ["", "@", "register_model", "\n", "def", "cait_s24_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "24", ",", "num_heads", "=", "8", ",", "init_values", "=", "1e-5", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_cait", "(", "'cait_s24_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.cait_s24_384": [[396, 401], ["dict", "cait._create_cait"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait._create_cait"], ["", "@", "register_model", "\n", "def", "cait_s24_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "24", ",", "num_heads", "=", "8", ",", "init_values", "=", "1e-5", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_cait", "(", "'cait_s24_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.cait_s36_384": [[403, 408], ["dict", "cait._create_cait"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait._create_cait"], ["", "@", "register_model", "\n", "def", "cait_s36_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "36", ",", "num_heads", "=", "8", ",", "init_values", "=", "1e-6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_cait", "(", "'cait_s36_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.cait_m36_384": [[410, 415], ["dict", "cait._create_cait"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait._create_cait"], ["", "@", "register_model", "\n", "def", "cait_m36_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "36", ",", "num_heads", "=", "16", ",", "init_values", "=", "1e-6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_cait", "(", "'cait_m36_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait.cait_m48_448": [[417, 422], ["dict", "cait._create_cait"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cait._create_cait"], ["", "@", "register_model", "\n", "def", "cait_m48_448", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "48", ",", "num_heads", "=", "16", ",", "init_values", "=", "1e-6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_cait", "(", "'cait_m48_448'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest.ResNestBottleneck.__init__": [[60, 105], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "norm_layer", "act_layer", "torch.nn.Conv2d", "norm_layer", "act_layer", "int", "torch.nn.AvgPool2d", "layers.SplitAttn", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Identity", "torch.nn.Conv2d", "norm_layer", "act_layer", "torch.nn.AvgPool2d", "drop_block", "torch.nn.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "downsample", "=", "None", ",", "\n", "radix", "=", "1", ",", "cardinality", "=", "1", ",", "base_width", "=", "64", ",", "avd", "=", "False", ",", "avd_first", "=", "False", ",", "is_first", "=", "False", ",", "\n", "reduce_first", "=", "1", ",", "dilation", "=", "1", ",", "first_dilation", "=", "None", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "\n", "attn_layer", "=", "None", ",", "aa_layer", "=", "None", ",", "drop_block", "=", "None", ",", "drop_path", "=", "None", ")", ":", "\n", "        ", "super", "(", "ResNestBottleneck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "reduce_first", "==", "1", "# not supported", "\n", "assert", "attn_layer", "is", "None", "# not supported", "\n", "assert", "aa_layer", "is", "None", "# TODO not yet supported", "\n", "assert", "drop_path", "is", "None", "# TODO not yet supported", "\n", "\n", "group_width", "=", "int", "(", "planes", "*", "(", "base_width", "/", "64.", ")", ")", "*", "cardinality", "\n", "first_dilation", "=", "first_dilation", "or", "dilation", "\n", "if", "avd", "and", "(", "stride", ">", "1", "or", "is_first", ")", ":", "\n", "            ", "avd_stride", "=", "stride", "\n", "stride", "=", "1", "\n", "", "else", ":", "\n", "            ", "avd_stride", "=", "0", "\n", "", "self", ".", "radix", "=", "radix", "\n", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "inplanes", ",", "group_width", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "group_width", ")", "\n", "self", ".", "act1", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "avd_first", "=", "nn", ".", "AvgPool2d", "(", "3", ",", "avd_stride", ",", "padding", "=", "1", ")", "if", "avd_stride", ">", "0", "and", "avd_first", "else", "None", "\n", "\n", "if", "self", ".", "radix", ">=", "1", ":", "\n", "            ", "self", ".", "conv2", "=", "SplitAttn", "(", "\n", "group_width", ",", "group_width", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "padding", "=", "first_dilation", ",", "\n", "dilation", "=", "first_dilation", ",", "groups", "=", "cardinality", ",", "radix", "=", "radix", ",", "norm_layer", "=", "norm_layer", ",", "drop_layer", "=", "drop_block", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "drop_block", "=", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "act2", "=", "nn", ".", "Identity", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "\n", "group_width", ",", "group_width", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "padding", "=", "first_dilation", ",", "\n", "dilation", "=", "first_dilation", ",", "groups", "=", "cardinality", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn2", "=", "norm_layer", "(", "group_width", ")", "\n", "self", ".", "drop_block", "=", "drop_block", "(", ")", "if", "drop_block", "is", "not", "None", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "act2", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "", "self", ".", "avd_last", "=", "nn", ".", "AvgPool2d", "(", "3", ",", "avd_stride", ",", "padding", "=", "1", ")", "if", "avd_stride", ">", "0", "and", "not", "avd_first", "else", "None", "\n", "\n", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "group_width", ",", "planes", "*", "4", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn3", "=", "norm_layer", "(", "planes", "*", "4", ")", "\n", "self", ".", "act3", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest.ResNestBottleneck.zero_init_last": [[106, 108], ["torch.nn.init.zeros_"], "methods", ["None"], ["", "def", "zero_init_last", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "bn3", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest.ResNestBottleneck.forward": [[109, 136], ["resnest.ResNestBottleneck.conv1", "resnest.ResNestBottleneck.bn1", "resnest.ResNestBottleneck.act1", "resnest.ResNestBottleneck.conv2", "resnest.ResNestBottleneck.bn2", "resnest.ResNestBottleneck.drop_block", "resnest.ResNestBottleneck.act2", "resnest.ResNestBottleneck.conv3", "resnest.ResNestBottleneck.bn3", "resnest.ResNestBottleneck.act3", "resnest.ResNestBottleneck.avd_first", "resnest.ResNestBottleneck.avd_last", "resnest.ResNestBottleneck.downsample"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "act1", "(", "out", ")", "\n", "\n", "if", "self", ".", "avd_first", "is", "not", "None", ":", "\n", "            ", "out", "=", "self", ".", "avd_first", "(", "out", ")", "\n", "\n", "", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "bn2", "(", "out", ")", "\n", "out", "=", "self", ".", "drop_block", "(", "out", ")", "\n", "out", "=", "self", ".", "act2", "(", "out", ")", "\n", "\n", "if", "self", ".", "avd_last", "is", "not", "None", ":", "\n", "            ", "out", "=", "self", ".", "avd_last", "(", "out", ")", "\n", "\n", "", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "out", "=", "self", ".", "bn3", "(", "out", ")", "\n", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "shortcut", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "+=", "shortcut", "\n", "out", "=", "self", ".", "act3", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest._cfg": [[19, 27], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bilinear'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'conv1.0'", ",", "'classifier'", ":", "'fc'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest._create_resnest": [[138, 140], ["helpers.build_model_with_cfg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_resnest", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "ResNet", ",", "variant", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest.resnest14d": [[142, 151], ["dict", "resnest._create_resnest", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest._create_resnest"], ["", "@", "register_model", "\n", "def", "resnest14d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResNeSt-14d model. Weights ported from GluonCV.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "block", "=", "ResNestBottleneck", ",", "layers", "=", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "\n", "stem_type", "=", "'deep'", ",", "stem_width", "=", "32", ",", "avg_down", "=", "True", ",", "base_width", "=", "64", ",", "cardinality", "=", "1", ",", "\n", "block_args", "=", "dict", "(", "radix", "=", "2", ",", "avd", "=", "True", ",", "avd_first", "=", "False", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnest", "(", "'resnest14d'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest.resnest26d": [[153, 162], ["dict", "resnest._create_resnest", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest._create_resnest"], ["", "@", "register_model", "\n", "def", "resnest26d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResNeSt-26d model. Weights ported from GluonCV.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "block", "=", "ResNestBottleneck", ",", "layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "\n", "stem_type", "=", "'deep'", ",", "stem_width", "=", "32", ",", "avg_down", "=", "True", ",", "base_width", "=", "64", ",", "cardinality", "=", "1", ",", "\n", "block_args", "=", "dict", "(", "radix", "=", "2", ",", "avd", "=", "True", ",", "avd_first", "=", "False", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnest", "(", "'resnest26d'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest.resnest50d": [[164, 174], ["dict", "resnest._create_resnest", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest._create_resnest"], ["", "@", "register_model", "\n", "def", "resnest50d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResNeSt-50d model. Matches paper ResNeSt-50 model, https://arxiv.org/abs/2004.08955\n    Since this codebase supports all possible variations, 'd' for deep stem, stem_width 32, avg in downsample.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "block", "=", "ResNestBottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "\n", "stem_type", "=", "'deep'", ",", "stem_width", "=", "32", ",", "avg_down", "=", "True", ",", "base_width", "=", "64", ",", "cardinality", "=", "1", ",", "\n", "block_args", "=", "dict", "(", "radix", "=", "2", ",", "avd", "=", "True", ",", "avd_first", "=", "False", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnest", "(", "'resnest50d'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest.resnest101e": [[176, 186], ["dict", "resnest._create_resnest", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest._create_resnest"], ["", "@", "register_model", "\n", "def", "resnest101e", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResNeSt-101e model. Matches paper ResNeSt-101 model, https://arxiv.org/abs/2004.08955\n     Since this codebase supports all possible variations, 'e' for deep stem, stem_width 64, avg in downsample.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "block", "=", "ResNestBottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "\n", "stem_type", "=", "'deep'", ",", "stem_width", "=", "64", ",", "avg_down", "=", "True", ",", "base_width", "=", "64", ",", "cardinality", "=", "1", ",", "\n", "block_args", "=", "dict", "(", "radix", "=", "2", ",", "avd", "=", "True", ",", "avd_first", "=", "False", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnest", "(", "'resnest101e'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest.resnest200e": [[188, 198], ["dict", "resnest._create_resnest", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest._create_resnest"], ["", "@", "register_model", "\n", "def", "resnest200e", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResNeSt-200e model. Matches paper ResNeSt-200 model, https://arxiv.org/abs/2004.08955\n    Since this codebase supports all possible variations, 'e' for deep stem, stem_width 64, avg in downsample.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "block", "=", "ResNestBottleneck", ",", "layers", "=", "[", "3", ",", "24", ",", "36", ",", "3", "]", ",", "\n", "stem_type", "=", "'deep'", ",", "stem_width", "=", "64", ",", "avg_down", "=", "True", ",", "base_width", "=", "64", ",", "cardinality", "=", "1", ",", "\n", "block_args", "=", "dict", "(", "radix", "=", "2", ",", "avd", "=", "True", ",", "avd_first", "=", "False", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnest", "(", "'resnest200e'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest.resnest269e": [[200, 210], ["dict", "resnest._create_resnest", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest._create_resnest"], ["", "@", "register_model", "\n", "def", "resnest269e", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResNeSt-269e model. Matches paper ResNeSt-269 model, https://arxiv.org/abs/2004.08955\n    Since this codebase supports all possible variations, 'e' for deep stem, stem_width 64, avg in downsample.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "block", "=", "ResNestBottleneck", ",", "layers", "=", "[", "3", ",", "30", ",", "48", ",", "8", "]", ",", "\n", "stem_type", "=", "'deep'", ",", "stem_width", "=", "64", ",", "avg_down", "=", "True", ",", "base_width", "=", "64", ",", "cardinality", "=", "1", ",", "\n", "block_args", "=", "dict", "(", "radix", "=", "2", ",", "avd", "=", "True", ",", "avd_first", "=", "False", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnest", "(", "'resnest269e'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest.resnest50d_4s2x40d": [[212, 221], ["dict", "resnest._create_resnest", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest._create_resnest"], ["", "@", "register_model", "\n", "def", "resnest50d_4s2x40d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"ResNeSt-50 4s2x40d from https://github.com/zhanghang1989/ResNeSt/blob/master/ablation.md\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "block", "=", "ResNestBottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "\n", "stem_type", "=", "'deep'", ",", "stem_width", "=", "32", ",", "avg_down", "=", "True", ",", "base_width", "=", "40", ",", "cardinality", "=", "2", ",", "\n", "block_args", "=", "dict", "(", "radix", "=", "4", ",", "avd", "=", "True", ",", "avd_first", "=", "True", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnest", "(", "'resnest50d_4s2x40d'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest.resnest50d_1s4x24d": [[223, 232], ["dict", "resnest._create_resnest", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnest._create_resnest"], ["", "@", "register_model", "\n", "def", "resnest50d_1s4x24d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"ResNeSt-50 1s4x24d from https://github.com/zhanghang1989/ResNeSt/blob/master/ablation.md\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "block", "=", "ResNestBottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "\n", "stem_type", "=", "'deep'", ",", "stem_width", "=", "32", ",", "avg_down", "=", "True", ",", "base_width", "=", "24", ",", "cardinality", "=", "4", ",", "\n", "block_args", "=", "dict", "(", "radix", "=", "1", ",", "avd", "=", "True", ",", "avd_first", "=", "True", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnest", "(", "'resnest50d_1s4x24d'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.OutlookAttention.__init__": [[86, 104], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Unfold", "torch.Unfold", "torch.Unfold", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ",", "num_heads", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ",", "stride", "=", "1", ",", "qkv_bias", "=", "False", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "head_dim", "=", "dim", "//", "num_heads", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "padding", "=", "padding", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "scale", "=", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "v", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "attn", "=", "nn", ".", "Linear", "(", "dim", ",", "kernel_size", "**", "4", "*", "num_heads", ")", "\n", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "\n", "self", ".", "unfold", "=", "nn", ".", "Unfold", "(", "kernel_size", "=", "kernel_size", ",", "padding", "=", "padding", ",", "stride", "=", "stride", ")", "\n", "self", ".", "pool", "=", "nn", ".", "AvgPool2d", "(", "kernel_size", "=", "stride", ",", "stride", "=", "stride", ",", "ceil_mode", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.OutlookAttention.forward": [[105, 130], ["volo.OutlookAttention.v().permute", "volo.OutlookAttention.unfold().reshape().permute", "volo.OutlookAttention.pool().permute", "volo.OutlookAttention.attn().reshape().permute", "volo.OutlookAttention.softmax", "volo.OutlookAttention.attn_drop", "torch.fold", "torch.fold", "torch.fold", "volo.OutlookAttention.proj", "volo.OutlookAttention.proj_drop", "math.ceil", "math.ceil", "volo.OutlookAttention.permute", "volo.OutlookAttention.v", "volo.OutlookAttention.unfold().reshape", "volo.OutlookAttention.pool", "volo.OutlookAttention.attn().reshape", "volo.OutlookAttention.permute", "volo.OutlookAttention.unfold", "volo.OutlookAttention.attn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "H", ",", "W", ",", "C", "=", "x", ".", "shape", "\n", "\n", "v", "=", "self", ".", "v", "(", "x", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "# B, C, H, W", "\n", "\n", "h", ",", "w", "=", "math", ".", "ceil", "(", "H", "/", "self", ".", "stride", ")", ",", "math", ".", "ceil", "(", "W", "/", "self", ".", "stride", ")", "\n", "v", "=", "self", ".", "unfold", "(", "v", ")", ".", "reshape", "(", "\n", "B", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ",", "\n", "self", ".", "kernel_size", "*", "self", ".", "kernel_size", ",", "h", "*", "w", ")", ".", "permute", "(", "0", ",", "1", ",", "4", ",", "3", ",", "2", ")", "# B,H,N,kxk,C/H", "\n", "\n", "attn", "=", "self", ".", "pool", "(", "x", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ")", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "\n", "attn", "=", "self", ".", "attn", "(", "attn", ")", ".", "reshape", "(", "\n", "B", ",", "h", "*", "w", ",", "self", ".", "num_heads", ",", "self", ".", "kernel_size", "*", "self", ".", "kernel_size", ",", "\n", "self", ".", "kernel_size", "*", "self", ".", "kernel_size", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ",", "4", ")", "# B,H,N,kxk,kxk", "\n", "attn", "=", "attn", "*", "self", ".", "scale", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "permute", "(", "0", ",", "1", ",", "4", ",", "3", ",", "2", ")", ".", "reshape", "(", "B", ",", "C", "*", "self", ".", "kernel_size", "*", "self", ".", "kernel_size", ",", "h", "*", "w", ")", "\n", "x", "=", "F", ".", "fold", "(", "x", ",", "output_size", "=", "(", "H", ",", "W", ")", ",", "kernel_size", "=", "self", ".", "kernel_size", ",", "padding", "=", "self", ".", "padding", ",", "stride", "=", "self", ".", "stride", ")", "\n", "\n", "x", "=", "self", ".", "proj", "(", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.Outlooker.__init__": [[133, 149], ["torch.Module.__init__", "norm_layer", "volo.OutlookAttention", "norm_layer", "int", "timm.models.layers.Mlp", "timm.models.layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "kernel_size", ",", "padding", ",", "stride", "=", "1", ",", "num_heads", "=", "1", ",", "mlp_ratio", "=", "3.", ",", "attn_drop", "=", "0.", ",", "\n", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ",", "qkv_bias", "=", "False", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "attn", "=", "OutlookAttention", "(", "\n", "dim", ",", "num_heads", ",", "kernel_size", "=", "kernel_size", ",", "\n", "padding", "=", "padding", ",", "stride", "=", "stride", ",", "\n", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ")", "\n", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "mlp_hidden_dim", "=", "int", "(", "dim", "*", "mlp_ratio", ")", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "mlp_hidden_dim", ",", "act_layer", "=", "act_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.Outlooker.forward": [[150, 154], ["volo.Outlooker.drop_path", "volo.Outlooker.drop_path", "volo.Outlooker.attn", "volo.Outlooker.mlp", "volo.Outlooker.norm1", "volo.Outlooker.norm2"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "attn", "(", "self", ".", "norm1", "(", "x", ")", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.Attention.__init__": [[158, 169], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "num_heads", "=", "8", ",", "qkv_bias", "=", "False", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "head_dim", "=", "dim", "//", "num_heads", "\n", "self", ".", "scale", "=", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", "*", "3", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.Attention.forward": [[170, 185], ["volo.Attention.qkv().reshape().permute", "volo.Attention.unbind", "volo.Attention.softmax", "volo.Attention.attn_drop", "volo.Attention.proj", "volo.Attention.proj_drop", "volo.Attention.qkv().reshape", "k.transpose", "volo.Attention.qkv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "H", ",", "W", ",", "C", "=", "x", ".", "shape", "\n", "\n", "qkv", "=", "self", ".", "qkv", "(", "x", ")", ".", "reshape", "(", "B", ",", "H", "*", "W", ",", "3", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "q", ",", "k", ",", "v", "=", "qkv", ".", "unbind", "(", "0", ")", "\n", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "scale", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "H", ",", "W", ",", "C", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.Transformer.__init__": [[189, 202], ["torch.Module.__init__", "norm_layer", "volo.Attention", "norm_layer", "int", "timm.models.layers.Mlp", "timm.models.layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "num_heads", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "\n", "attn_drop", "=", "0.", ",", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "attn", "=", "Attention", "(", "dim", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ")", "\n", "\n", "# NOTE: drop path for stochastic depth, we shall see if this is better than dropout here", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "mlp_hidden_dim", "=", "int", "(", "dim", "*", "mlp_ratio", ")", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "mlp_hidden_dim", ",", "act_layer", "=", "act_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.Transformer.forward": [[203, 207], ["volo.Transformer.drop_path", "volo.Transformer.drop_path", "volo.Transformer.attn", "volo.Transformer.mlp", "volo.Transformer.norm1", "volo.Transformer.norm2"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "attn", "(", "self", ".", "norm1", "(", "x", ")", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.ClassAttention.__init__": [[211, 227], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "num_heads", "=", "8", ",", "head_dim", "=", "None", ",", "qkv_bias", "=", "False", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "if", "head_dim", "is", "not", "None", ":", "\n", "            ", "self", ".", "head_dim", "=", "head_dim", "\n", "", "else", ":", "\n", "            ", "head_dim", "=", "dim", "//", "num_heads", "\n", "self", ".", "head_dim", "=", "head_dim", "\n", "", "self", ".", "scale", "=", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "kv", "=", "nn", ".", "Linear", "(", "dim", ",", "self", ".", "head_dim", "*", "self", ".", "num_heads", "*", "2", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "q", "=", "nn", ".", "Linear", "(", "dim", ",", "self", ".", "head_dim", "*", "self", ".", "num_heads", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "self", ".", "head_dim", "*", "self", ".", "num_heads", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.ClassAttention.forward": [[228, 242], ["volo.ClassAttention.kv().reshape().permute", "volo.ClassAttention.unbind", "volo.ClassAttention.q().reshape", "volo.ClassAttention.softmax", "volo.ClassAttention.attn_drop", "volo.ClassAttention.proj", "volo.ClassAttention.proj_drop", "k.transpose", "volo.ClassAttention.kv().reshape", "volo.ClassAttention.q", "volo.ClassAttention.kv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "\n", "kv", "=", "self", ".", "kv", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "2", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "k", ",", "v", "=", "kv", ".", "unbind", "(", "0", ")", "\n", "q", "=", "self", ".", "q", "(", "x", "[", ":", ",", ":", "1", ",", ":", "]", ")", ".", "reshape", "(", "B", ",", "self", ".", "num_heads", ",", "1", ",", "self", ".", "head_dim", ")", "\n", "attn", "=", "(", "(", "q", "*", "self", ".", "scale", ")", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "cls_embed", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "1", ",", "self", ".", "head_dim", "*", "self", ".", "num_heads", ")", "\n", "cls_embed", "=", "self", ".", "proj", "(", "cls_embed", ")", "\n", "cls_embed", "=", "self", ".", "proj_drop", "(", "cls_embed", ")", "\n", "return", "cls_embed", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.ClassBlock.__init__": [[246, 258], ["torch.Module.__init__", "norm_layer", "volo.ClassAttention", "norm_layer", "int", "timm.models.layers.Mlp", "timm.models.layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "num_heads", ",", "head_dim", "=", "None", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "\n", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "attn", "=", "ClassAttention", "(", "\n", "dim", ",", "num_heads", "=", "num_heads", ",", "head_dim", "=", "head_dim", ",", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ")", "\n", "# NOTE: drop path for stochastic depth", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "mlp_hidden_dim", "=", "int", "(", "dim", "*", "mlp_ratio", ")", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "mlp_hidden_dim", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.ClassBlock.forward": [[259, 264], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "volo.ClassBlock.drop_path", "volo.ClassBlock.drop_path", "volo.ClassBlock.attn", "volo.ClassBlock.mlp", "volo.ClassBlock.norm1", "volo.ClassBlock.norm2"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "cls_embed", "=", "x", "[", ":", ",", ":", "1", "]", "\n", "cls_embed", "=", "cls_embed", "+", "self", ".", "drop_path", "(", "self", ".", "attn", "(", "self", ".", "norm1", "(", "x", ")", ")", ")", "\n", "cls_embed", "=", "cls_embed", "+", "self", ".", "drop_path", "(", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "cls_embed", ")", ")", ")", "\n", "return", "torch", ".", "cat", "(", "[", "cls_embed", ",", "x", "[", ":", ",", "1", ":", "]", "]", ",", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.PatchEmbed.__init__": [[299, 322], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "img_size", "=", "224", ",", "stem_conv", "=", "False", ",", "stem_stride", "=", "1", ",", "\n", "patch_size", "=", "8", ",", "in_chans", "=", "3", ",", "hidden_dim", "=", "64", ",", "embed_dim", "=", "384", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "patch_size", "in", "[", "4", ",", "8", ",", "16", "]", "\n", "if", "stem_conv", ":", "\n", "            ", "self", ".", "conv", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "in_chans", ",", "hidden_dim", ",", "kernel_size", "=", "7", ",", "stride", "=", "stem_stride", ",", "padding", "=", "3", ",", "bias", "=", "False", ")", ",", "# 112x112", "\n", "nn", ".", "BatchNorm2d", "(", "hidden_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv2d", "(", "hidden_dim", ",", "hidden_dim", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ",", "# 112x112", "\n", "nn", ".", "BatchNorm2d", "(", "hidden_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv2d", "(", "hidden_dim", ",", "hidden_dim", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ",", "# 112x112", "\n", "nn", ".", "BatchNorm2d", "(", "hidden_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv", "=", "None", "\n", "\n", "", "self", ".", "proj", "=", "nn", ".", "Conv2d", "(", "\n", "hidden_dim", ",", "embed_dim", ",", "kernel_size", "=", "patch_size", "//", "stem_stride", ",", "stride", "=", "patch_size", "//", "stem_stride", ")", "\n", "self", ".", "num_patches", "=", "(", "img_size", "//", "patch_size", ")", "*", "(", "img_size", "//", "patch_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.PatchEmbed.forward": [[323, 328], ["volo.PatchEmbed.proj", "volo.PatchEmbed.conv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "conv", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "", "x", "=", "self", ".", "proj", "(", "x", ")", "# B, C, H, W", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.Downsample.__init__": [[334, 337], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "in_embed_dim", ",", "out_embed_dim", ",", "patch_size", "=", "2", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Conv2d", "(", "in_embed_dim", ",", "out_embed_dim", ",", "kernel_size", "=", "patch_size", ",", "stride", "=", "patch_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.Downsample.forward": [[338, 343], ["x.permute.permute.permute", "volo.Downsample.proj", "x.permute.permute.permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "# B, C, H, W", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.VOLO.__init__": [[390, 492], ["torch.Module.__init__", "len", "timm.models.layers.to_2tuple", "volo.PatchEmbed", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Dropout", "torch.Dropout", "torch.Dropout", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "norm_layer", "timm.models.layers.trunc_normal_", "volo.VOLO.apply", "timm.models.layers.to_ntuple", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "len", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Parameter", "torch.Parameter", "torch.Parameter", "timm.models.layers.trunc_normal_", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "volo.outlooker_blocks", "network.append", "volo.transformer_blocks", "network.append", "network.append", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "volo.Downsample", "volo.get_block", "range", "len"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.outlooker_blocks", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.transformer_blocks", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.get_block"], ["def", "__init__", "(", "\n", "self", ",", "\n", "layers", ",", "\n", "img_size", "=", "224", ",", "\n", "in_chans", "=", "3", ",", "\n", "num_classes", "=", "1000", ",", "\n", "global_pool", "=", "'token'", ",", "\n", "patch_size", "=", "8", ",", "\n", "stem_hidden_dim", "=", "64", ",", "\n", "embed_dims", "=", "None", ",", "\n", "num_heads", "=", "None", ",", "\n", "downsamples", "=", "(", "True", ",", "False", ",", "False", ",", "False", ")", ",", "\n", "outlook_attention", "=", "(", "True", ",", "False", ",", "False", ",", "False", ")", ",", "\n", "mlp_ratio", "=", "3.0", ",", "\n", "qkv_bias", "=", "False", ",", "\n", "drop_rate", "=", "0.", ",", "\n", "attn_drop_rate", "=", "0.", ",", "\n", "drop_path_rate", "=", "0.", ",", "\n", "norm_layer", "=", "nn", ".", "LayerNorm", ",", "\n", "post_layers", "=", "(", "'ca'", ",", "'ca'", ")", ",", "\n", "use_aux_head", "=", "True", ",", "\n", "use_mix_token", "=", "False", ",", "\n", "pooling_scale", "=", "2", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "num_layers", "=", "len", "(", "layers", ")", "\n", "mlp_ratio", "=", "to_ntuple", "(", "num_layers", ")", "(", "mlp_ratio", ")", "\n", "img_size", "=", "to_2tuple", "(", "img_size", ")", "\n", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "self", ".", "mix_token", "=", "use_mix_token", "\n", "self", ".", "pooling_scale", "=", "pooling_scale", "\n", "self", ".", "num_features", "=", "embed_dims", "[", "-", "1", "]", "\n", "if", "use_mix_token", ":", "# enable token mixing, see token labeling for details.", "\n", "            ", "self", ".", "beta", "=", "1.0", "\n", "assert", "global_pool", "==", "'token'", ",", "\"return all tokens if mix_token is enabled\"", "\n", "", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "self", ".", "patch_embed", "=", "PatchEmbed", "(", "\n", "stem_conv", "=", "True", ",", "stem_stride", "=", "2", ",", "patch_size", "=", "patch_size", ",", "\n", "in_chans", "=", "in_chans", ",", "hidden_dim", "=", "stem_hidden_dim", ",", "\n", "embed_dim", "=", "embed_dims", "[", "0", "]", ")", "\n", "\n", "# inital positional encoding, we add positional encoding after outlooker blocks", "\n", "patch_grid", "=", "(", "img_size", "[", "0", "]", "//", "patch_size", "//", "pooling_scale", ",", "img_size", "[", "1", "]", "//", "patch_size", "//", "pooling_scale", ")", "\n", "self", ".", "pos_embed", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "patch_grid", "[", "0", "]", ",", "patch_grid", "[", "1", "]", ",", "embed_dims", "[", "-", "1", "]", ")", ")", "\n", "self", ".", "pos_drop", "=", "nn", ".", "Dropout", "(", "p", "=", "drop_rate", ")", "\n", "\n", "# set the main block in network", "\n", "network", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "layers", ")", ")", ":", "\n", "            ", "if", "outlook_attention", "[", "i", "]", ":", "\n", "# stage 1", "\n", "                ", "stage", "=", "outlooker_blocks", "(", "\n", "Outlooker", ",", "i", ",", "embed_dims", "[", "i", "]", ",", "layers", ",", "num_heads", "[", "i", "]", ",", "mlp_ratio", "=", "mlp_ratio", "[", "i", "]", ",", "\n", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop_rate", ",", "norm_layer", "=", "norm_layer", ")", "\n", "network", ".", "append", "(", "stage", ")", "\n", "", "else", ":", "\n", "# stage 2", "\n", "                ", "stage", "=", "transformer_blocks", "(", "\n", "Transformer", ",", "i", ",", "embed_dims", "[", "i", "]", ",", "layers", ",", "num_heads", "[", "i", "]", ",", "mlp_ratio", "=", "mlp_ratio", "[", "i", "]", ",", "qkv_bias", "=", "qkv_bias", ",", "\n", "drop_path_rate", "=", "drop_path_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "norm_layer", "=", "norm_layer", ")", "\n", "network", ".", "append", "(", "stage", ")", "\n", "\n", "", "if", "downsamples", "[", "i", "]", ":", "\n", "# downsampling between two stages", "\n", "                ", "network", ".", "append", "(", "Downsample", "(", "embed_dims", "[", "i", "]", ",", "embed_dims", "[", "i", "+", "1", "]", ",", "2", ")", ")", "\n", "\n", "", "", "self", ".", "network", "=", "nn", ".", "ModuleList", "(", "network", ")", "\n", "\n", "# set post block, for example, class attention layers", "\n", "self", ".", "post_network", "=", "None", "\n", "if", "post_layers", "is", "not", "None", ":", "\n", "            ", "self", ".", "post_network", "=", "nn", ".", "ModuleList", "(", "\n", "[", "\n", "get_block", "(", "\n", "post_layers", "[", "i", "]", ",", "\n", "dim", "=", "embed_dims", "[", "-", "1", "]", ",", "\n", "num_heads", "=", "num_heads", "[", "-", "1", "]", ",", "\n", "mlp_ratio", "=", "mlp_ratio", "[", "-", "1", "]", ",", "\n", "qkv_bias", "=", "qkv_bias", ",", "\n", "attn_drop", "=", "attn_drop_rate", ",", "\n", "drop_path", "=", "0.", ",", "\n", "norm_layer", "=", "norm_layer", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "post_layers", ")", ")", "\n", "]", ")", "\n", "self", ".", "cls_token", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "embed_dims", "[", "-", "1", "]", ")", ")", "\n", "trunc_normal_", "(", "self", ".", "cls_token", ",", "std", "=", ".02", ")", "\n", "\n", "# set output type", "\n", "", "if", "use_aux_head", ":", "\n", "            ", "self", ".", "aux_head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "aux_head", "=", "None", "\n", "", "self", ".", "norm", "=", "norm_layer", "(", "self", ".", "num_features", ")", "\n", "\n", "# Classifier head", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "trunc_normal_", "(", "self", ".", "pos_embed", ",", "std", "=", ".02", ")", "\n", "self", ".", "apply", "(", "self", ".", "_init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.VOLO._init_weights": [[493, 498], ["isinstance", "timm.models.layers.trunc_normal_", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["", "def", "_init_weights", "(", "self", ",", "m", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "            ", "trunc_normal_", "(", "m", ".", "weight", ",", "std", "=", ".02", ")", "\n", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.VOLO.no_weight_decay": [[499, 502], ["None"], "methods", ["None"], ["", "", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "no_weight_decay", "(", "self", ")", ":", "\n", "        ", "return", "{", "'pos_embed'", ",", "'cls_token'", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.VOLO.group_matcher": [[503, 515], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^cls_token|pos_embed|patch_embed'", ",", "# stem and embed", "\n", "blocks", "=", "[", "\n", "(", "r'^network\\.(\\d+)\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^network\\.(\\d+)'", ",", "(", "0", ",", ")", ")", ",", "\n", "]", ",", "\n", "blocks2", "=", "[", "\n", "(", "r'^cls_token'", ",", "(", "0", ",", ")", ")", ",", "\n", "(", "r'^post_network\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^norm'", ",", "(", "99999", ",", ")", ")", "\n", "]", ",", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.VOLO.set_grad_checkpointing": [[518, 521], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.VOLO.get_classifier": [[522, 525], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.VOLO.reset_classifier": [[526, 533], ["torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "self", ".", "global_pool", "=", "global_pool", "\n", "", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "if", "self", ".", "aux_head", "is", "not", "None", ":", "\n", "            ", "self", ".", "aux_head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.VOLO.forward_tokens": [[534, 548], ["enumerate", "block.reshape", "volo.VOLO.pos_drop", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "block", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["None"], ["", "", "def", "forward_tokens", "(", "self", ",", "x", ")", ":", "\n", "        ", "for", "idx", ",", "block", "in", "enumerate", "(", "self", ".", "network", ")", ":", "\n", "            ", "if", "idx", "==", "2", ":", "\n", "# add positional encoding after outlooker blocks", "\n", "                ", "x", "=", "x", "+", "self", ".", "pos_embed", "\n", "x", "=", "self", ".", "pos_drop", "(", "x", ")", "\n", "", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "                ", "x", "=", "checkpoint", "(", "block", ",", "x", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "block", "(", "x", ")", "\n", "\n", "", "", "B", ",", "H", ",", "W", ",", "C", "=", "x", ".", "shape", "\n", "x", "=", "x", ".", "reshape", "(", "B", ",", "-", "1", ",", "C", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.VOLO.forward_cls": [[549, 559], ["volo.VOLO.cls_token.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "block", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["None"], ["", "def", "forward_cls", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "cls_tokens", "=", "self", ".", "cls_token", ".", "expand", "(", "B", ",", "-", "1", ",", "-", "1", ")", "\n", "x", "=", "torch", ".", "cat", "(", "[", "cls_tokens", ",", "x", "]", ",", "dim", "=", "1", ")", "\n", "for", "block", "in", "self", ".", "post_network", ":", "\n", "            ", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "                ", "x", "=", "checkpoint", "(", "block", ",", "x", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "block", "(", "x", ")", "\n", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.VOLO.forward_train": [[560, 611], ["volo.VOLO.patch_embed", "volo.VOLO.permute", "volo.VOLO.forward_tokens", "volo.VOLO.norm", "volo.VOLO.aux_head", "numpy.random.beta", "volo.rand_bbox", "volo.VOLO.clone", "volo.VOLO.forward_cls", "volo.VOLO.mean", "x_aux.reshape.reshape.reshape", "x_aux.reshape.reshape.clone", "x_aux.reshape.reshape.reshape", "volo.VOLO.size", "volo.VOLO.flip", "x_aux.reshape.reshape.flip", "x_aux.reshape.reshape.max"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.VOLO.forward_tokens", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.rand_bbox", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.VOLO.forward_cls"], ["", "def", "forward_train", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\" A separate forward fn for training with mix_token (if a train script supports).\n        Combining multiple modes in as single forward with different return types is torchscript hell.\n        \"\"\"", "\n", "x", "=", "self", ".", "patch_embed", "(", "x", ")", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "# B,C,H,W-> B,H,W,C", "\n", "\n", "# mix token, see token labeling for details.", "\n", "if", "self", ".", "mix_token", "and", "self", ".", "training", ":", "\n", "            ", "lam", "=", "np", ".", "random", ".", "beta", "(", "self", ".", "beta", ",", "self", ".", "beta", ")", "\n", "patch_h", ",", "patch_w", "=", "x", ".", "shape", "[", "1", "]", "//", "self", ".", "pooling_scale", ",", "x", ".", "shape", "[", "2", "]", "//", "self", ".", "pooling_scale", "\n", "bbx1", ",", "bby1", ",", "bbx2", ",", "bby2", "=", "rand_bbox", "(", "x", ".", "size", "(", ")", ",", "lam", ",", "scale", "=", "self", ".", "pooling_scale", ")", "\n", "temp_x", "=", "x", ".", "clone", "(", ")", "\n", "sbbx1", ",", "sbby1", "=", "self", ".", "pooling_scale", "*", "bbx1", ",", "self", ".", "pooling_scale", "*", "bby1", "\n", "sbbx2", ",", "sbby2", "=", "self", ".", "pooling_scale", "*", "bbx2", ",", "self", ".", "pooling_scale", "*", "bby2", "\n", "temp_x", "[", ":", ",", "sbbx1", ":", "sbbx2", ",", "sbby1", ":", "sbby2", ",", ":", "]", "=", "x", ".", "flip", "(", "0", ")", "[", ":", ",", "sbbx1", ":", "sbbx2", ",", "sbby1", ":", "sbby2", ",", ":", "]", "\n", "x", "=", "temp_x", "\n", "", "else", ":", "\n", "            ", "bbx1", ",", "bby1", ",", "bbx2", ",", "bby2", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "\n", "# step2: tokens learning in the two stages", "\n", "", "x", "=", "self", ".", "forward_tokens", "(", "x", ")", "\n", "\n", "# step3: post network, apply class attention or not", "\n", "if", "self", ".", "post_network", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "forward_cls", "(", "x", ")", "\n", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "\n", "if", "self", ".", "global_pool", "==", "'avg'", ":", "\n", "            ", "x_cls", "=", "x", ".", "mean", "(", "dim", "=", "1", ")", "\n", "", "elif", "self", ".", "global_pool", "==", "'token'", ":", "\n", "            ", "x_cls", "=", "x", "[", ":", ",", "0", "]", "\n", "", "else", ":", "\n", "            ", "x_cls", "=", "x", "\n", "\n", "", "if", "self", ".", "aux_head", "is", "None", ":", "\n", "            ", "return", "x_cls", "\n", "\n", "", "x_aux", "=", "self", ".", "aux_head", "(", "x", "[", ":", ",", "1", ":", "]", ")", "# generate classes in all feature tokens, see token labeling", "\n", "if", "not", "self", ".", "training", ":", "\n", "            ", "return", "x_cls", "+", "0.5", "*", "x_aux", ".", "max", "(", "1", ")", "[", "0", "]", "\n", "\n", "", "if", "self", ".", "mix_token", "and", "self", ".", "training", ":", "# reverse \"mix token\", see token labeling for details.", "\n", "            ", "x_aux", "=", "x_aux", ".", "reshape", "(", "x_aux", ".", "shape", "[", "0", "]", ",", "patch_h", ",", "patch_w", ",", "x_aux", ".", "shape", "[", "-", "1", "]", ")", "\n", "temp_x", "=", "x_aux", ".", "clone", "(", ")", "\n", "temp_x", "[", ":", ",", "bbx1", ":", "bbx2", ",", "bby1", ":", "bby2", ",", ":", "]", "=", "x_aux", ".", "flip", "(", "0", ")", "[", ":", ",", "bbx1", ":", "bbx2", ",", "bby1", ":", "bby2", ",", ":", "]", "\n", "x_aux", "=", "temp_x", "\n", "x_aux", "=", "x_aux", ".", "reshape", "(", "x_aux", ".", "shape", "[", "0", "]", ",", "patch_h", "*", "patch_w", ",", "x_aux", ".", "shape", "[", "-", "1", "]", ")", "\n", "\n", "# return these: 1. class token, 2. classes from all feature tokens, 3. bounding box", "\n", "", "return", "x_cls", ",", "x_aux", ",", "(", "bbx1", ",", "bby1", ",", "bbx2", ",", "bby2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.VOLO.forward_features": [[612, 623], ["volo.VOLO.patch_embed().permute", "volo.VOLO.forward_tokens", "volo.VOLO.norm", "volo.VOLO.forward_cls", "volo.VOLO.patch_embed"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.VOLO.forward_tokens", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.VOLO.forward_cls"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "patch_embed", "(", "x", ")", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "# B,C,H,W-> B,H,W,C", "\n", "\n", "# step2: tokens learning in the two stages", "\n", "x", "=", "self", ".", "forward_tokens", "(", "x", ")", "\n", "\n", "# step3: post network, apply class attention or not", "\n", "if", "self", ".", "post_network", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "forward_cls", "(", "x", ")", "\n", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.VOLO.forward_head": [[624, 639], ["volo.VOLO.head", "x.mean", "volo.VOLO.aux_head", "volo.VOLO.max"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "global_pool", "==", "'avg'", ":", "\n", "            ", "out", "=", "x", ".", "mean", "(", "dim", "=", "1", ")", "\n", "", "elif", "self", ".", "global_pool", "==", "'token'", ":", "\n", "            ", "out", "=", "x", "[", ":", ",", "0", "]", "\n", "", "else", ":", "\n", "            ", "out", "=", "x", "\n", "", "if", "pre_logits", ":", "\n", "            ", "return", "out", "\n", "", "out", "=", "self", ".", "head", "(", "out", ")", "\n", "if", "self", ".", "aux_head", "is", "not", "None", ":", "\n", "# generate classes in all feature tokens, see token labeling", "\n", "            ", "aux", "=", "self", ".", "aux_head", "(", "x", "[", ":", ",", "1", ":", "]", ")", "\n", "out", "=", "out", "+", "0.5", "*", "aux", ".", "max", "(", "1", ")", "[", "0", "]", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.VOLO.forward": [[640, 645], ["volo.VOLO.forward_features", "volo.VOLO.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\" simplified forward (without mix token training) \"\"\"", "\n", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo._cfg": [[36, 44], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", ".96", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'patch_embed.conv.0'", ",", "'classifier'", ":", "(", "'head'", ",", "'aux_head'", ")", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.get_block": [[266, 269], ["volo.ClassBlock"], "function", ["None"], ["", "", "def", "get_block", "(", "block_type", ",", "**", "kargs", ")", ":", "\n", "    ", "if", "block_type", "==", "'ca'", ":", "\n", "        ", "return", "ClassBlock", "(", "**", "kargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.rand_bbox": [[271, 292], ["numpy.sqrt", "numpy.int", "numpy.int", "numpy.random.randint", "numpy.random.randint", "numpy.clip", "numpy.clip", "numpy.clip", "numpy.clip"], "function", ["None"], ["", "", "def", "rand_bbox", "(", "size", ",", "lam", ",", "scale", "=", "1", ")", ":", "\n", "    ", "\"\"\"\n    get bounding box as token labeling (https://github.com/zihangJiang/TokenLabeling)\n    return: bounding box\n    \"\"\"", "\n", "W", "=", "size", "[", "1", "]", "//", "scale", "\n", "H", "=", "size", "[", "2", "]", "//", "scale", "\n", "cut_rat", "=", "np", ".", "sqrt", "(", "1.", "-", "lam", ")", "\n", "cut_w", "=", "np", ".", "int", "(", "W", "*", "cut_rat", ")", "\n", "cut_h", "=", "np", ".", "int", "(", "H", "*", "cut_rat", ")", "\n", "\n", "# uniform", "\n", "cx", "=", "np", ".", "random", ".", "randint", "(", "W", ")", "\n", "cy", "=", "np", ".", "random", ".", "randint", "(", "H", ")", "\n", "\n", "bbx1", "=", "np", ".", "clip", "(", "cx", "-", "cut_w", "//", "2", ",", "0", ",", "W", ")", "\n", "bby1", "=", "np", ".", "clip", "(", "cy", "-", "cut_h", "//", "2", ",", "0", ",", "H", ")", "\n", "bbx2", "=", "np", ".", "clip", "(", "cx", "+", "cut_w", "//", "2", ",", "0", ",", "W", ")", "\n", "bby2", "=", "np", ".", "clip", "(", "cy", "+", "cut_h", "//", "2", ",", "0", ",", "H", ")", "\n", "\n", "return", "bbx1", ",", "bby1", ",", "bbx2", ",", "bby2", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.outlooker_blocks": [[345, 362], ["range", "torch.Sequential", "nn.Sequential.append", "block_fn", "sum", "sum"], "function", ["None"], ["", "", "def", "outlooker_blocks", "(", "\n", "block_fn", ",", "index", ",", "dim", ",", "layers", ",", "num_heads", "=", "1", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ",", "stride", "=", "2", ",", "\n", "mlp_ratio", "=", "3.", ",", "qkv_bias", "=", "False", ",", "attn_drop", "=", "0", ",", "drop_path_rate", "=", "0.", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    generate outlooker layer in stage1\n    return: outlooker layers\n    \"\"\"", "\n", "blocks", "=", "[", "]", "\n", "for", "block_idx", "in", "range", "(", "layers", "[", "index", "]", ")", ":", "\n", "        ", "block_dpr", "=", "drop_path_rate", "*", "(", "block_idx", "+", "sum", "(", "layers", "[", ":", "index", "]", ")", ")", "/", "(", "sum", "(", "layers", ")", "-", "1", ")", "\n", "blocks", ".", "append", "(", "\n", "block_fn", "(", "\n", "dim", ",", "kernel_size", "=", "kernel_size", ",", "padding", "=", "padding", ",", "\n", "stride", "=", "stride", ",", "num_heads", "=", "num_heads", ",", "mlp_ratio", "=", "mlp_ratio", ",", "\n", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ",", "drop_path", "=", "block_dpr", ")", ")", "\n", "", "blocks", "=", "nn", ".", "Sequential", "(", "*", "blocks", ")", "\n", "return", "blocks", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.transformer_blocks": [[364, 383], ["range", "torch.Sequential", "nn.Sequential.append", "block_fn", "sum", "sum"], "function", ["None"], ["", "def", "transformer_blocks", "(", "\n", "block_fn", ",", "index", ",", "dim", ",", "layers", ",", "num_heads", ",", "mlp_ratio", "=", "3.", ",", "\n", "qkv_bias", "=", "False", ",", "attn_drop", "=", "0", ",", "drop_path_rate", "=", "0.", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    generate transformer layers in stage2\n    return: transformer layers\n    \"\"\"", "\n", "blocks", "=", "[", "]", "\n", "for", "block_idx", "in", "range", "(", "layers", "[", "index", "]", ")", ":", "\n", "        ", "block_dpr", "=", "drop_path_rate", "*", "(", "block_idx", "+", "sum", "(", "layers", "[", ":", "index", "]", ")", ")", "/", "(", "sum", "(", "layers", ")", "-", "1", ")", "\n", "blocks", ".", "append", "(", "\n", "block_fn", "(", "\n", "dim", ",", "num_heads", ",", "\n", "mlp_ratio", "=", "mlp_ratio", ",", "\n", "qkv_bias", "=", "qkv_bias", ",", "\n", "attn_drop", "=", "attn_drop", ",", "\n", "drop_path", "=", "block_dpr", ")", ")", "\n", "", "blocks", "=", "nn", ".", "Sequential", "(", "*", "blocks", ")", "\n", "return", "blocks", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo._create_volo": [[647, 651], ["kwargs.get", "timm.models.helpers.build_model_with_cfg", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_volo", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "kwargs", ".", "get", "(", "'features_only'", ",", "None", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'features_only not implemented for Vision Transformer models.'", ")", "\n", "", "return", "build_model_with_cfg", "(", "VOLO", ",", "variant", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.volo_d1_224": [[653, 659], ["dict", "volo._create_volo"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo._create_volo"], ["", "@", "register_model", "\n", "def", "volo_d1_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" VOLO-D1 model, Params: 27M \"\"\"", "\n", "model_args", "=", "dict", "(", "layers", "=", "(", "4", ",", "4", ",", "8", ",", "2", ")", ",", "embed_dims", "=", "(", "192", ",", "384", ",", "384", ",", "384", ")", ",", "num_heads", "=", "(", "6", ",", "12", ",", "12", ",", "12", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_volo", "(", "'volo_d1_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.volo_d1_384": [[661, 667], ["dict", "volo._create_volo"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo._create_volo"], ["", "@", "register_model", "\n", "def", "volo_d1_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" VOLO-D1 model, Params: 27M \"\"\"", "\n", "model_args", "=", "dict", "(", "layers", "=", "(", "4", ",", "4", ",", "8", ",", "2", ")", ",", "embed_dims", "=", "(", "192", ",", "384", ",", "384", ",", "384", ")", ",", "num_heads", "=", "(", "6", ",", "12", ",", "12", ",", "12", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_volo", "(", "'volo_d1_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.volo_d2_224": [[669, 675], ["dict", "volo._create_volo"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo._create_volo"], ["", "@", "register_model", "\n", "def", "volo_d2_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" VOLO-D2 model, Params: 59M \"\"\"", "\n", "model_args", "=", "dict", "(", "layers", "=", "(", "6", ",", "4", ",", "10", ",", "4", ")", ",", "embed_dims", "=", "(", "256", ",", "512", ",", "512", ",", "512", ")", ",", "num_heads", "=", "(", "8", ",", "16", ",", "16", ",", "16", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_volo", "(", "'volo_d2_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.volo_d2_384": [[677, 683], ["dict", "volo._create_volo"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo._create_volo"], ["", "@", "register_model", "\n", "def", "volo_d2_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" VOLO-D2 model, Params: 59M \"\"\"", "\n", "model_args", "=", "dict", "(", "layers", "=", "(", "6", ",", "4", ",", "10", ",", "4", ")", ",", "embed_dims", "=", "(", "256", ",", "512", ",", "512", ",", "512", ")", ",", "num_heads", "=", "(", "8", ",", "16", ",", "16", ",", "16", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_volo", "(", "'volo_d2_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.volo_d3_224": [[685, 691], ["dict", "volo._create_volo"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo._create_volo"], ["", "@", "register_model", "\n", "def", "volo_d3_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" VOLO-D3 model, Params: 86M \"\"\"", "\n", "model_args", "=", "dict", "(", "layers", "=", "(", "8", ",", "8", ",", "16", ",", "4", ")", ",", "embed_dims", "=", "(", "256", ",", "512", ",", "512", ",", "512", ")", ",", "num_heads", "=", "(", "8", ",", "16", ",", "16", ",", "16", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_volo", "(", "'volo_d3_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.volo_d3_448": [[693, 699], ["dict", "volo._create_volo"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo._create_volo"], ["", "@", "register_model", "\n", "def", "volo_d3_448", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" VOLO-D3 model, Params: 86M \"\"\"", "\n", "model_args", "=", "dict", "(", "layers", "=", "(", "8", ",", "8", ",", "16", ",", "4", ")", ",", "embed_dims", "=", "(", "256", ",", "512", ",", "512", ",", "512", ")", ",", "num_heads", "=", "(", "8", ",", "16", ",", "16", ",", "16", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_volo", "(", "'volo_d3_448'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.volo_d4_224": [[701, 707], ["dict", "volo._create_volo"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo._create_volo"], ["", "@", "register_model", "\n", "def", "volo_d4_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" VOLO-D4 model, Params: 193M \"\"\"", "\n", "model_args", "=", "dict", "(", "layers", "=", "(", "8", ",", "8", ",", "16", ",", "4", ")", ",", "embed_dims", "=", "(", "384", ",", "768", ",", "768", ",", "768", ")", ",", "num_heads", "=", "(", "12", ",", "16", ",", "16", ",", "16", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_volo", "(", "'volo_d4_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.volo_d4_448": [[709, 715], ["dict", "volo._create_volo"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo._create_volo"], ["", "@", "register_model", "\n", "def", "volo_d4_448", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" VOLO-D4 model, Params: 193M \"\"\"", "\n", "model_args", "=", "dict", "(", "layers", "=", "(", "8", ",", "8", ",", "16", ",", "4", ")", ",", "embed_dims", "=", "(", "384", ",", "768", ",", "768", ",", "768", ")", ",", "num_heads", "=", "(", "12", ",", "16", ",", "16", ",", "16", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_volo", "(", "'volo_d4_448'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.volo_d5_224": [[717, 727], ["dict", "volo._create_volo"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo._create_volo"], ["", "@", "register_model", "\n", "def", "volo_d5_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" VOLO-D5 model, Params: 296M\n    stem_hidden_dim=128, the dim in patch embedding is 128 for VOLO-D5\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "layers", "=", "(", "12", ",", "12", ",", "20", ",", "4", ")", ",", "embed_dims", "=", "(", "384", ",", "768", ",", "768", ",", "768", ")", ",", "num_heads", "=", "(", "12", ",", "16", ",", "16", ",", "16", ")", ",", "\n", "mlp_ratio", "=", "4", ",", "stem_hidden_dim", "=", "128", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_volo", "(", "'volo_d5_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.volo_d5_448": [[729, 739], ["dict", "volo._create_volo"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo._create_volo"], ["", "@", "register_model", "\n", "def", "volo_d5_448", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" VOLO-D5 model, Params: 296M\n    stem_hidden_dim=128, the dim in patch embedding is 128 for VOLO-D5\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "layers", "=", "(", "12", ",", "12", ",", "20", ",", "4", ")", ",", "embed_dims", "=", "(", "384", ",", "768", ",", "768", ",", "768", ")", ",", "num_heads", "=", "(", "12", ",", "16", ",", "16", ",", "16", ")", ",", "\n", "mlp_ratio", "=", "4", ",", "stem_hidden_dim", "=", "128", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_volo", "(", "'volo_d5_448'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo.volo_d5_512": [[741, 751], ["dict", "volo._create_volo"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.volo._create_volo"], ["", "@", "register_model", "\n", "def", "volo_d5_512", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" VOLO-D5 model, Params: 296M\n    stem_hidden_dim=128, the dim in patch embedding is 128 for VOLO-D5\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "layers", "=", "(", "12", ",", "12", ",", "20", ",", "4", ")", ",", "embed_dims", "=", "(", "384", ",", "768", ",", "768", ",", "768", ")", ",", "num_heads", "=", "(", "12", ",", "16", ",", "16", ",", "16", ")", ",", "\n", "mlp_ratio", "=", "4", ",", "stem_hidden_dim", "=", "128", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_volo", "(", "'volo_d5_512'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionModule.__init__": [[389, 405], ["torch.Module.__init__", "hrnet.HighResolutionModule._check_branches", "hrnet.HighResolutionModule._make_branches", "hrnet.HighResolutionModule._make_fuse_layers", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionModule._check_branches", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionModule._make_branches", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionModule._make_fuse_layers"], ["    ", "def", "__init__", "(", "self", ",", "num_branches", ",", "blocks", ",", "num_blocks", ",", "num_in_chs", ",", "\n", "num_channels", ",", "fuse_method", ",", "multi_scale_output", "=", "True", ")", ":", "\n", "        ", "super", "(", "HighResolutionModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_check_branches", "(", "\n", "num_branches", ",", "blocks", ",", "num_blocks", ",", "num_in_chs", ",", "num_channels", ")", "\n", "\n", "self", ".", "num_in_chs", "=", "num_in_chs", "\n", "self", ".", "fuse_method", "=", "fuse_method", "\n", "self", ".", "num_branches", "=", "num_branches", "\n", "\n", "self", ".", "multi_scale_output", "=", "multi_scale_output", "\n", "\n", "self", ".", "branches", "=", "self", ".", "_make_branches", "(", "\n", "num_branches", ",", "blocks", ",", "num_blocks", ",", "num_channels", ")", "\n", "self", ".", "fuse_layers", "=", "self", ".", "_make_fuse_layers", "(", ")", "\n", "self", ".", "fuse_act", "=", "nn", ".", "ReLU", "(", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionModule._check_branches": [[406, 417], ["len", "_logger.error", "ValueError", "len", "len", "len", "len", "len"], "methods", ["None"], ["", "def", "_check_branches", "(", "self", ",", "num_branches", ",", "blocks", ",", "num_blocks", ",", "num_in_chs", ",", "num_channels", ")", ":", "\n", "        ", "error_msg", "=", "''", "\n", "if", "num_branches", "!=", "len", "(", "num_blocks", ")", ":", "\n", "            ", "error_msg", "=", "'NUM_BRANCHES({}) <> NUM_BLOCKS({})'", ".", "format", "(", "num_branches", ",", "len", "(", "num_blocks", ")", ")", "\n", "", "elif", "num_branches", "!=", "len", "(", "num_channels", ")", ":", "\n", "            ", "error_msg", "=", "'NUM_BRANCHES({}) <> NUM_CHANNELS({})'", ".", "format", "(", "num_branches", ",", "len", "(", "num_channels", ")", ")", "\n", "", "elif", "num_branches", "!=", "len", "(", "num_in_chs", ")", ":", "\n", "            ", "error_msg", "=", "'NUM_BRANCHES({}) <> num_in_chs({})'", ".", "format", "(", "num_branches", ",", "len", "(", "num_in_chs", ")", ")", "\n", "", "if", "error_msg", ":", "\n", "            ", "_logger", ".", "error", "(", "error_msg", ")", "\n", "raise", "ValueError", "(", "error_msg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionModule._make_one_branch": [[418, 434], ["range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "block", "layers.append", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "block"], "methods", ["None"], ["", "", "def", "_make_one_branch", "(", "self", ",", "branch_index", ",", "block", ",", "num_blocks", ",", "num_channels", ",", "stride", "=", "1", ")", ":", "\n", "        ", "downsample", "=", "None", "\n", "if", "stride", "!=", "1", "or", "self", ".", "num_in_chs", "[", "branch_index", "]", "!=", "num_channels", "[", "branch_index", "]", "*", "block", ".", "expansion", ":", "\n", "            ", "downsample", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "\n", "self", ".", "num_in_chs", "[", "branch_index", "]", ",", "num_channels", "[", "branch_index", "]", "*", "block", ".", "expansion", ",", "\n", "kernel_size", "=", "1", ",", "stride", "=", "stride", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "num_channels", "[", "branch_index", "]", "*", "block", ".", "expansion", ",", "momentum", "=", "_BN_MOMENTUM", ")", ",", "\n", ")", "\n", "\n", "", "layers", "=", "[", "block", "(", "self", ".", "num_in_chs", "[", "branch_index", "]", ",", "num_channels", "[", "branch_index", "]", ",", "stride", ",", "downsample", ")", "]", "\n", "self", ".", "num_in_chs", "[", "branch_index", "]", "=", "num_channels", "[", "branch_index", "]", "*", "block", ".", "expansion", "\n", "for", "i", "in", "range", "(", "1", ",", "num_blocks", "[", "branch_index", "]", ")", ":", "\n", "            ", "layers", ".", "append", "(", "block", "(", "self", ".", "num_in_chs", "[", "branch_index", "]", ",", "num_channels", "[", "branch_index", "]", ")", ")", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionModule._make_branches": [[435, 441], ["range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "branches.append", "hrnet.HighResolutionModule._make_one_branch"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionModule._make_one_branch"], ["", "def", "_make_branches", "(", "self", ",", "num_branches", ",", "block", ",", "num_blocks", ",", "num_channels", ")", ":", "\n", "        ", "branches", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_branches", ")", ":", "\n", "            ", "branches", ".", "append", "(", "self", ".", "_make_one_branch", "(", "i", ",", "block", ",", "num_blocks", ",", "num_channels", ")", ")", "\n", "\n", "", "return", "nn", ".", "ModuleList", "(", "branches", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionModule._make_fuse_layers": [[442, 477], ["range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Identity", "torch.Identity", "torch.Identity", "range", "fuse_layers.append", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "fuse_layer.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "fuse_layer.append", "range", "fuse_layer.append", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Upsample", "torch.Upsample", "torch.Upsample", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Sequential", "torch.Sequential", "torch.Sequential", "conv3x3s.append", "conv3x3s.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["None"], ["", "def", "_make_fuse_layers", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "num_branches", "==", "1", ":", "\n", "            ", "return", "nn", ".", "Identity", "(", ")", "\n", "\n", "", "num_branches", "=", "self", ".", "num_branches", "\n", "num_in_chs", "=", "self", ".", "num_in_chs", "\n", "fuse_layers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_branches", "if", "self", ".", "multi_scale_output", "else", "1", ")", ":", "\n", "            ", "fuse_layer", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "num_branches", ")", ":", "\n", "                ", "if", "j", ">", "i", ":", "\n", "                    ", "fuse_layer", ".", "append", "(", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "num_in_chs", "[", "j", "]", ",", "num_in_chs", "[", "i", "]", ",", "1", ",", "1", ",", "0", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "num_in_chs", "[", "i", "]", ",", "momentum", "=", "_BN_MOMENTUM", ")", ",", "\n", "nn", ".", "Upsample", "(", "scale_factor", "=", "2", "**", "(", "j", "-", "i", ")", ",", "mode", "=", "'nearest'", ")", ")", ")", "\n", "", "elif", "j", "==", "i", ":", "\n", "                    ", "fuse_layer", ".", "append", "(", "nn", ".", "Identity", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "conv3x3s", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "i", "-", "j", ")", ":", "\n", "                        ", "if", "k", "==", "i", "-", "j", "-", "1", ":", "\n", "                            ", "num_outchannels_conv3x3", "=", "num_in_chs", "[", "i", "]", "\n", "conv3x3s", ".", "append", "(", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "num_in_chs", "[", "j", "]", ",", "num_outchannels_conv3x3", ",", "3", ",", "2", ",", "1", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "num_outchannels_conv3x3", ",", "momentum", "=", "_BN_MOMENTUM", ")", ")", ")", "\n", "", "else", ":", "\n", "                            ", "num_outchannels_conv3x3", "=", "num_in_chs", "[", "j", "]", "\n", "conv3x3s", ".", "append", "(", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "num_in_chs", "[", "j", "]", ",", "num_outchannels_conv3x3", ",", "3", ",", "2", ",", "1", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "num_outchannels_conv3x3", ",", "momentum", "=", "_BN_MOMENTUM", ")", ",", "\n", "nn", ".", "ReLU", "(", "False", ")", ")", ")", "\n", "", "", "fuse_layer", ".", "append", "(", "nn", ".", "Sequential", "(", "*", "conv3x3s", ")", ")", "\n", "", "", "fuse_layers", ".", "append", "(", "nn", ".", "ModuleList", "(", "fuse_layer", ")", ")", "\n", "\n", "", "return", "nn", ".", "ModuleList", "(", "fuse_layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionModule.get_num_in_chs": [[478, 480], ["None"], "methods", ["None"], ["", "def", "get_num_in_chs", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_in_chs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionModule.forward": [[481, 499], ["enumerate", "enumerate", "branch", "range", "x_fuse.append", "hrnet.HighResolutionModule.fuse_act"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "List", "[", "torch", ".", "Tensor", "]", ")", ":", "\n", "        ", "if", "self", ".", "num_branches", "==", "1", ":", "\n", "            ", "return", "[", "self", ".", "branches", "[", "0", "]", "(", "x", "[", "0", "]", ")", "]", "\n", "\n", "", "for", "i", ",", "branch", "in", "enumerate", "(", "self", ".", "branches", ")", ":", "\n", "            ", "x", "[", "i", "]", "=", "branch", "(", "x", "[", "i", "]", ")", "\n", "\n", "", "x_fuse", "=", "[", "]", "\n", "for", "i", ",", "fuse_outer", "in", "enumerate", "(", "self", ".", "fuse_layers", ")", ":", "\n", "            ", "y", "=", "x", "[", "0", "]", "if", "i", "==", "0", "else", "fuse_outer", "[", "0", "]", "(", "x", "[", "0", "]", ")", "\n", "for", "j", "in", "range", "(", "1", ",", "self", ".", "num_branches", ")", ":", "\n", "                ", "if", "i", "==", "j", ":", "\n", "                    ", "y", "=", "y", "+", "x", "[", "j", "]", "\n", "", "else", ":", "\n", "                    ", "y", "=", "y", "+", "fuse_outer", "[", "j", "]", "(", "x", "[", "j", "]", ")", "\n", "", "", "x_fuse", ".", "append", "(", "self", ".", "fuse_act", "(", "y", ")", ")", "\n", "\n", "", "return", "x_fuse", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet.__init__": [[509, 574], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "hrnet.HighResolutionNet._make_layer", "hrnet.HighResolutionNet._make_transition_layer", "hrnet.HighResolutionNet._make_stage", "hrnet.HighResolutionNet._make_transition_layer", "hrnet.HighResolutionNet._make_stage", "hrnet.HighResolutionNet._make_transition_layer", "hrnet.HighResolutionNet._make_stage", "enumerate", "hrnet.HighResolutionNet.init_weights", "hrnet.HighResolutionNet._make_head", "layers.create_classifier", "dict", "range", "range", "range", "hrnet.HighResolutionNet._make_head", "dict", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.TResNet._make_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet._make_transition_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet._make_stage", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet._make_transition_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet._make_stage", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet._make_transition_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet._make_stage", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet._make_head", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet._make_head"], ["    ", "def", "__init__", "(", "self", ",", "cfg", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "global_pool", "=", "'avg'", ",", "drop_rate", "=", "0.0", ",", "head", "=", "'classification'", ")", ":", "\n", "        ", "super", "(", "HighResolutionNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "\n", "stem_width", "=", "cfg", "[", "'STEM_WIDTH'", "]", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "in_chans", ",", "stem_width", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm2d", "(", "stem_width", ",", "momentum", "=", "_BN_MOMENTUM", ")", "\n", "self", ".", "act1", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "stem_width", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "BatchNorm2d", "(", "64", ",", "momentum", "=", "_BN_MOMENTUM", ")", "\n", "self", ".", "act2", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n", "self", ".", "stage1_cfg", "=", "cfg", "[", "'STAGE1'", "]", "\n", "num_channels", "=", "self", ".", "stage1_cfg", "[", "'NUM_CHANNELS'", "]", "[", "0", "]", "\n", "block", "=", "blocks_dict", "[", "self", ".", "stage1_cfg", "[", "'BLOCK'", "]", "]", "\n", "num_blocks", "=", "self", ".", "stage1_cfg", "[", "'NUM_BLOCKS'", "]", "[", "0", "]", "\n", "self", ".", "layer1", "=", "self", ".", "_make_layer", "(", "block", ",", "64", ",", "num_channels", ",", "num_blocks", ")", "\n", "stage1_out_channel", "=", "block", ".", "expansion", "*", "num_channels", "\n", "\n", "self", ".", "stage2_cfg", "=", "cfg", "[", "'STAGE2'", "]", "\n", "num_channels", "=", "self", ".", "stage2_cfg", "[", "'NUM_CHANNELS'", "]", "\n", "block", "=", "blocks_dict", "[", "self", ".", "stage2_cfg", "[", "'BLOCK'", "]", "]", "\n", "num_channels", "=", "[", "num_channels", "[", "i", "]", "*", "block", ".", "expansion", "for", "i", "in", "range", "(", "len", "(", "num_channels", ")", ")", "]", "\n", "self", ".", "transition1", "=", "self", ".", "_make_transition_layer", "(", "[", "stage1_out_channel", "]", ",", "num_channels", ")", "\n", "self", ".", "stage2", ",", "pre_stage_channels", "=", "self", ".", "_make_stage", "(", "self", ".", "stage2_cfg", ",", "num_channels", ")", "\n", "\n", "self", ".", "stage3_cfg", "=", "cfg", "[", "'STAGE3'", "]", "\n", "num_channels", "=", "self", ".", "stage3_cfg", "[", "'NUM_CHANNELS'", "]", "\n", "block", "=", "blocks_dict", "[", "self", ".", "stage3_cfg", "[", "'BLOCK'", "]", "]", "\n", "num_channels", "=", "[", "num_channels", "[", "i", "]", "*", "block", ".", "expansion", "for", "i", "in", "range", "(", "len", "(", "num_channels", ")", ")", "]", "\n", "self", ".", "transition2", "=", "self", ".", "_make_transition_layer", "(", "pre_stage_channels", ",", "num_channels", ")", "\n", "self", ".", "stage3", ",", "pre_stage_channels", "=", "self", ".", "_make_stage", "(", "self", ".", "stage3_cfg", ",", "num_channels", ")", "\n", "\n", "self", ".", "stage4_cfg", "=", "cfg", "[", "'STAGE4'", "]", "\n", "num_channels", "=", "self", ".", "stage4_cfg", "[", "'NUM_CHANNELS'", "]", "\n", "block", "=", "blocks_dict", "[", "self", ".", "stage4_cfg", "[", "'BLOCK'", "]", "]", "\n", "num_channels", "=", "[", "num_channels", "[", "i", "]", "*", "block", ".", "expansion", "for", "i", "in", "range", "(", "len", "(", "num_channels", ")", ")", "]", "\n", "self", ".", "transition3", "=", "self", ".", "_make_transition_layer", "(", "pre_stage_channels", ",", "num_channels", ")", "\n", "self", ".", "stage4", ",", "pre_stage_channels", "=", "self", ".", "_make_stage", "(", "self", ".", "stage4_cfg", ",", "num_channels", ",", "multi_scale_output", "=", "True", ")", "\n", "\n", "self", ".", "head", "=", "head", "\n", "self", ".", "head_channels", "=", "None", "# set if _make_head called", "\n", "if", "head", "==", "'classification'", ":", "\n", "# Classification Head", "\n", "            ", "self", ".", "num_features", "=", "2048", "\n", "self", ".", "incre_modules", ",", "self", ".", "downsamp_modules", ",", "self", ".", "final_layer", "=", "self", ".", "_make_head", "(", "pre_stage_channels", ")", "\n", "self", ".", "global_pool", ",", "self", ".", "classifier", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "", "elif", "head", "==", "'incre'", ":", "\n", "            ", "self", ".", "num_features", "=", "2048", "\n", "self", ".", "incre_modules", ",", "_", ",", "_", "=", "self", ".", "_make_head", "(", "pre_stage_channels", ",", "True", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "incre_modules", "=", "None", "\n", "self", ".", "num_features", "=", "256", "\n", "\n", "", "curr_stride", "=", "2", "\n", "# module names aren't actually valid here, hook or FeatureNet based extraction would not work", "\n", "self", ".", "feature_info", "=", "[", "dict", "(", "num_chs", "=", "64", ",", "reduction", "=", "curr_stride", ",", "module", "=", "'stem'", ")", "]", "\n", "for", "i", ",", "c", "in", "enumerate", "(", "self", ".", "head_channels", "if", "self", ".", "head_channels", "else", "num_channels", ")", ":", "\n", "            ", "curr_stride", "*=", "2", "\n", "c", "=", "c", "*", "4", "if", "self", ".", "head_channels", "else", "c", "# head block expansion factor of 4", "\n", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "c", ",", "reduction", "=", "curr_stride", ",", "module", "=", "f'stage{i + 1}'", ")", "]", "\n", "\n", "", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet._make_head": [[575, 612], ["enumerate", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.ModuleList.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.ModuleList.append", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "hrnet.HighResolutionNet._make_layer", "len", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.TResNet._make_layer"], ["", "def", "_make_head", "(", "self", ",", "pre_stage_channels", ",", "incre_only", "=", "False", ")", ":", "\n", "        ", "head_block", "=", "Bottleneck", "\n", "self", ".", "head_channels", "=", "[", "32", ",", "64", ",", "128", ",", "256", "]", "\n", "\n", "# Increasing the #channels on each resolution", "\n", "# from C, 2C, 4C, 8C to 128, 256, 512, 1024", "\n", "incre_modules", "=", "[", "]", "\n", "for", "i", ",", "channels", "in", "enumerate", "(", "pre_stage_channels", ")", ":", "\n", "            ", "incre_modules", ".", "append", "(", "self", ".", "_make_layer", "(", "head_block", ",", "channels", ",", "self", ".", "head_channels", "[", "i", "]", ",", "1", ",", "stride", "=", "1", ")", ")", "\n", "", "incre_modules", "=", "nn", ".", "ModuleList", "(", "incre_modules", ")", "\n", "if", "incre_only", ":", "\n", "            ", "return", "incre_modules", ",", "None", ",", "None", "\n", "\n", "# downsampling modules", "\n", "", "downsamp_modules", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "pre_stage_channels", ")", "-", "1", ")", ":", "\n", "            ", "in_channels", "=", "self", ".", "head_channels", "[", "i", "]", "*", "head_block", ".", "expansion", "\n", "out_channels", "=", "self", ".", "head_channels", "[", "i", "+", "1", "]", "*", "head_block", ".", "expansion", "\n", "downsamp_module", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "\n", "in_channels", "=", "in_channels", ",", "out_channels", "=", "out_channels", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "out_channels", ",", "momentum", "=", "_BN_MOMENTUM", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", ")", "\n", "downsamp_modules", ".", "append", "(", "downsamp_module", ")", "\n", "", "downsamp_modules", "=", "nn", ".", "ModuleList", "(", "downsamp_modules", ")", "\n", "\n", "final_layer", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "\n", "in_channels", "=", "self", ".", "head_channels", "[", "3", "]", "*", "head_block", ".", "expansion", ",", "\n", "out_channels", "=", "self", ".", "num_features", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", "\n", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "self", ".", "num_features", ",", "momentum", "=", "_BN_MOMENTUM", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", ")", "\n", "\n", "return", "incre_modules", ",", "downsamp_modules", ",", "final_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet._make_transition_layer": [[613, 639], ["len", "len", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "transition_layers.append", "transition_layers.append", "transition_layers.append", "conv3x3s.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["None"], ["", "def", "_make_transition_layer", "(", "self", ",", "num_channels_pre_layer", ",", "num_channels_cur_layer", ")", ":", "\n", "        ", "num_branches_cur", "=", "len", "(", "num_channels_cur_layer", ")", "\n", "num_branches_pre", "=", "len", "(", "num_channels_pre_layer", ")", "\n", "\n", "transition_layers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_branches_cur", ")", ":", "\n", "            ", "if", "i", "<", "num_branches_pre", ":", "\n", "                ", "if", "num_channels_cur_layer", "[", "i", "]", "!=", "num_channels_pre_layer", "[", "i", "]", ":", "\n", "                    ", "transition_layers", ".", "append", "(", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "num_channels_pre_layer", "[", "i", "]", ",", "num_channels_cur_layer", "[", "i", "]", ",", "3", ",", "1", ",", "1", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "num_channels_cur_layer", "[", "i", "]", ",", "momentum", "=", "_BN_MOMENTUM", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", ")", "\n", "", "else", ":", "\n", "                    ", "transition_layers", ".", "append", "(", "nn", ".", "Identity", "(", ")", ")", "\n", "", "", "else", ":", "\n", "                ", "conv3x3s", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "i", "+", "1", "-", "num_branches_pre", ")", ":", "\n", "                    ", "inchannels", "=", "num_channels_pre_layer", "[", "-", "1", "]", "\n", "outchannels", "=", "num_channels_cur_layer", "[", "i", "]", "if", "j", "==", "i", "-", "num_branches_pre", "else", "inchannels", "\n", "conv3x3s", ".", "append", "(", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "inchannels", ",", "outchannels", ",", "3", ",", "2", ",", "1", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "outchannels", ",", "momentum", "=", "_BN_MOMENTUM", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", ")", "\n", "", "transition_layers", ".", "append", "(", "nn", ".", "Sequential", "(", "*", "conv3x3s", ")", ")", "\n", "\n", "", "", "return", "nn", ".", "ModuleList", "(", "transition_layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet._make_layer": [[640, 654], ["range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "block", "layers.append", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "block"], "methods", ["None"], ["", "def", "_make_layer", "(", "self", ",", "block", ",", "inplanes", ",", "planes", ",", "blocks", ",", "stride", "=", "1", ")", ":", "\n", "        ", "downsample", "=", "None", "\n", "if", "stride", "!=", "1", "or", "inplanes", "!=", "planes", "*", "block", ".", "expansion", ":", "\n", "            ", "downsample", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "inplanes", ",", "planes", "*", "block", ".", "expansion", ",", "kernel_size", "=", "1", ",", "stride", "=", "stride", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "planes", "*", "block", ".", "expansion", ",", "momentum", "=", "_BN_MOMENTUM", ")", ",", "\n", ")", "\n", "\n", "", "layers", "=", "[", "block", "(", "inplanes", ",", "planes", ",", "stride", ",", "downsample", ")", "]", "\n", "inplanes", "=", "planes", "*", "block", ".", "expansion", "\n", "for", "i", "in", "range", "(", "1", ",", "blocks", ")", ":", "\n", "            ", "layers", ".", "append", "(", "block", "(", "inplanes", ",", "planes", ")", ")", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet._make_stage": [[655, 673], ["range", "modules.append", "modules[].get_num_in_chs", "torch.Sequential", "torch.Sequential", "torch.Sequential", "hrnet.HighResolutionModule"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionModule.get_num_in_chs"], ["", "def", "_make_stage", "(", "self", ",", "layer_config", ",", "num_in_chs", ",", "multi_scale_output", "=", "True", ")", ":", "\n", "        ", "num_modules", "=", "layer_config", "[", "'NUM_MODULES'", "]", "\n", "num_branches", "=", "layer_config", "[", "'NUM_BRANCHES'", "]", "\n", "num_blocks", "=", "layer_config", "[", "'NUM_BLOCKS'", "]", "\n", "num_channels", "=", "layer_config", "[", "'NUM_CHANNELS'", "]", "\n", "block", "=", "blocks_dict", "[", "layer_config", "[", "'BLOCK'", "]", "]", "\n", "fuse_method", "=", "layer_config", "[", "'FUSE_METHOD'", "]", "\n", "\n", "modules", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_modules", ")", ":", "\n", "# multi_scale_output is only used last module", "\n", "            ", "reset_multi_scale_output", "=", "multi_scale_output", "or", "i", "<", "num_modules", "-", "1", "\n", "modules", ".", "append", "(", "HighResolutionModule", "(", "\n", "num_branches", ",", "block", ",", "num_blocks", ",", "num_in_chs", ",", "num_channels", ",", "fuse_method", ",", "reset_multi_scale_output", ")", "\n", ")", "\n", "num_in_chs", "=", "modules", "[", "-", "1", "]", ".", "get_num_in_chs", "(", ")", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "modules", ")", ",", "num_in_chs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet.init_weights": [[674, 683], ["hrnet.HighResolutionNet.modules", "isinstance", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "\n", "m", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet.group_matcher": [[684, 695], ["dict"], "methods", ["None"], ["", "", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "\n", "stem", "=", "r'^conv[12]|bn[12]'", ",", "\n", "blocks", "=", "r'^(?:layer|stage|transition)(\\d+)'", "if", "coarse", "else", "[", "\n", "(", "r'^layer(\\d+)\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^stage(\\d+)\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^transition(\\d+)'", ",", "(", "99999", ",", ")", ")", ",", "\n", "]", ",", "\n", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet.set_grad_checkpointing": [[696, 699], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "\"gradient checkpointing not supported\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet.get_classifier": [[700, 703], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "classifier", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet.reset_classifier": [[704, 708], ["layers.create_classifier"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", ",", "self", ".", "classifier", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet.stages": [[709, 721], ["hrnet.HighResolutionNet.layer1", "hrnet.HighResolutionNet.stage2", "hrnet.HighResolutionNet.stage3", "hrnet.HighResolutionNet.stage4", "t", "enumerate", "t", "enumerate", "t", "enumerate", "isinstance", "isinstance"], "methods", ["None"], ["", "def", "stages", "(", "self", ",", "x", ")", "->", "List", "[", "torch", ".", "Tensor", "]", ":", "\n", "        ", "x", "=", "self", ".", "layer1", "(", "x", ")", "\n", "\n", "xl", "=", "[", "t", "(", "x", ")", "for", "i", ",", "t", "in", "enumerate", "(", "self", ".", "transition1", ")", "]", "\n", "yl", "=", "self", ".", "stage2", "(", "xl", ")", "\n", "\n", "xl", "=", "[", "t", "(", "yl", "[", "-", "1", "]", ")", "if", "not", "isinstance", "(", "t", ",", "nn", ".", "Identity", ")", "else", "yl", "[", "i", "]", "for", "i", ",", "t", "in", "enumerate", "(", "self", ".", "transition2", ")", "]", "\n", "yl", "=", "self", ".", "stage3", "(", "xl", ")", "\n", "\n", "xl", "=", "[", "t", "(", "yl", "[", "-", "1", "]", ")", "if", "not", "isinstance", "(", "t", ",", "nn", ".", "Identity", ")", "else", "yl", "[", "i", "]", "for", "i", ",", "t", "in", "enumerate", "(", "self", ".", "transition3", ")", "]", "\n", "yl", "=", "self", ".", "stage4", "(", "xl", ")", "\n", "return", "yl", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet.forward_features": [[722, 740], ["hrnet.HighResolutionNet.conv1", "hrnet.HighResolutionNet.bn1", "hrnet.HighResolutionNet.act1", "hrnet.HighResolutionNet.conv2", "hrnet.HighResolutionNet.bn2", "hrnet.HighResolutionNet.act2", "hrnet.HighResolutionNet.stages", "enumerate", "hrnet.HighResolutionNet.final_layer", "down"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet.stages"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "# Stem", "\n", "        ", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "x", "=", "self", ".", "act1", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "x", "=", "self", ".", "bn2", "(", "x", ")", "\n", "x", "=", "self", ".", "act2", "(", "x", ")", "\n", "\n", "# Stages", "\n", "yl", "=", "self", ".", "stages", "(", "x", ")", "\n", "if", "self", ".", "incre_modules", "is", "None", "or", "self", ".", "downsamp_modules", "is", "None", ":", "\n", "            ", "return", "yl", "\n", "", "y", "=", "self", ".", "incre_modules", "[", "0", "]", "(", "yl", "[", "0", "]", ")", "\n", "for", "i", ",", "down", "in", "enumerate", "(", "self", ".", "downsamp_modules", ")", ":", "\n", "            ", "y", "=", "self", ".", "incre_modules", "[", "i", "+", "1", "]", "(", "yl", "[", "i", "+", "1", "]", ")", "+", "down", "(", "y", ")", "\n", "", "y", "=", "self", ".", "final_layer", "(", "y", ")", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet.forward_head": [[741, 747], ["hrnet.HighResolutionNet.global_pool", "torch.dropout", "torch.dropout", "torch.dropout", "hrnet.HighResolutionNet.classifier"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "# Classification Head", "\n", "        ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "if", "self", ".", "drop_rate", ">", "0.", ":", "\n", "            ", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "drop_rate", ",", "training", "=", "self", ".", "training", ")", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "classifier", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet.forward": [[748, 752], ["hrnet.HighResolutionNet.forward_features", "hrnet.HighResolutionNet.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "y", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "y", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNetFeatures.__init__": [[765, 773], ["hrnet.HighResolutionNet.__init__", "features.FeatureInfo"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "cfg", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "global_pool", "=", "'avg'", ",", "drop_rate", "=", "0.0", ",", "\n", "feature_location", "=", "'incre'", ",", "out_indices", "=", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ")", ":", "\n", "        ", "assert", "feature_location", "in", "(", "'incre'", ",", "''", ")", "\n", "super", "(", "HighResolutionNetFeatures", ",", "self", ")", ".", "__init__", "(", "\n", "cfg", ",", "in_chans", "=", "in_chans", ",", "num_classes", "=", "num_classes", ",", "global_pool", "=", "global_pool", ",", "\n", "drop_rate", "=", "drop_rate", ",", "head", "=", "feature_location", ")", "\n", "self", ".", "feature_info", "=", "FeatureInfo", "(", "self", ".", "feature_info", ",", "out_indices", ")", "\n", "self", ".", "_out_idx", "=", "{", "i", "for", "i", "in", "out_indices", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNetFeatures.forward_features": [[774, 776], ["None"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "assert", "False", ",", "'Not supported'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNetFeatures.forward": [[777, 794], ["hrnet.HighResolutionNetFeatures.conv1", "hrnet.HighResolutionNetFeatures.bn1", "hrnet.HighResolutionNetFeatures.act1", "hrnet.HighResolutionNetFeatures.conv2", "hrnet.HighResolutionNetFeatures.bn2", "hrnet.HighResolutionNetFeatures.act2", "hrnet.HighResolutionNetFeatures.stages", "enumerate", "out.append", "incre", "out.append", "zip"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet.stages"], ["", "def", "forward", "(", "self", ",", "x", ")", "->", "List", "[", "torch", ".", "tensor", "]", ":", "\n", "        ", "out", "=", "[", "]", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "x", "=", "self", ".", "act1", "(", "x", ")", "\n", "if", "0", "in", "self", ".", "_out_idx", ":", "\n", "            ", "out", ".", "append", "(", "x", ")", "\n", "", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "x", "=", "self", ".", "bn2", "(", "x", ")", "\n", "x", "=", "self", ".", "act2", "(", "x", ")", "\n", "x", "=", "self", ".", "stages", "(", "x", ")", "\n", "if", "self", ".", "incre_modules", "is", "not", "None", ":", "\n", "            ", "x", "=", "[", "incre", "(", "f", ")", "for", "f", ",", "incre", "in", "zip", "(", "x", ",", "self", ".", "incre_modules", ")", "]", "\n", "", "for", "i", ",", "f", "in", "enumerate", "(", "x", ")", ":", "\n", "            ", "if", "i", "+", "1", "in", "self", ".", "_out_idx", ":", "\n", "                ", "out", ".", "append", "(", "f", ")", "\n", "", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet._cfg": [[29, 37], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bilinear'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'conv1'", ",", "'classifier'", ":", "'classifier'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet._create_hrnet": [[796, 814], ["model_kwargs.pop", "helpers.build_model_with_cfg", "helpers.pretrained_cfg_for_features"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.pretrained_cfg_for_features"], ["", "", "def", "_create_hrnet", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", ":", "\n", "    ", "model_cls", "=", "HighResolutionNet", "\n", "features_only", "=", "False", "\n", "kwargs_filter", "=", "None", "\n", "if", "model_kwargs", ".", "pop", "(", "'features_only'", ",", "False", ")", ":", "\n", "        ", "model_cls", "=", "HighResolutionNetFeatures", "\n", "kwargs_filter", "=", "(", "'num_classes'", ",", "'global_pool'", ")", "\n", "features_only", "=", "True", "\n", "", "model", "=", "build_model_with_cfg", "(", "\n", "model_cls", ",", "variant", ",", "pretrained", ",", "\n", "model_cfg", "=", "cfg_cls", "[", "variant", "]", ",", "\n", "pretrained_strict", "=", "not", "features_only", ",", "\n", "kwargs_filter", "=", "kwargs_filter", ",", "\n", "**", "model_kwargs", ")", "\n", "if", "features_only", ":", "\n", "        ", "model", ".", "pretrained_cfg", "=", "pretrained_cfg_for_features", "(", "model", ".", "default_cfg", ")", "\n", "model", ".", "default_cfg", "=", "model", ".", "pretrained_cfg", "# backwards compat", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.hrnet_w18_small": [[816, 819], ["hrnet._create_hrnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet._create_hrnet"], ["", "@", "register_model", "\n", "def", "hrnet_w18_small", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_hrnet", "(", "'hrnet_w18_small'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.hrnet_w18_small_v2": [[821, 824], ["hrnet._create_hrnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet._create_hrnet"], ["", "@", "register_model", "\n", "def", "hrnet_w18_small_v2", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_hrnet", "(", "'hrnet_w18_small_v2'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.hrnet_w18": [[826, 829], ["hrnet._create_hrnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet._create_hrnet"], ["", "@", "register_model", "\n", "def", "hrnet_w18", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_hrnet", "(", "'hrnet_w18'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.hrnet_w30": [[831, 834], ["hrnet._create_hrnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet._create_hrnet"], ["", "@", "register_model", "\n", "def", "hrnet_w30", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_hrnet", "(", "'hrnet_w30'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.hrnet_w32": [[836, 839], ["hrnet._create_hrnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet._create_hrnet"], ["", "@", "register_model", "\n", "def", "hrnet_w32", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_hrnet", "(", "'hrnet_w32'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.hrnet_w40": [[841, 844], ["hrnet._create_hrnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet._create_hrnet"], ["", "@", "register_model", "\n", "def", "hrnet_w40", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_hrnet", "(", "'hrnet_w40'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.hrnet_w44": [[846, 849], ["hrnet._create_hrnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet._create_hrnet"], ["", "@", "register_model", "\n", "def", "hrnet_w44", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_hrnet", "(", "'hrnet_w44'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.hrnet_w48": [[851, 854], ["hrnet._create_hrnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet._create_hrnet"], ["", "@", "register_model", "\n", "def", "hrnet_w48", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_hrnet", "(", "'hrnet_w48'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.hrnet_w64": [[856, 859], ["hrnet._create_hrnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet._create_hrnet"], ["", "@", "register_model", "\n", "def", "hrnet_w64", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_hrnet", "(", "'hrnet_w64'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.SequentialTuple.__init__": [[68, 70], ["torch.nn.Sequential.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "super", "(", "SequentialTuple", ",", "self", ")", ".", "__init__", "(", "*", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.SequentialTuple.forward": [[71, 75], ["module"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "for", "module", "in", "self", ":", "\n", "            ", "x", "=", "module", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.Transformer.__init__": [[78, 98], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "torch.nn.Sequential", "vision_transformer.Block", "range", "functools.partial"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "base_dim", ",", "depth", ",", "heads", ",", "mlp_ratio", ",", "pool", "=", "None", ",", "drop_rate", "=", ".0", ",", "attn_drop_rate", "=", ".0", ",", "drop_path_prob", "=", "None", ")", ":", "\n", "        ", "super", "(", "Transformer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "embed_dim", "=", "base_dim", "*", "heads", "\n", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "Block", "(", "\n", "dim", "=", "embed_dim", ",", "\n", "num_heads", "=", "heads", ",", "\n", "mlp_ratio", "=", "mlp_ratio", ",", "\n", "qkv_bias", "=", "True", ",", "\n", "drop", "=", "drop_rate", ",", "\n", "attn_drop", "=", "attn_drop_rate", ",", "\n", "drop_path", "=", "drop_path_prob", "[", "i", "]", ",", "\n", "norm_layer", "=", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", "\n", ")", "\n", "for", "i", "in", "range", "(", "depth", ")", "]", ")", "\n", "\n", "self", ".", "pool", "=", "pool", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.Transformer.forward": [[99, 116], ["x.transpose().reshape.transpose().reshape.flatten().transpose", "torch.cat", "pit.Transformer.blocks", "x.transpose().reshape.transpose().reshape.transpose().reshape", "pit.Transformer.pool", "x.transpose().reshape.transpose().reshape.flatten", "x.transpose().reshape.transpose().reshape.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "x", ",", "cls_tokens", "=", "x", "\n", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "token_length", "=", "cls_tokens", ".", "shape", "[", "1", "]", "\n", "\n", "x", "=", "x", ".", "flatten", "(", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "x", "=", "torch", ".", "cat", "(", "(", "cls_tokens", ",", "x", ")", ",", "dim", "=", "1", ")", "\n", "\n", "x", "=", "self", ".", "blocks", "(", "x", ")", "\n", "\n", "cls_tokens", "=", "x", "[", ":", ",", ":", "token_length", "]", "\n", "x", "=", "x", "[", ":", ",", "token_length", ":", "]", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "C", ",", "H", ",", "W", ")", "\n", "\n", "if", "self", ".", "pool", "is", "not", "None", ":", "\n", "            ", "x", ",", "cls_tokens", "=", "self", ".", "pool", "(", "x", ",", "cls_tokens", ")", "\n", "", "return", "x", ",", "cls_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.ConvHeadPooling.__init__": [[119, 126], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_feature", ",", "out_feature", ",", "stride", ",", "padding_mode", "=", "'zeros'", ")", ":", "\n", "        ", "super", "(", "ConvHeadPooling", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "conv", "=", "nn", ".", "Conv2d", "(", "\n", "in_feature", ",", "out_feature", ",", "kernel_size", "=", "stride", "+", "1", ",", "padding", "=", "stride", "//", "2", ",", "stride", "=", "stride", ",", "\n", "padding_mode", "=", "padding_mode", ",", "groups", "=", "in_feature", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "in_feature", ",", "out_feature", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.ConvHeadPooling.forward": [[127, 131], ["pit.ConvHeadPooling.conv", "pit.ConvHeadPooling.fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "cls_token", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "cls_token", "=", "self", ".", "fc", "(", "cls_token", ")", "\n", "return", "x", ",", "cls_token", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.ConvEmbedding.__init__": [[134, 138], ["torch.nn.Module.__init__", "torch.nn.Conv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "patch_size", ",", "stride", ",", "padding", ")", ":", "\n", "        ", "super", "(", "ConvEmbedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv", "=", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "patch_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "bias", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.ConvEmbedding.forward": [[139, 142], ["pit.ConvEmbedding.conv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.PoolingVisionTransformer.__init__": [[150, 202], ["torch.nn.Module.__init__", "layers.to_2tuple", "layers.to_2tuple", "math.floor", "math.floor", "torch.nn.Parameter", "pit.ConvEmbedding", "torch.nn.Parameter", "torch.nn.Dropout", "range", "pit.SequentialTuple", "torch.nn.LayerNorm", "layers.trunc_normal_", "layers.trunc_normal_", "pit.PoolingVisionTransformer.apply", "torch.randn", "torch.randn", "x.tolist", "len", "torch.nn.Linear", "torch.nn.Identity", "torch.linspace().split", "pit.ConvHeadPooling", "pit.Transformer", "torch.nn.Linear", "torch.nn.Identity", "len", "torch.linspace", "sum"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["def", "__init__", "(", "\n", "self", ",", "img_size", ",", "patch_size", ",", "stride", ",", "base_dims", ",", "depth", ",", "heads", ",", "\n", "mlp_ratio", ",", "num_classes", "=", "1000", ",", "in_chans", "=", "3", ",", "global_pool", "=", "'token'", ",", "\n", "distilled", "=", "False", ",", "attn_drop_rate", "=", ".0", ",", "drop_rate", "=", ".0", ",", "drop_path_rate", "=", ".0", ")", ":", "\n", "        ", "super", "(", "PoolingVisionTransformer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "global_pool", "in", "(", "'token'", ",", ")", "\n", "\n", "padding", "=", "0", "\n", "img_size", "=", "to_2tuple", "(", "img_size", ")", "\n", "patch_size", "=", "to_2tuple", "(", "patch_size", ")", "\n", "height", "=", "math", ".", "floor", "(", "(", "img_size", "[", "0", "]", "+", "2", "*", "padding", "-", "patch_size", "[", "0", "]", ")", "/", "stride", "+", "1", ")", "\n", "width", "=", "math", ".", "floor", "(", "(", "img_size", "[", "1", "]", "+", "2", "*", "padding", "-", "patch_size", "[", "1", "]", ")", "/", "stride", "+", "1", ")", "\n", "\n", "self", ".", "base_dims", "=", "base_dims", "\n", "self", ".", "heads", "=", "heads", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "self", ".", "num_tokens", "=", "2", "if", "distilled", "else", "1", "\n", "\n", "self", ".", "patch_size", "=", "patch_size", "\n", "self", ".", "pos_embed", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "1", ",", "base_dims", "[", "0", "]", "*", "heads", "[", "0", "]", ",", "height", ",", "width", ")", ")", "\n", "self", ".", "patch_embed", "=", "ConvEmbedding", "(", "in_chans", ",", "base_dims", "[", "0", "]", "*", "heads", "[", "0", "]", ",", "patch_size", ",", "stride", ",", "padding", ")", "\n", "\n", "self", ".", "cls_token", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "1", ",", "self", ".", "num_tokens", ",", "base_dims", "[", "0", "]", "*", "heads", "[", "0", "]", ")", ")", "\n", "self", ".", "pos_drop", "=", "nn", ".", "Dropout", "(", "p", "=", "drop_rate", ")", "\n", "\n", "transformers", "=", "[", "]", "\n", "# stochastic depth decay rule", "\n", "dpr", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "sum", "(", "depth", ")", ")", ".", "split", "(", "depth", ")", "]", "\n", "for", "stage", "in", "range", "(", "len", "(", "depth", ")", ")", ":", "\n", "            ", "pool", "=", "None", "\n", "if", "stage", "<", "len", "(", "heads", ")", "-", "1", ":", "\n", "                ", "pool", "=", "ConvHeadPooling", "(", "\n", "base_dims", "[", "stage", "]", "*", "heads", "[", "stage", "]", ",", "base_dims", "[", "stage", "+", "1", "]", "*", "heads", "[", "stage", "+", "1", "]", ",", "stride", "=", "2", ")", "\n", "", "transformers", "+=", "[", "Transformer", "(", "\n", "base_dims", "[", "stage", "]", ",", "depth", "[", "stage", "]", ",", "heads", "[", "stage", "]", ",", "mlp_ratio", ",", "pool", "=", "pool", ",", "\n", "drop_rate", "=", "drop_rate", ",", "attn_drop_rate", "=", "attn_drop_rate", ",", "drop_path_prob", "=", "dpr", "[", "stage", "]", ")", "\n", "]", "\n", "", "self", ".", "transformers", "=", "SequentialTuple", "(", "*", "transformers", ")", "\n", "self", ".", "norm", "=", "nn", ".", "LayerNorm", "(", "base_dims", "[", "-", "1", "]", "*", "heads", "[", "-", "1", "]", ",", "eps", "=", "1e-6", ")", "\n", "self", ".", "num_features", "=", "self", ".", "embed_dim", "=", "base_dims", "[", "-", "1", "]", "*", "heads", "[", "-", "1", "]", "\n", "\n", "# Classifier head", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "head_dist", "=", "None", "\n", "if", "distilled", ":", "\n", "            ", "self", ".", "head_dist", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "", "self", ".", "distilled_training", "=", "False", "# must set this True to train w/ distillation token", "\n", "\n", "trunc_normal_", "(", "self", ".", "pos_embed", ",", "std", "=", ".02", ")", "\n", "trunc_normal_", "(", "self", ".", "cls_token", ",", "std", "=", ".02", ")", "\n", "self", ".", "apply", "(", "self", ".", "_init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.PoolingVisionTransformer._init_weights": [[203, 207], ["isinstance", "torch.nn.init.constant_", "torch.nn.init.constant_"], "methods", ["None"], ["", "def", "_init_weights", "(", "self", ",", "m", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.PoolingVisionTransformer.no_weight_decay": [[208, 211], ["None"], "methods", ["None"], ["", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "no_weight_decay", "(", "self", ")", ":", "\n", "        ", "return", "{", "'pos_embed'", ",", "'cls_token'", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.PoolingVisionTransformer.set_distilled_training": [[212, 215], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_distilled_training", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "distilled_training", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.PoolingVisionTransformer.set_grad_checkpointing": [[216, 219], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "'gradient checkpointing not supported'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.PoolingVisionTransformer.get_classifier": [[220, 225], ["None"], "methods", ["None"], ["", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "head_dist", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "head", ",", "self", ".", "head_dist", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.PoolingVisionTransformer.reset_classifier": [[226, 231], ["torch.nn.Linear", "torch.nn.Identity", "torch.nn.Linear", "torch.nn.Identity"], "methods", ["None"], ["", "", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "if", "self", ".", "head_dist", "is", "not", "None", ":", "\n", "            ", "self", ".", "head_dist", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.PoolingVisionTransformer.forward_features": [[232, 239], ["pit.PoolingVisionTransformer.patch_embed", "pit.PoolingVisionTransformer.pos_drop", "pit.PoolingVisionTransformer.cls_token.expand", "pit.PoolingVisionTransformer.transformers", "pit.PoolingVisionTransformer.norm"], "methods", ["None"], ["", "", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "patch_embed", "(", "x", ")", "\n", "x", "=", "self", ".", "pos_drop", "(", "x", "+", "self", ".", "pos_embed", ")", "\n", "cls_tokens", "=", "self", ".", "cls_token", ".", "expand", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ",", "-", "1", ")", "\n", "x", ",", "cls_tokens", "=", "self", ".", "transformers", "(", "(", "x", ",", "cls_tokens", ")", ")", "\n", "cls_tokens", "=", "self", ".", "norm", "(", "cls_tokens", ")", "\n", "return", "cls_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.PoolingVisionTransformer.forward_head": [[240, 259], ["pit.PoolingVisionTransformer.head", "pit.PoolingVisionTransformer.head_dist", "pit.PoolingVisionTransformer.head", "torch.jit.is_scripting"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "if", "self", ".", "head_dist", "is", "not", "None", ":", "\n", "            ", "assert", "self", ".", "global_pool", "==", "'token'", "\n", "x", ",", "x_dist", "=", "x", "[", ":", ",", "0", "]", ",", "x", "[", ":", ",", "1", "]", "\n", "if", "not", "pre_logits", ":", "\n", "                ", "x", "=", "self", ".", "head", "(", "x", ")", "\n", "x_dist", "=", "self", ".", "head_dist", "(", "x_dist", ")", "\n", "", "if", "self", ".", "distilled_training", "and", "self", ".", "training", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "# only return separate classification predictions when training in distilled mode", "\n", "                ", "return", "x", ",", "x_dist", "\n", "", "else", ":", "\n", "# during standard train / finetune, inference average the classifier predictions", "\n", "                ", "return", "(", "x", "+", "x_dist", ")", "/", "2", "\n", "", "", "else", ":", "\n", "            ", "if", "self", ".", "global_pool", "==", "'token'", ":", "\n", "                ", "x", "=", "x", "[", ":", ",", "0", "]", "\n", "", "if", "not", "pre_logits", ":", "\n", "                ", "x", "=", "self", ".", "head", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.PoolingVisionTransformer.forward": [[260, 264], ["pit.PoolingVisionTransformer.forward_features", "pit.PoolingVisionTransformer.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit._cfg": [[30, 38], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", ".9", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'patch_embed.conv'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.checkpoint_filter_fn": [[266, 278], ["re.compile", "state_dict.items", "re.compile.sub", "int", "exp.group"], "function", ["None"], ["", "", "def", "checkpoint_filter_fn", "(", "state_dict", ",", "model", ")", ":", "\n", "    ", "\"\"\" preprocess checkpoints \"\"\"", "\n", "out_dict", "=", "{", "}", "\n", "p_blocks", "=", "re", ".", "compile", "(", "r'pools\\.(\\d)\\.'", ")", "\n", "for", "k", ",", "v", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "# FIXME need to update resize for PiT impl", "\n", "# if k == 'pos_embed' and v.shape != model.pos_embed.shape:", "\n", "#     # To resize pos embedding when using model at different size from pretrained weights", "\n", "#     v = resize_pos_embed(v, model.pos_embed)", "\n", "        ", "k", "=", "p_blocks", ".", "sub", "(", "lambda", "exp", ":", "f'transformers.{int(exp.group(1))}.pool.'", ",", "k", ")", "\n", "out_dict", "[", "k", "]", "=", "v", "\n", "", "return", "out_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit._create_pit": [[280, 289], ["kwargs.get", "helpers.build_model_with_cfg", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "def", "_create_pit", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "kwargs", ".", "get", "(", "'features_only'", ",", "None", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'features_only not implemented for Vision Transformer models.'", ")", "\n", "\n", "", "model", "=", "build_model_with_cfg", "(", "\n", "PoolingVisionTransformer", ",", "variant", ",", "pretrained", ",", "\n", "pretrained_filter_fn", "=", "checkpoint_filter_fn", ",", "\n", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.pit_b_224": [[291, 303], ["dict", "pit._create_pit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit._create_pit"], ["", "@", "register_model", "\n", "def", "pit_b_224", "(", "pretrained", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "14", ",", "\n", "stride", "=", "7", ",", "\n", "base_dims", "=", "[", "64", ",", "64", ",", "64", "]", ",", "\n", "depth", "=", "[", "3", ",", "6", ",", "4", "]", ",", "\n", "heads", "=", "[", "4", ",", "8", ",", "16", "]", ",", "\n", "mlp_ratio", "=", "4", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_pit", "(", "'pit_b_224'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.pit_s_224": [[305, 317], ["dict", "pit._create_pit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit._create_pit"], ["", "@", "register_model", "\n", "def", "pit_s_224", "(", "pretrained", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "\n", "stride", "=", "8", ",", "\n", "base_dims", "=", "[", "48", ",", "48", ",", "48", "]", ",", "\n", "depth", "=", "[", "2", ",", "6", ",", "4", "]", ",", "\n", "heads", "=", "[", "3", ",", "6", ",", "12", "]", ",", "\n", "mlp_ratio", "=", "4", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_pit", "(", "'pit_s_224'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.pit_xs_224": [[319, 331], ["dict", "pit._create_pit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit._create_pit"], ["", "@", "register_model", "\n", "def", "pit_xs_224", "(", "pretrained", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "\n", "stride", "=", "8", ",", "\n", "base_dims", "=", "[", "48", ",", "48", ",", "48", "]", ",", "\n", "depth", "=", "[", "2", ",", "6", ",", "4", "]", ",", "\n", "heads", "=", "[", "2", ",", "4", ",", "8", "]", ",", "\n", "mlp_ratio", "=", "4", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_pit", "(", "'pit_xs_224'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.pit_ti_224": [[333, 345], ["dict", "pit._create_pit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit._create_pit"], ["", "@", "register_model", "\n", "def", "pit_ti_224", "(", "pretrained", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "\n", "stride", "=", "8", ",", "\n", "base_dims", "=", "[", "32", ",", "32", ",", "32", "]", ",", "\n", "depth", "=", "[", "2", ",", "6", ",", "4", "]", ",", "\n", "heads", "=", "[", "2", ",", "4", ",", "8", "]", ",", "\n", "mlp_ratio", "=", "4", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_pit", "(", "'pit_ti_224'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.pit_b_distilled_224": [[347, 360], ["dict", "pit._create_pit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit._create_pit"], ["", "@", "register_model", "\n", "def", "pit_b_distilled_224", "(", "pretrained", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "14", ",", "\n", "stride", "=", "7", ",", "\n", "base_dims", "=", "[", "64", ",", "64", ",", "64", "]", ",", "\n", "depth", "=", "[", "3", ",", "6", ",", "4", "]", ",", "\n", "heads", "=", "[", "4", ",", "8", ",", "16", "]", ",", "\n", "mlp_ratio", "=", "4", ",", "\n", "distilled", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_pit", "(", "'pit_b_distilled_224'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.pit_s_distilled_224": [[362, 375], ["dict", "pit._create_pit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit._create_pit"], ["", "@", "register_model", "\n", "def", "pit_s_distilled_224", "(", "pretrained", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "\n", "stride", "=", "8", ",", "\n", "base_dims", "=", "[", "48", ",", "48", ",", "48", "]", ",", "\n", "depth", "=", "[", "2", ",", "6", ",", "4", "]", ",", "\n", "heads", "=", "[", "3", ",", "6", ",", "12", "]", ",", "\n", "mlp_ratio", "=", "4", ",", "\n", "distilled", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_pit", "(", "'pit_s_distilled_224'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.pit_xs_distilled_224": [[377, 390], ["dict", "pit._create_pit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit._create_pit"], ["", "@", "register_model", "\n", "def", "pit_xs_distilled_224", "(", "pretrained", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "\n", "stride", "=", "8", ",", "\n", "base_dims", "=", "[", "48", ",", "48", ",", "48", "]", ",", "\n", "depth", "=", "[", "2", ",", "6", ",", "4", "]", ",", "\n", "heads", "=", "[", "2", ",", "4", ",", "8", "]", ",", "\n", "mlp_ratio", "=", "4", ",", "\n", "distilled", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_pit", "(", "'pit_xs_distilled_224'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit.pit_ti_distilled_224": [[392, 405], ["dict", "pit._create_pit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.pit._create_pit"], ["", "@", "register_model", "\n", "def", "pit_ti_distilled_224", "(", "pretrained", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "\n", "stride", "=", "8", ",", "\n", "base_dims", "=", "[", "32", ",", "32", ",", "32", "]", ",", "\n", "depth", "=", "[", "2", ",", "6", ",", "4", "]", ",", "\n", "heads", "=", "[", "2", ",", "4", ",", "8", "]", ",", "\n", "mlp_ratio", "=", "4", ",", "\n", "distilled", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", "\n", "return", "_create_pit", "(", "'pit_ti_distilled_224'", ",", "pretrained", ",", "**", "model_kwargs", ")", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.MobileNetV3.__init__": [[125, 162], ["torch.Module.__init__", "layers.get_norm_act_layer", "layers.create_conv2d", "layers.get_norm_act_layer.", "efficientnet_builder.EfficientNetBuilder", "torch.Sequential", "torch.Sequential", "torch.Sequential", "layers.SelectAdaptivePool2d", "layers.create_conv2d", "act_layer", "efficientnet_builder.efficientnet_init_weights", "round_chs_fn", "mobilenetv3.MobileNetV3.global_pool.feat_mult", "torch.Flatten", "torch.Flatten", "torch.Flatten", "torch.Identity", "torch.Identity", "torch.Identity", "layers.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "efficientnet_builder.EfficientNetBuilder."], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.get_norm_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.efficientnet_init_weights", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.SelectAdaptivePool2d.feat_mult"], ["def", "__init__", "(", "\n", "self", ",", "block_args", ",", "num_classes", "=", "1000", ",", "in_chans", "=", "3", ",", "stem_size", "=", "16", ",", "fix_stem", "=", "False", ",", "num_features", "=", "1280", ",", "\n", "head_bias", "=", "True", ",", "pad_type", "=", "''", ",", "act_layer", "=", "None", ",", "norm_layer", "=", "None", ",", "se_layer", "=", "None", ",", "se_from_exp", "=", "True", ",", "\n", "round_chs_fn", "=", "round_channels", ",", "drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "super", "(", "MobileNetV3", ",", "self", ")", ".", "__init__", "(", ")", "\n", "act_layer", "=", "act_layer", "or", "nn", ".", "ReLU", "\n", "norm_layer", "=", "norm_layer", "or", "nn", ".", "BatchNorm2d", "\n", "norm_act_layer", "=", "get_norm_act_layer", "(", "norm_layer", ",", "act_layer", ")", "\n", "se_layer", "=", "se_layer", "or", "SqueezeExcite", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "num_features", "=", "num_features", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "# Stem", "\n", "if", "not", "fix_stem", ":", "\n", "            ", "stem_size", "=", "round_chs_fn", "(", "stem_size", ")", "\n", "", "self", ".", "conv_stem", "=", "create_conv2d", "(", "in_chans", ",", "stem_size", ",", "3", ",", "stride", "=", "2", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "bn1", "=", "norm_act_layer", "(", "stem_size", ",", "inplace", "=", "True", ")", "\n", "\n", "# Middle stages (IR/ER/DS Blocks)", "\n", "builder", "=", "EfficientNetBuilder", "(", "\n", "output_stride", "=", "32", ",", "pad_type", "=", "pad_type", ",", "round_chs_fn", "=", "round_chs_fn", ",", "se_from_exp", "=", "se_from_exp", ",", "\n", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ",", "se_layer", "=", "se_layer", ",", "drop_path_rate", "=", "drop_path_rate", ")", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "*", "builder", "(", "stem_size", ",", "block_args", ")", ")", "\n", "self", ".", "feature_info", "=", "builder", ".", "features", "\n", "head_chs", "=", "builder", ".", "in_chs", "\n", "\n", "# Head + Pooling", "\n", "self", ".", "global_pool", "=", "SelectAdaptivePool2d", "(", "pool_type", "=", "global_pool", ")", "\n", "num_pooled_chs", "=", "head_chs", "*", "self", ".", "global_pool", ".", "feat_mult", "(", ")", "\n", "self", ".", "conv_head", "=", "create_conv2d", "(", "num_pooled_chs", ",", "self", ".", "num_features", ",", "1", ",", "padding", "=", "pad_type", ",", "bias", "=", "head_bias", ")", "\n", "self", ".", "act2", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "flatten", "=", "nn", ".", "Flatten", "(", "1", ")", "if", "global_pool", "else", "nn", ".", "Identity", "(", ")", "# don't flatten if pooling disabled", "\n", "self", ".", "classifier", "=", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "efficientnet_init_weights", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.MobileNetV3.as_sequential": [[163, 169], ["layers.extend", "layers.extend", "layers.extend", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Flatten", "torch.Flatten", "torch.Flatten", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["None"], ["", "def", "as_sequential", "(", "self", ")", ":", "\n", "        ", "layers", "=", "[", "self", ".", "conv_stem", ",", "self", ".", "bn1", "]", "\n", "layers", ".", "extend", "(", "self", ".", "blocks", ")", "\n", "layers", ".", "extend", "(", "[", "self", ".", "global_pool", ",", "self", ".", "conv_head", ",", "self", ".", "act2", "]", ")", "\n", "layers", ".", "extend", "(", "[", "nn", ".", "Flatten", "(", ")", ",", "nn", ".", "Dropout", "(", "self", ".", "drop_rate", ")", ",", "self", ".", "classifier", "]", ")", "\n", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.MobileNetV3.group_matcher": [[170, 175], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^conv_stem|bn1'", ",", "\n", "blocks", "=", "r'^blocks\\.(\\d+)'", "if", "coarse", "else", "r'^blocks\\.(\\d+)\\.(\\d+)'", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.MobileNetV3.set_grad_checkpointing": [[177, 180], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.MobileNetV3.get_classifier": [[181, 184], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "classifier", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.MobileNetV3.reset_classifier": [[185, 191], ["layers.SelectAdaptivePool2d", "torch.Flatten", "torch.Flatten", "torch.Flatten", "torch.Identity", "torch.Identity", "torch.Identity", "layers.Linear", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "# cannot meaningfully change pooling of efficient head after creation", "\n", "self", ".", "global_pool", "=", "SelectAdaptivePool2d", "(", "pool_type", "=", "global_pool", ")", "\n", "self", ".", "flatten", "=", "nn", ".", "Flatten", "(", "1", ")", "if", "global_pool", "else", "nn", ".", "Identity", "(", ")", "# don't flatten if pooling disabled", "\n", "self", ".", "classifier", "=", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.MobileNetV3.forward_features": [[192, 200], ["mobilenetv3.MobileNetV3.conv_stem", "mobilenetv3.MobileNetV3.bn1", "helpers.checkpoint_seq", "mobilenetv3.MobileNetV3.blocks", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv_stem", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "blocks", ",", "x", ",", "flatten", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "blocks", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.MobileNetV3.forward_head": [[201, 212], ["mobilenetv3.MobileNetV3.global_pool", "mobilenetv3.MobileNetV3.conv_head", "mobilenetv3.MobileNetV3.act2", "torch.dropout.flatten", "mobilenetv3.MobileNetV3.flatten", "mobilenetv3.MobileNetV3.classifier", "torch.dropout", "torch.dropout", "torch.dropout"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_head", "(", "x", ")", "\n", "x", "=", "self", ".", "act2", "(", "x", ")", "\n", "if", "pre_logits", ":", "\n", "            ", "return", "x", ".", "flatten", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "flatten", "(", "x", ")", "\n", "if", "self", ".", "drop_rate", ">", "0.", ":", "\n", "                ", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "drop_rate", ",", "training", "=", "self", ".", "training", ")", "\n", "", "return", "self", ".", "classifier", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.MobileNetV3.forward": [[213, 217], ["mobilenetv3.MobileNetV3.forward_features", "mobilenetv3.MobileNetV3.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.MobileNetV3Features.__init__": [[226, 259], ["torch.Module.__init__", "layers.create_conv2d", "norm_layer", "act_layer", "efficientnet_builder.EfficientNetBuilder", "torch.Sequential", "torch.Sequential", "torch.Sequential", "features.FeatureInfo", "efficientnet_builder.efficientnet_init_weights", "round_chs_fn", "mobilenetv3.MobileNetV3Features.feature_info.get_dicts", "features.FeatureHooks", "efficientnet_builder.EfficientNetBuilder.", "enumerate", "mobilenetv3.MobileNetV3Features.named_modules"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.efficientnet_init_weights", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get_dicts", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_modules"], ["def", "__init__", "(", "\n", "self", ",", "block_args", ",", "out_indices", "=", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ",", "feature_location", "=", "'bottleneck'", ",", "in_chans", "=", "3", ",", "\n", "stem_size", "=", "16", ",", "fix_stem", "=", "False", ",", "output_stride", "=", "32", ",", "pad_type", "=", "''", ",", "round_chs_fn", "=", "round_channels", ",", "\n", "se_from_exp", "=", "True", ",", "act_layer", "=", "None", ",", "norm_layer", "=", "None", ",", "se_layer", "=", "None", ",", "drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "        ", "super", "(", "MobileNetV3Features", ",", "self", ")", ".", "__init__", "(", ")", "\n", "act_layer", "=", "act_layer", "or", "nn", ".", "ReLU", "\n", "norm_layer", "=", "norm_layer", "or", "nn", ".", "BatchNorm2d", "\n", "se_layer", "=", "se_layer", "or", "SqueezeExcite", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "\n", "# Stem", "\n", "if", "not", "fix_stem", ":", "\n", "            ", "stem_size", "=", "round_chs_fn", "(", "stem_size", ")", "\n", "", "self", ".", "conv_stem", "=", "create_conv2d", "(", "in_chans", ",", "stem_size", ",", "3", ",", "stride", "=", "2", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "stem_size", ")", "\n", "self", ".", "act1", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "\n", "# Middle stages (IR/ER/DS Blocks)", "\n", "builder", "=", "EfficientNetBuilder", "(", "\n", "output_stride", "=", "output_stride", ",", "pad_type", "=", "pad_type", ",", "round_chs_fn", "=", "round_chs_fn", ",", "se_from_exp", "=", "se_from_exp", ",", "\n", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ",", "se_layer", "=", "se_layer", ",", "\n", "drop_path_rate", "=", "drop_path_rate", ",", "feature_location", "=", "feature_location", ")", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "*", "builder", "(", "stem_size", ",", "block_args", ")", ")", "\n", "self", ".", "feature_info", "=", "FeatureInfo", "(", "builder", ".", "features", ",", "out_indices", ")", "\n", "self", ".", "_stage_out_idx", "=", "{", "v", "[", "'stage'", "]", ":", "i", "for", "i", ",", "v", "in", "enumerate", "(", "self", ".", "feature_info", ")", "if", "i", "in", "out_indices", "}", "\n", "\n", "efficientnet_init_weights", "(", "self", ")", "\n", "\n", "# Register feature extraction hooks with FeatureHooks helper", "\n", "self", ".", "feature_hooks", "=", "None", "\n", "if", "feature_location", "!=", "'bottleneck'", ":", "\n", "            ", "hooks", "=", "self", ".", "feature_info", ".", "get_dicts", "(", "keys", "=", "(", "'module'", ",", "'hook_type'", ")", ")", "\n", "self", ".", "feature_hooks", "=", "FeatureHooks", "(", "hooks", ",", "self", ".", "named_modules", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.MobileNetV3Features.forward": [[260, 277], ["mobilenetv3.MobileNetV3Features.conv_stem", "mobilenetv3.MobileNetV3Features.bn1", "mobilenetv3.MobileNetV3Features.act1", "enumerate", "mobilenetv3.MobileNetV3Features.blocks", "mobilenetv3.MobileNetV3Features.feature_hooks.get_output", "list", "features.append", "b", "mobilenetv3.MobileNetV3Features.values", "features.append"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureHooks.get_output"], ["", "", "def", "forward", "(", "self", ",", "x", ")", "->", "List", "[", "torch", ".", "Tensor", "]", ":", "\n", "        ", "x", "=", "self", ".", "conv_stem", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "x", "=", "self", ".", "act1", "(", "x", ")", "\n", "if", "self", ".", "feature_hooks", "is", "None", ":", "\n", "            ", "features", "=", "[", "]", "\n", "if", "0", "in", "self", ".", "_stage_out_idx", ":", "\n", "                ", "features", ".", "append", "(", "x", ")", "# add stem out", "\n", "", "for", "i", ",", "b", "in", "enumerate", "(", "self", ".", "blocks", ")", ":", "\n", "                ", "x", "=", "b", "(", "x", ")", "\n", "if", "i", "+", "1", "in", "self", ".", "_stage_out_idx", ":", "\n", "                    ", "features", ".", "append", "(", "x", ")", "\n", "", "", "return", "features", "\n", "", "else", ":", "\n", "            ", "self", ".", "blocks", "(", "x", ")", "\n", "out", "=", "self", ".", "feature_hooks", ".", "get_output", "(", "x", ".", "device", ")", "\n", "return", "list", "(", "out", ".", "values", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._cfg": [[28, 35], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bilinear'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'conv_stem'", ",", "'classifier'", ":", "'classifier'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._create_mnv3": [[279, 295], ["kwargs.pop", "helpers.build_model_with_cfg", "helpers.pretrained_cfg_for_features"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.pretrained_cfg_for_features"], ["", "", "", "def", "_create_mnv3", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "features_only", "=", "False", "\n", "model_cls", "=", "MobileNetV3", "\n", "kwargs_filter", "=", "None", "\n", "if", "kwargs", ".", "pop", "(", "'features_only'", ",", "False", ")", ":", "\n", "        ", "features_only", "=", "True", "\n", "kwargs_filter", "=", "(", "'num_classes'", ",", "'num_features'", ",", "'head_conv'", ",", "'head_bias'", ",", "'global_pool'", ")", "\n", "model_cls", "=", "MobileNetV3Features", "\n", "", "model", "=", "build_model_with_cfg", "(", "\n", "model_cls", ",", "variant", ",", "pretrained", ",", "\n", "pretrained_strict", "=", "not", "features_only", ",", "\n", "kwargs_filter", "=", "kwargs_filter", ",", "\n", "**", "kwargs", ")", "\n", "if", "features_only", ":", "\n", "        ", "model", ".", "default_cfg", "=", "pretrained_cfg_for_features", "(", "model", ".", "default_cfg", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_mobilenet_v3_rw": [[297, 333], ["dict", "mobilenetv3._create_mnv3", "efficientnet_builder.decode_arch_def", "functools.partial", "functools.partial", "efficientnet_builder.resolve_act_layer", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._create_mnv3", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_mobilenet_v3_rw", "(", "variant", ",", "channel_multiplier", "=", "1.0", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Creates a MobileNet-V3 model.\n\n    Ref impl: ?\n    Paper: https://arxiv.org/abs/1905.02244\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    \"\"\"", "\n", "arch_def", "=", "[", "\n", "# stage 0, 112x112 in", "\n", "[", "'ds_r1_k3_s1_e1_c16_nre_noskip'", "]", ",", "# relu", "\n", "# stage 1, 112x112 in", "\n", "[", "'ir_r1_k3_s2_e4_c24_nre'", ",", "'ir_r1_k3_s1_e3_c24_nre'", "]", ",", "# relu", "\n", "# stage 2, 56x56 in", "\n", "[", "'ir_r3_k5_s2_e3_c40_se0.25_nre'", "]", ",", "# relu", "\n", "# stage 3, 28x28 in", "\n", "[", "'ir_r1_k3_s2_e6_c80'", ",", "'ir_r1_k3_s1_e2.5_c80'", ",", "'ir_r2_k3_s1_e2.3_c80'", "]", ",", "# hard-swish", "\n", "# stage 4, 14x14in", "\n", "[", "'ir_r2_k3_s1_e6_c112_se0.25'", "]", ",", "# hard-swish", "\n", "# stage 5, 14x14in", "\n", "[", "'ir_r3_k5_s2_e6_c160_se0.25'", "]", ",", "# hard-swish", "\n", "# stage 6, 7x7 in", "\n", "[", "'cn_r1_k1_s1_c960'", "]", ",", "# hard-swish", "\n", "]", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ")", ",", "\n", "head_bias", "=", "False", ",", "\n", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ")", ",", "\n", "norm_layer", "=", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "act_layer", "=", "resolve_act_layer", "(", "kwargs", ",", "'hard_swish'", ")", ",", "\n", "se_layer", "=", "partial", "(", "SqueezeExcite", ",", "gate_layer", "=", "'hard_sigmoid'", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "model", "=", "_create_mnv3", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_mobilenet_v3": [[335, 430], ["functools.partial", "dict", "mobilenetv3._create_mnv3", "efficientnet_builder.resolve_act_layer", "efficientnet_builder.resolve_act_layer", "efficientnet_builder.resolve_act_layer", "efficientnet_builder.resolve_act_layer", "efficientnet_builder.decode_arch_def", "functools.partial", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._create_mnv3", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_mobilenet_v3", "(", "variant", ",", "channel_multiplier", "=", "1.0", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Creates a MobileNet-V3 model.\n\n    Ref impl: ?\n    Paper: https://arxiv.org/abs/1905.02244\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    \"\"\"", "\n", "if", "'small'", "in", "variant", ":", "\n", "        ", "num_features", "=", "1024", "\n", "if", "'minimal'", "in", "variant", ":", "\n", "            ", "act_layer", "=", "resolve_act_layer", "(", "kwargs", ",", "'relu'", ")", "\n", "arch_def", "=", "[", "\n", "# stage 0, 112x112 in", "\n", "[", "'ds_r1_k3_s2_e1_c16'", "]", ",", "\n", "# stage 1, 56x56 in", "\n", "[", "'ir_r1_k3_s2_e4.5_c24'", ",", "'ir_r1_k3_s1_e3.67_c24'", "]", ",", "\n", "# stage 2, 28x28 in", "\n", "[", "'ir_r1_k3_s2_e4_c40'", ",", "'ir_r2_k3_s1_e6_c40'", "]", ",", "\n", "# stage 3, 14x14 in", "\n", "[", "'ir_r2_k3_s1_e3_c48'", "]", ",", "\n", "# stage 4, 14x14in", "\n", "[", "'ir_r3_k3_s2_e6_c96'", "]", ",", "\n", "# stage 6, 7x7 in", "\n", "[", "'cn_r1_k1_s1_c576'", "]", ",", "\n", "]", "\n", "", "else", ":", "\n", "            ", "act_layer", "=", "resolve_act_layer", "(", "kwargs", ",", "'hard_swish'", ")", "\n", "arch_def", "=", "[", "\n", "# stage 0, 112x112 in", "\n", "[", "'ds_r1_k3_s2_e1_c16_se0.25_nre'", "]", ",", "# relu", "\n", "# stage 1, 56x56 in", "\n", "[", "'ir_r1_k3_s2_e4.5_c24_nre'", ",", "'ir_r1_k3_s1_e3.67_c24_nre'", "]", ",", "# relu", "\n", "# stage 2, 28x28 in", "\n", "[", "'ir_r1_k5_s2_e4_c40_se0.25'", ",", "'ir_r2_k5_s1_e6_c40_se0.25'", "]", ",", "# hard-swish", "\n", "# stage 3, 14x14 in", "\n", "[", "'ir_r2_k5_s1_e3_c48_se0.25'", "]", ",", "# hard-swish", "\n", "# stage 4, 14x14in", "\n", "[", "'ir_r3_k5_s2_e6_c96_se0.25'", "]", ",", "# hard-swish", "\n", "# stage 6, 7x7 in", "\n", "[", "'cn_r1_k1_s1_c576'", "]", ",", "# hard-swish", "\n", "]", "\n", "", "", "else", ":", "\n", "        ", "num_features", "=", "1280", "\n", "if", "'minimal'", "in", "variant", ":", "\n", "            ", "act_layer", "=", "resolve_act_layer", "(", "kwargs", ",", "'relu'", ")", "\n", "arch_def", "=", "[", "\n", "# stage 0, 112x112 in", "\n", "[", "'ds_r1_k3_s1_e1_c16'", "]", ",", "\n", "# stage 1, 112x112 in", "\n", "[", "'ir_r1_k3_s2_e4_c24'", ",", "'ir_r1_k3_s1_e3_c24'", "]", ",", "\n", "# stage 2, 56x56 in", "\n", "[", "'ir_r3_k3_s2_e3_c40'", "]", ",", "\n", "# stage 3, 28x28 in", "\n", "[", "'ir_r1_k3_s2_e6_c80'", ",", "'ir_r1_k3_s1_e2.5_c80'", ",", "'ir_r2_k3_s1_e2.3_c80'", "]", ",", "\n", "# stage 4, 14x14in", "\n", "[", "'ir_r2_k3_s1_e6_c112'", "]", ",", "\n", "# stage 5, 14x14in", "\n", "[", "'ir_r3_k3_s2_e6_c160'", "]", ",", "\n", "# stage 6, 7x7 in", "\n", "[", "'cn_r1_k1_s1_c960'", "]", ",", "\n", "]", "\n", "", "else", ":", "\n", "            ", "act_layer", "=", "resolve_act_layer", "(", "kwargs", ",", "'hard_swish'", ")", "\n", "arch_def", "=", "[", "\n", "# stage 0, 112x112 in", "\n", "[", "'ds_r1_k3_s1_e1_c16_nre'", "]", ",", "# relu", "\n", "# stage 1, 112x112 in", "\n", "[", "'ir_r1_k3_s2_e4_c24_nre'", ",", "'ir_r1_k3_s1_e3_c24_nre'", "]", ",", "# relu", "\n", "# stage 2, 56x56 in", "\n", "[", "'ir_r3_k5_s2_e3_c40_se0.25_nre'", "]", ",", "# relu", "\n", "# stage 3, 28x28 in", "\n", "[", "'ir_r1_k3_s2_e6_c80'", ",", "'ir_r1_k3_s1_e2.5_c80'", ",", "'ir_r2_k3_s1_e2.3_c80'", "]", ",", "# hard-swish", "\n", "# stage 4, 14x14in", "\n", "[", "'ir_r2_k3_s1_e6_c112_se0.25'", "]", ",", "# hard-swish", "\n", "# stage 5, 14x14in", "\n", "[", "'ir_r3_k5_s2_e6_c160_se0.25'", "]", ",", "# hard-swish", "\n", "# stage 6, 7x7 in", "\n", "[", "'cn_r1_k1_s1_c960'", "]", ",", "# hard-swish", "\n", "]", "\n", "", "", "se_layer", "=", "partial", "(", "SqueezeExcite", ",", "gate_layer", "=", "'hard_sigmoid'", ",", "force_act_layer", "=", "nn", ".", "ReLU", ",", "rd_round_fn", "=", "round_channels", ")", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ")", ",", "\n", "num_features", "=", "num_features", ",", "\n", "stem_size", "=", "16", ",", "\n", "fix_stem", "=", "channel_multiplier", "<", "0.75", ",", "\n", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ")", ",", "\n", "norm_layer", "=", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "act_layer", "=", "act_layer", ",", "\n", "se_layer", "=", "se_layer", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "model", "=", "_create_mnv3", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_fbnetv3": [[432, 491], ["functools.partial", "functools.partial", "efficientnet_builder.resolve_act_layer", "dict", "mobilenetv3._create_mnv3", "variant.split", "efficientnet_builder.decode_arch_def", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._create_mnv3", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_fbnetv3", "(", "variant", ",", "channel_multiplier", "=", "1.0", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" FBNetV3\n    Paper: `FBNetV3: Joint Architecture-Recipe Search using Predictor Pretraining`\n        - https://arxiv.org/abs/2006.02049\n    FIXME untested, this is a preliminary impl of some FBNet-V3 variants.\n    \"\"\"", "\n", "vl", "=", "variant", ".", "split", "(", "'_'", ")", "[", "-", "1", "]", "\n", "if", "vl", "in", "(", "'a'", ",", "'b'", ")", ":", "\n", "        ", "stem_size", "=", "16", "\n", "arch_def", "=", "[", "\n", "[", "'ds_r2_k3_s1_e1_c16'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e4_c24'", ",", "'ir_r3_k5_s1_e2_c24'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e5_c40_se0.25'", ",", "'ir_r4_k5_s1_e3_c40_se0.25'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e5_c72'", ",", "'ir_r4_k3_s1_e3_c72'", "]", ",", "\n", "[", "'ir_r1_k3_s1_e5_c120_se0.25'", ",", "'ir_r5_k5_s1_e3_c120_se0.25'", "]", ",", "\n", "[", "'ir_r1_k3_s2_e6_c184_se0.25'", ",", "'ir_r5_k5_s1_e4_c184_se0.25'", ",", "'ir_r1_k5_s1_e6_c224_se0.25'", "]", ",", "\n", "[", "'cn_r1_k1_s1_c1344'", "]", ",", "\n", "]", "\n", "", "elif", "vl", "==", "'d'", ":", "\n", "        ", "stem_size", "=", "24", "\n", "arch_def", "=", "[", "\n", "[", "'ds_r2_k3_s1_e1_c16'", "]", ",", "\n", "[", "'ir_r1_k3_s2_e5_c24'", ",", "'ir_r5_k3_s1_e2_c24'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e4_c40_se0.25'", ",", "'ir_r4_k3_s1_e3_c40_se0.25'", "]", ",", "\n", "[", "'ir_r1_k3_s2_e5_c72'", ",", "'ir_r4_k3_s1_e3_c72'", "]", ",", "\n", "[", "'ir_r1_k3_s1_e5_c128_se0.25'", ",", "'ir_r6_k5_s1_e3_c128_se0.25'", "]", ",", "\n", "[", "'ir_r1_k3_s2_e6_c208_se0.25'", ",", "'ir_r5_k5_s1_e5_c208_se0.25'", ",", "'ir_r1_k5_s1_e6_c240_se0.25'", "]", ",", "\n", "[", "'cn_r1_k1_s1_c1440'", "]", ",", "\n", "]", "\n", "", "elif", "vl", "==", "'g'", ":", "\n", "        ", "stem_size", "=", "32", "\n", "arch_def", "=", "[", "\n", "[", "'ds_r3_k3_s1_e1_c24'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e4_c40'", ",", "'ir_r4_k5_s1_e2_c40'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e4_c56_se0.25'", ",", "'ir_r4_k5_s1_e3_c56_se0.25'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e5_c104'", ",", "'ir_r4_k3_s1_e3_c104'", "]", ",", "\n", "[", "'ir_r1_k3_s1_e5_c160_se0.25'", ",", "'ir_r8_k5_s1_e3_c160_se0.25'", "]", ",", "\n", "[", "'ir_r1_k3_s2_e6_c264_se0.25'", ",", "'ir_r6_k5_s1_e5_c264_se0.25'", ",", "'ir_r2_k5_s1_e6_c288_se0.25'", "]", ",", "\n", "[", "'cn_r1_k1_s1_c1728'", "]", ",", "\n", "]", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplemented", "\n", "", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ",", "round_limit", "=", "0.95", ")", "\n", "se_layer", "=", "partial", "(", "SqueezeExcite", ",", "gate_layer", "=", "'hard_sigmoid'", ",", "rd_round_fn", "=", "round_chs_fn", ")", "\n", "act_layer", "=", "resolve_act_layer", "(", "kwargs", ",", "'hard_swish'", ")", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ")", ",", "\n", "num_features", "=", "1984", ",", "\n", "head_bias", "=", "False", ",", "\n", "stem_size", "=", "stem_size", ",", "\n", "round_chs_fn", "=", "round_chs_fn", ",", "\n", "se_from_exp", "=", "False", ",", "\n", "norm_layer", "=", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "act_layer", "=", "act_layer", ",", "\n", "se_layer", "=", "se_layer", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "model", "=", "_create_mnv3", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_lcnet": [[531, 567], ["dict", "mobilenetv3._create_mnv3", "efficientnet_builder.decode_arch_def", "functools.partial", "functools.partial", "efficientnet_builder.resolve_act_layer", "functools.partial", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._create_mnv3", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["", "def", "_gen_lcnet", "(", "variant", ",", "channel_multiplier", "=", "1.0", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" LCNet\n    Essentially a MobileNet-V3 crossed with a MobileNet-V1\n\n    Paper: `PP-LCNet: A Lightweight CPU Convolutional Neural Network` - https://arxiv.org/abs/2109.15099\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    \"\"\"", "\n", "arch_def", "=", "[", "\n", "# stage 0, 112x112 in", "\n", "[", "'dsa_r1_k3_s1_c32'", "]", ",", "\n", "# stage 1, 112x112 in", "\n", "[", "'dsa_r2_k3_s2_c64'", "]", ",", "\n", "# stage 2, 56x56 in", "\n", "[", "'dsa_r2_k3_s2_c128'", "]", ",", "\n", "# stage 3, 28x28 in", "\n", "[", "'dsa_r1_k3_s2_c256'", ",", "'dsa_r1_k5_s1_c256'", "]", ",", "\n", "# stage 4, 14x14in", "\n", "[", "'dsa_r4_k5_s1_c256'", "]", ",", "\n", "# stage 5, 14x14in", "\n", "[", "'dsa_r2_k5_s2_c512_se0.25'", "]", ",", "\n", "# 7x7", "\n", "]", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ")", ",", "\n", "stem_size", "=", "16", ",", "\n", "round_chs_fn", "=", "partial", "(", "round_channels", ",", "multiplier", "=", "channel_multiplier", ")", ",", "\n", "norm_layer", "=", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "act_layer", "=", "resolve_act_layer", "(", "kwargs", ",", "'hard_swish'", ")", ",", "\n", "se_layer", "=", "partial", "(", "SqueezeExcite", ",", "gate_layer", "=", "'hard_sigmoid'", ",", "force_act_layer", "=", "nn", ".", "ReLU", ")", ",", "\n", "num_features", "=", "1280", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "model", "=", "_create_mnv3", "(", "variant", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.mobilenetv3_large_075": [[569, 574], ["mobilenetv3._gen_mobilenet_v3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_mobilenet_v3"], ["", "@", "register_model", "\n", "def", "mobilenetv3_large_075", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V3 \"\"\"", "\n", "model", "=", "_gen_mobilenet_v3", "(", "'mobilenetv3_large_075'", ",", "0.75", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.mobilenetv3_large_100": [[576, 581], ["mobilenetv3._gen_mobilenet_v3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_mobilenet_v3"], ["", "@", "register_model", "\n", "def", "mobilenetv3_large_100", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V3 \"\"\"", "\n", "model", "=", "_gen_mobilenet_v3", "(", "'mobilenetv3_large_100'", ",", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.mobilenetv3_large_100_miil": [[583, 590], ["mobilenetv3._gen_mobilenet_v3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_mobilenet_v3"], ["", "@", "register_model", "\n", "def", "mobilenetv3_large_100_miil", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V3\n    Weights taken from: https://github.com/Alibaba-MIIL/ImageNet21K\n    \"\"\"", "\n", "model", "=", "_gen_mobilenet_v3", "(", "'mobilenetv3_large_100_miil'", ",", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.mobilenetv3_large_100_miil_in21k": [[592, 599], ["mobilenetv3._gen_mobilenet_v3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_mobilenet_v3"], ["", "@", "register_model", "\n", "def", "mobilenetv3_large_100_miil_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V3, 21k pretraining\n    Weights taken from: https://github.com/Alibaba-MIIL/ImageNet21K\n    \"\"\"", "\n", "model", "=", "_gen_mobilenet_v3", "(", "'mobilenetv3_large_100_miil_in21k'", ",", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.mobilenetv3_small_050": [[601, 606], ["mobilenetv3._gen_mobilenet_v3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_mobilenet_v3"], ["", "@", "register_model", "\n", "def", "mobilenetv3_small_050", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V3 \"\"\"", "\n", "model", "=", "_gen_mobilenet_v3", "(", "'mobilenetv3_small_050'", ",", "0.50", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.mobilenetv3_small_075": [[608, 613], ["mobilenetv3._gen_mobilenet_v3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_mobilenet_v3"], ["", "@", "register_model", "\n", "def", "mobilenetv3_small_075", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V3 \"\"\"", "\n", "model", "=", "_gen_mobilenet_v3", "(", "'mobilenetv3_small_075'", ",", "0.75", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.mobilenetv3_small_100": [[615, 620], ["mobilenetv3._gen_mobilenet_v3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_mobilenet_v3"], ["", "@", "register_model", "\n", "def", "mobilenetv3_small_100", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V3 \"\"\"", "\n", "model", "=", "_gen_mobilenet_v3", "(", "'mobilenetv3_small_100'", ",", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.mobilenetv3_rw": [[622, 630], ["mobilenetv3._gen_mobilenet_v3_rw"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_mobilenet_v3_rw"], ["", "@", "register_model", "\n", "def", "mobilenetv3_rw", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V3 \"\"\"", "\n", "if", "pretrained", ":", "\n", "# pretrained model trained with non-default BN epsilon", "\n", "        ", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "", "model", "=", "_gen_mobilenet_v3_rw", "(", "'mobilenetv3_rw'", ",", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.tf_mobilenetv3_large_075": [[632, 639], ["mobilenetv3._gen_mobilenet_v3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_mobilenet_v3"], ["", "@", "register_model", "\n", "def", "tf_mobilenetv3_large_075", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V3 \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_mobilenet_v3", "(", "'tf_mobilenetv3_large_075'", ",", "0.75", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.tf_mobilenetv3_large_100": [[641, 648], ["mobilenetv3._gen_mobilenet_v3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_mobilenet_v3"], ["", "@", "register_model", "\n", "def", "tf_mobilenetv3_large_100", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V3 \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_mobilenet_v3", "(", "'tf_mobilenetv3_large_100'", ",", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.tf_mobilenetv3_large_minimal_100": [[650, 657], ["mobilenetv3._gen_mobilenet_v3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_mobilenet_v3"], ["", "@", "register_model", "\n", "def", "tf_mobilenetv3_large_minimal_100", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V3 \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_mobilenet_v3", "(", "'tf_mobilenetv3_large_minimal_100'", ",", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.tf_mobilenetv3_small_075": [[659, 666], ["mobilenetv3._gen_mobilenet_v3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_mobilenet_v3"], ["", "@", "register_model", "\n", "def", "tf_mobilenetv3_small_075", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V3 \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_mobilenet_v3", "(", "'tf_mobilenetv3_small_075'", ",", "0.75", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.tf_mobilenetv3_small_100": [[668, 675], ["mobilenetv3._gen_mobilenet_v3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_mobilenet_v3"], ["", "@", "register_model", "\n", "def", "tf_mobilenetv3_small_100", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V3 \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_mobilenet_v3", "(", "'tf_mobilenetv3_small_100'", ",", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.tf_mobilenetv3_small_minimal_100": [[677, 684], ["mobilenetv3._gen_mobilenet_v3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_mobilenet_v3"], ["", "@", "register_model", "\n", "def", "tf_mobilenetv3_small_minimal_100", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" MobileNet V3 \"\"\"", "\n", "kwargs", "[", "'bn_eps'", "]", "=", "BN_EPS_TF_DEFAULT", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model", "=", "_gen_mobilenet_v3", "(", "'tf_mobilenetv3_small_minimal_100'", ",", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.fbnetv3_b": [[686, 691], ["mobilenetv3._gen_fbnetv3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_fbnetv3"], ["", "@", "register_model", "\n", "def", "fbnetv3_b", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" FBNetV3-B \"\"\"", "\n", "model", "=", "_gen_fbnetv3", "(", "'fbnetv3_b'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.fbnetv3_d": [[693, 698], ["mobilenetv3._gen_fbnetv3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_fbnetv3"], ["", "@", "register_model", "\n", "def", "fbnetv3_d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" FBNetV3-D \"\"\"", "\n", "model", "=", "_gen_fbnetv3", "(", "'fbnetv3_d'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.fbnetv3_g": [[700, 705], ["mobilenetv3._gen_fbnetv3"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_fbnetv3"], ["", "@", "register_model", "\n", "def", "fbnetv3_g", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" FBNetV3-G \"\"\"", "\n", "model", "=", "_gen_fbnetv3", "(", "'fbnetv3_g'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.lcnet_035": [[707, 712], ["mobilenetv3._gen_lcnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_lcnet"], ["", "@", "register_model", "\n", "def", "lcnet_035", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" PP-LCNet 0.35\"\"\"", "\n", "model", "=", "_gen_lcnet", "(", "'lcnet_035'", ",", "0.35", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.lcnet_050": [[714, 719], ["mobilenetv3._gen_lcnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_lcnet"], ["", "@", "register_model", "\n", "def", "lcnet_050", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" PP-LCNet 0.5\"\"\"", "\n", "model", "=", "_gen_lcnet", "(", "'lcnet_050'", ",", "0.5", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.lcnet_075": [[721, 726], ["mobilenetv3._gen_lcnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_lcnet"], ["", "@", "register_model", "\n", "def", "lcnet_075", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" PP-LCNet 1.0\"\"\"", "\n", "model", "=", "_gen_lcnet", "(", "'lcnet_075'", ",", "0.75", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.lcnet_100": [[728, 733], ["mobilenetv3._gen_lcnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_lcnet"], ["", "@", "register_model", "\n", "def", "lcnet_100", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" PP-LCNet 1.0\"\"\"", "\n", "model", "=", "_gen_lcnet", "(", "'lcnet_100'", ",", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3.lcnet_150": [[735, 740], ["mobilenetv3._gen_lcnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilenetv3._gen_lcnet"], ["", "@", "register_model", "\n", "def", "lcnet_150", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" PP-LCNet 1.5\"\"\"", "\n", "model", "=", "_gen_lcnet", "(", "'lcnet_150'", ",", "1.5", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn.CatBnAct.__init__": [[53, 56], ["torch.Module.__init__", "norm_layer"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_chs", ",", "norm_layer", "=", "BatchNormAct2d", ")", ":", "\n", "        ", "super", "(", "CatBnAct", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bn", "=", "norm_layer", "(", "in_chs", ",", "eps", "=", "0.001", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn.CatBnAct.forward": [[67, 71], ["isinstance", "dpn.CatBnAct.bn", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "isinstance", "(", "x", ",", "tuple", ")", ":", "\n", "            ", "x", "=", "torch", ".", "cat", "(", "x", ",", "dim", "=", "1", ")", "\n", "", "return", "self", ".", "bn", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn.BnActConv2d.__init__": [[74, 78], ["torch.Module.__init__", "norm_layer", "layers.create_conv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d"], ["    ", "def", "__init__", "(", "self", ",", "in_chs", ",", "out_chs", ",", "kernel_size", ",", "stride", ",", "groups", "=", "1", ",", "norm_layer", "=", "BatchNormAct2d", ")", ":", "\n", "        ", "super", "(", "BnActConv2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bn", "=", "norm_layer", "(", "in_chs", ",", "eps", "=", "0.001", ")", "\n", "self", ".", "conv", "=", "create_conv2d", "(", "in_chs", ",", "out_chs", ",", "kernel_size", ",", "stride", "=", "stride", ",", "groups", "=", "groups", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn.BnActConv2d.forward": [[79, 81], ["dpn.BnActConv2d.conv", "dpn.BnActConv2d.bn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "conv", "(", "self", ".", "bn", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn.DualPathBlock.__init__": [[84, 123], ["torch.Module.__init__", "dpn.BnActConv2d", "dpn.BnActConv2d", "dpn.CatBnAct", "layers.create_conv2d", "layers.create_conv2d", "dpn.BnActConv2d", "dpn.BnActConv2d", "dpn.BnActConv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "num_1x1_a", ",", "num_3x3_b", ",", "num_1x1_c", ",", "inc", ",", "groups", ",", "block_type", "=", "'normal'", ",", "b", "=", "False", ")", ":", "\n", "        ", "super", "(", "DualPathBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_1x1_c", "=", "num_1x1_c", "\n", "self", ".", "inc", "=", "inc", "\n", "self", ".", "b", "=", "b", "\n", "if", "block_type", "==", "'proj'", ":", "\n", "            ", "self", ".", "key_stride", "=", "1", "\n", "self", ".", "has_proj", "=", "True", "\n", "", "elif", "block_type", "==", "'down'", ":", "\n", "            ", "self", ".", "key_stride", "=", "2", "\n", "self", ".", "has_proj", "=", "True", "\n", "", "else", ":", "\n", "            ", "assert", "block_type", "==", "'normal'", "\n", "self", ".", "key_stride", "=", "1", "\n", "self", ".", "has_proj", "=", "False", "\n", "\n", "", "self", ".", "c1x1_w_s1", "=", "None", "\n", "self", ".", "c1x1_w_s2", "=", "None", "\n", "if", "self", ".", "has_proj", ":", "\n", "# Using different member names here to allow easier parameter key matching for conversion", "\n", "            ", "if", "self", ".", "key_stride", "==", "2", ":", "\n", "                ", "self", ".", "c1x1_w_s2", "=", "BnActConv2d", "(", "\n", "in_chs", "=", "in_chs", ",", "out_chs", "=", "num_1x1_c", "+", "2", "*", "inc", ",", "kernel_size", "=", "1", ",", "stride", "=", "2", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "c1x1_w_s1", "=", "BnActConv2d", "(", "\n", "in_chs", "=", "in_chs", ",", "out_chs", "=", "num_1x1_c", "+", "2", "*", "inc", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "\n", "", "", "self", ".", "c1x1_a", "=", "BnActConv2d", "(", "in_chs", "=", "in_chs", ",", "out_chs", "=", "num_1x1_a", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "self", ".", "c3x3_b", "=", "BnActConv2d", "(", "\n", "in_chs", "=", "num_1x1_a", ",", "out_chs", "=", "num_3x3_b", ",", "kernel_size", "=", "3", ",", "stride", "=", "self", ".", "key_stride", ",", "groups", "=", "groups", ")", "\n", "if", "b", ":", "\n", "            ", "self", ".", "c1x1_c", "=", "CatBnAct", "(", "in_chs", "=", "num_3x3_b", ")", "\n", "self", ".", "c1x1_c1", "=", "create_conv2d", "(", "num_3x3_b", ",", "num_1x1_c", ",", "kernel_size", "=", "1", ")", "\n", "self", ".", "c1x1_c2", "=", "create_conv2d", "(", "num_3x3_b", ",", "inc", ",", "kernel_size", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "c1x1_c", "=", "BnActConv2d", "(", "in_chs", "=", "num_3x3_b", ",", "out_chs", "=", "num_1x1_c", "+", "inc", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "self", ".", "c1x1_c1", "=", "None", "\n", "self", ".", "c1x1_c2", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn.DualPathBlock.forward": [[134, 166], ["isinstance", "dpn.DualPathBlock.c1x1_a", "dpn.DualPathBlock.c3x3_b", "dpn.DualPathBlock.c1x1_c", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "dpn.DualPathBlock.c1x1_c1", "dpn.DualPathBlock.c1x1_c2", "dpn.DualPathBlock.c1x1_w_s1", "dpn.DualPathBlock.c1x1_w_s2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "if", "isinstance", "(", "x", ",", "tuple", ")", ":", "\n", "            ", "x_in", "=", "torch", ".", "cat", "(", "x", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "x_in", "=", "x", "\n", "", "if", "self", ".", "c1x1_w_s1", "is", "None", "and", "self", ".", "c1x1_w_s2", "is", "None", ":", "\n", "# self.has_proj == False, torchscript requires condition on module == None", "\n", "            ", "x_s1", "=", "x", "[", "0", "]", "\n", "x_s2", "=", "x", "[", "1", "]", "\n", "", "else", ":", "\n", "# self.has_proj == True", "\n", "            ", "if", "self", ".", "c1x1_w_s1", "is", "not", "None", ":", "\n", "# self.key_stride = 1", "\n", "                ", "x_s", "=", "self", ".", "c1x1_w_s1", "(", "x_in", ")", "\n", "", "else", ":", "\n", "# self.key_stride = 2", "\n", "                ", "x_s", "=", "self", ".", "c1x1_w_s2", "(", "x_in", ")", "\n", "", "x_s1", "=", "x_s", "[", ":", ",", ":", "self", ".", "num_1x1_c", ",", ":", ",", ":", "]", "\n", "x_s2", "=", "x_s", "[", ":", ",", "self", ".", "num_1x1_c", ":", ",", ":", ",", ":", "]", "\n", "", "x_in", "=", "self", ".", "c1x1_a", "(", "x_in", ")", "\n", "x_in", "=", "self", ".", "c3x3_b", "(", "x_in", ")", "\n", "x_in", "=", "self", ".", "c1x1_c", "(", "x_in", ")", "\n", "if", "self", ".", "c1x1_c1", "is", "not", "None", ":", "\n", "# self.b == True, using None check for torchscript compat", "\n", "            ", "out1", "=", "self", ".", "c1x1_c1", "(", "x_in", ")", "\n", "out2", "=", "self", ".", "c1x1_c2", "(", "x_in", ")", "\n", "", "else", ":", "\n", "            ", "out1", "=", "x_in", "[", ":", ",", ":", "self", ".", "num_1x1_c", ",", ":", ",", ":", "]", "\n", "out2", "=", "x_in", "[", ":", ",", "self", ".", "num_1x1_c", ":", ",", ":", ",", ":", "]", "\n", "", "resid", "=", "x_s1", "+", "out1", "\n", "dense", "=", "torch", ".", "cat", "(", "[", "x_s2", ",", "out2", "]", ",", "dim", "=", "1", ")", "\n", "return", "resid", ",", "dense", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn.DPN.__init__": [[169, 242], ["torch.Module.__init__", "functools.partial", "functools.partial", "collections.OrderedDict", "layers.ConvNormAct", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "dpn.DualPathBlock", "range", "dpn.DualPathBlock", "range", "dpn.DualPathBlock", "range", "dpn.DualPathBlock", "range", "dpn.CatBnAct", "torch.Sequential", "torch.Sequential", "torch.Sequential", "layers.create_classifier", "dict", "dpn.DualPathBlock", "dict", "dpn.DualPathBlock", "dict", "dpn.DualPathBlock", "dict", "dpn.DualPathBlock", "dict", "torch.Flatten", "torch.Flatten", "torch.Flatten", "torch.Identity", "torch.Identity", "torch.Identity", "str", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["    ", "def", "__init__", "(", "\n", "self", ",", "small", "=", "False", ",", "num_init_features", "=", "64", ",", "k_r", "=", "96", ",", "groups", "=", "32", ",", "global_pool", "=", "'avg'", ",", "\n", "b", "=", "False", ",", "k_sec", "=", "(", "3", ",", "4", ",", "20", ",", "3", ")", ",", "inc_sec", "=", "(", "16", ",", "32", ",", "24", ",", "128", ")", ",", "output_stride", "=", "32", ",", "\n", "num_classes", "=", "1000", ",", "in_chans", "=", "3", ",", "drop_rate", "=", "0.", ",", "fc_act_layer", "=", "nn", ".", "ELU", ")", ":", "\n", "        ", "super", "(", "DPN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "self", ".", "b", "=", "b", "\n", "assert", "output_stride", "==", "32", "# FIXME look into dilation support", "\n", "norm_layer", "=", "partial", "(", "BatchNormAct2d", ",", "eps", "=", ".001", ")", "\n", "fc_norm_layer", "=", "partial", "(", "BatchNormAct2d", ",", "eps", "=", ".001", ",", "act_layer", "=", "fc_act_layer", ",", "inplace", "=", "False", ")", "\n", "bw_factor", "=", "1", "if", "small", "else", "4", "\n", "blocks", "=", "OrderedDict", "(", ")", "\n", "\n", "# conv1", "\n", "blocks", "[", "'conv1_1'", "]", "=", "ConvNormAct", "(", "\n", "in_chans", ",", "num_init_features", ",", "kernel_size", "=", "3", "if", "small", "else", "7", ",", "stride", "=", "2", ",", "norm_layer", "=", "norm_layer", ")", "\n", "blocks", "[", "'conv1_pool'", "]", "=", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", "\n", "self", ".", "feature_info", "=", "[", "dict", "(", "num_chs", "=", "num_init_features", ",", "reduction", "=", "2", ",", "module", "=", "'features.conv1_1'", ")", "]", "\n", "\n", "# conv2", "\n", "bw", "=", "64", "*", "bw_factor", "\n", "inc", "=", "inc_sec", "[", "0", "]", "\n", "r", "=", "(", "k_r", "*", "bw", ")", "//", "(", "64", "*", "bw_factor", ")", "\n", "blocks", "[", "'conv2_1'", "]", "=", "DualPathBlock", "(", "num_init_features", ",", "r", ",", "r", ",", "bw", ",", "inc", ",", "groups", ",", "'proj'", ",", "b", ")", "\n", "in_chs", "=", "bw", "+", "3", "*", "inc", "\n", "for", "i", "in", "range", "(", "2", ",", "k_sec", "[", "0", "]", "+", "1", ")", ":", "\n", "            ", "blocks", "[", "'conv2_'", "+", "str", "(", "i", ")", "]", "=", "DualPathBlock", "(", "in_chs", ",", "r", ",", "r", ",", "bw", ",", "inc", ",", "groups", ",", "'normal'", ",", "b", ")", "\n", "in_chs", "+=", "inc", "\n", "", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "in_chs", ",", "reduction", "=", "4", ",", "module", "=", "f'features.conv2_{k_sec[0]}'", ")", "]", "\n", "\n", "# conv3", "\n", "bw", "=", "128", "*", "bw_factor", "\n", "inc", "=", "inc_sec", "[", "1", "]", "\n", "r", "=", "(", "k_r", "*", "bw", ")", "//", "(", "64", "*", "bw_factor", ")", "\n", "blocks", "[", "'conv3_1'", "]", "=", "DualPathBlock", "(", "in_chs", ",", "r", ",", "r", ",", "bw", ",", "inc", ",", "groups", ",", "'down'", ",", "b", ")", "\n", "in_chs", "=", "bw", "+", "3", "*", "inc", "\n", "for", "i", "in", "range", "(", "2", ",", "k_sec", "[", "1", "]", "+", "1", ")", ":", "\n", "            ", "blocks", "[", "'conv3_'", "+", "str", "(", "i", ")", "]", "=", "DualPathBlock", "(", "in_chs", ",", "r", ",", "r", ",", "bw", ",", "inc", ",", "groups", ",", "'normal'", ",", "b", ")", "\n", "in_chs", "+=", "inc", "\n", "", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "in_chs", ",", "reduction", "=", "8", ",", "module", "=", "f'features.conv3_{k_sec[1]}'", ")", "]", "\n", "\n", "# conv4", "\n", "bw", "=", "256", "*", "bw_factor", "\n", "inc", "=", "inc_sec", "[", "2", "]", "\n", "r", "=", "(", "k_r", "*", "bw", ")", "//", "(", "64", "*", "bw_factor", ")", "\n", "blocks", "[", "'conv4_1'", "]", "=", "DualPathBlock", "(", "in_chs", ",", "r", ",", "r", ",", "bw", ",", "inc", ",", "groups", ",", "'down'", ",", "b", ")", "\n", "in_chs", "=", "bw", "+", "3", "*", "inc", "\n", "for", "i", "in", "range", "(", "2", ",", "k_sec", "[", "2", "]", "+", "1", ")", ":", "\n", "            ", "blocks", "[", "'conv4_'", "+", "str", "(", "i", ")", "]", "=", "DualPathBlock", "(", "in_chs", ",", "r", ",", "r", ",", "bw", ",", "inc", ",", "groups", ",", "'normal'", ",", "b", ")", "\n", "in_chs", "+=", "inc", "\n", "", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "in_chs", ",", "reduction", "=", "16", ",", "module", "=", "f'features.conv4_{k_sec[2]}'", ")", "]", "\n", "\n", "# conv5", "\n", "bw", "=", "512", "*", "bw_factor", "\n", "inc", "=", "inc_sec", "[", "3", "]", "\n", "r", "=", "(", "k_r", "*", "bw", ")", "//", "(", "64", "*", "bw_factor", ")", "\n", "blocks", "[", "'conv5_1'", "]", "=", "DualPathBlock", "(", "in_chs", ",", "r", ",", "r", ",", "bw", ",", "inc", ",", "groups", ",", "'down'", ",", "b", ")", "\n", "in_chs", "=", "bw", "+", "3", "*", "inc", "\n", "for", "i", "in", "range", "(", "2", ",", "k_sec", "[", "3", "]", "+", "1", ")", ":", "\n", "            ", "blocks", "[", "'conv5_'", "+", "str", "(", "i", ")", "]", "=", "DualPathBlock", "(", "in_chs", ",", "r", ",", "r", ",", "bw", ",", "inc", ",", "groups", ",", "'normal'", ",", "b", ")", "\n", "in_chs", "+=", "inc", "\n", "", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "in_chs", ",", "reduction", "=", "32", ",", "module", "=", "f'features.conv5_{k_sec[3]}'", ")", "]", "\n", "\n", "blocks", "[", "'conv5_bn_ac'", "]", "=", "CatBnAct", "(", "in_chs", ",", "norm_layer", "=", "fc_norm_layer", ")", "\n", "\n", "self", ".", "num_features", "=", "in_chs", "\n", "self", ".", "features", "=", "nn", ".", "Sequential", "(", "blocks", ")", "\n", "\n", "# Using 1x1 conv for the FC layer to allow the extra pooling scheme", "\n", "self", ".", "global_pool", ",", "self", ".", "classifier", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ",", "use_conv", "=", "True", ")", "\n", "self", ".", "flatten", "=", "nn", ".", "Flatten", "(", "1", ")", "if", "global_pool", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn.DPN.group_matcher": [[243, 253], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "\n", "stem", "=", "r'^features\\.conv1'", ",", "\n", "blocks", "=", "[", "\n", "(", "r'^features\\.conv(\\d+)'", "if", "coarse", "else", "r'^features\\.conv(\\d+)_(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^features\\.conv5_bn_ac'", ",", "(", "99999", ",", ")", ")", "\n", "]", "\n", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn.DPN.set_grad_checkpointing": [[254, 257], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "'gradient checkpointing not supported'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn.DPN.get_classifier": [[258, 261], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "classifier", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn.DPN.reset_classifier": [[262, 267], ["layers.create_classifier", "torch.Flatten", "torch.Flatten", "torch.Flatten", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", ",", "self", ".", "classifier", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ",", "use_conv", "=", "True", ")", "\n", "self", ".", "flatten", "=", "nn", ".", "Flatten", "(", "1", ")", "if", "global_pool", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn.DPN.forward_features": [[268, 270], ["dpn.DPN.features"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "features", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn.DPN.forward_head": [[271, 280], ["dpn.DPN.global_pool", "torch.dropout", "torch.dropout", "torch.dropout", "dpn.DPN.flatten", "dpn.DPN.classifier", "dpn.DPN.flatten"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "if", "self", ".", "drop_rate", ">", "0.", ":", "\n", "            ", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "drop_rate", ",", "training", "=", "self", ".", "training", ")", "\n", "", "if", "pre_logits", ":", "\n", "            ", "return", "x", ".", "flatten", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "classifier", "(", "x", ")", "\n", "return", "self", ".", "flatten", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn.DPN.forward": [[281, 285], ["dpn.DPN.forward_features", "dpn.DPN.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn._cfg": [[25, 32], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bicubic'", ",", "\n", "'mean'", ":", "IMAGENET_DPN_MEAN", ",", "'std'", ":", "IMAGENET_DPN_STD", ",", "\n", "'first_conv'", ":", "'features.conv1_1.conv'", ",", "'classifier'", ":", "'classifier'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn._create_dpn": [[287, 292], ["helpers.build_model_with_cfg", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_dpn", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "\n", "DPN", ",", "variant", ",", "pretrained", ",", "\n", "feature_cfg", "=", "dict", "(", "feature_concat", "=", "True", ",", "flatten_sequential", "=", "True", ")", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn.dpn68": [[294, 300], ["dict", "dpn._create_dpn"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn._create_dpn"], ["", "@", "register_model", "\n", "def", "dpn68", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "small", "=", "True", ",", "num_init_features", "=", "10", ",", "k_r", "=", "128", ",", "groups", "=", "32", ",", "\n", "k_sec", "=", "(", "3", ",", "4", ",", "12", ",", "3", ")", ",", "inc_sec", "=", "(", "16", ",", "32", ",", "32", ",", "64", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_dpn", "(", "'dpn68'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn.dpn68b": [[302, 308], ["dict", "dpn._create_dpn"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn._create_dpn"], ["", "@", "register_model", "\n", "def", "dpn68b", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "small", "=", "True", ",", "num_init_features", "=", "10", ",", "k_r", "=", "128", ",", "groups", "=", "32", ",", "\n", "b", "=", "True", ",", "k_sec", "=", "(", "3", ",", "4", ",", "12", ",", "3", ")", ",", "inc_sec", "=", "(", "16", ",", "32", ",", "32", ",", "64", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_dpn", "(", "'dpn68b'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn.dpn92": [[310, 316], ["dict", "dpn._create_dpn"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn._create_dpn"], ["", "@", "register_model", "\n", "def", "dpn92", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "num_init_features", "=", "64", ",", "k_r", "=", "96", ",", "groups", "=", "32", ",", "\n", "k_sec", "=", "(", "3", ",", "4", ",", "20", ",", "3", ")", ",", "inc_sec", "=", "(", "16", ",", "32", ",", "24", ",", "128", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_dpn", "(", "'dpn92'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn.dpn98": [[318, 324], ["dict", "dpn._create_dpn"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn._create_dpn"], ["", "@", "register_model", "\n", "def", "dpn98", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "num_init_features", "=", "96", ",", "k_r", "=", "160", ",", "groups", "=", "40", ",", "\n", "k_sec", "=", "(", "3", ",", "6", ",", "20", ",", "3", ")", ",", "inc_sec", "=", "(", "16", ",", "32", ",", "32", ",", "128", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_dpn", "(", "'dpn98'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn.dpn131": [[326, 332], ["dict", "dpn._create_dpn"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn._create_dpn"], ["", "@", "register_model", "\n", "def", "dpn131", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "num_init_features", "=", "128", ",", "k_r", "=", "160", ",", "groups", "=", "40", ",", "\n", "k_sec", "=", "(", "4", ",", "8", ",", "28", ",", "3", ")", ",", "inc_sec", "=", "(", "16", ",", "32", ",", "32", ",", "128", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_dpn", "(", "'dpn131'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn.dpn107": [[334, 340], ["dict", "dpn._create_dpn"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dpn._create_dpn"], ["", "@", "register_model", "\n", "def", "dpn107", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "num_init_features", "=", "128", ",", "k_r", "=", "200", ",", "groups", "=", "50", ",", "\n", "k_sec", "=", "(", "4", ",", "8", ",", "20", ",", "3", ")", ",", "inc_sec", "=", "(", "20", ",", "64", ",", "64", ",", "128", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_dpn", "(", "'dpn107'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.ConvNorm.__init__": [[128, 136], ["torch.Sequential.__init__", "levit.ConvNorm.add_module", "levit.ConvNorm.add_module", "torch.init.constant_", "torch.init.constant_", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "pad", "=", "0", ",", "dilation", "=", "1", ",", "\n", "groups", "=", "1", ",", "bn_weight_init", "=", "1", ",", "resolution", "=", "-", "10000", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "add_module", "(", "'c'", ",", "nn", ".", "Conv2d", "(", "in_chs", ",", "out_chs", ",", "kernel_size", ",", "stride", ",", "pad", ",", "dilation", ",", "groups", ",", "bias", "=", "False", ")", ")", "\n", "self", ".", "add_module", "(", "'bn'", ",", "nn", ".", "BatchNorm2d", "(", "out_chs", ")", ")", "\n", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "bn", ".", "weight", ",", "bn_weight_init", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.ConvNorm.fuse": [[137, 149], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "levit.ConvNorm._modules.values", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d.weight.data.copy_", "torch.Conv2d.bias.data.copy_", "w.size", "w.size"], "methods", ["None"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "fuse", "(", "self", ")", ":", "\n", "        ", "c", ",", "bn", "=", "self", ".", "_modules", ".", "values", "(", ")", "\n", "w", "=", "bn", ".", "weight", "/", "(", "bn", ".", "running_var", "+", "bn", ".", "eps", ")", "**", "0.5", "\n", "w", "=", "c", ".", "weight", "*", "w", "[", ":", ",", "None", ",", "None", ",", "None", "]", "\n", "b", "=", "bn", ".", "bias", "-", "bn", ".", "running_mean", "*", "bn", ".", "weight", "/", "(", "bn", ".", "running_var", "+", "bn", ".", "eps", ")", "**", "0.5", "\n", "m", "=", "nn", ".", "Conv2d", "(", "\n", "w", ".", "size", "(", "1", ")", ",", "w", ".", "size", "(", "0", ")", ",", "w", ".", "shape", "[", "2", ":", "]", ",", "stride", "=", "self", ".", "c", ".", "stride", ",", "\n", "padding", "=", "self", ".", "c", ".", "padding", ",", "dilation", "=", "self", ".", "c", ".", "dilation", ",", "groups", "=", "self", ".", "c", ".", "groups", ")", "\n", "m", ".", "weight", ".", "data", ".", "copy_", "(", "w", ")", "\n", "m", ".", "bias", ".", "data", ".", "copy_", "(", "b", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.LinearNorm.__init__": [[152, 158], ["torch.Sequential.__init__", "levit.LinearNorm.add_module", "levit.LinearNorm.add_module", "torch.init.constant_", "torch.init.constant_", "torch.Linear", "torch.Linear", "torch.BatchNorm1d", "torch.BatchNorm1d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_features", ",", "out_features", ",", "bn_weight_init", "=", "1", ",", "resolution", "=", "-", "100000", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "add_module", "(", "'c'", ",", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "False", ")", ")", "\n", "self", ".", "add_module", "(", "'bn'", ",", "nn", ".", "BatchNorm1d", "(", "out_features", ")", ")", "\n", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "bn", ".", "weight", ",", "bn_weight_init", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.LinearNorm.fuse": [[159, 169], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "levit.LinearNorm._modules.values", "torch.Linear", "torch.Linear", "torch.Linear.weight.data.copy_", "torch.Linear.bias.data.copy_", "w.size", "w.size"], "methods", ["None"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "fuse", "(", "self", ")", ":", "\n", "        ", "l", ",", "bn", "=", "self", ".", "_modules", ".", "values", "(", ")", "\n", "w", "=", "bn", ".", "weight", "/", "(", "bn", ".", "running_var", "+", "bn", ".", "eps", ")", "**", "0.5", "\n", "w", "=", "l", ".", "weight", "*", "w", "[", ":", ",", "None", "]", "\n", "b", "=", "bn", ".", "bias", "-", "bn", ".", "running_mean", "*", "bn", ".", "weight", "/", "(", "bn", ".", "running_var", "+", "bn", ".", "eps", ")", "**", "0.5", "\n", "m", "=", "nn", ".", "Linear", "(", "w", ".", "size", "(", "1", ")", ",", "w", ".", "size", "(", "0", ")", ")", "\n", "m", ".", "weight", ".", "data", ".", "copy_", "(", "w", ")", "\n", "m", ".", "bias", ".", "data", ".", "copy_", "(", "b", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.LinearNorm.forward": [[170, 173], ["levit.LinearNorm.c", "levit.LinearNorm.bn().reshape_as", "levit.LinearNorm.bn", "levit.LinearNorm.flatten"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "c", "(", "x", ")", "\n", "return", "self", ".", "bn", "(", "x", ".", "flatten", "(", "0", ",", "1", ")", ")", ".", "reshape_as", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.NormLinear.__init__": [[176, 184], ["torch.Sequential.__init__", "levit.NormLinear.add_module", "levit.NormLinear.add_module", "vision_transformer.trunc_normal_", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.Linear", "torch.Linear", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["    ", "def", "__init__", "(", "self", ",", "in_features", ",", "out_features", ",", "bias", "=", "True", ",", "std", "=", "0.02", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "add_module", "(", "'bn'", ",", "nn", ".", "BatchNorm1d", "(", "in_features", ")", ")", "\n", "self", ".", "add_module", "(", "'l'", ",", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "bias", ")", ")", "\n", "\n", "trunc_normal_", "(", "self", ".", "l", ".", "weight", ",", "std", "=", "std", ")", "\n", "if", "self", ".", "l", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "l", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.NormLinear.fuse": [[185, 199], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "levit.NormLinear._modules.values", "torch.Linear", "torch.Linear", "torch.Linear.weight.data.copy_", "torch.Linear.bias.data.copy_", "w.size", "w.size"], "methods", ["None"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "fuse", "(", "self", ")", ":", "\n", "        ", "bn", ",", "l", "=", "self", ".", "_modules", ".", "values", "(", ")", "\n", "w", "=", "bn", ".", "weight", "/", "(", "bn", ".", "running_var", "+", "bn", ".", "eps", ")", "**", "0.5", "\n", "b", "=", "bn", ".", "bias", "-", "self", ".", "bn", ".", "running_mean", "*", "self", ".", "bn", ".", "weight", "/", "(", "bn", ".", "running_var", "+", "bn", ".", "eps", ")", "**", "0.5", "\n", "w", "=", "l", ".", "weight", "*", "w", "[", "None", ",", ":", "]", "\n", "if", "l", ".", "bias", "is", "None", ":", "\n", "            ", "b", "=", "b", "@", "self", ".", "l", ".", "weight", ".", "T", "\n", "", "else", ":", "\n", "            ", "b", "=", "(", "l", ".", "weight", "@", "b", "[", ":", ",", "None", "]", ")", ".", "view", "(", "-", "1", ")", "+", "self", ".", "l", ".", "bias", "\n", "", "m", "=", "nn", ".", "Linear", "(", "w", ".", "size", "(", "1", ")", ",", "w", ".", "size", "(", "0", ")", ")", "\n", "m", ".", "weight", ".", "data", ".", "copy_", "(", "w", ")", "\n", "m", ".", "bias", ".", "data", ".", "copy_", "(", "b", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.Residual.__init__": [[213, 217], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "m", ",", "drop", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "m", "=", "m", "\n", "self", ".", "drop", "=", "drop", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.Residual.forward": [[218, 224], ["levit.Residual.m", "levit.Residual.m", "torch.rand().ge_().div().detach", "torch.rand().ge_().div().detach", "torch.rand().ge_().div().detach", "torch.rand().ge_().div().detach", "torch.rand().ge_().div", "torch.rand().ge_().div", "torch.rand().ge_().div", "torch.rand().ge_().div", "torch.rand().ge_", "torch.rand().ge_", "torch.rand().ge_", "torch.rand().ge_", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "x.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "training", "and", "self", ".", "drop", ">", "0", ":", "\n", "            ", "return", "x", "+", "self", ".", "m", "(", "x", ")", "*", "torch", ".", "rand", "(", "\n", "x", ".", "size", "(", "0", ")", ",", "1", ",", "1", ",", "device", "=", "x", ".", "device", ")", ".", "ge_", "(", "self", ".", "drop", ")", ".", "div", "(", "1", "-", "self", ".", "drop", ")", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "x", "+", "self", ".", "m", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.Subsample.__init__": [[227, 231], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "stride", ",", "resolution", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "resolution", "=", "resolution", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.Subsample.forward": [[232, 236], ["x.reshape", "x.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "x", "=", "x", ".", "view", "(", "B", ",", "self", ".", "resolution", ",", "self", ".", "resolution", ",", "C", ")", "[", ":", ",", ":", ":", "self", ".", "stride", ",", ":", ":", "self", ".", "stride", "]", "\n", "return", "x", ".", "reshape", "(", "B", ",", "-", "1", ",", "C", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.Attention.__init__": [[241, 265], ["torch.Module.__init__", "int", "ln_layer", "torch.Sequential", "torch.Sequential", "torch.Parameter", "torch.Parameter", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "levit.Attention.register_buffer", "int", "act_layer", "ln_layer", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "dim", ",", "key_dim", ",", "num_heads", "=", "8", ",", "attn_ratio", "=", "4", ",", "act_layer", "=", "None", ",", "resolution", "=", "14", ",", "use_conv", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "ln_layer", "=", "ConvNorm", "if", "use_conv", "else", "LinearNorm", "\n", "self", ".", "use_conv", "=", "use_conv", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "scale", "=", "key_dim", "**", "-", "0.5", "\n", "self", ".", "key_dim", "=", "key_dim", "\n", "self", ".", "key_attn_dim", "=", "key_dim", "*", "num_heads", "\n", "self", ".", "val_dim", "=", "int", "(", "attn_ratio", "*", "key_dim", ")", "\n", "self", ".", "val_attn_dim", "=", "int", "(", "attn_ratio", "*", "key_dim", ")", "*", "num_heads", "\n", "\n", "self", ".", "qkv", "=", "ln_layer", "(", "dim", ",", "self", ".", "val_attn_dim", "+", "self", ".", "key_attn_dim", "*", "2", ",", "resolution", "=", "resolution", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Sequential", "(", "\n", "act_layer", "(", ")", ",", "\n", "ln_layer", "(", "self", ".", "val_attn_dim", ",", "dim", ",", "bn_weight_init", "=", "0", ",", "resolution", "=", "resolution", ")", "\n", ")", "\n", "\n", "self", ".", "attention_biases", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "num_heads", ",", "resolution", "**", "2", ")", ")", "\n", "pos", "=", "torch", ".", "stack", "(", "torch", ".", "meshgrid", "(", "torch", ".", "arange", "(", "resolution", ")", ",", "torch", ".", "arange", "(", "resolution", ")", ")", ")", ".", "flatten", "(", "1", ")", "\n", "rel_pos", "=", "(", "pos", "[", "...", ",", ":", ",", "None", "]", "-", "pos", "[", "...", ",", "None", ",", ":", "]", ")", ".", "abs", "(", ")", "\n", "rel_pos", "=", "(", "rel_pos", "[", "0", "]", "*", "resolution", ")", "+", "rel_pos", "[", "1", "]", "\n", "self", ".", "register_buffer", "(", "'attention_bias_idxs'", ",", "rel_pos", ")", "\n", "self", ".", "ab", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.Attention.train": [[266, 271], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "super().train"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.AttentionSubsample.train"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "train", "(", "self", ",", "mode", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "train", "(", "mode", ")", "\n", "if", "mode", "and", "self", ".", "ab", ":", "\n", "            ", "self", ".", "ab", "=", "{", "}", "# clear ab cache", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.Attention.get_attention_biases": [[272, 280], ["str"], "methods", ["None"], ["", "", "def", "get_attention_biases", "(", "self", ",", "device", ":", "torch", ".", "device", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "if", "self", ".", "training", ":", "\n", "            ", "return", "self", ".", "attention_biases", "[", ":", ",", "self", ".", "attention_bias_idxs", "]", "\n", "", "else", ":", "\n", "            ", "device_key", "=", "str", "(", "device", ")", "\n", "if", "device_key", "not", "in", "self", ".", "ab", ":", "\n", "                ", "self", ".", "ab", "[", "device_key", "]", "=", "self", ".", "attention_biases", "[", ":", ",", "self", ".", "attention_bias_idxs", "]", "\n", "", "return", "self", ".", "ab", "[", "device_key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.Attention.forward": [[281, 305], ["levit.Attention.proj", "levit.Attention.qkv().view().split", "attn.softmax.softmax.softmax", "levit.Attention.qkv().view().split", "q.permute.permute.permute", "k.permute.permute.permute", "v.permute.permute.permute", "attn.softmax.softmax.softmax", "levit.Attention.get_attention_biases", "levit.Attention.get_attention_biases", "levit.Attention.qkv().view", "levit.Attention.qkv().view", "q.permute.permute.transpose", "attn.softmax.softmax.transpose", "levit.Attention.qkv", "levit.Attention.qkv"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.AttentionSubsample.get_attention_biases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.AttentionSubsample.get_attention_biases"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "# x (B,C,H,W)", "\n", "        ", "if", "self", ".", "use_conv", ":", "\n", "            ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "q", ",", "k", ",", "v", "=", "self", ".", "qkv", "(", "x", ")", ".", "view", "(", "\n", "B", ",", "self", ".", "num_heads", ",", "-", "1", ",", "H", "*", "W", ")", ".", "split", "(", "[", "self", ".", "key_dim", ",", "self", ".", "key_dim", ",", "self", ".", "val_dim", "]", ",", "dim", "=", "2", ")", "\n", "\n", "attn", "=", "(", "q", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", "@", "k", ")", "*", "self", ".", "scale", "+", "self", ".", "get_attention_biases", "(", "x", ".", "device", ")", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "x", "=", "(", "v", "@", "attn", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", ".", "view", "(", "B", ",", "-", "1", ",", "H", ",", "W", ")", "\n", "", "else", ":", "\n", "            ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "q", ",", "k", ",", "v", "=", "self", ".", "qkv", "(", "x", ")", ".", "view", "(", "\n", "B", ",", "N", ",", "self", ".", "num_heads", ",", "-", "1", ")", ".", "split", "(", "[", "self", ".", "key_dim", ",", "self", ".", "key_dim", ",", "self", ".", "val_dim", "]", ",", "dim", "=", "3", ")", "\n", "q", "=", "q", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "k", "=", "k", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "\n", "v", "=", "v", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n", "attn", "=", "q", "@", "k", "*", "self", ".", "scale", "+", "self", ".", "get_attention_biases", "(", "x", ".", "device", ")", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "N", ",", "self", ".", "val_attn_dim", ")", "\n", "", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.AttentionSubsample.__init__": [[310, 352], ["torch.Module.__init__", "int", "ln_layer", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Parameter", "torch.Parameter", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "levit.AttentionSubsample.register_buffer", "functools.partial", "functools.partial", "functools.partial.", "ln_layer", "act_layer", "ln_layer", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "in_dim", ",", "out_dim", ",", "key_dim", ",", "num_heads", "=", "8", ",", "attn_ratio", "=", "2", ",", "\n", "act_layer", "=", "None", ",", "stride", "=", "2", ",", "resolution", "=", "14", ",", "resolution_out", "=", "7", ",", "use_conv", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "scale", "=", "key_dim", "**", "-", "0.5", "\n", "self", ".", "key_dim", "=", "key_dim", "\n", "self", ".", "key_attn_dim", "=", "key_dim", "*", "num_heads", "\n", "self", ".", "val_dim", "=", "int", "(", "attn_ratio", "*", "key_dim", ")", "\n", "self", ".", "val_attn_dim", "=", "self", ".", "val_dim", "*", "self", ".", "num_heads", "\n", "self", ".", "resolution", "=", "resolution", "\n", "self", ".", "resolution_out_area", "=", "resolution_out", "**", "2", "\n", "\n", "self", ".", "use_conv", "=", "use_conv", "\n", "if", "self", ".", "use_conv", ":", "\n", "            ", "ln_layer", "=", "ConvNorm", "\n", "sub_layer", "=", "partial", "(", "nn", ".", "AvgPool2d", ",", "kernel_size", "=", "1", ",", "padding", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "ln_layer", "=", "LinearNorm", "\n", "sub_layer", "=", "partial", "(", "Subsample", ",", "resolution", "=", "resolution", ")", "\n", "\n", "", "self", ".", "kv", "=", "ln_layer", "(", "in_dim", ",", "self", ".", "val_attn_dim", "+", "self", ".", "key_attn_dim", ",", "resolution", "=", "resolution", ")", "\n", "self", ".", "q", "=", "nn", ".", "Sequential", "(", "\n", "sub_layer", "(", "stride", "=", "stride", ")", ",", "\n", "ln_layer", "(", "in_dim", ",", "self", ".", "key_attn_dim", ",", "resolution", "=", "resolution_out", ")", "\n", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Sequential", "(", "\n", "act_layer", "(", ")", ",", "\n", "ln_layer", "(", "self", ".", "val_attn_dim", ",", "out_dim", ",", "resolution", "=", "resolution_out", ")", "\n", ")", "\n", "\n", "self", ".", "attention_biases", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "num_heads", ",", "self", ".", "resolution", "**", "2", ")", ")", "\n", "k_pos", "=", "torch", ".", "stack", "(", "torch", ".", "meshgrid", "(", "torch", ".", "arange", "(", "resolution", ")", ",", "torch", ".", "arange", "(", "resolution", ")", ")", ")", ".", "flatten", "(", "1", ")", "\n", "q_pos", "=", "torch", ".", "stack", "(", "torch", ".", "meshgrid", "(", "\n", "torch", ".", "arange", "(", "0", ",", "resolution", ",", "step", "=", "stride", ")", ",", "\n", "torch", ".", "arange", "(", "0", ",", "resolution", ",", "step", "=", "stride", ")", ")", ")", ".", "flatten", "(", "1", ")", "\n", "rel_pos", "=", "(", "q_pos", "[", "...", ",", ":", ",", "None", "]", "-", "k_pos", "[", "...", ",", "None", ",", ":", "]", ")", ".", "abs", "(", ")", "\n", "rel_pos", "=", "(", "rel_pos", "[", "0", "]", "*", "resolution", ")", "+", "rel_pos", "[", "1", "]", "\n", "self", ".", "register_buffer", "(", "'attention_bias_idxs'", ",", "rel_pos", ")", "\n", "\n", "self", ".", "ab", "=", "{", "}", "# per-device attention_biases cache", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.AttentionSubsample.train": [[353, 358], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "super().train"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.AttentionSubsample.train"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "train", "(", "self", ",", "mode", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "train", "(", "mode", ")", "\n", "if", "mode", "and", "self", ".", "ab", ":", "\n", "            ", "self", ".", "ab", "=", "{", "}", "# clear ab cache", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.AttentionSubsample.get_attention_biases": [[359, 367], ["str"], "methods", ["None"], ["", "", "def", "get_attention_biases", "(", "self", ",", "device", ":", "torch", ".", "device", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "if", "self", ".", "training", ":", "\n", "            ", "return", "self", ".", "attention_biases", "[", ":", ",", "self", ".", "attention_bias_idxs", "]", "\n", "", "else", ":", "\n", "            ", "device_key", "=", "str", "(", "device", ")", "\n", "if", "device_key", "not", "in", "self", ".", "ab", ":", "\n", "                ", "self", ".", "ab", "[", "device_key", "]", "=", "self", ".", "attention_biases", "[", ":", ",", "self", ".", "attention_bias_idxs", "]", "\n", "", "return", "self", ".", "ab", "[", "device_key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.AttentionSubsample.forward": [[368, 391], ["levit.AttentionSubsample.proj", "levit.AttentionSubsample.kv().view().split", "levit.AttentionSubsample.q().view", "attn.softmax.softmax.softmax", "levit.AttentionSubsample.kv().view().split", "k.permute.permute.permute", "v.permute.permute.permute", "levit.AttentionSubsample.q().view().permute", "attn.softmax.softmax.softmax", "levit.AttentionSubsample.get_attention_biases", "levit.AttentionSubsample.get_attention_biases", "levit.AttentionSubsample.kv().view", "levit.AttentionSubsample.q", "levit.AttentionSubsample.kv().view", "levit.AttentionSubsample.q().view", "levit.AttentionSubsample.transpose", "attn.softmax.softmax.transpose", "levit.AttentionSubsample.kv", "levit.AttentionSubsample.kv", "levit.AttentionSubsample.q"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.AttentionSubsample.get_attention_biases", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.AttentionSubsample.get_attention_biases"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "use_conv", ":", "\n", "            ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "k", ",", "v", "=", "self", ".", "kv", "(", "x", ")", ".", "view", "(", "B", ",", "self", ".", "num_heads", ",", "-", "1", ",", "H", "*", "W", ")", ".", "split", "(", "[", "self", ".", "key_dim", ",", "self", ".", "val_dim", "]", ",", "dim", "=", "2", ")", "\n", "q", "=", "self", ".", "q", "(", "x", ")", ".", "view", "(", "B", ",", "self", ".", "num_heads", ",", "self", ".", "key_dim", ",", "self", ".", "resolution_out_area", ")", "\n", "\n", "attn", "=", "(", "q", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", "@", "k", ")", "*", "self", ".", "scale", "+", "self", ".", "get_attention_biases", "(", "x", ".", "device", ")", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "x", "=", "(", "v", "@", "attn", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", ".", "reshape", "(", "B", ",", "-", "1", ",", "self", ".", "resolution", ",", "self", ".", "resolution", ")", "\n", "", "else", ":", "\n", "            ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "k", ",", "v", "=", "self", ".", "kv", "(", "x", ")", ".", "view", "(", "B", ",", "N", ",", "self", ".", "num_heads", ",", "-", "1", ")", ".", "split", "(", "[", "self", ".", "key_dim", ",", "self", ".", "val_dim", "]", ",", "dim", "=", "3", ")", "\n", "k", "=", "k", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "# BHCN", "\n", "v", "=", "v", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "# BHNC", "\n", "q", "=", "self", ".", "q", "(", "x", ")", ".", "view", "(", "B", ",", "self", ".", "resolution_out_area", ",", "self", ".", "num_heads", ",", "self", ".", "key_dim", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n", "attn", "=", "q", "@", "k", "*", "self", ".", "scale", "+", "self", ".", "get_attention_biases", "(", "x", ".", "device", ")", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "-", "1", ",", "self", ".", "val_attn_dim", ")", "\n", "", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.Levit.__init__": [[400, 490], ["torch.Module.__init__", "layers.get_act_layer", "layers.get_act_layer", "isinstance", "len", "enumerate", "torch.Sequential", "torch.Sequential", "len", "len", "layers.to_ntuple", "layers.to_ntuple", "layers.to_ntuple", "levit.stem_b16", "zip", "range", "levit.NormLinear", "torch.Identity", "torch.Identity", "levit.Levit.blocks.append", "levit.Levit.blocks.append", "levit.Residual", "int", "levit.Levit.blocks.append", "levit.AttentionSubsample", "int", "levit.Levit.blocks.append", "levit.Attention", "levit.Residual", "levit.Residual", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "ln_layer", "layers.get_act_layer.", "ln_layer", "ln_layer", "layers.get_act_layer.", "ln_layer"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.stem_b16"], ["def", "__init__", "(", "\n", "self", ",", "\n", "img_size", "=", "224", ",", "\n", "patch_size", "=", "16", ",", "\n", "in_chans", "=", "3", ",", "\n", "num_classes", "=", "1000", ",", "\n", "embed_dim", "=", "(", "192", ",", ")", ",", "\n", "key_dim", "=", "64", ",", "\n", "depth", "=", "(", "12", ",", ")", ",", "\n", "num_heads", "=", "(", "3", ",", ")", ",", "\n", "attn_ratio", "=", "2", ",", "\n", "mlp_ratio", "=", "2", ",", "\n", "hybrid_backbone", "=", "None", ",", "\n", "down_ops", "=", "None", ",", "\n", "act_layer", "=", "'hard_swish'", ",", "\n", "attn_act_layer", "=", "'hard_swish'", ",", "\n", "use_conv", "=", "False", ",", "\n", "global_pool", "=", "'avg'", ",", "\n", "drop_rate", "=", "0.", ",", "\n", "drop_path_rate", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "act_layer", "=", "get_act_layer", "(", "act_layer", ")", "\n", "attn_act_layer", "=", "get_act_layer", "(", "attn_act_layer", ")", "\n", "ln_layer", "=", "ConvNorm", "if", "use_conv", "else", "LinearNorm", "\n", "self", ".", "use_conv", "=", "use_conv", "\n", "if", "isinstance", "(", "img_size", ",", "tuple", ")", ":", "\n", "# FIXME origin impl passes single img/res dim through whole hierarchy,", "\n", "# not sure this model will be used enough to spend time fixing it.", "\n", "            ", "assert", "img_size", "[", "0", "]", "==", "img_size", "[", "1", "]", "\n", "img_size", "=", "img_size", "[", "0", "]", "\n", "", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "self", ".", "num_features", "=", "embed_dim", "[", "-", "1", "]", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "num_stages", "=", "len", "(", "embed_dim", ")", "\n", "assert", "len", "(", "depth", ")", "==", "len", "(", "num_heads", ")", "==", "num_stages", "\n", "key_dim", "=", "to_ntuple", "(", "num_stages", ")", "(", "key_dim", ")", "\n", "attn_ratio", "=", "to_ntuple", "(", "num_stages", ")", "(", "attn_ratio", ")", "\n", "mlp_ratio", "=", "to_ntuple", "(", "num_stages", ")", "(", "mlp_ratio", ")", "\n", "down_ops", "=", "down_ops", "or", "(", "\n", "# ('Subsample',key_dim, num_heads, attn_ratio, mlp_ratio, stride)", "\n", "(", "'Subsample'", ",", "key_dim", "[", "0", "]", ",", "embed_dim", "[", "0", "]", "//", "key_dim", "[", "0", "]", ",", "4", ",", "2", ",", "2", ")", ",", "\n", "(", "'Subsample'", ",", "key_dim", "[", "0", "]", ",", "embed_dim", "[", "1", "]", "//", "key_dim", "[", "1", "]", ",", "4", ",", "2", ",", "2", ")", ",", "\n", "(", "''", ",", ")", "\n", ")", "\n", "\n", "self", ".", "patch_embed", "=", "hybrid_backbone", "or", "stem_b16", "(", "in_chans", ",", "embed_dim", "[", "0", "]", ",", "activation", "=", "act_layer", ")", "\n", "\n", "self", ".", "blocks", "=", "[", "]", "\n", "resolution", "=", "img_size", "//", "patch_size", "\n", "for", "i", ",", "(", "ed", ",", "kd", ",", "dpth", ",", "nh", ",", "ar", ",", "mr", ",", "do", ")", "in", "enumerate", "(", "\n", "zip", "(", "embed_dim", ",", "key_dim", ",", "depth", ",", "num_heads", ",", "attn_ratio", ",", "mlp_ratio", ",", "down_ops", ")", ")", ":", "\n", "            ", "for", "_", "in", "range", "(", "dpth", ")", ":", "\n", "                ", "self", ".", "blocks", ".", "append", "(", "\n", "Residual", "(", "\n", "Attention", "(", "\n", "ed", ",", "kd", ",", "nh", ",", "attn_ratio", "=", "ar", ",", "act_layer", "=", "attn_act_layer", ",", "\n", "resolution", "=", "resolution", ",", "use_conv", "=", "use_conv", ")", ",", "\n", "drop_path_rate", ")", ")", "\n", "if", "mr", ">", "0", ":", "\n", "                    ", "h", "=", "int", "(", "ed", "*", "mr", ")", "\n", "self", ".", "blocks", ".", "append", "(", "\n", "Residual", "(", "nn", ".", "Sequential", "(", "\n", "ln_layer", "(", "ed", ",", "h", ",", "resolution", "=", "resolution", ")", ",", "\n", "act_layer", "(", ")", ",", "\n", "ln_layer", "(", "h", ",", "ed", ",", "bn_weight_init", "=", "0", ",", "resolution", "=", "resolution", ")", ",", "\n", ")", ",", "drop_path_rate", ")", ")", "\n", "", "", "if", "do", "[", "0", "]", "==", "'Subsample'", ":", "\n", "# ('Subsample',key_dim, num_heads, attn_ratio, mlp_ratio, stride)", "\n", "                ", "resolution_out", "=", "(", "resolution", "-", "1", ")", "//", "do", "[", "5", "]", "+", "1", "\n", "self", ".", "blocks", ".", "append", "(", "\n", "AttentionSubsample", "(", "\n", "*", "embed_dim", "[", "i", ":", "i", "+", "2", "]", ",", "key_dim", "=", "do", "[", "1", "]", ",", "num_heads", "=", "do", "[", "2", "]", ",", "\n", "attn_ratio", "=", "do", "[", "3", "]", ",", "act_layer", "=", "attn_act_layer", ",", "stride", "=", "do", "[", "5", "]", ",", "\n", "resolution", "=", "resolution", ",", "resolution_out", "=", "resolution_out", ",", "use_conv", "=", "use_conv", ")", ")", "\n", "resolution", "=", "resolution_out", "\n", "if", "do", "[", "4", "]", ">", "0", ":", "# mlp_ratio", "\n", "                    ", "h", "=", "int", "(", "embed_dim", "[", "i", "+", "1", "]", "*", "do", "[", "4", "]", ")", "\n", "self", ".", "blocks", ".", "append", "(", "\n", "Residual", "(", "nn", ".", "Sequential", "(", "\n", "ln_layer", "(", "embed_dim", "[", "i", "+", "1", "]", ",", "h", ",", "resolution", "=", "resolution", ")", ",", "\n", "act_layer", "(", ")", ",", "\n", "ln_layer", "(", "h", ",", "embed_dim", "[", "i", "+", "1", "]", ",", "bn_weight_init", "=", "0", ",", "resolution", "=", "resolution", ")", ",", "\n", ")", ",", "drop_path_rate", ")", ")", "\n", "", "", "", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "*", "self", ".", "blocks", ")", "\n", "\n", "# Classifier head", "\n", "self", ".", "head", "=", "NormLinear", "(", "embed_dim", "[", "-", "1", "]", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.Levit.no_weight_decay": [[491, 494], ["levit.Levit.state_dict().keys", "levit.Levit.state_dict"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "no_weight_decay", "(", "self", ")", ":", "\n", "        ", "return", "{", "x", "for", "x", "in", "self", ".", "state_dict", "(", ")", ".", "keys", "(", ")", "if", "'attention_biases'", "in", "x", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.Levit.group_matcher": [[495, 502], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "\n", "stem", "=", "r'^cls_token|pos_embed|patch_embed'", ",", "# stem and embed", "\n", "blocks", "=", "[", "(", "r'^blocks\\.(\\d+)'", ",", "None", ")", ",", "(", "r'^norm'", ",", "(", "99999", ",", ")", ")", "]", "\n", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.Levit.set_grad_checkpointing": [[503, 506], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.Levit.get_classifier": [[507, 510], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.Levit.reset_classifier": [[511, 516], ["levit.NormLinear", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "None", ",", "distillation", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "self", ".", "global_pool", "=", "global_pool", "\n", "", "self", ".", "head", "=", "NormLinear", "(", "self", ".", "embed_dim", "[", "-", "1", "]", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.Levit.forward_features": [[517, 526], ["levit.Levit.patch_embed", "levit.Levit.flatten().transpose", "helpers.checkpoint_seq", "levit.Levit.blocks", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "levit.Levit.flatten"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "patch_embed", "(", "x", ")", "\n", "if", "not", "self", ".", "use_conv", ":", "\n", "            ", "x", "=", "x", ".", "flatten", "(", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "blocks", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "blocks", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.Levit.forward_head": [[527, 531], ["levit.Levit.head", "x.mean", "x.mean"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "global_pool", "==", "'avg'", ":", "\n", "            ", "x", "=", "x", ".", "mean", "(", "dim", "=", "(", "-", "2", ",", "-", "1", ")", ")", "if", "self", ".", "use_conv", "else", "x", ".", "mean", "(", "dim", "=", "1", ")", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.Levit.forward": [[532, 536], ["levit.Levit.forward_features", "levit.Levit.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.LevitDistilled.__init__": [[539, 543], ["levit.Levit.__init__", "levit.NormLinear", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "head_dist", "=", "NormLinear", "(", "self", ".", "num_features", ",", "self", ".", "num_classes", ")", "if", "self", ".", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "distilled_training", "=", "False", "# must set this True to train w/ distillation token", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.LevitDistilled.get_classifier": [[544, 547], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", ",", "self", ".", "head_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.LevitDistilled.reset_classifier": [[548, 554], ["levit.NormLinear", "torch.Identity", "torch.Identity", "levit.NormLinear", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "None", ",", "distillation", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "self", ".", "global_pool", "=", "global_pool", "\n", "", "self", ".", "head", "=", "NormLinear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "head_dist", "=", "NormLinear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.LevitDistilled.set_distilled_training": [[555, 558], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_distilled_training", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "distilled_training", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.LevitDistilled.forward_head": [[559, 569], ["levit.LevitDistilled.head", "levit.LevitDistilled.head_dist", "x.mean", "x.mean", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "global_pool", "==", "'avg'", ":", "\n", "            ", "x", "=", "x", ".", "mean", "(", "dim", "=", "(", "-", "2", ",", "-", "1", ")", ")", "if", "self", ".", "use_conv", "else", "x", ".", "mean", "(", "dim", "=", "1", ")", "\n", "", "x", ",", "x_dist", "=", "self", ".", "head", "(", "x", ")", ",", "self", ".", "head_dist", "(", "x", ")", "\n", "if", "self", ".", "distilled_training", "and", "self", ".", "training", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "# only return separate classification predictions when training in distilled mode", "\n", "            ", "return", "x", ",", "x_dist", "\n", "", "else", ":", "\n", "# during standard train/finetune, inference average the classifier predictions", "\n", "            ", "return", "(", "x", "+", "x_dist", ")", "/", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit._cfg": [[41, 49], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", ".9", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'patch_embed.0.c'", ",", "'classifier'", ":", "(", "'head.l'", ",", "'head_dist.l'", ")", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.levit_128s": [[91, 95], ["levit.create_levit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.create_levit"], ["@", "register_model", "\n", "def", "levit_128s", "(", "pretrained", "=", "False", ",", "use_conv", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "create_levit", "(", "\n", "'levit_128s'", ",", "pretrained", "=", "pretrained", ",", "use_conv", "=", "use_conv", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.levit_128": [[97, 101], ["levit.create_levit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.create_levit"], ["", "@", "register_model", "\n", "def", "levit_128", "(", "pretrained", "=", "False", ",", "use_conv", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "create_levit", "(", "\n", "'levit_128'", ",", "pretrained", "=", "pretrained", ",", "use_conv", "=", "use_conv", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.levit_192": [[103, 107], ["levit.create_levit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.create_levit"], ["", "@", "register_model", "\n", "def", "levit_192", "(", "pretrained", "=", "False", ",", "use_conv", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "create_levit", "(", "\n", "'levit_192'", ",", "pretrained", "=", "pretrained", ",", "use_conv", "=", "use_conv", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.levit_256": [[109, 113], ["levit.create_levit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.create_levit"], ["", "@", "register_model", "\n", "def", "levit_256", "(", "pretrained", "=", "False", ",", "use_conv", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "create_levit", "(", "\n", "'levit_256'", ",", "pretrained", "=", "pretrained", ",", "use_conv", "=", "use_conv", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.levit_384": [[115, 119], ["levit.create_levit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.create_levit"], ["", "@", "register_model", "\n", "def", "levit_384", "(", "pretrained", "=", "False", ",", "use_conv", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "create_levit", "(", "\n", "'levit_384'", ",", "pretrained", "=", "pretrained", ",", "use_conv", "=", "use_conv", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.levit_256d": [[121, 125], ["levit.create_levit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.create_levit"], ["", "@", "register_model", "\n", "def", "levit_256d", "(", "pretrained", "=", "False", ",", "use_conv", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "create_levit", "(", "\n", "'levit_256d'", ",", "pretrained", "=", "pretrained", ",", "use_conv", "=", "use_conv", ",", "distilled", "=", "False", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.stem_b16": [[201, 210], ["torch.Sequential", "levit.ConvNorm", "activation", "levit.ConvNorm", "activation", "levit.ConvNorm", "activation", "levit.ConvNorm"], "function", ["None"], ["", "", "def", "stem_b16", "(", "in_chs", ",", "out_chs", ",", "activation", ",", "resolution", "=", "224", ")", ":", "\n", "    ", "return", "nn", ".", "Sequential", "(", "\n", "ConvNorm", "(", "in_chs", ",", "out_chs", "//", "8", ",", "3", ",", "2", ",", "1", ",", "resolution", "=", "resolution", ")", ",", "\n", "activation", "(", ")", ",", "\n", "ConvNorm", "(", "out_chs", "//", "8", ",", "out_chs", "//", "4", ",", "3", ",", "2", ",", "1", ",", "resolution", "=", "resolution", "//", "2", ")", ",", "\n", "activation", "(", ")", ",", "\n", "ConvNorm", "(", "out_chs", "//", "4", ",", "out_chs", "//", "2", ",", "3", ",", "2", ",", "1", ",", "resolution", "=", "resolution", "//", "4", ")", ",", "\n", "activation", "(", ")", ",", "\n", "ConvNorm", "(", "out_chs", "//", "2", ",", "out_chs", ",", "3", ",", "2", ",", "1", ",", "resolution", "=", "resolution", "//", "8", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.checkpoint_filter_fn": [[571, 580], ["model.state_dict", "state_dict.keys"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict"], ["", "", "", "def", "checkpoint_filter_fn", "(", "state_dict", ",", "model", ")", ":", "\n", "    ", "if", "'model'", "in", "state_dict", ":", "\n", "# For deit models", "\n", "        ", "state_dict", "=", "state_dict", "[", "'model'", "]", "\n", "", "D", "=", "model", ".", "state_dict", "(", ")", "\n", "for", "k", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "        ", "if", "k", "in", "D", "and", "D", "[", "k", "]", ".", "ndim", "==", "4", "and", "state_dict", "[", "k", "]", ".", "ndim", "==", "2", ":", "\n", "            ", "state_dict", "[", "k", "]", "=", "state_dict", "[", "k", "]", "[", ":", ",", ":", ",", "None", ",", "None", "]", "\n", "", "", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.levit.create_levit": [[582, 592], ["kwargs.get", "dict", "helpers.build_model_with_cfg", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "def", "create_levit", "(", "variant", ",", "pretrained", "=", "False", ",", "distilled", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "kwargs", ".", "get", "(", "'features_only'", ",", "None", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'features_only not implemented for Vision Transformer models.'", ")", "\n", "\n", "", "model_cfg", "=", "dict", "(", "**", "model_cfgs", "[", "variant", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "build_model_with_cfg", "(", "\n", "LevitDistilled", "if", "distilled", "else", "Levit", ",", "variant", ",", "pretrained", ",", "\n", "pretrained_filter_fn", "=", "checkpoint_filter_fn", ",", "\n", "**", "model_cfg", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilevit.MobileViTBlock.__init__": [[148, 200], ["torch.nn.Module.__init__", "byobnet.num_groups", "layers.conv_norm_act", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Sequential", "torch.nn.Sequential", "transformer_norm_layer", "layers.conv_norm_act", "layers.to_2tuple", "byobnet.LayerFn", "layers.make_divisible", "layers.conv_norm_act", "vision_transformer.Block", "range"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.num_groups", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible"], ["def", "__init__", "(", "\n", "self", ",", "\n", "in_chs", ":", "int", ",", "\n", "out_chs", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "kernel_size", ":", "int", "=", "3", ",", "\n", "stride", ":", "int", "=", "1", ",", "\n", "bottle_ratio", ":", "float", "=", "1.0", ",", "\n", "group_size", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "dilation", ":", "Tuple", "[", "int", ",", "int", "]", "=", "(", "1", ",", "1", ")", ",", "\n", "mlp_ratio", ":", "float", "=", "2.0", ",", "\n", "transformer_dim", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "transformer_depth", ":", "int", "=", "2", ",", "\n", "patch_size", ":", "int", "=", "8", ",", "\n", "num_heads", ":", "int", "=", "4", ",", "\n", "attn_drop", ":", "float", "=", "0.", ",", "\n", "drop", ":", "int", "=", "0.", ",", "\n", "no_fusion", ":", "bool", "=", "False", ",", "\n", "drop_path_rate", ":", "float", "=", "0.", ",", "\n", "layers", ":", "LayerFn", "=", "None", ",", "\n", "transformer_norm_layer", ":", "Callable", "=", "nn", ".", "LayerNorm", ",", "\n", "downsample", ":", "str", "=", "''", "\n", ")", ":", "\n", "        ", "super", "(", "MobileViTBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "layers", "=", "layers", "or", "LayerFn", "(", ")", "\n", "groups", "=", "num_groups", "(", "group_size", ",", "in_chs", ")", "\n", "out_chs", "=", "out_chs", "or", "in_chs", "\n", "transformer_dim", "=", "transformer_dim", "or", "make_divisible", "(", "bottle_ratio", "*", "in_chs", ")", "\n", "\n", "self", ".", "conv_kxk", "=", "layers", ".", "conv_norm_act", "(", "\n", "in_chs", ",", "in_chs", ",", "kernel_size", "=", "kernel_size", ",", "\n", "stride", "=", "stride", ",", "groups", "=", "groups", ",", "dilation", "=", "dilation", "[", "0", "]", ")", "\n", "self", ".", "conv_1x1", "=", "nn", ".", "Conv2d", "(", "in_chs", ",", "transformer_dim", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "transformer", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "TransformerBlock", "(", "\n", "transformer_dim", ",", "mlp_ratio", "=", "mlp_ratio", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "True", ",", "\n", "attn_drop", "=", "attn_drop", ",", "drop", "=", "drop", ",", "drop_path", "=", "drop_path_rate", ",", "\n", "act_layer", "=", "layers", ".", "act", ",", "norm_layer", "=", "transformer_norm_layer", ")", "\n", "for", "_", "in", "range", "(", "transformer_depth", ")", "\n", "]", ")", "\n", "self", ".", "norm", "=", "transformer_norm_layer", "(", "transformer_dim", ")", "\n", "\n", "self", ".", "conv_proj", "=", "layers", ".", "conv_norm_act", "(", "transformer_dim", ",", "out_chs", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "\n", "if", "no_fusion", ":", "\n", "            ", "self", ".", "conv_fusion", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv_fusion", "=", "layers", ".", "conv_norm_act", "(", "in_chs", "+", "out_chs", ",", "out_chs", ",", "kernel_size", "=", "kernel_size", ",", "stride", "=", "1", ")", "\n", "\n", "", "self", ".", "patch_size", "=", "to_2tuple", "(", "patch_size", ")", "\n", "self", ".", "patch_area", "=", "self", ".", "patch_size", "[", "0", "]", "*", "self", ".", "patch_size", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilevit.MobileViTBlock.forward": [[201, 242], ["mobilevit.MobileViTBlock.conv_kxk", "mobilevit.MobileViTBlock.conv_1x1", "mobilevit.MobileViTBlock.reshape().transpose", "mobilevit.MobileViTBlock.reshape().transpose().reshape", "mobilevit.MobileViTBlock.transformer", "mobilevit.MobileViTBlock.norm", "mobilevit.MobileViTBlock.contiguous().view", "mobilevit.MobileViTBlock.transpose().reshape", "mobilevit.MobileViTBlock.transpose().reshape", "mobilevit.MobileViTBlock.conv_proj", "torch.interpolate", "torch.interpolate", "torch.interpolate", "torch.interpolate", "mobilevit.MobileViTBlock.conv_fusion", "math.ceil", "math.ceil", "mobilevit.MobileViTBlock.reshape", "mobilevit.MobileViTBlock.reshape().transpose", "mobilevit.MobileViTBlock.contiguous", "mobilevit.MobileViTBlock.transpose", "mobilevit.MobileViTBlock.transpose", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "mobilevit.MobileViTBlock.reshape"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.interpolate", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.interpolate", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.interpolate", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.interpolate"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "shortcut", "=", "x", "\n", "\n", "# Local representation", "\n", "x", "=", "self", ".", "conv_kxk", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_1x1", "(", "x", ")", "\n", "\n", "# Unfold (feature map -> patches)", "\n", "patch_h", ",", "patch_w", "=", "self", ".", "patch_size", "\n", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "new_h", ",", "new_w", "=", "math", ".", "ceil", "(", "H", "/", "patch_h", ")", "*", "patch_h", ",", "math", ".", "ceil", "(", "W", "/", "patch_w", ")", "*", "patch_w", "\n", "num_patch_h", ",", "num_patch_w", "=", "new_h", "//", "patch_h", ",", "new_w", "//", "patch_w", "# n_h, n_w", "\n", "num_patches", "=", "num_patch_h", "*", "num_patch_w", "# N", "\n", "interpolate", "=", "False", "\n", "if", "new_h", "!=", "H", "or", "new_w", "!=", "W", ":", "\n", "# Note: Padding can be done, but then it needs to be handled in attention function.", "\n", "            ", "x", "=", "F", ".", "interpolate", "(", "x", ",", "size", "=", "(", "new_h", ",", "new_w", ")", ",", "mode", "=", "\"bilinear\"", ",", "align_corners", "=", "False", ")", "\n", "interpolate", "=", "True", "\n", "\n", "# [B, C, H, W] --> [B * C * n_h, n_w, p_h, p_w]", "\n", "", "x", "=", "x", ".", "reshape", "(", "B", "*", "C", "*", "num_patch_h", ",", "patch_h", ",", "num_patch_w", ",", "patch_w", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "# [B * C * n_h, n_w, p_h, p_w] --> [BP, N, C] where P = p_h * p_w and N = n_h * n_w", "\n", "x", "=", "x", ".", "reshape", "(", "B", ",", "C", ",", "num_patches", ",", "self", ".", "patch_area", ")", ".", "transpose", "(", "1", ",", "3", ")", ".", "reshape", "(", "B", "*", "self", ".", "patch_area", ",", "num_patches", ",", "-", "1", ")", "\n", "\n", "# Global representations", "\n", "x", "=", "self", ".", "transformer", "(", "x", ")", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "\n", "# Fold (patch -> feature map)", "\n", "# [B, P, N, C] --> [B*C*n_h, n_w, p_h, p_w]", "\n", "x", "=", "x", ".", "contiguous", "(", ")", ".", "view", "(", "B", ",", "self", ".", "patch_area", ",", "num_patches", ",", "-", "1", ")", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "3", ")", ".", "reshape", "(", "B", "*", "C", "*", "num_patch_h", ",", "num_patch_w", ",", "patch_h", ",", "patch_w", ")", "\n", "# [B*C*n_h, n_w, p_h, p_w] --> [B*C*n_h, p_h, n_w, p_w] --> [B, C, H, W]", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "C", ",", "num_patch_h", "*", "patch_h", ",", "num_patch_w", "*", "patch_w", ")", "\n", "if", "interpolate", ":", "\n", "            ", "x", "=", "F", ".", "interpolate", "(", "x", ",", "size", "=", "(", "H", ",", "W", ")", ",", "mode", "=", "\"bilinear\"", ",", "align_corners", "=", "False", ")", "\n", "\n", "", "x", "=", "self", ".", "conv_proj", "(", "x", ")", "\n", "if", "self", ".", "conv_fusion", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "conv_fusion", "(", "torch", ".", "cat", "(", "(", "shortcut", ",", "x", ")", ",", "dim", "=", "1", ")", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilevit._cfg": [[32, 40], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "256", ",", "256", ")", ",", "'pool_size'", ":", "(", "8", ",", "8", ")", ",", "\n", "'crop_pct'", ":", "0.9", ",", "'interpolation'", ":", "'bicubic'", ",", "\n", "'mean'", ":", "(", "0", ",", "0", ",", "0", ")", ",", "'std'", ":", "(", "1", ",", "1", ",", "1", ")", ",", "\n", "'first_conv'", ":", "'stem.conv'", ",", "'classifier'", ":", "'head.fc'", ",", "\n", "'fixed_input_size'", ":", "False", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilevit._inverted_residual_block": [[54, 59], ["byobnet.ByoBlockCfg", "dict"], "function", ["None"], ["def", "_inverted_residual_block", "(", "d", ",", "c", ",", "s", ",", "br", "=", "4.0", ")", ":", "\n", "# inverted residual is a bottleneck block with bottle_ratio > 1 applied to in_chs, linear output, gs=1 (depthwise)", "\n", "    ", "return", "ByoBlockCfg", "(", "\n", "type", "=", "'bottle'", ",", "d", "=", "d", ",", "c", "=", "c", ",", "s", "=", "s", ",", "gs", "=", "1", ",", "br", "=", "br", ",", "\n", "block_kwargs", "=", "dict", "(", "bottle_in", "=", "True", ",", "linear_out", "=", "True", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilevit._mobilevit_block": [[61, 71], ["mobilevit._inverted_residual_block", "byobnet.ByoBlockCfg", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilevit._inverted_residual_block"], ["", "def", "_mobilevit_block", "(", "d", ",", "c", ",", "s", ",", "transformer_dim", ",", "transformer_depth", ",", "patch_size", "=", "4", ",", "br", "=", "4.0", ")", ":", "\n", "# inverted residual + mobilevit blocks as per MobileViT network", "\n", "    ", "return", "(", "\n", "_inverted_residual_block", "(", "d", "=", "d", ",", "c", "=", "c", ",", "s", "=", "s", ",", "br", "=", "br", ")", ",", "\n", "ByoBlockCfg", "(", "\n", "type", "=", "'mobilevit'", ",", "d", "=", "1", ",", "c", "=", "c", ",", "s", "=", "1", ",", "\n", "block_kwargs", "=", "dict", "(", "\n", "transformer_dim", "=", "transformer_dim", ",", "\n", "transformer_depth", "=", "transformer_depth", ",", "\n", "patch_size", "=", "patch_size", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilevit._create_mobilevit": [[247, 253], ["helpers.build_model_with_cfg", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["def", "_create_mobilevit", "(", "variant", ",", "cfg_variant", "=", "None", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "\n", "ByobNet", ",", "variant", ",", "pretrained", ",", "\n", "model_cfg", "=", "model_cfgs", "[", "variant", "]", "if", "not", "cfg_variant", "else", "model_cfgs", "[", "cfg_variant", "]", ",", "\n", "feature_cfg", "=", "dict", "(", "flatten_sequential", "=", "True", ")", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilevit.mobilevit_xxs": [[255, 258], ["mobilevit._create_mobilevit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilevit._create_mobilevit"], ["", "@", "register_model", "\n", "def", "mobilevit_xxs", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_mobilevit", "(", "'mobilevit_xxs'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilevit.mobilevit_xs": [[260, 263], ["mobilevit._create_mobilevit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilevit._create_mobilevit"], ["", "@", "register_model", "\n", "def", "mobilevit_xs", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_mobilevit", "(", "'mobilevit_xs'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilevit.mobilevit_s": [[265, 268], ["mobilevit._create_mobilevit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilevit._create_mobilevit"], ["", "@", "register_model", "\n", "def", "mobilevit_s", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_mobilevit", "(", "'mobilevit_s'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilevit.semobilevit_s": [[270, 273], ["mobilevit._create_mobilevit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.mobilevit._create_mobilevit"], ["", "@", "register_model", "\n", "def", "semobilevit_s", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_mobilevit", "(", "'semobilevit_s'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.Attention.__init__": [[98, 133], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Parameter", "torch.Parameter", "torch.Parameter", "beit.Attention.register_buffer", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "beit.Attention.register_buffer", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "beit.gen_relative_position_index"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.gen_relative_position_index"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "num_heads", "=", "8", ",", "qkv_bias", "=", "False", ",", "attn_drop", "=", "0.", ",", "\n", "proj_drop", "=", "0.", ",", "window_size", "=", "None", ",", "attn_head_dim", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "head_dim", "=", "dim", "//", "num_heads", "\n", "if", "attn_head_dim", "is", "not", "None", ":", "\n", "            ", "head_dim", "=", "attn_head_dim", "\n", "", "all_head_dim", "=", "head_dim", "*", "self", ".", "num_heads", "\n", "self", ".", "scale", "=", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "dim", ",", "all_head_dim", "*", "3", ",", "bias", "=", "False", ")", "\n", "if", "qkv_bias", ":", "\n", "            ", "self", ".", "q_bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "all_head_dim", ")", ")", "\n", "self", ".", "register_buffer", "(", "'k_bias'", ",", "torch", ".", "zeros", "(", "all_head_dim", ")", ",", "persistent", "=", "False", ")", "\n", "self", ".", "v_bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "all_head_dim", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "q_bias", "=", "None", "\n", "self", ".", "k_bias", "=", "None", "\n", "self", ".", "v_bias", "=", "None", "\n", "\n", "", "if", "window_size", ":", "\n", "            ", "self", ".", "window_size", "=", "window_size", "\n", "self", ".", "num_relative_distance", "=", "(", "2", "*", "window_size", "[", "0", "]", "-", "1", ")", "*", "(", "2", "*", "window_size", "[", "1", "]", "-", "1", ")", "+", "3", "\n", "self", ".", "relative_position_bias_table", "=", "nn", ".", "Parameter", "(", "\n", "torch", ".", "zeros", "(", "self", ".", "num_relative_distance", ",", "num_heads", ")", ")", "# 2*Wh-1 * 2*Ww-1, nH", "\n", "self", ".", "register_buffer", "(", "\"relative_position_index\"", ",", "gen_relative_position_index", "(", "window_size", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "window_size", "=", "None", "\n", "self", ".", "relative_position_bias_table", "=", "None", "\n", "self", ".", "relative_position_index", "=", "None", "\n", "\n", "", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "all_head_dim", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.Attention._get_rel_pos_bias": [[134, 141], ["beit.Attention.relative_position_bias_table[].view", "relative_position_bias.permute().contiguous.permute().contiguous.permute().contiguous", "relative_position_bias.permute().contiguous.permute().contiguous.unsqueeze", "relative_position_bias.permute().contiguous.permute().contiguous.permute", "beit.Attention.relative_position_index.view"], "methods", ["None"], ["", "def", "_get_rel_pos_bias", "(", "self", ")", ":", "\n", "        ", "relative_position_bias", "=", "self", ".", "relative_position_bias_table", "[", "\n", "self", ".", "relative_position_index", ".", "view", "(", "-", "1", ")", "]", ".", "view", "(", "\n", "self", ".", "window_size", "[", "0", "]", "*", "self", ".", "window_size", "[", "1", "]", "+", "1", ",", "\n", "self", ".", "window_size", "[", "0", "]", "*", "self", ".", "window_size", "[", "1", "]", "+", "1", ",", "-", "1", ")", "# Wh*Ww,Wh*Ww,nH", "\n", "relative_position_bias", "=", "relative_position_bias", ".", "permute", "(", "2", ",", "0", ",", "1", ")", ".", "contiguous", "(", ")", "# nH, Wh*Ww, Wh*Ww", "\n", "return", "relative_position_bias", ".", "unsqueeze", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.Attention.forward": [[142, 165], ["torch.linear", "torch.linear", "torch.linear", "qkv.reshape().permute.reshape().permute.reshape().permute", "qkv.reshape().permute.reshape().permute.unbind", "beit.Attention.softmax", "beit.Attention.attn_drop", "beit.Attention.proj", "beit.Attention.proj_drop", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "k.transpose", "qkv.reshape().permute.reshape().permute.reshape", "beit.Attention._get_rel_pos_bias"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.WindowAttention._get_rel_pos_bias"], ["", "def", "forward", "(", "self", ",", "x", ",", "shared_rel_pos_bias", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "\n", "qkv_bias", "=", "torch", ".", "cat", "(", "(", "self", ".", "q_bias", ",", "self", ".", "k_bias", ",", "self", ".", "v_bias", ")", ")", "if", "self", ".", "q_bias", "is", "not", "None", "else", "None", "\n", "qkv", "=", "F", ".", "linear", "(", "input", "=", "x", ",", "weight", "=", "self", ".", "qkv", ".", "weight", ",", "bias", "=", "qkv_bias", ")", "\n", "qkv", "=", "qkv", ".", "reshape", "(", "B", ",", "N", ",", "3", ",", "self", ".", "num_heads", ",", "-", "1", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "q", ",", "k", ",", "v", "=", "qkv", ".", "unbind", "(", "0", ")", "# make torchscript happy (cannot use tensor as tuple)", "\n", "\n", "q", "=", "q", "*", "self", ".", "scale", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "\n", "\n", "if", "self", ".", "relative_position_bias_table", "is", "not", "None", ":", "\n", "            ", "attn", "=", "attn", "+", "self", ".", "_get_rel_pos_bias", "(", ")", "\n", "", "if", "shared_rel_pos_bias", "is", "not", "None", ":", "\n", "            ", "attn", "=", "attn", "+", "shared_rel_pos_bias", "\n", "\n", "", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "N", ",", "-", "1", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.Block.__init__": [[169, 189], ["torch.Module.__init__", "norm_layer", "beit.Attention", "norm_layer", "int", "layers.Mlp", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "num_heads", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "\n", "drop_path", "=", "0.", ",", "init_values", "=", "None", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ",", "\n", "window_size", "=", "None", ",", "attn_head_dim", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "attn", "=", "Attention", "(", "\n", "dim", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ",", "\n", "window_size", "=", "window_size", ",", "attn_head_dim", "=", "attn_head_dim", ")", "\n", "# NOTE: drop path for stochastic depth, we shall see if this is better than dropout here", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "mlp_hidden_dim", "=", "int", "(", "dim", "*", "mlp_ratio", ")", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "mlp_hidden_dim", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "\n", "if", "init_values", ":", "\n", "            ", "self", ".", "gamma_1", "=", "nn", ".", "Parameter", "(", "init_values", "*", "torch", ".", "ones", "(", "dim", ")", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "gamma_2", "=", "nn", ".", "Parameter", "(", "init_values", "*", "torch", ".", "ones", "(", "dim", ")", ",", "requires_grad", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "gamma_1", ",", "self", ".", "gamma_2", "=", "None", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.Block.forward": [[190, 198], ["beit.Block.drop_path", "beit.Block.drop_path", "beit.Block.drop_path", "beit.Block.drop_path", "beit.Block.attn", "beit.Block.mlp", "beit.Block.norm1", "beit.Block.norm2", "beit.Block.attn", "beit.Block.mlp", "beit.Block.norm1", "beit.Block.norm2"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "shared_rel_pos_bias", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "gamma_1", "is", "None", ":", "\n", "            ", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "attn", "(", "self", ".", "norm1", "(", "x", ")", ",", "shared_rel_pos_bias", "=", "shared_rel_pos_bias", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "gamma_1", "*", "self", ".", "attn", "(", "self", ".", "norm1", "(", "x", ")", ",", "shared_rel_pos_bias", "=", "shared_rel_pos_bias", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "gamma_2", "*", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.RelativePositionBias.__init__": [[202, 210], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "beit.RelativePositionBias.register_buffer", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "beit.gen_relative_position_index"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.gen_relative_position_index"], ["    ", "def", "__init__", "(", "self", ",", "window_size", ",", "num_heads", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "window_size", "=", "window_size", "\n", "self", ".", "window_area", "=", "window_size", "[", "0", "]", "*", "window_size", "[", "1", "]", "\n", "num_relative_distance", "=", "(", "2", "*", "window_size", "[", "0", "]", "-", "1", ")", "*", "(", "2", "*", "window_size", "[", "1", "]", "-", "1", ")", "+", "3", "\n", "self", ".", "relative_position_bias_table", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "num_relative_distance", ",", "num_heads", ")", ")", "\n", "# trunc_normal_(self.relative_position_bias_table, std=.02)", "\n", "self", ".", "register_buffer", "(", "\"relative_position_index\"", ",", "gen_relative_position_index", "(", "window_size", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.RelativePositionBias.forward": [[211, 215], ["beit.RelativePositionBias.relative_position_bias_table[].view", "beit.RelativePositionBias.permute().contiguous", "beit.RelativePositionBias.permute", "beit.RelativePositionBias.relative_position_index.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ")", ":", "\n", "        ", "relative_position_bias", "=", "self", ".", "relative_position_bias_table", "[", "self", ".", "relative_position_index", ".", "view", "(", "-", "1", ")", "]", ".", "view", "(", "\n", "self", ".", "window_area", "+", "1", ",", "self", ".", "window_area", "+", "1", ",", "-", "1", ")", "# Wh*Ww,Wh*Ww,nH", "\n", "return", "relative_position_bias", ".", "permute", "(", "2", ",", "0", ",", "1", ")", ".", "contiguous", "(", ")", "# nH, Wh*Ww, Wh*Ww", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.Beit.__init__": [[221, 269], ["functools.partial", "torch.Module.__init__", "layers.PatchEmbed", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "beit.Beit.apply", "layers.trunc_normal_", "beit.Beit.fix_init_weight", "isinstance", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Parameter", "torch.Parameter", "torch.Parameter", "beit.RelativePositionBias", "x.item", "torch.Identity", "torch.Identity", "torch.Identity", "norm_layer", "norm_layer", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "layers.trunc_normal_", "layers.trunc_normal_", "beit.Beit.head.weight.data.mul_", "beit.Beit.head.bias.data.mul_", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "beit.Block", "range"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.Beit.fix_init_weight", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["def", "__init__", "(", "\n", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "global_pool", "=", "'avg'", ",", "\n", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "True", ",", "drop_rate", "=", "0.", ",", "\n", "attn_drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ",", "norm_layer", "=", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", ",", "\n", "init_values", "=", "None", ",", "use_abs_pos_emb", "=", "True", ",", "use_rel_pos_bias", "=", "False", ",", "use_shared_rel_pos_bias", "=", "False", ",", "\n", "head_init_scale", "=", "0.001", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "self", ".", "num_features", "=", "self", ".", "embed_dim", "=", "embed_dim", "# num_features for consistency with other models", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "self", ".", "patch_embed", "=", "PatchEmbed", "(", "\n", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", ",", "in_chans", "=", "in_chans", ",", "embed_dim", "=", "embed_dim", ")", "\n", "num_patches", "=", "self", ".", "patch_embed", ".", "num_patches", "\n", "\n", "self", ".", "cls_token", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "# self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))", "\n", "self", ".", "pos_embed", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "num_patches", "+", "1", ",", "embed_dim", ")", ")", "if", "use_abs_pos_emb", "else", "None", "\n", "self", ".", "pos_drop", "=", "nn", ".", "Dropout", "(", "p", "=", "drop_rate", ")", "\n", "\n", "if", "use_shared_rel_pos_bias", ":", "\n", "            ", "self", ".", "rel_pos_bias", "=", "RelativePositionBias", "(", "window_size", "=", "self", ".", "patch_embed", ".", "grid_size", ",", "num_heads", "=", "num_heads", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "rel_pos_bias", "=", "None", "\n", "\n", "", "dpr", "=", "[", "x", ".", "item", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "depth", ")", "]", "# stochastic depth decay rule", "\n", "self", ".", "blocks", "=", "nn", ".", "ModuleList", "(", "[", "\n", "Block", "(", "\n", "dim", "=", "embed_dim", ",", "num_heads", "=", "num_heads", ",", "mlp_ratio", "=", "mlp_ratio", ",", "qkv_bias", "=", "qkv_bias", ",", "\n", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "dpr", "[", "i", "]", ",", "norm_layer", "=", "norm_layer", ",", "\n", "init_values", "=", "init_values", ",", "window_size", "=", "self", ".", "patch_embed", ".", "grid_size", "if", "use_rel_pos_bias", "else", "None", ")", "\n", "for", "i", "in", "range", "(", "depth", ")", "]", ")", "\n", "use_fc_norm", "=", "self", ".", "global_pool", "==", "'avg'", "\n", "self", ".", "norm", "=", "nn", ".", "Identity", "(", ")", "if", "use_fc_norm", "else", "norm_layer", "(", "embed_dim", ")", "\n", "self", ".", "fc_norm", "=", "norm_layer", "(", "embed_dim", ")", "if", "use_fc_norm", "else", "None", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "apply", "(", "self", ".", "_init_weights", ")", "\n", "if", "self", ".", "pos_embed", "is", "not", "None", ":", "\n", "            ", "trunc_normal_", "(", "self", ".", "pos_embed", ",", "std", "=", ".02", ")", "\n", "", "trunc_normal_", "(", "self", ".", "cls_token", ",", "std", "=", ".02", ")", "\n", "# trunc_normal_(self.mask_token, std=.02)", "\n", "self", ".", "fix_init_weight", "(", ")", "\n", "if", "isinstance", "(", "self", ".", "head", ",", "nn", ".", "Linear", ")", ":", "\n", "            ", "trunc_normal_", "(", "self", ".", "head", ".", "weight", ",", "std", "=", ".02", ")", "\n", "self", ".", "head", ".", "weight", ".", "data", ".", "mul_", "(", "head_init_scale", ")", "\n", "self", ".", "head", ".", "bias", ".", "data", ".", "mul_", "(", "head_init_scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.Beit.fix_init_weight": [[270, 277], ["enumerate", "param.div_", "beit.Beit.fix_init_weight.rescale"], "methods", ["None"], ["", "", "def", "fix_init_weight", "(", "self", ")", ":", "\n", "        ", "def", "rescale", "(", "param", ",", "layer_id", ")", ":", "\n", "            ", "param", ".", "div_", "(", "math", ".", "sqrt", "(", "2.0", "*", "layer_id", ")", ")", "\n", "\n", "", "for", "layer_id", ",", "layer", "in", "enumerate", "(", "self", ".", "blocks", ")", ":", "\n", "            ", "rescale", "(", "layer", ".", "attn", ".", "proj", ".", "weight", ".", "data", ",", "layer_id", "+", "1", ")", "\n", "rescale", "(", "layer", ".", "mlp", ".", "fc2", ".", "weight", ".", "data", ",", "layer_id", "+", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.Beit._init_weights": [[278, 286], ["isinstance", "layers.trunc_normal_", "isinstance", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["", "", "def", "_init_weights", "(", "self", ",", "m", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "            ", "trunc_normal_", "(", "m", ".", "weight", ",", "std", "=", ".02", ")", "\n", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.Beit.no_weight_decay": [[287, 294], ["beit.Beit.named_parameters", "nwd.add"], "methods", ["None"], ["", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "no_weight_decay", "(", "self", ")", ":", "\n", "        ", "nwd", "=", "{", "'pos_embed'", ",", "'cls_token'", "}", "\n", "for", "n", ",", "_", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "'relative_position_bias_table'", "in", "n", ":", "\n", "                ", "nwd", ".", "add", "(", "n", ")", "\n", "", "", "return", "nwd", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.Beit.set_grad_checkpointing": [[295, 298], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.Beit.group_matcher": [[299, 306], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "\n", "stem", "=", "r'^cls_token|pos_embed|patch_embed|rel_pos_bias'", ",", "# stem and embed", "\n", "blocks", "=", "[", "(", "r'^blocks\\.(\\d+)'", ",", "None", ")", ",", "(", "r'^norm'", ",", "(", "99999", ",", ")", ")", "]", ",", "\n", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.Beit.get_classifier": [[307, 310], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.Beit.reset_classifier": [[311, 316], ["torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "self", ".", "global_pool", "=", "global_pool", "\n", "", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.Beit.forward_features": [[317, 332], ["beit.Beit.patch_embed", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "beit.Beit.pos_drop", "beit.Beit.norm", "beit.Beit.rel_pos_bias", "beit.Beit.cls_token.expand", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "blk", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "patch_embed", "(", "x", ")", "\n", "x", "=", "torch", ".", "cat", "(", "(", "self", ".", "cls_token", ".", "expand", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ",", "-", "1", ")", ",", "x", ")", ",", "dim", "=", "1", ")", "\n", "if", "self", ".", "pos_embed", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", "+", "self", ".", "pos_embed", "\n", "", "x", "=", "self", ".", "pos_drop", "(", "x", ")", "\n", "\n", "rel_pos_bias", "=", "self", ".", "rel_pos_bias", "(", ")", "if", "self", ".", "rel_pos_bias", "is", "not", "None", "else", "None", "\n", "for", "blk", "in", "self", ".", "blocks", ":", "\n", "            ", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "                ", "x", "=", "checkpoint", "(", "blk", ",", "x", ",", "shared_rel_pos_bias", "=", "rel_pos_bias", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "blk", "(", "x", ",", "shared_rel_pos_bias", "=", "rel_pos_bias", ")", "\n", "", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.Beit.forward_head": [[333, 340], ["x[].mean", "beit.Beit.fc_norm", "beit.Beit.head"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "fc_norm", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", "[", ":", ",", "1", ":", "]", ".", "mean", "(", "dim", "=", "1", ")", "\n", "x", "=", "self", ".", "fc_norm", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", "[", ":", ",", "0", "]", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.Beit.forward": [[341, 345], ["beit.Beit.forward_features", "beit.Beit.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit._cfg": [[36, 44], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", ".9", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "(", "0.5", ",", "0.5", ",", "0.5", ")", ",", "'std'", ":", "(", "0.5", ",", "0.5", ",", "0.5", ")", ",", "\n", "'first_conv'", ":", "'patch_embed.proj'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.gen_relative_position_index": [[75, 95], ["torch.stack", "torch.stack", "torch.stack", "torch.flatten", "torch.flatten", "torch.flatten", "relative_coords.permute().contiguous.permute().contiguous", "torch.zeros", "torch.zeros", "torch.zeros", "relative_coords.permute().contiguous.sum", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "relative_coords.permute().contiguous.permute", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "function", ["None"], ["def", "gen_relative_position_index", "(", "window_size", ":", "Tuple", "[", "int", ",", "int", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "num_relative_distance", "=", "(", "2", "*", "window_size", "[", "0", "]", "-", "1", ")", "*", "(", "2", "*", "window_size", "[", "1", "]", "-", "1", ")", "+", "3", "\n", "# cls to token & token 2 cls & cls to cls", "\n", "# get pair-wise relative position index for each token inside the window", "\n", "window_area", "=", "window_size", "[", "0", "]", "*", "window_size", "[", "1", "]", "\n", "coords", "=", "torch", ".", "stack", "(", "torch", ".", "meshgrid", "(", "\n", "[", "torch", ".", "arange", "(", "window_size", "[", "0", "]", ")", ",", "\n", "torch", ".", "arange", "(", "window_size", "[", "1", "]", ")", "]", ")", ")", "# 2, Wh, Ww", "\n", "coords_flatten", "=", "torch", ".", "flatten", "(", "coords", ",", "1", ")", "# 2, Wh*Ww", "\n", "relative_coords", "=", "coords_flatten", "[", ":", ",", ":", ",", "None", "]", "-", "coords_flatten", "[", ":", ",", "None", ",", ":", "]", "# 2, Wh*Ww, Wh*Ww", "\n", "relative_coords", "=", "relative_coords", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ".", "contiguous", "(", ")", "# Wh*Ww, Wh*Ww, 2", "\n", "relative_coords", "[", ":", ",", ":", ",", "0", "]", "+=", "window_size", "[", "0", "]", "-", "1", "# shift to start from 0", "\n", "relative_coords", "[", ":", ",", ":", ",", "1", "]", "+=", "window_size", "[", "1", "]", "-", "1", "\n", "relative_coords", "[", ":", ",", ":", ",", "0", "]", "*=", "2", "*", "window_size", "[", "1", "]", "-", "1", "\n", "relative_position_index", "=", "torch", ".", "zeros", "(", "size", "=", "(", "window_area", "+", "1", ",", ")", "*", "2", ",", "dtype", "=", "relative_coords", ".", "dtype", ")", "\n", "relative_position_index", "[", "1", ":", ",", "1", ":", "]", "=", "relative_coords", ".", "sum", "(", "-", "1", ")", "# Wh*Ww, Wh*Ww", "\n", "relative_position_index", "[", "0", ",", "0", ":", "]", "=", "num_relative_distance", "-", "3", "\n", "relative_position_index", "[", "0", ":", ",", "0", "]", "=", "num_relative_distance", "-", "2", "\n", "relative_position_index", "[", "0", ",", "0", "]", "=", "num_relative_distance", "-", "1", "\n", "return", "relative_position_index", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit._create_beit": [[347, 357], ["kwargs.get", "helpers.build_model_with_cfg", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_beit", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "kwargs", ".", "get", "(", "'features_only'", ",", "None", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'features_only not implemented for Beit models.'", ")", "\n", "\n", "", "model", "=", "build_model_with_cfg", "(", "\n", "Beit", ",", "variant", ",", "pretrained", ",", "\n", "# FIXME an updated filter fn needed to interpolate rel pos emb if fine tuning to diff model sizes", "\n", "pretrained_filter_fn", "=", "checkpoint_filter_fn", ",", "\n", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.beit_base_patch16_224": [[359, 366], ["dict", "beit._create_beit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit._create_beit"], ["", "@", "register_model", "\n", "def", "beit_base_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "mlp_ratio", "=", "4", ",", "\n", "use_abs_pos_emb", "=", "False", ",", "use_rel_pos_bias", "=", "True", ",", "init_values", "=", "0.1", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_beit", "(", "'beit_base_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.beit_base_patch16_384": [[368, 375], ["dict", "beit._create_beit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit._create_beit"], ["", "@", "register_model", "\n", "def", "beit_base_patch16_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "img_size", "=", "384", ",", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "mlp_ratio", "=", "4", ",", "\n", "use_abs_pos_emb", "=", "False", ",", "use_rel_pos_bias", "=", "True", ",", "init_values", "=", "0.1", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_beit", "(", "'beit_base_patch16_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.beit_base_patch16_224_in22k": [[377, 384], ["dict", "beit._create_beit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit._create_beit"], ["", "@", "register_model", "\n", "def", "beit_base_patch16_224_in22k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "mlp_ratio", "=", "4", ",", "\n", "use_abs_pos_emb", "=", "False", ",", "use_rel_pos_bias", "=", "True", ",", "init_values", "=", "0.1", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_beit", "(", "'beit_base_patch16_224_in22k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.beit_large_patch16_224": [[386, 393], ["dict", "beit._create_beit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit._create_beit"], ["", "@", "register_model", "\n", "def", "beit_large_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "1024", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "mlp_ratio", "=", "4", ",", "qkv_bias", "=", "True", ",", "\n", "use_abs_pos_emb", "=", "False", ",", "use_rel_pos_bias", "=", "True", ",", "init_values", "=", "1e-5", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_beit", "(", "'beit_large_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.beit_large_patch16_384": [[395, 402], ["dict", "beit._create_beit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit._create_beit"], ["", "@", "register_model", "\n", "def", "beit_large_patch16_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "img_size", "=", "384", ",", "patch_size", "=", "16", ",", "embed_dim", "=", "1024", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "mlp_ratio", "=", "4", ",", "qkv_bias", "=", "True", ",", "\n", "use_abs_pos_emb", "=", "False", ",", "use_rel_pos_bias", "=", "True", ",", "init_values", "=", "1e-5", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_beit", "(", "'beit_large_patch16_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.beit_large_patch16_512": [[404, 411], ["dict", "beit._create_beit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit._create_beit"], ["", "@", "register_model", "\n", "def", "beit_large_patch16_512", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "img_size", "=", "512", ",", "patch_size", "=", "16", ",", "embed_dim", "=", "1024", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "mlp_ratio", "=", "4", ",", "qkv_bias", "=", "True", ",", "\n", "use_abs_pos_emb", "=", "False", ",", "use_rel_pos_bias", "=", "True", ",", "init_values", "=", "1e-5", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_beit", "(", "'beit_large_patch16_512'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit.beit_large_patch16_224_in22k": [[413, 420], ["dict", "beit._create_beit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.beit._create_beit"], ["", "@", "register_model", "\n", "def", "beit_large_patch16_224_in22k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "1024", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "mlp_ratio", "=", "4", ",", "qkv_bias", "=", "True", ",", "\n", "use_abs_pos_emb", "=", "False", ",", "use_rel_pos_bias", "=", "True", ",", "init_values", "=", "1e-5", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_beit", "(", "'beit_large_patch16_224_in22k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.WindowAttention.__init__": [[160, 185], ["torch.Module.__init__", "layers.to_2tuple", "torch.Parameter", "torch.Parameter", "swin_transformer.WindowAttention.register_buffer", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "layers.trunc_normal_", "torch.Softmax", "torch.Softmax", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "swin_transformer.get_relative_position_index"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.get_relative_position_index"], ["def", "__init__", "(", "self", ",", "dim", ",", "num_heads", ",", "head_dim", "=", "None", ",", "window_size", "=", "7", ",", "qkv_bias", "=", "True", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "window_size", "=", "to_2tuple", "(", "window_size", ")", "# Wh, Ww", "\n", "win_h", ",", "win_w", "=", "self", ".", "window_size", "\n", "self", ".", "window_area", "=", "win_h", "*", "win_w", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "head_dim", "=", "head_dim", "or", "dim", "//", "num_heads", "\n", "attn_dim", "=", "head_dim", "*", "num_heads", "\n", "self", ".", "scale", "=", "head_dim", "**", "-", "0.5", "\n", "\n", "# define a parameter table of relative position bias, shape: 2*Wh-1 * 2*Ww-1, nH", "\n", "self", ".", "relative_position_bias_table", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "(", "2", "*", "win_h", "-", "1", ")", "*", "(", "2", "*", "win_w", "-", "1", ")", ",", "num_heads", ")", ")", "\n", "\n", "# get pair-wise relative position index for each token inside the window", "\n", "self", ".", "register_buffer", "(", "\"relative_position_index\"", ",", "get_relative_position_index", "(", "win_h", ",", "win_w", ")", ")", "\n", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "dim", ",", "attn_dim", "*", "3", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "attn_dim", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "\n", "trunc_normal_", "(", "self", ".", "relative_position_bias_table", ",", "std", "=", ".02", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.WindowAttention._get_rel_pos_bias": [[186, 191], ["swin_transformer.WindowAttention.relative_position_bias_table[].view", "relative_position_bias.permute().contiguous.permute().contiguous.permute().contiguous", "relative_position_bias.permute().contiguous.permute().contiguous.unsqueeze", "relative_position_bias.permute().contiguous.permute().contiguous.permute", "swin_transformer.WindowAttention.relative_position_index.view"], "methods", ["None"], ["", "def", "_get_rel_pos_bias", "(", "self", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "relative_position_bias", "=", "self", ".", "relative_position_bias_table", "[", "\n", "self", ".", "relative_position_index", ".", "view", "(", "-", "1", ")", "]", ".", "view", "(", "self", ".", "window_area", ",", "self", ".", "window_area", ",", "-", "1", ")", "# Wh*Ww,Wh*Ww,nH", "\n", "relative_position_bias", "=", "relative_position_bias", ".", "permute", "(", "2", ",", "0", ",", "1", ")", ".", "contiguous", "(", ")", "# nH, Wh*Ww, Wh*Ww", "\n", "return", "relative_position_bias", ".", "unsqueeze", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.WindowAttention.forward": [[192, 220], ["swin_transformer.WindowAttention.qkv().reshape().permute", "swin_transformer.WindowAttention.unbind", "swin_transformer.WindowAttention.attn_drop", "swin_transformer.WindowAttention.proj", "swin_transformer.WindowAttention.proj_drop", "k.transpose", "swin_transformer.WindowAttention._get_rel_pos_bias", "swin_transformer.WindowAttention.view", "swin_transformer.WindowAttention.softmax", "swin_transformer.WindowAttention.softmax", "swin_transformer.WindowAttention.qkv().reshape", "swin_transformer.WindowAttention.view", "mask.unsqueeze().unsqueeze", "swin_transformer.WindowAttention.qkv", "mask.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.WindowAttention._get_rel_pos_bias"], ["", "def", "forward", "(", "self", ",", "x", ",", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"", "\n", "B_", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "qkv", "=", "self", ".", "qkv", "(", "x", ")", ".", "reshape", "(", "B_", ",", "N", ",", "3", ",", "self", ".", "num_heads", ",", "-", "1", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "q", ",", "k", ",", "v", "=", "qkv", ".", "unbind", "(", "0", ")", "# make torchscript happy (cannot use tensor as tuple)", "\n", "\n", "q", "=", "q", "*", "self", ".", "scale", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "\n", "attn", "=", "attn", "+", "self", ".", "_get_rel_pos_bias", "(", ")", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "num_win", "=", "mask", ".", "shape", "[", "0", "]", "\n", "attn", "=", "attn", ".", "view", "(", "B_", "//", "num_win", ",", "num_win", ",", "self", ".", "num_heads", ",", "N", ",", "N", ")", "+", "mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "0", ")", "\n", "attn", "=", "attn", ".", "view", "(", "-", "1", ",", "self", ".", "num_heads", ",", "N", ",", "N", ")", "\n", "attn", "=", "self", ".", "softmax", "(", "attn", ")", "\n", "", "else", ":", "\n", "            ", "attn", "=", "self", ".", "softmax", "(", "attn", ")", "\n", "\n", "", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B_", ",", "N", ",", "-", "1", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.SwinTransformerBlock.__init__": [[241, 289], ["torch.Module.__init__", "norm_layer", "swin_transformer.WindowAttention", "norm_layer", "layers.Mlp", "swin_transformer.SwinTransformerBlock.register_buffer", "min", "min", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "swin_transformer.window_partition", "mask_windows.view.view.view", "attn_mask.masked_fill().masked_fill.masked_fill().masked_fill.masked_fill().masked_fill", "layers.to_2tuple", "int", "slice", "slice", "slice", "mask_windows.view.view.unsqueeze", "mask_windows.view.view.unsqueeze", "float", "slice", "slice", "slice", "attn_mask.masked_fill().masked_fill.masked_fill().masked_fill.masked_fill", "float"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.window_partition"], ["def", "__init__", "(", "\n", "self", ",", "dim", ",", "input_resolution", ",", "num_heads", "=", "4", ",", "head_dim", "=", "None", ",", "window_size", "=", "7", ",", "shift_size", "=", "0", ",", "\n", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "True", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "drop_path", "=", "0.", ",", "\n", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "input_resolution", "=", "input_resolution", "\n", "self", ".", "window_size", "=", "window_size", "\n", "self", ".", "shift_size", "=", "shift_size", "\n", "self", ".", "mlp_ratio", "=", "mlp_ratio", "\n", "if", "min", "(", "self", ".", "input_resolution", ")", "<=", "self", ".", "window_size", ":", "\n", "# if window size is larger than input resolution, we don't partition windows", "\n", "            ", "self", ".", "shift_size", "=", "0", "\n", "self", ".", "window_size", "=", "min", "(", "self", ".", "input_resolution", ")", "\n", "", "assert", "0", "<=", "self", ".", "shift_size", "<", "self", ".", "window_size", ",", "\"shift_size must in 0-window_size\"", "\n", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "attn", "=", "WindowAttention", "(", "\n", "dim", ",", "num_heads", "=", "num_heads", ",", "head_dim", "=", "head_dim", ",", "window_size", "=", "to_2tuple", "(", "self", ".", "window_size", ")", ",", "\n", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ")", "\n", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "int", "(", "dim", "*", "mlp_ratio", ")", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "\n", "if", "self", ".", "shift_size", ">", "0", ":", "\n", "# calculate attention mask for SW-MSA", "\n", "            ", "H", ",", "W", "=", "self", ".", "input_resolution", "\n", "img_mask", "=", "torch", ".", "zeros", "(", "(", "1", ",", "H", ",", "W", ",", "1", ")", ")", "# 1 H W 1", "\n", "cnt", "=", "0", "\n", "for", "h", "in", "(", "\n", "slice", "(", "0", ",", "-", "self", ".", "window_size", ")", ",", "\n", "slice", "(", "-", "self", ".", "window_size", ",", "-", "self", ".", "shift_size", ")", ",", "\n", "slice", "(", "-", "self", ".", "shift_size", ",", "None", ")", ")", ":", "\n", "                ", "for", "w", "in", "(", "\n", "slice", "(", "0", ",", "-", "self", ".", "window_size", ")", ",", "\n", "slice", "(", "-", "self", ".", "window_size", ",", "-", "self", ".", "shift_size", ")", ",", "\n", "slice", "(", "-", "self", ".", "shift_size", ",", "None", ")", ")", ":", "\n", "                    ", "img_mask", "[", ":", ",", "h", ",", "w", ",", ":", "]", "=", "cnt", "\n", "cnt", "+=", "1", "\n", "", "", "mask_windows", "=", "window_partition", "(", "img_mask", ",", "self", ".", "window_size", ")", "# num_win, window_size, window_size, 1", "\n", "mask_windows", "=", "mask_windows", ".", "view", "(", "-", "1", ",", "self", ".", "window_size", "*", "self", ".", "window_size", ")", "\n", "attn_mask", "=", "mask_windows", ".", "unsqueeze", "(", "1", ")", "-", "mask_windows", ".", "unsqueeze", "(", "2", ")", "\n", "attn_mask", "=", "attn_mask", ".", "masked_fill", "(", "attn_mask", "!=", "0", ",", "float", "(", "-", "100.0", ")", ")", ".", "masked_fill", "(", "attn_mask", "==", "0", ",", "float", "(", "0.0", ")", ")", "\n", "", "else", ":", "\n", "            ", "attn_mask", "=", "None", "\n", "\n", "", "self", ".", "register_buffer", "(", "\"attn_mask\"", ",", "attn_mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.SwinTransformerBlock.forward": [[290, 328], ["layers._assert", "swin_transformer.SwinTransformerBlock.norm1", "torch.roll.view", "torch.roll.view", "swin_transformer.window_partition", "x_windows.view.view.view", "swin_transformer.SwinTransformerBlock.attn", "attn_windows.view.view.view", "swin_transformer.window_reverse", "torch.roll.view", "torch.roll.view", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "swin_transformer.SwinTransformerBlock.drop_path", "swin_transformer.SwinTransformerBlock.drop_path", "swin_transformer.SwinTransformerBlock.mlp", "swin_transformer.SwinTransformerBlock.norm2"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.window_partition", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.window_reverse", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "H", ",", "W", "=", "self", ".", "input_resolution", "\n", "B", ",", "L", ",", "C", "=", "x", ".", "shape", "\n", "_assert", "(", "L", "==", "H", "*", "W", ",", "\"input feature has wrong size\"", ")", "\n", "\n", "shortcut", "=", "x", "\n", "x", "=", "self", ".", "norm1", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "B", ",", "H", ",", "W", ",", "C", ")", "\n", "\n", "# cyclic shift", "\n", "if", "self", ".", "shift_size", ">", "0", ":", "\n", "            ", "shifted_x", "=", "torch", ".", "roll", "(", "x", ",", "shifts", "=", "(", "-", "self", ".", "shift_size", ",", "-", "self", ".", "shift_size", ")", ",", "dims", "=", "(", "1", ",", "2", ")", ")", "\n", "", "else", ":", "\n", "            ", "shifted_x", "=", "x", "\n", "\n", "# partition windows", "\n", "", "x_windows", "=", "window_partition", "(", "shifted_x", ",", "self", ".", "window_size", ")", "# num_win*B, window_size, window_size, C", "\n", "x_windows", "=", "x_windows", ".", "view", "(", "-", "1", ",", "self", ".", "window_size", "*", "self", ".", "window_size", ",", "C", ")", "# num_win*B, window_size*window_size, C", "\n", "\n", "# W-MSA/SW-MSA", "\n", "attn_windows", "=", "self", ".", "attn", "(", "x_windows", ",", "mask", "=", "self", ".", "attn_mask", ")", "# num_win*B, window_size*window_size, C", "\n", "\n", "# merge windows", "\n", "attn_windows", "=", "attn_windows", ".", "view", "(", "-", "1", ",", "self", ".", "window_size", ",", "self", ".", "window_size", ",", "C", ")", "\n", "shifted_x", "=", "window_reverse", "(", "attn_windows", ",", "self", ".", "window_size", ",", "H", ",", "W", ")", "# B H' W' C", "\n", "\n", "# reverse cyclic shift", "\n", "if", "self", ".", "shift_size", ">", "0", ":", "\n", "            ", "x", "=", "torch", ".", "roll", "(", "shifted_x", ",", "shifts", "=", "(", "self", ".", "shift_size", ",", "self", ".", "shift_size", ")", ",", "dims", "=", "(", "1", ",", "2", ")", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "shifted_x", "\n", "", "x", "=", "x", ".", "view", "(", "B", ",", "H", "*", "W", ",", "C", ")", "\n", "\n", "# FFN", "\n", "x", "=", "shortcut", "+", "self", ".", "drop_path", "(", "x", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.PatchMerging.__init__": [[339, 346], ["torch.Module.__init__", "norm_layer", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "input_resolution", ",", "dim", ",", "out_dim", "=", "None", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_resolution", "=", "input_resolution", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "out_dim", "=", "out_dim", "or", "2", "*", "dim", "\n", "self", ".", "norm", "=", "norm_layer", "(", "4", "*", "dim", ")", "\n", "self", ".", "reduction", "=", "nn", ".", "Linear", "(", "4", "*", "dim", ",", "self", ".", "out_dim", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.PatchMerging.forward": [[347, 369], ["layers._assert", "layers._assert", "swin_transformer.PatchMerging.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "swin_transformer.PatchMerging.view", "swin_transformer.PatchMerging.norm", "swin_transformer.PatchMerging.reduction"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.reduction"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        x: B, H*W, C\n        \"\"\"", "\n", "H", ",", "W", "=", "self", ".", "input_resolution", "\n", "B", ",", "L", ",", "C", "=", "x", ".", "shape", "\n", "_assert", "(", "L", "==", "H", "*", "W", ",", "\"input feature has wrong size\"", ")", "\n", "_assert", "(", "H", "%", "2", "==", "0", "and", "W", "%", "2", "==", "0", ",", "f\"x size ({H}*{W}) are not even.\"", ")", "\n", "\n", "x", "=", "x", ".", "view", "(", "B", ",", "H", ",", "W", ",", "C", ")", "\n", "\n", "x0", "=", "x", "[", ":", ",", "0", ":", ":", "2", ",", "0", ":", ":", "2", ",", ":", "]", "# B H/2 W/2 C", "\n", "x1", "=", "x", "[", ":", ",", "1", ":", ":", "2", ",", "0", ":", ":", "2", ",", ":", "]", "# B H/2 W/2 C", "\n", "x2", "=", "x", "[", ":", ",", "0", ":", ":", "2", ",", "1", ":", ":", "2", ",", ":", "]", "# B H/2 W/2 C", "\n", "x3", "=", "x", "[", ":", ",", "1", ":", ":", "2", ",", "1", ":", ":", "2", ",", ":", "]", "# B H/2 W/2 C", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x0", ",", "x1", ",", "x2", ",", "x3", "]", ",", "-", "1", ")", "# B H/2 W/2 4*C", "\n", "x", "=", "x", ".", "view", "(", "B", ",", "-", "1", ",", "4", "*", "C", ")", "# B H/2*W/2 4*C", "\n", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "x", "=", "self", ".", "reduction", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.BasicLayer.__init__": [[390, 415], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "downsample", "swin_transformer.SwinTransformerBlock", "range", "isinstance"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample"], ["def", "__init__", "(", "\n", "self", ",", "dim", ",", "out_dim", ",", "input_resolution", ",", "depth", ",", "num_heads", "=", "4", ",", "head_dim", "=", "None", ",", "\n", "window_size", "=", "7", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "True", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "\n", "drop_path", "=", "0.", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ",", "downsample", "=", "None", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "input_resolution", "=", "input_resolution", "\n", "self", ".", "depth", "=", "depth", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "# build blocks", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "SwinTransformerBlock", "(", "\n", "dim", "=", "dim", ",", "input_resolution", "=", "input_resolution", ",", "num_heads", "=", "num_heads", ",", "head_dim", "=", "head_dim", ",", "\n", "window_size", "=", "window_size", ",", "shift_size", "=", "0", "if", "(", "i", "%", "2", "==", "0", ")", "else", "window_size", "//", "2", ",", "\n", "mlp_ratio", "=", "mlp_ratio", ",", "qkv_bias", "=", "qkv_bias", ",", "drop", "=", "drop", ",", "attn_drop", "=", "attn_drop", ",", "\n", "drop_path", "=", "drop_path", "[", "i", "]", "if", "isinstance", "(", "drop_path", ",", "list", ")", "else", "drop_path", ",", "norm_layer", "=", "norm_layer", ")", "\n", "for", "i", "in", "range", "(", "depth", ")", "]", ")", "\n", "\n", "# patch merging layer", "\n", "if", "downsample", "is", "not", "None", ":", "\n", "            ", "self", ".", "downsample", "=", "downsample", "(", "input_resolution", ",", "dim", "=", "dim", ",", "out_dim", "=", "out_dim", ",", "norm_layer", "=", "norm_layer", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "downsample", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.BasicLayer.forward": [[416, 424], ["helpers.checkpoint_seq", "swin_transformer.BasicLayer.blocks", "swin_transformer.BasicLayer.downsample", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "blocks", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "blocks", "(", "x", ")", "\n", "", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "downsample", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.SwinTransformer.__init__": [[451, 509], ["torch.Module.__init__", "len", "int", "layers.PatchEmbed", "torch.Dropout", "torch.Dropout", "range", "torch.Sequential", "torch.Sequential", "norm_layer", "torch.Parameter", "torch.Parameter", "isinstance", "layers.to_ntuple", "layers.to_ntuple", "layers.to_ntuple", "x.item", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "swin_transformer.SwinTransformer.init_weights", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "int", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "swin_transformer.BasicLayer", "range", "sum", "sum", "sum"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights"], ["def", "__init__", "(", "\n", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "4", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "global_pool", "=", "'avg'", ",", "\n", "embed_dim", "=", "96", ",", "depths", "=", "(", "2", ",", "2", ",", "6", ",", "2", ")", ",", "num_heads", "=", "(", "3", ",", "6", ",", "12", ",", "24", ")", ",", "head_dim", "=", "None", ",", "\n", "window_size", "=", "7", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "True", ",", "\n", "drop_rate", "=", "0.", ",", "attn_drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.1", ",", "\n", "norm_layer", "=", "nn", ".", "LayerNorm", ",", "ape", "=", "False", ",", "patch_norm", "=", "True", ",", "weight_init", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "global_pool", "in", "(", "''", ",", "'avg'", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "self", ".", "num_layers", "=", "len", "(", "depths", ")", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "num_features", "=", "int", "(", "embed_dim", "*", "2", "**", "(", "self", ".", "num_layers", "-", "1", ")", ")", "\n", "\n", "# split image into non-overlapping patches", "\n", "self", ".", "patch_embed", "=", "PatchEmbed", "(", "\n", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", ",", "in_chans", "=", "in_chans", ",", "embed_dim", "=", "embed_dim", ",", "\n", "norm_layer", "=", "norm_layer", "if", "patch_norm", "else", "None", ")", "\n", "num_patches", "=", "self", ".", "patch_embed", ".", "num_patches", "\n", "self", ".", "patch_grid", "=", "self", ".", "patch_embed", ".", "grid_size", "\n", "\n", "# absolute position embedding", "\n", "self", ".", "absolute_pos_embed", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "num_patches", ",", "embed_dim", ")", ")", "if", "ape", "else", "None", "\n", "self", ".", "pos_drop", "=", "nn", ".", "Dropout", "(", "p", "=", "drop_rate", ")", "\n", "\n", "# build layers", "\n", "if", "not", "isinstance", "(", "embed_dim", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "embed_dim", "=", "[", "int", "(", "embed_dim", "*", "2", "**", "i", ")", "for", "i", "in", "range", "(", "self", ".", "num_layers", ")", "]", "\n", "", "embed_out_dim", "=", "embed_dim", "[", "1", ":", "]", "+", "[", "None", "]", "\n", "head_dim", "=", "to_ntuple", "(", "self", ".", "num_layers", ")", "(", "head_dim", ")", "\n", "window_size", "=", "to_ntuple", "(", "self", ".", "num_layers", ")", "(", "window_size", ")", "\n", "mlp_ratio", "=", "to_ntuple", "(", "self", ".", "num_layers", ")", "(", "mlp_ratio", ")", "\n", "dpr", "=", "[", "x", ".", "item", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "sum", "(", "depths", ")", ")", "]", "# stochastic depth decay rule", "\n", "layers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "            ", "layers", "+=", "[", "BasicLayer", "(", "\n", "dim", "=", "embed_dim", "[", "i", "]", ",", "\n", "out_dim", "=", "embed_out_dim", "[", "i", "]", ",", "\n", "input_resolution", "=", "(", "self", ".", "patch_grid", "[", "0", "]", "//", "(", "2", "**", "i", ")", ",", "self", ".", "patch_grid", "[", "1", "]", "//", "(", "2", "**", "i", ")", ")", ",", "\n", "depth", "=", "depths", "[", "i", "]", ",", "\n", "num_heads", "=", "num_heads", "[", "i", "]", ",", "\n", "head_dim", "=", "head_dim", "[", "i", "]", ",", "\n", "window_size", "=", "window_size", "[", "i", "]", ",", "\n", "mlp_ratio", "=", "mlp_ratio", "[", "i", "]", ",", "\n", "qkv_bias", "=", "qkv_bias", ",", "\n", "drop", "=", "drop_rate", ",", "\n", "attn_drop", "=", "attn_drop_rate", ",", "\n", "drop_path", "=", "dpr", "[", "sum", "(", "depths", "[", ":", "i", "]", ")", ":", "sum", "(", "depths", "[", ":", "i", "+", "1", "]", ")", "]", ",", "\n", "norm_layer", "=", "norm_layer", ",", "\n", "downsample", "=", "PatchMerging", "if", "(", "i", "<", "self", ".", "num_layers", "-", "1", ")", "else", "None", "\n", ")", "]", "\n", "", "self", ".", "layers", "=", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n", "self", ".", "norm", "=", "norm_layer", "(", "self", ".", "num_features", ")", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "if", "weight_init", "!=", "'skip'", ":", "\n", "            ", "self", ".", "init_weights", "(", "weight_init", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.SwinTransformer.init_weights": [[510, 517], ["helpers.named_apply", "layers.trunc_normal_", "vision_transformer.get_init_weights_vit", "math.log"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_apply", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.get_init_weights_vit"], ["", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "init_weights", "(", "self", ",", "mode", "=", "''", ")", ":", "\n", "        ", "assert", "mode", "in", "(", "'jax'", ",", "'jax_nlhb'", ",", "'moco'", ",", "''", ")", "\n", "if", "self", ".", "absolute_pos_embed", "is", "not", "None", ":", "\n", "            ", "trunc_normal_", "(", "self", ".", "absolute_pos_embed", ",", "std", "=", ".02", ")", "\n", "", "head_bias", "=", "-", "math", ".", "log", "(", "self", ".", "num_classes", ")", "if", "'nlhb'", "in", "mode", "else", "0.", "\n", "named_apply", "(", "get_init_weights_vit", "(", "mode", ",", "head_bias", "=", "head_bias", ")", ",", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.SwinTransformer.no_weight_decay": [[518, 525], ["swin_transformer.SwinTransformer.named_parameters", "nwd.add"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "no_weight_decay", "(", "self", ")", ":", "\n", "        ", "nwd", "=", "{", "'absolute_pos_embed'", "}", "\n", "for", "n", ",", "_", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "'relative_position_bias_table'", "in", "n", ":", "\n", "                ", "nwd", ".", "add", "(", "n", ")", "\n", "", "", "return", "nwd", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.SwinTransformer.group_matcher": [[526, 534], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^absolute_pos_embed|patch_embed'", ",", "# stem and embed", "\n", "blocks", "=", "r'^layers\\.(\\d+)'", "if", "coarse", "else", "[", "\n", "(", "r'^layers\\.(\\d+).downsample'", ",", "(", "0", ",", ")", ")", ",", "\n", "(", "r'^layers\\.(\\d+)\\.\\w+\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^norm'", ",", "(", "99999", ",", ")", ")", ",", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.SwinTransformer.set_grad_checkpointing": [[537, 541], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "for", "l", "in", "self", ".", "layers", ":", "\n", "            ", "l", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.SwinTransformer.get_classifier": [[542, 545], ["None"], "methods", ["None"], ["", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.SwinTransformer.reset_classifier": [[546, 552], ["torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "assert", "global_pool", "in", "(", "''", ",", "'avg'", ")", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.SwinTransformer.forward_features": [[553, 561], ["swin_transformer.SwinTransformer.patch_embed", "swin_transformer.SwinTransformer.pos_drop", "swin_transformer.SwinTransformer.layers", "swin_transformer.SwinTransformer.norm"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "patch_embed", "(", "x", ")", "\n", "if", "self", ".", "absolute_pos_embed", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", "+", "self", ".", "absolute_pos_embed", "\n", "", "x", "=", "self", ".", "pos_drop", "(", "x", ")", "\n", "x", "=", "self", ".", "layers", "(", "x", ")", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "# B L C", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.SwinTransformer.forward_head": [[562, 566], ["x.mean.mean.mean", "swin_transformer.SwinTransformer.head"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "global_pool", "==", "'avg'", ":", "\n", "            ", "x", "=", "x", ".", "mean", "(", "dim", "=", "1", ")", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.SwinTransformer.forward": [[567, 571], ["swin_transformer.SwinTransformer.forward_features", "swin_transformer.SwinTransformer.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer._cfg": [[37, 45], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", ".9", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'patch_embed.proj'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.window_partition": [[101, 114], ["x.view.view", "x.view.permute().contiguous().view", "x.view.permute().contiguous", "x.view.permute"], "function", ["None"], ["def", "window_partition", "(", "x", ",", "window_size", ":", "int", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"", "\n", "B", ",", "H", ",", "W", ",", "C", "=", "x", ".", "shape", "\n", "x", "=", "x", ".", "view", "(", "B", ",", "H", "//", "window_size", ",", "window_size", ",", "W", "//", "window_size", ",", "window_size", ",", "C", ")", "\n", "windows", "=", "x", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ",", "4", ",", "5", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "window_size", ",", "window_size", ",", "C", ")", "\n", "return", "windows", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.window_reverse": [[116, 132], ["int", "windows.view", "x.permute().contiguous().view.permute().contiguous().view", "x.permute().contiguous().view.permute().contiguous", "x.permute().contiguous().view.permute"], "function", ["None"], ["", "@", "register_notrace_function", "# reason: int argument is a Proxy", "\n", "def", "window_reverse", "(", "windows", ",", "window_size", ":", "int", ",", "H", ":", "int", ",", "W", ":", "int", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"", "\n", "B", "=", "int", "(", "windows", ".", "shape", "[", "0", "]", "/", "(", "H", "*", "W", "/", "window_size", "/", "window_size", ")", ")", "\n", "x", "=", "windows", ".", "view", "(", "B", ",", "H", "//", "window_size", ",", "W", "//", "window_size", ",", "window_size", ",", "window_size", ",", "-", "1", ")", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ",", "4", ",", "5", ")", ".", "contiguous", "(", ")", ".", "view", "(", "B", ",", "H", ",", "W", ",", "-", "1", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.get_relative_position_index": [[134, 144], ["torch.stack", "torch.stack", "torch.flatten", "torch.flatten", "relative_coords.permute().contiguous.permute().contiguous", "relative_coords.permute().contiguous.sum", "torch.meshgrid", "torch.meshgrid", "relative_coords.permute().contiguous.permute", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "function", ["None"], ["", "def", "get_relative_position_index", "(", "win_h", ",", "win_w", ")", ":", "\n", "# get pair-wise relative position index for each token inside the window", "\n", "    ", "coords", "=", "torch", ".", "stack", "(", "torch", ".", "meshgrid", "(", "[", "torch", ".", "arange", "(", "win_h", ")", ",", "torch", ".", "arange", "(", "win_w", ")", "]", ")", ")", "# 2, Wh, Ww", "\n", "coords_flatten", "=", "torch", ".", "flatten", "(", "coords", ",", "1", ")", "# 2, Wh*Ww", "\n", "relative_coords", "=", "coords_flatten", "[", ":", ",", ":", ",", "None", "]", "-", "coords_flatten", "[", ":", ",", "None", ",", ":", "]", "# 2, Wh*Ww, Wh*Ww", "\n", "relative_coords", "=", "relative_coords", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ".", "contiguous", "(", ")", "# Wh*Ww, Wh*Ww, 2", "\n", "relative_coords", "[", ":", ",", ":", ",", "0", "]", "+=", "win_h", "-", "1", "# shift to start from 0", "\n", "relative_coords", "[", ":", ",", ":", ",", "1", "]", "+=", "win_w", "-", "1", "\n", "relative_coords", "[", ":", ",", ":", ",", "0", "]", "*=", "2", "*", "win_w", "-", "1", "\n", "return", "relative_coords", ".", "sum", "(", "-", "1", ")", "# Wh*Ww, Wh*Ww", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer._create_swin_transformer": [[573, 580], ["helpers.build_model_with_cfg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_swin_transformer", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model", "=", "build_model_with_cfg", "(", "\n", "SwinTransformer", ",", "variant", ",", "pretrained", ",", "\n", "pretrained_filter_fn", "=", "checkpoint_filter_fn", ",", "\n", "**", "kwargs", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.swin_base_patch4_window12_384": [[582, 589], ["dict", "swin_transformer._create_swin_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer._create_swin_transformer"], ["", "@", "register_model", "\n", "def", "swin_base_patch4_window12_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Swin-B @ 384x384, pretrained ImageNet-22k, fine tune 1k\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "window_size", "=", "12", ",", "embed_dim", "=", "128", ",", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "num_heads", "=", "(", "4", ",", "8", ",", "16", ",", "32", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer", "(", "'swin_base_patch4_window12_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.swin_base_patch4_window7_224": [[591, 598], ["dict", "swin_transformer._create_swin_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer._create_swin_transformer"], ["", "@", "register_model", "\n", "def", "swin_base_patch4_window7_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Swin-B @ 224x224, pretrained ImageNet-22k, fine tune 1k\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "window_size", "=", "7", ",", "embed_dim", "=", "128", ",", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "num_heads", "=", "(", "4", ",", "8", ",", "16", ",", "32", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer", "(", "'swin_base_patch4_window7_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.swin_large_patch4_window12_384": [[600, 607], ["dict", "swin_transformer._create_swin_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer._create_swin_transformer"], ["", "@", "register_model", "\n", "def", "swin_large_patch4_window12_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Swin-L @ 384x384, pretrained ImageNet-22k, fine tune 1k\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "window_size", "=", "12", ",", "embed_dim", "=", "192", ",", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "num_heads", "=", "(", "6", ",", "12", ",", "24", ",", "48", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer", "(", "'swin_large_patch4_window12_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.swin_large_patch4_window7_224": [[609, 616], ["dict", "swin_transformer._create_swin_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer._create_swin_transformer"], ["", "@", "register_model", "\n", "def", "swin_large_patch4_window7_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Swin-L @ 224x224, pretrained ImageNet-22k, fine tune 1k\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "window_size", "=", "7", ",", "embed_dim", "=", "192", ",", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "num_heads", "=", "(", "6", ",", "12", ",", "24", ",", "48", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer", "(", "'swin_large_patch4_window7_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.swin_small_patch4_window7_224": [[618, 625], ["dict", "swin_transformer._create_swin_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer._create_swin_transformer"], ["", "@", "register_model", "\n", "def", "swin_small_patch4_window7_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Swin-S @ 224x224, trained ImageNet-1k\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "window_size", "=", "7", ",", "embed_dim", "=", "96", ",", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "num_heads", "=", "(", "3", ",", "6", ",", "12", ",", "24", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer", "(", "'swin_small_patch4_window7_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.swin_tiny_patch4_window7_224": [[627, 634], ["dict", "swin_transformer._create_swin_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer._create_swin_transformer"], ["", "@", "register_model", "\n", "def", "swin_tiny_patch4_window7_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Swin-T @ 224x224, trained ImageNet-1k\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "window_size", "=", "7", ",", "embed_dim", "=", "96", ",", "depths", "=", "(", "2", ",", "2", ",", "6", ",", "2", ")", ",", "num_heads", "=", "(", "3", ",", "6", ",", "12", ",", "24", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer", "(", "'swin_tiny_patch4_window7_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.swin_base_patch4_window12_384_in22k": [[636, 643], ["dict", "swin_transformer._create_swin_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer._create_swin_transformer"], ["", "@", "register_model", "\n", "def", "swin_base_patch4_window12_384_in22k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Swin-B @ 384x384, trained ImageNet-22k\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "window_size", "=", "12", ",", "embed_dim", "=", "128", ",", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "num_heads", "=", "(", "4", ",", "8", ",", "16", ",", "32", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer", "(", "'swin_base_patch4_window12_384_in22k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.swin_base_patch4_window7_224_in22k": [[645, 652], ["dict", "swin_transformer._create_swin_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer._create_swin_transformer"], ["", "@", "register_model", "\n", "def", "swin_base_patch4_window7_224_in22k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Swin-B @ 224x224, trained ImageNet-22k\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "window_size", "=", "7", ",", "embed_dim", "=", "128", ",", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "num_heads", "=", "(", "4", ",", "8", ",", "16", ",", "32", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer", "(", "'swin_base_patch4_window7_224_in22k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.swin_large_patch4_window12_384_in22k": [[654, 661], ["dict", "swin_transformer._create_swin_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer._create_swin_transformer"], ["", "@", "register_model", "\n", "def", "swin_large_patch4_window12_384_in22k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Swin-L @ 384x384, trained ImageNet-22k\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "window_size", "=", "12", ",", "embed_dim", "=", "192", ",", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "num_heads", "=", "(", "6", ",", "12", ",", "24", ",", "48", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer", "(", "'swin_large_patch4_window12_384_in22k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.swin_large_patch4_window7_224_in22k": [[663, 670], ["dict", "swin_transformer._create_swin_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer._create_swin_transformer"], ["", "@", "register_model", "\n", "def", "swin_large_patch4_window7_224_in22k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Swin-L @ 224x224, trained ImageNet-22k\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "window_size", "=", "7", ",", "embed_dim", "=", "192", ",", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "num_heads", "=", "(", "6", ",", "12", ",", "24", ",", "48", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer", "(", "'swin_large_patch4_window7_224_in22k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.swin_s3_tiny_224": [[672, 680], ["dict", "swin_transformer._create_swin_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer._create_swin_transformer"], ["", "@", "register_model", "\n", "def", "swin_s3_tiny_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Swin-S3-T @ 224x224, ImageNet-1k. https://arxiv.org/abs/2111.14725\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "window_size", "=", "(", "7", ",", "7", ",", "14", ",", "7", ")", ",", "embed_dim", "=", "96", ",", "depths", "=", "(", "2", ",", "2", ",", "6", ",", "2", ")", ",", "\n", "num_heads", "=", "(", "3", ",", "6", ",", "12", ",", "24", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer", "(", "'swin_s3_tiny_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.swin_s3_small_224": [[682, 690], ["dict", "swin_transformer._create_swin_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer._create_swin_transformer"], ["", "@", "register_model", "\n", "def", "swin_s3_small_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Swin-S3-S @ 224x224, trained ImageNet-1k. https://arxiv.org/abs/2111.14725\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "window_size", "=", "(", "14", ",", "14", ",", "14", ",", "7", ")", ",", "embed_dim", "=", "96", ",", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "\n", "num_heads", "=", "(", "3", ",", "6", ",", "12", ",", "24", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer", "(", "'swin_s3_small_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer.swin_s3_base_224": [[692, 700], ["dict", "swin_transformer._create_swin_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer._create_swin_transformer"], ["", "@", "register_model", "\n", "def", "swin_s3_base_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Swin-S3-B @ 224x224, trained ImageNet-1k. https://arxiv.org/abs/2111.14725\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "window_size", "=", "(", "7", ",", "7", ",", "14", ",", "7", ")", ",", "embed_dim", "=", "96", ",", "depths", "=", "(", "2", ",", "2", ",", "30", ",", "2", ")", ",", "\n", "num_heads", "=", "(", "3", ",", "6", ",", "12", ",", "24", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer", "(", "'swin_s3_base_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.register_model": [[21, 52], ["fn.__module__.split", "hasattr", "_module_to_models[].add", "len", "mod.__all__.append", "hasattr", "_model_has_pretrained.add"], "function", ["None"], ["def", "register_model", "(", "fn", ")", ":", "\n", "# lookup containing module", "\n", "    ", "mod", "=", "sys", ".", "modules", "[", "fn", ".", "__module__", "]", "\n", "module_name_split", "=", "fn", ".", "__module__", ".", "split", "(", "'.'", ")", "\n", "module_name", "=", "module_name_split", "[", "-", "1", "]", "if", "len", "(", "module_name_split", ")", "else", "''", "\n", "\n", "# add model to __all__ in module", "\n", "model_name", "=", "fn", ".", "__name__", "\n", "if", "hasattr", "(", "mod", ",", "'__all__'", ")", ":", "\n", "        ", "mod", ".", "__all__", ".", "append", "(", "model_name", ")", "\n", "", "else", ":", "\n", "        ", "mod", ".", "__all__", "=", "[", "model_name", "]", "\n", "\n", "# add entries to registry dict/sets", "\n", "", "_model_entrypoints", "[", "model_name", "]", "=", "fn", "\n", "_model_to_module", "[", "model_name", "]", "=", "module_name", "\n", "_module_to_models", "[", "module_name", "]", ".", "add", "(", "model_name", ")", "\n", "has_valid_pretrained", "=", "False", "# check if model has a pretrained url to allow filtering on this", "\n", "if", "hasattr", "(", "mod", ",", "'default_cfgs'", ")", "and", "model_name", "in", "mod", ".", "default_cfgs", ":", "\n", "# this will catch all models that have entrypoint matching cfg key, but miss any aliasing", "\n", "# entrypoints or non-matching combos", "\n", "        ", "cfg", "=", "mod", ".", "default_cfgs", "[", "model_name", "]", "\n", "has_valid_pretrained", "=", "(", "\n", "(", "'url'", "in", "cfg", "and", "'http'", "in", "cfg", "[", "'url'", "]", ")", "or", "\n", "(", "'file'", "in", "cfg", "and", "cfg", "[", "'file'", "]", ")", "or", "\n", "(", "'hf_hub_id'", "in", "cfg", "and", "cfg", "[", "'hf_hub_id'", "]", ")", "\n", ")", "\n", "_model_pretrained_cfgs", "[", "model_name", "]", "=", "mod", ".", "default_cfgs", "[", "model_name", "]", "\n", "", "if", "has_valid_pretrained", ":", "\n", "        ", "_model_has_pretrained", ".", "add", "(", "model_name", ")", "\n", "", "return", "fn", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry._natural_key": [[54, 56], ["s.isdigit", "int", "re.split", "string_.lower"], "function", ["None"], ["", "def", "_natural_key", "(", "string_", ")", ":", "\n", "    ", "return", "[", "int", "(", "s", ")", "if", "s", ".", "isdigit", "(", ")", "else", "s", "for", "s", "in", "re", ".", "split", "(", "r'(\\d+)'", ",", "string_", ".", "lower", "(", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.list_models": [[58, 97], ["list", "list", "_model_entrypoints.keys", "_model_has_pretrained.intersection", "set().intersection", "sorted", "isinstance", "fnmatch.filter", "len", "isinstance", "fnmatch.filter", "len", "set().union", "set().difference", "set", "set", "set"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEmaV2.set"], ["", "def", "list_models", "(", "filter", "=", "''", ",", "module", "=", "''", ",", "pretrained", "=", "False", ",", "exclude_filters", "=", "''", ",", "name_matches_cfg", "=", "False", ")", ":", "\n", "    ", "\"\"\" Return list of available model names, sorted alphabetically\n\n    Args:\n        filter (str) - Wildcard filter string that works with fnmatch\n        module (str) - Limit model selection to a specific sub-module (ie 'gen_efficientnet')\n        pretrained (bool) - Include only models with pretrained weights if True\n        exclude_filters (str or list[str]) - Wildcard filters to exclude models after including them with filter\n        name_matches_cfg (bool) - Include only models w/ model_name matching default_cfg name (excludes some aliases)\n\n    Example:\n        model_list('gluon_resnet*') -- returns all models starting with 'gluon_resnet'\n        model_list('*resnext*, 'resnet') -- returns all models with 'resnext' in 'resnet' module\n    \"\"\"", "\n", "if", "module", ":", "\n", "        ", "all_models", "=", "list", "(", "_module_to_models", "[", "module", "]", ")", "\n", "", "else", ":", "\n", "        ", "all_models", "=", "_model_entrypoints", ".", "keys", "(", ")", "\n", "", "if", "filter", ":", "\n", "        ", "models", "=", "[", "]", "\n", "include_filters", "=", "filter", "if", "isinstance", "(", "filter", ",", "(", "tuple", ",", "list", ")", ")", "else", "[", "filter", "]", "\n", "for", "f", "in", "include_filters", ":", "\n", "            ", "include_models", "=", "fnmatch", ".", "filter", "(", "all_models", ",", "f", ")", "# include these models", "\n", "if", "len", "(", "include_models", ")", ":", "\n", "                ", "models", "=", "set", "(", "models", ")", ".", "union", "(", "include_models", ")", "\n", "", "", "", "else", ":", "\n", "        ", "models", "=", "all_models", "\n", "", "if", "exclude_filters", ":", "\n", "        ", "if", "not", "isinstance", "(", "exclude_filters", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "exclude_filters", "=", "[", "exclude_filters", "]", "\n", "", "for", "xf", "in", "exclude_filters", ":", "\n", "            ", "exclude_models", "=", "fnmatch", ".", "filter", "(", "models", ",", "xf", ")", "# exclude these models", "\n", "if", "len", "(", "exclude_models", ")", ":", "\n", "                ", "models", "=", "set", "(", "models", ")", ".", "difference", "(", "exclude_models", ")", "\n", "", "", "", "if", "pretrained", ":", "\n", "        ", "models", "=", "_model_has_pretrained", ".", "intersection", "(", "models", ")", "\n", "", "if", "name_matches_cfg", ":", "\n", "        ", "models", "=", "set", "(", "_model_pretrained_cfgs", ")", ".", "intersection", "(", "models", ")", "\n", "", "return", "list", "(", "sorted", "(", "models", ",", "key", "=", "_natural_key", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.is_model": [[99, 103], ["None"], "function", ["None"], ["", "def", "is_model", "(", "model_name", ")", ":", "\n", "    ", "\"\"\" Check if a model name exists\n    \"\"\"", "\n", "return", "model_name", "in", "_model_entrypoints", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.model_entrypoint": [[105, 109], ["None"], "function", ["None"], ["", "def", "model_entrypoint", "(", "model_name", ")", ":", "\n", "    ", "\"\"\"Fetch a model entrypoint for specified model name\n    \"\"\"", "\n", "return", "_model_entrypoints", "[", "model_name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.list_modules": [[111, 116], ["_module_to_models.keys", "list", "sorted"], "function", ["None"], ["", "def", "list_modules", "(", ")", ":", "\n", "    ", "\"\"\" Return list of module names that contain models / model entrypoints\n    \"\"\"", "\n", "modules", "=", "_module_to_models", ".", "keys", "(", ")", "\n", "return", "list", "(", "sorted", "(", "modules", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.is_model_in_modules": [[118, 126], ["isinstance", "any"], "function", ["None"], ["", "def", "is_model_in_modules", "(", "model_name", ",", "module_names", ")", ":", "\n", "    ", "\"\"\"Check if a model exists within a subset of modules\n    Args:\n        model_name (str) - name of model to check\n        module_names (tuple, list, set) - names of modules to search in\n    \"\"\"", "\n", "assert", "isinstance", "(", "module_names", ",", "(", "tuple", ",", "list", ",", "set", ")", ")", "\n", "return", "any", "(", "model_name", "in", "_module_to_models", "[", "n", "]", "for", "n", "in", "module_names", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.is_model_pretrained": [[128, 130], ["None"], "function", ["None"], ["", "def", "is_model_pretrained", "(", "model_name", ")", ":", "\n", "    ", "return", "model_name", "in", "_model_has_pretrained", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.get_pretrained_cfg": [[132, 136], ["copy.deepcopy"], "function", ["None"], ["", "def", "get_pretrained_cfg", "(", "model_name", ")", ":", "\n", "    ", "if", "model_name", "in", "_model_pretrained_cfgs", ":", "\n", "        ", "return", "deepcopy", "(", "_model_pretrained_cfgs", "[", "model_name", "]", ")", "\n", "", "return", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.has_pretrained_cfg_key": [[138, 144], ["None"], "function", ["None"], ["", "def", "has_pretrained_cfg_key", "(", "model_name", ",", "cfg_key", ")", ":", "\n", "    ", "\"\"\" Query model default_cfgs for existence of a specific key.\n    \"\"\"", "\n", "if", "model_name", "in", "_model_pretrained_cfgs", "and", "cfg_key", "in", "_model_pretrained_cfgs", "[", "model_name", "]", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.is_pretrained_cfg_key": [[146, 152], ["_model_pretrained_cfgs[].get"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["", "def", "is_pretrained_cfg_key", "(", "model_name", ",", "cfg_key", ")", ":", "\n", "    ", "\"\"\" Return truthy value for specified model default_cfg key, False if does not exist.\n    \"\"\"", "\n", "if", "model_name", "in", "_model_pretrained_cfgs", "and", "_model_pretrained_cfgs", "[", "model_name", "]", ".", "get", "(", "cfg_key", ",", "False", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.get_pretrained_cfg_value": [[154, 160], ["_model_pretrained_cfgs[].get"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["", "def", "get_pretrained_cfg_value", "(", "model_name", ",", "cfg_key", ")", ":", "\n", "    ", "\"\"\" Get a specific model default_cfg value by key. None if it doesn't exist.\n    \"\"\"", "\n", "if", "model_name", "in", "_model_pretrained_cfgs", ":", "\n", "        ", "return", "_model_pretrained_cfgs", "[", "model_name", "]", ".", "get", "(", "cfg_key", ",", "None", ")", "\n", "", "return", "None", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.ghostnet.GhostModule.__init__": [[47, 63], ["torch.Module.__init__", "math.ceil", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Sequential", "torch.Sequential", "torch.Sequential"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inp", ",", "oup", ",", "kernel_size", "=", "1", ",", "ratio", "=", "2", ",", "dw_size", "=", "3", ",", "stride", "=", "1", ",", "relu", "=", "True", ")", ":", "\n", "        ", "super", "(", "GhostModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "oup", "=", "oup", "\n", "init_channels", "=", "math", ".", "ceil", "(", "oup", "/", "ratio", ")", "\n", "new_channels", "=", "init_channels", "*", "(", "ratio", "-", "1", ")", "\n", "\n", "self", ".", "primary_conv", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "inp", ",", "init_channels", ",", "kernel_size", ",", "stride", ",", "kernel_size", "//", "2", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "init_channels", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "if", "relu", "else", "nn", ".", "Sequential", "(", ")", ",", "\n", ")", "\n", "\n", "self", ".", "cheap_operation", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "init_channels", ",", "new_channels", ",", "dw_size", ",", "1", ",", "dw_size", "//", "2", ",", "groups", "=", "init_channels", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "new_channels", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "if", "relu", "else", "nn", ".", "Sequential", "(", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.ghostnet.GhostModule.forward": [[65, 70], ["ghostnet.GhostModule.primary_conv", "ghostnet.GhostModule.cheap_operation", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x1", "=", "self", ".", "primary_conv", "(", "x", ")", "\n", "x2", "=", "self", ".", "cheap_operation", "(", "x1", ")", "\n", "out", "=", "torch", ".", "cat", "(", "[", "x1", ",", "x2", "]", ",", "dim", "=", "1", ")", "\n", "return", "out", "[", ":", ",", ":", "self", ".", "oup", ",", ":", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.ghostnet.GhostBottleneck.__init__": [[75, 111], ["torch.Module.__init__", "ghostnet.GhostModule", "ghostnet.GhostModule", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "_SE_LAYER", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "in_chs", ",", "mid_chs", ",", "out_chs", ",", "dw_kernel_size", "=", "3", ",", "\n", "stride", "=", "1", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "se_ratio", "=", "0.", ")", ":", "\n", "        ", "super", "(", "GhostBottleneck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "has_se", "=", "se_ratio", "is", "not", "None", "and", "se_ratio", ">", "0.", "\n", "self", ".", "stride", "=", "stride", "\n", "\n", "# Point-wise expansion", "\n", "self", ".", "ghost1", "=", "GhostModule", "(", "in_chs", ",", "mid_chs", ",", "relu", "=", "True", ")", "\n", "\n", "# Depth-wise convolution", "\n", "if", "self", ".", "stride", ">", "1", ":", "\n", "            ", "self", ".", "conv_dw", "=", "nn", ".", "Conv2d", "(", "\n", "mid_chs", ",", "mid_chs", ",", "dw_kernel_size", ",", "stride", "=", "stride", ",", "\n", "padding", "=", "(", "dw_kernel_size", "-", "1", ")", "//", "2", ",", "groups", "=", "mid_chs", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn_dw", "=", "nn", ".", "BatchNorm2d", "(", "mid_chs", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv_dw", "=", "None", "\n", "self", ".", "bn_dw", "=", "None", "\n", "\n", "# Squeeze-and-excitation", "\n", "", "self", ".", "se", "=", "_SE_LAYER", "(", "mid_chs", ",", "rd_ratio", "=", "se_ratio", ")", "if", "has_se", "else", "None", "\n", "\n", "# Point-wise linear projection", "\n", "self", ".", "ghost2", "=", "GhostModule", "(", "mid_chs", ",", "out_chs", ",", "relu", "=", "False", ")", "\n", "\n", "# shortcut", "\n", "if", "in_chs", "==", "out_chs", "and", "self", ".", "stride", "==", "1", ":", "\n", "            ", "self", ".", "shortcut", "=", "nn", ".", "Sequential", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "shortcut", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "\n", "in_chs", ",", "in_chs", ",", "dw_kernel_size", ",", "stride", "=", "stride", ",", "\n", "padding", "=", "(", "dw_kernel_size", "-", "1", ")", "//", "2", ",", "groups", "=", "in_chs", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "in_chs", ")", ",", "\n", "nn", ".", "Conv2d", "(", "in_chs", ",", "out_chs", ",", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "out_chs", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.ghostnet.GhostBottleneck.forward": [[113, 133], ["ghostnet.GhostBottleneck.ghost1", "ghostnet.GhostBottleneck.ghost2", "ghostnet.GhostBottleneck.shortcut", "ghostnet.GhostBottleneck.conv_dw", "ghostnet.GhostBottleneck.bn_dw", "ghostnet.GhostBottleneck.se"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "\n", "# 1st ghost bottleneck", "\n", "x", "=", "self", ".", "ghost1", "(", "x", ")", "\n", "\n", "# Depth-wise convolution", "\n", "if", "self", ".", "conv_dw", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "conv_dw", "(", "x", ")", "\n", "x", "=", "self", ".", "bn_dw", "(", "x", ")", "\n", "\n", "# Squeeze-and-excitation", "\n", "", "if", "self", ".", "se", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "se", "(", "x", ")", "\n", "\n", "# 2nd ghost bottleneck", "\n", "", "x", "=", "self", ".", "ghost2", "(", "x", ")", "\n", "\n", "x", "+=", "self", ".", "shortcut", "(", "shortcut", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.ghostnet.GhostNet.__init__": [[136, 188], ["torch.Module.__init__", "layers.make_divisible", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "ghostnet.GhostNet.feature_info.append", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "layers.make_divisible", "torch.ModuleList.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "layers.SelectAdaptivePool2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "dict", "torch.ModuleList.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Flatten", "torch.Flatten", "torch.Flatten", "torch.Identity", "torch.Identity", "torch.Identity", "layers.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "layers.make_divisible", "layers.make_divisible", "layers.append", "ghostnet.GhostNet.feature_info.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "efficientnet_blocks.ConvBnAct", "block", "dict"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible"], ["    ", "def", "__init__", "(", "\n", "self", ",", "cfgs", ",", "num_classes", "=", "1000", ",", "width", "=", "1.0", ",", "in_chans", "=", "3", ",", "output_stride", "=", "32", ",", "global_pool", "=", "'avg'", ",", "drop_rate", "=", "0.2", ")", ":", "\n", "        ", "super", "(", "GhostNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# setting of inverted residual blocks", "\n", "assert", "output_stride", "==", "32", ",", "'only output_stride==32 is valid, dilation not supported'", "\n", "self", ".", "cfgs", "=", "cfgs", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "self", ".", "feature_info", "=", "[", "]", "\n", "\n", "# building first layer", "\n", "stem_chs", "=", "make_divisible", "(", "16", "*", "width", ",", "4", ")", "\n", "self", ".", "conv_stem", "=", "nn", ".", "Conv2d", "(", "in_chans", ",", "stem_chs", ",", "3", ",", "2", ",", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "feature_info", ".", "append", "(", "dict", "(", "num_chs", "=", "stem_chs", ",", "reduction", "=", "2", ",", "module", "=", "f'conv_stem'", ")", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm2d", "(", "stem_chs", ")", "\n", "self", ".", "act1", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "prev_chs", "=", "stem_chs", "\n", "\n", "# building inverted residual blocks", "\n", "stages", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "block", "=", "GhostBottleneck", "\n", "stage_idx", "=", "0", "\n", "net_stride", "=", "2", "\n", "for", "cfg", "in", "self", ".", "cfgs", ":", "\n", "            ", "layers", "=", "[", "]", "\n", "s", "=", "1", "\n", "for", "k", ",", "exp_size", ",", "c", ",", "se_ratio", ",", "s", "in", "cfg", ":", "\n", "                ", "out_chs", "=", "make_divisible", "(", "c", "*", "width", ",", "4", ")", "\n", "mid_chs", "=", "make_divisible", "(", "exp_size", "*", "width", ",", "4", ")", "\n", "layers", ".", "append", "(", "block", "(", "prev_chs", ",", "mid_chs", ",", "out_chs", ",", "k", ",", "s", ",", "se_ratio", "=", "se_ratio", ")", ")", "\n", "prev_chs", "=", "out_chs", "\n", "", "if", "s", ">", "1", ":", "\n", "                ", "net_stride", "*=", "2", "\n", "self", ".", "feature_info", ".", "append", "(", "dict", "(", "\n", "num_chs", "=", "prev_chs", ",", "reduction", "=", "net_stride", ",", "module", "=", "f'blocks.{stage_idx}'", ")", ")", "\n", "", "stages", ".", "append", "(", "nn", ".", "Sequential", "(", "*", "layers", ")", ")", "\n", "stage_idx", "+=", "1", "\n", "\n", "", "out_chs", "=", "make_divisible", "(", "exp_size", "*", "width", ",", "4", ")", "\n", "stages", ".", "append", "(", "nn", ".", "Sequential", "(", "ConvBnAct", "(", "prev_chs", ",", "out_chs", ",", "1", ")", ")", ")", "\n", "self", ".", "pool_dim", "=", "prev_chs", "=", "out_chs", "\n", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "*", "stages", ")", "\n", "\n", "# building last several layers", "\n", "self", ".", "num_features", "=", "out_chs", "=", "1280", "\n", "self", ".", "global_pool", "=", "SelectAdaptivePool2d", "(", "pool_type", "=", "global_pool", ")", "\n", "self", ".", "conv_head", "=", "nn", ".", "Conv2d", "(", "prev_chs", ",", "out_chs", ",", "1", ",", "1", ",", "0", ",", "bias", "=", "True", ")", "\n", "self", ".", "act2", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "flatten", "=", "nn", ".", "Flatten", "(", "1", ")", "if", "global_pool", "else", "nn", ".", "Identity", "(", ")", "# don't flatten if pooling disabled", "\n", "self", ".", "classifier", "=", "Linear", "(", "out_chs", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.ghostnet.GhostNet.group_matcher": [[191, 201], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "\n", "stem", "=", "r'^conv_stem|bn1'", ",", "\n", "blocks", "=", "[", "\n", "(", "r'^blocks\\.(\\d+)'", "if", "coarse", "else", "r'^blocks\\.(\\d+)\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'conv_head'", ",", "(", "99999", ",", ")", ")", "\n", "]", "\n", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.ghostnet.GhostNet.set_grad_checkpointing": [[202, 205], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.ghostnet.GhostNet.get_classifier": [[206, 209], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "classifier", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.ghostnet.GhostNet.reset_classifier": [[210, 216], ["layers.SelectAdaptivePool2d", "torch.Flatten", "torch.Flatten", "torch.Flatten", "torch.Identity", "torch.Identity", "torch.Identity", "layers.Linear", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "# cannot meaningfully change pooling of efficient head after creation", "\n", "self", ".", "global_pool", "=", "SelectAdaptivePool2d", "(", "pool_type", "=", "global_pool", ")", "\n", "self", ".", "flatten", "=", "nn", ".", "Flatten", "(", "1", ")", "if", "global_pool", "else", "nn", ".", "Identity", "(", ")", "# don't flatten if pooling disabled", "\n", "self", ".", "classifier", "=", "Linear", "(", "self", ".", "pool_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.ghostnet.GhostNet.forward_features": [[217, 226], ["ghostnet.GhostNet.conv_stem", "ghostnet.GhostNet.bn1", "ghostnet.GhostNet.act1", "helpers.checkpoint_seq", "ghostnet.GhostNet.blocks", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv_stem", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "x", "=", "self", ".", "act1", "(", "x", ")", "\n", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "blocks", ",", "x", ",", "flatten", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "blocks", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.ghostnet.GhostNet.forward_head": [[227, 236], ["ghostnet.GhostNet.global_pool", "ghostnet.GhostNet.conv_head", "ghostnet.GhostNet.act2", "ghostnet.GhostNet.flatten", "ghostnet.GhostNet.classifier", "torch.dropout", "torch.dropout", "torch.dropout"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_head", "(", "x", ")", "\n", "x", "=", "self", ".", "act2", "(", "x", ")", "\n", "x", "=", "self", ".", "flatten", "(", "x", ")", "\n", "if", "self", ".", "drop_rate", ">", "0.", ":", "\n", "            ", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "drop_rate", ",", "training", "=", "self", ".", "training", ")", "\n", "", "x", "=", "self", ".", "classifier", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.ghostnet.GhostNet.forward": [[237, 241], ["ghostnet.GhostNet.forward_features", "ghostnet.GhostNet.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.ghostnet._cfg": [[25, 32], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bilinear'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'conv_stem'", ",", "'classifier'", ":", "'classifier'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.ghostnet._create_ghostnet": [[243, 282], ["dict", "helpers.build_model_with_cfg", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_ghostnet", "(", "variant", ",", "width", "=", "1.0", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    Constructs a GhostNet model\n    \"\"\"", "\n", "cfgs", "=", "[", "\n", "# k, t, c, SE, s ", "\n", "# stage1", "\n", "[", "[", "3", ",", "16", ",", "16", ",", "0", ",", "1", "]", "]", ",", "\n", "# stage2", "\n", "[", "[", "3", ",", "48", ",", "24", ",", "0", ",", "2", "]", "]", ",", "\n", "[", "[", "3", ",", "72", ",", "24", ",", "0", ",", "1", "]", "]", ",", "\n", "# stage3", "\n", "[", "[", "5", ",", "72", ",", "40", ",", "0.25", ",", "2", "]", "]", ",", "\n", "[", "[", "5", ",", "120", ",", "40", ",", "0.25", ",", "1", "]", "]", ",", "\n", "# stage4", "\n", "[", "[", "3", ",", "240", ",", "80", ",", "0", ",", "2", "]", "]", ",", "\n", "[", "[", "3", ",", "200", ",", "80", ",", "0", ",", "1", "]", ",", "\n", "[", "3", ",", "184", ",", "80", ",", "0", ",", "1", "]", ",", "\n", "[", "3", ",", "184", ",", "80", ",", "0", ",", "1", "]", ",", "\n", "[", "3", ",", "480", ",", "112", ",", "0.25", ",", "1", "]", ",", "\n", "[", "3", ",", "672", ",", "112", ",", "0.25", ",", "1", "]", "\n", "]", ",", "\n", "# stage5", "\n", "[", "[", "5", ",", "672", ",", "160", ",", "0.25", ",", "2", "]", "]", ",", "\n", "[", "[", "5", ",", "960", ",", "160", ",", "0", ",", "1", "]", ",", "\n", "[", "5", ",", "960", ",", "160", ",", "0.25", ",", "1", "]", ",", "\n", "[", "5", ",", "960", ",", "160", ",", "0", ",", "1", "]", ",", "\n", "[", "5", ",", "960", ",", "160", ",", "0.25", ",", "1", "]", "\n", "]", "\n", "]", "\n", "model_kwargs", "=", "dict", "(", "\n", "cfgs", "=", "cfgs", ",", "\n", "width", "=", "width", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "return", "build_model_with_cfg", "(", "\n", "GhostNet", ",", "variant", ",", "pretrained", ",", "\n", "feature_cfg", "=", "dict", "(", "flatten_sequential", "=", "True", ")", ",", "\n", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.ghostnet.ghostnet_050": [[284, 289], ["ghostnet._create_ghostnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.ghostnet._create_ghostnet"], ["", "@", "register_model", "\n", "def", "ghostnet_050", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" GhostNet-0.5x \"\"\"", "\n", "model", "=", "_create_ghostnet", "(", "'ghostnet_050'", ",", "width", "=", "0.5", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.ghostnet.ghostnet_100": [[291, 296], ["ghostnet._create_ghostnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.ghostnet._create_ghostnet"], ["", "@", "register_model", "\n", "def", "ghostnet_100", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" GhostNet-1.0x \"\"\"", "\n", "model", "=", "_create_ghostnet", "(", "'ghostnet_100'", ",", "width", "=", "1.0", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.ghostnet.ghostnet_130": [[298, 303], ["ghostnet._create_ghostnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.ghostnet._create_ghostnet"], ["", "@", "register_model", "\n", "def", "ghostnet_130", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" GhostNet-1.3x \"\"\"", "\n", "model", "=", "_create_ghostnet", "(", "'ghostnet_130'", ",", "width", "=", "1.3", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.ActConvBn.__init__": [[36, 42], ["torch.Module.__init__", "torch.ReLU", "torch.ReLU", "torch.ReLU", "layers.create_conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "padding", "=", "''", ")", ":", "\n", "        ", "super", "(", "ActConvBn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "act", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "conv", "=", "create_conv2d", "(", "\n", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ")", "\n", "self", ".", "bn", "=", "nn", ".", "BatchNorm2d", "(", "out_channels", ",", "eps", "=", "0.001", ",", "momentum", "=", "0.1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.ActConvBn.forward": [[43, 48], ["nasnet.ActConvBn.act", "nasnet.ActConvBn.conv", "nasnet.ActConvBn.bn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "x", "=", "self", ".", "bn", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.SeparableConv2d.__init__": [[52, 59], ["torch.Module.__init__", "layers.create_conv2d", "layers.create_conv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", ",", "padding", "=", "''", ")", ":", "\n", "        ", "super", "(", "SeparableConv2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "depthwise_conv2d", "=", "create_conv2d", "(", "\n", "in_channels", ",", "in_channels", ",", "kernel_size", "=", "kernel_size", ",", "\n", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "groups", "=", "in_channels", ")", "\n", "self", ".", "pointwise_conv2d", "=", "create_conv2d", "(", "\n", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "1", ",", "padding", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.SeparableConv2d.forward": [[60, 64], ["nasnet.SeparableConv2d.depthwise_conv2d", "nasnet.SeparableConv2d.pointwise_conv2d"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "depthwise_conv2d", "(", "x", ")", "\n", "x", "=", "self", ".", "pointwise_conv2d", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.BranchSeparables.__init__": [[68, 79], ["torch.Module.__init__", "torch.ReLU", "torch.ReLU", "torch.ReLU", "nasnet.SeparableConv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "nasnet.SeparableConv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "pad_type", "=", "''", ",", "stem_cell", "=", "False", ")", ":", "\n", "        ", "super", "(", "BranchSeparables", ",", "self", ")", ".", "__init__", "(", ")", "\n", "middle_channels", "=", "out_channels", "if", "stem_cell", "else", "in_channels", "\n", "self", ".", "act_1", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "separable_1", "=", "SeparableConv2d", "(", "\n", "in_channels", ",", "middle_channels", ",", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "bn_sep_1", "=", "nn", ".", "BatchNorm2d", "(", "middle_channels", ",", "eps", "=", "0.001", ",", "momentum", "=", "0.1", ")", "\n", "self", ".", "act_2", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "separable_2", "=", "SeparableConv2d", "(", "\n", "middle_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "bn_sep_2", "=", "nn", ".", "BatchNorm2d", "(", "out_channels", ",", "eps", "=", "0.001", ",", "momentum", "=", "0.1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.BranchSeparables.forward": [[80, 88], ["nasnet.BranchSeparables.act_1", "nasnet.BranchSeparables.separable_1", "nasnet.BranchSeparables.bn_sep_1", "nasnet.BranchSeparables.act_2", "nasnet.BranchSeparables.separable_2", "nasnet.BranchSeparables.bn_sep_2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "act_1", "(", "x", ")", "\n", "x", "=", "self", ".", "separable_1", "(", "x", ")", "\n", "x", "=", "self", ".", "bn_sep_1", "(", "x", ")", "\n", "x", "=", "self", ".", "act_2", "(", "x", ")", "\n", "x", "=", "self", ".", "separable_2", "(", "x", ")", "\n", "x", "=", "self", ".", "bn_sep_2", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.CellStem0.__init__": [[91, 110], ["torch.Module.__init__", "nasnet.ActConvBn", "nasnet.BranchSeparables", "nasnet.BranchSeparables", "layers.create_pool2d", "nasnet.BranchSeparables", "layers.create_pool2d", "nasnet.BranchSeparables", "layers.create_pool2d", "nasnet.BranchSeparables", "layers.create_pool2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d"], ["    ", "def", "__init__", "(", "self", ",", "stem_size", ",", "num_channels", "=", "42", ",", "pad_type", "=", "''", ")", ":", "\n", "        ", "super", "(", "CellStem0", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_channels", "=", "num_channels", "\n", "self", ".", "stem_size", "=", "stem_size", "\n", "self", ".", "conv_1x1", "=", "ActConvBn", "(", "self", ".", "stem_size", ",", "self", ".", "num_channels", ",", "1", ",", "stride", "=", "1", ")", "\n", "\n", "self", ".", "comb_iter_0_left", "=", "BranchSeparables", "(", "self", ".", "num_channels", ",", "self", ".", "num_channels", ",", "5", ",", "2", ",", "pad_type", ")", "\n", "self", ".", "comb_iter_0_right", "=", "BranchSeparables", "(", "self", ".", "stem_size", ",", "self", ".", "num_channels", ",", "7", ",", "2", ",", "pad_type", ",", "stem_cell", "=", "True", ")", "\n", "\n", "self", ".", "comb_iter_1_left", "=", "create_pool2d", "(", "'max'", ",", "3", ",", "2", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "comb_iter_1_right", "=", "BranchSeparables", "(", "self", ".", "stem_size", ",", "self", ".", "num_channels", ",", "7", ",", "2", ",", "pad_type", ",", "stem_cell", "=", "True", ")", "\n", "\n", "self", ".", "comb_iter_2_left", "=", "create_pool2d", "(", "'avg'", ",", "3", ",", "2", ",", "count_include_pad", "=", "False", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "comb_iter_2_right", "=", "BranchSeparables", "(", "self", ".", "stem_size", ",", "self", ".", "num_channels", ",", "5", ",", "2", ",", "pad_type", ",", "stem_cell", "=", "True", ")", "\n", "\n", "self", ".", "comb_iter_3_right", "=", "create_pool2d", "(", "'avg'", ",", "3", ",", "1", ",", "count_include_pad", "=", "False", ",", "padding", "=", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_4_left", "=", "BranchSeparables", "(", "self", ".", "num_channels", ",", "self", ".", "num_channels", ",", "3", ",", "1", ",", "pad_type", ")", "\n", "self", ".", "comb_iter_4_right", "=", "create_pool2d", "(", "'max'", ",", "3", ",", "2", ",", "padding", "=", "pad_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.CellStem0.forward": [[111, 135], ["nasnet.CellStem0.conv_1x1", "nasnet.CellStem0.comb_iter_0_left", "nasnet.CellStem0.comb_iter_0_right", "nasnet.CellStem0.comb_iter_1_left", "nasnet.CellStem0.comb_iter_1_right", "nasnet.CellStem0.comb_iter_2_left", "nasnet.CellStem0.comb_iter_2_right", "nasnet.CellStem0.comb_iter_3_right", "nasnet.CellStem0.comb_iter_4_left", "nasnet.CellStem0.comb_iter_4_right", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x1", "=", "self", ".", "conv_1x1", "(", "x", ")", "\n", "\n", "x_comb_iter_0_left", "=", "self", ".", "comb_iter_0_left", "(", "x1", ")", "\n", "x_comb_iter_0_right", "=", "self", ".", "comb_iter_0_right", "(", "x", ")", "\n", "x_comb_iter_0", "=", "x_comb_iter_0_left", "+", "x_comb_iter_0_right", "\n", "\n", "x_comb_iter_1_left", "=", "self", ".", "comb_iter_1_left", "(", "x1", ")", "\n", "x_comb_iter_1_right", "=", "self", ".", "comb_iter_1_right", "(", "x", ")", "\n", "x_comb_iter_1", "=", "x_comb_iter_1_left", "+", "x_comb_iter_1_right", "\n", "\n", "x_comb_iter_2_left", "=", "self", ".", "comb_iter_2_left", "(", "x1", ")", "\n", "x_comb_iter_2_right", "=", "self", ".", "comb_iter_2_right", "(", "x", ")", "\n", "x_comb_iter_2", "=", "x_comb_iter_2_left", "+", "x_comb_iter_2_right", "\n", "\n", "x_comb_iter_3_right", "=", "self", ".", "comb_iter_3_right", "(", "x_comb_iter_0", ")", "\n", "x_comb_iter_3", "=", "x_comb_iter_3_right", "+", "x_comb_iter_1", "\n", "\n", "x_comb_iter_4_left", "=", "self", ".", "comb_iter_4_left", "(", "x_comb_iter_0", ")", "\n", "x_comb_iter_4_right", "=", "self", ".", "comb_iter_4_right", "(", "x1", ")", "\n", "x_comb_iter_4", "=", "x_comb_iter_4_left", "+", "x_comb_iter_4_right", "\n", "\n", "x_out", "=", "torch", ".", "cat", "(", "[", "x_comb_iter_1", ",", "x_comb_iter_2", ",", "x_comb_iter_3", ",", "x_comb_iter_4", "]", ",", "1", ")", "\n", "return", "x_out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.CellStem1.__init__": [[139, 170], ["torch.Module.__init__", "nasnet.ActConvBn", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Sequential", "torch.Sequential", "torch.Sequential", "nasnet.CellStem1.path_1.add_module", "nasnet.CellStem1.path_1.add_module", "torch.Sequential", "torch.Sequential", "torch.Sequential", "nasnet.CellStem1.path_2.add_module", "nasnet.CellStem1.path_2.add_module", "nasnet.CellStem1.path_2.add_module", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "nasnet.BranchSeparables", "nasnet.BranchSeparables", "layers.create_pool2d", "nasnet.BranchSeparables", "layers.create_pool2d", "nasnet.BranchSeparables", "layers.create_pool2d", "nasnet.BranchSeparables", "layers.create_pool2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ZeroPad2d", "torch.ZeroPad2d", "torch.ZeroPad2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d"], ["    ", "def", "__init__", "(", "self", ",", "stem_size", ",", "num_channels", ",", "pad_type", "=", "''", ")", ":", "\n", "        ", "super", "(", "CellStem1", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_channels", "=", "num_channels", "\n", "self", ".", "stem_size", "=", "stem_size", "\n", "self", ".", "conv_1x1", "=", "ActConvBn", "(", "2", "*", "self", ".", "num_channels", ",", "self", ".", "num_channels", ",", "1", ",", "stride", "=", "1", ")", "\n", "\n", "self", ".", "act", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "path_1", "=", "nn", ".", "Sequential", "(", ")", "\n", "self", ".", "path_1", ".", "add_module", "(", "'avgpool'", ",", "nn", ".", "AvgPool2d", "(", "1", ",", "stride", "=", "2", ",", "count_include_pad", "=", "False", ")", ")", "\n", "self", ".", "path_1", ".", "add_module", "(", "'conv'", ",", "nn", ".", "Conv2d", "(", "self", ".", "stem_size", ",", "self", ".", "num_channels", "//", "2", ",", "1", ",", "stride", "=", "1", ",", "bias", "=", "False", ")", ")", "\n", "\n", "self", ".", "path_2", "=", "nn", ".", "Sequential", "(", ")", "\n", "self", ".", "path_2", ".", "add_module", "(", "'pad'", ",", "nn", ".", "ZeroPad2d", "(", "(", "-", "1", ",", "1", ",", "-", "1", ",", "1", ")", ")", ")", "\n", "self", ".", "path_2", ".", "add_module", "(", "'avgpool'", ",", "nn", ".", "AvgPool2d", "(", "1", ",", "stride", "=", "2", ",", "count_include_pad", "=", "False", ")", ")", "\n", "self", ".", "path_2", ".", "add_module", "(", "'conv'", ",", "nn", ".", "Conv2d", "(", "self", ".", "stem_size", ",", "self", ".", "num_channels", "//", "2", ",", "1", ",", "stride", "=", "1", ",", "bias", "=", "False", ")", ")", "\n", "\n", "self", ".", "final_path_bn", "=", "nn", ".", "BatchNorm2d", "(", "self", ".", "num_channels", ",", "eps", "=", "0.001", ",", "momentum", "=", "0.1", ")", "\n", "\n", "self", ".", "comb_iter_0_left", "=", "BranchSeparables", "(", "self", ".", "num_channels", ",", "self", ".", "num_channels", ",", "5", ",", "2", ",", "pad_type", ")", "\n", "self", ".", "comb_iter_0_right", "=", "BranchSeparables", "(", "self", ".", "num_channels", ",", "self", ".", "num_channels", ",", "7", ",", "2", ",", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_1_left", "=", "create_pool2d", "(", "'max'", ",", "3", ",", "2", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "comb_iter_1_right", "=", "BranchSeparables", "(", "self", ".", "num_channels", ",", "self", ".", "num_channels", ",", "7", ",", "2", ",", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_2_left", "=", "create_pool2d", "(", "'avg'", ",", "3", ",", "2", ",", "count_include_pad", "=", "False", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "comb_iter_2_right", "=", "BranchSeparables", "(", "self", ".", "num_channels", ",", "self", ".", "num_channels", ",", "5", ",", "2", ",", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_3_right", "=", "create_pool2d", "(", "'avg'", ",", "3", ",", "1", ",", "count_include_pad", "=", "False", ",", "padding", "=", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_4_left", "=", "BranchSeparables", "(", "self", ".", "num_channels", ",", "self", ".", "num_channels", ",", "3", ",", "1", ",", "pad_type", ")", "\n", "self", ".", "comb_iter_4_right", "=", "create_pool2d", "(", "'max'", ",", "3", ",", "2", ",", "padding", "=", "pad_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.CellStem1.forward": [[171, 203], ["nasnet.CellStem1.conv_1x1", "nasnet.CellStem1.act", "nasnet.CellStem1.path_1", "nasnet.CellStem1.path_2", "nasnet.CellStem1.final_path_bn", "nasnet.CellStem1.comb_iter_0_left", "nasnet.CellStem1.comb_iter_0_right", "nasnet.CellStem1.comb_iter_1_left", "nasnet.CellStem1.comb_iter_1_right", "nasnet.CellStem1.comb_iter_2_left", "nasnet.CellStem1.comb_iter_2_right", "nasnet.CellStem1.comb_iter_3_right", "nasnet.CellStem1.comb_iter_4_left", "nasnet.CellStem1.comb_iter_4_right", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x_conv0", ",", "x_stem_0", ")", ":", "\n", "        ", "x_left", "=", "self", ".", "conv_1x1", "(", "x_stem_0", ")", "\n", "\n", "x_relu", "=", "self", ".", "act", "(", "x_conv0", ")", "\n", "# path 1", "\n", "x_path1", "=", "self", ".", "path_1", "(", "x_relu", ")", "\n", "# path 2", "\n", "x_path2", "=", "self", ".", "path_2", "(", "x_relu", ")", "\n", "# final path", "\n", "x_right", "=", "self", ".", "final_path_bn", "(", "torch", ".", "cat", "(", "[", "x_path1", ",", "x_path2", "]", ",", "1", ")", ")", "\n", "\n", "x_comb_iter_0_left", "=", "self", ".", "comb_iter_0_left", "(", "x_left", ")", "\n", "x_comb_iter_0_right", "=", "self", ".", "comb_iter_0_right", "(", "x_right", ")", "\n", "x_comb_iter_0", "=", "x_comb_iter_0_left", "+", "x_comb_iter_0_right", "\n", "\n", "x_comb_iter_1_left", "=", "self", ".", "comb_iter_1_left", "(", "x_left", ")", "\n", "x_comb_iter_1_right", "=", "self", ".", "comb_iter_1_right", "(", "x_right", ")", "\n", "x_comb_iter_1", "=", "x_comb_iter_1_left", "+", "x_comb_iter_1_right", "\n", "\n", "x_comb_iter_2_left", "=", "self", ".", "comb_iter_2_left", "(", "x_left", ")", "\n", "x_comb_iter_2_right", "=", "self", ".", "comb_iter_2_right", "(", "x_right", ")", "\n", "x_comb_iter_2", "=", "x_comb_iter_2_left", "+", "x_comb_iter_2_right", "\n", "\n", "x_comb_iter_3_right", "=", "self", ".", "comb_iter_3_right", "(", "x_comb_iter_0", ")", "\n", "x_comb_iter_3", "=", "x_comb_iter_3_right", "+", "x_comb_iter_1", "\n", "\n", "x_comb_iter_4_left", "=", "self", ".", "comb_iter_4_left", "(", "x_comb_iter_0", ")", "\n", "x_comb_iter_4_right", "=", "self", ".", "comb_iter_4_right", "(", "x_left", ")", "\n", "x_comb_iter_4", "=", "x_comb_iter_4_left", "+", "x_comb_iter_4_right", "\n", "\n", "x_out", "=", "torch", ".", "cat", "(", "[", "x_comb_iter_1", ",", "x_comb_iter_2", ",", "x_comb_iter_3", ",", "x_comb_iter_4", "]", ",", "1", ")", "\n", "return", "x_out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.FirstCell.__init__": [[207, 235], ["torch.Module.__init__", "nasnet.ActConvBn", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Sequential", "torch.Sequential", "torch.Sequential", "nasnet.FirstCell.path_1.add_module", "nasnet.FirstCell.path_1.add_module", "torch.Sequential", "torch.Sequential", "torch.Sequential", "nasnet.FirstCell.path_2.add_module", "nasnet.FirstCell.path_2.add_module", "nasnet.FirstCell.path_2.add_module", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "nasnet.BranchSeparables", "nasnet.BranchSeparables", "nasnet.BranchSeparables", "nasnet.BranchSeparables", "layers.create_pool2d", "layers.create_pool2d", "layers.create_pool2d", "nasnet.BranchSeparables", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ZeroPad2d", "torch.ZeroPad2d", "torch.ZeroPad2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d"], ["    ", "def", "__init__", "(", "self", ",", "in_chs_left", ",", "out_chs_left", ",", "in_chs_right", ",", "out_chs_right", ",", "pad_type", "=", "''", ")", ":", "\n", "        ", "super", "(", "FirstCell", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv_1x1", "=", "ActConvBn", "(", "in_chs_right", ",", "out_chs_right", ",", "1", ",", "stride", "=", "1", ")", "\n", "\n", "self", ".", "act", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "path_1", "=", "nn", ".", "Sequential", "(", ")", "\n", "self", ".", "path_1", ".", "add_module", "(", "'avgpool'", ",", "nn", ".", "AvgPool2d", "(", "1", ",", "stride", "=", "2", ",", "count_include_pad", "=", "False", ")", ")", "\n", "self", ".", "path_1", ".", "add_module", "(", "'conv'", ",", "nn", ".", "Conv2d", "(", "in_chs_left", ",", "out_chs_left", ",", "1", ",", "stride", "=", "1", ",", "bias", "=", "False", ")", ")", "\n", "\n", "self", ".", "path_2", "=", "nn", ".", "Sequential", "(", ")", "\n", "self", ".", "path_2", ".", "add_module", "(", "'pad'", ",", "nn", ".", "ZeroPad2d", "(", "(", "-", "1", ",", "1", ",", "-", "1", ",", "1", ")", ")", ")", "\n", "self", ".", "path_2", ".", "add_module", "(", "'avgpool'", ",", "nn", ".", "AvgPool2d", "(", "1", ",", "stride", "=", "2", ",", "count_include_pad", "=", "False", ")", ")", "\n", "self", ".", "path_2", ".", "add_module", "(", "'conv'", ",", "nn", ".", "Conv2d", "(", "in_chs_left", ",", "out_chs_left", ",", "1", ",", "stride", "=", "1", ",", "bias", "=", "False", ")", ")", "\n", "\n", "self", ".", "final_path_bn", "=", "nn", ".", "BatchNorm2d", "(", "out_chs_left", "*", "2", ",", "eps", "=", "0.001", ",", "momentum", "=", "0.1", ")", "\n", "\n", "self", ".", "comb_iter_0_left", "=", "BranchSeparables", "(", "out_chs_right", ",", "out_chs_right", ",", "5", ",", "1", ",", "pad_type", ")", "\n", "self", ".", "comb_iter_0_right", "=", "BranchSeparables", "(", "out_chs_right", ",", "out_chs_right", ",", "3", ",", "1", ",", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_1_left", "=", "BranchSeparables", "(", "out_chs_right", ",", "out_chs_right", ",", "5", ",", "1", ",", "pad_type", ")", "\n", "self", ".", "comb_iter_1_right", "=", "BranchSeparables", "(", "out_chs_right", ",", "out_chs_right", ",", "3", ",", "1", ",", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_2_left", "=", "create_pool2d", "(", "'avg'", ",", "3", ",", "1", ",", "count_include_pad", "=", "False", ",", "padding", "=", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_3_left", "=", "create_pool2d", "(", "'avg'", ",", "3", ",", "1", ",", "count_include_pad", "=", "False", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "comb_iter_3_right", "=", "create_pool2d", "(", "'avg'", ",", "3", ",", "1", ",", "count_include_pad", "=", "False", ",", "padding", "=", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_4_left", "=", "BranchSeparables", "(", "out_chs_right", ",", "out_chs_right", ",", "3", ",", "1", ",", "pad_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.FirstCell.forward": [[236, 263], ["nasnet.FirstCell.act", "nasnet.FirstCell.path_1", "nasnet.FirstCell.path_2", "nasnet.FirstCell.final_path_bn", "nasnet.FirstCell.conv_1x1", "nasnet.FirstCell.comb_iter_0_left", "nasnet.FirstCell.comb_iter_0_right", "nasnet.FirstCell.comb_iter_1_left", "nasnet.FirstCell.comb_iter_1_right", "nasnet.FirstCell.comb_iter_2_left", "nasnet.FirstCell.comb_iter_3_left", "nasnet.FirstCell.comb_iter_3_right", "nasnet.FirstCell.comb_iter_4_left", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "x_prev", ")", ":", "\n", "        ", "x_relu", "=", "self", ".", "act", "(", "x_prev", ")", "\n", "x_path1", "=", "self", ".", "path_1", "(", "x_relu", ")", "\n", "x_path2", "=", "self", ".", "path_2", "(", "x_relu", ")", "\n", "x_left", "=", "self", ".", "final_path_bn", "(", "torch", ".", "cat", "(", "[", "x_path1", ",", "x_path2", "]", ",", "1", ")", ")", "\n", "x_right", "=", "self", ".", "conv_1x1", "(", "x", ")", "\n", "\n", "x_comb_iter_0_left", "=", "self", ".", "comb_iter_0_left", "(", "x_right", ")", "\n", "x_comb_iter_0_right", "=", "self", ".", "comb_iter_0_right", "(", "x_left", ")", "\n", "x_comb_iter_0", "=", "x_comb_iter_0_left", "+", "x_comb_iter_0_right", "\n", "\n", "x_comb_iter_1_left", "=", "self", ".", "comb_iter_1_left", "(", "x_left", ")", "\n", "x_comb_iter_1_right", "=", "self", ".", "comb_iter_1_right", "(", "x_left", ")", "\n", "x_comb_iter_1", "=", "x_comb_iter_1_left", "+", "x_comb_iter_1_right", "\n", "\n", "x_comb_iter_2_left", "=", "self", ".", "comb_iter_2_left", "(", "x_right", ")", "\n", "x_comb_iter_2", "=", "x_comb_iter_2_left", "+", "x_left", "\n", "\n", "x_comb_iter_3_left", "=", "self", ".", "comb_iter_3_left", "(", "x_left", ")", "\n", "x_comb_iter_3_right", "=", "self", ".", "comb_iter_3_right", "(", "x_left", ")", "\n", "x_comb_iter_3", "=", "x_comb_iter_3_left", "+", "x_comb_iter_3_right", "\n", "\n", "x_comb_iter_4_left", "=", "self", ".", "comb_iter_4_left", "(", "x_right", ")", "\n", "x_comb_iter_4", "=", "x_comb_iter_4_left", "+", "x_right", "\n", "\n", "x_out", "=", "torch", ".", "cat", "(", "[", "x_left", ",", "x_comb_iter_0", ",", "x_comb_iter_1", ",", "x_comb_iter_2", ",", "x_comb_iter_3", ",", "x_comb_iter_4", "]", ",", "1", ")", "\n", "return", "x_out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.NormalCell.__init__": [[267, 284], ["torch.Module.__init__", "nasnet.ActConvBn", "nasnet.ActConvBn", "nasnet.BranchSeparables", "nasnet.BranchSeparables", "nasnet.BranchSeparables", "nasnet.BranchSeparables", "layers.create_pool2d", "layers.create_pool2d", "layers.create_pool2d", "nasnet.BranchSeparables"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d"], ["    ", "def", "__init__", "(", "self", ",", "in_chs_left", ",", "out_chs_left", ",", "in_chs_right", ",", "out_chs_right", ",", "pad_type", "=", "''", ")", ":", "\n", "        ", "super", "(", "NormalCell", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv_prev_1x1", "=", "ActConvBn", "(", "in_chs_left", ",", "out_chs_left", ",", "1", ",", "stride", "=", "1", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "conv_1x1", "=", "ActConvBn", "(", "in_chs_right", ",", "out_chs_right", ",", "1", ",", "stride", "=", "1", ",", "padding", "=", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_0_left", "=", "BranchSeparables", "(", "out_chs_right", ",", "out_chs_right", ",", "5", ",", "1", ",", "pad_type", ")", "\n", "self", ".", "comb_iter_0_right", "=", "BranchSeparables", "(", "out_chs_left", ",", "out_chs_left", ",", "3", ",", "1", ",", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_1_left", "=", "BranchSeparables", "(", "out_chs_left", ",", "out_chs_left", ",", "5", ",", "1", ",", "pad_type", ")", "\n", "self", ".", "comb_iter_1_right", "=", "BranchSeparables", "(", "out_chs_left", ",", "out_chs_left", ",", "3", ",", "1", ",", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_2_left", "=", "create_pool2d", "(", "'avg'", ",", "3", ",", "1", ",", "count_include_pad", "=", "False", ",", "padding", "=", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_3_left", "=", "create_pool2d", "(", "'avg'", ",", "3", ",", "1", ",", "count_include_pad", "=", "False", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "comb_iter_3_right", "=", "create_pool2d", "(", "'avg'", ",", "3", ",", "1", ",", "count_include_pad", "=", "False", ",", "padding", "=", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_4_left", "=", "BranchSeparables", "(", "out_chs_right", ",", "out_chs_right", ",", "3", ",", "1", ",", "pad_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.NormalCell.forward": [[285, 309], ["nasnet.NormalCell.conv_prev_1x1", "nasnet.NormalCell.conv_1x1", "nasnet.NormalCell.comb_iter_0_left", "nasnet.NormalCell.comb_iter_0_right", "nasnet.NormalCell.comb_iter_1_left", "nasnet.NormalCell.comb_iter_1_right", "nasnet.NormalCell.comb_iter_2_left", "nasnet.NormalCell.comb_iter_3_left", "nasnet.NormalCell.comb_iter_3_right", "nasnet.NormalCell.comb_iter_4_left", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "x_prev", ")", ":", "\n", "        ", "x_left", "=", "self", ".", "conv_prev_1x1", "(", "x_prev", ")", "\n", "x_right", "=", "self", ".", "conv_1x1", "(", "x", ")", "\n", "\n", "x_comb_iter_0_left", "=", "self", ".", "comb_iter_0_left", "(", "x_right", ")", "\n", "x_comb_iter_0_right", "=", "self", ".", "comb_iter_0_right", "(", "x_left", ")", "\n", "x_comb_iter_0", "=", "x_comb_iter_0_left", "+", "x_comb_iter_0_right", "\n", "\n", "x_comb_iter_1_left", "=", "self", ".", "comb_iter_1_left", "(", "x_left", ")", "\n", "x_comb_iter_1_right", "=", "self", ".", "comb_iter_1_right", "(", "x_left", ")", "\n", "x_comb_iter_1", "=", "x_comb_iter_1_left", "+", "x_comb_iter_1_right", "\n", "\n", "x_comb_iter_2_left", "=", "self", ".", "comb_iter_2_left", "(", "x_right", ")", "\n", "x_comb_iter_2", "=", "x_comb_iter_2_left", "+", "x_left", "\n", "\n", "x_comb_iter_3_left", "=", "self", ".", "comb_iter_3_left", "(", "x_left", ")", "\n", "x_comb_iter_3_right", "=", "self", ".", "comb_iter_3_right", "(", "x_left", ")", "\n", "x_comb_iter_3", "=", "x_comb_iter_3_left", "+", "x_comb_iter_3_right", "\n", "\n", "x_comb_iter_4_left", "=", "self", ".", "comb_iter_4_left", "(", "x_right", ")", "\n", "x_comb_iter_4", "=", "x_comb_iter_4_left", "+", "x_right", "\n", "\n", "x_out", "=", "torch", ".", "cat", "(", "[", "x_left", ",", "x_comb_iter_0", ",", "x_comb_iter_1", ",", "x_comb_iter_2", ",", "x_comb_iter_3", ",", "x_comb_iter_4", "]", ",", "1", ")", "\n", "return", "x_out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.ReductionCell0.__init__": [[313, 331], ["torch.Module.__init__", "nasnet.ActConvBn", "nasnet.ActConvBn", "nasnet.BranchSeparables", "nasnet.BranchSeparables", "layers.create_pool2d", "nasnet.BranchSeparables", "layers.create_pool2d", "nasnet.BranchSeparables", "layers.create_pool2d", "nasnet.BranchSeparables", "layers.create_pool2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d"], ["    ", "def", "__init__", "(", "self", ",", "in_chs_left", ",", "out_chs_left", ",", "in_chs_right", ",", "out_chs_right", ",", "pad_type", "=", "''", ")", ":", "\n", "        ", "super", "(", "ReductionCell0", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv_prev_1x1", "=", "ActConvBn", "(", "in_chs_left", ",", "out_chs_left", ",", "1", ",", "stride", "=", "1", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "conv_1x1", "=", "ActConvBn", "(", "in_chs_right", ",", "out_chs_right", ",", "1", ",", "stride", "=", "1", ",", "padding", "=", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_0_left", "=", "BranchSeparables", "(", "out_chs_right", ",", "out_chs_right", ",", "5", ",", "2", ",", "pad_type", ")", "\n", "self", ".", "comb_iter_0_right", "=", "BranchSeparables", "(", "out_chs_right", ",", "out_chs_right", ",", "7", ",", "2", ",", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_1_left", "=", "create_pool2d", "(", "'max'", ",", "3", ",", "2", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "comb_iter_1_right", "=", "BranchSeparables", "(", "out_chs_right", ",", "out_chs_right", ",", "7", ",", "2", ",", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_2_left", "=", "create_pool2d", "(", "'avg'", ",", "3", ",", "2", ",", "count_include_pad", "=", "False", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "comb_iter_2_right", "=", "BranchSeparables", "(", "out_chs_right", ",", "out_chs_right", ",", "5", ",", "2", ",", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_3_right", "=", "create_pool2d", "(", "'avg'", ",", "3", ",", "1", ",", "count_include_pad", "=", "False", ",", "padding", "=", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_4_left", "=", "BranchSeparables", "(", "out_chs_right", ",", "out_chs_right", ",", "3", ",", "1", ",", "pad_type", ")", "\n", "self", ".", "comb_iter_4_right", "=", "create_pool2d", "(", "'max'", ",", "3", ",", "2", ",", "padding", "=", "pad_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.ReductionCell0.forward": [[332, 357], ["nasnet.ReductionCell0.conv_prev_1x1", "nasnet.ReductionCell0.conv_1x1", "nasnet.ReductionCell0.comb_iter_0_left", "nasnet.ReductionCell0.comb_iter_0_right", "nasnet.ReductionCell0.comb_iter_1_left", "nasnet.ReductionCell0.comb_iter_1_right", "nasnet.ReductionCell0.comb_iter_2_left", "nasnet.ReductionCell0.comb_iter_2_right", "nasnet.ReductionCell0.comb_iter_3_right", "nasnet.ReductionCell0.comb_iter_4_left", "nasnet.ReductionCell0.comb_iter_4_right", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "x_prev", ")", ":", "\n", "        ", "x_left", "=", "self", ".", "conv_prev_1x1", "(", "x_prev", ")", "\n", "x_right", "=", "self", ".", "conv_1x1", "(", "x", ")", "\n", "\n", "x_comb_iter_0_left", "=", "self", ".", "comb_iter_0_left", "(", "x_right", ")", "\n", "x_comb_iter_0_right", "=", "self", ".", "comb_iter_0_right", "(", "x_left", ")", "\n", "x_comb_iter_0", "=", "x_comb_iter_0_left", "+", "x_comb_iter_0_right", "\n", "\n", "x_comb_iter_1_left", "=", "self", ".", "comb_iter_1_left", "(", "x_right", ")", "\n", "x_comb_iter_1_right", "=", "self", ".", "comb_iter_1_right", "(", "x_left", ")", "\n", "x_comb_iter_1", "=", "x_comb_iter_1_left", "+", "x_comb_iter_1_right", "\n", "\n", "x_comb_iter_2_left", "=", "self", ".", "comb_iter_2_left", "(", "x_right", ")", "\n", "x_comb_iter_2_right", "=", "self", ".", "comb_iter_2_right", "(", "x_left", ")", "\n", "x_comb_iter_2", "=", "x_comb_iter_2_left", "+", "x_comb_iter_2_right", "\n", "\n", "x_comb_iter_3_right", "=", "self", ".", "comb_iter_3_right", "(", "x_comb_iter_0", ")", "\n", "x_comb_iter_3", "=", "x_comb_iter_3_right", "+", "x_comb_iter_1", "\n", "\n", "x_comb_iter_4_left", "=", "self", ".", "comb_iter_4_left", "(", "x_comb_iter_0", ")", "\n", "x_comb_iter_4_right", "=", "self", ".", "comb_iter_4_right", "(", "x_right", ")", "\n", "x_comb_iter_4", "=", "x_comb_iter_4_left", "+", "x_comb_iter_4_right", "\n", "\n", "x_out", "=", "torch", ".", "cat", "(", "[", "x_comb_iter_1", ",", "x_comb_iter_2", ",", "x_comb_iter_3", ",", "x_comb_iter_4", "]", ",", "1", ")", "\n", "return", "x_out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.ReductionCell1.__init__": [[361, 379], ["torch.Module.__init__", "nasnet.ActConvBn", "nasnet.ActConvBn", "nasnet.BranchSeparables", "nasnet.BranchSeparables", "layers.create_pool2d", "nasnet.BranchSeparables", "layers.create_pool2d", "nasnet.BranchSeparables", "layers.create_pool2d", "nasnet.BranchSeparables", "layers.create_pool2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d"], ["    ", "def", "__init__", "(", "self", ",", "in_chs_left", ",", "out_chs_left", ",", "in_chs_right", ",", "out_chs_right", ",", "pad_type", "=", "''", ")", ":", "\n", "        ", "super", "(", "ReductionCell1", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv_prev_1x1", "=", "ActConvBn", "(", "in_chs_left", ",", "out_chs_left", ",", "1", ",", "stride", "=", "1", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "conv_1x1", "=", "ActConvBn", "(", "in_chs_right", ",", "out_chs_right", ",", "1", ",", "stride", "=", "1", ",", "padding", "=", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_0_left", "=", "BranchSeparables", "(", "out_chs_right", ",", "out_chs_right", ",", "5", ",", "2", ",", "pad_type", ")", "\n", "self", ".", "comb_iter_0_right", "=", "BranchSeparables", "(", "out_chs_right", ",", "out_chs_right", ",", "7", ",", "2", ",", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_1_left", "=", "create_pool2d", "(", "'max'", ",", "3", ",", "2", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "comb_iter_1_right", "=", "BranchSeparables", "(", "out_chs_right", ",", "out_chs_right", ",", "7", ",", "2", ",", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_2_left", "=", "create_pool2d", "(", "'avg'", ",", "3", ",", "2", ",", "count_include_pad", "=", "False", ",", "padding", "=", "pad_type", ")", "\n", "self", ".", "comb_iter_2_right", "=", "BranchSeparables", "(", "out_chs_right", ",", "out_chs_right", ",", "5", ",", "2", ",", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_3_right", "=", "create_pool2d", "(", "'avg'", ",", "3", ",", "1", ",", "count_include_pad", "=", "False", ",", "padding", "=", "pad_type", ")", "\n", "\n", "self", ".", "comb_iter_4_left", "=", "BranchSeparables", "(", "out_chs_right", ",", "out_chs_right", ",", "3", ",", "1", ",", "pad_type", ")", "\n", "self", ".", "comb_iter_4_right", "=", "create_pool2d", "(", "'max'", ",", "3", ",", "2", ",", "padding", "=", "pad_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.ReductionCell1.forward": [[380, 405], ["nasnet.ReductionCell1.conv_prev_1x1", "nasnet.ReductionCell1.conv_1x1", "nasnet.ReductionCell1.comb_iter_0_left", "nasnet.ReductionCell1.comb_iter_0_right", "nasnet.ReductionCell1.comb_iter_1_left", "nasnet.ReductionCell1.comb_iter_1_right", "nasnet.ReductionCell1.comb_iter_2_left", "nasnet.ReductionCell1.comb_iter_2_right", "nasnet.ReductionCell1.comb_iter_3_right", "nasnet.ReductionCell1.comb_iter_4_left", "nasnet.ReductionCell1.comb_iter_4_right", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "x_prev", ")", ":", "\n", "        ", "x_left", "=", "self", ".", "conv_prev_1x1", "(", "x_prev", ")", "\n", "x_right", "=", "self", ".", "conv_1x1", "(", "x", ")", "\n", "\n", "x_comb_iter_0_left", "=", "self", ".", "comb_iter_0_left", "(", "x_right", ")", "\n", "x_comb_iter_0_right", "=", "self", ".", "comb_iter_0_right", "(", "x_left", ")", "\n", "x_comb_iter_0", "=", "x_comb_iter_0_left", "+", "x_comb_iter_0_right", "\n", "\n", "x_comb_iter_1_left", "=", "self", ".", "comb_iter_1_left", "(", "x_right", ")", "\n", "x_comb_iter_1_right", "=", "self", ".", "comb_iter_1_right", "(", "x_left", ")", "\n", "x_comb_iter_1", "=", "x_comb_iter_1_left", "+", "x_comb_iter_1_right", "\n", "\n", "x_comb_iter_2_left", "=", "self", ".", "comb_iter_2_left", "(", "x_right", ")", "\n", "x_comb_iter_2_right", "=", "self", ".", "comb_iter_2_right", "(", "x_left", ")", "\n", "x_comb_iter_2", "=", "x_comb_iter_2_left", "+", "x_comb_iter_2_right", "\n", "\n", "x_comb_iter_3_right", "=", "self", ".", "comb_iter_3_right", "(", "x_comb_iter_0", ")", "\n", "x_comb_iter_3", "=", "x_comb_iter_3_right", "+", "x_comb_iter_1", "\n", "\n", "x_comb_iter_4_left", "=", "self", ".", "comb_iter_4_left", "(", "x_comb_iter_0", ")", "\n", "x_comb_iter_4_right", "=", "self", ".", "comb_iter_4_right", "(", "x_right", ")", "\n", "x_comb_iter_4", "=", "x_comb_iter_4_left", "+", "x_comb_iter_4_right", "\n", "\n", "x_out", "=", "torch", ".", "cat", "(", "[", "x_comb_iter_1", ",", "x_comb_iter_2", ",", "x_comb_iter_3", ",", "x_comb_iter_4", "]", ",", "1", ")", "\n", "return", "x_out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.NASNetALarge.__init__": [[410, 506], ["torch.Module.__init__", "layers.ConvNormAct", "nasnet.CellStem0", "nasnet.CellStem1", "nasnet.FirstCell", "nasnet.NormalCell", "nasnet.NormalCell", "nasnet.NormalCell", "nasnet.NormalCell", "nasnet.NormalCell", "nasnet.ReductionCell0", "nasnet.FirstCell", "nasnet.NormalCell", "nasnet.NormalCell", "nasnet.NormalCell", "nasnet.NormalCell", "nasnet.NormalCell", "nasnet.ReductionCell1", "nasnet.FirstCell", "nasnet.NormalCell", "nasnet.NormalCell", "nasnet.NormalCell", "nasnet.NormalCell", "nasnet.NormalCell", "torch.ReLU", "torch.ReLU", "torch.ReLU", "layers.create_classifier", "dict", "dict", "dict", "dict", "dict", "functools.partial"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["def", "__init__", "(", "\n", "self", ",", "num_classes", "=", "1000", ",", "in_chans", "=", "3", ",", "stem_size", "=", "96", ",", "channel_multiplier", "=", "2", ",", "\n", "num_features", "=", "4032", ",", "output_stride", "=", "32", ",", "drop_rate", "=", "0.", ",", "global_pool", "=", "'avg'", ",", "pad_type", "=", "'same'", ")", ":", "\n", "        ", "super", "(", "NASNetALarge", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "stem_size", "=", "stem_size", "\n", "self", ".", "num_features", "=", "num_features", "\n", "self", ".", "channel_multiplier", "=", "channel_multiplier", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "assert", "output_stride", "==", "32", "\n", "\n", "channels", "=", "self", ".", "num_features", "//", "24", "\n", "# 24 is default value for the architecture", "\n", "\n", "self", ".", "conv0", "=", "ConvNormAct", "(", "\n", "in_channels", "=", "in_chans", ",", "out_channels", "=", "self", ".", "stem_size", ",", "kernel_size", "=", "3", ",", "padding", "=", "0", ",", "stride", "=", "2", ",", "\n", "norm_layer", "=", "partial", "(", "nn", ".", "BatchNorm2d", ",", "eps", "=", "0.001", ",", "momentum", "=", "0.1", ")", ",", "apply_act", "=", "False", ")", "\n", "\n", "self", ".", "cell_stem_0", "=", "CellStem0", "(", "\n", "self", ".", "stem_size", ",", "num_channels", "=", "channels", "//", "(", "channel_multiplier", "**", "2", ")", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_stem_1", "=", "CellStem1", "(", "\n", "self", ".", "stem_size", ",", "num_channels", "=", "channels", "//", "channel_multiplier", ",", "pad_type", "=", "pad_type", ")", "\n", "\n", "self", ".", "cell_0", "=", "FirstCell", "(", "\n", "in_chs_left", "=", "channels", ",", "out_chs_left", "=", "channels", "//", "2", ",", "\n", "in_chs_right", "=", "2", "*", "channels", ",", "out_chs_right", "=", "channels", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_1", "=", "NormalCell", "(", "\n", "in_chs_left", "=", "2", "*", "channels", ",", "out_chs_left", "=", "channels", ",", "\n", "in_chs_right", "=", "6", "*", "channels", ",", "out_chs_right", "=", "channels", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_2", "=", "NormalCell", "(", "\n", "in_chs_left", "=", "6", "*", "channels", ",", "out_chs_left", "=", "channels", ",", "\n", "in_chs_right", "=", "6", "*", "channels", ",", "out_chs_right", "=", "channels", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_3", "=", "NormalCell", "(", "\n", "in_chs_left", "=", "6", "*", "channels", ",", "out_chs_left", "=", "channels", ",", "\n", "in_chs_right", "=", "6", "*", "channels", ",", "out_chs_right", "=", "channels", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_4", "=", "NormalCell", "(", "\n", "in_chs_left", "=", "6", "*", "channels", ",", "out_chs_left", "=", "channels", ",", "\n", "in_chs_right", "=", "6", "*", "channels", ",", "out_chs_right", "=", "channels", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_5", "=", "NormalCell", "(", "\n", "in_chs_left", "=", "6", "*", "channels", ",", "out_chs_left", "=", "channels", ",", "\n", "in_chs_right", "=", "6", "*", "channels", ",", "out_chs_right", "=", "channels", ",", "pad_type", "=", "pad_type", ")", "\n", "\n", "self", ".", "reduction_cell_0", "=", "ReductionCell0", "(", "\n", "in_chs_left", "=", "6", "*", "channels", ",", "out_chs_left", "=", "2", "*", "channels", ",", "\n", "in_chs_right", "=", "6", "*", "channels", ",", "out_chs_right", "=", "2", "*", "channels", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_6", "=", "FirstCell", "(", "\n", "in_chs_left", "=", "6", "*", "channels", ",", "out_chs_left", "=", "channels", ",", "\n", "in_chs_right", "=", "8", "*", "channels", ",", "out_chs_right", "=", "2", "*", "channels", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_7", "=", "NormalCell", "(", "\n", "in_chs_left", "=", "8", "*", "channels", ",", "out_chs_left", "=", "2", "*", "channels", ",", "\n", "in_chs_right", "=", "12", "*", "channels", ",", "out_chs_right", "=", "2", "*", "channels", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_8", "=", "NormalCell", "(", "\n", "in_chs_left", "=", "12", "*", "channels", ",", "out_chs_left", "=", "2", "*", "channels", ",", "\n", "in_chs_right", "=", "12", "*", "channels", ",", "out_chs_right", "=", "2", "*", "channels", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_9", "=", "NormalCell", "(", "\n", "in_chs_left", "=", "12", "*", "channels", ",", "out_chs_left", "=", "2", "*", "channels", ",", "\n", "in_chs_right", "=", "12", "*", "channels", ",", "out_chs_right", "=", "2", "*", "channels", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_10", "=", "NormalCell", "(", "\n", "in_chs_left", "=", "12", "*", "channels", ",", "out_chs_left", "=", "2", "*", "channels", ",", "\n", "in_chs_right", "=", "12", "*", "channels", ",", "out_chs_right", "=", "2", "*", "channels", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_11", "=", "NormalCell", "(", "\n", "in_chs_left", "=", "12", "*", "channels", ",", "out_chs_left", "=", "2", "*", "channels", ",", "\n", "in_chs_right", "=", "12", "*", "channels", ",", "out_chs_right", "=", "2", "*", "channels", ",", "pad_type", "=", "pad_type", ")", "\n", "\n", "self", ".", "reduction_cell_1", "=", "ReductionCell1", "(", "\n", "in_chs_left", "=", "12", "*", "channels", ",", "out_chs_left", "=", "4", "*", "channels", ",", "\n", "in_chs_right", "=", "12", "*", "channels", ",", "out_chs_right", "=", "4", "*", "channels", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_12", "=", "FirstCell", "(", "\n", "in_chs_left", "=", "12", "*", "channels", ",", "out_chs_left", "=", "2", "*", "channels", ",", "\n", "in_chs_right", "=", "16", "*", "channels", ",", "out_chs_right", "=", "4", "*", "channels", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_13", "=", "NormalCell", "(", "\n", "in_chs_left", "=", "16", "*", "channels", ",", "out_chs_left", "=", "4", "*", "channels", ",", "\n", "in_chs_right", "=", "24", "*", "channels", ",", "out_chs_right", "=", "4", "*", "channels", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_14", "=", "NormalCell", "(", "\n", "in_chs_left", "=", "24", "*", "channels", ",", "out_chs_left", "=", "4", "*", "channels", ",", "\n", "in_chs_right", "=", "24", "*", "channels", ",", "out_chs_right", "=", "4", "*", "channels", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_15", "=", "NormalCell", "(", "\n", "in_chs_left", "=", "24", "*", "channels", ",", "out_chs_left", "=", "4", "*", "channels", ",", "\n", "in_chs_right", "=", "24", "*", "channels", ",", "out_chs_right", "=", "4", "*", "channels", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_16", "=", "NormalCell", "(", "\n", "in_chs_left", "=", "24", "*", "channels", ",", "out_chs_left", "=", "4", "*", "channels", ",", "\n", "in_chs_right", "=", "24", "*", "channels", ",", "out_chs_right", "=", "4", "*", "channels", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "cell_17", "=", "NormalCell", "(", "\n", "in_chs_left", "=", "24", "*", "channels", ",", "out_chs_left", "=", "4", "*", "channels", ",", "\n", "in_chs_right", "=", "24", "*", "channels", ",", "out_chs_right", "=", "4", "*", "channels", ",", "pad_type", "=", "pad_type", ")", "\n", "self", ".", "act", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "feature_info", "=", "[", "\n", "dict", "(", "num_chs", "=", "96", ",", "reduction", "=", "2", ",", "module", "=", "'conv0'", ")", ",", "\n", "dict", "(", "num_chs", "=", "168", ",", "reduction", "=", "4", ",", "module", "=", "'cell_stem_1.conv_1x1.act'", ")", ",", "\n", "dict", "(", "num_chs", "=", "1008", ",", "reduction", "=", "8", ",", "module", "=", "'reduction_cell_0.conv_1x1.act'", ")", ",", "\n", "dict", "(", "num_chs", "=", "2016", ",", "reduction", "=", "16", ",", "module", "=", "'reduction_cell_1.conv_1x1.act'", ")", ",", "\n", "dict", "(", "num_chs", "=", "4032", ",", "reduction", "=", "32", ",", "module", "=", "'act'", ")", ",", "\n", "]", "\n", "\n", "self", ".", "global_pool", ",", "self", ".", "last_linear", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.NASNetALarge.group_matcher": [[507, 518], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "\n", "stem", "=", "r'^conv0|cell_stem_[01]'", ",", "\n", "blocks", "=", "[", "\n", "(", "r'^cell_(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^reduction_cell_0'", ",", "(", "6", ",", ")", ")", ",", "\n", "(", "r'^reduction_cell_1'", ",", "(", "12", ",", ")", ")", ",", "\n", "]", "\n", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.NASNetALarge.set_grad_checkpointing": [[519, 522], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "'gradient checkpointing not supported'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.NASNetALarge.get_classifier": [[523, 526], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "last_linear", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.NASNetALarge.reset_classifier": [[527, 531], ["layers.create_classifier"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", ",", "self", ".", "last_linear", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.NASNetALarge.forward_features": [[532, 562], ["nasnet.NASNetALarge.conv0", "nasnet.NASNetALarge.cell_stem_0", "nasnet.NASNetALarge.cell_stem_1", "nasnet.NASNetALarge.cell_0", "nasnet.NASNetALarge.cell_1", "nasnet.NASNetALarge.cell_2", "nasnet.NASNetALarge.cell_3", "nasnet.NASNetALarge.cell_4", "nasnet.NASNetALarge.cell_5", "nasnet.NASNetALarge.reduction_cell_0", "nasnet.NASNetALarge.cell_6", "nasnet.NASNetALarge.cell_7", "nasnet.NASNetALarge.cell_8", "nasnet.NASNetALarge.cell_9", "nasnet.NASNetALarge.cell_10", "nasnet.NASNetALarge.cell_11", "nasnet.NASNetALarge.reduction_cell_1", "nasnet.NASNetALarge.cell_12", "nasnet.NASNetALarge.cell_13", "nasnet.NASNetALarge.cell_14", "nasnet.NASNetALarge.cell_15", "nasnet.NASNetALarge.cell_16", "nasnet.NASNetALarge.cell_17", "nasnet.NASNetALarge.act"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_conv0", "=", "self", ".", "conv0", "(", "x", ")", "\n", "\n", "x_stem_0", "=", "self", ".", "cell_stem_0", "(", "x_conv0", ")", "\n", "x_stem_1", "=", "self", ".", "cell_stem_1", "(", "x_conv0", ",", "x_stem_0", ")", "\n", "\n", "x_cell_0", "=", "self", ".", "cell_0", "(", "x_stem_1", ",", "x_stem_0", ")", "\n", "x_cell_1", "=", "self", ".", "cell_1", "(", "x_cell_0", ",", "x_stem_1", ")", "\n", "x_cell_2", "=", "self", ".", "cell_2", "(", "x_cell_1", ",", "x_cell_0", ")", "\n", "x_cell_3", "=", "self", ".", "cell_3", "(", "x_cell_2", ",", "x_cell_1", ")", "\n", "x_cell_4", "=", "self", ".", "cell_4", "(", "x_cell_3", ",", "x_cell_2", ")", "\n", "x_cell_5", "=", "self", ".", "cell_5", "(", "x_cell_4", ",", "x_cell_3", ")", "\n", "\n", "x_reduction_cell_0", "=", "self", ".", "reduction_cell_0", "(", "x_cell_5", ",", "x_cell_4", ")", "\n", "x_cell_6", "=", "self", ".", "cell_6", "(", "x_reduction_cell_0", ",", "x_cell_4", ")", "\n", "x_cell_7", "=", "self", ".", "cell_7", "(", "x_cell_6", ",", "x_reduction_cell_0", ")", "\n", "x_cell_8", "=", "self", ".", "cell_8", "(", "x_cell_7", ",", "x_cell_6", ")", "\n", "x_cell_9", "=", "self", ".", "cell_9", "(", "x_cell_8", ",", "x_cell_7", ")", "\n", "x_cell_10", "=", "self", ".", "cell_10", "(", "x_cell_9", ",", "x_cell_8", ")", "\n", "x_cell_11", "=", "self", ".", "cell_11", "(", "x_cell_10", ",", "x_cell_9", ")", "\n", "\n", "x_reduction_cell_1", "=", "self", ".", "reduction_cell_1", "(", "x_cell_11", ",", "x_cell_10", ")", "\n", "x_cell_12", "=", "self", ".", "cell_12", "(", "x_reduction_cell_1", ",", "x_cell_10", ")", "\n", "x_cell_13", "=", "self", ".", "cell_13", "(", "x_cell_12", ",", "x_reduction_cell_1", ")", "\n", "x_cell_14", "=", "self", ".", "cell_14", "(", "x_cell_13", ",", "x_cell_12", ")", "\n", "x_cell_15", "=", "self", ".", "cell_15", "(", "x_cell_14", ",", "x_cell_13", ")", "\n", "x_cell_16", "=", "self", ".", "cell_16", "(", "x_cell_15", ",", "x_cell_14", ")", "\n", "x_cell_17", "=", "self", ".", "cell_17", "(", "x_cell_16", ",", "x_cell_15", ")", "\n", "x", "=", "self", ".", "act", "(", "x_cell_17", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.NASNetALarge.forward_head": [[563, 569], ["nasnet.NASNetALarge.global_pool", "nasnet.NASNetALarge.last_linear", "torch.dropout", "torch.dropout", "torch.dropout"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "if", "self", ".", "drop_rate", ">", "0", ":", "\n", "            ", "x", "=", "F", ".", "dropout", "(", "x", ",", "self", ".", "drop_rate", ",", "training", "=", "self", ".", "training", ")", "\n", "", "x", "=", "self", ".", "last_linear", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.NASNetALarge.forward": [[570, 574], ["nasnet.NASNetALarge.forward_features", "nasnet.NASNetALarge.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet._create_nasnet": [[576, 581], ["helpers.build_model_with_cfg", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_nasnet", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "\n", "NASNetALarge", ",", "variant", ",", "pretrained", ",", "\n", "feature_cfg", "=", "dict", "(", "feature_cls", "=", "'hook'", ",", "no_rewrite", "=", "True", ")", ",", "# not possible to re-write this model", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet.nasnetalarge": [[583, 589], ["dict", "nasnet._create_nasnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nasnet._create_nasnet"], ["", "@", "register_model", "\n", "def", "nasnetalarge", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"NASNet-A large model architecture.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "pad_type", "=", "'same'", ",", "**", "kwargs", ")", "\n", "return", "_create_nasnet", "(", "'nasnetalarge'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.BasicBlock.__init__": [[324, 356], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "act_layer", "resnet.create_aa", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "layers.create_attn", "act_layer", "drop_block", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.create_aa", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.create_attn"], ["def", "__init__", "(", "\n", "self", ",", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "downsample", "=", "None", ",", "cardinality", "=", "1", ",", "base_width", "=", "64", ",", "\n", "reduce_first", "=", "1", ",", "dilation", "=", "1", ",", "first_dilation", "=", "None", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "\n", "attn_layer", "=", "None", ",", "aa_layer", "=", "None", ",", "drop_block", "=", "None", ",", "drop_path", "=", "None", ")", ":", "\n", "        ", "super", "(", "BasicBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "cardinality", "==", "1", ",", "'BasicBlock only supports cardinality of 1'", "\n", "assert", "base_width", "==", "64", ",", "'BasicBlock does not support changing base width'", "\n", "first_planes", "=", "planes", "//", "reduce_first", "\n", "outplanes", "=", "planes", "*", "self", ".", "expansion", "\n", "first_dilation", "=", "first_dilation", "or", "dilation", "\n", "use_aa", "=", "aa_layer", "is", "not", "None", "and", "(", "stride", "==", "2", "or", "first_dilation", "!=", "dilation", ")", "\n", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "\n", "inplanes", ",", "first_planes", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", "if", "use_aa", "else", "stride", ",", "padding", "=", "first_dilation", ",", "\n", "dilation", "=", "first_dilation", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "first_planes", ")", "\n", "self", ".", "drop_block", "=", "drop_block", "(", ")", "if", "drop_block", "is", "not", "None", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "act1", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "aa", "=", "create_aa", "(", "aa_layer", ",", "channels", "=", "first_planes", ",", "stride", "=", "stride", ",", "enable", "=", "use_aa", ")", "\n", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "\n", "first_planes", ",", "outplanes", ",", "kernel_size", "=", "3", ",", "padding", "=", "dilation", ",", "dilation", "=", "dilation", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn2", "=", "norm_layer", "(", "outplanes", ")", "\n", "\n", "self", ".", "se", "=", "create_attn", "(", "attn_layer", ",", "outplanes", ")", "\n", "\n", "self", ".", "act2", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "dilation", "=", "dilation", "\n", "self", ".", "drop_path", "=", "drop_path", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.BasicBlock.zero_init_last": [[357, 359], ["torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_"], "methods", ["None"], ["", "def", "zero_init_last", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "bn2", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.BasicBlock.forward": [[360, 384], ["resnet.BasicBlock.conv1", "resnet.BasicBlock.bn1", "resnet.BasicBlock.drop_block", "resnet.BasicBlock.act1", "resnet.BasicBlock.aa", "resnet.BasicBlock.conv2", "resnet.BasicBlock.bn2", "resnet.BasicBlock.act2", "resnet.BasicBlock.se", "resnet.BasicBlock.drop_path", "resnet.BasicBlock.downsample"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "x", "=", "self", ".", "drop_block", "(", "x", ")", "\n", "x", "=", "self", ".", "act1", "(", "x", ")", "\n", "x", "=", "self", ".", "aa", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "x", "=", "self", ".", "bn2", "(", "x", ")", "\n", "\n", "if", "self", ".", "se", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "se", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "drop_path", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "drop_path", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "shortcut", "=", "self", ".", "downsample", "(", "shortcut", ")", "\n", "", "x", "+=", "shortcut", "\n", "x", "=", "self", ".", "act2", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.Bottleneck.__init__": [[389, 423], ["torch.Module.__init__", "int", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "act_layer", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "act_layer", "resnet.create_aa", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "layers.create_attn", "act_layer", "drop_block", "torch.Identity", "torch.Identity", "torch.Identity", "math.floor"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.create_aa", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.create_attn"], ["def", "__init__", "(", "\n", "self", ",", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "downsample", "=", "None", ",", "cardinality", "=", "1", ",", "base_width", "=", "64", ",", "\n", "reduce_first", "=", "1", ",", "dilation", "=", "1", ",", "first_dilation", "=", "None", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "\n", "attn_layer", "=", "None", ",", "aa_layer", "=", "None", ",", "drop_block", "=", "None", ",", "drop_path", "=", "None", ")", ":", "\n", "        ", "super", "(", "Bottleneck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "width", "=", "int", "(", "math", ".", "floor", "(", "planes", "*", "(", "base_width", "/", "64", ")", ")", "*", "cardinality", ")", "\n", "first_planes", "=", "width", "//", "reduce_first", "\n", "outplanes", "=", "planes", "*", "self", ".", "expansion", "\n", "first_dilation", "=", "first_dilation", "or", "dilation", "\n", "use_aa", "=", "aa_layer", "is", "not", "None", "and", "(", "stride", "==", "2", "or", "first_dilation", "!=", "dilation", ")", "\n", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "inplanes", ",", "first_planes", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "first_planes", ")", "\n", "self", ".", "act1", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "\n", "first_planes", ",", "width", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", "if", "use_aa", "else", "stride", ",", "\n", "padding", "=", "first_dilation", ",", "dilation", "=", "first_dilation", ",", "groups", "=", "cardinality", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn2", "=", "norm_layer", "(", "width", ")", "\n", "self", ".", "drop_block", "=", "drop_block", "(", ")", "if", "drop_block", "is", "not", "None", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "act2", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "aa", "=", "create_aa", "(", "aa_layer", ",", "channels", "=", "width", ",", "stride", "=", "stride", ",", "enable", "=", "use_aa", ")", "\n", "\n", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "width", ",", "outplanes", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn3", "=", "norm_layer", "(", "outplanes", ")", "\n", "\n", "self", ".", "se", "=", "create_attn", "(", "attn_layer", ",", "outplanes", ")", "\n", "\n", "self", ".", "act3", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "dilation", "=", "dilation", "\n", "self", ".", "drop_path", "=", "drop_path", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.Bottleneck.zero_init_last": [[424, 426], ["torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_"], "methods", ["None"], ["", "def", "zero_init_last", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "bn3", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.Bottleneck.forward": [[427, 455], ["resnet.Bottleneck.conv1", "resnet.Bottleneck.bn1", "resnet.Bottleneck.act1", "resnet.Bottleneck.conv2", "resnet.Bottleneck.bn2", "resnet.Bottleneck.drop_block", "resnet.Bottleneck.act2", "resnet.Bottleneck.aa", "resnet.Bottleneck.conv3", "resnet.Bottleneck.bn3", "resnet.Bottleneck.act3", "resnet.Bottleneck.se", "resnet.Bottleneck.drop_path", "resnet.Bottleneck.downsample"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "x", "=", "self", ".", "act1", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "x", "=", "self", ".", "bn2", "(", "x", ")", "\n", "x", "=", "self", ".", "drop_block", "(", "x", ")", "\n", "x", "=", "self", ".", "act2", "(", "x", ")", "\n", "x", "=", "self", ".", "aa", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "conv3", "(", "x", ")", "\n", "x", "=", "self", ".", "bn3", "(", "x", ")", "\n", "\n", "if", "self", ".", "se", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "se", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "drop_path", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "drop_path", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "shortcut", "=", "self", ".", "downsample", "(", "shortcut", ")", "\n", "", "x", "+=", "shortcut", "\n", "x", "=", "self", ".", "act3", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ResNet.__init__": [[596, 664], ["torch.Module.__init__", "norm_layer", "act_layer", "resnet.make_blocks", "resnet.ResNet.feature_info.extend", "layers.create_classifier", "resnet.ResNet.init_weights", "dict", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "dict", "torch.Sequential", "torch.Sequential", "torch.Sequential", "resnet.ResNet.add_module", "issubclass", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "filter", "aa_layer", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "act_layer", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "act_layer", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "act_layer", "resnet.create_aa", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "aa_layer"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.make_blocks", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.create_aa"], ["def", "__init__", "(", "\n", "self", ",", "block", ",", "layers", ",", "num_classes", "=", "1000", ",", "in_chans", "=", "3", ",", "output_stride", "=", "32", ",", "global_pool", "=", "'avg'", ",", "\n", "cardinality", "=", "1", ",", "base_width", "=", "64", ",", "stem_width", "=", "64", ",", "stem_type", "=", "''", ",", "replace_stem_pool", "=", "False", ",", "block_reduce_first", "=", "1", ",", "\n", "down_kernel_size", "=", "1", ",", "avg_down", "=", "False", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "aa_layer", "=", "None", ",", "\n", "drop_rate", "=", "0.0", ",", "drop_path_rate", "=", "0.", ",", "drop_block_rate", "=", "0.", ",", "zero_init_last", "=", "True", ",", "block_args", "=", "None", ")", ":", "\n", "        ", "super", "(", "ResNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "block_args", "=", "block_args", "or", "dict", "(", ")", "\n", "assert", "output_stride", "in", "(", "8", ",", "16", ",", "32", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "# Stem", "\n", "deep_stem", "=", "'deep'", "in", "stem_type", "\n", "inplanes", "=", "stem_width", "*", "2", "if", "deep_stem", "else", "64", "\n", "if", "deep_stem", ":", "\n", "            ", "stem_chs", "=", "(", "stem_width", ",", "stem_width", ")", "\n", "if", "'tiered'", "in", "stem_type", ":", "\n", "                ", "stem_chs", "=", "(", "3", "*", "(", "stem_width", "//", "4", ")", ",", "stem_width", ")", "\n", "", "self", ".", "conv1", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "nn", ".", "Conv2d", "(", "in_chans", ",", "stem_chs", "[", "0", "]", ",", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ",", "\n", "norm_layer", "(", "stem_chs", "[", "0", "]", ")", ",", "\n", "act_layer", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv2d", "(", "stem_chs", "[", "0", "]", ",", "stem_chs", "[", "1", "]", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ",", "\n", "norm_layer", "(", "stem_chs", "[", "1", "]", ")", ",", "\n", "act_layer", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv2d", "(", "stem_chs", "[", "1", "]", ",", "inplanes", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "in_chans", ",", "inplanes", ",", "kernel_size", "=", "7", ",", "stride", "=", "2", ",", "padding", "=", "3", ",", "bias", "=", "False", ")", "\n", "", "self", ".", "bn1", "=", "norm_layer", "(", "inplanes", ")", "\n", "self", ".", "act1", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "feature_info", "=", "[", "dict", "(", "num_chs", "=", "inplanes", ",", "reduction", "=", "2", ",", "module", "=", "'act1'", ")", "]", "\n", "\n", "# Stem pooling. The name 'maxpool' remains for weight compatibility.", "\n", "if", "replace_stem_pool", ":", "\n", "            ", "self", ".", "maxpool", "=", "nn", ".", "Sequential", "(", "*", "filter", "(", "None", ",", "[", "\n", "nn", ".", "Conv2d", "(", "inplanes", ",", "inplanes", ",", "3", ",", "stride", "=", "1", "if", "aa_layer", "else", "2", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ",", "\n", "create_aa", "(", "aa_layer", ",", "channels", "=", "inplanes", ",", "stride", "=", "2", ")", "if", "aa_layer", "is", "not", "None", "else", "None", ",", "\n", "norm_layer", "(", "inplanes", ")", ",", "\n", "act_layer", "(", "inplace", "=", "True", ")", "\n", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "if", "aa_layer", "is", "not", "None", ":", "\n", "                ", "if", "issubclass", "(", "aa_layer", ",", "nn", ".", "AvgPool2d", ")", ":", "\n", "                    ", "self", ".", "maxpool", "=", "aa_layer", "(", "2", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "maxpool", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ",", "\n", "aa_layer", "(", "channels", "=", "inplanes", ",", "stride", "=", "2", ")", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "self", ".", "maxpool", "=", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", "\n", "\n", "# Feature Blocks", "\n", "", "", "channels", "=", "[", "64", ",", "128", ",", "256", ",", "512", "]", "\n", "stage_modules", ",", "stage_feature_info", "=", "make_blocks", "(", "\n", "block", ",", "channels", ",", "layers", ",", "inplanes", ",", "cardinality", "=", "cardinality", ",", "base_width", "=", "base_width", ",", "\n", "output_stride", "=", "output_stride", ",", "reduce_first", "=", "block_reduce_first", ",", "avg_down", "=", "avg_down", ",", "\n", "down_kernel_size", "=", "down_kernel_size", ",", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ",", "aa_layer", "=", "aa_layer", ",", "\n", "drop_block_rate", "=", "drop_block_rate", ",", "drop_path_rate", "=", "drop_path_rate", ",", "**", "block_args", ")", "\n", "for", "stage", "in", "stage_modules", ":", "\n", "            ", "self", ".", "add_module", "(", "*", "stage", ")", "# layer1, layer2, etc", "\n", "", "self", ".", "feature_info", ".", "extend", "(", "stage_feature_info", ")", "\n", "\n", "# Head (Pooling and Classifier)", "\n", "self", ".", "num_features", "=", "512", "*", "block", ".", "expansion", "\n", "self", ".", "global_pool", ",", "self", ".", "fc", "=", "create_classifier", "(", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n", "self", ".", "init_weights", "(", "zero_init_last", "=", "zero_init_last", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ResNet.init_weights": [[665, 677], ["resnet.ResNet.named_modules", "isinstance", "resnet.ResNet.modules", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "isinstance", "hasattr", "torch.init.ones_", "torch.init.ones_", "torch.init.ones_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_", "m.zero_init_last"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_modules", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet.SelectiveKernelBottleneck.zero_init_last"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "init_weights", "(", "self", ",", "zero_init_last", "=", "True", ")", ":", "\n", "        ", "for", "n", ",", "m", "in", "self", ".", "named_modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "ones_", "(", "m", ".", "weight", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "m", ".", "bias", ")", "\n", "", "", "if", "zero_init_last", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "hasattr", "(", "m", ",", "'zero_init_last'", ")", ":", "\n", "                    ", "m", ".", "zero_init_last", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ResNet.group_matcher": [[678, 682], ["dict"], "methods", ["None"], ["", "", "", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "stem", "=", "r'^conv1|bn1|maxpool'", ",", "blocks", "=", "r'^layer(\\d+)'", "if", "coarse", "else", "r'^layer(\\d+)\\.(\\d+)'", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ResNet.set_grad_checkpointing": [[683, 686], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ResNet.get_classifier": [[687, 690], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ",", "name_only", "=", "False", ")", ":", "\n", "        ", "return", "'fc'", "if", "name_only", "else", "self", ".", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ResNet.reset_classifier": [[691, 694], ["layers.create_classifier"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", ",", "self", ".", "fc", "=", "create_classifier", "(", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ResNet.forward_features": [[695, 709], ["resnet.ResNet.conv1", "resnet.ResNet.bn1", "resnet.ResNet.act1", "resnet.ResNet.maxpool", "helpers.checkpoint_seq", "resnet.ResNet.layer1", "resnet.ResNet.layer2", "resnet.ResNet.layer3", "resnet.ResNet.layer4", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "x", "=", "self", ".", "act1", "(", "x", ")", "\n", "x", "=", "self", ".", "maxpool", "(", "x", ")", "\n", "\n", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "[", "self", ".", "layer1", ",", "self", ".", "layer2", ",", "self", ".", "layer3", ",", "self", ".", "layer4", "]", ",", "x", ",", "flatten", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "layer1", "(", "x", ")", "\n", "x", "=", "self", ".", "layer2", "(", "x", ")", "\n", "x", "=", "self", ".", "layer3", "(", "x", ")", "\n", "x", "=", "self", ".", "layer4", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ResNet.forward_head": [[710, 715], ["resnet.ResNet.global_pool", "torch.dropout", "torch.dropout", "torch.dropout", "resnet.ResNet.fc", "float"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "if", "self", ".", "drop_rate", ":", "\n", "            ", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "float", "(", "self", ".", "drop_rate", ")", ",", "training", "=", "self", ".", "training", ")", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "fc", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ResNet.forward": [[716, 720], ["resnet.ResNet.forward_features", "resnet.ResNet.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet._cfg": [[25, 33], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bilinear'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'conv1'", ",", "'classifier'", ":", "'fc'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.get_padding": [[310, 313], ["None"], "function", ["None"], ["def", "get_padding", "(", "kernel_size", ",", "stride", ",", "dilation", "=", "1", ")", ":", "\n", "    ", "padding", "=", "(", "(", "stride", "-", "1", ")", "+", "dilation", "*", "(", "kernel_size", "-", "1", ")", ")", "//", "2", "\n", "return", "padding", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.create_aa": [[315, 319], ["torch.Identity", "issubclass", "aa_layer", "aa_layer"], "function", ["None"], ["", "def", "create_aa", "(", "aa_layer", ",", "channels", ",", "stride", "=", "2", ",", "enable", "=", "True", ")", ":", "\n", "    ", "if", "not", "aa_layer", "or", "not", "enable", ":", "\n", "        ", "return", "nn", ".", "Identity", "(", ")", "\n", "", "return", "aa_layer", "(", "stride", ")", "if", "issubclass", "(", "aa_layer", ",", "nn", ".", "AvgPool2d", ")", "else", "aa_layer", "(", "channels", "=", "channels", ",", "stride", "=", "stride", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.downsample_conv": [[457, 468], ["resnet.get_padding", "torch.Sequential", "torch.Conv2d", "norm_layer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.get_padding"], ["", "", "def", "downsample_conv", "(", "\n", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "first_dilation", "=", "None", ",", "norm_layer", "=", "None", ")", ":", "\n", "    ", "norm_layer", "=", "norm_layer", "or", "nn", ".", "BatchNorm2d", "\n", "kernel_size", "=", "1", "if", "stride", "==", "1", "and", "dilation", "==", "1", "else", "kernel_size", "\n", "first_dilation", "=", "(", "first_dilation", "or", "dilation", ")", "if", "kernel_size", ">", "1", "else", "1", "\n", "p", "=", "get_padding", "(", "kernel_size", ",", "stride", ",", "first_dilation", ")", "\n", "\n", "return", "nn", ".", "Sequential", "(", "*", "[", "\n", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "p", ",", "dilation", "=", "first_dilation", ",", "bias", "=", "False", ")", ",", "\n", "norm_layer", "(", "out_channels", ")", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.downsample_avg": [[471, 485], ["torch.Sequential", "torch.Identity", "avg_pool_fn", "torch.Conv2d", "norm_layer"], "function", ["None"], ["", "def", "downsample_avg", "(", "\n", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "first_dilation", "=", "None", ",", "norm_layer", "=", "None", ")", ":", "\n", "    ", "norm_layer", "=", "norm_layer", "or", "nn", ".", "BatchNorm2d", "\n", "avg_stride", "=", "stride", "if", "dilation", "==", "1", "else", "1", "\n", "if", "stride", "==", "1", "and", "dilation", "==", "1", ":", "\n", "        ", "pool", "=", "nn", ".", "Identity", "(", ")", "\n", "", "else", ":", "\n", "        ", "avg_pool_fn", "=", "AvgPool2dSame", "if", "avg_stride", "==", "1", "and", "dilation", ">", "1", "else", "nn", ".", "AvgPool2d", "\n", "pool", "=", "avg_pool_fn", "(", "2", ",", "avg_stride", ",", "ceil_mode", "=", "True", ",", "count_include_pad", "=", "False", ")", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "[", "\n", "pool", ",", "\n", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "False", ")", ",", "\n", "norm_layer", "(", "out_channels", ")", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.drop_blocks": [[488, 493], ["functools.partial", "functools.partial"], "function", ["None"], ["", "def", "drop_blocks", "(", "drop_prob", "=", "0.", ")", ":", "\n", "    ", "return", "[", "\n", "None", ",", "None", ",", "\n", "partial", "(", "DropBlock2d", ",", "drop_prob", "=", "drop_prob", ",", "block_size", "=", "5", ",", "gamma_scale", "=", "0.25", ")", "if", "drop_prob", "else", "None", ",", "\n", "partial", "(", "DropBlock2d", ",", "drop_prob", "=", "drop_prob", ",", "block_size", "=", "3", ",", "gamma_scale", "=", "1.00", ")", "if", "drop_prob", "else", "None", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.make_blocks": [[495, 537], ["sum", "enumerate", "zip", "dict", "range", "stages.append", "feature_info.append", "resnet.drop_blocks", "dict", "blocks.append", "dict", "resnet.downsample_avg", "resnet.downsample_conv", "block_fn", "torch.Sequential", "kwargs.get", "layers.DropPath"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.drop_blocks", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.downsample_avg", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.downsample_conv", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["", "def", "make_blocks", "(", "\n", "block_fn", ",", "channels", ",", "block_repeats", ",", "inplanes", ",", "reduce_first", "=", "1", ",", "output_stride", "=", "32", ",", "\n", "down_kernel_size", "=", "1", ",", "avg_down", "=", "False", ",", "drop_block_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ",", "**", "kwargs", ")", ":", "\n", "    ", "stages", "=", "[", "]", "\n", "feature_info", "=", "[", "]", "\n", "net_num_blocks", "=", "sum", "(", "block_repeats", ")", "\n", "net_block_idx", "=", "0", "\n", "net_stride", "=", "4", "\n", "dilation", "=", "prev_dilation", "=", "1", "\n", "for", "stage_idx", ",", "(", "planes", ",", "num_blocks", ",", "db", ")", "in", "enumerate", "(", "zip", "(", "channels", ",", "block_repeats", ",", "drop_blocks", "(", "drop_block_rate", ")", ")", ")", ":", "\n", "        ", "stage_name", "=", "f'layer{stage_idx + 1}'", "# never liked this name, but weight compat requires it", "\n", "stride", "=", "1", "if", "stage_idx", "==", "0", "else", "2", "\n", "if", "net_stride", ">=", "output_stride", ":", "\n", "            ", "dilation", "*=", "stride", "\n", "stride", "=", "1", "\n", "", "else", ":", "\n", "            ", "net_stride", "*=", "stride", "\n", "\n", "", "downsample", "=", "None", "\n", "if", "stride", "!=", "1", "or", "inplanes", "!=", "planes", "*", "block_fn", ".", "expansion", ":", "\n", "            ", "down_kwargs", "=", "dict", "(", "\n", "in_channels", "=", "inplanes", ",", "out_channels", "=", "planes", "*", "block_fn", ".", "expansion", ",", "kernel_size", "=", "down_kernel_size", ",", "\n", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "first_dilation", "=", "prev_dilation", ",", "norm_layer", "=", "kwargs", ".", "get", "(", "'norm_layer'", ")", ")", "\n", "downsample", "=", "downsample_avg", "(", "**", "down_kwargs", ")", "if", "avg_down", "else", "downsample_conv", "(", "**", "down_kwargs", ")", "\n", "\n", "", "block_kwargs", "=", "dict", "(", "reduce_first", "=", "reduce_first", ",", "dilation", "=", "dilation", ",", "drop_block", "=", "db", ",", "**", "kwargs", ")", "\n", "blocks", "=", "[", "]", "\n", "for", "block_idx", "in", "range", "(", "num_blocks", ")", ":", "\n", "            ", "downsample", "=", "downsample", "if", "block_idx", "==", "0", "else", "None", "\n", "stride", "=", "stride", "if", "block_idx", "==", "0", "else", "1", "\n", "block_dpr", "=", "drop_path_rate", "*", "net_block_idx", "/", "(", "net_num_blocks", "-", "1", ")", "# stochastic depth linear decay rule", "\n", "blocks", ".", "append", "(", "block_fn", "(", "\n", "inplanes", ",", "planes", ",", "stride", ",", "downsample", ",", "first_dilation", "=", "prev_dilation", ",", "\n", "drop_path", "=", "DropPath", "(", "block_dpr", ")", "if", "block_dpr", ">", "0.", "else", "None", ",", "**", "block_kwargs", ")", ")", "\n", "prev_dilation", "=", "dilation", "\n", "inplanes", "=", "planes", "*", "block_fn", ".", "expansion", "\n", "net_block_idx", "+=", "1", "\n", "\n", "", "stages", ".", "append", "(", "(", "stage_name", ",", "nn", ".", "Sequential", "(", "*", "blocks", ")", ")", ")", "\n", "feature_info", ".", "append", "(", "dict", "(", "num_chs", "=", "inplanes", ",", "reduction", "=", "net_stride", ",", "module", "=", "stage_name", ")", ")", "\n", "\n", "", "return", "stages", ",", "feature_info", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet._create_resnet": [[722, 724], ["helpers.build_model_with_cfg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_resnet", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "ResNet", ",", "variant", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet18": [[726, 732], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnet18", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-18 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "BasicBlock", ",", "layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnet18'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet18d": [[734, 741], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnet18d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-18-D model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "BasicBlock", ",", "layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnet18d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet34": [[743, 749], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnet34", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-34 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "BasicBlock", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnet34'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet34d": [[751, 758], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnet34d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-34-D model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "BasicBlock", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnet34d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet26": [[760, 766], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnet26", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-26 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnet26'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet26t": [[768, 775], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnet26t", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-26-T model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep_tiered'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnet26t'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet26d": [[777, 783], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnet26d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-26-D model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnet26d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet50": [[785, 791], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnet50", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-50 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnet50'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet50d": [[793, 800], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnet50d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-50-D model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnet50d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet50t": [[802, 809], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnet50t", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-50-T model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep_tiered'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnet50t'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet101": [[811, 817], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnet101", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-101 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnet101'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet101d": [[819, 825], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnet101d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-101-D model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnet101d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet152": [[827, 833], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnet152", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-152 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnet152'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet152d": [[835, 842], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnet152d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-152-D model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnet152d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet200": [[844, 850], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnet200", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-200 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "24", ",", "36", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnet200'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet200d": [[852, 859], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnet200d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-200-D model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "24", ",", "36", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnet200d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.tv_resnet34": [[861, 867], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "tv_resnet34", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-34 model with original Torchvision weights.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "BasicBlock", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'tv_resnet34'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.tv_resnet50": [[869, 875], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "tv_resnet50", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-50 model with original Torchvision weights.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'tv_resnet50'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.tv_resnet101": [[877, 883], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "tv_resnet101", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-101 model w/ Torchvision pretrained weights.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'tv_resnet101'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.tv_resnet152": [[885, 891], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "tv_resnet152", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-152 model w/ Torchvision pretrained weights.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'tv_resnet152'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.wide_resnet50_2": [[893, 903], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "wide_resnet50_2", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a Wide ResNet-50-2 model.\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "base_width", "=", "128", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'wide_resnet50_2'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.wide_resnet101_2": [[905, 914], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "wide_resnet101_2", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a Wide ResNet-101-2 model.\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "base_width", "=", "128", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'wide_resnet101_2'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnet50_gn": [[916, 922], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnet50_gn", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-50 model w/ GroupNorm\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnet50_gn'", ",", "pretrained", ",", "norm_layer", "=", "GroupNorm", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnext50_32x4d": [[924, 930], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnext50_32x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNeXt50-32x4d model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnext50_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnext50d_32x4d": [[932, 940], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnext50d_32x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNeXt50d-32x4d model. ResNext50 w/ deep stem & avg pool downsample\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "\n", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnext50d_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnext101_32x4d": [[942, 948], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnext101_32x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNeXt-101 32x4d model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnext101_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnext101_32x8d": [[950, 956], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnext101_32x8d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNeXt-101 32x8d model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "8", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnext101_32x8d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnext101_64x4d": [[958, 964], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnext101_64x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNeXt101-64x4d model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "64", ",", "base_width", "=", "4", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnext101_64x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.tv_resnext50_32x4d": [[966, 972], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "tv_resnext50_32x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNeXt50-32x4d model with original Torchvision weights.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'tv_resnext50_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ig_resnext101_32x8d": [[974, 983], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ig_resnext101_32x8d", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data\n    and finetuned on ImageNet from Figure 5 in\n    `\"Exploring the Limits of Weakly Supervised Pretraining\" <https://arxiv.org/abs/1805.00932>`_\n    Weights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "8", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ig_resnext101_32x8d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ig_resnext101_32x16d": [[985, 994], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ig_resnext101_32x16d", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNeXt-101 32x16 model pre-trained on weakly-supervised data\n    and finetuned on ImageNet from Figure 5 in\n    `\"Exploring the Limits of Weakly Supervised Pretraining\" <https://arxiv.org/abs/1805.00932>`_\n    Weights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "16", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ig_resnext101_32x16d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ig_resnext101_32x32d": [[996, 1005], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ig_resnext101_32x32d", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNeXt-101 32x32 model pre-trained on weakly-supervised data\n    and finetuned on ImageNet from Figure 5 in\n    `\"Exploring the Limits of Weakly Supervised Pretraining\" <https://arxiv.org/abs/1805.00932>`_\n    Weights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "32", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ig_resnext101_32x32d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ig_resnext101_32x48d": [[1007, 1016], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ig_resnext101_32x48d", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNeXt-101 32x48 model pre-trained on weakly-supervised data\n    and finetuned on ImageNet from Figure 5 in\n    `\"Exploring the Limits of Weakly Supervised Pretraining\" <https://arxiv.org/abs/1805.00932>`_\n    Weights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "48", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ig_resnext101_32x48d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ssl_resnet18": [[1018, 1026], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ssl_resnet18", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a semi-supervised ResNet-18 model pre-trained on YFCC100M dataset and finetuned on ImageNet\n    `\"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546>`_\n    Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "BasicBlock", ",", "layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ssl_resnet18'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ssl_resnet50": [[1028, 1036], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ssl_resnet50", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a semi-supervised ResNet-50 model pre-trained on YFCC100M dataset and finetuned on ImageNet\n    `\"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546>`_\n    Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ssl_resnet50'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ssl_resnext50_32x4d": [[1038, 1046], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ssl_resnext50_32x4d", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a semi-supervised ResNeXt-50 32x4 model pre-trained on YFCC100M dataset and finetuned on ImageNet\n    `\"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546>`_\n    Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ssl_resnext50_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ssl_resnext101_32x4d": [[1048, 1056], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ssl_resnext101_32x4d", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a semi-supervised ResNeXt-101 32x4 model pre-trained on YFCC100M dataset and finetuned on ImageNet\n    `\"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546>`_\n    Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ssl_resnext101_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ssl_resnext101_32x8d": [[1058, 1066], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ssl_resnext101_32x8d", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a semi-supervised ResNeXt-101 32x8 model pre-trained on YFCC100M dataset and finetuned on ImageNet\n    `\"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546>`_\n    Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "8", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ssl_resnext101_32x8d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ssl_resnext101_32x16d": [[1068, 1076], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ssl_resnext101_32x16d", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a semi-supervised ResNeXt-101 32x16 model pre-trained on YFCC100M dataset and finetuned on ImageNet\n    `\"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546>`_\n    Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "16", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ssl_resnext101_32x16d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.swsl_resnet18": [[1078, 1087], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "swsl_resnet18", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a semi-weakly supervised Resnet-18 model pre-trained on 1B weakly supervised\n       image dataset and finetuned on ImageNet.\n       `\"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546>`_\n       Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "BasicBlock", ",", "layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'swsl_resnet18'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.swsl_resnet50": [[1089, 1098], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "swsl_resnet50", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a semi-weakly supervised ResNet-50 model pre-trained on 1B weakly supervised\n       image dataset and finetuned on ImageNet.\n       `\"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546>`_\n       Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'swsl_resnet50'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.swsl_resnext50_32x4d": [[1100, 1109], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "swsl_resnext50_32x4d", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a semi-weakly supervised ResNeXt-50 32x4 model pre-trained on 1B weakly supervised\n       image dataset and finetuned on ImageNet.\n       `\"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546>`_\n       Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'swsl_resnext50_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.swsl_resnext101_32x4d": [[1111, 1120], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "swsl_resnext101_32x4d", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a semi-weakly supervised ResNeXt-101 32x4 model pre-trained on 1B weakly supervised\n       image dataset and finetuned on ImageNet.\n       `\"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546>`_\n       Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'swsl_resnext101_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.swsl_resnext101_32x8d": [[1122, 1131], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "swsl_resnext101_32x8d", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a semi-weakly supervised ResNeXt-101 32x8 model pre-trained on 1B weakly supervised\n       image dataset and finetuned on ImageNet.\n       `\"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546>`_\n       Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "8", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'swsl_resnext101_32x8d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.swsl_resnext101_32x16d": [[1133, 1142], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "swsl_resnext101_32x16d", "(", "pretrained", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a semi-weakly supervised ResNeXt-101 32x16 model pre-trained on 1B weakly supervised\n       image dataset and finetuned on ImageNet.\n       `\"Billion-scale Semi-Supervised Learning for Image Classification\" <https://arxiv.org/abs/1905.00546>`_\n       Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "16", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'swsl_resnext101_32x16d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ecaresnet26t": [[1144, 1154], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ecaresnet26t", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs an ECA-ResNeXt-26-T model.\n    This is technically a 28 layer ResNet, like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels\n    in the deep stem and ECA attn.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "stem_width", "=", "32", ",", "\n", "stem_type", "=", "'deep_tiered'", ",", "avg_down", "=", "True", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "'eca'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ecaresnet26t'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ecaresnet50d": [[1156, 1164], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ecaresnet50d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-50-D model with eca.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "\n", "block_args", "=", "dict", "(", "attn_layer", "=", "'eca'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ecaresnet50d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ecaresnet50d_pruned": [[1166, 1175], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ecaresnet50d_pruned", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-50-D model pruned with eca.\n        The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "\n", "block_args", "=", "dict", "(", "attn_layer", "=", "'eca'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ecaresnet50d_pruned'", ",", "pretrained", ",", "pruned", "=", "True", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ecaresnet50t": [[1177, 1186], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ecaresnet50t", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs an ECA-ResNet-50-T model.\n    Like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels in the deep stem and ECA attn.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "stem_width", "=", "32", ",", "\n", "stem_type", "=", "'deep_tiered'", ",", "avg_down", "=", "True", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "'eca'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ecaresnet50t'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ecaresnetlight": [[1188, 1196], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ecaresnetlight", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-50-D light model with eca.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "1", ",", "1", ",", "11", ",", "3", "]", ",", "stem_width", "=", "32", ",", "avg_down", "=", "True", ",", "\n", "block_args", "=", "dict", "(", "attn_layer", "=", "'eca'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ecaresnetlight'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ecaresnet101d": [[1198, 1206], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ecaresnet101d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-101-D model with eca.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "\n", "block_args", "=", "dict", "(", "attn_layer", "=", "'eca'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ecaresnet101d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ecaresnet101d_pruned": [[1208, 1217], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ecaresnet101d_pruned", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-101-D model pruned with eca.\n       The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "\n", "block_args", "=", "dict", "(", "attn_layer", "=", "'eca'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ecaresnet101d_pruned'", ",", "pretrained", ",", "pruned", "=", "True", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ecaresnet200d": [[1219, 1227], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ecaresnet200d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-200-D model with ECA.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "24", ",", "36", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "\n", "block_args", "=", "dict", "(", "attn_layer", "=", "'eca'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ecaresnet200d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ecaresnet269d": [[1229, 1237], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ecaresnet269d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-269-D model with ECA.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "30", ",", "48", ",", "8", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "\n", "block_args", "=", "dict", "(", "attn_layer", "=", "'eca'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ecaresnet269d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ecaresnext26t_32x4d": [[1239, 1249], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ecaresnext26t_32x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs an ECA-ResNeXt-26-T model.\n    This is technically a 28 layer ResNet, like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels\n    in the deep stem. This model replaces SE module with the ECA module\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "stem_width", "=", "32", ",", "\n", "stem_type", "=", "'deep_tiered'", ",", "avg_down", "=", "True", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "'eca'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ecaresnext26t_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.ecaresnext50t_32x4d": [[1251, 1261], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "ecaresnext50t_32x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs an ECA-ResNeXt-50-T model.\n    This is technically a 28 layer ResNet, like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels\n    in the deep stem. This model replaces SE module with the ECA module\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "stem_width", "=", "32", ",", "\n", "stem_type", "=", "'deep_tiered'", ",", "avg_down", "=", "True", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "'eca'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'ecaresnext50t_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.seresnet18": [[1263, 1267], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "seresnet18", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "block", "=", "BasicBlock", ",", "layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "'se'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'seresnet18'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.seresnet34": [[1269, 1273], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "seresnet34", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "block", "=", "BasicBlock", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "'se'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'seresnet34'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.seresnet50": [[1275, 1279], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "seresnet50", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "'se'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'seresnet50'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.seresnet50t": [[1281, 1287], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "seresnet50t", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep_tiered'", ",", "avg_down", "=", "True", ",", "\n", "block_args", "=", "dict", "(", "attn_layer", "=", "'se'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'seresnet50t'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.seresnet101": [[1289, 1293], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "seresnet101", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "'se'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'seresnet101'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.seresnet152": [[1295, 1299], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "seresnet152", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "'se'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'seresnet152'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.seresnet152d": [[1301, 1307], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "seresnet152d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "\n", "block_args", "=", "dict", "(", "attn_layer", "=", "'se'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'seresnet152d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.seresnet200d": [[1309, 1317], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "seresnet200d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-200-D model with SE attn.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "24", ",", "36", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "\n", "block_args", "=", "dict", "(", "attn_layer", "=", "'se'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'seresnet200d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.seresnet269d": [[1319, 1327], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "seresnet269d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-269-D model with SE attn.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "30", ",", "48", ",", "8", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "\n", "block_args", "=", "dict", "(", "attn_layer", "=", "'se'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'seresnet269d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.seresnext26d_32x4d": [[1329, 1339], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "seresnext26d_32x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a SE-ResNeXt-26-D model.`\n    This is technically a 28 layer ResNet, using the 'D' modifier from Gluon / bag-of-tricks for\n    combination of deep stem and avg_pool in downsample.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "stem_width", "=", "32", ",", "\n", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "'se'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'seresnext26d_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.seresnext26t_32x4d": [[1341, 1351], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "seresnext26t_32x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a SE-ResNet-26-T model.\n    This is technically a 28 layer ResNet, like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels\n    in the deep stem.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "stem_width", "=", "32", ",", "\n", "stem_type", "=", "'deep_tiered'", ",", "avg_down", "=", "True", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "'se'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'seresnext26t_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.seresnext26tn_32x4d": [[1353, 1360], ["resnet.seresnext26t_32x4d"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.seresnext26t_32x4d"], ["", "@", "register_model", "\n", "def", "seresnext26tn_32x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a SE-ResNeXt-26-T model.\n    NOTE I deprecated previous 't' model defs and replaced 't' with 'tn', this was the only tn model of note\n    so keeping this def for backwards compat with any uses out there. Old 't' model is lost.\n    \"\"\"", "\n", "return", "seresnext26t_32x4d", "(", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.seresnext50_32x4d": [[1362, 1368], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "seresnext50_32x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "\n", "block_args", "=", "dict", "(", "attn_layer", "=", "'se'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'seresnext50_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.seresnext101_32x4d": [[1370, 1376], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "seresnext101_32x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "\n", "block_args", "=", "dict", "(", "attn_layer", "=", "'se'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'seresnext101_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.seresnext101_32x8d": [[1378, 1384], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "seresnext101_32x8d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "8", ",", "\n", "block_args", "=", "dict", "(", "attn_layer", "=", "'se'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'seresnext101_32x8d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.seresnext101d_32x8d": [[1386, 1393], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "seresnext101d_32x8d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "8", ",", "\n", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "\n", "block_args", "=", "dict", "(", "attn_layer", "=", "'se'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'seresnext101d_32x8d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.senet154": [[1395, 1401], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "senet154", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "cardinality", "=", "64", ",", "base_width", "=", "4", ",", "stem_type", "=", "'deep'", ",", "\n", "down_kernel_size", "=", "3", ",", "block_reduce_first", "=", "2", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "'se'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'senet154'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnetblur18": [[1403, 1409], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnetblur18", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-18 model with blur anti-aliasing\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "BasicBlock", ",", "layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "aa_layer", "=", "BlurPool2d", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnetblur18'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnetblur50": [[1411, 1417], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnetblur50", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-50 model with blur anti-aliasing\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "aa_layer", "=", "BlurPool2d", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnetblur50'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnetblur50d": [[1419, 1427], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnetblur50d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-50-D model with blur anti-aliasing\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "aa_layer", "=", "BlurPool2d", ",", "\n", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnetblur50d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnetblur101d": [[1429, 1437], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnetblur101d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-101-D model with blur anti-aliasing\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "aa_layer", "=", "BlurPool2d", ",", "\n", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnetblur101d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnetaa50d": [[1439, 1447], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnetaa50d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-50-D model with avgpool anti-aliasing\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "aa_layer", "=", "nn", ".", "AvgPool2d", ",", "\n", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnetaa50d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnetaa101d": [[1449, 1457], ["dict", "resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "resnetaa101d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-101-D model with avgpool anti-aliasing\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "aa_layer", "=", "nn", ".", "AvgPool2d", ",", "\n", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnetaa101d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.seresnetaa50d": [[1459, 1467], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "seresnetaa50d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a SE=ResNet-50-D model with avgpool anti-aliasing\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "aa_layer", "=", "nn", ".", "AvgPool2d", ",", "\n", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "'se'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'seresnetaa50d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.seresnextaa101d_32x8d": [[1469, 1478], ["dict", "resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "seresnextaa101d_32x8d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a SE=ResNeXt-101-D 32x8d model with avgpool anti-aliasing\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "8", ",", "\n", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "aa_layer", "=", "nn", ".", "AvgPool2d", ",", "\n", "block_args", "=", "dict", "(", "attn_layer", "=", "'se'", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'seresnextaa101d_32x8d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnetrs50": [[1480, 1491], ["functools.partial", "dict", "resnet._create_resnet", "layers.get_attn", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.get_attn"], ["", "@", "register_model", "\n", "def", "resnetrs50", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-RS-50 model.\n    Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579\n    Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs\n    \"\"\"", "\n", "attn_layer", "=", "partial", "(", "get_attn", "(", "'se'", ")", ",", "rd_ratio", "=", "0.25", ")", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "replace_stem_pool", "=", "True", ",", "\n", "avg_down", "=", "True", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "attn_layer", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnetrs50'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnetrs101": [[1493, 1504], ["functools.partial", "dict", "resnet._create_resnet", "layers.get_attn", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.get_attn"], ["", "@", "register_model", "\n", "def", "resnetrs101", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-RS-101 model.\n    Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579\n    Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs\n    \"\"\"", "\n", "attn_layer", "=", "partial", "(", "get_attn", "(", "'se'", ")", ",", "rd_ratio", "=", "0.25", ")", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "replace_stem_pool", "=", "True", ",", "\n", "avg_down", "=", "True", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "attn_layer", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnetrs101'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnetrs152": [[1506, 1517], ["functools.partial", "dict", "resnet._create_resnet", "layers.get_attn", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.get_attn"], ["", "@", "register_model", "\n", "def", "resnetrs152", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-RS-152 model.\n    Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579\n    Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs\n    \"\"\"", "\n", "attn_layer", "=", "partial", "(", "get_attn", "(", "'se'", ")", ",", "rd_ratio", "=", "0.25", ")", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "replace_stem_pool", "=", "True", ",", "\n", "avg_down", "=", "True", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "attn_layer", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnetrs152'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnetrs200": [[1519, 1530], ["functools.partial", "dict", "resnet._create_resnet", "layers.get_attn", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.get_attn"], ["", "@", "register_model", "\n", "def", "resnetrs200", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-RS-200 model.\n    Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579\n    Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs\n    \"\"\"", "\n", "attn_layer", "=", "partial", "(", "get_attn", "(", "'se'", ")", ",", "rd_ratio", "=", "0.25", ")", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "24", ",", "36", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "replace_stem_pool", "=", "True", ",", "\n", "avg_down", "=", "True", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "attn_layer", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnetrs200'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnetrs270": [[1532, 1543], ["functools.partial", "dict", "resnet._create_resnet", "layers.get_attn", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.get_attn"], ["", "@", "register_model", "\n", "def", "resnetrs270", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-RS-270 model.\n    Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579\n    Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs\n    \"\"\"", "\n", "attn_layer", "=", "partial", "(", "get_attn", "(", "'se'", ")", ",", "rd_ratio", "=", "0.25", ")", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "4", ",", "29", ",", "53", ",", "4", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "replace_stem_pool", "=", "True", ",", "\n", "avg_down", "=", "True", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "attn_layer", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnetrs270'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnetrs350": [[1546, 1557], ["functools.partial", "dict", "resnet._create_resnet", "layers.get_attn", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.get_attn"], ["", "@", "register_model", "\n", "def", "resnetrs350", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-RS-350 model.\n    Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579\n    Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs\n    \"\"\"", "\n", "attn_layer", "=", "partial", "(", "get_attn", "(", "'se'", ")", ",", "rd_ratio", "=", "0.25", ")", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "4", ",", "36", ",", "72", ",", "4", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "replace_stem_pool", "=", "True", ",", "\n", "avg_down", "=", "True", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "attn_layer", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnetrs350'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnet.resnetrs420": [[1559, 1570], ["functools.partial", "dict", "resnet._create_resnet", "layers.get_attn", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.get_attn"], ["", "@", "register_model", "\n", "def", "resnetrs420", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-RS-420 model\n    Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579\n    Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs\n    \"\"\"", "\n", "attn_layer", "=", "partial", "(", "get_attn", "(", "'se'", ")", ",", "rd_ratio", "=", "0.25", ")", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "4", ",", "44", ",", "87", ",", "4", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "replace_stem_pool", "=", "True", ",", "\n", "avg_down", "=", "True", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "attn_layer", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'resnetrs420'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.SEModule.__init__": [[73, 79], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "channels", ",", "reduction", ")", ":", "\n", "        ", "super", "(", "SEModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Conv2d", "(", "channels", ",", "channels", "//", "reduction", ",", "kernel_size", "=", "1", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Conv2d", "(", "channels", "//", "reduction", ",", "channels", ",", "kernel_size", "=", "1", ")", "\n", "self", ".", "sigmoid", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.SEModule.forward": [[80, 88], ["senet.SEModule.mean", "senet.SEModule.fc1", "senet.SEModule.relu", "senet.SEModule.fc2", "senet.SEModule.sigmoid"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "module_input", "=", "x", "\n", "x", "=", "x", ".", "mean", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "self", ".", "sigmoid", "(", "x", ")", "\n", "return", "module_input", "*", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.Bottleneck.forward": [[95, 116], ["senet.Bottleneck.conv1", "senet.Bottleneck.bn1", "senet.Bottleneck.relu", "senet.Bottleneck.conv2", "senet.Bottleneck.bn2", "senet.Bottleneck.relu", "senet.Bottleneck.conv3", "senet.Bottleneck.bn3", "senet.Bottleneck.relu", "senet.Bottleneck.downsample", "senet.Bottleneck.se_module"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample"], ["def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "bn2", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "out", "=", "self", ".", "bn3", "(", "out", ")", "\n", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "shortcut", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "=", "self", ".", "se_module", "(", "out", ")", "+", "shortcut", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.SEBottleneck.__init__": [[124, 138], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "senet.SEModule"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "inplanes", ",", "planes", ",", "groups", ",", "reduction", ",", "stride", "=", "1", ",", "downsample", "=", "None", ")", ":", "\n", "        ", "super", "(", "SEBottleneck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "inplanes", ",", "planes", "*", "2", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm2d", "(", "planes", "*", "2", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "\n", "planes", "*", "2", ",", "planes", "*", "4", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "\n", "padding", "=", "1", ",", "groups", "=", "groups", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "BatchNorm2d", "(", "planes", "*", "4", ")", "\n", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "planes", "*", "4", ",", "planes", "*", "4", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn3", "=", "nn", ".", "BatchNorm2d", "(", "planes", "*", "4", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "se_module", "=", "SEModule", "(", "planes", "*", "4", ",", "reduction", "=", "reduction", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.SEResNetBottleneck.__init__": [[148, 160], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "senet.SEModule"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "inplanes", ",", "planes", ",", "groups", ",", "reduction", ",", "stride", "=", "1", ",", "downsample", "=", "None", ")", ":", "\n", "        ", "super", "(", "SEResNetBottleneck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "inplanes", ",", "planes", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ",", "stride", "=", "stride", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm2d", "(", "planes", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "planes", ",", "planes", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ",", "groups", "=", "groups", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "BatchNorm2d", "(", "planes", ")", "\n", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "planes", ",", "planes", "*", "4", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn3", "=", "nn", ".", "BatchNorm2d", "(", "planes", "*", "4", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "se_module", "=", "SEModule", "(", "planes", "*", "4", ",", "reduction", "=", "reduction", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.SEResNeXtBottleneck.__init__": [[168, 181], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "senet.SEModule", "math.floor"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "inplanes", ",", "planes", ",", "groups", ",", "reduction", ",", "stride", "=", "1", ",", "downsample", "=", "None", ",", "base_width", "=", "4", ")", ":", "\n", "        ", "super", "(", "SEResNeXtBottleneck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "width", "=", "math", ".", "floor", "(", "planes", "*", "(", "base_width", "/", "64", ")", ")", "*", "groups", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "inplanes", ",", "width", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ",", "stride", "=", "1", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm2d", "(", "width", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "width", ",", "width", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "padding", "=", "1", ",", "groups", "=", "groups", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "BatchNorm2d", "(", "width", ")", "\n", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "width", ",", "planes", "*", "4", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn3", "=", "nn", ".", "BatchNorm2d", "(", "planes", "*", "4", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "se_module", "=", "SEModule", "(", "planes", "*", "4", ",", "reduction", "=", "reduction", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.SEResNetBlock.__init__": [[186, 196], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "senet.SEModule"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "inplanes", ",", "planes", ",", "groups", ",", "reduction", ",", "stride", "=", "1", ",", "downsample", "=", "None", ")", ":", "\n", "        ", "super", "(", "SEResNetBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "inplanes", ",", "planes", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ",", "stride", "=", "stride", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm2d", "(", "planes", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "planes", ",", "planes", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ",", "groups", "=", "groups", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "BatchNorm2d", "(", "planes", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "se_module", "=", "SEModule", "(", "planes", ",", "reduction", "=", "reduction", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.SEResNetBlock.forward": [[197, 215], ["senet.SEResNetBlock.conv1", "senet.SEResNetBlock.bn1", "senet.SEResNetBlock.relu", "senet.SEResNetBlock.conv2", "senet.SEResNetBlock.bn2", "senet.SEResNetBlock.relu", "senet.SEResNetBlock.relu", "senet.SEResNetBlock.downsample", "senet.SEResNetBlock.se_module"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "bn2", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "shortcut", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "=", "self", ".", "se_module", "(", "out", ")", "+", "shortcut", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.SENet.__init__": [[219, 342], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "senet.SENet._make_layer", "senet.SENet._make_layer", "senet.SENet._make_layer", "senet.SENet._make_layer", "layers.create_classifier", "senet.SENet.modules", "collections.OrderedDict", "dict", "dict", "dict", "dict", "dict", "senet._weight_init", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.TResNet._make_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.TResNet._make_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.TResNet._make_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.TResNet._make_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet._weight_init"], ["    ", "def", "__init__", "(", "\n", "self", ",", "block", ",", "layers", ",", "groups", ",", "reduction", ",", "drop_rate", "=", "0.2", ",", "\n", "in_chans", "=", "3", ",", "inplanes", "=", "64", ",", "input_3x3", "=", "False", ",", "downsample_kernel_size", "=", "1", ",", "\n", "downsample_padding", "=", "0", ",", "num_classes", "=", "1000", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        \"\"\"", "\n", "super", "(", "SENet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inplanes", "=", "inplanes", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "if", "input_3x3", ":", "\n", "            ", "layer0_modules", "=", "[", "\n", "(", "'conv1'", ",", "nn", ".", "Conv2d", "(", "in_chans", ",", "64", ",", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ")", ",", "\n", "(", "'bn1'", ",", "nn", ".", "BatchNorm2d", "(", "64", ")", ")", ",", "\n", "(", "'relu1'", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", ",", "\n", "(", "'conv2'", ",", "nn", ".", "Conv2d", "(", "64", ",", "64", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ")", ",", "\n", "(", "'bn2'", ",", "nn", ".", "BatchNorm2d", "(", "64", ")", ")", ",", "\n", "(", "'relu2'", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", ",", "\n", "(", "'conv3'", ",", "nn", ".", "Conv2d", "(", "64", ",", "inplanes", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ")", ",", "\n", "(", "'bn3'", ",", "nn", ".", "BatchNorm2d", "(", "inplanes", ")", ")", ",", "\n", "(", "'relu3'", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", ",", "\n", "]", "\n", "", "else", ":", "\n", "            ", "layer0_modules", "=", "[", "\n", "(", "'conv1'", ",", "nn", ".", "Conv2d", "(", "\n", "in_chans", ",", "inplanes", ",", "kernel_size", "=", "7", ",", "stride", "=", "2", ",", "padding", "=", "3", ",", "bias", "=", "False", ")", ")", ",", "\n", "(", "'bn1'", ",", "nn", ".", "BatchNorm2d", "(", "inplanes", ")", ")", ",", "\n", "(", "'relu1'", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", ",", "\n", "]", "\n", "", "self", ".", "layer0", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "layer0_modules", ")", ")", "\n", "# To preserve compatibility with Caffe weights `ceil_mode=True` is used instead of `padding=1`.", "\n", "self", ".", "pool0", "=", "nn", ".", "MaxPool2d", "(", "3", ",", "stride", "=", "2", ",", "ceil_mode", "=", "True", ")", "\n", "self", ".", "feature_info", "=", "[", "dict", "(", "num_chs", "=", "inplanes", ",", "reduction", "=", "2", ",", "module", "=", "'layer0'", ")", "]", "\n", "self", ".", "layer1", "=", "self", ".", "_make_layer", "(", "\n", "block", ",", "\n", "planes", "=", "64", ",", "\n", "blocks", "=", "layers", "[", "0", "]", ",", "\n", "groups", "=", "groups", ",", "\n", "reduction", "=", "reduction", ",", "\n", "downsample_kernel_size", "=", "1", ",", "\n", "downsample_padding", "=", "0", "\n", ")", "\n", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "64", "*", "block", ".", "expansion", ",", "reduction", "=", "4", ",", "module", "=", "'layer1'", ")", "]", "\n", "self", ".", "layer2", "=", "self", ".", "_make_layer", "(", "\n", "block", ",", "\n", "planes", "=", "128", ",", "\n", "blocks", "=", "layers", "[", "1", "]", ",", "\n", "stride", "=", "2", ",", "\n", "groups", "=", "groups", ",", "\n", "reduction", "=", "reduction", ",", "\n", "downsample_kernel_size", "=", "downsample_kernel_size", ",", "\n", "downsample_padding", "=", "downsample_padding", "\n", ")", "\n", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "128", "*", "block", ".", "expansion", ",", "reduction", "=", "8", ",", "module", "=", "'layer2'", ")", "]", "\n", "self", ".", "layer3", "=", "self", ".", "_make_layer", "(", "\n", "block", ",", "\n", "planes", "=", "256", ",", "\n", "blocks", "=", "layers", "[", "2", "]", ",", "\n", "stride", "=", "2", ",", "\n", "groups", "=", "groups", ",", "\n", "reduction", "=", "reduction", ",", "\n", "downsample_kernel_size", "=", "downsample_kernel_size", ",", "\n", "downsample_padding", "=", "downsample_padding", "\n", ")", "\n", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "256", "*", "block", ".", "expansion", ",", "reduction", "=", "16", ",", "module", "=", "'layer3'", ")", "]", "\n", "self", ".", "layer4", "=", "self", ".", "_make_layer", "(", "\n", "block", ",", "\n", "planes", "=", "512", ",", "\n", "blocks", "=", "layers", "[", "3", "]", ",", "\n", "stride", "=", "2", ",", "\n", "groups", "=", "groups", ",", "\n", "reduction", "=", "reduction", ",", "\n", "downsample_kernel_size", "=", "downsample_kernel_size", ",", "\n", "downsample_padding", "=", "downsample_padding", "\n", ")", "\n", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "512", "*", "block", ".", "expansion", ",", "reduction", "=", "32", ",", "module", "=", "'layer4'", ")", "]", "\n", "self", ".", "num_features", "=", "512", "*", "block", ".", "expansion", "\n", "self", ".", "global_pool", ",", "self", ".", "last_linear", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "_weight_init", "(", "m", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.SENet._make_layer": [[343, 360], ["range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "block", "layers.append", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "block"], "methods", ["None"], ["", "", "def", "_make_layer", "(", "self", ",", "block", ",", "planes", ",", "blocks", ",", "groups", ",", "reduction", ",", "stride", "=", "1", ",", "\n", "downsample_kernel_size", "=", "1", ",", "downsample_padding", "=", "0", ")", ":", "\n", "        ", "downsample", "=", "None", "\n", "if", "stride", "!=", "1", "or", "self", ".", "inplanes", "!=", "planes", "*", "block", ".", "expansion", ":", "\n", "            ", "downsample", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "\n", "self", ".", "inplanes", ",", "planes", "*", "block", ".", "expansion", ",", "kernel_size", "=", "downsample_kernel_size", ",", "\n", "stride", "=", "stride", ",", "padding", "=", "downsample_padding", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "planes", "*", "block", ".", "expansion", ")", ",", "\n", ")", "\n", "\n", "", "layers", "=", "[", "block", "(", "self", ".", "inplanes", ",", "planes", ",", "groups", ",", "reduction", ",", "stride", ",", "downsample", ")", "]", "\n", "self", ".", "inplanes", "=", "planes", "*", "block", ".", "expansion", "\n", "for", "i", "in", "range", "(", "1", ",", "blocks", ")", ":", "\n", "            ", "layers", ".", "append", "(", "block", "(", "self", ".", "inplanes", ",", "planes", ",", "groups", ",", "reduction", ")", ")", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.SENet.group_matcher": [[361, 365], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "stem", "=", "r'^layer0'", ",", "blocks", "=", "r'^layer(\\d+)'", "if", "coarse", "else", "r'^layer(\\d+)\\.(\\d+)'", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.SENet.set_grad_checkpointing": [[366, 369], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "'gradient checkpointing not supported'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.SENet.get_classifier": [[370, 373], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "last_linear", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.SENet.reset_classifier": [[374, 378], ["layers.create_classifier"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", ",", "self", ".", "last_linear", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.SENet.forward_features": [[379, 387], ["senet.SENet.layer0", "senet.SENet.pool0", "senet.SENet.layer1", "senet.SENet.layer2", "senet.SENet.layer3", "senet.SENet.layer4"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "layer0", "(", "x", ")", "\n", "x", "=", "self", ".", "pool0", "(", "x", ")", "\n", "x", "=", "self", ".", "layer1", "(", "x", ")", "\n", "x", "=", "self", ".", "layer2", "(", "x", ")", "\n", "x", "=", "self", ".", "layer3", "(", "x", ")", "\n", "x", "=", "self", ".", "layer4", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.SENet.forward_head": [[388, 393], ["senet.SENet.global_pool", "torch.dropout", "torch.dropout", "torch.dropout", "senet.SENet.last_linear"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "if", "self", ".", "drop_rate", ">", "0.", ":", "\n", "            ", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "drop_rate", ",", "training", "=", "self", ".", "training", ")", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "last_linear", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.SENet.forward": [[394, 398], ["senet.SENet.forward_features", "senet.SENet.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet._cfg": [[29, 36], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bilinear'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'layer0.conv1'", ",", "'classifier'", ":", "'last_linear'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet._weight_init": [[63, 69], ["isinstance", "torch.init.kaiming_normal_", "isinstance", "torch.init.constant_", "torch.init.constant_"], "function", ["None"], ["def", "_weight_init", "(", "m", ")", ":", "\n", "    ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "        ", "nn", ".", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "        ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1.", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0.", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet._create_senet": [[400, 402], ["helpers.build_model_with_cfg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_senet", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "SENet", ",", "variant", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.legacy_seresnet18": [[404, 409], ["dict", "senet._create_senet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet._create_senet"], ["", "@", "register_model", "\n", "def", "legacy_seresnet18", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "block", "=", "SEResNetBlock", ",", "layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "groups", "=", "1", ",", "reduction", "=", "16", ",", "**", "kwargs", ")", "\n", "return", "_create_senet", "(", "'legacy_seresnet18'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.legacy_seresnet34": [[411, 416], ["dict", "senet._create_senet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet._create_senet"], ["", "@", "register_model", "\n", "def", "legacy_seresnet34", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "block", "=", "SEResNetBlock", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "groups", "=", "1", ",", "reduction", "=", "16", ",", "**", "kwargs", ")", "\n", "return", "_create_senet", "(", "'legacy_seresnet34'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.legacy_seresnet50": [[418, 423], ["dict", "senet._create_senet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet._create_senet"], ["", "@", "register_model", "\n", "def", "legacy_seresnet50", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "block", "=", "SEResNetBottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "groups", "=", "1", ",", "reduction", "=", "16", ",", "**", "kwargs", ")", "\n", "return", "_create_senet", "(", "'legacy_seresnet50'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.legacy_seresnet101": [[425, 430], ["dict", "senet._create_senet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet._create_senet"], ["", "@", "register_model", "\n", "def", "legacy_seresnet101", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "block", "=", "SEResNetBottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "groups", "=", "1", ",", "reduction", "=", "16", ",", "**", "kwargs", ")", "\n", "return", "_create_senet", "(", "'legacy_seresnet101'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.legacy_seresnet152": [[432, 437], ["dict", "senet._create_senet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet._create_senet"], ["", "@", "register_model", "\n", "def", "legacy_seresnet152", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "block", "=", "SEResNetBottleneck", ",", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "groups", "=", "1", ",", "reduction", "=", "16", ",", "**", "kwargs", ")", "\n", "return", "_create_senet", "(", "'legacy_seresnet152'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.legacy_senet154": [[439, 445], ["dict", "senet._create_senet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet._create_senet"], ["", "@", "register_model", "\n", "def", "legacy_senet154", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "block", "=", "SEBottleneck", ",", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "groups", "=", "64", ",", "reduction", "=", "16", ",", "\n", "downsample_kernel_size", "=", "3", ",", "downsample_padding", "=", "1", ",", "inplanes", "=", "128", ",", "input_3x3", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_senet", "(", "'legacy_senet154'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.legacy_seresnext26_32x4d": [[447, 452], ["dict", "senet._create_senet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet._create_senet"], ["", "@", "register_model", "\n", "def", "legacy_seresnext26_32x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "block", "=", "SEResNeXtBottleneck", ",", "layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "groups", "=", "32", ",", "reduction", "=", "16", ",", "**", "kwargs", ")", "\n", "return", "_create_senet", "(", "'legacy_seresnext26_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.legacy_seresnext50_32x4d": [[454, 459], ["dict", "senet._create_senet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet._create_senet"], ["", "@", "register_model", "\n", "def", "legacy_seresnext50_32x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "block", "=", "SEResNeXtBottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "groups", "=", "32", ",", "reduction", "=", "16", ",", "**", "kwargs", ")", "\n", "return", "_create_senet", "(", "'legacy_seresnext50_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet.legacy_seresnext101_32x4d": [[461, 466], ["dict", "senet._create_senet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.senet._create_senet"], ["", "@", "register_model", "\n", "def", "legacy_seresnext101_32x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "block", "=", "SEResNeXtBottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "groups", "=", "32", ",", "reduction", "=", "16", ",", "**", "kwargs", ")", "\n", "return", "_create_senet", "(", "'legacy_seresnext101_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.DownsampleAvg.__init__": [[931, 942], ["torch.Module.__init__", "layers.conv_norm_act", "byobnet.LayerFn", "avg_pool_fn", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_chs", ",", "out_chs", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "apply_act", "=", "False", ",", "layers", ":", "LayerFn", "=", "None", ")", ":", "\n", "        ", "\"\"\" AvgPool Downsampling as in 'D' ResNet variants.\"\"\"", "\n", "super", "(", "DownsampleAvg", ",", "self", ")", ".", "__init__", "(", ")", "\n", "layers", "=", "layers", "or", "LayerFn", "(", ")", "\n", "avg_stride", "=", "stride", "if", "dilation", "==", "1", "else", "1", "\n", "if", "stride", ">", "1", "or", "dilation", ">", "1", ":", "\n", "            ", "avg_pool_fn", "=", "AvgPool2dSame", "if", "avg_stride", "==", "1", "and", "dilation", ">", "1", "else", "nn", ".", "AvgPool2d", "\n", "self", ".", "pool", "=", "avg_pool_fn", "(", "2", ",", "avg_stride", ",", "ceil_mode", "=", "True", ",", "count_include_pad", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "pool", "=", "nn", ".", "Identity", "(", ")", "\n", "", "self", ".", "conv", "=", "layers", ".", "conv_norm_act", "(", "in_chs", ",", "out_chs", ",", "1", ",", "apply_act", "=", "apply_act", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.DownsampleAvg.forward": [[943, 945], ["byobnet.DownsampleAvg.conv", "byobnet.DownsampleAvg.pool"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "conv", "(", "self", ".", "pool", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.BasicBlock.__init__": [[964, 984], ["torch.Module.__init__", "layers.make_divisible", "byobnet.num_groups", "byobnet.create_shortcut", "layers.conv_norm_act", "layers.conv_norm_act", "byobnet.LayerFn", "torch.Identity", "torch.Identity", "layers.attn", "torch.Identity", "torch.Identity", "layers.attn", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "layers.act"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.num_groups", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.create_shortcut"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "dilation", "=", "(", "1", ",", "1", ")", ",", "group_size", "=", "None", ",", "bottle_ratio", "=", "1.0", ",", "\n", "downsample", "=", "'avg'", ",", "attn_last", "=", "True", ",", "linear_out", "=", "False", ",", "layers", ":", "LayerFn", "=", "None", ",", "drop_block", "=", "None", ",", "\n", "drop_path_rate", "=", "0.", ")", ":", "\n", "        ", "super", "(", "BasicBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "layers", "=", "layers", "or", "LayerFn", "(", ")", "\n", "mid_chs", "=", "make_divisible", "(", "out_chs", "*", "bottle_ratio", ")", "\n", "groups", "=", "num_groups", "(", "group_size", ",", "mid_chs", ")", "\n", "\n", "self", ".", "shortcut", "=", "create_shortcut", "(", "\n", "downsample", ",", "in_chs", "=", "in_chs", ",", "out_chs", "=", "out_chs", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "\n", "apply_act", "=", "False", ",", "layers", "=", "layers", ")", "\n", "\n", "self", ".", "conv1_kxk", "=", "layers", ".", "conv_norm_act", "(", "in_chs", ",", "mid_chs", ",", "kernel_size", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", "[", "0", "]", ")", "\n", "self", ".", "attn", "=", "nn", ".", "Identity", "(", ")", "if", "attn_last", "or", "layers", ".", "attn", "is", "None", "else", "layers", ".", "attn", "(", "mid_chs", ")", "\n", "self", ".", "conv2_kxk", "=", "layers", ".", "conv_norm_act", "(", "\n", "mid_chs", ",", "out_chs", ",", "kernel_size", ",", "dilation", "=", "dilation", "[", "1", "]", ",", "groups", "=", "groups", ",", "drop_layer", "=", "drop_block", ",", "apply_act", "=", "False", ")", "\n", "self", ".", "attn_last", "=", "nn", ".", "Identity", "(", ")", "if", "not", "attn_last", "or", "layers", ".", "attn", "is", "None", "else", "layers", ".", "attn", "(", "out_chs", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path_rate", ")", "if", "drop_path_rate", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "act", "=", "nn", ".", "Identity", "(", ")", "if", "linear_out", "else", "layers", ".", "act", "(", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.BasicBlock.init_weights": [[985, 991], ["torch.init.zeros_", "torch.init.zeros_", "hasattr", "attn.reset_parameters"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters"], ["", "def", "init_weights", "(", "self", ",", "zero_init_last", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "zero_init_last", "and", "self", ".", "shortcut", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "conv2_kxk", ".", "bn", ".", "weight", ")", "\n", "", "for", "attn", "in", "(", "self", ".", "attn", ",", "self", ".", "attn_last", ")", ":", "\n", "            ", "if", "hasattr", "(", "attn", ",", "'reset_parameters'", ")", ":", "\n", "                ", "attn", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.BasicBlock.forward": [[992, 1001], ["byobnet.BasicBlock.conv1_kxk", "byobnet.BasicBlock.conv2_kxk", "byobnet.BasicBlock.attn", "byobnet.BasicBlock.drop_path", "byobnet.BasicBlock.act", "byobnet.BasicBlock.shortcut"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "x", "=", "self", ".", "conv1_kxk", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2_kxk", "(", "x", ")", "\n", "x", "=", "self", ".", "attn", "(", "x", ")", "\n", "x", "=", "self", ".", "drop_path", "(", "x", ")", "\n", "if", "self", ".", "shortcut", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", "+", "self", ".", "shortcut", "(", "shortcut", ")", "\n", "", "return", "self", ".", "act", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.BottleneckBlock.__init__": [[1007, 1032], ["torch.Module.__init__", "layers.make_divisible", "byobnet.num_groups", "byobnet.create_shortcut", "layers.conv_norm_act", "layers.conv_norm_act", "layers.conv_norm_act", "byobnet.LayerFn", "layers.conv_norm_act", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "layers.attn", "torch.Identity", "torch.Identity", "layers.attn", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "layers.act"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.num_groups", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.create_shortcut"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "dilation", "=", "(", "1", ",", "1", ")", ",", "bottle_ratio", "=", "1.", ",", "group_size", "=", "None", ",", "\n", "downsample", "=", "'avg'", ",", "attn_last", "=", "False", ",", "linear_out", "=", "False", ",", "extra_conv", "=", "False", ",", "bottle_in", "=", "False", ",", "\n", "layers", ":", "LayerFn", "=", "None", ",", "drop_block", "=", "None", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "        ", "super", "(", "BottleneckBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "layers", "=", "layers", "or", "LayerFn", "(", ")", "\n", "mid_chs", "=", "make_divisible", "(", "(", "in_chs", "if", "bottle_in", "else", "out_chs", ")", "*", "bottle_ratio", ")", "\n", "groups", "=", "num_groups", "(", "group_size", ",", "mid_chs", ")", "\n", "\n", "self", ".", "shortcut", "=", "create_shortcut", "(", "\n", "downsample", ",", "in_chs", "=", "in_chs", ",", "out_chs", "=", "out_chs", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "\n", "apply_act", "=", "False", ",", "layers", "=", "layers", ")", "\n", "\n", "self", ".", "conv1_1x1", "=", "layers", ".", "conv_norm_act", "(", "in_chs", ",", "mid_chs", ",", "1", ")", "\n", "self", ".", "conv2_kxk", "=", "layers", ".", "conv_norm_act", "(", "\n", "mid_chs", ",", "mid_chs", ",", "kernel_size", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", "[", "0", "]", ",", "groups", "=", "groups", ",", "drop_layer", "=", "drop_block", ")", "\n", "if", "extra_conv", ":", "\n", "            ", "self", ".", "conv2b_kxk", "=", "layers", ".", "conv_norm_act", "(", "mid_chs", ",", "mid_chs", ",", "kernel_size", ",", "dilation", "=", "dilation", "[", "1", "]", ",", "groups", "=", "groups", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv2b_kxk", "=", "nn", ".", "Identity", "(", ")", "\n", "", "self", ".", "attn", "=", "nn", ".", "Identity", "(", ")", "if", "attn_last", "or", "layers", ".", "attn", "is", "None", "else", "layers", ".", "attn", "(", "mid_chs", ")", "\n", "self", ".", "conv3_1x1", "=", "layers", ".", "conv_norm_act", "(", "mid_chs", ",", "out_chs", ",", "1", ",", "apply_act", "=", "False", ")", "\n", "self", ".", "attn_last", "=", "nn", ".", "Identity", "(", ")", "if", "not", "attn_last", "or", "layers", ".", "attn", "is", "None", "else", "layers", ".", "attn", "(", "out_chs", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path_rate", ")", "if", "drop_path_rate", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "act", "=", "nn", ".", "Identity", "(", ")", "if", "linear_out", "else", "layers", ".", "act", "(", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.BottleneckBlock.init_weights": [[1033, 1039], ["torch.init.zeros_", "torch.init.zeros_", "hasattr", "attn.reset_parameters"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters"], ["", "def", "init_weights", "(", "self", ",", "zero_init_last", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "zero_init_last", "and", "self", ".", "shortcut", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "conv3_1x1", ".", "bn", ".", "weight", ")", "\n", "", "for", "attn", "in", "(", "self", ".", "attn", ",", "self", ".", "attn_last", ")", ":", "\n", "            ", "if", "hasattr", "(", "attn", ",", "'reset_parameters'", ")", ":", "\n", "                ", "attn", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.BottleneckBlock.forward": [[1040, 1052], ["byobnet.BottleneckBlock.conv1_1x1", "byobnet.BottleneckBlock.conv2_kxk", "byobnet.BottleneckBlock.conv2b_kxk", "byobnet.BottleneckBlock.attn", "byobnet.BottleneckBlock.conv3_1x1", "byobnet.BottleneckBlock.attn_last", "byobnet.BottleneckBlock.drop_path", "byobnet.BottleneckBlock.act", "byobnet.BottleneckBlock.shortcut"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "x", "=", "self", ".", "conv1_1x1", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2_kxk", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2b_kxk", "(", "x", ")", "\n", "x", "=", "self", ".", "attn", "(", "x", ")", "\n", "x", "=", "self", ".", "conv3_1x1", "(", "x", ")", "\n", "x", "=", "self", ".", "attn_last", "(", "x", ")", "\n", "x", "=", "self", ".", "drop_path", "(", "x", ")", "\n", "if", "self", ".", "shortcut", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", "+", "self", ".", "shortcut", "(", "shortcut", ")", "\n", "", "return", "self", ".", "act", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.DarkBlock.__init__": [[1065, 1086], ["torch.Module.__init__", "layers.make_divisible", "byobnet.num_groups", "byobnet.create_shortcut", "layers.conv_norm_act", "layers.conv_norm_act", "byobnet.LayerFn", "torch.Identity", "torch.Identity", "layers.attn", "torch.Identity", "torch.Identity", "layers.attn", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "layers.act"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.num_groups", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.create_shortcut"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "dilation", "=", "(", "1", ",", "1", ")", ",", "bottle_ratio", "=", "1.0", ",", "group_size", "=", "None", ",", "\n", "downsample", "=", "'avg'", ",", "attn_last", "=", "True", ",", "linear_out", "=", "False", ",", "layers", ":", "LayerFn", "=", "None", ",", "drop_block", "=", "None", ",", "\n", "drop_path_rate", "=", "0.", ")", ":", "\n", "        ", "super", "(", "DarkBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "layers", "=", "layers", "or", "LayerFn", "(", ")", "\n", "mid_chs", "=", "make_divisible", "(", "out_chs", "*", "bottle_ratio", ")", "\n", "groups", "=", "num_groups", "(", "group_size", ",", "mid_chs", ")", "\n", "\n", "self", ".", "shortcut", "=", "create_shortcut", "(", "\n", "downsample", ",", "in_chs", "=", "in_chs", ",", "out_chs", "=", "out_chs", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "\n", "apply_act", "=", "False", ",", "layers", "=", "layers", ")", "\n", "\n", "self", ".", "conv1_1x1", "=", "layers", ".", "conv_norm_act", "(", "in_chs", ",", "mid_chs", ",", "1", ")", "\n", "self", ".", "attn", "=", "nn", ".", "Identity", "(", ")", "if", "attn_last", "or", "layers", ".", "attn", "is", "None", "else", "layers", ".", "attn", "(", "mid_chs", ")", "\n", "self", ".", "conv2_kxk", "=", "layers", ".", "conv_norm_act", "(", "\n", "mid_chs", ",", "out_chs", ",", "kernel_size", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", "[", "0", "]", ",", "\n", "groups", "=", "groups", ",", "drop_layer", "=", "drop_block", ",", "apply_act", "=", "False", ")", "\n", "self", ".", "attn_last", "=", "nn", ".", "Identity", "(", ")", "if", "not", "attn_last", "or", "layers", ".", "attn", "is", "None", "else", "layers", ".", "attn", "(", "out_chs", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path_rate", ")", "if", "drop_path_rate", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "act", "=", "nn", ".", "Identity", "(", ")", "if", "linear_out", "else", "layers", ".", "act", "(", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.DarkBlock.init_weights": [[1087, 1093], ["torch.init.zeros_", "torch.init.zeros_", "hasattr", "attn.reset_parameters"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters"], ["", "def", "init_weights", "(", "self", ",", "zero_init_last", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "zero_init_last", "and", "self", ".", "shortcut", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "conv2_kxk", ".", "bn", ".", "weight", ")", "\n", "", "for", "attn", "in", "(", "self", ".", "attn", ",", "self", ".", "attn_last", ")", ":", "\n", "            ", "if", "hasattr", "(", "attn", ",", "'reset_parameters'", ")", ":", "\n", "                ", "attn", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.DarkBlock.forward": [[1094, 1104], ["byobnet.DarkBlock.conv1_1x1", "byobnet.DarkBlock.attn", "byobnet.DarkBlock.conv2_kxk", "byobnet.DarkBlock.attn_last", "byobnet.DarkBlock.drop_path", "byobnet.DarkBlock.act", "byobnet.DarkBlock.shortcut"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "x", "=", "self", ".", "conv1_1x1", "(", "x", ")", "\n", "x", "=", "self", ".", "attn", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2_kxk", "(", "x", ")", "\n", "x", "=", "self", ".", "attn_last", "(", "x", ")", "\n", "x", "=", "self", ".", "drop_path", "(", "x", ")", "\n", "if", "self", ".", "shortcut", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", "+", "self", ".", "shortcut", "(", "shortcut", ")", "\n", "", "return", "self", ".", "act", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.EdgeBlock.__init__": [[1116, 1136], ["torch.Module.__init__", "layers.make_divisible", "byobnet.num_groups", "byobnet.create_shortcut", "layers.conv_norm_act", "layers.conv_norm_act", "byobnet.LayerFn", "torch.Identity", "torch.Identity", "layers.attn", "torch.Identity", "torch.Identity", "layers.attn", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "layers.act"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.num_groups", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.create_shortcut"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "dilation", "=", "(", "1", ",", "1", ")", ",", "bottle_ratio", "=", "1.0", ",", "group_size", "=", "None", ",", "\n", "downsample", "=", "'avg'", ",", "attn_last", "=", "False", ",", "linear_out", "=", "False", ",", "layers", ":", "LayerFn", "=", "None", ",", "\n", "drop_block", "=", "None", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "        ", "super", "(", "EdgeBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "layers", "=", "layers", "or", "LayerFn", "(", ")", "\n", "mid_chs", "=", "make_divisible", "(", "out_chs", "*", "bottle_ratio", ")", "\n", "groups", "=", "num_groups", "(", "group_size", ",", "mid_chs", ")", "\n", "\n", "self", ".", "shortcut", "=", "create_shortcut", "(", "\n", "downsample", ",", "in_chs", "=", "in_chs", ",", "out_chs", "=", "out_chs", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "\n", "apply_act", "=", "False", ",", "layers", "=", "layers", ")", "\n", "\n", "self", ".", "conv1_kxk", "=", "layers", ".", "conv_norm_act", "(", "\n", "in_chs", ",", "mid_chs", ",", "kernel_size", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", "[", "0", "]", ",", "groups", "=", "groups", ",", "drop_layer", "=", "drop_block", ")", "\n", "self", ".", "attn", "=", "nn", ".", "Identity", "(", ")", "if", "attn_last", "or", "layers", ".", "attn", "is", "None", "else", "layers", ".", "attn", "(", "mid_chs", ")", "\n", "self", ".", "conv2_1x1", "=", "layers", ".", "conv_norm_act", "(", "mid_chs", ",", "out_chs", ",", "1", ",", "apply_act", "=", "False", ")", "\n", "self", ".", "attn_last", "=", "nn", ".", "Identity", "(", ")", "if", "not", "attn_last", "or", "layers", ".", "attn", "is", "None", "else", "layers", ".", "attn", "(", "out_chs", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path_rate", ")", "if", "drop_path_rate", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "act", "=", "nn", ".", "Identity", "(", ")", "if", "linear_out", "else", "layers", ".", "act", "(", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.EdgeBlock.init_weights": [[1137, 1143], ["torch.init.zeros_", "torch.init.zeros_", "hasattr", "attn.reset_parameters"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters"], ["", "def", "init_weights", "(", "self", ",", "zero_init_last", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "zero_init_last", "and", "self", ".", "shortcut", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "conv2_1x1", ".", "bn", ".", "weight", ")", "\n", "", "for", "attn", "in", "(", "self", ".", "attn", ",", "self", ".", "attn_last", ")", ":", "\n", "            ", "if", "hasattr", "(", "attn", ",", "'reset_parameters'", ")", ":", "\n", "                ", "attn", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.EdgeBlock.forward": [[1144, 1154], ["byobnet.EdgeBlock.conv1_kxk", "byobnet.EdgeBlock.attn", "byobnet.EdgeBlock.conv2_1x1", "byobnet.EdgeBlock.attn_last", "byobnet.EdgeBlock.drop_path", "byobnet.EdgeBlock.act", "byobnet.EdgeBlock.shortcut"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "x", "=", "self", ".", "conv1_kxk", "(", "x", ")", "\n", "x", "=", "self", ".", "attn", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2_1x1", "(", "x", ")", "\n", "x", "=", "self", ".", "attn_last", "(", "x", ")", "\n", "x", "=", "self", ".", "drop_path", "(", "x", ")", "\n", "if", "self", ".", "shortcut", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", "+", "self", ".", "shortcut", "(", "shortcut", ")", "\n", "", "return", "self", ".", "act", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.RepVggBlock.__init__": [[1164, 1180], ["torch.Module.__init__", "byobnet.num_groups", "layers.conv_norm_act", "layers.conv_norm_act", "layers.act", "byobnet.LayerFn", "layers.norm_act", "torch.Identity", "torch.Identity", "layers.attn", "layers.DropPath", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.num_groups"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "dilation", "=", "(", "1", ",", "1", ")", ",", "bottle_ratio", "=", "1.0", ",", "group_size", "=", "None", ",", "\n", "downsample", "=", "''", ",", "layers", ":", "LayerFn", "=", "None", ",", "drop_block", "=", "None", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "        ", "super", "(", "RepVggBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "layers", "=", "layers", "or", "LayerFn", "(", ")", "\n", "groups", "=", "num_groups", "(", "group_size", ",", "in_chs", ")", "\n", "\n", "use_ident", "=", "in_chs", "==", "out_chs", "and", "stride", "==", "1", "and", "dilation", "[", "0", "]", "==", "dilation", "[", "1", "]", "\n", "self", ".", "identity", "=", "layers", ".", "norm_act", "(", "out_chs", ",", "apply_act", "=", "False", ")", "if", "use_ident", "else", "None", "\n", "self", ".", "conv_kxk", "=", "layers", ".", "conv_norm_act", "(", "\n", "in_chs", ",", "out_chs", ",", "kernel_size", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", "[", "0", "]", ",", "\n", "groups", "=", "groups", ",", "drop_layer", "=", "drop_block", ",", "apply_act", "=", "False", ")", "\n", "self", ".", "conv_1x1", "=", "layers", ".", "conv_norm_act", "(", "in_chs", ",", "out_chs", ",", "1", ",", "stride", "=", "stride", ",", "groups", "=", "groups", ",", "apply_act", "=", "False", ")", "\n", "self", ".", "attn", "=", "nn", ".", "Identity", "(", ")", "if", "layers", ".", "attn", "is", "None", "else", "layers", ".", "attn", "(", "out_chs", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path_rate", ")", "if", "drop_path_rate", ">", "0.", "and", "use_ident", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "act", "=", "layers", ".", "act", "(", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.RepVggBlock.init_weights": [[1181, 1189], ["byobnet.RepVggBlock.modules", "hasattr", "isinstance", "byobnet.RepVggBlock.attn.reset_parameters", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters"], ["", "def", "init_weights", "(", "self", ",", "zero_init_last", ":", "bool", "=", "False", ")", ":", "\n", "# NOTE this init overrides that base model init with specific changes for the block type", "\n", "        ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", ".1", ",", ".1", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "bias", ",", "0", ",", ".1", ")", "\n", "", "", "if", "hasattr", "(", "self", ".", "attn", ",", "'reset_parameters'", ")", ":", "\n", "            ", "self", ".", "attn", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.RepVggBlock.forward": [[1190, 1200], ["byobnet.RepVggBlock.attn", "byobnet.RepVggBlock.act", "byobnet.RepVggBlock.identity", "byobnet.RepVggBlock.drop_path", "byobnet.RepVggBlock.conv_1x1", "byobnet.RepVggBlock.conv_kxk", "byobnet.RepVggBlock.conv_1x1", "byobnet.RepVggBlock.conv_kxk"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "identity", "is", "None", ":", "\n", "            ", "x", "=", "self", ".", "conv_1x1", "(", "x", ")", "+", "self", ".", "conv_kxk", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "identity", "=", "self", ".", "identity", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_1x1", "(", "x", ")", "+", "self", ".", "conv_kxk", "(", "x", ")", "\n", "x", "=", "self", ".", "drop_path", "(", "x", ")", "# not in the paper / official impl, experimental", "\n", "x", "=", "x", "+", "identity", "\n", "", "x", "=", "self", ".", "attn", "(", "x", ")", "# no attn in the paper / official impl, experimental", "\n", "return", "self", ".", "act", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.SelfAttnBlock.__init__": [[1206, 1234], ["torch.Module.__init__", "layers.make_divisible", "byobnet.num_groups", "byobnet.create_shortcut", "layers.conv_norm_act", "layers.self_attn", "layers.conv_norm_act", "layers.conv_norm_act", "torch.Identity", "torch.Identity", "dict", "layers.norm_act", "torch.Identity", "torch.Identity", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "layers.act"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.num_groups", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.create_shortcut"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "dilation", "=", "(", "1", ",", "1", ")", ",", "bottle_ratio", "=", "1.", ",", "group_size", "=", "None", ",", "\n", "downsample", "=", "'avg'", ",", "extra_conv", "=", "False", ",", "linear_out", "=", "False", ",", "bottle_in", "=", "False", ",", "post_attn_na", "=", "True", ",", "\n", "feat_size", "=", "None", ",", "layers", ":", "LayerFn", "=", "None", ",", "drop_block", "=", "None", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "        ", "super", "(", "SelfAttnBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "layers", "is", "not", "None", "\n", "mid_chs", "=", "make_divisible", "(", "(", "in_chs", "if", "bottle_in", "else", "out_chs", ")", "*", "bottle_ratio", ")", "\n", "groups", "=", "num_groups", "(", "group_size", ",", "mid_chs", ")", "\n", "\n", "self", ".", "shortcut", "=", "create_shortcut", "(", "\n", "downsample", ",", "in_chs", "=", "in_chs", ",", "out_chs", "=", "out_chs", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "\n", "apply_act", "=", "False", ",", "layers", "=", "layers", ")", "\n", "\n", "self", ".", "conv1_1x1", "=", "layers", ".", "conv_norm_act", "(", "in_chs", ",", "mid_chs", ",", "1", ")", "\n", "if", "extra_conv", ":", "\n", "            ", "self", ".", "conv2_kxk", "=", "layers", ".", "conv_norm_act", "(", "\n", "mid_chs", ",", "mid_chs", ",", "kernel_size", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", "[", "0", "]", ",", "\n", "groups", "=", "groups", ",", "drop_layer", "=", "drop_block", ")", "\n", "stride", "=", "1", "# striding done via conv if enabled", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv2_kxk", "=", "nn", ".", "Identity", "(", ")", "\n", "", "opt_kwargs", "=", "{", "}", "if", "feat_size", "is", "None", "else", "dict", "(", "feat_size", "=", "feat_size", ")", "\n", "# FIXME need to dilate self attn to have dilated network support, moop moop", "\n", "self", ".", "self_attn", "=", "layers", ".", "self_attn", "(", "mid_chs", ",", "stride", "=", "stride", ",", "**", "opt_kwargs", ")", "\n", "self", ".", "post_attn", "=", "layers", ".", "norm_act", "(", "mid_chs", ")", "if", "post_attn_na", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "conv3_1x1", "=", "layers", ".", "conv_norm_act", "(", "mid_chs", ",", "out_chs", ",", "1", ",", "apply_act", "=", "False", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path_rate", ")", "if", "drop_path_rate", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "act", "=", "nn", ".", "Identity", "(", ")", "if", "linear_out", "else", "layers", ".", "act", "(", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.SelfAttnBlock.init_weights": [[1235, 1240], ["hasattr", "torch.init.zeros_", "torch.init.zeros_", "byobnet.SelfAttnBlock.self_attn.reset_parameters"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters"], ["", "def", "init_weights", "(", "self", ",", "zero_init_last", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "zero_init_last", "and", "self", ".", "shortcut", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "conv3_1x1", ".", "bn", ".", "weight", ")", "\n", "", "if", "hasattr", "(", "self", ".", "self_attn", ",", "'reset_parameters'", ")", ":", "\n", "            ", "self", ".", "self_attn", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.SelfAttnBlock.forward": [[1241, 1252], ["byobnet.SelfAttnBlock.conv1_1x1", "byobnet.SelfAttnBlock.conv2_kxk", "byobnet.SelfAttnBlock.self_attn", "byobnet.SelfAttnBlock.post_attn", "byobnet.SelfAttnBlock.conv3_1x1", "byobnet.SelfAttnBlock.drop_path", "byobnet.SelfAttnBlock.act", "byobnet.SelfAttnBlock.shortcut"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "x", "=", "self", ".", "conv1_1x1", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2_kxk", "(", "x", ")", "\n", "x", "=", "self", ".", "self_attn", "(", "x", ")", "\n", "x", "=", "self", ".", "post_attn", "(", "x", ")", "\n", "x", "=", "self", ".", "conv3_1x1", "(", "x", ")", "\n", "x", "=", "self", ".", "drop_path", "(", "x", ")", "\n", "if", "self", ".", "shortcut", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", "+", "self", ".", "shortcut", "(", "shortcut", ")", "\n", "", "return", "self", ".", "act", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.Stem.__init__": [[1276, 1320], ["torch.Sequential.__init__", "isinstance", "enumerate", "byobnet.Stem.feature_info.append", "byobnet.LayerFn", "len", "zip", "byobnet.Stem.add_module", "byobnet.Stem.feature_info.append", "byobnet.Stem.add_module", "dict", "byobnet.Stem.feature_info.append", "layer_fn", "pool.lower", "dict", "torch.MaxPool2d", "torch.MaxPool2d", "round", "dict", "range"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "3", ",", "stride", "=", "4", ",", "pool", "=", "'maxpool'", ",", "\n", "num_rep", "=", "3", ",", "num_act", "=", "None", ",", "chs_decay", "=", "0.5", ",", "layers", ":", "LayerFn", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "stride", "in", "(", "2", ",", "4", ")", "\n", "layers", "=", "layers", "or", "LayerFn", "(", ")", "\n", "\n", "if", "isinstance", "(", "out_chs", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "num_rep", "=", "len", "(", "out_chs", ")", "\n", "stem_chs", "=", "out_chs", "\n", "", "else", ":", "\n", "            ", "stem_chs", "=", "[", "round", "(", "out_chs", "*", "chs_decay", "**", "i", ")", "for", "i", "in", "range", "(", "num_rep", ")", "]", "[", ":", ":", "-", "1", "]", "\n", "\n", "", "self", ".", "stride", "=", "stride", "\n", "self", ".", "feature_info", "=", "[", "]", "# track intermediate features", "\n", "prev_feat", "=", "''", "\n", "stem_strides", "=", "[", "2", "]", "+", "[", "1", "]", "*", "(", "num_rep", "-", "1", ")", "\n", "if", "stride", "==", "4", "and", "not", "pool", ":", "\n", "# set last conv in stack to be strided if stride == 4 and no pooling layer", "\n", "            ", "stem_strides", "[", "-", "1", "]", "=", "2", "\n", "\n", "", "num_act", "=", "num_rep", "if", "num_act", "is", "None", "else", "num_act", "\n", "# if num_act < num_rep, first convs in stack won't have bn + act", "\n", "stem_norm_acts", "=", "[", "False", "]", "*", "(", "num_rep", "-", "num_act", ")", "+", "[", "True", "]", "*", "num_act", "\n", "prev_chs", "=", "in_chs", "\n", "curr_stride", "=", "1", "\n", "for", "i", ",", "(", "ch", ",", "s", ",", "na", ")", "in", "enumerate", "(", "zip", "(", "stem_chs", ",", "stem_strides", ",", "stem_norm_acts", ")", ")", ":", "\n", "            ", "layer_fn", "=", "layers", ".", "conv_norm_act", "if", "na", "else", "create_conv2d", "\n", "conv_name", "=", "f'conv{i + 1}'", "\n", "if", "i", ">", "0", "and", "s", ">", "1", ":", "\n", "                ", "self", ".", "feature_info", ".", "append", "(", "dict", "(", "num_chs", "=", "prev_chs", ",", "reduction", "=", "curr_stride", ",", "module", "=", "prev_feat", ")", ")", "\n", "", "self", ".", "add_module", "(", "conv_name", ",", "layer_fn", "(", "prev_chs", ",", "ch", ",", "kernel_size", "=", "kernel_size", ",", "stride", "=", "s", ")", ")", "\n", "prev_chs", "=", "ch", "\n", "curr_stride", "*=", "s", "\n", "prev_feat", "=", "conv_name", "\n", "\n", "", "if", "pool", "and", "'max'", "in", "pool", ".", "lower", "(", ")", ":", "\n", "            ", "self", ".", "feature_info", ".", "append", "(", "dict", "(", "num_chs", "=", "prev_chs", ",", "reduction", "=", "curr_stride", ",", "module", "=", "prev_feat", ")", ")", "\n", "self", ".", "add_module", "(", "'pool'", ",", "nn", ".", "MaxPool2d", "(", "3", ",", "2", ",", "1", ")", ")", "\n", "curr_stride", "*=", "2", "\n", "prev_feat", "=", "'pool'", "\n", "\n", "", "self", ".", "feature_info", ".", "append", "(", "dict", "(", "num_chs", "=", "prev_chs", ",", "reduction", "=", "curr_stride", ",", "module", "=", "prev_feat", ")", ")", "\n", "assert", "curr_stride", "==", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.ByobNet.__init__": [[1487, 1523], ["torch.Module.__init__", "byobnet.get_layer_fns", "int", "byobnet.create_byob_stem", "byobnet.ByobNet.feature_info.extend", "byobnet.reduce_feat_size", "byobnet.create_byob_stages", "byobnet.ByobNet.feature_info.extend", "get_layer_fns.ClassifierHead", "helpers.named_apply", "get_layer_fns.to_2tuple", "round", "int", "get_layer_fns.conv_norm_act", "torch.Identity", "torch.Identity", "dict", "functools.partial", "round"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.get_layer_fns", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.create_byob_stem", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.reduce_feat_size", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.create_byob_stages", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_apply"], ["def", "__init__", "(", "\n", "self", ",", "cfg", ":", "ByoModelCfg", ",", "num_classes", "=", "1000", ",", "in_chans", "=", "3", ",", "global_pool", "=", "'avg'", ",", "output_stride", "=", "32", ",", "\n", "zero_init_last", "=", "True", ",", "img_size", "=", "None", ",", "drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "layers", "=", "get_layer_fns", "(", "cfg", ")", "\n", "if", "cfg", ".", "fixed_input_size", ":", "\n", "            ", "assert", "img_size", "is", "not", "None", ",", "'img_size argument is required for fixed input size model'", "\n", "", "feat_size", "=", "to_2tuple", "(", "img_size", ")", "if", "img_size", "is", "not", "None", "else", "None", "\n", "\n", "self", ".", "feature_info", "=", "[", "]", "\n", "stem_chs", "=", "int", "(", "round", "(", "(", "cfg", ".", "stem_chs", "or", "cfg", ".", "blocks", "[", "0", "]", ".", "c", ")", "*", "cfg", ".", "width_factor", ")", ")", "\n", "self", ".", "stem", ",", "stem_feat", "=", "create_byob_stem", "(", "in_chans", ",", "stem_chs", ",", "cfg", ".", "stem_type", ",", "cfg", ".", "stem_pool", ",", "layers", "=", "layers", ")", "\n", "self", ".", "feature_info", ".", "extend", "(", "stem_feat", "[", ":", "-", "1", "]", ")", "\n", "feat_size", "=", "reduce_feat_size", "(", "feat_size", ",", "stride", "=", "stem_feat", "[", "-", "1", "]", "[", "'reduction'", "]", ")", "\n", "\n", "self", ".", "stages", ",", "stage_feat", "=", "create_byob_stages", "(", "\n", "cfg", ",", "drop_path_rate", ",", "output_stride", ",", "stem_feat", "[", "-", "1", "]", ",", "layers", "=", "layers", ",", "feat_size", "=", "feat_size", ")", "\n", "self", ".", "feature_info", ".", "extend", "(", "stage_feat", "[", ":", "-", "1", "]", ")", "\n", "\n", "prev_chs", "=", "stage_feat", "[", "-", "1", "]", "[", "'num_chs'", "]", "\n", "if", "cfg", ".", "num_features", ":", "\n", "            ", "self", ".", "num_features", "=", "int", "(", "round", "(", "cfg", ".", "width_factor", "*", "cfg", ".", "num_features", ")", ")", "\n", "self", ".", "final_conv", "=", "layers", ".", "conv_norm_act", "(", "prev_chs", ",", "self", ".", "num_features", ",", "1", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_features", "=", "prev_chs", "\n", "self", ".", "final_conv", "=", "nn", ".", "Identity", "(", ")", "\n", "", "self", ".", "feature_info", "+=", "[", "\n", "dict", "(", "num_chs", "=", "self", ".", "num_features", ",", "reduction", "=", "stage_feat", "[", "-", "1", "]", "[", "'reduction'", "]", ",", "module", "=", "'final_conv'", ")", "]", "\n", "\n", "self", ".", "head", "=", "ClassifierHead", "(", "self", ".", "num_features", ",", "num_classes", ",", "pool_type", "=", "global_pool", ",", "drop_rate", "=", "self", ".", "drop_rate", ")", "\n", "\n", "# init weights", "\n", "named_apply", "(", "partial", "(", "_init_weights", ",", "zero_init_last", "=", "zero_init_last", ")", ",", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.ByobNet.group_matcher": [[1524, 1534], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "\n", "stem", "=", "r'^stem'", ",", "\n", "blocks", "=", "[", "\n", "(", "r'^stages\\.(\\d+)'", "if", "coarse", "else", "r'^stages\\.(\\d+)\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^final_conv'", ",", "(", "99999", ",", ")", ")", "\n", "]", "\n", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.ByobNet.set_grad_checkpointing": [[1535, 1538], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.ByobNet.get_classifier": [[1539, 1542], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", ".", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.ByobNet.reset_classifier": [[1543, 1545], ["layers.ClassifierHead"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "head", "=", "ClassifierHead", "(", "self", ".", "num_features", ",", "num_classes", ",", "pool_type", "=", "global_pool", ",", "drop_rate", "=", "self", ".", "drop_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.ByobNet.forward_features": [[1546, 1554], ["byobnet.ByobNet.stem", "byobnet.ByobNet.final_conv", "helpers.checkpoint_seq", "byobnet.ByobNet.stages", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet.stages"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "stem", "(", "x", ")", "\n", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "stages", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "stages", "(", "x", ")", "\n", "", "x", "=", "self", ".", "final_conv", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.ByobNet.forward_head": [[1555, 1557], ["byobnet.ByobNet.head"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "head", "(", "x", ",", "pre_logits", "=", "pre_logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.ByobNet.forward": [[1558, 1562], ["byobnet.ByobNet.forward_features", "byobnet.ByobNet.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._cfg": [[45, 52], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bilinear'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'stem.conv'", ",", "'classifier'", ":", "'head.fc'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._cfgr": [[55, 62], ["None"], "function", ["None"], ["", "def", "_cfgr", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "256", ",", "256", ")", ",", "'pool_size'", ":", "(", "8", ",", "8", ")", ",", "\n", "'crop_pct'", ":", "0.9", ",", "'interpolation'", ":", "'bicubic'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'stem.conv1.conv'", ",", "'classifier'", ":", "'head.fc'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._rep_vgg_bcfg": [[211, 218], ["tuple", "byobnet.ByoBlockCfg", "zip"], "function", ["None"], ["", "def", "_rep_vgg_bcfg", "(", "d", "=", "(", "4", ",", "6", ",", "16", ",", "1", ")", ",", "wf", "=", "(", "1.", ",", "1.", ",", "1.", ",", "1.", ")", ",", "groups", "=", "0", ")", ":", "\n", "    ", "c", "=", "(", "64", ",", "128", ",", "256", ",", "512", ")", "\n", "group_size", "=", "0", "\n", "if", "groups", ">", "0", ":", "\n", "        ", "group_size", "=", "lambda", "chs", ",", "idx", ":", "chs", "//", "groups", "if", "(", "idx", "+", "1", ")", "%", "2", "==", "0", "else", "0", "\n", "", "bcfg", "=", "tuple", "(", "[", "ByoBlockCfg", "(", "type", "=", "'rep'", ",", "d", "=", "d", ",", "c", "=", "c", "*", "wf", ",", "gs", "=", "group_size", ")", "for", "d", ",", "c", ",", "wf", "in", "zip", "(", "d", ",", "c", ",", "wf", ")", "]", ")", "\n", "return", "bcfg", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.interleave_blocks": [[220, 236], ["isinstance", "set", "range", "tuple", "len", "list", "range", "byobnet.ByoBlockCfg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEmaV2.set"], ["", "def", "interleave_blocks", "(", "\n", "types", ":", "Tuple", "[", "str", ",", "str", "]", ",", "d", ",", "every", ":", "Union", "[", "int", ",", "List", "[", "int", "]", "]", "=", "1", ",", "first", ":", "bool", "=", "False", ",", "**", "kwargs", "\n", ")", "->", "Tuple", "[", "ByoBlockCfg", "]", ":", "\n", "    ", "\"\"\" interleave 2 block types in stack\n    \"\"\"", "\n", "assert", "len", "(", "types", ")", "==", "2", "\n", "if", "isinstance", "(", "every", ",", "int", ")", ":", "\n", "        ", "every", "=", "list", "(", "range", "(", "0", "if", "first", "else", "every", ",", "d", ",", "every", "+", "1", ")", ")", "\n", "if", "not", "every", ":", "\n", "            ", "every", "=", "[", "d", "-", "1", "]", "\n", "", "", "set", "(", "every", ")", "\n", "blocks", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "d", ")", ":", "\n", "        ", "block_type", "=", "types", "[", "1", "]", "if", "i", "in", "every", "else", "types", "[", "0", "]", "\n", "blocks", "+=", "[", "ByoBlockCfg", "(", "type", "=", "block_type", ",", "d", "=", "1", ",", "**", "kwargs", ")", "]", "\n", "", "return", "tuple", "(", "blocks", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.gernet_l": [[661, 667], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["@", "register_model", "\n", "def", "gernet_l", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" GEResNet-Large (GENet-Large from official impl)\n    `Neural Architecture Design for GPU-Efficient Networks` - https://arxiv.org/abs/2006.14090\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'gernet_l'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.gernet_m": [[669, 675], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "gernet_m", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" GEResNet-Medium (GENet-Normal from official impl)\n    `Neural Architecture Design for GPU-Efficient Networks` - https://arxiv.org/abs/2006.14090\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'gernet_m'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.gernet_s": [[677, 683], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "gernet_s", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" EResNet-Small (GENet-Small from official impl)\n    `Neural Architecture Design for GPU-Efficient Networks` - https://arxiv.org/abs/2006.14090\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'gernet_s'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.repvgg_a2": [[685, 691], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "repvgg_a2", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" RepVGG-A2\n    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'repvgg_a2'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.repvgg_b0": [[693, 699], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "repvgg_b0", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" RepVGG-B0\n    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'repvgg_b0'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.repvgg_b1": [[701, 707], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "repvgg_b1", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" RepVGG-B1\n    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'repvgg_b1'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.repvgg_b1g4": [[709, 715], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "repvgg_b1g4", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" RepVGG-B1g4\n    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'repvgg_b1g4'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.repvgg_b2": [[717, 723], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "repvgg_b2", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" RepVGG-B2\n    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'repvgg_b2'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.repvgg_b2g4": [[725, 731], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "repvgg_b2g4", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" RepVGG-B2g4\n    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'repvgg_b2g4'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.repvgg_b3": [[733, 739], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "repvgg_b3", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" RepVGG-B3\n    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'repvgg_b3'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.repvgg_b3g4": [[741, 747], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "repvgg_b3g4", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" RepVGG-B3g4\n    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'repvgg_b3g4'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.resnet51q": [[749, 754], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "resnet51q", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'resnet51q'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.resnet61q": [[756, 761], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "resnet61q", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'resnet61q'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.resnext26ts": [[763, 768], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "resnext26ts", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'resnext26ts'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.gcresnext26ts": [[770, 775], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "gcresnext26ts", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'gcresnext26ts'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.seresnext26ts": [[777, 782], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "seresnext26ts", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'seresnext26ts'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.eca_resnext26ts": [[784, 789], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "eca_resnext26ts", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'eca_resnext26ts'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.bat_resnext26ts": [[791, 796], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "bat_resnext26ts", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'bat_resnext26ts'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.resnet32ts": [[798, 803], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "resnet32ts", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'resnet32ts'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.resnet33ts": [[805, 810], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "resnet33ts", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'resnet33ts'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.gcresnet33ts": [[812, 817], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "gcresnet33ts", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'gcresnet33ts'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.seresnet33ts": [[819, 824], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "seresnet33ts", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'seresnet33ts'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.eca_resnet33ts": [[826, 831], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "eca_resnet33ts", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'eca_resnet33ts'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.gcresnet50t": [[833, 838], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "gcresnet50t", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'gcresnet50t'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.gcresnext50ts": [[840, 845], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "gcresnext50ts", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'gcresnext50ts'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.regnetz_b16": [[847, 852], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "regnetz_b16", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'regnetz_b16'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.regnetz_c16": [[854, 859], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "regnetz_c16", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'regnetz_c16'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.regnetz_d32": [[861, 866], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "regnetz_d32", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'regnetz_d32'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.regnetz_d8": [[868, 873], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "regnetz_d8", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'regnetz_d8'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.regnetz_e8": [[875, 880], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "regnetz_e8", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'regnetz_e8'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.regnetz_b16_evos": [[882, 887], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "regnetz_b16_evos", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'regnetz_b16_evos'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.regnetz_c16_evos": [[889, 894], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "regnetz_c16_evos", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'regnetz_c16_evos'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.regnetz_d8_evos": [[896, 901], ["byobnet._create_byobnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet"], ["", "@", "register_model", "\n", "def", "regnetz_d8_evos", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "return", "_create_byobnet", "(", "'regnetz_d8_evos'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.expand_blocks_cfg": [[903, 910], ["enumerate", "isinstance", "dataclasses.replace", "range"], "function", ["None"], ["", "def", "expand_blocks_cfg", "(", "stage_blocks_cfg", ":", "Union", "[", "ByoBlockCfg", ",", "Sequence", "[", "ByoBlockCfg", "]", "]", ")", "->", "List", "[", "ByoBlockCfg", "]", ":", "\n", "    ", "if", "not", "isinstance", "(", "stage_blocks_cfg", ",", "Sequence", ")", ":", "\n", "        ", "stage_blocks_cfg", "=", "(", "stage_blocks_cfg", ",", ")", "\n", "", "block_cfgs", "=", "[", "]", "\n", "for", "i", ",", "cfg", "in", "enumerate", "(", "stage_blocks_cfg", ")", ":", "\n", "        ", "block_cfgs", "+=", "[", "replace", "(", "cfg", ",", "d", "=", "1", ")", "for", "_", "in", "range", "(", "cfg", ".", "d", ")", "]", "\n", "", "return", "block_cfgs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.num_groups": [[912, 919], ["None"], "function", ["None"], ["", "def", "num_groups", "(", "group_size", ",", "channels", ")", ":", "\n", "    ", "if", "not", "group_size", ":", "# 0 or None", "\n", "        ", "return", "1", "# normal conv with 1 group", "\n", "", "else", ":", "\n", "# NOTE group_size == 1 -> depthwise conv", "\n", "        ", "assert", "channels", "%", "group_size", "==", "0", "\n", "return", "channels", "//", "group_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.create_shortcut": [[947, 958], ["torch.Identity", "byobnet.DownsampleAvg", "layers.conv_norm_act"], "function", ["None"], ["", "", "def", "create_shortcut", "(", "downsample_type", ",", "layers", ":", "LayerFn", ",", "in_chs", ",", "out_chs", ",", "stride", ",", "dilation", ",", "**", "kwargs", ")", ":", "\n", "    ", "assert", "downsample_type", "in", "(", "'avg'", ",", "'conv1x1'", ",", "''", ")", "\n", "if", "in_chs", "!=", "out_chs", "or", "stride", "!=", "1", "or", "dilation", "[", "0", "]", "!=", "dilation", "[", "1", "]", ":", "\n", "        ", "if", "not", "downsample_type", ":", "\n", "            ", "return", "None", "# no shortcut", "\n", "", "elif", "downsample_type", "==", "'avg'", ":", "\n", "            ", "return", "DownsampleAvg", "(", "in_chs", ",", "out_chs", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", "[", "0", "]", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "            ", "return", "layers", ".", "conv_norm_act", "(", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "1", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", "[", "0", "]", ",", "**", "kwargs", ")", "\n", "", "", "else", ":", "\n", "        ", "return", "nn", ".", "Identity", "(", ")", "# identity shortcut", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.register_block": [[1263, 1265], ["None"], "function", ["None"], ["def", "register_block", "(", "block_type", ":", "str", ",", "block_fn", ":", "nn", ".", "Module", ")", ":", "\n", "    ", "_block_registry", "[", "block_type", "]", "=", "block_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.create_block": [[1267, 1272], ["isinstance", "block"], "function", ["None"], ["", "def", "create_block", "(", "block", ":", "Union", "[", "str", ",", "nn", ".", "Module", "]", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "isinstance", "(", "block", ",", "(", "nn", ".", "Module", ",", "partial", ")", ")", ":", "\n", "        ", "return", "block", "(", "**", "kwargs", ")", "\n", "", "assert", "block", "in", "_block_registry", ",", "f'Unknown block type ({block}'", "\n", "return", "_block_registry", "[", "block", "]", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.create_byob_stem": [[1322, 1355], ["isinstance", "byobnet.LayerFn", "byobnet.Stem", "byobnet.Stem", "dict", "dict", "byobnet.Stem", "byobnet.RepVggBlock", "byobnet.Stem", "layers.conv_norm_act", "byobnet.Stem", "layers.conv_norm_act"], "function", ["None"], ["", "", "def", "create_byob_stem", "(", "in_chs", ",", "out_chs", ",", "stem_type", "=", "''", ",", "pool_type", "=", "''", ",", "feat_prefix", "=", "'stem'", ",", "layers", ":", "LayerFn", "=", "None", ")", ":", "\n", "    ", "layers", "=", "layers", "or", "LayerFn", "(", ")", "\n", "assert", "stem_type", "in", "(", "''", ",", "'quad'", ",", "'quad2'", ",", "'tiered'", ",", "'deep'", ",", "'rep'", ",", "'7x7'", ",", "'3x3'", ")", "\n", "if", "'quad'", "in", "stem_type", ":", "\n", "# based on NFNet stem, stack of 4 3x3 convs", "\n", "        ", "num_act", "=", "2", "if", "'quad2'", "in", "stem_type", "else", "None", "\n", "stem", "=", "Stem", "(", "in_chs", ",", "out_chs", ",", "num_rep", "=", "4", ",", "num_act", "=", "num_act", ",", "pool", "=", "pool_type", ",", "layers", "=", "layers", ")", "\n", "", "elif", "'tiered'", "in", "stem_type", ":", "\n", "# 3x3 stack of 3 convs as in my ResNet-T", "\n", "        ", "stem", "=", "Stem", "(", "in_chs", ",", "(", "3", "*", "out_chs", "//", "8", ",", "out_chs", "//", "2", ",", "out_chs", ")", ",", "pool", "=", "pool_type", ",", "layers", "=", "layers", ")", "\n", "", "elif", "'deep'", "in", "stem_type", ":", "\n", "# 3x3 stack of 3 convs as in ResNet-D", "\n", "        ", "stem", "=", "Stem", "(", "in_chs", ",", "out_chs", ",", "num_rep", "=", "3", ",", "chs_decay", "=", "1.0", ",", "pool", "=", "pool_type", ",", "layers", "=", "layers", ")", "\n", "", "elif", "'rep'", "in", "stem_type", ":", "\n", "        ", "stem", "=", "RepVggBlock", "(", "in_chs", ",", "out_chs", ",", "stride", "=", "2", ",", "layers", "=", "layers", ")", "\n", "", "elif", "'7x7'", "in", "stem_type", ":", "\n", "# 7x7 stem conv as in ResNet", "\n", "        ", "if", "pool_type", ":", "\n", "            ", "stem", "=", "Stem", "(", "in_chs", ",", "out_chs", ",", "7", ",", "num_rep", "=", "1", ",", "pool", "=", "pool_type", ",", "layers", "=", "layers", ")", "\n", "", "else", ":", "\n", "            ", "stem", "=", "layers", ".", "conv_norm_act", "(", "in_chs", ",", "out_chs", ",", "7", ",", "stride", "=", "2", ")", "\n", "", "", "else", ":", "\n", "# 3x3 stem conv as in RegNet is the default", "\n", "        ", "if", "pool_type", ":", "\n", "            ", "stem", "=", "Stem", "(", "in_chs", ",", "out_chs", ",", "3", ",", "num_rep", "=", "1", ",", "pool", "=", "pool_type", ",", "layers", "=", "layers", ")", "\n", "", "else", ":", "\n", "            ", "stem", "=", "layers", ".", "conv_norm_act", "(", "in_chs", ",", "out_chs", ",", "3", ",", "stride", "=", "2", ")", "\n", "\n", "", "", "if", "isinstance", "(", "stem", ",", "Stem", ")", ":", "\n", "        ", "feature_info", "=", "[", "dict", "(", "f", ",", "module", "=", "'.'", ".", "join", "(", "[", "feat_prefix", ",", "f", "[", "'module'", "]", "]", ")", ")", "for", "f", "in", "stem", ".", "feature_info", "]", "\n", "", "else", ":", "\n", "        ", "feature_info", "=", "[", "dict", "(", "num_chs", "=", "out_chs", ",", "reduction", "=", "2", ",", "module", "=", "feat_prefix", ")", "]", "\n", "", "return", "stem", ",", "feature_info", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.reduce_feat_size": [[1357, 1359], ["tuple"], "function", ["None"], ["", "def", "reduce_feat_size", "(", "feat_size", ",", "stride", "=", "2", ")", ":", "\n", "    ", "return", "None", "if", "feat_size", "is", "None", "else", "tuple", "(", "[", "s", "//", "stride", "for", "s", "in", "feat_size", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.override_kwargs": [[1361, 1371], ["None"], "function", ["None"], ["", "def", "override_kwargs", "(", "block_kwargs", ",", "model_kwargs", ")", ":", "\n", "    ", "\"\"\" Override model level attn/self-attn/block kwargs w/ block level\n\n    NOTE: kwargs are NOT merged across levels, block_kwargs will fully replace model_kwargs\n    for the block if set to anything that isn't None.\n\n    i.e. an empty block_kwargs dict will remove kwargs set at model level for that block\n    \"\"\"", "\n", "out_kwargs", "=", "block_kwargs", "if", "block_kwargs", "is", "not", "None", "else", "model_kwargs", "\n", "return", "out_kwargs", "or", "{", "}", "# make sure None isn't returned", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.update_block_kwargs": [[1373, 1407], ["block_kwargs.update", "dataclasses.replace", "dataclasses.replace", "byobnet.override_kwargs", "byobnet.override_kwargs", "byobnet.override_kwargs", "functools.partial", "functools.partial", "layers.get_attn", "layers.get_attn"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.override_kwargs", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.override_kwargs", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.override_kwargs", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.get_attn", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.get_attn"], ["", "def", "update_block_kwargs", "(", "block_kwargs", ":", "Dict", "[", "str", ",", "Any", "]", ",", "block_cfg", ":", "ByoBlockCfg", ",", "model_cfg", ":", "ByoModelCfg", ",", ")", ":", "\n", "    ", "layer_fns", "=", "block_kwargs", "[", "'layers'", "]", "\n", "\n", "# override attn layer / args with block local config", "\n", "attn_set", "=", "block_cfg", ".", "attn_layer", "is", "not", "None", "\n", "if", "attn_set", "or", "block_cfg", ".", "attn_kwargs", "is", "not", "None", ":", "\n", "# override attn layer config", "\n", "        ", "if", "attn_set", "and", "not", "block_cfg", ".", "attn_layer", ":", "\n", "# empty string for attn_layer type will disable attn for this block", "\n", "            ", "attn_layer", "=", "None", "\n", "", "else", ":", "\n", "            ", "attn_kwargs", "=", "override_kwargs", "(", "block_cfg", ".", "attn_kwargs", ",", "model_cfg", ".", "attn_kwargs", ")", "\n", "attn_layer", "=", "block_cfg", ".", "attn_layer", "or", "model_cfg", ".", "attn_layer", "\n", "attn_layer", "=", "partial", "(", "get_attn", "(", "attn_layer", ")", ",", "**", "attn_kwargs", ")", "if", "attn_layer", "is", "not", "None", "else", "None", "\n", "", "layer_fns", "=", "replace", "(", "layer_fns", ",", "attn", "=", "attn_layer", ")", "\n", "\n", "# override self-attn layer / args with block local cfg", "\n", "", "self_attn_set", "=", "block_cfg", ".", "self_attn_layer", "is", "not", "None", "\n", "if", "self_attn_set", "or", "block_cfg", ".", "self_attn_kwargs", "is", "not", "None", ":", "\n", "# override attn layer config", "\n", "        ", "if", "self_attn_set", "and", "not", "block_cfg", ".", "self_attn_layer", ":", "# attn_layer == ''", "\n", "# empty string for self_attn_layer type will disable attn for this block", "\n", "            ", "self_attn_layer", "=", "None", "\n", "", "else", ":", "\n", "            ", "self_attn_kwargs", "=", "override_kwargs", "(", "block_cfg", ".", "self_attn_kwargs", ",", "model_cfg", ".", "self_attn_kwargs", ")", "\n", "self_attn_layer", "=", "block_cfg", ".", "self_attn_layer", "or", "model_cfg", ".", "self_attn_layer", "\n", "self_attn_layer", "=", "partial", "(", "get_attn", "(", "self_attn_layer", ")", ",", "**", "self_attn_kwargs", ")", "if", "self_attn_layer", "is", "not", "None", "else", "None", "\n", "", "layer_fns", "=", "replace", "(", "layer_fns", ",", "self_attn", "=", "self_attn_layer", ")", "\n", "\n", "", "block_kwargs", "[", "'layers'", "]", "=", "layer_fns", "\n", "\n", "# add additional block_kwargs specified in block_cfg or model_cfg, precedence to block if set", "\n", "block_kwargs", ".", "update", "(", "override_kwargs", "(", "block_cfg", ".", "block_kwargs", ",", "model_cfg", ".", "block_kwargs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.create_byob_stages": [[1409, 1467], ["enumerate", "feature_info.append", "byobnet.LayerFn", "byobnet.expand_blocks_cfg", "sum", "x.tolist", "enumerate", "dict", "torch.Sequential", "torch.linspace().split", "torch.linspace().split", "feature_info.append", "layers.make_divisible", "isinstance", "dict", "block_kwargs_fn", "torch.Sequential", "group_size.", "byobnet.create_block", "byobnet.reduce_feat_size", "torch.linspace", "torch.linspace", "sum"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.expand_blocks_cfg", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.create_block", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.reduce_feat_size"], ["", "def", "create_byob_stages", "(", "\n", "cfg", ":", "ByoModelCfg", ",", "drop_path_rate", ":", "float", ",", "output_stride", ":", "int", ",", "stem_feat", ":", "Dict", "[", "str", ",", "Any", "]", ",", "\n", "feat_size", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "layers", ":", "Optional", "[", "LayerFn", "]", "=", "None", ",", "\n", "block_kwargs_fn", ":", "Optional", "[", "Callable", "]", "=", "update_block_kwargs", ")", ":", "\n", "\n", "    ", "layers", "=", "layers", "or", "LayerFn", "(", ")", "\n", "feature_info", "=", "[", "]", "\n", "block_cfgs", "=", "[", "expand_blocks_cfg", "(", "s", ")", "for", "s", "in", "cfg", ".", "blocks", "]", "\n", "depths", "=", "[", "sum", "(", "[", "bc", ".", "d", "for", "bc", "in", "stage_bcs", "]", ")", "for", "stage_bcs", "in", "block_cfgs", "]", "\n", "dpr", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "sum", "(", "depths", ")", ")", ".", "split", "(", "depths", ")", "]", "\n", "dilation", "=", "1", "\n", "net_stride", "=", "stem_feat", "[", "'reduction'", "]", "\n", "prev_chs", "=", "stem_feat", "[", "'num_chs'", "]", "\n", "prev_feat", "=", "stem_feat", "\n", "stages", "=", "[", "]", "\n", "for", "stage_idx", ",", "stage_block_cfgs", "in", "enumerate", "(", "block_cfgs", ")", ":", "\n", "        ", "stride", "=", "stage_block_cfgs", "[", "0", "]", ".", "s", "\n", "if", "stride", "!=", "1", "and", "prev_feat", ":", "\n", "            ", "feature_info", ".", "append", "(", "prev_feat", ")", "\n", "", "if", "net_stride", ">=", "output_stride", "and", "stride", ">", "1", ":", "\n", "            ", "dilation", "*=", "stride", "\n", "stride", "=", "1", "\n", "", "net_stride", "*=", "stride", "\n", "first_dilation", "=", "1", "if", "dilation", "in", "(", "1", ",", "2", ")", "else", "2", "\n", "\n", "blocks", "=", "[", "]", "\n", "for", "block_idx", ",", "block_cfg", "in", "enumerate", "(", "stage_block_cfgs", ")", ":", "\n", "            ", "out_chs", "=", "make_divisible", "(", "block_cfg", ".", "c", "*", "cfg", ".", "width_factor", ")", "\n", "group_size", "=", "block_cfg", ".", "gs", "\n", "if", "isinstance", "(", "group_size", ",", "Callable", ")", ":", "\n", "                ", "group_size", "=", "group_size", "(", "out_chs", ",", "block_idx", ")", "\n", "", "block_kwargs", "=", "dict", "(", "# Blocks used in this model must accept these arguments", "\n", "in_chs", "=", "prev_chs", ",", "\n", "out_chs", "=", "out_chs", ",", "\n", "stride", "=", "stride", "if", "block_idx", "==", "0", "else", "1", ",", "\n", "dilation", "=", "(", "first_dilation", ",", "dilation", ")", ",", "\n", "group_size", "=", "group_size", ",", "\n", "bottle_ratio", "=", "block_cfg", ".", "br", ",", "\n", "downsample", "=", "cfg", ".", "downsample", ",", "\n", "drop_path_rate", "=", "dpr", "[", "stage_idx", "]", "[", "block_idx", "]", ",", "\n", "layers", "=", "layers", ",", "\n", ")", "\n", "if", "block_cfg", ".", "type", "in", "(", "'self_attn'", ",", ")", ":", "\n", "# add feat_size arg for blocks that support/need it", "\n", "                ", "block_kwargs", "[", "'feat_size'", "]", "=", "feat_size", "\n", "", "block_kwargs_fn", "(", "block_kwargs", ",", "block_cfg", "=", "block_cfg", ",", "model_cfg", "=", "cfg", ")", "\n", "blocks", "+=", "[", "create_block", "(", "block_cfg", ".", "type", ",", "**", "block_kwargs", ")", "]", "\n", "first_dilation", "=", "dilation", "\n", "prev_chs", "=", "out_chs", "\n", "if", "stride", ">", "1", "and", "block_idx", "==", "0", ":", "\n", "                ", "feat_size", "=", "reduce_feat_size", "(", "feat_size", ",", "stride", ")", "\n", "\n", "", "", "stages", "+=", "[", "nn", ".", "Sequential", "(", "*", "blocks", ")", "]", "\n", "prev_feat", "=", "dict", "(", "num_chs", "=", "prev_chs", ",", "reduction", "=", "net_stride", ",", "module", "=", "f'stages.{stage_idx}'", ")", "\n", "\n", "", "feature_info", ".", "append", "(", "prev_feat", ")", "\n", "return", "nn", ".", "Sequential", "(", "*", "stages", ")", ",", "feature_info", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet.get_layer_fns": [[1469, 1477], ["layers.get_act_layer", "layers.get_norm_act_layer", "functools.partial", "byobnet.LayerFn", "functools.partial", "functools.partial", "layers.get_attn", "layers.get_attn"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.get_norm_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.get_attn", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.get_attn"], ["", "def", "get_layer_fns", "(", "cfg", ":", "ByoModelCfg", ")", ":", "\n", "    ", "act", "=", "get_act_layer", "(", "cfg", ".", "act_layer", ")", "\n", "norm_act", "=", "get_norm_act_layer", "(", "norm_layer", "=", "cfg", ".", "norm_layer", ",", "act_layer", "=", "act", ")", "\n", "conv_norm_act", "=", "partial", "(", "ConvNormAct", ",", "norm_layer", "=", "cfg", ".", "norm_layer", ",", "act_layer", "=", "act", ")", "\n", "attn", "=", "partial", "(", "get_attn", "(", "cfg", ".", "attn_layer", ")", ",", "**", "cfg", ".", "attn_kwargs", ")", "if", "cfg", ".", "attn_layer", "else", "None", "\n", "self_attn", "=", "partial", "(", "get_attn", "(", "cfg", ".", "self_attn_layer", ")", ",", "**", "cfg", ".", "self_attn_kwargs", ")", "if", "cfg", ".", "self_attn_layer", "else", "None", "\n", "layer_fn", "=", "LayerFn", "(", "conv_norm_act", "=", "conv_norm_act", ",", "norm_act", "=", "norm_act", ",", "act", "=", "act", ",", "attn", "=", "attn", ",", "self_attn", "=", "self_attn", ")", "\n", "return", "layer_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._init_weights": [[1564, 1580], ["isinstance", "module.weight.data.normal_", "isinstance", "math.sqrt", "module.bias.data.zero_", "torch.init.normal_", "isinstance", "torch.init.zeros_", "torch.init.ones_", "torch.init.zeros_", "hasattr", "module.init_weights"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights"], ["", "", "def", "_init_weights", "(", "module", ",", "name", "=", "''", ",", "zero_init_last", "=", "False", ")", ":", "\n", "    ", "if", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", ":", "\n", "        ", "fan_out", "=", "module", ".", "kernel_size", "[", "0", "]", "*", "module", ".", "kernel_size", "[", "1", "]", "*", "module", ".", "out_channels", "\n", "fan_out", "//=", "module", ".", "groups", "\n", "module", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "math", ".", "sqrt", "(", "2.0", "/", "fan_out", ")", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "        ", "nn", ".", "init", ".", "normal_", "(", "module", ".", "weight", ",", "mean", "=", "0.0", ",", "std", "=", "0.01", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "        ", "nn", ".", "init", ".", "ones_", "(", "module", ".", "weight", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "elif", "hasattr", "(", "module", ",", "'init_weights'", ")", ":", "\n", "        ", "module", ".", "init_weights", "(", "zero_init_last", "=", "zero_init_last", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byobnet._create_byobnet": [[1582, 1588], ["helpers.build_model_with_cfg", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_byobnet", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "\n", "ByobNet", ",", "variant", ",", "pretrained", ",", "\n", "model_cfg", "=", "model_cfgs", "[", "variant", "]", ",", "\n", "feature_cfg", "=", "dict", "(", "flatten_sequential", "=", "True", ")", ",", "\n", "**", "kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.BasicConv2d.__init__": [[39, 45], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_planes", ",", "out_planes", ",", "kernel_size", ",", "stride", ",", "padding", "=", "0", ")", ":", "\n", "        ", "super", "(", "BasicConv2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv", "=", "nn", ".", "Conv2d", "(", "\n", "in_planes", ",", "out_planes", ",", "kernel_size", "=", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn", "=", "nn", ".", "BatchNorm2d", "(", "out_planes", ",", "eps", "=", ".001", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.BasicConv2d.forward": [[46, 51], ["inception_resnet_v2.BasicConv2d.conv", "inception_resnet_v2.BasicConv2d.bn", "inception_resnet_v2.BasicConv2d.relu"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "x", "=", "self", ".", "bn", "(", "x", ")", "\n", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.Mixed_5b.__init__": [[54, 73], ["torch.Module.__init__", "inception_resnet_v2.BasicConv2d", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "inception_resnet_v2.BasicConv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "Mixed_5b", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "branch0", "=", "BasicConv2d", "(", "192", ",", "96", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "\n", "self", ".", "branch1", "=", "nn", ".", "Sequential", "(", "\n", "BasicConv2d", "(", "192", ",", "48", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "48", ",", "64", ",", "kernel_size", "=", "5", ",", "stride", "=", "1", ",", "padding", "=", "2", ")", "\n", ")", "\n", "\n", "self", ".", "branch2", "=", "nn", ".", "Sequential", "(", "\n", "BasicConv2d", "(", "192", ",", "64", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "64", ",", "96", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "96", ",", "96", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", ")", "\n", "\n", "self", ".", "branch3", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "AvgPool2d", "(", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ",", "count_include_pad", "=", "False", ")", ",", "\n", "BasicConv2d", "(", "192", ",", "64", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.Mixed_5b.forward": [[75, 82], ["inception_resnet_v2.Mixed_5b.branch0", "inception_resnet_v2.Mixed_5b.branch1", "inception_resnet_v2.Mixed_5b.branch2", "inception_resnet_v2.Mixed_5b.branch3", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x0", "=", "self", ".", "branch0", "(", "x", ")", "\n", "x1", "=", "self", ".", "branch1", "(", "x", ")", "\n", "x2", "=", "self", ".", "branch2", "(", "x", ")", "\n", "x3", "=", "self", ".", "branch3", "(", "x", ")", "\n", "out", "=", "torch", ".", "cat", "(", "(", "x0", ",", "x1", ",", "x2", ",", "x3", ")", ",", "1", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.Block35.__init__": [[85, 105], ["torch.Module.__init__", "inception_resnet_v2.BasicConv2d", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "scale", "=", "1.0", ")", ":", "\n", "        ", "super", "(", "Block35", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "scale", "=", "scale", "\n", "\n", "self", ".", "branch0", "=", "BasicConv2d", "(", "320", ",", "32", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "\n", "self", ".", "branch1", "=", "nn", ".", "Sequential", "(", "\n", "BasicConv2d", "(", "320", ",", "32", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "32", ",", "32", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", ")", "\n", "\n", "self", ".", "branch2", "=", "nn", ".", "Sequential", "(", "\n", "BasicConv2d", "(", "320", ",", "32", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "32", ",", "48", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "48", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", ")", "\n", "\n", "self", ".", "conv2d", "=", "nn", ".", "Conv2d", "(", "128", ",", "320", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.Block35.forward": [[106, 115], ["inception_resnet_v2.Block35.branch0", "inception_resnet_v2.Block35.branch1", "inception_resnet_v2.Block35.branch2", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "inception_resnet_v2.Block35.conv2d", "inception_resnet_v2.Block35.relu"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x0", "=", "self", ".", "branch0", "(", "x", ")", "\n", "x1", "=", "self", ".", "branch1", "(", "x", ")", "\n", "x2", "=", "self", ".", "branch2", "(", "x", ")", "\n", "out", "=", "torch", ".", "cat", "(", "(", "x0", ",", "x1", ",", "x2", ")", ",", "1", ")", "\n", "out", "=", "self", ".", "conv2d", "(", "out", ")", "\n", "out", "=", "out", "*", "self", ".", "scale", "+", "x", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.Mixed_6a.__init__": [[118, 130], ["torch.Module.__init__", "inception_resnet_v2.BasicConv2d", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "Mixed_6a", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "branch0", "=", "BasicConv2d", "(", "320", ",", "384", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", "\n", "self", ".", "branch1", "=", "nn", ".", "Sequential", "(", "\n", "BasicConv2d", "(", "320", ",", "256", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "256", ",", "256", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "256", ",", "384", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", ")", "\n", "\n", "self", ".", "branch2", "=", "nn", ".", "MaxPool2d", "(", "3", ",", "stride", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.Mixed_6a.forward": [[131, 137], ["inception_resnet_v2.Mixed_6a.branch0", "inception_resnet_v2.Mixed_6a.branch1", "inception_resnet_v2.Mixed_6a.branch2", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x0", "=", "self", ".", "branch0", "(", "x", ")", "\n", "x1", "=", "self", ".", "branch1", "(", "x", ")", "\n", "x2", "=", "self", ".", "branch2", "(", "x", ")", "\n", "out", "=", "torch", ".", "cat", "(", "(", "x0", ",", "x1", ",", "x2", ")", ",", "1", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.Block17.__init__": [[140, 155], ["torch.Module.__init__", "inception_resnet_v2.BasicConv2d", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "scale", "=", "1.0", ")", ":", "\n", "        ", "super", "(", "Block17", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "scale", "=", "scale", "\n", "\n", "self", ".", "branch0", "=", "BasicConv2d", "(", "1088", ",", "192", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "\n", "self", ".", "branch1", "=", "nn", ".", "Sequential", "(", "\n", "BasicConv2d", "(", "1088", ",", "128", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "128", ",", "160", ",", "kernel_size", "=", "(", "1", ",", "7", ")", ",", "stride", "=", "1", ",", "padding", "=", "(", "0", ",", "3", ")", ")", ",", "\n", "BasicConv2d", "(", "160", ",", "192", ",", "kernel_size", "=", "(", "7", ",", "1", ")", ",", "stride", "=", "1", ",", "padding", "=", "(", "3", ",", "0", ")", ")", "\n", ")", "\n", "\n", "self", ".", "conv2d", "=", "nn", ".", "Conv2d", "(", "384", ",", "1088", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.Block17.forward": [[156, 164], ["inception_resnet_v2.Block17.branch0", "inception_resnet_v2.Block17.branch1", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "inception_resnet_v2.Block17.conv2d", "inception_resnet_v2.Block17.relu"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x0", "=", "self", ".", "branch0", "(", "x", ")", "\n", "x1", "=", "self", ".", "branch1", "(", "x", ")", "\n", "out", "=", "torch", ".", "cat", "(", "(", "x0", ",", "x1", ")", ",", "1", ")", "\n", "out", "=", "self", ".", "conv2d", "(", "out", ")", "\n", "out", "=", "out", "*", "self", ".", "scale", "+", "x", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.Mixed_7a.__init__": [[167, 187], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "Mixed_7a", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "branch0", "=", "nn", ".", "Sequential", "(", "\n", "BasicConv2d", "(", "1088", ",", "256", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "256", ",", "384", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", ")", "\n", "\n", "self", ".", "branch1", "=", "nn", ".", "Sequential", "(", "\n", "BasicConv2d", "(", "1088", ",", "256", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "256", ",", "288", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", ")", "\n", "\n", "self", ".", "branch2", "=", "nn", ".", "Sequential", "(", "\n", "BasicConv2d", "(", "1088", ",", "256", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "256", ",", "288", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "288", ",", "320", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", ")", "\n", "\n", "self", ".", "branch3", "=", "nn", ".", "MaxPool2d", "(", "3", ",", "stride", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.Mixed_7a.forward": [[188, 195], ["inception_resnet_v2.Mixed_7a.branch0", "inception_resnet_v2.Mixed_7a.branch1", "inception_resnet_v2.Mixed_7a.branch2", "inception_resnet_v2.Mixed_7a.branch3", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x0", "=", "self", ".", "branch0", "(", "x", ")", "\n", "x1", "=", "self", ".", "branch1", "(", "x", ")", "\n", "x2", "=", "self", ".", "branch2", "(", "x", ")", "\n", "x3", "=", "self", ".", "branch3", "(", "x", ")", "\n", "out", "=", "torch", ".", "cat", "(", "(", "x0", ",", "x1", ",", "x2", ",", "x3", ")", ",", "1", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.Block8.__init__": [[199, 214], ["torch.Module.__init__", "inception_resnet_v2.BasicConv2d", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "scale", "=", "1.0", ",", "no_relu", "=", "False", ")", ":", "\n", "        ", "super", "(", "Block8", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "scale", "=", "scale", "\n", "\n", "self", ".", "branch0", "=", "BasicConv2d", "(", "2080", ",", "192", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "\n", "self", ".", "branch1", "=", "nn", ".", "Sequential", "(", "\n", "BasicConv2d", "(", "2080", ",", "192", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", ",", "\n", "BasicConv2d", "(", "192", ",", "224", ",", "kernel_size", "=", "(", "1", ",", "3", ")", ",", "stride", "=", "1", ",", "padding", "=", "(", "0", ",", "1", ")", ")", ",", "\n", "BasicConv2d", "(", "224", ",", "256", ",", "kernel_size", "=", "(", "3", ",", "1", ")", ",", "stride", "=", "1", ",", "padding", "=", "(", "1", ",", "0", ")", ")", "\n", ")", "\n", "\n", "self", ".", "conv2d", "=", "nn", ".", "Conv2d", "(", "448", ",", "2080", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "self", ".", "relu", "=", "None", "if", "no_relu", "else", "nn", ".", "ReLU", "(", "inplace", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.Block8.forward": [[215, 224], ["inception_resnet_v2.Block8.branch0", "inception_resnet_v2.Block8.branch1", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "inception_resnet_v2.Block8.conv2d", "inception_resnet_v2.Block8.relu"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x0", "=", "self", ".", "branch0", "(", "x", ")", "\n", "x1", "=", "self", ".", "branch1", "(", "x", ")", "\n", "out", "=", "torch", ".", "cat", "(", "(", "x0", ",", "x1", ")", ",", "1", ")", "\n", "out", "=", "self", ".", "conv2d", "(", "out", ")", "\n", "out", "=", "out", "*", "self", ".", "scale", "+", "x", "\n", "if", "self", ".", "relu", "is", "not", "None", ":", "\n", "            ", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.InceptionResnetV2.__init__": [[227, 302], ["torch.Module.__init__", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "inception_resnet_v2.BasicConv2d", "inception_resnet_v2.BasicConv2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "inception_resnet_v2.Mixed_5b", "torch.Sequential", "torch.Sequential", "torch.Sequential", "inception_resnet_v2.Mixed_6a", "torch.Sequential", "torch.Sequential", "torch.Sequential", "inception_resnet_v2.Mixed_7a", "torch.Sequential", "torch.Sequential", "torch.Sequential", "inception_resnet_v2.Block8", "inception_resnet_v2.BasicConv2d", "layers.create_classifier", "dict", "dict", "inception_resnet_v2.Block35", "inception_resnet_v2.Block35", "inception_resnet_v2.Block35", "inception_resnet_v2.Block35", "inception_resnet_v2.Block35", "inception_resnet_v2.Block35", "inception_resnet_v2.Block35", "inception_resnet_v2.Block35", "inception_resnet_v2.Block35", "inception_resnet_v2.Block35", "dict", "inception_resnet_v2.Block17", "inception_resnet_v2.Block17", "inception_resnet_v2.Block17", "inception_resnet_v2.Block17", "inception_resnet_v2.Block17", "inception_resnet_v2.Block17", "inception_resnet_v2.Block17", "inception_resnet_v2.Block17", "inception_resnet_v2.Block17", "inception_resnet_v2.Block17", "inception_resnet_v2.Block17", "inception_resnet_v2.Block17", "inception_resnet_v2.Block17", "inception_resnet_v2.Block17", "inception_resnet_v2.Block17", "inception_resnet_v2.Block17", "inception_resnet_v2.Block17", "inception_resnet_v2.Block17", "inception_resnet_v2.Block17", "inception_resnet_v2.Block17", "dict", "inception_resnet_v2.Block8", "inception_resnet_v2.Block8", "inception_resnet_v2.Block8", "inception_resnet_v2.Block8", "inception_resnet_v2.Block8", "inception_resnet_v2.Block8", "inception_resnet_v2.Block8", "inception_resnet_v2.Block8", "inception_resnet_v2.Block8", "dict"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["    ", "def", "__init__", "(", "self", ",", "num_classes", "=", "1000", ",", "in_chans", "=", "3", ",", "drop_rate", "=", "0.", ",", "output_stride", "=", "32", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "super", "(", "InceptionResnetV2", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "num_features", "=", "1536", "\n", "assert", "output_stride", "==", "32", "\n", "\n", "self", ".", "conv2d_1a", "=", "BasicConv2d", "(", "in_chans", ",", "32", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", "self", ".", "conv2d_2a", "=", "BasicConv2d", "(", "32", ",", "32", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", "\n", "self", ".", "conv2d_2b", "=", "BasicConv2d", "(", "32", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "self", ".", "feature_info", "=", "[", "dict", "(", "num_chs", "=", "64", ",", "reduction", "=", "2", ",", "module", "=", "'conv2d_2b'", ")", "]", "\n", "\n", "self", ".", "maxpool_3a", "=", "nn", ".", "MaxPool2d", "(", "3", ",", "stride", "=", "2", ")", "\n", "self", ".", "conv2d_3b", "=", "BasicConv2d", "(", "64", ",", "80", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "self", ".", "conv2d_4a", "=", "BasicConv2d", "(", "80", ",", "192", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", "\n", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "192", ",", "reduction", "=", "4", ",", "module", "=", "'conv2d_4a'", ")", "]", "\n", "\n", "self", ".", "maxpool_5a", "=", "nn", ".", "MaxPool2d", "(", "3", ",", "stride", "=", "2", ")", "\n", "self", ".", "mixed_5b", "=", "Mixed_5b", "(", ")", "\n", "self", ".", "repeat", "=", "nn", ".", "Sequential", "(", "\n", "Block35", "(", "scale", "=", "0.17", ")", ",", "\n", "Block35", "(", "scale", "=", "0.17", ")", ",", "\n", "Block35", "(", "scale", "=", "0.17", ")", ",", "\n", "Block35", "(", "scale", "=", "0.17", ")", ",", "\n", "Block35", "(", "scale", "=", "0.17", ")", ",", "\n", "Block35", "(", "scale", "=", "0.17", ")", ",", "\n", "Block35", "(", "scale", "=", "0.17", ")", ",", "\n", "Block35", "(", "scale", "=", "0.17", ")", ",", "\n", "Block35", "(", "scale", "=", "0.17", ")", ",", "\n", "Block35", "(", "scale", "=", "0.17", ")", "\n", ")", "\n", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "320", ",", "reduction", "=", "8", ",", "module", "=", "'repeat'", ")", "]", "\n", "\n", "self", ".", "mixed_6a", "=", "Mixed_6a", "(", ")", "\n", "self", ".", "repeat_1", "=", "nn", ".", "Sequential", "(", "\n", "Block17", "(", "scale", "=", "0.10", ")", ",", "\n", "Block17", "(", "scale", "=", "0.10", ")", ",", "\n", "Block17", "(", "scale", "=", "0.10", ")", ",", "\n", "Block17", "(", "scale", "=", "0.10", ")", ",", "\n", "Block17", "(", "scale", "=", "0.10", ")", ",", "\n", "Block17", "(", "scale", "=", "0.10", ")", ",", "\n", "Block17", "(", "scale", "=", "0.10", ")", ",", "\n", "Block17", "(", "scale", "=", "0.10", ")", ",", "\n", "Block17", "(", "scale", "=", "0.10", ")", ",", "\n", "Block17", "(", "scale", "=", "0.10", ")", ",", "\n", "Block17", "(", "scale", "=", "0.10", ")", ",", "\n", "Block17", "(", "scale", "=", "0.10", ")", ",", "\n", "Block17", "(", "scale", "=", "0.10", ")", ",", "\n", "Block17", "(", "scale", "=", "0.10", ")", ",", "\n", "Block17", "(", "scale", "=", "0.10", ")", ",", "\n", "Block17", "(", "scale", "=", "0.10", ")", ",", "\n", "Block17", "(", "scale", "=", "0.10", ")", ",", "\n", "Block17", "(", "scale", "=", "0.10", ")", ",", "\n", "Block17", "(", "scale", "=", "0.10", ")", ",", "\n", "Block17", "(", "scale", "=", "0.10", ")", "\n", ")", "\n", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "1088", ",", "reduction", "=", "16", ",", "module", "=", "'repeat_1'", ")", "]", "\n", "\n", "self", ".", "mixed_7a", "=", "Mixed_7a", "(", ")", "\n", "self", ".", "repeat_2", "=", "nn", ".", "Sequential", "(", "\n", "Block8", "(", "scale", "=", "0.20", ")", ",", "\n", "Block8", "(", "scale", "=", "0.20", ")", ",", "\n", "Block8", "(", "scale", "=", "0.20", ")", ",", "\n", "Block8", "(", "scale", "=", "0.20", ")", ",", "\n", "Block8", "(", "scale", "=", "0.20", ")", ",", "\n", "Block8", "(", "scale", "=", "0.20", ")", ",", "\n", "Block8", "(", "scale", "=", "0.20", ")", ",", "\n", "Block8", "(", "scale", "=", "0.20", ")", ",", "\n", "Block8", "(", "scale", "=", "0.20", ")", "\n", ")", "\n", "self", ".", "block8", "=", "Block8", "(", "no_relu", "=", "True", ")", "\n", "self", ".", "conv2d_7b", "=", "BasicConv2d", "(", "2080", ",", "self", ".", "num_features", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "self", ".", "num_features", ",", "reduction", "=", "32", ",", "module", "=", "'conv2d_7b'", ")", "]", "\n", "\n", "self", ".", "global_pool", ",", "self", ".", "classif", "=", "create_classifier", "(", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.InceptionResnetV2.group_matcher": [[303, 321], ["module_map.pop", "any", "enumerate", "any", "helpers.flatten_modules", "name.startswith", "any", "inception_resnet_v2.InceptionResnetV2.named_children", "name.startswith", "module_map.keys", "float", "name.startswith", "len", "tuple", "name.split", "len"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.flatten_modules"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "module_map", "=", "{", "k", ":", "i", "for", "i", ",", "(", "k", ",", "_", ")", "in", "enumerate", "(", "flatten_modules", "(", "self", ".", "named_children", "(", ")", ",", "prefix", "=", "(", ")", ")", ")", "}", "\n", "module_map", ".", "pop", "(", "(", "'classif'", ",", ")", ")", "\n", "\n", "def", "_matcher", "(", "name", ")", ":", "\n", "            ", "if", "any", "(", "[", "name", ".", "startswith", "(", "n", ")", "for", "n", "in", "(", "'conv2d_1'", ",", "'conv2d_2'", ")", "]", ")", ":", "\n", "                ", "return", "0", "\n", "", "elif", "any", "(", "[", "name", ".", "startswith", "(", "n", ")", "for", "n", "in", "(", "'conv2d_3'", ",", "'conv2d_4'", ")", "]", ")", ":", "\n", "                ", "return", "1", "\n", "", "elif", "any", "(", "[", "name", ".", "startswith", "(", "n", ")", "for", "n", "in", "(", "'block8'", ",", "'conv2d_7'", ")", "]", ")", ":", "\n", "                ", "return", "len", "(", "module_map", ")", "+", "1", "\n", "", "else", ":", "\n", "                ", "for", "k", "in", "module_map", ".", "keys", "(", ")", ":", "\n", "                    ", "if", "k", "==", "tuple", "(", "name", ".", "split", "(", "'.'", ")", "[", ":", "len", "(", "k", ")", "]", ")", ":", "\n", "                        ", "return", "module_map", "[", "k", "]", "\n", "", "", "return", "float", "(", "'inf'", ")", "\n", "", "", "return", "_matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.InceptionResnetV2.set_grad_checkpointing": [[322, 325], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "\"checkpointing not supported\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.InceptionResnetV2.get_classifier": [[326, 329], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "classif", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.InceptionResnetV2.reset_classifier": [[330, 333], ["layers.create_classifier"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", ",", "self", ".", "classif", "=", "create_classifier", "(", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.InceptionResnetV2.forward_features": [[334, 351], ["inception_resnet_v2.InceptionResnetV2.conv2d_1a", "inception_resnet_v2.InceptionResnetV2.conv2d_2a", "inception_resnet_v2.InceptionResnetV2.conv2d_2b", "inception_resnet_v2.InceptionResnetV2.maxpool_3a", "inception_resnet_v2.InceptionResnetV2.conv2d_3b", "inception_resnet_v2.InceptionResnetV2.conv2d_4a", "inception_resnet_v2.InceptionResnetV2.maxpool_5a", "inception_resnet_v2.InceptionResnetV2.mixed_5b", "inception_resnet_v2.InceptionResnetV2.repeat", "inception_resnet_v2.InceptionResnetV2.mixed_6a", "inception_resnet_v2.InceptionResnetV2.repeat_1", "inception_resnet_v2.InceptionResnetV2.mixed_7a", "inception_resnet_v2.InceptionResnetV2.repeat_2", "inception_resnet_v2.InceptionResnetV2.block8", "inception_resnet_v2.InceptionResnetV2.conv2d_7b"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv2d_1a", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2d_2a", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2d_2b", "(", "x", ")", "\n", "x", "=", "self", ".", "maxpool_3a", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2d_3b", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2d_4a", "(", "x", ")", "\n", "x", "=", "self", ".", "maxpool_5a", "(", "x", ")", "\n", "x", "=", "self", ".", "mixed_5b", "(", "x", ")", "\n", "x", "=", "self", ".", "repeat", "(", "x", ")", "\n", "x", "=", "self", ".", "mixed_6a", "(", "x", ")", "\n", "x", "=", "self", ".", "repeat_1", "(", "x", ")", "\n", "x", "=", "self", ".", "mixed_7a", "(", "x", ")", "\n", "x", "=", "self", ".", "repeat_2", "(", "x", ")", "\n", "x", "=", "self", ".", "block8", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2d_7b", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.InceptionResnetV2.forward_head": [[352, 357], ["inception_resnet_v2.InceptionResnetV2.global_pool", "torch.dropout", "torch.dropout", "torch.dropout", "inception_resnet_v2.InceptionResnetV2.classif"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "if", "self", ".", "drop_rate", ">", "0", ":", "\n", "            ", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "drop_rate", ",", "training", "=", "self", ".", "training", ")", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "classif", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.InceptionResnetV2.forward": [[358, 362], ["inception_resnet_v2.InceptionResnetV2.forward_features", "inception_resnet_v2.InceptionResnetV2.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2._create_inception_resnet_v2": [[364, 366], ["helpers.build_model_with_cfg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_inception_resnet_v2", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "InceptionResnetV2", ",", "variant", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.inception_resnet_v2": [[368, 374], ["inception_resnet_v2._create_inception_resnet_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2._create_inception_resnet_v2"], ["", "@", "register_model", "\n", "def", "inception_resnet_v2", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"InceptionResnetV2 model architecture from the\n    `\"InceptionV4, Inception-ResNet...\" <https://arxiv.org/abs/1602.07261>` paper.\n    \"\"\"", "\n", "return", "_create_inception_resnet_v2", "(", "'inception_resnet_v2'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2.ens_adv_inception_resnet_v2": [[376, 383], ["inception_resnet_v2._create_inception_resnet_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.inception_resnet_v2._create_inception_resnet_v2"], ["", "@", "register_model", "\n", "def", "ens_adv_inception_resnet_v2", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\" Ensemble Adversarially trained InceptionResnetV2 model architecture\n    As per https://arxiv.org/abs/1705.07204 and\n    https://github.com/tensorflow/models/tree/master/research/adv_imagenet_models.\n    \"\"\"", "\n", "return", "_create_inception_resnet_v2", "(", "'ens_adv_inception_resnet_v2'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.fx_features.FeatureGraphNet.__init__": [[69, 79], ["torch.nn.Module.__init__", "features._get_feature_info", "fx_features.create_feature_extractor", "len", "len", "enumerate"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features._get_feature_info", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.fx_features.create_feature_extractor"], ["def", "__init__", "(", "self", ",", "model", ",", "out_indices", ",", "out_map", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "has_fx_feature_extraction", ",", "'Please update to PyTorch 1.10+, torchvision 0.11+ for FX feature extraction'", "\n", "self", ".", "feature_info", "=", "_get_feature_info", "(", "model", ",", "out_indices", ")", "\n", "if", "out_map", "is", "not", "None", ":", "\n", "            ", "assert", "len", "(", "out_map", ")", "==", "len", "(", "out_indices", ")", "\n", "", "return_nodes", "=", "{", "\n", "info", "[", "'module'", "]", ":", "out_map", "[", "i", "]", "if", "out_map", "is", "not", "None", "else", "info", "[", "'module'", "]", "\n", "for", "i", ",", "info", "in", "enumerate", "(", "self", ".", "feature_info", ")", "if", "i", "in", "out_indices", "}", "\n", "self", ".", "graph_module", "=", "create_feature_extractor", "(", "model", ",", "return_nodes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.fx_features.FeatureGraphNet.forward": [[80, 82], ["list", "fx_features.FeatureGraphNet.graph_module().values", "fx_features.FeatureGraphNet.graph_module"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "list", "(", "self", ".", "graph_module", "(", "x", ")", ".", "values", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.fx_features.GraphExtractNet.__init__": [[97, 101], ["torch.nn.Module.__init__", "fx_features.create_feature_extractor"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.fx_features.create_feature_extractor"], ["def", "__init__", "(", "self", ",", "model", ",", "return_nodes", ":", "Union", "[", "Dict", "[", "str", ",", "str", "]", ",", "List", "[", "str", "]", "]", ",", "squeeze_out", ":", "bool", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "squeeze_out", "=", "squeeze_out", "\n", "self", ".", "graph_module", "=", "create_feature_extractor", "(", "model", ",", "return_nodes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.fx_features.GraphExtractNet.forward": [[102, 107], ["list", "fx_features.GraphExtractNet.graph_module().values", "len", "fx_features.GraphExtractNet.graph_module"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", "->", "Union", "[", "List", "[", "torch", ".", "Tensor", "]", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "out", "=", "list", "(", "self", ".", "graph_module", "(", "x", ")", ".", "values", "(", ")", ")", "\n", "if", "self", ".", "squeeze_out", "and", "len", "(", "out", ")", "==", "1", ":", "\n", "            ", "return", "out", "[", "0", "]", "\n", "", "return", "out", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.fx_features.register_notrace_module": [[38, 44], ["_leaf_modules.add"], "function", ["None"], ["", "def", "register_notrace_module", "(", "module", ":", "nn", ".", "Module", ")", ":", "\n", "    ", "\"\"\"\n    Any module not under timm.models.layers should get this decorator if we don't want to trace through it.\n    \"\"\"", "\n", "_leaf_modules", ".", "add", "(", "module", ")", "\n", "return", "module", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.fx_features.register_notrace_function": [[50, 56], ["_autowrap_functions.add"], "function", ["None"], ["def", "register_notrace_function", "(", "func", ":", "Callable", ")", ":", "\n", "    ", "\"\"\"\n    Decorator for functions which ought not to be traced through\n    \"\"\"", "\n", "_autowrap_functions", ".", "add", "(", "func", ")", "\n", "return", "func", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.fx_features.create_feature_extractor": [[58, 63], ["_create_feature_extractor", "list", "list"], "function", ["None"], ["", "def", "create_feature_extractor", "(", "model", ":", "nn", ".", "Module", ",", "return_nodes", ":", "Union", "[", "Dict", "[", "str", ",", "str", "]", ",", "List", "[", "str", "]", "]", ")", ":", "\n", "    ", "assert", "has_fx_feature_extraction", ",", "'Please update to PyTorch 1.10+, torchvision 0.11+ for FX feature extraction'", "\n", "return", "_create_feature_extractor", "(", "\n", "model", ",", "return_nodes", ",", "\n", "tracer_kwargs", "=", "{", "'leaf_modules'", ":", "list", "(", "_leaf_modules", ")", ",", "'autowrap_functions'", ":", "list", "(", "_autowrap_functions", ")", "}", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.parse_model_name": [[10, 20], ["model_name.replace.replace", "urllib.parse.urlsplit", "os.path.split"], "function", ["None"], ["def", "parse_model_name", "(", "model_name", ")", ":", "\n", "    ", "model_name", "=", "model_name", ".", "replace", "(", "'hf_hub'", ",", "'hf-hub'", ")", "# NOTE for backwards compat, to deprecate hf_hub use", "\n", "parsed", "=", "urlsplit", "(", "model_name", ")", "\n", "assert", "parsed", ".", "scheme", "in", "(", "''", ",", "'timm'", ",", "'hf-hub'", ")", "\n", "if", "parsed", ".", "scheme", "==", "'hf-hub'", ":", "\n", "# FIXME may use fragment as revision, currently `@` in URI path", "\n", "        ", "return", "parsed", ".", "scheme", ",", "parsed", ".", "path", "\n", "", "else", ":", "\n", "        ", "model_name", "=", "os", ".", "path", ".", "split", "(", "parsed", ".", "path", ")", "[", "-", "1", "]", "\n", "return", "'timm'", ",", "model_name", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.safe_model_name": [[22, 28], ["factory.safe_model_name.make_safe"], "function", ["None"], ["", "", "def", "safe_model_name", "(", "model_name", ",", "remove_source", "=", "True", ")", ":", "\n", "    ", "def", "make_safe", "(", "name", ")", ":", "\n", "        ", "return", "''", ".", "join", "(", "c", "if", "c", ".", "isalnum", "(", ")", "else", "'_'", "for", "c", "in", "name", ")", ".", "rstrip", "(", "'_'", ")", "\n", "", "if", "remove_source", ":", "\n", "        ", "model_name", "=", "parse_model_name", "(", "model_name", ")", "[", "-", "1", "]", "\n", "", "return", "make_safe", "(", "model_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.create_model": [[30, 77], ["factory.parse_model_name", "registry.model_entrypoint", "hub.load_model_config_from_hf", "registry.is_model", "RuntimeError", "layers.set_layer_config", "registry.model_entrypoint.", "helpers.load_checkpoint", "kwargs.items"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.parse_model_name", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.model_entrypoint", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.load_model_config_from_hf", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.is_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_checkpoint"], ["", "def", "create_model", "(", "\n", "model_name", ",", "\n", "pretrained", "=", "False", ",", "\n", "pretrained_cfg", "=", "None", ",", "\n", "checkpoint_path", "=", "''", ",", "\n", "scriptable", "=", "None", ",", "\n", "exportable", "=", "None", ",", "\n", "no_jit", "=", "None", ",", "\n", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Create a model\n\n    Args:\n        model_name (str): name of model to instantiate\n        pretrained (bool): load pretrained ImageNet-1k weights if true\n        checkpoint_path (str): path of checkpoint to load after model is initialized\n        scriptable (bool): set layer config so that model is jit scriptable (not working for all models yet)\n        exportable (bool): set layer config so that model is traceable / ONNX exportable (not fully impl/obeyed yet)\n        no_jit (bool): set layer config so that model doesn't utilize jit scripted layers (so far activations only)\n\n    Keyword Args:\n        drop_rate (float): dropout rate for training (default: 0.0)\n        global_pool (str): global pool type (default: 'avg')\n        **: other kwargs are model specific\n    \"\"\"", "\n", "# Parameters that aren't supported by all models or are intended to only override model defaults if set", "\n", "# should default to None in command line args/cfg. Remove them if they are present and not set so that", "\n", "# non-supporting models don't break and default args remain in effect.", "\n", "kwargs", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", "if", "v", "is", "not", "None", "}", "\n", "\n", "model_source", ",", "model_name", "=", "parse_model_name", "(", "model_name", ")", "\n", "if", "model_source", "==", "'hf-hub'", ":", "\n", "# FIXME hf-hub source overrides any passed in pretrained_cfg, warn?", "\n", "# For model names specified in the form `hf-hub:path/architecture_name@revision`,", "\n", "# load model weights + pretrained_cfg from Hugging Face hub.", "\n", "        ", "pretrained_cfg", ",", "model_name", "=", "load_model_config_from_hf", "(", "model_name", ")", "\n", "\n", "", "if", "not", "is_model", "(", "model_name", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'Unknown model (%s)'", "%", "model_name", ")", "\n", "\n", "", "create_fn", "=", "model_entrypoint", "(", "model_name", ")", "\n", "with", "set_layer_config", "(", "scriptable", "=", "scriptable", ",", "exportable", "=", "exportable", ",", "no_jit", "=", "no_jit", ")", ":", "\n", "        ", "model", "=", "create_fn", "(", "pretrained", "=", "pretrained", ",", "pretrained_cfg", "=", "pretrained_cfg", ",", "**", "kwargs", ")", "\n", "\n", "", "if", "checkpoint_path", ":", "\n", "        ", "load_checkpoint", "(", "model", ",", "checkpoint_path", ")", "\n", "\n", "", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.DenseLayer.__init__": [[48, 60], ["torch.Module.__init__", "float", "densenet.DenseLayer.add_module", "densenet.DenseLayer.add_module", "densenet.DenseLayer.add_module", "densenet.DenseLayer.add_module", "norm_layer", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "num_input_features", ",", "growth_rate", ",", "bn_size", ",", "norm_layer", "=", "BatchNormAct2d", ",", "\n", "drop_rate", "=", "0.", ",", "memory_efficient", "=", "False", ")", ":", "\n", "        ", "super", "(", "DenseLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "add_module", "(", "'norm1'", ",", "norm_layer", "(", "num_input_features", ")", ")", ",", "\n", "self", ".", "add_module", "(", "'conv1'", ",", "nn", ".", "Conv2d", "(", "\n", "num_input_features", ",", "bn_size", "*", "growth_rate", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "bias", "=", "False", ")", ")", ",", "\n", "self", ".", "add_module", "(", "'norm2'", ",", "norm_layer", "(", "bn_size", "*", "growth_rate", ")", ")", ",", "\n", "self", ".", "add_module", "(", "'conv2'", ",", "nn", ".", "Conv2d", "(", "\n", "bn_size", "*", "growth_rate", ",", "growth_rate", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ")", ",", "\n", "self", ".", "drop_rate", "=", "float", "(", "drop_rate", ")", "\n", "self", ".", "memory_efficient", "=", "memory_efficient", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.DenseLayer.bottleneck_fn": [[61, 66], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "densenet.DenseLayer.conv1", "densenet.DenseLayer.norm1"], "methods", ["None"], ["", "def", "bottleneck_fn", "(", "self", ",", "xs", ")", ":", "\n", "# type: (List[torch.Tensor]) -> torch.Tensor", "\n", "        ", "concated_features", "=", "torch", ".", "cat", "(", "xs", ",", "1", ")", "\n", "bottleneck_output", "=", "self", ".", "conv1", "(", "self", ".", "norm1", "(", "concated_features", ")", ")", "# noqa: T484", "\n", "return", "bottleneck_output", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.DenseLayer.any_requires_grad": [[68, 74], ["None"], "methods", ["None"], ["", "def", "any_requires_grad", "(", "self", ",", "x", ")", ":", "\n", "# type: (List[torch.Tensor]) -> bool", "\n", "        ", "for", "tensor", "in", "x", ":", "\n", "            ", "if", "tensor", ".", "requires_grad", ":", "\n", "                ", "return", "True", "\n", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.DenseLayer.call_checkpoint_bottleneck": [[75, 82], ["torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "densenet.DenseLayer.bottleneck_fn"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.DenseLayer.bottleneck_fn"], ["", "@", "torch", ".", "jit", ".", "unused", "# noqa: T484", "\n", "def", "call_checkpoint_bottleneck", "(", "self", ",", "x", ")", ":", "\n", "# type: (List[torch.Tensor]) -> torch.Tensor", "\n", "        ", "def", "closure", "(", "*", "xs", ")", ":", "\n", "            ", "return", "self", ".", "bottleneck_fn", "(", "xs", ")", "\n", "\n", "", "return", "cp", ".", "checkpoint", "(", "closure", ",", "*", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.DenseLayer.forward": [[95, 112], ["isinstance", "densenet.DenseLayer.conv2", "densenet.DenseLayer.any_requires_grad", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "densenet.DenseLayer.call_checkpoint_bottleneck", "densenet.DenseLayer.bottleneck_fn", "densenet.DenseLayer.norm2", "torch.dropout", "torch.dropout", "torch.dropout", "torch.dropout", "Exception"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.DenseLayer.any_requires_grad", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.DenseLayer.call_checkpoint_bottleneck", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.DenseLayer.bottleneck_fn"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "# noqa: F811", "\n", "        ", "if", "isinstance", "(", "x", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "prev_features", "=", "[", "x", "]", "\n", "", "else", ":", "\n", "            ", "prev_features", "=", "x", "\n", "\n", "", "if", "self", ".", "memory_efficient", "and", "self", ".", "any_requires_grad", "(", "prev_features", ")", ":", "\n", "            ", "if", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "                ", "raise", "Exception", "(", "\"Memory Efficient not supported in JIT\"", ")", "\n", "", "bottleneck_output", "=", "self", ".", "call_checkpoint_bottleneck", "(", "prev_features", ")", "\n", "", "else", ":", "\n", "            ", "bottleneck_output", "=", "self", ".", "bottleneck_fn", "(", "prev_features", ")", "\n", "\n", "", "new_features", "=", "self", ".", "conv2", "(", "self", ".", "norm2", "(", "bottleneck_output", ")", ")", "\n", "if", "self", ".", "drop_rate", ">", "0", ":", "\n", "            ", "new_features", "=", "F", ".", "dropout", "(", "new_features", ",", "p", "=", "self", ".", "drop_rate", ",", "training", "=", "self", ".", "training", ")", "\n", "", "return", "new_features", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.DenseBlock.__init__": [[117, 131], ["torch.ModuleDict.__init__", "range", "densenet.DenseLayer", "densenet.DenseBlock.add_module"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "num_layers", ",", "num_input_features", ",", "bn_size", ",", "growth_rate", ",", "norm_layer", "=", "nn", ".", "ReLU", ",", "\n", "drop_rate", "=", "0.", ",", "memory_efficient", "=", "False", ")", ":", "\n", "        ", "super", "(", "DenseBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "layer", "=", "DenseLayer", "(", "\n", "num_input_features", "+", "i", "*", "growth_rate", ",", "\n", "growth_rate", "=", "growth_rate", ",", "\n", "bn_size", "=", "bn_size", ",", "\n", "norm_layer", "=", "norm_layer", ",", "\n", "drop_rate", "=", "drop_rate", ",", "\n", "memory_efficient", "=", "memory_efficient", ",", "\n", ")", "\n", "self", ".", "add_module", "(", "'denselayer%d'", "%", "(", "i", "+", "1", ")", ",", "layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.DenseBlock.forward": [[132, 138], ["densenet.DenseBlock.items", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "layer", "features.append"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "init_features", ")", ":", "\n", "        ", "features", "=", "[", "init_features", "]", "\n", "for", "name", ",", "layer", "in", "self", ".", "items", "(", ")", ":", "\n", "            ", "new_features", "=", "layer", "(", "features", ")", "\n", "features", ".", "append", "(", "new_features", ")", "\n", "", "return", "torch", ".", "cat", "(", "features", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.DenseTransition.__init__": [[141, 150], ["torch.Sequential.__init__", "densenet.DenseTransition.add_module", "densenet.DenseTransition.add_module", "norm_layer", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "densenet.DenseTransition.add_module", "densenet.DenseTransition.add_module", "aa_layer", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_input_features", ",", "num_output_features", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "aa_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "DenseTransition", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "add_module", "(", "'norm'", ",", "norm_layer", "(", "num_input_features", ")", ")", "\n", "self", ".", "add_module", "(", "'conv'", ",", "nn", ".", "Conv2d", "(", "\n", "num_input_features", ",", "num_output_features", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "bias", "=", "False", ")", ")", "\n", "if", "aa_layer", "is", "not", "None", ":", "\n", "            ", "self", ".", "add_module", "(", "'pool'", ",", "aa_layer", "(", "num_output_features", ",", "stride", "=", "2", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "add_module", "(", "'pool'", ",", "nn", ".", "AvgPool2d", "(", "kernel_size", "=", "2", ",", "stride", "=", "2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.DenseNet.__init__": [[167, 253], ["torch.Module.__init__", "enumerate", "densenet.DenseNet.features.add_module", "layers.create_classifier", "densenet.DenseNet.modules", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "dict", "densenet.DenseBlock", "densenet.DenseNet.features.add_module", "norm_layer", "dict", "isinstance", "collections.OrderedDict", "collections.OrderedDict", "densenet.DenseTransition", "densenet.DenseNet.features.add_module", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "isinstance", "len", "dict", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "isinstance", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "aa_layer", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["def", "__init__", "(", "\n", "self", ",", "growth_rate", "=", "32", ",", "block_config", "=", "(", "6", ",", "12", ",", "24", ",", "16", ")", ",", "num_classes", "=", "1000", ",", "in_chans", "=", "3", ",", "global_pool", "=", "'avg'", ",", "\n", "bn_size", "=", "4", ",", "stem_type", "=", "''", ",", "norm_layer", "=", "BatchNormAct2d", ",", "aa_layer", "=", "None", ",", "drop_rate", "=", "0", ",", "\n", "memory_efficient", "=", "False", ",", "aa_stem_only", "=", "True", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "super", "(", "DenseNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Stem", "\n", "deep_stem", "=", "'deep'", "in", "stem_type", "# 3x3 deep stem", "\n", "num_init_features", "=", "growth_rate", "*", "2", "\n", "if", "aa_layer", "is", "None", ":", "\n", "            ", "stem_pool", "=", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "stem_pool", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ",", "\n", "aa_layer", "(", "channels", "=", "num_init_features", ",", "stride", "=", "2", ")", "]", ")", "\n", "", "if", "deep_stem", ":", "\n", "            ", "stem_chs_1", "=", "stem_chs_2", "=", "growth_rate", "\n", "if", "'tiered'", "in", "stem_type", ":", "\n", "                ", "stem_chs_1", "=", "3", "*", "(", "growth_rate", "//", "4", ")", "\n", "stem_chs_2", "=", "num_init_features", "if", "'narrow'", "in", "stem_type", "else", "6", "*", "(", "growth_rate", "//", "4", ")", "\n", "", "self", ".", "features", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "'conv0'", ",", "nn", ".", "Conv2d", "(", "in_chans", ",", "stem_chs_1", ",", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ")", ",", "\n", "(", "'norm0'", ",", "norm_layer", "(", "stem_chs_1", ")", ")", ",", "\n", "(", "'conv1'", ",", "nn", ".", "Conv2d", "(", "stem_chs_1", ",", "stem_chs_2", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ")", ",", "\n", "(", "'norm1'", ",", "norm_layer", "(", "stem_chs_2", ")", ")", ",", "\n", "(", "'conv2'", ",", "nn", ".", "Conv2d", "(", "stem_chs_2", ",", "num_init_features", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ")", ",", "\n", "(", "'norm2'", ",", "norm_layer", "(", "num_init_features", ")", ")", ",", "\n", "(", "'pool0'", ",", "stem_pool", ")", ",", "\n", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "features", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "'conv0'", ",", "nn", ".", "Conv2d", "(", "in_chans", ",", "num_init_features", ",", "kernel_size", "=", "7", ",", "stride", "=", "2", ",", "padding", "=", "3", ",", "bias", "=", "False", ")", ")", ",", "\n", "(", "'norm0'", ",", "norm_layer", "(", "num_init_features", ")", ")", ",", "\n", "(", "'pool0'", ",", "stem_pool", ")", ",", "\n", "]", ")", ")", "\n", "", "self", ".", "feature_info", "=", "[", "\n", "dict", "(", "num_chs", "=", "num_init_features", ",", "reduction", "=", "2", ",", "module", "=", "f'features.norm{2 if deep_stem else 0}'", ")", "]", "\n", "current_stride", "=", "4", "\n", "\n", "# DenseBlocks", "\n", "num_features", "=", "num_init_features", "\n", "for", "i", ",", "num_layers", "in", "enumerate", "(", "block_config", ")", ":", "\n", "            ", "block", "=", "DenseBlock", "(", "\n", "num_layers", "=", "num_layers", ",", "\n", "num_input_features", "=", "num_features", ",", "\n", "bn_size", "=", "bn_size", ",", "\n", "growth_rate", "=", "growth_rate", ",", "\n", "norm_layer", "=", "norm_layer", ",", "\n", "drop_rate", "=", "drop_rate", ",", "\n", "memory_efficient", "=", "memory_efficient", "\n", ")", "\n", "module_name", "=", "f'denseblock{(i + 1)}'", "\n", "self", ".", "features", ".", "add_module", "(", "module_name", ",", "block", ")", "\n", "num_features", "=", "num_features", "+", "num_layers", "*", "growth_rate", "\n", "transition_aa_layer", "=", "None", "if", "aa_stem_only", "else", "aa_layer", "\n", "if", "i", "!=", "len", "(", "block_config", ")", "-", "1", ":", "\n", "                ", "self", ".", "feature_info", "+=", "[", "\n", "dict", "(", "num_chs", "=", "num_features", ",", "reduction", "=", "current_stride", ",", "module", "=", "'features.'", "+", "module_name", ")", "]", "\n", "current_stride", "*=", "2", "\n", "trans", "=", "DenseTransition", "(", "\n", "num_input_features", "=", "num_features", ",", "num_output_features", "=", "num_features", "//", "2", ",", "\n", "norm_layer", "=", "norm_layer", ",", "aa_layer", "=", "transition_aa_layer", ")", "\n", "self", ".", "features", ".", "add_module", "(", "f'transition{i + 1}'", ",", "trans", ")", "\n", "num_features", "=", "num_features", "//", "2", "\n", "\n", "# Final batch norm", "\n", "", "", "self", ".", "features", ".", "add_module", "(", "'norm5'", ",", "norm_layer", "(", "num_features", ")", ")", "\n", "\n", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "num_features", ",", "reduction", "=", "current_stride", ",", "module", "=", "'features.norm5'", ")", "]", "\n", "self", ".", "num_features", "=", "num_features", "\n", "\n", "# Linear layer", "\n", "self", ".", "global_pool", ",", "self", ".", "classifier", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n", "# Official init from torch repo.", "\n", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.DenseNet.group_matcher": [[254, 264], ["dict"], "methods", ["None"], ["", "", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "\n", "stem", "=", "r'^features\\.conv[012]|features\\.norm[012]|features\\.pool[012]'", ",", "\n", "blocks", "=", "r'^features\\.(?:denseblock|transition)(\\d+)'", "if", "coarse", "else", "[", "\n", "(", "r'^features\\.denseblock(\\d+)\\.denselayer(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^features\\.transition(\\d+)'", ",", "MATCH_PREV_GROUP", ")", "# FIXME combine with previous denselayer", "\n", "]", "\n", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.DenseNet.get_classifier": [[265, 268], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "classifier", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.DenseNet.reset_classifier": [[269, 273], ["layers.create_classifier"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", ",", "self", ".", "classifier", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.DenseNet.forward_features": [[274, 276], ["densenet.DenseNet.features"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "features", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.DenseNet.forward": [[277, 285], ["densenet.DenseNet.forward_features", "densenet.DenseNet.global_pool", "densenet.DenseNet.classifier"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "# both classifier and block drop?", "\n", "# if self.drop_rate > 0.:", "\n", "#     x = F.dropout(x, p=self.drop_rate, training=self.training)", "\n", "x", "=", "self", ".", "classifier", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet._cfg": [[23, 29], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bicubic'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'features.conv0'", ",", "'classifier'", ":", "'classifier'", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet._filter_torchvision_pretrained": [[287, 298], ["re.compile", "list", "state_dict.keys", "re.compile.match", "pattern.match.group", "pattern.match.group"], "function", ["None"], ["", "", "def", "_filter_torchvision_pretrained", "(", "state_dict", ")", ":", "\n", "    ", "pattern", "=", "re", ".", "compile", "(", "\n", "r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$'", ")", "\n", "\n", "for", "key", "in", "list", "(", "state_dict", ".", "keys", "(", ")", ")", ":", "\n", "        ", "res", "=", "pattern", ".", "match", "(", "key", ")", "\n", "if", "res", ":", "\n", "            ", "new_key", "=", "res", ".", "group", "(", "1", ")", "+", "res", ".", "group", "(", "2", ")", "\n", "state_dict", "[", "new_key", "]", "=", "state_dict", "[", "key", "]", "\n", "del", "state_dict", "[", "key", "]", "\n", "", "", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet._create_densenet": [[300, 307], ["helpers.build_model_with_cfg", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "def", "_create_densenet", "(", "variant", ",", "growth_rate", ",", "block_config", ",", "pretrained", ",", "**", "kwargs", ")", ":", "\n", "    ", "kwargs", "[", "'growth_rate'", "]", "=", "growth_rate", "\n", "kwargs", "[", "'block_config'", "]", "=", "block_config", "\n", "return", "build_model_with_cfg", "(", "\n", "DenseNet", ",", "variant", ",", "pretrained", ",", "\n", "feature_cfg", "=", "dict", "(", "flatten_sequential", "=", "True", ")", ",", "pretrained_filter_fn", "=", "_filter_torchvision_pretrained", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.densenet121": [[309, 317], ["densenet._create_densenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet._create_densenet"], ["", "@", "register_model", "\n", "def", "densenet121", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"Densenet-121 model from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n    \"\"\"", "\n", "model", "=", "_create_densenet", "(", "\n", "'densenet121'", ",", "growth_rate", "=", "32", ",", "block_config", "=", "(", "6", ",", "12", ",", "24", ",", "16", ")", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.densenetblur121d": [[319, 328], ["densenet._create_densenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet._create_densenet"], ["", "@", "register_model", "\n", "def", "densenetblur121d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"Densenet-121 model from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n    \"\"\"", "\n", "model", "=", "_create_densenet", "(", "\n", "'densenetblur121d'", ",", "growth_rate", "=", "32", ",", "block_config", "=", "(", "6", ",", "12", ",", "24", ",", "16", ")", ",", "pretrained", "=", "pretrained", ",", "stem_type", "=", "'deep'", ",", "\n", "aa_layer", "=", "BlurPool2d", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.densenet121d": [[330, 339], ["densenet._create_densenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet._create_densenet"], ["", "@", "register_model", "\n", "def", "densenet121d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"Densenet-121 model from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n    \"\"\"", "\n", "model", "=", "_create_densenet", "(", "\n", "'densenet121d'", ",", "growth_rate", "=", "32", ",", "block_config", "=", "(", "6", ",", "12", ",", "24", ",", "16", ")", ",", "stem_type", "=", "'deep'", ",", "\n", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.densenet169": [[341, 349], ["densenet._create_densenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet._create_densenet"], ["", "@", "register_model", "\n", "def", "densenet169", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"Densenet-169 model from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n    \"\"\"", "\n", "model", "=", "_create_densenet", "(", "\n", "'densenet169'", ",", "growth_rate", "=", "32", ",", "block_config", "=", "(", "6", ",", "12", ",", "32", ",", "32", ")", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.densenet201": [[351, 359], ["densenet._create_densenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet._create_densenet"], ["", "@", "register_model", "\n", "def", "densenet201", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"Densenet-201 model from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n    \"\"\"", "\n", "model", "=", "_create_densenet", "(", "\n", "'densenet201'", ",", "growth_rate", "=", "32", ",", "block_config", "=", "(", "6", ",", "12", ",", "48", ",", "32", ")", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.densenet161": [[361, 369], ["densenet._create_densenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet._create_densenet"], ["", "@", "register_model", "\n", "def", "densenet161", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"Densenet-161 model from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n    \"\"\"", "\n", "model", "=", "_create_densenet", "(", "\n", "'densenet161'", ",", "growth_rate", "=", "48", ",", "block_config", "=", "(", "6", ",", "12", ",", "36", ",", "24", ")", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.densenet264": [[371, 379], ["densenet._create_densenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet._create_densenet"], ["", "@", "register_model", "\n", "def", "densenet264", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"Densenet-264 model from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n    \"\"\"", "\n", "model", "=", "_create_densenet", "(", "\n", "'densenet264'", ",", "growth_rate", "=", "48", ",", "block_config", "=", "(", "6", ",", "12", ",", "64", ",", "48", ")", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.densenet264d_iabn": [[381, 391], ["densenet._create_densenet", "layers.create_norm_act_layer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet._create_densenet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.create_norm_act_layer"], ["", "@", "register_model", "\n", "def", "densenet264d_iabn", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"Densenet-264 model with deep stem and Inplace-ABN\n    \"\"\"", "\n", "def", "norm_act_fn", "(", "num_features", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "create_norm_act_layer", "(", "'iabn'", ",", "num_features", ",", "act_layer", "=", "'leaky_relu'", ",", "**", "kwargs", ")", "\n", "", "model", "=", "_create_densenet", "(", "\n", "'densenet264d_iabn'", ",", "growth_rate", "=", "48", ",", "block_config", "=", "(", "6", ",", "12", ",", "64", ",", "48", ")", ",", "stem_type", "=", "'deep'", ",", "\n", "norm_layer", "=", "norm_act_fn", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet.tv_densenet121": [[393, 401], ["densenet._create_densenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.densenet._create_densenet"], ["", "@", "register_model", "\n", "def", "tv_densenet121", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"Densenet-121 model with original Torchvision weights, from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n    \"\"\"", "\n", "model", "=", "_create_densenet", "(", "\n", "'tv_densenet121'", ",", "growth_rate", "=", "32", ",", "block_config", "=", "(", "6", ",", "12", ",", "24", ",", "16", ")", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit.VisionTransformerDistilled.__init__": [[66, 78], ["kwargs.pop", "timm.models.vision_transformer.VisionTransformer.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "deit.VisionTransformerDistilled.init_weights", "torch.zeros", "torch.zeros", "torch.nn.Linear", "torch.nn.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights"], ["def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "weight_init", "=", "kwargs", ".", "pop", "(", "'weight_init'", ",", "''", ")", "\n", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ",", "weight_init", "=", "'skip'", ")", "\n", "assert", "self", ".", "global_pool", "in", "(", "'token'", ",", ")", "\n", "\n", "self", ".", "num_tokens", "=", "2", "\n", "self", ".", "dist_token", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "self", ".", "embed_dim", ")", ")", "\n", "self", ".", "pos_embed", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "self", ".", "patch_embed", ".", "num_patches", "+", "self", ".", "num_tokens", ",", "self", ".", "embed_dim", ")", ")", "\n", "self", ".", "head_dist", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "num_classes", ")", "if", "self", ".", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "distilled_training", "=", "False", "# must set this True to train w/ distillation token", "\n", "\n", "self", ".", "init_weights", "(", "weight_init", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit.VisionTransformerDistilled.init_weights": [[79, 82], ["timm.models.vision_transformer.trunc_normal_", "super().init_weights"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights"], ["", "def", "init_weights", "(", "self", ",", "mode", "=", "''", ")", ":", "\n", "        ", "trunc_normal_", "(", "self", ".", "dist_token", ",", "std", "=", ".02", ")", "\n", "super", "(", ")", ".", "init_weights", "(", "mode", "=", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit.VisionTransformerDistilled.group_matcher": [[83, 90], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^cls_token|pos_embed|patch_embed|dist_token'", ",", "\n", "blocks", "=", "[", "\n", "(", "r'^blocks\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^norm'", ",", "(", "99999", ",", ")", ")", "]", "# final norm w/ last block", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit.VisionTransformerDistilled.get_classifier": [[92, 95], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", ",", "self", ".", "head_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit.VisionTransformerDistilled.reset_classifier": [[96, 100], ["torch.nn.Linear", "torch.nn.Identity", "torch.nn.Linear", "torch.nn.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "head_dist", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit.VisionTransformerDistilled.set_distilled_training": [[101, 104], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_distilled_training", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "distilled_training", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit.VisionTransformerDistilled.forward_features": [[105, 117], ["deit.VisionTransformerDistilled.patch_embed", "torch.cat", "deit.VisionTransformerDistilled.pos_drop", "deit.VisionTransformerDistilled.norm", "helpers.checkpoint_seq", "deit.VisionTransformerDistilled.blocks", "deit.VisionTransformerDistilled.cls_token.expand", "deit.VisionTransformerDistilled.dist_token.expand", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq"], ["", "def", "forward_features", "(", "self", ",", "x", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "x", "=", "self", ".", "patch_embed", "(", "x", ")", "\n", "x", "=", "torch", ".", "cat", "(", "(", "\n", "self", ".", "cls_token", ".", "expand", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ",", "-", "1", ")", ",", "\n", "self", ".", "dist_token", ".", "expand", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ",", "-", "1", ")", ",", "x", ")", ",", "dim", "=", "1", ")", "\n", "x", "=", "self", ".", "pos_drop", "(", "x", "+", "self", ".", "pos_embed", ")", "\n", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "blocks", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "blocks", "(", "x", ")", "\n", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit.VisionTransformerDistilled.forward_head": [[118, 128], ["deit.VisionTransformerDistilled.head", "deit.VisionTransformerDistilled.head_dist", "torch.jit.is_scripting"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "if", "pre_logits", ":", "\n", "            ", "return", "(", "x", "[", ":", ",", "0", "]", "+", "x", "[", ":", ",", "1", "]", ")", "/", "2", "\n", "", "x", ",", "x_dist", "=", "self", ".", "head", "(", "x", "[", ":", ",", "0", "]", ")", ",", "self", ".", "head_dist", "(", "x", "[", ":", ",", "1", "]", ")", "\n", "if", "self", ".", "distilled_training", "and", "self", ".", "training", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "# only return separate classification predictions when training in distilled mode", "\n", "            ", "return", "x", ",", "x_dist", "\n", "", "else", ":", "\n", "# during standard train / finetune, inference average the classifier predictions", "\n", "            ", "return", "(", "x", "+", "x_dist", ")", "/", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit._cfg": [[20, 28], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", ".9", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'patch_embed.proj'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit._create_deit": [[130, 139], ["kwargs.get", "helpers.build_model_with_cfg", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "", "def", "_create_deit", "(", "variant", ",", "pretrained", "=", "False", ",", "distilled", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "kwargs", ".", "get", "(", "'features_only'", ",", "None", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'features_only not implemented for Vision Transformer models.'", ")", "\n", "", "model_cls", "=", "VisionTransformerDistilled", "if", "distilled", "else", "VisionTransformer", "\n", "model", "=", "build_model_with_cfg", "(", "\n", "model_cls", ",", "variant", ",", "pretrained", ",", "\n", "pretrained_filter_fn", "=", "checkpoint_filter_fn", ",", "\n", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit.deit_tiny_patch16_224": [[141, 149], ["dict", "deit._create_deit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit._create_deit"], ["", "@", "register_model", "\n", "def", "deit_tiny_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" DeiT-tiny model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "12", ",", "num_heads", "=", "3", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_deit", "(", "'deit_tiny_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit.deit_small_patch16_224": [[151, 159], ["dict", "deit._create_deit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit._create_deit"], ["", "@", "register_model", "\n", "def", "deit_small_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" DeiT-small model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_deit", "(", "'deit_small_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit.deit_base_patch16_224": [[161, 169], ["dict", "deit._create_deit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit._create_deit"], ["", "@", "register_model", "\n", "def", "deit_base_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" DeiT base model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_deit", "(", "'deit_base_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit.deit_base_patch16_384": [[171, 179], ["dict", "deit._create_deit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit._create_deit"], ["", "@", "register_model", "\n", "def", "deit_base_patch16_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" DeiT base model @ 384x384 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_deit", "(", "'deit_base_patch16_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit.deit_tiny_distilled_patch16_224": [[181, 190], ["dict", "deit._create_deit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit._create_deit"], ["", "@", "register_model", "\n", "def", "deit_tiny_distilled_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" DeiT-tiny distilled model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "12", ",", "num_heads", "=", "3", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_deit", "(", "\n", "'deit_tiny_distilled_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "distilled", "=", "True", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit.deit_small_distilled_patch16_224": [[192, 201], ["dict", "deit._create_deit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit._create_deit"], ["", "@", "register_model", "\n", "def", "deit_small_distilled_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" DeiT-small distilled model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_deit", "(", "\n", "'deit_small_distilled_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "distilled", "=", "True", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit.deit_base_distilled_patch16_224": [[203, 212], ["dict", "deit._create_deit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit._create_deit"], ["", "@", "register_model", "\n", "def", "deit_base_distilled_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" DeiT-base distilled model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_deit", "(", "\n", "'deit_base_distilled_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "distilled", "=", "True", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit.deit_base_distilled_patch16_384": [[214, 223], ["dict", "deit._create_deit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.deit._create_deit"], ["", "@", "register_model", "\n", "def", "deit_base_distilled_patch16_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" DeiT-base distilled model @ 384x384 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_deit", "(", "\n", "'deit_base_distilled_patch16_384'", ",", "pretrained", "=", "pretrained", ",", "distilled", "=", "True", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DlaBasic.__init__": [[55, 65], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "**", "_", ")", ":", "\n", "        ", "super", "(", "DlaBasic", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "\n", "inplanes", ",", "planes", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "padding", "=", "dilation", ",", "bias", "=", "False", ",", "dilation", "=", "dilation", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm2d", "(", "planes", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "\n", "planes", ",", "planes", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "dilation", ",", "bias", "=", "False", ",", "dilation", "=", "dilation", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "BatchNorm2d", "(", "planes", ")", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DlaBasic.forward": [[66, 81], ["dla.DlaBasic.conv1", "dla.DlaBasic.bn1", "dla.DlaBasic.relu", "dla.DlaBasic.conv2", "dla.DlaBasic.bn2", "dla.DlaBasic.relu"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "shortcut", "=", "None", ",", "children", ":", "Optional", "[", "List", "[", "torch", ".", "Tensor", "]", "]", "=", "None", ")", ":", "\n", "        ", "if", "shortcut", "is", "None", ":", "\n", "            ", "shortcut", "=", "x", "\n", "\n", "", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "bn2", "(", "out", ")", "\n", "\n", "out", "+=", "shortcut", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DlaBottleneck.__init__": [[87, 102], ["torch.Module.__init__", "int", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "math.floor"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "inplanes", ",", "outplanes", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "cardinality", "=", "1", ",", "base_width", "=", "64", ")", ":", "\n", "        ", "super", "(", "DlaBottleneck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "stride", "=", "stride", "\n", "mid_planes", "=", "int", "(", "math", ".", "floor", "(", "outplanes", "*", "(", "base_width", "/", "64", ")", ")", "*", "cardinality", ")", "\n", "mid_planes", "=", "mid_planes", "//", "self", ".", "expansion", "\n", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "inplanes", ",", "mid_planes", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm2d", "(", "mid_planes", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "\n", "mid_planes", ",", "mid_planes", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "padding", "=", "dilation", ",", "\n", "bias", "=", "False", ",", "dilation", "=", "dilation", ",", "groups", "=", "cardinality", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "BatchNorm2d", "(", "mid_planes", ")", "\n", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "mid_planes", ",", "outplanes", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn3", "=", "nn", ".", "BatchNorm2d", "(", "outplanes", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DlaBottleneck.forward": [[103, 122], ["dla.DlaBottleneck.conv1", "dla.DlaBottleneck.bn1", "dla.DlaBottleneck.relu", "dla.DlaBottleneck.conv2", "dla.DlaBottleneck.bn2", "dla.DlaBottleneck.relu", "dla.DlaBottleneck.conv3", "dla.DlaBottleneck.bn3", "dla.DlaBottleneck.relu"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "shortcut", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "children", ":", "Optional", "[", "List", "[", "torch", ".", "Tensor", "]", "]", "=", "None", ")", ":", "\n", "        ", "if", "shortcut", "is", "None", ":", "\n", "            ", "shortcut", "=", "x", "\n", "\n", "", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "bn2", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "out", "=", "self", ".", "bn3", "(", "out", ")", "\n", "\n", "out", "+=", "shortcut", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DlaBottle2neck.__init__": [[130, 156], ["torch.Module.__init__", "int", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "max", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "convs.append", "bns.append", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "math.floor", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "inplanes", ",", "outplanes", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "scale", "=", "4", ",", "cardinality", "=", "8", ",", "base_width", "=", "4", ")", ":", "\n", "        ", "super", "(", "DlaBottle2neck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "is_first", "=", "stride", ">", "1", "\n", "self", ".", "scale", "=", "scale", "\n", "mid_planes", "=", "int", "(", "math", ".", "floor", "(", "outplanes", "*", "(", "base_width", "/", "64", ")", ")", "*", "cardinality", ")", "\n", "mid_planes", "=", "mid_planes", "//", "self", ".", "expansion", "\n", "self", ".", "width", "=", "mid_planes", "\n", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "inplanes", ",", "mid_planes", "*", "scale", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm2d", "(", "mid_planes", "*", "scale", ")", "\n", "\n", "num_scale_convs", "=", "max", "(", "1", ",", "scale", "-", "1", ")", "\n", "convs", "=", "[", "]", "\n", "bns", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "num_scale_convs", ")", ":", "\n", "            ", "convs", ".", "append", "(", "nn", ".", "Conv2d", "(", "\n", "mid_planes", ",", "mid_planes", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "\n", "padding", "=", "dilation", ",", "dilation", "=", "dilation", ",", "groups", "=", "cardinality", ",", "bias", "=", "False", ")", ")", "\n", "bns", ".", "append", "(", "nn", ".", "BatchNorm2d", "(", "mid_planes", ")", ")", "\n", "", "self", ".", "convs", "=", "nn", ".", "ModuleList", "(", "convs", ")", "\n", "self", ".", "bns", "=", "nn", ".", "ModuleList", "(", "bns", ")", "\n", "self", ".", "pool", "=", "nn", ".", "AvgPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "padding", "=", "1", ")", "if", "self", ".", "is_first", "else", "None", "\n", "\n", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "mid_planes", "*", "scale", ",", "outplanes", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn3", "=", "nn", ".", "BatchNorm2d", "(", "outplanes", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DlaBottle2neck.forward": [[157, 191], ["dla.DlaBottle2neck.conv1", "dla.DlaBottle2neck.bn1", "dla.DlaBottle2neck.relu", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "dla.DlaBottle2neck.conv3", "dla.DlaBottle2neck.bn3", "dla.DlaBottle2neck.relu", "zip", "conv", "bn", "dla.DlaBottle2neck.relu", "spo.append", "spo.append", "spo.append", "dla.DlaBottle2neck.pool"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "shortcut", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "children", ":", "Optional", "[", "List", "[", "torch", ".", "Tensor", "]", "]", "=", "None", ")", ":", "\n", "        ", "if", "shortcut", "is", "None", ":", "\n", "            ", "shortcut", "=", "x", "\n", "\n", "", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "spx", "=", "torch", ".", "split", "(", "out", ",", "self", ".", "width", ",", "1", ")", "\n", "spo", "=", "[", "]", "\n", "sp", "=", "spx", "[", "0", "]", "# redundant, for torchscript", "\n", "for", "i", ",", "(", "conv", ",", "bn", ")", "in", "enumerate", "(", "zip", "(", "self", ".", "convs", ",", "self", ".", "bns", ")", ")", ":", "\n", "            ", "if", "i", "==", "0", "or", "self", ".", "is_first", ":", "\n", "                ", "sp", "=", "spx", "[", "i", "]", "\n", "", "else", ":", "\n", "                ", "sp", "=", "sp", "+", "spx", "[", "i", "]", "\n", "", "sp", "=", "conv", "(", "sp", ")", "\n", "sp", "=", "bn", "(", "sp", ")", "\n", "sp", "=", "self", ".", "relu", "(", "sp", ")", "\n", "spo", ".", "append", "(", "sp", ")", "\n", "", "if", "self", ".", "scale", ">", "1", ":", "\n", "            ", "if", "self", ".", "pool", "is", "not", "None", ":", "# self.is_first == True, None check for torchscript", "\n", "                ", "spo", ".", "append", "(", "self", ".", "pool", "(", "spx", "[", "-", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "spo", ".", "append", "(", "spx", "[", "-", "1", "]", ")", "\n", "", "", "out", "=", "torch", ".", "cat", "(", "spo", ",", "1", ")", "\n", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "out", "=", "self", ".", "bn3", "(", "out", ")", "\n", "\n", "out", "+=", "shortcut", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DlaRoot.__init__": [[194, 201], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "shortcut", ")", ":", "\n", "        ", "super", "(", "DlaRoot", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv", "=", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "out_channels", ",", "1", ",", "stride", "=", "1", ",", "bias", "=", "False", ",", "padding", "=", "(", "kernel_size", "-", "1", ")", "//", "2", ")", "\n", "self", ".", "bn", "=", "nn", ".", "BatchNorm2d", "(", "out_channels", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "shortcut", "=", "shortcut", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DlaRoot.forward": [[202, 210], ["dla.DlaRoot.conv", "dla.DlaRoot.bn", "dla.DlaRoot.relu", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x_children", ":", "List", "[", "torch", ".", "Tensor", "]", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "torch", ".", "cat", "(", "x_children", ",", "1", ")", ")", "\n", "x", "=", "self", ".", "bn", "(", "x", ")", "\n", "if", "self", ".", "shortcut", ":", "\n", "            ", "x", "+=", "x_children", "[", "0", "]", "\n", "", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DlaTree.__init__": [[213, 245], ["torch.Module.__init__", "torch.Identity", "torch.Identity", "torch.Identity", "dict", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.Identity", "torch.Identity", "torch.Identity", "block", "block", "dla.DlaRoot", "dict.update", "dla.DlaTree", "dla.DlaTree", "torch.Sequential", "torch.Sequential", "torch.Sequential", "dict", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update"], ["    ", "def", "__init__", "(", "\n", "self", ",", "levels", ",", "block", ",", "in_channels", ",", "out_channels", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "cardinality", "=", "1", ",", "\n", "base_width", "=", "64", ",", "level_root", "=", "False", ",", "root_dim", "=", "0", ",", "root_kernel_size", "=", "1", ",", "root_shortcut", "=", "False", ")", ":", "\n", "        ", "super", "(", "DlaTree", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "root_dim", "==", "0", ":", "\n", "            ", "root_dim", "=", "2", "*", "out_channels", "\n", "", "if", "level_root", ":", "\n", "            ", "root_dim", "+=", "in_channels", "\n", "", "self", ".", "downsample", "=", "nn", ".", "MaxPool2d", "(", "stride", ",", "stride", "=", "stride", ")", "if", "stride", ">", "1", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "project", "=", "nn", ".", "Identity", "(", ")", "\n", "cargs", "=", "dict", "(", "dilation", "=", "dilation", ",", "cardinality", "=", "cardinality", ",", "base_width", "=", "base_width", ")", "\n", "if", "levels", "==", "1", ":", "\n", "            ", "self", ".", "tree1", "=", "block", "(", "in_channels", ",", "out_channels", ",", "stride", ",", "**", "cargs", ")", "\n", "self", ".", "tree2", "=", "block", "(", "out_channels", ",", "out_channels", ",", "1", ",", "**", "cargs", ")", "\n", "if", "in_channels", "!=", "out_channels", ":", "\n", "# NOTE the official impl/weights have  project layers in levels > 1 case that are never", "\n", "# used, I've moved the project layer here to avoid wasted params but old checkpoints will", "\n", "# need strict=False while loading.", "\n", "                ", "self", ".", "project", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "out_channels", ")", ")", "\n", "", "self", ".", "root", "=", "DlaRoot", "(", "root_dim", ",", "out_channels", ",", "root_kernel_size", ",", "root_shortcut", ")", "\n", "", "else", ":", "\n", "            ", "cargs", ".", "update", "(", "dict", "(", "root_kernel_size", "=", "root_kernel_size", ",", "root_shortcut", "=", "root_shortcut", ")", ")", "\n", "self", ".", "tree1", "=", "DlaTree", "(", "\n", "levels", "-", "1", ",", "block", ",", "in_channels", ",", "out_channels", ",", "stride", ",", "root_dim", "=", "0", ",", "**", "cargs", ")", "\n", "self", ".", "tree2", "=", "DlaTree", "(", "\n", "levels", "-", "1", ",", "block", ",", "out_channels", ",", "out_channels", ",", "root_dim", "=", "root_dim", "+", "out_channels", ",", "**", "cargs", ")", "\n", "self", ".", "root", "=", "None", "\n", "", "self", ".", "level_root", "=", "level_root", "\n", "self", ".", "root_dim", "=", "root_dim", "\n", "self", ".", "levels", "=", "levels", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DlaTree.forward": [[246, 261], ["dla.DlaTree.downsample", "dla.DlaTree.project", "dla.DlaTree.tree1", "children.append", "dla.DlaTree.tree2", "dla.DlaTree.root", "children.append", "dla.DlaTree.tree2"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample"], ["", "def", "forward", "(", "self", ",", "x", ",", "shortcut", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "children", ":", "Optional", "[", "List", "[", "torch", ".", "Tensor", "]", "]", "=", "None", ")", ":", "\n", "        ", "if", "children", "is", "None", ":", "\n", "            ", "children", "=", "[", "]", "\n", "", "bottom", "=", "self", ".", "downsample", "(", "x", ")", "\n", "shortcut", "=", "self", ".", "project", "(", "bottom", ")", "\n", "if", "self", ".", "level_root", ":", "\n", "            ", "children", ".", "append", "(", "bottom", ")", "\n", "", "x1", "=", "self", ".", "tree1", "(", "x", ",", "shortcut", ")", "\n", "if", "self", ".", "root", "is", "not", "None", ":", "# levels == 1", "\n", "            ", "x2", "=", "self", ".", "tree2", "(", "x1", ")", "\n", "x", "=", "self", ".", "root", "(", "[", "x2", ",", "x1", "]", "+", "children", ")", "\n", "", "else", ":", "\n", "            ", "children", ".", "append", "(", "x1", ")", "\n", "x", "=", "self", ".", "tree2", "(", "x1", ",", "None", ",", "children", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DLA.__init__": [[264, 307], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "dla.DLA._make_conv_level", "dla.DLA._make_conv_level", "dict", "dla.DlaTree", "dla.DlaTree", "dla.DlaTree", "dla.DlaTree", "layers.create_classifier", "dla.DLA.modules", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "dict", "dict", "dict", "dict", "dict", "dict", "torch.Flatten", "torch.Flatten", "torch.Flatten", "torch.Identity", "torch.Identity", "torch.Identity", "isinstance", "m.weight.data.normal_", "isinstance", "math.sqrt", "m.weight.data.fill_", "m.bias.data.zero_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DLA._make_conv_level", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DLA._make_conv_level", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["    ", "def", "__init__", "(", "\n", "self", ",", "levels", ",", "channels", ",", "output_stride", "=", "32", ",", "num_classes", "=", "1000", ",", "in_chans", "=", "3", ",", "global_pool", "=", "'avg'", ",", "\n", "cardinality", "=", "1", ",", "base_width", "=", "64", ",", "block", "=", "DlaBottle2neck", ",", "shortcut_root", "=", "False", ",", "drop_rate", "=", "0.0", ")", ":", "\n", "        ", "super", "(", "DLA", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "channels", "=", "channels", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "cardinality", "=", "cardinality", "\n", "self", ".", "base_width", "=", "base_width", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "assert", "output_stride", "==", "32", "# FIXME support dilation", "\n", "\n", "self", ".", "base_layer", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "in_chans", ",", "channels", "[", "0", "]", ",", "kernel_size", "=", "7", ",", "stride", "=", "1", ",", "padding", "=", "3", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "channels", "[", "0", "]", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "level0", "=", "self", ".", "_make_conv_level", "(", "channels", "[", "0", "]", ",", "channels", "[", "0", "]", ",", "levels", "[", "0", "]", ")", "\n", "self", ".", "level1", "=", "self", ".", "_make_conv_level", "(", "channels", "[", "0", "]", ",", "channels", "[", "1", "]", ",", "levels", "[", "1", "]", ",", "stride", "=", "2", ")", "\n", "cargs", "=", "dict", "(", "cardinality", "=", "cardinality", ",", "base_width", "=", "base_width", ",", "root_shortcut", "=", "shortcut_root", ")", "\n", "self", ".", "level2", "=", "DlaTree", "(", "levels", "[", "2", "]", ",", "block", ",", "channels", "[", "1", "]", ",", "channels", "[", "2", "]", ",", "2", ",", "level_root", "=", "False", ",", "**", "cargs", ")", "\n", "self", ".", "level3", "=", "DlaTree", "(", "levels", "[", "3", "]", ",", "block", ",", "channels", "[", "2", "]", ",", "channels", "[", "3", "]", ",", "2", ",", "level_root", "=", "True", ",", "**", "cargs", ")", "\n", "self", ".", "level4", "=", "DlaTree", "(", "levels", "[", "4", "]", ",", "block", ",", "channels", "[", "3", "]", ",", "channels", "[", "4", "]", ",", "2", ",", "level_root", "=", "True", ",", "**", "cargs", ")", "\n", "self", ".", "level5", "=", "DlaTree", "(", "levels", "[", "5", "]", ",", "block", ",", "channels", "[", "4", "]", ",", "channels", "[", "5", "]", ",", "2", ",", "level_root", "=", "True", ",", "**", "cargs", ")", "\n", "self", ".", "feature_info", "=", "[", "\n", "dict", "(", "num_chs", "=", "channels", "[", "0", "]", ",", "reduction", "=", "1", ",", "module", "=", "'level0'", ")", ",", "# rare to have a meaningful stride 1 level", "\n", "dict", "(", "num_chs", "=", "channels", "[", "1", "]", ",", "reduction", "=", "2", ",", "module", "=", "'level1'", ")", ",", "\n", "dict", "(", "num_chs", "=", "channels", "[", "2", "]", ",", "reduction", "=", "4", ",", "module", "=", "'level2'", ")", ",", "\n", "dict", "(", "num_chs", "=", "channels", "[", "3", "]", ",", "reduction", "=", "8", ",", "module", "=", "'level3'", ")", ",", "\n", "dict", "(", "num_chs", "=", "channels", "[", "4", "]", ",", "reduction", "=", "16", ",", "module", "=", "'level4'", ")", ",", "\n", "dict", "(", "num_chs", "=", "channels", "[", "5", "]", ",", "reduction", "=", "32", ",", "module", "=", "'level5'", ")", ",", "\n", "]", "\n", "\n", "self", ".", "num_features", "=", "channels", "[", "-", "1", "]", "\n", "self", ".", "global_pool", ",", "self", ".", "fc", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ",", "use_conv", "=", "True", ")", "\n", "self", ".", "flatten", "=", "nn", ".", "Flatten", "(", "1", ")", "if", "global_pool", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                ", "n", "=", "m", ".", "kernel_size", "[", "0", "]", "*", "m", ".", "kernel_size", "[", "1", "]", "*", "m", ".", "out_channels", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "math", ".", "sqrt", "(", "2.", "/", "n", ")", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "                ", "m", ".", "weight", ".", "data", ".", "fill_", "(", "1", ")", "\n", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DLA._make_conv_level": [[308, 319], ["range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "modules.extend", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["None"], ["", "", "", "def", "_make_conv_level", "(", "self", ",", "inplanes", ",", "planes", ",", "convs", ",", "stride", "=", "1", ",", "dilation", "=", "1", ")", ":", "\n", "        ", "modules", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "convs", ")", ":", "\n", "            ", "modules", ".", "extend", "(", "[", "\n", "nn", ".", "Conv2d", "(", "\n", "inplanes", ",", "planes", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", "if", "i", "==", "0", "else", "1", ",", "\n", "padding", "=", "dilation", ",", "bias", "=", "False", ",", "dilation", "=", "dilation", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "planes", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "]", ")", "\n", "inplanes", "=", "planes", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "modules", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DLA.group_matcher": [[320, 332], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "\n", "stem", "=", "r'^base_layer'", ",", "\n", "blocks", "=", "r'^level(\\d+)'", "if", "coarse", "else", "[", "\n", "# an unusual arch, this achieves somewhat more granularity without getting super messy", "\n", "(", "r'^level(\\d+)\\.tree(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^level(\\d+)\\.root'", ",", "(", "2", ",", ")", ")", ",", "\n", "(", "r'^level(\\d+)'", ",", "(", "1", ",", ")", ")", "\n", "]", "\n", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DLA.set_grad_checkpointing": [[333, 336], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "'gradient checkpointing not supported'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DLA.get_classifier": [[337, 340], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DLA.reset_classifier": [[341, 346], ["layers.create_classifier", "torch.Flatten", "torch.Flatten", "torch.Flatten", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", ",", "self", ".", "fc", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ",", "use_conv", "=", "True", ")", "\n", "self", ".", "flatten", "=", "nn", ".", "Flatten", "(", "1", ")", "if", "global_pool", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DLA.forward_features": [[347, 356], ["dla.DLA.base_layer", "dla.DLA.level0", "dla.DLA.level1", "dla.DLA.level2", "dla.DLA.level3", "dla.DLA.level4", "dla.DLA.level5"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "base_layer", "(", "x", ")", "\n", "x", "=", "self", ".", "level0", "(", "x", ")", "\n", "x", "=", "self", ".", "level1", "(", "x", ")", "\n", "x", "=", "self", ".", "level2", "(", "x", ")", "\n", "x", "=", "self", ".", "level3", "(", "x", ")", "\n", "x", "=", "self", ".", "level4", "(", "x", ")", "\n", "x", "=", "self", ".", "level5", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DLA.forward_head": [[357, 366], ["dla.DLA.global_pool", "torch.dropout", "torch.dropout", "torch.dropout", "dla.DLA.flatten", "dla.DLA.fc", "dla.DLA.flatten"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "if", "self", ".", "drop_rate", ">", "0.", ":", "\n", "            ", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "drop_rate", ",", "training", "=", "self", ".", "training", ")", "\n", "", "if", "pre_logits", ":", "\n", "            ", "return", "x", ".", "flatten", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "return", "self", ".", "flatten", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.DLA.forward": [[367, 371], ["dla.DLA.forward_features", "dla.DLA.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla._cfg": [[23, 31], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bilinear'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'base_layer.0'", ",", "'classifier'", ":", "'fc'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla._create_dla": [[373, 379], ["helpers.build_model_with_cfg", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_dla", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "\n", "DLA", ",", "variant", ",", "pretrained", ",", "\n", "pretrained_strict", "=", "False", ",", "\n", "feature_cfg", "=", "dict", "(", "out_indices", "=", "(", "1", ",", "2", ",", "3", ",", "4", ",", "5", ")", ")", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.dla60_res2net": [[381, 387], ["dict", "dla._create_dla"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla._create_dla"], ["", "@", "register_model", "\n", "def", "dla60_res2net", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "levels", "=", "(", "1", ",", "1", ",", "1", ",", "2", ",", "3", ",", "1", ")", ",", "channels", "=", "(", "16", ",", "32", ",", "128", ",", "256", ",", "512", ",", "1024", ")", ",", "\n", "block", "=", "DlaBottle2neck", ",", "cardinality", "=", "1", ",", "base_width", "=", "28", ",", "**", "kwargs", ")", "\n", "return", "_create_dla", "(", "'dla60_res2net'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.dla60_res2next": [[389, 395], ["dict", "dla._create_dla"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla._create_dla"], ["", "@", "register_model", "\n", "def", "dla60_res2next", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "levels", "=", "(", "1", ",", "1", ",", "1", ",", "2", ",", "3", ",", "1", ")", ",", "channels", "=", "(", "16", ",", "32", ",", "128", ",", "256", ",", "512", ",", "1024", ")", ",", "\n", "block", "=", "DlaBottle2neck", ",", "cardinality", "=", "8", ",", "base_width", "=", "4", ",", "**", "kwargs", ")", "\n", "return", "_create_dla", "(", "'dla60_res2next'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.dla34": [[397, 403], ["dict", "dla._create_dla"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla._create_dla"], ["", "@", "register_model", "\n", "def", "dla34", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "# DLA-34", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "levels", "=", "[", "1", ",", "1", ",", "1", ",", "2", ",", "2", ",", "1", "]", ",", "channels", "=", "[", "16", ",", "32", ",", "64", ",", "128", ",", "256", ",", "512", "]", ",", "\n", "block", "=", "DlaBasic", ",", "**", "kwargs", ")", "\n", "return", "_create_dla", "(", "'dla34'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.dla46_c": [[405, 411], ["dict", "dla._create_dla"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla._create_dla"], ["", "@", "register_model", "\n", "def", "dla46_c", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "# DLA-46-C", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "levels", "=", "[", "1", ",", "1", ",", "1", ",", "2", ",", "2", ",", "1", "]", ",", "channels", "=", "[", "16", ",", "32", ",", "64", ",", "64", ",", "128", ",", "256", "]", ",", "\n", "block", "=", "DlaBottleneck", ",", "**", "kwargs", ")", "\n", "return", "_create_dla", "(", "'dla46_c'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.dla46x_c": [[413, 419], ["dict", "dla._create_dla"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla._create_dla"], ["", "@", "register_model", "\n", "def", "dla46x_c", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "# DLA-X-46-C", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "levels", "=", "[", "1", ",", "1", ",", "1", ",", "2", ",", "2", ",", "1", "]", ",", "channels", "=", "[", "16", ",", "32", ",", "64", ",", "64", ",", "128", ",", "256", "]", ",", "\n", "block", "=", "DlaBottleneck", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "**", "kwargs", ")", "\n", "return", "_create_dla", "(", "'dla46x_c'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.dla60x_c": [[421, 427], ["dict", "dla._create_dla"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla._create_dla"], ["", "@", "register_model", "\n", "def", "dla60x_c", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "# DLA-X-60-C", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "levels", "=", "[", "1", ",", "1", ",", "1", ",", "2", ",", "3", ",", "1", "]", ",", "channels", "=", "[", "16", ",", "32", ",", "64", ",", "64", ",", "128", ",", "256", "]", ",", "\n", "block", "=", "DlaBottleneck", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "**", "kwargs", ")", "\n", "return", "_create_dla", "(", "'dla60x_c'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.dla60": [[429, 435], ["dict", "dla._create_dla"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla._create_dla"], ["", "@", "register_model", "\n", "def", "dla60", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "# DLA-60", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "levels", "=", "[", "1", ",", "1", ",", "1", ",", "2", ",", "3", ",", "1", "]", ",", "channels", "=", "[", "16", ",", "32", ",", "128", ",", "256", ",", "512", ",", "1024", "]", ",", "\n", "block", "=", "DlaBottleneck", ",", "**", "kwargs", ")", "\n", "return", "_create_dla", "(", "'dla60'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.dla60x": [[437, 443], ["dict", "dla._create_dla"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla._create_dla"], ["", "@", "register_model", "\n", "def", "dla60x", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "# DLA-X-60", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "levels", "=", "[", "1", ",", "1", ",", "1", ",", "2", ",", "3", ",", "1", "]", ",", "channels", "=", "[", "16", ",", "32", ",", "128", ",", "256", ",", "512", ",", "1024", "]", ",", "\n", "block", "=", "DlaBottleneck", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "**", "kwargs", ")", "\n", "return", "_create_dla", "(", "'dla60x'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.dla102": [[445, 451], ["dict", "dla._create_dla"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla._create_dla"], ["", "@", "register_model", "\n", "def", "dla102", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "# DLA-102", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "levels", "=", "[", "1", ",", "1", ",", "1", ",", "3", ",", "4", ",", "1", "]", ",", "channels", "=", "[", "16", ",", "32", ",", "128", ",", "256", ",", "512", ",", "1024", "]", ",", "\n", "block", "=", "DlaBottleneck", ",", "shortcut_root", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_dla", "(", "'dla102'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.dla102x": [[453, 459], ["dict", "dla._create_dla"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla._create_dla"], ["", "@", "register_model", "\n", "def", "dla102x", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "# DLA-X-102", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "levels", "=", "[", "1", ",", "1", ",", "1", ",", "3", ",", "4", ",", "1", "]", ",", "channels", "=", "[", "16", ",", "32", ",", "128", ",", "256", ",", "512", ",", "1024", "]", ",", "\n", "block", "=", "DlaBottleneck", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "shortcut_root", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_dla", "(", "'dla102x'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.dla102x2": [[461, 467], ["dict", "dla._create_dla"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla._create_dla"], ["", "@", "register_model", "\n", "def", "dla102x2", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "# DLA-X-102 64", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "levels", "=", "[", "1", ",", "1", ",", "1", ",", "3", ",", "4", ",", "1", "]", ",", "channels", "=", "[", "16", ",", "32", ",", "128", ",", "256", ",", "512", ",", "1024", "]", ",", "\n", "block", "=", "DlaBottleneck", ",", "cardinality", "=", "64", ",", "base_width", "=", "4", ",", "shortcut_root", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_dla", "(", "'dla102x2'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla.dla169": [[469, 475], ["dict", "dla._create_dla"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.dla._create_dla"], ["", "@", "register_model", "\n", "def", "dla169", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "# DLA-169", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "levels", "=", "[", "1", ",", "1", ",", "2", ",", "3", ",", "5", ",", "1", "]", ",", "channels", "=", "[", "16", ",", "32", ",", "128", ",", "256", ",", "512", ",", "1024", "]", ",", "\n", "block", "=", "DlaBottleneck", ",", "shortcut_root", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_dla", "(", "'dla169'", ",", "pretrained", ",", "**", "model_kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.clean_state_dict": [[36, 43], ["collections.OrderedDict", "state_dict.items", "k.startswith"], "function", ["None"], ["def", "clean_state_dict", "(", "state_dict", ")", ":", "\n", "# 'clean' checkpoint by removing .module prefix from state dict if it exists from parallel training", "\n", "    ", "cleaned_state_dict", "=", "OrderedDict", "(", ")", "\n", "for", "k", ",", "v", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "        ", "name", "=", "k", "[", "7", ":", "]", "if", "k", ".", "startswith", "(", "'module.'", ")", "else", "k", "\n", "cleaned_state_dict", "[", "name", "]", "=", "v", "\n", "", "return", "cleaned_state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_state_dict": [[45, 64], ["os.path.isfile", "torch.load", "torch.load", "isinstance", "helpers.clean_state_dict", "_logger.info", "_logger.error", "FileNotFoundError", "torch.load.get", "torch.load.get"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.clean_state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["", "def", "load_state_dict", "(", "checkpoint_path", ",", "use_ema", "=", "True", ")", ":", "\n", "    ", "if", "checkpoint_path", "and", "os", ".", "path", ".", "isfile", "(", "checkpoint_path", ")", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "checkpoint_path", ",", "map_location", "=", "'cpu'", ")", "\n", "state_dict_key", "=", "''", "\n", "if", "isinstance", "(", "checkpoint", ",", "dict", ")", ":", "\n", "            ", "if", "use_ema", "and", "checkpoint", ".", "get", "(", "'state_dict_ema'", ",", "None", ")", "is", "not", "None", ":", "\n", "                ", "state_dict_key", "=", "'state_dict_ema'", "\n", "", "elif", "use_ema", "and", "checkpoint", ".", "get", "(", "'model_ema'", ",", "None", ")", "is", "not", "None", ":", "\n", "                ", "state_dict_key", "=", "'model_ema'", "\n", "", "elif", "'state_dict'", "in", "checkpoint", ":", "\n", "                ", "state_dict_key", "=", "'state_dict'", "\n", "", "elif", "'model'", "in", "checkpoint", ":", "\n", "                ", "state_dict_key", "=", "'model'", "\n", "", "", "state_dict", "=", "clean_state_dict", "(", "checkpoint", "[", "state_dict_key", "]", "if", "state_dict_key", "else", "checkpoint", ")", "\n", "_logger", ".", "info", "(", "\"Loaded {} from checkpoint '{}'\"", ".", "format", "(", "state_dict_key", ",", "checkpoint_path", ")", ")", "\n", "return", "state_dict", "\n", "", "else", ":", "\n", "        ", "_logger", ".", "error", "(", "\"No checkpoint found at '{}'\"", ".", "format", "(", "checkpoint_path", ")", ")", "\n", "raise", "FileNotFoundError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_checkpoint": [[66, 77], ["helpers.load_state_dict", "model.load_state_dict", "[].lower", "hasattr", "model.load_pretrained", "NotImplementedError", "os.path.splitext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.VisionTransformer.load_pretrained"], ["", "", "def", "load_checkpoint", "(", "model", ",", "checkpoint_path", ",", "use_ema", "=", "True", ",", "strict", "=", "True", ")", ":", "\n", "    ", "if", "os", ".", "path", ".", "splitext", "(", "checkpoint_path", ")", "[", "-", "1", "]", ".", "lower", "(", ")", "in", "(", "'.npz'", ",", "'.npy'", ")", ":", "\n", "# numpy checkpoint, try to load via model specific load_pretrained fn", "\n", "        ", "if", "hasattr", "(", "model", ",", "'load_pretrained'", ")", ":", "\n", "            ", "model", ".", "load_pretrained", "(", "checkpoint_path", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Model cannot load numpy checkpoint'", ")", "\n", "", "return", "\n", "", "state_dict", "=", "load_state_dict", "(", "checkpoint_path", ",", "use_ema", ")", "\n", "incompatible_keys", "=", "model", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "strict", ")", "\n", "return", "incompatible_keys", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.resume_checkpoint": [[79, 114], ["os.path.isfile", "torch.load", "torch.load", "_logger.error", "FileNotFoundError", "isinstance", "helpers.clean_state_dict", "model.load_state_dict", "model.load_state_dict", "_logger.info", "optimizer.load_state_dict", "loss_scaler.load_state_dict", "_logger.info", "_logger.info", "_logger.info", "_logger.info"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.clean_state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_state_dict"], ["", "def", "resume_checkpoint", "(", "model", ",", "checkpoint_path", ",", "optimizer", "=", "None", ",", "loss_scaler", "=", "None", ",", "log_info", "=", "True", ")", ":", "\n", "    ", "resume_epoch", "=", "None", "\n", "if", "os", ".", "path", ".", "isfile", "(", "checkpoint_path", ")", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "checkpoint_path", ",", "map_location", "=", "'cpu'", ")", "\n", "if", "isinstance", "(", "checkpoint", ",", "dict", ")", "and", "'state_dict'", "in", "checkpoint", ":", "\n", "            ", "if", "log_info", ":", "\n", "                ", "_logger", ".", "info", "(", "'Restoring model state from checkpoint...'", ")", "\n", "", "state_dict", "=", "clean_state_dict", "(", "checkpoint", "[", "'state_dict'", "]", ")", "\n", "model", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n", "if", "optimizer", "is", "not", "None", "and", "'optimizer'", "in", "checkpoint", ":", "\n", "                ", "if", "log_info", ":", "\n", "                    ", "_logger", ".", "info", "(", "'Restoring optimizer state from checkpoint...'", ")", "\n", "", "optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'optimizer'", "]", ")", "\n", "\n", "", "if", "loss_scaler", "is", "not", "None", "and", "loss_scaler", ".", "state_dict_key", "in", "checkpoint", ":", "\n", "                ", "if", "log_info", ":", "\n", "                    ", "_logger", ".", "info", "(", "'Restoring AMP loss scaler state from checkpoint...'", ")", "\n", "", "loss_scaler", ".", "load_state_dict", "(", "checkpoint", "[", "loss_scaler", ".", "state_dict_key", "]", ")", "\n", "\n", "", "if", "'epoch'", "in", "checkpoint", ":", "\n", "                ", "resume_epoch", "=", "checkpoint", "[", "'epoch'", "]", "\n", "if", "'version'", "in", "checkpoint", "and", "checkpoint", "[", "'version'", "]", ">", "1", ":", "\n", "                    ", "resume_epoch", "+=", "1", "# start at the next epoch, old checkpoints incremented before save", "\n", "\n", "", "", "if", "log_info", ":", "\n", "                ", "_logger", ".", "info", "(", "\"Loaded checkpoint '{}' (epoch {})\"", ".", "format", "(", "checkpoint_path", ",", "checkpoint", "[", "'epoch'", "]", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "model", ".", "load_state_dict", "(", "checkpoint", ")", "\n", "if", "log_info", ":", "\n", "                ", "_logger", ".", "info", "(", "\"Loaded checkpoint '{}'\"", ".", "format", "(", "checkpoint_path", ")", ")", "\n", "", "", "return", "resume_epoch", "\n", "", "else", ":", "\n", "        ", "_logger", ".", "error", "(", "\"No checkpoint found at '{}'\"", ".", "format", "(", "checkpoint_path", ")", ")", "\n", "raise", "FileNotFoundError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers._resolve_pretrained_source": [[116, 142], ["pretrained_cfg.get", "pretrained_cfg.get", "pretrained_cfg.get", "pretrained_cfg.get", "hub.has_hf_hub", "hub.has_hf_hub"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.has_hf_hub", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.has_hf_hub"], ["", "", "def", "_resolve_pretrained_source", "(", "pretrained_cfg", ")", ":", "\n", "    ", "cfg_source", "=", "pretrained_cfg", ".", "get", "(", "'source'", ",", "''", ")", "\n", "pretrained_url", "=", "pretrained_cfg", ".", "get", "(", "'url'", ",", "None", ")", "\n", "pretrained_file", "=", "pretrained_cfg", ".", "get", "(", "'file'", ",", "None", ")", "\n", "hf_hub_id", "=", "pretrained_cfg", ".", "get", "(", "'hf_hub_id'", ",", "None", ")", "\n", "# resolve where to load pretrained weights from", "\n", "load_from", "=", "''", "\n", "pretrained_loc", "=", "''", "\n", "if", "cfg_source", "==", "'hf-hub'", "and", "has_hf_hub", "(", "necessary", "=", "True", ")", ":", "\n", "# hf-hub specified as source via model identifier", "\n", "        ", "load_from", "=", "'hf-hub'", "\n", "assert", "hf_hub_id", "\n", "pretrained_loc", "=", "hf_hub_id", "\n", "", "else", ":", "\n", "# default source == timm or unspecified", "\n", "        ", "if", "pretrained_file", ":", "\n", "            ", "load_from", "=", "'file'", "\n", "pretrained_loc", "=", "pretrained_file", "\n", "", "elif", "pretrained_url", ":", "\n", "            ", "load_from", "=", "'url'", "\n", "pretrained_loc", "=", "pretrained_url", "\n", "", "elif", "hf_hub_id", "and", "has_hf_hub", "(", "necessary", "=", "False", ")", ":", "\n", "# hf-hub available as alternate weight source in default_cfg", "\n", "            ", "load_from", "=", "'hf-hub'", "\n", "pretrained_loc", "=", "hf_hub_id", "\n", "", "", "return", "load_from", ",", "pretrained_loc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.set_pretrained_download_progress": [[144, 148], ["None"], "function", ["None"], ["", "def", "set_pretrained_download_progress", "(", "enable", "=", "True", ")", ":", "\n", "    ", "\"\"\" Set download progress for pretrained weights on/off (globally). \"\"\"", "\n", "global", "_DOWNLOAD_PROGRESS", "\n", "_DOWNLOAD_PROGRESS", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.set_pretrained_check_hash": [[150, 154], ["None"], "function", ["None"], ["", "def", "set_pretrained_check_hash", "(", "enable", "=", "True", ")", ":", "\n", "    ", "\"\"\" Set hash checking for pretrained weights on/off (globally). \"\"\"", "\n", "global", "_CHECK_HASH", "\n", "_CHECK_HASH", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_custom_pretrained": [[156, 192], ["helpers._resolve_pretrained_source", "getattr", "_logger.warning", "_logger.warning", "load_fn", "hasattr", "hub.download_cached_file", "model.load_pretrained", "_logger.warning"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers._resolve_pretrained_source", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.download_cached_file", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.VisionTransformer.load_pretrained"], ["", "def", "load_custom_pretrained", "(", "\n", "model", ":", "nn", ".", "Module", ",", "\n", "pretrained_cfg", ":", "Optional", "[", "Dict", "]", "=", "None", ",", "\n", "load_fn", ":", "Optional", "[", "Callable", "]", "=", "None", ",", "\n", ")", ":", "\n", "    ", "r\"\"\"Loads a custom (read non .pth) weight file\n\n    Downloads checkpoint file into cache-dir like torch.hub based loaders, but calls\n    a passed in custom load fun, or the `load_pretrained` model member fn.\n\n    If the object is already present in `model_dir`, it's deserialized and returned.\n    The default value of `model_dir` is ``<hub_dir>/checkpoints`` where\n    `hub_dir` is the directory returned by :func:`~torch.hub.get_dir`.\n\n    Args:\n        model: The instantiated model to load weights into\n        pretrained_cfg (dict): Default pretrained model cfg\n        load_fn: An external stand alone fn that loads weights into provided model, otherwise a fn named\n            'laod_pretrained' on the model will be called if it exists\n    \"\"\"", "\n", "pretrained_cfg", "=", "pretrained_cfg", "or", "getattr", "(", "model", ",", "'pretrained_cfg'", ",", "None", ")", "or", "{", "}", "\n", "load_from", ",", "pretrained_loc", "=", "_resolve_pretrained_source", "(", "pretrained_cfg", ")", "\n", "if", "not", "load_from", ":", "\n", "        ", "_logger", ".", "warning", "(", "\"No pretrained weights exist for this model. Using random initialization.\"", ")", "\n", "return", "\n", "", "if", "load_from", "==", "'hf-hub'", ":", "# FIXME", "\n", "        ", "_logger", ".", "warning", "(", "\"Hugging Face hub not currently supported for custom load pretrained models.\"", ")", "\n", "", "elif", "load_from", "==", "'url'", ":", "\n", "        ", "pretrained_loc", "=", "download_cached_file", "(", "pretrained_loc", ",", "check_hash", "=", "_CHECK_HASH", ",", "progress", "=", "_DOWNLOAD_PROGRESS", ")", "\n", "\n", "", "if", "load_fn", "is", "not", "None", ":", "\n", "        ", "load_fn", "(", "model", ",", "pretrained_loc", ")", "\n", "", "elif", "hasattr", "(", "model", ",", "'load_pretrained'", ")", ":", "\n", "        ", "model", ".", "load_pretrained", "(", "pretrained_loc", ")", "\n", "", "else", ":", "\n", "        ", "_logger", ".", "warning", "(", "\"Valid function to load pretrained weights is not available, using random initialization.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.adapt_input_conv": [[194, 217], ["conv_weight.sum.float", "conv_weight.sum.to", "conv_weight.sum.reshape", "conv_weight.sum.sum", "conv_weight.sum.sum", "NotImplementedError", "int", "math.ceil", "conv_weight.sum.repeat", "float"], "function", ["None"], ["", "", "def", "adapt_input_conv", "(", "in_chans", ",", "conv_weight", ")", ":", "\n", "    ", "conv_type", "=", "conv_weight", ".", "dtype", "\n", "conv_weight", "=", "conv_weight", ".", "float", "(", ")", "# Some weights are in torch.half, ensure it's float for sum on CPU", "\n", "O", ",", "I", ",", "J", ",", "K", "=", "conv_weight", ".", "shape", "\n", "if", "in_chans", "==", "1", ":", "\n", "        ", "if", "I", ">", "3", ":", "\n", "            ", "assert", "conv_weight", ".", "shape", "[", "1", "]", "%", "3", "==", "0", "\n", "# For models with space2depth stems", "\n", "conv_weight", "=", "conv_weight", ".", "reshape", "(", "O", ",", "I", "//", "3", ",", "3", ",", "J", ",", "K", ")", "\n", "conv_weight", "=", "conv_weight", ".", "sum", "(", "dim", "=", "2", ",", "keepdim", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "conv_weight", "=", "conv_weight", ".", "sum", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "", "", "elif", "in_chans", "!=", "3", ":", "\n", "        ", "if", "I", "!=", "3", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Weight format not supported by conversion.'", ")", "\n", "", "else", ":", "\n", "# NOTE this strategy should be better than random init, but there could be other combinations of", "\n", "# the original RGB input layer weights that'd work better for specific cases.", "\n", "            ", "repeat", "=", "int", "(", "math", ".", "ceil", "(", "in_chans", "/", "3", ")", ")", "\n", "conv_weight", "=", "conv_weight", ".", "repeat", "(", "1", ",", "repeat", ",", "1", ",", "1", ")", "[", ":", ",", ":", "in_chans", ",", ":", ",", ":", "]", "\n", "conv_weight", "*=", "(", "3", "/", "float", "(", "in_chans", ")", ")", "\n", "", "", "conv_weight", "=", "conv_weight", ".", "to", "(", "conv_type", ")", "\n", "return", "conv_weight", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_pretrained": [[219, 297], ["helpers._resolve_pretrained_source", "pretrained_cfg.get", "pretrained_cfg.get", "pretrained_cfg.get", "model.load_state_dict", "getattr", "_logger.info", "helpers.load_state_dict", "isinstance", "isinstance", "_logger.info", "torch.hub.load_state_dict_from_url", "filter_fn", "_logger.info", "hub.load_state_dict_from_hf", "_logger.warning", "filter_fn", "helpers.adapt_input_conv", "_logger.info", "filter_fn.pop", "filter_fn.pop", "_logger.warning"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers._resolve_pretrained_source", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hub.load_state_dict_from_hf", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.adapt_input_conv"], ["", "def", "load_pretrained", "(", "\n", "model", ":", "nn", ".", "Module", ",", "\n", "pretrained_cfg", ":", "Optional", "[", "Dict", "]", "=", "None", ",", "\n", "num_classes", ":", "int", "=", "1000", ",", "\n", "in_chans", ":", "int", "=", "3", ",", "\n", "filter_fn", ":", "Optional", "[", "Callable", "]", "=", "None", ",", "\n", "strict", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "    ", "\"\"\" Load pretrained checkpoint\n\n    Args:\n        model (nn.Module) : PyTorch model module\n        pretrained_cfg (Optional[Dict]): configuration for pretrained weights / target dataset\n        num_classes (int): num_classes for model\n        in_chans (int): in_chans for model\n        filter_fn (Optional[Callable]): state_dict filter fn for load (takes state_dict, model as args)\n        strict (bool): strict load of checkpoint\n\n    \"\"\"", "\n", "pretrained_cfg", "=", "pretrained_cfg", "or", "getattr", "(", "model", ",", "'pretrained_cfg'", ",", "None", ")", "or", "{", "}", "\n", "load_from", ",", "pretrained_loc", "=", "_resolve_pretrained_source", "(", "pretrained_cfg", ")", "\n", "if", "load_from", "==", "'file'", ":", "\n", "        ", "_logger", ".", "info", "(", "f'Loading pretrained weights from file ({pretrained_loc})'", ")", "\n", "state_dict", "=", "load_state_dict", "(", "pretrained_loc", ")", "\n", "", "elif", "load_from", "==", "'url'", ":", "\n", "        ", "_logger", ".", "info", "(", "f'Loading pretrained weights from url ({pretrained_loc})'", ")", "\n", "state_dict", "=", "load_state_dict_from_url", "(", "\n", "pretrained_loc", ",", "map_location", "=", "'cpu'", ",", "progress", "=", "_DOWNLOAD_PROGRESS", ",", "check_hash", "=", "_CHECK_HASH", ")", "\n", "", "elif", "load_from", "==", "'hf-hub'", ":", "\n", "        ", "_logger", ".", "info", "(", "f'Loading pretrained weights from Hugging Face hub ({pretrained_loc})'", ")", "\n", "state_dict", "=", "load_state_dict_from_hf", "(", "pretrained_loc", ")", "\n", "", "else", ":", "\n", "        ", "_logger", ".", "warning", "(", "\"No pretrained weights exist or were found for this model. Using random initialization.\"", ")", "\n", "return", "\n", "\n", "", "if", "filter_fn", "is", "not", "None", ":", "\n", "# for backwards compat with filter fn that take one arg, try one first, the two", "\n", "        ", "try", ":", "\n", "            ", "state_dict", "=", "filter_fn", "(", "state_dict", ")", "\n", "", "except", "TypeError", ":", "\n", "            ", "state_dict", "=", "filter_fn", "(", "state_dict", ",", "model", ")", "\n", "\n", "", "", "input_convs", "=", "pretrained_cfg", ".", "get", "(", "'first_conv'", ",", "None", ")", "\n", "if", "input_convs", "is", "not", "None", "and", "in_chans", "!=", "3", ":", "\n", "        ", "if", "isinstance", "(", "input_convs", ",", "str", ")", ":", "\n", "            ", "input_convs", "=", "(", "input_convs", ",", ")", "\n", "", "for", "input_conv_name", "in", "input_convs", ":", "\n", "            ", "weight_name", "=", "input_conv_name", "+", "'.weight'", "\n", "try", ":", "\n", "                ", "state_dict", "[", "weight_name", "]", "=", "adapt_input_conv", "(", "in_chans", ",", "state_dict", "[", "weight_name", "]", ")", "\n", "_logger", ".", "info", "(", "\n", "f'Converted input conv {input_conv_name} pretrained weights from 3 to {in_chans} channel(s)'", ")", "\n", "", "except", "NotImplementedError", "as", "e", ":", "\n", "                ", "del", "state_dict", "[", "weight_name", "]", "\n", "strict", "=", "False", "\n", "_logger", ".", "warning", "(", "\n", "f'Unable to convert pretrained {input_conv_name} weights, using random init for this layer.'", ")", "\n", "\n", "", "", "", "classifiers", "=", "pretrained_cfg", ".", "get", "(", "'classifier'", ",", "None", ")", "\n", "label_offset", "=", "pretrained_cfg", ".", "get", "(", "'label_offset'", ",", "0", ")", "\n", "if", "classifiers", "is", "not", "None", ":", "\n", "        ", "if", "isinstance", "(", "classifiers", ",", "str", ")", ":", "\n", "            ", "classifiers", "=", "(", "classifiers", ",", ")", "\n", "", "if", "num_classes", "!=", "pretrained_cfg", "[", "'num_classes'", "]", ":", "\n", "            ", "for", "classifier_name", "in", "classifiers", ":", "\n", "# completely discard fully connected if model num_classes doesn't match pretrained weights", "\n", "                ", "state_dict", ".", "pop", "(", "classifier_name", "+", "'.weight'", ",", "None", ")", "\n", "state_dict", ".", "pop", "(", "classifier_name", "+", "'.bias'", ",", "None", ")", "\n", "", "strict", "=", "False", "\n", "", "elif", "label_offset", ">", "0", ":", "\n", "            ", "for", "classifier_name", "in", "classifiers", ":", "\n", "# special case for pretrained weights with an extra background class in pretrained weights", "\n", "                ", "classifier_weight", "=", "state_dict", "[", "classifier_name", "+", "'.weight'", "]", "\n", "state_dict", "[", "classifier_name", "+", "'.weight'", "]", "=", "classifier_weight", "[", "label_offset", ":", "]", "\n", "classifier_bias", "=", "state_dict", "[", "classifier_name", "+", "'.bias'", "]", "\n", "state_dict", "[", "classifier_name", "+", "'.bias'", "]", "=", "classifier_bias", "[", "label_offset", ":", "]", "\n", "\n", "", "", "", "model", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "strict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.extract_layer": [[299, 315], ["layer.split.split", "hasattr", "hasattr", "hasattr", "l.isdigit", "getattr", "int"], "function", ["None"], ["", "def", "extract_layer", "(", "model", ",", "layer", ")", ":", "\n", "    ", "layer", "=", "layer", ".", "split", "(", "'.'", ")", "\n", "module", "=", "model", "\n", "if", "hasattr", "(", "model", ",", "'module'", ")", "and", "layer", "[", "0", "]", "!=", "'module'", ":", "\n", "        ", "module", "=", "model", ".", "module", "\n", "", "if", "not", "hasattr", "(", "model", ",", "'module'", ")", "and", "layer", "[", "0", "]", "==", "'module'", ":", "\n", "        ", "layer", "=", "layer", "[", "1", ":", "]", "\n", "", "for", "l", "in", "layer", ":", "\n", "        ", "if", "hasattr", "(", "module", ",", "l", ")", ":", "\n", "            ", "if", "not", "l", ".", "isdigit", "(", ")", ":", "\n", "                ", "module", "=", "getattr", "(", "module", ",", "l", ")", "\n", "", "else", ":", "\n", "                ", "module", "=", "module", "[", "int", "(", "l", ")", "]", "\n", "", "", "else", ":", "\n", "            ", "return", "module", "\n", "", "", "return", "module", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.set_layer": [[317, 339], ["layer.split.split", "setattr", "hasattr", "hasattr", "l.isdigit", "getattr", "l.isdigit", "getattr", "int", "int"], "function", ["None"], ["", "def", "set_layer", "(", "model", ",", "layer", ",", "val", ")", ":", "\n", "    ", "layer", "=", "layer", ".", "split", "(", "'.'", ")", "\n", "module", "=", "model", "\n", "if", "hasattr", "(", "model", ",", "'module'", ")", "and", "layer", "[", "0", "]", "!=", "'module'", ":", "\n", "        ", "module", "=", "model", ".", "module", "\n", "", "lst_index", "=", "0", "\n", "module2", "=", "module", "\n", "for", "l", "in", "layer", ":", "\n", "        ", "if", "hasattr", "(", "module2", ",", "l", ")", ":", "\n", "            ", "if", "not", "l", ".", "isdigit", "(", ")", ":", "\n", "                ", "module2", "=", "getattr", "(", "module2", ",", "l", ")", "\n", "", "else", ":", "\n", "                ", "module2", "=", "module2", "[", "int", "(", "l", ")", "]", "\n", "", "lst_index", "+=", "1", "\n", "", "", "lst_index", "-=", "1", "\n", "for", "l", "in", "layer", "[", ":", "lst_index", "]", ":", "\n", "        ", "if", "not", "l", ".", "isdigit", "(", ")", ":", "\n", "            ", "module", "=", "getattr", "(", "module", ",", "l", ")", "\n", "", "else", ":", "\n", "            ", "module", "=", "module", "[", "int", "(", "l", ")", "]", "\n", "", "", "l", "=", "layer", "[", "lst_index", "]", "\n", "setattr", "(", "module", ",", "l", ",", "val", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.adapt_model_from_string": [[341, 396], ["model_string.split", "copy.deepcopy", "parent_module.named_modules", "copy.deepcopy.eval", "parent_module.eval", "k.split.split", "[].split", "helpers.extract_layer", "isinstance", "isinstance", "isinstance", "conv", "helpers.set_layer", "isinstance", "int", "layers.BatchNormAct2d", "helpers.set_layer", "isinstance", "torch.BatchNorm2d", "helpers.set_layer", "isinstance", "layers.Linear", "helpers.set_layer", "hasattr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_modules", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.extract_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.set_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.set_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.set_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.set_layer"], ["", "def", "adapt_model_from_string", "(", "parent_module", ",", "model_string", ")", ":", "\n", "    ", "separator", "=", "'***'", "\n", "state_dict", "=", "{", "}", "\n", "lst_shape", "=", "model_string", ".", "split", "(", "separator", ")", "\n", "for", "k", "in", "lst_shape", ":", "\n", "        ", "k", "=", "k", ".", "split", "(", "':'", ")", "\n", "key", "=", "k", "[", "0", "]", "\n", "shape", "=", "k", "[", "1", "]", "[", "1", ":", "-", "1", "]", ".", "split", "(", "','", ")", "\n", "if", "shape", "[", "0", "]", "!=", "''", ":", "\n", "            ", "state_dict", "[", "key", "]", "=", "[", "int", "(", "i", ")", "for", "i", "in", "shape", "]", "\n", "\n", "", "", "new_module", "=", "deepcopy", "(", "parent_module", ")", "\n", "for", "n", ",", "m", "in", "parent_module", ".", "named_modules", "(", ")", ":", "\n", "        ", "old_module", "=", "extract_layer", "(", "parent_module", ",", "n", ")", "\n", "if", "isinstance", "(", "old_module", ",", "nn", ".", "Conv2d", ")", "or", "isinstance", "(", "old_module", ",", "Conv2dSame", ")", ":", "\n", "            ", "if", "isinstance", "(", "old_module", ",", "Conv2dSame", ")", ":", "\n", "                ", "conv", "=", "Conv2dSame", "\n", "", "else", ":", "\n", "                ", "conv", "=", "nn", ".", "Conv2d", "\n", "", "s", "=", "state_dict", "[", "n", "+", "'.weight'", "]", "\n", "in_channels", "=", "s", "[", "1", "]", "\n", "out_channels", "=", "s", "[", "0", "]", "\n", "g", "=", "1", "\n", "if", "old_module", ".", "groups", ">", "1", ":", "\n", "                ", "in_channels", "=", "out_channels", "\n", "g", "=", "in_channels", "\n", "", "new_conv", "=", "conv", "(", "\n", "in_channels", "=", "in_channels", ",", "out_channels", "=", "out_channels", ",", "kernel_size", "=", "old_module", ".", "kernel_size", ",", "\n", "bias", "=", "old_module", ".", "bias", "is", "not", "None", ",", "padding", "=", "old_module", ".", "padding", ",", "dilation", "=", "old_module", ".", "dilation", ",", "\n", "groups", "=", "g", ",", "stride", "=", "old_module", ".", "stride", ")", "\n", "set_layer", "(", "new_module", ",", "n", ",", "new_conv", ")", "\n", "", "elif", "isinstance", "(", "old_module", ",", "BatchNormAct2d", ")", ":", "\n", "            ", "new_bn", "=", "BatchNormAct2d", "(", "\n", "state_dict", "[", "n", "+", "'.weight'", "]", "[", "0", "]", ",", "eps", "=", "old_module", ".", "eps", ",", "momentum", "=", "old_module", ".", "momentum", ",", "\n", "affine", "=", "old_module", ".", "affine", ",", "track_running_stats", "=", "True", ")", "\n", "new_bn", ".", "drop", "=", "old_module", ".", "drop", "\n", "new_bn", ".", "act", "=", "old_module", ".", "act", "\n", "set_layer", "(", "new_module", ",", "n", ",", "new_bn", ")", "\n", "", "elif", "isinstance", "(", "old_module", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "            ", "new_bn", "=", "nn", ".", "BatchNorm2d", "(", "\n", "num_features", "=", "state_dict", "[", "n", "+", "'.weight'", "]", "[", "0", "]", ",", "eps", "=", "old_module", ".", "eps", ",", "momentum", "=", "old_module", ".", "momentum", ",", "\n", "affine", "=", "old_module", ".", "affine", ",", "track_running_stats", "=", "True", ")", "\n", "set_layer", "(", "new_module", ",", "n", ",", "new_bn", ")", "\n", "", "elif", "isinstance", "(", "old_module", ",", "nn", ".", "Linear", ")", ":", "\n", "# FIXME extra checks to ensure this is actually the FC classifier layer and not a diff Linear layer?", "\n", "            ", "num_features", "=", "state_dict", "[", "n", "+", "'.weight'", "]", "[", "1", "]", "\n", "new_fc", "=", "Linear", "(", "\n", "in_features", "=", "num_features", ",", "out_features", "=", "old_module", ".", "out_features", ",", "bias", "=", "old_module", ".", "bias", "is", "not", "None", ")", "\n", "set_layer", "(", "new_module", ",", "n", ",", "new_fc", ")", "\n", "if", "hasattr", "(", "new_module", ",", "'num_features'", ")", ":", "\n", "                ", "new_module", ".", "num_features", "=", "num_features", "\n", "", "", "", "new_module", ".", "eval", "(", ")", "\n", "parent_module", ".", "eval", "(", ")", "\n", "\n", "return", "new_module", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.adapt_model_from_file": [[398, 402], ["os.path.join", "os.path.dirname", "open", "helpers.adapt_model_from_string", "f.read().strip", "f.read"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.adapt_model_from_string"], ["", "def", "adapt_model_from_file", "(", "parent_module", ",", "model_variant", ")", ":", "\n", "    ", "adapt_file", "=", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "__file__", ")", ",", "'pruned'", ",", "model_variant", "+", "'.txt'", ")", "\n", "with", "open", "(", "adapt_file", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "return", "adapt_model_from_string", "(", "parent_module", ",", "f", ".", "read", "(", ")", ".", "strip", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.pretrained_cfg_for_features": [[404, 411], ["copy.deepcopy", "copy.deepcopy.pop"], "function", ["None"], ["", "", "def", "pretrained_cfg_for_features", "(", "pretrained_cfg", ")", ":", "\n", "    ", "pretrained_cfg", "=", "deepcopy", "(", "pretrained_cfg", ")", "\n", "# remove default pretrained cfg fields that don't have much relevance for feature backbone", "\n", "to_remove", "=", "(", "'num_classes'", ",", "'crop_pct'", ",", "'classifier'", ",", "'global_pool'", ")", "# add default final pool size?", "\n", "for", "tr", "in", "to_remove", ":", "\n", "        ", "pretrained_cfg", ".", "pop", "(", "tr", ",", "None", ")", "\n", "", "return", "pretrained_cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.set_default_kwargs": [[413, 431], ["pretrained_cfg.get", "kwargs.setdefault", "pretrained_cfg.get", "pretrained_cfg.get", "len", "kwargs.setdefault", "kwargs.setdefault", "len"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["", "def", "set_default_kwargs", "(", "kwargs", ",", "names", ",", "pretrained_cfg", ")", ":", "\n", "    ", "for", "n", "in", "names", ":", "\n", "# for legacy reasons, model __init__args uses img_size + in_chans as separate args while", "\n", "# pretrained_cfg has one input_size=(C, H ,W) entry", "\n", "        ", "if", "n", "==", "'img_size'", ":", "\n", "            ", "input_size", "=", "pretrained_cfg", ".", "get", "(", "'input_size'", ",", "None", ")", "\n", "if", "input_size", "is", "not", "None", ":", "\n", "                ", "assert", "len", "(", "input_size", ")", "==", "3", "\n", "kwargs", ".", "setdefault", "(", "n", ",", "input_size", "[", "-", "2", ":", "]", ")", "\n", "", "", "elif", "n", "==", "'in_chans'", ":", "\n", "            ", "input_size", "=", "pretrained_cfg", ".", "get", "(", "'input_size'", ",", "None", ")", "\n", "if", "input_size", "is", "not", "None", ":", "\n", "                ", "assert", "len", "(", "input_size", ")", "==", "3", "\n", "kwargs", ".", "setdefault", "(", "n", ",", "input_size", "[", "0", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "default_val", "=", "pretrained_cfg", ".", "get", "(", "n", ",", "None", ")", "\n", "if", "default_val", "is", "not", "None", ":", "\n", "                ", "kwargs", ".", "setdefault", "(", "n", ",", "pretrained_cfg", "[", "n", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.filter_kwargs": [[433, 438], ["kwargs.pop"], "function", ["None"], ["", "", "", "", "def", "filter_kwargs", "(", "kwargs", ",", "names", ")", ":", "\n", "    ", "if", "not", "kwargs", "or", "not", "names", ":", "\n", "        ", "return", "\n", "", "for", "n", "in", "names", ":", "\n", "        ", "kwargs", ".", "pop", "(", "n", ",", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.update_pretrained_cfg_and_kwargs": [[440, 456], ["pretrained_cfg.get", "helpers.set_default_kwargs", "helpers.filter_kwargs"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.set_default_kwargs", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.filter_kwargs"], ["", "", "def", "update_pretrained_cfg_and_kwargs", "(", "pretrained_cfg", ",", "kwargs", ",", "kwargs_filter", ")", ":", "\n", "    ", "\"\"\" Update the default_cfg and kwargs before passing to model\n\n    Args:\n        pretrained_cfg: input pretrained cfg (updated in-place)\n        kwargs: keyword args passed to model build fn (updated in-place)\n        kwargs_filter: keyword arg keys that must be removed before model __init__\n    \"\"\"", "\n", "# Set model __init__ args that can be determined by default_cfg (if not already passed as kwargs)", "\n", "default_kwarg_names", "=", "(", "'num_classes'", ",", "'global_pool'", ",", "'in_chans'", ")", "\n", "if", "pretrained_cfg", ".", "get", "(", "'fixed_input_size'", ",", "False", ")", ":", "\n", "# if fixed_input_size exists and is True, model takes an img_size arg that fixes its input size", "\n", "        ", "default_kwarg_names", "+=", "(", "'img_size'", ",", ")", "\n", "", "set_default_kwargs", "(", "kwargs", ",", "names", "=", "default_kwarg_names", ",", "pretrained_cfg", "=", "pretrained_cfg", ")", "\n", "# Filter keyword args for task specific model variants (some 'features only' models, etc.)", "\n", "filter_kwargs", "(", "kwargs", ",", "names", "=", "kwargs_filter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.resolve_pretrained_cfg": [[458, 479], ["registry.get_pretrained_cfg", "isinstance", "copy.deepcopy", "_logger.warning", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.registry.get_pretrained_cfg"], ["", "def", "resolve_pretrained_cfg", "(", "variant", ":", "str", ",", "pretrained_cfg", "=", "None", ")", ":", "\n", "    ", "if", "pretrained_cfg", "and", "isinstance", "(", "pretrained_cfg", ",", "dict", ")", ":", "\n", "# highest priority, pretrained_cfg available and passed as arg", "\n", "        ", "return", "deepcopy", "(", "pretrained_cfg", ")", "\n", "# fallback to looking up pretrained cfg in model registry by variant identifier", "\n", "", "pretrained_cfg", "=", "get_pretrained_cfg", "(", "variant", ")", "\n", "if", "not", "pretrained_cfg", ":", "\n", "        ", "_logger", ".", "warning", "(", "\n", "f\"No pretrained configuration specified for {variant} model. Using a default.\"", "\n", "f\" Please add a config to the model pretrained_cfg registry or pass explicitly.\"", ")", "\n", "pretrained_cfg", "=", "dict", "(", "\n", "url", "=", "''", ",", "\n", "num_classes", "=", "1000", ",", "\n", "input_size", "=", "(", "3", ",", "224", ",", "224", ")", ",", "\n", "pool_size", "=", "None", ",", "\n", "crop_pct", "=", ".9", ",", "\n", "interpolation", "=", "'bicubic'", ",", "\n", "first_conv", "=", "''", ",", "\n", "classifier", "=", "''", ",", "\n", ")", "\n", "", "return", "pretrained_cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg": [[481, 571], ["kwargs.pop", "helpers.resolve_pretrained_cfg", "helpers.update_pretrained_cfg_and_kwargs", "resolve_pretrained_cfg.setdefault", "kwargs.pop", "feature_cfg.setdefault", "model_cls", "model_cls", "helpers.adapt_model_from_file", "getattr", "feature_cls.lower.", "helpers.pretrained_cfg_for_features", "kwargs.pop", "kwargs.get", "helpers.load_custom_pretrained", "helpers.load_pretrained", "feature_cfg.pop", "isinstance", "feature_cls.lower.lower", "kwargs.get"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.resolve_pretrained_cfg", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.update_pretrained_cfg_and_kwargs", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.adapt_model_from_file", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.pretrained_cfg_for_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_custom_pretrained", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.VisionTransformer.load_pretrained", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["", "def", "build_model_with_cfg", "(", "\n", "model_cls", ":", "Callable", ",", "\n", "variant", ":", "str", ",", "\n", "pretrained", ":", "bool", ",", "\n", "pretrained_cfg", ":", "Optional", "[", "Dict", "]", "=", "None", ",", "\n", "model_cfg", ":", "Optional", "[", "Any", "]", "=", "None", ",", "\n", "feature_cfg", ":", "Optional", "[", "Dict", "]", "=", "None", ",", "\n", "pretrained_strict", ":", "bool", "=", "True", ",", "\n", "pretrained_filter_fn", ":", "Optional", "[", "Callable", "]", "=", "None", ",", "\n", "pretrained_custom_load", ":", "bool", "=", "False", ",", "\n", "kwargs_filter", ":", "Optional", "[", "Tuple", "[", "str", "]", "]", "=", "None", ",", "\n", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Build model with specified default_cfg and optional model_cfg\n\n    This helper fn aids in the construction of a model including:\n      * handling default_cfg and associated pretrained weight loading\n      * passing through optional model_cfg for models with config based arch spec\n      * features_only model adaptation\n      * pruning config / model adaptation\n\n    Args:\n        model_cls (nn.Module): model class\n        variant (str): model variant name\n        pretrained (bool): load pretrained weights\n        pretrained_cfg (dict): model's pretrained weight/task config\n        model_cfg (Optional[Dict]): model's architecture config\n        feature_cfg (Optional[Dict]: feature extraction adapter config\n        pretrained_strict (bool): load pretrained weights strictly\n        pretrained_filter_fn (Optional[Callable]): filter callable for pretrained weights\n        pretrained_custom_load (bool): use custom load fn, to load numpy or other non PyTorch weights\n        kwargs_filter (Optional[Tuple]): kwargs to filter before passing to model\n        **kwargs: model args passed through to model __init__\n    \"\"\"", "\n", "pruned", "=", "kwargs", ".", "pop", "(", "'pruned'", ",", "False", ")", "\n", "features", "=", "False", "\n", "feature_cfg", "=", "feature_cfg", "or", "{", "}", "\n", "\n", "# resolve and update model pretrained config and model kwargs", "\n", "pretrained_cfg", "=", "resolve_pretrained_cfg", "(", "variant", ",", "pretrained_cfg", "=", "pretrained_cfg", ")", "\n", "update_pretrained_cfg_and_kwargs", "(", "pretrained_cfg", ",", "kwargs", ",", "kwargs_filter", ")", "\n", "pretrained_cfg", ".", "setdefault", "(", "'architecture'", ",", "variant", ")", "\n", "\n", "# Setup for feature extraction wrapper done at end of this fn", "\n", "if", "kwargs", ".", "pop", "(", "'features_only'", ",", "False", ")", ":", "\n", "        ", "features", "=", "True", "\n", "feature_cfg", ".", "setdefault", "(", "'out_indices'", ",", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ")", "\n", "if", "'out_indices'", "in", "kwargs", ":", "\n", "            ", "feature_cfg", "[", "'out_indices'", "]", "=", "kwargs", ".", "pop", "(", "'out_indices'", ")", "\n", "\n", "# Build the model", "\n", "", "", "model", "=", "model_cls", "(", "**", "kwargs", ")", "if", "model_cfg", "is", "None", "else", "model_cls", "(", "cfg", "=", "model_cfg", ",", "**", "kwargs", ")", "\n", "model", ".", "pretrained_cfg", "=", "pretrained_cfg", "\n", "model", ".", "default_cfg", "=", "model", ".", "pretrained_cfg", "# alias for backwards compat", "\n", "\n", "if", "pruned", ":", "\n", "        ", "model", "=", "adapt_model_from_file", "(", "model", ",", "variant", ")", "\n", "\n", "# For classification models, check class attr, then kwargs, then default to 1k, otherwise 0 for feats", "\n", "", "num_classes_pretrained", "=", "0", "if", "features", "else", "getattr", "(", "model", ",", "'num_classes'", ",", "kwargs", ".", "get", "(", "'num_classes'", ",", "1000", ")", ")", "\n", "if", "pretrained", ":", "\n", "        ", "if", "pretrained_custom_load", ":", "\n", "# FIXME improve custom load trigger", "\n", "            ", "load_custom_pretrained", "(", "model", ",", "pretrained_cfg", "=", "pretrained_cfg", ")", "\n", "", "else", ":", "\n", "            ", "load_pretrained", "(", "\n", "model", ",", "\n", "pretrained_cfg", "=", "pretrained_cfg", ",", "\n", "num_classes", "=", "num_classes_pretrained", ",", "\n", "in_chans", "=", "kwargs", ".", "get", "(", "'in_chans'", ",", "3", ")", ",", "\n", "filter_fn", "=", "pretrained_filter_fn", ",", "\n", "strict", "=", "pretrained_strict", ")", "\n", "\n", "# Wrap the model in a feature extraction module if enabled", "\n", "", "", "if", "features", ":", "\n", "        ", "feature_cls", "=", "FeatureListNet", "\n", "if", "'feature_cls'", "in", "feature_cfg", ":", "\n", "            ", "feature_cls", "=", "feature_cfg", ".", "pop", "(", "'feature_cls'", ")", "\n", "if", "isinstance", "(", "feature_cls", ",", "str", ")", ":", "\n", "                ", "feature_cls", "=", "feature_cls", ".", "lower", "(", ")", "\n", "if", "'hook'", "in", "feature_cls", ":", "\n", "                    ", "feature_cls", "=", "FeatureHookNet", "\n", "", "elif", "feature_cls", "==", "'fx'", ":", "\n", "                    ", "feature_cls", "=", "FeatureGraphNet", "\n", "", "else", ":", "\n", "                    ", "assert", "False", ",", "f'Unknown feature class {feature_cls}'", "\n", "", "", "", "model", "=", "feature_cls", "(", "model", ",", "**", "feature_cfg", ")", "\n", "model", ".", "pretrained_cfg", "=", "pretrained_cfg_for_features", "(", "pretrained_cfg", ")", "# add back default_cfg", "\n", "model", ".", "default_cfg", "=", "model", ".", "pretrained_cfg", "# alias for backwards compat", "\n", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.model_parameters": [[573, 579], ["model.parameters", "model.parameters"], "function", ["None"], ["", "def", "model_parameters", "(", "model", ",", "exclude_head", "=", "False", ")", ":", "\n", "    ", "if", "exclude_head", ":", "\n", "# FIXME this a bit of a quick and dirty hack to skip classifier head params based on ordering", "\n", "        ", "return", "[", "p", "for", "p", "in", "model", ".", "parameters", "(", ")", "]", "[", ":", "-", "2", "]", "\n", "", "else", ":", "\n", "        ", "return", "model", ".", "parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_apply": [[581, 590], ["module.named_children", "fn", "helpers.named_apply", "fn"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_apply"], ["", "", "def", "named_apply", "(", "fn", ":", "Callable", ",", "module", ":", "nn", ".", "Module", ",", "name", "=", "''", ",", "depth_first", "=", "True", ",", "include_root", "=", "False", ")", "->", "nn", ".", "Module", ":", "\n", "    ", "if", "not", "depth_first", "and", "include_root", ":", "\n", "        ", "fn", "(", "module", "=", "module", ",", "name", "=", "name", ")", "\n", "", "for", "child_name", ",", "child_module", "in", "module", ".", "named_children", "(", ")", ":", "\n", "        ", "child_name", "=", "'.'", ".", "join", "(", "(", "name", ",", "child_name", ")", ")", "if", "name", "else", "child_name", "\n", "named_apply", "(", "fn", "=", "fn", ",", "module", "=", "child_module", ",", "name", "=", "child_name", ",", "depth_first", "=", "depth_first", ",", "include_root", "=", "True", ")", "\n", "", "if", "depth_first", "and", "include_root", ":", "\n", "        ", "fn", "(", "module", "=", "module", ",", "name", "=", "name", ")", "\n", "", "return", "module", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_modules": [[592, 601], ["module.named_children", "helpers.named_modules"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_modules"], ["", "def", "named_modules", "(", "module", ":", "nn", ".", "Module", ",", "name", "=", "''", ",", "depth_first", "=", "True", ",", "include_root", "=", "False", ")", ":", "\n", "    ", "if", "not", "depth_first", "and", "include_root", ":", "\n", "        ", "yield", "name", ",", "module", "\n", "", "for", "child_name", ",", "child_module", "in", "module", ".", "named_children", "(", ")", ":", "\n", "        ", "child_name", "=", "'.'", ".", "join", "(", "(", "name", ",", "child_name", ")", ")", "if", "name", "else", "child_name", "\n", "yield", "from", "named_modules", "(", "\n", "module", "=", "child_module", ",", "name", "=", "child_name", ",", "depth_first", "=", "depth_first", ",", "include_root", "=", "True", ")", "\n", "", "if", "depth_first", "and", "include_root", ":", "\n", "        ", "yield", "name", ",", "module", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_modules_with_params": [[603, 612], ["module.named_children", "helpers.named_modules_with_params"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_modules_with_params"], ["", "", "def", "named_modules_with_params", "(", "module", ":", "nn", ".", "Module", ",", "name", "=", "''", ",", "depth_first", "=", "True", ",", "include_root", "=", "False", ")", ":", "\n", "    ", "if", "module", ".", "_parameters", "and", "not", "depth_first", "and", "include_root", ":", "\n", "        ", "yield", "name", ",", "module", "\n", "", "for", "child_name", ",", "child_module", "in", "module", ".", "named_children", "(", ")", ":", "\n", "        ", "child_name", "=", "'.'", ".", "join", "(", "(", "name", ",", "child_name", ")", ")", "if", "name", "else", "child_name", "\n", "yield", "from", "named_modules_with_params", "(", "\n", "module", "=", "child_module", ",", "name", "=", "child_name", ",", "depth_first", "=", "depth_first", ",", "include_root", "=", "True", ")", "\n", "", "if", "module", ".", "_parameters", "and", "depth_first", "and", "include_root", ":", "\n", "        ", "yield", "name", ",", "module", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.group_with_matcher": [[617, 676], ["isinstance", "collections.defaultdict", "collections.defaultdict", "sorted", "enumerate", "isinstance", "grouping[].append", "filter", "layer_id_to_param[].extend", "collections.defaultdict.items", "group_matcher.items", "isinstance", "group_matcher", "tuple", "collections.defaultdict.keys", "match_fn.match", "float", "isinstance", "tuple", "re.compile", "match_fn.match.groups", "map", "helpers.group_with_matcher._get_grouping"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.group_matcher"], ["def", "group_with_matcher", "(", "\n", "named_objects", ",", "\n", "group_matcher", ":", "Union", "[", "Dict", ",", "Callable", "]", ",", "\n", "output_values", ":", "bool", "=", "False", ",", "\n", "reverse", ":", "bool", "=", "False", "\n", ")", ":", "\n", "    ", "if", "isinstance", "(", "group_matcher", ",", "dict", ")", ":", "\n", "# dictionary matcher contains a dict of raw-string regex expr that must be compiled", "\n", "        ", "compiled", "=", "[", "]", "\n", "for", "group_ordinal", ",", "(", "group_name", ",", "mspec", ")", "in", "enumerate", "(", "group_matcher", ".", "items", "(", ")", ")", ":", "\n", "            ", "if", "mspec", "is", "None", ":", "\n", "                ", "continue", "\n", "# map all matching specifications into 3-tuple (compiled re, prefix, suffix)", "\n", "", "if", "isinstance", "(", "mspec", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "# multi-entry match specifications require each sub-spec to be a 2-tuple (re, suffix)", "\n", "                ", "for", "sspec", "in", "mspec", ":", "\n", "                    ", "compiled", "+=", "[", "(", "re", ".", "compile", "(", "sspec", "[", "0", "]", ")", ",", "(", "group_ordinal", ",", ")", ",", "sspec", "[", "1", "]", ")", "]", "\n", "", "", "else", ":", "\n", "                ", "compiled", "+=", "[", "(", "re", ".", "compile", "(", "mspec", ")", ",", "(", "group_ordinal", ",", ")", ",", "None", ")", "]", "\n", "", "", "group_matcher", "=", "compiled", "\n", "\n", "", "def", "_get_grouping", "(", "name", ")", ":", "\n", "        ", "if", "isinstance", "(", "group_matcher", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "for", "match_fn", ",", "prefix", ",", "suffix", "in", "group_matcher", ":", "\n", "                ", "r", "=", "match_fn", ".", "match", "(", "name", ")", "\n", "if", "r", ":", "\n", "                    ", "parts", "=", "(", "prefix", ",", "r", ".", "groups", "(", ")", ",", "suffix", ")", "\n", "# map all tuple elem to int for numeric sort, filter out None entries", "\n", "return", "tuple", "(", "map", "(", "float", ",", "chain", ".", "from_iterable", "(", "filter", "(", "None", ",", "parts", ")", ")", ")", ")", "\n", "", "", "return", "float", "(", "'inf'", ")", ",", "# un-matched layers (neck, head) mapped to largest ordinal", "\n", "", "else", ":", "\n", "            ", "ord", "=", "group_matcher", "(", "name", ")", "\n", "if", "not", "isinstance", "(", "ord", ",", "collections", ".", "abc", ".", "Iterable", ")", ":", "\n", "                ", "return", "ord", ",", "\n", "", "return", "tuple", "(", "ord", ")", "\n", "\n", "# map layers into groups via ordinals (ints or tuples of ints) from matcher", "\n", "", "", "grouping", "=", "defaultdict", "(", "list", ")", "\n", "for", "k", ",", "v", "in", "named_objects", ":", "\n", "        ", "grouping", "[", "_get_grouping", "(", "k", ")", "]", ".", "append", "(", "v", "if", "output_values", "else", "k", ")", "\n", "\n", "# remap to integers", "\n", "", "layer_id_to_param", "=", "defaultdict", "(", "list", ")", "\n", "lid", "=", "-", "1", "\n", "for", "k", "in", "sorted", "(", "filter", "(", "lambda", "x", ":", "x", "is", "not", "None", ",", "grouping", ".", "keys", "(", ")", ")", ")", ":", "\n", "        ", "if", "lid", "<", "0", "or", "k", "[", "-", "1", "]", "!=", "MATCH_PREV_GROUP", "[", "0", "]", ":", "\n", "            ", "lid", "+=", "1", "\n", "", "layer_id_to_param", "[", "lid", "]", ".", "extend", "(", "grouping", "[", "k", "]", ")", "\n", "\n", "", "if", "reverse", ":", "\n", "        ", "assert", "not", "output_values", ",", "\"reverse mapping only sensible for name output\"", "\n", "# output reverse mapping", "\n", "param_to_layer_id", "=", "{", "}", "\n", "for", "lid", ",", "lm", "in", "layer_id_to_param", ".", "items", "(", ")", ":", "\n", "            ", "for", "n", "in", "lm", ":", "\n", "                ", "param_to_layer_id", "[", "n", "]", "=", "lid", "\n", "", "", "return", "param_to_layer_id", "\n", "\n", "", "return", "layer_id_to_param", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.group_parameters": [[678, 686], ["helpers.group_with_matcher", "module.named_parameters"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.group_with_matcher"], ["", "def", "group_parameters", "(", "\n", "module", ":", "nn", ".", "Module", ",", "\n", "group_matcher", ",", "\n", "output_values", "=", "False", ",", "\n", "reverse", "=", "False", ",", "\n", ")", ":", "\n", "    ", "return", "group_with_matcher", "(", "\n", "module", ".", "named_parameters", "(", ")", ",", "group_matcher", ",", "output_values", "=", "output_values", ",", "reverse", "=", "reverse", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.group_modules": [[688, 696], ["helpers.group_with_matcher", "helpers.named_modules_with_params"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.group_with_matcher", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_modules_with_params"], ["", "def", "group_modules", "(", "\n", "module", ":", "nn", ".", "Module", ",", "\n", "group_matcher", ",", "\n", "output_values", "=", "False", ",", "\n", "reverse", "=", "False", ",", "\n", ")", ":", "\n", "    ", "return", "group_with_matcher", "(", "\n", "named_modules_with_params", "(", "module", ")", ",", "group_matcher", ",", "output_values", "=", "output_values", ",", "reverse", "=", "reverse", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq": [[698, 766], ["isinstance", "len", "range", "tuple.children", "itertools.chain.from_iterable", "isinstance", "tuple", "min", "torch.utils.checkpoint.checkpoint", "range", "helpers.checkpoint_seq.run_function"], "function", ["None"], ["", "def", "checkpoint_seq", "(", "\n", "functions", ",", "\n", "x", ",", "\n", "every", "=", "1", ",", "\n", "flatten", "=", "False", ",", "\n", "skip_last", "=", "False", ",", "\n", "preserve_rng_state", "=", "True", "\n", ")", ":", "\n", "    ", "r\"\"\"A helper function for checkpointing sequential models.\n\n    Sequential models execute a list of modules/functions in order\n    (sequentially). Therefore, we can divide such a sequence into segments\n    and checkpoint each segment. All segments except run in :func:`torch.no_grad`\n    manner, i.e., not storing the intermediate activations. The inputs of each\n    checkpointed segment will be saved for re-running the segment in the backward pass.\n\n    See :func:`~torch.utils.checkpoint.checkpoint` on how checkpointing works.\n\n    .. warning::\n        Checkpointing currently only supports :func:`torch.autograd.backward`\n        and only if its `inputs` argument is not passed. :func:`torch.autograd.grad`\n        is not supported.\n\n    .. warning:\n        At least one of the inputs needs to have :code:`requires_grad=True` if\n        grads are needed for model inputs, otherwise the checkpointed part of the\n        model won't have gradients.\n\n    Args:\n        functions: A :class:`torch.nn.Sequential` or the list of modules or functions to run sequentially.\n        x: A Tensor that is input to :attr:`functions`\n        every: checkpoint every-n functions (default: 1)\n        flatten (bool): flatten nn.Sequential of nn.Sequentials\n        skip_last (bool): skip checkpointing the last function in the sequence if True\n        preserve_rng_state (bool, optional, default=True):  Omit stashing and restoring\n            the RNG state during each checkpoint.\n\n    Returns:\n        Output of running :attr:`functions` sequentially on :attr:`*inputs`\n\n    Example:\n        >>> model = nn.Sequential(...)\n        >>> input_var = checkpoint_seq(model, input_var, every=2)\n    \"\"\"", "\n", "def", "run_function", "(", "start", ",", "end", ",", "functions", ")", ":", "\n", "        ", "def", "forward", "(", "_x", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "start", ",", "end", "+", "1", ")", ":", "\n", "                ", "_x", "=", "functions", "[", "j", "]", "(", "_x", ")", "\n", "", "return", "_x", "\n", "", "return", "forward", "\n", "\n", "", "if", "isinstance", "(", "functions", ",", "torch", ".", "nn", ".", "Sequential", ")", ":", "\n", "        ", "functions", "=", "functions", ".", "children", "(", ")", "\n", "", "if", "flatten", ":", "\n", "        ", "functions", "=", "chain", ".", "from_iterable", "(", "functions", ")", "\n", "", "if", "not", "isinstance", "(", "functions", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "functions", "=", "tuple", "(", "functions", ")", "\n", "\n", "", "num_checkpointed", "=", "len", "(", "functions", ")", "\n", "if", "skip_last", ":", "\n", "        ", "num_checkpointed", "-=", "1", "\n", "", "end", "=", "-", "1", "\n", "for", "start", "in", "range", "(", "0", ",", "num_checkpointed", ",", "every", ")", ":", "\n", "        ", "end", "=", "min", "(", "start", "+", "every", "-", "1", ",", "num_checkpointed", "-", "1", ")", "\n", "x", "=", "checkpoint", "(", "run_function", "(", "start", ",", "end", ",", "functions", ")", ",", "x", ",", "preserve_rng_state", "=", "preserve_rng_state", ")", "\n", "", "if", "skip_last", ":", "\n", "        ", "return", "run_function", "(", "end", "+", "1", ",", "len", "(", "functions", ")", "-", "1", ",", "functions", ")", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.flatten_modules": [[768, 791], ["isinstance", "isinstance", "isinstance", "helpers.flatten_modules", "module.named_children"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.flatten_modules"], ["", "def", "flatten_modules", "(", "named_modules", ",", "depth", "=", "1", ",", "prefix", "=", "''", ",", "module_types", "=", "'sequential'", ")", ":", "\n", "    ", "prefix_is_tuple", "=", "isinstance", "(", "prefix", ",", "tuple", ")", "\n", "if", "isinstance", "(", "module_types", ",", "str", ")", ":", "\n", "        ", "if", "module_types", "==", "'container'", ":", "\n", "            ", "module_types", "=", "(", "nn", ".", "Sequential", ",", "nn", ".", "ModuleList", ",", "nn", ".", "ModuleDict", ")", "\n", "", "else", ":", "\n", "            ", "module_types", "=", "(", "nn", ".", "Sequential", ",", ")", "\n", "", "", "for", "name", ",", "module", "in", "named_modules", ":", "\n", "        ", "if", "depth", "and", "isinstance", "(", "module", ",", "module_types", ")", ":", "\n", "            ", "yield", "from", "flatten_modules", "(", "\n", "module", ".", "named_children", "(", ")", ",", "\n", "depth", "-", "1", ",", "\n", "prefix", "=", "(", "name", ",", ")", "if", "prefix_is_tuple", "else", "name", ",", "\n", "module_types", "=", "module_types", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "if", "prefix_is_tuple", ":", "\n", "                ", "name", "=", "prefix", "+", "(", "name", ",", ")", "\n", "yield", "name", ",", "module", "\n", "", "else", ":", "\n", "                ", "if", "prefix", ":", "\n", "                    ", "name", "=", "'.'", ".", "join", "(", "[", "prefix", ",", "name", "]", ")", "\n", "", "yield", "name", ",", "module", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._cfg": [[14, 22], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bicubic'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'conv1'", ",", "'classifier'", ":", "'fc'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet": [[60, 62], ["helpers.build_model_with_cfg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["def", "_create_resnet", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "ResNet", ",", "variant", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_resnet18_v1b": [[64, 70], ["dict", "gluon_resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_resnet18_v1b", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-18 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "BasicBlock", ",", "layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_resnet18_v1b'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_resnet34_v1b": [[72, 78], ["dict", "gluon_resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_resnet34_v1b", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-34 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "BasicBlock", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_resnet34_v1b'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_resnet50_v1b": [[80, 86], ["dict", "gluon_resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_resnet50_v1b", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-50 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_resnet50_v1b'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_resnet101_v1b": [[88, 94], ["dict", "gluon_resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_resnet101_v1b", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-101 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_resnet101_v1b'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_resnet152_v1b": [[96, 102], ["dict", "gluon_resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_resnet152_v1b", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-152 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_resnet152_v1b'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_resnet50_v1c": [[104, 110], ["dict", "gluon_resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_resnet50_v1c", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-50 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_resnet50_v1c'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_resnet101_v1c": [[112, 118], ["dict", "gluon_resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_resnet101_v1c", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-101 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_resnet101_v1c'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_resnet152_v1c": [[120, 126], ["dict", "gluon_resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_resnet152_v1c", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-152 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_resnet152_v1c'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_resnet50_v1d": [[128, 135], ["dict", "gluon_resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_resnet50_v1d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-50 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_resnet50_v1d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_resnet101_v1d": [[137, 144], ["dict", "gluon_resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_resnet101_v1d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-101 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_resnet101_v1d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_resnet152_v1d": [[146, 153], ["dict", "gluon_resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_resnet152_v1d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-152 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_resnet152_v1d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_resnet50_v1s": [[155, 162], ["dict", "gluon_resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_resnet50_v1s", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-50 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "stem_width", "=", "64", ",", "stem_type", "=", "'deep'", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_resnet50_v1s'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_resnet101_v1s": [[165, 172], ["dict", "gluon_resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_resnet101_v1s", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-101 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "stem_width", "=", "64", ",", "stem_type", "=", "'deep'", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_resnet101_v1s'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_resnet152_v1s": [[174, 181], ["dict", "gluon_resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_resnet152_v1s", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNet-152 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "stem_width", "=", "64", ",", "stem_type", "=", "'deep'", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_resnet152_v1s'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_resnext50_32x4d": [[184, 190], ["dict", "gluon_resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_resnext50_32x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNeXt50-32x4d model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_resnext50_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_resnext101_32x4d": [[192, 198], ["dict", "gluon_resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_resnext101_32x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNeXt-101 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_resnext101_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_resnext101_64x4d": [[200, 206], ["dict", "gluon_resnet._create_resnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_resnext101_64x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a ResNeXt-101 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "64", ",", "base_width", "=", "4", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_resnext101_64x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_seresnext50_32x4d": [[208, 216], ["dict", "gluon_resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_seresnext50_32x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a SEResNeXt50-32x4d model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "\n", "block_args", "=", "dict", "(", "attn_layer", "=", "SEModule", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_seresnext50_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_seresnext101_32x4d": [[218, 226], ["dict", "gluon_resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_seresnext101_32x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a SEResNeXt-101-32x4d model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "\n", "block_args", "=", "dict", "(", "attn_layer", "=", "SEModule", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_seresnext101_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_seresnext101_64x4d": [[228, 236], ["dict", "gluon_resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_seresnext101_64x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a SEResNeXt-101-64x4d model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "cardinality", "=", "64", ",", "base_width", "=", "4", ",", "\n", "block_args", "=", "dict", "(", "attn_layer", "=", "SEModule", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_seresnext101_64x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet.gluon_senet154": [[238, 246], ["dict", "gluon_resnet._create_resnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.gluon_resnet._create_resnet"], ["", "@", "register_model", "\n", "def", "gluon_senet154", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs an SENet-154 model.\n    \"\"\"", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "Bottleneck", ",", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "cardinality", "=", "64", ",", "base_width", "=", "4", ",", "stem_type", "=", "'deep'", ",", "\n", "down_kernel_size", "=", "3", ",", "block_reduce_first", "=", "2", ",", "block_args", "=", "dict", "(", "attn_layer", "=", "SEModule", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_resnet", "(", "'gluon_senet154'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.PatchEmbed.__init__": [[92, 119], ["torch.Module.__init__", "layers.to_2tuple", "layers.to_2tuple", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "embed_dim", "=", "768", ",", "multi_conv", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "img_size", "=", "to_2tuple", "(", "img_size", ")", "\n", "patch_size", "=", "to_2tuple", "(", "patch_size", ")", "\n", "num_patches", "=", "(", "img_size", "[", "1", "]", "//", "patch_size", "[", "1", "]", ")", "*", "(", "img_size", "[", "0", "]", "//", "patch_size", "[", "0", "]", ")", "\n", "self", ".", "img_size", "=", "img_size", "\n", "self", ".", "patch_size", "=", "patch_size", "\n", "self", ".", "num_patches", "=", "num_patches", "\n", "if", "multi_conv", ":", "\n", "            ", "if", "patch_size", "[", "0", "]", "==", "12", ":", "\n", "                ", "self", ".", "proj", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "in_chans", ",", "embed_dim", "//", "4", ",", "kernel_size", "=", "7", ",", "stride", "=", "4", ",", "padding", "=", "3", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv2d", "(", "embed_dim", "//", "4", ",", "embed_dim", "//", "2", ",", "kernel_size", "=", "3", ",", "stride", "=", "3", ",", "padding", "=", "0", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv2d", "(", "embed_dim", "//", "2", ",", "embed_dim", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ",", "\n", ")", "\n", "", "elif", "patch_size", "[", "0", "]", "==", "16", ":", "\n", "                ", "self", ".", "proj", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "in_chans", ",", "embed_dim", "//", "4", ",", "kernel_size", "=", "7", ",", "stride", "=", "4", ",", "padding", "=", "3", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv2d", "(", "embed_dim", "//", "4", ",", "embed_dim", "//", "2", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv2d", "(", "embed_dim", "//", "2", ",", "embed_dim", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "proj", "=", "nn", ".", "Conv2d", "(", "in_chans", ",", "embed_dim", ",", "kernel_size", "=", "patch_size", ",", "stride", "=", "patch_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.PatchEmbed.forward": [[120, 129], ["layers._assert", "layers._assert", "crossvit.PatchEmbed.proj().flatten().transpose", "crossvit.PatchEmbed.proj().flatten", "crossvit.PatchEmbed.proj"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "# FIXME look at relaxing size constraints", "\n", "_assert", "(", "H", "==", "self", ".", "img_size", "[", "0", "]", ",", "\n", "f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"", ")", "\n", "_assert", "(", "W", "==", "self", ".", "img_size", "[", "1", "]", ",", "\n", "f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", ".", "flatten", "(", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.CrossAttention.__init__": [[132, 145], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ",", "num_heads", "=", "8", ",", "qkv_bias", "=", "False", ",", "qk_scale", "=", "None", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "head_dim", "=", "dim", "//", "num_heads", "\n", "# NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights", "\n", "self", ".", "scale", "=", "qk_scale", "or", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "wq", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "wk", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "wv", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.CrossAttention.forward": [[146, 163], ["crossvit.CrossAttention.wq().reshape().permute", "crossvit.CrossAttention.wk().reshape().permute", "crossvit.CrossAttention.wv().reshape().permute", "crossvit.CrossAttention.softmax", "crossvit.CrossAttention.attn_drop", "crossvit.CrossAttention.proj", "crossvit.CrossAttention.proj_drop", "crossvit.CrossAttention.wq().reshape", "crossvit.CrossAttention.wk().reshape", "crossvit.CrossAttention.wv().reshape", "crossvit.CrossAttention.transpose", "crossvit.CrossAttention.wq", "crossvit.CrossAttention.wk", "crossvit.CrossAttention.wv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "# B1C -> B1H(C/H) -> BH1(C/H)", "\n", "q", "=", "self", ".", "wq", "(", "x", "[", ":", ",", "0", ":", "1", ",", "...", "]", ")", ".", "reshape", "(", "B", ",", "1", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "# BNC -> BNH(C/H) -> BHN(C/H)", "\n", "k", "=", "self", ".", "wk", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "# BNC -> BNH(C/H) -> BHN(C/H)", "\n", "v", "=", "self", ".", "wv", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "scale", "# BH1(C/H) @ BH(C/H)N -> BH1N", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "1", ",", "C", ")", "# (BH1N @ BHN(C/H)) -> BH1(C/H) -> B1H(C/H) -> B1C", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.CrossAttentionBlock.__init__": [[167, 176], ["torch.Module.__init__", "norm_layer", "crossvit.CrossAttention", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "num_heads", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "qk_scale", "=", "None", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "\n", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "attn", "=", "CrossAttention", "(", "\n", "dim", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "qk_scale", "=", "qk_scale", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ")", "\n", "# NOTE: drop path for stochastic depth, we shall see if this is better than dropout here", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.CrossAttentionBlock.forward": [[177, 180], ["crossvit.CrossAttentionBlock.drop_path", "crossvit.CrossAttentionBlock.attn", "crossvit.CrossAttentionBlock.norm1"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", "[", ":", ",", "0", ":", "1", ",", "...", "]", "+", "self", ".", "drop_path", "(", "self", ".", "attn", "(", "self", ".", "norm1", "(", "x", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.MultiScaleBlock.__init__": [[184, 237], ["torch.Module.__init__", "len", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "range", "len", "crossvit.MultiScaleBlock.projs.append", "crossvit.MultiScaleBlock.revert_projs.append", "tmp.append", "len", "crossvit.MultiScaleBlock.blocks.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "crossvit.MultiScaleBlock.fusion.append", "range", "crossvit.MultiScaleBlock.fusion.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "vision_transformer.Block", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "norm_layer", "act_layer", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "crossvit.CrossAttentionBlock", "tmp.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "norm_layer", "act_layer", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "crossvit.CrossAttentionBlock"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ",", "patches", ",", "depth", ",", "num_heads", ",", "mlp_ratio", ",", "qkv_bias", "=", "False", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "\n", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "num_branches", "=", "len", "(", "dim", ")", "\n", "self", ".", "num_branches", "=", "num_branches", "\n", "# different branch could have different embedding size, the first one is the base", "\n", "self", ".", "blocks", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "d", "in", "range", "(", "num_branches", ")", ":", "\n", "            ", "tmp", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "depth", "[", "d", "]", ")", ":", "\n", "                ", "tmp", ".", "append", "(", "Block", "(", "\n", "dim", "=", "dim", "[", "d", "]", ",", "num_heads", "=", "num_heads", "[", "d", "]", ",", "mlp_ratio", "=", "mlp_ratio", "[", "d", "]", ",", "qkv_bias", "=", "qkv_bias", ",", "\n", "drop", "=", "drop", ",", "attn_drop", "=", "attn_drop", ",", "drop_path", "=", "drop_path", "[", "i", "]", ",", "norm_layer", "=", "norm_layer", ")", ")", "\n", "", "if", "len", "(", "tmp", ")", "!=", "0", ":", "\n", "                ", "self", ".", "blocks", ".", "append", "(", "nn", ".", "Sequential", "(", "*", "tmp", ")", ")", "\n", "\n", "", "", "if", "len", "(", "self", ".", "blocks", ")", "==", "0", ":", "\n", "            ", "self", ".", "blocks", "=", "None", "\n", "\n", "", "self", ".", "projs", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "d", "in", "range", "(", "num_branches", ")", ":", "\n", "            ", "if", "dim", "[", "d", "]", "==", "dim", "[", "(", "d", "+", "1", ")", "%", "num_branches", "]", "and", "False", ":", "\n", "                ", "tmp", "=", "[", "nn", ".", "Identity", "(", ")", "]", "\n", "", "else", ":", "\n", "                ", "tmp", "=", "[", "norm_layer", "(", "dim", "[", "d", "]", ")", ",", "act_layer", "(", ")", ",", "nn", ".", "Linear", "(", "dim", "[", "d", "]", ",", "dim", "[", "(", "d", "+", "1", ")", "%", "num_branches", "]", ")", "]", "\n", "", "self", ".", "projs", ".", "append", "(", "nn", ".", "Sequential", "(", "*", "tmp", ")", ")", "\n", "\n", "", "self", ".", "fusion", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "d", "in", "range", "(", "num_branches", ")", ":", "\n", "            ", "d_", "=", "(", "d", "+", "1", ")", "%", "num_branches", "\n", "nh", "=", "num_heads", "[", "d_", "]", "\n", "if", "depth", "[", "-", "1", "]", "==", "0", ":", "# backward capability:", "\n", "                ", "self", ".", "fusion", ".", "append", "(", "\n", "CrossAttentionBlock", "(", "\n", "dim", "=", "dim", "[", "d_", "]", ",", "num_heads", "=", "nh", ",", "mlp_ratio", "=", "mlp_ratio", "[", "d", "]", ",", "qkv_bias", "=", "qkv_bias", ",", "\n", "drop", "=", "drop", ",", "attn_drop", "=", "attn_drop", ",", "drop_path", "=", "drop_path", "[", "-", "1", "]", ",", "norm_layer", "=", "norm_layer", ")", ")", "\n", "", "else", ":", "\n", "                ", "tmp", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "depth", "[", "-", "1", "]", ")", ":", "\n", "                    ", "tmp", ".", "append", "(", "CrossAttentionBlock", "(", "\n", "dim", "=", "dim", "[", "d_", "]", ",", "num_heads", "=", "nh", ",", "mlp_ratio", "=", "mlp_ratio", "[", "d", "]", ",", "qkv_bias", "=", "qkv_bias", ",", "\n", "drop", "=", "drop", ",", "attn_drop", "=", "attn_drop", ",", "drop_path", "=", "drop_path", "[", "-", "1", "]", ",", "norm_layer", "=", "norm_layer", ")", ")", "\n", "", "self", ".", "fusion", ".", "append", "(", "nn", ".", "Sequential", "(", "*", "tmp", ")", ")", "\n", "\n", "", "", "self", ".", "revert_projs", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "d", "in", "range", "(", "num_branches", ")", ":", "\n", "            ", "if", "dim", "[", "(", "d", "+", "1", ")", "%", "num_branches", "]", "==", "dim", "[", "d", "]", "and", "False", ":", "\n", "                ", "tmp", "=", "[", "nn", ".", "Identity", "(", ")", "]", "\n", "", "else", ":", "\n", "                ", "tmp", "=", "[", "norm_layer", "(", "dim", "[", "(", "d", "+", "1", ")", "%", "num_branches", "]", ")", ",", "act_layer", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "dim", "[", "(", "d", "+", "1", ")", "%", "num_branches", "]", ",", "dim", "[", "d", "]", ")", "]", "\n", "", "self", ".", "revert_projs", ".", "append", "(", "nn", ".", "Sequential", "(", "*", "tmp", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.MultiScaleBlock.forward": [[238, 258], ["enumerate", "torch.jit.annotate", "torch.jit.annotate", "torch.jit.annotate", "torch.jit.annotate", "torch.jit.annotate", "torch.jit.annotate", "torch.jit.annotate", "torch.jit.annotate", "torch.jit.annotate", "torch.jit.annotate", "torch.jit.annotate", "torch.jit.annotate", "torch.jit.annotate", "torch.jit.annotate", "torch.jit.annotate", "torch.jit.annotate", "enumerate", "enumerate", "outs_b.append", "torch.jit.annotate.append", "torch.jit.annotate.append", "torch.jit.annotate.append", "torch.jit.annotate.append", "zip", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "fusion", "revert_proj", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "outs.append", "block", "proj"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ":", "List", "[", "torch", ".", "Tensor", "]", ")", "->", "List", "[", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "outs_b", "=", "[", "]", "\n", "for", "i", ",", "block", "in", "enumerate", "(", "self", ".", "blocks", ")", ":", "\n", "            ", "outs_b", ".", "append", "(", "block", "(", "x", "[", "i", "]", ")", ")", "\n", "\n", "# only take the cls token out", "\n", "", "proj_cls_token", "=", "torch", ".", "jit", ".", "annotate", "(", "List", "[", "torch", ".", "Tensor", "]", ",", "[", "]", ")", "\n", "for", "i", ",", "proj", "in", "enumerate", "(", "self", ".", "projs", ")", ":", "\n", "            ", "proj_cls_token", ".", "append", "(", "proj", "(", "outs_b", "[", "i", "]", "[", ":", ",", "0", ":", "1", ",", "...", "]", ")", ")", "\n", "\n", "# cross attention", "\n", "", "outs", "=", "[", "]", "\n", "for", "i", ",", "(", "fusion", ",", "revert_proj", ")", "in", "enumerate", "(", "zip", "(", "self", ".", "fusion", ",", "self", ".", "revert_projs", ")", ")", ":", "\n", "            ", "tmp", "=", "torch", ".", "cat", "(", "(", "proj_cls_token", "[", "i", "]", ",", "outs_b", "[", "(", "i", "+", "1", ")", "%", "self", ".", "num_branches", "]", "[", ":", ",", "1", ":", ",", "...", "]", ")", ",", "dim", "=", "1", ")", "\n", "tmp", "=", "fusion", "(", "tmp", ")", "\n", "reverted_proj_cls_token", "=", "revert_proj", "(", "tmp", "[", ":", ",", "0", ":", "1", ",", "...", "]", ")", "\n", "tmp", "=", "torch", ".", "cat", "(", "(", "reverted_proj_cls_token", ",", "outs_b", "[", "i", "]", "[", ":", ",", "1", ":", ",", "...", "]", ")", ",", "dim", "=", "1", ")", "\n", "outs", ".", "append", "(", "tmp", ")", "\n", "", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.CrossViT.__init__": [[289, 344], ["functools.partial", "torch.Module.__init__", "layers.to_2tuple", "layers.to_2tuple", "crossvit._compute_num_patches", "len", "sum", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "zip", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "sum", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "enumerate", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "crossvit.CrossViT.apply", "tuple", "setattr", "setattr", "crossvit.CrossViT.patch_embed.append", "x.item", "crossvit.MultiScaleBlock", "crossvit.CrossViT.blocks.append", "layers.trunc_normal_", "layers.trunc_normal_", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "crossvit.PatchEmbed", "sum", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "max", "norm_layer", "getattr", "getattr", "int", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "range", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "range"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit._compute_num_patches", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["def", "__init__", "(", "\n", "self", ",", "img_size", "=", "224", ",", "img_scale", "=", "(", "1.0", ",", "1.0", ")", ",", "patch_size", "=", "(", "8", ",", "16", ")", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "\n", "embed_dim", "=", "(", "192", ",", "384", ")", ",", "depth", "=", "(", "(", "1", ",", "3", ",", "1", ")", ",", "(", "1", ",", "3", ",", "1", ")", ",", "(", "1", ",", "3", ",", "1", ")", ")", ",", "num_heads", "=", "(", "6", ",", "12", ")", ",", "mlp_ratio", "=", "(", "2.", ",", "2.", ",", "4.", ")", ",", "\n", "multi_conv", "=", "False", ",", "crop_scale", "=", "False", ",", "qkv_bias", "=", "True", ",", "drop_rate", "=", "0.", ",", "attn_drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ",", "\n", "norm_layer", "=", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", ",", "global_pool", "=", "'token'", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "global_pool", "in", "(", "'token'", ",", "'avg'", ")", "\n", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "self", ".", "img_size", "=", "to_2tuple", "(", "img_size", ")", "\n", "img_scale", "=", "to_2tuple", "(", "img_scale", ")", "\n", "self", ".", "img_size_scaled", "=", "[", "tuple", "(", "[", "int", "(", "sj", "*", "si", ")", "for", "sj", "in", "self", ".", "img_size", "]", ")", "for", "si", "in", "img_scale", "]", "\n", "self", ".", "crop_scale", "=", "crop_scale", "# crop instead of interpolate for scale", "\n", "num_patches", "=", "_compute_num_patches", "(", "self", ".", "img_size_scaled", ",", "patch_size", ")", "\n", "self", ".", "num_branches", "=", "len", "(", "patch_size", ")", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "num_features", "=", "sum", "(", "embed_dim", ")", "\n", "self", ".", "patch_embed", "=", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "# hard-coded for torch jit script", "\n", "for", "i", "in", "range", "(", "self", ".", "num_branches", ")", ":", "\n", "            ", "setattr", "(", "self", ",", "f'pos_embed_{i}'", ",", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", "+", "num_patches", "[", "i", "]", ",", "embed_dim", "[", "i", "]", ")", ")", ")", "\n", "setattr", "(", "self", ",", "f'cls_token_{i}'", ",", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "embed_dim", "[", "i", "]", ")", ")", ")", "\n", "\n", "", "for", "im_s", ",", "p", ",", "d", "in", "zip", "(", "self", ".", "img_size_scaled", ",", "patch_size", ",", "embed_dim", ")", ":", "\n", "            ", "self", ".", "patch_embed", ".", "append", "(", "\n", "PatchEmbed", "(", "img_size", "=", "im_s", ",", "patch_size", "=", "p", ",", "in_chans", "=", "in_chans", ",", "embed_dim", "=", "d", ",", "multi_conv", "=", "multi_conv", ")", ")", "\n", "\n", "", "self", ".", "pos_drop", "=", "nn", ".", "Dropout", "(", "p", "=", "drop_rate", ")", "\n", "\n", "total_depth", "=", "sum", "(", "[", "sum", "(", "x", "[", "-", "2", ":", "]", ")", "for", "x", "in", "depth", "]", ")", "\n", "dpr", "=", "[", "x", ".", "item", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "total_depth", ")", "]", "# stochastic depth decay rule", "\n", "dpr_ptr", "=", "0", "\n", "self", ".", "blocks", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "idx", ",", "block_cfg", "in", "enumerate", "(", "depth", ")", ":", "\n", "            ", "curr_depth", "=", "max", "(", "block_cfg", "[", ":", "-", "1", "]", ")", "+", "block_cfg", "[", "-", "1", "]", "\n", "dpr_", "=", "dpr", "[", "dpr_ptr", ":", "dpr_ptr", "+", "curr_depth", "]", "\n", "blk", "=", "MultiScaleBlock", "(", "\n", "embed_dim", ",", "num_patches", ",", "block_cfg", ",", "num_heads", "=", "num_heads", ",", "mlp_ratio", "=", "mlp_ratio", ",", "\n", "qkv_bias", "=", "qkv_bias", ",", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "dpr_", ",", "norm_layer", "=", "norm_layer", ")", "\n", "dpr_ptr", "+=", "curr_depth", "\n", "self", ".", "blocks", ".", "append", "(", "blk", ")", "\n", "\n", "", "self", ".", "norm", "=", "nn", ".", "ModuleList", "(", "[", "norm_layer", "(", "embed_dim", "[", "i", "]", ")", "for", "i", "in", "range", "(", "self", ".", "num_branches", ")", "]", ")", "\n", "self", ".", "head", "=", "nn", ".", "ModuleList", "(", "[", "\n", "nn", ".", "Linear", "(", "embed_dim", "[", "i", "]", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "num_branches", ")", "]", ")", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "num_branches", ")", ":", "\n", "            ", "trunc_normal_", "(", "getattr", "(", "self", ",", "f'pos_embed_{i}'", ")", ",", "std", "=", ".02", ")", "\n", "trunc_normal_", "(", "getattr", "(", "self", ",", "f'cls_token_{i}'", ")", ",", "std", "=", ".02", ")", "\n", "\n", "", "self", ".", "apply", "(", "self", ".", "_init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.CrossViT._init_weights": [[345, 353], ["isinstance", "layers.trunc_normal_", "isinstance", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["", "def", "_init_weights", "(", "self", ",", "m", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "            ", "trunc_normal_", "(", "m", ".", "weight", ",", "std", "=", ".02", ")", "\n", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.CrossViT.no_weight_decay": [[354, 363], ["set", "range", "set.add", "getattr", "set.add"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEmaV2.set"], ["", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "no_weight_decay", "(", "self", ")", ":", "\n", "        ", "out", "=", "set", "(", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "num_branches", ")", ":", "\n", "            ", "out", ".", "add", "(", "f'cls_token_{i}'", ")", "\n", "pe", "=", "getattr", "(", "self", ",", "f'pos_embed_{i}'", ",", "None", ")", "\n", "if", "pe", "is", "not", "None", "and", "pe", ".", "requires_grad", ":", "\n", "                ", "out", ".", "add", "(", "f'pos_embed_{i}'", ")", "\n", "", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.CrossViT.group_matcher": [[364, 369], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^cls_token|pos_embed|patch_embed'", ",", "# stem and embed", "\n", "blocks", "=", "[", "(", "r'^blocks\\.(\\d+)'", ",", "None", ")", ",", "(", "r'^norm'", ",", "(", "99999", ",", ")", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.CrossViT.set_grad_checkpointing": [[371, 374], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "'gradient checkpointing not supported'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.CrossViT.get_classifier": [[375, 378], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.CrossViT.reset_classifier": [[379, 387], ["torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "range"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "assert", "global_pool", "in", "(", "'token'", ",", "'avg'", ")", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "", "self", ".", "head", "=", "nn", ".", "ModuleList", "(", "\n", "[", "nn", ".", "Linear", "(", "self", ".", "embed_dim", "[", "i", "]", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "for", "i", "in", "\n", "range", "(", "self", ".", "num_branches", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.CrossViT.forward_features": [[388, 410], ["enumerate", "enumerate", "crossvit.scale_image", "patch_embed", "cls_tokens.expand.expand.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "crossvit.CrossViT.pos_drop", "blk.append", "blk", "norm", "enumerate"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.scale_image"], ["", "def", "forward_features", "(", "self", ",", "x", ")", "->", "List", "[", "torch", ".", "Tensor", "]", ":", "\n", "        ", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "xs", "=", "[", "]", "\n", "for", "i", ",", "patch_embed", "in", "enumerate", "(", "self", ".", "patch_embed", ")", ":", "\n", "            ", "x_", "=", "x", "\n", "ss", "=", "self", ".", "img_size_scaled", "[", "i", "]", "\n", "x_", "=", "scale_image", "(", "x_", ",", "ss", ",", "self", ".", "crop_scale", ")", "\n", "x_", "=", "patch_embed", "(", "x_", ")", "\n", "cls_tokens", "=", "self", ".", "cls_token_0", "if", "i", "==", "0", "else", "self", ".", "cls_token_1", "# hard-coded for torch jit script", "\n", "cls_tokens", "=", "cls_tokens", ".", "expand", "(", "B", ",", "-", "1", ",", "-", "1", ")", "\n", "x_", "=", "torch", ".", "cat", "(", "(", "cls_tokens", ",", "x_", ")", ",", "dim", "=", "1", ")", "\n", "pos_embed", "=", "self", ".", "pos_embed_0", "if", "i", "==", "0", "else", "self", ".", "pos_embed_1", "# hard-coded for torch jit script", "\n", "x_", "=", "x_", "+", "pos_embed", "\n", "x_", "=", "self", ".", "pos_drop", "(", "x_", ")", "\n", "xs", ".", "append", "(", "x_", ")", "\n", "\n", "", "for", "i", ",", "blk", "in", "enumerate", "(", "self", ".", "blocks", ")", ":", "\n", "            ", "xs", "=", "blk", "(", "xs", ")", "\n", "\n", "# NOTE: was before branch token section, move to here to assure all branch token are before layer norm", "\n", "", "xs", "=", "[", "norm", "(", "xs", "[", "i", "]", ")", "for", "i", ",", "norm", "in", "enumerate", "(", "self", ".", "norm", ")", "]", "\n", "return", "xs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.CrossViT.forward_head": [[411, 416], ["torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "isinstance", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "x[].mean", "head", "enumerate"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "xs", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "pre_logits", ":", "bool", "=", "False", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "xs", "=", "[", "x", "[", ":", ",", "1", ":", "]", ".", "mean", "(", "dim", "=", "1", ")", "for", "x", "in", "xs", "]", "if", "self", ".", "global_pool", "==", "'avg'", "else", "[", "x", "[", ":", ",", "0", "]", "for", "x", "in", "xs", "]", "\n", "if", "pre_logits", "or", "isinstance", "(", "self", ".", "head", "[", "0", "]", ",", "nn", ".", "Identity", ")", ":", "\n", "            ", "return", "torch", ".", "cat", "(", "[", "x", "for", "x", "in", "xs", "]", ",", "dim", "=", "1", ")", "\n", "", "return", "torch", ".", "mean", "(", "torch", ".", "stack", "(", "[", "head", "(", "xs", "[", "i", "]", ")", "for", "i", ",", "head", "in", "enumerate", "(", "self", ".", "head", ")", "]", ",", "dim", "=", "0", ")", ",", "dim", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.CrossViT.forward": [[417, 421], ["crossvit.CrossViT.forward_features", "crossvit.CrossViT.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "xs", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "xs", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit._cfg": [[44, 52], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "240", ",", "240", ")", ",", "'pool_size'", ":", "None", ",", "'crop_pct'", ":", "0.875", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'first_conv'", ":", "(", "'patch_embed.0.proj'", ",", "'patch_embed.1.proj'", ")", ",", "\n", "'classifier'", ":", "(", "'head.0'", ",", "'head.1'", ")", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit._compute_num_patches": [[260, 262], ["zip"], "function", ["None"], ["", "", "def", "_compute_num_patches", "(", "img_size", ",", "patches", ")", ":", "\n", "    ", "return", "[", "i", "[", "0", "]", "//", "p", "*", "i", "[", "1", "]", "//", "p", "for", "i", ",", "p", "in", "zip", "(", "img_size", ",", "patches", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.scale_image": [[264, 283], ["torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "int", "int", "round", "round"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.interpolate", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.interpolate", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.interpolate", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.interpolate"], ["", "@", "register_notrace_function", "\n", "def", "scale_image", "(", "x", ",", "ss", ":", "Tuple", "[", "int", ",", "int", "]", ",", "crop_scale", ":", "bool", "=", "False", ")", ":", "# annotations for torchscript", "\n", "    ", "\"\"\"\n    Pulled out of CrossViT.forward_features to bury conditional logic in a leaf node for FX tracing.\n    Args:\n        x (Tensor): input image\n        ss (tuple[int, int]): height and width to scale to\n        crop_scale (bool): whether to crop instead of interpolate to achieve the desired scale. Defaults to False\n    Returns:\n        Tensor: the \"scaled\" image batch tensor\n    \"\"\"", "\n", "H", ",", "W", "=", "x", ".", "shape", "[", "-", "2", ":", "]", "\n", "if", "H", "!=", "ss", "[", "0", "]", "or", "W", "!=", "ss", "[", "1", "]", ":", "\n", "        ", "if", "crop_scale", "and", "ss", "[", "0", "]", "<=", "H", "and", "ss", "[", "1", "]", "<=", "W", ":", "\n", "            ", "cu", ",", "cl", "=", "int", "(", "round", "(", "(", "H", "-", "ss", "[", "0", "]", ")", "/", "2.", ")", ")", ",", "int", "(", "round", "(", "(", "W", "-", "ss", "[", "1", "]", ")", "/", "2.", ")", ")", "\n", "x", "=", "x", "[", ":", ",", ":", ",", "cu", ":", "cu", "+", "ss", "[", "0", "]", ",", "cl", ":", "cl", "+", "ss", "[", "1", "]", "]", "\n", "", "else", ":", "\n", "            ", "x", "=", "torch", ".", "nn", ".", "functional", ".", "interpolate", "(", "x", ",", "size", "=", "ss", ",", "mode", "=", "'bicubic'", ",", "align_corners", "=", "False", ")", "\n", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit._create_crossvit": [[423, 441], ["kwargs.get", "helpers.build_model_with_cfg", "RuntimeError", "state_dict.keys", "key.replace"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_crossvit", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "kwargs", ".", "get", "(", "'features_only'", ",", "None", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'features_only not implemented for Vision Transformer models.'", ")", "\n", "\n", "", "def", "pretrained_filter_fn", "(", "state_dict", ")", ":", "\n", "        ", "new_state_dict", "=", "{", "}", "\n", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "if", "'pos_embed'", "in", "key", "or", "'cls_token'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "\".\"", ",", "\"_\"", ")", "\n", "", "else", ":", "\n", "                ", "new_key", "=", "key", "\n", "", "new_state_dict", "[", "new_key", "]", "=", "state_dict", "[", "key", "]", "\n", "", "return", "new_state_dict", "\n", "\n", "", "return", "build_model_with_cfg", "(", "\n", "CrossViT", ",", "variant", ",", "pretrained", ",", "\n", "pretrained_filter_fn", "=", "pretrained_filter_fn", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.crossvit_tiny_240": [[443, 450], ["dict", "crossvit._create_crossvit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit._create_crossvit"], ["", "@", "register_model", "\n", "def", "crossvit_tiny_240", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "img_scale", "=", "(", "1.0", ",", "224", "/", "240", ")", ",", "patch_size", "=", "[", "12", ",", "16", "]", ",", "embed_dim", "=", "[", "96", ",", "192", "]", ",", "depth", "=", "[", "[", "1", ",", "4", ",", "0", "]", ",", "[", "1", ",", "4", ",", "0", "]", ",", "[", "1", ",", "4", ",", "0", "]", "]", ",", "\n", "num_heads", "=", "[", "3", ",", "3", "]", ",", "mlp_ratio", "=", "[", "4", ",", "4", ",", "1", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_crossvit", "(", "variant", "=", "'crossvit_tiny_240'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.crossvit_small_240": [[452, 459], ["dict", "crossvit._create_crossvit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit._create_crossvit"], ["", "@", "register_model", "\n", "def", "crossvit_small_240", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "img_scale", "=", "(", "1.0", ",", "224", "/", "240", ")", ",", "patch_size", "=", "[", "12", ",", "16", "]", ",", "embed_dim", "=", "[", "192", ",", "384", "]", ",", "depth", "=", "[", "[", "1", ",", "4", ",", "0", "]", ",", "[", "1", ",", "4", ",", "0", "]", ",", "[", "1", ",", "4", ",", "0", "]", "]", ",", "\n", "num_heads", "=", "[", "6", ",", "6", "]", ",", "mlp_ratio", "=", "[", "4", ",", "4", ",", "1", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_crossvit", "(", "variant", "=", "'crossvit_small_240'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.crossvit_base_240": [[461, 468], ["dict", "crossvit._create_crossvit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit._create_crossvit"], ["", "@", "register_model", "\n", "def", "crossvit_base_240", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "img_scale", "=", "(", "1.0", ",", "224", "/", "240", ")", ",", "patch_size", "=", "[", "12", ",", "16", "]", ",", "embed_dim", "=", "[", "384", ",", "768", "]", ",", "depth", "=", "[", "[", "1", ",", "4", ",", "0", "]", ",", "[", "1", ",", "4", ",", "0", "]", ",", "[", "1", ",", "4", ",", "0", "]", "]", ",", "\n", "num_heads", "=", "[", "12", ",", "12", "]", ",", "mlp_ratio", "=", "[", "4", ",", "4", ",", "1", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_crossvit", "(", "variant", "=", "'crossvit_base_240'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.crossvit_9_240": [[470, 477], ["dict", "crossvit._create_crossvit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit._create_crossvit"], ["", "@", "register_model", "\n", "def", "crossvit_9_240", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "img_scale", "=", "(", "1.0", ",", "224", "/", "240", ")", ",", "patch_size", "=", "[", "12", ",", "16", "]", ",", "embed_dim", "=", "[", "128", ",", "256", "]", ",", "depth", "=", "[", "[", "1", ",", "3", ",", "0", "]", ",", "[", "1", ",", "3", ",", "0", "]", ",", "[", "1", ",", "3", ",", "0", "]", "]", ",", "\n", "num_heads", "=", "[", "4", ",", "4", "]", ",", "mlp_ratio", "=", "[", "3", ",", "3", ",", "1", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_crossvit", "(", "variant", "=", "'crossvit_9_240'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.crossvit_15_240": [[479, 486], ["dict", "crossvit._create_crossvit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit._create_crossvit"], ["", "@", "register_model", "\n", "def", "crossvit_15_240", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "img_scale", "=", "(", "1.0", ",", "224", "/", "240", ")", ",", "patch_size", "=", "[", "12", ",", "16", "]", ",", "embed_dim", "=", "[", "192", ",", "384", "]", ",", "depth", "=", "[", "[", "1", ",", "5", ",", "0", "]", ",", "[", "1", ",", "5", ",", "0", "]", ",", "[", "1", ",", "5", ",", "0", "]", "]", ",", "\n", "num_heads", "=", "[", "6", ",", "6", "]", ",", "mlp_ratio", "=", "[", "3", ",", "3", ",", "1", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_crossvit", "(", "variant", "=", "'crossvit_15_240'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.crossvit_18_240": [[488, 495], ["dict", "crossvit._create_crossvit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit._create_crossvit"], ["", "@", "register_model", "\n", "def", "crossvit_18_240", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "img_scale", "=", "(", "1.0", ",", "224", "/", "240", ")", ",", "patch_size", "=", "[", "12", ",", "16", "]", ",", "embed_dim", "=", "[", "224", ",", "448", "]", ",", "depth", "=", "[", "[", "1", ",", "6", ",", "0", "]", ",", "[", "1", ",", "6", ",", "0", "]", ",", "[", "1", ",", "6", ",", "0", "]", "]", ",", "\n", "num_heads", "=", "[", "7", ",", "7", "]", ",", "mlp_ratio", "=", "[", "3", ",", "3", ",", "1", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_crossvit", "(", "variant", "=", "'crossvit_18_240'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.crossvit_9_dagger_240": [[497, 504], ["dict", "crossvit._create_crossvit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit._create_crossvit"], ["", "@", "register_model", "\n", "def", "crossvit_9_dagger_240", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "img_scale", "=", "(", "1.0", ",", "224", "/", "240", ")", ",", "patch_size", "=", "[", "12", ",", "16", "]", ",", "embed_dim", "=", "[", "128", ",", "256", "]", ",", "depth", "=", "[", "[", "1", ",", "3", ",", "0", "]", ",", "[", "1", ",", "3", ",", "0", "]", ",", "[", "1", ",", "3", ",", "0", "]", "]", ",", "\n", "num_heads", "=", "[", "4", ",", "4", "]", ",", "mlp_ratio", "=", "[", "3", ",", "3", ",", "1", "]", ",", "multi_conv", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_crossvit", "(", "variant", "=", "'crossvit_9_dagger_240'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.crossvit_15_dagger_240": [[506, 513], ["dict", "crossvit._create_crossvit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit._create_crossvit"], ["", "@", "register_model", "\n", "def", "crossvit_15_dagger_240", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "img_scale", "=", "(", "1.0", ",", "224", "/", "240", ")", ",", "patch_size", "=", "[", "12", ",", "16", "]", ",", "embed_dim", "=", "[", "192", ",", "384", "]", ",", "depth", "=", "[", "[", "1", ",", "5", ",", "0", "]", ",", "[", "1", ",", "5", ",", "0", "]", ",", "[", "1", ",", "5", ",", "0", "]", "]", ",", "\n", "num_heads", "=", "[", "6", ",", "6", "]", ",", "mlp_ratio", "=", "[", "3", ",", "3", ",", "1", "]", ",", "multi_conv", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_crossvit", "(", "variant", "=", "'crossvit_15_dagger_240'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.crossvit_15_dagger_408": [[515, 522], ["dict", "crossvit._create_crossvit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit._create_crossvit"], ["", "@", "register_model", "\n", "def", "crossvit_15_dagger_408", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "img_scale", "=", "(", "1.0", ",", "384", "/", "408", ")", ",", "patch_size", "=", "[", "12", ",", "16", "]", ",", "embed_dim", "=", "[", "192", ",", "384", "]", ",", "depth", "=", "[", "[", "1", ",", "5", ",", "0", "]", ",", "[", "1", ",", "5", ",", "0", "]", ",", "[", "1", ",", "5", ",", "0", "]", "]", ",", "\n", "num_heads", "=", "[", "6", ",", "6", "]", ",", "mlp_ratio", "=", "[", "3", ",", "3", ",", "1", "]", ",", "multi_conv", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_crossvit", "(", "variant", "=", "'crossvit_15_dagger_408'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.crossvit_18_dagger_240": [[524, 531], ["dict", "crossvit._create_crossvit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit._create_crossvit"], ["", "@", "register_model", "\n", "def", "crossvit_18_dagger_240", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "img_scale", "=", "(", "1.0", ",", "224", "/", "240", ")", ",", "patch_size", "=", "[", "12", ",", "16", "]", ",", "embed_dim", "=", "[", "224", ",", "448", "]", ",", "depth", "=", "[", "[", "1", ",", "6", ",", "0", "]", ",", "[", "1", ",", "6", ",", "0", "]", ",", "[", "1", ",", "6", ",", "0", "]", "]", ",", "\n", "num_heads", "=", "[", "7", ",", "7", "]", ",", "mlp_ratio", "=", "[", "3", ",", "3", ",", "1", "]", ",", "multi_conv", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_crossvit", "(", "variant", "=", "'crossvit_18_dagger_240'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit.crossvit_18_dagger_408": [[533, 540], ["dict", "crossvit._create_crossvit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.crossvit._create_crossvit"], ["", "@", "register_model", "\n", "def", "crossvit_18_dagger_408", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "\n", "img_scale", "=", "(", "1.0", ",", "384", "/", "408", ")", ",", "patch_size", "=", "[", "12", ",", "16", "]", ",", "embed_dim", "=", "[", "224", ",", "448", "]", ",", "depth", "=", "[", "[", "1", ",", "6", ",", "0", "]", ",", "[", "1", ",", "6", ",", "0", "]", ",", "[", "1", ",", "6", ",", "0", "]", "]", ",", "\n", "num_heads", "=", "[", "7", ",", "7", "]", ",", "mlp_ratio", "=", "[", "3", ",", "3", ",", "1", "]", ",", "multi_conv", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_crossvit", "(", "variant", "=", "'crossvit_18_dagger_408'", ",", "pretrained", "=", "pretrained", ",", "**", "model_args", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet.LinearBottleneck.__init__": [[57, 81], ["torch.Module.__init__", "layers.ConvNormAct", "layers.create_act_layer", "layers.ConvNormAct", "layers.make_divisible", "layers.ConvNormAct", "SEWithNorm", "round", "layers.make_divisible", "int"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.create_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "stride", ",", "exp_ratio", "=", "1.0", ",", "se_ratio", "=", "0.", ",", "ch_div", "=", "1", ",", "\n", "act_layer", "=", "'swish'", ",", "dw_act_layer", "=", "'relu6'", ",", "drop_path", "=", "None", ")", ":", "\n", "        ", "super", "(", "LinearBottleneck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "use_shortcut", "=", "stride", "==", "1", "and", "in_chs", "<=", "out_chs", "\n", "self", ".", "in_channels", "=", "in_chs", "\n", "self", ".", "out_channels", "=", "out_chs", "\n", "\n", "if", "exp_ratio", "!=", "1.", ":", "\n", "            ", "dw_chs", "=", "make_divisible", "(", "round", "(", "in_chs", "*", "exp_ratio", ")", ",", "divisor", "=", "ch_div", ")", "\n", "self", ".", "conv_exp", "=", "ConvNormAct", "(", "in_chs", ",", "dw_chs", ",", "act_layer", "=", "act_layer", ")", "\n", "", "else", ":", "\n", "            ", "dw_chs", "=", "in_chs", "\n", "self", ".", "conv_exp", "=", "None", "\n", "\n", "", "self", ".", "conv_dw", "=", "ConvNormAct", "(", "dw_chs", ",", "dw_chs", ",", "3", ",", "stride", "=", "stride", ",", "groups", "=", "dw_chs", ",", "apply_act", "=", "False", ")", "\n", "if", "se_ratio", ">", "0", ":", "\n", "            ", "self", ".", "se", "=", "SEWithNorm", "(", "dw_chs", ",", "rd_channels", "=", "make_divisible", "(", "int", "(", "dw_chs", "*", "se_ratio", ")", ",", "ch_div", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "se", "=", "None", "\n", "", "self", ".", "act_dw", "=", "create_act_layer", "(", "dw_act_layer", ")", "\n", "\n", "self", ".", "conv_pwl", "=", "ConvNormAct", "(", "dw_chs", ",", "out_chs", ",", "1", ",", "apply_act", "=", "False", ")", "\n", "self", ".", "drop_path", "=", "drop_path", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet.LinearBottleneck.feat_channels": [[82, 84], ["None"], "methods", ["None"], ["", "def", "feat_channels", "(", "self", ",", "exp", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "conv_dw", ".", "out_channels", "if", "exp", "else", "self", ".", "out_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet.LinearBottleneck.forward": [[85, 99], ["rexnet.LinearBottleneck.conv_dw", "rexnet.LinearBottleneck.act_dw", "rexnet.LinearBottleneck.conv_pwl", "rexnet.LinearBottleneck.conv_exp", "rexnet.LinearBottleneck.se", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "rexnet.LinearBottleneck.drop_path"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "if", "self", ".", "conv_exp", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "conv_exp", "(", "x", ")", "\n", "", "x", "=", "self", ".", "conv_dw", "(", "x", ")", "\n", "if", "self", ".", "se", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "se", "(", "x", ")", "\n", "", "x", "=", "self", ".", "act_dw", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_pwl", "(", "x", ")", "\n", "if", "self", ".", "use_shortcut", ":", "\n", "            ", "if", "self", ".", "drop_path", "is", "not", "None", ":", "\n", "                ", "x", "=", "self", ".", "drop_path", "(", "x", ")", "\n", "", "x", "=", "torch", ".", "cat", "(", "[", "x", "[", ":", ",", "0", ":", "self", ".", "in_channels", "]", "+", "shortcut", ",", "x", "[", ":", ",", "self", ".", "in_channels", ":", "]", "]", ",", "dim", "=", "1", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet.ReXNetV1.__init__": [[147, 171], ["torch.Module.__init__", "layers.make_divisible", "layers.ConvNormAct", "rexnet._block_cfg", "rexnet._build_blocks", "torch.Sequential", "torch.Sequential", "layers.ClassifierHead", "efficientnet_builder.efficientnet_init_weights", "round"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet._block_cfg", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet._build_blocks", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.efficientnet_init_weights"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "global_pool", "=", "'avg'", ",", "output_stride", "=", "32", ",", "\n", "initial_chs", "=", "16", ",", "final_chs", "=", "180", ",", "width_mult", "=", "1.0", ",", "depth_mult", "=", "1.0", ",", "se_ratio", "=", "1", "/", "12.", ",", "\n", "ch_div", "=", "1", ",", "act_layer", "=", "'swish'", ",", "dw_act_layer", "=", "'relu6'", ",", "drop_rate", "=", "0.2", ",", "drop_path_rate", "=", "0.", "\n", ")", ":", "\n", "        ", "super", "(", "ReXNetV1", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "assert", "output_stride", "==", "32", "# FIXME support dilation", "\n", "stem_base_chs", "=", "32", "/", "width_mult", "if", "width_mult", "<", "1.0", "else", "32", "\n", "stem_chs", "=", "make_divisible", "(", "round", "(", "stem_base_chs", "*", "width_mult", ")", ",", "divisor", "=", "ch_div", ")", "\n", "self", ".", "stem", "=", "ConvNormAct", "(", "in_chans", ",", "stem_chs", ",", "3", ",", "stride", "=", "2", ",", "act_layer", "=", "act_layer", ")", "\n", "\n", "block_cfg", "=", "_block_cfg", "(", "width_mult", ",", "depth_mult", ",", "initial_chs", ",", "final_chs", ",", "se_ratio", ",", "ch_div", ")", "\n", "features", ",", "self", ".", "feature_info", "=", "_build_blocks", "(", "\n", "block_cfg", ",", "stem_chs", ",", "width_mult", ",", "ch_div", ",", "act_layer", ",", "dw_act_layer", ",", "drop_path_rate", ")", "\n", "self", ".", "num_features", "=", "features", "[", "-", "1", "]", ".", "out_channels", "\n", "self", ".", "features", "=", "nn", ".", "Sequential", "(", "*", "features", ")", "\n", "\n", "self", ".", "head", "=", "ClassifierHead", "(", "self", ".", "num_features", ",", "num_classes", ",", "global_pool", ",", "drop_rate", ")", "\n", "\n", "efficientnet_init_weights", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet.ReXNetV1.group_matcher": [[172, 179], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "\n", "stem", "=", "r'^stem'", ",", "\n", "blocks", "=", "r'^features\\.(\\d+)'", ",", "\n", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet.ReXNetV1.set_grad_checkpointing": [[180, 183], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet.ReXNetV1.get_classifier": [[184, 187], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", ".", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet.ReXNetV1.reset_classifier": [[188, 190], ["layers.ClassifierHead"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "head", "=", "ClassifierHead", "(", "self", ".", "num_features", ",", "num_classes", ",", "pool_type", "=", "global_pool", ",", "drop_rate", "=", "self", ".", "drop_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet.ReXNetV1.forward_features": [[191, 198], ["rexnet.ReXNetV1.stem", "helpers.checkpoint_seq", "rexnet.ReXNetV1.features", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "stem", "(", "x", ")", "\n", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "features", ",", "x", ",", "flatten", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "features", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet.ReXNetV1.forward_head": [[199, 201], ["rexnet.ReXNetV1.head"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "head", "(", "x", ",", "pre_logits", "=", "pre_logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet.ReXNetV1.forward": [[202, 206], ["rexnet.ReXNetV1.forward_features", "rexnet.ReXNetV1.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet._cfg": [[25, 31], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bicubic'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'stem.conv'", ",", "'classifier'", ":", "'head.fc'", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet._block_cfg": [[101, 119], ["sum", "range", "list", "math.ceil", "sum", "out_chs_list.append", "zip", "sum", "layers.make_divisible", "sum", "enumerate", "round"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible"], ["", "", "def", "_block_cfg", "(", "width_mult", "=", "1.0", ",", "depth_mult", "=", "1.0", ",", "initial_chs", "=", "16", ",", "final_chs", "=", "180", ",", "se_ratio", "=", "0.", ",", "ch_div", "=", "1", ")", ":", "\n", "    ", "layers", "=", "[", "1", ",", "2", ",", "2", ",", "3", ",", "3", ",", "5", "]", "\n", "strides", "=", "[", "1", ",", "2", ",", "2", ",", "2", ",", "1", ",", "2", "]", "\n", "layers", "=", "[", "ceil", "(", "element", "*", "depth_mult", ")", "for", "element", "in", "layers", "]", "\n", "strides", "=", "sum", "(", "[", "[", "element", "]", "+", "[", "1", "]", "*", "(", "layers", "[", "idx", "]", "-", "1", ")", "for", "idx", ",", "element", "in", "enumerate", "(", "strides", ")", "]", ",", "[", "]", ")", "\n", "exp_ratios", "=", "[", "1", "]", "*", "layers", "[", "0", "]", "+", "[", "6", "]", "*", "sum", "(", "layers", "[", "1", ":", "]", ")", "\n", "depth", "=", "sum", "(", "layers", "[", ":", "]", ")", "*", "3", "\n", "base_chs", "=", "initial_chs", "/", "width_mult", "if", "width_mult", "<", "1.0", "else", "initial_chs", "\n", "\n", "# The following channel configuration is a simple instance to make each layer become an expand layer.", "\n", "out_chs_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "depth", "//", "3", ")", ":", "\n", "        ", "out_chs_list", ".", "append", "(", "make_divisible", "(", "round", "(", "base_chs", "*", "width_mult", ")", ",", "divisor", "=", "ch_div", ")", ")", "\n", "base_chs", "+=", "final_chs", "/", "(", "depth", "//", "3", "*", "1.0", ")", "\n", "\n", "", "se_ratios", "=", "[", "0.", "]", "*", "(", "layers", "[", "0", "]", "+", "layers", "[", "1", "]", ")", "+", "[", "se_ratio", "]", "*", "sum", "(", "layers", "[", "2", ":", "]", ")", "\n", "\n", "return", "list", "(", "zip", "(", "out_chs_list", ",", "exp_ratios", ",", "strides", ",", "se_ratios", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet._build_blocks": [[121, 144], ["len", "enumerate", "layers.make_divisible", "features.append", "features.append", "dict", "layers.ConvNormAct", "layers.DropPath", "rexnet.LinearBottleneck", "features[].feat_channels", "dict", "len"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet.LinearBottleneck.feat_channels"], ["", "def", "_build_blocks", "(", "\n", "block_cfg", ",", "prev_chs", ",", "width_mult", ",", "ch_div", "=", "1", ",", "act_layer", "=", "'swish'", ",", "dw_act_layer", "=", "'relu6'", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "    ", "feat_chs", "=", "[", "prev_chs", "]", "\n", "feature_info", "=", "[", "]", "\n", "curr_stride", "=", "2", "\n", "features", "=", "[", "]", "\n", "num_blocks", "=", "len", "(", "block_cfg", ")", "\n", "for", "block_idx", ",", "(", "chs", ",", "exp_ratio", ",", "stride", ",", "se_ratio", ")", "in", "enumerate", "(", "block_cfg", ")", ":", "\n", "        ", "if", "stride", ">", "1", ":", "\n", "            ", "fname", "=", "'stem'", "if", "block_idx", "==", "0", "else", "f'features.{block_idx - 1}'", "\n", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "feat_chs", "[", "-", "1", "]", ",", "reduction", "=", "curr_stride", ",", "module", "=", "fname", ")", "]", "\n", "curr_stride", "*=", "stride", "\n", "", "block_dpr", "=", "drop_path_rate", "*", "block_idx", "/", "(", "num_blocks", "-", "1", ")", "# stochastic depth linear decay rule", "\n", "drop_path", "=", "DropPath", "(", "block_dpr", ")", "if", "block_dpr", ">", "0.", "else", "None", "\n", "features", ".", "append", "(", "LinearBottleneck", "(", "\n", "in_chs", "=", "prev_chs", ",", "out_chs", "=", "chs", ",", "exp_ratio", "=", "exp_ratio", ",", "stride", "=", "stride", ",", "se_ratio", "=", "se_ratio", ",", "\n", "ch_div", "=", "ch_div", ",", "act_layer", "=", "act_layer", ",", "dw_act_layer", "=", "dw_act_layer", ",", "drop_path", "=", "drop_path", ")", ")", "\n", "prev_chs", "=", "chs", "\n", "feat_chs", "+=", "[", "features", "[", "-", "1", "]", ".", "feat_channels", "(", ")", "]", "\n", "", "pen_chs", "=", "make_divisible", "(", "1280", "*", "width_mult", ",", "divisor", "=", "ch_div", ")", "\n", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "feat_chs", "[", "-", "1", "]", ",", "reduction", "=", "curr_stride", ",", "module", "=", "f'features.{len(features) - 1}'", ")", "]", "\n", "features", ".", "append", "(", "ConvNormAct", "(", "prev_chs", ",", "pen_chs", ",", "act_layer", "=", "act_layer", ")", ")", "\n", "return", "features", ",", "feature_info", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet._create_rexnet": [[208, 214], ["dict", "helpers.build_model_with_cfg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_rexnet", "(", "variant", ",", "pretrained", ",", "**", "kwargs", ")", ":", "\n", "    ", "feature_cfg", "=", "dict", "(", "flatten_sequential", "=", "True", ")", "\n", "return", "build_model_with_cfg", "(", "\n", "ReXNetV1", ",", "variant", ",", "pretrained", ",", "\n", "feature_cfg", "=", "feature_cfg", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet.rexnet_100": [[216, 220], ["rexnet._create_rexnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet._create_rexnet"], ["", "@", "register_model", "\n", "def", "rexnet_100", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"ReXNet V1 1.0x\"\"\"", "\n", "return", "_create_rexnet", "(", "'rexnet_100'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet.rexnet_130": [[222, 226], ["rexnet._create_rexnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet._create_rexnet"], ["", "@", "register_model", "\n", "def", "rexnet_130", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"ReXNet V1 1.3x\"\"\"", "\n", "return", "_create_rexnet", "(", "'rexnet_130'", ",", "pretrained", ",", "width_mult", "=", "1.3", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet.rexnet_150": [[228, 232], ["rexnet._create_rexnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet._create_rexnet"], ["", "@", "register_model", "\n", "def", "rexnet_150", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"ReXNet V1 1.5x\"\"\"", "\n", "return", "_create_rexnet", "(", "'rexnet_150'", ",", "pretrained", ",", "width_mult", "=", "1.5", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet.rexnet_200": [[234, 238], ["rexnet._create_rexnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet._create_rexnet"], ["", "@", "register_model", "\n", "def", "rexnet_200", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"ReXNet V1 2.0x\"\"\"", "\n", "return", "_create_rexnet", "(", "'rexnet_200'", ",", "pretrained", ",", "width_mult", "=", "2.0", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet.rexnetr_100": [[240, 244], ["rexnet._create_rexnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet._create_rexnet"], ["", "@", "register_model", "\n", "def", "rexnetr_100", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"ReXNet V1 1.0x w/ rounded (mod 8) channels\"\"\"", "\n", "return", "_create_rexnet", "(", "'rexnetr_100'", ",", "pretrained", ",", "ch_div", "=", "8", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet.rexnetr_130": [[246, 250], ["rexnet._create_rexnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet._create_rexnet"], ["", "@", "register_model", "\n", "def", "rexnetr_130", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"ReXNet V1 1.3x w/ rounded (mod 8) channels\"\"\"", "\n", "return", "_create_rexnet", "(", "'rexnetr_130'", ",", "pretrained", ",", "width_mult", "=", "1.3", ",", "ch_div", "=", "8", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet.rexnetr_150": [[252, 256], ["rexnet._create_rexnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet._create_rexnet"], ["", "@", "register_model", "\n", "def", "rexnetr_150", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"ReXNet V1 1.5x w/ rounded (mod 8) channels\"\"\"", "\n", "return", "_create_rexnet", "(", "'rexnetr_150'", ",", "pretrained", ",", "width_mult", "=", "1.5", ",", "ch_div", "=", "8", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet.rexnetr_200": [[258, 262], ["rexnet._create_rexnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.rexnet._create_rexnet"], ["", "@", "register_model", "\n", "def", "rexnetr_200", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"ReXNet V1 2.0x w/ rounded (mod 8) channels\"\"\"", "\n", "return", "_create_rexnet", "(", "'rexnetr_200'", ",", "pretrained", ",", "width_mult", "=", "2.0", ",", "ch_div", "=", "8", ",", "**", "kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.PreActBottleneck.__init__": [[154, 178], ["torch.Module.__init__", "resnetv2.make_div", "norm_layer", "conv_layer", "norm_layer", "conv_layer", "norm_layer", "conv_layer", "functools.partial", "proj_layer", "layers.DropPath", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.make_div"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", "=", "None", ",", "bottle_ratio", "=", "0.25", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "first_dilation", "=", "None", ",", "groups", "=", "1", ",", "\n", "act_layer", "=", "None", ",", "conv_layer", "=", "None", ",", "norm_layer", "=", "None", ",", "proj_layer", "=", "None", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "first_dilation", "=", "first_dilation", "or", "dilation", "\n", "conv_layer", "=", "conv_layer", "or", "StdConv2d", "\n", "norm_layer", "=", "norm_layer", "or", "partial", "(", "GroupNormAct", ",", "num_groups", "=", "32", ")", "\n", "out_chs", "=", "out_chs", "or", "in_chs", "\n", "mid_chs", "=", "make_div", "(", "out_chs", "*", "bottle_ratio", ")", "\n", "\n", "if", "proj_layer", "is", "not", "None", ":", "\n", "            ", "self", ".", "downsample", "=", "proj_layer", "(", "\n", "in_chs", ",", "out_chs", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "first_dilation", "=", "first_dilation", ",", "preact", "=", "True", ",", "\n", "conv_layer", "=", "conv_layer", ",", "norm_layer", "=", "norm_layer", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "downsample", "=", "None", "\n", "\n", "", "self", ".", "norm1", "=", "norm_layer", "(", "in_chs", ")", "\n", "self", ".", "conv1", "=", "conv_layer", "(", "in_chs", ",", "mid_chs", ",", "1", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "mid_chs", ")", "\n", "self", ".", "conv2", "=", "conv_layer", "(", "mid_chs", ",", "mid_chs", ",", "3", ",", "stride", "=", "stride", ",", "dilation", "=", "first_dilation", ",", "groups", "=", "groups", ")", "\n", "self", ".", "norm3", "=", "norm_layer", "(", "mid_chs", ")", "\n", "self", ".", "conv3", "=", "conv_layer", "(", "mid_chs", ",", "out_chs", ",", "1", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path_rate", ")", "if", "drop_path_rate", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.PreActBottleneck.zero_init_last": [[179, 181], ["torch.init.zeros_", "torch.init.zeros_"], "methods", ["None"], ["", "def", "zero_init_last", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "conv3", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.PreActBottleneck.forward": [[182, 196], ["resnetv2.PreActBottleneck.norm1", "resnetv2.PreActBottleneck.conv1", "resnetv2.PreActBottleneck.conv2", "resnetv2.PreActBottleneck.conv3", "resnetv2.PreActBottleneck.drop_path", "resnetv2.PreActBottleneck.downsample", "resnetv2.PreActBottleneck.norm2", "resnetv2.PreActBottleneck.norm3"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_preact", "=", "self", ".", "norm1", "(", "x", ")", "\n", "\n", "# shortcut branch", "\n", "shortcut", "=", "x", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "shortcut", "=", "self", ".", "downsample", "(", "x_preact", ")", "\n", "\n", "# residual branch", "\n", "", "x", "=", "self", ".", "conv1", "(", "x_preact", ")", "\n", "x", "=", "self", ".", "conv2", "(", "self", ".", "norm2", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "conv3", "(", "self", ".", "norm3", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "drop_path", "(", "x", ")", "\n", "return", "x", "+", "shortcut", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.Bottleneck.__init__": [[201, 227], ["torch.Module.__init__", "resnetv2.make_div", "conv_layer", "norm_layer", "conv_layer", "norm_layer", "conv_layer", "norm_layer", "act_layer", "functools.partial", "proj_layer", "layers.DropPath", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.make_div"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", "=", "None", ",", "bottle_ratio", "=", "0.25", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "first_dilation", "=", "None", ",", "groups", "=", "1", ",", "\n", "act_layer", "=", "None", ",", "conv_layer", "=", "None", ",", "norm_layer", "=", "None", ",", "proj_layer", "=", "None", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "first_dilation", "=", "first_dilation", "or", "dilation", "\n", "act_layer", "=", "act_layer", "or", "nn", ".", "ReLU", "\n", "conv_layer", "=", "conv_layer", "or", "StdConv2d", "\n", "norm_layer", "=", "norm_layer", "or", "partial", "(", "GroupNormAct", ",", "num_groups", "=", "32", ")", "\n", "out_chs", "=", "out_chs", "or", "in_chs", "\n", "mid_chs", "=", "make_div", "(", "out_chs", "*", "bottle_ratio", ")", "\n", "\n", "if", "proj_layer", "is", "not", "None", ":", "\n", "            ", "self", ".", "downsample", "=", "proj_layer", "(", "\n", "in_chs", ",", "out_chs", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "preact", "=", "False", ",", "\n", "conv_layer", "=", "conv_layer", ",", "norm_layer", "=", "norm_layer", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "downsample", "=", "None", "\n", "\n", "", "self", ".", "conv1", "=", "conv_layer", "(", "in_chs", ",", "mid_chs", ",", "1", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "mid_chs", ")", "\n", "self", ".", "conv2", "=", "conv_layer", "(", "mid_chs", ",", "mid_chs", ",", "3", ",", "stride", "=", "stride", ",", "dilation", "=", "first_dilation", ",", "groups", "=", "groups", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "mid_chs", ")", "\n", "self", ".", "conv3", "=", "conv_layer", "(", "mid_chs", ",", "out_chs", ",", "1", ")", "\n", "self", ".", "norm3", "=", "norm_layer", "(", "out_chs", ",", "apply_act", "=", "False", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path_rate", ")", "if", "drop_path_rate", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "act3", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.Bottleneck.zero_init_last": [[228, 230], ["torch.init.zeros_", "torch.init.zeros_"], "methods", ["None"], ["", "def", "zero_init_last", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "norm3", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.Bottleneck.forward": [[231, 247], ["resnetv2.Bottleneck.conv1", "resnetv2.Bottleneck.norm1", "resnetv2.Bottleneck.conv2", "resnetv2.Bottleneck.norm2", "resnetv2.Bottleneck.conv3", "resnetv2.Bottleneck.norm3", "resnetv2.Bottleneck.drop_path", "resnetv2.Bottleneck.act3", "resnetv2.Bottleneck.downsample"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# shortcut branch", "\n", "        ", "shortcut", "=", "x", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "shortcut", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "# residual", "\n", "", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "norm1", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "x", "=", "self", ".", "norm2", "(", "x", ")", "\n", "x", "=", "self", ".", "conv3", "(", "x", ")", "\n", "x", "=", "self", ".", "norm3", "(", "x", ")", "\n", "x", "=", "self", ".", "drop_path", "(", "x", ")", "\n", "x", "=", "self", ".", "act3", "(", "x", "+", "shortcut", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.DownsampleConv.__init__": [[250, 256], ["torch.Module.__init__", "conv_layer", "torch.Identity", "torch.Identity", "norm_layer"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "first_dilation", "=", "None", ",", "preact", "=", "True", ",", "\n", "conv_layer", "=", "None", ",", "norm_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "DownsampleConv", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv", "=", "conv_layer", "(", "in_chs", ",", "out_chs", ",", "1", ",", "stride", "=", "stride", ")", "\n", "self", ".", "norm", "=", "nn", ".", "Identity", "(", ")", "if", "preact", "else", "norm_layer", "(", "out_chs", ",", "apply_act", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.DownsampleConv.forward": [[257, 259], ["resnetv2.DownsampleConv.norm", "resnetv2.DownsampleConv.conv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "norm", "(", "self", ".", "conv", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.DownsampleAvg.__init__": [[262, 275], ["torch.Module.__init__", "conv_layer", "avg_pool_fn", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "norm_layer"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "first_dilation", "=", "None", ",", "\n", "preact", "=", "True", ",", "conv_layer", "=", "None", ",", "norm_layer", "=", "None", ")", ":", "\n", "        ", "\"\"\" AvgPool Downsampling as in 'D' ResNet variants. This is not in RegNet space but I might experiment.\"\"\"", "\n", "super", "(", "DownsampleAvg", ",", "self", ")", ".", "__init__", "(", ")", "\n", "avg_stride", "=", "stride", "if", "dilation", "==", "1", "else", "1", "\n", "if", "stride", ">", "1", "or", "dilation", ">", "1", ":", "\n", "            ", "avg_pool_fn", "=", "AvgPool2dSame", "if", "avg_stride", "==", "1", "and", "dilation", ">", "1", "else", "nn", ".", "AvgPool2d", "\n", "self", ".", "pool", "=", "avg_pool_fn", "(", "2", ",", "avg_stride", ",", "ceil_mode", "=", "True", ",", "count_include_pad", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "pool", "=", "nn", ".", "Identity", "(", ")", "\n", "", "self", ".", "conv", "=", "conv_layer", "(", "in_chs", ",", "out_chs", ",", "1", ",", "stride", "=", "1", ")", "\n", "self", ".", "norm", "=", "nn", ".", "Identity", "(", ")", "if", "preact", "else", "norm_layer", "(", "out_chs", ",", "apply_act", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.DownsampleAvg.forward": [[276, 278], ["resnetv2.DownsampleAvg.norm", "resnetv2.DownsampleAvg.conv", "resnetv2.DownsampleAvg.pool"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "norm", "(", "self", ".", "conv", "(", "self", ".", "pool", "(", "x", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.ResNetStage.__init__": [[282, 302], ["torch.Module.__init__", "dict", "torch.Sequential", "torch.Sequential", "range", "resnetv2.ResNetStage.blocks.add_module", "str", "block_fn"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "stride", ",", "dilation", ",", "depth", ",", "bottle_ratio", "=", "0.25", ",", "groups", "=", "1", ",", "\n", "avg_down", "=", "False", ",", "block_dpr", "=", "None", ",", "block_fn", "=", "PreActBottleneck", ",", "\n", "act_layer", "=", "None", ",", "conv_layer", "=", "None", ",", "norm_layer", "=", "None", ",", "**", "block_kwargs", ")", ":", "\n", "        ", "super", "(", "ResNetStage", ",", "self", ")", ".", "__init__", "(", ")", "\n", "first_dilation", "=", "1", "if", "dilation", "in", "(", "1", ",", "2", ")", "else", "2", "\n", "layer_kwargs", "=", "dict", "(", "act_layer", "=", "act_layer", ",", "conv_layer", "=", "conv_layer", ",", "norm_layer", "=", "norm_layer", ")", "\n", "proj_layer", "=", "DownsampleAvg", "if", "avg_down", "else", "DownsampleConv", "\n", "prev_chs", "=", "in_chs", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", ")", "\n", "for", "block_idx", "in", "range", "(", "depth", ")", ":", "\n", "            ", "drop_path_rate", "=", "block_dpr", "[", "block_idx", "]", "if", "block_dpr", "else", "0.", "\n", "stride", "=", "stride", "if", "block_idx", "==", "0", "else", "1", "\n", "self", ".", "blocks", ".", "add_module", "(", "str", "(", "block_idx", ")", ",", "block_fn", "(", "\n", "prev_chs", ",", "out_chs", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "bottle_ratio", "=", "bottle_ratio", ",", "groups", "=", "groups", ",", "\n", "first_dilation", "=", "first_dilation", ",", "proj_layer", "=", "proj_layer", ",", "drop_path_rate", "=", "drop_path_rate", ",", "\n", "**", "layer_kwargs", ",", "**", "block_kwargs", ")", ")", "\n", "prev_chs", "=", "out_chs", "\n", "first_dilation", "=", "dilation", "\n", "proj_layer", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.ResNetStage.forward": [[303, 306], ["resnetv2.ResNetStage.blocks"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "blocks", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.ResNetV2.__init__": [[356, 401], ["functools.partial", "torch.Module.__init__", "resnetv2.make_div", "resnetv2.create_resnetv2_stem", "resnetv2.ResNetV2.feature_info.append", "torch.Sequential", "torch.Sequential", "enumerate", "layers.ClassifierHead", "resnetv2.ResNetV2.init_weights", "dict", "x.tolist", "zip", "resnetv2.make_div", "resnetv2.ResNetStage", "resnetv2.ResNetV2.stages.add_module", "norm_layer", "torch.Identity", "torch.Identity", "resnetv2.is_stem_deep", "torch.linspace().split", "torch.linspace().split", "torch.linspace().split", "torch.linspace().split", "dict", "str", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "sum"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.make_div", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.create_resnetv2_stem", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.make_div", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.is_stem_deep"], ["def", "__init__", "(", "\n", "self", ",", "layers", ",", "channels", "=", "(", "256", ",", "512", ",", "1024", ",", "2048", ")", ",", "\n", "num_classes", "=", "1000", ",", "in_chans", "=", "3", ",", "global_pool", "=", "'avg'", ",", "output_stride", "=", "32", ",", "\n", "width_factor", "=", "1", ",", "stem_chs", "=", "64", ",", "stem_type", "=", "''", ",", "avg_down", "=", "False", ",", "preact", "=", "True", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "conv_layer", "=", "StdConv2d", ",", "norm_layer", "=", "partial", "(", "GroupNormAct", ",", "num_groups", "=", "32", ")", ",", "\n", "drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ",", "zero_init_last", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "wf", "=", "width_factor", "\n", "\n", "self", ".", "feature_info", "=", "[", "]", "\n", "stem_chs", "=", "make_div", "(", "stem_chs", "*", "wf", ")", "\n", "self", ".", "stem", "=", "create_resnetv2_stem", "(", "\n", "in_chans", ",", "stem_chs", ",", "stem_type", ",", "preact", ",", "conv_layer", "=", "conv_layer", ",", "norm_layer", "=", "norm_layer", ")", "\n", "stem_feat", "=", "(", "'stem.conv3'", "if", "is_stem_deep", "(", "stem_type", ")", "else", "'stem.conv'", ")", "if", "preact", "else", "'stem.norm'", "\n", "self", ".", "feature_info", ".", "append", "(", "dict", "(", "num_chs", "=", "stem_chs", ",", "reduction", "=", "2", ",", "module", "=", "stem_feat", ")", ")", "\n", "\n", "prev_chs", "=", "stem_chs", "\n", "curr_stride", "=", "4", "\n", "dilation", "=", "1", "\n", "block_dprs", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "sum", "(", "layers", ")", ")", ".", "split", "(", "layers", ")", "]", "\n", "block_fn", "=", "PreActBottleneck", "if", "preact", "else", "Bottleneck", "\n", "self", ".", "stages", "=", "nn", ".", "Sequential", "(", ")", "\n", "for", "stage_idx", ",", "(", "d", ",", "c", ",", "bdpr", ")", "in", "enumerate", "(", "zip", "(", "layers", ",", "channels", ",", "block_dprs", ")", ")", ":", "\n", "            ", "out_chs", "=", "make_div", "(", "c", "*", "wf", ")", "\n", "stride", "=", "1", "if", "stage_idx", "==", "0", "else", "2", "\n", "if", "curr_stride", ">=", "output_stride", ":", "\n", "                ", "dilation", "*=", "stride", "\n", "stride", "=", "1", "\n", "", "stage", "=", "ResNetStage", "(", "\n", "prev_chs", ",", "out_chs", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "depth", "=", "d", ",", "avg_down", "=", "avg_down", ",", "\n", "act_layer", "=", "act_layer", ",", "conv_layer", "=", "conv_layer", ",", "norm_layer", "=", "norm_layer", ",", "block_dpr", "=", "bdpr", ",", "block_fn", "=", "block_fn", ")", "\n", "prev_chs", "=", "out_chs", "\n", "curr_stride", "*=", "stride", "\n", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "prev_chs", ",", "reduction", "=", "curr_stride", ",", "module", "=", "f'stages.{stage_idx}'", ")", "]", "\n", "self", ".", "stages", ".", "add_module", "(", "str", "(", "stage_idx", ")", ",", "stage", ")", "\n", "\n", "", "self", ".", "num_features", "=", "prev_chs", "\n", "self", ".", "norm", "=", "norm_layer", "(", "self", ".", "num_features", ")", "if", "preact", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "head", "=", "ClassifierHead", "(", "\n", "self", ".", "num_features", ",", "num_classes", ",", "pool_type", "=", "global_pool", ",", "drop_rate", "=", "self", ".", "drop_rate", ",", "use_conv", "=", "True", ")", "\n", "\n", "self", ".", "init_weights", "(", "zero_init_last", "=", "zero_init_last", ")", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.ResNetV2.init_weights": [[402, 405], ["helpers.named_apply", "functools.partial"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_apply"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "init_weights", "(", "self", ",", "zero_init_last", "=", "True", ")", ":", "\n", "        ", "named_apply", "(", "partial", "(", "_init_weights", ",", "zero_init_last", "=", "zero_init_last", ")", ",", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.ResNetV2.load_pretrained": [[406, 409], ["torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "resnetv2._load_weights"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._load_weights"], ["", "@", "torch", ".", "jit", ".", "ignore", "(", ")", "\n", "def", "load_pretrained", "(", "self", ",", "checkpoint_path", ",", "prefix", "=", "'resnet/'", ")", ":", "\n", "        ", "_load_weights", "(", "self", ",", "checkpoint_path", ",", "prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.ResNetV2.group_matcher": [[410, 420], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "\n", "stem", "=", "r'^stem'", ",", "\n", "blocks", "=", "r'^stages\\.(\\d+)'", "if", "coarse", "else", "[", "\n", "(", "r'^stages\\.(\\d+)\\.blocks\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^norm'", ",", "(", "99999", ",", ")", ")", "\n", "]", "\n", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.ResNetV2.set_grad_checkpointing": [[421, 424], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.ResNetV2.get_classifier": [[425, 428], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", ".", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.ResNetV2.reset_classifier": [[429, 433], ["layers.ClassifierHead"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "head", "=", "ClassifierHead", "(", "\n", "self", ".", "num_features", ",", "num_classes", ",", "pool_type", "=", "global_pool", ",", "drop_rate", "=", "self", ".", "drop_rate", ",", "use_conv", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.ResNetV2.forward_features": [[434, 442], ["resnetv2.ResNetV2.stem", "resnetv2.ResNetV2.norm", "helpers.checkpoint_seq", "resnetv2.ResNetV2.stages", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet.stages"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "stem", "(", "x", ")", "\n", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "stages", ",", "x", ",", "flatten", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "stages", "(", "x", ")", "\n", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.ResNetV2.forward_head": [[443, 445], ["resnetv2.ResNetV2.head"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "head", "(", "x", ",", "pre_logits", "=", "pre_logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.ResNetV2.forward": [[446, 450], ["resnetv2.ResNetV2.forward_features", "resnetv2.ResNetV2.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._cfg": [[45, 53], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bilinear'", ",", "\n", "'mean'", ":", "IMAGENET_INCEPTION_MEAN", ",", "'std'", ":", "IMAGENET_INCEPTION_STD", ",", "\n", "'first_conv'", ":", "'stem.conv'", ",", "'classifier'", ":", "'head.fc'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.make_div": [[137, 143], ["max", "int"], "function", ["None"], ["def", "make_div", "(", "v", ",", "divisor", "=", "8", ")", ":", "\n", "    ", "min_value", "=", "divisor", "\n", "new_v", "=", "max", "(", "min_value", ",", "int", "(", "v", "+", "divisor", "/", "2", ")", "//", "divisor", "*", "divisor", ")", "\n", "if", "new_v", "<", "0.9", "*", "v", ":", "\n", "        ", "new_v", "+=", "divisor", "\n", "", "return", "new_v", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.is_stem_deep": [[308, 310], ["any"], "function", ["None"], ["", "", "def", "is_stem_deep", "(", "stem_type", ")", ":", "\n", "    ", "return", "any", "(", "[", "s", "in", "stem_type", "for", "s", "in", "(", "'deep'", ",", "'tiered'", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.create_resnetv2_stem": [[312, 350], ["functools.partial", "collections.OrderedDict", "resnetv2.is_stem_deep", "torch.Sequential", "conv_layer", "norm_layer", "conv_layer", "norm_layer", "conv_layer", "conv_layer", "torch.ConstantPad2d", "torch.MaxPool2d", "norm_layer", "norm_layer", "layers.create_pool2d", "torch.MaxPool2d"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.is_stem_deep", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d"], ["", "def", "create_resnetv2_stem", "(", "\n", "in_chs", ",", "out_chs", "=", "64", ",", "stem_type", "=", "''", ",", "preact", "=", "True", ",", "\n", "conv_layer", "=", "StdConv2d", ",", "norm_layer", "=", "partial", "(", "GroupNormAct", ",", "num_groups", "=", "32", ")", ")", ":", "\n", "    ", "stem", "=", "OrderedDict", "(", ")", "\n", "assert", "stem_type", "in", "(", "''", ",", "'fixed'", ",", "'same'", ",", "'deep'", ",", "'deep_fixed'", ",", "'deep_same'", ",", "'tiered'", ")", "\n", "\n", "# NOTE conv padding mode can be changed by overriding the conv_layer def", "\n", "if", "is_stem_deep", "(", "stem_type", ")", ":", "\n", "# A 3 deep 3x3  conv stack as in ResNet V1D models", "\n", "        ", "if", "'tiered'", "in", "stem_type", ":", "\n", "            ", "stem_chs", "=", "(", "3", "*", "out_chs", "//", "8", ",", "out_chs", "//", "2", ")", "# 'T' resnets in resnet.py", "\n", "", "else", ":", "\n", "            ", "stem_chs", "=", "(", "out_chs", "//", "2", ",", "out_chs", "//", "2", ")", "# 'D' ResNets", "\n", "", "stem", "[", "'conv1'", "]", "=", "conv_layer", "(", "in_chs", ",", "stem_chs", "[", "0", "]", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", "stem", "[", "'norm1'", "]", "=", "norm_layer", "(", "stem_chs", "[", "0", "]", ")", "\n", "stem", "[", "'conv2'", "]", "=", "conv_layer", "(", "stem_chs", "[", "0", "]", ",", "stem_chs", "[", "1", "]", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", "\n", "stem", "[", "'norm2'", "]", "=", "norm_layer", "(", "stem_chs", "[", "1", "]", ")", "\n", "stem", "[", "'conv3'", "]", "=", "conv_layer", "(", "stem_chs", "[", "1", "]", ",", "out_chs", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", "\n", "if", "not", "preact", ":", "\n", "            ", "stem", "[", "'norm3'", "]", "=", "norm_layer", "(", "out_chs", ")", "\n", "", "", "else", ":", "\n", "# The usual 7x7 stem conv", "\n", "        ", "stem", "[", "'conv'", "]", "=", "conv_layer", "(", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "7", ",", "stride", "=", "2", ")", "\n", "if", "not", "preact", ":", "\n", "            ", "stem", "[", "'norm'", "]", "=", "norm_layer", "(", "out_chs", ")", "\n", "\n", "", "", "if", "'fixed'", "in", "stem_type", ":", "\n", "# 'fixed' SAME padding approximation that is used in BiT models", "\n", "        ", "stem", "[", "'pad'", "]", "=", "nn", ".", "ConstantPad2d", "(", "1", ",", "0.", ")", "\n", "stem", "[", "'pool'", "]", "=", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "0", ")", "\n", "", "elif", "'same'", "in", "stem_type", ":", "\n", "# full, input size based 'SAME' padding, used in ViT Hybrid model", "\n", "        ", "stem", "[", "'pool'", "]", "=", "create_pool2d", "(", "'max'", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "'same'", ")", "\n", "", "else", ":", "\n", "# the usual PyTorch symmetric padding", "\n", "        ", "stem", "[", "'pool'", "]", "=", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "stem", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._init_weights": [[452, 465], ["isinstance", "torch.init.normal_", "torch.init.zeros_", "isinstance", "isinstance", "torch.init.kaiming_normal_", "isinstance", "torch.init.zeros_", "torch.init.ones_", "torch.init.zeros_", "hasattr", "module.zero_init_last"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet.SelectiveKernelBottleneck.zero_init_last"], ["", "", "def", "_init_weights", "(", "module", ":", "nn", ".", "Module", ",", "name", ":", "str", "=", "''", ",", "zero_init_last", "=", "True", ")", ":", "\n", "    ", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "or", "(", "'head.fc'", "in", "name", "and", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", ")", ":", "\n", "        ", "nn", ".", "init", ".", "normal_", "(", "module", ".", "weight", ",", "mean", "=", "0.0", ",", "std", "=", "0.01", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", ":", "\n", "        ", "nn", ".", "init", ".", "kaiming_normal_", "(", "module", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "(", "nn", ".", "BatchNorm2d", ",", "nn", ".", "LayerNorm", ",", "nn", ".", "GroupNorm", ")", ")", ":", "\n", "        ", "nn", ".", "init", ".", "ones_", "(", "module", ".", "weight", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "elif", "zero_init_last", "and", "hasattr", "(", "module", ",", "'zero_init_last'", ")", ":", "\n", "        ", "module", ".", "zero_init_last", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._load_weights": [[467, 503], ["torch.no_grad", "torch.no_grad", "np.load", "helpers.adapt_input_conv", "model.stem.conv.weight.copy_", "model.norm.weight.copy_", "model.norm.bias.copy_", "enumerate", "torch.from_numpy", "torch.from_numpy", "resnetv2._load_weights.t2p"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.adapt_input_conv"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "_load_weights", "(", "model", ":", "nn", ".", "Module", ",", "checkpoint_path", ":", "str", ",", "prefix", ":", "str", "=", "'resnet/'", ")", ":", "\n", "    ", "import", "numpy", "as", "np", "\n", "\n", "def", "t2p", "(", "conv_weights", ")", ":", "\n", "        ", "\"\"\"Possibly convert HWIO to OIHW.\"\"\"", "\n", "if", "conv_weights", ".", "ndim", "==", "4", ":", "\n", "            ", "conv_weights", "=", "conv_weights", ".", "transpose", "(", "[", "3", ",", "2", ",", "0", ",", "1", "]", ")", "\n", "", "return", "torch", ".", "from_numpy", "(", "conv_weights", ")", "\n", "\n", "", "weights", "=", "np", ".", "load", "(", "checkpoint_path", ")", "\n", "stem_conv_w", "=", "adapt_input_conv", "(", "\n", "model", ".", "stem", ".", "conv", ".", "weight", ".", "shape", "[", "1", "]", ",", "t2p", "(", "weights", "[", "f'{prefix}root_block/standardized_conv2d/kernel'", "]", ")", ")", "\n", "model", ".", "stem", ".", "conv", ".", "weight", ".", "copy_", "(", "stem_conv_w", ")", "\n", "model", ".", "norm", ".", "weight", ".", "copy_", "(", "t2p", "(", "weights", "[", "f'{prefix}group_norm/gamma'", "]", ")", ")", "\n", "model", ".", "norm", ".", "bias", ".", "copy_", "(", "t2p", "(", "weights", "[", "f'{prefix}group_norm/beta'", "]", ")", ")", "\n", "if", "isinstance", "(", "getattr", "(", "model", ".", "head", ",", "'fc'", ",", "None", ")", ",", "nn", ".", "Conv2d", ")", "and", "model", ".", "head", ".", "fc", ".", "weight", ".", "shape", "[", "0", "]", "==", "weights", "[", "f'{prefix}head/conv2d/kernel'", "]", ".", "shape", "[", "-", "1", "]", ":", "\n", "        ", "model", ".", "head", ".", "fc", ".", "weight", ".", "copy_", "(", "t2p", "(", "weights", "[", "f'{prefix}head/conv2d/kernel'", "]", ")", ")", "\n", "model", ".", "head", ".", "fc", ".", "bias", ".", "copy_", "(", "t2p", "(", "weights", "[", "f'{prefix}head/conv2d/bias'", "]", ")", ")", "\n", "", "for", "i", ",", "(", "sname", ",", "stage", ")", "in", "enumerate", "(", "model", ".", "stages", ".", "named_children", "(", ")", ")", ":", "\n", "        ", "for", "j", ",", "(", "bname", ",", "block", ")", "in", "enumerate", "(", "stage", ".", "blocks", ".", "named_children", "(", ")", ")", ":", "\n", "            ", "cname", "=", "'standardized_conv2d'", "\n", "block_prefix", "=", "f'{prefix}block{i + 1}/unit{j + 1:02d}/'", "\n", "block", ".", "conv1", ".", "weight", ".", "copy_", "(", "t2p", "(", "weights", "[", "f'{block_prefix}a/{cname}/kernel'", "]", ")", ")", "\n", "block", ".", "conv2", ".", "weight", ".", "copy_", "(", "t2p", "(", "weights", "[", "f'{block_prefix}b/{cname}/kernel'", "]", ")", ")", "\n", "block", ".", "conv3", ".", "weight", ".", "copy_", "(", "t2p", "(", "weights", "[", "f'{block_prefix}c/{cname}/kernel'", "]", ")", ")", "\n", "block", ".", "norm1", ".", "weight", ".", "copy_", "(", "t2p", "(", "weights", "[", "f'{block_prefix}a/group_norm/gamma'", "]", ")", ")", "\n", "block", ".", "norm2", ".", "weight", ".", "copy_", "(", "t2p", "(", "weights", "[", "f'{block_prefix}b/group_norm/gamma'", "]", ")", ")", "\n", "block", ".", "norm3", ".", "weight", ".", "copy_", "(", "t2p", "(", "weights", "[", "f'{block_prefix}c/group_norm/gamma'", "]", ")", ")", "\n", "block", ".", "norm1", ".", "bias", ".", "copy_", "(", "t2p", "(", "weights", "[", "f'{block_prefix}a/group_norm/beta'", "]", ")", ")", "\n", "block", ".", "norm2", ".", "bias", ".", "copy_", "(", "t2p", "(", "weights", "[", "f'{block_prefix}b/group_norm/beta'", "]", ")", ")", "\n", "block", ".", "norm3", ".", "bias", ".", "copy_", "(", "t2p", "(", "weights", "[", "f'{block_prefix}c/group_norm/beta'", "]", ")", ")", "\n", "if", "block", ".", "downsample", "is", "not", "None", ":", "\n", "                ", "w", "=", "weights", "[", "f'{block_prefix}a/proj/{cname}/kernel'", "]", "\n", "block", ".", "downsample", ".", "conv", ".", "weight", ".", "copy_", "(", "t2p", "(", "w", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2": [[505, 512], ["dict", "helpers.build_model_with_cfg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "", "", "def", "_create_resnetv2", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "feature_cfg", "=", "dict", "(", "flatten_sequential", "=", "True", ")", "\n", "return", "build_model_with_cfg", "(", "\n", "ResNetV2", ",", "variant", ",", "pretrained", ",", "\n", "feature_cfg", "=", "feature_cfg", ",", "\n", "pretrained_custom_load", "=", "'_bit'", "in", "variant", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2_bit": [[514, 517], ["resnetv2._create_resnetv2", "functools.partial"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2"], ["", "def", "_create_resnetv2_bit", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2", "(", "\n", "variant", ",", "pretrained", "=", "pretrained", ",", "stem_type", "=", "'fixed'", ",", "conv_layer", "=", "partial", "(", "StdConv2d", ",", "eps", "=", "1e-8", ")", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_50x1_bitm": [[519, 523], ["resnetv2._create_resnetv2_bit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2_bit"], ["", "@", "register_model", "\n", "def", "resnetv2_50x1_bitm", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2_bit", "(", "\n", "'resnetv2_50x1_bitm'", ",", "pretrained", "=", "pretrained", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "width_factor", "=", "1", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_50x3_bitm": [[525, 529], ["resnetv2._create_resnetv2_bit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2_bit"], ["", "@", "register_model", "\n", "def", "resnetv2_50x3_bitm", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2_bit", "(", "\n", "'resnetv2_50x3_bitm'", ",", "pretrained", "=", "pretrained", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "width_factor", "=", "3", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_101x1_bitm": [[531, 535], ["resnetv2._create_resnetv2_bit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2_bit"], ["", "@", "register_model", "\n", "def", "resnetv2_101x1_bitm", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2_bit", "(", "\n", "'resnetv2_101x1_bitm'", ",", "pretrained", "=", "pretrained", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "width_factor", "=", "1", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_101x3_bitm": [[537, 541], ["resnetv2._create_resnetv2_bit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2_bit"], ["", "@", "register_model", "\n", "def", "resnetv2_101x3_bitm", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2_bit", "(", "\n", "'resnetv2_101x3_bitm'", ",", "pretrained", "=", "pretrained", ",", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "width_factor", "=", "3", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_152x2_bitm": [[543, 547], ["resnetv2._create_resnetv2_bit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2_bit"], ["", "@", "register_model", "\n", "def", "resnetv2_152x2_bitm", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2_bit", "(", "\n", "'resnetv2_152x2_bitm'", ",", "pretrained", "=", "pretrained", ",", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "width_factor", "=", "2", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_152x4_bitm": [[549, 553], ["resnetv2._create_resnetv2_bit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2_bit"], ["", "@", "register_model", "\n", "def", "resnetv2_152x4_bitm", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2_bit", "(", "\n", "'resnetv2_152x4_bitm'", ",", "pretrained", "=", "pretrained", ",", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "width_factor", "=", "4", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_50x1_bitm_in21k": [[555, 560], ["resnetv2._create_resnetv2_bit", "kwargs.pop"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2_bit"], ["", "@", "register_model", "\n", "def", "resnetv2_50x1_bitm_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2_bit", "(", "\n", "'resnetv2_50x1_bitm_in21k'", ",", "pretrained", "=", "pretrained", ",", "num_classes", "=", "kwargs", ".", "pop", "(", "'num_classes'", ",", "21843", ")", ",", "\n", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "width_factor", "=", "1", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_50x3_bitm_in21k": [[562, 567], ["resnetv2._create_resnetv2_bit", "kwargs.pop"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2_bit"], ["", "@", "register_model", "\n", "def", "resnetv2_50x3_bitm_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2_bit", "(", "\n", "'resnetv2_50x3_bitm_in21k'", ",", "pretrained", "=", "pretrained", ",", "num_classes", "=", "kwargs", ".", "pop", "(", "'num_classes'", ",", "21843", ")", ",", "\n", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "width_factor", "=", "3", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_101x1_bitm_in21k": [[569, 574], ["resnetv2._create_resnetv2", "kwargs.pop"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2"], ["", "@", "register_model", "\n", "def", "resnetv2_101x1_bitm_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2", "(", "\n", "'resnetv2_101x1_bitm_in21k'", ",", "pretrained", "=", "pretrained", ",", "num_classes", "=", "kwargs", ".", "pop", "(", "'num_classes'", ",", "21843", ")", ",", "\n", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "width_factor", "=", "1", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_101x3_bitm_in21k": [[576, 581], ["resnetv2._create_resnetv2_bit", "kwargs.pop"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2_bit"], ["", "@", "register_model", "\n", "def", "resnetv2_101x3_bitm_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2_bit", "(", "\n", "'resnetv2_101x3_bitm_in21k'", ",", "pretrained", "=", "pretrained", ",", "num_classes", "=", "kwargs", ".", "pop", "(", "'num_classes'", ",", "21843", ")", ",", "\n", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "width_factor", "=", "3", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_152x2_bitm_in21k": [[583, 588], ["resnetv2._create_resnetv2_bit", "kwargs.pop"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2_bit"], ["", "@", "register_model", "\n", "def", "resnetv2_152x2_bitm_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2_bit", "(", "\n", "'resnetv2_152x2_bitm_in21k'", ",", "pretrained", "=", "pretrained", ",", "num_classes", "=", "kwargs", ".", "pop", "(", "'num_classes'", ",", "21843", ")", ",", "\n", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "width_factor", "=", "2", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_152x4_bitm_in21k": [[590, 595], ["resnetv2._create_resnetv2_bit", "kwargs.pop"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2_bit"], ["", "@", "register_model", "\n", "def", "resnetv2_152x4_bitm_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2_bit", "(", "\n", "'resnetv2_152x4_bitm_in21k'", ",", "pretrained", "=", "pretrained", ",", "num_classes", "=", "kwargs", ".", "pop", "(", "'num_classes'", ",", "21843", ")", ",", "\n", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "width_factor", "=", "4", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_50x1_bit_distilled": [[597, 604], ["resnetv2._create_resnetv2_bit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2_bit"], ["", "@", "register_model", "\n", "def", "resnetv2_50x1_bit_distilled", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResNetV2-50x1-BiT Distilled\n    Paper: Knowledge distillation: A good teacher is patient and consistent - https://arxiv.org/abs/2106.05237\n    \"\"\"", "\n", "return", "_create_resnetv2_bit", "(", "\n", "'resnetv2_50x1_bit_distilled'", ",", "pretrained", "=", "pretrained", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "width_factor", "=", "1", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_152x2_bit_teacher": [[606, 613], ["resnetv2._create_resnetv2_bit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2_bit"], ["", "@", "register_model", "\n", "def", "resnetv2_152x2_bit_teacher", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResNetV2-152x2-BiT Teacher\n    Paper: Knowledge distillation: A good teacher is patient and consistent - https://arxiv.org/abs/2106.05237\n    \"\"\"", "\n", "return", "_create_resnetv2_bit", "(", "\n", "'resnetv2_152x2_bit_teacher'", ",", "pretrained", "=", "pretrained", ",", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "width_factor", "=", "2", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_152x2_bit_teacher_384": [[615, 622], ["resnetv2._create_resnetv2_bit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2_bit"], ["", "@", "register_model", "\n", "def", "resnetv2_152x2_bit_teacher_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ResNetV2-152xx-BiT Teacher @ 384x384\n    Paper: Knowledge distillation: A good teacher is patient and consistent - https://arxiv.org/abs/2106.05237\n    \"\"\"", "\n", "return", "_create_resnetv2_bit", "(", "\n", "'resnetv2_152x2_bit_teacher_384'", ",", "pretrained", "=", "pretrained", ",", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "width_factor", "=", "2", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_50": [[624, 629], ["resnetv2._create_resnetv2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2"], ["", "@", "register_model", "\n", "def", "resnetv2_50", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2", "(", "\n", "'resnetv2_50'", ",", "pretrained", "=", "pretrained", ",", "\n", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "conv_layer", "=", "create_conv2d", ",", "norm_layer", "=", "BatchNormAct2d", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_50d": [[631, 637], ["resnetv2._create_resnetv2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2"], ["", "@", "register_model", "\n", "def", "resnetv2_50d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2", "(", "\n", "'resnetv2_50d'", ",", "pretrained", "=", "pretrained", ",", "\n", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "conv_layer", "=", "create_conv2d", ",", "norm_layer", "=", "BatchNormAct2d", ",", "\n", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_50t": [[639, 645], ["resnetv2._create_resnetv2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2"], ["", "@", "register_model", "\n", "def", "resnetv2_50t", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2", "(", "\n", "'resnetv2_50t'", ",", "pretrained", "=", "pretrained", ",", "\n", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "conv_layer", "=", "create_conv2d", ",", "norm_layer", "=", "BatchNormAct2d", ",", "\n", "stem_type", "=", "'tiered'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_101": [[647, 652], ["resnetv2._create_resnetv2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2"], ["", "@", "register_model", "\n", "def", "resnetv2_101", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2", "(", "\n", "'resnetv2_101'", ",", "pretrained", "=", "pretrained", ",", "\n", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "conv_layer", "=", "create_conv2d", ",", "norm_layer", "=", "BatchNormAct2d", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_101d": [[654, 660], ["resnetv2._create_resnetv2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2"], ["", "@", "register_model", "\n", "def", "resnetv2_101d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2", "(", "\n", "'resnetv2_101d'", ",", "pretrained", "=", "pretrained", ",", "\n", "layers", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "conv_layer", "=", "create_conv2d", ",", "norm_layer", "=", "BatchNormAct2d", ",", "\n", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_152": [[662, 667], ["resnetv2._create_resnetv2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2"], ["", "@", "register_model", "\n", "def", "resnetv2_152", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2", "(", "\n", "'resnetv2_152'", ",", "pretrained", "=", "pretrained", ",", "\n", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "conv_layer", "=", "create_conv2d", ",", "norm_layer", "=", "BatchNormAct2d", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_152d": [[669, 675], ["resnetv2._create_resnetv2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2"], ["", "@", "register_model", "\n", "def", "resnetv2_152d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2", "(", "\n", "'resnetv2_152d'", ",", "pretrained", "=", "pretrained", ",", "\n", "layers", "=", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "conv_layer", "=", "create_conv2d", ",", "norm_layer", "=", "BatchNormAct2d", ",", "\n", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_50d_gn": [[679, 685], ["resnetv2._create_resnetv2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2"], ["", "@", "register_model", "\n", "def", "resnetv2_50d_gn", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2", "(", "\n", "'resnetv2_50d_gn'", ",", "pretrained", "=", "pretrained", ",", "\n", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "conv_layer", "=", "create_conv2d", ",", "norm_layer", "=", "GroupNormAct", ",", "\n", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_50d_evob": [[687, 693], ["resnetv2._create_resnetv2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2"], ["", "@", "register_model", "\n", "def", "resnetv2_50d_evob", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2", "(", "\n", "'resnetv2_50d_evob'", ",", "pretrained", "=", "pretrained", ",", "\n", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "conv_layer", "=", "create_conv2d", ",", "norm_layer", "=", "EvoNorm2dB0", ",", "\n", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "zero_init_last", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_50d_evos": [[695, 701], ["resnetv2._create_resnetv2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2"], ["", "@", "register_model", "\n", "def", "resnetv2_50d_evos", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2", "(", "\n", "'resnetv2_50d_evos'", ",", "pretrained", "=", "pretrained", ",", "\n", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "conv_layer", "=", "create_conv2d", ",", "norm_layer", "=", "EvoNorm2dS0", ",", "\n", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2.resnetv2_50d_frn": [[703, 709], ["resnetv2._create_resnetv2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.resnetv2._create_resnetv2"], ["", "@", "register_model", "\n", "def", "resnetv2_50d_frn", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_resnetv2", "(", "\n", "'resnetv2_50d_frn'", ",", "pretrained", "=", "pretrained", ",", "\n", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "conv_layer", "=", "create_conv2d", ",", "norm_layer", "=", "FilterResponseNormTlu2d", ",", "\n", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "**", "kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception.SeparableConv2d.__init__": [[52, 58], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "dilation", "=", "1", ")", ":", "\n", "        ", "super", "(", "SeparableConv2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "in_channels", ",", "kernel_size", ",", "stride", ",", "padding", ",", "dilation", ",", "groups", "=", "in_channels", ",", "bias", "=", "False", ")", "\n", "self", ".", "pointwise", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception.SeparableConv2d.forward": [[59, 63], ["xception.SeparableConv2d.conv1", "xception.SeparableConv2d.pointwise"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "pointwise", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception.Block.__init__": [[66, 95], ["torch.Module.__init__", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "rep.append", "rep.append", "rep.append", "torch.ReLU", "torch.ReLU", "torch.ReLU", "rep.append", "torch.ReLU", "torch.ReLU", "torch.ReLU", "xception.SeparableConv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "reps", ",", "strides", "=", "1", ",", "start_with_relu", "=", "True", ",", "grow_first", "=", "True", ")", ":", "\n", "        ", "super", "(", "Block", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "out_channels", "!=", "in_channels", "or", "strides", "!=", "1", ":", "\n", "            ", "self", ".", "skip", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "1", ",", "stride", "=", "strides", ",", "bias", "=", "False", ")", "\n", "self", ".", "skipbn", "=", "nn", ".", "BatchNorm2d", "(", "out_channels", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "skip", "=", "None", "\n", "\n", "", "rep", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "reps", ")", ":", "\n", "            ", "if", "grow_first", ":", "\n", "                ", "inc", "=", "in_channels", "if", "i", "==", "0", "else", "out_channels", "\n", "outc", "=", "out_channels", "\n", "", "else", ":", "\n", "                ", "inc", "=", "in_channels", "\n", "outc", "=", "in_channels", "if", "i", "<", "(", "reps", "-", "1", ")", "else", "out_channels", "\n", "", "rep", ".", "append", "(", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "rep", ".", "append", "(", "SeparableConv2d", "(", "inc", ",", "outc", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ")", "\n", "rep", ".", "append", "(", "nn", ".", "BatchNorm2d", "(", "outc", ")", ")", "\n", "\n", "", "if", "not", "start_with_relu", ":", "\n", "            ", "rep", "=", "rep", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "rep", "[", "0", "]", "=", "nn", ".", "ReLU", "(", "inplace", "=", "False", ")", "\n", "\n", "", "if", "strides", "!=", "1", ":", "\n", "            ", "rep", ".", "append", "(", "nn", ".", "MaxPool2d", "(", "3", ",", "strides", ",", "1", ")", ")", "\n", "", "self", ".", "rep", "=", "nn", ".", "Sequential", "(", "*", "rep", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception.Block.forward": [[96, 107], ["xception.Block.rep", "xception.Block.skip", "xception.Block.skipbn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inp", ")", ":", "\n", "        ", "x", "=", "self", ".", "rep", "(", "inp", ")", "\n", "\n", "if", "self", ".", "skip", "is", "not", "None", ":", "\n", "            ", "skip", "=", "self", ".", "skip", "(", "inp", ")", "\n", "skip", "=", "self", ".", "skipbn", "(", "skip", ")", "\n", "", "else", ":", "\n", "            ", "skip", "=", "inp", "\n", "\n", "", "x", "+=", "skip", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception.Xception.__init__": [[115, 174], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "xception.Block", "xception.Block", "xception.Block", "xception.Block", "xception.Block", "xception.Block", "xception.Block", "xception.Block", "xception.Block", "xception.Block", "xception.Block", "xception.Block", "xception.SeparableConv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "xception.SeparableConv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "layers.create_classifier", "xception.Xception.modules", "dict", "dict", "dict", "dict", "dict", "isinstance", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "isinstance", "m.weight.data.fill_", "m.bias.data.zero_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["def", "__init__", "(", "self", ",", "num_classes", "=", "1000", ",", "in_chans", "=", "3", ",", "drop_rate", "=", "0.", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "\"\"\" Constructor\n        Args:\n            num_classes: number of classes\n        \"\"\"", "\n", "super", "(", "Xception", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "num_features", "=", "2048", "\n", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "in_chans", ",", "32", ",", "3", ",", "2", ",", "0", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm2d", "(", "32", ")", "\n", "self", ".", "act1", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "3", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "BatchNorm2d", "(", "64", ")", "\n", "self", ".", "act2", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n", "self", ".", "block1", "=", "Block", "(", "64", ",", "128", ",", "2", ",", "2", ",", "start_with_relu", "=", "False", ")", "\n", "self", ".", "block2", "=", "Block", "(", "128", ",", "256", ",", "2", ",", "2", ")", "\n", "self", ".", "block3", "=", "Block", "(", "256", ",", "728", ",", "2", ",", "2", ")", "\n", "\n", "self", ".", "block4", "=", "Block", "(", "728", ",", "728", ",", "3", ",", "1", ")", "\n", "self", ".", "block5", "=", "Block", "(", "728", ",", "728", ",", "3", ",", "1", ")", "\n", "self", ".", "block6", "=", "Block", "(", "728", ",", "728", ",", "3", ",", "1", ")", "\n", "self", ".", "block7", "=", "Block", "(", "728", ",", "728", ",", "3", ",", "1", ")", "\n", "\n", "self", ".", "block8", "=", "Block", "(", "728", ",", "728", ",", "3", ",", "1", ")", "\n", "self", ".", "block9", "=", "Block", "(", "728", ",", "728", ",", "3", ",", "1", ")", "\n", "self", ".", "block10", "=", "Block", "(", "728", ",", "728", ",", "3", ",", "1", ")", "\n", "self", ".", "block11", "=", "Block", "(", "728", ",", "728", ",", "3", ",", "1", ")", "\n", "\n", "self", ".", "block12", "=", "Block", "(", "728", ",", "1024", ",", "2", ",", "2", ",", "grow_first", "=", "False", ")", "\n", "\n", "self", ".", "conv3", "=", "SeparableConv2d", "(", "1024", ",", "1536", ",", "3", ",", "1", ",", "1", ")", "\n", "self", ".", "bn3", "=", "nn", ".", "BatchNorm2d", "(", "1536", ")", "\n", "self", ".", "act3", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n", "self", ".", "conv4", "=", "SeparableConv2d", "(", "1536", ",", "self", ".", "num_features", ",", "3", ",", "1", ",", "1", ")", "\n", "self", ".", "bn4", "=", "nn", ".", "BatchNorm2d", "(", "self", ".", "num_features", ")", "\n", "self", ".", "act4", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "feature_info", "=", "[", "\n", "dict", "(", "num_chs", "=", "64", ",", "reduction", "=", "2", ",", "module", "=", "'act2'", ")", ",", "\n", "dict", "(", "num_chs", "=", "128", ",", "reduction", "=", "4", ",", "module", "=", "'block2.rep.0'", ")", ",", "\n", "dict", "(", "num_chs", "=", "256", ",", "reduction", "=", "8", ",", "module", "=", "'block3.rep.0'", ")", ",", "\n", "dict", "(", "num_chs", "=", "728", ",", "reduction", "=", "16", ",", "module", "=", "'block12.rep.0'", ")", ",", "\n", "dict", "(", "num_chs", "=", "2048", ",", "reduction", "=", "32", ",", "module", "=", "'act4'", ")", ",", "\n", "]", "\n", "\n", "self", ".", "global_pool", ",", "self", ".", "fc", "=", "create_classifier", "(", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n", "# #------- init weights --------", "\n", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "                ", "m", ".", "weight", ".", "data", ".", "fill_", "(", "1", ")", "\n", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception.Xception.group_matcher": [[175, 182], ["dict"], "methods", ["None"], ["", "", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^conv[12]|bn[12]'", ",", "\n", "blocks", "=", "[", "\n", "(", "r'^block(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^conv[34]|bn[34]'", ",", "(", "99", ",", ")", ")", ",", "\n", "]", ",", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception.Xception.set_grad_checkpointing": [[185, 188], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "\"gradient checkpointing not supported\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception.Xception.get_classifier": [[189, 192], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception.Xception.reset_classifier": [[193, 196], ["layers.create_classifier"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", ",", "self", ".", "fc", "=", "create_classifier", "(", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception.Xception.forward_features": [[197, 227], ["xception.Xception.conv1", "xception.Xception.bn1", "xception.Xception.act1", "xception.Xception.conv2", "xception.Xception.bn2", "xception.Xception.act2", "xception.Xception.block1", "xception.Xception.block2", "xception.Xception.block3", "xception.Xception.block4", "xception.Xception.block5", "xception.Xception.block6", "xception.Xception.block7", "xception.Xception.block8", "xception.Xception.block9", "xception.Xception.block10", "xception.Xception.block11", "xception.Xception.block12", "xception.Xception.conv3", "xception.Xception.bn3", "xception.Xception.act3", "xception.Xception.conv4", "xception.Xception.bn4", "xception.Xception.act4"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "x", "=", "self", ".", "act1", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "x", "=", "self", ".", "bn2", "(", "x", ")", "\n", "x", "=", "self", ".", "act2", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "block1", "(", "x", ")", "\n", "x", "=", "self", ".", "block2", "(", "x", ")", "\n", "x", "=", "self", ".", "block3", "(", "x", ")", "\n", "x", "=", "self", ".", "block4", "(", "x", ")", "\n", "x", "=", "self", ".", "block5", "(", "x", ")", "\n", "x", "=", "self", ".", "block6", "(", "x", ")", "\n", "x", "=", "self", ".", "block7", "(", "x", ")", "\n", "x", "=", "self", ".", "block8", "(", "x", ")", "\n", "x", "=", "self", ".", "block9", "(", "x", ")", "\n", "x", "=", "self", ".", "block10", "(", "x", ")", "\n", "x", "=", "self", ".", "block11", "(", "x", ")", "\n", "x", "=", "self", ".", "block12", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "conv3", "(", "x", ")", "\n", "x", "=", "self", ".", "bn3", "(", "x", ")", "\n", "x", "=", "self", ".", "act3", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "conv4", "(", "x", ")", "\n", "x", "=", "self", ".", "bn4", "(", "x", ")", "\n", "x", "=", "self", ".", "act4", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception.Xception.forward_head": [[228, 233], ["xception.Xception.global_pool", "torch.dropout", "torch.dropout", "torch.dropout", "xception.Xception.fc"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "if", "self", ".", "drop_rate", ":", "\n", "            ", "F", ".", "dropout", "(", "x", ",", "self", ".", "drop_rate", ",", "training", "=", "self", ".", "training", ")", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "fc", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception.Xception.forward": [[234, 238], ["xception.Xception.forward_features", "xception.Xception.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception._xception": [[240, 245], ["helpers.build_model_with_cfg", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_xception", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "\n", "Xception", ",", "variant", ",", "pretrained", ",", "\n", "feature_cfg", "=", "dict", "(", "feature_cls", "=", "'hook'", ")", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception.xception": [[247, 250], ["xception._xception"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xception._xception"], ["", "@", "register_model", "\n", "def", "xception", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_xception", "(", "'xception'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.SequentialList.__init__": [[58, 60], ["torch.Sequential.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "super", "(", "SequentialList", ",", "self", ")", ".", "__init__", "(", "*", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.SequentialList.forward": [[71, 75], ["module"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", "->", "List", "[", "torch", ".", "Tensor", "]", ":", "\n", "        ", "for", "module", "in", "self", ":", "\n", "            ", "x", "=", "module", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.SelectSeq.__init__": [[78, 82], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "mode", "=", "'index'", ",", "index", "=", "0", ")", ":", "\n", "        ", "super", "(", "SelectSeq", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "index", "=", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.SelectSeq.forward": [[93, 98], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "if", "self", ".", "mode", "==", "'index'", ":", "\n", "            ", "return", "x", "[", "self", ".", "index", "]", "\n", "", "else", ":", "\n", "            ", "return", "torch", ".", "cat", "(", "x", ",", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.SelecSLSBlock.__init__": [[111, 124], ["torch.Module.__init__", "selecsls.conv_bn", "selecsls.conv_bn", "selecsls.conv_bn", "selecsls.conv_bn", "selecsls.conv_bn", "selecsls.conv_bn"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.conv_bn", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.conv_bn", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.conv_bn", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.conv_bn", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.conv_bn", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.conv_bn"], ["    ", "def", "__init__", "(", "self", ",", "in_chs", ",", "skip_chs", ",", "mid_chs", ",", "out_chs", ",", "is_first", ",", "stride", ",", "dilation", "=", "1", ")", ":", "\n", "        ", "super", "(", "SelecSLSBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "is_first", "=", "is_first", "\n", "assert", "stride", "in", "[", "1", ",", "2", "]", "\n", "\n", "# Process input with 4 conv blocks with the same number of input and output channels", "\n", "self", ".", "conv1", "=", "conv_bn", "(", "in_chs", ",", "mid_chs", ",", "3", ",", "stride", ",", "dilation", "=", "dilation", ")", "\n", "self", ".", "conv2", "=", "conv_bn", "(", "mid_chs", ",", "mid_chs", ",", "1", ")", "\n", "self", ".", "conv3", "=", "conv_bn", "(", "mid_chs", ",", "mid_chs", "//", "2", ",", "3", ")", "\n", "self", ".", "conv4", "=", "conv_bn", "(", "mid_chs", "//", "2", ",", "mid_chs", ",", "1", ")", "\n", "self", ".", "conv5", "=", "conv_bn", "(", "mid_chs", ",", "mid_chs", "//", "2", ",", "3", ")", "\n", "self", ".", "conv6", "=", "conv_bn", "(", "2", "*", "mid_chs", "+", "(", "0", "if", "is_first", "else", "skip_chs", ")", ",", "out_chs", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.SelecSLSBlock.forward": [[125, 138], ["selecsls.SelecSLSBlock.conv1", "selecsls.SelecSLSBlock.conv3", "selecsls.SelecSLSBlock.conv5", "isinstance", "len", "selecsls.SelecSLSBlock.conv2", "selecsls.SelecSLSBlock.conv4", "selecsls.SelecSLSBlock.conv6", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "selecsls.SelecSLSBlock.conv6", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "List", "[", "torch", ".", "Tensor", "]", ")", "->", "List", "[", "torch", ".", "Tensor", "]", ":", "\n", "        ", "if", "not", "isinstance", "(", "x", ",", "list", ")", ":", "\n", "            ", "x", "=", "[", "x", "]", "\n", "", "assert", "len", "(", "x", ")", "in", "[", "1", ",", "2", "]", "\n", "\n", "d1", "=", "self", ".", "conv1", "(", "x", "[", "0", "]", ")", "\n", "d2", "=", "self", ".", "conv3", "(", "self", ".", "conv2", "(", "d1", ")", ")", "\n", "d3", "=", "self", ".", "conv5", "(", "self", ".", "conv4", "(", "d2", ")", ")", "\n", "if", "self", ".", "is_first", ":", "\n", "            ", "out", "=", "self", ".", "conv6", "(", "torch", ".", "cat", "(", "[", "d1", ",", "d2", ",", "d3", "]", ",", "1", ")", ")", "\n", "return", "[", "out", ",", "out", "]", "\n", "", "else", ":", "\n", "            ", "return", "[", "self", ".", "conv6", "(", "torch", ".", "cat", "(", "[", "d1", ",", "d2", ",", "d3", ",", "x", "[", "1", "]", "]", ",", "1", ")", ")", ",", "x", "[", "1", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.SelecSLS.__init__": [[156, 176], ["torch.Module.__init__", "selecsls.conv_bn", "selecsls.SequentialList", "selecsls.SelectSeq", "torch.Sequential", "torch.Sequential", "torch.Sequential", "layers.create_classifier", "selecsls.SelecSLS.named_modules", "isinstance", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "isinstance", "selecsls.conv_bn", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.conv_bn", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_modules", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.conv_bn"], ["def", "__init__", "(", "self", ",", "cfg", ",", "num_classes", "=", "1000", ",", "in_chans", "=", "3", ",", "drop_rate", "=", "0.0", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "super", "(", "SelecSLS", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "stem", "=", "conv_bn", "(", "in_chans", ",", "32", ",", "stride", "=", "2", ")", "\n", "self", ".", "features", "=", "SequentialList", "(", "*", "[", "cfg", "[", "'block'", "]", "(", "*", "block_args", ")", "for", "block_args", "in", "cfg", "[", "'features'", "]", "]", ")", "\n", "self", ".", "from_seq", "=", "SelectSeq", "(", ")", "# from List[tensor] -> Tensor in module compatible way", "\n", "self", ".", "head", "=", "nn", ".", "Sequential", "(", "*", "[", "conv_bn", "(", "*", "conv_args", ")", "for", "conv_args", "in", "cfg", "[", "'head'", "]", "]", ")", "\n", "self", ".", "num_features", "=", "cfg", "[", "'num_features'", "]", "\n", "self", ".", "feature_info", "=", "cfg", "[", "'feature_info'", "]", "\n", "\n", "self", ".", "global_pool", ",", "self", ".", "fc", "=", "create_classifier", "(", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n", "for", "n", ",", "m", "in", "self", ".", "named_modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1.", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0.", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.SelecSLS.group_matcher": [[177, 183], ["dict"], "methods", ["None"], ["", "", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^stem'", ",", "\n", "blocks", "=", "r'^features\\.(\\d+)'", ",", "\n", "blocks_head", "=", "r'^head'", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.SelecSLS.set_grad_checkpointing": [[185, 188], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "'gradient checkpointing not supported'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.SelecSLS.get_classifier": [[189, 192], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.SelecSLS.reset_classifier": [[193, 196], ["layers.create_classifier"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", ",", "self", ".", "fc", "=", "create_classifier", "(", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.SelecSLS.forward_features": [[197, 202], ["selecsls.SelecSLS.stem", "selecsls.SelecSLS.features", "selecsls.SelecSLS.head", "selecsls.SelecSLS.from_seq"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "stem", "(", "x", ")", "\n", "x", "=", "self", ".", "features", "(", "x", ")", "\n", "x", "=", "self", ".", "head", "(", "self", ".", "from_seq", "(", "x", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.SelecSLS.forward_head": [[203, 208], ["selecsls.SelecSLS.global_pool", "torch.dropout", "torch.dropout", "torch.dropout", "selecsls.SelecSLS.fc"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "if", "self", ".", "drop_rate", ">", "0.", ":", "\n", "            ", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "drop_rate", ",", "training", "=", "self", ".", "training", ")", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "fc", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.SelecSLS.forward": [[209, 213], ["selecsls.SelecSLS.forward_features", "selecsls.SelecSLS.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls._cfg": [[26, 34], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "4", ",", "4", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bilinear'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'stem.0'", ",", "'classifier'", ":", "'fc'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.conv_bn": [[100, 107], ["torch.Sequential", "torch.Conv2d", "torch.BatchNorm2d", "torch.ReLU"], "function", ["None"], ["", "", "", "def", "conv_bn", "(", "in_chs", ",", "out_chs", ",", "k", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "None", ",", "dilation", "=", "1", ")", ":", "\n", "    ", "if", "padding", "is", "None", ":", "\n", "        ", "padding", "=", "(", "(", "stride", "-", "1", ")", "+", "dilation", "*", "(", "k", "-", "1", ")", ")", "//", "2", "\n", "", "return", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "in_chs", ",", "out_chs", ",", "k", ",", "stride", ",", "padding", "=", "padding", ",", "dilation", "=", "dilation", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "out_chs", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls._create_selecsls": [[215, 343], ["variant.startswith", "helpers.build_model_with_cfg", "dict", "feature_info.extend", "feature_info.append", "variant.startswith", "dict", "feature_info.append", "feature_info.append", "feature_info.extend", "feature_info.append", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "feature_info.append", "feature_info.append", "feature_info.extend", "feature_info.extend", "ValueError", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_selecsls", "(", "variant", ",", "pretrained", ",", "**", "kwargs", ")", ":", "\n", "    ", "cfg", "=", "{", "}", "\n", "feature_info", "=", "[", "dict", "(", "num_chs", "=", "32", ",", "reduction", "=", "2", ",", "module", "=", "'stem.2'", ")", "]", "\n", "if", "variant", ".", "startswith", "(", "'selecsls42'", ")", ":", "\n", "        ", "cfg", "[", "'block'", "]", "=", "SelecSLSBlock", "\n", "# Define configuration of the network after the initial neck", "\n", "cfg", "[", "'features'", "]", "=", "[", "\n", "# in_chs, skip_chs, mid_chs, out_chs, is_first, stride", "\n", "(", "32", ",", "0", ",", "64", ",", "64", ",", "True", ",", "2", ")", ",", "\n", "(", "64", ",", "64", ",", "64", ",", "128", ",", "False", ",", "1", ")", ",", "\n", "(", "128", ",", "0", ",", "144", ",", "144", ",", "True", ",", "2", ")", ",", "\n", "(", "144", ",", "144", ",", "144", ",", "288", ",", "False", ",", "1", ")", ",", "\n", "(", "288", ",", "0", ",", "304", ",", "304", ",", "True", ",", "2", ")", ",", "\n", "(", "304", ",", "304", ",", "304", ",", "480", ",", "False", ",", "1", ")", ",", "\n", "]", "\n", "feature_info", ".", "extend", "(", "[", "\n", "dict", "(", "num_chs", "=", "128", ",", "reduction", "=", "4", ",", "module", "=", "'features.1'", ")", ",", "\n", "dict", "(", "num_chs", "=", "288", ",", "reduction", "=", "8", ",", "module", "=", "'features.3'", ")", ",", "\n", "dict", "(", "num_chs", "=", "480", ",", "reduction", "=", "16", ",", "module", "=", "'features.5'", ")", ",", "\n", "]", ")", "\n", "# Head can be replaced with alternative configurations depending on the problem", "\n", "feature_info", ".", "append", "(", "dict", "(", "num_chs", "=", "1024", ",", "reduction", "=", "32", ",", "module", "=", "'head.1'", ")", ")", "\n", "if", "variant", "==", "'selecsls42b'", ":", "\n", "            ", "cfg", "[", "'head'", "]", "=", "[", "\n", "(", "480", ",", "960", ",", "3", ",", "2", ")", ",", "\n", "(", "960", ",", "1024", ",", "3", ",", "1", ")", ",", "\n", "(", "1024", ",", "1280", ",", "3", ",", "2", ")", ",", "\n", "(", "1280", ",", "1024", ",", "1", ",", "1", ")", ",", "\n", "]", "\n", "feature_info", ".", "append", "(", "dict", "(", "num_chs", "=", "1024", ",", "reduction", "=", "64", ",", "module", "=", "'head.3'", ")", ")", "\n", "cfg", "[", "'num_features'", "]", "=", "1024", "\n", "", "else", ":", "\n", "            ", "cfg", "[", "'head'", "]", "=", "[", "\n", "(", "480", ",", "960", ",", "3", ",", "2", ")", ",", "\n", "(", "960", ",", "1024", ",", "3", ",", "1", ")", ",", "\n", "(", "1024", ",", "1024", ",", "3", ",", "2", ")", ",", "\n", "(", "1024", ",", "1280", ",", "1", ",", "1", ")", ",", "\n", "]", "\n", "feature_info", ".", "append", "(", "dict", "(", "num_chs", "=", "1280", ",", "reduction", "=", "64", ",", "module", "=", "'head.3'", ")", ")", "\n", "cfg", "[", "'num_features'", "]", "=", "1280", "\n", "\n", "", "", "elif", "variant", ".", "startswith", "(", "'selecsls60'", ")", ":", "\n", "        ", "cfg", "[", "'block'", "]", "=", "SelecSLSBlock", "\n", "# Define configuration of the network after the initial neck", "\n", "cfg", "[", "'features'", "]", "=", "[", "\n", "# in_chs, skip_chs, mid_chs, out_chs, is_first, stride", "\n", "(", "32", ",", "0", ",", "64", ",", "64", ",", "True", ",", "2", ")", ",", "\n", "(", "64", ",", "64", ",", "64", ",", "128", ",", "False", ",", "1", ")", ",", "\n", "(", "128", ",", "0", ",", "128", ",", "128", ",", "True", ",", "2", ")", ",", "\n", "(", "128", ",", "128", ",", "128", ",", "128", ",", "False", ",", "1", ")", ",", "\n", "(", "128", ",", "128", ",", "128", ",", "288", ",", "False", ",", "1", ")", ",", "\n", "(", "288", ",", "0", ",", "288", ",", "288", ",", "True", ",", "2", ")", ",", "\n", "(", "288", ",", "288", ",", "288", ",", "288", ",", "False", ",", "1", ")", ",", "\n", "(", "288", ",", "288", ",", "288", ",", "288", ",", "False", ",", "1", ")", ",", "\n", "(", "288", ",", "288", ",", "288", ",", "416", ",", "False", ",", "1", ")", ",", "\n", "]", "\n", "feature_info", ".", "extend", "(", "[", "\n", "dict", "(", "num_chs", "=", "128", ",", "reduction", "=", "4", ",", "module", "=", "'features.1'", ")", ",", "\n", "dict", "(", "num_chs", "=", "288", ",", "reduction", "=", "8", ",", "module", "=", "'features.4'", ")", ",", "\n", "dict", "(", "num_chs", "=", "416", ",", "reduction", "=", "16", ",", "module", "=", "'features.8'", ")", ",", "\n", "]", ")", "\n", "# Head can be replaced with alternative configurations depending on the problem", "\n", "feature_info", ".", "append", "(", "dict", "(", "num_chs", "=", "1024", ",", "reduction", "=", "32", ",", "module", "=", "'head.1'", ")", ")", "\n", "if", "variant", "==", "'selecsls60b'", ":", "\n", "            ", "cfg", "[", "'head'", "]", "=", "[", "\n", "(", "416", ",", "756", ",", "3", ",", "2", ")", ",", "\n", "(", "756", ",", "1024", ",", "3", ",", "1", ")", ",", "\n", "(", "1024", ",", "1280", ",", "3", ",", "2", ")", ",", "\n", "(", "1280", ",", "1024", ",", "1", ",", "1", ")", ",", "\n", "]", "\n", "feature_info", ".", "append", "(", "dict", "(", "num_chs", "=", "1024", ",", "reduction", "=", "64", ",", "module", "=", "'head.3'", ")", ")", "\n", "cfg", "[", "'num_features'", "]", "=", "1024", "\n", "", "else", ":", "\n", "            ", "cfg", "[", "'head'", "]", "=", "[", "\n", "(", "416", ",", "756", ",", "3", ",", "2", ")", ",", "\n", "(", "756", ",", "1024", ",", "3", ",", "1", ")", ",", "\n", "(", "1024", ",", "1024", ",", "3", ",", "2", ")", ",", "\n", "(", "1024", ",", "1280", ",", "1", ",", "1", ")", ",", "\n", "]", "\n", "feature_info", ".", "append", "(", "dict", "(", "num_chs", "=", "1280", ",", "reduction", "=", "64", ",", "module", "=", "'head.3'", ")", ")", "\n", "cfg", "[", "'num_features'", "]", "=", "1280", "\n", "\n", "", "", "elif", "variant", "==", "'selecsls84'", ":", "\n", "        ", "cfg", "[", "'block'", "]", "=", "SelecSLSBlock", "\n", "# Define configuration of the network after the initial neck", "\n", "cfg", "[", "'features'", "]", "=", "[", "\n", "# in_chs, skip_chs, mid_chs, out_chs, is_first, stride", "\n", "(", "32", ",", "0", ",", "64", ",", "64", ",", "True", ",", "2", ")", ",", "\n", "(", "64", ",", "64", ",", "64", ",", "144", ",", "False", ",", "1", ")", ",", "\n", "(", "144", ",", "0", ",", "144", ",", "144", ",", "True", ",", "2", ")", ",", "\n", "(", "144", ",", "144", ",", "144", ",", "144", ",", "False", ",", "1", ")", ",", "\n", "(", "144", ",", "144", ",", "144", ",", "144", ",", "False", ",", "1", ")", ",", "\n", "(", "144", ",", "144", ",", "144", ",", "144", ",", "False", ",", "1", ")", ",", "\n", "(", "144", ",", "144", ",", "144", ",", "304", ",", "False", ",", "1", ")", ",", "\n", "(", "304", ",", "0", ",", "304", ",", "304", ",", "True", ",", "2", ")", ",", "\n", "(", "304", ",", "304", ",", "304", ",", "304", ",", "False", ",", "1", ")", ",", "\n", "(", "304", ",", "304", ",", "304", ",", "304", ",", "False", ",", "1", ")", ",", "\n", "(", "304", ",", "304", ",", "304", ",", "304", ",", "False", ",", "1", ")", ",", "\n", "(", "304", ",", "304", ",", "304", ",", "304", ",", "False", ",", "1", ")", ",", "\n", "(", "304", ",", "304", ",", "304", ",", "512", ",", "False", ",", "1", ")", ",", "\n", "]", "\n", "feature_info", ".", "extend", "(", "[", "\n", "dict", "(", "num_chs", "=", "144", ",", "reduction", "=", "4", ",", "module", "=", "'features.1'", ")", ",", "\n", "dict", "(", "num_chs", "=", "304", ",", "reduction", "=", "8", ",", "module", "=", "'features.6'", ")", ",", "\n", "dict", "(", "num_chs", "=", "512", ",", "reduction", "=", "16", ",", "module", "=", "'features.12'", ")", ",", "\n", "]", ")", "\n", "# Head can be replaced with alternative configurations depending on the problem", "\n", "cfg", "[", "'head'", "]", "=", "[", "\n", "(", "512", ",", "960", ",", "3", ",", "2", ")", ",", "\n", "(", "960", ",", "1024", ",", "3", ",", "1", ")", ",", "\n", "(", "1024", ",", "1024", ",", "3", ",", "2", ")", ",", "\n", "(", "1024", ",", "1280", ",", "3", ",", "1", ")", ",", "\n", "]", "\n", "cfg", "[", "'num_features'", "]", "=", "1280", "\n", "feature_info", ".", "extend", "(", "[", "\n", "dict", "(", "num_chs", "=", "1024", ",", "reduction", "=", "32", ",", "module", "=", "'head.1'", ")", ",", "\n", "dict", "(", "num_chs", "=", "1280", ",", "reduction", "=", "64", ",", "module", "=", "'head.3'", ")", "\n", "]", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Invalid net configuration '", "+", "variant", "+", "' !!!'", ")", "\n", "", "cfg", "[", "'feature_info'", "]", "=", "feature_info", "\n", "\n", "# this model can do 6 feature levels by default, unlike most others, leave as 0-4 to avoid surprises?", "\n", "return", "build_model_with_cfg", "(", "\n", "SelecSLS", ",", "variant", ",", "pretrained", ",", "\n", "model_cfg", "=", "cfg", ",", "\n", "feature_cfg", "=", "dict", "(", "out_indices", "=", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ",", "flatten_sequential", "=", "True", ")", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.selecsls42": [[345, 350], ["selecsls._create_selecsls"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls._create_selecsls"], ["", "@", "register_model", "\n", "def", "selecsls42", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a SelecSLS42 model.\n    \"\"\"", "\n", "return", "_create_selecsls", "(", "'selecsls42'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.selecsls42b": [[352, 357], ["selecsls._create_selecsls"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls._create_selecsls"], ["", "@", "register_model", "\n", "def", "selecsls42b", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a SelecSLS42_B model.\n    \"\"\"", "\n", "return", "_create_selecsls", "(", "'selecsls42b'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.selecsls60": [[359, 364], ["selecsls._create_selecsls"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls._create_selecsls"], ["", "@", "register_model", "\n", "def", "selecsls60", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a SelecSLS60 model.\n    \"\"\"", "\n", "return", "_create_selecsls", "(", "'selecsls60'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.selecsls60b": [[366, 371], ["selecsls._create_selecsls"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls._create_selecsls"], ["", "@", "register_model", "\n", "def", "selecsls60b", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a SelecSLS60_B model.\n    \"\"\"", "\n", "return", "_create_selecsls", "(", "'selecsls60b'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls.selecsls84": [[373, 378], ["selecsls._create_selecsls"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.selecsls._create_selecsls"], ["", "@", "register_model", "\n", "def", "selecsls84", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a SelecSLS84 model.\n    \"\"\"", "\n", "return", "_create_selecsls", "(", "'selecsls84'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convmixer.Residual.__init__": [[32, 35], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "fn", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fn", "=", "fn", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convmixer.Residual.forward": [[36, 38], ["convmixer.Residual.fn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "fn", "(", "x", ")", "+", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convmixer.ConvMixer.__init__": [[41, 68], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "layers.SelectAdaptivePool2d", "torch.Conv2d", "torch.Conv2d", "act_layer", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Sequential", "torch.Sequential", "convmixer.Residual", "torch.Conv2d", "torch.Conv2d", "act_layer", "torch.BatchNorm2d", "torch.BatchNorm2d", "range", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "act_layer", "torch.BatchNorm2d", "torch.BatchNorm2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "depth", ",", "kernel_size", "=", "9", ",", "patch_size", "=", "7", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "global_pool", "=", "'avg'", ",", "\n", "act_layer", "=", "nn", ".", "GELU", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "num_features", "=", "dim", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "self", ".", "stem", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "in_chans", ",", "dim", ",", "kernel_size", "=", "patch_size", ",", "stride", "=", "patch_size", ")", ",", "\n", "act_layer", "(", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "dim", ")", "\n", ")", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "\n", "*", "[", "nn", ".", "Sequential", "(", "\n", "Residual", "(", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "dim", ",", "dim", ",", "kernel_size", ",", "groups", "=", "dim", ",", "padding", "=", "\"same\"", ")", ",", "\n", "act_layer", "(", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "dim", ")", "\n", ")", ")", ",", "\n", "nn", ".", "Conv2d", "(", "dim", ",", "dim", ",", "kernel_size", "=", "1", ")", ",", "\n", "act_layer", "(", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "dim", ")", "\n", ")", "for", "i", "in", "range", "(", "depth", ")", "]", "\n", ")", "\n", "self", ".", "pooling", "=", "SelectAdaptivePool2d", "(", "pool_type", "=", "global_pool", ",", "flatten", "=", "True", ")", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convmixer.ConvMixer.group_matcher": [[69, 73], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "stem", "=", "r'^stem'", ",", "blocks", "=", "r'^blocks\\.(\\d+)'", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convmixer.ConvMixer.set_grad_checkpointing": [[74, 77], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convmixer.ConvMixer.get_classifier": [[78, 81], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convmixer.ConvMixer.reset_classifier": [[82, 87], ["layers.SelectAdaptivePool2d", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "self", ".", "pooling", "=", "SelectAdaptivePool2d", "(", "pool_type", "=", "global_pool", ",", "flatten", "=", "True", ")", "\n", "", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convmixer.ConvMixer.forward_features": [[88, 95], ["convmixer.ConvMixer.stem", "helpers.checkpoint_seq", "convmixer.ConvMixer.blocks", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "stem", "(", "x", ")", "\n", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "blocks", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "blocks", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convmixer.ConvMixer.forward_head": [[96, 99], ["convmixer.ConvMixer.pooling", "convmixer.ConvMixer.head"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "x", "=", "self", ".", "pooling", "(", "x", ")", "\n", "return", "x", "if", "pre_logits", "else", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convmixer.ConvMixer.forward": [[100, 104], ["convmixer.ConvMixer.forward_features", "convmixer.ConvMixer.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convmixer._cfg": [[13, 21], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", ".96", ",", "'interpolation'", ":", "'bicubic'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "'classifier'", ":", "'head'", ",", "\n", "'first_conv'", ":", "'stem.0'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convmixer._create_convmixer": [[106, 108], ["helpers.build_model_with_cfg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_convmixer", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "ConvMixer", ",", "variant", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convmixer.convmixer_1536_20": [[110, 114], ["dict", "convmixer._create_convmixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convmixer._create_convmixer"], ["", "@", "register_model", "\n", "def", "convmixer_1536_20", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "dim", "=", "1536", ",", "depth", "=", "20", ",", "kernel_size", "=", "9", ",", "patch_size", "=", "7", ",", "**", "kwargs", ")", "\n", "return", "_create_convmixer", "(", "'convmixer_1536_20'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convmixer.convmixer_768_32": [[116, 120], ["dict", "convmixer._create_convmixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convmixer._create_convmixer"], ["", "@", "register_model", "\n", "def", "convmixer_768_32", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "dim", "=", "768", ",", "depth", "=", "32", ",", "kernel_size", "=", "7", ",", "patch_size", "=", "7", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "**", "kwargs", ")", "\n", "return", "_create_convmixer", "(", "'convmixer_768_32'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convmixer.convmixer_1024_20_ks9_p14": [[122, 126], ["dict", "convmixer._create_convmixer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.convmixer._create_convmixer"], ["", "@", "register_model", "\n", "def", "convmixer_1024_20_ks9_p14", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_args", "=", "dict", "(", "dim", "=", "1024", ",", "depth", "=", "20", ",", "kernel_size", "=", "9", ",", "patch_size", "=", "14", ",", "**", "kwargs", ")", "\n", "return", "_create_convmixer", "(", "'convmixer_1024_20_ks9_p14'", ",", "pretrained", ",", "**", "model_args", ")", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.ResBottleneck.__init__": [[160, 177], ["torch.Module.__init__", "int", "dict", "layers.ConvNormAct", "layers.ConvNormActAa", "layers.ConvNormAct", "act_layer", "round", "layers.create_attn", "layers.create_attn"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.create_attn", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.create_attn"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "dilation", "=", "1", ",", "bottle_ratio", "=", "0.25", ",", "groups", "=", "1", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "attn_last", "=", "False", ",", "\n", "attn_layer", "=", "None", ",", "aa_layer", "=", "None", ",", "drop_block", "=", "None", ",", "drop_path", "=", "None", ")", ":", "\n", "        ", "super", "(", "ResBottleneck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "mid_chs", "=", "int", "(", "round", "(", "out_chs", "*", "bottle_ratio", ")", ")", "\n", "ckwargs", "=", "dict", "(", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ")", "\n", "\n", "self", ".", "conv1", "=", "ConvNormAct", "(", "in_chs", ",", "mid_chs", ",", "kernel_size", "=", "1", ",", "**", "ckwargs", ")", "\n", "self", ".", "conv2", "=", "ConvNormActAa", "(", "\n", "mid_chs", ",", "mid_chs", ",", "kernel_size", "=", "3", ",", "dilation", "=", "dilation", ",", "groups", "=", "groups", ",", "\n", "aa_layer", "=", "aa_layer", ",", "drop_layer", "=", "drop_block", ",", "**", "ckwargs", ")", "\n", "self", ".", "attn2", "=", "create_attn", "(", "attn_layer", ",", "channels", "=", "mid_chs", ")", "if", "not", "attn_last", "else", "None", "\n", "self", ".", "conv3", "=", "ConvNormAct", "(", "mid_chs", ",", "out_chs", ",", "kernel_size", "=", "1", ",", "apply_act", "=", "False", ",", "**", "ckwargs", ")", "\n", "self", ".", "attn3", "=", "create_attn", "(", "attn_layer", ",", "channels", "=", "out_chs", ")", "if", "attn_last", "else", "None", "\n", "self", ".", "drop_path", "=", "drop_path", "\n", "self", ".", "act3", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.ResBottleneck.zero_init_last": [[178, 180], ["torch.init.zeros_", "torch.init.zeros_"], "methods", ["None"], ["", "def", "zero_init_last", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "conv3", ".", "bn", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.ResBottleneck.forward": [[181, 197], ["cspnet.ResBottleneck.conv1", "cspnet.ResBottleneck.conv2", "cspnet.ResBottleneck.conv3", "cspnet.ResBottleneck.act3", "cspnet.ResBottleneck.attn2", "cspnet.ResBottleneck.attn3", "cspnet.ResBottleneck.drop_path"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "if", "self", ".", "attn2", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "attn2", "(", "x", ")", "\n", "", "x", "=", "self", ".", "conv3", "(", "x", ")", "\n", "if", "self", ".", "attn3", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "attn3", "(", "x", ")", "\n", "", "if", "self", ".", "drop_path", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "drop_path", "(", "x", ")", "\n", "", "x", "=", "x", "+", "shortcut", "\n", "# FIXME partial shortcut needed if first block handled as per original, not used for my current impl", "\n", "#x[:, :shortcut.size(1)] += shortcut", "\n", "x", "=", "self", ".", "act3", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.DarkBlock.__init__": [[203, 216], ["torch.Module.__init__", "int", "dict", "layers.ConvNormAct", "layers.ConvNormActAa", "layers.create_attn", "round"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.create_attn"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "dilation", "=", "1", ",", "bottle_ratio", "=", "0.5", ",", "groups", "=", "1", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "attn_layer", "=", "None", ",", "aa_layer", "=", "None", ",", "\n", "drop_block", "=", "None", ",", "drop_path", "=", "None", ")", ":", "\n", "        ", "super", "(", "DarkBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "mid_chs", "=", "int", "(", "round", "(", "out_chs", "*", "bottle_ratio", ")", ")", "\n", "ckwargs", "=", "dict", "(", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "conv1", "=", "ConvNormAct", "(", "in_chs", ",", "mid_chs", ",", "kernel_size", "=", "1", ",", "**", "ckwargs", ")", "\n", "self", ".", "conv2", "=", "ConvNormActAa", "(", "\n", "mid_chs", ",", "out_chs", ",", "kernel_size", "=", "3", ",", "dilation", "=", "dilation", ",", "groups", "=", "groups", ",", "\n", "aa_layer", "=", "aa_layer", ",", "drop_layer", "=", "drop_block", ",", "**", "ckwargs", ")", "\n", "self", ".", "attn", "=", "create_attn", "(", "attn_layer", ",", "channels", "=", "out_chs", ")", "\n", "self", ".", "drop_path", "=", "drop_path", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.DarkBlock.zero_init_last": [[217, 219], ["torch.init.zeros_", "torch.init.zeros_"], "methods", ["None"], ["", "def", "zero_init_last", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "conv2", ".", "bn", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.DarkBlock.forward": [[220, 230], ["cspnet.DarkBlock.conv1", "cspnet.DarkBlock.conv2", "cspnet.DarkBlock.attn", "cspnet.DarkBlock.drop_path"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "if", "self", ".", "attn", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "attn", "(", "x", ")", "\n", "", "if", "self", ".", "drop_path", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "drop_path", "(", "x", ")", "\n", "", "x", "=", "x", "+", "shortcut", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.CrossStage.__init__": [[234, 270], ["torch.Module.__init__", "int", "int", "dict", "layers.ConvNormAct", "torch.Sequential", "torch.Sequential", "range", "layers.ConvNormAct", "layers.ConvNormAct", "round", "round", "layers.ConvNormActAa", "cspnet.CrossStage.blocks.add_module", "block_kwargs.get", "block_kwargs.get", "layers.DropPath", "str", "block_fn", "block_kwargs.get"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "stride", ",", "dilation", ",", "depth", ",", "block_ratio", "=", "1.", ",", "bottle_ratio", "=", "1.", ",", "exp_ratio", "=", "1.", ",", "\n", "groups", "=", "1", ",", "first_dilation", "=", "None", ",", "down_growth", "=", "False", ",", "cross_linear", "=", "False", ",", "block_dpr", "=", "None", ",", "\n", "block_fn", "=", "ResBottleneck", ",", "**", "block_kwargs", ")", ":", "\n", "        ", "super", "(", "CrossStage", ",", "self", ")", ".", "__init__", "(", ")", "\n", "first_dilation", "=", "first_dilation", "or", "dilation", "\n", "down_chs", "=", "out_chs", "if", "down_growth", "else", "in_chs", "# grow downsample channels to output channels", "\n", "exp_chs", "=", "int", "(", "round", "(", "out_chs", "*", "exp_ratio", ")", ")", "\n", "block_out_chs", "=", "int", "(", "round", "(", "out_chs", "*", "block_ratio", ")", ")", "\n", "conv_kwargs", "=", "dict", "(", "act_layer", "=", "block_kwargs", ".", "get", "(", "'act_layer'", ")", ",", "norm_layer", "=", "block_kwargs", ".", "get", "(", "'norm_layer'", ")", ")", "\n", "\n", "if", "stride", "!=", "1", "or", "first_dilation", "!=", "dilation", ":", "\n", "            ", "self", ".", "conv_down", "=", "ConvNormActAa", "(", "\n", "in_chs", ",", "down_chs", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "dilation", "=", "first_dilation", ",", "groups", "=", "groups", ",", "\n", "aa_layer", "=", "block_kwargs", ".", "get", "(", "'aa_layer'", ",", "None", ")", ",", "**", "conv_kwargs", ")", "\n", "prev_chs", "=", "down_chs", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv_down", "=", "None", "\n", "prev_chs", "=", "in_chs", "\n", "\n", "# FIXME this 1x1 expansion is pushed down into the cross and block paths in the darknet cfgs. Also,", "\n", "# there is also special case for the first stage for some of the model that results in uneven split", "\n", "# across the two paths. I did it this way for simplicity for now.", "\n", "", "self", ".", "conv_exp", "=", "ConvNormAct", "(", "prev_chs", ",", "exp_chs", ",", "kernel_size", "=", "1", ",", "apply_act", "=", "not", "cross_linear", ",", "**", "conv_kwargs", ")", "\n", "prev_chs", "=", "exp_chs", "//", "2", "# output of conv_exp is always split in two", "\n", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", ")", "\n", "for", "i", "in", "range", "(", "depth", ")", ":", "\n", "            ", "drop_path", "=", "DropPath", "(", "block_dpr", "[", "i", "]", ")", "if", "block_dpr", "and", "block_dpr", "[", "i", "]", "else", "None", "\n", "self", ".", "blocks", ".", "add_module", "(", "str", "(", "i", ")", ",", "block_fn", "(", "\n", "prev_chs", ",", "block_out_chs", ",", "dilation", ",", "bottle_ratio", ",", "groups", ",", "drop_path", "=", "drop_path", ",", "**", "block_kwargs", ")", ")", "\n", "prev_chs", "=", "block_out_chs", "\n", "\n", "# transition convs", "\n", "", "self", ".", "conv_transition_b", "=", "ConvNormAct", "(", "prev_chs", ",", "exp_chs", "//", "2", ",", "kernel_size", "=", "1", ",", "**", "conv_kwargs", ")", "\n", "self", ".", "conv_transition", "=", "ConvNormAct", "(", "exp_chs", ",", "out_chs", ",", "kernel_size", "=", "1", ",", "**", "conv_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.CrossStage.forward": [[271, 281], ["cspnet.CrossStage.conv_exp", "cspnet.CrossStage.blocks", "cspnet.CrossStage.conv_transition_b().contiguous", "cspnet.CrossStage.conv_transition", "cspnet.CrossStage.conv_down", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "cspnet.CrossStage.conv_transition_b"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "conv_down", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "conv_down", "(", "x", ")", "\n", "", "x", "=", "self", ".", "conv_exp", "(", "x", ")", "\n", "split", "=", "x", ".", "shape", "[", "1", "]", "//", "2", "\n", "xs", ",", "xb", "=", "x", "[", ":", ",", ":", "split", "]", ",", "x", "[", ":", ",", "split", ":", "]", "\n", "xb", "=", "self", ".", "blocks", "(", "xb", ")", "\n", "xb", "=", "self", ".", "conv_transition_b", "(", "xb", ")", ".", "contiguous", "(", ")", "\n", "out", "=", "self", ".", "conv_transition", "(", "torch", ".", "cat", "(", "[", "xs", ",", "xb", "]", ",", "dim", "=", "1", ")", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.DarkStage.__init__": [[286, 305], ["torch.Module.__init__", "layers.ConvNormActAa", "int", "torch.Sequential", "torch.Sequential", "range", "round", "cspnet.DarkStage.blocks.add_module", "block_kwargs.get", "block_kwargs.get", "block_kwargs.get", "layers.DropPath", "str", "block_fn"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "stride", ",", "dilation", ",", "depth", ",", "block_ratio", "=", "1.", ",", "bottle_ratio", "=", "1.", ",", "groups", "=", "1", ",", "\n", "first_dilation", "=", "None", ",", "block_fn", "=", "ResBottleneck", ",", "block_dpr", "=", "None", ",", "**", "block_kwargs", ")", ":", "\n", "        ", "super", "(", "DarkStage", ",", "self", ")", ".", "__init__", "(", ")", "\n", "first_dilation", "=", "first_dilation", "or", "dilation", "\n", "\n", "self", ".", "conv_down", "=", "ConvNormActAa", "(", "\n", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "dilation", "=", "first_dilation", ",", "groups", "=", "groups", ",", "\n", "act_layer", "=", "block_kwargs", ".", "get", "(", "'act_layer'", ")", ",", "norm_layer", "=", "block_kwargs", ".", "get", "(", "'norm_layer'", ")", ",", "\n", "aa_layer", "=", "block_kwargs", ".", "get", "(", "'aa_layer'", ",", "None", ")", ")", "\n", "\n", "prev_chs", "=", "out_chs", "\n", "block_out_chs", "=", "int", "(", "round", "(", "out_chs", "*", "block_ratio", ")", ")", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", ")", "\n", "for", "i", "in", "range", "(", "depth", ")", ":", "\n", "            ", "drop_path", "=", "DropPath", "(", "block_dpr", "[", "i", "]", ")", "if", "block_dpr", "and", "block_dpr", "[", "i", "]", "else", "None", "\n", "self", ".", "blocks", ".", "add_module", "(", "str", "(", "i", ")", ",", "block_fn", "(", "\n", "prev_chs", ",", "block_out_chs", ",", "dilation", ",", "bottle_ratio", ",", "groups", ",", "drop_path", "=", "drop_path", ",", "**", "block_kwargs", ")", ")", "\n", "prev_chs", "=", "block_out_chs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.DarkStage.forward": [[306, 310], ["cspnet.DarkStage.conv_down", "cspnet.DarkStage.blocks"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv_down", "(", "x", ")", "\n", "x", "=", "self", ".", "blocks", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.CspNet.__init__": [[354, 389], ["torch.Module.__init__", "dict", "cspnet.create_stem", "cspnet._cfg_to_stage_args", "torch.Sequential", "torch.Sequential", "enumerate", "layers.ClassifierHead", "helpers.named_apply", "cspnet.CspNet.stages.add_module", "functools.partial", "str", "stage_fn", "dict"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.create_stem", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet._cfg_to_stage_args", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_apply"], ["def", "__init__", "(", "\n", "self", ",", "cfg", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "output_stride", "=", "32", ",", "global_pool", "=", "'avg'", ",", "drop_rate", "=", "0.", ",", "\n", "act_layer", "=", "nn", ".", "LeakyReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "aa_layer", "=", "None", ",", "drop_path_rate", "=", "0.", ",", "\n", "zero_init_last", "=", "True", ",", "stage_fn", "=", "CrossStage", ",", "block_fn", "=", "ResBottleneck", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "assert", "output_stride", "in", "(", "8", ",", "16", ",", "32", ")", "\n", "layer_args", "=", "dict", "(", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ",", "aa_layer", "=", "aa_layer", ")", "\n", "\n", "# Construct the stem", "\n", "self", ".", "stem", ",", "stem_feat_info", "=", "create_stem", "(", "in_chans", ",", "**", "cfg", "[", "'stem'", "]", ",", "**", "layer_args", ")", "\n", "self", ".", "feature_info", "=", "[", "stem_feat_info", "]", "\n", "prev_chs", "=", "stem_feat_info", "[", "'num_chs'", "]", "\n", "curr_stride", "=", "stem_feat_info", "[", "'reduction'", "]", "# reduction does not include pool", "\n", "if", "cfg", "[", "'stem'", "]", "[", "'pool'", "]", ":", "\n", "            ", "curr_stride", "*=", "2", "\n", "\n", "# Construct the stages", "\n", "", "per_stage_args", "=", "_cfg_to_stage_args", "(", "\n", "cfg", "[", "'stage'", "]", ",", "curr_stride", "=", "curr_stride", ",", "output_stride", "=", "output_stride", ",", "drop_path_rate", "=", "drop_path_rate", ")", "\n", "self", ".", "stages", "=", "nn", ".", "Sequential", "(", ")", "\n", "for", "i", ",", "sa", "in", "enumerate", "(", "per_stage_args", ")", ":", "\n", "            ", "self", ".", "stages", ".", "add_module", "(", "\n", "str", "(", "i", ")", ",", "stage_fn", "(", "prev_chs", ",", "**", "sa", ",", "**", "layer_args", ",", "block_fn", "=", "block_fn", ")", ")", "\n", "prev_chs", "=", "sa", "[", "'out_chs'", "]", "\n", "curr_stride", "*=", "sa", "[", "'stride'", "]", "\n", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "prev_chs", ",", "reduction", "=", "curr_stride", ",", "module", "=", "f'stages.{i}'", ")", "]", "\n", "\n", "# Construct the head", "\n", "", "self", ".", "num_features", "=", "prev_chs", "\n", "self", ".", "head", "=", "ClassifierHead", "(", "\n", "in_chs", "=", "prev_chs", ",", "num_classes", "=", "num_classes", ",", "pool_type", "=", "global_pool", ",", "drop_rate", "=", "drop_rate", ")", "\n", "\n", "named_apply", "(", "partial", "(", "_init_weights", ",", "zero_init_last", "=", "zero_init_last", ")", ",", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.CspNet.group_matcher": [[390, 401], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "\n", "stem", "=", "r'^stem'", ",", "\n", "blocks", "=", "r'^stages\\.(\\d+)'", "if", "coarse", "else", "[", "\n", "(", "r'^stages\\.(\\d+)\\.blocks\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^stages\\.(\\d+)\\..*transition'", ",", "MATCH_PREV_GROUP", ")", ",", "# map to last block in stage", "\n", "(", "r'^stages\\.(\\d+)'", ",", "(", "0", ",", ")", ")", ",", "\n", "]", "\n", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.CspNet.set_grad_checkpointing": [[402, 405], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "'gradient checkpointing not supported'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.CspNet.get_classifier": [[406, 409], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", ".", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.CspNet.reset_classifier": [[410, 412], ["layers.ClassifierHead"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "head", "=", "ClassifierHead", "(", "self", ".", "num_features", ",", "num_classes", ",", "pool_type", "=", "global_pool", ",", "drop_rate", "=", "self", ".", "drop_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.CspNet.forward_features": [[413, 417], ["cspnet.CspNet.stem", "cspnet.CspNet.stages"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet.stages"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "stem", "(", "x", ")", "\n", "x", "=", "self", ".", "stages", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.CspNet.forward_head": [[418, 420], ["cspnet.CspNet.head"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "head", "(", "x", ",", "pre_logits", "=", "pre_logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.CspNet.forward": [[421, 425], ["cspnet.CspNet.forward_features", "cspnet.CspNet.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet._cfg": [[29, 37], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "256", ",", "256", ")", ",", "'pool_size'", ":", "(", "8", ",", "8", ")", ",", "\n", "'crop_pct'", ":", "0.887", ",", "'interpolation'", ":", "'bilinear'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'stem.conv1.conv'", ",", "'classifier'", ":", "'head.fc'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.create_stem": [[132, 154], ["torch.Sequential", "len", "enumerate", "isinstance", "nn.Sequential.add_module", "dict", "layers.ConvNormAct", "nn.Sequential.add_module", "nn.Sequential.add_module", "nn.Sequential.add_module", "torch.MaxPool2d", "aa_layer", "torch.MaxPool2d"], "function", ["None"], ["def", "create_stem", "(", "\n", "in_chans", "=", "3", ",", "out_chs", "=", "32", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "pool", "=", "''", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "aa_layer", "=", "None", ")", ":", "\n", "    ", "stem", "=", "nn", ".", "Sequential", "(", ")", "\n", "if", "not", "isinstance", "(", "out_chs", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "out_chs", "=", "[", "out_chs", "]", "\n", "", "assert", "len", "(", "out_chs", ")", "\n", "in_c", "=", "in_chans", "\n", "for", "i", ",", "out_c", "in", "enumerate", "(", "out_chs", ")", ":", "\n", "        ", "conv_name", "=", "f'conv{i + 1}'", "\n", "stem", ".", "add_module", "(", "conv_name", ",", "ConvNormAct", "(", "\n", "in_c", ",", "out_c", ",", "kernel_size", ",", "stride", "=", "stride", "if", "i", "==", "0", "else", "1", ",", "\n", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ")", ")", "\n", "in_c", "=", "out_c", "\n", "last_conv", "=", "conv_name", "\n", "", "if", "pool", ":", "\n", "        ", "if", "aa_layer", "is", "not", "None", ":", "\n", "            ", "stem", ".", "add_module", "(", "'pool'", ",", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ")", "\n", "stem", ".", "add_module", "(", "'aa'", ",", "aa_layer", "(", "channels", "=", "in_c", ",", "stride", "=", "2", ")", ")", "\n", "", "else", ":", "\n", "            ", "stem", ".", "add_module", "(", "'pool'", ",", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", ")", "\n", "", "", "return", "stem", ",", "dict", "(", "num_chs", "=", "in_c", ",", "reduction", "=", "stride", ",", "module", "=", "'.'", ".", "join", "(", "[", "'stem'", ",", "last_conv", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet._cfg_to_stage_args": [[312, 342], ["len", "stage_first_dilations.append", "stage_strides.append", "stage_dilations.append", "dict", "isinstance", "isinstance", "x.tolist", "zip", "zip", "torch.linspace().split", "torch.linspace().split", "cfg.keys", "cfg.values", "torch.linspace", "torch.linspace", "sum"], "function", ["None"], ["", "", "def", "_cfg_to_stage_args", "(", "cfg", ",", "curr_stride", "=", "2", ",", "output_stride", "=", "32", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "# get per stage args for stage and containing blocks, calculate strides to meet target output_stride", "\n", "    ", "num_stages", "=", "len", "(", "cfg", "[", "'depth'", "]", ")", "\n", "if", "'groups'", "not", "in", "cfg", ":", "\n", "        ", "cfg", "[", "'groups'", "]", "=", "(", "1", ",", ")", "*", "num_stages", "\n", "", "if", "'down_growth'", "in", "cfg", "and", "not", "isinstance", "(", "cfg", "[", "'down_growth'", "]", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "cfg", "[", "'down_growth'", "]", "=", "(", "cfg", "[", "'down_growth'", "]", ",", ")", "*", "num_stages", "\n", "", "if", "'cross_linear'", "in", "cfg", "and", "not", "isinstance", "(", "cfg", "[", "'cross_linear'", "]", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "cfg", "[", "'cross_linear'", "]", "=", "(", "cfg", "[", "'cross_linear'", "]", ",", ")", "*", "num_stages", "\n", "", "cfg", "[", "'block_dpr'", "]", "=", "[", "None", "]", "*", "num_stages", "if", "not", "drop_path_rate", "else", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "sum", "(", "cfg", "[", "'depth'", "]", ")", ")", ".", "split", "(", "cfg", "[", "'depth'", "]", ")", "]", "\n", "stage_strides", "=", "[", "]", "\n", "stage_dilations", "=", "[", "]", "\n", "stage_first_dilations", "=", "[", "]", "\n", "dilation", "=", "1", "\n", "for", "cfg_stride", "in", "cfg", "[", "'stride'", "]", ":", "\n", "        ", "stage_first_dilations", ".", "append", "(", "dilation", ")", "\n", "if", "curr_stride", ">=", "output_stride", ":", "\n", "            ", "dilation", "*=", "cfg_stride", "\n", "stride", "=", "1", "\n", "", "else", ":", "\n", "            ", "stride", "=", "cfg_stride", "\n", "curr_stride", "*=", "stride", "\n", "", "stage_strides", ".", "append", "(", "stride", ")", "\n", "stage_dilations", ".", "append", "(", "dilation", ")", "\n", "", "cfg", "[", "'stride'", "]", "=", "stage_strides", "\n", "cfg", "[", "'dilation'", "]", "=", "stage_dilations", "\n", "cfg", "[", "'first_dilation'", "]", "=", "stage_first_dilations", "\n", "stage_args", "=", "[", "dict", "(", "zip", "(", "cfg", ".", "keys", "(", ")", ",", "values", ")", ")", "for", "values", "in", "zip", "(", "*", "cfg", ".", "values", "(", ")", ")", "]", "\n", "return", "stage_args", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet._init_weights": [[427, 438], ["isinstance", "torch.init.kaiming_normal_", "isinstance", "torch.init.ones_", "torch.init.zeros_", "isinstance", "torch.init.normal_", "torch.init.zeros_", "hasattr", "module.zero_init_last"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet.SelectiveKernelBottleneck.zero_init_last"], ["", "", "def", "_init_weights", "(", "module", ",", "name", ",", "zero_init_last", "=", "False", ")", ":", "\n", "    ", "if", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", ":", "\n", "        ", "nn", ".", "init", ".", "kaiming_normal_", "(", "module", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "        ", "nn", ".", "init", ".", "ones_", "(", "module", ".", "weight", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "        ", "nn", ".", "init", ".", "normal_", "(", "module", ".", "weight", ",", "mean", "=", "0.0", ",", "std", "=", "0.01", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "elif", "zero_init_last", "and", "hasattr", "(", "module", ",", "'zero_init_last'", ")", ":", "\n", "        ", "module", ".", "zero_init_last", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet._create_cspnet": [[440, 449], ["kwargs.pop", "helpers.build_model_with_cfg", "variant.split", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_cspnet", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "cfg_variant", "=", "variant", ".", "split", "(", "'_'", ")", "[", "0", "]", "\n", "# NOTE: DarkNet is one of few models with stride==1 features w/ 6 out_indices [0..5]", "\n", "out_indices", "=", "kwargs", ".", "pop", "(", "'out_indices'", ",", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ",", "5", ")", "if", "'darknet'", "in", "variant", "else", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ")", "\n", "return", "build_model_with_cfg", "(", "\n", "CspNet", ",", "variant", ",", "pretrained", ",", "\n", "model_cfg", "=", "model_cfgs", "[", "cfg_variant", "]", ",", "\n", "feature_cfg", "=", "dict", "(", "flatten_sequential", "=", "True", ",", "out_indices", "=", "out_indices", ")", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.cspresnet50": [[451, 454], ["cspnet._create_cspnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet._create_cspnet"], ["", "@", "register_model", "\n", "def", "cspresnet50", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_cspnet", "(", "'cspresnet50'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.cspresnet50d": [[456, 459], ["cspnet._create_cspnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet._create_cspnet"], ["", "@", "register_model", "\n", "def", "cspresnet50d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_cspnet", "(", "'cspresnet50d'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.cspresnet50w": [[461, 464], ["cspnet._create_cspnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet._create_cspnet"], ["", "@", "register_model", "\n", "def", "cspresnet50w", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_cspnet", "(", "'cspresnet50w'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.cspresnext50": [[466, 469], ["cspnet._create_cspnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet._create_cspnet"], ["", "@", "register_model", "\n", "def", "cspresnext50", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_cspnet", "(", "'cspresnext50'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.cspresnext50_iabn": [[471, 475], ["layers.get_norm_act_layer", "cspnet._create_cspnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.get_norm_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet._create_cspnet"], ["", "@", "register_model", "\n", "def", "cspresnext50_iabn", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "norm_layer", "=", "get_norm_act_layer", "(", "'iabn'", ",", "act_layer", "=", "'leaky_relu'", ")", "\n", "return", "_create_cspnet", "(", "'cspresnext50_iabn'", ",", "pretrained", "=", "pretrained", ",", "norm_layer", "=", "norm_layer", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.cspdarknet53": [[477, 480], ["cspnet._create_cspnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet._create_cspnet"], ["", "@", "register_model", "\n", "def", "cspdarknet53", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_cspnet", "(", "'cspdarknet53'", ",", "pretrained", "=", "pretrained", ",", "block_fn", "=", "DarkBlock", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.cspdarknet53_iabn": [[482, 486], ["layers.get_norm_act_layer", "cspnet._create_cspnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.get_norm_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet._create_cspnet"], ["", "@", "register_model", "\n", "def", "cspdarknet53_iabn", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "norm_layer", "=", "get_norm_act_layer", "(", "'iabn'", ",", "act_layer", "=", "'leaky_relu'", ")", "\n", "return", "_create_cspnet", "(", "'cspdarknet53_iabn'", ",", "pretrained", "=", "pretrained", ",", "block_fn", "=", "DarkBlock", ",", "norm_layer", "=", "norm_layer", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet.darknet53": [[488, 491], ["cspnet._create_cspnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.cspnet._create_cspnet"], ["", "@", "register_model", "\n", "def", "darknet53", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_cspnet", "(", "'darknet53'", ",", "pretrained", "=", "pretrained", ",", "block_fn", "=", "DarkBlock", ",", "stage_fn", "=", "DarkStage", ",", "**", "kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.SequentialAppendList.__init__": [[166, 168], ["torch.Sequential.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "super", "(", "SequentialAppendList", ",", "self", ")", ".", "__init__", "(", "*", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.SequentialAppendList.forward": [[169, 177], ["enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "concat_list.append", "concat_list.append", "module", "module"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ",", "concat_list", ":", "List", "[", "torch", ".", "Tensor", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "for", "i", ",", "module", "in", "enumerate", "(", "self", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "concat_list", ".", "append", "(", "module", "(", "x", ")", ")", "\n", "", "else", ":", "\n", "                ", "concat_list", ".", "append", "(", "module", "(", "concat_list", "[", "-", "1", "]", ")", ")", "\n", "", "", "x", "=", "torch", ".", "cat", "(", "concat_list", ",", "dim", "=", "1", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.OsaBlock.__init__": [[181, 214], ["torch.Module.__init__", "dict", "range", "vovnet.SequentialAppendList", "layers.ConvNormAct", "layers.ConvNormAct", "mid_convs.append", "layers.create_attn", "layers.SeparableConvNormAct", "layers.ConvNormAct"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.create_attn"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "mid_chs", ",", "out_chs", ",", "layer_per_block", ",", "residual", "=", "False", ",", "\n", "depthwise", "=", "False", ",", "attn", "=", "''", ",", "norm_layer", "=", "BatchNormAct2d", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "drop_path", "=", "None", ")", ":", "\n", "        ", "super", "(", "OsaBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "residual", "=", "residual", "\n", "self", ".", "depthwise", "=", "depthwise", "\n", "conv_kwargs", "=", "dict", "(", "norm_layer", "=", "norm_layer", ",", "act_layer", "=", "act_layer", ")", "\n", "\n", "next_in_chs", "=", "in_chs", "\n", "if", "self", ".", "depthwise", "and", "next_in_chs", "!=", "mid_chs", ":", "\n", "            ", "assert", "not", "residual", "\n", "self", ".", "conv_reduction", "=", "ConvNormAct", "(", "next_in_chs", ",", "mid_chs", ",", "1", ",", "**", "conv_kwargs", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv_reduction", "=", "None", "\n", "\n", "", "mid_convs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "layer_per_block", ")", ":", "\n", "            ", "if", "self", ".", "depthwise", ":", "\n", "                ", "conv", "=", "SeparableConvNormAct", "(", "mid_chs", ",", "mid_chs", ",", "**", "conv_kwargs", ")", "\n", "", "else", ":", "\n", "                ", "conv", "=", "ConvNormAct", "(", "next_in_chs", ",", "mid_chs", ",", "3", ",", "**", "conv_kwargs", ")", "\n", "", "next_in_chs", "=", "mid_chs", "\n", "mid_convs", ".", "append", "(", "conv", ")", "\n", "", "self", ".", "conv_mid", "=", "SequentialAppendList", "(", "*", "mid_convs", ")", "\n", "\n", "# feature aggregation", "\n", "next_in_chs", "=", "in_chs", "+", "layer_per_block", "*", "mid_chs", "\n", "self", ".", "conv_concat", "=", "ConvNormAct", "(", "next_in_chs", ",", "out_chs", ",", "**", "conv_kwargs", ")", "\n", "\n", "self", ".", "attn", "=", "create_attn", "(", "attn", ",", "out_chs", ")", "if", "attn", "else", "None", "\n", "\n", "self", ".", "drop_path", "=", "drop_path", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.OsaBlock.forward": [[215, 228], ["vovnet.OsaBlock.conv_mid", "vovnet.OsaBlock.conv_concat", "vovnet.OsaBlock.conv_reduction", "vovnet.OsaBlock.attn", "vovnet.OsaBlock.drop_path"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "output", "=", "[", "x", "]", "\n", "if", "self", ".", "conv_reduction", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "conv_reduction", "(", "x", ")", "\n", "", "x", "=", "self", ".", "conv_mid", "(", "x", ",", "output", ")", "\n", "x", "=", "self", ".", "conv_concat", "(", "x", ")", "\n", "if", "self", ".", "attn", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "attn", "(", "x", ")", "\n", "", "if", "self", ".", "drop_path", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "drop_path", "(", "x", ")", "\n", "", "if", "self", ".", "residual", ":", "\n", "            ", "x", "=", "x", "+", "output", "[", "0", "]", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.OsaStage.__init__": [[232, 257], ["torch.Module.__init__", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "layers.DropPath", "vovnet.OsaBlock"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "mid_chs", ",", "out_chs", ",", "block_per_stage", ",", "layer_per_block", ",", "downsample", "=", "True", ",", "\n", "residual", "=", "True", ",", "depthwise", "=", "False", ",", "attn", "=", "'ese'", ",", "norm_layer", "=", "BatchNormAct2d", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "\n", "drop_path_rates", "=", "None", ")", ":", "\n", "        ", "super", "(", "OsaStage", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "if", "downsample", ":", "\n", "            ", "self", ".", "pool", "=", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "ceil_mode", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "pool", "=", "None", "\n", "\n", "", "blocks", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "block_per_stage", ")", ":", "\n", "            ", "last_block", "=", "i", "==", "block_per_stage", "-", "1", "\n", "if", "drop_path_rates", "is", "not", "None", "and", "drop_path_rates", "[", "i", "]", ">", "0.", ":", "\n", "                ", "drop_path", "=", "DropPath", "(", "drop_path_rates", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "                ", "drop_path", "=", "None", "\n", "", "blocks", "+=", "[", "OsaBlock", "(", "\n", "in_chs", ",", "mid_chs", ",", "out_chs", ",", "layer_per_block", ",", "residual", "=", "residual", "and", "i", ">", "0", ",", "depthwise", "=", "depthwise", ",", "\n", "attn", "=", "attn", "if", "last_block", "else", "''", ",", "norm_layer", "=", "norm_layer", ",", "act_layer", "=", "act_layer", ",", "drop_path", "=", "drop_path", ")", "\n", "]", "\n", "in_chs", "=", "out_chs", "\n", "", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "*", "blocks", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.OsaStage.forward": [[258, 266], ["vovnet.OsaStage.pool", "helpers.checkpoint_seq", "vovnet.OsaStage.blocks", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "pool", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "pool", "(", "x", ")", "\n", "", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "blocks", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "blocks", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.VovNet.__init__": [[270, 324], ["torch.Module.__init__", "dict", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "dict", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "layers.ClassifierHead", "vovnet.VovNet.named_modules", "dict", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "isinstance", "sum", "vovnet.OsaStage", "dict", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "isinstance", "layers.ConvNormAct", "conv_type", "conv_type", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_modules"], ["    ", "def", "__init__", "(", "\n", "self", ",", "cfg", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "global_pool", "=", "'avg'", ",", "drop_rate", "=", "0.", ",", "stem_stride", "=", "4", ",", "\n", "output_stride", "=", "32", ",", "norm_layer", "=", "BatchNormAct2d", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "        ", "\"\"\" VovNet (v2)\n        \"\"\"", "\n", "super", "(", "VovNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "assert", "stem_stride", "in", "(", "4", ",", "2", ")", "\n", "assert", "output_stride", "==", "32", "# FIXME support dilation", "\n", "\n", "stem_chs", "=", "cfg", "[", "\"stem_chs\"", "]", "\n", "stage_conv_chs", "=", "cfg", "[", "\"stage_conv_chs\"", "]", "\n", "stage_out_chs", "=", "cfg", "[", "\"stage_out_chs\"", "]", "\n", "block_per_stage", "=", "cfg", "[", "\"block_per_stage\"", "]", "\n", "layer_per_block", "=", "cfg", "[", "\"layer_per_block\"", "]", "\n", "conv_kwargs", "=", "dict", "(", "norm_layer", "=", "norm_layer", ",", "act_layer", "=", "act_layer", ")", "\n", "\n", "# Stem module", "\n", "last_stem_stride", "=", "stem_stride", "//", "2", "\n", "conv_type", "=", "SeparableConvNormAct", "if", "cfg", "[", "\"depthwise\"", "]", "else", "ConvNormAct", "\n", "self", ".", "stem", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "ConvNormAct", "(", "in_chans", ",", "stem_chs", "[", "0", "]", ",", "3", ",", "stride", "=", "2", ",", "**", "conv_kwargs", ")", ",", "\n", "conv_type", "(", "stem_chs", "[", "0", "]", ",", "stem_chs", "[", "1", "]", ",", "3", ",", "stride", "=", "1", ",", "**", "conv_kwargs", ")", ",", "\n", "conv_type", "(", "stem_chs", "[", "1", "]", ",", "stem_chs", "[", "2", "]", ",", "3", ",", "stride", "=", "last_stem_stride", ",", "**", "conv_kwargs", ")", ",", "\n", "]", ")", "\n", "self", ".", "feature_info", "=", "[", "dict", "(", "\n", "num_chs", "=", "stem_chs", "[", "1", "]", ",", "reduction", "=", "2", ",", "module", "=", "f'stem.{1 if stem_stride == 4 else 2}'", ")", "]", "\n", "current_stride", "=", "stem_stride", "\n", "\n", "# OSA stages", "\n", "stage_dpr", "=", "torch", ".", "split", "(", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "sum", "(", "block_per_stage", ")", ")", ",", "block_per_stage", ")", "\n", "in_ch_list", "=", "stem_chs", "[", "-", "1", ":", "]", "+", "stage_out_chs", "[", ":", "-", "1", "]", "\n", "stage_args", "=", "dict", "(", "residual", "=", "cfg", "[", "\"residual\"", "]", ",", "depthwise", "=", "cfg", "[", "\"depthwise\"", "]", ",", "attn", "=", "cfg", "[", "\"attn\"", "]", ",", "**", "conv_kwargs", ")", "\n", "stages", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "4", ")", ":", "# num_stages", "\n", "            ", "downsample", "=", "stem_stride", "==", "2", "or", "i", ">", "0", "# first stage has no stride/downsample if stem_stride is 4", "\n", "stages", "+=", "[", "OsaStage", "(", "\n", "in_ch_list", "[", "i", "]", ",", "stage_conv_chs", "[", "i", "]", ",", "stage_out_chs", "[", "i", "]", ",", "block_per_stage", "[", "i", "]", ",", "layer_per_block", ",", "\n", "downsample", "=", "downsample", ",", "drop_path_rates", "=", "stage_dpr", "[", "i", "]", ",", "**", "stage_args", ")", "\n", "]", "\n", "self", ".", "num_features", "=", "stage_out_chs", "[", "i", "]", "\n", "current_stride", "*=", "2", "if", "downsample", "else", "1", "\n", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "self", ".", "num_features", ",", "reduction", "=", "current_stride", ",", "module", "=", "f'stages.{i}'", ")", "]", "\n", "\n", "", "self", ".", "stages", "=", "nn", ".", "Sequential", "(", "*", "stages", ")", "\n", "\n", "self", ".", "head", "=", "ClassifierHead", "(", "self", ".", "num_features", ",", "num_classes", ",", "pool_type", "=", "global_pool", ",", "drop_rate", "=", "drop_rate", ")", "\n", "\n", "for", "n", ",", "m", "in", "self", ".", "named_modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "nn", ".", "init", ".", "zeros_", "(", "m", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.VovNet.group_matcher": [[326, 331], ["dict"], "methods", ["None"], ["", "", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^stem'", ",", "\n", "blocks", "=", "r'^stages\\.(\\d+)'", "if", "coarse", "else", "r'^stages\\.(\\d+).blocks\\.(\\d+)'", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.VovNet.set_grad_checkpointing": [[333, 337], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "for", "s", "in", "self", ".", "stages", ":", "\n", "            ", "s", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.VovNet.get_classifier": [[338, 341], ["None"], "methods", ["None"], ["", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", ".", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.VovNet.reset_classifier": [[342, 344], ["layers.ClassifierHead"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "head", "=", "ClassifierHead", "(", "self", ".", "num_features", ",", "num_classes", ",", "pool_type", "=", "global_pool", ",", "drop_rate", "=", "self", ".", "drop_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.VovNet.forward_features": [[345, 348], ["vovnet.VovNet.stem", "vovnet.VovNet.stages"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet.stages"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "stem", "(", "x", ")", "\n", "return", "self", ".", "stages", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.VovNet.forward_head": [[349, 351], ["vovnet.VovNet.head"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "head", "(", "x", ",", "pre_logits", "=", "pre_logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.VovNet.forward": [[352, 356], ["vovnet.VovNet.forward_features", "vovnet.VovNet.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet._cfg": [[139, 145], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bicubic'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'stem.0.conv'", ",", "'classifier'", ":", "'head.fc'", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet._create_vovnet": [[358, 364], ["helpers.build_model_with_cfg", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_vovnet", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "\n", "VovNet", ",", "variant", ",", "pretrained", ",", "\n", "model_cfg", "=", "model_cfgs", "[", "variant", "]", ",", "\n", "feature_cfg", "=", "dict", "(", "flatten_sequential", "=", "True", ")", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.vovnet39a": [[366, 369], ["vovnet._create_vovnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet._create_vovnet"], ["", "@", "register_model", "\n", "def", "vovnet39a", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_vovnet", "(", "'vovnet39a'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.vovnet57a": [[371, 374], ["vovnet._create_vovnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet._create_vovnet"], ["", "@", "register_model", "\n", "def", "vovnet57a", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_vovnet", "(", "'vovnet57a'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.ese_vovnet19b_slim_dw": [[376, 379], ["vovnet._create_vovnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet._create_vovnet"], ["", "@", "register_model", "\n", "def", "ese_vovnet19b_slim_dw", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_vovnet", "(", "'ese_vovnet19b_slim_dw'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.ese_vovnet19b_dw": [[381, 384], ["vovnet._create_vovnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet._create_vovnet"], ["", "@", "register_model", "\n", "def", "ese_vovnet19b_dw", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_vovnet", "(", "'ese_vovnet19b_dw'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.ese_vovnet19b_slim": [[386, 389], ["vovnet._create_vovnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet._create_vovnet"], ["", "@", "register_model", "\n", "def", "ese_vovnet19b_slim", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_vovnet", "(", "'ese_vovnet19b_slim'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.ese_vovnet39b": [[391, 394], ["vovnet._create_vovnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet._create_vovnet"], ["", "@", "register_model", "\n", "def", "ese_vovnet39b", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_vovnet", "(", "'ese_vovnet39b'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.ese_vovnet57b": [[396, 399], ["vovnet._create_vovnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet._create_vovnet"], ["", "@", "register_model", "\n", "def", "ese_vovnet57b", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_vovnet", "(", "'ese_vovnet57b'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.ese_vovnet99b": [[401, 404], ["vovnet._create_vovnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet._create_vovnet"], ["", "@", "register_model", "\n", "def", "ese_vovnet99b", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_vovnet", "(", "'ese_vovnet99b'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.eca_vovnet39b": [[406, 409], ["vovnet._create_vovnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet._create_vovnet"], ["", "@", "register_model", "\n", "def", "eca_vovnet39b", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_create_vovnet", "(", "'eca_vovnet39b'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.ese_vovnet39b_evos": [[413, 418], ["vovnet._create_vovnet", "layers.create_norm_act_layer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet._create_vovnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.create_norm_act_layer"], ["", "@", "register_model", "\n", "def", "ese_vovnet39b_evos", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "def", "norm_act_fn", "(", "num_features", ",", "**", "nkwargs", ")", ":", "\n", "        ", "return", "create_norm_act_layer", "(", "'evonorms0'", ",", "num_features", ",", "jit", "=", "False", ",", "**", "nkwargs", ")", "\n", "", "return", "_create_vovnet", "(", "'ese_vovnet39b_evos'", ",", "pretrained", "=", "pretrained", ",", "norm_layer", "=", "norm_act_fn", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet.ese_vovnet99b_iabn": [[420, 425], ["layers.get_norm_act_layer", "vovnet._create_vovnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.get_norm_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vovnet._create_vovnet"], ["", "@", "register_model", "\n", "def", "ese_vovnet99b_iabn", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "norm_layer", "=", "get_norm_act_layer", "(", "'iabn'", ",", "act_layer", "=", "'leaky_relu'", ")", "\n", "return", "_create_vovnet", "(", "\n", "'ese_vovnet99b_iabn'", ",", "pretrained", "=", "pretrained", ",", "norm_layer", "=", "norm_layer", ",", "act_layer", "=", "nn", ".", "LeakyReLU", ",", "**", "kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt.Attention.__init__": [[48, 61], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "dim", ",", "hidden_dim", ",", "num_heads", "=", "8", ",", "qkv_bias", "=", "False", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "head_dim", "=", "hidden_dim", "//", "num_heads", "\n", "self", ".", "head_dim", "=", "head_dim", "\n", "self", ".", "scale", "=", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "qk", "=", "nn", ".", "Linear", "(", "dim", ",", "hidden_dim", "*", "2", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "v", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ",", "inplace", "=", "True", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ",", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt.Attention.forward": [[62, 76], ["tnt.Attention.qk().reshape().permute", "tnt.Attention.unbind", "tnt.Attention.v().reshape().permute", "tnt.Attention.softmax", "tnt.Attention.attn_drop", "tnt.Attention.proj", "tnt.Attention.proj_drop", "tnt.Attention.qk().reshape", "tnt.Attention.v().reshape", "k.transpose", "tnt.Attention.qk", "tnt.Attention.v"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "qk", "=", "self", ".", "qk", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "2", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "q", ",", "k", "=", "qk", ".", "unbind", "(", "0", ")", "# make torchscript happy (cannot use tensor as tuple)", "\n", "v", "=", "self", ".", "v", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "self", ".", "num_heads", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "scale", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "N", ",", "-", "1", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt.Block.__init__": [[81, 107], ["torch.Module.__init__", "norm_layer", "tnt.Attention", "norm_layer", "timm.models.layers.Mlp", "norm_layer", "torch.Linear", "torch.Linear", "norm_layer", "tnt.Attention", "norm_layer", "timm.models.layers.Mlp", "timm.models.layers.DropPath", "torch.Identity", "torch.Identity", "int", "int"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "dim", ",", "in_dim", ",", "num_pixel", ",", "num_heads", "=", "12", ",", "in_num_head", "=", "4", ",", "mlp_ratio", "=", "4.", ",", "\n", "qkv_bias", "=", "False", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Inner transformer", "\n", "self", ".", "norm_in", "=", "norm_layer", "(", "in_dim", ")", "\n", "self", ".", "attn_in", "=", "Attention", "(", "\n", "in_dim", ",", "in_dim", ",", "num_heads", "=", "in_num_head", ",", "qkv_bias", "=", "qkv_bias", ",", "\n", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ")", "\n", "\n", "self", ".", "norm_mlp_in", "=", "norm_layer", "(", "in_dim", ")", "\n", "self", ".", "mlp_in", "=", "Mlp", "(", "in_features", "=", "in_dim", ",", "hidden_features", "=", "int", "(", "in_dim", "*", "4", ")", ",", "\n", "out_features", "=", "in_dim", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "\n", "self", ".", "norm1_proj", "=", "norm_layer", "(", "in_dim", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "in_dim", "*", "num_pixel", ",", "dim", ",", "bias", "=", "True", ")", "\n", "# Outer transformer", "\n", "self", ".", "norm_out", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "attn_out", "=", "Attention", "(", "\n", "dim", ",", "dim", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "\n", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "norm_mlp", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "int", "(", "dim", "*", "mlp_ratio", ")", ",", "\n", "out_features", "=", "dim", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt.Block.forward": [[108, 120], ["torch.cat.size", "torch.cat.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "tnt.Block.drop_path", "tnt.Block.drop_path", "tnt.Block.drop_path", "tnt.Block.drop_path", "tnt.Block.attn_in", "tnt.Block.mlp_in", "tnt.Block.attn_out", "tnt.Block.mlp", "tnt.Block.norm_in", "tnt.Block.norm_mlp_in", "tnt.Block.proj", "tnt.Block.norm_out", "tnt.Block.norm_mlp", "tnt.Block.norm1_proj().reshape", "tnt.Block.norm1_proj"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "pixel_embed", ",", "patch_embed", ")", ":", "\n", "# inner", "\n", "        ", "pixel_embed", "=", "pixel_embed", "+", "self", ".", "drop_path", "(", "self", ".", "attn_in", "(", "self", ".", "norm_in", "(", "pixel_embed", ")", ")", ")", "\n", "pixel_embed", "=", "pixel_embed", "+", "self", ".", "drop_path", "(", "self", ".", "mlp_in", "(", "self", ".", "norm_mlp_in", "(", "pixel_embed", ")", ")", ")", "\n", "# outer", "\n", "B", ",", "N", ",", "C", "=", "patch_embed", ".", "size", "(", ")", "\n", "patch_embed", "=", "torch", ".", "cat", "(", "\n", "[", "patch_embed", "[", ":", ",", "0", ":", "1", "]", ",", "patch_embed", "[", ":", ",", "1", ":", "]", "+", "self", ".", "proj", "(", "self", ".", "norm1_proj", "(", "pixel_embed", ")", ".", "reshape", "(", "B", ",", "N", "-", "1", ",", "-", "1", ")", ")", "]", ",", "\n", "dim", "=", "1", ")", "\n", "patch_embed", "=", "patch_embed", "+", "self", ".", "drop_path", "(", "self", ".", "attn_out", "(", "self", ".", "norm_out", "(", "patch_embed", ")", ")", ")", "\n", "patch_embed", "=", "patch_embed", "+", "self", ".", "drop_path", "(", "self", ".", "mlp", "(", "self", ".", "norm_mlp", "(", "patch_embed", ")", ")", ")", "\n", "return", "pixel_embed", ",", "patch_embed", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt.PixelEmbed.__init__": [[125, 140], ["torch.Module.__init__", "timm.models.layers.helpers.to_2tuple", "timm.models.layers.helpers.to_2tuple", "torch.Conv2d", "torch.Conv2d", "torch.Unfold", "torch.Unfold", "math.ceil"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "in_dim", "=", "48", ",", "stride", "=", "4", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "img_size", "=", "to_2tuple", "(", "img_size", ")", "\n", "patch_size", "=", "to_2tuple", "(", "patch_size", ")", "\n", "# grid_size property necessary for resizing positional embedding", "\n", "self", ".", "grid_size", "=", "(", "img_size", "[", "0", "]", "//", "patch_size", "[", "0", "]", ",", "img_size", "[", "1", "]", "//", "patch_size", "[", "1", "]", ")", "\n", "num_patches", "=", "(", "self", ".", "grid_size", "[", "0", "]", ")", "*", "(", "self", ".", "grid_size", "[", "1", "]", ")", "\n", "self", ".", "img_size", "=", "img_size", "\n", "self", ".", "num_patches", "=", "num_patches", "\n", "self", ".", "in_dim", "=", "in_dim", "\n", "new_patch_size", "=", "[", "math", ".", "ceil", "(", "ps", "/", "stride", ")", "for", "ps", "in", "patch_size", "]", "\n", "self", ".", "new_patch_size", "=", "new_patch_size", "\n", "\n", "self", ".", "proj", "=", "nn", ".", "Conv2d", "(", "in_chans", ",", "self", ".", "in_dim", ",", "kernel_size", "=", "7", ",", "padding", "=", "3", ",", "stride", "=", "stride", ")", "\n", "self", ".", "unfold", "=", "nn", ".", "Unfold", "(", "kernel_size", "=", "new_patch_size", ",", "stride", "=", "new_patch_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt.PixelEmbed.forward": [[141, 153], ["timm.models.layers._assert", "timm.models.layers._assert", "tnt.PixelEmbed.proj", "tnt.PixelEmbed.unfold", "x.reshape().transpose.reshape().transpose.transpose().reshape", "x.reshape().transpose.reshape().transpose.reshape().transpose", "x.reshape().transpose.reshape().transpose.transpose", "x.reshape().transpose.reshape().transpose.reshape"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "pixel_pos", ")", ":", "\n", "        ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "_assert", "(", "H", "==", "self", ".", "img_size", "[", "0", "]", ",", "\n", "f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"", ")", "\n", "_assert", "(", "W", "==", "self", ".", "img_size", "[", "1", "]", ",", "\n", "f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "unfold", "(", "x", ")", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", "*", "self", ".", "num_patches", ",", "self", ".", "in_dim", ",", "self", ".", "new_patch_size", "[", "0", "]", ",", "self", ".", "new_patch_size", "[", "1", "]", ")", "\n", "x", "=", "x", "+", "pixel_pos", "\n", "x", "=", "x", ".", "reshape", "(", "B", "*", "self", ".", "num_patches", ",", "self", ".", "in_dim", ",", "-", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt.TNT.__init__": [[158, 201], ["torch.Module.__init__", "tnt.PixelEmbed", "norm_layer", "torch.Linear", "torch.Linear", "norm_layer", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Dropout", "torch.Dropout", "range", "torch.ModuleList", "torch.ModuleList", "norm_layer", "timm.models.layers.trunc_normal_", "timm.models.layers.trunc_normal_", "timm.models.layers.trunc_normal_", "tnt.TNT.apply", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "x.item", "blocks.append", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "tnt.Block"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["def", "__init__", "(", "\n", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "global_pool", "=", "'token'", ",", "\n", "embed_dim", "=", "768", ",", "in_dim", "=", "48", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "in_num_head", "=", "4", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "\n", "drop_rate", "=", "0.", ",", "attn_drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ",", "first_stride", "=", "4", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "global_pool", "in", "(", "''", ",", "'token'", ",", "'avg'", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "self", ".", "num_features", "=", "self", ".", "embed_dim", "=", "embed_dim", "# num_features for consistency with other models", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "self", ".", "pixel_embed", "=", "PixelEmbed", "(", "\n", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", ",", "in_chans", "=", "in_chans", ",", "in_dim", "=", "in_dim", ",", "stride", "=", "first_stride", ")", "\n", "num_patches", "=", "self", ".", "pixel_embed", ".", "num_patches", "\n", "self", ".", "num_patches", "=", "num_patches", "\n", "new_patch_size", "=", "self", ".", "pixel_embed", ".", "new_patch_size", "\n", "num_pixel", "=", "new_patch_size", "[", "0", "]", "*", "new_patch_size", "[", "1", "]", "\n", "\n", "self", ".", "norm1_proj", "=", "norm_layer", "(", "num_pixel", "*", "in_dim", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "num_pixel", "*", "in_dim", ",", "embed_dim", ")", "\n", "self", ".", "norm2_proj", "=", "norm_layer", "(", "embed_dim", ")", "\n", "\n", "self", ".", "cls_token", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "self", ".", "patch_pos", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "num_patches", "+", "1", ",", "embed_dim", ")", ")", "\n", "self", ".", "pixel_pos", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "in_dim", ",", "new_patch_size", "[", "0", "]", ",", "new_patch_size", "[", "1", "]", ")", ")", "\n", "self", ".", "pos_drop", "=", "nn", ".", "Dropout", "(", "p", "=", "drop_rate", ")", "\n", "\n", "dpr", "=", "[", "x", ".", "item", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "depth", ")", "]", "# stochastic depth decay rule", "\n", "blocks", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "depth", ")", ":", "\n", "            ", "blocks", ".", "append", "(", "Block", "(", "\n", "dim", "=", "embed_dim", ",", "in_dim", "=", "in_dim", ",", "num_pixel", "=", "num_pixel", ",", "num_heads", "=", "num_heads", ",", "in_num_head", "=", "in_num_head", ",", "\n", "mlp_ratio", "=", "mlp_ratio", ",", "qkv_bias", "=", "qkv_bias", ",", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "\n", "drop_path", "=", "dpr", "[", "i", "]", ",", "norm_layer", "=", "norm_layer", ")", ")", "\n", "", "self", ".", "blocks", "=", "nn", ".", "ModuleList", "(", "blocks", ")", "\n", "self", ".", "norm", "=", "norm_layer", "(", "embed_dim", ")", "\n", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "trunc_normal_", "(", "self", ".", "cls_token", ",", "std", "=", ".02", ")", "\n", "trunc_normal_", "(", "self", ".", "patch_pos", ",", "std", "=", ".02", ")", "\n", "trunc_normal_", "(", "self", ".", "pixel_pos", ",", "std", "=", ".02", ")", "\n", "self", ".", "apply", "(", "self", ".", "_init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt.TNT._init_weights": [[202, 210], ["isinstance", "timm.models.layers.trunc_normal_", "isinstance", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["", "def", "_init_weights", "(", "self", ",", "m", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "            ", "trunc_normal_", "(", "m", ".", "weight", ",", "std", "=", ".02", ")", "\n", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt.TNT.no_weight_decay": [[211, 214], ["None"], "methods", ["None"], ["", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "no_weight_decay", "(", "self", ")", ":", "\n", "        ", "return", "{", "'patch_pos'", ",", "'pixel_pos'", ",", "'cls_token'", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt.TNT.group_matcher": [[215, 225], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "\n", "stem", "=", "r'^cls_token|patch_pos|pixel_pos|pixel_embed|norm[12]_proj|proj'", ",", "# stem and embed / pos", "\n", "blocks", "=", "[", "\n", "(", "r'^blocks\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^norm'", ",", "(", "99999", ",", ")", ")", ",", "\n", "]", "\n", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt.TNT.set_grad_checkpointing": [[226, 229], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt.TNT.get_classifier": [[230, 233], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt.TNT.reset_classifier": [[234, 239], ["torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "assert", "global_pool", "in", "(", "''", ",", "'token'", ",", "'avg'", ")", "\n", "", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt.TNT.forward_features": [[240, 258], ["tnt.TNT.pixel_embed", "tnt.TNT.norm2_proj", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "tnt.TNT.pos_drop", "tnt.TNT.norm", "tnt.TNT.proj", "tnt.TNT.norm1_proj", "tnt.TNT.cls_token.expand", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "blk", "tnt.TNT.reshape"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "pixel_embed", "=", "self", ".", "pixel_embed", "(", "x", ",", "self", ".", "pixel_pos", ")", "\n", "\n", "patch_embed", "=", "self", ".", "norm2_proj", "(", "self", ".", "proj", "(", "self", ".", "norm1_proj", "(", "pixel_embed", ".", "reshape", "(", "B", ",", "self", ".", "num_patches", ",", "-", "1", ")", ")", ")", ")", "\n", "patch_embed", "=", "torch", ".", "cat", "(", "(", "self", ".", "cls_token", ".", "expand", "(", "B", ",", "-", "1", ",", "-", "1", ")", ",", "patch_embed", ")", ",", "dim", "=", "1", ")", "\n", "patch_embed", "=", "patch_embed", "+", "self", ".", "patch_pos", "\n", "patch_embed", "=", "self", ".", "pos_drop", "(", "patch_embed", ")", "\n", "\n", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "for", "blk", "in", "self", ".", "blocks", ":", "\n", "                ", "pixel_embed", ",", "patch_embed", "=", "checkpoint", "(", "blk", ",", "pixel_embed", ",", "patch_embed", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "blk", "in", "self", ".", "blocks", ":", "\n", "                ", "pixel_embed", ",", "patch_embed", "=", "blk", "(", "pixel_embed", ",", "patch_embed", ")", "\n", "\n", "", "", "patch_embed", "=", "self", ".", "norm", "(", "patch_embed", ")", "\n", "return", "patch_embed", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt.TNT.forward_head": [[259, 263], ["tnt.TNT.head", "x[].mean"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "global_pool", ":", "\n", "            ", "x", "=", "x", "[", ":", ",", "1", ":", "]", ".", "mean", "(", "dim", "=", "1", ")", "if", "self", ".", "global_pool", "==", "'avg'", "else", "x", "[", ":", ",", "0", "]", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt.TNT.forward": [[264, 268], ["tnt.TNT.forward_features", "tnt.TNT.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt._cfg": [[23, 31], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", ".9", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'pixel_embed.proj'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt.checkpoint_filter_fn": [[270, 276], ["timm.models.vision_transformer.resize_pos_embed", "getattr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.resize_pos_embed"], ["", "", "def", "checkpoint_filter_fn", "(", "state_dict", ",", "model", ")", ":", "\n", "    ", "\"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"", "\n", "if", "state_dict", "[", "'patch_pos'", "]", ".", "shape", "!=", "model", ".", "patch_pos", ".", "shape", ":", "\n", "        ", "state_dict", "[", "'patch_pos'", "]", "=", "resize_pos_embed", "(", "state_dict", "[", "'patch_pos'", "]", ",", "\n", "model", ".", "patch_pos", ",", "getattr", "(", "model", ",", "'num_tokens'", ",", "1", ")", ",", "model", ".", "pixel_embed", ".", "grid_size", ")", "\n", "", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt._create_tnt": [[278, 287], ["kwargs.get", "timm.models.helpers.build_model_with_cfg", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "def", "_create_tnt", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "kwargs", ".", "get", "(", "'features_only'", ",", "None", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'features_only not implemented for Vision Transformer models.'", ")", "\n", "\n", "", "model", "=", "build_model_with_cfg", "(", "\n", "TNT", ",", "variant", ",", "pretrained", ",", "\n", "pretrained_filter_fn", "=", "checkpoint_filter_fn", ",", "\n", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt.tnt_s_patch16_224": [[289, 296], ["dict", "tnt._create_tnt"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt._create_tnt"], ["", "@", "register_model", "\n", "def", "tnt_s_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_cfg", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "in_dim", "=", "24", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "in_num_head", "=", "4", ",", "\n", "qkv_bias", "=", "False", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_tnt", "(", "'tnt_s_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_cfg", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt.tnt_b_patch16_224": [[298, 305], ["dict", "tnt._create_tnt"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tnt._create_tnt"], ["", "@", "register_model", "\n", "def", "tnt_b_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_cfg", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "640", ",", "in_dim", "=", "40", ",", "depth", "=", "12", ",", "num_heads", "=", "10", ",", "in_num_head", "=", "4", ",", "\n", "qkv_bias", "=", "False", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_tnt", "(", "'tnt_b_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_cfg", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.Bottleneck.__init__": [[250, 273], ["torch.Module.__init__", "layers.get_act_layer", "int", "dict", "layers.ConvNormAct", "layers.ConvNormAct", "layers.ConvNormAct", "regnet.create_shortcut", "round", "int", "layers.SEModule", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "layers.get_act_layer.", "layers.DropPath", "torch.Identity", "torch.Identity", "round"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.create_shortcut"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "stride", "=", "1", ",", "dilation", "=", "(", "1", ",", "1", ")", ",", "bottle_ratio", "=", "1", ",", "group_size", "=", "1", ",", "se_ratio", "=", "0.25", ",", "\n", "downsample", "=", "'conv1x1'", ",", "linear_out", "=", "False", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "\n", "drop_block", "=", "None", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "        ", "super", "(", "Bottleneck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "act_layer", "=", "get_act_layer", "(", "act_layer", ")", "\n", "bottleneck_chs", "=", "int", "(", "round", "(", "out_chs", "*", "bottle_ratio", ")", ")", "\n", "groups", "=", "bottleneck_chs", "//", "group_size", "\n", "\n", "cargs", "=", "dict", "(", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "conv1", "=", "ConvNormAct", "(", "in_chs", ",", "bottleneck_chs", ",", "kernel_size", "=", "1", ",", "**", "cargs", ")", "\n", "self", ".", "conv2", "=", "ConvNormAct", "(", "\n", "bottleneck_chs", ",", "bottleneck_chs", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", "[", "0", "]", ",", "\n", "groups", "=", "groups", ",", "drop_layer", "=", "drop_block", ",", "**", "cargs", ")", "\n", "if", "se_ratio", ":", "\n", "            ", "se_channels", "=", "int", "(", "round", "(", "in_chs", "*", "se_ratio", ")", ")", "\n", "self", ".", "se", "=", "SEModule", "(", "bottleneck_chs", ",", "rd_channels", "=", "se_channels", ",", "act_layer", "=", "act_layer", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "se", "=", "nn", ".", "Identity", "(", ")", "\n", "", "self", ".", "conv3", "=", "ConvNormAct", "(", "bottleneck_chs", ",", "out_chs", ",", "kernel_size", "=", "1", ",", "apply_act", "=", "False", ",", "**", "cargs", ")", "\n", "self", ".", "act3", "=", "nn", ".", "Identity", "(", ")", "if", "linear_out", "else", "act_layer", "(", ")", "\n", "self", ".", "downsample", "=", "create_shortcut", "(", "downsample", ",", "in_chs", ",", "out_chs", ",", "1", ",", "stride", ",", "dilation", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path_rate", ")", "if", "drop_path_rate", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.Bottleneck.zero_init_last": [[274, 276], ["torch.init.zeros_", "torch.init.zeros_"], "methods", ["None"], ["", "def", "zero_init_last", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "conv3", ".", "bn", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.Bottleneck.forward": [[277, 289], ["regnet.Bottleneck.conv1", "regnet.Bottleneck.conv2", "regnet.Bottleneck.se", "regnet.Bottleneck.conv3", "regnet.Bottleneck.act3", "regnet.Bottleneck.drop_path", "regnet.Bottleneck.downsample"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "x", "=", "self", ".", "se", "(", "x", ")", "\n", "x", "=", "self", ".", "conv3", "(", "x", ")", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "# NOTE stuck with downsample as the attr name due to weight compatibility", "\n", "# now represents the shortcut, no shortcut if None, and non-downsample shortcut == nn.Identity()", "\n", "            ", "x", "=", "self", ".", "drop_path", "(", "x", ")", "+", "self", ".", "downsample", "(", "shortcut", ")", "\n", "", "x", "=", "self", ".", "act3", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.PreBottleneck.__init__": [[298, 321], ["torch.Module.__init__", "layers.get_norm_act_layer", "int", "layers.get_norm_act_layer.", "layers.create_conv2d", "layers.get_norm_act_layer.", "layers.create_conv2d", "layers.get_norm_act_layer.", "layers.create_conv2d", "regnet.create_shortcut", "round", "int", "layers.SEModule", "torch.Identity", "torch.Identity", "layers.DropPath", "torch.Identity", "torch.Identity", "round"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.get_norm_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.create_shortcut"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "stride", "=", "1", ",", "dilation", "=", "(", "1", ",", "1", ")", ",", "bottle_ratio", "=", "1", ",", "group_size", "=", "1", ",", "se_ratio", "=", "0.25", ",", "\n", "downsample", "=", "'conv1x1'", ",", "linear_out", "=", "False", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "\n", "drop_block", "=", "None", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "        ", "super", "(", "PreBottleneck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "norm_act_layer", "=", "get_norm_act_layer", "(", "norm_layer", ",", "act_layer", ")", "\n", "bottleneck_chs", "=", "int", "(", "round", "(", "out_chs", "*", "bottle_ratio", ")", ")", "\n", "groups", "=", "bottleneck_chs", "//", "group_size", "\n", "\n", "self", ".", "norm1", "=", "norm_act_layer", "(", "in_chs", ")", "\n", "self", ".", "conv1", "=", "create_conv2d", "(", "in_chs", ",", "bottleneck_chs", ",", "kernel_size", "=", "1", ")", "\n", "self", ".", "norm2", "=", "norm_act_layer", "(", "bottleneck_chs", ")", "\n", "self", ".", "conv2", "=", "create_conv2d", "(", "\n", "bottleneck_chs", ",", "bottleneck_chs", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", "[", "0", "]", ",", "groups", "=", "groups", ")", "\n", "if", "se_ratio", ":", "\n", "            ", "se_channels", "=", "int", "(", "round", "(", "in_chs", "*", "se_ratio", ")", ")", "\n", "self", ".", "se", "=", "SEModule", "(", "bottleneck_chs", ",", "rd_channels", "=", "se_channels", ",", "act_layer", "=", "act_layer", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "se", "=", "nn", ".", "Identity", "(", ")", "\n", "", "self", ".", "norm3", "=", "norm_act_layer", "(", "bottleneck_chs", ")", "\n", "self", ".", "conv3", "=", "create_conv2d", "(", "bottleneck_chs", ",", "out_chs", ",", "kernel_size", "=", "1", ")", "\n", "self", ".", "downsample", "=", "create_shortcut", "(", "downsample", ",", "in_chs", ",", "out_chs", ",", "1", ",", "stride", ",", "dilation", ",", "preact", "=", "True", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path_rate", ")", "if", "drop_path_rate", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.PreBottleneck.zero_init_last": [[322, 324], ["None"], "methods", ["None"], ["", "def", "zero_init_last", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.PreBottleneck.forward": [[325, 339], ["regnet.PreBottleneck.norm1", "regnet.PreBottleneck.conv1", "regnet.PreBottleneck.norm2", "regnet.PreBottleneck.conv2", "regnet.PreBottleneck.se", "regnet.PreBottleneck.norm3", "regnet.PreBottleneck.conv3", "regnet.PreBottleneck.drop_path", "regnet.PreBottleneck.downsample"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "norm1", "(", "x", ")", "\n", "shortcut", "=", "x", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "norm2", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "x", "=", "self", ".", "se", "(", "x", ")", "\n", "x", "=", "self", ".", "norm3", "(", "x", ")", "\n", "x", "=", "self", ".", "conv3", "(", "x", ")", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "# NOTE stuck with downsample as the attr name due to weight compatibility", "\n", "# now represents the shortcut, no shortcut if None, and non-downsample shortcut == nn.Identity()", "\n", "            ", "x", "=", "self", ".", "drop_path", "(", "x", ")", "+", "self", ".", "downsample", "(", "shortcut", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.RegStage.__init__": [[344, 363], ["torch.Module.__init__", "range", "regnet.RegStage.add_module", "block_fn"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "depth", ",", "in_chs", ",", "out_chs", ",", "stride", ",", "dilation", ",", "\n", "drop_path_rates", "=", "None", ",", "block_fn", "=", "Bottleneck", ",", "**", "block_kwargs", ")", ":", "\n", "        ", "super", "(", "RegStage", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "first_dilation", "=", "1", "if", "dilation", "in", "(", "1", ",", "2", ")", "else", "2", "\n", "for", "i", "in", "range", "(", "depth", ")", ":", "\n", "            ", "block_stride", "=", "stride", "if", "i", "==", "0", "else", "1", "\n", "block_in_chs", "=", "in_chs", "if", "i", "==", "0", "else", "out_chs", "\n", "block_dilation", "=", "(", "first_dilation", ",", "dilation", ")", "\n", "dpr", "=", "drop_path_rates", "[", "i", "]", "if", "drop_path_rates", "is", "not", "None", "else", "0.", "\n", "name", "=", "\"b{}\"", ".", "format", "(", "i", "+", "1", ")", "\n", "self", ".", "add_module", "(", "\n", "name", ",", "block_fn", "(", "\n", "block_in_chs", ",", "out_chs", ",", "stride", "=", "block_stride", ",", "dilation", "=", "block_dilation", ",", "\n", "drop_path_rate", "=", "dpr", ",", "**", "block_kwargs", ")", "\n", ")", "\n", "first_dilation", "=", "dilation", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.RegStage.forward": [[364, 371], ["helpers.checkpoint_seq", "regnet.RegStage.children", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "regnet.RegStage.children", "block"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "children", "(", ")", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "for", "block", "in", "self", ".", "children", "(", ")", ":", "\n", "                ", "x", "=", "block", "(", "x", ")", "\n", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.RegNet.__init__": [[380, 423], ["torch.Module.__init__", "dict", "regnet.RegNet._get_stage_args", "enumerate", "layers.ClassifierHead", "helpers.named_apply", "layers.create_conv2d", "layers.ConvNormAct", "dict", "len", "regnet.RegNet.add_module", "layers.ConvNormAct", "functools.partial", "regnet.RegStage", "dict", "torch.Identity", "torch.Identity", "layers.get_act_layer"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.RegNet._get_stage_args", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_apply", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_layer"], ["def", "__init__", "(", "\n", "self", ",", "cfg", ":", "RegNetCfg", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "output_stride", "=", "32", ",", "global_pool", "=", "'avg'", ",", "\n", "drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ",", "zero_init_last", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "assert", "output_stride", "in", "(", "8", ",", "16", ",", "32", ")", "\n", "\n", "# Construct the stem", "\n", "stem_width", "=", "cfg", ".", "stem_width", "\n", "na_args", "=", "dict", "(", "act_layer", "=", "cfg", ".", "act_layer", ",", "norm_layer", "=", "cfg", ".", "norm_layer", ")", "\n", "if", "cfg", ".", "preact", ":", "\n", "            ", "self", ".", "stem", "=", "create_conv2d", "(", "in_chans", ",", "stem_width", ",", "3", ",", "stride", "=", "2", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "stem", "=", "ConvNormAct", "(", "in_chans", ",", "stem_width", ",", "3", ",", "stride", "=", "2", ",", "**", "na_args", ")", "\n", "", "self", ".", "feature_info", "=", "[", "dict", "(", "num_chs", "=", "stem_width", ",", "reduction", "=", "2", ",", "module", "=", "'stem'", ")", "]", "\n", "\n", "# Construct the stages", "\n", "prev_width", "=", "stem_width", "\n", "curr_stride", "=", "2", "\n", "per_stage_args", ",", "common_args", "=", "self", ".", "_get_stage_args", "(", "\n", "cfg", ",", "output_stride", "=", "output_stride", ",", "drop_path_rate", "=", "drop_path_rate", ")", "\n", "assert", "len", "(", "per_stage_args", ")", "==", "4", "\n", "block_fn", "=", "PreBottleneck", "if", "cfg", ".", "preact", "else", "Bottleneck", "\n", "for", "i", ",", "stage_args", "in", "enumerate", "(", "per_stage_args", ")", ":", "\n", "            ", "stage_name", "=", "\"s{}\"", ".", "format", "(", "i", "+", "1", ")", "\n", "self", ".", "add_module", "(", "stage_name", ",", "RegStage", "(", "in_chs", "=", "prev_width", ",", "block_fn", "=", "block_fn", ",", "**", "stage_args", ",", "**", "common_args", ")", ")", "\n", "prev_width", "=", "stage_args", "[", "'out_chs'", "]", "\n", "curr_stride", "*=", "stage_args", "[", "'stride'", "]", "\n", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "prev_width", ",", "reduction", "=", "curr_stride", ",", "module", "=", "stage_name", ")", "]", "\n", "\n", "# Construct the head", "\n", "", "if", "cfg", ".", "num_features", ":", "\n", "            ", "self", ".", "final_conv", "=", "ConvNormAct", "(", "prev_width", ",", "cfg", ".", "num_features", ",", "kernel_size", "=", "1", ",", "**", "na_args", ")", "\n", "self", ".", "num_features", "=", "cfg", ".", "num_features", "\n", "", "else", ":", "\n", "            ", "final_act", "=", "cfg", ".", "linear_out", "or", "cfg", ".", "preact", "\n", "self", ".", "final_conv", "=", "get_act_layer", "(", "cfg", ".", "act_layer", ")", "(", ")", "if", "final_act", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "num_features", "=", "prev_width", "\n", "", "self", ".", "head", "=", "ClassifierHead", "(", "\n", "in_chs", "=", "self", ".", "num_features", ",", "num_classes", "=", "num_classes", ",", "pool_type", "=", "global_pool", ",", "drop_rate", "=", "drop_rate", ")", "\n", "\n", "named_apply", "(", "partial", "(", "_init_weights", ",", "zero_init_last", "=", "zero_init_last", ")", ",", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.RegNet._get_stage_args": [[424, 456], ["regnet.generate_regnet", "numpy.unique", "range", "numpy.split", "regnet.adjust_widths_groups_comp", "dict", "stage_strides.append", "stage_dilations.append", "numpy.linspace", "numpy.cumsum", "dict", "range", "sum", "zip", "zip"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.generate_regnet", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.adjust_widths_groups_comp"], ["", "def", "_get_stage_args", "(", "self", ",", "cfg", ":", "RegNetCfg", ",", "default_stride", "=", "2", ",", "output_stride", "=", "32", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "# Generate RegNet ws per block", "\n", "        ", "widths", ",", "num_stages", ",", "stage_gs", "=", "generate_regnet", "(", "cfg", ".", "wa", ",", "cfg", ".", "w0", ",", "cfg", ".", "wm", ",", "cfg", ".", "depth", ",", "cfg", ".", "group_size", ")", "\n", "\n", "# Convert to per stage format", "\n", "stage_widths", ",", "stage_depths", "=", "np", ".", "unique", "(", "widths", ",", "return_counts", "=", "True", ")", "\n", "stage_br", "=", "[", "cfg", ".", "bottle_ratio", "for", "_", "in", "range", "(", "num_stages", ")", "]", "\n", "stage_strides", "=", "[", "]", "\n", "stage_dilations", "=", "[", "]", "\n", "net_stride", "=", "2", "\n", "dilation", "=", "1", "\n", "for", "_", "in", "range", "(", "num_stages", ")", ":", "\n", "            ", "if", "net_stride", ">=", "output_stride", ":", "\n", "                ", "dilation", "*=", "default_stride", "\n", "stride", "=", "1", "\n", "", "else", ":", "\n", "                ", "stride", "=", "default_stride", "\n", "net_stride", "*=", "stride", "\n", "", "stage_strides", ".", "append", "(", "stride", ")", "\n", "stage_dilations", ".", "append", "(", "dilation", ")", "\n", "", "stage_dpr", "=", "np", ".", "split", "(", "np", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "sum", "(", "stage_depths", ")", ")", ",", "np", ".", "cumsum", "(", "stage_depths", "[", ":", "-", "1", "]", ")", ")", "\n", "\n", "# Adjust the compatibility of ws and gws", "\n", "stage_widths", ",", "stage_gs", "=", "adjust_widths_groups_comp", "(", "stage_widths", ",", "stage_br", ",", "stage_gs", ")", "\n", "arg_names", "=", "[", "'out_chs'", ",", "'stride'", ",", "'dilation'", ",", "'depth'", ",", "'bottle_ratio'", ",", "'group_size'", ",", "'drop_path_rates'", "]", "\n", "per_stage_args", "=", "[", "\n", "dict", "(", "zip", "(", "arg_names", ",", "params", ")", ")", "for", "params", "in", "\n", "zip", "(", "stage_widths", ",", "stage_strides", ",", "stage_dilations", ",", "stage_depths", ",", "stage_br", ",", "stage_gs", ",", "stage_dpr", ")", "]", "\n", "common_args", "=", "dict", "(", "\n", "downsample", "=", "cfg", ".", "downsample", ",", "se_ratio", "=", "cfg", ".", "se_ratio", ",", "linear_out", "=", "cfg", ".", "linear_out", ",", "\n", "act_layer", "=", "cfg", ".", "act_layer", ",", "norm_layer", "=", "cfg", ".", "norm_layer", ")", "\n", "return", "per_stage_args", ",", "common_args", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.RegNet.group_matcher": [[457, 462], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^stem'", ",", "\n", "blocks", "=", "r'^s(\\d+)'", "if", "coarse", "else", "r'^s(\\d+)\\.b(\\d+)'", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.RegNet.set_grad_checkpointing": [[464, 468], ["list", "regnet.RegNet.children"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "for", "s", "in", "list", "(", "self", ".", "children", "(", ")", ")", "[", "1", ":", "-", "1", "]", ":", "\n", "            ", "s", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.RegNet.get_classifier": [[469, 472], ["None"], "methods", ["None"], ["", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", ".", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.RegNet.reset_classifier": [[473, 475], ["layers.ClassifierHead"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "head", "=", "ClassifierHead", "(", "self", ".", "num_features", ",", "num_classes", ",", "pool_type", "=", "global_pool", ",", "drop_rate", "=", "self", ".", "drop_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.RegNet.forward_features": [[476, 484], ["regnet.RegNet.stem", "regnet.RegNet.s1", "regnet.RegNet.s2", "regnet.RegNet.s3", "regnet.RegNet.s4", "regnet.RegNet.final_conv"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "stem", "(", "x", ")", "\n", "x", "=", "self", ".", "s1", "(", "x", ")", "\n", "x", "=", "self", ".", "s2", "(", "x", ")", "\n", "x", "=", "self", ".", "s3", "(", "x", ")", "\n", "x", "=", "self", ".", "s4", "(", "x", ")", "\n", "x", "=", "self", ".", "final_conv", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.RegNet.forward_head": [[485, 487], ["regnet.RegNet.head"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "head", "(", "x", ",", "pre_logits", "=", "pre_logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.RegNet.forward": [[488, 492], ["regnet.RegNet.forward_features", "regnet.RegNet.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._cfg": [[108, 115], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bicubic'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'stem.conv'", ",", "'classifier'", ":", "'head.fc'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.quantize_float": [[173, 176], ["int", "round"], "function", ["None"], ["def", "quantize_float", "(", "f", ",", "q", ")", ":", "\n", "    ", "\"\"\"Converts a float to closest non-zero int divisible by q.\"\"\"", "\n", "return", "int", "(", "round", "(", "f", "/", "q", ")", "*", "q", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.adjust_widths_groups_comp": [[178, 185], ["int", "min", "regnet.quantize_float", "int", "zip", "zip", "zip", "zip"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.quantize_float"], ["", "def", "adjust_widths_groups_comp", "(", "widths", ",", "bottle_ratios", ",", "groups", ")", ":", "\n", "    ", "\"\"\"Adjusts the compatibility of widths and groups.\"\"\"", "\n", "bottleneck_widths", "=", "[", "int", "(", "w", "*", "b", ")", "for", "w", ",", "b", "in", "zip", "(", "widths", ",", "bottle_ratios", ")", "]", "\n", "groups", "=", "[", "min", "(", "g", ",", "w_bot", ")", "for", "g", ",", "w_bot", "in", "zip", "(", "groups", ",", "bottleneck_widths", ")", "]", "\n", "bottleneck_widths", "=", "[", "quantize_float", "(", "w_bot", ",", "g", ")", "for", "w_bot", ",", "g", "in", "zip", "(", "bottleneck_widths", ",", "groups", ")", "]", "\n", "widths", "=", "[", "int", "(", "w_bot", "/", "b", ")", "for", "w_bot", ",", "b", "in", "zip", "(", "bottleneck_widths", ",", "bottle_ratios", ")", "]", "\n", "return", "widths", ",", "groups", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.generate_regnet": [[187, 200], ["numpy.round", "numpy.array", "numpy.power", "numpy.round", "len", "widths.astype().tolist", "np.array.astype().tolist", "numpy.arange", "numpy.log", "numpy.log", "numpy.divide", "numpy.unique", "np.round.max", "range", "widths.astype", "np.array.astype"], "function", ["None"], ["", "def", "generate_regnet", "(", "width_slope", ",", "width_initial", ",", "width_mult", ",", "depth", ",", "group_size", ",", "q", "=", "8", ")", ":", "\n", "    ", "\"\"\"Generates per block widths from RegNet parameters.\"\"\"", "\n", "assert", "width_slope", ">=", "0", "and", "width_initial", ">", "0", "and", "width_mult", ">", "1", "and", "width_initial", "%", "q", "==", "0", "\n", "# TODO dWr scaling?", "\n", "# depth = int(depth * (scale ** 0.1))", "\n", "# width_scale = scale ** 0.4  # dWr scale, exp 0.8 / 2, applied to both group and layer widths", "\n", "widths_cont", "=", "np", ".", "arange", "(", "depth", ")", "*", "width_slope", "+", "width_initial", "\n", "width_exps", "=", "np", ".", "round", "(", "np", ".", "log", "(", "widths_cont", "/", "width_initial", ")", "/", "np", ".", "log", "(", "width_mult", ")", ")", "\n", "widths", "=", "width_initial", "*", "np", ".", "power", "(", "width_mult", ",", "width_exps", ")", "\n", "widths", "=", "np", ".", "round", "(", "np", ".", "divide", "(", "widths", ",", "q", ")", ")", "*", "q", "\n", "num_stages", ",", "max_stage", "=", "len", "(", "np", ".", "unique", "(", "widths", ")", ")", ",", "width_exps", ".", "max", "(", ")", "+", "1", "\n", "groups", "=", "np", ".", "array", "(", "[", "group_size", "for", "_", "in", "range", "(", "num_stages", ")", "]", ")", "\n", "return", "widths", ".", "astype", "(", "int", ")", ".", "tolist", "(", ")", ",", "num_stages", ",", "groups", ".", "astype", "(", "int", ")", ".", "tolist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.downsample_conv": [[202, 211], ["layers.create_conv2d", "layers.ConvNormAct"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d"], ["", "def", "downsample_conv", "(", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "norm_layer", "=", "None", ",", "preact", "=", "False", ")", ":", "\n", "    ", "norm_layer", "=", "norm_layer", "or", "nn", ".", "BatchNorm2d", "\n", "kernel_size", "=", "1", "if", "stride", "==", "1", "and", "dilation", "==", "1", "else", "kernel_size", "\n", "dilation", "=", "dilation", "if", "kernel_size", ">", "1", "else", "1", "\n", "if", "preact", ":", "\n", "        ", "return", "create_conv2d", "(", "in_chs", ",", "out_chs", ",", "kernel_size", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ")", "\n", "", "else", ":", "\n", "        ", "return", "ConvNormAct", "(", "\n", "in_chs", ",", "out_chs", ",", "kernel_size", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "norm_layer", "=", "norm_layer", ",", "apply_act", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.downsample_avg": [[213, 226], ["torch.Identity", "torch.Sequential", "avg_pool_fn", "layers.create_conv2d", "layers.ConvNormAct"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d"], ["", "", "def", "downsample_avg", "(", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "norm_layer", "=", "None", ",", "preact", "=", "False", ")", ":", "\n", "    ", "\"\"\" AvgPool Downsampling as in 'D' ResNet variants. This is not in RegNet space but I might experiment.\"\"\"", "\n", "norm_layer", "=", "norm_layer", "or", "nn", ".", "BatchNorm2d", "\n", "avg_stride", "=", "stride", "if", "dilation", "==", "1", "else", "1", "\n", "pool", "=", "nn", ".", "Identity", "(", ")", "\n", "if", "stride", ">", "1", "or", "dilation", ">", "1", ":", "\n", "        ", "avg_pool_fn", "=", "AvgPool2dSame", "if", "avg_stride", "==", "1", "and", "dilation", ">", "1", "else", "nn", ".", "AvgPool2d", "\n", "pool", "=", "avg_pool_fn", "(", "2", ",", "avg_stride", ",", "ceil_mode", "=", "True", ",", "count_include_pad", "=", "False", ")", "\n", "", "if", "preact", ":", "\n", "        ", "conv", "=", "create_conv2d", "(", "in_chs", ",", "out_chs", ",", "1", ",", "stride", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "conv", "=", "ConvNormAct", "(", "in_chs", ",", "out_chs", ",", "1", ",", "stride", "=", "1", ",", "norm_layer", "=", "norm_layer", ",", "apply_act", "=", "False", ")", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "[", "pool", ",", "conv", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.create_shortcut": [[228, 241], ["dict", "torch.Identity", "regnet.downsample_avg", "regnet.downsample_conv"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.downsample_avg", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.downsample_conv"], ["", "def", "create_shortcut", "(", "\n", "downsample_type", ",", "in_chs", ",", "out_chs", ",", "kernel_size", ",", "stride", ",", "dilation", "=", "(", "1", ",", "1", ")", ",", "norm_layer", "=", "None", ",", "preact", "=", "False", ")", ":", "\n", "    ", "assert", "downsample_type", "in", "(", "'avg'", ",", "'conv1x1'", ",", "''", ",", "None", ")", "\n", "if", "in_chs", "!=", "out_chs", "or", "stride", "!=", "1", "or", "dilation", "[", "0", "]", "!=", "dilation", "[", "1", "]", ":", "\n", "        ", "dargs", "=", "dict", "(", "stride", "=", "stride", ",", "dilation", "=", "dilation", "[", "0", "]", ",", "norm_layer", "=", "norm_layer", ",", "preact", "=", "preact", ")", "\n", "if", "not", "downsample_type", ":", "\n", "            ", "return", "None", "# no shortcut, no downsample", "\n", "", "elif", "downsample_type", "==", "'avg'", ":", "\n", "            ", "return", "downsample_avg", "(", "in_chs", ",", "out_chs", ",", "**", "dargs", ")", "\n", "", "else", ":", "\n", "            ", "return", "downsample_conv", "(", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "kernel_size", ",", "**", "dargs", ")", "\n", "", "", "else", ":", "\n", "        ", "return", "nn", ".", "Identity", "(", ")", "# identity shortcut (no downsample)", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._init_weights": [[494, 507], ["isinstance", "module.weight.data.normal_", "isinstance", "math.sqrt", "module.bias.data.zero_", "torch.init.normal_", "torch.init.zeros_", "hasattr", "module.zero_init_last"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet.SelectiveKernelBottleneck.zero_init_last"], ["", "", "def", "_init_weights", "(", "module", ",", "name", "=", "''", ",", "zero_init_last", "=", "False", ")", ":", "\n", "    ", "if", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", ":", "\n", "        ", "fan_out", "=", "module", ".", "kernel_size", "[", "0", "]", "*", "module", ".", "kernel_size", "[", "1", "]", "*", "module", ".", "out_channels", "\n", "fan_out", "//=", "module", ".", "groups", "\n", "module", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "math", ".", "sqrt", "(", "2.0", "/", "fan_out", ")", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "        ", "nn", ".", "init", ".", "normal_", "(", "module", ".", "weight", ",", "mean", "=", "0.0", ",", "std", "=", "0.01", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "", "elif", "zero_init_last", "and", "hasattr", "(", "module", ",", "'zero_init_last'", ")", ":", "\n", "        ", "module", ".", "zero_init_last", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._filter_fn": [[509, 515], ["None"], "function", ["None"], ["", "", "def", "_filter_fn", "(", "state_dict", ")", ":", "\n", "    ", "\"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"", "\n", "if", "'model'", "in", "state_dict", ":", "\n", "# For DeiT trained regnety_160 pretraiend model", "\n", "        ", "state_dict", "=", "state_dict", "[", "'model'", "]", "\n", "", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet": [[517, 523], ["helpers.build_model_with_cfg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "def", "_create_regnet", "(", "variant", ",", "pretrained", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "\n", "RegNet", ",", "variant", ",", "pretrained", ",", "\n", "model_cfg", "=", "model_cfgs", "[", "variant", "]", ",", "\n", "pretrained_filter_fn", "=", "_filter_fn", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnetx_002": [[525, 529], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnetx_002", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetX-200MF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnetx_002'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnetx_004": [[531, 535], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnetx_004", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetX-400MF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnetx_004'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnetx_006": [[537, 541], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnetx_006", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetX-600MF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnetx_006'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnetx_008": [[543, 547], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnetx_008", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetX-800MF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnetx_008'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnetx_016": [[549, 553], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnetx_016", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetX-1.6GF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnetx_016'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnetx_032": [[555, 559], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnetx_032", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetX-3.2GF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnetx_032'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnetx_040": [[561, 565], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnetx_040", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetX-4.0GF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnetx_040'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnetx_064": [[567, 571], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnetx_064", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetX-6.4GF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnetx_064'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnetx_080": [[573, 577], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnetx_080", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetX-8.0GF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnetx_080'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnetx_120": [[579, 583], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnetx_120", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetX-12GF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnetx_120'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnetx_160": [[585, 589], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnetx_160", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetX-16GF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnetx_160'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnetx_320": [[591, 595], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnetx_320", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetX-32GF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnetx_320'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnety_002": [[597, 601], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnety_002", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetY-200MF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnety_002'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnety_004": [[603, 607], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnety_004", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetY-400MF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnety_004'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnety_006": [[609, 613], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnety_006", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetY-600MF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnety_006'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnety_008": [[615, 619], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnety_008", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetY-800MF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnety_008'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnety_016": [[621, 625], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnety_016", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetY-1.6GF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnety_016'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnety_032": [[627, 631], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnety_032", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetY-3.2GF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnety_032'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnety_040": [[633, 637], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnety_040", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetY-4.0GF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnety_040'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnety_064": [[639, 643], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnety_064", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetY-6.4GF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnety_064'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnety_080": [[645, 649], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnety_080", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetY-8.0GF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnety_080'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnety_120": [[651, 655], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnety_120", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetY-12GF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnety_120'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnety_160": [[657, 661], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnety_160", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetY-16GF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnety_160'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnety_320": [[663, 667], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnety_320", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetY-32GF\"\"\"", "\n", "return", "_create_regnet", "(", "'regnety_320'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnety_040s_gn": [[669, 673], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnety_040s_gn", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetY-4.0GF w/ GroupNorm \"\"\"", "\n", "return", "_create_regnet", "(", "'regnety_040s_gn'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnetv_040": [[675, 679], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnetv_040", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\"\"\"", "\n", "return", "_create_regnet", "(", "'regnetv_040'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnetv_064": [[681, 685], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnetv_064", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\"\"\"", "\n", "return", "_create_regnet", "(", "'regnetv_064'", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnetz_005": [[687, 694], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnetz_005", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetZ-500MF\n    NOTE: config found in https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/models/regnet.py\n    but it's not clear it is equivalent to paper model as not detailed in the paper.\n    \"\"\"", "\n", "return", "_create_regnet", "(", "'regnetz_005'", ",", "pretrained", ",", "zero_init_last", "=", "False", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnetz_040": [[696, 703], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnetz_040", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetZ-4.0GF\n    NOTE: config found in https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/models/regnet.py\n    but it's not clear it is equivalent to paper model as not detailed in the paper.\n    \"\"\"", "\n", "return", "_create_regnet", "(", "'regnetz_040'", ",", "pretrained", ",", "zero_init_last", "=", "False", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet.regnetz_040h": [[705, 712], ["regnet._create_regnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.regnet._create_regnet"], ["", "@", "register_model", "\n", "def", "regnetz_040h", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"RegNetZ-4.0GF\n    NOTE: config found in https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/models/regnet.py\n    but it's not clear it is equivalent to paper model as not detailed in the paper.\n    \"\"\"", "\n", "return", "_create_regnet", "(", "'regnetz_040h'", ",", "pretrained", ",", "zero_init_last", "=", "False", ",", "**", "kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.BasicBlock.__init__": [[71, 89], ["torch.Module.__init__", "tresnet.conv2d_iabn", "torch.ReLU", "torch.ReLU", "max", "tresnet.conv2d_iabn", "layers.SEModule", "tresnet.conv2d_iabn", "torch.Sequential", "torch.Sequential", "tresnet.conv2d_iabn", "aa_layer"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.conv2d_iabn", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.conv2d_iabn", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.conv2d_iabn", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.conv2d_iabn"], ["def", "__init__", "(", "self", ",", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "downsample", "=", "None", ",", "use_se", "=", "True", ",", "aa_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "BasicBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "stride", "==", "1", ":", "\n", "            ", "self", ".", "conv1", "=", "conv2d_iabn", "(", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "act_param", "=", "1e-3", ")", "\n", "", "else", ":", "\n", "            ", "if", "aa_layer", "is", "None", ":", "\n", "                ", "self", ".", "conv1", "=", "conv2d_iabn", "(", "inplanes", ",", "planes", ",", "stride", "=", "2", ",", "act_param", "=", "1e-3", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "conv1", "=", "nn", ".", "Sequential", "(", "\n", "conv2d_iabn", "(", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "act_param", "=", "1e-3", ")", ",", "\n", "aa_layer", "(", "channels", "=", "planes", ",", "filt_size", "=", "3", ",", "stride", "=", "2", ")", ")", "\n", "\n", "", "", "self", ".", "conv2", "=", "conv2d_iabn", "(", "planes", ",", "planes", ",", "stride", "=", "1", ",", "act_layer", "=", "\"identity\"", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "stride", "=", "stride", "\n", "rd_chs", "=", "max", "(", "planes", "*", "self", ".", "expansion", "//", "4", ",", "64", ")", "\n", "self", ".", "se", "=", "SEModule", "(", "planes", "*", "self", ".", "expansion", ",", "rd_channels", "=", "rd_chs", ")", "if", "use_se", "else", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.BasicBlock.forward": [[90, 105], ["tresnet.BasicBlock.conv1", "tresnet.BasicBlock.conv2", "tresnet.BasicBlock.relu", "tresnet.BasicBlock.downsample", "tresnet.BasicBlock.se"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "shortcut", "=", "self", ".", "downsample", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "shortcut", "=", "x", "\n", "\n", "", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "\n", "if", "self", ".", "se", "is", "not", "None", ":", "\n", "            ", "out", "=", "self", ".", "se", "(", "out", ")", "\n", "\n", "", "out", "+=", "shortcut", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.Bottleneck.__init__": [[110, 137], ["torch.Module.__init__", "tresnet.conv2d_iabn", "max", "tresnet.conv2d_iabn", "torch.ReLU", "torch.ReLU", "tresnet.conv2d_iabn", "layers.SEModule", "tresnet.conv2d_iabn", "torch.Sequential", "torch.Sequential", "tresnet.conv2d_iabn", "aa_layer"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.conv2d_iabn", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.conv2d_iabn", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.conv2d_iabn", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.conv2d_iabn", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.conv2d_iabn"], ["def", "__init__", "(", "\n", "self", ",", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "downsample", "=", "None", ",", "use_se", "=", "True", ",", "\n", "act_layer", "=", "\"leaky_relu\"", ",", "aa_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "Bottleneck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv1", "=", "conv2d_iabn", "(", "\n", "inplanes", ",", "planes", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "act_layer", "=", "act_layer", ",", "act_param", "=", "1e-3", ")", "\n", "if", "stride", "==", "1", ":", "\n", "            ", "self", ".", "conv2", "=", "conv2d_iabn", "(", "\n", "planes", ",", "planes", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "act_layer", "=", "act_layer", ",", "act_param", "=", "1e-3", ")", "\n", "", "else", ":", "\n", "            ", "if", "aa_layer", "is", "None", ":", "\n", "                ", "self", ".", "conv2", "=", "conv2d_iabn", "(", "\n", "planes", ",", "planes", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "act_layer", "=", "act_layer", ",", "act_param", "=", "1e-3", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "conv2", "=", "nn", ".", "Sequential", "(", "\n", "conv2d_iabn", "(", "planes", ",", "planes", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "act_layer", "=", "act_layer", ",", "act_param", "=", "1e-3", ")", ",", "\n", "aa_layer", "(", "channels", "=", "planes", ",", "filt_size", "=", "3", ",", "stride", "=", "2", ")", ")", "\n", "\n", "", "", "reduction_chs", "=", "max", "(", "planes", "*", "self", ".", "expansion", "//", "8", ",", "64", ")", "\n", "self", ".", "se", "=", "SEModule", "(", "planes", ",", "rd_channels", "=", "reduction_chs", ")", "if", "use_se", "else", "None", "\n", "\n", "self", ".", "conv3", "=", "conv2d_iabn", "(", "\n", "planes", ",", "planes", "*", "self", ".", "expansion", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "act_layer", "=", "\"identity\"", ")", "\n", "\n", "self", ".", "act", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.Bottleneck.forward": [[138, 153], ["tresnet.Bottleneck.conv1", "tresnet.Bottleneck.conv2", "tresnet.Bottleneck.conv3", "tresnet.Bottleneck.act", "tresnet.Bottleneck.downsample", "tresnet.Bottleneck.se"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "shortcut", "=", "self", ".", "downsample", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "shortcut", "=", "x", "\n", "\n", "", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "if", "self", ".", "se", "is", "not", "None", ":", "\n", "            ", "out", "=", "self", ".", "se", "(", "out", ")", "\n", "", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "out", "=", "out", "+", "shortcut", "# no inplace", "\n", "out", "=", "self", ".", "act", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.TResNet.__init__": [[156, 213], ["torch.Module.__init__", "int", "int", "tresnet.conv2d_iabn", "tresnet.TResNet._make_layer", "tresnet.TResNet._make_layer", "tresnet.TResNet._make_layer", "tresnet.TResNet._make_layer", "torch.Sequential", "torch.Sequential", "layers.ClassifierHead", "tresnet.TResNet.modules", "tresnet.TResNet.modules", "collections.OrderedDict", "dict", "dict", "dict", "dict", "dict", "isinstance", "isinstance", "isinstance", "isinstance", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "m.weight.data.normal_", "isinstance", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "layers.SpaceToDepthModule"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.conv2d_iabn", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.TResNet._make_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.TResNet._make_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.TResNet._make_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.TResNet._make_layer"], ["    ", "def", "__init__", "(", "self", ",", "layers", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "width_factor", "=", "1.0", ",", "global_pool", "=", "'fast'", ",", "drop_rate", "=", "0.", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "super", "(", "TResNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "aa_layer", "=", "BlurPool2d", "\n", "\n", "# TResnet stages", "\n", "self", ".", "inplanes", "=", "int", "(", "64", "*", "width_factor", ")", "\n", "self", ".", "planes", "=", "int", "(", "64", "*", "width_factor", ")", "\n", "conv1", "=", "conv2d_iabn", "(", "in_chans", "*", "16", ",", "self", ".", "planes", ",", "stride", "=", "1", ",", "kernel_size", "=", "3", ")", "\n", "layer1", "=", "self", ".", "_make_layer", "(", "\n", "BasicBlock", ",", "self", ".", "planes", ",", "layers", "[", "0", "]", ",", "stride", "=", "1", ",", "use_se", "=", "True", ",", "aa_layer", "=", "aa_layer", ")", "# 56x56", "\n", "layer2", "=", "self", ".", "_make_layer", "(", "\n", "BasicBlock", ",", "self", ".", "planes", "*", "2", ",", "layers", "[", "1", "]", ",", "stride", "=", "2", ",", "use_se", "=", "True", ",", "aa_layer", "=", "aa_layer", ")", "# 28x28", "\n", "layer3", "=", "self", ".", "_make_layer", "(", "\n", "Bottleneck", ",", "self", ".", "planes", "*", "4", ",", "layers", "[", "2", "]", ",", "stride", "=", "2", ",", "use_se", "=", "True", ",", "aa_layer", "=", "aa_layer", ")", "# 14x14", "\n", "layer4", "=", "self", ".", "_make_layer", "(", "\n", "Bottleneck", ",", "self", ".", "planes", "*", "8", ",", "layers", "[", "3", "]", ",", "stride", "=", "2", ",", "use_se", "=", "False", ",", "aa_layer", "=", "aa_layer", ")", "# 7x7", "\n", "\n", "# body", "\n", "self", ".", "body", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "'SpaceToDepth'", ",", "SpaceToDepthModule", "(", ")", ")", ",", "\n", "(", "'conv1'", ",", "conv1", ")", ",", "\n", "(", "'layer1'", ",", "layer1", ")", ",", "\n", "(", "'layer2'", ",", "layer2", ")", ",", "\n", "(", "'layer3'", ",", "layer3", ")", ",", "\n", "(", "'layer4'", ",", "layer4", ")", "]", ")", ")", "\n", "\n", "self", ".", "feature_info", "=", "[", "\n", "dict", "(", "num_chs", "=", "self", ".", "planes", ",", "reduction", "=", "2", ",", "module", "=", "''", ")", ",", "# Not with S2D?", "\n", "dict", "(", "num_chs", "=", "self", ".", "planes", ",", "reduction", "=", "4", ",", "module", "=", "'body.layer1'", ")", ",", "\n", "dict", "(", "num_chs", "=", "self", ".", "planes", "*", "2", ",", "reduction", "=", "8", ",", "module", "=", "'body.layer2'", ")", ",", "\n", "dict", "(", "num_chs", "=", "self", ".", "planes", "*", "4", "*", "Bottleneck", ".", "expansion", ",", "reduction", "=", "16", ",", "module", "=", "'body.layer3'", ")", ",", "\n", "dict", "(", "num_chs", "=", "self", ".", "planes", "*", "8", "*", "Bottleneck", ".", "expansion", ",", "reduction", "=", "32", ",", "module", "=", "'body.layer4'", ")", ",", "\n", "]", "\n", "\n", "# head", "\n", "self", ".", "num_features", "=", "(", "self", ".", "planes", "*", "8", ")", "*", "Bottleneck", ".", "expansion", "\n", "self", ".", "head", "=", "ClassifierHead", "(", "self", ".", "num_features", ",", "num_classes", ",", "pool_type", "=", "global_pool", ",", "drop_rate", "=", "drop_rate", ")", "\n", "\n", "# model initialization", "\n", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'leaky_relu'", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm2d", ")", "or", "isinstance", "(", "m", ",", "InplaceAbn", ")", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "\n", "# residual connections special initialization", "\n", "", "", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "BasicBlock", ")", ":", "\n", "                ", "m", ".", "conv2", "[", "1", "]", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros_like", "(", "m", ".", "conv2", "[", "1", "]", ".", "weight", ")", ")", "# BN to zero", "\n", "", "if", "isinstance", "(", "m", ",", "Bottleneck", ")", ":", "\n", "                ", "m", ".", "conv3", "[", "1", "]", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros_like", "(", "m", ".", "conv3", "[", "1", "]", ".", "weight", ")", ")", "# BN to zero", "\n", "", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "m", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "0.01", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.TResNet._make_layer": [[214, 233], ["layers.append", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "block", "layers.append", "layers.append", "tresnet.conv2d_iabn", "block", "torch.AvgPool2d", "torch.AvgPool2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.conv2d_iabn"], ["", "", "", "def", "_make_layer", "(", "self", ",", "block", ",", "planes", ",", "blocks", ",", "stride", "=", "1", ",", "use_se", "=", "True", ",", "aa_layer", "=", "None", ")", ":", "\n", "        ", "downsample", "=", "None", "\n", "if", "stride", "!=", "1", "or", "self", ".", "inplanes", "!=", "planes", "*", "block", ".", "expansion", ":", "\n", "            ", "layers", "=", "[", "]", "\n", "if", "stride", "==", "2", ":", "\n", "# avg pooling before 1x1 conv", "\n", "                ", "layers", ".", "append", "(", "nn", ".", "AvgPool2d", "(", "kernel_size", "=", "2", ",", "stride", "=", "2", ",", "ceil_mode", "=", "True", ",", "count_include_pad", "=", "False", ")", ")", "\n", "", "layers", "+=", "[", "conv2d_iabn", "(", "\n", "self", ".", "inplanes", ",", "planes", "*", "block", ".", "expansion", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "act_layer", "=", "\"identity\"", ")", "]", "\n", "downsample", "=", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n", "", "layers", "=", "[", "]", "\n", "layers", ".", "append", "(", "block", "(", "\n", "self", ".", "inplanes", ",", "planes", ",", "stride", ",", "downsample", ",", "use_se", "=", "use_se", ",", "aa_layer", "=", "aa_layer", ")", ")", "\n", "self", ".", "inplanes", "=", "planes", "*", "block", ".", "expansion", "\n", "for", "i", "in", "range", "(", "1", ",", "blocks", ")", ":", "\n", "            ", "layers", ".", "append", "(", "\n", "block", "(", "self", ".", "inplanes", ",", "planes", ",", "use_se", "=", "use_se", ",", "aa_layer", "=", "aa_layer", ")", ")", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.TResNet.group_matcher": [[234, 238], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "stem", "=", "r'^body\\.conv1'", ",", "blocks", "=", "r'^body\\.layer(\\d+)'", "if", "coarse", "else", "r'^body\\.layer(\\d+)\\.(\\d+)'", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.TResNet.set_grad_checkpointing": [[239, 242], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "'gradient checkpointing not supported'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.TResNet.get_classifier": [[243, 246], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", ".", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.TResNet.reset_classifier": [[247, 250], ["layers.ClassifierHead"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'fast'", ")", ":", "\n", "        ", "self", ".", "head", "=", "ClassifierHead", "(", "\n", "self", ".", "num_features", ",", "num_classes", ",", "pool_type", "=", "global_pool", ",", "drop_rate", "=", "self", ".", "drop_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.TResNet.forward_features": [[251, 253], ["tresnet.TResNet.body"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "body", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.TResNet.forward_head": [[254, 256], ["tresnet.TResNet.head"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "return", "x", "if", "pre_logits", "else", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.TResNet.forward": [[257, 261], ["tresnet.TResNet.forward_features", "tresnet.TResNet.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet._cfg": [[20, 27], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bilinear'", ",", "\n", "'mean'", ":", "(", "0", ",", "0", ",", "0", ")", ",", "'std'", ":", "(", "1", ",", "1", ",", "1", ")", ",", "\n", "'first_conv'", ":", "'body.conv1.0'", ",", "'classifier'", ":", "'head.fc'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.IABN2Float": [[51, 58], ["isinstance", "module.children", "module.float", "tresnet.IABN2Float"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.IABN2Float"], ["def", "IABN2Float", "(", "module", ":", "nn", ".", "Module", ")", "->", "nn", ".", "Module", ":", "\n", "    ", "\"\"\"If `module` is IABN don't use half precision.\"\"\"", "\n", "if", "isinstance", "(", "module", ",", "InplaceAbn", ")", ":", "\n", "        ", "module", ".", "float", "(", ")", "\n", "", "for", "child", "in", "module", ".", "children", "(", ")", ":", "\n", "        ", "IABN2Float", "(", "child", ")", "\n", "", "return", "module", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.conv2d_iabn": [[60, 65], ["torch.Sequential", "torch.Conv2d", "layers.InplaceAbn"], "function", ["None"], ["", "def", "conv2d_iabn", "(", "ni", ",", "nf", ",", "stride", ",", "kernel_size", "=", "3", ",", "groups", "=", "1", ",", "act_layer", "=", "\"leaky_relu\"", ",", "act_param", "=", "1e-2", ")", ":", "\n", "    ", "return", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "\n", "ni", ",", "nf", ",", "kernel_size", "=", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "kernel_size", "//", "2", ",", "groups", "=", "groups", ",", "bias", "=", "False", ")", ",", "\n", "InplaceAbn", "(", "nf", ",", "act_layer", "=", "act_layer", ",", "act_param", "=", "act_param", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet._create_tresnet": [[263, 268], ["helpers.build_model_with_cfg", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_tresnet", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "\n", "TResNet", ",", "variant", ",", "pretrained", ",", "\n", "feature_cfg", "=", "dict", "(", "out_indices", "=", "(", "1", ",", "2", ",", "3", ",", "4", ")", ",", "flatten_sequential", "=", "True", ")", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.tresnet_m": [[270, 274], ["dict", "tresnet._create_tresnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet._create_tresnet"], ["", "@", "register_model", "\n", "def", "tresnet_m", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "layers", "=", "[", "3", ",", "4", ",", "11", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_tresnet", "(", "'tresnet_m'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.tresnet_m_miil_in21k": [[276, 280], ["dict", "tresnet._create_tresnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet._create_tresnet"], ["", "@", "register_model", "\n", "def", "tresnet_m_miil_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "layers", "=", "[", "3", ",", "4", ",", "11", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_tresnet", "(", "'tresnet_m_miil_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.tresnet_l": [[282, 286], ["dict", "tresnet._create_tresnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet._create_tresnet"], ["", "@", "register_model", "\n", "def", "tresnet_l", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "layers", "=", "[", "4", ",", "5", ",", "18", ",", "3", "]", ",", "width_factor", "=", "1.2", ",", "**", "kwargs", ")", "\n", "return", "_create_tresnet", "(", "'tresnet_l'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.tresnet_xl": [[288, 292], ["dict", "tresnet._create_tresnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet._create_tresnet"], ["", "@", "register_model", "\n", "def", "tresnet_xl", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "layers", "=", "[", "4", ",", "5", ",", "24", ",", "3", "]", ",", "width_factor", "=", "1.3", ",", "**", "kwargs", ")", "\n", "return", "_create_tresnet", "(", "'tresnet_xl'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.tresnet_m_448": [[294, 298], ["dict", "tresnet._create_tresnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet._create_tresnet"], ["", "@", "register_model", "\n", "def", "tresnet_m_448", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "layers", "=", "[", "3", ",", "4", ",", "11", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "return", "_create_tresnet", "(", "'tresnet_m_448'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.tresnet_l_448": [[300, 304], ["dict", "tresnet._create_tresnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet._create_tresnet"], ["", "@", "register_model", "\n", "def", "tresnet_l_448", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "layers", "=", "[", "4", ",", "5", ",", "18", ",", "3", "]", ",", "width_factor", "=", "1.2", ",", "**", "kwargs", ")", "\n", "return", "_create_tresnet", "(", "'tresnet_l_448'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet.tresnet_xl_448": [[306, 310], ["dict", "tresnet._create_tresnet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.tresnet._create_tresnet"], ["", "@", "register_model", "\n", "def", "tresnet_xl_448", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "layers", "=", "[", "4", ",", "5", ",", "24", ",", "3", "]", ",", "width_factor", "=", "1.3", ",", "**", "kwargs", ")", "\n", "return", "_create_tresnet", "(", "'tresnet_xl_448'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.Attention.__init__": [[187, 198], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ",", "num_heads", "=", "8", ",", "qkv_bias", "=", "False", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "dim", "%", "num_heads", "==", "0", ",", "'dim should be divisible by num_heads'", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "head_dim", "=", "dim", "//", "num_heads", "\n", "self", ".", "scale", "=", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", "*", "3", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.Attention.forward": [[199, 212], ["vision_transformer.Attention.qkv().reshape().permute", "vision_transformer.Attention.unbind", "vision_transformer.Attention.softmax", "vision_transformer.Attention.attn_drop", "vision_transformer.Attention.proj", "vision_transformer.Attention.proj_drop", "vision_transformer.Attention.qkv().reshape", "k.transpose", "vision_transformer.Attention.qkv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "qkv", "=", "self", ".", "qkv", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "3", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "q", ",", "k", ",", "v", "=", "qkv", ".", "unbind", "(", "0", ")", "# make torchscript happy (cannot use tensor as tuple)", "\n", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "scale", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "N", ",", "C", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.LayerScale.__init__": [[215, 219], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ",", "init_values", "=", "1e-5", ",", "inplace", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inplace", "=", "inplace", "\n", "self", ".", "gamma", "=", "nn", ".", "Parameter", "(", "init_values", "*", "torch", ".", "ones", "(", "dim", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.LayerScale.forward": [[220, 222], ["x.mul_"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", ".", "mul_", "(", "self", ".", "gamma", ")", "if", "self", ".", "inplace", "else", "x", "*", "self", ".", "gamma", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.Block.__init__": [[226, 240], ["torch.Module.__init__", "norm_layer", "vision_transformer.Attention", "norm_layer", "layers.Mlp", "vision_transformer.LayerScale", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "vision_transformer.LayerScale", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "int"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "num_heads", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "init_values", "=", "None", ",", "\n", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "attn", "=", "Attention", "(", "dim", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ")", "\n", "self", ".", "ls1", "=", "LayerScale", "(", "dim", ",", "init_values", "=", "init_values", ")", "if", "init_values", "else", "nn", ".", "Identity", "(", ")", "\n", "# NOTE: drop path for stochastic depth, we shall see if this is better than dropout here", "\n", "self", ".", "drop_path1", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "int", "(", "dim", "*", "mlp_ratio", ")", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "self", ".", "ls2", "=", "LayerScale", "(", "dim", ",", "init_values", "=", "init_values", ")", "if", "init_values", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "drop_path2", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.Block.forward": [[241, 245], ["vision_transformer.Block.drop_path1", "vision_transformer.Block.drop_path2", "vision_transformer.Block.ls1", "vision_transformer.Block.ls2", "vision_transformer.Block.attn", "vision_transformer.Block.mlp", "vision_transformer.Block.norm1", "vision_transformer.Block.norm2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", "+", "self", ".", "drop_path1", "(", "self", ".", "ls1", "(", "self", ".", "attn", "(", "self", ".", "norm1", "(", "x", ")", ")", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path2", "(", "self", ".", "ls2", "(", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.ResPostBlock.__init__": [[249, 264], ["torch.Module.__init__", "vision_transformer.Attention", "norm_layer", "layers.Mlp", "norm_layer", "vision_transformer.ResPostBlock.init_weights", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "int"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "num_heads", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "init_values", "=", "None", ",", "\n", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "init_values", "=", "init_values", "\n", "\n", "self", ".", "attn", "=", "Attention", "(", "dim", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "drop_path1", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "int", "(", "dim", "*", "mlp_ratio", ")", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "drop_path2", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.ResPostBlock.init_weights": [[265, 270], ["torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "# NOTE this init overrides that base model init with specific changes for the block type", "\n", "        ", "if", "self", ".", "init_values", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "norm1", ".", "weight", ",", "self", ".", "init_values", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "norm2", ".", "weight", ",", "self", ".", "init_values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.ResPostBlock.forward": [[271, 275], ["vision_transformer.ResPostBlock.drop_path1", "vision_transformer.ResPostBlock.drop_path2", "vision_transformer.ResPostBlock.norm1", "vision_transformer.ResPostBlock.norm2", "vision_transformer.ResPostBlock.attn", "vision_transformer.ResPostBlock.mlp"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", "+", "self", ".", "drop_path1", "(", "self", ".", "norm1", "(", "self", ".", "attn", "(", "x", ")", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path2", "(", "self", ".", "norm2", "(", "self", ".", "mlp", "(", "x", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.ParallelBlock.__init__": [[279, 298], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "vision_transformer.ParallelBlock.attns.append", "vision_transformer.ParallelBlock.ffns.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "collections.OrderedDict", "collections.OrderedDict", "norm_layer", "vision_transformer.Attention", "norm_layer", "layers.Mlp", "vision_transformer.LayerScale", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "vision_transformer.LayerScale", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "int"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "num_heads", ",", "num_parallel", "=", "2", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "init_values", "=", "None", ",", "\n", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_parallel", "=", "num_parallel", "\n", "self", ".", "attns", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "ffns", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "_", "in", "range", "(", "num_parallel", ")", ":", "\n", "            ", "self", ".", "attns", ".", "append", "(", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "'norm'", ",", "norm_layer", "(", "dim", ")", ")", ",", "\n", "(", "'attn'", ",", "Attention", "(", "dim", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ")", ")", ",", "\n", "(", "'ls'", ",", "LayerScale", "(", "dim", ",", "init_values", "=", "init_values", ")", "if", "init_values", "else", "nn", ".", "Identity", "(", ")", ")", ",", "\n", "(", "'drop_path'", ",", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", ")", "\n", "]", ")", ")", ")", "\n", "self", ".", "ffns", ".", "append", "(", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "'norm'", ",", "norm_layer", "(", "dim", ")", ")", ",", "\n", "(", "'mlp'", ",", "Mlp", "(", "dim", ",", "hidden_features", "=", "int", "(", "dim", "*", "mlp_ratio", ")", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", ")", ",", "\n", "(", "'ls'", ",", "LayerScale", "(", "dim", ",", "init_values", "=", "init_values", ")", "if", "init_values", "else", "nn", ".", "Identity", "(", ")", ")", ",", "\n", "(", "'drop_path'", ",", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", ")", "\n", "]", ")", ")", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.ParallelBlock._forward_jit": [[300, 304], ["torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "attn", "ffn"], "methods", ["None"], ["", "", "def", "_forward_jit", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", "+", "torch", ".", "stack", "(", "[", "attn", "(", "x", ")", "for", "attn", "in", "self", ".", "attns", "]", ")", ".", "sum", "(", "dim", "=", "0", ")", "\n", "x", "=", "x", "+", "torch", ".", "stack", "(", "[", "ffn", "(", "x", ")", "for", "ffn", "in", "self", ".", "ffns", "]", ")", ".", "sum", "(", "dim", "=", "0", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.ParallelBlock._forward": [[305, 310], ["sum", "sum", "attn", "ffn"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "_forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", "+", "sum", "(", "attn", "(", "x", ")", "for", "attn", "in", "self", ".", "attns", ")", "\n", "x", "=", "x", "+", "sum", "(", "ffn", "(", "x", ")", "for", "ffn", "in", "self", ".", "ffns", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.ParallelBlock.forward": [[311, 316], ["torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_tracing", "torch.jit.is_tracing", "torch.jit.is_tracing", "torch.jit.is_tracing", "torch.jit.is_tracing", "torch.jit.is_tracing", "torch.jit.is_tracing", "torch.jit.is_tracing", "torch.jit.is_tracing", "torch.jit.is_tracing", "torch.jit.is_tracing", "torch.jit.is_tracing", "torch.jit.is_tracing", "torch.jit.is_tracing", "torch.jit.is_tracing", "torch.jit.is_tracing", "vision_transformer.ParallelBlock._forward_jit", "vision_transformer.ParallelBlock._forward"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.ParallelBlock._forward_jit", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.ParallelBlock._forward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "torch", ".", "jit", ".", "is_scripting", "(", ")", "or", "torch", ".", "jit", ".", "is_tracing", "(", ")", ":", "\n", "            ", "return", "self", ".", "_forward_jit", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_forward", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.VisionTransformer.__init__": [[325, 388], ["torch.Module.__init__", "embed_layer", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "functools.partial", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "x.item", "norm_layer", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "norm_layer", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "vision_transformer.VisionTransformer.init_weights", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "block_fn", "range"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights"], ["def", "__init__", "(", "\n", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "global_pool", "=", "'token'", ",", "\n", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "True", ",", "init_values", "=", "None", ",", "\n", "class_token", "=", "True", ",", "fc_norm", "=", "None", ",", "drop_rate", "=", "0.", ",", "attn_drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ",", "weight_init", "=", "''", ",", "\n", "embed_layer", "=", "PatchEmbed", ",", "norm_layer", "=", "None", ",", "act_layer", "=", "None", ",", "block_fn", "=", "Block", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img_size (int, tuple): input image size\n            patch_size (int, tuple): patch size\n            in_chans (int): number of input channels\n            num_classes (int): number of classes for classification head\n            global_pool (str): type of global pooling for final sequence (default: 'token')\n            embed_dim (int): embedding dimension\n            depth (int): depth of transformer\n            num_heads (int): number of attention heads\n            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n            qkv_bias (bool): enable bias for qkv if True\n            init_values: (float): layer-scale init values\n            class_token (bool): use class token\n            fc_norm (Optional[bool]): pre-fc norm after pool, set if global_pool == 'avg' if None (default: None)\n            drop_rate (float): dropout rate\n            attn_drop_rate (float): attention dropout rate\n            drop_path_rate (float): stochastic depth rate\n            weight_init (str): weight init scheme\n            embed_layer (nn.Module): patch embedding layer\n            norm_layer: (nn.Module): normalization layer\n            act_layer: (nn.Module): MLP activation layer\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "global_pool", "in", "(", "''", ",", "'avg'", ",", "'token'", ")", "\n", "assert", "class_token", "or", "global_pool", "!=", "'token'", "\n", "use_fc_norm", "=", "global_pool", "==", "'avg'", "if", "fc_norm", "is", "None", "else", "fc_norm", "\n", "norm_layer", "=", "norm_layer", "or", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", "\n", "act_layer", "=", "act_layer", "or", "nn", ".", "GELU", "\n", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "self", ".", "num_features", "=", "self", ".", "embed_dim", "=", "embed_dim", "# num_features for consistency with other models", "\n", "self", ".", "num_tokens", "=", "1", "if", "class_token", "else", "0", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "self", ".", "patch_embed", "=", "embed_layer", "(", "\n", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", ",", "in_chans", "=", "in_chans", ",", "embed_dim", "=", "embed_dim", ")", "\n", "num_patches", "=", "self", ".", "patch_embed", ".", "num_patches", "\n", "\n", "self", ".", "cls_token", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "if", "self", ".", "num_tokens", ">", "0", "else", "None", "\n", "self", ".", "pos_embed", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "1", ",", "num_patches", "+", "self", ".", "num_tokens", ",", "embed_dim", ")", "*", ".02", ")", "\n", "self", ".", "pos_drop", "=", "nn", ".", "Dropout", "(", "p", "=", "drop_rate", ")", "\n", "\n", "dpr", "=", "[", "x", ".", "item", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "depth", ")", "]", "# stochastic depth decay rule", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "block_fn", "(", "\n", "dim", "=", "embed_dim", ",", "num_heads", "=", "num_heads", ",", "mlp_ratio", "=", "mlp_ratio", ",", "qkv_bias", "=", "qkv_bias", ",", "init_values", "=", "init_values", ",", "\n", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "dpr", "[", "i", "]", ",", "norm_layer", "=", "norm_layer", ",", "act_layer", "=", "act_layer", ")", "\n", "for", "i", "in", "range", "(", "depth", ")", "]", ")", "\n", "self", ".", "norm", "=", "norm_layer", "(", "embed_dim", ")", "if", "not", "use_fc_norm", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "# Classifier Head", "\n", "self", ".", "fc_norm", "=", "norm_layer", "(", "embed_dim", ")", "if", "use_fc_norm", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "if", "weight_init", "!=", "'skip'", ":", "\n", "            ", "self", ".", "init_weights", "(", "weight_init", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.VisionTransformer.init_weights": [[389, 396], ["layers.trunc_normal_", "helpers.named_apply", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "vision_transformer.get_init_weights_vit", "math.log"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_apply", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.get_init_weights_vit"], ["", "", "def", "init_weights", "(", "self", ",", "mode", "=", "''", ")", ":", "\n", "        ", "assert", "mode", "in", "(", "'jax'", ",", "'jax_nlhb'", ",", "'moco'", ",", "''", ")", "\n", "head_bias", "=", "-", "math", ".", "log", "(", "self", ".", "num_classes", ")", "if", "'nlhb'", "in", "mode", "else", "0.", "\n", "trunc_normal_", "(", "self", ".", "pos_embed", ",", "std", "=", ".02", ")", "\n", "if", "self", ".", "cls_token", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "normal_", "(", "self", ".", "cls_token", ",", "std", "=", "1e-6", ")", "\n", "", "named_apply", "(", "get_init_weights_vit", "(", "mode", ",", "head_bias", ")", ",", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.VisionTransformer._init_weights": [[397, 400], ["vision_transformer.init_weights_vit_timm"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.init_weights_vit_timm"], ["", "def", "_init_weights", "(", "self", ",", "m", ")", ":", "\n", "# this fn left here for compat with downstream users", "\n", "        ", "init_weights_vit_timm", "(", "m", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.VisionTransformer.load_pretrained": [[401, 404], ["torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "vision_transformer._load_weights"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._load_weights"], ["", "@", "torch", ".", "jit", ".", "ignore", "(", ")", "\n", "def", "load_pretrained", "(", "self", ",", "checkpoint_path", ",", "prefix", "=", "''", ")", ":", "\n", "        ", "_load_weights", "(", "self", ",", "checkpoint_path", ",", "prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.VisionTransformer.no_weight_decay": [[405, 408], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "no_weight_decay", "(", "self", ")", ":", "\n", "        ", "return", "{", "'pos_embed'", ",", "'cls_token'", ",", "'dist_token'", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.VisionTransformer.group_matcher": [[409, 414], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^cls_token|pos_embed|patch_embed'", ",", "# stem and embed", "\n", "blocks", "=", "[", "(", "r'^blocks\\.(\\d+)'", ",", "None", ")", ",", "(", "r'^norm'", ",", "(", "99999", ",", ")", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.VisionTransformer.set_grad_checkpointing": [[416, 419], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.VisionTransformer.get_classifier": [[420, 423], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.VisionTransformer.reset_classifier": [[424, 430], ["torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ":", "int", ",", "global_pool", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "assert", "global_pool", "in", "(", "''", ",", "'avg'", ",", "'token'", ")", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.VisionTransformer.forward_features": [[431, 442], ["vision_transformer.VisionTransformer.patch_embed", "vision_transformer.VisionTransformer.pos_drop", "vision_transformer.VisionTransformer.norm", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "helpers.checkpoint_seq", "vision_transformer.VisionTransformer.blocks", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "vision_transformer.VisionTransformer.cls_token.expand"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "patch_embed", "(", "x", ")", "\n", "if", "self", ".", "cls_token", "is", "not", "None", ":", "\n", "            ", "x", "=", "torch", ".", "cat", "(", "(", "self", ".", "cls_token", ".", "expand", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ",", "-", "1", ")", ",", "x", ")", ",", "dim", "=", "1", ")", "\n", "", "x", "=", "self", ".", "pos_drop", "(", "x", "+", "self", ".", "pos_embed", ")", "\n", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "blocks", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "blocks", "(", "x", ")", "\n", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.VisionTransformer.forward_head": [[443, 448], ["vision_transformer.VisionTransformer.fc_norm", "vision_transformer.VisionTransformer.head", "x[].mean"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "global_pool", ":", "\n", "            ", "x", "=", "x", "[", ":", ",", "self", ".", "num_tokens", ":", "]", ".", "mean", "(", "dim", "=", "1", ")", "if", "self", ".", "global_pool", "==", "'avg'", "else", "x", "[", ":", ",", "0", "]", "\n", "", "x", "=", "self", ".", "fc_norm", "(", "x", ")", "\n", "return", "x", "if", "pre_logits", "else", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.VisionTransformer.forward": [[449, 453], ["vision_transformer.VisionTransformer.forward_features", "vision_transformer.VisionTransformer.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._cfg": [[41, 49], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", ".9", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "IMAGENET_INCEPTION_MEAN", ",", "'std'", ":", "IMAGENET_INCEPTION_STD", ",", "\n", "'first_conv'", ":", "'patch_embed.proj'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.init_weights_vit_timm": [[455, 463], ["isinstance", "layers.trunc_normal_", "hasattr", "torch.init.zeros_", "module.init_weights"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights"], ["", "", "def", "init_weights_vit_timm", "(", "module", ":", "nn", ".", "Module", ",", "name", ":", "str", "=", "''", ")", ":", "\n", "    ", "\"\"\" ViT weight initialization, original timm impl (for reproducibility) \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "        ", "trunc_normal_", "(", "module", ".", "weight", ",", "std", "=", ".02", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "", "elif", "hasattr", "(", "module", ",", "'init_weights'", ")", ":", "\n", "        ", "module", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.init_weights_vit_jax": [[465, 481], ["isinstance", "name.startswith", "isinstance", "torch.init.zeros_", "torch.init.constant_", "torch.init.xavier_uniform_", "layers.lecun_normal_", "hasattr", "torch.init.zeros_", "module.init_weights", "torch.init.normal_", "torch.init.zeros_"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.lecun_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights"], ["", "", "def", "init_weights_vit_jax", "(", "module", ":", "nn", ".", "Module", ",", "name", ":", "str", "=", "''", ",", "head_bias", ":", "float", "=", "0.", ")", ":", "\n", "    ", "\"\"\" ViT weight initialization, matching JAX (Flax) impl \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "        ", "if", "name", ".", "startswith", "(", "'head'", ")", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "weight", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "module", ".", "bias", ",", "head_bias", ")", "\n", "", "else", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_uniform_", "(", "module", ".", "weight", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "nn", ".", "init", ".", "normal_", "(", "module", ".", "bias", ",", "std", "=", "1e-6", ")", "if", "'mlp'", "in", "name", "else", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", ":", "\n", "        ", "lecun_normal_", "(", "module", ".", "weight", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "", "elif", "hasattr", "(", "module", ",", "'init_weights'", ")", ":", "\n", "        ", "module", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.init_weights_vit_moco": [[483, 496], ["isinstance", "hasattr", "math.sqrt", "torch.init.uniform_", "torch.init.xavier_uniform_", "torch.init.zeros_", "module.init_weights", "float"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights"], ["", "", "def", "init_weights_vit_moco", "(", "module", ":", "nn", ".", "Module", ",", "name", ":", "str", "=", "''", ")", ":", "\n", "    ", "\"\"\" ViT weight initialization, matching moco-v3 impl minus fixed PatchEmbed \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "        ", "if", "'qkv'", "in", "name", ":", "\n", "# treat the weights of Q, K, V separately", "\n", "            ", "val", "=", "math", ".", "sqrt", "(", "6.", "/", "float", "(", "module", ".", "weight", ".", "shape", "[", "0", "]", "//", "3", "+", "module", ".", "weight", ".", "shape", "[", "1", "]", ")", ")", "\n", "nn", ".", "init", ".", "uniform_", "(", "module", ".", "weight", ",", "-", "val", ",", "val", ")", "\n", "", "else", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_uniform_", "(", "module", ".", "weight", ")", "\n", "", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "", "elif", "hasattr", "(", "module", ",", "'init_weights'", ")", ":", "\n", "        ", "module", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.get_init_weights_vit": [[498, 505], ["functools.partial"], "function", ["None"], ["", "", "def", "get_init_weights_vit", "(", "mode", "=", "'jax'", ",", "head_bias", ":", "float", "=", "0.", ")", ":", "\n", "    ", "if", "'jax'", "in", "mode", ":", "\n", "        ", "return", "partial", "(", "init_weights_vit_jax", ",", "head_bias", "=", "head_bias", ")", "\n", "", "elif", "'moco'", "in", "mode", ":", "\n", "        ", "return", "init_weights_vit_moco", "\n", "", "else", ":", "\n", "        ", "return", "init_weights_vit_timm", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._load_weights": [[507, 586], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "np.load", "hasattr", "model.patch_embed.proj.weight.copy_", "model.patch_embed.proj.bias.copy_", "model.cls_token.copy_", "vision_transformer._load_weights._n2p"], "function", ["None"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "_load_weights", "(", "model", ":", "VisionTransformer", ",", "checkpoint_path", ":", "str", ",", "prefix", ":", "str", "=", "''", ")", ":", "\n", "    ", "\"\"\" Load weights from .npz checkpoints for official Google Brain Flax implementation\n    \"\"\"", "\n", "import", "numpy", "as", "np", "\n", "\n", "def", "_n2p", "(", "w", ",", "t", "=", "True", ")", ":", "\n", "        ", "if", "w", ".", "ndim", "==", "4", "and", "w", ".", "shape", "[", "0", "]", "==", "w", ".", "shape", "[", "1", "]", "==", "w", ".", "shape", "[", "2", "]", "==", "1", ":", "\n", "            ", "w", "=", "w", ".", "flatten", "(", ")", "\n", "", "if", "t", ":", "\n", "            ", "if", "w", ".", "ndim", "==", "4", ":", "\n", "                ", "w", "=", "w", ".", "transpose", "(", "[", "3", ",", "2", ",", "0", ",", "1", "]", ")", "\n", "", "elif", "w", ".", "ndim", "==", "3", ":", "\n", "                ", "w", "=", "w", ".", "transpose", "(", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "", "elif", "w", ".", "ndim", "==", "2", ":", "\n", "                ", "w", "=", "w", ".", "transpose", "(", "[", "1", ",", "0", "]", ")", "\n", "", "", "return", "torch", ".", "from_numpy", "(", "w", ")", "\n", "\n", "", "w", "=", "np", ".", "load", "(", "checkpoint_path", ")", "\n", "if", "not", "prefix", "and", "'opt/target/embedding/kernel'", "in", "w", ":", "\n", "        ", "prefix", "=", "'opt/target/'", "\n", "\n", "", "if", "hasattr", "(", "model", ".", "patch_embed", ",", "'backbone'", ")", ":", "\n", "# hybrid", "\n", "        ", "backbone", "=", "model", ".", "patch_embed", ".", "backbone", "\n", "stem_only", "=", "not", "hasattr", "(", "backbone", ",", "'stem'", ")", "\n", "stem", "=", "backbone", "if", "stem_only", "else", "backbone", ".", "stem", "\n", "stem", ".", "conv", ".", "weight", ".", "copy_", "(", "adapt_input_conv", "(", "stem", ".", "conv", ".", "weight", ".", "shape", "[", "1", "]", ",", "_n2p", "(", "w", "[", "f'{prefix}conv_root/kernel'", "]", ")", ")", ")", "\n", "stem", ".", "norm", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{prefix}gn_root/scale'", "]", ")", ")", "\n", "stem", ".", "norm", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{prefix}gn_root/bias'", "]", ")", ")", "\n", "if", "not", "stem_only", ":", "\n", "            ", "for", "i", ",", "stage", "in", "enumerate", "(", "backbone", ".", "stages", ")", ":", "\n", "                ", "for", "j", ",", "block", "in", "enumerate", "(", "stage", ".", "blocks", ")", ":", "\n", "                    ", "bp", "=", "f'{prefix}block{i + 1}/unit{j + 1}/'", "\n", "for", "r", "in", "range", "(", "3", ")", ":", "\n", "                        ", "getattr", "(", "block", ",", "f'conv{r + 1}'", ")", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{bp}conv{r + 1}/kernel'", "]", ")", ")", "\n", "getattr", "(", "block", ",", "f'norm{r + 1}'", ")", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{bp}gn{r + 1}/scale'", "]", ")", ")", "\n", "getattr", "(", "block", ",", "f'norm{r + 1}'", ")", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{bp}gn{r + 1}/bias'", "]", ")", ")", "\n", "", "if", "block", ".", "downsample", "is", "not", "None", ":", "\n", "                        ", "block", ".", "downsample", ".", "conv", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{bp}conv_proj/kernel'", "]", ")", ")", "\n", "block", ".", "downsample", ".", "norm", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{bp}gn_proj/scale'", "]", ")", ")", "\n", "block", ".", "downsample", ".", "norm", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{bp}gn_proj/bias'", "]", ")", ")", "\n", "", "", "", "", "embed_conv_w", "=", "_n2p", "(", "w", "[", "f'{prefix}embedding/kernel'", "]", ")", "\n", "", "else", ":", "\n", "        ", "embed_conv_w", "=", "adapt_input_conv", "(", "\n", "model", ".", "patch_embed", ".", "proj", ".", "weight", ".", "shape", "[", "1", "]", ",", "_n2p", "(", "w", "[", "f'{prefix}embedding/kernel'", "]", ")", ")", "\n", "", "model", ".", "patch_embed", ".", "proj", ".", "weight", ".", "copy_", "(", "embed_conv_w", ")", "\n", "model", ".", "patch_embed", ".", "proj", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{prefix}embedding/bias'", "]", ")", ")", "\n", "model", ".", "cls_token", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{prefix}cls'", "]", ",", "t", "=", "False", ")", ")", "\n", "pos_embed_w", "=", "_n2p", "(", "w", "[", "f'{prefix}Transformer/posembed_input/pos_embedding'", "]", ",", "t", "=", "False", ")", "\n", "if", "pos_embed_w", ".", "shape", "!=", "model", ".", "pos_embed", ".", "shape", ":", "\n", "        ", "pos_embed_w", "=", "resize_pos_embed", "(", "# resize pos embedding when different size from pretrained weights", "\n", "pos_embed_w", ",", "model", ".", "pos_embed", ",", "getattr", "(", "model", ",", "'num_tokens'", ",", "1", ")", ",", "model", ".", "patch_embed", ".", "grid_size", ")", "\n", "", "model", ".", "pos_embed", ".", "copy_", "(", "pos_embed_w", ")", "\n", "model", ".", "norm", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{prefix}Transformer/encoder_norm/scale'", "]", ")", ")", "\n", "model", ".", "norm", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{prefix}Transformer/encoder_norm/bias'", "]", ")", ")", "\n", "if", "isinstance", "(", "model", ".", "head", ",", "nn", ".", "Linear", ")", "and", "model", ".", "head", ".", "bias", ".", "shape", "[", "0", "]", "==", "w", "[", "f'{prefix}head/bias'", "]", ".", "shape", "[", "-", "1", "]", ":", "\n", "        ", "model", ".", "head", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{prefix}head/kernel'", "]", ")", ")", "\n", "model", ".", "head", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{prefix}head/bias'", "]", ")", ")", "\n", "# NOTE representation layer has been removed, not used in latest 21k/1k pretrained weights", "\n", "# if isinstance(getattr(model.pre_logits, 'fc', None), nn.Linear) and f'{prefix}pre_logits/bias' in w:", "\n", "#     model.pre_logits.fc.weight.copy_(_n2p(w[f'{prefix}pre_logits/kernel']))", "\n", "#     model.pre_logits.fc.bias.copy_(_n2p(w[f'{prefix}pre_logits/bias']))", "\n", "", "for", "i", ",", "block", "in", "enumerate", "(", "model", ".", "blocks", ".", "children", "(", ")", ")", ":", "\n", "        ", "block_prefix", "=", "f'{prefix}Transformer/encoderblock_{i}/'", "\n", "mha_prefix", "=", "block_prefix", "+", "'MultiHeadDotProductAttention_1/'", "\n", "block", ".", "norm1", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{block_prefix}LayerNorm_0/scale'", "]", ")", ")", "\n", "block", ".", "norm1", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{block_prefix}LayerNorm_0/bias'", "]", ")", ")", "\n", "block", ".", "attn", ".", "qkv", ".", "weight", ".", "copy_", "(", "torch", ".", "cat", "(", "[", "\n", "_n2p", "(", "w", "[", "f'{mha_prefix}{n}/kernel'", "]", ",", "t", "=", "False", ")", ".", "flatten", "(", "1", ")", ".", "T", "for", "n", "in", "(", "'query'", ",", "'key'", ",", "'value'", ")", "]", ")", ")", "\n", "block", ".", "attn", ".", "qkv", ".", "bias", ".", "copy_", "(", "torch", ".", "cat", "(", "[", "\n", "_n2p", "(", "w", "[", "f'{mha_prefix}{n}/bias'", "]", ",", "t", "=", "False", ")", ".", "reshape", "(", "-", "1", ")", "for", "n", "in", "(", "'query'", ",", "'key'", ",", "'value'", ")", "]", ")", ")", "\n", "block", ".", "attn", ".", "proj", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{mha_prefix}out/kernel'", "]", ")", ".", "flatten", "(", "1", ")", ")", "\n", "block", ".", "attn", ".", "proj", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{mha_prefix}out/bias'", "]", ")", ")", "\n", "for", "r", "in", "range", "(", "2", ")", ":", "\n", "            ", "getattr", "(", "block", ".", "mlp", ",", "f'fc{r + 1}'", ")", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{block_prefix}MlpBlock_3/Dense_{r}/kernel'", "]", ")", ")", "\n", "getattr", "(", "block", ".", "mlp", ",", "f'fc{r + 1}'", ")", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{block_prefix}MlpBlock_3/Dense_{r}/bias'", "]", ")", ")", "\n", "", "block", ".", "norm2", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{block_prefix}LayerNorm_2/scale'", "]", ")", ")", "\n", "block", ".", "norm2", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{block_prefix}LayerNorm_2/bias'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.resize_pos_embed": [[588, 608], ["_logger.info", "int", "_logger.info", "posemb_grid.permute().reshape.reshape().permute", "torch.interpolate", "posemb_grid.permute().reshape.permute().reshape", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "math.sqrt", "len", "len", "len", "posemb_grid.permute().reshape.reshape", "posemb_grid.permute().reshape.permute", "int", "math.sqrt"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.interpolate"], ["", "", "def", "resize_pos_embed", "(", "posemb", ",", "posemb_new", ",", "num_tokens", "=", "1", ",", "gs_new", "=", "(", ")", ")", ":", "\n", "# Rescale the grid of position embeddings when loading from state_dict. Adapted from", "\n", "# https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224", "\n", "    ", "_logger", ".", "info", "(", "'Resized position embedding: %s to %s'", ",", "posemb", ".", "shape", ",", "posemb_new", ".", "shape", ")", "\n", "ntok_new", "=", "posemb_new", ".", "shape", "[", "1", "]", "\n", "if", "num_tokens", ":", "\n", "        ", "posemb_tok", ",", "posemb_grid", "=", "posemb", "[", ":", ",", ":", "num_tokens", "]", ",", "posemb", "[", "0", ",", "num_tokens", ":", "]", "\n", "ntok_new", "-=", "num_tokens", "\n", "", "else", ":", "\n", "        ", "posemb_tok", ",", "posemb_grid", "=", "posemb", "[", ":", ",", ":", "0", "]", ",", "posemb", "[", "0", "]", "\n", "", "gs_old", "=", "int", "(", "math", ".", "sqrt", "(", "len", "(", "posemb_grid", ")", ")", ")", "\n", "if", "not", "len", "(", "gs_new", ")", ":", "# backwards compatibility", "\n", "        ", "gs_new", "=", "[", "int", "(", "math", ".", "sqrt", "(", "ntok_new", ")", ")", "]", "*", "2", "\n", "", "assert", "len", "(", "gs_new", ")", ">=", "2", "\n", "_logger", ".", "info", "(", "'Position embedding grid-size from %s to %s'", ",", "[", "gs_old", ",", "gs_old", "]", ",", "gs_new", ")", "\n", "posemb_grid", "=", "posemb_grid", ".", "reshape", "(", "1", ",", "gs_old", ",", "gs_old", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "posemb_grid", "=", "F", ".", "interpolate", "(", "posemb_grid", ",", "size", "=", "gs_new", ",", "mode", "=", "'bicubic'", ",", "align_corners", "=", "False", ")", "\n", "posemb_grid", "=", "posemb_grid", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ".", "reshape", "(", "1", ",", "gs_new", "[", "0", "]", "*", "gs_new", "[", "1", "]", ",", "-", "1", ")", "\n", "posemb", "=", "torch", ".", "cat", "(", "[", "posemb_tok", ",", "posemb_grid", "]", ",", "dim", "=", "1", ")", "\n", "return", "posemb", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.checkpoint_filter_fn": [[610, 630], ["state_dict.items", "resize_pos_embed.reshape", "len", "vision_transformer.resize_pos_embed", "getattr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.resize_pos_embed"], ["", "def", "checkpoint_filter_fn", "(", "state_dict", ",", "model", ")", ":", "\n", "    ", "\"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"", "\n", "out_dict", "=", "{", "}", "\n", "if", "'model'", "in", "state_dict", ":", "\n", "# For deit models", "\n", "        ", "state_dict", "=", "state_dict", "[", "'model'", "]", "\n", "", "for", "k", ",", "v", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "        ", "if", "'patch_embed.proj.weight'", "in", "k", "and", "len", "(", "v", ".", "shape", ")", "<", "4", ":", "\n", "# For old models that I trained prior to conv based patchification", "\n", "            ", "O", ",", "I", ",", "H", ",", "W", "=", "model", ".", "patch_embed", ".", "proj", ".", "weight", ".", "shape", "\n", "v", "=", "v", ".", "reshape", "(", "O", ",", "-", "1", ",", "H", ",", "W", ")", "\n", "", "elif", "k", "==", "'pos_embed'", "and", "v", ".", "shape", "!=", "model", ".", "pos_embed", ".", "shape", ":", "\n", "# To resize pos embedding when using model at different size from pretrained weights", "\n", "            ", "v", "=", "resize_pos_embed", "(", "\n", "v", ",", "model", ".", "pos_embed", ",", "getattr", "(", "model", ",", "'num_tokens'", ",", "1", ")", ",", "model", ".", "patch_embed", ".", "grid_size", ")", "\n", "", "elif", "'pre_logits'", "in", "k", ":", "\n", "# NOTE representation layer removed as not used in latest 21k/1k pretrained weights", "\n", "            ", "continue", "\n", "", "out_dict", "[", "k", "]", "=", "v", "\n", "", "return", "out_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer": [[632, 644], ["kwargs.get", "helpers.resolve_pretrained_cfg", "helpers.build_model_with_cfg", "RuntimeError", "kwargs.pop"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.resolve_pretrained_cfg", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "def", "_create_vision_transformer", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "kwargs", ".", "get", "(", "'features_only'", ",", "None", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'features_only not implemented for Vision Transformer models.'", ")", "\n", "\n", "", "pretrained_cfg", "=", "resolve_pretrained_cfg", "(", "variant", ",", "pretrained_cfg", "=", "kwargs", ".", "pop", "(", "'pretrained_cfg'", ",", "None", ")", ")", "\n", "model", "=", "build_model_with_cfg", "(", "\n", "VisionTransformer", ",", "variant", ",", "pretrained", ",", "\n", "pretrained_cfg", "=", "pretrained_cfg", ",", "\n", "pretrained_filter_fn", "=", "checkpoint_filter_fn", ",", "\n", "pretrained_custom_load", "=", "'npz'", "in", "pretrained_cfg", "[", "'url'", "]", ",", "\n", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_tiny_patch16_224": [[646, 653], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_tiny_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Tiny (Vit-Ti/16)\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "12", ",", "num_heads", "=", "3", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_tiny_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_tiny_patch16_384": [[655, 662], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_tiny_patch16_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Tiny (Vit-Ti/16) @ 384x384.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "12", ",", "num_heads", "=", "3", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_tiny_patch16_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_small_patch32_224": [[664, 671], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_small_patch32_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Small (ViT-S/32)\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "32", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_small_patch32_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_small_patch32_384": [[673, 680], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_small_patch32_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Small (ViT-S/32) at 384x384.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "32", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_small_patch32_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_small_patch16_224": [[682, 690], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_small_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Small (ViT-S/16)\n    NOTE I've replaced my previous 'small' model definition and weights with the small variant from the DeiT paper\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_small_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_small_patch16_384": [[692, 700], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_small_patch16_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Small (ViT-S/16)\n    NOTE I've replaced my previous 'small' model definition and weights with the small variant from the DeiT paper\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_small_patch16_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_base_patch32_224": [[702, 710], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch32_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/32) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k, source https://github.com/google-research/vision_transformer.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "32", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch32_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_base_patch32_384": [[712, 720], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch32_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base model (ViT-B/32) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "32", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch32_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_base_patch16_224": [[722, 730], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_base_patch16_384": [[732, 740], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch16_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch16_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_base_patch8_224": [[742, 750], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch8_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/8) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "8", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch8_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_large_patch32_224": [[752, 759], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_large_patch32_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Large model (ViT-L/32) from original paper (https://arxiv.org/abs/2010.11929). No pretrained weights.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "32", ",", "embed_dim", "=", "1024", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_large_patch32_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_large_patch32_384": [[761, 769], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_large_patch32_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Large model (ViT-L/32) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "32", ",", "embed_dim", "=", "1024", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_large_patch32_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_large_patch16_224": [[771, 779], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_large_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Large model (ViT-L/16) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "1024", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_large_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_large_patch16_384": [[781, 789], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_large_patch16_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Large model (ViT-L/16) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "1024", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_large_patch16_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_large_patch14_224": [[791, 798], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_large_patch14_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Large model (ViT-L/14)\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "14", ",", "embed_dim", "=", "1024", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_large_patch14_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_huge_patch14_224": [[800, 807], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_huge_patch14_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Huge model (ViT-H/14) from original paper (https://arxiv.org/abs/2010.11929).\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "14", ",", "embed_dim", "=", "1280", ",", "depth", "=", "32", ",", "num_heads", "=", "16", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_huge_patch14_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_giant_patch14_224": [[809, 816], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_giant_patch14_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Giant model (ViT-g/14) from `Scaling Vision Transformers` - https://arxiv.org/abs/2106.04560\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "14", ",", "embed_dim", "=", "1408", ",", "mlp_ratio", "=", "48", "/", "11", ",", "depth", "=", "40", ",", "num_heads", "=", "16", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_giant_patch14_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_gigantic_patch14_224": [[818, 825], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_gigantic_patch14_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Gigantic model (ViT-G/14) from `Scaling Vision Transformers` - https://arxiv.org/abs/2106.04560\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "14", ",", "embed_dim", "=", "1664", ",", "mlp_ratio", "=", "64", "/", "13", ",", "depth", "=", "48", ",", "num_heads", "=", "16", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_gigantic_patch14_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_tiny_patch16_224_in21k": [[827, 836], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_tiny_patch16_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Tiny (Vit-Ti/16).\n    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n    NOTE: this model has valid 21k classifier head and no representation (pre-logits) layer\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "12", ",", "num_heads", "=", "3", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_tiny_patch16_224_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_small_patch32_224_in21k": [[838, 847], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_small_patch32_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Small (ViT-S/16)\n    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n    NOTE: this model has valid 21k classifier head and no representation (pre-logits) layer\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "32", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_small_patch32_224_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_small_patch16_224_in21k": [[849, 858], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_small_patch16_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Small (ViT-S/16)\n    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n    NOTE: this model has valid 21k classifier head and no representation (pre-logits) layer\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_small_patch16_224_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_base_patch32_224_in21k": [[860, 869], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch32_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base model (ViT-B/32) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n    NOTE: this model has valid 21k classifier head and no representation (pre-logits) layer\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "32", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch32_224_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_base_patch16_224_in21k": [[871, 880], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch16_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n    NOTE: this model has valid 21k classifier head and no representation (pre-logits) layer\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch16_224_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_base_patch8_224_in21k": [[882, 891], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch8_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base model (ViT-B/8) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n    NOTE: this model has valid 21k classifier head and no representation (pre-logits) layer\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "8", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch8_224_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_large_patch32_224_in21k": [[893, 902], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_large_patch32_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Large model (ViT-L/32) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n    NOTE: this model has a representation layer but the 21k classifier head is zero'd out in original weights\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "32", ",", "embed_dim", "=", "1024", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_large_patch32_224_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_large_patch16_224_in21k": [[904, 913], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_large_patch16_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Large model (ViT-L/16) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n    NOTE: this model has valid 21k classifier head and no representation (pre-logits) layer\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "1024", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_large_patch16_224_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_huge_patch14_224_in21k": [[915, 924], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_huge_patch14_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Huge model (ViT-H/14) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n    NOTE: this model has a representation layer but the 21k classifier head is zero'd out in original weights\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "14", ",", "embed_dim", "=", "1280", ",", "depth", "=", "32", ",", "num_heads", "=", "16", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_huge_patch14_224_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_base_patch16_224_sam": [[926, 933], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch16_224_sam", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/16) w/ SAM pretrained weights. Paper: https://arxiv.org/abs/2106.01548\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch16_224_sam'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_base_patch32_224_sam": [[935, 942], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch32_224_sam", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/32) w/ SAM pretrained weights. Paper: https://arxiv.org/abs/2106.01548\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "32", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch32_224_sam'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_small_patch16_224_dino": [[944, 951], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_small_patch16_224_dino", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Small (ViT-S/16) w/ DINO pretrained weights (no head) - https://arxiv.org/abs/2104.14294\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_small_patch16_224_dino'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_small_patch8_224_dino": [[953, 960], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_small_patch8_224_dino", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Small (ViT-S/8) w/ DINO pretrained weights (no head) - https://arxiv.org/abs/2104.14294\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "8", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_small_patch8_224_dino'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_base_patch16_224_dino": [[962, 969], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch16_224_dino", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/16) /w DINO pretrained weights (no head) - https://arxiv.org/abs/2104.14294\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch16_224_dino'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_base_patch8_224_dino": [[971, 978], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch8_224_dino", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/8) w/ DINO pretrained weights (no head) - https://arxiv.org/abs/2104.14294\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "8", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch8_224_dino'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_base_patch16_224_miil_in21k": [[980, 988], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch16_224_miil_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n    Weights taken from: https://github.com/Alibaba-MIIL/ImageNet21K\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "qkv_bias", "=", "False", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch16_224_miil_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_base_patch16_224_miil": [[990, 998], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch16_224_miil", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n    Weights taken from: https://github.com/Alibaba-MIIL/ImageNet21K\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "qkv_bias", "=", "False", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch16_224_miil'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_base_patch32_plus_256": [[1002, 1009], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch32_plus_256", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/32+)\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "32", ",", "embed_dim", "=", "896", ",", "depth", "=", "12", ",", "num_heads", "=", "14", ",", "init_values", "=", "1e-5", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch32_plus_256'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_base_patch16_plus_240": [[1011, 1018], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch16_plus_240", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/16+)\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "896", ",", "depth", "=", "12", ",", "num_heads", "=", "14", ",", "init_values", "=", "1e-5", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch16_plus_240'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_base_patch16_rpn_224": [[1020, 1029], ["dict", "vision_transformer._create_vision_transformer", "kwargs.pop"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch16_rpn_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/16) w/ residual post-norm\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "qkv_bias", "=", "False", ",", "init_values", "=", "1e-5", ",", "class_token", "=", "False", ",", "\n", "block_fn", "=", "ResPostBlock", ",", "global_pool", "=", "kwargs", ".", "pop", "(", "'global_pool'", ",", "'avg'", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch16_rpn_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_small_patch16_36x1_224": [[1031, 1040], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_small_patch16_36x1_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base w/ LayerScale + 36 x 1 (36 block serial) config. Experimental, may remove.\n    Based on `Three things everyone should know about Vision Transformers` - https://arxiv.org/abs/2203.09795\n    Paper focuses on 24x2 + 48x1 for 'Small' width but those are extremely slow.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "36", ",", "num_heads", "=", "6", ",", "init_values", "=", "1e-5", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_small_patch16_36x1_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_small_patch16_18x2_224": [[1042, 1052], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_small_patch16_18x2_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Small w/ LayerScale + 18 x 2 (36 block parallel) config. Experimental, may remove.\n    Based on `Three things everyone should know about Vision Transformers` - https://arxiv.org/abs/2203.09795\n    Paper focuses on 24x2 + 48x1 for 'Small' width but those are extremely slow.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "18", ",", "num_heads", "=", "6", ",", "init_values", "=", "1e-5", ",", "block_fn", "=", "ParallelBlock", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_small_patch16_18x2_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer.vit_base_patch16_18x2_224": [[1054, 1063], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch16_18x2_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base w/ LayerScale + 18 x 2 (36 block parallel) config. Experimental, may remove.\n    Based on `Three things everyone should know about Vision Transformers` - https://arxiv.org/abs/2203.09795\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "18", ",", "num_heads", "=", "12", ",", "init_values", "=", "1e-5", ",", "block_fn", "=", "ParallelBlock", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch16_18x2_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.Attention.__init__": [[68, 78], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "dim", ",", "num_heads", "=", "8", ",", "qkv_bias", "=", "False", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "head_dim", "=", "dim", "//", "num_heads", "\n", "self", ".", "scale", "=", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "dim", ",", "3", "*", "dim", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.Attention.forward": [[79, 97], ["nest.Attention.qkv().reshape().permute", "nest.Attention.unbind", "nest.Attention.softmax", "nest.Attention.attn_drop", "nest.Attention.proj", "nest.Attention.proj_drop", "nest.Attention.qkv().reshape", "k.transpose", "nest.Attention.qkv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        x is shape: B (batch_size), T (image blocks), N (seq length per image block), C (embed dim)\n        \"\"\"", "\n", "B", ",", "T", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "# result of next line is (qkv, B, num (H)eads, T, N, (C')hannels per head)", "\n", "qkv", "=", "self", ".", "qkv", "(", "x", ")", ".", "reshape", "(", "B", ",", "T", ",", "N", ",", "3", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "3", ",", "0", ",", "4", ",", "1", ",", "2", ",", "5", ")", "\n", "q", ",", "k", ",", "v", "=", "qkv", ".", "unbind", "(", "0", ")", "# make torchscript happy (cannot use tensor as tuple)", "\n", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "scale", "# (B, H, T, N, N)", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "# (B, H, T, N, C'), permute -> (B, T, N, C', H)", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "4", ",", "1", ")", ".", "reshape", "(", "B", ",", "T", ",", "N", ",", "C", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "return", "x", "# (B, T, N, C)", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.TransformerLayer.__init__": [[105, 114], ["torch.nn.Module.__init__", "norm_layer", "nest.Attention", "norm_layer", "int", "layers.Mlp", "layers.DropPath", "torch.nn.Identity", "torch.nn.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "dim", ",", "num_heads", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "drop_path", "=", "0.", ",", "\n", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "attn", "=", "Attention", "(", "dim", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "mlp_hidden_dim", "=", "int", "(", "dim", "*", "mlp_ratio", ")", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "mlp_hidden_dim", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.TransformerLayer.forward": [[115, 120], ["nest.TransformerLayer.norm1", "nest.TransformerLayer.drop_path", "nest.TransformerLayer.drop_path", "nest.TransformerLayer.attn", "nest.TransformerLayer.mlp", "nest.TransformerLayer.norm2"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "y", "=", "self", ".", "norm1", "(", "x", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "attn", "(", "y", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.ConvPool.__init__": [[123, 128], ["torch.nn.Module.__init__", "layers.create_conv2d", "norm_layer", "layers.create_pool2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "norm_layer", ",", "pad_type", "=", "''", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv", "=", "create_conv2d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "3", ",", "padding", "=", "pad_type", ",", "bias", "=", "True", ")", "\n", "self", ".", "norm", "=", "norm_layer", "(", "out_channels", ")", "\n", "self", ".", "pool", "=", "create_pool2d", "(", "'max'", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "pad_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.ConvPool.forward": [[129, 140], ["layers._assert", "layers._assert", "nest.ConvPool.conv", "nest.ConvPool.norm().permute", "nest.ConvPool.pool", "nest.ConvPool.norm", "nest.ConvPool.permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        x is expected to have shape (B, C, H, W)\n        \"\"\"", "\n", "_assert", "(", "x", ".", "shape", "[", "-", "2", "]", "%", "2", "==", "0", ",", "'BlockAggregation requires even input spatial dims'", ")", "\n", "_assert", "(", "x", ".", "shape", "[", "-", "1", "]", "%", "2", "==", "0", ",", "'BlockAggregation requires even input spatial dims'", ")", "\n", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "# Layer norm done over channel dim only", "\n", "x", "=", "self", ".", "norm", "(", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "x", "=", "self", ".", "pool", "(", "x", ")", "\n", "return", "x", "# (B, C, H//2, W//2)", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.NestLevel.__init__": [[176, 200], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "len", "torch.nn.Sequential", "torch.nn.Sequential", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "nest.ConvPool", "torch.nn.Identity", "torch.nn.Identity", "len", "nest.TransformerLayer", "range"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "num_blocks", ",", "block_size", ",", "seq_length", ",", "num_heads", ",", "depth", ",", "embed_dim", ",", "prev_embed_dim", "=", "None", ",", "\n", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "True", ",", "drop_rate", "=", "0.", ",", "attn_drop_rate", "=", "0.", ",", "drop_path_rates", "=", "[", "]", ",", "\n", "norm_layer", "=", "None", ",", "act_layer", "=", "None", ",", "pad_type", "=", "''", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "block_size", "=", "block_size", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "self", ".", "pos_embed", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "num_blocks", ",", "seq_length", ",", "embed_dim", ")", ")", "\n", "\n", "if", "prev_embed_dim", "is", "not", "None", ":", "\n", "            ", "self", ".", "pool", "=", "ConvPool", "(", "prev_embed_dim", ",", "embed_dim", ",", "norm_layer", "=", "norm_layer", ",", "pad_type", "=", "pad_type", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "pool", "=", "nn", ".", "Identity", "(", ")", "\n", "\n", "# Transformer encoder", "\n", "", "if", "len", "(", "drop_path_rates", ")", ":", "\n", "            ", "assert", "len", "(", "drop_path_rates", ")", "==", "depth", ",", "'Must provide as many drop path rates as there are transformer layers'", "\n", "", "self", ".", "transformer_encoder", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "TransformerLayer", "(", "\n", "dim", "=", "embed_dim", ",", "num_heads", "=", "num_heads", ",", "mlp_ratio", "=", "mlp_ratio", ",", "qkv_bias", "=", "qkv_bias", ",", "\n", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "drop_path_rates", "[", "i", "]", ",", "\n", "norm_layer", "=", "norm_layer", ",", "act_layer", "=", "act_layer", ")", "\n", "for", "i", "in", "range", "(", "depth", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.NestLevel.forward": [[201, 216], ["nest.NestLevel.pool", "nest.NestLevel.permute", "nest.blockify", "nest.deblockify", "nest.NestLevel.permute", "helpers.checkpoint_seq", "nest.NestLevel.transformer_encoder", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.blockify", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.deblockify", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        expects x as (B, C, H, W)\n        \"\"\"", "\n", "x", "=", "self", ".", "pool", "(", "x", ")", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "# (B, H', W', C), switch to channels last for transformer", "\n", "x", "=", "blockify", "(", "x", ",", "self", ".", "block_size", ")", "# (B, T, N, C')", "\n", "x", "=", "x", "+", "self", ".", "pos_embed", "\n", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "transformer_encoder", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "transformer_encoder", "(", "x", ")", "# (B, T, N, C')", "\n", "", "x", "=", "deblockify", "(", "x", ",", "self", ".", "block_size", ")", "# (B, H', W', C')", "\n", "# Channel-first for block aggregation, and generally to replicate convnet feature map at each stage", "\n", "return", "x", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "# (B, C, H', W')", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.Nest.__init__": [[225, 319], ["torch.nn.Module.__init__", "isinstance", "int", "layers.PatchEmbed", "range", "torch.nn.Sequential", "torch.nn.Sequential", "norm_layer", "layers.create_classifier", "nest.Nest.init_weights", "isinstance", "layers.to_ntuple", "layers.to_ntuple", "layers.to_ntuple", "functools.partial", "x.tolist", "len", "levels.append", "locals", "math.sqrt", "math.sqrt", "torch.linspace().split", "torch.linspace().split", "torch.linspace().split", "torch.linspace().split", "nest.NestLevel", "dict", "len", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "sum"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights"], ["def", "__init__", "(", "\n", "self", ",", "img_size", "=", "224", ",", "in_chans", "=", "3", ",", "patch_size", "=", "4", ",", "num_levels", "=", "3", ",", "embed_dims", "=", "(", "128", ",", "256", ",", "512", ")", ",", "\n", "num_heads", "=", "(", "4", ",", "8", ",", "16", ")", ",", "depths", "=", "(", "2", ",", "2", ",", "20", ")", ",", "num_classes", "=", "1000", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "True", ",", "\n", "drop_rate", "=", "0.", ",", "attn_drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.5", ",", "norm_layer", "=", "None", ",", "act_layer", "=", "None", ",", "\n", "pad_type", "=", "''", ",", "weight_init", "=", "''", ",", "global_pool", "=", "'avg'", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img_size (int, tuple): input image size\n            in_chans (int): number of input channels\n            patch_size (int): patch size\n            num_levels (int): number of block hierarchies (T_d in the paper)\n            embed_dims (int, tuple): embedding dimensions of each level\n            num_heads (int, tuple): number of attention heads for each level\n            depths (int, tuple): number of transformer layers for each level\n            num_classes (int): number of classes for classification head\n            mlp_ratio (int): ratio of mlp hidden dim to embedding dim for MLP of transformer layers\n            qkv_bias (bool): enable bias for qkv if True\n            drop_rate (float): dropout rate for MLP of transformer layers, MSA final projection layer, and classifier\n            attn_drop_rate (float): attention dropout rate\n            drop_path_rate (float): stochastic depth rate\n            norm_layer: (nn.Module): normalization layer for transformer layers\n            act_layer: (nn.Module): activation layer in MLP of transformer layers\n            pad_type: str: Type of padding to use '' for PyTorch symmetric, 'same' for TF SAME\n            weight_init: (str): weight init scheme\n            global_pool: (str): type of pooling operation to apply to final feature map\n\n        Notes:\n            - Default values follow NesT-B from the original Jax code.\n            - `embed_dims`, `num_heads`, `depths` should be ints or tuples with length `num_levels`.\n            - For those following the paper, Table A1 may have errors!\n                - https://github.com/google-research/nested-transformer/issues/2\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "for", "param_name", "in", "[", "'embed_dims'", ",", "'num_heads'", ",", "'depths'", "]", ":", "\n", "            ", "param_value", "=", "locals", "(", ")", "[", "param_name", "]", "\n", "if", "isinstance", "(", "param_value", ",", "collections", ".", "abc", ".", "Sequence", ")", ":", "\n", "                ", "assert", "len", "(", "param_value", ")", "==", "num_levels", ",", "f'Require `len({param_name}) == num_levels`'", "\n", "\n", "", "", "embed_dims", "=", "to_ntuple", "(", "num_levels", ")", "(", "embed_dims", ")", "\n", "num_heads", "=", "to_ntuple", "(", "num_levels", ")", "(", "num_heads", ")", "\n", "depths", "=", "to_ntuple", "(", "num_levels", ")", "(", "depths", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "num_features", "=", "embed_dims", "[", "-", "1", "]", "\n", "self", ".", "feature_info", "=", "[", "]", "\n", "norm_layer", "=", "norm_layer", "or", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", "\n", "act_layer", "=", "act_layer", "or", "nn", ".", "GELU", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "self", ".", "num_levels", "=", "num_levels", "\n", "if", "isinstance", "(", "img_size", ",", "collections", ".", "abc", ".", "Sequence", ")", ":", "\n", "            ", "assert", "img_size", "[", "0", "]", "==", "img_size", "[", "1", "]", ",", "'Model only handles square inputs'", "\n", "img_size", "=", "img_size", "[", "0", "]", "\n", "", "assert", "img_size", "%", "patch_size", "==", "0", ",", "'`patch_size` must divide `img_size` evenly'", "\n", "self", ".", "patch_size", "=", "patch_size", "\n", "\n", "# Number of blocks at each level", "\n", "self", ".", "num_blocks", "=", "(", "4", "**", "torch", ".", "arange", "(", "num_levels", ")", ")", ".", "flip", "(", "0", ")", ".", "tolist", "(", ")", "\n", "assert", "(", "img_size", "//", "patch_size", ")", "%", "math", ".", "sqrt", "(", "self", ".", "num_blocks", "[", "0", "]", ")", "==", "0", ",", "'First level blocks don\\'t fit evenly. Check `img_size`, `patch_size`, and `num_levels`'", "\n", "\n", "# Block edge size in units of patches", "\n", "# Hint: (img_size // patch_size) gives number of patches along edge of image. sqrt(self.num_blocks[0]) is the", "\n", "#  number of blocks along edge of image", "\n", "self", ".", "block_size", "=", "int", "(", "(", "img_size", "//", "patch_size", ")", "//", "math", ".", "sqrt", "(", "self", ".", "num_blocks", "[", "0", "]", ")", ")", "\n", "\n", "# Patch embedding", "\n", "self", ".", "patch_embed", "=", "PatchEmbed", "(", "\n", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", ",", "in_chans", "=", "in_chans", ",", "embed_dim", "=", "embed_dims", "[", "0", "]", ",", "flatten", "=", "False", ")", "\n", "self", ".", "num_patches", "=", "self", ".", "patch_embed", ".", "num_patches", "\n", "self", ".", "seq_length", "=", "self", ".", "num_patches", "//", "self", ".", "num_blocks", "[", "0", "]", "\n", "\n", "# Build up each hierarchical level", "\n", "levels", "=", "[", "]", "\n", "dp_rates", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "sum", "(", "depths", ")", ")", ".", "split", "(", "depths", ")", "]", "\n", "prev_dim", "=", "None", "\n", "curr_stride", "=", "4", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "num_blocks", ")", ")", ":", "\n", "            ", "dim", "=", "embed_dims", "[", "i", "]", "\n", "levels", ".", "append", "(", "NestLevel", "(", "\n", "self", ".", "num_blocks", "[", "i", "]", ",", "self", ".", "block_size", ",", "self", ".", "seq_length", ",", "num_heads", "[", "i", "]", ",", "depths", "[", "i", "]", ",", "dim", ",", "prev_dim", ",", "\n", "mlp_ratio", ",", "qkv_bias", ",", "drop_rate", ",", "attn_drop_rate", ",", "dp_rates", "[", "i", "]", ",", "norm_layer", ",", "act_layer", ",", "pad_type", "=", "pad_type", ")", ")", "\n", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "dim", ",", "reduction", "=", "curr_stride", ",", "module", "=", "f'levels.{i}'", ")", "]", "\n", "prev_dim", "=", "dim", "\n", "curr_stride", "*=", "2", "\n", "", "self", ".", "levels", "=", "nn", ".", "Sequential", "(", "*", "levels", ")", "\n", "\n", "# Final normalization layer", "\n", "self", ".", "norm", "=", "norm_layer", "(", "embed_dims", "[", "-", "1", "]", ")", "\n", "\n", "# Classifier", "\n", "self", ".", "global_pool", ",", "self", ".", "head", "=", "create_classifier", "(", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n", "self", ".", "init_weights", "(", "weight_init", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.Nest.init_weights": [[320, 327], ["helpers.named_apply", "layers.trunc_normal_", "functools.partial", "math.log"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_apply", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "init_weights", "(", "self", ",", "mode", "=", "''", ")", ":", "\n", "        ", "assert", "mode", "in", "(", "'nlhb'", ",", "''", ")", "\n", "head_bias", "=", "-", "math", ".", "log", "(", "self", ".", "num_classes", ")", "if", "'nlhb'", "in", "mode", "else", "0.", "\n", "for", "level", "in", "self", ".", "levels", ":", "\n", "            ", "trunc_normal_", "(", "level", ".", "pos_embed", ",", "std", "=", ".02", ",", "a", "=", "-", "2", ",", "b", "=", "2", ")", "\n", "", "named_apply", "(", "partial", "(", "_init_nest_weights", ",", "head_bias", "=", "head_bias", ")", ",", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.Nest.no_weight_decay": [[328, 331], ["range", "len"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "no_weight_decay", "(", "self", ")", ":", "\n", "        ", "return", "{", "f'level.{i}.pos_embed'", "for", "i", "in", "range", "(", "len", "(", "self", ".", "levels", ")", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.Nest.group_matcher": [[332, 343], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "\n", "stem", "=", "r'^patch_embed'", ",", "# stem and embed", "\n", "blocks", "=", "[", "\n", "(", "r'^levels\\.(\\d+)'", "if", "coarse", "else", "r'^levels\\.(\\d+)\\.transformer_encoder\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^levels\\.(\\d+)\\.(?:pool|pos_embed)'", ",", "(", "0", ",", ")", ")", ",", "\n", "(", "r'^norm'", ",", "(", "99999", ",", ")", ")", "\n", "]", "\n", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.Nest.set_grad_checkpointing": [[344, 348], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "for", "l", "in", "self", ".", "levels", ":", "\n", "            ", "l", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.Nest.get_classifier": [[349, 352], ["None"], "methods", ["None"], ["", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.Nest.reset_classifier": [[353, 357], ["layers.create_classifier"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", ",", "self", ".", "head", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.Nest.forward_features": [[358, 364], ["nest.Nest.patch_embed", "nest.Nest.levels", "nest.Nest.norm().permute", "nest.Nest.norm", "nest.Nest.permute"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "patch_embed", "(", "x", ")", "\n", "x", "=", "self", ".", "levels", "(", "x", ")", "\n", "# Layer norm done over channel dim only (to NHWC and back)", "\n", "x", "=", "self", ".", "norm", "(", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.Nest.forward_head": [[365, 370], ["nest.Nest.global_pool", "torch.dropout", "torch.dropout", "nest.Nest.head"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "if", "self", ".", "drop_rate", ">", "0.", ":", "\n", "            ", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "drop_rate", ",", "training", "=", "self", ".", "training", ")", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.Nest.forward": [[371, 375], ["nest.Nest.forward_features", "nest.Nest.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest._cfg": [[38, 46], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "[", "14", ",", "14", "]", ",", "\n", "'crop_pct'", ":", ".875", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'patch_embed.proj'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.blockify": [[142, 156], ["layers._assert", "layers._assert", "x.transpose().reshape.reshape", "x.transpose().reshape.transpose().reshape", "x.transpose().reshape.transpose"], "function", ["None"], ["", "", "def", "blockify", "(", "x", ",", "block_size", ":", "int", ")", ":", "\n", "    ", "\"\"\"image to blocks\n    Args:\n        x (Tensor): with shape (B, H, W, C)\n        block_size (int): edge length of a single square block in units of H, W\n    \"\"\"", "\n", "B", ",", "H", ",", "W", ",", "C", "=", "x", ".", "shape", "\n", "_assert", "(", "H", "%", "block_size", "==", "0", ",", "'`block_size` must divide input height evenly'", ")", "\n", "_assert", "(", "W", "%", "block_size", "==", "0", ",", "'`block_size` must divide input width evenly'", ")", "\n", "grid_height", "=", "H", "//", "block_size", "\n", "grid_width", "=", "W", "//", "block_size", "\n", "x", "=", "x", ".", "reshape", "(", "B", ",", "grid_height", ",", "block_size", ",", "grid_width", ",", "block_size", ",", "C", ")", "\n", "x", "=", "x", ".", "transpose", "(", "2", ",", "3", ")", ".", "reshape", "(", "B", ",", "grid_height", "*", "grid_width", ",", "-", "1", ",", "C", ")", "\n", "return", "x", "# (B, T, N, C)", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.deblockify": [[158, 171], ["int", "x.transpose().reshape.reshape", "x.transpose().reshape.transpose().reshape", "math.sqrt", "x.transpose().reshape.transpose"], "function", ["None"], ["", "@", "register_notrace_function", "# reason: int receives Proxy", "\n", "def", "deblockify", "(", "x", ",", "block_size", ":", "int", ")", ":", "\n", "    ", "\"\"\"blocks to image\n    Args:\n        x (Tensor): with shape (B, T, N, C) where T is number of blocks and N is sequence size per block\n        block_size (int): edge length of a single square block in units of desired H, W\n    \"\"\"", "\n", "B", ",", "T", ",", "_", ",", "C", "=", "x", ".", "shape", "\n", "grid_size", "=", "int", "(", "math", ".", "sqrt", "(", "T", ")", ")", "\n", "height", "=", "width", "=", "grid_size", "*", "block_size", "\n", "x", "=", "x", ".", "reshape", "(", "B", ",", "grid_size", ",", "grid_size", ",", "block_size", ",", "block_size", ",", "C", ")", "\n", "x", "=", "x", ".", "transpose", "(", "2", ",", "3", ")", ".", "reshape", "(", "B", ",", "height", ",", "width", ",", "C", ")", "\n", "return", "x", "# (B, H, W, C)", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest._init_nest_weights": [[377, 393], ["isinstance", "name.startswith", "isinstance", "layers.trunc_normal_", "torch.nn.init.constant_", "layers.trunc_normal_", "layers.trunc_normal_", "torch.nn.init.zeros_", "torch.nn.init.zeros_"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["", "", "def", "_init_nest_weights", "(", "module", ":", "nn", ".", "Module", ",", "name", ":", "str", "=", "''", ",", "head_bias", ":", "float", "=", "0.", ")", ":", "\n", "    ", "\"\"\" NesT weight initialization\n    Can replicate Jax implementation. Otherwise follows vision_transformer.py\n    \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "        ", "if", "name", ".", "startswith", "(", "'head'", ")", ":", "\n", "            ", "trunc_normal_", "(", "module", ".", "weight", ",", "std", "=", ".02", ",", "a", "=", "-", "2", ",", "b", "=", "2", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "module", ".", "bias", ",", "head_bias", ")", "\n", "", "else", ":", "\n", "            ", "trunc_normal_", "(", "module", ".", "weight", ",", "std", "=", ".02", ",", "a", "=", "-", "2", ",", "b", "=", "2", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", ":", "\n", "        ", "trunc_normal_", "(", "module", ".", "weight", ",", "std", "=", ".02", ",", "a", "=", "-", "2", ",", "b", "=", "2", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.resize_pos_embed": [[395, 410], ["_logger.info", "int", "deblockify().permute", "torch.interpolate", "nest.blockify", "math.sqrt", "blockify.permute", "int", "nest.deblockify", "math.sqrt", "int", "math.sqrt"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.interpolate", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.blockify", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.deblockify"], ["", "", "", "def", "resize_pos_embed", "(", "posemb", ",", "posemb_new", ")", ":", "\n", "    ", "\"\"\"\n    Rescale the grid of position embeddings when loading from state_dict\n    Expected shape of position embeddings is (1, T, N, C), and considers only square images\n    \"\"\"", "\n", "_logger", ".", "info", "(", "'Resized position embedding: %s to %s'", ",", "posemb", ".", "shape", ",", "posemb_new", ".", "shape", ")", "\n", "seq_length_old", "=", "posemb", ".", "shape", "[", "2", "]", "\n", "num_blocks_new", ",", "seq_length_new", "=", "posemb_new", ".", "shape", "[", "1", ":", "3", "]", "\n", "size_new", "=", "int", "(", "math", ".", "sqrt", "(", "num_blocks_new", "*", "seq_length_new", ")", ")", "\n", "# First change to (1, C, H, W)", "\n", "posemb", "=", "deblockify", "(", "posemb", ",", "int", "(", "math", ".", "sqrt", "(", "seq_length_old", ")", ")", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "posemb", "=", "F", ".", "interpolate", "(", "posemb", ",", "size", "=", "[", "size_new", ",", "size_new", "]", ",", "mode", "=", "'bicubic'", ",", "align_corners", "=", "False", ")", "\n", "# Now change to new (1, T, N, C)", "\n", "posemb", "=", "blockify", "(", "posemb", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ",", "int", "(", "math", ".", "sqrt", "(", "seq_length_new", ")", ")", ")", "\n", "return", "posemb", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.checkpoint_filter_fn": [[412, 419], ["state_dict.keys", "k.startswith", "nest.resize_pos_embed", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.resize_pos_embed"], ["", "def", "checkpoint_filter_fn", "(", "state_dict", ",", "model", ")", ":", "\n", "    ", "\"\"\" resize positional embeddings of pretrained weights \"\"\"", "\n", "pos_embed_keys", "=", "[", "k", "for", "k", "in", "state_dict", ".", "keys", "(", ")", "if", "k", ".", "startswith", "(", "'pos_embed_'", ")", "]", "\n", "for", "k", "in", "pos_embed_keys", ":", "\n", "        ", "if", "state_dict", "[", "k", "]", ".", "shape", "!=", "getattr", "(", "model", ",", "k", ")", ".", "shape", ":", "\n", "            ", "state_dict", "[", "k", "]", "=", "resize_pos_embed", "(", "state_dict", "[", "k", "]", ",", "getattr", "(", "model", ",", "k", ")", ")", "\n", "", "", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest._create_nest": [[421, 429], ["helpers.build_model_with_cfg", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "def", "_create_nest", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model", "=", "build_model_with_cfg", "(", "\n", "Nest", ",", "variant", ",", "pretrained", ",", "\n", "feature_cfg", "=", "dict", "(", "out_indices", "=", "(", "0", ",", "1", ",", "2", ")", ",", "flatten_sequential", "=", "True", ")", ",", "\n", "pretrained_filter_fn", "=", "checkpoint_filter_fn", ",", "\n", "**", "kwargs", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.nest_base": [[431, 439], ["dict", "nest._create_nest"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest._create_nest"], ["", "@", "register_model", "\n", "def", "nest_base", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Nest-B @ 224x224\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "embed_dims", "=", "(", "128", ",", "256", ",", "512", ")", ",", "num_heads", "=", "(", "4", ",", "8", ",", "16", ")", ",", "depths", "=", "(", "2", ",", "2", ",", "20", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_nest", "(", "'nest_base'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.nest_small": [[441, 448], ["dict", "nest._create_nest"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest._create_nest"], ["", "@", "register_model", "\n", "def", "nest_small", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Nest-S @ 224x224\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "embed_dims", "=", "(", "96", ",", "192", ",", "384", ")", ",", "num_heads", "=", "(", "3", ",", "6", ",", "12", ")", ",", "depths", "=", "(", "2", ",", "2", ",", "20", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_nest", "(", "'nest_small'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.nest_tiny": [[450, 457], ["dict", "nest._create_nest"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest._create_nest"], ["", "@", "register_model", "\n", "def", "nest_tiny", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Nest-T @ 224x224\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "embed_dims", "=", "(", "96", ",", "192", ",", "384", ")", ",", "num_heads", "=", "(", "3", ",", "6", ",", "12", ")", ",", "depths", "=", "(", "2", ",", "2", ",", "8", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_nest", "(", "'nest_tiny'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.jx_nest_base": [[459, 467], ["dict", "nest._create_nest"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest._create_nest"], ["", "@", "register_model", "\n", "def", "jx_nest_base", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Nest-B @ 224x224, Pretrained weights converted from official Jax impl.\n    \"\"\"", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model_kwargs", "=", "dict", "(", "embed_dims", "=", "(", "128", ",", "256", ",", "512", ")", ",", "num_heads", "=", "(", "4", ",", "8", ",", "16", ")", ",", "depths", "=", "(", "2", ",", "2", ",", "20", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_nest", "(", "'jx_nest_base'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.jx_nest_small": [[469, 477], ["dict", "nest._create_nest"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest._create_nest"], ["", "@", "register_model", "\n", "def", "jx_nest_small", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Nest-S @ 224x224, Pretrained weights converted from official Jax impl.\n    \"\"\"", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model_kwargs", "=", "dict", "(", "embed_dims", "=", "(", "96", ",", "192", ",", "384", ")", ",", "num_heads", "=", "(", "3", ",", "6", ",", "12", ")", ",", "depths", "=", "(", "2", ",", "2", ",", "20", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_nest", "(", "'jx_nest_small'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest.jx_nest_tiny": [[479, 487], ["dict", "nest._create_nest"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nest._create_nest"], ["", "@", "register_model", "\n", "def", "jx_nest_tiny", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Nest-T @ 224x224, Pretrained weights converted from official Jax impl.\n    \"\"\"", "\n", "kwargs", "[", "'pad_type'", "]", "=", "'same'", "\n", "model_kwargs", "=", "dict", "(", "embed_dims", "=", "(", "96", ",", "192", ",", "384", ")", ",", "num_heads", "=", "(", "3", ",", "6", ",", "12", ")", ",", "depths", "=", "(", "2", ",", "2", ",", "8", ")", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_nest", "(", "'jx_nest_tiny'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.GammaAct.__init__": [[261, 266], ["torch.Module.__init__", "layers.get_act_fn"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_fn"], ["    ", "def", "__init__", "(", "self", ",", "act_type", "=", "'relu'", ",", "gamma", ":", "float", "=", "1.0", ",", "inplace", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "act_fn", "=", "get_act_fn", "(", "act_type", ")", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "inplace", "=", "inplace", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.GammaAct.forward": [[267, 269], ["nfnet.GammaAct.act_fn().mul_", "nfnet.GammaAct.act_fn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "act_fn", "(", "x", ",", "inplace", "=", "self", ".", "inplace", ")", ".", "mul_", "(", "self", ".", "gamma", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.DownsampleAvg.__init__": [[278, 289], ["torch.Module.__init__", "conv_layer", "avg_pool_fn", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "first_dilation", "=", "None", ",", "conv_layer", "=", "ScaledStdConv2d", ")", ":", "\n", "        ", "\"\"\" AvgPool Downsampling as in 'D' ResNet variants. Support for dilation.\"\"\"", "\n", "super", "(", "DownsampleAvg", ",", "self", ")", ".", "__init__", "(", ")", "\n", "avg_stride", "=", "stride", "if", "dilation", "==", "1", "else", "1", "\n", "if", "stride", ">", "1", "or", "dilation", ">", "1", ":", "\n", "            ", "avg_pool_fn", "=", "AvgPool2dSame", "if", "avg_stride", "==", "1", "and", "dilation", ">", "1", "else", "nn", ".", "AvgPool2d", "\n", "self", ".", "pool", "=", "avg_pool_fn", "(", "2", ",", "avg_stride", ",", "ceil_mode", "=", "True", ",", "count_include_pad", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "pool", "=", "nn", ".", "Identity", "(", ")", "\n", "", "self", ".", "conv", "=", "conv_layer", "(", "in_chs", ",", "out_chs", ",", "1", ",", "stride", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.DownsampleAvg.forward": [[290, 292], ["nfnet.DownsampleAvg.conv", "nfnet.DownsampleAvg.pool"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "conv", "(", "self", ".", "pool", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.NormFreeBlock.__init__": [[299, 343], ["torch.Module.__init__", "layers.make_divisible", "act_layer", "conv_layer", "act_layer", "conv_layer", "act_layer", "conv_layer", "nfnet.DownsampleAvg", "act_layer", "conv_layer", "attn_layer", "attn_layer", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Parameter", "torch.Parameter", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible"], ["def", "__init__", "(", "\n", "self", ",", "in_chs", ",", "out_chs", "=", "None", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "first_dilation", "=", "None", ",", "\n", "alpha", "=", "1.0", ",", "beta", "=", "1.0", ",", "bottle_ratio", "=", "0.25", ",", "group_size", "=", "None", ",", "ch_div", "=", "1", ",", "reg", "=", "True", ",", "extra_conv", "=", "False", ",", "\n", "skipinit", "=", "False", ",", "attn_layer", "=", "None", ",", "attn_gain", "=", "2.0", ",", "act_layer", "=", "None", ",", "conv_layer", "=", "None", ",", "drop_path_rate", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "first_dilation", "=", "first_dilation", "or", "dilation", "\n", "out_chs", "=", "out_chs", "or", "in_chs", "\n", "# RegNet variants scale bottleneck from in_chs, otherwise scale from out_chs like ResNet", "\n", "mid_chs", "=", "make_divisible", "(", "in_chs", "*", "bottle_ratio", "if", "reg", "else", "out_chs", "*", "bottle_ratio", ",", "ch_div", ")", "\n", "groups", "=", "1", "if", "not", "group_size", "else", "mid_chs", "//", "group_size", "\n", "if", "group_size", "and", "group_size", "%", "ch_div", "==", "0", ":", "\n", "            ", "mid_chs", "=", "group_size", "*", "groups", "# correct mid_chs if group_size divisible by ch_div, otherwise error", "\n", "", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "beta", "=", "beta", "\n", "self", ".", "attn_gain", "=", "attn_gain", "\n", "\n", "if", "in_chs", "!=", "out_chs", "or", "stride", "!=", "1", "or", "dilation", "!=", "first_dilation", ":", "\n", "            ", "self", ".", "downsample", "=", "DownsampleAvg", "(", "\n", "in_chs", ",", "out_chs", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "first_dilation", "=", "first_dilation", ",", "conv_layer", "=", "conv_layer", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "downsample", "=", "None", "\n", "\n", "", "self", ".", "act1", "=", "act_layer", "(", ")", "\n", "self", ".", "conv1", "=", "conv_layer", "(", "in_chs", ",", "mid_chs", ",", "1", ")", "\n", "self", ".", "act2", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "conv2", "=", "conv_layer", "(", "mid_chs", ",", "mid_chs", ",", "3", ",", "stride", "=", "stride", ",", "dilation", "=", "first_dilation", ",", "groups", "=", "groups", ")", "\n", "if", "extra_conv", ":", "\n", "            ", "self", ".", "act2b", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "conv2b", "=", "conv_layer", "(", "mid_chs", ",", "mid_chs", ",", "3", ",", "stride", "=", "1", ",", "dilation", "=", "dilation", ",", "groups", "=", "groups", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "act2b", "=", "None", "\n", "self", ".", "conv2b", "=", "None", "\n", "", "if", "reg", "and", "attn_layer", "is", "not", "None", ":", "\n", "            ", "self", ".", "attn", "=", "attn_layer", "(", "mid_chs", ")", "# RegNet blocks apply attn btw conv2 & 3", "\n", "", "else", ":", "\n", "            ", "self", ".", "attn", "=", "None", "\n", "", "self", ".", "act3", "=", "act_layer", "(", ")", "\n", "self", ".", "conv3", "=", "conv_layer", "(", "mid_chs", ",", "out_chs", ",", "1", ",", "gain_init", "=", "1.", "if", "skipinit", "else", "0.", ")", "\n", "if", "not", "reg", "and", "attn_layer", "is", "not", "None", ":", "\n", "            ", "self", ".", "attn_last", "=", "attn_layer", "(", "out_chs", ")", "# ResNet blocks apply attn after conv3", "\n", "", "else", ":", "\n", "            ", "self", ".", "attn_last", "=", "None", "\n", "", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path_rate", ")", "if", "drop_path_rate", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "skipinit_gain", "=", "nn", ".", "Parameter", "(", "torch", ".", "tensor", "(", "0.", ")", ")", "if", "skipinit", "else", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.NormFreeBlock.forward": [[344, 368], ["nfnet.NormFreeBlock.conv1", "nfnet.NormFreeBlock.conv2", "nfnet.NormFreeBlock.conv3", "nfnet.NormFreeBlock.drop_path", "nfnet.NormFreeBlock.act1", "nfnet.NormFreeBlock.downsample", "nfnet.NormFreeBlock.act2", "nfnet.NormFreeBlock.conv2b", "nfnet.NormFreeBlock.act3", "nfnet.NormFreeBlock.mul_", "nfnet.NormFreeBlock.act2b", "nfnet.NormFreeBlock.attn", "nfnet.NormFreeBlock.attn_last"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "out", "=", "self", ".", "act1", "(", "x", ")", "*", "self", ".", "beta", "\n", "\n", "# shortcut branch", "\n", "shortcut", "=", "x", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "shortcut", "=", "self", ".", "downsample", "(", "out", ")", "\n", "\n", "# residual branch", "\n", "", "out", "=", "self", ".", "conv1", "(", "out", ")", "\n", "out", "=", "self", ".", "conv2", "(", "self", ".", "act2", "(", "out", ")", ")", "\n", "if", "self", ".", "conv2b", "is", "not", "None", ":", "\n", "            ", "out", "=", "self", ".", "conv2b", "(", "self", ".", "act2b", "(", "out", ")", ")", "\n", "", "if", "self", ".", "attn", "is", "not", "None", ":", "\n", "            ", "out", "=", "self", ".", "attn_gain", "*", "self", ".", "attn", "(", "out", ")", "\n", "", "out", "=", "self", ".", "conv3", "(", "self", ".", "act3", "(", "out", ")", ")", "\n", "if", "self", ".", "attn_last", "is", "not", "None", ":", "\n", "            ", "out", "=", "self", ".", "attn_gain", "*", "self", ".", "attn_last", "(", "out", ")", "\n", "", "out", "=", "self", ".", "drop_path", "(", "out", ")", "\n", "\n", "if", "self", ".", "skipinit_gain", "is", "not", "None", ":", "\n", "            ", "out", ".", "mul_", "(", "self", ".", "skipinit_gain", ")", "# this slows things down more than expected, TBD", "\n", "", "out", "=", "out", "*", "self", ".", "alpha", "+", "shortcut", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.NormFreeNet.__init__": [[452, 546], ["torch.Module.__init__", "layers.make_divisible", "nfnet.create_stem", "enumerate", "torch.Sequential", "torch.Sequential", "layers.get_act_layer.", "layers.ClassifierHead", "nfnet.NormFreeNet.named_modules", "nfnet.act_with_gamma", "functools.partial", "layers.get_act_layer", "functools.partial", "functools.partial", "x.tolist", "range", "layers.make_divisible", "functools.partial.", "dict", "torch.Identity", "torch.Identity", "layers.get_attn", "torch.linspace().split", "torch.linspace().split", "torch.linspace().split", "torch.linspace().split", "layers.make_divisible", "dict", "torch.Sequential", "torch.Sequential", "isinstance", "isinstance", "nfnet.NormFreeBlock", "torch.init.zeros_", "torch.init.zeros_", "torch.init.normal_", "torch.init.normal_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.init.zeros_", "torch.init.zeros_", "sum"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.create_stem", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_modules", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.act_with_gamma", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.get_attn", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible"], ["def", "__init__", "(", "\n", "self", ",", "cfg", ":", "NfCfg", ",", "num_classes", "=", "1000", ",", "in_chans", "=", "3", ",", "global_pool", "=", "'avg'", ",", "output_stride", "=", "32", ",", "\n", "drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "assert", "cfg", ".", "act_layer", "in", "_nonlin_gamma", ",", "f\"Please add non-linearity constants for activation ({cfg.act_layer}).\"", "\n", "conv_layer", "=", "ScaledStdConv2dSame", "if", "cfg", ".", "same_padding", "else", "ScaledStdConv2d", "\n", "if", "cfg", ".", "gamma_in_act", ":", "\n", "            ", "act_layer", "=", "act_with_gamma", "(", "cfg", ".", "act_layer", ",", "gamma", "=", "_nonlin_gamma", "[", "cfg", ".", "act_layer", "]", ")", "\n", "conv_layer", "=", "partial", "(", "conv_layer", ",", "eps", "=", "cfg", ".", "std_conv_eps", ")", "\n", "", "else", ":", "\n", "            ", "act_layer", "=", "get_act_layer", "(", "cfg", ".", "act_layer", ")", "\n", "conv_layer", "=", "partial", "(", "conv_layer", ",", "gamma", "=", "_nonlin_gamma", "[", "cfg", ".", "act_layer", "]", ",", "eps", "=", "cfg", ".", "std_conv_eps", ")", "\n", "", "attn_layer", "=", "partial", "(", "get_attn", "(", "cfg", ".", "attn_layer", ")", ",", "**", "cfg", ".", "attn_kwargs", ")", "if", "cfg", ".", "attn_layer", "else", "None", "\n", "\n", "stem_chs", "=", "make_divisible", "(", "(", "cfg", ".", "stem_chs", "or", "cfg", ".", "channels", "[", "0", "]", ")", "*", "cfg", ".", "width_factor", ",", "cfg", ".", "ch_div", ")", "\n", "self", ".", "stem", ",", "stem_stride", ",", "stem_feat", "=", "create_stem", "(", "\n", "in_chans", ",", "stem_chs", ",", "cfg", ".", "stem_type", ",", "conv_layer", "=", "conv_layer", ",", "act_layer", "=", "act_layer", ")", "\n", "\n", "self", ".", "feature_info", "=", "[", "stem_feat", "]", "\n", "drop_path_rates", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "sum", "(", "cfg", ".", "depths", ")", ")", ".", "split", "(", "cfg", ".", "depths", ")", "]", "\n", "prev_chs", "=", "stem_chs", "\n", "net_stride", "=", "stem_stride", "\n", "dilation", "=", "1", "\n", "expected_var", "=", "1.0", "\n", "stages", "=", "[", "]", "\n", "for", "stage_idx", ",", "stage_depth", "in", "enumerate", "(", "cfg", ".", "depths", ")", ":", "\n", "            ", "stride", "=", "1", "if", "stage_idx", "==", "0", "and", "stem_stride", ">", "2", "else", "2", "\n", "if", "net_stride", ">=", "output_stride", "and", "stride", ">", "1", ":", "\n", "                ", "dilation", "*=", "stride", "\n", "stride", "=", "1", "\n", "", "net_stride", "*=", "stride", "\n", "first_dilation", "=", "1", "if", "dilation", "in", "(", "1", ",", "2", ")", "else", "2", "\n", "\n", "blocks", "=", "[", "]", "\n", "for", "block_idx", "in", "range", "(", "cfg", ".", "depths", "[", "stage_idx", "]", ")", ":", "\n", "                ", "first_block", "=", "block_idx", "==", "0", "and", "stage_idx", "==", "0", "\n", "out_chs", "=", "make_divisible", "(", "cfg", ".", "channels", "[", "stage_idx", "]", "*", "cfg", ".", "width_factor", ",", "cfg", ".", "ch_div", ")", "\n", "blocks", "+=", "[", "NormFreeBlock", "(", "\n", "in_chs", "=", "prev_chs", ",", "out_chs", "=", "out_chs", ",", "\n", "alpha", "=", "cfg", ".", "alpha", ",", "\n", "beta", "=", "1.", "/", "expected_var", "**", "0.5", ",", "\n", "stride", "=", "stride", "if", "block_idx", "==", "0", "else", "1", ",", "\n", "dilation", "=", "dilation", ",", "\n", "first_dilation", "=", "first_dilation", ",", "\n", "group_size", "=", "cfg", ".", "group_size", ",", "\n", "bottle_ratio", "=", "1.", "if", "cfg", ".", "reg", "and", "first_block", "else", "cfg", ".", "bottle_ratio", ",", "\n", "ch_div", "=", "cfg", ".", "ch_div", ",", "\n", "reg", "=", "cfg", ".", "reg", ",", "\n", "extra_conv", "=", "cfg", ".", "extra_conv", ",", "\n", "skipinit", "=", "cfg", ".", "skipinit", ",", "\n", "attn_layer", "=", "attn_layer", ",", "\n", "attn_gain", "=", "cfg", ".", "attn_gain", ",", "\n", "act_layer", "=", "act_layer", ",", "\n", "conv_layer", "=", "conv_layer", ",", "\n", "drop_path_rate", "=", "drop_path_rates", "[", "stage_idx", "]", "[", "block_idx", "]", ",", "\n", ")", "]", "\n", "if", "block_idx", "==", "0", ":", "\n", "                    ", "expected_var", "=", "1.", "# expected var is reset after first block of each stage", "\n", "", "expected_var", "+=", "cfg", ".", "alpha", "**", "2", "# Even if reset occurs, increment expected variance", "\n", "first_dilation", "=", "dilation", "\n", "prev_chs", "=", "out_chs", "\n", "", "self", ".", "feature_info", "+=", "[", "dict", "(", "num_chs", "=", "prev_chs", ",", "reduction", "=", "net_stride", ",", "module", "=", "f'stages.{stage_idx}'", ")", "]", "\n", "stages", "+=", "[", "nn", ".", "Sequential", "(", "*", "blocks", ")", "]", "\n", "", "self", ".", "stages", "=", "nn", ".", "Sequential", "(", "*", "stages", ")", "\n", "\n", "if", "cfg", ".", "num_features", ":", "\n", "# The paper NFRegNet models have an EfficientNet-like final head convolution.", "\n", "            ", "self", ".", "num_features", "=", "make_divisible", "(", "cfg", ".", "width_factor", "*", "cfg", ".", "num_features", ",", "cfg", ".", "ch_div", ")", "\n", "self", ".", "final_conv", "=", "conv_layer", "(", "prev_chs", ",", "self", ".", "num_features", ",", "1", ")", "\n", "self", ".", "feature_info", "[", "-", "1", "]", "=", "dict", "(", "num_chs", "=", "self", ".", "num_features", ",", "reduction", "=", "net_stride", ",", "module", "=", "f'final_conv'", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_features", "=", "prev_chs", "\n", "self", ".", "final_conv", "=", "nn", ".", "Identity", "(", ")", "\n", "", "self", ".", "final_act", "=", "act_layer", "(", "inplace", "=", "cfg", ".", "num_features", ">", "0", ")", "\n", "\n", "self", ".", "head", "=", "ClassifierHead", "(", "self", ".", "num_features", ",", "num_classes", ",", "pool_type", "=", "global_pool", ",", "drop_rate", "=", "self", ".", "drop_rate", ")", "\n", "\n", "for", "n", ",", "m", "in", "self", ".", "named_modules", "(", ")", ":", "\n", "            ", "if", "'fc'", "in", "n", "and", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "if", "cfg", ".", "zero_init_fc", ":", "\n", "                    ", "nn", ".", "init", ".", "zeros_", "(", "m", ".", "weight", ")", "\n", "", "else", ":", "\n", "                    ", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "0.", ",", ".01", ")", "\n", "", "if", "m", ".", "bias", "is", "not", "None", ":", "\n", "                    ", "nn", ".", "init", ".", "zeros_", "(", "m", ".", "bias", ")", "\n", "", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ",", "mode", "=", "'fan_in'", ",", "nonlinearity", "=", "'linear'", ")", "\n", "if", "m", ".", "bias", "is", "not", "None", ":", "\n", "                    ", "nn", ".", "init", ".", "zeros_", "(", "m", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.NormFreeNet.group_matcher": [[547, 557], ["dict"], "methods", ["None"], ["", "", "", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "\n", "stem", "=", "r'^stem'", ",", "\n", "blocks", "=", "[", "\n", "(", "r'^stages\\.(\\d+)'", "if", "coarse", "else", "r'^stages\\.(\\d+)\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^final_conv'", ",", "(", "99999", ",", ")", ")", "\n", "]", "\n", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.NormFreeNet.set_grad_checkpointing": [[558, 561], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.NormFreeNet.get_classifier": [[562, 565], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", ".", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.NormFreeNet.reset_classifier": [[566, 568], ["layers.ClassifierHead"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "head", "=", "ClassifierHead", "(", "self", ".", "num_features", ",", "num_classes", ",", "pool_type", "=", "global_pool", ",", "drop_rate", "=", "self", ".", "drop_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.NormFreeNet.forward_features": [[569, 578], ["nfnet.NormFreeNet.stem", "nfnet.NormFreeNet.final_conv", "nfnet.NormFreeNet.final_act", "helpers.checkpoint_seq", "nfnet.NormFreeNet.stages", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.checkpoint_seq", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hrnet.HighResolutionNet.stages"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "stem", "(", "x", ")", "\n", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "checkpoint_seq", "(", "self", ".", "stages", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "stages", "(", "x", ")", "\n", "", "x", "=", "self", ".", "final_conv", "(", "x", ")", "\n", "x", "=", "self", ".", "final_act", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.NormFreeNet.forward_head": [[579, 581], ["nfnet.NormFreeNet.head"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.NormFreeNet.forward": [[582, 586], ["nfnet.NormFreeNet.forward_features", "nfnet.NormFreeNet.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._dcfg": [[36, 44], ["None"], "function", ["None"], ["def", "_dcfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.9", ",", "'interpolation'", ":", "'bicubic'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'stem.conv1'", ",", "'classifier'", ":", "'head.fc'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._nfres_cfg": [[159, 166], ["nfnet.NfCfg"], "function", ["None"], ["", "def", "_nfres_cfg", "(", "\n", "depths", ",", "channels", "=", "(", "256", ",", "512", ",", "1024", ",", "2048", ")", ",", "group_size", "=", "None", ",", "act_layer", "=", "'relu'", ",", "attn_layer", "=", "None", ",", "attn_kwargs", "=", "None", ")", ":", "\n", "    ", "attn_kwargs", "=", "attn_kwargs", "or", "{", "}", "\n", "cfg", "=", "NfCfg", "(", "\n", "depths", "=", "depths", ",", "channels", "=", "channels", ",", "stem_type", "=", "'7x7_pool'", ",", "stem_chs", "=", "64", ",", "bottle_ratio", "=", "0.25", ",", "\n", "group_size", "=", "group_size", ",", "act_layer", "=", "act_layer", ",", "attn_layer", "=", "attn_layer", ",", "attn_kwargs", "=", "attn_kwargs", ")", "\n", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._nfreg_cfg": [[168, 175], ["dict", "nfnet.NfCfg"], "function", ["None"], ["", "def", "_nfreg_cfg", "(", "depths", ",", "channels", "=", "(", "48", ",", "104", ",", "208", ",", "440", ")", ")", ":", "\n", "    ", "num_features", "=", "1280", "*", "channels", "[", "-", "1", "]", "//", "440", "\n", "attn_kwargs", "=", "dict", "(", "rd_ratio", "=", "0.5", ")", "\n", "cfg", "=", "NfCfg", "(", "\n", "depths", "=", "depths", ",", "channels", "=", "channels", ",", "stem_type", "=", "'3x3'", ",", "group_size", "=", "8", ",", "width_factor", "=", "0.75", ",", "bottle_ratio", "=", "2.25", ",", "\n", "num_features", "=", "num_features", ",", "reg", "=", "True", ",", "attn_layer", "=", "'se'", ",", "attn_kwargs", "=", "attn_kwargs", ")", "\n", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._nfnet_cfg": [[177, 187], ["int", "nfnet.NfCfg", "dict"], "function", ["None"], ["", "def", "_nfnet_cfg", "(", "\n", "depths", ",", "channels", "=", "(", "256", ",", "512", ",", "1536", ",", "1536", ")", ",", "group_size", "=", "128", ",", "bottle_ratio", "=", "0.5", ",", "feat_mult", "=", "2.", ",", "\n", "act_layer", "=", "'gelu'", ",", "attn_layer", "=", "'se'", ",", "attn_kwargs", "=", "None", ")", ":", "\n", "    ", "num_features", "=", "int", "(", "channels", "[", "-", "1", "]", "*", "feat_mult", ")", "\n", "attn_kwargs", "=", "attn_kwargs", "if", "attn_kwargs", "is", "not", "None", "else", "dict", "(", "rd_ratio", "=", "0.5", ")", "\n", "cfg", "=", "NfCfg", "(", "\n", "depths", "=", "depths", ",", "channels", "=", "channels", ",", "stem_type", "=", "'deep_quad'", ",", "stem_chs", "=", "128", ",", "group_size", "=", "group_size", ",", "\n", "bottle_ratio", "=", "bottle_ratio", ",", "extra_conv", "=", "True", ",", "num_features", "=", "num_features", ",", "act_layer", "=", "act_layer", ",", "\n", "attn_layer", "=", "attn_layer", ",", "attn_kwargs", "=", "attn_kwargs", ")", "\n", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._dm_nfnet_cfg": [[189, 195], ["nfnet.NfCfg", "int", "dict"], "function", ["None"], ["", "def", "_dm_nfnet_cfg", "(", "depths", ",", "channels", "=", "(", "256", ",", "512", ",", "1536", ",", "1536", ")", ",", "act_layer", "=", "'gelu'", ",", "skipinit", "=", "True", ")", ":", "\n", "    ", "cfg", "=", "NfCfg", "(", "\n", "depths", "=", "depths", ",", "channels", "=", "channels", ",", "stem_type", "=", "'deep_quad'", ",", "stem_chs", "=", "128", ",", "group_size", "=", "128", ",", "\n", "bottle_ratio", "=", "0.5", ",", "extra_conv", "=", "True", ",", "gamma_in_act", "=", "True", ",", "same_padding", "=", "True", ",", "skipinit", "=", "skipinit", ",", "\n", "num_features", "=", "int", "(", "channels", "[", "-", "1", "]", "*", "2.0", ")", ",", "act_layer", "=", "act_layer", ",", "attn_layer", "=", "'se'", ",", "attn_kwargs", "=", "dict", "(", "rd_ratio", "=", "0.5", ")", ")", "\n", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.act_with_gamma": [[271, 275], ["nfnet.GammaAct"], "function", ["None"], ["", "", "def", "act_with_gamma", "(", "act_type", ",", "gamma", ":", "float", "=", "1.", ")", ":", "\n", "    ", "def", "_create", "(", "inplace", "=", "False", ")", ":", "\n", "        ", "return", "GammaAct", "(", "act_type", ",", "gamma", "=", "gamma", ",", "inplace", "=", "inplace", ")", "\n", "", "return", "_create", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.create_stem": [[370, 408], ["dict", "collections.OrderedDict", "enumerate", "torch.MaxPool2d", "torch.Sequential", "dict", "dict", "len", "zip", "conv_layer", "conv_layer", "conv_layer", "act_layer"], "function", ["None"], ["", "", "def", "create_stem", "(", "in_chs", ",", "out_chs", ",", "stem_type", "=", "''", ",", "conv_layer", "=", "None", ",", "act_layer", "=", "None", ",", "preact_feature", "=", "True", ")", ":", "\n", "    ", "stem_stride", "=", "2", "\n", "stem_feature", "=", "dict", "(", "num_chs", "=", "out_chs", ",", "reduction", "=", "2", ",", "module", "=", "'stem.conv'", ")", "\n", "stem", "=", "OrderedDict", "(", ")", "\n", "assert", "stem_type", "in", "(", "''", ",", "'deep'", ",", "'deep_tiered'", ",", "'deep_quad'", ",", "'3x3'", ",", "'7x7'", ",", "'deep_pool'", ",", "'3x3_pool'", ",", "'7x7_pool'", ")", "\n", "if", "'deep'", "in", "stem_type", ":", "\n", "        ", "if", "'quad'", "in", "stem_type", ":", "\n", "# 4 deep conv stack as in NFNet-F models", "\n", "            ", "assert", "not", "'pool'", "in", "stem_type", "\n", "stem_chs", "=", "(", "out_chs", "//", "8", ",", "out_chs", "//", "4", ",", "out_chs", "//", "2", ",", "out_chs", ")", "\n", "strides", "=", "(", "2", ",", "1", ",", "1", ",", "2", ")", "\n", "stem_stride", "=", "4", "\n", "stem_feature", "=", "dict", "(", "num_chs", "=", "out_chs", "//", "2", ",", "reduction", "=", "2", ",", "module", "=", "'stem.conv3'", ")", "\n", "", "else", ":", "\n", "            ", "if", "'tiered'", "in", "stem_type", ":", "\n", "                ", "stem_chs", "=", "(", "3", "*", "out_chs", "//", "8", ",", "out_chs", "//", "2", ",", "out_chs", ")", "# 'T' resnets in resnet.py", "\n", "", "else", ":", "\n", "                ", "stem_chs", "=", "(", "out_chs", "//", "2", ",", "out_chs", "//", "2", ",", "out_chs", ")", "# 'D' ResNets", "\n", "", "strides", "=", "(", "2", ",", "1", ",", "1", ")", "\n", "stem_feature", "=", "dict", "(", "num_chs", "=", "out_chs", "//", "2", ",", "reduction", "=", "2", ",", "module", "=", "'stem.conv2'", ")", "\n", "", "last_idx", "=", "len", "(", "stem_chs", ")", "-", "1", "\n", "for", "i", ",", "(", "c", ",", "s", ")", "in", "enumerate", "(", "zip", "(", "stem_chs", ",", "strides", ")", ")", ":", "\n", "            ", "stem", "[", "f'conv{i + 1}'", "]", "=", "conv_layer", "(", "in_chs", ",", "c", ",", "kernel_size", "=", "3", ",", "stride", "=", "s", ")", "\n", "if", "i", "!=", "last_idx", ":", "\n", "                ", "stem", "[", "f'act{i + 2}'", "]", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "", "in_chs", "=", "c", "\n", "", "", "elif", "'3x3'", "in", "stem_type", ":", "\n", "# 3x3 stem conv as in RegNet", "\n", "        ", "stem", "[", "'conv'", "]", "=", "conv_layer", "(", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", "", "else", ":", "\n", "# 7x7 stem conv as in ResNet", "\n", "        ", "stem", "[", "'conv'", "]", "=", "conv_layer", "(", "in_chs", ",", "out_chs", ",", "kernel_size", "=", "7", ",", "stride", "=", "2", ")", "\n", "\n", "", "if", "'pool'", "in", "stem_type", ":", "\n", "        ", "stem", "[", "'pool'", "]", "=", "nn", ".", "MaxPool2d", "(", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", "\n", "stem_stride", "=", "4", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "stem", ")", ",", "stem_stride", ",", "stem_feature", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet": [[588, 596], ["dict", "helpers.build_model_with_cfg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_normfreenet", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_cfg", "=", "model_cfgs", "[", "variant", "]", "\n", "feature_cfg", "=", "dict", "(", "flatten_sequential", "=", "True", ")", "\n", "return", "build_model_with_cfg", "(", "\n", "NormFreeNet", ",", "variant", ",", "pretrained", ",", "\n", "model_cfg", "=", "model_cfg", ",", "\n", "feature_cfg", "=", "feature_cfg", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.dm_nfnet_f0": [[598, 605], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "dm_nfnet_f0", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" NFNet-F0 (DeepMind weight compatible)\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'dm_nfnet_f0'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.dm_nfnet_f1": [[607, 614], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "dm_nfnet_f1", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" NFNet-F1 (DeepMind weight compatible)\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'dm_nfnet_f1'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.dm_nfnet_f2": [[616, 623], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "dm_nfnet_f2", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" NFNet-F2 (DeepMind weight compatible)\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'dm_nfnet_f2'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.dm_nfnet_f3": [[625, 632], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "dm_nfnet_f3", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" NFNet-F3 (DeepMind weight compatible)\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'dm_nfnet_f3'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.dm_nfnet_f4": [[634, 641], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "dm_nfnet_f4", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" NFNet-F4 (DeepMind weight compatible)\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'dm_nfnet_f4'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.dm_nfnet_f5": [[643, 650], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "dm_nfnet_f5", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" NFNet-F5 (DeepMind weight compatible)\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'dm_nfnet_f5'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.dm_nfnet_f6": [[652, 659], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "dm_nfnet_f6", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" NFNet-F6 (DeepMind weight compatible)\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'dm_nfnet_f6'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nfnet_f0": [[661, 668], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nfnet_f0", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" NFNet-F0\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nfnet_f0'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nfnet_f1": [[670, 677], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nfnet_f1", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" NFNet-F1\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nfnet_f1'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nfnet_f2": [[679, 686], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nfnet_f2", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" NFNet-F2\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nfnet_f2'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nfnet_f3": [[688, 695], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nfnet_f3", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" NFNet-F3\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nfnet_f3'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nfnet_f4": [[697, 704], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nfnet_f4", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" NFNet-F4\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nfnet_f4'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nfnet_f5": [[706, 713], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nfnet_f5", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" NFNet-F5\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nfnet_f5'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nfnet_f6": [[715, 722], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nfnet_f6", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" NFNet-F6\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nfnet_f6'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nfnet_f7": [[724, 731], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nfnet_f7", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" NFNet-F7\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nfnet_f7'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nfnet_l0": [[733, 739], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nfnet_l0", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" NFNet-L0b w/ SiLU\n    My experimental 'light' model w/ F0 repeats, 1.5x final_conv mult, 64 group_size, .25 bottleneck & SE ratio\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nfnet_l0'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.eca_nfnet_l0": [[741, 747], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "eca_nfnet_l0", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ECA-NFNet-L0 w/ SiLU\n    My experimental 'light' model w/ F0 repeats, 1.5x final_conv mult, 64 group_size, .25 bottleneck & ECA attn\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'eca_nfnet_l0'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.eca_nfnet_l1": [[749, 755], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "eca_nfnet_l1", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ECA-NFNet-L1 w/ SiLU\n    My experimental 'light' model w/ F1 repeats, 2.0x final_conv mult, 64 group_size, .25 bottleneck & ECA attn\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'eca_nfnet_l1'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.eca_nfnet_l2": [[757, 763], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "eca_nfnet_l2", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ECA-NFNet-L2 w/ SiLU\n    My experimental 'light' model w/ F2 repeats, 2.0x final_conv mult, 64 group_size, .25 bottleneck & ECA attn\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'eca_nfnet_l2'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.eca_nfnet_l3": [[765, 771], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "eca_nfnet_l3", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ECA-NFNet-L3 w/ SiLU\n    My experimental 'light' model w/ F3 repeats, 2.0x final_conv mult, 64 group_size, .25 bottleneck & ECA attn\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'eca_nfnet_l3'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nf_regnet_b0": [[773, 780], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nf_regnet_b0", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Normalization-Free RegNet-B0\n    `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n        - https://arxiv.org/abs/2101.08692\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nf_regnet_b0'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nf_regnet_b1": [[782, 789], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nf_regnet_b1", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Normalization-Free RegNet-B1\n    `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n        - https://arxiv.org/abs/2101.08692\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nf_regnet_b1'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nf_regnet_b2": [[791, 798], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nf_regnet_b2", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Normalization-Free RegNet-B2\n    `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n        - https://arxiv.org/abs/2101.08692\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nf_regnet_b2'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nf_regnet_b3": [[800, 807], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nf_regnet_b3", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Normalization-Free RegNet-B3\n    `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n        - https://arxiv.org/abs/2101.08692\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nf_regnet_b3'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nf_regnet_b4": [[809, 816], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nf_regnet_b4", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Normalization-Free RegNet-B4\n    `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n        - https://arxiv.org/abs/2101.08692\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nf_regnet_b4'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nf_regnet_b5": [[818, 825], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nf_regnet_b5", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Normalization-Free RegNet-B5\n    `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n        - https://arxiv.org/abs/2101.08692\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nf_regnet_b5'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nf_resnet26": [[827, 834], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nf_resnet26", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Normalization-Free ResNet-26\n    `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n        - https://arxiv.org/abs/2101.08692\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nf_resnet26'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nf_resnet50": [[836, 843], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nf_resnet50", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Normalization-Free ResNet-50\n    `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n        - https://arxiv.org/abs/2101.08692\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nf_resnet50'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nf_resnet101": [[845, 852], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nf_resnet101", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Normalization-Free ResNet-101\n    `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n        - https://arxiv.org/abs/2101.08692\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nf_resnet101'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nf_seresnet26": [[854, 859], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nf_seresnet26", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Normalization-Free SE-ResNet26\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nf_seresnet26'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nf_seresnet50": [[861, 866], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nf_seresnet50", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Normalization-Free SE-ResNet50\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nf_seresnet50'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nf_seresnet101": [[868, 873], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nf_seresnet101", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Normalization-Free SE-ResNet101\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nf_seresnet101'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nf_ecaresnet26": [[875, 880], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nf_ecaresnet26", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Normalization-Free ECA-ResNet26\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nf_ecaresnet26'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nf_ecaresnet50": [[882, 887], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nf_ecaresnet50", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Normalization-Free ECA-ResNet50\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nf_ecaresnet50'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet.nf_ecaresnet101": [[889, 894], ["nfnet._create_normfreenet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.nfnet._create_normfreenet"], ["", "@", "register_model", "\n", "def", "nf_ecaresnet101", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Normalization-Free ECA-ResNet101\n    \"\"\"", "\n", "return", "_create_normfreenet", "(", "'nf_ecaresnet101'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hardcorenas._cfg": [[14, 21], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bilinear'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'conv_stem'", ",", "'classifier'", ":", "'classifier'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hardcorenas._gen_hardcorenas": [[34, 68], ["functools.partial", "dict", "dict.pop", "helpers.build_model_with_cfg", "helpers.pretrained_cfg_for_features", "efficientnet_builder.decode_arch_def", "functools.partial", "efficientnet_builder.resolve_act_layer", "efficientnet_builder.resolve_bn_args"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.pretrained_cfg_for_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.decode_arch_def", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.efficientnet_builder.resolve_bn_args"], ["def", "_gen_hardcorenas", "(", "pretrained", ",", "variant", ",", "arch_def", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Creates a hardcorenas model\n\n    Ref impl: https://github.com/Alibaba-MIIL/HardCoReNAS\n    Paper: https://arxiv.org/abs/2102.11646\n\n    \"\"\"", "\n", "num_features", "=", "1280", "\n", "se_layer", "=", "partial", "(", "SqueezeExcite", ",", "gate_layer", "=", "'hard_sigmoid'", ",", "force_act_layer", "=", "nn", ".", "ReLU", ",", "rd_round_fn", "=", "round_channels", ")", "\n", "model_kwargs", "=", "dict", "(", "\n", "block_args", "=", "decode_arch_def", "(", "arch_def", ")", ",", "\n", "num_features", "=", "num_features", ",", "\n", "stem_size", "=", "32", ",", "\n", "norm_layer", "=", "partial", "(", "nn", ".", "BatchNorm2d", ",", "**", "resolve_bn_args", "(", "kwargs", ")", ")", ",", "\n", "act_layer", "=", "resolve_act_layer", "(", "kwargs", ",", "'hard_swish'", ")", ",", "\n", "se_layer", "=", "se_layer", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "\n", "features_only", "=", "False", "\n", "model_cls", "=", "MobileNetV3", "\n", "kwargs_filter", "=", "None", "\n", "if", "model_kwargs", ".", "pop", "(", "'features_only'", ",", "False", ")", ":", "\n", "        ", "features_only", "=", "True", "\n", "kwargs_filter", "=", "(", "'num_classes'", ",", "'num_features'", ",", "'global_pool'", ",", "'head_conv'", ",", "'head_bias'", ",", "'global_pool'", ")", "\n", "model_cls", "=", "MobileNetV3Features", "\n", "", "model", "=", "build_model_with_cfg", "(", "\n", "model_cls", ",", "variant", ",", "pretrained", ",", "\n", "pretrained_strict", "=", "not", "features_only", ",", "\n", "kwargs_filter", "=", "kwargs_filter", ",", "\n", "**", "model_kwargs", ")", "\n", "if", "features_only", ":", "\n", "        ", "model", ".", "default_cfg", "=", "pretrained_cfg_for_features", "(", "model", ".", "default_cfg", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hardcorenas.hardcorenas_a": [[70, 80], ["hardcorenas._gen_hardcorenas"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hardcorenas._gen_hardcorenas"], ["", "@", "register_model", "\n", "def", "hardcorenas_a", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" hardcorenas_A \"\"\"", "\n", "arch_def", "=", "[", "[", "'ds_r1_k3_s1_e1_c16_nre'", "]", ",", "[", "'ir_r1_k5_s2_e3_c24_nre'", ",", "'ir_r1_k5_s1_e3_c24_nre_se0.25'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e3_c40_nre'", ",", "'ir_r1_k5_s1_e6_c40_nre_se0.25'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e6_c80_se0.25'", ",", "'ir_r1_k5_s1_e6_c80_se0.25'", "]", ",", "\n", "[", "'ir_r1_k5_s1_e6_c112_se0.25'", ",", "'ir_r1_k5_s1_e6_c112_se0.25'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e6_c192_se0.25'", ",", "'ir_r1_k5_s1_e6_c192_se0.25'", "]", ",", "[", "'cn_r1_k1_s1_c960'", "]", "]", "\n", "model", "=", "_gen_hardcorenas", "(", "pretrained", "=", "pretrained", ",", "variant", "=", "'hardcorenas_a'", ",", "arch_def", "=", "arch_def", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hardcorenas.hardcorenas_b": [[82, 94], ["hardcorenas._gen_hardcorenas"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hardcorenas._gen_hardcorenas"], ["", "@", "register_model", "\n", "def", "hardcorenas_b", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" hardcorenas_B \"\"\"", "\n", "arch_def", "=", "[", "[", "'ds_r1_k3_s1_e1_c16_nre'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e3_c24_nre'", ",", "'ir_r1_k5_s1_e3_c24_nre_se0.25'", ",", "'ir_r1_k3_s1_e3_c24_nre'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e3_c40_nre'", ",", "'ir_r1_k5_s1_e3_c40_nre'", ",", "'ir_r1_k5_s1_e3_c40_nre'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e3_c80'", ",", "'ir_r1_k5_s1_e3_c80'", ",", "'ir_r1_k3_s1_e3_c80'", ",", "'ir_r1_k3_s1_e3_c80'", "]", ",", "\n", "[", "'ir_r1_k5_s1_e3_c112'", ",", "'ir_r1_k3_s1_e3_c112'", ",", "'ir_r1_k3_s1_e3_c112'", ",", "'ir_r1_k3_s1_e3_c112'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e6_c192_se0.25'", ",", "'ir_r1_k5_s1_e6_c192_se0.25'", ",", "'ir_r1_k3_s1_e3_c192_se0.25'", "]", ",", "\n", "[", "'cn_r1_k1_s1_c960'", "]", "]", "\n", "model", "=", "_gen_hardcorenas", "(", "pretrained", "=", "pretrained", ",", "variant", "=", "'hardcorenas_b'", ",", "arch_def", "=", "arch_def", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hardcorenas.hardcorenas_c": [[96, 108], ["hardcorenas._gen_hardcorenas"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hardcorenas._gen_hardcorenas"], ["", "@", "register_model", "\n", "def", "hardcorenas_c", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" hardcorenas_C \"\"\"", "\n", "arch_def", "=", "[", "[", "'ds_r1_k3_s1_e1_c16_nre'", "]", ",", "[", "'ir_r1_k5_s2_e3_c24_nre'", ",", "'ir_r1_k5_s1_e3_c24_nre_se0.25'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e3_c40_nre'", ",", "'ir_r1_k5_s1_e3_c40_nre'", ",", "'ir_r1_k5_s1_e3_c40_nre'", ",", "\n", "'ir_r1_k5_s1_e3_c40_nre'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e4_c80'", ",", "'ir_r1_k5_s1_e6_c80_se0.25'", ",", "'ir_r1_k3_s1_e3_c80'", ",", "'ir_r1_k3_s1_e3_c80'", "]", ",", "\n", "[", "'ir_r1_k5_s1_e6_c112_se0.25'", ",", "'ir_r1_k3_s1_e3_c112'", ",", "'ir_r1_k3_s1_e3_c112'", ",", "'ir_r1_k3_s1_e3_c112'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e6_c192_se0.25'", ",", "'ir_r1_k5_s1_e6_c192_se0.25'", ",", "'ir_r1_k3_s1_e3_c192_se0.25'", "]", ",", "\n", "[", "'cn_r1_k1_s1_c960'", "]", "]", "\n", "model", "=", "_gen_hardcorenas", "(", "pretrained", "=", "pretrained", ",", "variant", "=", "'hardcorenas_c'", ",", "arch_def", "=", "arch_def", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hardcorenas.hardcorenas_d": [[110, 123], ["hardcorenas._gen_hardcorenas"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hardcorenas._gen_hardcorenas"], ["", "@", "register_model", "\n", "def", "hardcorenas_d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" hardcorenas_D \"\"\"", "\n", "arch_def", "=", "[", "[", "'ds_r1_k3_s1_e1_c16_nre'", "]", ",", "[", "'ir_r1_k5_s2_e3_c24_nre_se0.25'", ",", "'ir_r1_k5_s1_e3_c24_nre_se0.25'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e3_c40_nre_se0.25'", ",", "'ir_r1_k5_s1_e4_c40_nre_se0.25'", ",", "'ir_r1_k3_s1_e3_c40_nre_se0.25'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e4_c80_se0.25'", ",", "'ir_r1_k3_s1_e3_c80_se0.25'", ",", "'ir_r1_k3_s1_e3_c80_se0.25'", ",", "\n", "'ir_r1_k3_s1_e3_c80_se0.25'", "]", ",", "\n", "[", "'ir_r1_k3_s1_e4_c112_se0.25'", ",", "'ir_r1_k5_s1_e4_c112_se0.25'", ",", "'ir_r1_k3_s1_e3_c112_se0.25'", ",", "\n", "'ir_r1_k5_s1_e3_c112_se0.25'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e6_c192_se0.25'", ",", "'ir_r1_k5_s1_e6_c192_se0.25'", ",", "'ir_r1_k5_s1_e6_c192_se0.25'", ",", "\n", "'ir_r1_k3_s1_e6_c192_se0.25'", "]", ",", "[", "'cn_r1_k1_s1_c960'", "]", "]", "\n", "model", "=", "_gen_hardcorenas", "(", "pretrained", "=", "pretrained", ",", "variant", "=", "'hardcorenas_d'", ",", "arch_def", "=", "arch_def", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hardcorenas.hardcorenas_e": [[125, 137], ["hardcorenas._gen_hardcorenas"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hardcorenas._gen_hardcorenas"], ["", "@", "register_model", "\n", "def", "hardcorenas_e", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" hardcorenas_E \"\"\"", "\n", "arch_def", "=", "[", "[", "'ds_r1_k3_s1_e1_c16_nre'", "]", ",", "[", "'ir_r1_k5_s2_e3_c24_nre_se0.25'", ",", "'ir_r1_k5_s1_e3_c24_nre_se0.25'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e6_c40_nre_se0.25'", ",", "'ir_r1_k5_s1_e4_c40_nre_se0.25'", ",", "'ir_r1_k5_s1_e4_c40_nre_se0.25'", ",", "\n", "'ir_r1_k3_s1_e3_c40_nre_se0.25'", "]", ",", "[", "'ir_r1_k5_s2_e4_c80_se0.25'", ",", "'ir_r1_k3_s1_e6_c80_se0.25'", "]", ",", "\n", "[", "'ir_r1_k5_s1_e6_c112_se0.25'", ",", "'ir_r1_k5_s1_e6_c112_se0.25'", ",", "'ir_r1_k5_s1_e6_c112_se0.25'", ",", "\n", "'ir_r1_k5_s1_e3_c112_se0.25'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e6_c192_se0.25'", ",", "'ir_r1_k5_s1_e6_c192_se0.25'", ",", "'ir_r1_k5_s1_e6_c192_se0.25'", ",", "\n", "'ir_r1_k3_s1_e6_c192_se0.25'", "]", ",", "[", "'cn_r1_k1_s1_c960'", "]", "]", "\n", "model", "=", "_gen_hardcorenas", "(", "pretrained", "=", "pretrained", ",", "variant", "=", "'hardcorenas_e'", ",", "arch_def", "=", "arch_def", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hardcorenas.hardcorenas_f": [[139, 152], ["hardcorenas._gen_hardcorenas"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.hardcorenas._gen_hardcorenas"], ["", "@", "register_model", "\n", "def", "hardcorenas_f", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" hardcorenas_F \"\"\"", "\n", "arch_def", "=", "[", "[", "'ds_r1_k3_s1_e1_c16_nre'", "]", ",", "[", "'ir_r1_k5_s2_e3_c24_nre_se0.25'", ",", "'ir_r1_k5_s1_e3_c24_nre_se0.25'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e6_c40_nre_se0.25'", ",", "'ir_r1_k5_s1_e6_c40_nre_se0.25'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e6_c80_se0.25'", ",", "'ir_r1_k5_s1_e6_c80_se0.25'", ",", "'ir_r1_k3_s1_e3_c80_se0.25'", ",", "\n", "'ir_r1_k3_s1_e3_c80_se0.25'", "]", ",", "\n", "[", "'ir_r1_k3_s1_e6_c112_se0.25'", ",", "'ir_r1_k5_s1_e6_c112_se0.25'", ",", "'ir_r1_k5_s1_e6_c112_se0.25'", ",", "\n", "'ir_r1_k3_s1_e3_c112_se0.25'", "]", ",", "\n", "[", "'ir_r1_k5_s2_e6_c192_se0.25'", ",", "'ir_r1_k5_s1_e6_c192_se0.25'", ",", "'ir_r1_k3_s1_e6_c192_se0.25'", ",", "\n", "'ir_r1_k3_s1_e6_c192_se0.25'", "]", ",", "[", "'cn_r1_k1_s1_c960'", "]", "]", "\n", "model", "=", "_gen_hardcorenas", "(", "pretrained", "=", "pretrained", ",", "variant", "=", "'hardcorenas_f'", ",", "arch_def", "=", "arch_def", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.__init__": [[22, 32], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "feature_info", ":", "List", "[", "Dict", "]", ",", "out_indices", ":", "Tuple", "[", "int", "]", ")", ":", "\n", "        ", "prev_reduction", "=", "1", "\n", "for", "fi", "in", "feature_info", ":", "\n", "# sanity check the mandatory fields, there may be additional fields depending on the model", "\n", "            ", "assert", "'num_chs'", "in", "fi", "and", "fi", "[", "'num_chs'", "]", ">", "0", "\n", "assert", "'reduction'", "in", "fi", "and", "fi", "[", "'reduction'", "]", ">=", "prev_reduction", "\n", "prev_reduction", "=", "fi", "[", "'reduction'", "]", "\n", "assert", "'module'", "in", "fi", "\n", "", "self", ".", "out_indices", "=", "out_indices", "\n", "self", ".", "info", "=", "feature_info", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.from_other": [[33, 35], ["features.FeatureInfo", "copy.deepcopy"], "methods", ["None"], ["", "def", "from_other", "(", "self", ",", "out_indices", ":", "Tuple", "[", "int", "]", ")", ":", "\n", "        ", "return", "FeatureInfo", "(", "deepcopy", "(", "self", ".", "info", ")", ",", "out_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get": [[36, 48], ["isinstance"], "methods", ["None"], ["", "def", "get", "(", "self", ",", "key", ",", "idx", "=", "None", ")", ":", "\n", "        ", "\"\"\" Get value by key at specified index (indices)\n        if idx == None, returns value for key at each output index\n        if idx is an integer, return value for that feature module index (ignoring output indices)\n        if idx is a list/tupple, return value for each module index (ignoring output indices)\n        \"\"\"", "\n", "if", "idx", "is", "None", ":", "\n", "            ", "return", "[", "self", ".", "info", "[", "i", "]", "[", "key", "]", "for", "i", "in", "self", ".", "out_indices", "]", "\n", "", "if", "isinstance", "(", "idx", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "return", "[", "self", ".", "info", "[", "i", "]", "[", "key", "]", "for", "i", "in", "idx", "]", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "info", "[", "idx", "]", "[", "key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get_dicts": [[49, 61], ["isinstance"], "methods", ["None"], ["", "", "def", "get_dicts", "(", "self", ",", "keys", "=", "None", ",", "idx", "=", "None", ")", ":", "\n", "        ", "\"\"\" return info dicts for specified keys (or all if None) at specified indices (or out_indices if None)\n        \"\"\"", "\n", "if", "idx", "is", "None", ":", "\n", "            ", "if", "keys", "is", "None", ":", "\n", "                ", "return", "[", "self", ".", "info", "[", "i", "]", "for", "i", "in", "self", ".", "out_indices", "]", "\n", "", "else", ":", "\n", "                ", "return", "[", "{", "k", ":", "self", ".", "info", "[", "i", "]", "[", "k", "]", "for", "k", "in", "keys", "}", "for", "i", "in", "self", ".", "out_indices", "]", "\n", "", "", "if", "isinstance", "(", "idx", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "return", "[", "self", ".", "info", "[", "i", "]", "if", "keys", "is", "None", "else", "{", "k", ":", "self", ".", "info", "[", "i", "]", "[", "k", "]", "for", "k", "in", "keys", "}", "for", "i", "in", "idx", "]", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "info", "[", "idx", "]", "if", "keys", "is", "None", "else", "{", "k", ":", "self", ".", "info", "[", "idx", "]", "[", "k", "]", "for", "k", "in", "keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.channels": [[62, 66], ["features.FeatureInfo.get"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["", "", "def", "channels", "(", "self", ",", "idx", "=", "None", ")", ":", "\n", "        ", "\"\"\" feature channels accessor\n        \"\"\"", "\n", "return", "self", ".", "get", "(", "'num_chs'", ",", "idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.reduction": [[67, 71], ["features.FeatureInfo.get"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["", "def", "reduction", "(", "self", ",", "idx", "=", "None", ")", ":", "\n", "        ", "\"\"\" feature reduction (output stride) accessor\n        \"\"\"", "\n", "return", "self", ".", "get", "(", "'reduction'", ",", "idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.module_name": [[72, 76], ["features.FeatureInfo.get"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["", "def", "module_name", "(", "self", ",", "idx", "=", "None", ")", ":", "\n", "        ", "\"\"\" feature module name accessor\n        \"\"\"", "\n", "return", "self", ".", "get", "(", "'module'", ",", "idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.__getitem__": [[77, 79], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "        ", "return", "self", ".", "info", "[", "item", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.__len__": [[80, 82], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "info", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureHooks.__init__": [[92, 108], ["enumerate", "collections.defaultdict", "functools.partial", "h.get", "m.register_forward_pre_hook", "m.register_forward_hook"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["def", "__init__", "(", "self", ",", "hooks", ",", "named_modules", ",", "out_map", "=", "None", ",", "default_hook_type", "=", "'forward'", ")", ":", "\n", "# setup feature hooks", "\n", "        ", "modules", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "named_modules", "}", "\n", "for", "i", ",", "h", "in", "enumerate", "(", "hooks", ")", ":", "\n", "            ", "hook_name", "=", "h", "[", "'module'", "]", "\n", "m", "=", "modules", "[", "hook_name", "]", "\n", "hook_id", "=", "out_map", "[", "i", "]", "if", "out_map", "else", "hook_name", "\n", "hook_fn", "=", "partial", "(", "self", ".", "_collect_output_hook", ",", "hook_id", ")", "\n", "hook_type", "=", "h", ".", "get", "(", "'hook_type'", ",", "default_hook_type", ")", "\n", "if", "hook_type", "==", "'forward_pre'", ":", "\n", "                ", "m", ".", "register_forward_pre_hook", "(", "hook_fn", ")", "\n", "", "elif", "hook_type", "==", "'forward'", ":", "\n", "                ", "m", ".", "register_forward_hook", "(", "hook_fn", ")", "\n", "", "else", ":", "\n", "                ", "assert", "False", ",", "\"Unsupported hook type\"", "\n", "", "", "self", ".", "_feature_outputs", "=", "defaultdict", "(", "OrderedDict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureHooks._collect_output_hook": [[109, 114], ["isinstance"], "methods", ["None"], ["", "def", "_collect_output_hook", "(", "self", ",", "hook_id", ",", "*", "args", ")", ":", "\n", "        ", "x", "=", "args", "[", "-", "1", "]", "# tensor we want is last argument, output for fwd, input for fwd_pre", "\n", "if", "isinstance", "(", "x", ",", "tuple", ")", ":", "\n", "            ", "x", "=", "x", "[", "0", "]", "# unwrap input tuple", "\n", "", "self", ".", "_feature_outputs", "[", "x", ".", "device", "]", "[", "hook_id", "]", "=", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureHooks.get_output": [[115, 119], ["collections.OrderedDict"], "methods", ["None"], ["", "def", "get_output", "(", "self", ",", "device", ")", "->", "Dict", "[", "str", ",", "torch", ".", "tensor", "]", ":", "\n", "        ", "output", "=", "self", ".", "_feature_outputs", "[", "device", "]", "\n", "self", ".", "_feature_outputs", "[", "device", "]", "=", "OrderedDict", "(", ")", "# clear after reading", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureDictNet.__init__": [[177, 199], ["torch.ModuleDict.__init__", "features._get_feature_info", "features._get_return_layers", "features._module_list", "set", "collections.OrderedDict", "features.FeatureDictNet.update", "_get_return_layers.keys", "str", "set.remove", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features._get_feature_info", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features._get_return_layers", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features._module_list", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update"], ["def", "__init__", "(", "\n", "self", ",", "model", ",", "\n", "out_indices", "=", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ",", "out_map", "=", "None", ",", "feature_concat", "=", "False", ",", "flatten_sequential", "=", "False", ")", ":", "\n", "        ", "super", "(", "FeatureDictNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "feature_info", "=", "_get_feature_info", "(", "model", ",", "out_indices", ")", "\n", "self", ".", "concat", "=", "feature_concat", "\n", "self", ".", "return_layers", "=", "{", "}", "\n", "return_layers", "=", "_get_return_layers", "(", "self", ".", "feature_info", ",", "out_map", ")", "\n", "modules", "=", "_module_list", "(", "model", ",", "flatten_sequential", "=", "flatten_sequential", ")", "\n", "remaining", "=", "set", "(", "return_layers", ".", "keys", "(", ")", ")", "\n", "layers", "=", "OrderedDict", "(", ")", "\n", "for", "new_name", ",", "old_name", ",", "module", "in", "modules", ":", "\n", "            ", "layers", "[", "new_name", "]", "=", "module", "\n", "if", "old_name", "in", "remaining", ":", "\n", "# return id has to be consistently str type for torchscript", "\n", "                ", "self", ".", "return_layers", "[", "new_name", "]", "=", "str", "(", "return_layers", "[", "old_name", "]", ")", "\n", "remaining", ".", "remove", "(", "old_name", ")", "\n", "", "if", "not", "remaining", ":", "\n", "                ", "break", "\n", "", "", "assert", "not", "remaining", "and", "len", "(", "self", ".", "return_layers", ")", "==", "len", "(", "return_layers", ")", ",", "f'Return layers ({remaining}) are not present in model'", "\n", "self", ".", "update", "(", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureDictNet._collect": [[200, 213], ["collections.OrderedDict", "features.FeatureDictNet.items", "module", "isinstance", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "_collect", "(", "self", ",", "x", ")", "->", "(", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", ":", "\n", "        ", "out", "=", "OrderedDict", "(", ")", "\n", "for", "name", ",", "module", "in", "self", ".", "items", "(", ")", ":", "\n", "            ", "x", "=", "module", "(", "x", ")", "\n", "if", "name", "in", "self", ".", "return_layers", ":", "\n", "                ", "out_id", "=", "self", ".", "return_layers", "[", "name", "]", "\n", "if", "isinstance", "(", "x", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "# If model tap is a tuple or list, concat or select first element", "\n", "# FIXME this may need to be more generic / flexible for some nets", "\n", "                    ", "out", "[", "out_id", "]", "=", "torch", ".", "cat", "(", "x", ",", "1", ")", "if", "self", ".", "concat", "else", "x", "[", "0", "]", "\n", "", "else", ":", "\n", "                    ", "out", "[", "out_id", "]", "=", "x", "\n", "", "", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureDictNet.forward": [[214, 216], ["features.FeatureDictNet._collect"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureDictNet._collect"], ["", "def", "forward", "(", "self", ",", "x", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "return", "self", ".", "_collect", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureListNet.__init__": [[224, 230], ["features.FeatureDictNet.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "model", ",", "\n", "out_indices", "=", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ",", "out_map", "=", "None", ",", "feature_concat", "=", "False", ",", "flatten_sequential", "=", "False", ")", ":", "\n", "        ", "super", "(", "FeatureListNet", ",", "self", ")", ".", "__init__", "(", "\n", "model", ",", "out_indices", "=", "out_indices", ",", "out_map", "=", "out_map", ",", "feature_concat", "=", "feature_concat", ",", "\n", "flatten_sequential", "=", "flatten_sequential", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureListNet.forward": [[231, 233], ["list", "features.FeatureListNet._collect().values", "features.FeatureListNet._collect"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureDictNet._collect"], ["", "def", "forward", "(", "self", ",", "x", ")", "->", "(", "List", "[", "torch", ".", "Tensor", "]", ")", ":", "\n", "        ", "return", "list", "(", "self", ".", "_collect", "(", "x", ")", ".", "values", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureHookNet.__init__": [[248, 279], ["torch.ModuleDict.__init__", "features._get_feature_info", "collections.OrderedDict", "features.FeatureHookNet.update", "features.FeatureHooks", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "hasattr", "hooks.extend", "features._module_list", "model.named_modules", "model.reset_classifier", "features.FeatureHookNet.feature_info.get_dicts", "module.named_modules", "features.FeatureHookNet.feature_info.get_dicts", "hooks.append", "dict"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features._get_feature_info", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features._module_list", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_modules", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.reset_classifier", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get_dicts", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_modules", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get_dicts"], ["def", "__init__", "(", "\n", "self", ",", "model", ",", "\n", "out_indices", "=", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ",", "out_map", "=", "None", ",", "out_as_dict", "=", "False", ",", "no_rewrite", "=", "False", ",", "\n", "feature_concat", "=", "False", ",", "flatten_sequential", "=", "False", ",", "default_hook_type", "=", "'forward'", ")", ":", "\n", "        ", "super", "(", "FeatureHookNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", "\n", "self", ".", "feature_info", "=", "_get_feature_info", "(", "model", ",", "out_indices", ")", "\n", "self", ".", "out_as_dict", "=", "out_as_dict", "\n", "layers", "=", "OrderedDict", "(", ")", "\n", "hooks", "=", "[", "]", "\n", "if", "no_rewrite", ":", "\n", "            ", "assert", "not", "flatten_sequential", "\n", "if", "hasattr", "(", "model", ",", "'reset_classifier'", ")", ":", "# make sure classifier is removed?", "\n", "                ", "model", ".", "reset_classifier", "(", "0", ")", "\n", "", "layers", "[", "'body'", "]", "=", "model", "\n", "hooks", ".", "extend", "(", "self", ".", "feature_info", ".", "get_dicts", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "modules", "=", "_module_list", "(", "model", ",", "flatten_sequential", "=", "flatten_sequential", ")", "\n", "remaining", "=", "{", "f", "[", "'module'", "]", ":", "f", "[", "'hook_type'", "]", "if", "'hook_type'", "in", "f", "else", "default_hook_type", "\n", "for", "f", "in", "self", ".", "feature_info", ".", "get_dicts", "(", ")", "}", "\n", "for", "new_name", ",", "old_name", ",", "module", "in", "modules", ":", "\n", "                ", "layers", "[", "new_name", "]", "=", "module", "\n", "for", "fn", ",", "fm", "in", "module", ".", "named_modules", "(", "prefix", "=", "old_name", ")", ":", "\n", "                    ", "if", "fn", "in", "remaining", ":", "\n", "                        ", "hooks", ".", "append", "(", "dict", "(", "module", "=", "fn", ",", "hook_type", "=", "remaining", "[", "fn", "]", ")", ")", "\n", "del", "remaining", "[", "fn", "]", "\n", "", "", "if", "not", "remaining", ":", "\n", "                    ", "break", "\n", "", "", "assert", "not", "remaining", ",", "f'Return layers ({remaining}) are not present in model'", "\n", "", "self", ".", "update", "(", "layers", ")", "\n", "self", ".", "hooks", "=", "FeatureHooks", "(", "hooks", ",", "model", ".", "named_modules", "(", ")", ",", "out_map", "=", "out_map", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureHookNet.forward": [[280, 285], ["features.FeatureHookNet.items", "features.FeatureHookNet.hooks.get_output", "module", "list", "features.FeatureHookNet.values"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureHooks.get_output"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "for", "name", ",", "module", "in", "self", ".", "items", "(", ")", ":", "\n", "            ", "x", "=", "module", "(", "x", ")", "\n", "", "out", "=", "self", ".", "hooks", ".", "get_output", "(", "x", ".", "device", ")", "\n", "return", "out", "if", "self", ".", "out_as_dict", "else", "list", "(", "out", ".", "values", "(", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features._module_list": [[121, 133], ["module.named_children", "isinstance", "module.named_children", "ml.append", "ml.append"], "function", ["None"], ["", "", "def", "_module_list", "(", "module", ",", "flatten_sequential", "=", "False", ")", ":", "\n", "# a yield/iter would be better for this but wouldn't be compatible with torchscript", "\n", "    ", "ml", "=", "[", "]", "\n", "for", "name", ",", "module", "in", "module", ".", "named_children", "(", ")", ":", "\n", "        ", "if", "flatten_sequential", "and", "isinstance", "(", "module", ",", "nn", ".", "Sequential", ")", ":", "\n", "# first level of Sequential containers is flattened into containing model", "\n", "            ", "for", "child_name", ",", "child_module", "in", "module", ".", "named_children", "(", ")", ":", "\n", "                ", "combined", "=", "[", "name", ",", "child_name", "]", "\n", "ml", ".", "append", "(", "(", "'_'", ".", "join", "(", "combined", ")", ",", "'.'", ".", "join", "(", "combined", ")", ",", "child_module", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "ml", ".", "append", "(", "(", "name", ",", "name", ",", "module", ")", ")", "\n", "", "", "return", "ml", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features._get_feature_info": [[135, 143], ["getattr", "isinstance", "getattr.from_other", "isinstance", "features.FeatureInfo"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.from_other"], ["", "def", "_get_feature_info", "(", "net", ",", "out_indices", ")", ":", "\n", "    ", "feature_info", "=", "getattr", "(", "net", ",", "'feature_info'", ")", "\n", "if", "isinstance", "(", "feature_info", ",", "FeatureInfo", ")", ":", "\n", "        ", "return", "feature_info", ".", "from_other", "(", "out_indices", ")", "\n", "", "elif", "isinstance", "(", "feature_info", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "return", "FeatureInfo", "(", "net", ".", "feature_info", ",", "out_indices", ")", "\n", "", "else", ":", "\n", "        ", "assert", "False", ",", "\"Provided feature_info is not valid\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features._get_return_layers": [[145, 151], ["feature_info.module_name", "enumerate"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.module_name"], ["", "", "def", "_get_return_layers", "(", "feature_info", ",", "out_map", ")", ":", "\n", "    ", "module_names", "=", "feature_info", ".", "module_name", "(", ")", "\n", "return_layers", "=", "{", "}", "\n", "for", "i", ",", "name", "in", "enumerate", "(", "module_names", ")", ":", "\n", "        ", "return_layers", "[", "name", "]", "=", "out_map", "[", "i", "]", "if", "out_map", "is", "not", "None", "else", "feature_info", ".", "out_indices", "[", "i", "]", "\n", "", "return", "return_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.PositionalEncodingFourier.__init__": [[112, 120], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "hidden_dim", "=", "32", ",", "dim", "=", "768", ",", "temperature", "=", "10000", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "token_projection", "=", "nn", ".", "Conv2d", "(", "hidden_dim", "*", "2", ",", "dim", ",", "kernel_size", "=", "1", ")", "\n", "self", ".", "scale", "=", "2", "*", "math", ".", "pi", "\n", "self", ".", "temperature", "=", "temperature", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "eps", "=", "1e-6", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.PositionalEncodingFourier.forward": [[121, 136], ["torch.arange().unsqueeze().repeat", "torch.arange().unsqueeze().repeat", "torch.arange().unsqueeze().repeat", "torch.arange().unsqueeze().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.cat().permute", "torch.cat().permute", "torch.cat().permute", "torch.cat().permute", "xcit.PositionalEncodingFourier.token_projection", "xcit.PositionalEncodingFourier.repeat", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.div", "torch.div", "torch.div", "torch.div", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "pos_x[].sin", "pos_x[].cos", "pos_y[].sin", "pos_y[].cos"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "B", ":", "int", ",", "H", ":", "int", ",", "W", ":", "int", ")", ":", "\n", "        ", "device", "=", "self", ".", "token_projection", ".", "weight", ".", "device", "\n", "y_embed", "=", "torch", ".", "arange", "(", "1", ",", "H", "+", "1", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", ")", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "1", ",", "W", ")", "\n", "x_embed", "=", "torch", ".", "arange", "(", "1", ",", "W", "+", "1", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", ")", ".", "repeat", "(", "1", ",", "H", ",", "1", ")", "\n", "y_embed", "=", "y_embed", "/", "(", "y_embed", "[", ":", ",", "-", "1", ":", ",", ":", "]", "+", "self", ".", "eps", ")", "*", "self", ".", "scale", "\n", "x_embed", "=", "x_embed", "/", "(", "x_embed", "[", ":", ",", ":", ",", "-", "1", ":", "]", "+", "self", ".", "eps", ")", "*", "self", ".", "scale", "\n", "dim_t", "=", "torch", ".", "arange", "(", "self", ".", "hidden_dim", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", ")", "\n", "dim_t", "=", "self", ".", "temperature", "**", "(", "2", "*", "torch", ".", "div", "(", "dim_t", ",", "2", ",", "rounding_mode", "=", "'floor'", ")", "/", "self", ".", "hidden_dim", ")", "\n", "pos_x", "=", "x_embed", "[", ":", ",", ":", ",", ":", ",", "None", "]", "/", "dim_t", "\n", "pos_y", "=", "y_embed", "[", ":", ",", ":", ",", ":", ",", "None", "]", "/", "dim_t", "\n", "pos_x", "=", "torch", ".", "stack", "(", "[", "pos_x", "[", ":", ",", ":", ",", ":", ",", "0", ":", ":", "2", "]", ".", "sin", "(", ")", ",", "pos_x", "[", ":", ",", ":", ",", ":", ",", "1", ":", ":", "2", "]", ".", "cos", "(", ")", "]", ",", "dim", "=", "4", ")", ".", "flatten", "(", "3", ")", "\n", "pos_y", "=", "torch", ".", "stack", "(", "[", "pos_y", "[", ":", ",", ":", ",", ":", ",", "0", ":", ":", "2", "]", ".", "sin", "(", ")", ",", "pos_y", "[", ":", ",", ":", ",", ":", ",", "1", ":", ":", "2", "]", ".", "cos", "(", ")", "]", ",", "dim", "=", "4", ")", ".", "flatten", "(", "3", ")", "\n", "pos", "=", "torch", ".", "cat", "(", "(", "pos_y", ",", "pos_x", ")", ",", "dim", "=", "3", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "pos", "=", "self", ".", "token_projection", "(", "pos", ")", "\n", "return", "pos", ".", "repeat", "(", "B", ",", "1", ",", "1", ",", "1", ")", "# (B, C, H, W)", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.ConvPatchEmbed.__init__": [[149, 177], ["torch.Module.__init__", "layers.to_2tuple", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "xcit.conv3x3", "act_layer", "xcit.conv3x3", "act_layer", "xcit.conv3x3", "act_layer", "xcit.conv3x3", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "xcit.conv3x3", "act_layer", "xcit.conv3x3", "act_layer", "xcit.conv3x3"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.conv3x3", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.conv3x3", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.conv3x3", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.conv3x3", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.conv3x3", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.conv3x3", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.conv3x3"], ["def", "__init__", "(", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "embed_dim", "=", "768", ",", "act_layer", "=", "nn", ".", "GELU", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "img_size", "=", "to_2tuple", "(", "img_size", ")", "\n", "num_patches", "=", "(", "img_size", "[", "1", "]", "//", "patch_size", ")", "*", "(", "img_size", "[", "0", "]", "//", "patch_size", ")", "\n", "self", ".", "img_size", "=", "img_size", "\n", "self", ".", "patch_size", "=", "patch_size", "\n", "self", ".", "num_patches", "=", "num_patches", "\n", "\n", "if", "patch_size", "==", "16", ":", "\n", "            ", "self", ".", "proj", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "conv3x3", "(", "in_chans", ",", "embed_dim", "//", "8", ",", "2", ")", ",", "\n", "act_layer", "(", ")", ",", "\n", "conv3x3", "(", "embed_dim", "//", "8", ",", "embed_dim", "//", "4", ",", "2", ")", ",", "\n", "act_layer", "(", ")", ",", "\n", "conv3x3", "(", "embed_dim", "//", "4", ",", "embed_dim", "//", "2", ",", "2", ")", ",", "\n", "act_layer", "(", ")", ",", "\n", "conv3x3", "(", "embed_dim", "//", "2", ",", "embed_dim", ",", "2", ")", ",", "\n", ")", "\n", "", "elif", "patch_size", "==", "8", ":", "\n", "            ", "self", ".", "proj", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "conv3x3", "(", "in_chans", ",", "embed_dim", "//", "4", ",", "2", ")", ",", "\n", "act_layer", "(", ")", ",", "\n", "conv3x3", "(", "embed_dim", "//", "4", ",", "embed_dim", "//", "2", ",", "2", ")", ",", "\n", "act_layer", "(", ")", ",", "\n", "conv3x3", "(", "embed_dim", "//", "2", ",", "embed_dim", ",", "2", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "raise", "(", "'For convolutional projection, patch size has to be in [8, 16]'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.ConvPatchEmbed.forward": [[178, 183], ["xcit.ConvPatchEmbed.proj", "x.flatten().transpose.flatten().transpose.flatten().transpose", "x.flatten().transpose.flatten().transpose.flatten"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "Hp", ",", "Wp", "=", "x", ".", "shape", "[", "2", "]", ",", "x", ".", "shape", "[", "3", "]", "\n", "x", "=", "x", ".", "flatten", "(", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "# (B, N, C)", "\n", "return", "x", ",", "(", "Hp", ",", "Wp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.LPI.__init__": [[192, 204], ["torch.Module.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "act_layer", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "in_features", ",", "out_features", "=", "None", ",", "act_layer", "=", "nn", ".", "GELU", ",", "kernel_size", "=", "3", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "out_features", "=", "out_features", "or", "in_features", "\n", "\n", "padding", "=", "kernel_size", "//", "2", "\n", "\n", "self", ".", "conv1", "=", "torch", ".", "nn", ".", "Conv2d", "(", "\n", "in_features", ",", "in_features", ",", "kernel_size", "=", "kernel_size", ",", "padding", "=", "padding", ",", "groups", "=", "in_features", ")", "\n", "self", ".", "act", "=", "act_layer", "(", ")", "\n", "self", ".", "bn", "=", "nn", ".", "BatchNorm2d", "(", "in_features", ")", "\n", "self", ".", "conv2", "=", "torch", ".", "nn", ".", "Conv2d", "(", "\n", "in_features", ",", "out_features", ",", "kernel_size", "=", "kernel_size", ",", "padding", "=", "padding", ",", "groups", "=", "out_features", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.LPI.forward": [[205, 214], ["x.reshape().permute.reshape().permute.permute().reshape", "xcit.LPI.conv1", "xcit.LPI.act", "xcit.LPI.bn", "xcit.LPI.conv2", "x.reshape().permute.reshape().permute.reshape().permute", "x.reshape().permute.reshape().permute.permute", "x.reshape().permute.reshape().permute.reshape"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "H", ":", "int", ",", "W", ":", "int", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ".", "reshape", "(", "B", ",", "C", ",", "H", ",", "W", ")", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "x", "=", "self", ".", "bn", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "x", "=", "x", ".", "reshape", "(", "B", ",", "C", ",", "N", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.ClassAttentionBlock.__init__": [[219, 240], ["torch.Module.__init__", "norm_layer", "cait.ClassAttn", "norm_layer", "vision_transformer.Mlp", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "int", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "dim", ",", "num_heads", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "drop_path", "=", "0.", ",", "\n", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ",", "eta", "=", "1.", ",", "tokens_norm", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "\n", "self", ".", "attn", "=", "ClassAttn", "(", "\n", "dim", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ")", "\n", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "int", "(", "dim", "*", "mlp_ratio", ")", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "\n", "if", "eta", "is", "not", "None", ":", "# LayerScale Initialization (no layerscale when None)", "\n", "            ", "self", ".", "gamma1", "=", "nn", ".", "Parameter", "(", "eta", "*", "torch", ".", "ones", "(", "dim", ")", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "gamma2", "=", "nn", ".", "Parameter", "(", "eta", "*", "torch", ".", "ones", "(", "dim", ")", ",", "requires_grad", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "gamma1", ",", "self", ".", "gamma2", "=", "1.0", ",", "1.0", "\n", "\n", "# See https://github.com/rwightman/pytorch-image-models/pull/747#issuecomment-877795721", "\n", "", "self", ".", "tokens_norm", "=", "tokens_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.ClassAttentionBlock.forward": [[241, 255], ["xcit.ClassAttentionBlock.norm1", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "xcit.ClassAttentionBlock.drop_path", "xcit.ClassAttentionBlock.norm2", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "xcit.ClassAttentionBlock.mlp", "xcit.ClassAttentionBlock.drop_path", "xcit.ClassAttentionBlock.attn", "xcit.ClassAttentionBlock.norm2"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_norm1", "=", "self", ".", "norm1", "(", "x", ")", "\n", "x_attn", "=", "torch", ".", "cat", "(", "[", "self", ".", "attn", "(", "x_norm1", ")", ",", "x_norm1", "[", ":", ",", "1", ":", "]", "]", ",", "dim", "=", "1", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "gamma1", "*", "x_attn", ")", "\n", "if", "self", ".", "tokens_norm", ":", "\n", "            ", "x", "=", "self", ".", "norm2", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "torch", ".", "cat", "(", "[", "self", ".", "norm2", "(", "x", "[", ":", ",", "0", ":", "1", "]", ")", ",", "x", "[", ":", ",", "1", ":", "]", "]", ",", "dim", "=", "1", ")", "\n", "", "x_res", "=", "x", "\n", "cls_token", "=", "x", "[", ":", ",", "0", ":", "1", "]", "\n", "cls_token", "=", "self", ".", "gamma2", "*", "self", ".", "mlp", "(", "cls_token", ")", "\n", "x", "=", "torch", ".", "cat", "(", "[", "cls_token", ",", "x", "[", ":", ",", "1", ":", "]", "]", ",", "dim", "=", "1", ")", "\n", "x", "=", "x_res", "+", "self", ".", "drop_path", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.XCA.__init__": [[263, 271], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "dim", ",", "num_heads", "=", "8", ",", "qkv_bias", "=", "False", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "temperature", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "num_heads", ",", "1", ",", "1", ")", ")", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", "*", "3", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.XCA.forward": [[272, 290], ["xcit.XCA.qkv().reshape().permute", "xcit.XCA.unbind", "torch.nn.functional.normalize", "torch.nn.functional.normalize", "torch.nn.functional.normalize", "torch.nn.functional.normalize", "torch.nn.functional.normalize", "torch.nn.functional.normalize", "torch.nn.functional.normalize", "torch.nn.functional.normalize", "xcit.XCA.softmax", "xcit.XCA.attn_drop", "xcit.XCA.proj", "xcit.XCA.proj_drop", "xcit.XCA.qkv().reshape", "torch.nn.functional.normalize.transpose", "torch.nn.functional.normalize.transpose", "xcit.XCA.qkv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "# Result of next line is (qkv, B, num (H)eads,  (C')hannels per head, N)", "\n", "qkv", "=", "self", ".", "qkv", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "3", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "4", ",", "1", ")", "\n", "q", ",", "k", ",", "v", "=", "qkv", ".", "unbind", "(", "0", ")", "# make torchscript happy (cannot use tensor as tuple)", "\n", "\n", "# Paper section 3.2 l2-Normalization and temperature scaling", "\n", "q", "=", "torch", ".", "nn", ".", "functional", ".", "normalize", "(", "q", ",", "dim", "=", "-", "1", ")", "\n", "k", "=", "torch", ".", "nn", ".", "functional", ".", "normalize", "(", "k", ",", "dim", "=", "-", "1", ")", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "temperature", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "# (B, H, C', N), permute -> (B, N, H, C')", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "N", ",", "C", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.XCA.no_weight_decay": [[291, 294], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "no_weight_decay", "(", "self", ")", ":", "\n", "        ", "return", "{", "'temperature'", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.XCABlock.__init__": [[297, 314], ["torch.Module.__init__", "norm_layer", "xcit.XCA", "norm_layer", "xcit.LPI", "norm_layer", "vision_transformer.Mlp", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "layers.DropPath", "torch.Identity", "torch.Identity", "int", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dim", ",", "num_heads", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "\n", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ",", "eta", "=", "1.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "attn", "=", "XCA", "(", "dim", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "norm3", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "local_mp", "=", "LPI", "(", "in_features", "=", "dim", ",", "act_layer", "=", "act_layer", ")", "\n", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "int", "(", "dim", "*", "mlp_ratio", ")", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "\n", "self", ".", "gamma1", "=", "nn", ".", "Parameter", "(", "eta", "*", "torch", ".", "ones", "(", "dim", ")", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "gamma3", "=", "nn", ".", "Parameter", "(", "eta", "*", "torch", ".", "ones", "(", "dim", ")", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "gamma2", "=", "nn", ".", "Parameter", "(", "eta", "*", "torch", ".", "ones", "(", "dim", ")", ",", "requires_grad", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.XCABlock.forward": [[315, 322], ["xcit.XCABlock.drop_path", "xcit.XCABlock.drop_path", "xcit.XCABlock.drop_path", "xcit.XCABlock.attn", "xcit.XCABlock.local_mp", "xcit.XCABlock.mlp", "xcit.XCABlock.norm1", "xcit.XCABlock.norm3", "xcit.XCABlock.norm2"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ",", "H", ":", "int", ",", "W", ":", "int", ")", ":", "\n", "        ", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "gamma1", "*", "self", ".", "attn", "(", "self", ".", "norm1", "(", "x", ")", ")", ")", "\n", "# NOTE official code has 3 then 2, so keeping it the same to be consistent with loaded weights", "\n", "# See https://github.com/rwightman/pytorch-image-models/pull/747#issuecomment-877795721", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "gamma3", "*", "self", ".", "local_mp", "(", "self", ".", "norm3", "(", "x", ")", ",", "H", ",", "W", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "gamma2", "*", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.XCiT.__init__": [[331, 400], ["torch.Module.__init__", "layers.to_2tuple", "xcit.ConvPatchEmbed", "torch.Parameter", "torch.Parameter", "torch.Dropout", "torch.Dropout", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "norm_layer", "layers.trunc_normal_", "xcit.XCiT.apply", "functools.partial", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "xcit.PositionalEncodingFourier", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "xcit.XCABlock", "xcit.ClassAttentionBlock", "range", "range"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["def", "__init__", "(", "\n", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "global_pool", "=", "'token'", ",", "embed_dim", "=", "768", ",", "\n", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "True", ",", "drop_rate", "=", "0.", ",", "attn_drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ",", "\n", "act_layer", "=", "None", ",", "norm_layer", "=", "None", ",", "cls_attn_layers", "=", "2", ",", "use_pos_embed", "=", "True", ",", "eta", "=", "1.", ",", "tokens_norm", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img_size (int, tuple): input image size\n            patch_size (int): patch size\n            in_chans (int): number of input channels\n            num_classes (int): number of classes for classification head\n            embed_dim (int): embedding dimension\n            depth (int): depth of transformer\n            num_heads (int): number of attention heads\n            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n            qkv_bias (bool): enable bias for qkv if True\n            drop_rate (float): dropout rate after positional embedding, and in XCA/CA projection + MLP\n            attn_drop_rate (float): attention dropout rate\n            drop_path_rate (float): stochastic depth rate (constant across all layers)\n            norm_layer: (nn.Module): normalization layer\n            cls_attn_layers: (int) Depth of Class attention layers\n            use_pos_embed: (bool) whether to use positional encoding\n            eta: (float) layerscale initialization value\n            tokens_norm: (bool) Whether to normalize all tokens or just the cls_token in the CA\n\n        Notes:\n            - Although `layer_norm` is user specifiable, there are hard-coded `BatchNorm2d`s in the local patch\n              interaction (class LPI) and the patch embedding (class ConvPatchEmbed)\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "global_pool", "in", "(", "''", ",", "'avg'", ",", "'token'", ")", "\n", "img_size", "=", "to_2tuple", "(", "img_size", ")", "\n", "assert", "(", "img_size", "[", "0", "]", "%", "patch_size", "==", "0", ")", "and", "(", "img_size", "[", "0", "]", "%", "patch_size", "==", "0", ")", ",", "'`patch_size` should divide image dimensions evenly'", "\n", "norm_layer", "=", "norm_layer", "or", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", "\n", "act_layer", "=", "act_layer", "or", "nn", ".", "GELU", "\n", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "num_features", "=", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "self", ".", "patch_embed", "=", "ConvPatchEmbed", "(", "\n", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", ",", "in_chans", "=", "in_chans", ",", "embed_dim", "=", "embed_dim", ",", "act_layer", "=", "act_layer", ")", "\n", "\n", "self", ".", "cls_token", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "self", ".", "use_pos_embed", "=", "use_pos_embed", "\n", "if", "use_pos_embed", ":", "\n", "            ", "self", ".", "pos_embed", "=", "PositionalEncodingFourier", "(", "dim", "=", "embed_dim", ")", "\n", "", "self", ".", "pos_drop", "=", "nn", ".", "Dropout", "(", "p", "=", "drop_rate", ")", "\n", "\n", "self", ".", "blocks", "=", "nn", ".", "ModuleList", "(", "[", "\n", "XCABlock", "(", "\n", "dim", "=", "embed_dim", ",", "num_heads", "=", "num_heads", ",", "mlp_ratio", "=", "mlp_ratio", ",", "qkv_bias", "=", "qkv_bias", ",", "drop", "=", "drop_rate", ",", "\n", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "drop_path_rate", ",", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ",", "eta", "=", "eta", ")", "\n", "for", "_", "in", "range", "(", "depth", ")", "]", ")", "\n", "\n", "self", ".", "cls_attn_blocks", "=", "nn", ".", "ModuleList", "(", "[", "\n", "ClassAttentionBlock", "(", "\n", "dim", "=", "embed_dim", ",", "num_heads", "=", "num_heads", ",", "mlp_ratio", "=", "mlp_ratio", ",", "qkv_bias", "=", "qkv_bias", ",", "drop", "=", "drop_rate", ",", "\n", "attn_drop", "=", "attn_drop_rate", ",", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ",", "eta", "=", "eta", ",", "tokens_norm", "=", "tokens_norm", ")", "\n", "for", "_", "in", "range", "(", "cls_attn_layers", ")", "]", ")", "\n", "\n", "# Classifier head", "\n", "self", ".", "norm", "=", "norm_layer", "(", "embed_dim", ")", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "# Init weights", "\n", "trunc_normal_", "(", "self", ".", "cls_token", ",", "std", "=", ".02", ")", "\n", "self", ".", "apply", "(", "self", ".", "_init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.XCiT._init_weights": [[401, 406], ["isinstance", "layers.trunc_normal_", "isinstance", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["", "def", "_init_weights", "(", "self", ",", "m", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "            ", "trunc_normal_", "(", "m", ".", "weight", ",", "std", "=", ".02", ")", "\n", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.XCiT.no_weight_decay": [[407, 410], ["None"], "methods", ["None"], ["", "", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "no_weight_decay", "(", "self", ")", ":", "\n", "        ", "return", "{", "'pos_embed'", ",", "'cls_token'", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.XCiT.group_matcher": [[411, 417], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^cls_token|pos_embed|patch_embed'", ",", "# stem and embed", "\n", "blocks", "=", "r'^blocks\\.(\\d+)'", ",", "\n", "cls_attn_blocks", "=", "[", "(", "r'^cls_attn_blocks\\.(\\d+)'", ",", "None", ")", ",", "(", "r'^norm'", ",", "(", "99999", ",", ")", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.XCiT.set_grad_checkpointing": [[419, 422], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "self", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.XCiT.get_classifier": [[423, 426], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.XCiT.reset_classifier": [[427, 433], ["torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "''", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "assert", "global_pool", "in", "(", "''", ",", "'avg'", ",", "'token'", ")", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.XCiT.forward_features": [[434, 461], ["xcit.XCiT.patch_embed", "xcit.XCiT.pos_drop", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "xcit.XCiT.norm", "xcit.XCiT.pos_embed().reshape().permute", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "blk", "xcit.XCiT.cls_token.expand", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "blk", "xcit.XCiT.pos_embed().reshape", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "xcit.XCiT.pos_embed"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "# x is (B, N, C). (Hp, Hw) is (height in units of patches, width in units of patches)", "\n", "x", ",", "(", "Hp", ",", "Wp", ")", "=", "self", ".", "patch_embed", "(", "x", ")", "\n", "\n", "if", "self", ".", "use_pos_embed", ":", "\n", "# `pos_embed` (B, C, Hp, Wp), reshape -> (B, C, N), permute -> (B, N, C)", "\n", "            ", "pos_encoding", "=", "self", ".", "pos_embed", "(", "B", ",", "Hp", ",", "Wp", ")", ".", "reshape", "(", "B", ",", "-", "1", ",", "x", ".", "shape", "[", "1", "]", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "x", "=", "x", "+", "pos_encoding", "\n", "", "x", "=", "self", ".", "pos_drop", "(", "x", ")", "\n", "\n", "for", "blk", "in", "self", ".", "blocks", ":", "\n", "            ", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "                ", "x", "=", "checkpoint", "(", "blk", ",", "x", ",", "Hp", ",", "Wp", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "blk", "(", "x", ",", "Hp", ",", "Wp", ")", "\n", "\n", "", "", "x", "=", "torch", ".", "cat", "(", "(", "self", ".", "cls_token", ".", "expand", "(", "B", ",", "-", "1", ",", "-", "1", ")", ",", "x", ")", ",", "dim", "=", "1", ")", "\n", "\n", "for", "blk", "in", "self", ".", "cls_attn_blocks", ":", "\n", "            ", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "                ", "x", "=", "checkpoint", "(", "blk", ",", "x", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "blk", "(", "x", ")", "\n", "\n", "", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.XCiT.forward_head": [[462, 466], ["xcit.XCiT.head", "x[].mean"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "global_pool", ":", "\n", "            ", "x", "=", "x", "[", ":", ",", "1", ":", "]", ".", "mean", "(", "dim", "=", "1", ")", "if", "self", ".", "global_pool", "==", "'avg'", "else", "x", "[", ":", ",", "0", "]", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.XCiT.forward": [[467, 471], ["xcit.XCiT.forward_features", "xcit.XCiT.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._cfg": [[30, 38], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", "1.0", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'patch_embed.proj.0.0'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.conv3x3": [[138, 143], ["torch.nn.Sequential", "torch.nn.Sequential", "torch.Conv2d", "torch.BatchNorm2d"], "function", ["None"], ["", "", "def", "conv3x3", "(", "in_planes", ",", "out_planes", ",", "stride", "=", "1", ")", ":", "\n", "    ", "\"\"\"3x3 convolution + batch norm\"\"\"", "\n", "return", "torch", ".", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "in_planes", ",", "out_planes", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "out_planes", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.checkpoint_filter_fn": [[473, 500], ["getattr", "len", "range", "k.startswith", "state_dict.pop", "model.state_dict", "state_dict.pop", "qkv_weight.reshape.reshape", "enumerate", "state_dict.pop", "qkv_bias.reshape.reshape", "enumerate", "k.replace"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict"], ["", "", "def", "checkpoint_filter_fn", "(", "state_dict", ",", "model", ")", ":", "\n", "    ", "if", "'model'", "in", "state_dict", ":", "\n", "        ", "state_dict", "=", "state_dict", "[", "'model'", "]", "\n", "# For consistency with timm's transformer models while being compatible with official weights source we rename", "\n", "# pos_embeder to pos_embed. Also account for use_pos_embed == False", "\n", "", "use_pos_embed", "=", "getattr", "(", "model", ",", "'pos_embed'", ",", "None", ")", "is", "not", "None", "\n", "pos_embed_keys", "=", "[", "k", "for", "k", "in", "state_dict", "if", "k", ".", "startswith", "(", "'pos_embed'", ")", "]", "\n", "for", "k", "in", "pos_embed_keys", ":", "\n", "        ", "if", "use_pos_embed", ":", "\n", "            ", "state_dict", "[", "k", ".", "replace", "(", "'pos_embeder.'", ",", "'pos_embed.'", ")", "]", "=", "state_dict", ".", "pop", "(", "k", ")", "\n", "", "else", ":", "\n", "            ", "del", "state_dict", "[", "k", "]", "\n", "# timm's implementation of class attention in CaiT is slightly more efficient as it does not compute query vectors", "\n", "# for all tokens, just the class token. To use official weights source we must split qkv into q, k, v", "\n", "", "", "if", "'cls_attn_blocks.0.attn.qkv.weight'", "in", "state_dict", "and", "'cls_attn_blocks.0.attn.q.weight'", "in", "model", ".", "state_dict", "(", ")", ":", "\n", "        ", "num_ca_blocks", "=", "len", "(", "model", ".", "cls_attn_blocks", ")", "\n", "for", "i", "in", "range", "(", "num_ca_blocks", ")", ":", "\n", "            ", "qkv_weight", "=", "state_dict", ".", "pop", "(", "f'cls_attn_blocks.{i}.attn.qkv.weight'", ")", "\n", "qkv_weight", "=", "qkv_weight", ".", "reshape", "(", "3", ",", "-", "1", ",", "qkv_weight", ".", "shape", "[", "-", "1", "]", ")", "\n", "for", "j", ",", "subscript", "in", "enumerate", "(", "'qkv'", ")", ":", "\n", "                ", "state_dict", "[", "f'cls_attn_blocks.{i}.attn.{subscript}.weight'", "]", "=", "qkv_weight", "[", "j", "]", "\n", "", "qkv_bias", "=", "state_dict", ".", "pop", "(", "f'cls_attn_blocks.{i}.attn.qkv.bias'", ",", "None", ")", "\n", "if", "qkv_bias", "is", "not", "None", ":", "\n", "                ", "qkv_bias", "=", "qkv_bias", ".", "reshape", "(", "3", ",", "-", "1", ")", "\n", "for", "j", ",", "subscript", "in", "enumerate", "(", "'qkv'", ")", ":", "\n", "                    ", "state_dict", "[", "f'cls_attn_blocks.{i}.attn.{subscript}.bias'", "]", "=", "qkv_bias", "[", "j", "]", "\n", "", "", "", "", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit": [[502, 506], ["helpers.build_model_with_cfg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "def", "_create_xcit", "(", "variant", ",", "pretrained", "=", "False", ",", "default_cfg", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "model", "=", "build_model_with_cfg", "(", "\n", "XCiT", ",", "variant", ",", "pretrained", ",", "pretrained_filter_fn", "=", "checkpoint_filter_fn", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_nano_12_p16_224": [[508, 514], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_nano_12_p16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "128", ",", "depth", "=", "12", ",", "num_heads", "=", "4", ",", "eta", "=", "1.0", ",", "tokens_norm", "=", "False", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_nano_12_p16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_nano_12_p16_224_dist": [[516, 522], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_nano_12_p16_224_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "128", ",", "depth", "=", "12", ",", "num_heads", "=", "4", ",", "eta", "=", "1.0", ",", "tokens_norm", "=", "False", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_nano_12_p16_224_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_nano_12_p16_384_dist": [[524, 530], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_nano_12_p16_384_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "128", ",", "depth", "=", "12", ",", "num_heads", "=", "4", ",", "eta", "=", "1.0", ",", "tokens_norm", "=", "False", ",", "img_size", "=", "384", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_nano_12_p16_384_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_tiny_12_p16_224": [[532, 538], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_tiny_12_p16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "12", ",", "num_heads", "=", "4", ",", "eta", "=", "1.0", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_tiny_12_p16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_tiny_12_p16_224_dist": [[540, 546], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_tiny_12_p16_224_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "12", ",", "num_heads", "=", "4", ",", "eta", "=", "1.0", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_tiny_12_p16_224_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_tiny_12_p16_384_dist": [[548, 554], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_tiny_12_p16_384_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "12", ",", "num_heads", "=", "4", ",", "eta", "=", "1.0", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_tiny_12_p16_384_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_small_12_p16_224": [[556, 562], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_small_12_p16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "8", ",", "eta", "=", "1.0", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_small_12_p16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_small_12_p16_224_dist": [[564, 570], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_small_12_p16_224_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "8", ",", "eta", "=", "1.0", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_small_12_p16_224_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_small_12_p16_384_dist": [[572, 578], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_small_12_p16_384_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "8", ",", "eta", "=", "1.0", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_small_12_p16_384_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_tiny_24_p16_224": [[580, 586], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_tiny_24_p16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "24", ",", "num_heads", "=", "4", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_tiny_24_p16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_tiny_24_p16_224_dist": [[588, 594], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_tiny_24_p16_224_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "24", ",", "num_heads", "=", "4", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_tiny_24_p16_224_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_tiny_24_p16_384_dist": [[596, 602], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_tiny_24_p16_384_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "24", ",", "num_heads", "=", "4", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_tiny_24_p16_384_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_small_24_p16_224": [[604, 610], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_small_24_p16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "24", ",", "num_heads", "=", "8", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_small_24_p16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_small_24_p16_224_dist": [[612, 618], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_small_24_p16_224_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "24", ",", "num_heads", "=", "8", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_small_24_p16_224_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_small_24_p16_384_dist": [[620, 626], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_small_24_p16_384_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "24", ",", "num_heads", "=", "8", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_small_24_p16_384_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_medium_24_p16_224": [[628, 634], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_medium_24_p16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "512", ",", "depth", "=", "24", ",", "num_heads", "=", "8", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_medium_24_p16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_medium_24_p16_224_dist": [[636, 642], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_medium_24_p16_224_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "512", ",", "depth", "=", "24", ",", "num_heads", "=", "8", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_medium_24_p16_224_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_medium_24_p16_384_dist": [[644, 650], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_medium_24_p16_384_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "512", ",", "depth", "=", "24", ",", "num_heads", "=", "8", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_medium_24_p16_384_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_large_24_p16_224": [[652, 658], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_large_24_p16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_large_24_p16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_large_24_p16_224_dist": [[660, 666], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_large_24_p16_224_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_large_24_p16_224_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_large_24_p16_384_dist": [[668, 674], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_large_24_p16_384_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_large_24_p16_384_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_nano_12_p8_224": [[677, 683], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_nano_12_p8_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "128", ",", "depth", "=", "12", ",", "num_heads", "=", "4", ",", "eta", "=", "1.0", ",", "tokens_norm", "=", "False", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_nano_12_p8_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_nano_12_p8_224_dist": [[685, 691], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_nano_12_p8_224_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "128", ",", "depth", "=", "12", ",", "num_heads", "=", "4", ",", "eta", "=", "1.0", ",", "tokens_norm", "=", "False", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_nano_12_p8_224_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_nano_12_p8_384_dist": [[693, 699], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_nano_12_p8_384_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "128", ",", "depth", "=", "12", ",", "num_heads", "=", "4", ",", "eta", "=", "1.0", ",", "tokens_norm", "=", "False", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_nano_12_p8_384_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_tiny_12_p8_224": [[701, 707], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_tiny_12_p8_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "192", ",", "depth", "=", "12", ",", "num_heads", "=", "4", ",", "eta", "=", "1.0", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_tiny_12_p8_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_tiny_12_p8_224_dist": [[709, 715], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_tiny_12_p8_224_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "192", ",", "depth", "=", "12", ",", "num_heads", "=", "4", ",", "eta", "=", "1.0", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_tiny_12_p8_224_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_tiny_12_p8_384_dist": [[717, 723], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_tiny_12_p8_384_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "192", ",", "depth", "=", "12", ",", "num_heads", "=", "4", ",", "eta", "=", "1.0", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_tiny_12_p8_384_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_small_12_p8_224": [[725, 731], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_small_12_p8_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "8", ",", "eta", "=", "1.0", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_small_12_p8_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_small_12_p8_224_dist": [[733, 739], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_small_12_p8_224_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "8", ",", "eta", "=", "1.0", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_small_12_p8_224_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_small_12_p8_384_dist": [[741, 747], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_small_12_p8_384_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "8", ",", "eta", "=", "1.0", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_small_12_p8_384_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_tiny_24_p8_224": [[749, 755], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_tiny_24_p8_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "192", ",", "depth", "=", "24", ",", "num_heads", "=", "4", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_tiny_24_p8_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_tiny_24_p8_224_dist": [[757, 763], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_tiny_24_p8_224_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "192", ",", "depth", "=", "24", ",", "num_heads", "=", "4", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_tiny_24_p8_224_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_tiny_24_p8_384_dist": [[765, 771], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_tiny_24_p8_384_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "192", ",", "depth", "=", "24", ",", "num_heads", "=", "4", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_tiny_24_p8_384_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_small_24_p8_224": [[773, 779], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_small_24_p8_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "384", ",", "depth", "=", "24", ",", "num_heads", "=", "8", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_small_24_p8_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_small_24_p8_224_dist": [[781, 787], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_small_24_p8_224_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "384", ",", "depth", "=", "24", ",", "num_heads", "=", "8", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_small_24_p8_224_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_small_24_p8_384_dist": [[789, 795], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_small_24_p8_384_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "384", ",", "depth", "=", "24", ",", "num_heads", "=", "8", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_small_24_p8_384_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_medium_24_p8_224": [[797, 803], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_medium_24_p8_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "512", ",", "depth", "=", "24", ",", "num_heads", "=", "8", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_medium_24_p8_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_medium_24_p8_224_dist": [[805, 811], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_medium_24_p8_224_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "512", ",", "depth", "=", "24", ",", "num_heads", "=", "8", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_medium_24_p8_224_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_medium_24_p8_384_dist": [[813, 819], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_medium_24_p8_384_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "512", ",", "depth", "=", "24", ",", "num_heads", "=", "8", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_medium_24_p8_384_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_large_24_p8_224": [[821, 827], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_large_24_p8_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "768", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_large_24_p8_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_large_24_p8_224_dist": [[829, 835], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_large_24_p8_224_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "768", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_large_24_p8_224_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit.xcit_large_24_p8_384_dist": [[837, 843], ["dict", "xcit._create_xcit"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.xcit._create_xcit"], ["", "@", "register_model", "\n", "def", "xcit_large_24_p8_384_dist", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "768", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "eta", "=", "1e-5", ",", "tokens_norm", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_xcit", "(", "'xcit_large_24_p8_384_dist'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet._cfg": [[23, 31], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.95", ",", "'interpolation'", ":", "'bicubic'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'stem.conv1.conv'", ",", "'classifier'", ":", "'head.fc'", ",", "\n", "'fixed_input_size'", ":", "False", ",", "'min_input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet._create_byoanet": [[327, 333], ["helpers.build_model_with_cfg", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["def", "_create_byoanet", "(", "variant", ",", "cfg_variant", "=", "None", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "\n", "ByobNet", ",", "variant", ",", "pretrained", ",", "\n", "model_cfg", "=", "model_cfgs", "[", "variant", "]", "if", "not", "cfg_variant", "else", "model_cfgs", "[", "cfg_variant", "]", ",", "\n", "feature_cfg", "=", "dict", "(", "flatten_sequential", "=", "True", ")", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet.botnet26t_256": [[335, 341], ["kwargs.setdefault", "byoanet._create_byoanet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet._create_byoanet"], ["", "@", "register_model", "\n", "def", "botnet26t_256", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Bottleneck Transformer w/ ResNet26-T backbone.\n    \"\"\"", "\n", "kwargs", ".", "setdefault", "(", "'img_size'", ",", "256", ")", "\n", "return", "_create_byoanet", "(", "'botnet26t_256'", ",", "'botnet26t'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet.sebotnet33ts_256": [[343, 348], ["byoanet._create_byoanet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet._create_byoanet"], ["", "@", "register_model", "\n", "def", "sebotnet33ts_256", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Bottleneck Transformer w/ a ResNet33-t backbone, SE attn for non Halo blocks, SiLU,\n    \"\"\"", "\n", "return", "_create_byoanet", "(", "'sebotnet33ts_256'", ",", "'sebotnet33ts'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet.botnet50ts_256": [[350, 356], ["kwargs.setdefault", "byoanet._create_byoanet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet._create_byoanet"], ["", "@", "register_model", "\n", "def", "botnet50ts_256", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Bottleneck Transformer w/ ResNet50-T backbone, silu act.\n    \"\"\"", "\n", "kwargs", ".", "setdefault", "(", "'img_size'", ",", "256", ")", "\n", "return", "_create_byoanet", "(", "'botnet50ts_256'", ",", "'botnet50ts'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet.eca_botnext26ts_256": [[358, 364], ["kwargs.setdefault", "byoanet._create_byoanet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet._create_byoanet"], ["", "@", "register_model", "\n", "def", "eca_botnext26ts_256", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Bottleneck Transformer w/ ResNet26-T backbone, silu act.\n    \"\"\"", "\n", "kwargs", ".", "setdefault", "(", "'img_size'", ",", "256", ")", "\n", "return", "_create_byoanet", "(", "'eca_botnext26ts_256'", ",", "'eca_botnext26ts'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet.halonet_h1": [[366, 372], ["byoanet._create_byoanet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet._create_byoanet"], ["", "@", "register_model", "\n", "def", "halonet_h1", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" HaloNet-H1. Halo attention in all stages as per the paper.\n    NOTE: This runs very slowly!\n    \"\"\"", "\n", "return", "_create_byoanet", "(", "'halonet_h1'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet.halonet26t": [[374, 379], ["byoanet._create_byoanet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet._create_byoanet"], ["", "@", "register_model", "\n", "def", "halonet26t", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" HaloNet w/ a ResNet26-t backbone. Halo attention in final two stages\n    \"\"\"", "\n", "return", "_create_byoanet", "(", "'halonet26t'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet.sehalonet33ts": [[381, 386], ["byoanet._create_byoanet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet._create_byoanet"], ["", "@", "register_model", "\n", "def", "sehalonet33ts", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" HaloNet w/ a ResNet33-t backbone, SE attn for non Halo blocks, SiLU, 1-2 Halo in stage 2,3,4.\n    \"\"\"", "\n", "return", "_create_byoanet", "(", "'sehalonet33ts'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet.halonet50ts": [[388, 393], ["byoanet._create_byoanet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet._create_byoanet"], ["", "@", "register_model", "\n", "def", "halonet50ts", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" HaloNet w/ a ResNet50-t backbone, silu act. Halo attention in final two stages\n    \"\"\"", "\n", "return", "_create_byoanet", "(", "'halonet50ts'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet.eca_halonext26ts": [[395, 400], ["byoanet._create_byoanet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet._create_byoanet"], ["", "@", "register_model", "\n", "def", "eca_halonext26ts", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" HaloNet w/ a ResNet26-t backbone, silu act. Halo attention in final two stages\n    \"\"\"", "\n", "return", "_create_byoanet", "(", "'eca_halonext26ts'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet.lambda_resnet26t": [[402, 407], ["byoanet._create_byoanet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet._create_byoanet"], ["", "@", "register_model", "\n", "def", "lambda_resnet26t", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Lambda-ResNet-26-T. Lambda layers w/ conv pos in last two stages.\n    \"\"\"", "\n", "return", "_create_byoanet", "(", "'lambda_resnet26t'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet.lambda_resnet50ts": [[409, 414], ["byoanet._create_byoanet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet._create_byoanet"], ["", "@", "register_model", "\n", "def", "lambda_resnet50ts", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Lambda-ResNet-50-TS. SiLU act. Lambda layers w/ conv pos in last two stages.\n    \"\"\"", "\n", "return", "_create_byoanet", "(", "'lambda_resnet50ts'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet.lambda_resnet26rpt_256": [[416, 422], ["kwargs.setdefault", "byoanet._create_byoanet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet._create_byoanet"], ["", "@", "register_model", "\n", "def", "lambda_resnet26rpt_256", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Lambda-ResNet-26-R-T. Lambda layers w/ rel pos embed in last two stages.\n    \"\"\"", "\n", "kwargs", ".", "setdefault", "(", "'img_size'", ",", "256", ")", "\n", "return", "_create_byoanet", "(", "'lambda_resnet26rpt_256'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet.haloregnetz_b": [[424, 429], ["byoanet._create_byoanet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet._create_byoanet"], ["", "@", "register_model", "\n", "def", "haloregnetz_b", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Halo + RegNetZ\n    \"\"\"", "\n", "return", "_create_byoanet", "(", "'haloregnetz_b'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet.lamhalobotnet50ts_256": [[431, 436], ["byoanet._create_byoanet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet._create_byoanet"], ["", "@", "register_model", "\n", "def", "lamhalobotnet50ts_256", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Combo Attention (Lambda + Halo + Bot) Network\n    \"\"\"", "\n", "return", "_create_byoanet", "(", "'lamhalobotnet50ts_256'", ",", "'lamhalobotnet50ts'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet.halo2botnet50ts_256": [[438, 443], ["byoanet._create_byoanet"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.byoanet._create_byoanet"], ["", "@", "register_model", "\n", "def", "halo2botnet50ts_256", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Combo Attention (Halo + Halo + Bot) Network\n    \"\"\"", "\n", "return", "_create_byoanet", "(", "'halo2botnet50ts_256'", ",", "'halo2botnet50ts'", ",", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ConvRelPosEnc.__init__": [[66, 105], ["torch.Module.__init__", "isinstance", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "window.items", "isinstance", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "coat.ConvRelPosEnc.conv_list.append", "coat.ConvRelPosEnc.head_splits.append", "ValueError"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "Ch", ",", "h", ",", "window", ")", ":", "\n", "        ", "\"\"\"\n        Initialization.\n            Ch: Channels per head.\n            h: Number of heads.\n            window: Window size(s) in convolutional relative positional encoding. It can have two forms:\n                1. An integer of window size, which assigns all attention heads with the same window s\n                    size in ConvRelPosEnc.\n                2. A dict mapping window size to #attention head splits (\n                    e.g. {window size 1: #attention head split 1, window size 2: #attention head split 2})\n                    It will apply different window size to the attention head splits.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "isinstance", "(", "window", ",", "int", ")", ":", "\n", "# Set the same window size for all attention heads.", "\n", "            ", "window", "=", "{", "window", ":", "h", "}", "\n", "self", ".", "window", "=", "window", "\n", "", "elif", "isinstance", "(", "window", ",", "dict", ")", ":", "\n", "            ", "self", ".", "window", "=", "window", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", ")", "\n", "\n", "", "self", ".", "conv_list", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "head_splits", "=", "[", "]", "\n", "for", "cur_window", ",", "cur_head_split", "in", "window", ".", "items", "(", ")", ":", "\n", "            ", "dilation", "=", "1", "\n", "# Determine padding size.", "\n", "# Ref: https://discuss.pytorch.org/t/how-to-keep-the-shape-of-input-and-output-same-when-dilation-conv/14338", "\n", "padding_size", "=", "(", "cur_window", "+", "(", "cur_window", "-", "1", ")", "*", "(", "dilation", "-", "1", ")", ")", "//", "2", "\n", "cur_conv", "=", "nn", ".", "Conv2d", "(", "cur_head_split", "*", "Ch", ",", "cur_head_split", "*", "Ch", ",", "\n", "kernel_size", "=", "(", "cur_window", ",", "cur_window", ")", ",", "\n", "padding", "=", "(", "padding_size", ",", "padding_size", ")", ",", "\n", "dilation", "=", "(", "dilation", ",", "dilation", ")", ",", "\n", "groups", "=", "cur_head_split", "*", "Ch", ",", "\n", ")", "\n", "self", ".", "conv_list", ".", "append", "(", "cur_conv", ")", "\n", "self", ".", "head_splits", ".", "append", "(", "cur_head_split", ")", "\n", "", "self", ".", "channel_splits", "=", "[", "x", "*", "Ch", "for", "x", "in", "self", ".", "head_splits", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ConvRelPosEnc.forward": [[106, 126], ["layers._assert", "v_img.transpose().reshape.transpose().reshape.transpose().reshape", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "conv_v_img.reshape().transpose.reshape().transpose.reshape().transpose", "torch.pad", "torch.pad", "torch.pad", "conv_v_img_list.append", "v_img.transpose().reshape.transpose().reshape.transpose", "conv", "conv_v_img.reshape().transpose.reshape().transpose.reshape"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "q", ",", "v", ",", "size", ":", "Tuple", "[", "int", ",", "int", "]", ")", ":", "\n", "        ", "B", ",", "h", ",", "N", ",", "Ch", "=", "q", ".", "shape", "\n", "H", ",", "W", "=", "size", "\n", "_assert", "(", "N", "==", "1", "+", "H", "*", "W", ",", "''", ")", "\n", "\n", "# Convolutional relative position encoding.", "\n", "q_img", "=", "q", "[", ":", ",", ":", ",", "1", ":", ",", ":", "]", "# [B, h, H*W, Ch]", "\n", "v_img", "=", "v", "[", ":", ",", ":", ",", "1", ":", ",", ":", "]", "# [B, h, H*W, Ch]", "\n", "\n", "v_img", "=", "v_img", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ".", "reshape", "(", "B", ",", "h", "*", "Ch", ",", "H", ",", "W", ")", "\n", "v_img_list", "=", "torch", ".", "split", "(", "v_img", ",", "self", ".", "channel_splits", ",", "dim", "=", "1", ")", "# Split according to channels", "\n", "conv_v_img_list", "=", "[", "]", "\n", "for", "i", ",", "conv", "in", "enumerate", "(", "self", ".", "conv_list", ")", ":", "\n", "            ", "conv_v_img_list", ".", "append", "(", "conv", "(", "v_img_list", "[", "i", "]", ")", ")", "\n", "", "conv_v_img", "=", "torch", ".", "cat", "(", "conv_v_img_list", ",", "dim", "=", "1", ")", "\n", "conv_v_img", "=", "conv_v_img", ".", "reshape", "(", "B", ",", "h", ",", "Ch", ",", "H", "*", "W", ")", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "\n", "EV_hat", "=", "q_img", "*", "conv_v_img", "\n", "EV_hat", "=", "F", ".", "pad", "(", "EV_hat", ",", "(", "0", ",", "0", ",", "1", ",", "0", ",", "0", ",", "0", ")", ")", "# [B, h, N, Ch].", "\n", "return", "EV_hat", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.FactorAttnConvRelPosEnc.__init__": [[130, 143], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "dim", ",", "num_heads", "=", "8", ",", "qkv_bias", "=", "False", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ",", "shared_crpe", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "head_dim", "=", "dim", "//", "num_heads", "\n", "self", ".", "scale", "=", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", "*", "3", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "# Note: attn_drop is actually not used.", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "\n", "# Shared convolutional relative position encoding.", "\n", "self", ".", "crpe", "=", "shared_crpe", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.FactorAttnConvRelPosEnc.forward": [[144, 168], ["coat.FactorAttnConvRelPosEnc.qkv().reshape().permute", "k.softmax", "coat.FactorAttnConvRelPosEnc.crpe", "coat.FactorAttnConvRelPosEnc.transpose().reshape", "coat.FactorAttnConvRelPosEnc.proj", "coat.FactorAttnConvRelPosEnc.proj_drop", "k.softmax.transpose", "coat.FactorAttnConvRelPosEnc.qkv().reshape", "coat.FactorAttnConvRelPosEnc.transpose", "coat.FactorAttnConvRelPosEnc.qkv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "size", ":", "Tuple", "[", "int", ",", "int", "]", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "\n", "# Generate Q, K, V.", "\n", "qkv", "=", "self", ".", "qkv", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "3", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "q", ",", "k", ",", "v", "=", "qkv", "[", "0", "]", ",", "qkv", "[", "1", "]", ",", "qkv", "[", "2", "]", "# [B, h, N, Ch]", "\n", "\n", "# Factorized attention.", "\n", "k_softmax", "=", "k", ".", "softmax", "(", "dim", "=", "2", ")", "\n", "factor_att", "=", "k_softmax", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "@", "v", "\n", "factor_att", "=", "q", "@", "factor_att", "\n", "\n", "# Convolutional relative position encoding.", "\n", "crpe", "=", "self", ".", "crpe", "(", "q", ",", "v", ",", "size", "=", "size", ")", "# [B, h, N, Ch]", "\n", "\n", "# Merge and reshape.", "\n", "x", "=", "self", ".", "scale", "*", "factor_att", "+", "crpe", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "N", ",", "C", ")", "# [B, h, N, Ch] -> [B, N, h, Ch] -> [B, N, C]", "\n", "\n", "# Output projection.", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ConvPosEnc.__init__": [[174, 177], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "dim", ",", "k", "=", "3", ")", ":", "\n", "        ", "super", "(", "ConvPosEnc", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Conv2d", "(", "dim", ",", "dim", ",", "k", ",", "1", ",", "k", "//", "2", ",", "groups", "=", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ConvPosEnc.forward": [[178, 195], ["layers._assert", "img_tokens.transpose().view", "torch.cat.flatten().transpose", "torch.cat.flatten().transpose", "torch.cat.flatten().transpose", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "coat.ConvPosEnc.proj", "img_tokens.transpose", "torch.cat.flatten", "torch.cat.flatten", "torch.cat.flatten"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "size", ":", "Tuple", "[", "int", ",", "int", "]", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "H", ",", "W", "=", "size", "\n", "_assert", "(", "N", "==", "1", "+", "H", "*", "W", ",", "''", ")", "\n", "\n", "# Extract CLS token and image tokens.", "\n", "cls_token", ",", "img_tokens", "=", "x", "[", ":", ",", ":", "1", "]", ",", "x", "[", ":", ",", "1", ":", "]", "# [B, 1, C], [B, H*W, C]", "\n", "\n", "# Depthwise convolution.", "\n", "feat", "=", "img_tokens", ".", "transpose", "(", "1", ",", "2", ")", ".", "view", "(", "B", ",", "C", ",", "H", ",", "W", ")", "\n", "x", "=", "self", ".", "proj", "(", "feat", ")", "+", "feat", "\n", "x", "=", "x", ".", "flatten", "(", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "# Combine with CLS token.", "\n", "x", "=", "torch", ".", "cat", "(", "(", "cls_token", ",", "x", ")", ",", "dim", "=", "1", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.SerialBlock.__init__": [[200, 216], ["torch.Module.__init__", "norm_layer", "coat.FactorAttnConvRelPosEnc", "norm_layer", "int", "layers.Mlp", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "dim", ",", "num_heads", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "\n", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ",", "shared_cpe", "=", "None", ",", "shared_crpe", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# Conv-Attention.", "\n", "self", ".", "cpe", "=", "shared_cpe", "\n", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "factoratt_crpe", "=", "FactorAttnConvRelPosEnc", "(", "\n", "dim", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ",", "shared_crpe", "=", "shared_crpe", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "# MLP.", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "mlp_hidden_dim", "=", "int", "(", "dim", "*", "mlp_ratio", ")", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "mlp_hidden_dim", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.SerialBlock.forward": [[217, 230], ["coat.SerialBlock.cpe", "coat.SerialBlock.norm1", "coat.SerialBlock.factoratt_crpe", "coat.SerialBlock.norm2", "coat.SerialBlock.mlp", "coat.SerialBlock.drop_path", "coat.SerialBlock.drop_path"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ",", "size", ":", "Tuple", "[", "int", ",", "int", "]", ")", ":", "\n", "# Conv-Attention.", "\n", "        ", "x", "=", "self", ".", "cpe", "(", "x", ",", "size", ")", "\n", "cur", "=", "self", ".", "norm1", "(", "x", ")", "\n", "cur", "=", "self", ".", "factoratt_crpe", "(", "cur", ",", "size", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "cur", ")", "\n", "\n", "# MLP. ", "\n", "cur", "=", "self", ".", "norm2", "(", "x", ")", "\n", "cur", "=", "self", ".", "mlp", "(", "cur", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "cur", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.__init__": [[234, 266], ["torch.Module.__init__", "norm_layer", "norm_layer", "norm_layer", "coat.FactorAttnConvRelPosEnc", "coat.FactorAttnConvRelPosEnc", "coat.FactorAttnConvRelPosEnc", "norm_layer", "norm_layer", "norm_layer", "int", "layers.Mlp", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "dims", ",", "num_heads", ",", "mlp_ratios", "=", "[", "]", ",", "qkv_bias", "=", "False", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "\n", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ",", "shared_crpes", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# Conv-Attention.", "\n", "self", ".", "norm12", "=", "norm_layer", "(", "dims", "[", "1", "]", ")", "\n", "self", ".", "norm13", "=", "norm_layer", "(", "dims", "[", "2", "]", ")", "\n", "self", ".", "norm14", "=", "norm_layer", "(", "dims", "[", "3", "]", ")", "\n", "self", ".", "factoratt_crpe2", "=", "FactorAttnConvRelPosEnc", "(", "\n", "dims", "[", "1", "]", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ",", "\n", "shared_crpe", "=", "shared_crpes", "[", "1", "]", "\n", ")", "\n", "self", ".", "factoratt_crpe3", "=", "FactorAttnConvRelPosEnc", "(", "\n", "dims", "[", "2", "]", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ",", "\n", "shared_crpe", "=", "shared_crpes", "[", "2", "]", "\n", ")", "\n", "self", ".", "factoratt_crpe4", "=", "FactorAttnConvRelPosEnc", "(", "\n", "dims", "[", "3", "]", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ",", "\n", "shared_crpe", "=", "shared_crpes", "[", "3", "]", "\n", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "# MLP.", "\n", "self", ".", "norm22", "=", "norm_layer", "(", "dims", "[", "1", "]", ")", "\n", "self", ".", "norm23", "=", "norm_layer", "(", "dims", "[", "2", "]", ")", "\n", "self", ".", "norm24", "=", "norm_layer", "(", "dims", "[", "3", "]", ")", "\n", "# In parallel block, we assume dimensions are the same and share the linear transformation.", "\n", "assert", "dims", "[", "1", "]", "==", "dims", "[", "2", "]", "==", "dims", "[", "3", "]", "\n", "assert", "mlp_ratios", "[", "1", "]", "==", "mlp_ratios", "[", "2", "]", "==", "mlp_ratios", "[", "3", "]", "\n", "mlp_hidden_dim", "=", "int", "(", "dims", "[", "1", "]", "*", "mlp_ratios", "[", "1", "]", ")", "\n", "self", ".", "mlp2", "=", "self", ".", "mlp3", "=", "self", ".", "mlp4", "=", "Mlp", "(", "\n", "in_features", "=", "dims", "[", "1", "]", ",", "hidden_features", "=", "mlp_hidden_dim", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.upsample": [[267, 270], ["coat.ParallelBlock.interpolate"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.interpolate"], ["", "def", "upsample", "(", "self", ",", "x", ",", "factor", ":", "float", ",", "size", ":", "Tuple", "[", "int", ",", "int", "]", ")", ":", "\n", "        ", "\"\"\" Feature map up-sampling. \"\"\"", "\n", "return", "self", ".", "interpolate", "(", "x", ",", "scale_factor", "=", "factor", ",", "size", "=", "size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample": [[271, 274], ["coat.ParallelBlock.interpolate"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.interpolate"], ["", "def", "downsample", "(", "self", ",", "x", ",", "factor", ":", "float", ",", "size", ":", "Tuple", "[", "int", ",", "int", "]", ")", ":", "\n", "        ", "\"\"\" Feature map down-sampling. \"\"\"", "\n", "return", "self", ".", "interpolate", "(", "x", ",", "scale_factor", "=", "1.0", "/", "factor", ",", "size", "=", "size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.interpolate": [[275, 292], ["layers._assert", "img_tokens.reshape().transpose.reshape().transpose.transpose().reshape", "torch.interpolate", "torch.interpolate", "torch.interpolate", "img_tokens.reshape().transpose.reshape().transpose.reshape().transpose", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "img_tokens.reshape().transpose.reshape().transpose.transpose", "img_tokens.reshape().transpose.reshape().transpose.reshape"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.interpolate", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.interpolate", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.interpolate"], ["", "def", "interpolate", "(", "self", ",", "x", ",", "scale_factor", ":", "float", ",", "size", ":", "Tuple", "[", "int", ",", "int", "]", ")", ":", "\n", "        ", "\"\"\" Feature map interpolation. \"\"\"", "\n", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "H", ",", "W", "=", "size", "\n", "_assert", "(", "N", "==", "1", "+", "H", "*", "W", ",", "''", ")", "\n", "\n", "cls_token", "=", "x", "[", ":", ",", ":", "1", ",", ":", "]", "\n", "img_tokens", "=", "x", "[", ":", ",", "1", ":", ",", ":", "]", "\n", "\n", "img_tokens", "=", "img_tokens", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "C", ",", "H", ",", "W", ")", "\n", "img_tokens", "=", "F", ".", "interpolate", "(", "\n", "img_tokens", ",", "scale_factor", "=", "scale_factor", ",", "recompute_scale_factor", "=", "False", ",", "mode", "=", "'bilinear'", ",", "align_corners", "=", "False", ")", "\n", "img_tokens", "=", "img_tokens", ".", "reshape", "(", "B", ",", "C", ",", "-", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "out", "=", "torch", ".", "cat", "(", "(", "cls_token", ",", "img_tokens", ")", ",", "dim", "=", "1", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.forward": [[293, 326], ["coat.ParallelBlock.norm12", "coat.ParallelBlock.norm13", "coat.ParallelBlock.norm14", "coat.ParallelBlock.factoratt_crpe2", "coat.ParallelBlock.factoratt_crpe3", "coat.ParallelBlock.factoratt_crpe4", "coat.ParallelBlock.upsample", "coat.ParallelBlock.upsample", "coat.ParallelBlock.upsample", "coat.ParallelBlock.downsample", "coat.ParallelBlock.downsample", "coat.ParallelBlock.downsample", "coat.ParallelBlock.norm22", "coat.ParallelBlock.norm23", "coat.ParallelBlock.norm24", "coat.ParallelBlock.mlp2", "coat.ParallelBlock.mlp3", "coat.ParallelBlock.mlp4", "coat.ParallelBlock.drop_path", "coat.ParallelBlock.drop_path", "coat.ParallelBlock.drop_path", "coat.ParallelBlock.drop_path", "coat.ParallelBlock.drop_path", "coat.ParallelBlock.drop_path"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.upsample", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.upsample", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.upsample", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x1", ",", "x2", ",", "x3", ",", "x4", ",", "sizes", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ")", ":", "\n", "        ", "_", ",", "S2", ",", "S3", ",", "S4", "=", "sizes", "\n", "cur2", "=", "self", ".", "norm12", "(", "x2", ")", "\n", "cur3", "=", "self", ".", "norm13", "(", "x3", ")", "\n", "cur4", "=", "self", ".", "norm14", "(", "x4", ")", "\n", "cur2", "=", "self", ".", "factoratt_crpe2", "(", "cur2", ",", "size", "=", "S2", ")", "\n", "cur3", "=", "self", ".", "factoratt_crpe3", "(", "cur3", ",", "size", "=", "S3", ")", "\n", "cur4", "=", "self", ".", "factoratt_crpe4", "(", "cur4", ",", "size", "=", "S4", ")", "\n", "upsample3_2", "=", "self", ".", "upsample", "(", "cur3", ",", "factor", "=", "2.", ",", "size", "=", "S3", ")", "\n", "upsample4_3", "=", "self", ".", "upsample", "(", "cur4", ",", "factor", "=", "2.", ",", "size", "=", "S4", ")", "\n", "upsample4_2", "=", "self", ".", "upsample", "(", "cur4", ",", "factor", "=", "4.", ",", "size", "=", "S4", ")", "\n", "downsample2_3", "=", "self", ".", "downsample", "(", "cur2", ",", "factor", "=", "2.", ",", "size", "=", "S2", ")", "\n", "downsample3_4", "=", "self", ".", "downsample", "(", "cur3", ",", "factor", "=", "2.", ",", "size", "=", "S3", ")", "\n", "downsample2_4", "=", "self", ".", "downsample", "(", "cur2", ",", "factor", "=", "4.", ",", "size", "=", "S2", ")", "\n", "cur2", "=", "cur2", "+", "upsample3_2", "+", "upsample4_2", "\n", "cur3", "=", "cur3", "+", "upsample4_3", "+", "downsample2_3", "\n", "cur4", "=", "cur4", "+", "downsample3_4", "+", "downsample2_4", "\n", "x2", "=", "x2", "+", "self", ".", "drop_path", "(", "cur2", ")", "\n", "x3", "=", "x3", "+", "self", ".", "drop_path", "(", "cur3", ")", "\n", "x4", "=", "x4", "+", "self", ".", "drop_path", "(", "cur4", ")", "\n", "\n", "# MLP. ", "\n", "cur2", "=", "self", ".", "norm22", "(", "x2", ")", "\n", "cur3", "=", "self", ".", "norm23", "(", "x3", ")", "\n", "cur4", "=", "self", ".", "norm24", "(", "x4", ")", "\n", "cur2", "=", "self", ".", "mlp2", "(", "cur2", ")", "\n", "cur3", "=", "self", ".", "mlp3", "(", "cur3", ")", "\n", "cur4", "=", "self", ".", "mlp4", "(", "cur4", ")", "\n", "x2", "=", "x2", "+", "self", ".", "drop_path", "(", "cur2", ")", "\n", "x3", "=", "x3", "+", "self", ".", "drop_path", "(", "cur3", ")", "\n", "x4", "=", "x4", "+", "self", ".", "drop_path", "(", "cur4", ")", "\n", "\n", "return", "x1", ",", "x2", ",", "x3", ",", "x4", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.CoaT.__init__": [[330, 461], ["functools.partial", "torch.Module.__init__", "layers.to_2tuple", "layers.PatchEmbed", "layers.PatchEmbed", "layers.PatchEmbed", "layers.PatchEmbed", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "coat.ConvPosEnc", "coat.ConvPosEnc", "coat.ConvPosEnc", "coat.ConvPosEnc", "coat.ConvRelPosEnc", "coat.ConvRelPosEnc", "coat.ConvRelPosEnc", "coat.ConvRelPosEnc", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "layers.trunc_normal_", "layers.trunc_normal_", "layers.trunc_normal_", "layers.trunc_normal_", "coat.CoaT.apply", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "norm_layer", "coat.SerialBlock", "coat.SerialBlock", "coat.SerialBlock", "coat.SerialBlock", "norm_layer", "norm_layer", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "range", "range", "range", "range", "coat.ParallelBlock", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "range"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["def", "__init__", "(", "\n", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "embed_dims", "=", "(", "0", ",", "0", ",", "0", ",", "0", ")", ",", "\n", "serial_depths", "=", "(", "0", ",", "0", ",", "0", ",", "0", ")", ",", "parallel_depth", "=", "0", ",", "num_heads", "=", "0", ",", "mlp_ratios", "=", "(", "0", ",", "0", ",", "0", ",", "0", ")", ",", "qkv_bias", "=", "True", ",", "\n", "drop_rate", "=", "0.", ",", "attn_drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ",", "norm_layer", "=", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", ",", "\n", "return_interm_layers", "=", "False", ",", "out_features", "=", "None", ",", "crpe_window", "=", "None", ",", "global_pool", "=", "'token'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "global_pool", "in", "(", "'token'", ",", "'avg'", ")", "\n", "crpe_window", "=", "crpe_window", "or", "{", "3", ":", "2", ",", "5", ":", "3", ",", "7", ":", "3", "}", "\n", "self", ".", "return_interm_layers", "=", "return_interm_layers", "\n", "self", ".", "out_features", "=", "out_features", "\n", "self", ".", "embed_dims", "=", "embed_dims", "\n", "self", ".", "num_features", "=", "embed_dims", "[", "-", "1", "]", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "\n", "# Patch embeddings.", "\n", "img_size", "=", "to_2tuple", "(", "img_size", ")", "\n", "self", ".", "patch_embed1", "=", "PatchEmbed", "(", "\n", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", ",", "in_chans", "=", "in_chans", ",", "\n", "embed_dim", "=", "embed_dims", "[", "0", "]", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", "\n", "self", ".", "patch_embed2", "=", "PatchEmbed", "(", "\n", "img_size", "=", "[", "x", "//", "4", "for", "x", "in", "img_size", "]", ",", "patch_size", "=", "2", ",", "in_chans", "=", "embed_dims", "[", "0", "]", ",", "\n", "embed_dim", "=", "embed_dims", "[", "1", "]", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", "\n", "self", ".", "patch_embed3", "=", "PatchEmbed", "(", "\n", "img_size", "=", "[", "x", "//", "8", "for", "x", "in", "img_size", "]", ",", "patch_size", "=", "2", ",", "in_chans", "=", "embed_dims", "[", "1", "]", ",", "\n", "embed_dim", "=", "embed_dims", "[", "2", "]", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", "\n", "self", ".", "patch_embed4", "=", "PatchEmbed", "(", "\n", "img_size", "=", "[", "x", "//", "16", "for", "x", "in", "img_size", "]", ",", "patch_size", "=", "2", ",", "in_chans", "=", "embed_dims", "[", "2", "]", ",", "\n", "embed_dim", "=", "embed_dims", "[", "3", "]", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", "\n", "\n", "# Class tokens.", "\n", "self", ".", "cls_token1", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "embed_dims", "[", "0", "]", ")", ")", "\n", "self", ".", "cls_token2", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "embed_dims", "[", "1", "]", ")", ")", "\n", "self", ".", "cls_token3", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "embed_dims", "[", "2", "]", ")", ")", "\n", "self", ".", "cls_token4", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "embed_dims", "[", "3", "]", ")", ")", "\n", "\n", "# Convolutional position encodings.", "\n", "self", ".", "cpe1", "=", "ConvPosEnc", "(", "dim", "=", "embed_dims", "[", "0", "]", ",", "k", "=", "3", ")", "\n", "self", ".", "cpe2", "=", "ConvPosEnc", "(", "dim", "=", "embed_dims", "[", "1", "]", ",", "k", "=", "3", ")", "\n", "self", ".", "cpe3", "=", "ConvPosEnc", "(", "dim", "=", "embed_dims", "[", "2", "]", ",", "k", "=", "3", ")", "\n", "self", ".", "cpe4", "=", "ConvPosEnc", "(", "dim", "=", "embed_dims", "[", "3", "]", ",", "k", "=", "3", ")", "\n", "\n", "# Convolutional relative position encodings.", "\n", "self", ".", "crpe1", "=", "ConvRelPosEnc", "(", "Ch", "=", "embed_dims", "[", "0", "]", "//", "num_heads", ",", "h", "=", "num_heads", ",", "window", "=", "crpe_window", ")", "\n", "self", ".", "crpe2", "=", "ConvRelPosEnc", "(", "Ch", "=", "embed_dims", "[", "1", "]", "//", "num_heads", ",", "h", "=", "num_heads", ",", "window", "=", "crpe_window", ")", "\n", "self", ".", "crpe3", "=", "ConvRelPosEnc", "(", "Ch", "=", "embed_dims", "[", "2", "]", "//", "num_heads", ",", "h", "=", "num_heads", ",", "window", "=", "crpe_window", ")", "\n", "self", ".", "crpe4", "=", "ConvRelPosEnc", "(", "Ch", "=", "embed_dims", "[", "3", "]", "//", "num_heads", ",", "h", "=", "num_heads", ",", "window", "=", "crpe_window", ")", "\n", "\n", "# Disable stochastic depth.", "\n", "dpr", "=", "drop_path_rate", "\n", "assert", "dpr", "==", "0.0", "\n", "\n", "# Serial blocks 1.", "\n", "self", ".", "serial_blocks1", "=", "nn", ".", "ModuleList", "(", "[", "\n", "SerialBlock", "(", "\n", "dim", "=", "embed_dims", "[", "0", "]", ",", "num_heads", "=", "num_heads", ",", "mlp_ratio", "=", "mlp_ratios", "[", "0", "]", ",", "qkv_bias", "=", "qkv_bias", ",", "\n", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "dpr", ",", "norm_layer", "=", "norm_layer", ",", "\n", "shared_cpe", "=", "self", ".", "cpe1", ",", "shared_crpe", "=", "self", ".", "crpe1", "\n", ")", "\n", "for", "_", "in", "range", "(", "serial_depths", "[", "0", "]", ")", "]", "\n", ")", "\n", "\n", "# Serial blocks 2.", "\n", "self", ".", "serial_blocks2", "=", "nn", ".", "ModuleList", "(", "[", "\n", "SerialBlock", "(", "\n", "dim", "=", "embed_dims", "[", "1", "]", ",", "num_heads", "=", "num_heads", ",", "mlp_ratio", "=", "mlp_ratios", "[", "1", "]", ",", "qkv_bias", "=", "qkv_bias", ",", "\n", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "dpr", ",", "norm_layer", "=", "norm_layer", ",", "\n", "shared_cpe", "=", "self", ".", "cpe2", ",", "shared_crpe", "=", "self", ".", "crpe2", "\n", ")", "\n", "for", "_", "in", "range", "(", "serial_depths", "[", "1", "]", ")", "]", "\n", ")", "\n", "\n", "# Serial blocks 3.", "\n", "self", ".", "serial_blocks3", "=", "nn", ".", "ModuleList", "(", "[", "\n", "SerialBlock", "(", "\n", "dim", "=", "embed_dims", "[", "2", "]", ",", "num_heads", "=", "num_heads", ",", "mlp_ratio", "=", "mlp_ratios", "[", "2", "]", ",", "qkv_bias", "=", "qkv_bias", ",", "\n", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "dpr", ",", "norm_layer", "=", "norm_layer", ",", "\n", "shared_cpe", "=", "self", ".", "cpe3", ",", "shared_crpe", "=", "self", ".", "crpe3", "\n", ")", "\n", "for", "_", "in", "range", "(", "serial_depths", "[", "2", "]", ")", "]", "\n", ")", "\n", "\n", "# Serial blocks 4.", "\n", "self", ".", "serial_blocks4", "=", "nn", ".", "ModuleList", "(", "[", "\n", "SerialBlock", "(", "\n", "dim", "=", "embed_dims", "[", "3", "]", ",", "num_heads", "=", "num_heads", ",", "mlp_ratio", "=", "mlp_ratios", "[", "3", "]", ",", "qkv_bias", "=", "qkv_bias", ",", "\n", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "dpr", ",", "norm_layer", "=", "norm_layer", ",", "\n", "shared_cpe", "=", "self", ".", "cpe4", ",", "shared_crpe", "=", "self", ".", "crpe4", "\n", ")", "\n", "for", "_", "in", "range", "(", "serial_depths", "[", "3", "]", ")", "]", "\n", ")", "\n", "\n", "# Parallel blocks.", "\n", "self", ".", "parallel_depth", "=", "parallel_depth", "\n", "if", "self", ".", "parallel_depth", ">", "0", ":", "\n", "            ", "self", ".", "parallel_blocks", "=", "nn", ".", "ModuleList", "(", "[", "\n", "ParallelBlock", "(", "\n", "dims", "=", "embed_dims", ",", "num_heads", "=", "num_heads", ",", "mlp_ratios", "=", "mlp_ratios", ",", "qkv_bias", "=", "qkv_bias", ",", "\n", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "dpr", ",", "norm_layer", "=", "norm_layer", ",", "\n", "shared_crpes", "=", "(", "self", ".", "crpe1", ",", "self", ".", "crpe2", ",", "self", ".", "crpe3", ",", "self", ".", "crpe4", ")", "\n", ")", "\n", "for", "_", "in", "range", "(", "parallel_depth", ")", "]", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "parallel_blocks", "=", "None", "\n", "\n", "# Classification head(s).", "\n", "", "if", "not", "self", ".", "return_interm_layers", ":", "\n", "            ", "if", "self", ".", "parallel_blocks", "is", "not", "None", ":", "\n", "                ", "self", ".", "norm2", "=", "norm_layer", "(", "embed_dims", "[", "1", "]", ")", "\n", "self", ".", "norm3", "=", "norm_layer", "(", "embed_dims", "[", "2", "]", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "norm2", "=", "self", ".", "norm3", "=", "None", "\n", "", "self", ".", "norm4", "=", "norm_layer", "(", "embed_dims", "[", "3", "]", ")", "\n", "\n", "if", "self", ".", "parallel_depth", ">", "0", ":", "\n", "# CoaT series: Aggregate features of last three scales for classification.", "\n", "                ", "assert", "embed_dims", "[", "1", "]", "==", "embed_dims", "[", "2", "]", "==", "embed_dims", "[", "3", "]", "\n", "self", ".", "aggregate", "=", "torch", ".", "nn", ".", "Conv1d", "(", "in_channels", "=", "3", ",", "out_channels", "=", "1", ",", "kernel_size", "=", "1", ")", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "", "else", ":", "\n", "# CoaT-Lite series: Use feature of last scale for classification.", "\n", "                ", "self", ".", "aggregate", "=", "None", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "# Initialize weights.", "\n", "", "", "trunc_normal_", "(", "self", ".", "cls_token1", ",", "std", "=", ".02", ")", "\n", "trunc_normal_", "(", "self", ".", "cls_token2", ",", "std", "=", ".02", ")", "\n", "trunc_normal_", "(", "self", ".", "cls_token3", ",", "std", "=", ".02", ")", "\n", "trunc_normal_", "(", "self", ".", "cls_token4", ",", "std", "=", ".02", ")", "\n", "self", ".", "apply", "(", "self", ".", "_init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.CoaT._init_weights": [[462, 470], ["isinstance", "layers.trunc_normal_", "isinstance", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["", "def", "_init_weights", "(", "self", ",", "m", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "            ", "trunc_normal_", "(", "m", ".", "weight", ",", "std", "=", ".02", ")", "\n", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.CoaT.no_weight_decay": [[471, 474], ["None"], "methods", ["None"], ["", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "no_weight_decay", "(", "self", ")", ":", "\n", "        ", "return", "{", "'cls_token1'", ",", "'cls_token2'", ",", "'cls_token3'", ",", "'cls_token4'", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.CoaT.set_grad_checkpointing": [[475, 478], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "assert", "not", "enable", ",", "'gradient checkpointing not supported'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.CoaT.group_matcher": [[479, 496], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "matcher", "=", "dict", "(", "\n", "stem1", "=", "r'^cls_token1|patch_embed1|crpe1|cpe1'", ",", "\n", "serial_blocks1", "=", "r'^serial_blocks1\\.(\\d+)'", ",", "\n", "stem2", "=", "r'^cls_token2|patch_embed2|crpe2|cpe2'", ",", "\n", "serial_blocks2", "=", "r'^serial_blocks2\\.(\\d+)'", ",", "\n", "stem3", "=", "r'^cls_token3|patch_embed3|crpe3|cpe3'", ",", "\n", "serial_blocks3", "=", "r'^serial_blocks3\\.(\\d+)'", ",", "\n", "stem4", "=", "r'^cls_token4|patch_embed4|crpe4|cpe4'", ",", "\n", "serial_blocks4", "=", "r'^serial_blocks4\\.(\\d+)'", ",", "\n", "parallel_blocks", "=", "[", "# FIXME (partially?) overlap parallel w/ serial blocks??", "\n", "(", "r'^parallel_blocks\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^norm|aggregate'", ",", "(", "99999", ",", ")", ")", ",", "\n", "]", "\n", ")", "\n", "return", "matcher", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.CoaT.get_classifier": [[497, 500], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.CoaT.reset_classifier": [[501, 507], ["torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "assert", "global_pool", "in", "(", "'token'", ",", "'avg'", ")", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.CoaT.forward_features": [[508, 588], ["coat.CoaT.patch_embed1", "coat.insert_cls", "remove_cls().reshape().permute().contiguous", "coat.CoaT.patch_embed2", "coat.insert_cls", "remove_cls().reshape().permute().contiguous", "coat.CoaT.patch_embed3", "coat.insert_cls", "remove_cls().reshape().permute().contiguous", "coat.CoaT.patch_embed4", "coat.insert_cls", "remove_cls().reshape().permute().contiguous", "blk", "blk", "blk", "blk", "blk", "coat.CoaT.norm2", "coat.CoaT.norm3", "coat.CoaT.norm4", "remove_cls().reshape().permute", "remove_cls().reshape().permute", "remove_cls().reshape().permute", "remove_cls().reshape().permute", "coat.CoaT.norm4", "coat.CoaT.cpe2", "coat.CoaT.cpe3", "coat.CoaT.cpe4", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "remove_cls().reshape().permute().contiguous", "remove_cls().reshape().permute().contiguous", "remove_cls().reshape().permute().contiguous", "remove_cls().reshape().permute().contiguous", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "remove_cls().reshape", "remove_cls().reshape", "remove_cls().reshape", "remove_cls().reshape", "remove_cls().reshape().permute", "remove_cls().reshape().permute", "remove_cls().reshape().permute", "remove_cls().reshape().permute", "coat.remove_cls", "coat.remove_cls", "coat.remove_cls", "coat.remove_cls", "remove_cls().reshape", "remove_cls().reshape", "remove_cls().reshape", "remove_cls().reshape", "coat.remove_cls", "coat.remove_cls", "coat.remove_cls", "coat.remove_cls"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.insert_cls", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.insert_cls", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.insert_cls", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.insert_cls", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.remove_cls", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.remove_cls", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.remove_cls", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.remove_cls", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.remove_cls", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.remove_cls", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.remove_cls", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.remove_cls"], ["", "def", "forward_features", "(", "self", ",", "x0", ")", ":", "\n", "        ", "B", "=", "x0", ".", "shape", "[", "0", "]", "\n", "\n", "# Serial blocks 1.", "\n", "x1", "=", "self", ".", "patch_embed1", "(", "x0", ")", "\n", "H1", ",", "W1", "=", "self", ".", "patch_embed1", ".", "grid_size", "\n", "x1", "=", "insert_cls", "(", "x1", ",", "self", ".", "cls_token1", ")", "\n", "for", "blk", "in", "self", ".", "serial_blocks1", ":", "\n", "            ", "x1", "=", "blk", "(", "x1", ",", "size", "=", "(", "H1", ",", "W1", ")", ")", "\n", "", "x1_nocls", "=", "remove_cls", "(", "x1", ")", ".", "reshape", "(", "B", ",", "H1", ",", "W1", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "\n", "# Serial blocks 2.", "\n", "x2", "=", "self", ".", "patch_embed2", "(", "x1_nocls", ")", "\n", "H2", ",", "W2", "=", "self", ".", "patch_embed2", ".", "grid_size", "\n", "x2", "=", "insert_cls", "(", "x2", ",", "self", ".", "cls_token2", ")", "\n", "for", "blk", "in", "self", ".", "serial_blocks2", ":", "\n", "            ", "x2", "=", "blk", "(", "x2", ",", "size", "=", "(", "H2", ",", "W2", ")", ")", "\n", "", "x2_nocls", "=", "remove_cls", "(", "x2", ")", ".", "reshape", "(", "B", ",", "H2", ",", "W2", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "\n", "# Serial blocks 3.", "\n", "x3", "=", "self", ".", "patch_embed3", "(", "x2_nocls", ")", "\n", "H3", ",", "W3", "=", "self", ".", "patch_embed3", ".", "grid_size", "\n", "x3", "=", "insert_cls", "(", "x3", ",", "self", ".", "cls_token3", ")", "\n", "for", "blk", "in", "self", ".", "serial_blocks3", ":", "\n", "            ", "x3", "=", "blk", "(", "x3", ",", "size", "=", "(", "H3", ",", "W3", ")", ")", "\n", "", "x3_nocls", "=", "remove_cls", "(", "x3", ")", ".", "reshape", "(", "B", ",", "H3", ",", "W3", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "\n", "# Serial blocks 4.", "\n", "x4", "=", "self", ".", "patch_embed4", "(", "x3_nocls", ")", "\n", "H4", ",", "W4", "=", "self", ".", "patch_embed4", ".", "grid_size", "\n", "x4", "=", "insert_cls", "(", "x4", ",", "self", ".", "cls_token4", ")", "\n", "for", "blk", "in", "self", ".", "serial_blocks4", ":", "\n", "            ", "x4", "=", "blk", "(", "x4", ",", "size", "=", "(", "H4", ",", "W4", ")", ")", "\n", "", "x4_nocls", "=", "remove_cls", "(", "x4", ")", ".", "reshape", "(", "B", ",", "H4", ",", "W4", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "\n", "# Only serial blocks: Early return.", "\n", "if", "self", ".", "parallel_blocks", "is", "None", ":", "\n", "            ", "if", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", "and", "self", ".", "return_interm_layers", ":", "\n", "# Return intermediate features for down-stream tasks (e.g. Deformable DETR and Detectron2).", "\n", "                ", "feat_out", "=", "{", "}", "\n", "if", "'x1_nocls'", "in", "self", ".", "out_features", ":", "\n", "                    ", "feat_out", "[", "'x1_nocls'", "]", "=", "x1_nocls", "\n", "", "if", "'x2_nocls'", "in", "self", ".", "out_features", ":", "\n", "                    ", "feat_out", "[", "'x2_nocls'", "]", "=", "x2_nocls", "\n", "", "if", "'x3_nocls'", "in", "self", ".", "out_features", ":", "\n", "                    ", "feat_out", "[", "'x3_nocls'", "]", "=", "x3_nocls", "\n", "", "if", "'x4_nocls'", "in", "self", ".", "out_features", ":", "\n", "                    ", "feat_out", "[", "'x4_nocls'", "]", "=", "x4_nocls", "\n", "", "return", "feat_out", "\n", "", "else", ":", "\n", "# Return features for classification.", "\n", "                ", "x4", "=", "self", ".", "norm4", "(", "x4", ")", "\n", "return", "x4", "\n", "\n", "# Parallel blocks.", "\n", "", "", "for", "blk", "in", "self", ".", "parallel_blocks", ":", "\n", "            ", "x2", ",", "x3", ",", "x4", "=", "self", ".", "cpe2", "(", "x2", ",", "(", "H2", ",", "W2", ")", ")", ",", "self", ".", "cpe3", "(", "x3", ",", "(", "H3", ",", "W3", ")", ")", ",", "self", ".", "cpe4", "(", "x4", ",", "(", "H4", ",", "W4", ")", ")", "\n", "x1", ",", "x2", ",", "x3", ",", "x4", "=", "blk", "(", "x1", ",", "x2", ",", "x3", ",", "x4", ",", "sizes", "=", "[", "(", "H1", ",", "W1", ")", ",", "(", "H2", ",", "W2", ")", ",", "(", "H3", ",", "W3", ")", ",", "(", "H4", ",", "W4", ")", "]", ")", "\n", "\n", "", "if", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", "and", "self", ".", "return_interm_layers", ":", "\n", "# Return intermediate features for down-stream tasks (e.g. Deformable DETR and Detectron2).", "\n", "            ", "feat_out", "=", "{", "}", "\n", "if", "'x1_nocls'", "in", "self", ".", "out_features", ":", "\n", "                ", "x1_nocls", "=", "remove_cls", "(", "x1", ")", ".", "reshape", "(", "B", ",", "H1", ",", "W1", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "feat_out", "[", "'x1_nocls'", "]", "=", "x1_nocls", "\n", "", "if", "'x2_nocls'", "in", "self", ".", "out_features", ":", "\n", "                ", "x2_nocls", "=", "remove_cls", "(", "x2", ")", ".", "reshape", "(", "B", ",", "H2", ",", "W2", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "feat_out", "[", "'x2_nocls'", "]", "=", "x2_nocls", "\n", "", "if", "'x3_nocls'", "in", "self", ".", "out_features", ":", "\n", "                ", "x3_nocls", "=", "remove_cls", "(", "x3", ")", ".", "reshape", "(", "B", ",", "H3", ",", "W3", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "feat_out", "[", "'x3_nocls'", "]", "=", "x3_nocls", "\n", "", "if", "'x4_nocls'", "in", "self", ".", "out_features", ":", "\n", "                ", "x4_nocls", "=", "remove_cls", "(", "x4", ")", ".", "reshape", "(", "B", ",", "H4", ",", "W4", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "feat_out", "[", "'x4_nocls'", "]", "=", "x4_nocls", "\n", "", "return", "feat_out", "\n", "", "else", ":", "\n", "            ", "x2", "=", "self", ".", "norm2", "(", "x2", ")", "\n", "x3", "=", "self", ".", "norm3", "(", "x3", ")", "\n", "x4", "=", "self", ".", "norm4", "(", "x4", ")", "\n", "return", "[", "x2", ",", "x3", ",", "x4", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.CoaT.forward_head": [[589, 600], ["isinstance", "coat.CoaT.aggregate().squeeze", "coat.CoaT.head", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "x_feat[].mean", "coat.CoaT.aggregate", "xl[].mean"], "methods", ["None"], ["", "", "def", "forward_head", "(", "self", ",", "x_feat", ":", "Union", "[", "torch", ".", "Tensor", ",", "List", "[", "torch", ".", "Tensor", "]", "]", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "isinstance", "(", "x_feat", ",", "list", ")", ":", "\n", "            ", "assert", "self", ".", "aggregate", "is", "not", "None", "\n", "if", "self", ".", "global_pool", "==", "'avg'", ":", "\n", "                ", "x", "=", "torch", ".", "cat", "(", "[", "xl", "[", ":", ",", "1", ":", "]", ".", "mean", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "for", "xl", "in", "x_feat", "]", ",", "dim", "=", "1", ")", "# [B, 3, C]", "\n", "", "else", ":", "\n", "                ", "x", "=", "torch", ".", "stack", "(", "[", "xl", "[", ":", ",", "0", "]", "for", "xl", "in", "x_feat", "]", ",", "dim", "=", "1", ")", "# [B, 3, C]", "\n", "", "x", "=", "self", ".", "aggregate", "(", "x", ")", ".", "squeeze", "(", "dim", "=", "1", ")", "# Shape: [B, C]", "\n", "", "else", ":", "\n", "            ", "x", "=", "x_feat", "[", ":", ",", "1", ":", "]", ".", "mean", "(", "dim", "=", "1", ")", "if", "self", ".", "global_pool", "==", "'avg'", "else", "x_feat", "[", ":", ",", "0", "]", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.CoaT.forward": [[601, 610], ["coat.CoaT.forward_features", "coat.CoaT.forward_features", "coat.CoaT.forward_head", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "if", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", "and", "self", ".", "return_interm_layers", ":", "\n", "# Return intermediate features (for down-stream tasks).", "\n", "            ", "return", "self", ".", "forward_features", "(", "x", ")", "\n", "", "else", ":", "\n", "# Return features for classification.", "\n", "            ", "x_feat", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x_feat", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat._cfg_coat": [[34, 42], ["None"], "function", ["None"], ["def", "_cfg_coat", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", ".9", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'patch_embed1.proj'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.insert_cls": [[612, 617], ["cls_token.expand", "torch.cat", "torch.cat", "torch.cat"], "function", ["None"], ["", "", "", "def", "insert_cls", "(", "x", ",", "cls_token", ")", ":", "\n", "    ", "\"\"\" Insert CLS token. \"\"\"", "\n", "cls_tokens", "=", "cls_token", ".", "expand", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ",", "-", "1", ")", "\n", "x", "=", "torch", ".", "cat", "(", "(", "cls_tokens", ",", "x", ")", ",", "dim", "=", "1", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.remove_cls": [[619, 622], ["None"], "function", ["None"], ["", "def", "remove_cls", "(", "x", ")", ":", "\n", "    ", "\"\"\" Remove CLS token. \"\"\"", "\n", "return", "x", "[", ":", ",", "1", ":", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.checkpoint_filter_fn": [[624, 634], ["state_dict.items", "k.startswith", "k.startswith", "k.startswith"], "function", ["None"], ["", "def", "checkpoint_filter_fn", "(", "state_dict", ",", "model", ")", ":", "\n", "    ", "out_dict", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "# original model had unused norm layers, removing them requires filtering pretrained checkpoints", "\n", "        ", "if", "k", ".", "startswith", "(", "'norm1'", ")", "or", "(", "model", ".", "norm2", "is", "None", "and", "k", ".", "startswith", "(", "'norm2'", ")", ")", "or", "(", "model", ".", "norm3", "is", "None", "and", "k", ".", "startswith", "(", "'norm3'", ")", ")", ":", "\n", "            ", "continue", "\n", "", "out_dict", "[", "k", "]", "=", "v", "\n", "", "return", "out_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat._create_coat": [[636, 645], ["kwargs.get", "helpers.build_model_with_cfg", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "def", "_create_coat", "(", "variant", ",", "pretrained", "=", "False", ",", "default_cfg", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "kwargs", ".", "get", "(", "'features_only'", ",", "None", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'features_only not implemented for Vision Transformer models.'", ")", "\n", "\n", "", "model", "=", "build_model_with_cfg", "(", "\n", "CoaT", ",", "variant", ",", "pretrained", ",", "\n", "pretrained_filter_fn", "=", "checkpoint_filter_fn", ",", "\n", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.coat_tiny": [[647, 654], ["dict", "coat._create_coat"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat._create_coat"], ["", "@", "register_model", "\n", "def", "coat_tiny", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_cfg", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "embed_dims", "=", "[", "152", ",", "152", ",", "152", ",", "152", "]", ",", "serial_depths", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "parallel_depth", "=", "6", ",", "\n", "num_heads", "=", "8", ",", "mlp_ratios", "=", "[", "4", ",", "4", ",", "4", ",", "4", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_coat", "(", "'coat_tiny'", ",", "pretrained", "=", "pretrained", ",", "**", "model_cfg", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.coat_mini": [[656, 663], ["dict", "coat._create_coat"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat._create_coat"], ["", "@", "register_model", "\n", "def", "coat_mini", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_cfg", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "embed_dims", "=", "[", "152", ",", "216", ",", "216", ",", "216", "]", ",", "serial_depths", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "parallel_depth", "=", "6", ",", "\n", "num_heads", "=", "8", ",", "mlp_ratios", "=", "[", "4", ",", "4", ",", "4", ",", "4", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_coat", "(", "'coat_mini'", ",", "pretrained", "=", "pretrained", ",", "**", "model_cfg", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.coat_lite_tiny": [[665, 672], ["dict", "coat._create_coat"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat._create_coat"], ["", "@", "register_model", "\n", "def", "coat_lite_tiny", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_cfg", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "embed_dims", "=", "[", "64", ",", "128", ",", "256", ",", "320", "]", ",", "serial_depths", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "parallel_depth", "=", "0", ",", "\n", "num_heads", "=", "8", ",", "mlp_ratios", "=", "[", "8", ",", "8", ",", "4", ",", "4", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_coat", "(", "'coat_lite_tiny'", ",", "pretrained", "=", "pretrained", ",", "**", "model_cfg", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.coat_lite_mini": [[674, 681], ["dict", "coat._create_coat"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat._create_coat"], ["", "@", "register_model", "\n", "def", "coat_lite_mini", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_cfg", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "embed_dims", "=", "[", "64", ",", "128", ",", "320", ",", "512", "]", ",", "serial_depths", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "parallel_depth", "=", "0", ",", "\n", "num_heads", "=", "8", ",", "mlp_ratios", "=", "[", "8", ",", "8", ",", "4", ",", "4", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_coat", "(", "'coat_lite_mini'", ",", "pretrained", "=", "pretrained", ",", "**", "model_cfg", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.coat_lite_small": [[683, 690], ["dict", "coat._create_coat"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat._create_coat"], ["", "@", "register_model", "\n", "def", "coat_lite_small", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model_cfg", "=", "dict", "(", "\n", "patch_size", "=", "4", ",", "embed_dims", "=", "[", "64", ",", "128", ",", "320", ",", "512", "]", ",", "serial_depths", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "parallel_depth", "=", "0", ",", "\n", "num_heads", "=", "8", ",", "mlp_ratios", "=", "[", "8", ",", "8", ",", "4", ",", "4", "]", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_coat", "(", "'coat_lite_small'", ",", "pretrained", "=", "pretrained", ",", "**", "model_cfg", ")", "\n", "return", "model", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.WindowAttention.__init__": [[141, 204], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.stack().permute().contiguous().unsqueeze", "torch.stack().permute().contiguous().unsqueeze", "torch.stack().permute().contiguous().unsqueeze", "torch.stack().permute().contiguous().unsqueeze", "torch.stack().permute().contiguous().unsqueeze", "torch.stack().permute().contiguous().unsqueeze", "torch.stack().permute().contiguous().unsqueeze", "torch.stack().permute().contiguous().unsqueeze", "torch.stack().permute().contiguous().unsqueeze", "torch.stack().permute().contiguous().unsqueeze", "torch.stack().permute().contiguous().unsqueeze", "torch.stack().permute().contiguous().unsqueeze", "torch.stack().permute().contiguous().unsqueeze", "torch.stack().permute().contiguous().unsqueeze", "torch.stack().permute().contiguous().unsqueeze", "torch.stack().permute().contiguous().unsqueeze", "swin_transformer_v2.WindowAttention.register_buffer", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "relative_coords.permute().contiguous.permute().contiguous.permute().contiguous", "relative_coords.permute().contiguous.permute().contiguous.sum", "swin_transformer_v2.WindowAttention.register_buffer", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "math.log2", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "swin_transformer_v2.WindowAttention.register_buffer", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.stack().permute().contiguous", "torch.stack().permute().contiguous", "torch.stack().permute().contiguous", "torch.stack().permute().contiguous", "torch.stack().permute().contiguous", "torch.stack().permute().contiguous", "torch.stack().permute().contiguous", "torch.stack().permute().contiguous", "torch.stack().permute().contiguous", "torch.stack().permute().contiguous", "torch.stack().permute().contiguous", "torch.stack().permute().contiguous", "torch.stack().permute().contiguous", "torch.stack().permute().contiguous", "torch.stack().permute().contiguous", "torch.stack().permute().contiguous", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.log2", "torch.log2", "torch.log2", "torch.log2", "torch.log2", "torch.log2", "torch.log2", "torch.log2", "torch.log2", "torch.log2", "torch.log2", "torch.log2", "torch.log2", "torch.log2", "torch.log2", "torch.log2", "relative_coords.permute().contiguous.permute().contiguous.permute", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "dim", ",", "window_size", ",", "num_heads", ",", "qkv_bias", "=", "True", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ",", "\n", "pretrained_window_size", "=", "[", "0", ",", "0", "]", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "window_size", "=", "window_size", "# Wh, Ww", "\n", "self", ".", "pretrained_window_size", "=", "pretrained_window_size", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "\n", "self", ".", "logit_scale", "=", "nn", ".", "Parameter", "(", "torch", ".", "log", "(", "10", "*", "torch", ".", "ones", "(", "(", "num_heads", ",", "1", ",", "1", ")", ")", ")", ")", "\n", "\n", "# mlp to generate continuous relative position bias", "\n", "self", ".", "cpb_mlp", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "2", ",", "512", ",", "bias", "=", "True", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Linear", "(", "512", ",", "num_heads", ",", "bias", "=", "False", ")", "\n", ")", "\n", "\n", "# get relative_coords_table", "\n", "relative_coords_h", "=", "torch", ".", "arange", "(", "-", "(", "self", ".", "window_size", "[", "0", "]", "-", "1", ")", ",", "self", ".", "window_size", "[", "0", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "relative_coords_w", "=", "torch", ".", "arange", "(", "-", "(", "self", ".", "window_size", "[", "1", "]", "-", "1", ")", ",", "self", ".", "window_size", "[", "1", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "relative_coords_table", "=", "torch", ".", "stack", "(", "torch", ".", "meshgrid", "(", "[", "\n", "relative_coords_h", ",", "\n", "relative_coords_w", "]", ")", ")", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ".", "contiguous", "(", ")", ".", "unsqueeze", "(", "0", ")", "# 1, 2*Wh-1, 2*Ww-1, 2", "\n", "if", "pretrained_window_size", "[", "0", "]", ">", "0", ":", "\n", "            ", "relative_coords_table", "[", ":", ",", ":", ",", ":", ",", "0", "]", "/=", "(", "pretrained_window_size", "[", "0", "]", "-", "1", ")", "\n", "relative_coords_table", "[", ":", ",", ":", ",", ":", ",", "1", "]", "/=", "(", "pretrained_window_size", "[", "1", "]", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "relative_coords_table", "[", ":", ",", ":", ",", ":", ",", "0", "]", "/=", "(", "self", ".", "window_size", "[", "0", "]", "-", "1", ")", "\n", "relative_coords_table", "[", ":", ",", ":", ",", ":", ",", "1", "]", "/=", "(", "self", ".", "window_size", "[", "1", "]", "-", "1", ")", "\n", "", "relative_coords_table", "*=", "8", "# normalize to -8, 8", "\n", "relative_coords_table", "=", "torch", ".", "sign", "(", "relative_coords_table", ")", "*", "torch", ".", "log2", "(", "\n", "torch", ".", "abs", "(", "relative_coords_table", ")", "+", "1.0", ")", "/", "math", ".", "log2", "(", "8", ")", "\n", "\n", "self", ".", "register_buffer", "(", "\"relative_coords_table\"", ",", "relative_coords_table", ",", "persistent", "=", "False", ")", "\n", "\n", "# get pair-wise relative position index for each token inside the window", "\n", "coords_h", "=", "torch", ".", "arange", "(", "self", ".", "window_size", "[", "0", "]", ")", "\n", "coords_w", "=", "torch", ".", "arange", "(", "self", ".", "window_size", "[", "1", "]", ")", "\n", "coords", "=", "torch", ".", "stack", "(", "torch", ".", "meshgrid", "(", "[", "coords_h", ",", "coords_w", "]", ")", ")", "# 2, Wh, Ww", "\n", "coords_flatten", "=", "torch", ".", "flatten", "(", "coords", ",", "1", ")", "# 2, Wh*Ww", "\n", "relative_coords", "=", "coords_flatten", "[", ":", ",", ":", ",", "None", "]", "-", "coords_flatten", "[", ":", ",", "None", ",", ":", "]", "# 2, Wh*Ww, Wh*Ww", "\n", "relative_coords", "=", "relative_coords", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ".", "contiguous", "(", ")", "# Wh*Ww, Wh*Ww, 2", "\n", "relative_coords", "[", ":", ",", ":", ",", "0", "]", "+=", "self", ".", "window_size", "[", "0", "]", "-", "1", "# shift to start from 0", "\n", "relative_coords", "[", ":", ",", ":", ",", "1", "]", "+=", "self", ".", "window_size", "[", "1", "]", "-", "1", "\n", "relative_coords", "[", ":", ",", ":", ",", "0", "]", "*=", "2", "*", "self", ".", "window_size", "[", "1", "]", "-", "1", "\n", "relative_position_index", "=", "relative_coords", ".", "sum", "(", "-", "1", ")", "# Wh*Ww, Wh*Ww", "\n", "self", ".", "register_buffer", "(", "\"relative_position_index\"", ",", "relative_position_index", ",", "persistent", "=", "False", ")", "\n", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", "*", "3", ",", "bias", "=", "False", ")", "\n", "if", "qkv_bias", ":", "\n", "            ", "self", ".", "q_bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "dim", ")", ")", "\n", "self", ".", "register_buffer", "(", "'k_bias'", ",", "torch", ".", "zeros", "(", "dim", ")", ",", "persistent", "=", "False", ")", "\n", "self", ".", "v_bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "dim", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "q_bias", "=", "None", "\n", "self", ".", "k_bias", "=", "None", "\n", "self", ".", "v_bias", "=", "None", "\n", "", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.WindowAttention.forward": [[205, 245], ["torch.linear", "torch.linear", "torch.linear", "torch.linear", "qkv.reshape().permute.reshape().permute.reshape().permute", "qkv.reshape().permute.reshape().permute.unbind", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "torch.clamp().exp", "swin_transformer_v2.WindowAttention.cpb_mlp().view", "relative_position_bias_table[].view", "relative_position_bias.permute().contiguous.permute().contiguous.permute().contiguous", "swin_transformer_v2.WindowAttention.attn_drop", "swin_transformer_v2.WindowAttention.proj", "swin_transformer_v2.WindowAttention.proj_drop", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize().transpose", "torch.normalize().transpose", "torch.normalize().transpose", "torch.normalize().transpose", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "relative_position_bias.permute().contiguous.permute().contiguous.unsqueeze", "swin_transformer_v2.WindowAttention.view", "swin_transformer_v2.WindowAttention.softmax", "swin_transformer_v2.WindowAttention.softmax", "qkv.reshape().permute.reshape().permute.reshape", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "swin_transformer_v2.WindowAttention.cpb_mlp", "relative_position_bias.permute().contiguous.permute().contiguous.permute", "swin_transformer_v2.WindowAttention.view", "mask.unsqueeze().unsqueeze", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "math.log", "swin_transformer_v2.WindowAttention.relative_position_index.view", "mask.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid"], ["", "def", "forward", "(", "self", ",", "x", ",", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"", "\n", "B_", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "qkv_bias", "=", "None", "\n", "if", "self", ".", "q_bias", "is", "not", "None", ":", "\n", "            ", "qkv_bias", "=", "torch", ".", "cat", "(", "(", "self", ".", "q_bias", ",", "self", ".", "k_bias", ",", "self", ".", "v_bias", ")", ")", "\n", "", "qkv", "=", "F", ".", "linear", "(", "input", "=", "x", ",", "weight", "=", "self", ".", "qkv", ".", "weight", ",", "bias", "=", "qkv_bias", ")", "\n", "qkv", "=", "qkv", ".", "reshape", "(", "B_", ",", "N", ",", "3", ",", "self", ".", "num_heads", ",", "-", "1", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "q", ",", "k", ",", "v", "=", "qkv", ".", "unbind", "(", "0", ")", "\n", "\n", "# cosine attention", "\n", "attn", "=", "(", "F", ".", "normalize", "(", "q", ",", "dim", "=", "-", "1", ")", "@", "F", ".", "normalize", "(", "k", ",", "dim", "=", "-", "1", ")", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "\n", "logit_scale", "=", "torch", ".", "clamp", "(", "self", ".", "logit_scale", ",", "max", "=", "math", ".", "log", "(", "1.", "/", "0.01", ")", ")", ".", "exp", "(", ")", "\n", "attn", "=", "attn", "*", "logit_scale", "\n", "\n", "relative_position_bias_table", "=", "self", ".", "cpb_mlp", "(", "self", ".", "relative_coords_table", ")", ".", "view", "(", "-", "1", ",", "self", ".", "num_heads", ")", "\n", "relative_position_bias", "=", "relative_position_bias_table", "[", "self", ".", "relative_position_index", ".", "view", "(", "-", "1", ")", "]", ".", "view", "(", "\n", "self", ".", "window_size", "[", "0", "]", "*", "self", ".", "window_size", "[", "1", "]", ",", "self", ".", "window_size", "[", "0", "]", "*", "self", ".", "window_size", "[", "1", "]", ",", "-", "1", ")", "# Wh*Ww,Wh*Ww,nH", "\n", "relative_position_bias", "=", "relative_position_bias", ".", "permute", "(", "2", ",", "0", ",", "1", ")", ".", "contiguous", "(", ")", "# nH, Wh*Ww, Wh*Ww", "\n", "relative_position_bias", "=", "16", "*", "torch", ".", "sigmoid", "(", "relative_position_bias", ")", "\n", "attn", "=", "attn", "+", "relative_position_bias", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "nW", "=", "mask", ".", "shape", "[", "0", "]", "\n", "attn", "=", "attn", ".", "view", "(", "B_", "//", "nW", ",", "nW", ",", "self", ".", "num_heads", ",", "N", ",", "N", ")", "+", "mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "0", ")", "\n", "attn", "=", "attn", ".", "view", "(", "-", "1", ",", "self", ".", "num_heads", ",", "N", ",", "N", ")", "\n", "attn", "=", "self", ".", "softmax", "(", "attn", ")", "\n", "", "else", ":", "\n", "            ", "attn", "=", "self", ".", "softmax", "(", "attn", ")", "\n", "\n", "", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B_", ",", "N", ",", "C", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerBlock.__init__": [[266, 314], ["torch.Module.__init__", "layers.to_2tuple", "swin_transformer_v2.SwinTransformerBlock._calc_window_shift", "swin_transformer_v2.WindowAttention", "norm_layer", "layers.Mlp", "norm_layer", "any", "swin_transformer_v2.SwinTransformerBlock.register_buffer", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "swin_transformer_v2.window_partition", "mask_windows.view.view.view", "attn_mask.masked_fill().masked_fill.masked_fill().masked_fill.masked_fill().masked_fill", "layers.to_2tuple", "layers.to_2tuple", "int", "slice", "slice", "slice", "mask_windows.view.view.unsqueeze", "mask_windows.view.view.unsqueeze", "float", "slice", "slice", "slice", "attn_mask.masked_fill().masked_fill.masked_fill().masked_fill.masked_fill", "float"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerBlock._calc_window_shift", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.window_partition"], ["def", "__init__", "(", "\n", "self", ",", "dim", ",", "input_resolution", ",", "num_heads", ",", "window_size", "=", "7", ",", "shift_size", "=", "0", ",", "\n", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "True", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "drop_path", "=", "0.", ",", "\n", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ",", "pretrained_window_size", "=", "0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "input_resolution", "=", "to_2tuple", "(", "input_resolution", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "ws", ",", "ss", "=", "self", ".", "_calc_window_shift", "(", "window_size", ",", "shift_size", ")", "\n", "self", ".", "window_size", ":", "Tuple", "[", "int", ",", "int", "]", "=", "ws", "\n", "self", ".", "shift_size", ":", "Tuple", "[", "int", ",", "int", "]", "=", "ss", "\n", "self", ".", "window_area", "=", "self", ".", "window_size", "[", "0", "]", "*", "self", ".", "window_size", "[", "1", "]", "\n", "self", ".", "mlp_ratio", "=", "mlp_ratio", "\n", "\n", "self", ".", "attn", "=", "WindowAttention", "(", "\n", "dim", ",", "window_size", "=", "to_2tuple", "(", "self", ".", "window_size", ")", ",", "num_heads", "=", "num_heads", ",", "\n", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ",", "\n", "pretrained_window_size", "=", "to_2tuple", "(", "pretrained_window_size", ")", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "drop_path1", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "int", "(", "dim", "*", "mlp_ratio", ")", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "drop_path2", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "if", "any", "(", "self", ".", "shift_size", ")", ":", "\n", "# calculate attention mask for SW-MSA", "\n", "            ", "H", ",", "W", "=", "self", ".", "input_resolution", "\n", "img_mask", "=", "torch", ".", "zeros", "(", "(", "1", ",", "H", ",", "W", ",", "1", ")", ")", "# 1 H W 1", "\n", "cnt", "=", "0", "\n", "for", "h", "in", "(", "\n", "slice", "(", "0", ",", "-", "self", ".", "window_size", "[", "0", "]", ")", ",", "\n", "slice", "(", "-", "self", ".", "window_size", "[", "0", "]", ",", "-", "self", ".", "shift_size", "[", "0", "]", ")", ",", "\n", "slice", "(", "-", "self", ".", "shift_size", "[", "0", "]", ",", "None", ")", ")", ":", "\n", "                ", "for", "w", "in", "(", "\n", "slice", "(", "0", ",", "-", "self", ".", "window_size", "[", "1", "]", ")", ",", "\n", "slice", "(", "-", "self", ".", "window_size", "[", "1", "]", ",", "-", "self", ".", "shift_size", "[", "1", "]", ")", ",", "\n", "slice", "(", "-", "self", ".", "shift_size", "[", "1", "]", ",", "None", ")", ")", ":", "\n", "                    ", "img_mask", "[", ":", ",", "h", ",", "w", ",", ":", "]", "=", "cnt", "\n", "cnt", "+=", "1", "\n", "", "", "mask_windows", "=", "window_partition", "(", "img_mask", ",", "self", ".", "window_size", ")", "# nW, window_size, window_size, 1", "\n", "mask_windows", "=", "mask_windows", ".", "view", "(", "-", "1", ",", "self", ".", "window_area", ")", "\n", "attn_mask", "=", "mask_windows", ".", "unsqueeze", "(", "1", ")", "-", "mask_windows", ".", "unsqueeze", "(", "2", ")", "\n", "attn_mask", "=", "attn_mask", ".", "masked_fill", "(", "attn_mask", "!=", "0", ",", "float", "(", "-", "100.0", ")", ")", ".", "masked_fill", "(", "attn_mask", "==", "0", ",", "float", "(", "0.0", ")", ")", "\n", "", "else", ":", "\n", "            ", "attn_mask", "=", "None", "\n", "\n", "", "self", ".", "register_buffer", "(", "\"attn_mask\"", ",", "attn_mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerBlock._calc_window_shift": [[315, 321], ["layers.to_2tuple", "layers.to_2tuple", "tuple", "tuple", "zip", "zip"], "methods", ["None"], ["", "def", "_calc_window_shift", "(", "self", ",", "target_window_size", ",", "target_shift_size", ")", "->", "Tuple", "[", "Tuple", "[", "int", ",", "int", "]", ",", "Tuple", "[", "int", ",", "int", "]", "]", ":", "\n", "        ", "target_window_size", "=", "to_2tuple", "(", "target_window_size", ")", "\n", "target_shift_size", "=", "to_2tuple", "(", "target_shift_size", ")", "\n", "window_size", "=", "[", "r", "if", "r", "<=", "w", "else", "w", "for", "r", ",", "w", "in", "zip", "(", "self", ".", "input_resolution", ",", "target_window_size", ")", "]", "\n", "shift_size", "=", "[", "0", "if", "r", "<=", "w", "else", "s", "for", "r", ",", "w", ",", "s", "in", "zip", "(", "self", ".", "input_resolution", ",", "window_size", ",", "target_shift_size", ")", "]", "\n", "return", "tuple", "(", "window_size", ")", ",", "tuple", "(", "shift_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerBlock._attn": [[322, 353], ["layers._assert", "torch.roll.view", "torch.roll.view", "torch.roll.view", "torch.roll.view", "any", "swin_transformer_v2.window_partition", "x_windows.view.view.view", "swin_transformer_v2.SwinTransformerBlock.attn", "attn_windows.view.view.view", "swin_transformer_v2.window_reverse", "torch.roll.view", "torch.roll.view", "torch.roll.view", "torch.roll.view", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.window_partition", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.window_reverse"], ["", "def", "_attn", "(", "self", ",", "x", ")", ":", "\n", "        ", "H", ",", "W", "=", "self", ".", "input_resolution", "\n", "B", ",", "L", ",", "C", "=", "x", ".", "shape", "\n", "_assert", "(", "L", "==", "H", "*", "W", ",", "\"input feature has wrong size\"", ")", "\n", "x", "=", "x", ".", "view", "(", "B", ",", "H", ",", "W", ",", "C", ")", "\n", "\n", "# cyclic shift", "\n", "has_shift", "=", "any", "(", "self", ".", "shift_size", ")", "\n", "if", "has_shift", ":", "\n", "            ", "shifted_x", "=", "torch", ".", "roll", "(", "x", ",", "shifts", "=", "(", "-", "self", ".", "shift_size", "[", "0", "]", ",", "-", "self", ".", "shift_size", "[", "1", "]", ")", ",", "dims", "=", "(", "1", ",", "2", ")", ")", "\n", "", "else", ":", "\n", "            ", "shifted_x", "=", "x", "\n", "\n", "# partition windows", "\n", "", "x_windows", "=", "window_partition", "(", "shifted_x", ",", "self", ".", "window_size", ")", "# nW*B, window_size, window_size, C", "\n", "x_windows", "=", "x_windows", ".", "view", "(", "-", "1", ",", "self", ".", "window_area", ",", "C", ")", "# nW*B, window_size*window_size, C", "\n", "\n", "# W-MSA/SW-MSA", "\n", "attn_windows", "=", "self", ".", "attn", "(", "x_windows", ",", "mask", "=", "self", ".", "attn_mask", ")", "# nW*B, window_size*window_size, C", "\n", "\n", "# merge windows", "\n", "attn_windows", "=", "attn_windows", ".", "view", "(", "-", "1", ",", "self", ".", "window_size", "[", "0", "]", ",", "self", ".", "window_size", "[", "1", "]", ",", "C", ")", "\n", "shifted_x", "=", "window_reverse", "(", "attn_windows", ",", "self", ".", "window_size", ",", "self", ".", "input_resolution", ")", "# B H' W' C", "\n", "\n", "# reverse cyclic shift", "\n", "if", "has_shift", ":", "\n", "            ", "x", "=", "torch", ".", "roll", "(", "shifted_x", ",", "shifts", "=", "self", ".", "shift_size", ",", "dims", "=", "(", "1", ",", "2", ")", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "shifted_x", "\n", "", "x", "=", "x", ".", "view", "(", "B", ",", "H", "*", "W", ",", "C", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerBlock.forward": [[354, 358], ["swin_transformer_v2.SwinTransformerBlock.drop_path1", "swin_transformer_v2.SwinTransformerBlock.drop_path2", "swin_transformer_v2.SwinTransformerBlock.norm1", "swin_transformer_v2.SwinTransformerBlock.norm2", "swin_transformer_v2.SwinTransformerBlock._attn", "swin_transformer_v2.SwinTransformerBlock.mlp"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerBlock._attn"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", "+", "self", ".", "drop_path1", "(", "self", ".", "norm1", "(", "self", ".", "_attn", "(", "x", ")", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path2", "(", "self", ".", "norm2", "(", "self", ".", "mlp", "(", "x", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.PatchMerging.__init__": [[369, 375], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "norm_layer"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "input_resolution", ",", "dim", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_resolution", "=", "input_resolution", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "reduction", "=", "nn", ".", "Linear", "(", "4", "*", "dim", ",", "2", "*", "dim", ",", "bias", "=", "False", ")", "\n", "self", ".", "norm", "=", "norm_layer", "(", "2", "*", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.PatchMerging.forward": [[376, 399], ["layers._assert", "layers._assert", "layers._assert", "swin_transformer_v2.PatchMerging.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "swin_transformer_v2.PatchMerging.view", "swin_transformer_v2.PatchMerging.reduction", "swin_transformer_v2.PatchMerging.norm"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.reduction"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        x: B, H*W, C\n        \"\"\"", "\n", "H", ",", "W", "=", "self", ".", "input_resolution", "\n", "B", ",", "L", ",", "C", "=", "x", ".", "shape", "\n", "_assert", "(", "L", "==", "H", "*", "W", ",", "\"input feature has wrong size\"", ")", "\n", "_assert", "(", "H", "%", "2", "==", "0", ",", "f\"x size ({H}*{W}) are not even.\"", ")", "\n", "_assert", "(", "W", "%", "2", "==", "0", ",", "f\"x size ({H}*{W}) are not even.\"", ")", "\n", "\n", "x", "=", "x", ".", "view", "(", "B", ",", "H", ",", "W", ",", "C", ")", "\n", "\n", "x0", "=", "x", "[", ":", ",", "0", ":", ":", "2", ",", "0", ":", ":", "2", ",", ":", "]", "# B H/2 W/2 C", "\n", "x1", "=", "x", "[", ":", ",", "1", ":", ":", "2", ",", "0", ":", ":", "2", ",", ":", "]", "# B H/2 W/2 C", "\n", "x2", "=", "x", "[", ":", ",", "0", ":", ":", "2", ",", "1", ":", ":", "2", ",", ":", "]", "# B H/2 W/2 C", "\n", "x3", "=", "x", "[", ":", ",", "1", ":", ":", "2", ",", "1", ":", ":", "2", ",", ":", "]", "# B H/2 W/2 C", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x0", ",", "x1", ",", "x2", ",", "x3", "]", ",", "-", "1", ")", "# B H/2 W/2 4*C", "\n", "x", "=", "x", ".", "view", "(", "B", ",", "-", "1", ",", "4", "*", "C", ")", "# B H/2*W/2 4*C", "\n", "\n", "x", "=", "self", ".", "reduction", "(", "x", ")", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.BasicLayer.__init__": [[420, 450], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "downsample", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "swin_transformer_v2.SwinTransformerBlock", "range", "isinstance"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample"], ["def", "__init__", "(", "\n", "self", ",", "dim", ",", "input_resolution", ",", "depth", ",", "num_heads", ",", "window_size", ",", "\n", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "True", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "drop_path", "=", "0.", ",", "\n", "norm_layer", "=", "nn", ".", "LayerNorm", ",", "downsample", "=", "None", ",", "pretrained_window_size", "=", "0", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "input_resolution", "=", "input_resolution", "\n", "self", ".", "depth", "=", "depth", "\n", "self", ".", "grad_checkpointing", "=", "False", "\n", "\n", "# build blocks", "\n", "self", ".", "blocks", "=", "nn", ".", "ModuleList", "(", "[", "\n", "SwinTransformerBlock", "(", "\n", "dim", "=", "dim", ",", "input_resolution", "=", "input_resolution", ",", "\n", "num_heads", "=", "num_heads", ",", "window_size", "=", "window_size", ",", "\n", "shift_size", "=", "0", "if", "(", "i", "%", "2", "==", "0", ")", "else", "window_size", "//", "2", ",", "\n", "mlp_ratio", "=", "mlp_ratio", ",", "\n", "qkv_bias", "=", "qkv_bias", ",", "\n", "drop", "=", "drop", ",", "attn_drop", "=", "attn_drop", ",", "\n", "drop_path", "=", "drop_path", "[", "i", "]", "if", "isinstance", "(", "drop_path", ",", "list", ")", "else", "drop_path", ",", "\n", "norm_layer", "=", "norm_layer", ",", "\n", "pretrained_window_size", "=", "pretrained_window_size", ")", "\n", "for", "i", "in", "range", "(", "depth", ")", "]", ")", "\n", "\n", "# patch merging layer", "\n", "if", "downsample", "is", "not", "None", ":", "\n", "            ", "self", ".", "downsample", "=", "downsample", "(", "input_resolution", ",", "dim", "=", "dim", ",", "norm_layer", "=", "norm_layer", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "downsample", "=", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.BasicLayer.forward": [[451, 459], ["swin_transformer_v2.BasicLayer.downsample", "torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "blk", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "for", "blk", "in", "self", ".", "blocks", ":", "\n", "            ", "if", "self", ".", "grad_checkpointing", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "                ", "x", "=", "checkpoint", ".", "checkpoint", "(", "blk", ",", "x", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "blk", "(", "x", ")", "\n", "", "", "x", "=", "self", ".", "downsample", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.BasicLayer._init_respostnorm": [[460, 466], ["torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["None"], ["", "def", "_init_respostnorm", "(", "self", ")", ":", "\n", "        ", "for", "blk", "in", "self", ".", "blocks", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "blk", ".", "norm1", ".", "bias", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "blk", ".", "norm1", ".", "weight", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "blk", ".", "norm2", ".", "bias", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "blk", ".", "norm2", ".", "weight", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.__init__": [[493, 555], ["torch.Module.__init__", "len", "int", "layers.PatchEmbed", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "norm_layer", "swin_transformer_v2.SwinTransformerV2.apply", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "layers.trunc_normal_", "x.item", "swin_transformer_v2.BasicLayer", "swin_transformer_v2.SwinTransformerV2.layers.append", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "bly._init_respostnorm", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "sum", "int", "sum", "sum"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.BasicLayer._init_respostnorm"], ["def", "__init__", "(", "\n", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "4", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "global_pool", "=", "'avg'", ",", "\n", "embed_dim", "=", "96", ",", "depths", "=", "(", "2", ",", "2", ",", "6", ",", "2", ")", ",", "num_heads", "=", "(", "3", ",", "6", ",", "12", ",", "24", ")", ",", "\n", "window_size", "=", "7", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "True", ",", "\n", "drop_rate", "=", "0.", ",", "attn_drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.1", ",", "\n", "norm_layer", "=", "nn", ".", "LayerNorm", ",", "ape", "=", "False", ",", "patch_norm", "=", "True", ",", "\n", "pretrained_window_sizes", "=", "(", "0", ",", "0", ",", "0", ",", "0", ")", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "assert", "global_pool", "in", "(", "''", ",", "'avg'", ")", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "self", ".", "num_layers", "=", "len", "(", "depths", ")", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "patch_norm", "=", "patch_norm", "\n", "self", ".", "num_features", "=", "int", "(", "embed_dim", "*", "2", "**", "(", "self", ".", "num_layers", "-", "1", ")", ")", "\n", "\n", "# split image into non-overlapping patches", "\n", "self", ".", "patch_embed", "=", "PatchEmbed", "(", "\n", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", ",", "in_chans", "=", "in_chans", ",", "embed_dim", "=", "embed_dim", ",", "\n", "norm_layer", "=", "norm_layer", "if", "self", ".", "patch_norm", "else", "None", ")", "\n", "num_patches", "=", "self", ".", "patch_embed", ".", "num_patches", "\n", "\n", "# absolute position embedding", "\n", "if", "ape", ":", "\n", "            ", "self", ".", "absolute_pos_embed", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "num_patches", ",", "embed_dim", ")", ")", "\n", "trunc_normal_", "(", "self", ".", "absolute_pos_embed", ",", "std", "=", ".02", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "absolute_pos_embed", "=", "None", "\n", "\n", "", "self", ".", "pos_drop", "=", "nn", ".", "Dropout", "(", "p", "=", "drop_rate", ")", "\n", "\n", "# stochastic depth", "\n", "dpr", "=", "[", "x", ".", "item", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "sum", "(", "depths", ")", ")", "]", "# stochastic depth decay rule", "\n", "\n", "# build layers", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i_layer", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "            ", "layer", "=", "BasicLayer", "(", "\n", "dim", "=", "int", "(", "embed_dim", "*", "2", "**", "i_layer", ")", ",", "\n", "input_resolution", "=", "(", "\n", "self", ".", "patch_embed", ".", "grid_size", "[", "0", "]", "//", "(", "2", "**", "i_layer", ")", ",", "\n", "self", ".", "patch_embed", ".", "grid_size", "[", "1", "]", "//", "(", "2", "**", "i_layer", ")", ")", ",", "\n", "depth", "=", "depths", "[", "i_layer", "]", ",", "\n", "num_heads", "=", "num_heads", "[", "i_layer", "]", ",", "\n", "window_size", "=", "window_size", ",", "\n", "mlp_ratio", "=", "mlp_ratio", ",", "\n", "qkv_bias", "=", "qkv_bias", ",", "\n", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "\n", "drop_path", "=", "dpr", "[", "sum", "(", "depths", "[", ":", "i_layer", "]", ")", ":", "sum", "(", "depths", "[", ":", "i_layer", "+", "1", "]", ")", "]", ",", "\n", "norm_layer", "=", "norm_layer", ",", "\n", "downsample", "=", "PatchMerging", "if", "(", "i_layer", "<", "self", ".", "num_layers", "-", "1", ")", "else", "None", ",", "\n", "pretrained_window_size", "=", "pretrained_window_sizes", "[", "i_layer", "]", "\n", ")", "\n", "self", ".", "layers", ".", "append", "(", "layer", ")", "\n", "\n", "", "self", ".", "norm", "=", "norm_layer", "(", "self", ".", "num_features", ")", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "apply", "(", "self", ".", "_init_weights", ")", "\n", "for", "bly", "in", "self", ".", "layers", ":", "\n", "            ", "bly", ".", "_init_respostnorm", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2._init_weights": [[556, 561], ["isinstance", "layers.trunc_normal_", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["", "", "def", "_init_weights", "(", "self", ",", "m", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "            ", "trunc_normal_", "(", "m", ".", "weight", ",", "std", "=", ".02", ")", "\n", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.no_weight_decay": [[562, 569], ["swin_transformer_v2.SwinTransformerV2.named_modules", "any", "nod.add"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_modules"], ["", "", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "no_weight_decay", "(", "self", ")", ":", "\n", "        ", "nod", "=", "{", "'absolute_pos_embed'", "}", "\n", "for", "n", ",", "m", "in", "self", ".", "named_modules", "(", ")", ":", "\n", "            ", "if", "any", "(", "[", "kw", "in", "n", "for", "kw", "in", "(", "\"cpb_mlp\"", ",", "\"logit_scale\"", ",", "'relative_position_bias_table'", ")", "]", ")", ":", "\n", "                ", "nod", ".", "add", "(", "n", ")", "\n", "", "", "return", "nod", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.group_matcher": [[570, 578], ["dict"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "group_matcher", "(", "self", ",", "coarse", "=", "False", ")", ":", "\n", "        ", "return", "dict", "(", "\n", "stem", "=", "r'^absolute_pos_embed|patch_embed'", ",", "# stem and embed", "\n", "blocks", "=", "r'^layers\\.(\\d+)'", "if", "coarse", "else", "[", "\n", "(", "r'^layers\\.(\\d+).downsample'", ",", "(", "0", ",", ")", ")", ",", "\n", "(", "r'^layers\\.(\\d+)\\.\\w+\\.(\\d+)'", ",", "None", ")", ",", "\n", "(", "r'^norm'", ",", "(", "99999", ",", ")", ")", ",", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.set_grad_checkpointing": [[581, 585], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "set_grad_checkpointing", "(", "self", ",", "enable", "=", "True", ")", ":", "\n", "        ", "for", "l", "in", "self", ".", "layers", ":", "\n", "            ", "l", ".", "grad_checkpointing", "=", "enable", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.get_classifier": [[586, 589], ["None"], "methods", ["None"], ["", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.reset_classifier": [[590, 596], ["torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "if", "global_pool", "is", "not", "None", ":", "\n", "            ", "assert", "global_pool", "in", "(", "''", ",", "'avg'", ")", "\n", "self", ".", "global_pool", "=", "global_pool", "\n", "", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features": [[597, 608], ["swin_transformer_v2.SwinTransformerV2.patch_embed", "swin_transformer_v2.SwinTransformerV2.pos_drop", "swin_transformer_v2.SwinTransformerV2.norm", "layer"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "patch_embed", "(", "x", ")", "\n", "if", "self", ".", "absolute_pos_embed", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", "+", "self", ".", "absolute_pos_embed", "\n", "", "x", "=", "self", ".", "pos_drop", "(", "x", ")", "\n", "\n", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "x", "=", "layer", "(", "x", ")", "\n", "\n", "", "x", "=", "self", ".", "norm", "(", "x", ")", "# B L C", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head": [[609, 613], ["x.mean.mean.mean", "swin_transformer_v2.SwinTransformerV2.head"], "methods", ["None"], ["", "def", "forward_head", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "global_pool", "==", "'avg'", ":", "\n", "            ", "x", "=", "x", ".", "mean", "(", "dim", "=", "1", ")", "\n", "", "return", "x", "if", "pre_logits", "else", "self", ".", "head", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward": [[614, 618], ["swin_transformer_v2.SwinTransformerV2.forward_features", "swin_transformer_v2.SwinTransformerV2.forward_head"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_head"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "self", ".", "forward_head", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2._cfg": [[30, 38], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", ".9", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'patch_embed.proj'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.window_partition": [[94, 107], ["x.view.view", "x.view.permute().contiguous().view", "x.view.permute().contiguous", "x.view.permute"], "function", ["None"], ["def", "window_partition", "(", "x", ",", "window_size", ":", "Tuple", "[", "int", ",", "int", "]", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"", "\n", "B", ",", "H", ",", "W", ",", "C", "=", "x", ".", "shape", "\n", "x", "=", "x", ".", "view", "(", "B", ",", "H", "//", "window_size", "[", "0", "]", ",", "window_size", "[", "0", "]", ",", "W", "//", "window_size", "[", "1", "]", ",", "window_size", "[", "1", "]", ",", "C", ")", "\n", "windows", "=", "x", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ",", "4", ",", "5", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "window_size", "[", "0", "]", ",", "window_size", "[", "1", "]", ",", "C", ")", "\n", "return", "windows", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.window_reverse": [[109, 125], ["int", "windows.view", "x.permute().contiguous().view.permute().contiguous().view", "x.permute().contiguous().view.permute().contiguous", "x.permute().contiguous().view.permute"], "function", ["None"], ["", "@", "register_notrace_function", "# reason: int argument is a Proxy", "\n", "def", "window_reverse", "(", "windows", ",", "window_size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "img_size", ":", "Tuple", "[", "int", ",", "int", "]", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        windows: (num_windows * B, window_size[0], window_size[1], C)\n        window_size (Tuple[int, int]): Window size\n        img_size (Tuple[int, int]): Image size\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"", "\n", "H", ",", "W", "=", "img_size", "\n", "B", "=", "int", "(", "windows", ".", "shape", "[", "0", "]", "/", "(", "H", "*", "W", "/", "window_size", "[", "0", "]", "/", "window_size", "[", "1", "]", ")", ")", "\n", "x", "=", "windows", ".", "view", "(", "B", ",", "H", "//", "window_size", "[", "0", "]", ",", "W", "//", "window_size", "[", "1", "]", ",", "window_size", "[", "0", "]", ",", "window_size", "[", "1", "]", ",", "-", "1", ")", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ",", "4", ",", "5", ")", ".", "contiguous", "(", ")", ".", "view", "(", "B", ",", "H", ",", "W", ",", "-", "1", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.checkpoint_filter_fn": [[620, 630], ["state_dict.items", "any"], "function", ["None"], ["", "", "def", "checkpoint_filter_fn", "(", "state_dict", ",", "model", ")", ":", "\n", "    ", "out_dict", "=", "{", "}", "\n", "if", "'model'", "in", "state_dict", ":", "\n", "# For deit models", "\n", "        ", "state_dict", "=", "state_dict", "[", "'model'", "]", "\n", "", "for", "k", ",", "v", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "        ", "if", "any", "(", "[", "n", "in", "k", "for", "n", "in", "(", "'relative_position_index'", ",", "'relative_coords_table'", ")", "]", ")", ":", "\n", "            ", "continue", "# skip buffers that should not be persistent", "\n", "", "out_dict", "[", "k", "]", "=", "v", "\n", "", "return", "out_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2._create_swin_transformer_v2": [[632, 638], ["helpers.build_model_with_cfg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "def", "_create_swin_transformer_v2", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model", "=", "build_model_with_cfg", "(", "\n", "SwinTransformerV2", ",", "variant", ",", "pretrained", ",", "\n", "pretrained_filter_fn", "=", "checkpoint_filter_fn", ",", "\n", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.swinv2_tiny_window16_256": [[640, 647], ["dict", "swin_transformer_v2._create_swin_transformer_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2._create_swin_transformer_v2"], ["", "@", "register_model", "\n", "def", "swinv2_tiny_window16_256", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "window_size", "=", "16", ",", "embed_dim", "=", "96", ",", "depths", "=", "(", "2", ",", "2", ",", "6", ",", "2", ")", ",", "num_heads", "=", "(", "3", ",", "6", ",", "12", ",", "24", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer_v2", "(", "'swinv2_tiny_window16_256'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.swinv2_tiny_window8_256": [[649, 656], ["dict", "swin_transformer_v2._create_swin_transformer_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2._create_swin_transformer_v2"], ["", "@", "register_model", "\n", "def", "swinv2_tiny_window8_256", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "window_size", "=", "8", ",", "embed_dim", "=", "96", ",", "depths", "=", "(", "2", ",", "2", ",", "6", ",", "2", ")", ",", "num_heads", "=", "(", "3", ",", "6", ",", "12", ",", "24", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer_v2", "(", "'swinv2_tiny_window8_256'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.swinv2_small_window16_256": [[658, 665], ["dict", "swin_transformer_v2._create_swin_transformer_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2._create_swin_transformer_v2"], ["", "@", "register_model", "\n", "def", "swinv2_small_window16_256", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "window_size", "=", "16", ",", "embed_dim", "=", "96", ",", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "num_heads", "=", "(", "3", ",", "6", ",", "12", ",", "24", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer_v2", "(", "'swinv2_small_window16_256'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.swinv2_small_window8_256": [[667, 674], ["dict", "swin_transformer_v2._create_swin_transformer_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2._create_swin_transformer_v2"], ["", "@", "register_model", "\n", "def", "swinv2_small_window8_256", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "window_size", "=", "8", ",", "embed_dim", "=", "96", ",", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "num_heads", "=", "(", "3", ",", "6", ",", "12", ",", "24", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer_v2", "(", "'swinv2_small_window8_256'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.swinv2_base_window16_256": [[676, 683], ["dict", "swin_transformer_v2._create_swin_transformer_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2._create_swin_transformer_v2"], ["", "@", "register_model", "\n", "def", "swinv2_base_window16_256", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "window_size", "=", "16", ",", "embed_dim", "=", "128", ",", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "num_heads", "=", "(", "4", ",", "8", ",", "16", ",", "32", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer_v2", "(", "'swinv2_base_window16_256'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.swinv2_base_window8_256": [[685, 692], ["dict", "swin_transformer_v2._create_swin_transformer_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2._create_swin_transformer_v2"], ["", "@", "register_model", "\n", "def", "swinv2_base_window8_256", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "window_size", "=", "8", ",", "embed_dim", "=", "128", ",", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "num_heads", "=", "(", "4", ",", "8", ",", "16", ",", "32", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer_v2", "(", "'swinv2_base_window8_256'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.swinv2_base_window12_192_22k": [[694, 701], ["dict", "swin_transformer_v2._create_swin_transformer_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2._create_swin_transformer_v2"], ["", "@", "register_model", "\n", "def", "swinv2_base_window12_192_22k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "window_size", "=", "12", ",", "embed_dim", "=", "128", ",", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "num_heads", "=", "(", "4", ",", "8", ",", "16", ",", "32", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer_v2", "(", "'swinv2_base_window12_192_22k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.swinv2_base_window12to16_192to256_22kft1k": [[703, 712], ["dict", "swin_transformer_v2._create_swin_transformer_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2._create_swin_transformer_v2"], ["", "@", "register_model", "\n", "def", "swinv2_base_window12to16_192to256_22kft1k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "window_size", "=", "16", ",", "embed_dim", "=", "128", ",", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "num_heads", "=", "(", "4", ",", "8", ",", "16", ",", "32", ")", ",", "\n", "pretrained_window_sizes", "=", "(", "12", ",", "12", ",", "12", ",", "6", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer_v2", "(", "\n", "'swinv2_base_window12to16_192to256_22kft1k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.swinv2_base_window12to24_192to384_22kft1k": [[714, 723], ["dict", "swin_transformer_v2._create_swin_transformer_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2._create_swin_transformer_v2"], ["", "@", "register_model", "\n", "def", "swinv2_base_window12to24_192to384_22kft1k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "window_size", "=", "24", ",", "embed_dim", "=", "128", ",", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "num_heads", "=", "(", "4", ",", "8", ",", "16", ",", "32", ")", ",", "\n", "pretrained_window_sizes", "=", "(", "12", ",", "12", ",", "12", ",", "6", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer_v2", "(", "\n", "'swinv2_base_window12to24_192to384_22kft1k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.swinv2_large_window12_192_22k": [[725, 732], ["dict", "swin_transformer_v2._create_swin_transformer_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2._create_swin_transformer_v2"], ["", "@", "register_model", "\n", "def", "swinv2_large_window12_192_22k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "window_size", "=", "12", ",", "embed_dim", "=", "192", ",", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "num_heads", "=", "(", "6", ",", "12", ",", "24", ",", "48", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer_v2", "(", "'swinv2_large_window12_192_22k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.swinv2_large_window12to16_192to256_22kft1k": [[734, 743], ["dict", "swin_transformer_v2._create_swin_transformer_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2._create_swin_transformer_v2"], ["", "@", "register_model", "\n", "def", "swinv2_large_window12to16_192to256_22kft1k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "window_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "num_heads", "=", "(", "6", ",", "12", ",", "24", ",", "48", ")", ",", "\n", "pretrained_window_sizes", "=", "(", "12", ",", "12", ",", "12", ",", "6", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer_v2", "(", "\n", "'swinv2_large_window12to16_192to256_22kft1k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.swinv2_large_window12to24_192to384_22kft1k": [[745, 754], ["dict", "swin_transformer_v2._create_swin_transformer_v2"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2._create_swin_transformer_v2"], ["", "@", "register_model", "\n", "def", "swinv2_large_window12to24_192to384_22kft1k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "window_size", "=", "24", ",", "embed_dim", "=", "192", ",", "depths", "=", "(", "2", ",", "2", ",", "18", ",", "2", ")", ",", "num_heads", "=", "(", "6", ",", "12", ",", "24", ",", "48", ")", ",", "\n", "pretrained_window_sizes", "=", "(", "12", ",", "12", ",", "12", ",", "6", ")", ",", "**", "kwargs", ")", "\n", "return", "_create_swin_transformer_v2", "(", "\n", "'swinv2_large_window12to24_192to384_22kft1k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet.SelectiveKernelBasic.__init__": [[49, 72], ["torch.nn.Module.__init__", "dict", "layers.SelectiveKernel", "layers.ConvNormAct", "layers.create_attn", "act_layer"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.create_attn"], ["def", "__init__", "(", "\n", "self", ",", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "downsample", "=", "None", ",", "cardinality", "=", "1", ",", "base_width", "=", "64", ",", "\n", "sk_kwargs", "=", "None", ",", "reduce_first", "=", "1", ",", "dilation", "=", "1", ",", "first_dilation", "=", "None", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "\n", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "attn_layer", "=", "None", ",", "aa_layer", "=", "None", ",", "drop_block", "=", "None", ",", "drop_path", "=", "None", ")", ":", "\n", "        ", "super", "(", "SelectiveKernelBasic", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "sk_kwargs", "=", "sk_kwargs", "or", "{", "}", "\n", "conv_kwargs", "=", "dict", "(", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ")", "\n", "assert", "cardinality", "==", "1", ",", "'BasicBlock only supports cardinality of 1'", "\n", "assert", "base_width", "==", "64", ",", "'BasicBlock doest not support changing base width'", "\n", "first_planes", "=", "planes", "//", "reduce_first", "\n", "outplanes", "=", "planes", "*", "self", ".", "expansion", "\n", "first_dilation", "=", "first_dilation", "or", "dilation", "\n", "\n", "self", ".", "conv1", "=", "SelectiveKernel", "(", "\n", "inplanes", ",", "first_planes", ",", "stride", "=", "stride", ",", "dilation", "=", "first_dilation", ",", "\n", "aa_layer", "=", "aa_layer", ",", "drop_layer", "=", "drop_block", ",", "**", "conv_kwargs", ",", "**", "sk_kwargs", ")", "\n", "self", ".", "conv2", "=", "ConvNormAct", "(", "\n", "first_planes", ",", "outplanes", ",", "kernel_size", "=", "3", ",", "dilation", "=", "dilation", ",", "apply_act", "=", "False", ",", "**", "conv_kwargs", ")", "\n", "self", ".", "se", "=", "create_attn", "(", "attn_layer", ",", "outplanes", ")", "\n", "self", ".", "act", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "drop_path", "=", "drop_path", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet.SelectiveKernelBasic.zero_init_last": [[73, 75], ["torch.nn.init.zeros_"], "methods", ["None"], ["", "def", "zero_init_last", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "conv2", ".", "bn", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet.SelectiveKernelBasic.forward": [[76, 89], ["sknet.SelectiveKernelBasic.conv1", "sknet.SelectiveKernelBasic.conv2", "sknet.SelectiveKernelBasic.act", "sknet.SelectiveKernelBasic.se", "sknet.SelectiveKernelBasic.drop_path", "sknet.SelectiveKernelBasic.downsample"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "if", "self", ".", "se", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "se", "(", "x", ")", "\n", "", "if", "self", ".", "drop_path", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "drop_path", "(", "x", ")", "\n", "", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "shortcut", "=", "self", ".", "downsample", "(", "shortcut", ")", "\n", "", "x", "+=", "shortcut", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet.SelectiveKernelBottleneck.__init__": [[94, 116], ["torch.nn.Module.__init__", "dict", "int", "layers.ConvNormAct", "layers.SelectiveKernel", "layers.ConvNormAct", "layers.create_attn", "act_layer", "math.floor"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.create_attn"], ["def", "__init__", "(", "\n", "self", ",", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "downsample", "=", "None", ",", "cardinality", "=", "1", ",", "base_width", "=", "64", ",", "sk_kwargs", "=", "None", ",", "\n", "reduce_first", "=", "1", ",", "dilation", "=", "1", ",", "first_dilation", "=", "None", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "\n", "attn_layer", "=", "None", ",", "aa_layer", "=", "None", ",", "drop_block", "=", "None", ",", "drop_path", "=", "None", ")", ":", "\n", "        ", "super", "(", "SelectiveKernelBottleneck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "sk_kwargs", "=", "sk_kwargs", "or", "{", "}", "\n", "conv_kwargs", "=", "dict", "(", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ")", "\n", "width", "=", "int", "(", "math", ".", "floor", "(", "planes", "*", "(", "base_width", "/", "64", ")", ")", "*", "cardinality", ")", "\n", "first_planes", "=", "width", "//", "reduce_first", "\n", "outplanes", "=", "planes", "*", "self", ".", "expansion", "\n", "first_dilation", "=", "first_dilation", "or", "dilation", "\n", "\n", "self", ".", "conv1", "=", "ConvNormAct", "(", "inplanes", ",", "first_planes", ",", "kernel_size", "=", "1", ",", "**", "conv_kwargs", ")", "\n", "self", ".", "conv2", "=", "SelectiveKernel", "(", "\n", "first_planes", ",", "width", ",", "stride", "=", "stride", ",", "dilation", "=", "first_dilation", ",", "groups", "=", "cardinality", ",", "\n", "aa_layer", "=", "aa_layer", ",", "drop_layer", "=", "drop_block", ",", "**", "conv_kwargs", ",", "**", "sk_kwargs", ")", "\n", "self", ".", "conv3", "=", "ConvNormAct", "(", "width", ",", "outplanes", ",", "kernel_size", "=", "1", ",", "apply_act", "=", "False", ",", "**", "conv_kwargs", ")", "\n", "self", ".", "se", "=", "create_attn", "(", "attn_layer", ",", "outplanes", ")", "\n", "self", ".", "act", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "drop_path", "=", "drop_path", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet.SelectiveKernelBottleneck.zero_init_last": [[117, 119], ["torch.nn.init.zeros_"], "methods", ["None"], ["", "def", "zero_init_last", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "conv3", ".", "bn", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet.SelectiveKernelBottleneck.forward": [[120, 134], ["sknet.SelectiveKernelBottleneck.conv1", "sknet.SelectiveKernelBottleneck.conv2", "sknet.SelectiveKernelBottleneck.conv3", "sknet.SelectiveKernelBottleneck.act", "sknet.SelectiveKernelBottleneck.se", "sknet.SelectiveKernelBottleneck.drop_path", "sknet.SelectiveKernelBottleneck.downsample"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.downsample"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "x", "=", "self", ".", "conv3", "(", "x", ")", "\n", "if", "self", ".", "se", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "se", "(", "x", ")", "\n", "", "if", "self", ".", "drop_path", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "drop_path", "(", "x", ")", "\n", "", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "shortcut", "=", "self", ".", "downsample", "(", "shortcut", ")", "\n", "", "x", "+=", "shortcut", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet._cfg": [[22, 30], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "(", "7", ",", "7", ")", ",", "\n", "'crop_pct'", ":", "0.875", ",", "'interpolation'", ":", "'bicubic'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'conv1'", ",", "'classifier'", ":", "'fc'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet._create_skresnet": [[136, 138], ["helpers.build_model_with_cfg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.build_model_with_cfg"], ["", "", "def", "_create_skresnet", "(", "variant", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "build_model_with_cfg", "(", "ResNet", ",", "variant", ",", "pretrained", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet.skresnet18": [[140, 152], ["dict", "dict", "sknet._create_skresnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet._create_skresnet"], ["", "@", "register_model", "\n", "def", "skresnet18", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a Selective Kernel ResNet-18 model.\n\n    Different from configs in Select Kernel paper or \"Compounding the Performance Improvements...\" this\n    variation splits the input channels to the selective convolutions to keep param count down.\n    \"\"\"", "\n", "sk_kwargs", "=", "dict", "(", "rd_ratio", "=", "1", "/", "8", ",", "rd_divisor", "=", "16", ",", "split_input", "=", "True", ")", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "SelectiveKernelBasic", ",", "layers", "=", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "block_args", "=", "dict", "(", "sk_kwargs", "=", "sk_kwargs", ")", ",", "\n", "zero_init_last", "=", "False", ",", "**", "kwargs", ")", "\n", "return", "_create_skresnet", "(", "'skresnet18'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet.skresnet34": [[154, 166], ["dict", "dict", "sknet._create_skresnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet._create_skresnet"], ["", "@", "register_model", "\n", "def", "skresnet34", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a Selective Kernel ResNet-34 model.\n\n    Different from configs in Select Kernel paper or \"Compounding the Performance Improvements...\" this\n    variation splits the input channels to the selective convolutions to keep param count down.\n    \"\"\"", "\n", "sk_kwargs", "=", "dict", "(", "rd_ratio", "=", "1", "/", "8", ",", "rd_divisor", "=", "16", ",", "split_input", "=", "True", ")", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "SelectiveKernelBasic", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "block_args", "=", "dict", "(", "sk_kwargs", "=", "sk_kwargs", ")", ",", "\n", "zero_init_last", "=", "False", ",", "**", "kwargs", ")", "\n", "return", "_create_skresnet", "(", "'skresnet34'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet.skresnet50": [[168, 180], ["dict", "dict", "sknet._create_skresnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet._create_skresnet"], ["", "@", "register_model", "\n", "def", "skresnet50", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a Select Kernel ResNet-50 model.\n\n    Different from configs in Select Kernel paper or \"Compounding the Performance Improvements...\" this\n    variation splits the input channels to the selective convolutions to keep param count down.\n    \"\"\"", "\n", "sk_kwargs", "=", "dict", "(", "split_input", "=", "True", ")", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "SelectiveKernelBottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "block_args", "=", "dict", "(", "sk_kwargs", "=", "sk_kwargs", ")", ",", "\n", "zero_init_last", "=", "False", ",", "**", "kwargs", ")", "\n", "return", "_create_skresnet", "(", "'skresnet50'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet.skresnet50d": [[182, 194], ["dict", "dict", "sknet._create_skresnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet._create_skresnet"], ["", "@", "register_model", "\n", "def", "skresnet50d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a Select Kernel ResNet-50-D model.\n\n    Different from configs in Select Kernel paper or \"Compounding the Performance Improvements...\" this\n    variation splits the input channels to the selective convolutions to keep param count down.\n    \"\"\"", "\n", "sk_kwargs", "=", "dict", "(", "split_input", "=", "True", ")", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "SelectiveKernelBottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "stem_width", "=", "32", ",", "stem_type", "=", "'deep'", ",", "avg_down", "=", "True", ",", "\n", "block_args", "=", "dict", "(", "sk_kwargs", "=", "sk_kwargs", ")", ",", "zero_init_last", "=", "False", ",", "**", "kwargs", ")", "\n", "return", "_create_skresnet", "(", "'skresnet50d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet.skresnext50_32x4d": [[196, 206], ["dict", "dict", "sknet._create_skresnet", "dict"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.sknet._create_skresnet"], ["", "@", "register_model", "\n", "def", "skresnext50_32x4d", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Constructs a Select Kernel ResNeXt50-32x4d model. This should be equivalent to\n    the SKNet-50 model in the Select Kernel Paper\n    \"\"\"", "\n", "sk_kwargs", "=", "dict", "(", "rd_ratio", "=", "1", "/", "16", ",", "rd_divisor", "=", "32", ",", "split_input", "=", "False", ")", "\n", "model_args", "=", "dict", "(", "\n", "block", "=", "SelectiveKernelBottleneck", ",", "layers", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "cardinality", "=", "32", ",", "base_width", "=", "4", ",", "\n", "block_args", "=", "dict", "(", "sk_kwargs", "=", "sk_kwargs", ")", ",", "zero_init_last", "=", "False", ",", "**", "kwargs", ")", "\n", "return", "_create_skresnet", "(", "'skresnext50_32x4d'", ",", "pretrained", ",", "**", "model_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.trace_utils._float_to_int": [[8, 14], ["int"], "function", ["None"], ["", "", "def", "_float_to_int", "(", "x", ":", "float", ")", "->", "int", ":", "\n", "    ", "\"\"\"\n    Symbolic tracing helper to substitute for inbuilt `int`.\n    Hint: Inbuilt `int` can't accept an argument of type `Proxy`\n    \"\"\"", "\n", "return", "int", "(", "x", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_jit.SwishJit.__init__": [[33, 35], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "SwishJit", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_jit.SwishJit.forward": [[36, 38], ["activations_jit.swish_jit"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_jit.swish_jit"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "swish_jit", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_jit.MishJit.__init__": [[41, 43], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "MishJit", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_jit.MishJit.forward": [[44, 46], ["activations_jit.mish_jit"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_jit.mish_jit"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "mish_jit", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_jit.HardSigmoidJit.__init__": [[55, 57], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "HardSigmoidJit", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_jit.HardSigmoidJit.forward": [[58, 60], ["activations_jit.hard_sigmoid_jit"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_jit.hard_sigmoid_jit"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "hard_sigmoid_jit", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_jit.HardSwishJit.__init__": [[69, 71], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "HardSwishJit", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_jit.HardSwishJit.forward": [[72, 74], ["activations_jit.hard_swish_jit"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_jit.hard_swish_jit"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "hard_swish_jit", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_jit.HardMishJit.__init__": [[86, 88], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "HardMishJit", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_jit.HardMishJit.forward": [[89, 91], ["activations_jit.hard_mish_jit"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_jit.hard_mish_jit"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "hard_mish_jit", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_jit.swish_jit": [[18, 23], ["x.mul", "x.sigmoid"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid"], ["@", "torch", ".", "jit", ".", "script", "\n", "def", "swish_jit", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\"Swish - Described in: https://arxiv.org/abs/1710.05941\n    \"\"\"", "\n", "return", "x", ".", "mul", "(", "x", ".", "sigmoid", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_jit.mish_jit": [[25, 30], ["x.mul", "torch.nn.functional.softplus().tanh", "torch.nn.functional.softplus"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.tanh"], ["", "@", "torch", ".", "jit", ".", "script", "\n", "def", "mish_jit", "(", "x", ",", "_inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n    \"\"\"", "\n", "return", "x", ".", "mul", "(", "F", ".", "softplus", "(", "x", ")", ".", "tanh", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_jit.hard_sigmoid_jit": [[48, 52], ["None"], "function", ["None"], ["", "", "@", "torch", ".", "jit", ".", "script", "\n", "def", "hard_sigmoid_jit", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "# return F.relu6(x + 3.) / 6.", "\n", "    ", "return", "(", "x", "+", "3", ")", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "6", ")", ".", "div", "(", "6.", ")", "# clamp seems ever so slightly faster?", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_jit.hard_swish_jit": [[62, 66], ["None"], "function", ["None"], ["", "", "@", "torch", ".", "jit", ".", "script", "\n", "def", "hard_swish_jit", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "# return x * (F.relu6(x + 3.) / 6)", "\n", "    ", "return", "x", "*", "(", "x", "+", "3", ")", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "6", ")", ".", "div", "(", "6.", ")", "# clamp seems ever so slightly faster?", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_jit.hard_mish_jit": [[76, 83], ["None"], "function", ["None"], ["", "", "@", "torch", ".", "jit", ".", "script", "\n", "def", "hard_mish_jit", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\" Hard Mish\n    Experimental, based on notes by Mish author Diganta Misra at\n      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n    \"\"\"", "\n", "return", "0.5", "*", "x", "*", "(", "x", "+", "2", ")", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.test_time_pool.TestTimePoolHead.__init__": [[17, 30], ["torch.nn.Module.__init__", "test_time_pool.TestTimePoolHead.base.get_classifier", "isinstance", "test_time_pool.TestTimePoolHead.base.reset_classifier", "torch.nn.Conv2d", "test_time_pool.TestTimePoolHead.fc.weight.data.copy_", "test_time_pool.TestTimePoolHead.fc.bias.data.copy_", "test_time_pool.TestTimePoolHead.weight.data.view", "test_time_pool.TestTimePoolHead.bias.data.view", "test_time_pool.TestTimePoolHead.fc.weight.size", "test_time_pool.TestTimePoolHead.fc.bias.size"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.get_classifier", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.reset_classifier"], ["    ", "def", "__init__", "(", "self", ",", "base", ",", "original_pool", "=", "7", ")", ":", "\n", "        ", "super", "(", "TestTimePoolHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "base", "=", "base", "\n", "self", ".", "original_pool", "=", "original_pool", "\n", "base_fc", "=", "self", ".", "base", ".", "get_classifier", "(", ")", "\n", "if", "isinstance", "(", "base_fc", ",", "nn", ".", "Conv2d", ")", ":", "\n", "            ", "self", ".", "fc", "=", "base_fc", "\n", "", "else", ":", "\n", "            ", "self", ".", "fc", "=", "nn", ".", "Conv2d", "(", "\n", "self", ".", "base", ".", "num_features", ",", "self", ".", "base", ".", "num_classes", ",", "kernel_size", "=", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "fc", ".", "weight", ".", "data", ".", "copy_", "(", "base_fc", ".", "weight", ".", "data", ".", "view", "(", "self", ".", "fc", ".", "weight", ".", "size", "(", ")", ")", ")", "\n", "self", ".", "fc", ".", "bias", ".", "data", ".", "copy_", "(", "base_fc", ".", "bias", ".", "data", ".", "view", "(", "self", ".", "fc", ".", "bias", ".", "size", "(", ")", ")", ")", "\n", "", "self", ".", "base", ".", "reset_classifier", "(", "0", ")", "# delete original fc layer", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.test_time_pool.TestTimePoolHead.forward": [[31, 37], ["test_time_pool.TestTimePoolHead.base.forward_features", "torch.avg_pool2d", "test_time_pool.TestTimePoolHead.fc", "adaptive_avgmax_pool.adaptive_avgmax_pool2d", "adaptive_avgmax_pool.adaptive_avgmax_pool2d.view", "adaptive_avgmax_pool.adaptive_avgmax_pool2d.size"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.swin_transformer_v2.SwinTransformerV2.forward_features", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.adaptive_avgmax_pool2d"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "base", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "F", ".", "avg_pool2d", "(", "x", ",", "kernel_size", "=", "self", ".", "original_pool", ",", "stride", "=", "1", ")", "\n", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "x", "=", "adaptive_avgmax_pool2d", "(", "x", ",", "1", ")", "\n", "return", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.test_time_pool.apply_test_time_pool": [[39, 53], ["_logger.info", "test_time_pool.TestTimePoolHead", "hasattr", "str", "str"], "function", ["None"], ["", "", "def", "apply_test_time_pool", "(", "model", ",", "config", ",", "use_test_size", "=", "True", ")", ":", "\n", "    ", "test_time_pool", "=", "False", "\n", "if", "not", "hasattr", "(", "model", ",", "'default_cfg'", ")", "or", "not", "model", ".", "default_cfg", ":", "\n", "        ", "return", "model", ",", "False", "\n", "", "if", "use_test_size", "and", "'test_input_size'", "in", "model", ".", "default_cfg", ":", "\n", "        ", "df_input_size", "=", "model", ".", "default_cfg", "[", "'test_input_size'", "]", "\n", "", "else", ":", "\n", "        ", "df_input_size", "=", "model", ".", "default_cfg", "[", "'input_size'", "]", "\n", "", "if", "config", "[", "'input_size'", "]", "[", "-", "1", "]", ">", "df_input_size", "[", "-", "1", "]", "and", "config", "[", "'input_size'", "]", "[", "-", "2", "]", ">", "df_input_size", "[", "-", "2", "]", ":", "\n", "        ", "_logger", ".", "info", "(", "'Target input size %s > pretrained default %s, using test time pooling'", "%", "\n", "(", "str", "(", "config", "[", "'input_size'", "]", "[", "-", "2", ":", "]", ")", ",", "str", "(", "df_input_size", "[", "-", "2", ":", "]", ")", ")", ")", "\n", "model", "=", "TestTimePoolHead", "(", "model", ",", "original_pool", "=", "model", ".", "default_cfg", "[", "'pool_size'", "]", ")", "\n", "test_time_pool", "=", "True", "\n", "", "return", "model", ",", "test_time_pool", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.conv2d_same.Conv2dSame.__init__": [[24, 28], ["torch.Conv2d.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "dilation", "=", "1", ",", "groups", "=", "1", ",", "bias", "=", "True", ")", ":", "\n", "        ", "super", "(", "Conv2dSame", ",", "self", ")", ".", "__init__", "(", "\n", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", ",", "0", ",", "dilation", ",", "groups", ",", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.conv2d_same.Conv2dSame.forward": [[29, 31], ["conv2d_same.conv2d_same"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.conv2d_same.conv2d_same"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "conv2d_same", "(", "x", ",", "self", ".", "weight", ",", "self", ".", "bias", ",", "self", ".", "stride", ",", "self", ".", "padding", ",", "self", ".", "dilation", ",", "self", ".", "groups", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.conv2d_same.conv2d_same": [[13, 18], ["padding.pad_same", "torch.conv2d"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.pad_same"], ["def", "conv2d_same", "(", "\n", "x", ",", "weight", ":", "torch", ".", "Tensor", ",", "bias", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "stride", ":", "Tuple", "[", "int", ",", "int", "]", "=", "(", "1", ",", "1", ")", ",", "\n", "padding", ":", "Tuple", "[", "int", ",", "int", "]", "=", "(", "0", ",", "0", ")", ",", "dilation", ":", "Tuple", "[", "int", ",", "int", "]", "=", "(", "1", ",", "1", ")", ",", "groups", ":", "int", "=", "1", ")", ":", "\n", "    ", "x", "=", "pad_same", "(", "x", ",", "weight", ".", "shape", "[", "-", "2", ":", "]", ",", "stride", ",", "dilation", ")", "\n", "return", "F", ".", "conv2d", "(", "x", ",", "weight", ",", "bias", ",", "stride", ",", "(", "0", ",", "0", ")", ",", "dilation", ",", "groups", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.conv2d_same.create_conv2d_pad": [[33, 41], ["kwargs.pop", "kwargs.setdefault", "padding.get_padding_value", "conv2d_same.Conv2dSame", "torch.Conv2d"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.get_padding_value"], ["", "", "def", "create_conv2d_pad", "(", "in_chs", ",", "out_chs", ",", "kernel_size", ",", "**", "kwargs", ")", ":", "\n", "    ", "padding", "=", "kwargs", ".", "pop", "(", "'padding'", ",", "''", ")", "\n", "kwargs", ".", "setdefault", "(", "'bias'", ",", "False", ")", "\n", "padding", ",", "is_dynamic", "=", "get_padding_value", "(", "padding", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "if", "is_dynamic", ":", "\n", "        ", "return", "Conv2dSame", "(", "in_chs", ",", "out_chs", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "        ", "return", "nn", ".", "Conv2d", "(", "in_chs", ",", "out_chs", ",", "kernel_size", ",", "padding", "=", "padding", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.std_conv.StdConv2d.__init__": [[32, 41], ["torch.Conv2d.__init__", "padding.get_padding.get_padding"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.get_padding"], ["def", "__init__", "(", "\n", "self", ",", "in_channel", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "padding", "=", "None", ",", "\n", "dilation", "=", "1", ",", "groups", "=", "1", ",", "bias", "=", "False", ",", "eps", "=", "1e-6", ")", ":", "\n", "        ", "if", "padding", "is", "None", ":", "\n", "            ", "padding", "=", "get_padding", "(", "kernel_size", ",", "stride", ",", "dilation", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "\n", "in_channel", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "stride", ",", "\n", "padding", "=", "padding", ",", "dilation", "=", "dilation", ",", "groups", "=", "groups", ",", "bias", "=", "bias", ")", "\n", "self", ".", "eps", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.std_conv.StdConv2d.forward": [[42, 48], ["torch.batch_norm().reshape_as", "torch.batch_norm().reshape_as", "torch.batch_norm().reshape_as", "torch.conv2d", "torch.conv2d", "torch.conv2d", "torch.batch_norm", "torch.batch_norm", "torch.batch_norm", "std_conv.StdConv2d.weight.reshape"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "weight", "=", "F", ".", "batch_norm", "(", "\n", "self", ".", "weight", ".", "reshape", "(", "1", ",", "self", ".", "out_channels", ",", "-", "1", ")", ",", "None", ",", "None", ",", "\n", "training", "=", "True", ",", "momentum", "=", "0.", ",", "eps", "=", "self", ".", "eps", ")", ".", "reshape_as", "(", "self", ".", "weight", ")", "\n", "x", "=", "F", ".", "conv2d", "(", "x", ",", "weight", ",", "self", ".", "bias", ",", "self", ".", "stride", ",", "self", ".", "padding", ",", "self", ".", "dilation", ",", "self", ".", "groups", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.std_conv.StdConv2dSame.__init__": [[56, 65], ["padding.get_padding_value", "torch.Conv2d.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.get_padding_value", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "in_channel", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "padding", "=", "'SAME'", ",", "\n", "dilation", "=", "1", ",", "groups", "=", "1", ",", "bias", "=", "False", ",", "eps", "=", "1e-6", ")", ":", "\n", "        ", "padding", ",", "is_dynamic", "=", "get_padding_value", "(", "padding", ",", "kernel_size", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ")", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "in_channel", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "dilation", "=", "dilation", ",", "\n", "groups", "=", "groups", ",", "bias", "=", "bias", ")", "\n", "self", ".", "same_pad", "=", "is_dynamic", "\n", "self", ".", "eps", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.std_conv.StdConv2dSame.forward": [[66, 74], ["torch.batch_norm().reshape_as", "torch.batch_norm().reshape_as", "torch.batch_norm().reshape_as", "torch.conv2d", "torch.conv2d", "torch.conv2d", "padding.pad_same", "torch.batch_norm", "torch.batch_norm", "torch.batch_norm", "std_conv.StdConv2dSame.weight.reshape"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.pad_same"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "same_pad", ":", "\n", "            ", "x", "=", "pad_same", "(", "x", ",", "self", ".", "kernel_size", ",", "self", ".", "stride", ",", "self", ".", "dilation", ")", "\n", "", "weight", "=", "F", ".", "batch_norm", "(", "\n", "self", ".", "weight", ".", "reshape", "(", "1", ",", "self", ".", "out_channels", ",", "-", "1", ")", ",", "None", ",", "None", ",", "\n", "training", "=", "True", ",", "momentum", "=", "0.", ",", "eps", "=", "self", ".", "eps", ")", ".", "reshape_as", "(", "self", ".", "weight", ")", "\n", "x", "=", "F", ".", "conv2d", "(", "x", ",", "weight", ",", "self", ".", "bias", ",", "self", ".", "stride", ",", "self", ".", "padding", ",", "self", ".", "dilation", ",", "self", ".", "groups", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.std_conv.ScaledStdConv2d.__init__": [[85, 96], ["torch.Conv2d.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "padding.get_padding.get_padding", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "std_conv.ScaledStdConv2d.weight[].numel"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.get_padding"], ["def", "__init__", "(", "\n", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "padding", "=", "None", ",", "\n", "dilation", "=", "1", ",", "groups", "=", "1", ",", "bias", "=", "True", ",", "gamma", "=", "1.0", ",", "eps", "=", "1e-6", ",", "gain_init", "=", "1.0", ")", ":", "\n", "        ", "if", "padding", "is", "None", ":", "\n", "            ", "padding", "=", "get_padding", "(", "kernel_size", ",", "stride", ",", "dilation", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "\n", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "dilation", "=", "dilation", ",", "\n", "groups", "=", "groups", ",", "bias", "=", "bias", ")", "\n", "self", ".", "gain", "=", "nn", ".", "Parameter", "(", "torch", ".", "full", "(", "(", "self", ".", "out_channels", ",", "1", ",", "1", ",", "1", ")", ",", "gain_init", ")", ")", "\n", "self", ".", "scale", "=", "gamma", "*", "self", ".", "weight", "[", "0", "]", ".", "numel", "(", ")", "**", "-", "0.5", "# gamma * 1 / sqrt(fan-in)", "\n", "self", ".", "eps", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.std_conv.ScaledStdConv2d.forward": [[97, 103], ["torch.batch_norm().reshape_as", "torch.batch_norm().reshape_as", "torch.batch_norm().reshape_as", "torch.conv2d", "torch.conv2d", "torch.conv2d", "torch.batch_norm", "torch.batch_norm", "torch.batch_norm", "std_conv.ScaledStdConv2d.weight.reshape"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "weight", "=", "F", ".", "batch_norm", "(", "\n", "self", ".", "weight", ".", "reshape", "(", "1", ",", "self", ".", "out_channels", ",", "-", "1", ")", ",", "None", ",", "None", ",", "\n", "weight", "=", "(", "self", ".", "gain", "*", "self", ".", "scale", ")", ".", "view", "(", "-", "1", ")", ",", "\n", "training", "=", "True", ",", "momentum", "=", "0.", ",", "eps", "=", "self", ".", "eps", ")", ".", "reshape_as", "(", "self", ".", "weight", ")", "\n", "return", "F", ".", "conv2d", "(", "x", ",", "weight", ",", "self", ".", "bias", ",", "self", ".", "stride", ",", "self", ".", "padding", ",", "self", ".", "dilation", ",", "self", ".", "groups", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.std_conv.ScaledStdConv2dSame.__init__": [[114, 125], ["padding.get_padding_value", "torch.Conv2d.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "std_conv.ScaledStdConv2dSame.weight[].numel"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.get_padding_value", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "padding", "=", "'SAME'", ",", "\n", "dilation", "=", "1", ",", "groups", "=", "1", ",", "bias", "=", "True", ",", "gamma", "=", "1.0", ",", "eps", "=", "1e-6", ",", "gain_init", "=", "1.0", ")", ":", "\n", "        ", "padding", ",", "is_dynamic", "=", "get_padding_value", "(", "padding", ",", "kernel_size", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ")", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "dilation", "=", "dilation", ",", "\n", "groups", "=", "groups", ",", "bias", "=", "bias", ")", "\n", "self", ".", "gain", "=", "nn", ".", "Parameter", "(", "torch", ".", "full", "(", "(", "self", ".", "out_channels", ",", "1", ",", "1", ",", "1", ")", ",", "gain_init", ")", ")", "\n", "self", ".", "scale", "=", "gamma", "*", "self", ".", "weight", "[", "0", "]", ".", "numel", "(", ")", "**", "-", "0.5", "\n", "self", ".", "same_pad", "=", "is_dynamic", "\n", "self", ".", "eps", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.std_conv.ScaledStdConv2dSame.forward": [[126, 134], ["torch.batch_norm().reshape_as", "torch.batch_norm().reshape_as", "torch.batch_norm().reshape_as", "torch.conv2d", "torch.conv2d", "torch.conv2d", "padding.pad_same", "torch.batch_norm", "torch.batch_norm", "torch.batch_norm", "std_conv.ScaledStdConv2dSame.weight.reshape"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.pad_same"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "same_pad", ":", "\n", "            ", "x", "=", "pad_same", "(", "x", ",", "self", ".", "kernel_size", ",", "self", ".", "stride", ",", "self", ".", "dilation", ")", "\n", "", "weight", "=", "F", ".", "batch_norm", "(", "\n", "self", ".", "weight", ".", "reshape", "(", "1", ",", "self", ".", "out_channels", ",", "-", "1", ")", ",", "None", ",", "None", ",", "\n", "weight", "=", "(", "self", ".", "gain", "*", "self", ".", "scale", ")", ".", "view", "(", "-", "1", ")", ",", "\n", "training", "=", "True", ",", "momentum", "=", "0.", ",", "eps", "=", "self", ".", "eps", ")", ".", "reshape_as", "(", "self", ".", "weight", ")", "\n", "return", "F", ".", "conv2d", "(", "x", ",", "weight", ",", "self", ".", "bias", ",", "self", ".", "stride", ",", "self", ".", "padding", ",", "self", ".", "dilation", ",", "self", ".", "groups", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d": [[11, 37], ["isinstance", "mixed_conv2d.MixedConv2d", "kwargs.pop", "kwargs.pop", "kwargs.pop", "cond_conv2d.CondConv2d", "conv2d_same.create_conv2d_pad"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.conv2d_same.create_conv2d_pad"], ["def", "create_conv2d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Select a 2d convolution implementation based on arguments\n    Creates and returns one of torch.nn.Conv2d, Conv2dSame, MixedConv2d, or CondConv2d.\n\n    Used extensively by EfficientNet, MobileNetv3 and related networks.\n    \"\"\"", "\n", "if", "isinstance", "(", "kernel_size", ",", "list", ")", ":", "\n", "        ", "assert", "'num_experts'", "not", "in", "kwargs", "# MixNet + CondConv combo not supported currently", "\n", "if", "'groups'", "in", "kwargs", ":", "\n", "            ", "groups", "=", "kwargs", ".", "pop", "(", "'groups'", ")", "\n", "if", "groups", "==", "in_channels", ":", "\n", "                ", "kwargs", "[", "'depthwise'", "]", "=", "True", "\n", "", "else", ":", "\n", "                ", "assert", "groups", "==", "1", "\n", "# We're going to use only lists for defining the MixedConv2d kernel groups,", "\n", "# ints, tuples, other iterables will continue to pass to normal conv and specify h, w.", "\n", "", "", "m", "=", "MixedConv2d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "        ", "depthwise", "=", "kwargs", ".", "pop", "(", "'depthwise'", ",", "False", ")", "\n", "# for DW out_channels must be multiple of in_channels as must have out_channels % groups == 0", "\n", "groups", "=", "in_channels", "if", "depthwise", "else", "kwargs", ".", "pop", "(", "'groups'", ",", "1", ")", "\n", "if", "'num_experts'", "in", "kwargs", "and", "kwargs", "[", "'num_experts'", "]", ">", "0", ":", "\n", "            ", "m", "=", "CondConv2d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "groups", "=", "groups", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "            ", "m", "=", "create_conv2d_pad", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "groups", "=", "groups", ",", "**", "kwargs", ")", "\n", "", "", "return", "m", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.FourierEmbed.__init__": [[114, 121], ["torch.nn.Module.__init__", "pos_embed.FourierEmbed.register_buffer", "pos_embed.pixel_freq_bands"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.pixel_freq_bands"], ["    ", "def", "__init__", "(", "self", ",", "max_res", ":", "int", "=", "224", ",", "num_bands", ":", "int", "=", "64", ",", "concat_grid", "=", "True", ",", "keep_spatial", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "max_res", "=", "max_res", "\n", "self", ".", "num_bands", "=", "num_bands", "\n", "self", ".", "concat_grid", "=", "concat_grid", "\n", "self", ".", "keep_spatial", "=", "keep_spatial", "\n", "self", ".", "register_buffer", "(", "'bands'", ",", "pixel_freq_bands", "(", "max_res", ",", "num_bands", ")", ",", "persistent", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.FourierEmbed.forward": [[122, 142], ["pos_embed.build_fourier_pos_embed", "emb.transpose().flatten.transpose().flatten.transpose().flatten", "len", "torch.cat", "torch.cat", "x.reshape.reshape.reshape", "emb.transpose().flatten.transpose().flatten.transpose", "feat_shape.numel", "emb.transpose().flatten.transpose().flatten.unsqueeze().expand().permute", "x.reshape.reshape.permute", "emb.transpose().flatten.transpose().flatten.unsqueeze().expand", "emb.transpose().flatten.transpose().flatten.unsqueeze().expand", "emb.transpose().flatten.transpose().flatten.unsqueeze", "emb.transpose().flatten.transpose().flatten.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.build_fourier_pos_embed"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "C", "=", "x", ".", "shape", "[", ":", "2", "]", "\n", "feat_shape", "=", "x", ".", "shape", "[", "2", ":", "]", "\n", "emb", "=", "build_fourier_pos_embed", "(", "\n", "feat_shape", ",", "\n", "self", ".", "bands", ",", "\n", "include_grid", "=", "self", ".", "concat_grid", ",", "\n", "dtype", "=", "x", ".", "dtype", ",", "\n", "device", "=", "x", ".", "device", ")", "\n", "emb", "=", "emb", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ".", "flatten", "(", "len", "(", "feat_shape", ")", ")", "\n", "batch_expand", "=", "(", "B", ",", ")", "+", "(", "-", "1", ",", ")", "*", "(", "x", ".", "ndim", "-", "1", ")", "\n", "\n", "# FIXME support nD", "\n", "if", "self", ".", "keep_spatial", ":", "\n", "            ", "x", "=", "torch", ".", "cat", "(", "[", "x", ",", "emb", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "batch_expand", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "torch", ".", "cat", "(", "[", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ",", "emb", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "batch_expand", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "x", "=", "x", ".", "reshape", "(", "B", ",", "feat_shape", ".", "numel", "(", ")", ",", "-", "1", ")", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.RotaryEmbedding.__init__": [[196, 200], ["torch.nn.Module.__init__", "pos_embed.RotaryEmbedding.register_buffer", "pos_embed.pixel_freq_bands"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.pixel_freq_bands"], ["def", "__init__", "(", "self", ",", "dim", ",", "max_res", "=", "224", ",", "linear_bands", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "register_buffer", "(", "'bands'", ",", "pixel_freq_bands", "(", "dim", "//", "4", ",", "max_res", ",", "linear_bands", "=", "linear_bands", ")", ",", "persistent", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.RotaryEmbedding.get_embed": [[201, 203], ["pos_embed.build_rotary_pos_embed"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.build_rotary_pos_embed"], ["", "def", "get_embed", "(", "self", ",", "shape", ":", "List", "[", "int", "]", ")", ":", "\n", "        ", "return", "build_rotary_pos_embed", "(", "shape", ",", "self", ".", "bands", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.RotaryEmbedding.forward": [[204, 208], ["pos_embed.RotaryEmbedding.get_embed", "pos_embed.apply_rot_embed"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.RotaryEmbedding.get_embed", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.apply_rot_embed"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# assuming channel-first tensor where spatial dim are >= 2", "\n", "        ", "sin_emb", ",", "cos_emb", "=", "self", ".", "get_embed", "(", "x", ".", "shape", "[", "2", ":", "]", ")", "\n", "return", "apply_rot_embed", "(", "x", ",", "sin_emb", ",", "cos_emb", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.pixel_freq_bands": [[8, 20], ["torch.linspace", "torch.linspace", "math.log"], "function", ["None"], ["def", "pixel_freq_bands", "(", "\n", "num_bands", ":", "int", ",", "\n", "max_freq", ":", "float", "=", "224.", ",", "\n", "linear_bands", ":", "bool", "=", "True", ",", "\n", "dtype", ":", "torch", ".", "dtype", "=", "torch", ".", "float32", ",", "\n", "device", ":", "Optional", "[", "torch", ".", "device", "]", "=", "None", ",", "\n", ")", ":", "\n", "    ", "if", "linear_bands", ":", "\n", "        ", "bands", "=", "torch", ".", "linspace", "(", "1.0", ",", "max_freq", "/", "2", ",", "num_bands", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", "\n", "", "else", ":", "\n", "        ", "bands", "=", "2", "**", "torch", ".", "linspace", "(", "0", ",", "math", ".", "log", "(", "max_freq", ",", "2", ")", "-", "1", ",", "num_bands", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", "\n", "", "return", "bands", "*", "torch", ".", "pi", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.inv_freq_bands": [[22, 31], ["torch.arange"], "function", ["None"], ["", "def", "inv_freq_bands", "(", "\n", "num_bands", ":", "int", ",", "\n", "temperature", ":", "float", "=", "100000.", ",", "\n", "step", ":", "int", "=", "2", ",", "\n", "dtype", ":", "torch", ".", "dtype", "=", "torch", ".", "float32", ",", "\n", "device", ":", "Optional", "[", "torch", ".", "device", "]", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "inv_freq", "=", "1.", "/", "(", "temperature", "**", "(", "torch", ".", "arange", "(", "0", ",", "num_bands", ",", "step", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", "/", "num_bands", ")", ")", "\n", "return", "inv_freq", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.build_sincos2d_pos_embed": [[33, 70], ["pos_embed.inv_freq_bands", "torch.stack().flatten().transpose", "torch.stack().flatten", "torch.stack().flatten().transpose.unsqueeze", "inv_freq_bands.unsqueeze", "torch.stack().flatten", "torch.stack", "torch.stack", "torch.sin", "torch.cos", "torch.meshgrid", "torch.arange"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.inv_freq_bands"], ["", "def", "build_sincos2d_pos_embed", "(", "\n", "feat_shape", ":", "List", "[", "int", "]", ",", "\n", "dim", ":", "int", "=", "64", ",", "\n", "temperature", ":", "float", "=", "10000.", ",", "\n", "reverse_coord", ":", "bool", "=", "False", ",", "\n", "interleave_sin_cos", ":", "bool", "=", "False", ",", "\n", "dtype", ":", "torch", ".", "dtype", "=", "torch", ".", "float32", ",", "\n", "device", ":", "Optional", "[", "torch", ".", "device", "]", "=", "None", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n\n    Args:\n        feat_shape:\n        dim:\n        temperature:\n        reverse_coord: stack grid order W, H instead of H, W\n        interleave_sin_cos: sin, cos, sin, cos stack instead of sin, sin, cos, cos\n        dtype:\n        device:\n\n    Returns:\n\n    \"\"\"", "\n", "assert", "dim", "%", "4", "==", "0", ",", "'Embed dimension must be divisible by 4 for sin-cos 2D position embedding'", "\n", "pos_dim", "=", "dim", "//", "4", "\n", "bands", "=", "inv_freq_bands", "(", "pos_dim", ",", "temperature", "=", "temperature", ",", "step", "=", "1", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", "\n", "\n", "if", "reverse_coord", ":", "\n", "        ", "feat_shape", "=", "feat_shape", "[", ":", ":", "-", "1", "]", "# stack W, H instead of H, W", "\n", "", "grid", "=", "torch", ".", "stack", "(", "\n", "torch", ".", "meshgrid", "(", "[", "torch", ".", "arange", "(", "s", ",", "device", "=", "device", ",", "dtype", "=", "dtype", ")", "for", "s", "in", "feat_shape", "]", ")", ")", ".", "flatten", "(", "1", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "pos2", "=", "grid", ".", "unsqueeze", "(", "-", "1", ")", "*", "bands", ".", "unsqueeze", "(", "0", ")", "\n", "# FIXME add support for unflattened spatial dim?", "\n", "\n", "stack_dim", "=", "2", "if", "interleave_sin_cos", "else", "1", "# stack sin, cos, sin, cos  instead of sin sin cos cos", "\n", "pos_emb", "=", "torch", ".", "stack", "(", "[", "torch", ".", "sin", "(", "pos2", ")", ",", "torch", ".", "cos", "(", "pos2", ")", "]", ",", "dim", "=", "stack_dim", ")", ".", "flatten", "(", "1", ")", "\n", "return", "pos_emb", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.build_fourier_pos_embed": [[72, 110], ["torch.stack.unsqueeze", "torch.stack", "torch.stack", "pos.sin", "pos.cos", "torch.cat", "pos_embed.pixel_freq_bands", "pos_embed.inv_freq_bands", "torch.meshgrid", "torch.meshgrid", "float", "torch.linspace", "torch.arange"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.pixel_freq_bands", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.inv_freq_bands"], ["", "def", "build_fourier_pos_embed", "(", "\n", "feat_shape", ":", "List", "[", "int", "]", ",", "\n", "bands", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "num_bands", ":", "int", "=", "64", ",", "\n", "max_res", ":", "int", "=", "224", ",", "\n", "linear_bands", ":", "bool", "=", "False", ",", "\n", "include_grid", ":", "bool", "=", "False", ",", "\n", "concat_out", ":", "bool", "=", "True", ",", "\n", "in_pixels", ":", "bool", "=", "True", ",", "\n", "dtype", ":", "torch", ".", "dtype", "=", "torch", ".", "float32", ",", "\n", "device", ":", "Optional", "[", "torch", ".", "device", "]", "=", "None", ",", "\n", ")", "->", "List", "[", "torch", ".", "Tensor", "]", ":", "\n", "    ", "if", "bands", "is", "None", ":", "\n", "        ", "if", "in_pixels", ":", "\n", "            ", "bands", "=", "pixel_freq_bands", "(", "num_bands", ",", "float", "(", "max_res", ")", ",", "linear_bands", "=", "linear_bands", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", "\n", "", "else", ":", "\n", "            ", "bands", "=", "inv_freq_bands", "(", "num_bands", ",", "step", "=", "1", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "device", "is", "None", ":", "\n", "            ", "device", "=", "bands", ".", "device", "\n", "", "if", "dtype", "is", "None", ":", "\n", "            ", "dtype", "=", "bands", ".", "dtype", "\n", "\n", "", "", "if", "in_pixels", ":", "\n", "        ", "grid", "=", "torch", ".", "stack", "(", "torch", ".", "meshgrid", "(", "\n", "[", "torch", ".", "linspace", "(", "-", "1.", ",", "1.", ",", "steps", "=", "s", ",", "device", "=", "device", ",", "dtype", "=", "dtype", ")", "for", "s", "in", "feat_shape", "]", ")", ",", "dim", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "        ", "grid", "=", "torch", ".", "stack", "(", "torch", ".", "meshgrid", "(", "\n", "[", "torch", ".", "arange", "(", "s", ",", "device", "=", "device", ",", "dtype", "=", "dtype", ")", "for", "s", "in", "feat_shape", "]", ")", ",", "dim", "=", "-", "1", ")", "\n", "", "grid", "=", "grid", ".", "unsqueeze", "(", "-", "1", ")", "\n", "pos", "=", "grid", "*", "bands", "\n", "\n", "pos_sin", ",", "pos_cos", "=", "pos", ".", "sin", "(", ")", ",", "pos", ".", "cos", "(", ")", "\n", "out", "=", "(", "grid", ",", "pos_sin", ",", "pos_cos", ")", "if", "include_grid", "else", "(", "pos_sin", ",", "pos_cos", ")", "\n", "# FIXME torchscript doesn't like multiple return types, probably need to always cat?", "\n", "if", "concat_out", ":", "\n", "        ", "out", "=", "torch", ".", "cat", "(", "out", ",", "dim", "=", "-", "1", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.rot": [[144, 146], ["torch.stack().reshape", "torch.stack"], "function", ["None"], ["", "", "def", "rot", "(", "x", ")", ":", "\n", "    ", "return", "torch", ".", "stack", "(", "[", "-", "x", "[", "...", ",", "1", ":", ":", "2", "]", ",", "x", "[", "...", ",", ":", ":", "2", "]", "]", ",", "-", "1", ")", ".", "reshape", "(", "x", ".", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.apply_rot_embed": [[148, 150], ["pos_embed.rot"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.rot"], ["", "def", "apply_rot_embed", "(", "x", ":", "torch", ".", "Tensor", ",", "sin_emb", ",", "cos_emb", ")", ":", "\n", "    ", "return", "x", "*", "cos_emb", "+", "rot", "(", "x", ")", "*", "sin_emb", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.apply_rot_embed_list": [[152, 156], ["isinstance", "pos_embed.rot"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.rot"], ["", "def", "apply_rot_embed_list", "(", "x", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "sin_emb", ",", "cos_emb", ")", ":", "\n", "    ", "if", "isinstance", "(", "x", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "x", "=", "[", "x", "]", "\n", "", "return", "[", "t", "*", "cos_emb", "+", "rot", "(", "t", ")", "*", "sin_emb", "for", "t", "in", "x", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.apply_rot_embed_split": [[158, 161], ["pos_embed.rot"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.rot"], ["", "def", "apply_rot_embed_split", "(", "x", ":", "torch", ".", "Tensor", ",", "emb", ")", ":", "\n", "    ", "split", "=", "emb", ".", "shape", "[", "-", "1", "]", "//", "2", "\n", "return", "x", "*", "emb", "[", ":", ",", ":", "split", "]", "+", "rot", "(", "x", ")", "*", "emb", "[", ":", ",", "split", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.build_rotary_pos_embed": [[163, 184], ["torch.Size", "pos_embed.build_fourier_pos_embed", "torch.Size.numel", "sin_emb.reshape().repeat_interleave.reshape().repeat_interleave", "cos_emb.reshape().repeat_interleave.reshape().repeat_interleave", "sin_emb.reshape().repeat_interleave.reshape", "cos_emb.reshape().repeat_interleave.reshape"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.build_fourier_pos_embed"], ["", "def", "build_rotary_pos_embed", "(", "\n", "feat_shape", ":", "List", "[", "int", "]", ",", "\n", "bands", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "dim", ":", "int", "=", "64", ",", "\n", "max_freq", ":", "float", "=", "224", ",", "\n", "linear_bands", ":", "bool", "=", "False", ",", "\n", "dtype", ":", "torch", ".", "dtype", "=", "torch", ".", "float32", ",", "\n", "device", ":", "Optional", "[", "torch", ".", "device", "]", "=", "None", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    NOTE: shape arg should include spatial dim only\n    \"\"\"", "\n", "feat_shape", "=", "torch", ".", "Size", "(", "feat_shape", ")", "\n", "\n", "sin_emb", ",", "cos_emb", "=", "build_fourier_pos_embed", "(", "\n", "feat_shape", ",", "bands", "=", "bands", ",", "num_bands", "=", "dim", "//", "4", ",", "max_res", "=", "max_freq", ",", "linear_bands", "=", "linear_bands", ",", "\n", "concat_out", "=", "False", ",", "device", "=", "device", ",", "dtype", "=", "dtype", ")", "\n", "N", "=", "feat_shape", ".", "numel", "(", ")", "\n", "sin_emb", "=", "sin_emb", ".", "reshape", "(", "N", ",", "-", "1", ")", ".", "repeat_interleave", "(", "2", ",", "-", "1", ")", "\n", "cos_emb", "=", "cos_emb", ".", "reshape", "(", "N", ",", "-", "1", ")", ".", "repeat_interleave", "(", "2", ",", "-", "1", ")", "\n", "return", "sin_emb", ",", "cos_emb", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.bottleneck_attn.PosEmbedRel.__init__": [[61, 67], ["torch.Module.__init__", "helpers.to_2tuple", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "feat_size", ",", "dim_head", ",", "scale", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "height", ",", "self", ".", "width", "=", "to_2tuple", "(", "feat_size", ")", "\n", "self", ".", "dim_head", "=", "dim_head", "\n", "self", ".", "height_rel", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "self", ".", "height", "*", "2", "-", "1", ",", "dim_head", ")", "*", "scale", ")", "\n", "self", ".", "width_rel", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "self", ".", "width", "*", "2", "-", "1", ",", "dim_head", ")", "*", "scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.bottleneck_attn.PosEmbedRel.forward": [[68, 82], ["q.transpose.transpose.reshape", "bottleneck_attn.rel_logits_1d", "q.transpose.transpose.transpose", "bottleneck_attn.rel_logits_1d", "rel_logits.reshape.reshape.reshape"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.halo_attn.rel_logits_1d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.halo_attn.rel_logits_1d"], ["", "def", "forward", "(", "self", ",", "q", ")", ":", "\n", "        ", "B", ",", "HW", ",", "_", "=", "q", ".", "shape", "\n", "\n", "# relative logits in width dimension.", "\n", "q", "=", "q", ".", "reshape", "(", "B", ",", "self", ".", "height", ",", "self", ".", "width", ",", "-", "1", ")", "\n", "rel_logits_w", "=", "rel_logits_1d", "(", "q", ",", "self", ".", "width_rel", ",", "permute_mask", "=", "(", "0", ",", "1", ",", "3", ",", "2", ",", "4", ")", ")", "\n", "\n", "# relative logits in height dimension.", "\n", "q", "=", "q", ".", "transpose", "(", "1", ",", "2", ")", "\n", "rel_logits_h", "=", "rel_logits_1d", "(", "q", ",", "self", ".", "height_rel", ",", "permute_mask", "=", "(", "0", ",", "3", ",", "1", ",", "4", ",", "2", ")", ")", "\n", "\n", "rel_logits", "=", "rel_logits_h", "+", "rel_logits_w", "\n", "rel_logits", "=", "rel_logits", ".", "reshape", "(", "B", ",", "HW", ",", "HW", ")", "\n", "return", "rel_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.bottleneck_attn.BottleneckAttn.__init__": [[106, 129], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "bottleneck_attn.PosEmbedRel", "bottleneck_attn.BottleneckAttn.reset_parameters", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.Identity", "torch.Identity", "torch.Identity", "helpers.make_divisible"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible"], ["def", "__init__", "(", "\n", "self", ",", "dim", ",", "dim_out", "=", "None", ",", "feat_size", "=", "None", ",", "stride", "=", "1", ",", "num_heads", "=", "4", ",", "dim_head", "=", "None", ",", "\n", "qk_ratio", "=", "1.0", ",", "qkv_bias", "=", "False", ",", "scale_pos_embed", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "feat_size", "is", "not", "None", ",", "'A concrete feature size matching expected input (H, W) is required'", "\n", "dim_out", "=", "dim_out", "or", "dim", "\n", "assert", "dim_out", "%", "num_heads", "==", "0", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "dim_head_qk", "=", "dim_head", "or", "make_divisible", "(", "dim_out", "*", "qk_ratio", ",", "divisor", "=", "8", ")", "//", "num_heads", "\n", "self", ".", "dim_head_v", "=", "dim_out", "//", "self", ".", "num_heads", "\n", "self", ".", "dim_out_qk", "=", "num_heads", "*", "self", ".", "dim_head_qk", "\n", "self", ".", "dim_out_v", "=", "num_heads", "*", "self", ".", "dim_head_v", "\n", "self", ".", "scale", "=", "self", ".", "dim_head_qk", "**", "-", "0.5", "\n", "self", ".", "scale_pos_embed", "=", "scale_pos_embed", "\n", "\n", "self", ".", "qkv", "=", "nn", ".", "Conv2d", "(", "dim", ",", "self", ".", "dim_out_qk", "*", "2", "+", "self", ".", "dim_out_v", ",", "1", ",", "bias", "=", "qkv_bias", ")", "\n", "\n", "# NOTE I'm only supporting relative pos embedding for now", "\n", "self", ".", "pos_embed", "=", "PosEmbedRel", "(", "feat_size", ",", "dim_head", "=", "self", ".", "dim_head_qk", ",", "scale", "=", "self", ".", "scale", ")", "\n", "\n", "self", ".", "pool", "=", "nn", ".", "AvgPool2d", "(", "2", ",", "2", ")", "if", "stride", "==", "2", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.bottleneck_attn.BottleneckAttn.reset_parameters": [[130, 134], ["weight_init.trunc_normal_", "weight_init.trunc_normal_", "weight_init.trunc_normal_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "trunc_normal_", "(", "self", ".", "qkv", ".", "weight", ",", "std", "=", "self", ".", "qkv", ".", "weight", ".", "shape", "[", "1", "]", "**", "-", "0.5", ")", "# fan-in", "\n", "trunc_normal_", "(", "self", ".", "pos_embed", ".", "height_rel", ",", "std", "=", "self", ".", "scale", ")", "\n", "trunc_normal_", "(", "self", ".", "pos_embed", ".", "width_rel", ",", "std", "=", "self", ".", "scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.bottleneck_attn.BottleneckAttn.forward": [[135, 158], ["trace_utils._assert", "trace_utils._assert", "bottleneck_attn.BottleneckAttn.qkv", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "q.reshape().transpose.reshape().transpose.reshape().transpose", "k.reshape.reshape.reshape", "v.reshape().transpose.reshape().transpose.reshape().transpose", "attn.softmax.softmax.softmax", "bottleneck_attn.BottleneckAttn.pool", "q.reshape().transpose.reshape().transpose.reshape", "v.reshape().transpose.reshape().transpose.reshape", "bottleneck_attn.BottleneckAttn.pos_embed", "bottleneck_attn.BottleneckAttn.pos_embed"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "_assert", "(", "H", "==", "self", ".", "pos_embed", ".", "height", ",", "''", ")", "\n", "_assert", "(", "W", "==", "self", ".", "pos_embed", ".", "width", ",", "''", ")", "\n", "\n", "x", "=", "self", ".", "qkv", "(", "x", ")", "# B, (2 * dim_head_qk + dim_head_v) * num_heads, H, W", "\n", "\n", "# NOTE head vs channel split ordering in qkv projection was decided before I allowed qk to differ from v", "\n", "# So, this is more verbose than if heads were before qkv splits, but throughput is not impacted.", "\n", "q", ",", "k", ",", "v", "=", "torch", ".", "split", "(", "x", ",", "[", "self", ".", "dim_out_qk", ",", "self", ".", "dim_out_qk", ",", "self", ".", "dim_out_v", "]", ",", "dim", "=", "1", ")", "\n", "q", "=", "q", ".", "reshape", "(", "B", "*", "self", ".", "num_heads", ",", "self", ".", "dim_head_qk", ",", "-", "1", ")", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "k", "=", "k", ".", "reshape", "(", "B", "*", "self", ".", "num_heads", ",", "self", ".", "dim_head_qk", ",", "-", "1", ")", "# no transpose, for q @ k", "\n", "v", "=", "v", ".", "reshape", "(", "B", "*", "self", ".", "num_heads", ",", "self", ".", "dim_head_v", ",", "-", "1", ")", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "\n", "if", "self", ".", "scale_pos_embed", ":", "\n", "            ", "attn", "=", "(", "q", "@", "k", "+", "self", ".", "pos_embed", "(", "q", ")", ")", "*", "self", ".", "scale", "# B * num_heads, H * W, H * W", "\n", "", "else", ":", "\n", "            ", "attn", "=", "(", "q", "@", "k", ")", "*", "self", ".", "scale", "+", "self", ".", "pos_embed", "(", "q", ")", "\n", "", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "out", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ".", "reshape", "(", "B", ",", "self", ".", "dim_out_v", ",", "H", ",", "W", ")", "# B, dim_out, H, W", "\n", "out", "=", "self", ".", "pool", "(", "out", ")", "\n", "return", "out", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.bottleneck_attn.rel_logits_1d": [[28, 54], ["x.reshape().expand.reshape", "torch.pad().flatten", "torch.pad", "x_pad.reshape.reshape", "x.reshape().expand.reshape().expand", "x.reshape().expand.permute", "rel_k.transpose", "torch.pad", "x.reshape().expand.reshape"], "function", ["None"], ["def", "rel_logits_1d", "(", "q", ",", "rel_k", ",", "permute_mask", ":", "List", "[", "int", "]", ")", ":", "\n", "    ", "\"\"\" Compute relative logits along one dimension\n\n    As per: https://gist.github.com/aravindsrinivas/56359b79f0ce4449bcb04ab4b56a57a2\n    Originally from: `Attention Augmented Convolutional Networks` - https://arxiv.org/abs/1904.09925\n\n    Args:\n        q: (batch, heads, height, width, dim)\n        rel_k: (2 * width - 1, dim)\n        permute_mask: permute output dim according to this\n    \"\"\"", "\n", "B", ",", "H", ",", "W", ",", "dim", "=", "q", ".", "shape", "\n", "x", "=", "(", "q", "@", "rel_k", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "x", "=", "x", ".", "reshape", "(", "-", "1", ",", "W", ",", "2", "*", "W", "-", "1", ")", "\n", "\n", "# pad to shift from relative to absolute indexing", "\n", "x_pad", "=", "F", ".", "pad", "(", "x", ",", "[", "0", ",", "1", "]", ")", ".", "flatten", "(", "1", ")", "\n", "x_pad", "=", "F", ".", "pad", "(", "x_pad", ",", "[", "0", ",", "W", "-", "1", "]", ")", "\n", "\n", "# reshape and slice out the padded elements", "\n", "x_pad", "=", "x_pad", ".", "reshape", "(", "-", "1", ",", "W", "+", "1", ",", "2", "*", "W", "-", "1", ")", "\n", "x", "=", "x_pad", "[", ":", ",", ":", "W", ",", "W", "-", "1", ":", "]", "\n", "\n", "# reshape and tile", "\n", "x", "=", "x", ".", "reshape", "(", "B", ",", "H", ",", "1", ",", "W", ",", "W", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "H", ",", "-", "1", ",", "-", "1", ")", "\n", "return", "x", ".", "permute", "(", "permute_mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.blur_pool.BlurPool2d.__init__": [[29, 39], ["torch.Module.__init__", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "[].repeat", "blur_pool.BlurPool2d.register_buffer", "padding.get_padding", "numpy.poly1d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.get_padding"], ["def", "__init__", "(", "self", ",", "channels", ",", "filt_size", "=", "3", ",", "stride", "=", "2", ")", "->", "None", ":", "\n", "        ", "super", "(", "BlurPool2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "filt_size", ">", "1", "\n", "self", ".", "channels", "=", "channels", "\n", "self", ".", "filt_size", "=", "filt_size", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "padding", "=", "[", "get_padding", "(", "filt_size", ",", "stride", ",", "dilation", "=", "1", ")", "]", "*", "4", "\n", "coeffs", "=", "torch", ".", "tensor", "(", "(", "np", ".", "poly1d", "(", "(", "0.5", ",", "0.5", ")", ")", "**", "(", "self", ".", "filt_size", "-", "1", ")", ")", ".", "coeffs", ".", "astype", "(", "np", ".", "float32", ")", ")", "\n", "blur_filter", "=", "(", "coeffs", "[", ":", ",", "None", "]", "*", "coeffs", "[", "None", ",", ":", "]", ")", "[", "None", ",", "None", ",", ":", ",", ":", "]", ".", "repeat", "(", "self", ".", "channels", ",", "1", ",", "1", ",", "1", ")", "\n", "self", ".", "register_buffer", "(", "'filt'", ",", "blur_filter", ",", "persistent", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.blur_pool.BlurPool2d.forward": [[40, 43], ["torch.pad", "torch.pad", "torch.pad", "torch.conv2d", "torch.conv2d", "torch.conv2d"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "x", "=", "F", ".", "pad", "(", "x", ",", "self", ".", "padding", ",", "'reflect'", ")", "\n", "return", "F", ".", "conv2d", "(", "x", ",", "self", ".", "filt", ",", "stride", "=", "self", ".", "stride", ",", "groups", "=", "self", ".", "channels", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.set_no_jit.__init__": [[30, 34], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "mode", ":", "bool", ")", "->", "None", ":", "\n", "        ", "global", "_NO_JIT", "\n", "self", ".", "prev", "=", "_NO_JIT", "\n", "_NO_JIT", "=", "mode", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.set_no_jit.__enter__": [[35, 37], ["None"], "methods", ["None"], ["", "def", "__enter__", "(", "self", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.set_no_jit.__exit__": [[38, 42], ["None"], "methods", ["None"], ["", "def", "__exit__", "(", "self", ",", "*", "args", ":", "Any", ")", "->", "bool", ":", "\n", "        ", "global", "_NO_JIT", "\n", "_NO_JIT", "=", "self", ".", "prev", "\n", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.set_exportable.__init__": [[49, 53], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "mode", ":", "bool", ")", "->", "None", ":", "\n", "        ", "global", "_EXPORTABLE", "\n", "self", ".", "prev", "=", "_EXPORTABLE", "\n", "_EXPORTABLE", "=", "mode", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.set_exportable.__enter__": [[54, 56], ["None"], "methods", ["None"], ["", "def", "__enter__", "(", "self", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.set_exportable.__exit__": [[57, 61], ["None"], "methods", ["None"], ["", "def", "__exit__", "(", "self", ",", "*", "args", ":", "Any", ")", "->", "bool", ":", "\n", "        ", "global", "_EXPORTABLE", "\n", "_EXPORTABLE", "=", "self", ".", "prev", "\n", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.set_scriptable.__init__": [[68, 72], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "mode", ":", "bool", ")", "->", "None", ":", "\n", "        ", "global", "_SCRIPTABLE", "\n", "self", ".", "prev", "=", "_SCRIPTABLE", "\n", "_SCRIPTABLE", "=", "mode", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.set_scriptable.__enter__": [[73, 75], ["None"], "methods", ["None"], ["", "def", "__enter__", "(", "self", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.set_scriptable.__exit__": [[76, 80], ["None"], "methods", ["None"], ["", "def", "__exit__", "(", "self", ",", "*", "args", ":", "Any", ")", "->", "bool", ":", "\n", "        ", "global", "_SCRIPTABLE", "\n", "_SCRIPTABLE", "=", "self", ".", "prev", "\n", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.set_layer_config.__init__": [[86, 105], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "scriptable", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "exportable", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "no_jit", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "no_activation_jit", ":", "Optional", "[", "bool", "]", "=", "None", ")", ":", "\n", "        ", "global", "_SCRIPTABLE", "\n", "global", "_EXPORTABLE", "\n", "global", "_NO_JIT", "\n", "global", "_NO_ACTIVATION_JIT", "\n", "self", ".", "prev", "=", "_SCRIPTABLE", ",", "_EXPORTABLE", ",", "_NO_JIT", ",", "_NO_ACTIVATION_JIT", "\n", "if", "scriptable", "is", "not", "None", ":", "\n", "            ", "_SCRIPTABLE", "=", "scriptable", "\n", "", "if", "exportable", "is", "not", "None", ":", "\n", "            ", "_EXPORTABLE", "=", "exportable", "\n", "", "if", "no_jit", "is", "not", "None", ":", "\n", "            ", "_NO_JIT", "=", "no_jit", "\n", "", "if", "no_activation_jit", "is", "not", "None", ":", "\n", "            ", "_NO_ACTIVATION_JIT", "=", "no_activation_jit", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.set_layer_config.__enter__": [[106, 108], ["None"], "methods", ["None"], ["", "", "def", "__enter__", "(", "self", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.set_layer_config.__exit__": [[109, 116], ["None"], "methods", ["None"], ["", "def", "__exit__", "(", "self", ",", "*", "args", ":", "Any", ")", "->", "bool", ":", "\n", "        ", "global", "_SCRIPTABLE", "\n", "global", "_EXPORTABLE", "\n", "global", "_NO_JIT", "\n", "global", "_NO_ACTIVATION_JIT", "\n", "_SCRIPTABLE", ",", "_EXPORTABLE", ",", "_NO_JIT", ",", "_NO_ACTIVATION_JIT", "=", "self", ".", "prev", "\n", "return", "False", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.is_no_jit": [[25, 27], ["None"], "function", ["None"], ["def", "is_no_jit", "(", ")", ":", "\n", "    ", "return", "_NO_JIT", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.is_exportable": [[44, 46], ["None"], "function", ["None"], ["", "", "def", "is_exportable", "(", ")", ":", "\n", "    ", "return", "_EXPORTABLE", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.is_scriptable": [[63, 65], ["None"], "function", ["None"], ["", "", "def", "is_scriptable", "(", ")", ":", "\n", "    ", "return", "_SCRIPTABLE", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.cbam.ChannelAttn.__init__": [[22, 32], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "act_layer", "torch.nn.Conv2d", "torch.nn.Conv2d", "create_act.create_act_layer", "helpers.make_divisible"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.create_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible"], ["def", "__init__", "(", "\n", "self", ",", "channels", ",", "rd_ratio", "=", "1.", "/", "16", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "1", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "gate_layer", "=", "'sigmoid'", ",", "mlp_bias", "=", "False", ")", ":", "\n", "        ", "super", "(", "ChannelAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "not", "rd_channels", ":", "\n", "            ", "rd_channels", "=", "make_divisible", "(", "channels", "*", "rd_ratio", ",", "rd_divisor", ",", "round_limit", "=", "0.", ")", "\n", "", "self", ".", "fc1", "=", "nn", ".", "Conv2d", "(", "channels", ",", "rd_channels", ",", "1", ",", "bias", "=", "mlp_bias", ")", "\n", "self", ".", "act", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Conv2d", "(", "rd_channels", ",", "channels", ",", "1", ",", "bias", "=", "mlp_bias", ")", "\n", "self", ".", "gate", "=", "create_act_layer", "(", "gate_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.cbam.ChannelAttn.forward": [[33, 37], ["cbam.ChannelAttn.fc2", "cbam.ChannelAttn.fc2", "cbam.ChannelAttn.act", "cbam.ChannelAttn.act", "cbam.ChannelAttn.gate", "cbam.ChannelAttn.fc1", "cbam.ChannelAttn.fc1", "x.mean", "x.amax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_avg", "=", "self", ".", "fc2", "(", "self", ".", "act", "(", "self", ".", "fc1", "(", "x", ".", "mean", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", ")", ")", ")", "\n", "x_max", "=", "self", ".", "fc2", "(", "self", ".", "act", "(", "self", ".", "fc1", "(", "x", ".", "amax", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", ")", ")", ")", "\n", "return", "x", "*", "self", ".", "gate", "(", "x_avg", "+", "x_max", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.cbam.LightChannelAttn.__init__": [[42, 47], ["cbam.ChannelAttn.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "channels", ",", "rd_ratio", "=", "1.", "/", "16", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "1", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "gate_layer", "=", "'sigmoid'", ",", "mlp_bias", "=", "False", ")", ":", "\n", "        ", "super", "(", "LightChannelAttn", ",", "self", ")", ".", "__init__", "(", "\n", "channels", ",", "rd_ratio", ",", "rd_channels", ",", "rd_divisor", ",", "act_layer", ",", "gate_layer", ",", "mlp_bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.cbam.LightChannelAttn.forward": [[48, 52], ["cbam.LightChannelAttn.fc2", "cbam.LightChannelAttn.act", "torch.sigmoid", "torch.sigmoid", "x.mean", "x.amax", "cbam.LightChannelAttn.fc1"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_pool", "=", "0.5", "*", "x", ".", "mean", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "+", "0.5", "*", "x", ".", "amax", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "x_attn", "=", "self", ".", "fc2", "(", "self", ".", "act", "(", "self", ".", "fc1", "(", "x_pool", ")", ")", ")", "\n", "return", "x", "*", "F", ".", "sigmoid", "(", "x_attn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.cbam.SpatialAttn.__init__": [[57, 61], ["torch.nn.Module.__init__", "conv_bn_act.ConvNormAct", "create_act.create_act_layer"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.create_act_layer"], ["def", "__init__", "(", "self", ",", "kernel_size", "=", "7", ",", "gate_layer", "=", "'sigmoid'", ")", ":", "\n", "        ", "super", "(", "SpatialAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv", "=", "ConvNormAct", "(", "2", ",", "1", ",", "kernel_size", ",", "apply_act", "=", "False", ")", "\n", "self", ".", "gate", "=", "create_act_layer", "(", "gate_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.cbam.SpatialAttn.forward": [[62, 66], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "cbam.SpatialAttn.conv", "cbam.SpatialAttn.gate", "x.mean", "x.amax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_attn", "=", "torch", ".", "cat", "(", "[", "x", ".", "mean", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", ",", "x", ".", "amax", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "]", ",", "dim", "=", "1", ")", "\n", "x_attn", "=", "self", ".", "conv", "(", "x_attn", ")", "\n", "return", "x", "*", "self", ".", "gate", "(", "x_attn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.cbam.LightSpatialAttn.__init__": [[71, 75], ["torch.nn.Module.__init__", "conv_bn_act.ConvNormAct", "create_act.create_act_layer"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.create_act_layer"], ["def", "__init__", "(", "self", ",", "kernel_size", "=", "7", ",", "gate_layer", "=", "'sigmoid'", ")", ":", "\n", "        ", "super", "(", "LightSpatialAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv", "=", "ConvNormAct", "(", "1", ",", "1", ",", "kernel_size", ",", "apply_act", "=", "False", ")", "\n", "self", ".", "gate", "=", "create_act_layer", "(", "gate_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.cbam.LightSpatialAttn.forward": [[76, 80], ["cbam.LightSpatialAttn.conv", "cbam.LightSpatialAttn.gate", "x.mean", "x.amax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_attn", "=", "0.5", "*", "x", ".", "mean", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "+", "0.5", "*", "x", ".", "amax", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "x_attn", "=", "self", ".", "conv", "(", "x_attn", ")", "\n", "return", "x", "*", "self", ".", "gate", "(", "x_attn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.cbam.CbamModule.__init__": [[83, 91], ["torch.nn.Module.__init__", "cbam.ChannelAttn", "cbam.SpatialAttn"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "channels", ",", "rd_ratio", "=", "1.", "/", "16", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "1", ",", "\n", "spatial_kernel_size", "=", "7", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "gate_layer", "=", "'sigmoid'", ",", "mlp_bias", "=", "False", ")", ":", "\n", "        ", "super", "(", "CbamModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "channel", "=", "ChannelAttn", "(", "\n", "channels", ",", "rd_ratio", "=", "rd_ratio", ",", "rd_channels", "=", "rd_channels", ",", "\n", "rd_divisor", "=", "rd_divisor", ",", "act_layer", "=", "act_layer", ",", "gate_layer", "=", "gate_layer", ",", "mlp_bias", "=", "mlp_bias", ")", "\n", "self", ".", "spatial", "=", "SpatialAttn", "(", "spatial_kernel_size", ",", "gate_layer", "=", "gate_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.cbam.CbamModule.forward": [[92, 96], ["cbam.CbamModule.channel", "cbam.CbamModule.spatial"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "channel", "(", "x", ")", "\n", "x", "=", "self", ".", "spatial", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.cbam.LightCbamModule.__init__": [[99, 107], ["torch.nn.Module.__init__", "cbam.LightChannelAttn", "cbam.LightSpatialAttn"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "channels", ",", "rd_ratio", "=", "1.", "/", "16", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "1", ",", "\n", "spatial_kernel_size", "=", "7", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "gate_layer", "=", "'sigmoid'", ",", "mlp_bias", "=", "False", ")", ":", "\n", "        ", "super", "(", "LightCbamModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "channel", "=", "LightChannelAttn", "(", "\n", "channels", ",", "rd_ratio", "=", "rd_ratio", ",", "rd_channels", "=", "rd_channels", ",", "\n", "rd_divisor", "=", "rd_divisor", ",", "act_layer", "=", "act_layer", ",", "gate_layer", "=", "gate_layer", ",", "mlp_bias", "=", "mlp_bias", ")", "\n", "self", ".", "spatial", "=", "LightSpatialAttn", "(", "spatial_kernel_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.cbam.LightCbamModule.forward": [[108, 112], ["cbam.LightCbamModule.channel", "cbam.LightCbamModule.spatial"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "channel", "(", "x", ")", "\n", "x", "=", "self", ".", "spatial", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.eca.EcaModule.__init__": [[60, 83], ["torch.nn.Module.__init__", "create_act.create_act_layer", "int", "max", "torch.nn.Conv1d", "create_act.create_act_layer", "torch.nn.Conv1d", "torch.nn.Conv1d", "helpers.make_divisible", "abs", "math.log"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.create_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.create_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible"], ["def", "__init__", "(", "\n", "self", ",", "channels", "=", "None", ",", "kernel_size", "=", "3", ",", "gamma", "=", "2", ",", "beta", "=", "1", ",", "act_layer", "=", "None", ",", "gate_layer", "=", "'sigmoid'", ",", "\n", "rd_ratio", "=", "1", "/", "8", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "8", ",", "use_mlp", "=", "False", ")", ":", "\n", "        ", "super", "(", "EcaModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "channels", "is", "not", "None", ":", "\n", "            ", "t", "=", "int", "(", "abs", "(", "math", ".", "log", "(", "channels", ",", "2", ")", "+", "beta", ")", "/", "gamma", ")", "\n", "kernel_size", "=", "max", "(", "t", "if", "t", "%", "2", "else", "t", "+", "1", ",", "3", ")", "\n", "", "assert", "kernel_size", "%", "2", "==", "1", "\n", "padding", "=", "(", "kernel_size", "-", "1", ")", "//", "2", "\n", "if", "use_mlp", ":", "\n", "# NOTE 'mlp' mode is a timm experiment, not in paper", "\n", "            ", "assert", "channels", "is", "not", "None", "\n", "if", "rd_channels", "is", "None", ":", "\n", "                ", "rd_channels", "=", "make_divisible", "(", "channels", "*", "rd_ratio", ",", "divisor", "=", "rd_divisor", ")", "\n", "", "act_layer", "=", "act_layer", "or", "nn", ".", "ReLU", "\n", "self", ".", "conv", "=", "nn", ".", "Conv1d", "(", "1", ",", "rd_channels", ",", "kernel_size", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "True", ")", "\n", "self", ".", "act", "=", "create_act_layer", "(", "act_layer", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv1d", "(", "rd_channels", ",", "1", ",", "kernel_size", "=", "kernel_size", ",", "padding", "=", "padding", ",", "bias", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv", "=", "nn", ".", "Conv1d", "(", "1", ",", "1", ",", "kernel_size", "=", "kernel_size", ",", "padding", "=", "padding", ",", "bias", "=", "False", ")", "\n", "self", ".", "act", "=", "None", "\n", "self", ".", "conv2", "=", "None", "\n", "", "self", ".", "gate", "=", "create_act_layer", "(", "gate_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.eca.EcaModule.forward": [[84, 92], ["x.mean().view", "eca.EcaModule.conv", "eca.EcaModule.gate().view", "eca.EcaModule.act", "eca.EcaModule.conv2", "eca.EcaModule.expand_as", "x.mean", "eca.EcaModule.gate"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "y", "=", "x", ".", "mean", "(", "(", "2", ",", "3", ")", ")", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "1", ",", "-", "1", ")", "# view for 1d conv", "\n", "y", "=", "self", ".", "conv", "(", "y", ")", "\n", "if", "self", ".", "conv2", "is", "not", "None", ":", "\n", "            ", "y", "=", "self", ".", "act", "(", "y", ")", "\n", "y", "=", "self", ".", "conv2", "(", "y", ")", "\n", "", "y", "=", "self", ".", "gate", "(", "y", ")", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "return", "x", "*", "y", ".", "expand_as", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.eca.CecaModule.__init__": [[121, 135], ["torch.nn.Module.__init__", "torch.nn.Conv1d", "create_act.create_act_layer", "int", "max", "abs", "math.log"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.create_act_layer"], ["def", "__init__", "(", "self", ",", "channels", "=", "None", ",", "kernel_size", "=", "3", ",", "gamma", "=", "2", ",", "beta", "=", "1", ",", "act_layer", "=", "None", ",", "gate_layer", "=", "'sigmoid'", ")", ":", "\n", "        ", "super", "(", "CecaModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "channels", "is", "not", "None", ":", "\n", "            ", "t", "=", "int", "(", "abs", "(", "math", ".", "log", "(", "channels", ",", "2", ")", "+", "beta", ")", "/", "gamma", ")", "\n", "kernel_size", "=", "max", "(", "t", "if", "t", "%", "2", "else", "t", "+", "1", ",", "3", ")", "\n", "", "has_act", "=", "act_layer", "is", "not", "None", "\n", "assert", "kernel_size", "%", "2", "==", "1", "\n", "\n", "# PyTorch circular padding mode is buggy as of pytorch 1.4", "\n", "# see https://github.com/pytorch/pytorch/pull/17240", "\n", "# implement manual circular padding", "\n", "self", ".", "padding", "=", "(", "kernel_size", "-", "1", ")", "//", "2", "\n", "self", ".", "conv", "=", "nn", ".", "Conv1d", "(", "1", ",", "1", ",", "kernel_size", "=", "kernel_size", ",", "padding", "=", "0", ",", "bias", "=", "has_act", ")", "\n", "self", ".", "gate", "=", "create_act_layer", "(", "gate_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.eca.CecaModule.forward": [[136, 143], ["x.mean().view", "torch.pad", "eca.CecaModule.conv", "eca.CecaModule.gate().view", "eca.CecaModule.expand_as", "x.mean", "eca.CecaModule.gate"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "y", "=", "x", ".", "mean", "(", "(", "2", ",", "3", ")", ")", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "1", ",", "-", "1", ")", "\n", "# Manually implement circular padding, F.pad does not seemed to be bugged", "\n", "y", "=", "F", ".", "pad", "(", "y", ",", "(", "self", ".", "padding", ",", "self", ".", "padding", ")", ",", "mode", "=", "'circular'", ")", "\n", "y", "=", "self", ".", "conv", "(", "y", ")", "\n", "y", "=", "self", ".", "gate", "(", "y", ")", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "return", "x", "*", "y", ".", "expand_as", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.global_context.GlobalContext.__init__": [[21, 42], ["torch.nn.Module.__init__", "create_act.get_act_layer", "create_act.create_act_layer", "global_context.GlobalContext.reset_parameters", "torch.nn.Conv2d", "helpers.make_divisible", "mlp.ConvMlp", "mlp.ConvMlp"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.create_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible"], ["    ", "def", "__init__", "(", "self", ",", "channels", ",", "use_attn", "=", "True", ",", "fuse_add", "=", "False", ",", "fuse_scale", "=", "True", ",", "init_last_zero", "=", "False", ",", "\n", "rd_ratio", "=", "1.", "/", "8", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "1", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "gate_layer", "=", "'sigmoid'", ")", ":", "\n", "        ", "super", "(", "GlobalContext", ",", "self", ")", ".", "__init__", "(", ")", "\n", "act_layer", "=", "get_act_layer", "(", "act_layer", ")", "\n", "\n", "self", ".", "conv_attn", "=", "nn", ".", "Conv2d", "(", "channels", ",", "1", ",", "kernel_size", "=", "1", ",", "bias", "=", "True", ")", "if", "use_attn", "else", "None", "\n", "\n", "if", "rd_channels", "is", "None", ":", "\n", "            ", "rd_channels", "=", "make_divisible", "(", "channels", "*", "rd_ratio", ",", "rd_divisor", ",", "round_limit", "=", "0.", ")", "\n", "", "if", "fuse_add", ":", "\n", "            ", "self", ".", "mlp_add", "=", "ConvMlp", "(", "channels", ",", "rd_channels", ",", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "LayerNorm2d", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "mlp_add", "=", "None", "\n", "", "if", "fuse_scale", ":", "\n", "            ", "self", ".", "mlp_scale", "=", "ConvMlp", "(", "channels", ",", "rd_channels", ",", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "LayerNorm2d", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "mlp_scale", "=", "None", "\n", "\n", "", "self", ".", "gate", "=", "create_act_layer", "(", "gate_layer", ")", "\n", "self", ".", "init_last_zero", "=", "init_last_zero", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.global_context.GlobalContext.reset_parameters": [[43, 48], ["torch.nn.init.kaiming_normal_", "torch.nn.init.zeros_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "conv_attn", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "kaiming_normal_", "(", "self", ".", "conv_attn", ".", "weight", ",", "mode", "=", "'fan_in'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "", "if", "self", ".", "mlp_add", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "mlp_add", ".", "fc2", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.global_context.GlobalContext.forward": [[49, 68], ["global_context.GlobalContext.conv_attn().reshape", "torch.softmax().unsqueeze", "x.mean.view", "x.mean", "global_context.GlobalContext.mlp_scale", "global_context.GlobalContext.mlp_add", "x.reshape().unsqueeze", "global_context.GlobalContext.gate", "global_context.GlobalContext.conv_attn", "torch.softmax", "x.reshape"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "\n", "if", "self", ".", "conv_attn", "is", "not", "None", ":", "\n", "            ", "attn", "=", "self", ".", "conv_attn", "(", "x", ")", ".", "reshape", "(", "B", ",", "1", ",", "H", "*", "W", ")", "# (B, 1, H * W)", "\n", "attn", "=", "F", ".", "softmax", "(", "attn", ",", "dim", "=", "-", "1", ")", ".", "unsqueeze", "(", "3", ")", "# (B, 1, H * W, 1)", "\n", "context", "=", "x", ".", "reshape", "(", "B", ",", "C", ",", "H", "*", "W", ")", ".", "unsqueeze", "(", "1", ")", "@", "attn", "\n", "context", "=", "context", ".", "view", "(", "B", ",", "C", ",", "1", ",", "1", ")", "\n", "", "else", ":", "\n", "            ", "context", "=", "x", ".", "mean", "(", "dim", "=", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "\n", "", "if", "self", ".", "mlp_scale", "is", "not", "None", ":", "\n", "            ", "mlp_x", "=", "self", ".", "mlp_scale", "(", "context", ")", "\n", "x", "=", "x", "*", "self", ".", "gate", "(", "mlp_x", ")", "\n", "", "if", "self", ".", "mlp_add", "is", "not", "None", ":", "\n", "            ", "mlp_x", "=", "self", ".", "mlp_add", "(", "context", ")", "\n", "x", "=", "x", "+", "mlp_x", "\n", "\n", "", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.median_pool.MedianPool2d.__init__": [[18, 24], ["torch.Module.__init__", "helpers.to_2tuple", "helpers.to_2tuple", "helpers.to_4tuple"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "same", "=", "False", ")", ":", "\n", "        ", "super", "(", "MedianPool2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "k", "=", "to_2tuple", "(", "kernel_size", ")", "\n", "self", ".", "stride", "=", "to_2tuple", "(", "stride", ")", "\n", "self", ".", "padding", "=", "to_4tuple", "(", "padding", ")", "# convert to l, r, t, b", "\n", "self", ".", "same", "=", "same", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.median_pool.MedianPool2d._padding": [[25, 44], ["x.size", "max", "max", "max", "max"], "methods", ["None"], ["", "def", "_padding", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "same", ":", "\n", "            ", "ih", ",", "iw", "=", "x", ".", "size", "(", ")", "[", "2", ":", "]", "\n", "if", "ih", "%", "self", ".", "stride", "[", "0", "]", "==", "0", ":", "\n", "                ", "ph", "=", "max", "(", "self", ".", "k", "[", "0", "]", "-", "self", ".", "stride", "[", "0", "]", ",", "0", ")", "\n", "", "else", ":", "\n", "                ", "ph", "=", "max", "(", "self", ".", "k", "[", "0", "]", "-", "(", "ih", "%", "self", ".", "stride", "[", "0", "]", ")", ",", "0", ")", "\n", "", "if", "iw", "%", "self", ".", "stride", "[", "1", "]", "==", "0", ":", "\n", "                ", "pw", "=", "max", "(", "self", ".", "k", "[", "1", "]", "-", "self", ".", "stride", "[", "1", "]", ",", "0", ")", "\n", "", "else", ":", "\n", "                ", "pw", "=", "max", "(", "self", ".", "k", "[", "1", "]", "-", "(", "iw", "%", "self", ".", "stride", "[", "1", "]", ")", ",", "0", ")", "\n", "", "pl", "=", "pw", "//", "2", "\n", "pr", "=", "pw", "-", "pl", "\n", "pt", "=", "ph", "//", "2", "\n", "pb", "=", "ph", "-", "pt", "\n", "padding", "=", "(", "pl", ",", "pr", ",", "pt", ",", "pb", ")", "\n", "", "else", ":", "\n", "            ", "padding", "=", "self", ".", "padding", "\n", "", "return", "padding", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.median_pool.MedianPool2d.forward": [[45, 50], ["torch.pad", "torch.pad", "x.unfold().unfold.unfold().unfold.unfold().unfold", "median_pool.MedianPool2d._padding", "x.unfold().unfold.unfold().unfold.contiguous().view().median", "x.unfold().unfold.unfold().unfold.unfold", "x.unfold().unfold.unfold().unfold.contiguous().view", "x.unfold().unfold.unfold().unfold.contiguous", "x.unfold().unfold.unfold().unfold.size"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.median_pool.MedianPool2d._padding"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "F", ".", "pad", "(", "x", ",", "self", ".", "_padding", "(", "x", ")", ",", "mode", "=", "'reflect'", ")", "\n", "x", "=", "x", ".", "unfold", "(", "2", ",", "self", ".", "k", "[", "0", "]", ",", "self", ".", "stride", "[", "0", "]", ")", ".", "unfold", "(", "3", ",", "self", ".", "k", "[", "1", "]", ",", "self", ".", "stride", "[", "1", "]", ")", "\n", "x", "=", "x", ".", "contiguous", "(", ")", ".", "view", "(", "x", ".", "size", "(", ")", "[", ":", "4", "]", "+", "(", "-", "1", ",", ")", ")", ".", "median", "(", "dim", "=", "-", "1", ")", "[", "0", "]", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.split_batchnorm.SplitBatchNorm2d.__init__": [[20, 27], ["super().__init__", "torch.ModuleList", "torch.ModuleList", "torch.BatchNorm2d", "torch.BatchNorm2d", "range"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_features", ",", "eps", "=", "1e-5", ",", "momentum", "=", "0.1", ",", "affine", "=", "True", ",", "\n", "track_running_stats", "=", "True", ",", "num_splits", "=", "2", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_features", ",", "eps", ",", "momentum", ",", "affine", ",", "track_running_stats", ")", "\n", "assert", "num_splits", ">", "1", ",", "'Should have at least one aux BN layer (num_splits at least 2)'", "\n", "self", ".", "num_splits", "=", "num_splits", "\n", "self", ".", "aux_bn", "=", "nn", ".", "ModuleList", "(", "[", "\n", "nn", ".", "BatchNorm2d", "(", "num_features", ",", "eps", ",", "momentum", ",", "affine", ",", "track_running_stats", ")", "for", "_", "in", "range", "(", "num_splits", "-", "1", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.split_batchnorm.SplitBatchNorm2d.forward": [[28, 39], ["input.split", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "super().forward", "super().forward", "x.append", "a"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.forward", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.forward"], ["", "def", "forward", "(", "self", ",", "input", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "if", "self", ".", "training", ":", "# aux BN only relevant while training", "\n", "            ", "split_size", "=", "input", ".", "shape", "[", "0", "]", "//", "self", ".", "num_splits", "\n", "assert", "input", ".", "shape", "[", "0", "]", "==", "split_size", "*", "self", ".", "num_splits", ",", "\"batch size must be evenly divisible by num_splits\"", "\n", "split_input", "=", "input", ".", "split", "(", "split_size", ")", "\n", "x", "=", "[", "super", "(", ")", ".", "forward", "(", "split_input", "[", "0", "]", ")", "]", "\n", "for", "i", ",", "a", "in", "enumerate", "(", "self", ".", "aux_bn", ")", ":", "\n", "                ", "x", ".", "append", "(", "a", "(", "split_input", "[", "i", "+", "1", "]", ")", ")", "\n", "", "return", "torch", ".", "cat", "(", "x", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "return", "super", "(", ")", ".", "forward", "(", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.split_batchnorm.convert_splitbn_model": [[41, 76], ["isinstance", "isinstance", "module.named_children", "split_batchnorm.SplitBatchNorm2d", "SplitBatchNorm2d.add_module", "module.weight.data.clone().detach", "module.bias.data.clone().detach", "module.running_mean.clone", "module.running_var.clone", "module.num_batches_tracked.clone", "split_batchnorm.convert_splitbn_model", "module.weight.data.clone().detach", "module.bias.data.clone().detach", "module.weight.data.clone", "module.bias.data.clone", "module.weight.data.clone", "module.bias.data.clone"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.split_batchnorm.convert_splitbn_model"], ["", "", "", "def", "convert_splitbn_model", "(", "module", ",", "num_splits", "=", "2", ")", ":", "\n", "    ", "\"\"\"\n    Recursively traverse module and its children to replace all instances of\n    ``torch.nn.modules.batchnorm._BatchNorm`` with `SplitBatchnorm2d`.\n    Args:\n        module (torch.nn.Module): input module\n        num_splits: number of separate batchnorm layers to split input across\n    Example::\n        >>> # model is an instance of torch.nn.Module\n        >>> model = timm.models.convert_splitbn_model(model, num_splits=2)\n    \"\"\"", "\n", "mod", "=", "module", "\n", "if", "isinstance", "(", "module", ",", "torch", ".", "nn", ".", "modules", ".", "instancenorm", ".", "_InstanceNorm", ")", ":", "\n", "        ", "return", "module", "\n", "", "if", "isinstance", "(", "module", ",", "torch", ".", "nn", ".", "modules", ".", "batchnorm", ".", "_BatchNorm", ")", ":", "\n", "        ", "mod", "=", "SplitBatchNorm2d", "(", "\n", "module", ".", "num_features", ",", "module", ".", "eps", ",", "module", ".", "momentum", ",", "module", ".", "affine", ",", "\n", "module", ".", "track_running_stats", ",", "num_splits", "=", "num_splits", ")", "\n", "mod", ".", "running_mean", "=", "module", ".", "running_mean", "\n", "mod", ".", "running_var", "=", "module", ".", "running_var", "\n", "mod", ".", "num_batches_tracked", "=", "module", ".", "num_batches_tracked", "\n", "if", "module", ".", "affine", ":", "\n", "            ", "mod", ".", "weight", ".", "data", "=", "module", ".", "weight", ".", "data", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "mod", ".", "bias", ".", "data", "=", "module", ".", "bias", ".", "data", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "for", "aux", "in", "mod", ".", "aux_bn", ":", "\n", "            ", "aux", ".", "running_mean", "=", "module", ".", "running_mean", ".", "clone", "(", ")", "\n", "aux", ".", "running_var", "=", "module", ".", "running_var", ".", "clone", "(", ")", "\n", "aux", ".", "num_batches_tracked", "=", "module", ".", "num_batches_tracked", ".", "clone", "(", ")", "\n", "if", "module", ".", "affine", ":", "\n", "                ", "aux", ".", "weight", ".", "data", "=", "module", ".", "weight", ".", "data", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "aux", ".", "bias", ".", "data", "=", "module", ".", "bias", ".", "data", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "", "", "for", "name", ",", "child", "in", "module", ".", "named_children", "(", ")", ":", "\n", "        ", "mod", ".", "add_module", "(", "name", ",", "convert_splitbn_model", "(", "child", ",", "num_splits", "=", "num_splits", ")", ")", "\n", "", "del", "module", "\n", "return", "mod", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.halo_attn.PosEmbedRel.__init__": [[67, 80], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "block_size", ",", "win_size", ",", "dim_head", ",", "scale", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            block_size (int): block size\n            win_size (int): neighbourhood window size\n            dim_head (int): attention head dim\n            scale (float): scale factor (for init)\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "block_size", "=", "block_size", "\n", "self", ".", "dim_head", "=", "dim_head", "\n", "self", ".", "height_rel", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "win_size", "*", "2", "-", "1", ",", "dim_head", ")", "*", "scale", ")", "\n", "self", ".", "width_rel", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "win_size", "*", "2", "-", "1", ",", "dim_head", ")", "*", "scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.halo_attn.PosEmbedRel.forward": [[81, 95], ["q.transpose.transpose.reshape", "halo_attn.rel_logits_1d", "q.transpose.transpose.transpose", "halo_attn.rel_logits_1d", "rel_logits.reshape.reshape.reshape"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.halo_attn.rel_logits_1d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.halo_attn.rel_logits_1d"], ["", "def", "forward", "(", "self", ",", "q", ")", ":", "\n", "        ", "B", ",", "BB", ",", "HW", ",", "_", "=", "q", ".", "shape", "\n", "\n", "# relative logits in width dimension.", "\n", "q", "=", "q", ".", "reshape", "(", "-", "1", ",", "self", ".", "block_size", ",", "self", ".", "block_size", ",", "self", ".", "dim_head", ")", "\n", "rel_logits_w", "=", "rel_logits_1d", "(", "q", ",", "self", ".", "width_rel", ",", "permute_mask", "=", "(", "0", ",", "1", ",", "3", ",", "2", ",", "4", ")", ")", "\n", "\n", "# relative logits in height dimension.", "\n", "q", "=", "q", ".", "transpose", "(", "1", ",", "2", ")", "\n", "rel_logits_h", "=", "rel_logits_1d", "(", "q", ",", "self", ".", "height_rel", ",", "permute_mask", "=", "(", "0", ",", "3", ",", "1", ",", "4", ",", "2", ")", ")", "\n", "\n", "rel_logits", "=", "rel_logits_h", "+", "rel_logits_w", "\n", "rel_logits", "=", "rel_logits", ".", "reshape", "(", "B", ",", "BB", ",", "HW", ",", "-", "1", ")", "\n", "return", "rel_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.halo_attn.HaloAttn.__init__": [[125, 161], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "halo_attn.PosEmbedRel", "halo_attn.HaloAttn.reset_parameters", "torch.nn.AvgPool2d", "torch.nn.AvgPool2d", "torch.nn.Identity", "torch.nn.Identity", "helpers.make_divisible"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible"], ["def", "__init__", "(", "\n", "self", ",", "dim", ",", "dim_out", "=", "None", ",", "feat_size", "=", "None", ",", "stride", "=", "1", ",", "num_heads", "=", "8", ",", "dim_head", "=", "None", ",", "block_size", "=", "8", ",", "halo_size", "=", "3", ",", "\n", "qk_ratio", "=", "1.0", ",", "qkv_bias", "=", "False", ",", "avg_down", "=", "False", ",", "scale_pos_embed", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "dim_out", "=", "dim_out", "or", "dim", "\n", "assert", "dim_out", "%", "num_heads", "==", "0", "\n", "assert", "stride", "in", "(", "1", ",", "2", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "dim_head_qk", "=", "dim_head", "or", "make_divisible", "(", "dim_out", "*", "qk_ratio", ",", "divisor", "=", "8", ")", "//", "num_heads", "\n", "self", ".", "dim_head_v", "=", "dim_out", "//", "self", ".", "num_heads", "\n", "self", ".", "dim_out_qk", "=", "num_heads", "*", "self", ".", "dim_head_qk", "\n", "self", ".", "dim_out_v", "=", "num_heads", "*", "self", ".", "dim_head_v", "\n", "self", ".", "scale", "=", "self", ".", "dim_head_qk", "**", "-", "0.5", "\n", "self", ".", "scale_pos_embed", "=", "scale_pos_embed", "\n", "self", ".", "block_size", "=", "self", ".", "block_size_ds", "=", "block_size", "\n", "self", ".", "halo_size", "=", "halo_size", "\n", "self", ".", "win_size", "=", "block_size", "+", "halo_size", "*", "2", "# neighbourhood window size", "\n", "self", ".", "block_stride", "=", "1", "\n", "use_avg_pool", "=", "False", "\n", "if", "stride", ">", "1", ":", "\n", "            ", "use_avg_pool", "=", "avg_down", "or", "block_size", "%", "stride", "!=", "0", "\n", "self", ".", "block_stride", "=", "1", "if", "use_avg_pool", "else", "stride", "\n", "self", ".", "block_size_ds", "=", "self", ".", "block_size", "//", "self", ".", "block_stride", "\n", "\n", "# FIXME not clear if this stride behaviour is what the paper intended", "\n", "# Also, the paper mentions using a 3D conv for dealing with the blocking/gather, and leaving", "\n", "# data in unfolded block form. I haven't wrapped my head around how that'd look.", "\n", "", "self", ".", "q", "=", "nn", ".", "Conv2d", "(", "dim", ",", "self", ".", "dim_out_qk", ",", "1", ",", "stride", "=", "self", ".", "block_stride", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "kv", "=", "nn", ".", "Conv2d", "(", "dim", ",", "self", ".", "dim_out_qk", "+", "self", ".", "dim_out_v", ",", "1", ",", "bias", "=", "qkv_bias", ")", "\n", "\n", "self", ".", "pos_embed", "=", "PosEmbedRel", "(", "\n", "block_size", "=", "self", ".", "block_size_ds", ",", "win_size", "=", "self", ".", "win_size", ",", "dim_head", "=", "self", ".", "dim_head_qk", ",", "scale", "=", "self", ".", "scale", ")", "\n", "\n", "self", ".", "pool", "=", "nn", ".", "AvgPool2d", "(", "2", ",", "2", ")", "if", "use_avg_pool", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.halo_attn.HaloAttn.reset_parameters": [[162, 168], ["weight_init.trunc_normal_", "weight_init.trunc_normal_", "weight_init.trunc_normal_", "weight_init.trunc_normal_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "std", "=", "self", ".", "q", ".", "weight", ".", "shape", "[", "1", "]", "**", "-", "0.5", "# fan-in", "\n", "trunc_normal_", "(", "self", ".", "q", ".", "weight", ",", "std", "=", "std", ")", "\n", "trunc_normal_", "(", "self", ".", "kv", ".", "weight", ",", "std", "=", "std", ")", "\n", "trunc_normal_", "(", "self", ".", "pos_embed", ".", "height_rel", ",", "std", "=", "self", ".", "scale", ")", "\n", "trunc_normal_", "(", "self", ".", "pos_embed", ".", "width_rel", ",", "std", "=", "self", ".", "scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.halo_attn.HaloAttn.forward": [[169, 211], ["trace_utils._assert", "trace_utils._assert", "halo_attn.HaloAttn.q", "q.reshape().transpose.reshape().transpose.reshape().permute", "q.reshape().transpose.reshape().transpose.reshape().transpose", "halo_attn.HaloAttn.kv", "torch.pad", "torch.pad", "kv.unfold().unfold().reshape().permute.unfold().unfold().reshape().permute.unfold().unfold().reshape().permute", "torch.split", "torch.split", "torch.split", "torch.split", "attn.softmax.softmax.softmax", "halo_attn.HaloAttn.reshape", "halo_attn.HaloAttn.permute().contiguous().view", "halo_attn.HaloAttn.pool", "q.reshape().transpose.reshape().transpose.reshape", "q.reshape().transpose.reshape().transpose.reshape", "kv.unfold().unfold().reshape().permute.unfold().unfold().reshape().permute.unfold().unfold().reshape", "halo_attn.HaloAttn.pos_embed", "halo_attn.HaloAttn.permute().contiguous", "halo_attn.HaloAttn.pos_embed", "kv.unfold().unfold().reshape().permute.unfold().unfold().reshape().permute.unfold().unfold", "k.transpose", "k.transpose", "halo_attn.HaloAttn.permute", "kv.unfold().unfold().reshape().permute.unfold().unfold().reshape().permute.unfold"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "_assert", "(", "H", "%", "self", ".", "block_size", "==", "0", ",", "''", ")", "\n", "_assert", "(", "W", "%", "self", ".", "block_size", "==", "0", ",", "''", ")", "\n", "num_h_blocks", "=", "H", "//", "self", ".", "block_size", "\n", "num_w_blocks", "=", "W", "//", "self", ".", "block_size", "\n", "num_blocks", "=", "num_h_blocks", "*", "num_w_blocks", "\n", "\n", "q", "=", "self", ".", "q", "(", "x", ")", "\n", "# unfold", "\n", "q", "=", "q", ".", "reshape", "(", "\n", "-", "1", ",", "self", ".", "dim_head_qk", ",", "\n", "num_h_blocks", ",", "self", ".", "block_size_ds", ",", "num_w_blocks", ",", "self", ".", "block_size_ds", ")", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "5", ",", "2", ",", "4", ")", "\n", "# B, num_heads * dim_head * block_size ** 2, num_blocks", "\n", "q", "=", "q", ".", "reshape", "(", "B", "*", "self", ".", "num_heads", ",", "self", ".", "dim_head_qk", ",", "-", "1", ",", "num_blocks", ")", ".", "transpose", "(", "1", ",", "3", ")", "\n", "# B * num_heads, num_blocks, block_size ** 2, dim_head", "\n", "\n", "kv", "=", "self", ".", "kv", "(", "x", ")", "\n", "# Generate overlapping windows for kv. This approach is good for GPU and CPU. However, unfold() is not", "\n", "# lowered for PyTorch XLA so it will be very slow. See code at bottom of file for XLA friendly approach.", "\n", "# FIXME figure out how to switch impl between this and conv2d if XLA being used.", "\n", "kv", "=", "F", ".", "pad", "(", "kv", ",", "[", "self", ".", "halo_size", ",", "self", ".", "halo_size", ",", "self", ".", "halo_size", ",", "self", ".", "halo_size", "]", ")", "\n", "kv", "=", "kv", ".", "unfold", "(", "2", ",", "self", ".", "win_size", ",", "self", ".", "block_size", ")", ".", "unfold", "(", "3", ",", "self", ".", "win_size", ",", "self", ".", "block_size", ")", ".", "reshape", "(", "\n", "B", "*", "self", ".", "num_heads", ",", "self", ".", "dim_head_qk", "+", "self", ".", "dim_head_v", ",", "num_blocks", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "\n", "k", ",", "v", "=", "torch", ".", "split", "(", "kv", ",", "[", "self", ".", "dim_head_qk", ",", "self", ".", "dim_head_v", "]", ",", "dim", "=", "-", "1", ")", "\n", "# B * num_heads, num_blocks, win_size ** 2, dim_head_qk or dim_head_v", "\n", "\n", "if", "self", ".", "scale_pos_embed", ":", "\n", "            ", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "+", "self", ".", "pos_embed", "(", "q", ")", ")", "*", "self", ".", "scale", "\n", "", "else", ":", "\n", "            ", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "*", "self", ".", "scale", "+", "self", ".", "pos_embed", "(", "q", ")", "\n", "# B * num_heads, num_blocks, block_size ** 2, win_size ** 2", "\n", "", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "out", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "3", ")", "# B * num_heads, dim_head_v, block_size ** 2, num_blocks", "\n", "# fold", "\n", "out", "=", "out", ".", "reshape", "(", "-", "1", ",", "self", ".", "block_size_ds", ",", "self", ".", "block_size_ds", ",", "num_h_blocks", ",", "num_w_blocks", ")", "\n", "out", "=", "out", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "4", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "B", ",", "self", ".", "dim_out_v", ",", "H", "//", "self", ".", "block_stride", ",", "W", "//", "self", ".", "block_stride", ")", "\n", "# B, dim_out, H // block_stride, W // block_stride", "\n", "out", "=", "self", ".", "pool", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.halo_attn.rel_logits_1d": [[30, 59], ["x.reshape().expand.reshape", "torch.pad().flatten", "torch.pad", "x_pad.reshape.reshape", "x.reshape().expand.reshape().expand", "x.reshape().expand.permute", "rel_k.transpose", "torch.pad", "x.reshape().expand.reshape"], "function", ["None"], ["def", "rel_logits_1d", "(", "q", ",", "rel_k", ",", "permute_mask", ":", "List", "[", "int", "]", ")", ":", "\n", "    ", "\"\"\" Compute relative logits along one dimension\n\n    As per: https://gist.github.com/aravindsrinivas/56359b79f0ce4449bcb04ab4b56a57a2\n    Originally from: `Attention Augmented Convolutional Networks` - https://arxiv.org/abs/1904.09925\n\n    Args:\n        q: (batch, height, width, dim)\n        rel_k: (2 * window - 1, dim)\n        permute_mask: permute output dim according to this\n    \"\"\"", "\n", "B", ",", "H", ",", "W", ",", "dim", "=", "q", ".", "shape", "\n", "rel_size", "=", "rel_k", ".", "shape", "[", "0", "]", "\n", "win_size", "=", "(", "rel_size", "+", "1", ")", "//", "2", "\n", "\n", "x", "=", "(", "q", "@", "rel_k", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "x", "=", "x", ".", "reshape", "(", "-", "1", ",", "W", ",", "rel_size", ")", "\n", "\n", "# pad to shift from relative to absolute indexing", "\n", "x_pad", "=", "F", ".", "pad", "(", "x", ",", "[", "0", ",", "1", "]", ")", ".", "flatten", "(", "1", ")", "\n", "x_pad", "=", "F", ".", "pad", "(", "x_pad", ",", "[", "0", ",", "rel_size", "-", "W", "]", ")", "\n", "\n", "# reshape and slice out the padded elements", "\n", "x_pad", "=", "x_pad", ".", "reshape", "(", "-", "1", ",", "W", "+", "1", ",", "rel_size", ")", "\n", "x", "=", "x_pad", "[", ":", ",", ":", "W", ",", "win_size", "-", "1", ":", "]", "\n", "\n", "# reshape and tile", "\n", "x", "=", "x", ".", "reshape", "(", "B", ",", "H", ",", "1", ",", "W", ",", "win_size", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "win_size", ",", "-", "1", ",", "-", "1", ")", "\n", "return", "x", ".", "permute", "(", "permute_mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.conv_bn_act.ConvNormAct.__init__": [[12, 25], ["torch.nn.Module.__init__", "create_conv2d.create_conv2d.create_conv2d", "create_norm_act.get_norm_act_layer", "create_norm_act.get_norm_act_layer.", "dict"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.get_norm_act_layer"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "''", ",", "dilation", "=", "1", ",", "groups", "=", "1", ",", "\n", "bias", "=", "False", ",", "apply_act", "=", "True", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "drop_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "ConvNormAct", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv", "=", "create_conv2d", "(", "\n", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "stride", ",", "\n", "padding", "=", "padding", ",", "dilation", "=", "dilation", ",", "groups", "=", "groups", ",", "bias", "=", "bias", ")", "\n", "\n", "# NOTE for backwards compatibility with models that use separate norm and act layer definitions", "\n", "norm_act_layer", "=", "get_norm_act_layer", "(", "norm_layer", ",", "act_layer", ")", "\n", "# NOTE for backwards (weight) compatibility, norm layer name remains `.bn`", "\n", "norm_kwargs", "=", "dict", "(", "drop_layer", "=", "drop_layer", ")", "if", "drop_layer", "is", "not", "None", "else", "{", "}", "\n", "self", ".", "bn", "=", "norm_act_layer", "(", "out_channels", ",", "apply_act", "=", "apply_act", ",", "**", "norm_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.conv_bn_act.ConvNormAct.in_channels": [[26, 29], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "in_channels", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "conv", ".", "in_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.conv_bn_act.ConvNormAct.out_channels": [[30, 33], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "out_channels", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "conv", ".", "out_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.conv_bn_act.ConvNormAct.forward": [[34, 38], ["conv_bn_act.ConvNormAct.conv", "conv_bn_act.ConvNormAct.bn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "x", "=", "self", ".", "bn", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.conv_bn_act.ConvNormActAa.__init__": [[44, 60], ["torch.nn.Module.__init__", "create_conv2d.create_conv2d.create_conv2d", "create_norm_act.get_norm_act_layer", "create_norm_act.get_norm_act_layer.", "dict", "aa_layer", "torch.nn.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.get_norm_act_layer"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "''", ",", "dilation", "=", "1", ",", "groups", "=", "1", ",", "\n", "bias", "=", "False", ",", "apply_act", "=", "True", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "aa_layer", "=", "None", ",", "drop_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "ConvNormActAa", ",", "self", ")", ".", "__init__", "(", ")", "\n", "use_aa", "=", "aa_layer", "is", "not", "None", "\n", "\n", "self", ".", "conv", "=", "create_conv2d", "(", "\n", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", "if", "use_aa", "else", "stride", ",", "\n", "padding", "=", "padding", ",", "dilation", "=", "dilation", ",", "groups", "=", "groups", ",", "bias", "=", "bias", ")", "\n", "\n", "# NOTE for backwards compatibility with models that use separate norm and act layer definitions", "\n", "norm_act_layer", "=", "get_norm_act_layer", "(", "norm_layer", ",", "act_layer", ")", "\n", "# NOTE for backwards (weight) compatibility, norm layer name remains `.bn`", "\n", "norm_kwargs", "=", "dict", "(", "drop_layer", "=", "drop_layer", ")", "if", "drop_layer", "is", "not", "None", "else", "{", "}", "\n", "self", ".", "bn", "=", "norm_act_layer", "(", "out_channels", ",", "apply_act", "=", "apply_act", ",", "**", "norm_kwargs", ")", "\n", "self", ".", "aa", "=", "aa_layer", "(", "channels", "=", "out_channels", ")", "if", "stride", "==", "2", "and", "use_aa", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.conv_bn_act.ConvNormActAa.in_channels": [[61, 64], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "in_channels", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "conv", ".", "in_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.conv_bn_act.ConvNormActAa.out_channels": [[65, 68], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "out_channels", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "conv", ".", "out_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.conv_bn_act.ConvNormActAa.forward": [[69, 74], ["conv_bn_act.ConvNormActAa.conv", "conv_bn_act.ConvNormActAa.bn", "conv_bn_act.ConvNormActAa.aa"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "x", "=", "self", ".", "bn", "(", "x", ")", "\n", "x", "=", "self", ".", "aa", "(", "x", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.ml_decoder.TransformerDecoderLayerOptimal.__init__": [[36, 55], ["torch.nn.Module.__init__", "torch.nn.LayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.MultiheadAttention", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.modules.transformer._get_activation_fn"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "d_model", ",", "nhead", "=", "8", ",", "dim_feedforward", "=", "2048", ",", "dropout", "=", "0.1", ",", "activation", "=", "\"relu\"", ",", "\n", "layer_norm_eps", "=", "1e-5", ")", "->", "None", ":", "\n", "        ", "super", "(", "TransformerDecoderLayerOptimal", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm1", "=", "nn", ".", "LayerNorm", "(", "d_model", ",", "eps", "=", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout1", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout2", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout3", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n", "self", ".", "multihead_attn", "=", "nn", ".", "MultiheadAttention", "(", "d_model", ",", "nhead", ",", "dropout", "=", "dropout", ")", "\n", "\n", "# Implementation of Feedforward model", "\n", "self", ".", "linear1", "=", "nn", ".", "Linear", "(", "d_model", ",", "dim_feedforward", ")", "\n", "self", ".", "linear2", "=", "nn", ".", "Linear", "(", "dim_feedforward", ",", "d_model", ")", "\n", "\n", "self", ".", "norm2", "=", "nn", ".", "LayerNorm", "(", "d_model", ",", "eps", "=", "layer_norm_eps", ")", "\n", "self", ".", "norm3", "=", "nn", ".", "LayerNorm", "(", "d_model", ",", "eps", "=", "layer_norm_eps", ")", "\n", "\n", "self", ".", "activation", "=", "_get_activation_fn", "(", "activation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.ml_decoder.TransformerDecoderLayerOptimal.__setstate__": [[56, 60], ["super().__setstate__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.ml_decoder.TransformerDecoderLayerOptimal.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "if", "'activation'", "not", "in", "state", ":", "\n", "            ", "state", "[", "'activation'", "]", "=", "torch", ".", "nn", ".", "functional", ".", "relu", "\n", "", "super", "(", "TransformerDecoderLayerOptimal", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.ml_decoder.TransformerDecoderLayerOptimal.forward": [[61, 74], ["ml_decoder.TransformerDecoderLayerOptimal.norm1", "ml_decoder.TransformerDecoderLayerOptimal.norm2", "ml_decoder.TransformerDecoderLayerOptimal.linear2", "ml_decoder.TransformerDecoderLayerOptimal.norm3", "ml_decoder.TransformerDecoderLayerOptimal.dropout1", "ml_decoder.TransformerDecoderLayerOptimal.multihead_attn", "ml_decoder.TransformerDecoderLayerOptimal.dropout2", "ml_decoder.TransformerDecoderLayerOptimal.dropout", "ml_decoder.TransformerDecoderLayerOptimal.dropout3", "ml_decoder.TransformerDecoderLayerOptimal.activation", "ml_decoder.TransformerDecoderLayerOptimal.linear1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tgt", ":", "Tensor", ",", "memory", ":", "Tensor", ",", "tgt_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "\n", "memory_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "\n", "tgt_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "\n", "memory_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ")", "->", "Tensor", ":", "\n", "        ", "tgt", "=", "tgt", "+", "self", ".", "dropout1", "(", "tgt", ")", "\n", "tgt", "=", "self", ".", "norm1", "(", "tgt", ")", "\n", "tgt2", "=", "self", ".", "multihead_attn", "(", "tgt", ",", "memory", ",", "memory", ")", "[", "0", "]", "\n", "tgt", "=", "tgt", "+", "self", ".", "dropout2", "(", "tgt2", ")", "\n", "tgt", "=", "self", ".", "norm2", "(", "tgt", ")", "\n", "tgt2", "=", "self", ".", "linear2", "(", "self", ".", "dropout", "(", "self", ".", "activation", "(", "self", ".", "linear1", "(", "tgt", ")", ")", ")", ")", "\n", "tgt", "=", "tgt", "+", "self", ".", "dropout3", "(", "tgt2", ")", "\n", "tgt", "=", "self", ".", "norm3", "(", "tgt", ")", "\n", "return", "tgt", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.ml_decoder.GroupFC.__init__": [[93, 95], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "embed_len_decoder", ":", "int", ")", ":", "\n", "        ", "self", ".", "embed_len_decoder", "=", "embed_len_decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.ml_decoder.GroupFC.__call__": [[96, 101], ["range", "torch.matmul"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "h", ":", "torch", ".", "Tensor", ",", "duplicate_pooling", ":", "torch", ".", "Tensor", ",", "out_extrap", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "self", ".", "embed_len_decoder", ")", ":", "\n", "            ", "h_i", "=", "h", "[", ":", ",", "i", ",", ":", "]", "\n", "w_i", "=", "duplicate_pooling", "[", "i", ",", ":", ",", ":", "]", "\n", "out_extrap", "[", ":", ",", "i", ",", ":", "]", "=", "torch", ".", "matmul", "(", "h_i", ",", "w_i", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.ml_decoder.MLDecoder.__init__": [[104, 135], ["torch.nn.Module.__init__", "torch.nn.Linear", "ml_decoder.TransformerDecoderLayerOptimal", "torch.nn.TransformerDecoder", "torch.nn.Embedding", "ml_decoder.MLDecoder.query_embed.requires_grad_", "int", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.init.xavier_normal_", "torch.nn.init.constant_", "ml_decoder.GroupFC", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_classes", ",", "num_of_groups", "=", "-", "1", ",", "decoder_embedding", "=", "768", ",", "initial_num_features", "=", "2048", ")", ":", "\n", "        ", "super", "(", "MLDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "embed_len_decoder", "=", "100", "if", "num_of_groups", "<", "0", "else", "num_of_groups", "\n", "if", "embed_len_decoder", ">", "num_classes", ":", "\n", "            ", "embed_len_decoder", "=", "num_classes", "\n", "\n", "# switching to 768 initial embeddings", "\n", "", "decoder_embedding", "=", "768", "if", "decoder_embedding", "<", "0", "else", "decoder_embedding", "\n", "self", ".", "embed_standart", "=", "nn", ".", "Linear", "(", "initial_num_features", ",", "decoder_embedding", ")", "\n", "\n", "# decoder", "\n", "decoder_dropout", "=", "0.1", "\n", "num_layers_decoder", "=", "1", "\n", "dim_feedforward", "=", "2048", "\n", "layer_decode", "=", "TransformerDecoderLayerOptimal", "(", "d_model", "=", "decoder_embedding", ",", "\n", "dim_feedforward", "=", "dim_feedforward", ",", "dropout", "=", "decoder_dropout", ")", "\n", "self", ".", "decoder", "=", "nn", ".", "TransformerDecoder", "(", "layer_decode", ",", "num_layers", "=", "num_layers_decoder", ")", "\n", "\n", "# non-learnable queries", "\n", "self", ".", "query_embed", "=", "nn", ".", "Embedding", "(", "embed_len_decoder", ",", "decoder_embedding", ")", "\n", "self", ".", "query_embed", ".", "requires_grad_", "(", "False", ")", "\n", "\n", "# group fully-connected", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "duplicate_factor", "=", "int", "(", "num_classes", "/", "embed_len_decoder", "+", "0.999", ")", "\n", "self", ".", "duplicate_pooling", "=", "torch", ".", "nn", ".", "Parameter", "(", "\n", "torch", ".", "Tensor", "(", "embed_len_decoder", ",", "decoder_embedding", ",", "self", ".", "duplicate_factor", ")", ")", "\n", "self", ".", "duplicate_pooling_bias", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "num_classes", ")", ")", "\n", "torch", ".", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "duplicate_pooling", ")", "\n", "torch", ".", "nn", ".", "init", ".", "constant_", "(", "self", ".", "duplicate_pooling_bias", ",", "0", ")", "\n", "self", ".", "group_fc", "=", "GroupFC", "(", "embed_len_decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.ml_decoder.MLDecoder.forward": [[136, 157], ["ml_decoder.MLDecoder.embed_standart", "torch.nn.functional.relu", "query_embed.unsqueeze().expand", "ml_decoder.MLDecoder.decoder", "h.transpose.transpose.transpose", "torch.zeros", "ml_decoder.MLDecoder.group_fc", "len", "x.flatten().transpose", "torch.nn.functional.relu.transpose", "torch.zeros.flatten", "query_embed.unsqueeze", "x.flatten"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "len", "(", "x", ".", "shape", ")", "==", "4", ":", "# [bs,2048, 7,7]", "\n", "            ", "embedding_spatial", "=", "x", ".", "flatten", "(", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "", "else", ":", "# [bs, 197,468]", "\n", "            ", "embedding_spatial", "=", "x", "\n", "", "embedding_spatial_786", "=", "self", ".", "embed_standart", "(", "embedding_spatial", ")", "\n", "embedding_spatial_786", "=", "torch", ".", "nn", ".", "functional", ".", "relu", "(", "embedding_spatial_786", ",", "inplace", "=", "True", ")", "\n", "\n", "bs", "=", "embedding_spatial_786", ".", "shape", "[", "0", "]", "\n", "query_embed", "=", "self", ".", "query_embed", ".", "weight", "\n", "# tgt = query_embed.unsqueeze(1).repeat(1, bs, 1)", "\n", "tgt", "=", "query_embed", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "bs", ",", "-", "1", ")", "# no allocation of memory with expand", "\n", "h", "=", "self", ".", "decoder", "(", "tgt", ",", "embedding_spatial_786", ".", "transpose", "(", "0", ",", "1", ")", ")", "# [embed_len_decoder, batch, 768]", "\n", "h", "=", "h", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "out_extrap", "=", "torch", ".", "zeros", "(", "h", ".", "shape", "[", "0", "]", ",", "h", ".", "shape", "[", "1", "]", ",", "self", ".", "duplicate_factor", ",", "device", "=", "h", ".", "device", ",", "dtype", "=", "h", ".", "dtype", ")", "\n", "self", ".", "group_fc", "(", "h", ",", "self", ".", "duplicate_pooling", ",", "out_extrap", ")", "\n", "h_out", "=", "out_extrap", ".", "flatten", "(", "1", ")", "[", ":", ",", ":", "self", ".", "num_classes", "]", "\n", "h_out", "+=", "self", ".", "duplicate_pooling_bias", "\n", "logits", "=", "h_out", "\n", "return", "logits", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.ml_decoder.add_ml_decoder_head": [[9, 33], ["hasattr", "hasattr", "hasattr", "torch.nn.Identity", "ml_decoder.MLDecoder", "hasattr", "hasattr", "torch.nn.Identity", "ml_decoder.MLDecoder", "ml_decoder.MLDecoder", "print", "exit", "model._get_name", "model._get_name"], "function", ["None"], ["def", "add_ml_decoder_head", "(", "model", ")", ":", "\n", "    ", "if", "hasattr", "(", "model", ",", "'global_pool'", ")", "and", "hasattr", "(", "model", ",", "'fc'", ")", ":", "# most CNN models, like Resnet50", "\n", "        ", "model", ".", "global_pool", "=", "nn", ".", "Identity", "(", ")", "\n", "del", "model", ".", "fc", "\n", "num_classes", "=", "model", ".", "num_classes", "\n", "num_features", "=", "model", ".", "num_features", "\n", "model", ".", "fc", "=", "MLDecoder", "(", "num_classes", "=", "num_classes", ",", "initial_num_features", "=", "num_features", ")", "\n", "", "elif", "hasattr", "(", "model", ",", "'global_pool'", ")", "and", "hasattr", "(", "model", ",", "'classifier'", ")", ":", "# EfficientNet", "\n", "        ", "model", ".", "global_pool", "=", "nn", ".", "Identity", "(", ")", "\n", "del", "model", ".", "classifier", "\n", "num_classes", "=", "model", ".", "num_classes", "\n", "num_features", "=", "model", ".", "num_features", "\n", "model", ".", "classifier", "=", "MLDecoder", "(", "num_classes", "=", "num_classes", ",", "initial_num_features", "=", "num_features", ")", "\n", "", "elif", "'RegNet'", "in", "model", ".", "_get_name", "(", ")", "or", "'TResNet'", "in", "model", ".", "_get_name", "(", ")", ":", "# hasattr(model, 'head')", "\n", "        ", "del", "model", ".", "head", "\n", "num_classes", "=", "model", ".", "num_classes", "\n", "num_features", "=", "model", ".", "num_features", "\n", "model", ".", "head", "=", "MLDecoder", "(", "num_classes", "=", "num_classes", ",", "initial_num_features", "=", "num_features", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Model code-writing is not aligned currently with ml-decoder\"", ")", "\n", "exit", "(", "-", "1", ")", "\n", "", "if", "hasattr", "(", "model", ",", "'drop_rate'", ")", ":", "# Ml-Decoder has inner dropout", "\n", "        ", "model", ".", "drop_rate", "=", "0", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.norm.GroupNorm.__init__": [[9, 12], ["torch.GroupNorm.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_channels", ",", "num_groups", "=", "32", ",", "eps", "=", "1e-5", ",", "affine", "=", "True", ")", ":", "\n", "# NOTE num_channels is swapped to first arg for consistency in swapping norm layers with BN", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_groups", ",", "num_channels", ",", "eps", "=", "eps", ",", "affine", "=", "affine", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.norm.GroupNorm.forward": [[13, 15], ["torch.group_norm", "torch.group_norm", "torch.group_norm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "F", ".", "group_norm", "(", "x", ",", "self", ".", "num_groups", ",", "self", ".", "weight", ",", "self", ".", "bias", ",", "self", ".", "eps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.norm.LayerNorm2d.__init__": [[19, 21], ["torch.LayerNorm.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "num_channels", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_channels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.norm.LayerNorm2d.forward": [[22, 25], ["torch.layer_norm().permute", "torch.layer_norm().permute", "torch.layer_norm().permute", "torch.layer_norm", "torch.layer_norm", "torch.layer_norm", "x.permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "return", "F", ".", "layer_norm", "(", "\n", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ",", "self", ".", "normalized_shape", ",", "self", ".", "weight", ",", "self", ".", "bias", ",", "self", ".", "eps", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mixed_conv2d.MixedConv2d.__init__": [[26, 46], ["torch.nn.ModuleDict.__init__", "len", "mixed_conv2d._split_channels", "mixed_conv2d._split_channels", "sum", "sum", "enumerate", "isinstance", "zip", "mixed_conv2d.MixedConv2d.add_module", "str", "conv2d_same.create_conv2d_pad"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mixed_conv2d._split_channels", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mixed_conv2d._split_channels", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.conv2d_same.create_conv2d_pad"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "3", ",", "\n", "stride", "=", "1", ",", "padding", "=", "''", ",", "dilation", "=", "1", ",", "depthwise", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "MixedConv2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "kernel_size", "=", "kernel_size", "if", "isinstance", "(", "kernel_size", ",", "list", ")", "else", "[", "kernel_size", "]", "\n", "num_groups", "=", "len", "(", "kernel_size", ")", "\n", "in_splits", "=", "_split_channels", "(", "in_channels", ",", "num_groups", ")", "\n", "out_splits", "=", "_split_channels", "(", "out_channels", ",", "num_groups", ")", "\n", "self", ".", "in_channels", "=", "sum", "(", "in_splits", ")", "\n", "self", ".", "out_channels", "=", "sum", "(", "out_splits", ")", "\n", "for", "idx", ",", "(", "k", ",", "in_ch", ",", "out_ch", ")", "in", "enumerate", "(", "zip", "(", "kernel_size", ",", "in_splits", ",", "out_splits", ")", ")", ":", "\n", "            ", "conv_groups", "=", "in_ch", "if", "depthwise", "else", "1", "\n", "# use add_module to keep key space clean", "\n", "self", ".", "add_module", "(", "\n", "str", "(", "idx", ")", ",", "\n", "create_conv2d_pad", "(", "\n", "in_ch", ",", "out_ch", ",", "k", ",", "stride", "=", "stride", ",", "\n", "padding", "=", "padding", ",", "dilation", "=", "dilation", ",", "groups", "=", "conv_groups", ",", "**", "kwargs", ")", "\n", ")", "\n", "", "self", ".", "splits", "=", "in_splits", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mixed_conv2d.MixedConv2d.forward": [[47, 52], ["torch.split", "torch.cat", "c", "enumerate", "mixed_conv2d.MixedConv2d.values"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_split", "=", "torch", ".", "split", "(", "x", ",", "self", ".", "splits", ",", "1", ")", "\n", "x_out", "=", "[", "c", "(", "x_split", "[", "i", "]", ")", "for", "i", ",", "c", "in", "enumerate", "(", "self", ".", "values", "(", ")", ")", "]", "\n", "x", "=", "torch", ".", "cat", "(", "x_out", ",", "1", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mixed_conv2d._split_channels": [[14, 18], ["sum", "range"], "function", ["None"], ["def", "_split_channels", "(", "num_chan", ",", "num_groups", ")", ":", "\n", "    ", "split", "=", "[", "num_chan", "//", "num_groups", "for", "_", "in", "range", "(", "num_groups", ")", "]", "\n", "split", "[", "0", "]", "+=", "num_chan", "-", "sum", "(", "split", ")", "\n", "return", "split", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_fn": [[105, 123], ["isinstance", "config.is_no_jit", "config.is_exportable", "config.is_scriptable", "config.is_no_jit", "config.is_exportable"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.is_no_jit", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.is_exportable", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.is_scriptable", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.is_no_jit", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.is_exportable"], ["", "def", "get_act_fn", "(", "name", ":", "Union", "[", "Callable", ",", "str", "]", "=", "'relu'", ")", ":", "\n", "    ", "\"\"\" Activation Function Factory\n    Fetching activation fns by name with this function allows export or torch script friendly\n    functions to be returned dynamically based on current config.\n    \"\"\"", "\n", "if", "not", "name", ":", "\n", "        ", "return", "None", "\n", "", "if", "isinstance", "(", "name", ",", "Callable", ")", ":", "\n", "        ", "return", "name", "\n", "", "if", "not", "(", "is_no_jit", "(", ")", "or", "is_exportable", "(", ")", "or", "is_scriptable", "(", ")", ")", ":", "\n", "# If not exporting or scripting the model, first look for a memory-efficient version with", "\n", "# custom autograd, then fallback", "\n", "        ", "if", "name", "in", "_ACT_FN_ME", ":", "\n", "            ", "return", "_ACT_FN_ME", "[", "name", "]", "\n", "", "", "if", "not", "(", "is_no_jit", "(", ")", "or", "is_exportable", "(", ")", ")", ":", "\n", "        ", "if", "name", "in", "_ACT_FN_JIT", ":", "\n", "            ", "return", "_ACT_FN_JIT", "[", "name", "]", "\n", "", "", "return", "_ACT_FN_DEFAULT", "[", "name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_layer": [[125, 142], ["isinstance", "config.is_no_jit", "config.is_exportable", "config.is_scriptable", "config.is_no_jit", "config.is_exportable"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.is_no_jit", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.is_exportable", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.is_scriptable", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.is_no_jit", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.config.is_exportable"], ["", "def", "get_act_layer", "(", "name", ":", "Union", "[", "Type", "[", "nn", ".", "Module", "]", ",", "str", "]", "=", "'relu'", ")", ":", "\n", "    ", "\"\"\" Activation Layer Factory\n    Fetching activation layers by name with this function allows export or torch script friendly\n    functions to be returned dynamically based on current config.\n    \"\"\"", "\n", "if", "not", "name", ":", "\n", "        ", "return", "None", "\n", "", "if", "not", "isinstance", "(", "name", ",", "str", ")", ":", "\n", "# callable, module, etc", "\n", "        ", "return", "name", "\n", "", "if", "not", "(", "is_no_jit", "(", ")", "or", "is_exportable", "(", ")", "or", "is_scriptable", "(", ")", ")", ":", "\n", "        ", "if", "name", "in", "_ACT_LAYER_ME", ":", "\n", "            ", "return", "_ACT_LAYER_ME", "[", "name", "]", "\n", "", "", "if", "not", "(", "is_no_jit", "(", ")", "or", "is_exportable", "(", ")", ")", ":", "\n", "        ", "if", "name", "in", "_ACT_LAYER_JIT", ":", "\n", "            ", "return", "_ACT_LAYER_JIT", "[", "name", "]", "\n", "", "", "return", "_ACT_LAYER_DEFAULT", "[", "name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.create_act_layer": [[144, 149], ["create_act.get_act_layer", "get_act_layer.", "get_act_layer."], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_layer"], ["", "def", "create_act_layer", "(", "name", ":", "Union", "[", "nn", ".", "Module", ",", "str", "]", ",", "inplace", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "act_layer", "=", "get_act_layer", "(", "name", ")", "\n", "if", "act_layer", "is", "None", ":", "\n", "        ", "return", "None", "\n", "", "return", "act_layer", "(", "**", "kwargs", ")", "if", "inplace", "is", "None", "else", "act_layer", "(", "inplace", "=", "inplace", ",", "**", "kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.DropBlock2d.__init__": [[108, 125], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "drop_prob", ":", "float", "=", "0.1", ",", "\n", "block_size", ":", "int", "=", "7", ",", "\n", "gamma_scale", ":", "float", "=", "1.0", ",", "\n", "with_noise", ":", "bool", "=", "False", ",", "\n", "inplace", ":", "bool", "=", "False", ",", "\n", "batchwise", ":", "bool", "=", "False", ",", "\n", "fast", ":", "bool", "=", "True", ")", ":", "\n", "        ", "super", "(", "DropBlock2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "drop_prob", "=", "drop_prob", "\n", "self", ".", "gamma_scale", "=", "gamma_scale", "\n", "self", ".", "block_size", "=", "block_size", "\n", "self", ".", "with_noise", "=", "with_noise", "\n", "self", ".", "inplace", "=", "inplace", "\n", "self", ".", "batchwise", "=", "batchwise", "\n", "self", ".", "fast", "=", "fast", "# FIXME finish comparisons of fast vs not", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.DropBlock2d.forward": [[126, 135], ["drop.drop_block_fast_2d", "drop.drop_block_2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_block_fast_2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_block_2d"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "not", "self", ".", "training", "or", "not", "self", ".", "drop_prob", ":", "\n", "            ", "return", "x", "\n", "", "if", "self", ".", "fast", ":", "\n", "            ", "return", "drop_block_fast_2d", "(", "\n", "x", ",", "self", ".", "drop_prob", ",", "self", ".", "block_size", ",", "self", ".", "gamma_scale", ",", "self", ".", "with_noise", ",", "self", ".", "inplace", ")", "\n", "", "else", ":", "\n", "            ", "return", "drop_block_2d", "(", "\n", "x", ",", "self", ".", "drop_prob", ",", "self", ".", "block_size", ",", "self", ".", "gamma_scale", ",", "self", ".", "with_noise", ",", "self", ".", "inplace", ",", "self", ".", "batchwise", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.DropPath.__init__": [[160, 164], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "drop_prob", ":", "float", "=", "0.", ",", "scale_by_keep", ":", "bool", "=", "True", ")", ":", "\n", "        ", "super", "(", "DropPath", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "drop_prob", "=", "drop_prob", "\n", "self", ".", "scale_by_keep", "=", "scale_by_keep", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.DropPath.forward": [[165, 167], ["drop.drop_path"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "drop_path", "(", "x", ",", "self", ".", "drop_prob", ",", "self", ".", "training", ",", "self", ".", "scale_by_keep", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.DropPath.extra_repr": [[168, 170], ["round"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "f'drop_prob={round(self.drop_prob,3):0.3f}'", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_block_2d": [[22, 68], ["min", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.reshape().to", "torch.reshape().to", "torch.reshape().to", "min", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.rand", "torch.rand", "torch.rand", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.max_pool2d", "torch.reshape", "torch.reshape", "torch.reshape", "torch.randn", "torch.randn", "torch.randn", "torch.randn_like", "torch.randn_like", "torch.randn_like", "x.mul_().add_", "x.mul_", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "x.mul_", "block_mask.numel", "block_mask.to().sum().add", "block_mask.to().sum", "block_mask.to"], "function", ["None"], ["def", "drop_block_2d", "(", "\n", "x", ",", "drop_prob", ":", "float", "=", "0.1", ",", "block_size", ":", "int", "=", "7", ",", "gamma_scale", ":", "float", "=", "1.0", ",", "\n", "with_noise", ":", "bool", "=", "False", ",", "inplace", ":", "bool", "=", "False", ",", "batchwise", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\" DropBlock. See https://arxiv.org/pdf/1810.12890.pdf\n\n    DropBlock with an experimental gaussian noise option. This layer has been tested on a few training\n    runs with success, but needs further validation and possibly optimization for lower runtime impact.\n    \"\"\"", "\n", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "total_size", "=", "W", "*", "H", "\n", "clipped_block_size", "=", "min", "(", "block_size", ",", "min", "(", "W", ",", "H", ")", ")", "\n", "# seed_drop_rate, the gamma parameter", "\n", "gamma", "=", "gamma_scale", "*", "drop_prob", "*", "total_size", "/", "clipped_block_size", "**", "2", "/", "(", "\n", "(", "W", "-", "block_size", "+", "1", ")", "*", "(", "H", "-", "block_size", "+", "1", ")", ")", "\n", "\n", "# Forces the block to be inside the feature map.", "\n", "w_i", ",", "h_i", "=", "torch", ".", "meshgrid", "(", "torch", ".", "arange", "(", "W", ")", ".", "to", "(", "x", ".", "device", ")", ",", "torch", ".", "arange", "(", "H", ")", ".", "to", "(", "x", ".", "device", ")", ")", "\n", "valid_block", "=", "(", "(", "w_i", ">=", "clipped_block_size", "//", "2", ")", "&", "(", "w_i", "<", "W", "-", "(", "clipped_block_size", "-", "1", ")", "//", "2", ")", ")", "&", "(", "(", "h_i", ">=", "clipped_block_size", "//", "2", ")", "&", "(", "h_i", "<", "H", "-", "(", "clipped_block_size", "-", "1", ")", "//", "2", ")", ")", "\n", "valid_block", "=", "torch", ".", "reshape", "(", "valid_block", ",", "(", "1", ",", "1", ",", "H", ",", "W", ")", ")", ".", "to", "(", "dtype", "=", "x", ".", "dtype", ")", "\n", "\n", "if", "batchwise", ":", "\n", "# one mask for whole batch, quite a bit faster", "\n", "        ", "uniform_noise", "=", "torch", ".", "rand", "(", "(", "1", ",", "C", ",", "H", ",", "W", ")", ",", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "\n", "", "else", ":", "\n", "        ", "uniform_noise", "=", "torch", ".", "rand_like", "(", "x", ")", "\n", "", "block_mask", "=", "(", "(", "2", "-", "gamma", "-", "valid_block", "+", "uniform_noise", ")", ">=", "1", ")", ".", "to", "(", "dtype", "=", "x", ".", "dtype", ")", "\n", "block_mask", "=", "-", "F", ".", "max_pool2d", "(", "\n", "-", "block_mask", ",", "\n", "kernel_size", "=", "clipped_block_size", ",", "# block_size,", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "clipped_block_size", "//", "2", ")", "\n", "\n", "if", "with_noise", ":", "\n", "        ", "normal_noise", "=", "torch", ".", "randn", "(", "(", "1", ",", "C", ",", "H", ",", "W", ")", ",", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "if", "batchwise", "else", "torch", ".", "randn_like", "(", "x", ")", "\n", "if", "inplace", ":", "\n", "            ", "x", ".", "mul_", "(", "block_mask", ")", ".", "add_", "(", "normal_noise", "*", "(", "1", "-", "block_mask", ")", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", "*", "block_mask", "+", "normal_noise", "*", "(", "1", "-", "block_mask", ")", "\n", "", "", "else", ":", "\n", "        ", "normalize_scale", "=", "(", "block_mask", ".", "numel", "(", ")", "/", "block_mask", ".", "to", "(", "dtype", "=", "torch", ".", "float32", ")", ".", "sum", "(", ")", ".", "add", "(", "1e-7", ")", ")", ".", "to", "(", "x", ".", "dtype", ")", "\n", "if", "inplace", ":", "\n", "            ", "x", ".", "mul_", "(", "block_mask", "*", "normalize_scale", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", "*", "block_mask", "*", "normalize_scale", "\n", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_block_fast_2d": [[70, 102], ["min", "torch.empty_like().bernoulli_", "torch.empty_like().bernoulli_", "torch.empty_like().bernoulli_", "torch.max_pool2d", "min", "F.max_pool2d.to", "torch.empty_like().normal_", "torch.empty_like().normal_", "torch.empty_like().normal_", "torch.empty_like", "torch.empty_like", "torch.empty_like", "x.mul_().add_", "x.mul_", "torch.empty_like", "torch.empty_like", "torch.empty_like", "x.mul_", "F.max_pool2d.numel", "F.max_pool2d.to().sum().add", "F.max_pool2d.to().sum", "F.max_pool2d.to"], "function", ["None"], ["", "def", "drop_block_fast_2d", "(", "\n", "x", ":", "torch", ".", "Tensor", ",", "drop_prob", ":", "float", "=", "0.1", ",", "block_size", ":", "int", "=", "7", ",", "\n", "gamma_scale", ":", "float", "=", "1.0", ",", "with_noise", ":", "bool", "=", "False", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\" DropBlock. See https://arxiv.org/pdf/1810.12890.pdf\n\n    DropBlock with an experimental gaussian noise option. Simplied from above without concern for valid\n    block mask at edges.\n    \"\"\"", "\n", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "total_size", "=", "W", "*", "H", "\n", "clipped_block_size", "=", "min", "(", "block_size", ",", "min", "(", "W", ",", "H", ")", ")", "\n", "gamma", "=", "gamma_scale", "*", "drop_prob", "*", "total_size", "/", "clipped_block_size", "**", "2", "/", "(", "\n", "(", "W", "-", "block_size", "+", "1", ")", "*", "(", "H", "-", "block_size", "+", "1", ")", ")", "\n", "\n", "block_mask", "=", "torch", ".", "empty_like", "(", "x", ")", ".", "bernoulli_", "(", "gamma", ")", "\n", "block_mask", "=", "F", ".", "max_pool2d", "(", "\n", "block_mask", ".", "to", "(", "x", ".", "dtype", ")", ",", "kernel_size", "=", "clipped_block_size", ",", "stride", "=", "1", ",", "padding", "=", "clipped_block_size", "//", "2", ")", "\n", "\n", "if", "with_noise", ":", "\n", "        ", "normal_noise", "=", "torch", ".", "empty_like", "(", "x", ")", ".", "normal_", "(", ")", "\n", "if", "inplace", ":", "\n", "            ", "x", ".", "mul_", "(", "1.", "-", "block_mask", ")", ".", "add_", "(", "normal_noise", "*", "block_mask", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", "*", "(", "1.", "-", "block_mask", ")", "+", "normal_noise", "*", "block_mask", "\n", "", "", "else", ":", "\n", "        ", "block_mask", "=", "1", "-", "block_mask", "\n", "normalize_scale", "=", "(", "block_mask", ".", "numel", "(", ")", "/", "block_mask", ".", "to", "(", "dtype", "=", "torch", ".", "float32", ")", ".", "sum", "(", ")", ".", "add", "(", "1e-6", ")", ")", ".", "to", "(", "dtype", "=", "x", ".", "dtype", ")", "\n", "if", "inplace", ":", "\n", "            ", "x", ".", "mul_", "(", "block_mask", "*", "normalize_scale", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", "*", "block_mask", "*", "normalize_scale", "\n", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.drop.drop_path": [[137, 155], ["x.new_empty().bernoulli_", "x.new_empty().bernoulli_.div_", "x.new_empty"], "function", ["None"], ["", "", "", "def", "drop_path", "(", "x", ",", "drop_prob", ":", "float", "=", "0.", ",", "training", ":", "bool", "=", "False", ",", "scale_by_keep", ":", "bool", "=", "True", ")", ":", "\n", "    ", "\"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\n    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n    'survival rate' as the argument.\n\n    \"\"\"", "\n", "if", "drop_prob", "==", "0.", "or", "not", "training", ":", "\n", "        ", "return", "x", "\n", "", "keep_prob", "=", "1", "-", "drop_prob", "\n", "shape", "=", "(", "x", ".", "shape", "[", "0", "]", ",", ")", "+", "(", "1", ",", ")", "*", "(", "x", ".", "ndim", "-", "1", ")", "# work with diff dim tensors, not just 2D ConvNets", "\n", "random_tensor", "=", "x", ".", "new_empty", "(", "shape", ")", ".", "bernoulli_", "(", "keep_prob", ")", "\n", "if", "keep_prob", ">", "0.0", "and", "scale_by_keep", ":", "\n", "        ", "random_tensor", ".", "div_", "(", "keep_prob", ")", "\n", "", "return", "x", "*", "random_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.SwishJitAutoFn.symbolic": [[33, 36], ["g.op", "g.op"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "symbolic", "(", "g", ",", "x", ")", ":", "\n", "        ", "return", "g", ".", "op", "(", "\"Mul\"", ",", "x", ",", "g", ".", "op", "(", "\"Sigmoid\"", ",", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.SwishJitAutoFn.forward": [[37, 41], ["ctx.save_for_backward", "activations_me.swish_jit_fwd"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.swish_jit_fwd"], ["", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "x", ")", "\n", "return", "swish_jit_fwd", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.SwishJitAutoFn.backward": [[42, 46], ["activations_me.swish_jit_bwd"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.swish_jit_bwd"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "x", "=", "ctx", ".", "saved_tensors", "[", "0", "]", "\n", "return", "swish_jit_bwd", "(", "x", ",", "grad_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.SwishMe.__init__": [[53, 55], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "SwishMe", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.SwishMe.forward": [[56, 58], ["SwishJitAutoFn.apply"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "SwishJitAutoFn", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.MishJitAutoFn.forward": [[76, 80], ["ctx.save_for_backward", "activations_me.mish_jit_fwd"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.mish_jit_fwd"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "x", ")", "\n", "return", "mish_jit_fwd", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.MishJitAutoFn.backward": [[81, 85], ["activations_me.mish_jit_bwd"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.mish_jit_bwd"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "x", "=", "ctx", ".", "saved_tensors", "[", "0", "]", "\n", "return", "mish_jit_bwd", "(", "x", ",", "grad_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.MishMe.__init__": [[92, 94], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "MishMe", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.MishMe.forward": [[95, 97], ["MishJitAutoFn.apply"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "MishJitAutoFn", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardSigmoidJitAutoFn.forward": [[111, 115], ["ctx.save_for_backward", "activations_me.hard_sigmoid_jit_fwd"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.hard_sigmoid_jit_fwd"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "x", ")", "\n", "return", "hard_sigmoid_jit_fwd", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardSigmoidJitAutoFn.backward": [[116, 120], ["activations_me.hard_sigmoid_jit_bwd"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.hard_sigmoid_jit_bwd"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "x", "=", "ctx", ".", "saved_tensors", "[", "0", "]", "\n", "return", "hard_sigmoid_jit_bwd", "(", "x", ",", "grad_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardSigmoidMe.__init__": [[127, 129], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "HardSigmoidMe", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardSigmoidMe.forward": [[130, 132], ["HardSigmoidJitAutoFn.apply"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "HardSigmoidJitAutoFn", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardSwishJitAutoFn.forward": [[148, 152], ["ctx.save_for_backward", "activations_me.hard_swish_jit_fwd"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.hard_swish_jit_fwd"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "x", ")", "\n", "return", "hard_swish_jit_fwd", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardSwishJitAutoFn.backward": [[153, 157], ["activations_me.hard_swish_jit_bwd"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.hard_swish_jit_bwd"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "x", "=", "ctx", ".", "saved_tensors", "[", "0", "]", "\n", "return", "hard_swish_jit_bwd", "(", "x", ",", "grad_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardSwishJitAutoFn.symbolic": [[158, 164], ["g.op", "g.op", "g.op", "g.op", "g.op", "g.op", "g.op", "g.op", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "symbolic", "(", "g", ",", "self", ")", ":", "\n", "        ", "input", "=", "g", ".", "op", "(", "\"Add\"", ",", "self", ",", "g", ".", "op", "(", "'Constant'", ",", "value_t", "=", "torch", ".", "tensor", "(", "3", ",", "dtype", "=", "torch", ".", "float", ")", ")", ")", "\n", "hardtanh_", "=", "g", ".", "op", "(", "\"Clip\"", ",", "input", ",", "g", ".", "op", "(", "'Constant'", ",", "value_t", "=", "torch", ".", "tensor", "(", "0", ",", "dtype", "=", "torch", ".", "float", ")", ")", ",", "g", ".", "op", "(", "'Constant'", ",", "value_t", "=", "torch", ".", "tensor", "(", "6", ",", "dtype", "=", "torch", ".", "float", ")", ")", ")", "\n", "hardtanh_", "=", "g", ".", "op", "(", "\"Div\"", ",", "hardtanh_", ",", "g", ".", "op", "(", "'Constant'", ",", "value_t", "=", "torch", ".", "tensor", "(", "6", ",", "dtype", "=", "torch", ".", "float", ")", ")", ")", "\n", "return", "g", ".", "op", "(", "\"Mul\"", ",", "self", ",", "hardtanh_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardSwishMe.__init__": [[171, 173], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "HardSwishMe", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardSwishMe.forward": [[174, 176], ["HardSwishJitAutoFn.apply"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "HardSwishJitAutoFn", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardMishJitAutoFn.forward": [[195, 199], ["ctx.save_for_backward", "activations_me.hard_mish_jit_fwd"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.hard_mish_jit_fwd"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "x", ")", "\n", "return", "hard_mish_jit_fwd", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardMishJitAutoFn.backward": [[200, 204], ["activations_me.hard_mish_jit_bwd"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.hard_mish_jit_bwd"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "x", "=", "ctx", ".", "saved_tensors", "[", "0", "]", "\n", "return", "hard_mish_jit_bwd", "(", "x", ",", "grad_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardMishMe.__init__": [[211, 213], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "HardMishMe", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.HardMishMe.forward": [[214, 216], ["HardMishJitAutoFn.apply"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "HardMishJitAutoFn", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.swish_jit_fwd": [[17, 20], ["x.mul", "torch.sigmoid"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid"], ["@", "torch", ".", "jit", ".", "script", "\n", "def", "swish_jit_fwd", "(", "x", ")", ":", "\n", "    ", "return", "x", ".", "mul", "(", "torch", ".", "sigmoid", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.swish_jit_bwd": [[22, 26], ["torch.sigmoid"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid"], ["", "@", "torch", ".", "jit", ".", "script", "\n", "def", "swish_jit_bwd", "(", "x", ",", "grad_output", ")", ":", "\n", "    ", "x_sigmoid", "=", "torch", ".", "sigmoid", "(", "x", ")", "\n", "return", "grad_output", "*", "(", "x_sigmoid", "*", "(", "1", "+", "x", "*", "(", "1", "-", "x_sigmoid", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.swish_me": [[48, 50], ["SwishJitAutoFn.apply"], "function", ["None"], ["", "", "def", "swish_me", "(", "x", ",", "inplace", "=", "False", ")", ":", "\n", "    ", "return", "SwishJitAutoFn", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.mish_jit_fwd": [[60, 63], ["x.mul", "torch.tanh", "torch.nn.functional.softplus"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.tanh"], ["", "", "@", "torch", ".", "jit", ".", "script", "\n", "def", "mish_jit_fwd", "(", "x", ")", ":", "\n", "    ", "return", "x", ".", "mul", "(", "torch", ".", "tanh", "(", "F", ".", "softplus", "(", "x", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.mish_jit_bwd": [[65, 70], ["torch.sigmoid", "torch.nn.functional.softplus().tanh", "grad_output.mul", "torch.nn.functional.softplus"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.tanh"], ["", "@", "torch", ".", "jit", ".", "script", "\n", "def", "mish_jit_bwd", "(", "x", ",", "grad_output", ")", ":", "\n", "    ", "x_sigmoid", "=", "torch", ".", "sigmoid", "(", "x", ")", "\n", "x_tanh_sp", "=", "F", ".", "softplus", "(", "x", ")", ".", "tanh", "(", ")", "\n", "return", "grad_output", ".", "mul", "(", "x_tanh_sp", "+", "x", "*", "x_sigmoid", "*", "(", "1", "-", "x_tanh_sp", "*", "x_tanh_sp", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.mish_me": [[87, 89], ["MishJitAutoFn.apply"], "function", ["None"], ["", "", "def", "mish_me", "(", "x", ",", "inplace", "=", "False", ")", ":", "\n", "    ", "return", "MishJitAutoFn", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.hard_sigmoid_jit_fwd": [[99, 102], ["None"], "function", ["None"], ["", "", "@", "torch", ".", "jit", ".", "script", "\n", "def", "hard_sigmoid_jit_fwd", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "return", "(", "x", "+", "3", ")", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "6", ")", ".", "div", "(", "6.", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.hard_sigmoid_jit_bwd": [[104, 108], ["torch.ones_like"], "function", ["None"], ["", "@", "torch", ".", "jit", ".", "script", "\n", "def", "hard_sigmoid_jit_bwd", "(", "x", ",", "grad_output", ")", ":", "\n", "    ", "m", "=", "torch", ".", "ones_like", "(", "x", ")", "*", "(", "(", "x", ">=", "-", "3.", ")", "&", "(", "x", "<=", "3.", ")", ")", "/", "6.", "\n", "return", "grad_output", "*", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.hard_sigmoid_me": [[122, 124], ["HardSigmoidJitAutoFn.apply"], "function", ["None"], ["", "", "def", "hard_sigmoid_me", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "return", "HardSigmoidJitAutoFn", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.hard_swish_jit_fwd": [[134, 137], ["None"], "function", ["None"], ["", "", "@", "torch", ".", "jit", ".", "script", "\n", "def", "hard_swish_jit_fwd", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "(", "x", "+", "3", ")", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "6", ")", ".", "div", "(", "6.", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.hard_swish_jit_bwd": [[139, 144], ["torch.where", "torch.ones_like"], "function", ["None"], ["", "@", "torch", ".", "jit", ".", "script", "\n", "def", "hard_swish_jit_bwd", "(", "x", ",", "grad_output", ")", ":", "\n", "    ", "m", "=", "torch", ".", "ones_like", "(", "x", ")", "*", "(", "x", ">=", "3.", ")", "\n", "m", "=", "torch", ".", "where", "(", "(", "x", ">=", "-", "3.", ")", "&", "(", "x", "<=", "3.", ")", ",", "x", "/", "3.", "+", ".5", ",", "m", ")", "\n", "return", "grad_output", "*", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.hard_swish_me": [[166, 168], ["HardSwishJitAutoFn.apply"], "function", ["None"], ["", "", "def", "hard_swish_me", "(", "x", ",", "inplace", "=", "False", ")", ":", "\n", "    ", "return", "HardSwishJitAutoFn", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.hard_mish_jit_fwd": [[178, 181], ["None"], "function", ["None"], ["", "", "@", "torch", ".", "jit", ".", "script", "\n", "def", "hard_mish_jit_fwd", "(", "x", ")", ":", "\n", "    ", "return", "0.5", "*", "x", "*", "(", "x", "+", "2", ")", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.hard_mish_jit_bwd": [[183, 188], ["torch.where", "torch.ones_like"], "function", ["None"], ["", "@", "torch", ".", "jit", ".", "script", "\n", "def", "hard_mish_jit_bwd", "(", "x", ",", "grad_output", ")", ":", "\n", "    ", "m", "=", "torch", ".", "ones_like", "(", "x", ")", "*", "(", "x", ">=", "-", "2.", ")", "\n", "m", "=", "torch", ".", "where", "(", "(", "x", ">=", "-", "2.", ")", "&", "(", "x", "<=", "0.", ")", ",", "x", "+", "1.", ",", "m", ")", "\n", "return", "grad_output", "*", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations_me.hard_mish_me": [[206, 208], ["HardMishJitAutoFn.apply"], "function", ["None"], ["", "", "def", "hard_mish_me", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "return", "HardMishJitAutoFn", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.filter_response_norm.FilterResponseNormTlu2d.__init__": [[20, 29], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "filter_response_norm.FilterResponseNormTlu2d.reset_parameters", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Parameter", "torch.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "num_features", ",", "apply_act", "=", "True", ",", "eps", "=", "1e-5", ",", "rms", "=", "True", ",", "**", "_", ")", ":", "\n", "        ", "super", "(", "FilterResponseNormTlu2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "apply_act", "=", "apply_act", "# apply activation (non-linearity)", "\n", "self", ".", "rms", "=", "rms", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "num_features", ")", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "num_features", ")", ")", "\n", "self", ".", "tau", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "num_features", ")", ")", "if", "apply_act", "else", "None", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.filter_response_norm.FilterResponseNormTlu2d.reset_parameters": [[30, 35], ["torch.init.ones_", "torch.init.ones_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "ones_", "(", "self", ".", "weight", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "bias", ")", "\n", "if", "self", ".", "tau", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "tau", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.filter_response_norm.FilterResponseNormTlu2d.forward": [[36, 43], ["trace_utils._assert", "filter_response_norm.inv_instance_rms", "filter_response_norm.FilterResponseNormTlu2d.bias.view().to", "torch.maximum", "torch.maximum", "torch.maximum", "torch.maximum", "x.dim", "filter_response_norm.FilterResponseNormTlu2d.weight.view().to", "filter_response_norm.FilterResponseNormTlu2d.tau.reshape().to", "filter_response_norm.FilterResponseNormTlu2d.bias.view", "filter_response_norm.FilterResponseNormTlu2d.weight.view", "filter_response_norm.FilterResponseNormTlu2d.tau.reshape"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.filter_response_norm.inv_instance_rms"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "_assert", "(", "x", ".", "dim", "(", ")", "==", "4", ",", "'expected 4D input'", ")", "\n", "x_dtype", "=", "x", ".", "dtype", "\n", "v_shape", "=", "(", "1", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "x", "=", "x", "*", "inv_instance_rms", "(", "x", ",", "self", ".", "eps", ")", "\n", "x", "=", "x", "*", "self", ".", "weight", ".", "view", "(", "v_shape", ")", ".", "to", "(", "dtype", "=", "x_dtype", ")", "+", "self", ".", "bias", ".", "view", "(", "v_shape", ")", ".", "to", "(", "dtype", "=", "x_dtype", ")", "\n", "return", "torch", ".", "maximum", "(", "x", ",", "self", ".", "tau", ".", "reshape", "(", "v_shape", ")", ".", "to", "(", "dtype", "=", "x_dtype", ")", ")", "if", "self", ".", "tau", "is", "not", "None", "else", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.filter_response_norm.FilterResponseNormAct2d.__init__": [[46, 57], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "filter_response_norm.FilterResponseNormAct2d.reset_parameters", "create_act.create_act_layer", "torch.Identity", "torch.Identity", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.create_act_layer"], ["    ", "def", "__init__", "(", "self", ",", "num_features", ",", "apply_act", "=", "True", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "inplace", "=", "None", ",", "rms", "=", "True", ",", "eps", "=", "1e-5", ",", "**", "_", ")", ":", "\n", "        ", "super", "(", "FilterResponseNormAct2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "act_layer", "is", "not", "None", "and", "apply_act", ":", "\n", "            ", "self", ".", "act", "=", "create_act_layer", "(", "act_layer", ",", "inplace", "=", "inplace", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "act", "=", "nn", ".", "Identity", "(", ")", "\n", "", "self", ".", "rms", "=", "rms", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "num_features", ")", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "num_features", ")", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.filter_response_norm.FilterResponseNormAct2d.reset_parameters": [[58, 61], ["torch.init.ones_", "torch.init.ones_", "torch.init.zeros_", "torch.init.zeros_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "ones_", "(", "self", ".", "weight", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.filter_response_norm.FilterResponseNormAct2d.forward": [[62, 69], ["trace_utils._assert", "filter_response_norm.FilterResponseNormAct2d.act", "filter_response_norm.inv_instance_rms", "filter_response_norm.FilterResponseNormAct2d.bias.view().to", "x.dim", "filter_response_norm.FilterResponseNormAct2d.weight.view().to", "filter_response_norm.FilterResponseNormAct2d.bias.view", "filter_response_norm.FilterResponseNormAct2d.weight.view"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.filter_response_norm.inv_instance_rms"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "_assert", "(", "x", ".", "dim", "(", ")", "==", "4", ",", "'expected 4D input'", ")", "\n", "x_dtype", "=", "x", ".", "dtype", "\n", "v_shape", "=", "(", "1", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "x", "=", "x", "*", "inv_instance_rms", "(", "x", ",", "self", ".", "eps", ")", "\n", "x", "=", "x", "*", "self", ".", "weight", ".", "view", "(", "v_shape", ")", ".", "to", "(", "dtype", "=", "x_dtype", ")", "+", "self", ".", "bias", ".", "view", "(", "v_shape", ")", ".", "to", "(", "dtype", "=", "x_dtype", ")", "\n", "return", "self", ".", "act", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.filter_response_norm.inv_instance_rms": [[14, 17], ["x.square().float().mean().add().rsqrt().to", "x.square().float().mean().add().rsqrt().to.expand", "x.square().float().mean().add().rsqrt", "x.square().float().mean().add", "x.square().float().mean", "x.square().float", "x.square"], "function", ["None"], ["def", "inv_instance_rms", "(", "x", ",", "eps", ":", "float", "=", "1e-5", ")", ":", "\n", "    ", "rms", "=", "x", ".", "square", "(", ")", ".", "float", "(", ")", ".", "mean", "(", "dim", "=", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", ".", "add", "(", "eps", ")", ".", "rsqrt", "(", ")", ".", "to", "(", "x", ".", "dtype", ")", "\n", "return", "rms", ".", "expand", "(", "x", ".", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.AvgPool2dSame.__init__": [[24, 28], ["helpers.to_2tuple", "helpers.to_2tuple", "torch.AvgPool2d.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "kernel_size", ":", "int", ",", "stride", "=", "None", ",", "padding", "=", "0", ",", "ceil_mode", "=", "False", ",", "count_include_pad", "=", "True", ")", ":", "\n", "        ", "kernel_size", "=", "to_2tuple", "(", "kernel_size", ")", "\n", "stride", "=", "to_2tuple", "(", "stride", ")", "\n", "super", "(", "AvgPool2dSame", ",", "self", ")", ".", "__init__", "(", "kernel_size", ",", "stride", ",", "(", "0", ",", "0", ")", ",", "ceil_mode", ",", "count_include_pad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.AvgPool2dSame.forward": [[29, 33], ["padding.pad_same", "torch.avg_pool2d", "torch.avg_pool2d", "torch.avg_pool2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.pad_same"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "pad_same", "(", "x", ",", "self", ".", "kernel_size", ",", "self", ".", "stride", ")", "\n", "return", "F", ".", "avg_pool2d", "(", "\n", "x", ",", "self", ".", "kernel_size", ",", "self", ".", "stride", ",", "self", ".", "padding", ",", "self", ".", "ceil_mode", ",", "self", ".", "count_include_pad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.MaxPool2dSame.__init__": [[45, 50], ["helpers.to_2tuple", "helpers.to_2tuple", "helpers.to_2tuple", "torch.MaxPool2d.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "kernel_size", ":", "int", ",", "stride", "=", "None", ",", "padding", "=", "0", ",", "dilation", "=", "1", ",", "ceil_mode", "=", "False", ")", ":", "\n", "        ", "kernel_size", "=", "to_2tuple", "(", "kernel_size", ")", "\n", "stride", "=", "to_2tuple", "(", "stride", ")", "\n", "dilation", "=", "to_2tuple", "(", "dilation", ")", "\n", "super", "(", "MaxPool2dSame", ",", "self", ")", ".", "__init__", "(", "kernel_size", ",", "stride", ",", "(", "0", ",", "0", ")", ",", "dilation", ",", "ceil_mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.MaxPool2dSame.forward": [[51, 54], ["padding.pad_same", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "float"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.pad_same"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "pad_same", "(", "x", ",", "self", ".", "kernel_size", ",", "self", ".", "stride", ",", "value", "=", "-", "float", "(", "'inf'", ")", ")", "\n", "return", "F", ".", "max_pool2d", "(", "x", ",", "self", ".", "kernel_size", ",", "self", ".", "stride", ",", "(", "0", ",", "0", ")", ",", "self", ".", "dilation", ",", "self", ".", "ceil_mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.avg_pool2d_same": [[14, 19], ["padding.pad_same", "torch.avg_pool2d"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.pad_same"], ["def", "avg_pool2d_same", "(", "x", ",", "kernel_size", ":", "List", "[", "int", "]", ",", "stride", ":", "List", "[", "int", "]", ",", "padding", ":", "List", "[", "int", "]", "=", "(", "0", ",", "0", ")", ",", "\n", "ceil_mode", ":", "bool", "=", "False", ",", "count_include_pad", ":", "bool", "=", "True", ")", ":", "\n", "# FIXME how to deal with count_include_pad vs not for external padding?", "\n", "    ", "x", "=", "pad_same", "(", "x", ",", "kernel_size", ",", "stride", ")", "\n", "return", "F", ".", "avg_pool2d", "(", "x", ",", "kernel_size", ",", "stride", ",", "(", "0", ",", "0", ")", ",", "ceil_mode", ",", "count_include_pad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.max_pool2d_same": [[35, 40], ["padding.pad_same", "torch.max_pool2d", "float"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.pad_same"], ["", "", "def", "max_pool2d_same", "(", "\n", "x", ",", "kernel_size", ":", "List", "[", "int", "]", ",", "stride", ":", "List", "[", "int", "]", ",", "padding", ":", "List", "[", "int", "]", "=", "(", "0", ",", "0", ")", ",", "\n", "dilation", ":", "List", "[", "int", "]", "=", "(", "1", ",", "1", ")", ",", "ceil_mode", ":", "bool", "=", "False", ")", ":", "\n", "    ", "x", "=", "pad_same", "(", "x", ",", "kernel_size", ",", "stride", ",", "value", "=", "-", "float", "(", "'inf'", ")", ")", "\n", "return", "F", ".", "max_pool2d", "(", "x", ",", "kernel_size", ",", "stride", ",", "(", "0", ",", "0", ")", ",", "dilation", ",", "ceil_mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pool2d_same.create_pool2d": [[56, 74], ["kwargs.pop", "padding.get_padding_value", "pool2d_same.AvgPool2dSame", "torch.AvgPool2d", "pool2d_same.MaxPool2dSame", "torch.MaxPool2d"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.get_padding_value"], ["", "", "def", "create_pool2d", "(", "pool_type", ",", "kernel_size", ",", "stride", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "stride", "=", "stride", "or", "kernel_size", "\n", "padding", "=", "kwargs", ".", "pop", "(", "'padding'", ",", "''", ")", "\n", "padding", ",", "is_dynamic", "=", "get_padding_value", "(", "padding", ",", "kernel_size", ",", "stride", "=", "stride", ",", "**", "kwargs", ")", "\n", "if", "is_dynamic", ":", "\n", "        ", "if", "pool_type", "==", "'avg'", ":", "\n", "            ", "return", "AvgPool2dSame", "(", "kernel_size", ",", "stride", "=", "stride", ",", "**", "kwargs", ")", "\n", "", "elif", "pool_type", "==", "'max'", ":", "\n", "            ", "return", "MaxPool2dSame", "(", "kernel_size", ",", "stride", "=", "stride", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "f'Unsupported pool type {pool_type}'", "\n", "", "", "else", ":", "\n", "        ", "if", "pool_type", "==", "'avg'", ":", "\n", "            ", "return", "nn", ".", "AvgPool2d", "(", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "**", "kwargs", ")", "\n", "", "elif", "pool_type", "==", "'max'", ":", "\n", "            ", "return", "nn", ".", "MaxPool2d", "(", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "f'Unsupported pool type {pool_type}'", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.Mlp.__init__": [[13, 25], ["torch.nn.Module.__init__", "helpers.to_2tuple", "helpers.to_2tuple", "torch.nn.Linear", "act_layer", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "in_features", ",", "hidden_features", "=", "None", ",", "out_features", "=", "None", ",", "act_layer", "=", "nn", ".", "GELU", ",", "bias", "=", "True", ",", "drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "out_features", "=", "out_features", "or", "in_features", "\n", "hidden_features", "=", "hidden_features", "or", "in_features", "\n", "bias", "=", "to_2tuple", "(", "bias", ")", "\n", "drop_probs", "=", "to_2tuple", "(", "drop", ")", "\n", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "in_features", ",", "hidden_features", ",", "bias", "=", "bias", "[", "0", "]", ")", "\n", "self", ".", "act", "=", "act_layer", "(", ")", "\n", "self", ".", "drop1", "=", "nn", ".", "Dropout", "(", "drop_probs", "[", "0", "]", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "hidden_features", ",", "out_features", ",", "bias", "=", "bias", "[", "1", "]", ")", "\n", "self", ".", "drop2", "=", "nn", ".", "Dropout", "(", "drop_probs", "[", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.Mlp.forward": [[26, 33], ["mlp.Mlp.fc1", "mlp.Mlp.act", "mlp.Mlp.drop1", "mlp.Mlp.fc2", "mlp.Mlp.drop2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "x", "=", "self", ".", "drop1", "(", "x", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "self", ".", "drop2", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.__init__": [[39, 52], ["torch.nn.Module.__init__", "helpers.to_2tuple", "helpers.to_2tuple", "torch.nn.Linear", "act_layer", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "in_features", ",", "hidden_features", "=", "None", ",", "out_features", "=", "None", ",", "act_layer", "=", "nn", ".", "Sigmoid", ",", "bias", "=", "True", ",", "drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "out_features", "=", "out_features", "or", "in_features", "\n", "hidden_features", "=", "hidden_features", "or", "in_features", "\n", "assert", "hidden_features", "%", "2", "==", "0", "\n", "bias", "=", "to_2tuple", "(", "bias", ")", "\n", "drop_probs", "=", "to_2tuple", "(", "drop", ")", "\n", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "in_features", ",", "hidden_features", ",", "bias", "=", "bias", "[", "0", "]", ")", "\n", "self", ".", "act", "=", "act_layer", "(", ")", "\n", "self", ".", "drop1", "=", "nn", ".", "Dropout", "(", "drop_probs", "[", "0", "]", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "hidden_features", "//", "2", ",", "out_features", ",", "bias", "=", "bias", "[", "1", "]", ")", "\n", "self", ".", "drop2", "=", "nn", ".", "Dropout", "(", "drop_probs", "[", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.init_weights": [[53, 58], ["torch.nn.init.ones_", "torch.nn.init.normal_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "# override init of fc1 w/ gate portion set to weight near zero, bias=1", "\n", "        ", "fc1_mid", "=", "self", ".", "fc1", ".", "bias", ".", "shape", "[", "0", "]", "//", "2", "\n", "nn", ".", "init", ".", "ones_", "(", "self", ".", "fc1", ".", "bias", "[", "fc1_mid", ":", "]", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "fc1", ".", "weight", "[", "fc1_mid", ":", "]", ",", "std", "=", "1e-6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GluMlp.forward": [[59, 67], ["mlp.GluMlp.fc1", "mlp.GluMlp.chunk", "mlp.GluMlp.drop1", "mlp.GluMlp.fc2", "mlp.GluMlp.drop2", "mlp.GluMlp.act"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "x", ",", "gates", "=", "x", ".", "chunk", "(", "2", ",", "dim", "=", "-", "1", ")", "\n", "x", "=", "x", "*", "self", ".", "act", "(", "gates", ")", "\n", "x", "=", "self", ".", "drop1", "(", "x", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "self", ".", "drop2", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GatedMlp.__init__": [[72, 92], ["torch.nn.Module.__init__", "helpers.to_2tuple", "helpers.to_2tuple", "torch.nn.Linear", "act_layer", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Dropout", "gate_layer", "torch.nn.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "in_features", ",", "hidden_features", "=", "None", ",", "out_features", "=", "None", ",", "act_layer", "=", "nn", ".", "GELU", ",", "\n", "gate_layer", "=", "None", ",", "bias", "=", "True", ",", "drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "out_features", "=", "out_features", "or", "in_features", "\n", "hidden_features", "=", "hidden_features", "or", "in_features", "\n", "bias", "=", "to_2tuple", "(", "bias", ")", "\n", "drop_probs", "=", "to_2tuple", "(", "drop", ")", "\n", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "in_features", ",", "hidden_features", ",", "bias", "=", "bias", "[", "0", "]", ")", "\n", "self", ".", "act", "=", "act_layer", "(", ")", "\n", "self", ".", "drop1", "=", "nn", ".", "Dropout", "(", "drop_probs", "[", "0", "]", ")", "\n", "if", "gate_layer", "is", "not", "None", ":", "\n", "            ", "assert", "hidden_features", "%", "2", "==", "0", "\n", "self", ".", "gate", "=", "gate_layer", "(", "hidden_features", ")", "\n", "hidden_features", "=", "hidden_features", "//", "2", "# FIXME base reduction on gate property?", "\n", "", "else", ":", "\n", "            ", "self", ".", "gate", "=", "nn", ".", "Identity", "(", ")", "\n", "", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "hidden_features", ",", "out_features", ",", "bias", "=", "bias", "[", "1", "]", ")", "\n", "self", ".", "drop2", "=", "nn", ".", "Dropout", "(", "drop_probs", "[", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.GatedMlp.forward": [[93, 101], ["mlp.GatedMlp.fc1", "mlp.GatedMlp.act", "mlp.GatedMlp.drop1", "mlp.GatedMlp.gate", "mlp.GatedMlp.fc2", "mlp.GatedMlp.drop2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "x", "=", "self", ".", "drop1", "(", "x", ")", "\n", "x", "=", "self", ".", "gate", "(", "x", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "self", ".", "drop2", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.ConvMlp.__init__": [[106, 119], ["torch.nn.Module.__init__", "helpers.to_2tuple", "torch.nn.Conv2d", "act_layer", "torch.nn.Dropout", "torch.nn.Conv2d", "norm_layer", "torch.nn.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "in_features", ",", "hidden_features", "=", "None", ",", "out_features", "=", "None", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "\n", "norm_layer", "=", "None", ",", "bias", "=", "True", ",", "drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "out_features", "=", "out_features", "or", "in_features", "\n", "hidden_features", "=", "hidden_features", "or", "in_features", "\n", "bias", "=", "to_2tuple", "(", "bias", ")", "\n", "\n", "self", ".", "fc1", "=", "nn", ".", "Conv2d", "(", "in_features", ",", "hidden_features", ",", "kernel_size", "=", "1", ",", "bias", "=", "bias", "[", "0", "]", ")", "\n", "self", ".", "norm", "=", "norm_layer", "(", "hidden_features", ")", "if", "norm_layer", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "act", "=", "act_layer", "(", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "drop", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Conv2d", "(", "hidden_features", ",", "out_features", ",", "kernel_size", "=", "1", ",", "bias", "=", "bias", "[", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.mlp.ConvMlp.forward": [[120, 127], ["mlp.ConvMlp.fc1", "mlp.ConvMlp.norm", "mlp.ConvMlp.act", "mlp.ConvMlp.drop", "mlp.ConvMlp.fc2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "x", "=", "self", ".", "drop", "(", "x", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.split_attn.RadixSoftmax.__init__": [[17, 21], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "radix", ",", "cardinality", ")", ":", "\n", "        ", "super", "(", "RadixSoftmax", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "radix", "=", "radix", "\n", "self", ".", "cardinality", "=", "cardinality", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.split_attn.RadixSoftmax.forward": [[22, 31], ["torch.sigmoid.size", "torch.sigmoid.size", "torch.sigmoid.view().transpose", "torch.sigmoid.view().transpose", "torch.softmax", "torch.softmax", "torch.sigmoid.reshape", "torch.sigmoid.reshape", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid.view", "torch.sigmoid.view"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "batch", "=", "x", ".", "size", "(", "0", ")", "\n", "if", "self", ".", "radix", ">", "1", ":", "\n", "            ", "x", "=", "x", ".", "view", "(", "batch", ",", "self", ".", "cardinality", ",", "self", ".", "radix", ",", "-", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "x", "=", "F", ".", "softmax", "(", "x", ",", "dim", "=", "1", ")", "\n", "x", "=", "x", ".", "reshape", "(", "batch", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "torch", ".", "sigmoid", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.split_attn.SplitAttn.__init__": [[36, 60], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "act_layer", "torch.nn.Conv2d", "torch.nn.Conv2d", "act_layer", "torch.nn.Conv2d", "torch.nn.Conv2d", "split_attn.RadixSoftmax", "helpers.make_divisible", "norm_layer", "torch.nn.Identity", "torch.nn.Identity", "drop_layer", "torch.nn.Identity", "torch.nn.Identity", "norm_layer", "torch.nn.Identity", "torch.nn.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", "=", "None", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "None", ",", "\n", "dilation", "=", "1", ",", "groups", "=", "1", ",", "bias", "=", "False", ",", "radix", "=", "2", ",", "rd_ratio", "=", "0.25", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "8", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "None", ",", "drop_layer", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "SplitAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "out_channels", "=", "out_channels", "or", "in_channels", "\n", "self", ".", "radix", "=", "radix", "\n", "mid_chs", "=", "out_channels", "*", "radix", "\n", "if", "rd_channels", "is", "None", ":", "\n", "            ", "attn_chs", "=", "make_divisible", "(", "in_channels", "*", "radix", "*", "rd_ratio", ",", "min_value", "=", "32", ",", "divisor", "=", "rd_divisor", ")", "\n", "", "else", ":", "\n", "            ", "attn_chs", "=", "rd_channels", "*", "radix", "\n", "\n", "", "padding", "=", "kernel_size", "//", "2", "if", "padding", "is", "None", "else", "padding", "\n", "self", ".", "conv", "=", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "mid_chs", ",", "kernel_size", ",", "stride", ",", "padding", ",", "dilation", ",", "\n", "groups", "=", "groups", "*", "radix", ",", "bias", "=", "bias", ",", "**", "kwargs", ")", "\n", "self", ".", "bn0", "=", "norm_layer", "(", "mid_chs", ")", "if", "norm_layer", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "drop", "=", "drop_layer", "(", ")", "if", "drop_layer", "is", "not", "None", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "act0", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Conv2d", "(", "out_channels", ",", "attn_chs", ",", "1", ",", "groups", "=", "groups", ")", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "attn_chs", ")", "if", "norm_layer", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "act1", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Conv2d", "(", "attn_chs", ",", "mid_chs", ",", "1", ",", "groups", "=", "groups", ")", "\n", "self", ".", "rsoftmax", "=", "RadixSoftmax", "(", "radix", ",", "groups", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.split_attn.SplitAttn.forward": [[61, 85], ["split_attn.SplitAttn.conv", "split_attn.SplitAttn.bn0", "split_attn.SplitAttn.drop", "split_attn.SplitAttn.act0", "x.reshape.sum.mean", "split_attn.SplitAttn.fc1", "split_attn.SplitAttn.bn1", "split_attn.SplitAttn.act1", "split_attn.SplitAttn.fc2", "split_attn.SplitAttn.rsoftmax().view", "out.contiguous", "x.reshape.reshape.reshape", "x.reshape.reshape.sum", "split_attn.SplitAttn.rsoftmax", "split_attn.SplitAttn.reshape"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "x", "=", "self", ".", "bn0", "(", "x", ")", "\n", "x", "=", "self", ".", "drop", "(", "x", ")", "\n", "x", "=", "self", ".", "act0", "(", "x", ")", "\n", "\n", "B", ",", "RC", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "if", "self", ".", "radix", ">", "1", ":", "\n", "            ", "x", "=", "x", ".", "reshape", "(", "(", "B", ",", "self", ".", "radix", ",", "RC", "//", "self", ".", "radix", ",", "H", ",", "W", ")", ")", "\n", "x_gap", "=", "x", ".", "sum", "(", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "x_gap", "=", "x", "\n", "", "x_gap", "=", "x_gap", ".", "mean", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "x_gap", "=", "self", ".", "fc1", "(", "x_gap", ")", "\n", "x_gap", "=", "self", ".", "bn1", "(", "x_gap", ")", "\n", "x_gap", "=", "self", ".", "act1", "(", "x_gap", ")", "\n", "x_attn", "=", "self", ".", "fc2", "(", "x_gap", ")", "\n", "\n", "x_attn", "=", "self", ".", "rsoftmax", "(", "x_attn", ")", ".", "view", "(", "B", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "if", "self", ".", "radix", ">", "1", ":", "\n", "            ", "out", "=", "(", "x", "*", "x_attn", ".", "reshape", "(", "(", "B", ",", "self", ".", "radix", ",", "RC", "//", "self", ".", "radix", ",", "1", ",", "1", ")", ")", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "x", "*", "x_attn", "\n", "", "return", "out", ".", "contiguous", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.patch_embed.PatchEmbed.__init__": [[18, 30], ["torch.nn.Module.__init__", "helpers.to_2tuple", "helpers.to_2tuple", "torch.nn.Conv2d", "norm_layer", "torch.nn.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "embed_dim", "=", "768", ",", "norm_layer", "=", "None", ",", "flatten", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "img_size", "=", "to_2tuple", "(", "img_size", ")", "\n", "patch_size", "=", "to_2tuple", "(", "patch_size", ")", "\n", "self", ".", "img_size", "=", "img_size", "\n", "self", ".", "patch_size", "=", "patch_size", "\n", "self", ".", "grid_size", "=", "(", "img_size", "[", "0", "]", "//", "patch_size", "[", "0", "]", ",", "img_size", "[", "1", "]", "//", "patch_size", "[", "1", "]", ")", "\n", "self", ".", "num_patches", "=", "self", ".", "grid_size", "[", "0", "]", "*", "self", ".", "grid_size", "[", "1", "]", "\n", "self", ".", "flatten", "=", "flatten", "\n", "\n", "self", ".", "proj", "=", "nn", ".", "Conv2d", "(", "in_chans", ",", "embed_dim", ",", "kernel_size", "=", "patch_size", ",", "stride", "=", "patch_size", ")", "\n", "self", ".", "norm", "=", "norm_layer", "(", "embed_dim", ")", "if", "norm_layer", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.patch_embed.PatchEmbed.forward": [[31, 40], ["trace_utils._assert", "trace_utils._assert", "patch_embed.PatchEmbed.proj", "patch_embed.PatchEmbed.norm", "x.flatten().transpose.flatten().transpose.flatten().transpose", "x.flatten().transpose.flatten().transpose.flatten"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "_assert", "(", "H", "==", "self", ".", "img_size", "[", "0", "]", ",", "f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\"", ")", "\n", "_assert", "(", "W", "==", "self", ".", "img_size", "[", "1", "]", ",", "f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\"", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "if", "self", ".", "flatten", ":", "\n", "            ", "x", "=", "x", ".", "flatten", "(", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "# BCHW -> BNC", "\n", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.FastAdaptiveAvgPool2d.__init__": [[53, 56], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "flatten", "=", "False", ")", ":", "\n", "        ", "super", "(", "FastAdaptiveAvgPool2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "flatten", "=", "flatten", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.FastAdaptiveAvgPool2d.forward": [[57, 59], ["x.mean"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", ".", "mean", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "not", "self", ".", "flatten", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.AdaptiveAvgMaxPool2d.__init__": [[62, 65], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "output_size", "=", "1", ")", ":", "\n", "        ", "super", "(", "AdaptiveAvgMaxPool2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "output_size", "=", "output_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.AdaptiveAvgMaxPool2d.forward": [[66, 68], ["adaptive_avgmax_pool.adaptive_avgmax_pool2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.adaptive_avgmax_pool2d"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "adaptive_avgmax_pool2d", "(", "x", ",", "self", ".", "output_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.AdaptiveCatAvgMaxPool2d.__init__": [[71, 74], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "output_size", "=", "1", ")", ":", "\n", "        ", "super", "(", "AdaptiveCatAvgMaxPool2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "output_size", "=", "output_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.AdaptiveCatAvgMaxPool2d.forward": [[75, 77], ["adaptive_avgmax_pool.adaptive_catavgmax_pool2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.adaptive_catavgmax_pool2d"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "adaptive_catavgmax_pool2d", "(", "x", ",", "self", ".", "output_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.SelectAdaptivePool2d.__init__": [[82, 102], ["torch.Module.__init__", "torch.Flatten", "torch.Flatten", "torch.Flatten", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "adaptive_avgmax_pool.FastAdaptiveAvgPool2d", "torch.Identity", "torch.Identity", "torch.Identity", "torch.AdaptiveAvgPool2d", "torch.AdaptiveAvgPool2d", "torch.AdaptiveAvgPool2d", "adaptive_avgmax_pool.AdaptiveAvgMaxPool2d", "adaptive_avgmax_pool.AdaptiveCatAvgMaxPool2d", "torch.AdaptiveMaxPool2d", "torch.AdaptiveMaxPool2d", "torch.AdaptiveMaxPool2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "output_size", "=", "1", ",", "pool_type", "=", "'fast'", ",", "flatten", "=", "False", ")", ":", "\n", "        ", "super", "(", "SelectAdaptivePool2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "pool_type", "=", "pool_type", "or", "''", "# convert other falsy values to empty string for consistent TS typing", "\n", "self", ".", "flatten", "=", "nn", ".", "Flatten", "(", "1", ")", "if", "flatten", "else", "nn", ".", "Identity", "(", ")", "\n", "if", "pool_type", "==", "''", ":", "\n", "            ", "self", ".", "pool", "=", "nn", ".", "Identity", "(", ")", "# pass through", "\n", "", "elif", "pool_type", "==", "'fast'", ":", "\n", "            ", "assert", "output_size", "==", "1", "\n", "self", ".", "pool", "=", "FastAdaptiveAvgPool2d", "(", "flatten", ")", "\n", "self", ".", "flatten", "=", "nn", ".", "Identity", "(", ")", "\n", "", "elif", "pool_type", "==", "'avg'", ":", "\n", "            ", "self", ".", "pool", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "output_size", ")", "\n", "", "elif", "pool_type", "==", "'avgmax'", ":", "\n", "            ", "self", ".", "pool", "=", "AdaptiveAvgMaxPool2d", "(", "output_size", ")", "\n", "", "elif", "pool_type", "==", "'catavgmax'", ":", "\n", "            ", "self", ".", "pool", "=", "AdaptiveCatAvgMaxPool2d", "(", "output_size", ")", "\n", "", "elif", "pool_type", "==", "'max'", ":", "\n", "            ", "self", ".", "pool", "=", "nn", ".", "AdaptiveMaxPool2d", "(", "output_size", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "'Invalid pool type: %s'", "%", "pool_type", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.SelectAdaptivePool2d.is_identity": [[103, 105], ["None"], "methods", ["None"], ["", "", "def", "is_identity", "(", "self", ")", ":", "\n", "        ", "return", "not", "self", ".", "pool_type", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.SelectAdaptivePool2d.forward": [[106, 110], ["adaptive_avgmax_pool.SelectAdaptivePool2d.pool", "adaptive_avgmax_pool.SelectAdaptivePool2d.flatten"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "pool", "(", "x", ")", "\n", "x", "=", "self", ".", "flatten", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.SelectAdaptivePool2d.feat_mult": [[111, 113], ["adaptive_avgmax_pool.adaptive_pool_feat_mult"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.adaptive_pool_feat_mult"], ["", "def", "feat_mult", "(", "self", ")", ":", "\n", "        ", "return", "adaptive_pool_feat_mult", "(", "self", ".", "pool_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.SelectAdaptivePool2d.__repr__": [[114, 118], ["str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "' ('", "+", "'pool_type='", "+", "self", ".", "pool_type", "+", "', flatten='", "+", "str", "(", "self", ".", "flatten", ")", "+", "')'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.adaptive_pool_feat_mult": [[17, 22], ["None"], "function", ["None"], ["def", "adaptive_pool_feat_mult", "(", "pool_type", "=", "'avg'", ")", ":", "\n", "    ", "if", "pool_type", "==", "'catavgmax'", ":", "\n", "        ", "return", "2", "\n", "", "else", ":", "\n", "        ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.adaptive_avgmax_pool2d": [[24, 28], ["torch.adaptive_avg_pool2d", "torch.adaptive_max_pool2d"], "function", ["None"], ["", "", "def", "adaptive_avgmax_pool2d", "(", "x", ",", "output_size", "=", "1", ")", ":", "\n", "    ", "x_avg", "=", "F", ".", "adaptive_avg_pool2d", "(", "x", ",", "output_size", ")", "\n", "x_max", "=", "F", ".", "adaptive_max_pool2d", "(", "x", ",", "output_size", ")", "\n", "return", "0.5", "*", "(", "x_avg", "+", "x_max", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.adaptive_catavgmax_pool2d": [[30, 34], ["torch.adaptive_avg_pool2d", "torch.adaptive_max_pool2d", "torch.cat", "torch.cat", "torch.cat"], "function", ["None"], ["", "def", "adaptive_catavgmax_pool2d", "(", "x", ",", "output_size", "=", "1", ")", ":", "\n", "    ", "x_avg", "=", "F", ".", "adaptive_avg_pool2d", "(", "x", ",", "output_size", ")", "\n", "x_max", "=", "F", ".", "adaptive_max_pool2d", "(", "x", ",", "output_size", ")", "\n", "return", "torch", ".", "cat", "(", "(", "x_avg", ",", "x_max", ")", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.select_adaptive_pool2d": [[36, 50], ["torch.adaptive_avg_pool2d", "adaptive_avgmax_pool.adaptive_avgmax_pool2d", "adaptive_avgmax_pool.adaptive_catavgmax_pool2d", "torch.adaptive_max_pool2d"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.adaptive_avgmax_pool2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.adaptive_catavgmax_pool2d"], ["", "def", "select_adaptive_pool2d", "(", "x", ",", "pool_type", "=", "'avg'", ",", "output_size", "=", "1", ")", ":", "\n", "    ", "\"\"\"Selectable global pooling function with dynamic input kernel size\n    \"\"\"", "\n", "if", "pool_type", "==", "'avg'", ":", "\n", "        ", "x", "=", "F", ".", "adaptive_avg_pool2d", "(", "x", ",", "output_size", ")", "\n", "", "elif", "pool_type", "==", "'avgmax'", ":", "\n", "        ", "x", "=", "adaptive_avgmax_pool2d", "(", "x", ",", "output_size", ")", "\n", "", "elif", "pool_type", "==", "'catavgmax'", ":", "\n", "        ", "x", "=", "adaptive_catavgmax_pool2d", "(", "x", ",", "output_size", ")", "\n", "", "elif", "pool_type", "==", "'max'", ":", "\n", "        ", "x", "=", "F", ".", "adaptive_max_pool2d", "(", "x", ",", "output_size", ")", "\n", "", "else", ":", "\n", "        ", "assert", "False", ",", "'Invalid pool type: %s'", "%", "pool_type", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers._ntuple": [[10, 16], ["isinstance", "tuple", "itertools.repeat"], "function", ["None"], ["from", "collections", "import", "OrderedDict", ",", "defaultdict", "\n", "from", "copy", "import", "deepcopy", "\n", "from", "itertools", "import", "chain", "\n", "from", "typing", "import", "Any", ",", "Callable", ",", "Optional", ",", "Tuple", ",", "Dict", ",", "Union", "\n", "\n", "import", "torch", "\n", "import", "torch", ".", "nn", "as", "nn", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible": [[25, 32], ["max", "int"], "function", ["None"], ["\n", "\n", "_logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "\n", "\n", "# Global variables for rarely used pretrained checkpoint download progress and hash check.", "\n", "# Use set_pretrained_download_progress / set_pretrained_check_hash functions to toggle.", "\n", "_DOWNLOAD_PROGRESS", "=", "False", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.separable_conv.SeparableConvNormAct.__init__": [[17, 32], ["torch.nn.Module.__init__", "create_conv2d.create_conv2d.create_conv2d", "create_conv2d.create_conv2d.create_conv2d", "create_norm_act.get_norm_act_layer", "create_norm_act.get_norm_act_layer.", "int", "int", "dict"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.get_norm_act_layer"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "padding", "=", "''", ",", "bias", "=", "False", ",", "\n", "channel_multiplier", "=", "1.0", ",", "pw_kernel_size", "=", "1", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "\n", "apply_act", "=", "True", ",", "drop_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "SeparableConvNormAct", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "conv_dw", "=", "create_conv2d", "(", "\n", "in_channels", ",", "int", "(", "in_channels", "*", "channel_multiplier", ")", ",", "kernel_size", ",", "\n", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "padding", "=", "padding", ",", "depthwise", "=", "True", ")", "\n", "\n", "self", ".", "conv_pw", "=", "create_conv2d", "(", "\n", "int", "(", "in_channels", "*", "channel_multiplier", ")", ",", "out_channels", ",", "pw_kernel_size", ",", "padding", "=", "padding", ",", "bias", "=", "bias", ")", "\n", "\n", "norm_act_layer", "=", "get_norm_act_layer", "(", "norm_layer", ",", "act_layer", ")", "\n", "norm_kwargs", "=", "dict", "(", "drop_layer", "=", "drop_layer", ")", "if", "drop_layer", "is", "not", "None", "else", "{", "}", "\n", "self", ".", "bn", "=", "norm_act_layer", "(", "out_channels", ",", "apply_act", "=", "apply_act", ",", "**", "norm_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.separable_conv.SeparableConvNormAct.in_channels": [[33, 36], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "in_channels", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "conv_dw", ".", "in_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.separable_conv.SeparableConvNormAct.out_channels": [[37, 40], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "out_channels", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "conv_pw", ".", "out_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.separable_conv.SeparableConvNormAct.forward": [[41, 46], ["separable_conv.SeparableConvNormAct.conv_dw", "separable_conv.SeparableConvNormAct.conv_pw", "separable_conv.SeparableConvNormAct.bn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv_dw", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_pw", "(", "x", ")", "\n", "x", "=", "self", ".", "bn", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.separable_conv.SeparableConv2d.__init__": [[54, 64], ["torch.nn.Module.__init__", "create_conv2d.create_conv2d.create_conv2d", "create_conv2d.create_conv2d.create_conv2d", "int", "int"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "padding", "=", "''", ",", "bias", "=", "False", ",", "\n", "channel_multiplier", "=", "1.0", ",", "pw_kernel_size", "=", "1", ")", ":", "\n", "        ", "super", "(", "SeparableConv2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "conv_dw", "=", "create_conv2d", "(", "\n", "in_channels", ",", "int", "(", "in_channels", "*", "channel_multiplier", ")", ",", "kernel_size", ",", "\n", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "padding", "=", "padding", ",", "depthwise", "=", "True", ")", "\n", "\n", "self", ".", "conv_pw", "=", "create_conv2d", "(", "\n", "int", "(", "in_channels", "*", "channel_multiplier", ")", ",", "out_channels", ",", "pw_kernel_size", ",", "padding", "=", "padding", ",", "bias", "=", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.separable_conv.SeparableConv2d.in_channels": [[65, 68], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "in_channels", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "conv_dw", ".", "in_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.separable_conv.SeparableConv2d.out_channels": [[69, 72], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "out_channels", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "conv_pw", ".", "out_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.separable_conv.SeparableConv2d.forward": [[73, 77], ["separable_conv.SeparableConv2d.conv_dw", "separable_conv.SeparableConv2d.conv_pw"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv_dw", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_pw", "(", "x", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init._no_grad_trunc_normal_": [[8, 42], ["warnings.warn", "torch.no_grad", "weight_init._no_grad_trunc_normal_.norm_cdf"], "function", ["None"], ["def", "_no_grad_trunc_normal_", "(", "tensor", ",", "mean", ",", "std", ",", "a", ",", "b", ")", ":", "\n", "# Cut & paste from PyTorch official master until it's in a few official releases - RW", "\n", "# Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf", "\n", "    ", "def", "norm_cdf", "(", "x", ")", ":", "\n", "# Computes standard normal cumulative distribution function", "\n", "        ", "return", "(", "1.", "+", "math", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.", ")", ")", ")", "/", "2.", "\n", "\n", "", "if", "(", "mean", "<", "a", "-", "2", "*", "std", ")", "or", "(", "mean", ">", "b", "+", "2", "*", "std", ")", ":", "\n", "        ", "warnings", ".", "warn", "(", "\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"", "\n", "\"The distribution of values may be incorrect.\"", ",", "\n", "stacklevel", "=", "2", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# Values are generated by using a truncated uniform distribution and", "\n", "# then using the inverse CDF for the normal distribution.", "\n", "# Get upper and lower cdf values", "\n", "        ", "l", "=", "norm_cdf", "(", "(", "a", "-", "mean", ")", "/", "std", ")", "\n", "u", "=", "norm_cdf", "(", "(", "b", "-", "mean", ")", "/", "std", ")", "\n", "\n", "# Uniformly fill tensor with values from [l, u], then translate to", "\n", "# [2l-1, 2u-1].", "\n", "tensor", ".", "uniform_", "(", "2", "*", "l", "-", "1", ",", "2", "*", "u", "-", "1", ")", "\n", "\n", "# Use inverse cdf transform for normal distribution to get truncated", "\n", "# standard normal", "\n", "tensor", ".", "erfinv_", "(", ")", "\n", "\n", "# Transform to proper mean, std", "\n", "tensor", ".", "mul_", "(", "std", "*", "math", ".", "sqrt", "(", "2.", ")", ")", "\n", "tensor", ".", "add_", "(", "mean", ")", "\n", "\n", "# Clamp to ensure it's in the proper range", "\n", "tensor", ".", "clamp_", "(", "min", "=", "a", ",", "max", "=", "b", ")", "\n", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_": [[44, 63], ["weight_init._no_grad_trunc_normal_"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init._no_grad_trunc_normal_"], ["", "", "def", "trunc_normal_", "(", "tensor", ",", "mean", "=", "0.", ",", "std", "=", "1.", ",", "a", "=", "-", "2.", ",", "b", "=", "2.", ")", ":", "\n", "# type: (Tensor, float, float, float, float) -> Tensor", "\n", "    ", "r\"\"\"Fills the input Tensor with values drawn from a truncated\n    normal distribution. The values are effectively drawn from the\n    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n    with values outside :math:`[a, b]` redrawn until they are within\n    the bounds. The method used for generating the random values works\n    best when :math:`a \\leq \\text{mean} \\leq b`.\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        mean: the mean of the normal distribution\n        std: the standard deviation of the normal distribution\n        a: the minimum cutoff value\n        b: the maximum cutoff value\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.trunc_normal_(w)\n    \"\"\"", "\n", "return", "_no_grad_trunc_normal_", "(", "tensor", ",", "mean", ",", "std", ",", "a", ",", "b", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.variance_scaling_": [[65, 86], ["torch.nn.init._calculate_fan_in_and_fan_out", "weight_init.trunc_normal_", "tensor.normal_", "math.sqrt", "tensor.uniform_", "ValueError", "math.sqrt", "math.sqrt"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["", "def", "variance_scaling_", "(", "tensor", ",", "scale", "=", "1.0", ",", "mode", "=", "'fan_in'", ",", "distribution", "=", "'normal'", ")", ":", "\n", "    ", "fan_in", ",", "fan_out", "=", "_calculate_fan_in_and_fan_out", "(", "tensor", ")", "\n", "if", "mode", "==", "'fan_in'", ":", "\n", "        ", "denom", "=", "fan_in", "\n", "", "elif", "mode", "==", "'fan_out'", ":", "\n", "        ", "denom", "=", "fan_out", "\n", "", "elif", "mode", "==", "'fan_avg'", ":", "\n", "        ", "denom", "=", "(", "fan_in", "+", "fan_out", ")", "/", "2", "\n", "\n", "", "variance", "=", "scale", "/", "denom", "\n", "\n", "if", "distribution", "==", "\"truncated_normal\"", ":", "\n", "# constant is stddev of standard normal truncated to (-2, 2)", "\n", "        ", "trunc_normal_", "(", "tensor", ",", "std", "=", "math", ".", "sqrt", "(", "variance", ")", "/", ".87962566103423978", ")", "\n", "", "elif", "distribution", "==", "\"normal\"", ":", "\n", "        ", "tensor", ".", "normal_", "(", "std", "=", "math", ".", "sqrt", "(", "variance", ")", ")", "\n", "", "elif", "distribution", "==", "\"uniform\"", ":", "\n", "        ", "bound", "=", "math", ".", "sqrt", "(", "3", "*", "variance", ")", "\n", "tensor", ".", "uniform_", "(", "-", "bound", ",", "bound", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"invalid distribution {distribution}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.lecun_normal_": [[88, 90], ["weight_init.variance_scaling_"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.variance_scaling_"], ["", "", "def", "lecun_normal_", "(", "tensor", ")", ":", "\n", "    ", "variance_scaling_", "(", "tensor", ",", "mode", "=", "'fan_in'", ",", "distribution", "=", "'truncated_normal'", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.lambda_layer.LambdaLayer.__init__": [[67, 101], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.BatchNorm2d", "torch.nn.BatchNorm2d", "torch.nn.BatchNorm2d", "torch.nn.BatchNorm2d", "lambda_layer.LambdaLayer.reset_parameters", "torch.nn.Conv3d", "torch.nn.Conv3d", "helpers.to_2tuple", "torch.nn.Parameter", "torch.nn.Parameter", "lambda_layer.LambdaLayer.register_buffer", "torch.nn.AvgPool2d", "torch.nn.AvgPool2d", "torch.nn.Identity", "torch.nn.Identity", "helpers.make_divisible", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "lambda_layer.rel_pos_indices"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.lambda_layer.rel_pos_indices"], ["def", "__init__", "(", "\n", "self", ",", "dim", ",", "dim_out", "=", "None", ",", "feat_size", "=", "None", ",", "stride", "=", "1", ",", "num_heads", "=", "4", ",", "dim_head", "=", "16", ",", "r", "=", "9", ",", "\n", "qk_ratio", "=", "1.0", ",", "qkv_bias", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "dim_out", "=", "dim_out", "or", "dim", "\n", "assert", "dim_out", "%", "num_heads", "==", "0", ",", "' should be divided by num_heads'", "\n", "self", ".", "dim_qk", "=", "dim_head", "or", "make_divisible", "(", "dim_out", "*", "qk_ratio", ",", "divisor", "=", "8", ")", "//", "num_heads", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "dim_v", "=", "dim_out", "//", "num_heads", "\n", "\n", "self", ".", "qkv", "=", "nn", ".", "Conv2d", "(", "\n", "dim", ",", "\n", "num_heads", "*", "self", ".", "dim_qk", "+", "self", ".", "dim_qk", "+", "self", ".", "dim_v", ",", "\n", "kernel_size", "=", "1", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "norm_q", "=", "nn", ".", "BatchNorm2d", "(", "num_heads", "*", "self", ".", "dim_qk", ")", "\n", "self", ".", "norm_v", "=", "nn", ".", "BatchNorm2d", "(", "self", ".", "dim_v", ")", "\n", "\n", "if", "r", "is", "not", "None", ":", "\n", "# local lambda convolution for pos", "\n", "            ", "self", ".", "conv_lambda", "=", "nn", ".", "Conv3d", "(", "1", ",", "self", ".", "dim_qk", ",", "(", "r", ",", "r", ",", "1", ")", ",", "padding", "=", "(", "r", "//", "2", ",", "r", "//", "2", ",", "0", ")", ")", "\n", "self", ".", "pos_emb", "=", "None", "\n", "self", ".", "rel_pos_indices", "=", "None", "\n", "", "else", ":", "\n", "# relative pos embedding", "\n", "            ", "assert", "feat_size", "is", "not", "None", "\n", "feat_size", "=", "to_2tuple", "(", "feat_size", ")", "\n", "rel_size", "=", "[", "2", "*", "s", "-", "1", "for", "s", "in", "feat_size", "]", "\n", "self", ".", "conv_lambda", "=", "None", "\n", "self", ".", "pos_emb", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "rel_size", "[", "0", "]", ",", "rel_size", "[", "1", "]", ",", "self", ".", "dim_qk", ")", ")", "\n", "self", ".", "register_buffer", "(", "'rel_pos_indices'", ",", "rel_pos_indices", "(", "feat_size", ")", ",", "persistent", "=", "False", ")", "\n", "\n", "", "self", ".", "pool", "=", "nn", ".", "AvgPool2d", "(", "2", ",", "2", ")", "if", "stride", "==", "2", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.lambda_layer.LambdaLayer.reset_parameters": [[102, 108], ["weight_init.trunc_normal_", "weight_init.trunc_normal_", "weight_init.trunc_normal_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "trunc_normal_", "(", "self", ".", "qkv", ".", "weight", ",", "std", "=", "self", ".", "qkv", ".", "weight", ".", "shape", "[", "1", "]", "**", "-", "0.5", ")", "# fan-in", "\n", "if", "self", ".", "conv_lambda", "is", "not", "None", ":", "\n", "            ", "trunc_normal_", "(", "self", ".", "conv_lambda", ".", "weight", ",", "std", "=", "self", ".", "dim_qk", "**", "-", "0.5", ")", "\n", "", "if", "self", ".", "pos_emb", "is", "not", "None", ":", "\n", "            ", "trunc_normal_", "(", "self", ".", "pos_emb", ",", "std", "=", ".02", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.lambda_layer.LambdaLayer.forward": [[109, 134], ["lambda_layer.LambdaLayer.qkv", "torch.split", "torch.split", "torch.split", "torch.split", "lambda_layer.LambdaLayer.norm_q().reshape().transpose", "lambda_layer.LambdaLayer.norm_v().reshape().transpose", "torch.softmax", "torch.softmax", "lambda_layer.LambdaLayer.pool", "torch.softmax.reshape", "content_lam.unsqueeze", "lambda_layer.LambdaLayer.conv_lambda", "position_lam.reshape().transpose.reshape().transpose.reshape().transpose", "lambda_layer.LambdaLayer.pos_emb[].expand", "lambda_layer.LambdaLayer.norm_q().reshape", "lambda_layer.LambdaLayer.norm_v().reshape", "lambda_layer.LambdaLayer.reshape", "position_lam.reshape().transpose.reshape().transpose.reshape", "lambda_layer.LambdaLayer.unsqueeze", "lambda_layer.LambdaLayer.norm_q", "lambda_layer.LambdaLayer.norm_v", "lambda_layer.LambdaLayer.transpose", "lambda_layer.LambdaLayer.unsqueeze"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "M", "=", "H", "*", "W", "\n", "qkv", "=", "self", ".", "qkv", "(", "x", ")", "\n", "q", ",", "k", ",", "v", "=", "torch", ".", "split", "(", "qkv", ",", "[", "\n", "self", ".", "num_heads", "*", "self", ".", "dim_qk", ",", "self", ".", "dim_qk", ",", "self", ".", "dim_v", "]", ",", "dim", "=", "1", ")", "\n", "q", "=", "self", ".", "norm_q", "(", "q", ")", ".", "reshape", "(", "B", ",", "self", ".", "num_heads", ",", "self", ".", "dim_qk", ",", "M", ")", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "# B, num_heads, M, K", "\n", "v", "=", "self", ".", "norm_v", "(", "v", ")", ".", "reshape", "(", "B", ",", "self", ".", "dim_v", ",", "M", ")", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "# B, M, V", "\n", "k", "=", "F", ".", "softmax", "(", "k", ".", "reshape", "(", "B", ",", "self", ".", "dim_qk", ",", "M", ")", ",", "dim", "=", "-", "1", ")", "# B, K, M", "\n", "\n", "content_lam", "=", "k", "@", "v", "# B, K, V", "\n", "content_out", "=", "q", "@", "content_lam", ".", "unsqueeze", "(", "1", ")", "# B, num_heads, M, V", "\n", "\n", "if", "self", ".", "pos_emb", "is", "None", ":", "\n", "            ", "position_lam", "=", "self", ".", "conv_lambda", "(", "v", ".", "reshape", "(", "B", ",", "1", ",", "H", ",", "W", ",", "self", ".", "dim_v", ")", ")", "# B, H, W, V, K", "\n", "position_lam", "=", "position_lam", ".", "reshape", "(", "B", ",", "1", ",", "self", ".", "dim_qk", ",", "H", "*", "W", ",", "self", ".", "dim_v", ")", ".", "transpose", "(", "2", ",", "3", ")", "# B, 1, M, K, V", "\n", "", "else", ":", "\n", "# FIXME relative pos embedding path not fully verified", "\n", "            ", "pos_emb", "=", "self", ".", "pos_emb", "[", "self", ".", "rel_pos_indices", "[", "0", "]", ",", "self", ".", "rel_pos_indices", "[", "1", "]", "]", ".", "expand", "(", "B", ",", "-", "1", ",", "-", "1", ",", "-", "1", ")", "\n", "position_lam", "=", "(", "pos_emb", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "@", "v", ".", "unsqueeze", "(", "1", ")", ")", ".", "unsqueeze", "(", "1", ")", "# B, 1, M, K, V", "\n", "", "position_out", "=", "(", "q", ".", "unsqueeze", "(", "-", "2", ")", "@", "position_lam", ")", ".", "squeeze", "(", "-", "2", ")", "# B, num_heads, M, V", "\n", "\n", "out", "=", "(", "content_out", "+", "position_out", ")", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ".", "reshape", "(", "B", ",", "C", ",", "H", ",", "W", ")", "# B, C (num_heads * V), H, W", "\n", "out", "=", "self", ".", "pool", "(", "out", ")", "\n", "return", "out", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.lambda_layer.rel_pos_indices": [[31, 38], ["helpers.to_2tuple", "torch.stack().flatten", "torch.stack().flatten", "torch.stack", "torch.stack", "torch.meshgrid", "torch.meshgrid", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "function", ["None"], ["def", "rel_pos_indices", "(", "size", ")", ":", "\n", "    ", "size", "=", "to_2tuple", "(", "size", ")", "\n", "pos", "=", "torch", ".", "stack", "(", "torch", ".", "meshgrid", "(", "torch", ".", "arange", "(", "size", "[", "0", "]", ")", ",", "torch", ".", "arange", "(", "size", "[", "1", "]", ")", ")", ")", ".", "flatten", "(", "1", ")", "\n", "rel_pos", "=", "pos", "[", ":", ",", "None", ",", ":", "]", "-", "pos", "[", ":", ",", ":", ",", "None", "]", "\n", "rel_pos", "[", "0", "]", "+=", "size", "[", "0", "]", "-", "1", "\n", "rel_pos", "[", "1", "]", "+=", "size", "[", "1", "]", "-", "1", "\n", "return", "rel_pos", "# 2, H * W, H * W", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.attention_pool2d.RotAttentionPool2d.__init__": [[30, 51], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "pos_embed.RotaryEmbedding", "weight_init.trunc_normal_", "torch.init.zeros_", "torch.init.zeros_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["def", "__init__", "(", "\n", "self", ",", "\n", "in_features", ":", "int", ",", "\n", "out_features", ":", "int", "=", "None", ",", "\n", "embed_dim", ":", "int", "=", "None", ",", "\n", "num_heads", ":", "int", "=", "4", ",", "\n", "qkv_bias", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "embed_dim", "=", "embed_dim", "or", "in_features", "\n", "out_features", "=", "out_features", "or", "in_features", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "in_features", ",", "embed_dim", "*", "3", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "out_features", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "assert", "embed_dim", "%", "num_heads", "==", "0", "\n", "self", ".", "head_dim", "=", "embed_dim", "//", "num_heads", "\n", "self", ".", "scale", "=", "self", ".", "head_dim", "**", "-", "0.5", "\n", "self", ".", "pos_embed", "=", "RotaryEmbedding", "(", "self", ".", "head_dim", ")", "\n", "\n", "trunc_normal_", "(", "self", ".", "qkv", ".", "weight", ",", "std", "=", "in_features", "**", "-", "0.5", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "qkv", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.attention_pool2d.RotAttentionPool2d.forward": [[52, 77], ["attention_pool2d.RotAttentionPool2d.reshape().permute", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "attention_pool2d.RotAttentionPool2d.qkv().reshape().permute", "attention_pool2d.RotAttentionPool2d.pos_embed.get_embed", "pos_embed.apply_rot_embed", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "pos_embed.apply_rot_embed", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "attn.softmax.softmax.softmax", "attention_pool2d.RotAttentionPool2d.proj", "attention_pool2d.RotAttentionPool2d.reshape", "attention_pool2d.RotAttentionPool2d.mean", "attention_pool2d.RotAttentionPool2d.qkv().reshape", "torch.cat.transpose", "torch.cat.transpose", "attention_pool2d.RotAttentionPool2d.qkv"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.RotaryEmbedding.get_embed", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.apply_rot_embed", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.pos_embed.apply_rot_embed"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "_", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "N", "=", "H", "*", "W", "\n", "x", "=", "x", ".", "reshape", "(", "B", ",", "-", "1", ",", "N", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", ".", "mean", "(", "1", ",", "keepdim", "=", "True", ")", ",", "x", "]", ",", "dim", "=", "1", ")", "\n", "\n", "x", "=", "self", ".", "qkv", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", "+", "1", ",", "3", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "q", ",", "k", ",", "v", "=", "x", "[", "0", "]", ",", "x", "[", "1", "]", ",", "x", "[", "2", "]", "\n", "\n", "qc", ",", "q", "=", "q", "[", ":", ",", ":", ",", ":", "1", "]", ",", "q", "[", ":", ",", ":", ",", "1", ":", "]", "\n", "sin_emb", ",", "cos_emb", "=", "self", ".", "pos_embed", ".", "get_embed", "(", "(", "H", ",", "W", ")", ")", "\n", "q", "=", "apply_rot_embed", "(", "q", ",", "sin_emb", ",", "cos_emb", ")", "\n", "q", "=", "torch", ".", "cat", "(", "[", "qc", ",", "q", "]", ",", "dim", "=", "2", ")", "\n", "\n", "kc", ",", "k", "=", "k", "[", ":", ",", ":", ",", ":", "1", "]", ",", "k", "[", ":", ",", ":", ",", "1", ":", "]", "\n", "k", "=", "apply_rot_embed", "(", "k", ",", "sin_emb", ",", "cos_emb", ")", "\n", "k", "=", "torch", ".", "cat", "(", "[", "kc", ",", "k", "]", ",", "dim", "=", "2", ")", "\n", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "scale", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "N", "+", "1", ",", "-", "1", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "return", "x", "[", ":", ",", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.attention_pool2d.AttentionPool2d.__init__": [[88, 114], ["torch.Module.__init__", "helpers.to_2tuple", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "weight_init.trunc_normal_", "weight_init.trunc_normal_", "torch.init.zeros_", "torch.init.zeros_", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.weight_init.trunc_normal_"], ["def", "__init__", "(", "\n", "self", ",", "\n", "in_features", ":", "int", ",", "\n", "feat_size", ":", "Union", "[", "int", ",", "Tuple", "[", "int", ",", "int", "]", "]", ",", "\n", "out_features", ":", "int", "=", "None", ",", "\n", "embed_dim", ":", "int", "=", "None", ",", "\n", "num_heads", ":", "int", "=", "4", ",", "\n", "qkv_bias", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "embed_dim", "=", "embed_dim", "or", "in_features", "\n", "out_features", "=", "out_features", "or", "in_features", "\n", "assert", "embed_dim", "%", "num_heads", "==", "0", "\n", "self", ".", "feat_size", "=", "to_2tuple", "(", "feat_size", ")", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "in_features", ",", "embed_dim", "*", "3", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "out_features", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "head_dim", "=", "embed_dim", "//", "num_heads", "\n", "self", ".", "scale", "=", "self", ".", "head_dim", "**", "-", "0.5", "\n", "\n", "spatial_dim", "=", "self", ".", "feat_size", "[", "0", "]", "*", "self", ".", "feat_size", "[", "1", "]", "\n", "self", ".", "pos_embed", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "spatial_dim", "+", "1", ",", "in_features", ")", ")", "\n", "trunc_normal_", "(", "self", ".", "pos_embed", ",", "std", "=", "in_features", "**", "-", "0.5", ")", "\n", "trunc_normal_", "(", "self", ".", "qkv", ".", "weight", ",", "std", "=", "in_features", "**", "-", "0.5", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "qkv", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.attention_pool2d.AttentionPool2d.forward": [[115, 132], ["attention_pool2d.AttentionPool2d.reshape().permute", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "attention_pool2d.AttentionPool2d.qkv().reshape().permute", "attn.softmax.softmax.softmax", "attention_pool2d.AttentionPool2d.proj", "attention_pool2d.AttentionPool2d.pos_embed.unsqueeze().to", "attention_pool2d.AttentionPool2d.reshape", "attention_pool2d.AttentionPool2d.mean", "attention_pool2d.AttentionPool2d.qkv().reshape", "k.transpose", "attention_pool2d.AttentionPool2d.pos_embed.unsqueeze", "attention_pool2d.AttentionPool2d.qkv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "_", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "N", "=", "H", "*", "W", "\n", "assert", "self", ".", "feat_size", "[", "0", "]", "==", "H", "\n", "assert", "self", ".", "feat_size", "[", "1", "]", "==", "W", "\n", "x", "=", "x", ".", "reshape", "(", "B", ",", "-", "1", ",", "N", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", ".", "mean", "(", "1", ",", "keepdim", "=", "True", ")", ",", "x", "]", ",", "dim", "=", "1", ")", "\n", "x", "=", "x", "+", "self", ".", "pos_embed", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "x", ".", "dtype", ")", "\n", "\n", "x", "=", "self", ".", "qkv", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", "+", "1", ",", "3", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "q", ",", "k", ",", "v", "=", "x", "[", "0", "]", ",", "x", "[", "1", "]", ",", "x", "[", "2", "]", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "scale", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "N", "+", "1", ",", "-", "1", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "return", "x", "[", ":", ",", "0", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.get_padding": [[12, 15], ["None"], "function", ["None"], ["def", "get_padding", "(", "kernel_size", ":", "int", ",", "stride", ":", "int", "=", "1", ",", "dilation", ":", "int", "=", "1", ",", "**", "_", ")", "->", "int", ":", "\n", "    ", "padding", "=", "(", "(", "stride", "-", "1", ")", "+", "dilation", "*", "(", "kernel_size", "-", "1", ")", ")", "//", "2", "\n", "return", "padding", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.get_same_padding": [[18, 20], ["max", "math.ceil"], "function", ["None"], ["", "def", "get_same_padding", "(", "x", ":", "int", ",", "k", ":", "int", ",", "s", ":", "int", ",", "d", ":", "int", ")", ":", "\n", "    ", "return", "max", "(", "(", "math", ".", "ceil", "(", "x", "/", "s", ")", "-", "1", ")", "*", "s", "+", "(", "k", "-", "1", ")", "*", "d", "+", "1", "-", "x", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.is_static_pad": [[23, 25], ["None"], "function", ["None"], ["", "def", "is_static_pad", "(", "kernel_size", ":", "int", ",", "stride", ":", "int", "=", "1", ",", "dilation", ":", "int", "=", "1", ",", "**", "_", ")", ":", "\n", "    ", "return", "stride", "==", "1", "and", "(", "dilation", "*", "(", "kernel_size", "-", "1", ")", ")", "%", "2", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.pad_same": [[28, 34], ["F.pad.size", "padding.get_same_padding", "padding.get_same_padding", "torch.pad"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.get_same_padding", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.get_same_padding"], ["", "def", "pad_same", "(", "x", ",", "k", ":", "List", "[", "int", "]", ",", "s", ":", "List", "[", "int", "]", ",", "d", ":", "List", "[", "int", "]", "=", "(", "1", ",", "1", ")", ",", "value", ":", "float", "=", "0", ")", ":", "\n", "    ", "ih", ",", "iw", "=", "x", ".", "size", "(", ")", "[", "-", "2", ":", "]", "\n", "pad_h", ",", "pad_w", "=", "get_same_padding", "(", "ih", ",", "k", "[", "0", "]", ",", "s", "[", "0", "]", ",", "d", "[", "0", "]", ")", ",", "get_same_padding", "(", "iw", ",", "k", "[", "1", "]", ",", "s", "[", "1", "]", ",", "d", "[", "1", "]", ")", "\n", "if", "pad_h", ">", "0", "or", "pad_w", ">", "0", ":", "\n", "        ", "x", "=", "F", ".", "pad", "(", "x", ",", "[", "pad_w", "//", "2", ",", "pad_w", "-", "pad_w", "//", "2", ",", "pad_h", "//", "2", ",", "pad_h", "-", "pad_h", "//", "2", "]", ",", "value", "=", "value", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.get_padding_value": [[36, 57], ["isinstance", "get_padding.lower", "padding.is_static_pad", "padding.get_padding", "padding.get_padding"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.is_static_pad", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.get_padding", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.get_padding"], ["", "def", "get_padding_value", "(", "padding", ",", "kernel_size", ",", "**", "kwargs", ")", "->", "Tuple", "[", "Tuple", ",", "bool", "]", ":", "\n", "    ", "dynamic", "=", "False", "\n", "if", "isinstance", "(", "padding", ",", "str", ")", ":", "\n", "# for any string padding, the padding will be calculated for you, one of three ways", "\n", "        ", "padding", "=", "padding", ".", "lower", "(", ")", "\n", "if", "padding", "==", "'same'", ":", "\n", "# TF compatible 'SAME' padding, has a performance and GPU memory allocation impact", "\n", "            ", "if", "is_static_pad", "(", "kernel_size", ",", "**", "kwargs", ")", ":", "\n", "# static case, no extra overhead", "\n", "                ", "padding", "=", "get_padding", "(", "kernel_size", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "# dynamic 'SAME' padding, has runtime/GPU memory overhead", "\n", "                ", "padding", "=", "0", "\n", "dynamic", "=", "True", "\n", "", "", "elif", "padding", "==", "'valid'", ":", "\n", "# 'VALID' padding, same as padding=0", "\n", "            ", "padding", "=", "0", "\n", "", "else", ":", "\n", "# Default to PyTorch style 'same'-ish symmetric padding", "\n", "            ", "padding", "=", "get_padding", "(", "kernel_size", ",", "**", "kwargs", ")", "\n", "", "", "return", "padding", ",", "dynamic", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.non_local_attn.NonLocalAttn.__init__": [[23, 34], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.BatchNorm2d", "non_local_attn.NonLocalAttn.reset_parameters", "helpers.make_divisible"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "use_scale", "=", "True", ",", "rd_ratio", "=", "1", "/", "8", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "8", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "NonLocalAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "rd_channels", "is", "None", ":", "\n", "            ", "rd_channels", "=", "make_divisible", "(", "in_channels", "*", "rd_ratio", ",", "divisor", "=", "rd_divisor", ")", "\n", "", "self", ".", "scale", "=", "in_channels", "**", "-", "0.5", "if", "use_scale", "else", "1.0", "\n", "self", ".", "t", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "rd_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "p", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "rd_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "g", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "rd_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "z", "=", "nn", ".", "Conv2d", "(", "rd_channels", ",", "in_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "norm", "=", "nn", ".", "BatchNorm2d", "(", "in_channels", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.non_local_attn.NonLocalAttn.forward": [[35, 56], ["non_local_attn.NonLocalAttn.t", "non_local_attn.NonLocalAttn.p", "non_local_attn.NonLocalAttn.g", "t.view().permute.view().permute.size", "t.view().permute.view().permute.view().permute", "p.view.view.view", "g.view().permute.view().permute.view().permute", "torch.nn.functional.softmax", "torch.bmm", "non_local_attn.NonLocalAttn.permute().reshape", "non_local_attn.NonLocalAttn.z", "torch.bmm", "non_local_attn.NonLocalAttn.norm", "t.view().permute.view().permute.view", "g.view().permute.view().permute.view", "non_local_attn.NonLocalAttn.permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "\n", "t", "=", "self", ".", "t", "(", "x", ")", "\n", "p", "=", "self", ".", "p", "(", "x", ")", "\n", "g", "=", "self", ".", "g", "(", "x", ")", "\n", "\n", "B", ",", "C", ",", "H", ",", "W", "=", "t", ".", "size", "(", ")", "\n", "t", "=", "t", ".", "view", "(", "B", ",", "C", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "p", "=", "p", ".", "view", "(", "B", ",", "C", ",", "-", "1", ")", "\n", "g", "=", "g", ".", "view", "(", "B", ",", "C", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "\n", "att", "=", "torch", ".", "bmm", "(", "t", ",", "p", ")", "*", "self", ".", "scale", "\n", "att", "=", "F", ".", "softmax", "(", "att", ",", "dim", "=", "2", ")", "\n", "x", "=", "torch", ".", "bmm", "(", "att", ",", "g", ")", "\n", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ".", "reshape", "(", "B", ",", "C", ",", "H", ",", "W", ")", "\n", "x", "=", "self", ".", "z", "(", "x", ")", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "+", "shortcut", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.non_local_attn.NonLocalAttn.reset_parameters": [[57, 70], ["non_local_attn.NonLocalAttn.named_modules", "isinstance", "torch.nn.init.kaiming_normal_", "isinstance", "len", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "isinstance", "list", "torch.nn.init.constant_", "torch.nn.init.constant_", "m.parameters"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.named_modules"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "for", "name", ",", "m", "in", "self", ".", "named_modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "\n", "m", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "if", "len", "(", "list", "(", "m", ".", "parameters", "(", ")", ")", ")", ">", "1", ":", "\n", "                    ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0.0", ")", "\n", "", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "GroupNorm", ")", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.non_local_attn.BilinearAttnTransform.__init__": [[74, 84], ["torch.nn.Module.__init__", "conv_bn_act.ConvNormAct", "torch.nn.Conv2d", "torch.nn.Conv2d", "conv_bn_act.ConvNormAct"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "block_size", ",", "groups", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ")", ":", "\n", "        ", "super", "(", "BilinearAttnTransform", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "conv1", "=", "ConvNormAct", "(", "in_channels", ",", "groups", ",", "1", ",", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "conv_p", "=", "nn", ".", "Conv2d", "(", "groups", ",", "block_size", "*", "block_size", "*", "groups", ",", "kernel_size", "=", "(", "block_size", ",", "1", ")", ")", "\n", "self", ".", "conv_q", "=", "nn", ".", "Conv2d", "(", "groups", ",", "block_size", "*", "block_size", "*", "groups", ",", "kernel_size", "=", "(", "1", ",", "block_size", ")", ")", "\n", "self", ".", "conv2", "=", "ConvNormAct", "(", "in_channels", ",", "in_channels", ",", "1", ",", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "block_size", "=", "block_size", "\n", "self", ".", "groups", "=", "groups", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.non_local_attn.BilinearAttnTransform.resize_mat": [[85, 97], ["trace_utils._assert", "x.view.view.view", "x.view.view.view", "torch.cat", "torch.cat", "x.view.view.view", "torch.eye", "torch.split", "torch.split"], "methods", ["None"], ["", "def", "resize_mat", "(", "self", ",", "x", ",", "t", ":", "int", ")", ":", "\n", "        ", "B", ",", "C", ",", "block_size", ",", "block_size1", "=", "x", ".", "shape", "\n", "_assert", "(", "block_size", "==", "block_size1", ",", "''", ")", "\n", "if", "t", "<=", "1", ":", "\n", "            ", "return", "x", "\n", "", "x", "=", "x", ".", "view", "(", "B", "*", "C", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "x", "=", "x", "*", "torch", ".", "eye", "(", "t", ",", "t", ",", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "\n", "x", "=", "x", ".", "view", "(", "B", "*", "C", ",", "block_size", ",", "block_size", ",", "t", ",", "t", ")", "\n", "x", "=", "torch", ".", "cat", "(", "torch", ".", "split", "(", "x", ",", "1", ",", "dim", "=", "1", ")", ",", "dim", "=", "3", ")", "\n", "x", "=", "torch", ".", "cat", "(", "torch", ".", "split", "(", "x", ",", "1", ",", "dim", "=", "2", ")", ",", "dim", "=", "4", ")", "\n", "x", "=", "x", ".", "view", "(", "B", ",", "C", ",", "block_size", "*", "t", ",", "block_size", "*", "t", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.non_local_attn.BilinearAttnTransform.forward": [[98, 122], ["trace_utils._assert", "trace_utils._assert", "non_local_attn.BilinearAttnTransform.conv1", "torch.nn.functional.adaptive_max_pool2d", "torch.nn.functional.adaptive_max_pool2d", "non_local_attn.BilinearAttnTransform.conv_p().view().sigmoid", "non_local_attn.BilinearAttnTransform.conv_q().view().sigmoid", "non_local_attn.BilinearAttnTransform.view().expand().contiguous", "non_local_attn.BilinearAttnTransform.view", "non_local_attn.BilinearAttnTransform.view().expand().contiguous", "non_local_attn.BilinearAttnTransform.view", "non_local_attn.BilinearAttnTransform.resize_mat", "non_local_attn.BilinearAttnTransform.resize_mat", "non_local_attn.BilinearAttnTransform.matmul", "non_local_attn.BilinearAttnTransform.matmul", "non_local_attn.BilinearAttnTransform.conv2", "non_local_attn.BilinearAttnTransform.sum", "non_local_attn.BilinearAttnTransform.sum", "non_local_attn.BilinearAttnTransform.conv_p().view", "non_local_attn.BilinearAttnTransform.conv_q().view", "non_local_attn.BilinearAttnTransform.view().expand", "non_local_attn.BilinearAttnTransform.view().expand", "x.size", "x.size", "non_local_attn.BilinearAttnTransform.conv_p", "non_local_attn.BilinearAttnTransform.conv_q", "non_local_attn.BilinearAttnTransform.view", "non_local_attn.BilinearAttnTransform.view"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.non_local_attn.BilinearAttnTransform.resize_mat", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.non_local_attn.BilinearAttnTransform.resize_mat"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "_assert", "(", "x", ".", "shape", "[", "-", "1", "]", "%", "self", ".", "block_size", "==", "0", ",", "''", ")", "\n", "_assert", "(", "x", ".", "shape", "[", "-", "2", "]", "%", "self", ".", "block_size", "==", "0", ",", "''", ")", "\n", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "rp", "=", "F", ".", "adaptive_max_pool2d", "(", "out", ",", "(", "self", ".", "block_size", ",", "1", ")", ")", "\n", "cp", "=", "F", ".", "adaptive_max_pool2d", "(", "out", ",", "(", "1", ",", "self", ".", "block_size", ")", ")", "\n", "p", "=", "self", ".", "conv_p", "(", "rp", ")", ".", "view", "(", "B", ",", "self", ".", "groups", ",", "self", ".", "block_size", ",", "self", ".", "block_size", ")", ".", "sigmoid", "(", ")", "\n", "q", "=", "self", ".", "conv_q", "(", "cp", ")", ".", "view", "(", "B", ",", "self", ".", "groups", ",", "self", ".", "block_size", ",", "self", ".", "block_size", ")", ".", "sigmoid", "(", ")", "\n", "p", "=", "p", "/", "p", ".", "sum", "(", "dim", "=", "3", ",", "keepdim", "=", "True", ")", "\n", "q", "=", "q", "/", "q", ".", "sum", "(", "dim", "=", "2", ",", "keepdim", "=", "True", ")", "\n", "p", "=", "p", ".", "view", "(", "B", ",", "self", ".", "groups", ",", "1", ",", "self", ".", "block_size", ",", "self", ".", "block_size", ")", ".", "expand", "(", "x", ".", "size", "(", "\n", "0", ")", ",", "self", ".", "groups", ",", "C", "//", "self", ".", "groups", ",", "self", ".", "block_size", ",", "self", ".", "block_size", ")", ".", "contiguous", "(", ")", "\n", "p", "=", "p", ".", "view", "(", "B", ",", "C", ",", "self", ".", "block_size", ",", "self", ".", "block_size", ")", "\n", "q", "=", "q", ".", "view", "(", "B", ",", "self", ".", "groups", ",", "1", ",", "self", ".", "block_size", ",", "self", ".", "block_size", ")", ".", "expand", "(", "x", ".", "size", "(", "\n", "0", ")", ",", "self", ".", "groups", ",", "C", "//", "self", ".", "groups", ",", "self", ".", "block_size", ",", "self", ".", "block_size", ")", ".", "contiguous", "(", ")", "\n", "q", "=", "q", ".", "view", "(", "B", ",", "C", ",", "self", ".", "block_size", ",", "self", ".", "block_size", ")", "\n", "p", "=", "self", ".", "resize_mat", "(", "p", ",", "H", "//", "self", ".", "block_size", ")", "\n", "q", "=", "self", ".", "resize_mat", "(", "q", ",", "W", "//", "self", ".", "block_size", ")", "\n", "y", "=", "p", ".", "matmul", "(", "x", ")", "\n", "y", "=", "y", ".", "matmul", "(", "q", ")", "\n", "\n", "y", "=", "self", ".", "conv2", "(", "y", ")", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.non_local_attn.BatNonLocalAttn.__init__": [[129, 139], ["torch.nn.Module.__init__", "conv_bn_act.ConvNormAct", "non_local_attn.BilinearAttnTransform", "conv_bn_act.ConvNormAct", "torch.nn.Dropout2d", "helpers.make_divisible"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible"], ["def", "__init__", "(", "\n", "self", ",", "in_channels", ",", "block_size", "=", "7", ",", "groups", "=", "2", ",", "rd_ratio", "=", "0.25", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "8", ",", "\n", "drop_rate", "=", "0.2", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "**", "_", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "rd_channels", "is", "None", ":", "\n", "            ", "rd_channels", "=", "make_divisible", "(", "in_channels", "*", "rd_ratio", ",", "divisor", "=", "rd_divisor", ")", "\n", "", "self", ".", "conv1", "=", "ConvNormAct", "(", "in_channels", ",", "rd_channels", ",", "1", ",", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "ba", "=", "BilinearAttnTransform", "(", "rd_channels", ",", "block_size", ",", "groups", ",", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "conv2", "=", "ConvNormAct", "(", "rd_channels", ",", "in_channels", ",", "1", ",", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout2d", "(", "p", "=", "drop_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.non_local_attn.BatNonLocalAttn.forward": [[140, 146], ["non_local_attn.BatNonLocalAttn.conv1", "non_local_attn.BatNonLocalAttn.ba", "non_local_attn.BatNonLocalAttn.conv2", "non_local_attn.BatNonLocalAttn.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "xl", "=", "self", ".", "conv1", "(", "x", ")", "\n", "y", "=", "self", ".", "ba", "(", "xl", ")", "\n", "y", "=", "self", ".", "conv2", "(", "y", ")", "\n", "y", "=", "self", ".", "dropout", "(", "y", ")", "\n", "return", "y", "+", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.Swish.__init__": [[21, 24], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "Swish", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inplace", "=", "inplace", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.Swish.forward": [[25, 27], ["activations.swish"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.swish"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "swish", "(", "x", ",", "self", ".", "inplace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.Mish.__init__": [[39, 41], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "Mish", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.Mish.forward": [[42, 44], ["activations.mish"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.mish"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "mish", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.Sigmoid.__init__": [[52, 55], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "Sigmoid", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inplace", "=", "inplace", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.Sigmoid.forward": [[56, 58], ["x.sigmoid_", "x.sigmoid"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", ".", "sigmoid_", "(", ")", "if", "self", ".", "inplace", "else", "x", ".", "sigmoid", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.Tanh.__init__": [[66, 69], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "Tanh", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inplace", "=", "inplace", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.Tanh.forward": [[70, 72], ["x.tanh_", "x.tanh"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.tanh"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", ".", "tanh_", "(", ")", "if", "self", ".", "inplace", "else", "x", ".", "tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.HardSwish.__init__": [[80, 83], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "HardSwish", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inplace", "=", "inplace", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.HardSwish.forward": [[84, 86], ["activations.hard_swish"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.hard_swish"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "hard_swish", "(", "x", ",", "self", ".", "inplace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.HardSigmoid.__init__": [[96, 99], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "HardSigmoid", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inplace", "=", "inplace", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.HardSigmoid.forward": [[100, 102], ["activations.hard_sigmoid"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.hard_sigmoid"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "hard_sigmoid", "(", "x", ",", "self", ".", "inplace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.HardMish.__init__": [[116, 119], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "HardMish", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inplace", "=", "inplace", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.HardMish.forward": [[120, 122], ["activations.hard_mish"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.hard_mish"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "hard_mish", "(", "x", ",", "self", ".", "inplace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.PReLU.__init__": [[127, 129], ["torch.nn.PReLU.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "num_parameters", ":", "int", "=", "1", ",", "init", ":", "float", "=", "0.25", ",", "inplace", ":", "bool", "=", "False", ")", "->", "None", ":", "\n", "        ", "super", "(", "PReLU", ",", "self", ")", ".", "__init__", "(", "num_parameters", "=", "num_parameters", ",", "init", "=", "init", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.PReLU.forward": [[130, 132], ["torch.nn.functional.prelu"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "return", "F", ".", "prelu", "(", "input", ",", "self", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.GELU.__init__": [[141, 143], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "GELU", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.GELU.forward": [[144, 146], ["torch.nn.functional.gelu"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.gelu"], ["", "def", "forward", "(", "self", ",", "input", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "return", "F", ".", "gelu", "(", "input", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.swish": [[14, 18], ["x.mul_", "x.mul", "x.sigmoid", "x.sigmoid"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid"], ["def", "swish", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\"Swish - Described in: https://arxiv.org/abs/1710.05941\n    \"\"\"", "\n", "return", "x", ".", "mul_", "(", "x", ".", "sigmoid", "(", ")", ")", "if", "inplace", "else", "x", ".", "mul", "(", "x", ".", "sigmoid", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.mish": [[29, 34], ["x.mul", "torch.nn.functional.softplus().tanh", "torch.nn.functional.softplus"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.tanh"], ["", "", "def", "mish", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n    NOTE: I don't have a working inplace variant\n    \"\"\"", "\n", "return", "x", ".", "mul", "(", "F", ".", "softplus", "(", "x", ")", ".", "tanh", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid": [[46, 48], ["x.sigmoid_", "x.sigmoid"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid"], ["", "", "def", "sigmoid", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "return", "x", ".", "sigmoid_", "(", ")", "if", "inplace", "else", "x", ".", "sigmoid", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.tanh": [[60, 62], ["x.tanh_", "x.tanh"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.tanh"], ["", "", "def", "tanh", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "return", "x", ".", "tanh_", "(", ")", "if", "inplace", "else", "x", ".", "tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.hard_swish": [[74, 77], ["torch.nn.functional.relu6().div_", "x.mul_", "x.mul", "torch.nn.functional.relu6"], "function", ["None"], ["", "", "def", "hard_swish", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "inner", "=", "F", ".", "relu6", "(", "x", "+", "3.", ")", ".", "div_", "(", "6.", ")", "\n", "return", "x", ".", "mul_", "(", "inner", ")", "if", "inplace", "else", "x", ".", "mul", "(", "inner", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.hard_sigmoid": [[88, 93], ["x.add_().clamp_().div_", "torch.nn.functional.relu6", "x.add_().clamp_", "x.add_"], "function", ["None"], ["", "", "def", "hard_sigmoid", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "if", "inplace", ":", "\n", "        ", "return", "x", ".", "add_", "(", "3.", ")", ".", "clamp_", "(", "0.", ",", "6.", ")", ".", "div_", "(", "6.", ")", "\n", "", "else", ":", "\n", "        ", "return", "F", ".", "relu6", "(", "x", "+", "3.", ")", "/", "6.", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.hard_mish": [[104, 113], ["x.mul_"], "function", ["None"], ["", "", "def", "hard_mish", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\" Hard Mish\n    Experimental, based on notes by Mish author Diganta Misra at\n      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n    \"\"\"", "\n", "if", "inplace", ":", "\n", "        ", "return", "x", ".", "mul_", "(", "0.5", "*", "(", "x", "+", "2", ")", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "2", ")", ")", "\n", "", "else", ":", "\n", "        ", "return", "0.5", "*", "x", "*", "(", "x", "+", "2", ")", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.gelu": [[134, 136], ["torch.nn.functional.gelu"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.gelu"], ["", "", "def", "gelu", "(", "x", ":", "torch", ".", "Tensor", ",", "inplace", ":", "bool", "=", "False", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "return", "F", ".", "gelu", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.linear.Linear.forward": [[14, 20], ["torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "linear.Linear.bias.to", "linear.Linear.weight.to"], "methods", ["None"], ["def", "forward", "(", "self", ",", "input", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "if", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "bias", "=", "self", ".", "bias", ".", "to", "(", "dtype", "=", "input", ".", "dtype", ")", "if", "self", ".", "bias", "is", "not", "None", "else", "None", "\n", "return", "F", ".", "linear", "(", "input", ",", "self", ".", "weight", ".", "to", "(", "dtype", "=", "input", ".", "dtype", ")", ",", "bias", "=", "bias", ")", "\n", "", "else", ":", "\n", "            ", "return", "F", ".", "linear", "(", "input", ",", "self", ".", "weight", ",", "self", ".", "bias", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dB0.__init__": [[100, 110], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "evo_norm.EvoNorm2dB0.register_buffer", "evo_norm.EvoNorm2dB0.reset_parameters", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "num_features", ",", "apply_act", "=", "True", ",", "momentum", "=", "0.1", ",", "eps", "=", "1e-3", ",", "**", "_", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "apply_act", "=", "apply_act", "# apply activation (non-linearity)", "\n", "self", ".", "momentum", "=", "momentum", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "num_features", ")", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "num_features", ")", ")", "\n", "self", ".", "v", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "num_features", ")", ")", "if", "apply_act", "else", "None", "\n", "self", ".", "register_buffer", "(", "'running_var'", ",", "torch", ".", "ones", "(", "num_features", ")", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dB0.reset_parameters": [[111, 116], ["torch.init.ones_", "torch.init.ones_", "torch.init.ones_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.ones_", "torch.init.ones_", "torch.init.ones_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "ones_", "(", "self", ".", "weight", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "bias", ")", "\n", "if", "self", ".", "v", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "ones_", "(", "self", ".", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dB0.forward": [[117, 136], ["trace_utils._assert", "x.float().var.add().sqrt_().to().view().expand_as", "evo_norm.EvoNorm2dB0.v.to().view", "evo_norm.EvoNorm2dB0.bias.to().view", "x.dim", "x.float().var", "evo_norm.EvoNorm2dB0.running_var.copy_", "evo_norm.instance_std", "x.float().var.add().sqrt_().to().view().expand_as.max", "evo_norm.EvoNorm2dB0.weight.to().view", "x.numel", "x.float().var.add().sqrt_().to().view", "evo_norm.EvoNorm2dB0.v.to", "evo_norm.EvoNorm2dB0.bias.to", "x.float", "evo_norm.EvoNorm2dB0.weight.to", "x.float().var.add().sqrt_().to", "x.float().var.detach", "x.float().var.add().sqrt_", "x.float().var.add"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.instance_std"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "_assert", "(", "x", ".", "dim", "(", ")", "==", "4", ",", "'expected 4D input'", ")", "\n", "x_dtype", "=", "x", ".", "dtype", "\n", "v_shape", "=", "(", "1", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "if", "self", ".", "v", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "training", ":", "\n", "                ", "var", "=", "x", ".", "float", "(", ")", ".", "var", "(", "dim", "=", "(", "0", ",", "2", ",", "3", ")", ",", "unbiased", "=", "False", ")", "\n", "# var = manual_var(x, dim=(0, 2, 3)).squeeze()", "\n", "n", "=", "x", ".", "numel", "(", ")", "/", "x", ".", "shape", "[", "1", "]", "\n", "self", ".", "running_var", ".", "copy_", "(", "\n", "self", ".", "running_var", "*", "(", "1", "-", "self", ".", "momentum", ")", "+", "\n", "var", ".", "detach", "(", ")", "*", "self", ".", "momentum", "*", "(", "n", "/", "(", "n", "-", "1", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "var", "=", "self", ".", "running_var", "\n", "", "left", "=", "var", ".", "add", "(", "self", ".", "eps", ")", ".", "sqrt_", "(", ")", ".", "to", "(", "x_dtype", ")", ".", "view", "(", "v_shape", ")", ".", "expand_as", "(", "x", ")", "\n", "v", "=", "self", ".", "v", ".", "to", "(", "x_dtype", ")", ".", "view", "(", "v_shape", ")", "\n", "right", "=", "x", "*", "v", "+", "instance_std", "(", "x", ",", "self", ".", "eps", ")", "\n", "x", "=", "x", "/", "left", ".", "max", "(", "right", ")", "\n", "", "return", "x", "*", "self", ".", "weight", ".", "to", "(", "x_dtype", ")", ".", "view", "(", "v_shape", ")", "+", "self", ".", "bias", ".", "to", "(", "x_dtype", ")", ".", "view", "(", "v_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dB1.__init__": [[139, 148], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "evo_norm.EvoNorm2dB1.register_buffer", "evo_norm.EvoNorm2dB1.reset_parameters", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "num_features", ",", "apply_act", "=", "True", ",", "momentum", "=", "0.1", ",", "eps", "=", "1e-5", ",", "**", "_", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "apply_act", "=", "apply_act", "# apply activation (non-linearity)", "\n", "self", ".", "momentum", "=", "momentum", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "num_features", ")", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "num_features", ")", ")", "\n", "self", ".", "register_buffer", "(", "'running_var'", ",", "torch", ".", "ones", "(", "num_features", ")", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dB1.reset_parameters": [[149, 152], ["torch.init.ones_", "torch.init.ones_", "torch.init.ones_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "ones_", "(", "self", ".", "weight", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dB1.forward": [[153, 171], ["trace_utils._assert", "x.float().var.to().view", "x.float().var.add().sqrt_", "evo_norm.EvoNorm2dB1.bias.view().to", "x.dim", "x.float().var", "evo_norm.EvoNorm2dB1.running_var.copy_", "evo_norm.instance_rms", "x.float().var.add().sqrt_.max", "evo_norm.EvoNorm2dB1.weight.view().to", "x.numel", "x.float().var.to", "x.float().var.add", "evo_norm.EvoNorm2dB1.bias.view", "x.float", "evo_norm.EvoNorm2dB1.weight.view", "x.float().var.detach().to", "x.float().var.detach"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.instance_rms"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "_assert", "(", "x", ".", "dim", "(", ")", "==", "4", ",", "'expected 4D input'", ")", "\n", "x_dtype", "=", "x", ".", "dtype", "\n", "v_shape", "=", "(", "1", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "if", "self", ".", "apply_act", ":", "\n", "            ", "if", "self", ".", "training", ":", "\n", "                ", "var", "=", "x", ".", "float", "(", ")", ".", "var", "(", "dim", "=", "(", "0", ",", "2", ",", "3", ")", ",", "unbiased", "=", "False", ")", "\n", "n", "=", "x", ".", "numel", "(", ")", "/", "x", ".", "shape", "[", "1", "]", "\n", "self", ".", "running_var", ".", "copy_", "(", "\n", "self", ".", "running_var", "*", "(", "1", "-", "self", ".", "momentum", ")", "+", "\n", "var", ".", "detach", "(", ")", ".", "to", "(", "self", ".", "running_var", ".", "dtype", ")", "*", "self", ".", "momentum", "*", "(", "n", "/", "(", "n", "-", "1", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "var", "=", "self", ".", "running_var", "\n", "", "var", "=", "var", ".", "to", "(", "x_dtype", ")", ".", "view", "(", "v_shape", ")", "\n", "left", "=", "var", ".", "add", "(", "self", ".", "eps", ")", ".", "sqrt_", "(", ")", "\n", "right", "=", "(", "x", "+", "1", ")", "*", "instance_rms", "(", "x", ",", "self", ".", "eps", ")", "\n", "x", "=", "x", "/", "left", ".", "max", "(", "right", ")", "\n", "", "return", "x", "*", "self", ".", "weight", ".", "view", "(", "v_shape", ")", ".", "to", "(", "x_dtype", ")", "+", "self", ".", "bias", ".", "view", "(", "v_shape", ")", ".", "to", "(", "x_dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dB2.__init__": [[174, 183], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "evo_norm.EvoNorm2dB2.register_buffer", "evo_norm.EvoNorm2dB2.reset_parameters", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "num_features", ",", "apply_act", "=", "True", ",", "momentum", "=", "0.1", ",", "eps", "=", "1e-5", ",", "**", "_", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "apply_act", "=", "apply_act", "# apply activation (non-linearity)", "\n", "self", ".", "momentum", "=", "momentum", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "num_features", ")", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "num_features", ")", ")", "\n", "self", ".", "register_buffer", "(", "'running_var'", ",", "torch", ".", "ones", "(", "num_features", ")", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dB2.reset_parameters": [[184, 187], ["torch.init.ones_", "torch.init.ones_", "torch.init.ones_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "ones_", "(", "self", ".", "weight", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dB2.forward": [[188, 206], ["trace_utils._assert", "x.float().var.to().view", "x.float().var.add().sqrt_", "evo_norm.EvoNorm2dB2.bias.view().to", "x.dim", "x.float().var", "evo_norm.EvoNorm2dB2.running_var.copy_", "evo_norm.instance_rms", "x.float().var.add().sqrt_.max", "evo_norm.EvoNorm2dB2.weight.view().to", "x.numel", "x.float().var.to", "x.float().var.add", "evo_norm.EvoNorm2dB2.bias.view", "x.float", "evo_norm.EvoNorm2dB2.weight.view", "x.float().var.detach().to", "x.float().var.detach"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.instance_rms"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "_assert", "(", "x", ".", "dim", "(", ")", "==", "4", ",", "'expected 4D input'", ")", "\n", "x_dtype", "=", "x", ".", "dtype", "\n", "v_shape", "=", "(", "1", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "if", "self", ".", "apply_act", ":", "\n", "            ", "if", "self", ".", "training", ":", "\n", "                ", "var", "=", "x", ".", "float", "(", ")", ".", "var", "(", "dim", "=", "(", "0", ",", "2", ",", "3", ")", ",", "unbiased", "=", "False", ")", "\n", "n", "=", "x", ".", "numel", "(", ")", "/", "x", ".", "shape", "[", "1", "]", "\n", "self", ".", "running_var", ".", "copy_", "(", "\n", "self", ".", "running_var", "*", "(", "1", "-", "self", ".", "momentum", ")", "+", "\n", "var", ".", "detach", "(", ")", ".", "to", "(", "self", ".", "running_var", ".", "dtype", ")", "*", "self", ".", "momentum", "*", "(", "n", "/", "(", "n", "-", "1", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "var", "=", "self", ".", "running_var", "\n", "", "var", "=", "var", ".", "to", "(", "x_dtype", ")", ".", "view", "(", "v_shape", ")", "\n", "left", "=", "var", ".", "add", "(", "self", ".", "eps", ")", ".", "sqrt_", "(", ")", "\n", "right", "=", "instance_rms", "(", "x", ",", "self", ".", "eps", ")", "-", "x", "\n", "x", "=", "x", "/", "left", ".", "max", "(", "right", ")", "\n", "", "return", "x", "*", "self", ".", "weight", ".", "view", "(", "v_shape", ")", ".", "to", "(", "x_dtype", ")", "+", "self", ".", "bias", ".", "view", "(", "v_shape", ")", ".", "to", "(", "x_dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dS0.__init__": [[209, 222], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "evo_norm.EvoNorm2dS0.reset_parameters", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "num_features", ",", "groups", "=", "32", ",", "group_size", "=", "None", ",", "apply_act", "=", "True", ",", "eps", "=", "1e-5", ",", "**", "_", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "apply_act", "=", "apply_act", "# apply activation (non-linearity)", "\n", "if", "group_size", ":", "\n", "            ", "assert", "num_features", "%", "group_size", "==", "0", "\n", "self", ".", "groups", "=", "num_features", "//", "group_size", "\n", "", "else", ":", "\n", "            ", "self", ".", "groups", "=", "groups", "\n", "", "self", ".", "eps", "=", "eps", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "num_features", ")", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "num_features", ")", ")", "\n", "self", ".", "v", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "num_features", ")", ")", "if", "apply_act", "else", "None", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dS0.reset_parameters": [[223, 228], ["torch.init.ones_", "torch.init.ones_", "torch.init.ones_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.ones_", "torch.init.ones_", "torch.init.ones_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "ones_", "(", "self", ".", "weight", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "bias", ")", "\n", "if", "self", ".", "v", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "ones_", "(", "self", ".", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dS0.forward": [[229, 237], ["trace_utils._assert", "evo_norm.EvoNorm2dS0.v.view().to", "evo_norm.EvoNorm2dS0.bias.view().to", "x.dim", "evo_norm.group_std", "evo_norm.EvoNorm2dS0.weight.view().to", "evo_norm.EvoNorm2dS0.v.view", "evo_norm.EvoNorm2dS0.bias.view", "evo_norm.EvoNorm2dS0.weight.view"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.group_std"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "_assert", "(", "x", ".", "dim", "(", ")", "==", "4", ",", "'expected 4D input'", ")", "\n", "x_dtype", "=", "x", ".", "dtype", "\n", "v_shape", "=", "(", "1", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "if", "self", ".", "v", "is", "not", "None", ":", "\n", "            ", "v", "=", "self", ".", "v", ".", "view", "(", "v_shape", ")", ".", "to", "(", "x_dtype", ")", "\n", "x", "=", "x", "*", "(", "x", "*", "v", ")", ".", "sigmoid", "(", ")", "/", "group_std", "(", "x", ",", "self", ".", "groups", ",", "self", ".", "eps", ")", "\n", "", "return", "x", "*", "self", ".", "weight", ".", "view", "(", "v_shape", ")", ".", "to", "(", "x_dtype", ")", "+", "self", ".", "bias", ".", "view", "(", "v_shape", ")", ".", "to", "(", "x_dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dS0a.__init__": [[240, 243], ["evo_norm.EvoNorm2dS0.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_features", ",", "groups", "=", "32", ",", "group_size", "=", "None", ",", "apply_act", "=", "True", ",", "eps", "=", "1e-3", ",", "**", "_", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "num_features", ",", "groups", "=", "groups", ",", "group_size", "=", "group_size", ",", "apply_act", "=", "apply_act", ",", "eps", "=", "eps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dS0a.forward": [[244, 254], ["trace_utils._assert", "evo_norm.group_std", "evo_norm.EvoNorm2dS0a.v.view().to", "evo_norm.EvoNorm2dS0a.bias.view().to", "x.dim", "evo_norm.EvoNorm2dS0a.weight.view().to", "evo_norm.EvoNorm2dS0a.v.view", "evo_norm.EvoNorm2dS0a.bias.view", "evo_norm.EvoNorm2dS0a.weight.view"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.group_std"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "_assert", "(", "x", ".", "dim", "(", ")", "==", "4", ",", "'expected 4D input'", ")", "\n", "x_dtype", "=", "x", ".", "dtype", "\n", "v_shape", "=", "(", "1", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "d", "=", "group_std", "(", "x", ",", "self", ".", "groups", ",", "self", ".", "eps", ")", "\n", "if", "self", ".", "v", "is", "not", "None", ":", "\n", "            ", "v", "=", "self", ".", "v", ".", "view", "(", "v_shape", ")", ".", "to", "(", "x_dtype", ")", "\n", "x", "=", "x", "*", "(", "x", "*", "v", ")", ".", "sigmoid", "(", ")", "\n", "", "x", "=", "x", "/", "d", "\n", "return", "x", "*", "self", ".", "weight", ".", "view", "(", "v_shape", ")", ".", "to", "(", "x_dtype", ")", "+", "self", ".", "bias", ".", "view", "(", "v_shape", ")", ".", "to", "(", "x_dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dS1.__init__": [[257, 277], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "evo_norm.EvoNorm2dS1.reset_parameters", "create_act.create_act_layer", "torch.Identity", "torch.Identity", "torch.Identity", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.create_act_layer"], ["    ", "def", "__init__", "(", "\n", "self", ",", "num_features", ",", "groups", "=", "32", ",", "group_size", "=", "None", ",", "\n", "apply_act", "=", "True", ",", "act_layer", "=", "None", ",", "eps", "=", "1e-5", ",", "**", "_", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "act_layer", "=", "act_layer", "or", "nn", ".", "SiLU", "\n", "self", ".", "apply_act", "=", "apply_act", "# apply activation (non-linearity)", "\n", "if", "act_layer", "is", "not", "None", "and", "apply_act", ":", "\n", "            ", "self", ".", "act", "=", "create_act_layer", "(", "act_layer", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "act", "=", "nn", ".", "Identity", "(", ")", "\n", "", "if", "group_size", ":", "\n", "            ", "assert", "num_features", "%", "group_size", "==", "0", "\n", "self", ".", "groups", "=", "num_features", "//", "group_size", "\n", "", "else", ":", "\n", "            ", "self", ".", "groups", "=", "groups", "\n", "", "self", ".", "eps", "=", "eps", "\n", "self", ".", "pre_act_norm", "=", "False", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "num_features", ")", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "num_features", ")", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dS1.reset_parameters": [[278, 281], ["torch.init.ones_", "torch.init.ones_", "torch.init.ones_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "ones_", "(", "self", ".", "weight", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dS1.forward": [[282, 289], ["trace_utils._assert", "evo_norm.EvoNorm2dS1.bias.view().to", "x.dim", "evo_norm.EvoNorm2dS1.act", "evo_norm.group_std", "evo_norm.EvoNorm2dS1.weight.view().to", "evo_norm.EvoNorm2dS1.bias.view", "evo_norm.EvoNorm2dS1.weight.view"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.group_std"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "_assert", "(", "x", ".", "dim", "(", ")", "==", "4", ",", "'expected 4D input'", ")", "\n", "x_dtype", "=", "x", ".", "dtype", "\n", "v_shape", "=", "(", "1", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "if", "self", ".", "apply_act", ":", "\n", "            ", "x", "=", "self", ".", "act", "(", "x", ")", "/", "group_std", "(", "x", ",", "self", ".", "groups", ",", "self", ".", "eps", ")", "\n", "", "return", "x", "*", "self", ".", "weight", ".", "view", "(", "v_shape", ")", ".", "to", "(", "x_dtype", ")", "+", "self", ".", "bias", ".", "view", "(", "v_shape", ")", ".", "to", "(", "x_dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dS1a.__init__": [[292, 297], ["evo_norm.EvoNorm2dS1.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "num_features", ",", "groups", "=", "32", ",", "group_size", "=", "None", ",", "\n", "apply_act", "=", "True", ",", "act_layer", "=", "None", ",", "eps", "=", "1e-3", ",", "**", "_", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "num_features", ",", "groups", "=", "groups", ",", "group_size", "=", "group_size", ",", "apply_act", "=", "apply_act", ",", "act_layer", "=", "act_layer", ",", "eps", "=", "eps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dS1a.forward": [[298, 304], ["trace_utils._assert", "evo_norm.EvoNorm2dS1a.act", "evo_norm.group_std", "evo_norm.EvoNorm2dS1a.bias.view().to", "x.dim", "evo_norm.EvoNorm2dS1a.weight.view().to", "evo_norm.EvoNorm2dS1a.bias.view", "evo_norm.EvoNorm2dS1a.weight.view"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.group_std"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "_assert", "(", "x", ".", "dim", "(", ")", "==", "4", ",", "'expected 4D input'", ")", "\n", "x_dtype", "=", "x", ".", "dtype", "\n", "v_shape", "=", "(", "1", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "/", "group_std", "(", "x", ",", "self", ".", "groups", ",", "self", ".", "eps", ")", "\n", "return", "x", "*", "self", ".", "weight", ".", "view", "(", "v_shape", ")", ".", "to", "(", "x_dtype", ")", "+", "self", ".", "bias", ".", "view", "(", "v_shape", ")", ".", "to", "(", "x_dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dS2.__init__": [[307, 326], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "evo_norm.EvoNorm2dS2.reset_parameters", "create_act.create_act_layer", "torch.Identity", "torch.Identity", "torch.Identity", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.create_act_layer"], ["    ", "def", "__init__", "(", "\n", "self", ",", "num_features", ",", "groups", "=", "32", ",", "group_size", "=", "None", ",", "\n", "apply_act", "=", "True", ",", "act_layer", "=", "None", ",", "eps", "=", "1e-5", ",", "**", "_", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "act_layer", "=", "act_layer", "or", "nn", ".", "SiLU", "\n", "self", ".", "apply_act", "=", "apply_act", "# apply activation (non-linearity)", "\n", "if", "act_layer", "is", "not", "None", "and", "apply_act", ":", "\n", "            ", "self", ".", "act", "=", "create_act_layer", "(", "act_layer", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "act", "=", "nn", ".", "Identity", "(", ")", "\n", "", "if", "group_size", ":", "\n", "            ", "assert", "num_features", "%", "group_size", "==", "0", "\n", "self", ".", "groups", "=", "num_features", "//", "group_size", "\n", "", "else", ":", "\n", "            ", "self", ".", "groups", "=", "groups", "\n", "", "self", ".", "eps", "=", "eps", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "num_features", ")", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "num_features", ")", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dS2.reset_parameters": [[327, 330], ["torch.init.ones_", "torch.init.ones_", "torch.init.ones_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "ones_", "(", "self", ".", "weight", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dS2.forward": [[331, 338], ["trace_utils._assert", "evo_norm.EvoNorm2dS2.bias.view().to", "x.dim", "evo_norm.EvoNorm2dS2.act", "evo_norm.group_rms", "evo_norm.EvoNorm2dS2.weight.view().to", "evo_norm.EvoNorm2dS2.bias.view", "evo_norm.EvoNorm2dS2.weight.view"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.group_rms"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "_assert", "(", "x", ".", "dim", "(", ")", "==", "4", ",", "'expected 4D input'", ")", "\n", "x_dtype", "=", "x", ".", "dtype", "\n", "v_shape", "=", "(", "1", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "if", "self", ".", "apply_act", ":", "\n", "            ", "x", "=", "self", ".", "act", "(", "x", ")", "/", "group_rms", "(", "x", ",", "self", ".", "groups", ",", "self", ".", "eps", ")", "\n", "", "return", "x", "*", "self", ".", "weight", ".", "view", "(", "v_shape", ")", ".", "to", "(", "x_dtype", ")", "+", "self", ".", "bias", ".", "view", "(", "v_shape", ")", ".", "to", "(", "x_dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dS2a.__init__": [[341, 346], ["evo_norm.EvoNorm2dS2.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "num_features", ",", "groups", "=", "32", ",", "group_size", "=", "None", ",", "\n", "apply_act", "=", "True", ",", "act_layer", "=", "None", ",", "eps", "=", "1e-3", ",", "**", "_", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "num_features", ",", "groups", "=", "groups", ",", "group_size", "=", "group_size", ",", "apply_act", "=", "apply_act", ",", "act_layer", "=", "act_layer", ",", "eps", "=", "eps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.EvoNorm2dS2a.forward": [[347, 353], ["trace_utils._assert", "evo_norm.EvoNorm2dS2a.act", "evo_norm.group_rms", "evo_norm.EvoNorm2dS2a.bias.view().to", "x.dim", "evo_norm.EvoNorm2dS2a.weight.view().to", "evo_norm.EvoNorm2dS2a.bias.view", "evo_norm.EvoNorm2dS2a.weight.view"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.group_rms"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "_assert", "(", "x", ".", "dim", "(", ")", "==", "4", ",", "'expected 4D input'", ")", "\n", "x_dtype", "=", "x", ".", "dtype", "\n", "v_shape", "=", "(", "1", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "/", "group_rms", "(", "x", ",", "self", ".", "groups", ",", "self", ".", "eps", ")", "\n", "return", "x", "*", "self", ".", "weight", ".", "view", "(", "v_shape", ")", ".", "to", "(", "x_dtype", ")", "+", "self", ".", "bias", ".", "view", "(", "v_shape", ")", ".", "to", "(", "x_dtype", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.instance_std": [[36, 39], ["x.float().var().add().sqrt().to", "x.float().var().add().sqrt().to.expand", "x.float().var().add().sqrt", "x.float().var().add", "x.float().var", "x.float"], "function", ["None"], ["def", "instance_std", "(", "x", ",", "eps", ":", "float", "=", "1e-5", ")", ":", "\n", "    ", "std", "=", "x", ".", "float", "(", ")", ".", "var", "(", "dim", "=", "(", "2", ",", "3", ")", ",", "unbiased", "=", "False", ",", "keepdim", "=", "True", ")", ".", "add", "(", "eps", ")", ".", "sqrt", "(", ")", ".", "to", "(", "x", ".", "dtype", ")", "\n", "return", "std", ".", "expand", "(", "x", ".", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.instance_std_tpu": [[41, 44], ["manual_var().add().sqrt", "manual_var().add().sqrt.expand", "manual_var().add", "evo_norm.manual_var"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.manual_var"], ["", "def", "instance_std_tpu", "(", "x", ",", "eps", ":", "float", "=", "1e-5", ")", ":", "\n", "    ", "std", "=", "manual_var", "(", "x", ",", "dim", "=", "(", "2", ",", "3", ")", ")", ".", "add", "(", "eps", ")", ".", "sqrt", "(", ")", "\n", "return", "std", ".", "expand", "(", "x", ".", "shape", ")", "\n", "# instance_std = instance_std_tpu", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.instance_rms": [[47, 50], ["x.float().square().mean().add().sqrt().to", "x.float().square().mean().add().sqrt().to.expand", "x.float().square().mean().add().sqrt", "x.float().square().mean().add", "x.float().square().mean", "x.float().square", "x.float"], "function", ["None"], ["", "def", "instance_rms", "(", "x", ",", "eps", ":", "float", "=", "1e-5", ")", ":", "\n", "    ", "rms", "=", "x", ".", "float", "(", ")", ".", "square", "(", ")", ".", "mean", "(", "dim", "=", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", ".", "add", "(", "eps", ")", ".", "sqrt", "(", ")", ".", "to", "(", "x", ".", "dtype", ")", "\n", "return", "rms", ".", "expand", "(", "x", ".", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.manual_var": [[52, 60], ["x.mean"], "function", ["None"], ["", "def", "manual_var", "(", "x", ",", "dim", ":", "Union", "[", "int", ",", "Sequence", "[", "int", "]", "]", ",", "diff_sqm", ":", "bool", "=", "False", ")", ":", "\n", "    ", "xm", "=", "x", ".", "mean", "(", "dim", "=", "dim", ",", "keepdim", "=", "True", ")", "\n", "if", "diff_sqm", ":", "\n", "# difference of squared mean and mean squared, faster on TPU can be less stable", "\n", "        ", "var", "=", "(", "(", "x", "*", "x", ")", ".", "mean", "(", "dim", "=", "dim", ",", "keepdim", "=", "True", ")", "-", "(", "xm", "*", "xm", ")", ")", ".", "clamp", "(", "0", ")", "\n", "", "else", ":", "\n", "        ", "var", "=", "(", "(", "x", "-", "xm", ")", "*", "(", "x", "-", "xm", ")", ")", ".", "mean", "(", "dim", "=", "dim", ",", "keepdim", "=", "True", ")", "\n", "", "return", "var", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.group_std": [[62, 73], ["trace_utils._assert", "x.float().var().add().sqrt().to.expand().reshape", "x.reshape.reshape", "x.reshape.float().var().add().sqrt().to", "x.reshape.reshape", "x.reshape.float().var().add().sqrt().to", "x.float().var().add().sqrt().to.expand", "x.reshape.float().var().add().sqrt", "x.reshape.float().var().add().sqrt", "x.reshape.float().var().add", "x.reshape.float().var().add", "x.reshape.float().var", "x.reshape.float().var", "x.reshape.float", "x.reshape.float"], "function", ["None"], ["", "def", "group_std", "(", "x", ",", "groups", ":", "int", "=", "32", ",", "eps", ":", "float", "=", "1e-5", ",", "flatten", ":", "bool", "=", "False", ")", ":", "\n", "    ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "x_dtype", "=", "x", ".", "dtype", "\n", "_assert", "(", "C", "%", "groups", "==", "0", ",", "''", ")", "\n", "if", "flatten", ":", "\n", "        ", "x", "=", "x", ".", "reshape", "(", "B", ",", "groups", ",", "-", "1", ")", "# FIXME simpler shape causing TPU / XLA issues", "\n", "std", "=", "x", ".", "float", "(", ")", ".", "var", "(", "dim", "=", "2", ",", "unbiased", "=", "False", ",", "keepdim", "=", "True", ")", ".", "add", "(", "eps", ")", ".", "sqrt", "(", ")", ".", "to", "(", "x_dtype", ")", "\n", "", "else", ":", "\n", "        ", "x", "=", "x", ".", "reshape", "(", "B", ",", "groups", ",", "C", "//", "groups", ",", "H", ",", "W", ")", "\n", "std", "=", "x", ".", "float", "(", ")", ".", "var", "(", "dim", "=", "(", "2", ",", "3", ",", "4", ")", ",", "unbiased", "=", "False", ",", "keepdim", "=", "True", ")", ".", "add", "(", "eps", ")", ".", "sqrt", "(", ")", ".", "to", "(", "x_dtype", ")", "\n", "", "return", "std", ".", "expand", "(", "x", ".", "shape", ")", ".", "reshape", "(", "B", ",", "C", ",", "H", ",", "W", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.group_std_tpu": [[75, 87], ["trace_utils._assert", "manual_var.add().sqrt().expand().reshape", "x.reshape.reshape", "evo_norm.manual_var", "x.reshape.reshape", "evo_norm.manual_var", "manual_var.add().sqrt().expand", "manual_var.add().sqrt", "manual_var.add"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.manual_var", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.manual_var"], ["", "def", "group_std_tpu", "(", "x", ",", "groups", ":", "int", "=", "32", ",", "eps", ":", "float", "=", "1e-5", ",", "diff_sqm", ":", "bool", "=", "False", ",", "flatten", ":", "bool", "=", "False", ")", ":", "\n", "# This is a workaround for some stability / odd behaviour of .var and .std", "\n", "# running on PyTorch XLA w/ TPUs. These manual var impl are producing much better results", "\n", "    ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "_assert", "(", "C", "%", "groups", "==", "0", ",", "''", ")", "\n", "if", "flatten", ":", "\n", "        ", "x", "=", "x", ".", "reshape", "(", "B", ",", "groups", ",", "-", "1", ")", "# FIXME simpler shape causing TPU / XLA issues", "\n", "var", "=", "manual_var", "(", "x", ",", "dim", "=", "-", "1", ",", "diff_sqm", "=", "diff_sqm", ")", "\n", "", "else", ":", "\n", "        ", "x", "=", "x", ".", "reshape", "(", "B", ",", "groups", ",", "C", "//", "groups", ",", "H", ",", "W", ")", "\n", "var", "=", "manual_var", "(", "x", ",", "dim", "=", "(", "2", ",", "3", ",", "4", ")", ",", "diff_sqm", "=", "diff_sqm", ")", "\n", "", "return", "var", ".", "add", "(", "eps", ")", ".", "sqrt", "(", ")", ".", "expand", "(", "x", ".", "shape", ")", ".", "reshape", "(", "B", ",", "C", ",", "H", ",", "W", ")", "\n", "#group_std = group_std_tpu  # FIXME TPU temporary", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.evo_norm.group_rms": [[90, 97], ["trace_utils._assert", "x.reshape.reshape", "x.reshape.float().square().mean().add().sqrt_().to", "x.float().square().mean().add().sqrt_().to.expand().reshape", "x.reshape.float().square().mean().add().sqrt_", "x.float().square().mean().add().sqrt_().to.expand", "x.reshape.float().square().mean().add", "x.reshape.float().square().mean", "x.reshape.float().square", "x.reshape.float"], "function", ["None"], ["", "def", "group_rms", "(", "x", ",", "groups", ":", "int", "=", "32", ",", "eps", ":", "float", "=", "1e-5", ")", ":", "\n", "    ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "_assert", "(", "C", "%", "groups", "==", "0", ",", "''", ")", "\n", "x_dtype", "=", "x", ".", "dtype", "\n", "x", "=", "x", ".", "reshape", "(", "B", ",", "groups", ",", "C", "//", "groups", ",", "H", ",", "W", ")", "\n", "rms", "=", "x", ".", "float", "(", ")", ".", "square", "(", ")", ".", "mean", "(", "dim", "=", "(", "2", ",", "3", ",", "4", ")", ",", "keepdim", "=", "True", ")", ".", "add", "(", "eps", ")", ".", "sqrt_", "(", ")", ".", "to", "(", "x_dtype", ")", "\n", "return", "rms", ".", "expand", "(", "x", ".", "shape", ")", ".", "reshape", "(", "B", ",", "C", ",", "H", ",", "W", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.squeeze_excite.SEModule.__init__": [[28, 40], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "create_act.create_act_layer", "torch.nn.Conv2d", "create_act.create_act_layer", "helpers.make_divisible", "norm_layer", "torch.nn.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.create_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.create_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible"], ["def", "__init__", "(", "\n", "self", ",", "channels", ",", "rd_ratio", "=", "1.", "/", "16", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "8", ",", "add_maxpool", "=", "False", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "None", ",", "gate_layer", "=", "'sigmoid'", ")", ":", "\n", "        ", "super", "(", "SEModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "add_maxpool", "=", "add_maxpool", "\n", "if", "not", "rd_channels", ":", "\n", "            ", "rd_channels", "=", "make_divisible", "(", "channels", "*", "rd_ratio", ",", "rd_divisor", ",", "round_limit", "=", "0.", ")", "\n", "", "self", ".", "fc1", "=", "nn", ".", "Conv2d", "(", "channels", ",", "rd_channels", ",", "kernel_size", "=", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "bn", "=", "norm_layer", "(", "rd_channels", ")", "if", "norm_layer", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "act", "=", "create_act_layer", "(", "act_layer", ",", "inplace", "=", "True", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Conv2d", "(", "rd_channels", ",", "channels", ",", "kernel_size", "=", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "gate", "=", "create_act_layer", "(", "gate_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.squeeze_excite.SEModule.forward": [[41, 50], ["x.mean", "squeeze_excite.SEModule.fc1", "squeeze_excite.SEModule.act", "squeeze_excite.SEModule.fc2", "squeeze_excite.SEModule.bn", "squeeze_excite.SEModule.gate", "x.amax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_se", "=", "x", ".", "mean", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "if", "self", ".", "add_maxpool", ":", "\n", "# experimental codepath, may remove or change", "\n", "            ", "x_se", "=", "0.5", "*", "x_se", "+", "0.5", "*", "x", ".", "amax", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "", "x_se", "=", "self", ".", "fc1", "(", "x_se", ")", "\n", "x_se", "=", "self", ".", "act", "(", "self", ".", "bn", "(", "x_se", ")", ")", "\n", "x_se", "=", "self", ".", "fc2", "(", "x_se", ")", "\n", "return", "x", "*", "self", ".", "gate", "(", "x_se", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.squeeze_excite.EffectiveSEModule.__init__": [[59, 64], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "create_act.create_act_layer"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.create_act_layer"], ["def", "__init__", "(", "self", ",", "channels", ",", "add_maxpool", "=", "False", ",", "gate_layer", "=", "'hard_sigmoid'", ",", "**", "_", ")", ":", "\n", "        ", "super", "(", "EffectiveSEModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "add_maxpool", "=", "add_maxpool", "\n", "self", ".", "fc", "=", "nn", ".", "Conv2d", "(", "channels", ",", "channels", ",", "kernel_size", "=", "1", ",", "padding", "=", "0", ")", "\n", "self", ".", "gate", "=", "create_act_layer", "(", "gate_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.squeeze_excite.EffectiveSEModule.forward": [[65, 72], ["x.mean", "squeeze_excite.EffectiveSEModule.fc", "squeeze_excite.EffectiveSEModule.gate", "x.amax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_se", "=", "x", ".", "mean", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "if", "self", ".", "add_maxpool", ":", "\n", "# experimental codepath, may remove or change", "\n", "            ", "x_se", "=", "0.5", "*", "x_se", "+", "0.5", "*", "x", ".", "amax", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "", "x_se", "=", "self", ".", "fc", "(", "x_se", ")", "\n", "return", "x", "*", "self", ".", "gate", "(", "x_se", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.create_norm_act_layer": [[43, 49], ["create_norm_act.get_norm_act_layer", "get_norm_act_layer.", "torch.jit.script"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.get_norm_act_layer"], ["def", "create_norm_act_layer", "(", "layer_name", ",", "num_features", ",", "act_layer", "=", "None", ",", "apply_act", "=", "True", ",", "jit", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "layer", "=", "get_norm_act_layer", "(", "layer_name", ",", "act_layer", "=", "act_layer", ")", "\n", "layer_instance", "=", "layer", "(", "num_features", ",", "apply_act", "=", "apply_act", ",", "**", "kwargs", ")", "\n", "if", "jit", ":", "\n", "        ", "layer_instance", "=", "torch", ".", "jit", ".", "script", "(", "layer_instance", ")", "\n", "", "return", "layer_instance", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_norm_act.get_norm_act_layer": [[51, 89], ["isinstance", "isinstance", "isinstance", "isinstance", "norm_act_kwargs.update", "_NORM_ACT_MAP.get", "norm_act_kwargs.setdefault", "functools.partial", "norm_layer.replace().lower().split", "isinstance", "norm_layer.__name__.lower", "norm_layer.__name__.lower.startswith", "norm_layer.replace().lower", "norm_layer.__name__.lower.startswith", "norm_layer.__name__.lower.startswith", "norm_layer.replace", "norm_layer.__name__.lower.startswith"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["", "def", "get_norm_act_layer", "(", "norm_layer", ",", "act_layer", "=", "None", ")", ":", "\n", "    ", "assert", "isinstance", "(", "norm_layer", ",", "(", "type", ",", "str", ",", "types", ".", "FunctionType", ",", "functools", ".", "partial", ")", ")", "\n", "assert", "act_layer", "is", "None", "or", "isinstance", "(", "act_layer", ",", "(", "type", ",", "str", ",", "types", ".", "FunctionType", ",", "functools", ".", "partial", ")", ")", "\n", "norm_act_kwargs", "=", "{", "}", "\n", "\n", "# unbind partial fn, so args can be rebound later", "\n", "if", "isinstance", "(", "norm_layer", ",", "functools", ".", "partial", ")", ":", "\n", "        ", "norm_act_kwargs", ".", "update", "(", "norm_layer", ".", "keywords", ")", "\n", "norm_layer", "=", "norm_layer", ".", "func", "\n", "\n", "", "if", "isinstance", "(", "norm_layer", ",", "str", ")", ":", "\n", "        ", "layer_name", "=", "norm_layer", ".", "replace", "(", "'_'", ",", "''", ")", ".", "lower", "(", ")", ".", "split", "(", "'-'", ")", "[", "0", "]", "\n", "norm_act_layer", "=", "_NORM_ACT_MAP", ".", "get", "(", "layer_name", ",", "None", ")", "\n", "", "elif", "norm_layer", "in", "_NORM_ACT_TYPES", ":", "\n", "        ", "norm_act_layer", "=", "norm_layer", "\n", "", "elif", "isinstance", "(", "norm_layer", ",", "types", ".", "FunctionType", ")", ":", "\n", "# if function type, must be a lambda/fn that creates a norm_act layer", "\n", "        ", "norm_act_layer", "=", "norm_layer", "\n", "", "else", ":", "\n", "        ", "type_name", "=", "norm_layer", ".", "__name__", ".", "lower", "(", ")", "\n", "if", "type_name", ".", "startswith", "(", "'batchnorm'", ")", ":", "\n", "            ", "norm_act_layer", "=", "BatchNormAct2d", "\n", "", "elif", "type_name", ".", "startswith", "(", "'groupnorm'", ")", ":", "\n", "            ", "norm_act_layer", "=", "GroupNormAct", "\n", "", "elif", "type_name", ".", "startswith", "(", "'layernorm2d'", ")", ":", "\n", "            ", "norm_act_layer", "=", "LayerNormAct2d", "\n", "", "elif", "type_name", ".", "startswith", "(", "'layernorm'", ")", ":", "\n", "            ", "norm_act_layer", "=", "LayerNormAct", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "f\"No equivalent norm_act layer for {type_name}\"", "\n", "\n", "", "", "if", "norm_act_layer", "in", "_NORM_ACT_REQUIRES_ARG", ":", "\n", "# pass `act_layer` through for backwards compat where `act_layer=None` implies no activation.", "\n", "# In the future, may force use of `apply_act` with `act_layer` arg bound to relevant NormAct types", "\n", "        ", "norm_act_kwargs", ".", "setdefault", "(", "'act_layer'", ",", "act_layer", ")", "\n", "", "if", "norm_act_kwargs", ":", "\n", "        ", "norm_act_layer", "=", "functools", ".", "partial", "(", "norm_act_layer", ",", "**", "norm_act_kwargs", ")", "# bind/rebind args", "\n", "", "return", "norm_act_layer", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.gather_excite.GatherExcite.__init__": [[28, 69], ["torch.nn.Module.__init__", "create_act.get_act_layer", "create_act.create_act_layer", "torch.nn.Sequential", "helpers.make_divisible", "mlp.ConvMlp", "torch.nn.Identity", "gather_excite.GatherExcite.gather.add_module", "int", "range", "create_conv2d.create_conv2d.create_conv2d", "gather_excite.GatherExcite.gather.add_module", "math.log2", "gather_excite.GatherExcite.gather.add_module", "torch.nn.BatchNorm2d", "create_conv2d.create_conv2d.create_conv2d", "gather_excite.GatherExcite.gather.add_module", "gather_excite.GatherExcite.gather.add_module", "torch.nn.BatchNorm2d", "create_act.get_act_layer."], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.create_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_conv2d.create_conv2d"], ["def", "__init__", "(", "\n", "self", ",", "channels", ",", "feat_size", "=", "None", ",", "extra_params", "=", "False", ",", "extent", "=", "0", ",", "use_mlp", "=", "True", ",", "\n", "rd_ratio", "=", "1.", "/", "16", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "1", ",", "add_maxpool", "=", "False", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "gate_layer", "=", "'sigmoid'", ")", ":", "\n", "        ", "super", "(", "GatherExcite", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "add_maxpool", "=", "add_maxpool", "\n", "act_layer", "=", "get_act_layer", "(", "act_layer", ")", "\n", "self", ".", "extent", "=", "extent", "\n", "if", "extra_params", ":", "\n", "            ", "self", ".", "gather", "=", "nn", ".", "Sequential", "(", ")", "\n", "if", "extent", "==", "0", ":", "\n", "                ", "assert", "feat_size", "is", "not", "None", ",", "'spatial feature size must be specified for global extent w/ params'", "\n", "self", ".", "gather", ".", "add_module", "(", "\n", "'conv1'", ",", "create_conv2d", "(", "channels", ",", "channels", ",", "kernel_size", "=", "feat_size", ",", "stride", "=", "1", ",", "depthwise", "=", "True", ")", ")", "\n", "if", "norm_layer", ":", "\n", "                    ", "self", ".", "gather", ".", "add_module", "(", "f'norm1'", ",", "nn", ".", "BatchNorm2d", "(", "channels", ")", ")", "\n", "", "", "else", ":", "\n", "                ", "assert", "extent", "%", "2", "==", "0", "\n", "num_conv", "=", "int", "(", "math", ".", "log2", "(", "extent", ")", ")", "\n", "for", "i", "in", "range", "(", "num_conv", ")", ":", "\n", "                    ", "self", ".", "gather", ".", "add_module", "(", "\n", "f'conv{i + 1}'", ",", "\n", "create_conv2d", "(", "channels", ",", "channels", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "depthwise", "=", "True", ")", ")", "\n", "if", "norm_layer", ":", "\n", "                        ", "self", ".", "gather", ".", "add_module", "(", "f'norm{i + 1}'", ",", "nn", ".", "BatchNorm2d", "(", "channels", ")", ")", "\n", "", "if", "i", "!=", "num_conv", "-", "1", ":", "\n", "                        ", "self", ".", "gather", ".", "add_module", "(", "f'act{i + 1}'", ",", "act_layer", "(", "inplace", "=", "True", ")", ")", "\n", "", "", "", "", "else", ":", "\n", "            ", "self", ".", "gather", "=", "None", "\n", "if", "self", ".", "extent", "==", "0", ":", "\n", "                ", "self", ".", "gk", "=", "0", "\n", "self", ".", "gs", "=", "0", "\n", "", "else", ":", "\n", "                ", "assert", "extent", "%", "2", "==", "0", "\n", "self", ".", "gk", "=", "self", ".", "extent", "*", "2", "-", "1", "\n", "self", ".", "gs", "=", "self", ".", "extent", "\n", "\n", "", "", "if", "not", "rd_channels", ":", "\n", "            ", "rd_channels", "=", "make_divisible", "(", "channels", "*", "rd_ratio", ",", "rd_divisor", ",", "round_limit", "=", "0.", ")", "\n", "", "self", ".", "mlp", "=", "ConvMlp", "(", "channels", ",", "rd_channels", ",", "act_layer", "=", "act_layer", ")", "if", "use_mlp", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "gate", "=", "create_act_layer", "(", "gate_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.gather_excite.GatherExcite.forward": [[70, 91], ["gather_excite.GatherExcite.mlp", "gather_excite.GatherExcite.gather", "torch.interpolate", "gather_excite.GatherExcite.gate", "x.mean", "torch.avg_pool2d", "x.amax", "torch.max_pool2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.coat.ParallelBlock.interpolate"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "size", "=", "x", ".", "shape", "[", "-", "2", ":", "]", "\n", "if", "self", ".", "gather", "is", "not", "None", ":", "\n", "            ", "x_ge", "=", "self", ".", "gather", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "extent", "==", "0", ":", "\n", "# global extent", "\n", "                ", "x_ge", "=", "x", ".", "mean", "(", "dim", "=", "(", "2", ",", "3", ")", ",", "keepdims", "=", "True", ")", "\n", "if", "self", ".", "add_maxpool", ":", "\n", "# experimental codepath, may remove or change", "\n", "                    ", "x_ge", "=", "0.5", "*", "x_ge", "+", "0.5", "*", "x", ".", "amax", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "", "", "else", ":", "\n", "                ", "x_ge", "=", "F", ".", "avg_pool2d", "(", "\n", "x", ",", "kernel_size", "=", "self", ".", "gk", ",", "stride", "=", "self", ".", "gs", ",", "padding", "=", "self", ".", "gk", "//", "2", ",", "count_include_pad", "=", "False", ")", "\n", "if", "self", ".", "add_maxpool", ":", "\n", "# experimental codepath, may remove or change", "\n", "                    ", "x_ge", "=", "0.5", "*", "x_ge", "+", "0.5", "*", "F", ".", "max_pool2d", "(", "x", ",", "kernel_size", "=", "self", ".", "gk", ",", "stride", "=", "self", ".", "gs", ",", "padding", "=", "self", ".", "gk", "//", "2", ")", "\n", "", "", "", "x_ge", "=", "self", ".", "mlp", "(", "x_ge", ")", "\n", "if", "x_ge", ".", "shape", "[", "-", "1", "]", "!=", "1", "or", "x_ge", ".", "shape", "[", "-", "2", "]", "!=", "1", ":", "\n", "            ", "x_ge", "=", "F", ".", "interpolate", "(", "x_ge", ",", "size", "=", "size", ")", "\n", "", "return", "x", "*", "self", ".", "gate", "(", "x_ge", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.norm_act.BatchNormAct2d.__init__": [[20, 51], ["create_act.get_act_layer", "torch.nn.BatchNorm2d.__init__", "drop_layer", "torch.nn.Identity", "create_act.get_act_layer.", "torch.nn.Identity", "torch.nn.BatchNorm2d.__init__", "dict"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "num_features", ",", "\n", "eps", "=", "1e-5", ",", "\n", "momentum", "=", "0.1", ",", "\n", "affine", "=", "True", ",", "\n", "track_running_stats", "=", "True", ",", "\n", "apply_act", "=", "True", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "\n", "inplace", "=", "True", ",", "\n", "drop_layer", "=", "None", ",", "\n", "device", "=", "None", ",", "\n", "dtype", "=", "None", "\n", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "factory_kwargs", "=", "{", "'device'", ":", "device", ",", "'dtype'", ":", "dtype", "}", "\n", "super", "(", "BatchNormAct2d", ",", "self", ")", ".", "__init__", "(", "\n", "num_features", ",", "eps", "=", "eps", ",", "momentum", "=", "momentum", ",", "affine", "=", "affine", ",", "track_running_stats", "=", "track_running_stats", ",", "\n", "**", "factory_kwargs", "\n", ")", "\n", "", "except", "TypeError", ":", "\n", "# NOTE for backwards compat with old PyTorch w/o factory device/dtype support", "\n", "            ", "super", "(", "BatchNormAct2d", ",", "self", ")", ".", "__init__", "(", "\n", "num_features", ",", "eps", "=", "eps", ",", "momentum", "=", "momentum", ",", "affine", "=", "affine", ",", "track_running_stats", "=", "track_running_stats", ")", "\n", "", "self", ".", "drop", "=", "drop_layer", "(", ")", "if", "drop_layer", "is", "not", "None", "else", "nn", ".", "Identity", "(", ")", "\n", "act_layer", "=", "get_act_layer", "(", "act_layer", ")", "# string -> nn.Module", "\n", "if", "act_layer", "is", "not", "None", "and", "apply_act", ":", "\n", "            ", "act_args", "=", "dict", "(", "inplace", "=", "True", ")", "if", "inplace", "else", "{", "}", "\n", "self", ".", "act", "=", "act_layer", "(", "**", "act_args", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "act", "=", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.norm_act.BatchNormAct2d.forward": [[52, 101], ["trace_utils._assert", "torch.nn.functional.batch_norm", "norm_act.BatchNormAct2d.drop", "norm_act.BatchNormAct2d.act", "float"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# cut & paste of torch.nn.BatchNorm2d.forward impl to avoid issues with torchscript and tracing", "\n", "        ", "_assert", "(", "x", ".", "ndim", "==", "4", ",", "f'expected 4D input (got {x.ndim}D input)'", ")", "\n", "\n", "# exponential_average_factor is set to self.momentum", "\n", "# (when it is available) only so that it gets updated", "\n", "# in ONNX graph when this node is exported to ONNX.", "\n", "if", "self", ".", "momentum", "is", "None", ":", "\n", "            ", "exponential_average_factor", "=", "0.0", "\n", "", "else", ":", "\n", "            ", "exponential_average_factor", "=", "self", ".", "momentum", "\n", "\n", "", "if", "self", ".", "training", "and", "self", ".", "track_running_stats", ":", "\n", "# TODO: if statement only here to tell the jit to skip emitting this when it is None", "\n", "            ", "if", "self", ".", "num_batches_tracked", "is", "not", "None", ":", "# type: ignore[has-type]", "\n", "                ", "self", ".", "num_batches_tracked", "=", "self", ".", "num_batches_tracked", "+", "1", "# type: ignore[has-type]", "\n", "if", "self", ".", "momentum", "is", "None", ":", "# use cumulative moving average", "\n", "                    ", "exponential_average_factor", "=", "1.0", "/", "float", "(", "self", ".", "num_batches_tracked", ")", "\n", "", "else", ":", "# use exponential moving average", "\n", "                    ", "exponential_average_factor", "=", "self", ".", "momentum", "\n", "\n", "", "", "", "r\"\"\"\n        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n        \"\"\"", "\n", "if", "self", ".", "training", ":", "\n", "            ", "bn_training", "=", "True", "\n", "", "else", ":", "\n", "            ", "bn_training", "=", "(", "self", ".", "running_mean", "is", "None", ")", "and", "(", "self", ".", "running_var", "is", "None", ")", "\n", "\n", "", "r\"\"\"\n        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n        used for normalization (i.e. in eval mode when buffers are not None).\n        \"\"\"", "\n", "x", "=", "F", ".", "batch_norm", "(", "\n", "x", ",", "\n", "# If buffers are not to be tracked, ensure that they won't be updated", "\n", "self", ".", "running_mean", "if", "not", "self", ".", "training", "or", "self", ".", "track_running_stats", "else", "None", ",", "\n", "self", ".", "running_var", "if", "not", "self", ".", "training", "or", "self", ".", "track_running_stats", "else", "None", ",", "\n", "self", ".", "weight", ",", "\n", "self", ".", "bias", ",", "\n", "bn_training", ",", "\n", "exponential_average_factor", ",", "\n", "self", ".", "eps", ",", "\n", ")", "\n", "x", "=", "self", ".", "drop", "(", "x", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.norm_act.SyncBatchNormAct.forward": [[108, 115], ["super().forward", "hasattr", "hasattr", "norm_act.SyncBatchNormAct.drop", "norm_act.SyncBatchNormAct.act"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.forward"], ["    ", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "x", "=", "super", "(", ")", ".", "forward", "(", "x", ")", "# SyncBN doesn't work with torchscript anyways, so this is fine", "\n", "if", "hasattr", "(", "self", ",", "\"drop\"", ")", ":", "\n", "            ", "x", "=", "self", ".", "drop", "(", "x", ")", "\n", "", "if", "hasattr", "(", "self", ",", "\"act\"", ")", ":", "\n", "            ", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.norm_act.GroupNormAct.__init__": [[168, 180], ["torch.nn.GroupNorm.__init__", "create_act.get_act_layer", "norm_act._num_groups", "drop_layer", "torch.nn.Identity", "create_act.get_act_layer.", "torch.nn.Identity", "dict"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_layer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.norm_act._num_groups"], ["    ", "def", "__init__", "(", "\n", "self", ",", "num_channels", ",", "num_groups", "=", "32", ",", "eps", "=", "1e-5", ",", "affine", "=", "True", ",", "group_size", "=", "None", ",", "\n", "apply_act", "=", "True", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "inplace", "=", "True", ",", "drop_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "GroupNormAct", ",", "self", ")", ".", "__init__", "(", "\n", "_num_groups", "(", "num_channels", ",", "num_groups", ",", "group_size", ")", ",", "num_channels", ",", "eps", "=", "eps", ",", "affine", "=", "affine", ")", "\n", "self", ".", "drop", "=", "drop_layer", "(", ")", "if", "drop_layer", "is", "not", "None", "else", "nn", ".", "Identity", "(", ")", "\n", "act_layer", "=", "get_act_layer", "(", "act_layer", ")", "# string -> nn.Module", "\n", "if", "act_layer", "is", "not", "None", "and", "apply_act", ":", "\n", "            ", "act_args", "=", "dict", "(", "inplace", "=", "True", ")", "if", "inplace", "else", "{", "}", "\n", "self", ".", "act", "=", "act_layer", "(", "**", "act_args", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "act", "=", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.norm_act.GroupNormAct.forward": [[181, 186], ["torch.nn.functional.group_norm", "norm_act.GroupNormAct.drop", "norm_act.GroupNormAct.act"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "F", ".", "group_norm", "(", "x", ",", "self", ".", "num_groups", ",", "self", ".", "weight", ",", "self", ".", "bias", ",", "self", ".", "eps", ")", "\n", "x", "=", "self", ".", "drop", "(", "x", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.norm_act.LayerNormAct.__init__": [[189, 200], ["torch.nn.LayerNorm.__init__", "create_act.get_act_layer", "drop_layer", "torch.nn.Identity", "create_act.get_act_layer.", "torch.nn.Identity", "dict"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_layer"], ["    ", "def", "__init__", "(", "\n", "self", ",", "normalization_shape", ":", "Union", "[", "int", ",", "List", "[", "int", "]", ",", "torch", ".", "Size", "]", ",", "eps", "=", "1e-5", ",", "affine", "=", "True", ",", "\n", "apply_act", "=", "True", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "inplace", "=", "True", ",", "drop_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "LayerNormAct", ",", "self", ")", ".", "__init__", "(", "normalization_shape", ",", "eps", "=", "eps", ",", "elementwise_affine", "=", "affine", ")", "\n", "self", ".", "drop", "=", "drop_layer", "(", ")", "if", "drop_layer", "is", "not", "None", "else", "nn", ".", "Identity", "(", ")", "\n", "act_layer", "=", "get_act_layer", "(", "act_layer", ")", "# string -> nn.Module", "\n", "if", "act_layer", "is", "not", "None", "and", "apply_act", ":", "\n", "            ", "act_args", "=", "dict", "(", "inplace", "=", "True", ")", "if", "inplace", "else", "{", "}", "\n", "self", ".", "act", "=", "act_layer", "(", "**", "act_args", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "act", "=", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.norm_act.LayerNormAct.forward": [[201, 206], ["torch.nn.functional.layer_norm", "norm_act.LayerNormAct.drop", "norm_act.LayerNormAct.act"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "F", ".", "layer_norm", "(", "x", ",", "self", ".", "normalized_shape", ",", "self", ".", "weight", ",", "self", ".", "bias", ",", "self", ".", "eps", ")", "\n", "x", "=", "self", ".", "drop", "(", "x", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.norm_act.LayerNormAct2d.__init__": [[209, 220], ["torch.nn.LayerNorm.__init__", "create_act.get_act_layer", "drop_layer", "torch.nn.Identity", "create_act.get_act_layer.", "torch.nn.Identity", "dict"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_act.get_act_layer"], ["    ", "def", "__init__", "(", "\n", "self", ",", "num_channels", ",", "eps", "=", "1e-5", ",", "affine", "=", "True", ",", "\n", "apply_act", "=", "True", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "inplace", "=", "True", ",", "drop_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "LayerNormAct2d", ",", "self", ")", ".", "__init__", "(", "num_channels", ",", "eps", "=", "eps", ",", "elementwise_affine", "=", "affine", ")", "\n", "self", ".", "drop", "=", "drop_layer", "(", ")", "if", "drop_layer", "is", "not", "None", "else", "nn", ".", "Identity", "(", ")", "\n", "act_layer", "=", "get_act_layer", "(", "act_layer", ")", "# string -> nn.Module", "\n", "if", "act_layer", "is", "not", "None", "and", "apply_act", ":", "\n", "            ", "act_args", "=", "dict", "(", "inplace", "=", "True", ")", "if", "inplace", "else", "{", "}", "\n", "self", ".", "act", "=", "act_layer", "(", "**", "act_args", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "act", "=", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.norm_act.LayerNormAct2d.forward": [[221, 227], ["torch.nn.functional.layer_norm().permute", "norm_act.LayerNormAct2d.drop", "norm_act.LayerNormAct2d.act", "torch.nn.functional.layer_norm", "norm_act.LayerNormAct2d.permute"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "F", ".", "layer_norm", "(", "\n", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ",", "self", ".", "normalized_shape", ",", "self", ".", "weight", ",", "self", ".", "bias", ",", "self", ".", "eps", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "x", "=", "self", ".", "drop", "(", "x", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.norm_act.convert_sync_batchnorm": [[117, 157], ["isinstance", "module.named_children", "isinstance", "hasattr", "torch.nn.SyncBatchNorm.add_module", "norm_act.SyncBatchNormAct", "torch.nn.SyncBatchNorm", "norm_act.convert_sync_batchnorm", "torch.no_grad"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.norm_act.convert_sync_batchnorm"], ["", "", "def", "convert_sync_batchnorm", "(", "module", ",", "process_group", "=", "None", ")", ":", "\n", "# convert both BatchNorm and BatchNormAct layers to Synchronized variants", "\n", "    ", "module_output", "=", "module", "\n", "if", "isinstance", "(", "module", ",", "torch", ".", "nn", ".", "modules", ".", "batchnorm", ".", "_BatchNorm", ")", ":", "\n", "        ", "if", "isinstance", "(", "module", ",", "BatchNormAct2d", ")", ":", "\n", "# convert timm norm + act layer", "\n", "            ", "module_output", "=", "SyncBatchNormAct", "(", "\n", "module", ".", "num_features", ",", "\n", "module", ".", "eps", ",", "\n", "module", ".", "momentum", ",", "\n", "module", ".", "affine", ",", "\n", "module", ".", "track_running_stats", ",", "\n", "process_group", "=", "process_group", ",", "\n", ")", "\n", "# set act and drop attr from the original module", "\n", "module_output", ".", "act", "=", "module", ".", "act", "\n", "module_output", ".", "drop", "=", "module", ".", "drop", "\n", "", "else", ":", "\n", "# convert standard BatchNorm layers", "\n", "            ", "module_output", "=", "torch", ".", "nn", ".", "SyncBatchNorm", "(", "\n", "module", ".", "num_features", ",", "\n", "module", ".", "eps", ",", "\n", "module", ".", "momentum", ",", "\n", "module", ".", "affine", ",", "\n", "module", ".", "track_running_stats", ",", "\n", "process_group", ",", "\n", ")", "\n", "", "if", "module", ".", "affine", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "module_output", ".", "weight", "=", "module", ".", "weight", "\n", "module_output", ".", "bias", "=", "module", ".", "bias", "\n", "", "", "module_output", ".", "running_mean", "=", "module", ".", "running_mean", "\n", "module_output", ".", "running_var", "=", "module", ".", "running_var", "\n", "module_output", ".", "num_batches_tracked", "=", "module", ".", "num_batches_tracked", "\n", "if", "hasattr", "(", "module", ",", "\"qconfig\"", ")", ":", "\n", "            ", "module_output", ".", "qconfig", "=", "module", ".", "qconfig", "\n", "", "", "for", "name", ",", "child", "in", "module", ".", "named_children", "(", ")", ":", "\n", "        ", "module_output", ".", "add_module", "(", "name", ",", "convert_sync_batchnorm", "(", "child", ",", "process_group", ")", ")", "\n", "", "del", "module", "\n", "return", "module_output", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.norm_act._num_groups": [[159, 164], ["None"], "function", ["None"], ["", "def", "_num_groups", "(", "num_channels", ",", "num_groups", ",", "group_size", ")", ":", "\n", "    ", "if", "group_size", ":", "\n", "        ", "assert", "num_channels", "%", "group_size", "==", "0", "\n", "return", "num_channels", "//", "group_size", "\n", "", "return", "num_groups", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.get_attn": [[21, 82], ["isinstance", "isinstance", "attn_type.lower.lower", "isinstance", "functools.partial", "functools.partial"], "function", ["None"], ["def", "get_attn", "(", "attn_type", ")", ":", "\n", "    ", "if", "isinstance", "(", "attn_type", ",", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "        ", "return", "attn_type", "\n", "", "module_cls", "=", "None", "\n", "if", "attn_type", "is", "not", "None", ":", "\n", "        ", "if", "isinstance", "(", "attn_type", ",", "str", ")", ":", "\n", "            ", "attn_type", "=", "attn_type", ".", "lower", "(", ")", "\n", "# Lightweight attention modules (channel and/or coarse spatial).", "\n", "# Typically added to existing network architecture blocks in addition to existing convolutions.", "\n", "if", "attn_type", "==", "'se'", ":", "\n", "                ", "module_cls", "=", "SEModule", "\n", "", "elif", "attn_type", "==", "'ese'", ":", "\n", "                ", "module_cls", "=", "EffectiveSEModule", "\n", "", "elif", "attn_type", "==", "'eca'", ":", "\n", "                ", "module_cls", "=", "EcaModule", "\n", "", "elif", "attn_type", "==", "'ecam'", ":", "\n", "                ", "module_cls", "=", "partial", "(", "EcaModule", ",", "use_mlp", "=", "True", ")", "\n", "", "elif", "attn_type", "==", "'ceca'", ":", "\n", "                ", "module_cls", "=", "CecaModule", "\n", "", "elif", "attn_type", "==", "'ge'", ":", "\n", "                ", "module_cls", "=", "GatherExcite", "\n", "", "elif", "attn_type", "==", "'gc'", ":", "\n", "                ", "module_cls", "=", "GlobalContext", "\n", "", "elif", "attn_type", "==", "'gca'", ":", "\n", "                ", "module_cls", "=", "partial", "(", "GlobalContext", ",", "fuse_add", "=", "True", ",", "fuse_scale", "=", "False", ")", "\n", "", "elif", "attn_type", "==", "'cbam'", ":", "\n", "                ", "module_cls", "=", "CbamModule", "\n", "", "elif", "attn_type", "==", "'lcbam'", ":", "\n", "                ", "module_cls", "=", "LightCbamModule", "\n", "\n", "# Attention / attention-like modules w/ significant params", "\n", "# Typically replace some of the existing workhorse convs in a network architecture.", "\n", "# All of these accept a stride argument and can spatially downsample the input.", "\n", "", "elif", "attn_type", "==", "'sk'", ":", "\n", "                ", "module_cls", "=", "SelectiveKernel", "\n", "", "elif", "attn_type", "==", "'splat'", ":", "\n", "                ", "module_cls", "=", "SplitAttn", "\n", "\n", "# Self-attention / attention-like modules w/ significant compute and/or params", "\n", "# Typically replace some of the existing workhorse convs in a network architecture.", "\n", "# All of these accept a stride argument and can spatially downsample the input.", "\n", "", "elif", "attn_type", "==", "'lambda'", ":", "\n", "                ", "return", "LambdaLayer", "\n", "", "elif", "attn_type", "==", "'bottleneck'", ":", "\n", "                ", "return", "BottleneckAttn", "\n", "", "elif", "attn_type", "==", "'halo'", ":", "\n", "                ", "return", "HaloAttn", "\n", "", "elif", "attn_type", "==", "'nl'", ":", "\n", "                ", "module_cls", "=", "NonLocalAttn", "\n", "", "elif", "attn_type", "==", "'bat'", ":", "\n", "                ", "module_cls", "=", "BatNonLocalAttn", "\n", "\n", "# Woops!", "\n", "", "else", ":", "\n", "                ", "assert", "False", ",", "\"Invalid attn module (%s)\"", "%", "attn_type", "\n", "", "", "elif", "isinstance", "(", "attn_type", ",", "bool", ")", ":", "\n", "            ", "if", "attn_type", ":", "\n", "                ", "module_cls", "=", "SEModule", "\n", "", "", "else", ":", "\n", "            ", "module_cls", "=", "attn_type", "\n", "", "", "return", "module_cls", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.create_attn": [[84, 90], ["create_attn.get_attn", "get_attn."], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.create_attn.get_attn"], ["", "def", "create_attn", "(", "attn_type", ",", "channels", ",", "**", "kwargs", ")", ":", "\n", "    ", "module_cls", "=", "get_attn", "(", "attn_type", ")", "\n", "if", "module_cls", "is", "not", "None", ":", "\n", "# NOTE: it's expected the first (positional) argument of all attention layers is the # input channels", "\n", "        ", "return", "module_cls", "(", "channels", ",", "**", "kwargs", ")", "\n", "", "return", "None", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.cond_conv2d.CondConv2d.__init__": [[43, 72], ["torch.nn.Module.__init__", "helpers.to_2tuple", "helpers.to_2tuple", "padding.get_padding_value", "helpers.to_2tuple", "helpers.to_2tuple", "torch.nn.Parameter", "cond_conv2d.CondConv2d.reset_parameters", "torch.Tensor", "torch.nn.Parameter", "cond_conv2d.CondConv2d.register_parameter", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.padding.get_padding_value", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "3", ",", "\n", "stride", "=", "1", ",", "padding", "=", "''", ",", "dilation", "=", "1", ",", "groups", "=", "1", ",", "bias", "=", "False", ",", "num_experts", "=", "4", ")", ":", "\n", "        ", "super", "(", "CondConv2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "kernel_size", "=", "to_2tuple", "(", "kernel_size", ")", "\n", "self", ".", "stride", "=", "to_2tuple", "(", "stride", ")", "\n", "padding_val", ",", "is_padding_dynamic", "=", "get_padding_value", "(", "\n", "padding", ",", "kernel_size", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ")", "\n", "self", ".", "dynamic_padding", "=", "is_padding_dynamic", "# if in forward to work with torchscript", "\n", "self", ".", "padding", "=", "to_2tuple", "(", "padding_val", ")", "\n", "self", ".", "dilation", "=", "to_2tuple", "(", "dilation", ")", "\n", "self", ".", "groups", "=", "groups", "\n", "self", ".", "num_experts", "=", "num_experts", "\n", "\n", "self", ".", "weight_shape", "=", "(", "self", ".", "out_channels", ",", "self", ".", "in_channels", "//", "self", ".", "groups", ")", "+", "self", ".", "kernel_size", "\n", "weight_num_param", "=", "1", "\n", "for", "wd", "in", "self", ".", "weight_shape", ":", "\n", "            ", "weight_num_param", "*=", "wd", "\n", "", "self", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "num_experts", ",", "weight_num_param", ")", ")", "\n", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "bias_shape", "=", "(", "self", ".", "out_channels", ",", ")", "\n", "self", ".", "bias", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "num_experts", ",", "self", ".", "out_channels", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'bias'", ",", "None", ")", "\n", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.cond_conv2d.CondConv2d.reset_parameters": [[73, 83], ["cond_conv2d.get_condconv_initializer", "get_condconv_initializer.", "functools.partial", "numpy.prod", "cond_conv2d.get_condconv_initializer", "get_condconv_initializer.", "math.sqrt", "functools.partial", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.cond_conv2d.get_condconv_initializer", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.cond_conv2d.get_condconv_initializer"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "init_weight", "=", "get_condconv_initializer", "(", "\n", "partial", "(", "nn", ".", "init", ".", "kaiming_uniform_", ",", "a", "=", "math", ".", "sqrt", "(", "5", ")", ")", ",", "self", ".", "num_experts", ",", "self", ".", "weight_shape", ")", "\n", "init_weight", "(", "self", ".", "weight", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "fan_in", "=", "np", ".", "prod", "(", "self", ".", "weight_shape", "[", "1", ":", "]", ")", "\n", "bound", "=", "1", "/", "math", ".", "sqrt", "(", "fan_in", ")", "\n", "init_bias", "=", "get_condconv_initializer", "(", "\n", "partial", "(", "nn", ".", "init", ".", "uniform_", ",", "a", "=", "-", "bound", ",", "b", "=", "bound", ")", ",", "self", ".", "num_experts", ",", "self", ".", "bias_shape", ")", "\n", "init_bias", "(", "self", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.cond_conv2d.CondConv2d.forward": [[84, 124], ["torch.matmul", "weight.view.view.view", "x.reshape.reshape.reshape", "torch.nn.functional.conv2d.permute().view", "torch.matmul", "bias.view.view.view", "conv2d_same.conv2d_same.conv2d_same", "torch.nn.functional.conv2d", "torch.nn.functional.conv2d.permute"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.conv2d_same.conv2d_same"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "routing_weights", ")", ":", "\n", "        ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "weight", "=", "torch", ".", "matmul", "(", "routing_weights", ",", "self", ".", "weight", ")", "\n", "new_weight_shape", "=", "(", "B", "*", "self", ".", "out_channels", ",", "self", ".", "in_channels", "//", "self", ".", "groups", ")", "+", "self", ".", "kernel_size", "\n", "weight", "=", "weight", ".", "view", "(", "new_weight_shape", ")", "\n", "bias", "=", "None", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "bias", "=", "torch", ".", "matmul", "(", "routing_weights", ",", "self", ".", "bias", ")", "\n", "bias", "=", "bias", ".", "view", "(", "B", "*", "self", ".", "out_channels", ")", "\n", "# move batch elements with channels so each batch element can be efficiently convolved with separate kernel", "\n", "# reshape instead of view to work with channels_last input", "\n", "", "x", "=", "x", ".", "reshape", "(", "1", ",", "B", "*", "C", ",", "H", ",", "W", ")", "\n", "if", "self", ".", "dynamic_padding", ":", "\n", "            ", "out", "=", "conv2d_same", "(", "\n", "x", ",", "weight", ",", "bias", ",", "stride", "=", "self", ".", "stride", ",", "padding", "=", "self", ".", "padding", ",", "\n", "dilation", "=", "self", ".", "dilation", ",", "groups", "=", "self", ".", "groups", "*", "B", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "F", ".", "conv2d", "(", "\n", "x", ",", "weight", ",", "bias", ",", "stride", "=", "self", ".", "stride", ",", "padding", "=", "self", ".", "padding", ",", "\n", "dilation", "=", "self", ".", "dilation", ",", "groups", "=", "self", ".", "groups", "*", "B", ")", "\n", "", "out", "=", "out", ".", "permute", "(", "[", "1", ",", "0", ",", "2", ",", "3", "]", ")", ".", "view", "(", "B", ",", "self", ".", "out_channels", ",", "out", ".", "shape", "[", "-", "2", "]", ",", "out", ".", "shape", "[", "-", "1", "]", ")", "\n", "\n", "# Literal port (from TF definition)", "\n", "# x = torch.split(x, 1, 0)", "\n", "# weight = torch.split(weight, 1, 0)", "\n", "# if self.bias is not None:", "\n", "#     bias = torch.matmul(routing_weights, self.bias)", "\n", "#     bias = torch.split(bias, 1, 0)", "\n", "# else:", "\n", "#     bias = [None] * B", "\n", "# out = []", "\n", "# for xi, wi, bi in zip(x, weight, bias):", "\n", "#     wi = wi.view(*self.weight_shape)", "\n", "#     if bi is not None:", "\n", "#         bi = bi.view(*self.bias_shape)", "\n", "#     out.append(self.conv_fn(", "\n", "#         xi, wi, bi, stride=self.stride, padding=self.padding,", "\n", "#         dilation=self.dilation, groups=self.groups))", "\n", "# out = torch.cat(out, 0)", "\n", "return", "out", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.cond_conv2d.get_condconv_initializer": [[21, 32], ["numpy.prod", "range", "ValueError", "initializer", "len", "weight[].view"], "function", ["None"], ["def", "get_condconv_initializer", "(", "initializer", ",", "num_experts", ",", "expert_shape", ")", ":", "\n", "    ", "def", "condconv_initializer", "(", "weight", ")", ":", "\n", "        ", "\"\"\"CondConv initializer function.\"\"\"", "\n", "num_params", "=", "np", ".", "prod", "(", "expert_shape", ")", "\n", "if", "(", "len", "(", "weight", ".", "shape", ")", "!=", "2", "or", "weight", ".", "shape", "[", "0", "]", "!=", "num_experts", "or", "\n", "weight", ".", "shape", "[", "1", "]", "!=", "num_params", ")", ":", "\n", "            ", "raise", "(", "ValueError", "(", "\n", "'CondConv variables must have shape [num_experts, num_params]'", ")", ")", "\n", "", "for", "i", "in", "range", "(", "num_experts", ")", ":", "\n", "            ", "initializer", "(", "weight", "[", "i", "]", ".", "view", "(", "expert_shape", ")", ")", "\n", "", "", "return", "condconv_initializer", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.space_to_depth.SpaceToDepth.__init__": [[6, 10], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "block_size", "=", "4", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "block_size", "==", "4", "\n", "self", ".", "bs", "=", "block_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.space_to_depth.SpaceToDepth.forward": [[11, 17], ["x.view.view.size", "x.view.view.view", "x.view.view.permute().contiguous", "x.view.view.view", "x.view.view.permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "N", ",", "C", ",", "H", ",", "W", "=", "x", ".", "size", "(", ")", "\n", "x", "=", "x", ".", "view", "(", "N", ",", "C", ",", "H", "//", "self", ".", "bs", ",", "self", ".", "bs", ",", "W", "//", "self", ".", "bs", ",", "self", ".", "bs", ")", "# (N, C, H//bs, bs, W//bs, bs)", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "3", ",", "5", ",", "1", ",", "2", ",", "4", ")", ".", "contiguous", "(", ")", "# (N, bs, bs, C, H//bs, W//bs)", "\n", "x", "=", "x", ".", "view", "(", "N", ",", "C", "*", "(", "self", ".", "bs", "**", "2", ")", ",", "H", "//", "self", ".", "bs", ",", "W", "//", "self", ".", "bs", ")", "# (N, C*bs^2, H//bs, W//bs)", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.space_to_depth.SpaceToDepthJit.__call__": [[21, 28], ["x.view.view.size", "x.view.view.view", "x.view.view.permute().contiguous", "x.view.view.view", "x.view.view.permute"], "methods", ["None"], ["    ", "def", "__call__", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "# assuming hard-coded that block_size==4 for acceleration", "\n", "        ", "N", ",", "C", ",", "H", ",", "W", "=", "x", ".", "size", "(", ")", "\n", "x", "=", "x", ".", "view", "(", "N", ",", "C", ",", "H", "//", "4", ",", "4", ",", "W", "//", "4", ",", "4", ")", "# (N, C, H//bs, bs, W//bs, bs)", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "3", ",", "5", ",", "1", ",", "2", ",", "4", ")", ".", "contiguous", "(", ")", "# (N, bs, bs, C, H//bs, W//bs)", "\n", "x", "=", "x", ".", "view", "(", "N", ",", "C", "*", "16", ",", "H", "//", "4", ",", "W", "//", "4", ")", "# (N, C*bs^2, H//bs, W//bs)", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.space_to_depth.SpaceToDepthModule.__init__": [[31, 37], ["torch.Module.__init__", "space_to_depth.SpaceToDepthJit", "space_to_depth.SpaceToDepth"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "no_jit", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "not", "no_jit", ":", "\n", "            ", "self", ".", "op", "=", "SpaceToDepthJit", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "op", "=", "SpaceToDepth", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.space_to_depth.SpaceToDepthModule.forward": [[38, 40], ["space_to_depth.SpaceToDepthModule.op"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "op", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.space_to_depth.DepthToSpace.__init__": [[44, 47], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "block_size", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bs", "=", "block_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.space_to_depth.DepthToSpace.forward": [[48, 54], ["x.view.view.size", "x.view.view.view", "x.view.view.permute().contiguous", "x.view.view.view", "x.view.view.permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "N", ",", "C", ",", "H", ",", "W", "=", "x", ".", "size", "(", ")", "\n", "x", "=", "x", ".", "view", "(", "N", ",", "self", ".", "bs", ",", "self", ".", "bs", ",", "C", "//", "(", "self", ".", "bs", "**", "2", ")", ",", "H", ",", "W", ")", "# (N, bs, bs, C//bs^2, H, W)", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "3", ",", "4", ",", "1", ",", "5", ",", "2", ")", ".", "contiguous", "(", ")", "# (N, C//bs^2, H, bs, W, bs)", "\n", "x", "=", "x", ".", "view", "(", "N", ",", "C", "//", "(", "self", ".", "bs", "**", "2", ")", ",", "H", "*", "self", ".", "bs", ",", "W", "*", "self", ".", "bs", ")", "# (N, C//bs^2, H * bs, W * bs)", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.selective_kernel.SelectiveKernelAttn.__init__": [[23, 35], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "norm_layer", "act_layer", "torch.nn.Conv2d"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "channels", ",", "num_paths", "=", "2", ",", "attn_channels", "=", "32", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ")", ":", "\n", "        ", "\"\"\" Selective Kernel Attention Module\n\n        Selective Kernel attention mechanism factored out into its own module.\n\n        \"\"\"", "\n", "super", "(", "SelectiveKernelAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_paths", "=", "num_paths", "\n", "self", ".", "fc_reduce", "=", "nn", ".", "Conv2d", "(", "channels", ",", "attn_channels", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn", "=", "norm_layer", "(", "attn_channels", ")", "\n", "self", ".", "act", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "fc_select", "=", "nn", ".", "Conv2d", "(", "attn_channels", ",", "channels", "*", "num_paths", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.selective_kernel.SelectiveKernelAttn.forward": [[36, 47], ["trace_utils._assert", "torch.softmax.sum().mean", "selective_kernel.SelectiveKernelAttn.fc_reduce", "selective_kernel.SelectiveKernelAttn.bn", "selective_kernel.SelectiveKernelAttn.act", "selective_kernel.SelectiveKernelAttn.fc_select", "torch.softmax.view", "torch.softmax", "torch.softmax.sum"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "_assert", "(", "x", ".", "shape", "[", "1", "]", "==", "self", ".", "num_paths", ",", "''", ")", "\n", "x", "=", "x", ".", "sum", "(", "1", ")", ".", "mean", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "x", "=", "self", ".", "fc_reduce", "(", "x", ")", "\n", "x", "=", "self", ".", "bn", "(", "x", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "x", "=", "self", ".", "fc_select", "(", "x", ")", "\n", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "x", "=", "x", ".", "view", "(", "B", ",", "self", ".", "num_paths", ",", "C", "//", "self", ".", "num_paths", ",", "H", ",", "W", ")", "\n", "x", "=", "torch", ".", "softmax", "(", "x", ",", "dim", "=", "1", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.selective_kernel.SelectiveKernel.__init__": [[51, 108], ["torch.nn.Module.__init__", "selective_kernel._kernel_valid", "len", "min", "dict", "torch.nn.ModuleList", "selective_kernel.SelectiveKernelAttn", "isinstance", "helpers.make_divisible", "len", "len", "conv_bn_act.ConvNormActAa", "zip"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.selective_kernel._kernel_valid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.helpers.make_divisible"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", "=", "None", ",", "kernel_size", "=", "None", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "groups", "=", "1", ",", "\n", "rd_ratio", "=", "1.", "/", "16", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "8", ",", "keep_3x3", "=", "True", ",", "split_input", "=", "True", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "aa_layer", "=", "None", ",", "drop_layer", "=", "None", ")", ":", "\n", "        ", "\"\"\" Selective Kernel Convolution Module\n\n        As described in Selective Kernel Networks (https://arxiv.org/abs/1903.06586) with some modifications.\n\n        Largest change is the input split, which divides the input channels across each convolution path, this can\n        be viewed as a grouping of sorts, but the output channel counts expand to the module level value. This keeps\n        the parameter count from ballooning when the convolutions themselves don't have groups, but still provides\n        a noteworthy increase in performance over similar param count models without this attention layer. -Ross W\n\n        Args:\n            in_channels (int):  module input (feature) channel count\n            out_channels (int):  module output (feature) channel count\n            kernel_size (int, list): kernel size for each convolution branch\n            stride (int): stride for convolutions\n            dilation (int): dilation for module as a whole, impacts dilation of each branch\n            groups (int): number of groups for each branch\n            rd_ratio (int, float): reduction factor for attention features\n            keep_3x3 (bool): keep all branch convolution kernels as 3x3, changing larger kernels for dilations\n            split_input (bool): split input channels evenly across each convolution branch, keeps param count lower,\n                can be viewed as grouping by path, output expands to module out_channels count\n            act_layer (nn.Module): activation layer to use\n            norm_layer (nn.Module): batchnorm/norm layer to use\n            aa_layer (nn.Module): anti-aliasing module\n            drop_layer (nn.Module): spatial drop module in convs (drop block, etc)\n        \"\"\"", "\n", "super", "(", "SelectiveKernel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "out_channels", "=", "out_channels", "or", "in_channels", "\n", "kernel_size", "=", "kernel_size", "or", "[", "3", ",", "5", "]", "# default to one 3x3 and one 5x5 branch. 5x5 -> 3x3 + dilation", "\n", "_kernel_valid", "(", "kernel_size", ")", "\n", "if", "not", "isinstance", "(", "kernel_size", ",", "list", ")", ":", "\n", "            ", "kernel_size", "=", "[", "kernel_size", "]", "*", "2", "\n", "", "if", "keep_3x3", ":", "\n", "            ", "dilation", "=", "[", "dilation", "*", "(", "k", "-", "1", ")", "//", "2", "for", "k", "in", "kernel_size", "]", "\n", "kernel_size", "=", "[", "3", "]", "*", "len", "(", "kernel_size", ")", "\n", "", "else", ":", "\n", "            ", "dilation", "=", "[", "dilation", "]", "*", "len", "(", "kernel_size", ")", "\n", "", "self", ".", "num_paths", "=", "len", "(", "kernel_size", ")", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "split_input", "=", "split_input", "\n", "if", "self", ".", "split_input", ":", "\n", "            ", "assert", "in_channels", "%", "self", ".", "num_paths", "==", "0", "\n", "in_channels", "=", "in_channels", "//", "self", ".", "num_paths", "\n", "", "groups", "=", "min", "(", "out_channels", ",", "groups", ")", "\n", "\n", "conv_kwargs", "=", "dict", "(", "\n", "stride", "=", "stride", ",", "groups", "=", "groups", ",", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ",", "\n", "aa_layer", "=", "aa_layer", ",", "drop_layer", "=", "drop_layer", ")", "\n", "self", ".", "paths", "=", "nn", ".", "ModuleList", "(", "[", "\n", "ConvNormActAa", "(", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "k", ",", "dilation", "=", "d", ",", "**", "conv_kwargs", ")", "\n", "for", "k", ",", "d", "in", "zip", "(", "kernel_size", ",", "dilation", ")", "]", ")", "\n", "\n", "attn_channels", "=", "rd_channels", "or", "make_divisible", "(", "out_channels", "*", "rd_ratio", ",", "divisor", "=", "rd_divisor", ")", "\n", "self", ".", "attn", "=", "SelectiveKernelAttn", "(", "out_channels", ",", "self", ".", "num_paths", ",", "attn_channels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.selective_kernel.SelectiveKernel.forward": [[109, 120], ["torch.stack", "selective_kernel.SelectiveKernel.attn", "torch.sum", "torch.split", "op", "op", "enumerate"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "split_input", ":", "\n", "            ", "x_split", "=", "torch", ".", "split", "(", "x", ",", "self", ".", "in_channels", "//", "self", ".", "num_paths", ",", "1", ")", "\n", "x_paths", "=", "[", "op", "(", "x_split", "[", "i", "]", ")", "for", "i", ",", "op", "in", "enumerate", "(", "self", ".", "paths", ")", "]", "\n", "", "else", ":", "\n", "            ", "x_paths", "=", "[", "op", "(", "x", ")", "for", "op", "in", "self", ".", "paths", "]", "\n", "", "x", "=", "torch", ".", "stack", "(", "x_paths", ",", "dim", "=", "1", ")", "\n", "x_attn", "=", "self", ".", "attn", "(", "x", ")", "\n", "x", "=", "x", "*", "x_attn", "\n", "x", "=", "torch", ".", "sum", "(", "x", ",", "dim", "=", "1", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.selective_kernel._kernel_valid": [[15, 20], ["isinstance", "selective_kernel._kernel_valid"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.selective_kernel._kernel_valid"], ["def", "_kernel_valid", "(", "k", ")", ":", "\n", "    ", "if", "isinstance", "(", "k", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "for", "ki", "in", "k", ":", "\n", "            ", "return", "_kernel_valid", "(", "ki", ")", "\n", "", "", "assert", "k", ">=", "3", "and", "k", "%", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.ClassifierHead.__init__": [[41, 47], ["torch.nn.Module.__init__", "classifier._create_pool", "classifier._create_fc", "torch.nn.Flatten", "torch.nn.Identity"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier._create_pool", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier._create_fc"], ["def", "__init__", "(", "self", ",", "in_chs", ",", "num_classes", ",", "pool_type", "=", "'avg'", ",", "drop_rate", "=", "0.", ",", "use_conv", "=", "False", ")", ":", "\n", "        ", "super", "(", "ClassifierHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "self", ".", "global_pool", ",", "num_pooled_features", "=", "_create_pool", "(", "in_chs", ",", "num_classes", ",", "pool_type", ",", "use_conv", "=", "use_conv", ")", "\n", "self", ".", "fc", "=", "_create_fc", "(", "num_pooled_features", ",", "num_classes", ",", "use_conv", "=", "use_conv", ")", "\n", "self", ".", "flatten", "=", "nn", ".", "Flatten", "(", "1", ")", "if", "use_conv", "and", "pool_type", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.ClassifierHead.forward": [[48, 57], ["classifier.ClassifierHead.global_pool", "torch.nn.functional.dropout", "classifier.ClassifierHead.flatten", "classifier.ClassifierHead.fc", "classifier.ClassifierHead.flatten", "float"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "pre_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "if", "self", ".", "drop_rate", ":", "\n", "            ", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "float", "(", "self", ".", "drop_rate", ")", ",", "training", "=", "self", ".", "training", ")", "\n", "", "if", "pre_logits", ":", "\n", "            ", "return", "x", ".", "flatten", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "return", "self", ".", "flatten", "(", "x", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier._create_pool": [[11, 20], ["adaptive_avgmax_pool.SelectAdaptivePool2d", "adaptive_avgmax_pool.SelectAdaptivePool2d.feat_mult"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.adaptive_avgmax_pool.SelectAdaptivePool2d.feat_mult"], ["def", "_create_pool", "(", "num_features", ",", "num_classes", ",", "pool_type", "=", "'avg'", ",", "use_conv", "=", "False", ")", ":", "\n", "    ", "flatten_in_pool", "=", "not", "use_conv", "# flatten when we use a Linear layer after pooling", "\n", "if", "not", "pool_type", ":", "\n", "        ", "assert", "num_classes", "==", "0", "or", "use_conv", ",", "'Pooling can only be disabled if classifier is also removed or conv classifier is used'", "\n", "flatten_in_pool", "=", "False", "# disable flattening if pooling is pass-through (no pooling)", "\n", "", "global_pool", "=", "SelectAdaptivePool2d", "(", "pool_type", "=", "pool_type", ",", "flatten", "=", "flatten_in_pool", ")", "\n", "num_pooled_features", "=", "num_features", "*", "global_pool", ".", "feat_mult", "(", ")", "\n", "return", "global_pool", ",", "num_pooled_features", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier._create_fc": [[22, 30], ["torch.nn.Identity", "torch.nn.Conv2d", "torch.nn.Linear"], "function", ["None"], ["", "def", "_create_fc", "(", "num_features", ",", "num_classes", ",", "use_conv", "=", "False", ")", ":", "\n", "    ", "if", "num_classes", "<=", "0", ":", "\n", "        ", "fc", "=", "nn", ".", "Identity", "(", ")", "# pass-through (no classifier)", "\n", "", "elif", "use_conv", ":", "\n", "        ", "fc", "=", "nn", ".", "Conv2d", "(", "num_features", ",", "num_classes", ",", "1", ",", "bias", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "fc", "=", "nn", ".", "Linear", "(", "num_features", ",", "num_classes", ",", "bias", "=", "True", ")", "\n", "", "return", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier.create_classifier": [[32, 36], ["classifier._create_pool", "classifier._create_fc"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier._create_pool", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.classifier._create_fc"], ["", "def", "create_classifier", "(", "num_features", ",", "num_classes", ",", "pool_type", "=", "'avg'", ",", "use_conv", "=", "False", ")", ":", "\n", "    ", "global_pool", ",", "num_pooled_features", "=", "_create_pool", "(", "num_features", ",", "num_classes", ",", "pool_type", ",", "use_conv", "=", "use_conv", ")", "\n", "fc", "=", "_create_fc", "(", "num_pooled_features", ",", "num_classes", ",", "use_conv", "=", "use_conv", ")", "\n", "return", "global_pool", ",", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.__init__": [[40, 73], ["torch.nn.Module.__init__", "inplace_abn.InplaceAbn.register_buffer", "inplace_abn.InplaceAbn.register_buffer", "inplace_abn.InplaceAbn.reset_parameters", "isinstance", "torch.nn.Parameter", "torch.nn.Parameter", "inplace_abn.InplaceAbn.register_parameter", "inplace_abn.InplaceAbn.register_parameter", "torch.zeros", "torch.ones", "torch.ones", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters"], ["def", "__init__", "(", "self", ",", "num_features", ",", "eps", "=", "1e-5", ",", "momentum", "=", "0.1", ",", "affine", "=", "True", ",", "apply_act", "=", "True", ",", "\n", "act_layer", "=", "\"leaky_relu\"", ",", "act_param", "=", "0.01", ",", "drop_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "InplaceAbn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_features", "=", "num_features", "\n", "self", ".", "affine", "=", "affine", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "momentum", "=", "momentum", "\n", "if", "apply_act", ":", "\n", "            ", "if", "isinstance", "(", "act_layer", ",", "str", ")", ":", "\n", "                ", "assert", "act_layer", "in", "(", "'leaky_relu'", ",", "'elu'", ",", "'identity'", ",", "''", ")", "\n", "self", ".", "act_name", "=", "act_layer", "if", "act_layer", "else", "'identity'", "\n", "", "else", ":", "\n", "# convert act layer passed as type to string", "\n", "                ", "if", "act_layer", "==", "nn", ".", "ELU", ":", "\n", "                    ", "self", ".", "act_name", "=", "'elu'", "\n", "", "elif", "act_layer", "==", "nn", ".", "LeakyReLU", ":", "\n", "                    ", "self", ".", "act_name", "=", "'leaky_relu'", "\n", "", "elif", "act_layer", "is", "None", "or", "act_layer", "==", "nn", ".", "Identity", ":", "\n", "                    ", "self", ".", "act_name", "=", "'identity'", "\n", "", "else", ":", "\n", "                    ", "assert", "False", ",", "f'Invalid act layer {act_layer.__name__} for IABN'", "\n", "", "", "", "else", ":", "\n", "            ", "self", ".", "act_name", "=", "'identity'", "\n", "", "self", ".", "act_param", "=", "act_param", "\n", "if", "self", ".", "affine", ":", "\n", "            ", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "num_features", ")", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "num_features", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'weight'", ",", "None", ")", "\n", "self", ".", "register_parameter", "(", "'bias'", ",", "None", ")", "\n", "", "self", ".", "register_buffer", "(", "'running_mean'", ",", "torch", ".", "zeros", "(", "num_features", ")", ")", "\n", "self", ".", "register_buffer", "(", "'running_var'", ",", "torch", ".", "ones", "(", "num_features", ")", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.reset_parameters": [[74, 80], ["torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "running_mean", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "running_var", ",", "1", ")", "\n", "if", "self", ".", "affine", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "weight", ",", "1", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.inplace_abn.InplaceAbn.forward": [[81, 88], ["inplace_abn", "isinstance"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "output", "=", "inplace_abn", "(", "\n", "x", ",", "self", ".", "weight", ",", "self", ".", "bias", ",", "self", ".", "running_mean", ",", "self", ".", "running_var", ",", "\n", "self", ".", "training", ",", "self", ".", "momentum", ",", "self", ".", "eps", ",", "self", ".", "act_name", ",", "self", ".", "act_param", ")", "\n", "if", "isinstance", "(", "output", ",", "tuple", ")", ":", "\n", "            ", "output", "=", "output", "[", "0", "]", "\n", "", "return", "output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms.ToNumpy.__call__": [[17, 23], ["numpy.array", "numpy.rollaxis", "numpy.expand_dims"], "methods", ["None"], ["    ", "def", "__call__", "(", "self", ",", "pil_img", ")", ":", "\n", "        ", "np_img", "=", "np", ".", "array", "(", "pil_img", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "if", "np_img", ".", "ndim", "<", "3", ":", "\n", "            ", "np_img", "=", "np", ".", "expand_dims", "(", "np_img", ",", "axis", "=", "-", "1", ")", "\n", "", "np_img", "=", "np", ".", "rollaxis", "(", "np_img", ",", "2", ")", "# HWC to CHW", "\n", "return", "np_img", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms.ToTensor.__init__": [[27, 29], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "dtype", "=", "torch", ".", "float32", ")", ":", "\n", "        ", "self", ".", "dtype", "=", "dtype", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms.ToTensor.__call__": [[30, 36], ["numpy.array", "numpy.rollaxis", "torch.from_numpy().to", "numpy.expand_dims", "torch.from_numpy"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "pil_img", ")", ":", "\n", "        ", "np_img", "=", "np", ".", "array", "(", "pil_img", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "if", "np_img", ".", "ndim", "<", "3", ":", "\n", "            ", "np_img", "=", "np", ".", "expand_dims", "(", "np_img", ",", "axis", "=", "-", "1", ")", "\n", "", "np_img", "=", "np", ".", "rollaxis", "(", "np_img", ",", "2", ")", "# HWC to CHW", "\n", "return", "torch", ".", "from_numpy", "(", "np_img", ")", ".", "to", "(", "dtype", "=", "self", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms.RandomResizedCropAndInterpolation.__init__": [[114, 129], ["isinstance", "tuple", "warnings.warn", "transforms.str_to_interp_mode"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms.str_to_interp_mode"], ["def", "__init__", "(", "self", ",", "size", ",", "scale", "=", "(", "0.08", ",", "1.0", ")", ",", "ratio", "=", "(", "3.", "/", "4.", ",", "4.", "/", "3.", ")", ",", "\n", "interpolation", "=", "'bilinear'", ")", ":", "\n", "        ", "if", "isinstance", "(", "size", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "self", ".", "size", "=", "tuple", "(", "size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "size", "=", "(", "size", ",", "size", ")", "\n", "", "if", "(", "scale", "[", "0", "]", ">", "scale", "[", "1", "]", ")", "or", "(", "ratio", "[", "0", "]", ">", "ratio", "[", "1", "]", ")", ":", "\n", "            ", "warnings", ".", "warn", "(", "\"range should be of kind (min, max)\"", ")", "\n", "\n", "", "if", "interpolation", "==", "'random'", ":", "\n", "            ", "self", ".", "interpolation", "=", "_RANDOM_INTERPOLATION", "\n", "", "else", ":", "\n", "            ", "self", ".", "interpolation", "=", "str_to_interp_mode", "(", "interpolation", ")", "\n", "", "self", ".", "scale", "=", "scale", "\n", "self", ".", "ratio", "=", "ratio", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms.RandomResizedCropAndInterpolation.get_params": [[130, 172], ["range", "math.exp", "int", "int", "min", "int", "random.uniform", "math.log", "math.log", "random.uniform", "round", "round", "random.randint", "random.randint", "round", "max", "int", "math.sqrt", "math.sqrt", "round", "min", "max"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_params", "(", "img", ",", "scale", ",", "ratio", ")", ":", "\n", "        ", "\"\"\"Get parameters for ``crop`` for a random sized crop.\n\n        Args:\n            img (PIL Image): Image to be cropped.\n            scale (tuple): range of size of the origin size cropped\n            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n                sized crop.\n        \"\"\"", "\n", "area", "=", "img", ".", "size", "[", "0", "]", "*", "img", ".", "size", "[", "1", "]", "\n", "\n", "for", "attempt", "in", "range", "(", "10", ")", ":", "\n", "            ", "target_area", "=", "random", ".", "uniform", "(", "*", "scale", ")", "*", "area", "\n", "log_ratio", "=", "(", "math", ".", "log", "(", "ratio", "[", "0", "]", ")", ",", "math", ".", "log", "(", "ratio", "[", "1", "]", ")", ")", "\n", "aspect_ratio", "=", "math", ".", "exp", "(", "random", ".", "uniform", "(", "*", "log_ratio", ")", ")", "\n", "\n", "w", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "target_area", "*", "aspect_ratio", ")", ")", ")", "\n", "h", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "target_area", "/", "aspect_ratio", ")", ")", ")", "\n", "\n", "if", "w", "<=", "img", ".", "size", "[", "0", "]", "and", "h", "<=", "img", ".", "size", "[", "1", "]", ":", "\n", "                ", "i", "=", "random", ".", "randint", "(", "0", ",", "img", ".", "size", "[", "1", "]", "-", "h", ")", "\n", "j", "=", "random", ".", "randint", "(", "0", ",", "img", ".", "size", "[", "0", "]", "-", "w", ")", "\n", "return", "i", ",", "j", ",", "h", ",", "w", "\n", "\n", "# Fallback to central crop", "\n", "", "", "in_ratio", "=", "img", ".", "size", "[", "0", "]", "/", "img", ".", "size", "[", "1", "]", "\n", "if", "in_ratio", "<", "min", "(", "ratio", ")", ":", "\n", "            ", "w", "=", "img", ".", "size", "[", "0", "]", "\n", "h", "=", "int", "(", "round", "(", "w", "/", "min", "(", "ratio", ")", ")", ")", "\n", "", "elif", "in_ratio", ">", "max", "(", "ratio", ")", ":", "\n", "            ", "h", "=", "img", ".", "size", "[", "1", "]", "\n", "w", "=", "int", "(", "round", "(", "h", "*", "max", "(", "ratio", ")", ")", ")", "\n", "", "else", ":", "# whole image", "\n", "            ", "w", "=", "img", ".", "size", "[", "0", "]", "\n", "h", "=", "img", ".", "size", "[", "1", "]", "\n", "", "i", "=", "(", "img", ".", "size", "[", "1", "]", "-", "h", ")", "//", "2", "\n", "j", "=", "(", "img", ".", "size", "[", "0", "]", "-", "w", ")", "//", "2", "\n", "return", "i", ",", "j", ",", "h", ",", "w", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms.RandomResizedCropAndInterpolation.__call__": [[173, 187], ["transforms.RandomResizedCropAndInterpolation.get_params", "isinstance", "torchvision.resized_crop", "random.choice"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms.RandomResizedCropAndInterpolation.get_params"], ["", "def", "__call__", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image): Image to be cropped and resized.\n\n        Returns:\n            PIL Image: Randomly cropped and resized image.\n        \"\"\"", "\n", "i", ",", "j", ",", "h", ",", "w", "=", "self", ".", "get_params", "(", "img", ",", "self", ".", "scale", ",", "self", ".", "ratio", ")", "\n", "if", "isinstance", "(", "self", ".", "interpolation", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "interpolation", "=", "random", ".", "choice", "(", "self", ".", "interpolation", ")", "\n", "", "else", ":", "\n", "            ", "interpolation", "=", "self", ".", "interpolation", "\n", "", "return", "F", ".", "resized_crop", "(", "img", ",", "i", ",", "j", ",", "h", ",", "w", ",", "self", ".", "size", ",", "interpolation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms.RandomResizedCropAndInterpolation.__repr__": [[188, 198], ["isinstance", "transforms.interp_mode_to_str", "tuple", "tuple", "transforms.interp_mode_to_str", "round", "round"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms.interp_mode_to_str", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms.interp_mode_to_str"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "if", "isinstance", "(", "self", ".", "interpolation", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "interpolate_str", "=", "' '", ".", "join", "(", "[", "interp_mode_to_str", "(", "x", ")", "for", "x", "in", "self", ".", "interpolation", "]", ")", "\n", "", "else", ":", "\n", "            ", "interpolate_str", "=", "interp_mode_to_str", "(", "self", ".", "interpolation", ")", "\n", "", "format_string", "=", "self", ".", "__class__", ".", "__name__", "+", "'(size={0}'", ".", "format", "(", "self", ".", "size", ")", "\n", "format_string", "+=", "', scale={0}'", ".", "format", "(", "tuple", "(", "round", "(", "s", ",", "4", ")", "for", "s", "in", "self", ".", "scale", ")", ")", "\n", "format_string", "+=", "', ratio={0}'", ".", "format", "(", "tuple", "(", "round", "(", "r", ",", "4", ")", "for", "r", "in", "self", ".", "ratio", ")", ")", "\n", "format_string", "+=", "', interpolation={0})'", ".", "format", "(", "interpolate_str", ")", "\n", "return", "format_string", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms.str_to_pil_interp": [[78, 80], ["None"], "function", ["None"], ["", "def", "str_to_pil_interp", "(", "mode_str", ")", ":", "\n", "    ", "return", "_str_to_pil_interpolation", "[", "mode_str", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms.str_to_interp_mode": [[82, 87], ["None"], "function", ["None"], ["", "def", "str_to_interp_mode", "(", "mode_str", ")", ":", "\n", "    ", "if", "has_interpolation_mode", ":", "\n", "        ", "return", "_str_to_torch_interpolation", "[", "mode_str", "]", "\n", "", "else", ":", "\n", "        ", "return", "_str_to_pil_interpolation", "[", "mode_str", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms.interp_mode_to_str": [[89, 94], ["None"], "function", ["None"], ["", "", "def", "interp_mode_to_str", "(", "mode", ")", ":", "\n", "    ", "if", "has_interpolation_mode", ":", "\n", "        ", "return", "_torch_interpolation_to_str", "[", "mode", "]", "\n", "", "else", ":", "\n", "        ", "return", "_pil_interpolation_to_str", "[", "mode", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.config.resolve_data_config": [[8, 79], ["hasattr", "isinstance", "tuple", "tuple", "tuple", "_logger.info", "new_config.items", "len", "isinstance", "len", "tuple", "len", "tuple", "_logger.info", "len", "len", "list", "list", "str"], "function", ["None"], ["]", "\n", "\n", "# Set to True if prefer to have layers with no jit optimization (includes activations)", "\n", "_NO_JIT", "=", "False", "\n", "\n", "# Set to True if prefer to have activation layers with no jit optimization", "\n", "# NOTE not currently used as no difference between no_jit and no_activation jit as only layers obeying", "\n", "# the jit flags so far are activations. This will change as more layers are updated and/or added.", "\n", "_NO_ACTIVATION_JIT", "=", "False", "\n", "\n", "# Set to True if exporting a model with Same padding via ONNX", "\n", "_EXPORTABLE", "=", "False", "\n", "\n", "# Set to True if wanting to use torch.jit.script on a model", "\n", "_SCRIPTABLE", "=", "False", "\n", "\n", "\n", "def", "is_no_jit", "(", ")", ":", "\n", "    ", "return", "_NO_JIT", "\n", "\n", "\n", "", "class", "set_no_jit", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "mode", ":", "bool", ")", "->", "None", ":", "\n", "        ", "global", "_NO_JIT", "\n", "self", ".", "prev", "=", "_NO_JIT", "\n", "_NO_JIT", "=", "mode", "\n", "\n", "", "def", "__enter__", "(", "self", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n", "", "def", "__exit__", "(", "self", ",", "*", "args", ":", "Any", ")", "->", "bool", ":", "\n", "        ", "global", "_NO_JIT", "\n", "_NO_JIT", "=", "self", ".", "prev", "\n", "return", "False", "\n", "\n", "\n", "", "", "def", "is_exportable", "(", ")", ":", "\n", "    ", "return", "_EXPORTABLE", "\n", "\n", "\n", "", "class", "set_exportable", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "mode", ":", "bool", ")", "->", "None", ":", "\n", "        ", "global", "_EXPORTABLE", "\n", "self", ".", "prev", "=", "_EXPORTABLE", "\n", "_EXPORTABLE", "=", "mode", "\n", "\n", "", "def", "__enter__", "(", "self", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n", "", "def", "__exit__", "(", "self", ",", "*", "args", ":", "Any", ")", "->", "bool", ":", "\n", "        ", "global", "_EXPORTABLE", "\n", "_EXPORTABLE", "=", "self", ".", "prev", "\n", "return", "False", "\n", "\n", "\n", "", "", "def", "is_scriptable", "(", ")", ":", "\n", "    ", "return", "_SCRIPTABLE", "\n", "\n", "\n", "", "class", "set_scriptable", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "mode", ":", "bool", ")", "->", "None", ":", "\n", "        ", "global", "_SCRIPTABLE", "\n", "self", ".", "prev", "=", "_SCRIPTABLE", "\n", "_SCRIPTABLE", "=", "mode", "\n", "\n", "", "def", "__enter__", "(", "self", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n", "", "def", "__exit__", "(", "self", ",", "*", "args", ":", "Any", ")", "->", "bool", ":", "\n", "        ", "global", "_SCRIPTABLE", "\n", "_SCRIPTABLE", "=", "self", ".", "prev", "\n", "return", "False", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms_factory.transforms_noaug_train": [[17, 42], ["torchvision.transforms.Compose", "torchvision.transforms.Resize", "torchvision.transforms.CenterCrop", "timm.data.transforms.ToNumpy", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "timm.data.transforms.str_to_interp_mode", "torch.tensor", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms.str_to_interp_mode"], ["def", "transforms_noaug_train", "(", "\n", "img_size", "=", "224", ",", "\n", "interpolation", "=", "'bilinear'", ",", "\n", "use_prefetcher", "=", "False", ",", "\n", "mean", "=", "IMAGENET_DEFAULT_MEAN", ",", "\n", "std", "=", "IMAGENET_DEFAULT_STD", ",", "\n", ")", ":", "\n", "    ", "if", "interpolation", "==", "'random'", ":", "\n", "# random interpolation not supported with no-aug", "\n", "        ", "interpolation", "=", "'bilinear'", "\n", "", "tfl", "=", "[", "\n", "transforms", ".", "Resize", "(", "img_size", ",", "interpolation", "=", "str_to_interp_mode", "(", "interpolation", ")", ")", ",", "\n", "transforms", ".", "CenterCrop", "(", "img_size", ")", "\n", "]", "\n", "if", "use_prefetcher", ":", "\n", "# prefetcher and collate will handle tensor conversion and norm", "\n", "        ", "tfl", "+=", "[", "ToNumpy", "(", ")", "]", "\n", "", "else", ":", "\n", "        ", "tfl", "+=", "[", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "\n", "mean", "=", "torch", ".", "tensor", "(", "mean", ")", ",", "\n", "std", "=", "torch", ".", "tensor", "(", "std", ")", ")", "\n", "]", "\n", "", "return", "transforms", ".", "Compose", "(", "tfl", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms_factory.transforms_imagenet_train": [[44, 128], ["tuple", "tuple", "timm.data.transforms.RandomResizedCropAndInterpolation", "isinstance", "isinstance", "dict", "auto_augment.startswith", "torchvision.transforms.Compose", "torchvision.transforms.RandomHorizontalFlip", "torchvision.transforms.RandomVerticalFlip", "min", "timm.data.transforms.str_to_pil_interp", "auto_augment.startswith", "isinstance", "timm.data.transforms.ToNumpy", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "final_tfl.append", "torchvision.transforms.Compose", "torchvision.transforms.Compose", "torchvision.transforms.Compose", "int", "tuple", "timm.data.auto_augment.rand_augment_transform", "torchvision.transforms.ColorJitter", "timm.data.random_erasing.RandomErasing", "timm.data.auto_augment.augment_and_mix_transform", "timm.data.auto_augment.auto_augment_transform", "len", "torch.tensor", "torch.tensor", "min", "float", "round"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms.str_to_pil_interp", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.rand_augment_transform", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.augment_and_mix_transform", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.auto_augment_transform"], ["", "def", "transforms_imagenet_train", "(", "\n", "img_size", "=", "224", ",", "\n", "scale", "=", "None", ",", "\n", "ratio", "=", "None", ",", "\n", "hflip", "=", "0.5", ",", "\n", "vflip", "=", "0.", ",", "\n", "color_jitter", "=", "0.4", ",", "\n", "auto_augment", "=", "None", ",", "\n", "interpolation", "=", "'random'", ",", "\n", "use_prefetcher", "=", "False", ",", "\n", "mean", "=", "IMAGENET_DEFAULT_MEAN", ",", "\n", "std", "=", "IMAGENET_DEFAULT_STD", ",", "\n", "re_prob", "=", "0.", ",", "\n", "re_mode", "=", "'const'", ",", "\n", "re_count", "=", "1", ",", "\n", "re_num_splits", "=", "0", ",", "\n", "separate", "=", "False", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    If separate==True, the transforms are returned as a tuple of 3 separate transforms\n    for use in a mixing dataset that passes\n     * all data through the first (primary) transform, called the 'clean' data\n     * a portion of the data through the secondary transform\n     * normalizes and converts the branches above with the third, final transform\n    \"\"\"", "\n", "scale", "=", "tuple", "(", "scale", "or", "(", "0.08", ",", "1.0", ")", ")", "# default imagenet scale range", "\n", "ratio", "=", "tuple", "(", "ratio", "or", "(", "3.", "/", "4.", ",", "4.", "/", "3.", ")", ")", "# default imagenet ratio range", "\n", "primary_tfl", "=", "[", "\n", "RandomResizedCropAndInterpolation", "(", "img_size", ",", "scale", "=", "scale", ",", "ratio", "=", "ratio", ",", "interpolation", "=", "interpolation", ")", "]", "\n", "if", "hflip", ">", "0.", ":", "\n", "        ", "primary_tfl", "+=", "[", "transforms", ".", "RandomHorizontalFlip", "(", "p", "=", "hflip", ")", "]", "\n", "", "if", "vflip", ">", "0.", ":", "\n", "        ", "primary_tfl", "+=", "[", "transforms", ".", "RandomVerticalFlip", "(", "p", "=", "vflip", ")", "]", "\n", "\n", "", "secondary_tfl", "=", "[", "]", "\n", "if", "auto_augment", ":", "\n", "        ", "assert", "isinstance", "(", "auto_augment", ",", "str", ")", "\n", "if", "isinstance", "(", "img_size", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "img_size_min", "=", "min", "(", "img_size", ")", "\n", "", "else", ":", "\n", "            ", "img_size_min", "=", "img_size", "\n", "", "aa_params", "=", "dict", "(", "\n", "translate_const", "=", "int", "(", "img_size_min", "*", "0.45", ")", ",", "\n", "img_mean", "=", "tuple", "(", "[", "min", "(", "255", ",", "round", "(", "255", "*", "x", ")", ")", "for", "x", "in", "mean", "]", ")", ",", "\n", ")", "\n", "if", "interpolation", "and", "interpolation", "!=", "'random'", ":", "\n", "            ", "aa_params", "[", "'interpolation'", "]", "=", "str_to_pil_interp", "(", "interpolation", ")", "\n", "", "if", "auto_augment", ".", "startswith", "(", "'rand'", ")", ":", "\n", "            ", "secondary_tfl", "+=", "[", "rand_augment_transform", "(", "auto_augment", ",", "aa_params", ")", "]", "\n", "", "elif", "auto_augment", ".", "startswith", "(", "'augmix'", ")", ":", "\n", "            ", "aa_params", "[", "'translate_pct'", "]", "=", "0.3", "\n", "secondary_tfl", "+=", "[", "augment_and_mix_transform", "(", "auto_augment", ",", "aa_params", ")", "]", "\n", "", "else", ":", "\n", "            ", "secondary_tfl", "+=", "[", "auto_augment_transform", "(", "auto_augment", ",", "aa_params", ")", "]", "\n", "", "", "elif", "color_jitter", "is", "not", "None", ":", "\n", "# color jitter is enabled when not using AA", "\n", "        ", "if", "isinstance", "(", "color_jitter", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "# color jitter should be a 3-tuple/list if spec brightness/contrast/saturation", "\n", "# or 4 if also augmenting hue", "\n", "            ", "assert", "len", "(", "color_jitter", ")", "in", "(", "3", ",", "4", ")", "\n", "", "else", ":", "\n", "# if it's a scalar, duplicate for brightness, contrast, and saturation, no hue", "\n", "            ", "color_jitter", "=", "(", "float", "(", "color_jitter", ")", ",", ")", "*", "3", "\n", "", "secondary_tfl", "+=", "[", "transforms", ".", "ColorJitter", "(", "*", "color_jitter", ")", "]", "\n", "\n", "", "final_tfl", "=", "[", "]", "\n", "if", "use_prefetcher", ":", "\n", "# prefetcher and collate will handle tensor conversion and norm", "\n", "        ", "final_tfl", "+=", "[", "ToNumpy", "(", ")", "]", "\n", "", "else", ":", "\n", "        ", "final_tfl", "+=", "[", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "\n", "mean", "=", "torch", ".", "tensor", "(", "mean", ")", ",", "\n", "std", "=", "torch", ".", "tensor", "(", "std", ")", ")", "\n", "]", "\n", "if", "re_prob", ">", "0.", ":", "\n", "            ", "final_tfl", ".", "append", "(", "\n", "RandomErasing", "(", "re_prob", ",", "mode", "=", "re_mode", ",", "max_count", "=", "re_count", ",", "num_splits", "=", "re_num_splits", ",", "device", "=", "'cpu'", ")", ")", "\n", "\n", "", "", "if", "separate", ":", "\n", "        ", "return", "transforms", ".", "Compose", "(", "primary_tfl", ")", ",", "transforms", ".", "Compose", "(", "secondary_tfl", ")", ",", "transforms", ".", "Compose", "(", "final_tfl", ")", "\n", "", "else", ":", "\n", "        ", "return", "transforms", ".", "Compose", "(", "primary_tfl", "+", "secondary_tfl", "+", "final_tfl", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms_factory.transforms_imagenet_eval": [[130, 165], ["isinstance", "torchvision.transforms.Compose", "int", "torchvision.transforms.Resize", "torchvision.transforms.CenterCrop", "len", "int", "tuple", "math.floor", "timm.data.transforms.ToNumpy", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "math.floor", "timm.data.transforms.str_to_interp_mode", "int", "torch.tensor", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms.str_to_interp_mode"], ["", "", "def", "transforms_imagenet_eval", "(", "\n", "img_size", "=", "224", ",", "\n", "crop_pct", "=", "None", ",", "\n", "interpolation", "=", "'bilinear'", ",", "\n", "use_prefetcher", "=", "False", ",", "\n", "mean", "=", "IMAGENET_DEFAULT_MEAN", ",", "\n", "std", "=", "IMAGENET_DEFAULT_STD", ")", ":", "\n", "    ", "crop_pct", "=", "crop_pct", "or", "DEFAULT_CROP_PCT", "\n", "\n", "if", "isinstance", "(", "img_size", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "assert", "len", "(", "img_size", ")", "==", "2", "\n", "if", "img_size", "[", "-", "1", "]", "==", "img_size", "[", "-", "2", "]", ":", "\n", "# fall-back to older behaviour so Resize scales to shortest edge if target is square", "\n", "            ", "scale_size", "=", "int", "(", "math", ".", "floor", "(", "img_size", "[", "0", "]", "/", "crop_pct", ")", ")", "\n", "", "else", ":", "\n", "            ", "scale_size", "=", "tuple", "(", "[", "int", "(", "x", "/", "crop_pct", ")", "for", "x", "in", "img_size", "]", ")", "\n", "", "", "else", ":", "\n", "        ", "scale_size", "=", "int", "(", "math", ".", "floor", "(", "img_size", "/", "crop_pct", ")", ")", "\n", "\n", "", "tfl", "=", "[", "\n", "transforms", ".", "Resize", "(", "scale_size", ",", "interpolation", "=", "str_to_interp_mode", "(", "interpolation", ")", ")", ",", "\n", "transforms", ".", "CenterCrop", "(", "img_size", ")", ",", "\n", "]", "\n", "if", "use_prefetcher", ":", "\n", "# prefetcher and collate will handle tensor conversion and norm", "\n", "        ", "tfl", "+=", "[", "ToNumpy", "(", ")", "]", "\n", "", "else", ":", "\n", "        ", "tfl", "+=", "[", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "\n", "mean", "=", "torch", ".", "tensor", "(", "mean", ")", ",", "\n", "std", "=", "torch", ".", "tensor", "(", "std", ")", ")", "\n", "]", "\n", "\n", "", "return", "transforms", ".", "Compose", "(", "tfl", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms_factory.create_transform": [[167, 237], ["isinstance", "TfPreprocessTransform", "transforms_factory.transforms_noaug_train", "transforms_factory.transforms_imagenet_train", "transforms_factory.transforms_imagenet_eval"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms_factory.transforms_noaug_train", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms_factory.transforms_imagenet_train", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms_factory.transforms_imagenet_eval"], ["", "def", "create_transform", "(", "\n", "input_size", ",", "\n", "is_training", "=", "False", ",", "\n", "use_prefetcher", "=", "False", ",", "\n", "no_aug", "=", "False", ",", "\n", "scale", "=", "None", ",", "\n", "ratio", "=", "None", ",", "\n", "hflip", "=", "0.5", ",", "\n", "vflip", "=", "0.", ",", "\n", "color_jitter", "=", "0.4", ",", "\n", "auto_augment", "=", "None", ",", "\n", "interpolation", "=", "'bilinear'", ",", "\n", "mean", "=", "IMAGENET_DEFAULT_MEAN", ",", "\n", "std", "=", "IMAGENET_DEFAULT_STD", ",", "\n", "re_prob", "=", "0.", ",", "\n", "re_mode", "=", "'const'", ",", "\n", "re_count", "=", "1", ",", "\n", "re_num_splits", "=", "0", ",", "\n", "crop_pct", "=", "None", ",", "\n", "tf_preprocessing", "=", "False", ",", "\n", "separate", "=", "False", ")", ":", "\n", "\n", "    ", "if", "isinstance", "(", "input_size", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "img_size", "=", "input_size", "[", "-", "2", ":", "]", "\n", "", "else", ":", "\n", "        ", "img_size", "=", "input_size", "\n", "\n", "", "if", "tf_preprocessing", "and", "use_prefetcher", ":", "\n", "        ", "assert", "not", "separate", ",", "\"Separate transforms not supported for TF preprocessing\"", "\n", "from", "timm", ".", "data", ".", "tf_preprocessing", "import", "TfPreprocessTransform", "\n", "transform", "=", "TfPreprocessTransform", "(", "\n", "is_training", "=", "is_training", ",", "size", "=", "img_size", ",", "interpolation", "=", "interpolation", ")", "\n", "", "else", ":", "\n", "        ", "if", "is_training", "and", "no_aug", ":", "\n", "            ", "assert", "not", "separate", ",", "\"Cannot perform split augmentation with no_aug\"", "\n", "transform", "=", "transforms_noaug_train", "(", "\n", "img_size", ",", "\n", "interpolation", "=", "interpolation", ",", "\n", "use_prefetcher", "=", "use_prefetcher", ",", "\n", "mean", "=", "mean", ",", "\n", "std", "=", "std", ")", "\n", "", "elif", "is_training", ":", "\n", "            ", "transform", "=", "transforms_imagenet_train", "(", "\n", "img_size", ",", "\n", "scale", "=", "scale", ",", "\n", "ratio", "=", "ratio", ",", "\n", "hflip", "=", "hflip", ",", "\n", "vflip", "=", "vflip", ",", "\n", "color_jitter", "=", "color_jitter", ",", "\n", "auto_augment", "=", "auto_augment", ",", "\n", "interpolation", "=", "interpolation", ",", "\n", "use_prefetcher", "=", "use_prefetcher", ",", "\n", "mean", "=", "mean", ",", "\n", "std", "=", "std", ",", "\n", "re_prob", "=", "re_prob", ",", "\n", "re_mode", "=", "re_mode", ",", "\n", "re_count", "=", "re_count", ",", "\n", "re_num_splits", "=", "re_num_splits", ",", "\n", "separate", "=", "separate", ")", "\n", "", "else", ":", "\n", "            ", "assert", "not", "separate", ",", "\"Separate transforms not supported for validation preprocessing\"", "\n", "transform", "=", "transforms_imagenet_eval", "(", "\n", "img_size", ",", "\n", "interpolation", "=", "interpolation", ",", "\n", "use_prefetcher", "=", "use_prefetcher", ",", "\n", "mean", "=", "mean", ",", "\n", "std", "=", "std", ",", "\n", "crop_pct", "=", "crop_pct", ")", "\n", "\n", "", "", "return", "transform", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.real_labels.RealLabelsImagenet.__init__": [[14, 24], ["open", "json.load", "len", "len", "enumerate"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "filenames", ",", "real_json", "=", "'real.json'", ",", "topk", "=", "(", "1", ",", "5", ")", ")", ":", "\n", "        ", "with", "open", "(", "real_json", ")", "as", "real_labels", ":", "\n", "            ", "real_labels", "=", "json", ".", "load", "(", "real_labels", ")", "\n", "real_labels", "=", "{", "f'ILSVRC2012_val_{i + 1:08d}.JPEG'", ":", "labels", "for", "i", ",", "labels", "in", "enumerate", "(", "real_labels", ")", "}", "\n", "", "self", ".", "real_labels", "=", "real_labels", "\n", "self", ".", "filenames", "=", "filenames", "\n", "assert", "len", "(", "self", ".", "filenames", ")", "==", "len", "(", "self", ".", "real_labels", ")", "\n", "self", ".", "topk", "=", "topk", "\n", "self", ".", "is_correct", "=", "{", "k", ":", "[", "]", "for", "k", "in", "topk", "}", "\n", "self", ".", "sample_idx", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.real_labels.RealLabelsImagenet.add_result": [[25, 37], ["max", "output.topk", "pred_batch.cpu().numpy.cpu().numpy.cpu().numpy", "os.path.basename", "pred_batch.cpu().numpy.cpu().numpy.cpu", "real_labels.RealLabelsImagenet.is_correct[].append", "any"], "methods", ["None"], ["", "def", "add_result", "(", "self", ",", "output", ")", ":", "\n", "        ", "maxk", "=", "max", "(", "self", ".", "topk", ")", "\n", "_", ",", "pred_batch", "=", "output", ".", "topk", "(", "maxk", ",", "1", ",", "True", ",", "True", ")", "\n", "pred_batch", "=", "pred_batch", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "pred", "in", "pred_batch", ":", "\n", "            ", "filename", "=", "self", ".", "filenames", "[", "self", ".", "sample_idx", "]", "\n", "filename", "=", "os", ".", "path", ".", "basename", "(", "filename", ")", "\n", "if", "self", ".", "real_labels", "[", "filename", "]", ":", "\n", "                ", "for", "k", "in", "self", ".", "topk", ":", "\n", "                    ", "self", ".", "is_correct", "[", "k", "]", ".", "append", "(", "\n", "any", "(", "[", "p", "in", "self", ".", "real_labels", "[", "filename", "]", "for", "p", "in", "pred", "[", ":", "k", "]", "]", ")", ")", "\n", "", "", "self", ".", "sample_idx", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.real_labels.RealLabelsImagenet.get_accuracy": [[38, 43], ["float", "float", "numpy.mean", "numpy.mean"], "methods", ["None"], ["", "", "def", "get_accuracy", "(", "self", ",", "k", "=", "None", ")", ":", "\n", "        ", "if", "k", "is", "None", ":", "\n", "            ", "return", "{", "k", ":", "float", "(", "np", ".", "mean", "(", "self", ".", "is_correct", "[", "k", "]", ")", ")", "*", "100", "for", "k", "in", "self", ".", "topk", "}", "\n", "", "else", ":", "\n", "            ", "return", "float", "(", "np", ".", "mean", "(", "self", ".", "is_correct", "[", "k", "]", ")", ")", "*", "100", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.random_erasing.RandomErasing.__init__": [[45, 67], ["mode.lower", "math.log", "math.log"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "probability", "=", "0.5", ",", "min_area", "=", "0.02", ",", "max_area", "=", "1", "/", "3", ",", "min_aspect", "=", "0.3", ",", "max_aspect", "=", "None", ",", "\n", "mode", "=", "'const'", ",", "min_count", "=", "1", ",", "max_count", "=", "None", ",", "num_splits", "=", "0", ",", "device", "=", "'cuda'", ")", ":", "\n", "        ", "self", ".", "probability", "=", "probability", "\n", "self", ".", "min_area", "=", "min_area", "\n", "self", ".", "max_area", "=", "max_area", "\n", "max_aspect", "=", "max_aspect", "or", "1", "/", "min_aspect", "\n", "self", ".", "log_aspect_ratio", "=", "(", "math", ".", "log", "(", "min_aspect", ")", ",", "math", ".", "log", "(", "max_aspect", ")", ")", "\n", "self", ".", "min_count", "=", "min_count", "\n", "self", ".", "max_count", "=", "max_count", "or", "min_count", "\n", "self", ".", "num_splits", "=", "num_splits", "\n", "self", ".", "mode", "=", "mode", ".", "lower", "(", ")", "\n", "self", ".", "rand_color", "=", "False", "\n", "self", ".", "per_pixel", "=", "False", "\n", "if", "self", ".", "mode", "==", "'rand'", ":", "\n", "            ", "self", ".", "rand_color", "=", "True", "# per block random normal", "\n", "", "elif", "self", ".", "mode", "==", "'pixel'", ":", "\n", "            ", "self", ".", "per_pixel", "=", "True", "# per pixel random normal", "\n", "", "else", ":", "\n", "            ", "assert", "not", "self", ".", "mode", "or", "self", ".", "mode", "==", "'const'", "\n", "", "self", ".", "device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.random_erasing.RandomErasing._erase": [[68, 87], ["range", "random.random", "random.randint", "range", "math.exp", "int", "int", "random.uniform", "round", "round", "random.randint", "random.randint", "random_erasing._get_pixels", "random.uniform", "math.sqrt", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.random_erasing._get_pixels"], ["", "def", "_erase", "(", "self", ",", "img", ",", "chan", ",", "img_h", ",", "img_w", ",", "dtype", ")", ":", "\n", "        ", "if", "random", ".", "random", "(", ")", ">", "self", ".", "probability", ":", "\n", "            ", "return", "\n", "", "area", "=", "img_h", "*", "img_w", "\n", "count", "=", "self", ".", "min_count", "if", "self", ".", "min_count", "==", "self", ".", "max_count", "else", "random", ".", "randint", "(", "self", ".", "min_count", ",", "self", ".", "max_count", ")", "\n", "for", "_", "in", "range", "(", "count", ")", ":", "\n", "            ", "for", "attempt", "in", "range", "(", "10", ")", ":", "\n", "                ", "target_area", "=", "random", ".", "uniform", "(", "self", ".", "min_area", ",", "self", ".", "max_area", ")", "*", "area", "/", "count", "\n", "aspect_ratio", "=", "math", ".", "exp", "(", "random", ".", "uniform", "(", "*", "self", ".", "log_aspect_ratio", ")", ")", "\n", "h", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "target_area", "*", "aspect_ratio", ")", ")", ")", "\n", "w", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "target_area", "/", "aspect_ratio", ")", ")", ")", "\n", "if", "w", "<", "img_w", "and", "h", "<", "img_h", ":", "\n", "                    ", "top", "=", "random", ".", "randint", "(", "0", ",", "img_h", "-", "h", ")", "\n", "left", "=", "random", ".", "randint", "(", "0", ",", "img_w", "-", "w", ")", "\n", "img", "[", ":", ",", "top", ":", "top", "+", "h", ",", "left", ":", "left", "+", "w", "]", "=", "_get_pixels", "(", "\n", "self", ".", "per_pixel", ",", "self", ".", "rand_color", ",", "(", "chan", ",", "h", ",", "w", ")", ",", "\n", "dtype", "=", "dtype", ",", "device", "=", "self", ".", "device", ")", "\n", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.random_erasing.RandomErasing.__call__": [[88, 98], ["len", "random_erasing.RandomErasing._erase", "input.size", "range", "input.size", "random_erasing.RandomErasing._erase", "input.size"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.random_erasing.RandomErasing._erase", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.random_erasing.RandomErasing._erase"], ["", "", "", "", "def", "__call__", "(", "self", ",", "input", ")", ":", "\n", "        ", "if", "len", "(", "input", ".", "size", "(", ")", ")", "==", "3", ":", "\n", "            ", "self", ".", "_erase", "(", "input", ",", "*", "input", ".", "size", "(", ")", ",", "input", ".", "dtype", ")", "\n", "", "else", ":", "\n", "            ", "batch_size", ",", "chan", ",", "img_h", ",", "img_w", "=", "input", ".", "size", "(", ")", "\n", "# skip first slice of batch if num_splits is set (for clean portion of samples)", "\n", "batch_start", "=", "batch_size", "//", "self", ".", "num_splits", "if", "self", ".", "num_splits", ">", "1", "else", "0", "\n", "for", "i", "in", "range", "(", "batch_start", ",", "batch_size", ")", ":", "\n", "                ", "self", ".", "_erase", "(", "input", "[", "i", "]", ",", "chan", ",", "img_h", ",", "img_w", ",", "input", ".", "dtype", ")", "\n", "", "", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.random_erasing.RandomErasing.__repr__": [[99, 104], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "# NOTE simplified state for repr", "\n", "        ", "fs", "=", "self", ".", "__class__", ".", "__name__", "+", "f'(p={self.probability}, mode={self.mode}'", "\n", "fs", "+=", "f', count=({self.min_count}, {self.max_count}))'", "\n", "return", "fs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.random_erasing._get_pixels": [[13, 23], ["torch.empty().normal_", "torch.empty().normal_", "torch.zeros", "torch.empty", "torch.empty"], "function", ["None"], ["def", "_get_pixels", "(", "per_pixel", ",", "rand_color", ",", "patch_size", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "'cuda'", ")", ":", "\n", "# NOTE I've seen CUDA illegal memory access errors being caused by the normal_()", "\n", "# paths, flip the order so normal is run on CPU if this becomes a problem", "\n", "# Issue has been fixed in master https://github.com/pytorch/pytorch/issues/19508", "\n", "    ", "if", "per_pixel", ":", "\n", "        ", "return", "torch", ".", "empty", "(", "patch_size", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", ".", "normal_", "(", ")", "\n", "", "elif", "rand_color", ":", "\n", "        ", "return", "torch", ".", "empty", "(", "(", "patch_size", "[", "0", "]", ",", "1", ",", "1", ")", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", ".", "normal_", "(", ")", "\n", "", "else", ":", "\n", "        ", "return", "torch", ".", "zeros", "(", "(", "patch_size", "[", "0", "]", ",", "1", ",", "1", ")", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.AugmentOp.__init__": [[322, 342], ["hparams.copy", "dict", "auto_augment.AugmentOp.hparams.get", "auto_augment.AugmentOp.hparams.get"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get"], ["    ", "def", "__init__", "(", "self", ",", "name", ",", "prob", "=", "0.5", ",", "magnitude", "=", "10", ",", "hparams", "=", "None", ")", ":", "\n", "        ", "hparams", "=", "hparams", "or", "_HPARAMS_DEFAULT", "\n", "self", ".", "name", "=", "name", "\n", "self", ".", "aug_fn", "=", "NAME_TO_OP", "[", "name", "]", "\n", "self", ".", "level_fn", "=", "LEVEL_TO_ARG", "[", "name", "]", "\n", "self", ".", "prob", "=", "prob", "\n", "self", ".", "magnitude", "=", "magnitude", "\n", "self", ".", "hparams", "=", "hparams", ".", "copy", "(", ")", "\n", "self", ".", "kwargs", "=", "dict", "(", "\n", "fillcolor", "=", "hparams", "[", "'img_mean'", "]", "if", "'img_mean'", "in", "hparams", "else", "_FILL", ",", "\n", "resample", "=", "hparams", "[", "'interpolation'", "]", "if", "'interpolation'", "in", "hparams", "else", "_RANDOM_INTERPOLATION", ",", "\n", ")", "\n", "\n", "# If magnitude_std is > 0, we introduce some randomness", "\n", "# in the usually fixed policy and sample magnitude from a normal distribution", "\n", "# with mean `magnitude` and std-dev of `magnitude_std`.", "\n", "# NOTE This is my own hack, being tested, not in papers or reference impls.", "\n", "# If magnitude_std is inf, we sample magnitude from a uniform distribution", "\n", "self", ".", "magnitude_std", "=", "self", ".", "hparams", ".", "get", "(", "'magnitude_std'", ",", "0", ")", "\n", "self", ".", "magnitude_max", "=", "self", ".", "hparams", ".", "get", "(", "'magnitude_max'", ",", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.AugmentOp.__call__": [[343, 359], ["max", "auto_augment.AugmentOp.aug_fn", "min", "auto_augment.AugmentOp.level_fn", "tuple", "random.random", "float", "random.uniform", "random.gauss"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "img", ")", ":", "\n", "        ", "if", "self", ".", "prob", "<", "1.0", "and", "random", ".", "random", "(", ")", ">", "self", ".", "prob", ":", "\n", "            ", "return", "img", "\n", "", "magnitude", "=", "self", ".", "magnitude", "\n", "if", "self", ".", "magnitude_std", ">", "0", ":", "\n", "# magnitude randomization enabled", "\n", "            ", "if", "self", ".", "magnitude_std", "==", "float", "(", "'inf'", ")", ":", "\n", "                ", "magnitude", "=", "random", ".", "uniform", "(", "0", ",", "magnitude", ")", "\n", "", "elif", "self", ".", "magnitude_std", ">", "0", ":", "\n", "                ", "magnitude", "=", "random", ".", "gauss", "(", "magnitude", ",", "self", ".", "magnitude_std", ")", "\n", "# default upper_bound for the timm RA impl is _LEVEL_DENOM (10)", "\n", "# setting magnitude_max overrides this to allow M > 10 (behaviour closer to Google TF RA impl)", "\n", "", "", "upper_bound", "=", "self", ".", "magnitude_max", "or", "_LEVEL_DENOM", "\n", "magnitude", "=", "max", "(", "0.", ",", "min", "(", "magnitude", ",", "upper_bound", ")", ")", "\n", "level_args", "=", "self", ".", "level_fn", "(", "magnitude", ",", "self", ".", "hparams", ")", "if", "self", ".", "level_fn", "is", "not", "None", "else", "tuple", "(", ")", "\n", "return", "self", ".", "aug_fn", "(", "img", ",", "*", "level_args", ",", "**", "self", ".", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.AugmentOp.__repr__": [[360, 367], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "fs", "=", "self", ".", "__class__", ".", "__name__", "+", "f'(name={self.name}, p={self.prob}'", "\n", "fs", "+=", "f', m={self.magnitude}, mstd={self.magnitude_std}'", "\n", "if", "self", ".", "magnitude_max", "is", "not", "None", ":", "\n", "            ", "fs", "+=", "f', mmax={self.magnitude_max}'", "\n", "", "fs", "+=", "')'", "\n", "return", "fs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.AutoAugment.__init__": [[518, 520], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "policy", ")", ":", "\n", "        ", "self", ".", "policy", "=", "policy", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.AutoAugment.__call__": [[521, 526], ["random.choice", "op"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "img", ")", ":", "\n", "        ", "sub_policy", "=", "random", ".", "choice", "(", "self", ".", "policy", ")", "\n", "for", "op", "in", "sub_policy", ":", "\n", "            ", "img", "=", "op", "(", "img", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.AutoAugment.__repr__": [[527, 535], ["str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "fs", "=", "self", ".", "__class__", ".", "__name__", "+", "f'(policy='", "\n", "for", "p", "in", "self", ".", "policy", ":", "\n", "            ", "fs", "+=", "'\\n\\t['", "\n", "fs", "+=", "', '", ".", "join", "(", "[", "str", "(", "op", ")", "for", "op", "in", "p", "]", ")", "\n", "fs", "+=", "']'", "\n", "", "fs", "+=", "')'", "\n", "return", "fs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.RandAugment.__init__": [[647, 651], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "ops", ",", "num_layers", "=", "2", ",", "choice_weights", "=", "None", ")", ":", "\n", "        ", "self", ".", "ops", "=", "ops", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "choice_weights", "=", "choice_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.RandAugment.__call__": [[652, 659], ["numpy.random.choice", "op"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "img", ")", ":", "\n", "# no replacement when using weighted choice", "\n", "        ", "ops", "=", "np", ".", "random", ".", "choice", "(", "\n", "self", ".", "ops", ",", "self", ".", "num_layers", ",", "replace", "=", "self", ".", "choice_weights", "is", "None", ",", "p", "=", "self", ".", "choice_weights", ")", "\n", "for", "op", "in", "ops", ":", "\n", "            ", "img", "=", "op", "(", "img", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.RandAugment.__repr__": [[660, 666], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "fs", "=", "self", ".", "__class__", ".", "__name__", "+", "f'(n={self.num_layers}, ops='", "\n", "for", "op", "in", "self", ".", "ops", ":", "\n", "            ", "fs", "+=", "f'\\n\\t{op}'", "\n", "", "fs", "+=", "')'", "\n", "return", "fs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.AugMixAugment.__init__": [[756, 762], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "ops", ",", "alpha", "=", "1.", ",", "width", "=", "3", ",", "depth", "=", "-", "1", ",", "blended", "=", "False", ")", ":", "\n", "        ", "self", ".", "ops", "=", "ops", "\n", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "width", "=", "width", "\n", "self", ".", "depth", "=", "depth", "\n", "self", ".", "blended", "=", "blended", "# blended mode is faster but not well tested", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.AugMixAugment._calc_blended_weights": [[763, 772], ["numpy.array", "rws.append"], "methods", ["None"], ["", "def", "_calc_blended_weights", "(", "self", ",", "ws", ",", "m", ")", ":", "\n", "        ", "ws", "=", "ws", "*", "m", "\n", "cump", "=", "1.", "\n", "rws", "=", "[", "]", "\n", "for", "w", "in", "ws", "[", ":", ":", "-", "1", "]", ":", "\n", "            ", "alpha", "=", "w", "/", "cump", "\n", "cump", "*=", "(", "1", "-", "alpha", ")", "\n", "rws", ".", "append", "(", "alpha", ")", "\n", "", "return", "np", ".", "array", "(", "rws", "[", ":", ":", "-", "1", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.AugMixAugment._apply_blended": [[773, 788], ["PIL.Image.blend.copy", "auto_augment.AugMixAugment._calc_blended_weights", "numpy.random.choice", "PIL.Image.blend", "numpy.random.randint", "op"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.AugMixAugment._calc_blended_weights"], ["", "def", "_apply_blended", "(", "self", ",", "img", ",", "mixing_weights", ",", "m", ")", ":", "\n", "# This is my first crack and implementing a slightly faster mixed augmentation. Instead", "\n", "# of accumulating the mix for each chain in a Numpy array and then blending with original,", "\n", "# it recomputes the blending coefficients and applies one PIL image blend per chain.", "\n", "# TODO the results appear in the right ballpark but they differ by more than rounding.", "\n", "        ", "img_orig", "=", "img", ".", "copy", "(", ")", "\n", "ws", "=", "self", ".", "_calc_blended_weights", "(", "mixing_weights", ",", "m", ")", "\n", "for", "w", "in", "ws", ":", "\n", "            ", "depth", "=", "self", ".", "depth", "if", "self", ".", "depth", ">", "0", "else", "np", ".", "random", ".", "randint", "(", "1", ",", "4", ")", "\n", "ops", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "ops", ",", "depth", ",", "replace", "=", "True", ")", "\n", "img_aug", "=", "img_orig", "# no ops are in-place, deep copy not necessary", "\n", "for", "op", "in", "ops", ":", "\n", "                ", "img_aug", "=", "op", "(", "img_aug", ")", "\n", "", "img", "=", "Image", ".", "blend", "(", "img", ",", "img_aug", ",", "w", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.AugMixAugment._apply_basic": [[789, 805], ["numpy.zeros", "numpy.clip", "PIL.Image.fromarray", "PIL.Image.blend", "len", "numpy.random.choice", "PIL.Image.fromarray.astype", "img.getbands", "numpy.random.randint", "op", "numpy.asarray"], "methods", ["None"], ["", "def", "_apply_basic", "(", "self", ",", "img", ",", "mixing_weights", ",", "m", ")", ":", "\n", "# This is a literal adaptation of the paper/official implementation without normalizations and", "\n", "# PIL <-> Numpy conversions between every op. It is still quite CPU compute heavy compared to the", "\n", "# typical augmentation transforms, could use a GPU / Kornia implementation.", "\n", "        ", "img_shape", "=", "img", ".", "size", "[", "0", "]", ",", "img", ".", "size", "[", "1", "]", ",", "len", "(", "img", ".", "getbands", "(", ")", ")", "\n", "mixed", "=", "np", ".", "zeros", "(", "img_shape", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "mw", "in", "mixing_weights", ":", "\n", "            ", "depth", "=", "self", ".", "depth", "if", "self", ".", "depth", ">", "0", "else", "np", ".", "random", ".", "randint", "(", "1", ",", "4", ")", "\n", "ops", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "ops", ",", "depth", ",", "replace", "=", "True", ")", "\n", "img_aug", "=", "img", "# no ops are in-place, deep copy not necessary", "\n", "for", "op", "in", "ops", ":", "\n", "                ", "img_aug", "=", "op", "(", "img_aug", ")", "\n", "", "mixed", "+=", "mw", "*", "np", ".", "asarray", "(", "img_aug", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "np", ".", "clip", "(", "mixed", ",", "0", ",", "255.", ",", "out", "=", "mixed", ")", "\n", "mixed", "=", "Image", ".", "fromarray", "(", "mixed", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "return", "Image", ".", "blend", "(", "img", ",", "mixed", ",", "m", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.AugMixAugment.__call__": [[806, 814], ["numpy.float32", "numpy.float32", "numpy.random.dirichlet", "numpy.random.beta", "auto_augment.AugMixAugment._apply_blended", "auto_augment.AugMixAugment._apply_basic"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.AugMixAugment._apply_blended", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.AugMixAugment._apply_basic"], ["", "def", "__call__", "(", "self", ",", "img", ")", ":", "\n", "        ", "mixing_weights", "=", "np", ".", "float32", "(", "np", ".", "random", ".", "dirichlet", "(", "[", "self", ".", "alpha", "]", "*", "self", ".", "width", ")", ")", "\n", "m", "=", "np", ".", "float32", "(", "np", ".", "random", ".", "beta", "(", "self", ".", "alpha", ",", "self", ".", "alpha", ")", ")", "\n", "if", "self", ".", "blended", ":", "\n", "            ", "mixed", "=", "self", ".", "_apply_blended", "(", "img", ",", "mixing_weights", ",", "m", ")", "\n", "", "else", ":", "\n", "            ", "mixed", "=", "self", ".", "_apply_basic", "(", "img", ",", "mixing_weights", ",", "m", ")", "\n", "", "return", "mixed", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.AugMixAugment.__repr__": [[815, 821], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "fs", "=", "self", ".", "__class__", ".", "__name__", "+", "f'(alpha={self.alpha}, width={self.width}, depth={self.depth}, ops='", "\n", "for", "op", "in", "self", ".", "ops", ":", "\n", "            ", "fs", "+=", "f'\\n\\t{op}'", "\n", "", "fs", "+=", "')'", "\n", "return", "fs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._interpolation": [[47, 53], ["kwargs.pop", "isinstance", "random.choice"], "function", ["None"], ["", "def", "_interpolation", "(", "kwargs", ")", ":", "\n", "    ", "interpolation", "=", "kwargs", ".", "pop", "(", "'resample'", ",", "_DEFAULT_INTERPOLATION", ")", "\n", "if", "isinstance", "(", "interpolation", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "return", "random", ".", "choice", "(", "interpolation", ")", "\n", "", "else", ":", "\n", "        ", "return", "interpolation", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._check_args_tf": [[55, 59], ["auto_augment._interpolation", "kwargs.pop"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._interpolation"], ["", "", "def", "_check_args_tf", "(", "kwargs", ")", ":", "\n", "    ", "if", "'fillcolor'", "in", "kwargs", "and", "_PIL_VER", "<", "(", "5", ",", "0", ")", ":", "\n", "        ", "kwargs", ".", "pop", "(", "'fillcolor'", ")", "\n", "", "kwargs", "[", "'resample'", "]", "=", "_interpolation", "(", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.shear_x": [[61, 64], ["auto_augment._check_args_tf", "img.transform"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._check_args_tf", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.AugMixDataset.transform"], ["", "def", "shear_x", "(", "img", ",", "factor", ",", "**", "kwargs", ")", ":", "\n", "    ", "_check_args_tf", "(", "kwargs", ")", "\n", "return", "img", ".", "transform", "(", "img", ".", "size", ",", "Image", ".", "AFFINE", ",", "(", "1", ",", "factor", ",", "0", ",", "0", ",", "1", ",", "0", ")", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.shear_y": [[66, 69], ["auto_augment._check_args_tf", "img.transform"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._check_args_tf", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.AugMixDataset.transform"], ["", "def", "shear_y", "(", "img", ",", "factor", ",", "**", "kwargs", ")", ":", "\n", "    ", "_check_args_tf", "(", "kwargs", ")", "\n", "return", "img", ".", "transform", "(", "img", ".", "size", ",", "Image", ".", "AFFINE", ",", "(", "1", ",", "0", ",", "0", ",", "factor", ",", "1", ",", "0", ")", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.translate_x_rel": [[71, 75], ["auto_augment._check_args_tf", "img.transform"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._check_args_tf", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.AugMixDataset.transform"], ["", "def", "translate_x_rel", "(", "img", ",", "pct", ",", "**", "kwargs", ")", ":", "\n", "    ", "pixels", "=", "pct", "*", "img", ".", "size", "[", "0", "]", "\n", "_check_args_tf", "(", "kwargs", ")", "\n", "return", "img", ".", "transform", "(", "img", ".", "size", ",", "Image", ".", "AFFINE", ",", "(", "1", ",", "0", ",", "pixels", ",", "0", ",", "1", ",", "0", ")", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.translate_y_rel": [[77, 81], ["auto_augment._check_args_tf", "img.transform"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._check_args_tf", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.AugMixDataset.transform"], ["", "def", "translate_y_rel", "(", "img", ",", "pct", ",", "**", "kwargs", ")", ":", "\n", "    ", "pixels", "=", "pct", "*", "img", ".", "size", "[", "1", "]", "\n", "_check_args_tf", "(", "kwargs", ")", "\n", "return", "img", ".", "transform", "(", "img", ".", "size", ",", "Image", ".", "AFFINE", ",", "(", "1", ",", "0", ",", "0", ",", "0", ",", "1", ",", "pixels", ")", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.translate_x_abs": [[83, 86], ["auto_augment._check_args_tf", "img.transform"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._check_args_tf", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.AugMixDataset.transform"], ["", "def", "translate_x_abs", "(", "img", ",", "pixels", ",", "**", "kwargs", ")", ":", "\n", "    ", "_check_args_tf", "(", "kwargs", ")", "\n", "return", "img", ".", "transform", "(", "img", ".", "size", ",", "Image", ".", "AFFINE", ",", "(", "1", ",", "0", ",", "pixels", ",", "0", ",", "1", ",", "0", ")", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.translate_y_abs": [[88, 91], ["auto_augment._check_args_tf", "img.transform"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._check_args_tf", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.AugMixDataset.transform"], ["", "def", "translate_y_abs", "(", "img", ",", "pixels", ",", "**", "kwargs", ")", ":", "\n", "    ", "_check_args_tf", "(", "kwargs", ")", "\n", "return", "img", ".", "transform", "(", "img", ".", "size", ",", "Image", ".", "AFFINE", ",", "(", "1", ",", "0", ",", "0", ",", "0", ",", "1", ",", "pixels", ")", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.rotate": [[93, 123], ["auto_augment._check_args_tf", "img.rotate", "auto_augment.rotate.transform"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._check_args_tf", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.rotate", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.AugMixDataset.transform"], ["", "def", "rotate", "(", "img", ",", "degrees", ",", "**", "kwargs", ")", ":", "\n", "    ", "_check_args_tf", "(", "kwargs", ")", "\n", "if", "_PIL_VER", ">=", "(", "5", ",", "2", ")", ":", "\n", "        ", "return", "img", ".", "rotate", "(", "degrees", ",", "**", "kwargs", ")", "\n", "", "elif", "_PIL_VER", ">=", "(", "5", ",", "0", ")", ":", "\n", "        ", "w", ",", "h", "=", "img", ".", "size", "\n", "post_trans", "=", "(", "0", ",", "0", ")", "\n", "rotn_center", "=", "(", "w", "/", "2.0", ",", "h", "/", "2.0", ")", "\n", "angle", "=", "-", "math", ".", "radians", "(", "degrees", ")", "\n", "matrix", "=", "[", "\n", "round", "(", "math", ".", "cos", "(", "angle", ")", ",", "15", ")", ",", "\n", "round", "(", "math", ".", "sin", "(", "angle", ")", ",", "15", ")", ",", "\n", "0.0", ",", "\n", "round", "(", "-", "math", ".", "sin", "(", "angle", ")", ",", "15", ")", ",", "\n", "round", "(", "math", ".", "cos", "(", "angle", ")", ",", "15", ")", ",", "\n", "0.0", ",", "\n", "]", "\n", "\n", "def", "transform", "(", "x", ",", "y", ",", "matrix", ")", ":", "\n", "            ", "(", "a", ",", "b", ",", "c", ",", "d", ",", "e", ",", "f", ")", "=", "matrix", "\n", "return", "a", "*", "x", "+", "b", "*", "y", "+", "c", ",", "d", "*", "x", "+", "e", "*", "y", "+", "f", "\n", "\n", "", "matrix", "[", "2", "]", ",", "matrix", "[", "5", "]", "=", "transform", "(", "\n", "-", "rotn_center", "[", "0", "]", "-", "post_trans", "[", "0", "]", ",", "-", "rotn_center", "[", "1", "]", "-", "post_trans", "[", "1", "]", ",", "matrix", "\n", ")", "\n", "matrix", "[", "2", "]", "+=", "rotn_center", "[", "0", "]", "\n", "matrix", "[", "5", "]", "+=", "rotn_center", "[", "1", "]", "\n", "return", "img", ".", "transform", "(", "img", ".", "size", ",", "Image", ".", "AFFINE", ",", "matrix", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "        ", "return", "img", ".", "rotate", "(", "degrees", ",", "resample", "=", "kwargs", "[", "'resample'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.auto_contrast": [[125, 127], ["PIL.ImageOps.autocontrast"], "function", ["None"], ["", "", "def", "auto_contrast", "(", "img", ",", "**", "__", ")", ":", "\n", "    ", "return", "ImageOps", ".", "autocontrast", "(", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.invert": [[129, 131], ["PIL.ImageOps.invert"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.invert"], ["", "def", "invert", "(", "img", ",", "**", "__", ")", ":", "\n", "    ", "return", "ImageOps", ".", "invert", "(", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.equalize": [[133, 135], ["PIL.ImageOps.equalize"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.equalize"], ["", "def", "equalize", "(", "img", ",", "**", "__", ")", ":", "\n", "    ", "return", "ImageOps", ".", "equalize", "(", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.solarize": [[137, 139], ["PIL.ImageOps.solarize"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.solarize"], ["", "def", "solarize", "(", "img", ",", "thresh", ",", "**", "__", ")", ":", "\n", "    ", "return", "ImageOps", ".", "solarize", "(", "img", ",", "thresh", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.solarize_add": [[141, 154], ["range", "img.point", "lut.append", "lut.append", "min", "len"], "function", ["None"], ["", "def", "solarize_add", "(", "img", ",", "add", ",", "thresh", "=", "128", ",", "**", "__", ")", ":", "\n", "    ", "lut", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "256", ")", ":", "\n", "        ", "if", "i", "<", "thresh", ":", "\n", "            ", "lut", ".", "append", "(", "min", "(", "255", ",", "i", "+", "add", ")", ")", "\n", "", "else", ":", "\n", "            ", "lut", ".", "append", "(", "i", ")", "\n", "", "", "if", "img", ".", "mode", "in", "(", "\"L\"", ",", "\"RGB\"", ")", ":", "\n", "        ", "if", "img", ".", "mode", "==", "\"RGB\"", "and", "len", "(", "lut", ")", "==", "256", ":", "\n", "            ", "lut", "=", "lut", "+", "lut", "+", "lut", "\n", "", "return", "img", ".", "point", "(", "lut", ")", "\n", "", "else", ":", "\n", "        ", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.posterize": [[156, 160], ["PIL.ImageOps.posterize"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.posterize"], ["", "", "def", "posterize", "(", "img", ",", "bits_to_keep", ",", "**", "__", ")", ":", "\n", "    ", "if", "bits_to_keep", ">=", "8", ":", "\n", "        ", "return", "img", "\n", "", "return", "ImageOps", ".", "posterize", "(", "img", ",", "bits_to_keep", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.contrast": [[162, 164], ["PIL.ImageEnhance.Contrast().enhance", "PIL.ImageEnhance.Contrast"], "function", ["None"], ["", "def", "contrast", "(", "img", ",", "factor", ",", "**", "__", ")", ":", "\n", "    ", "return", "ImageEnhance", ".", "Contrast", "(", "img", ")", ".", "enhance", "(", "factor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.color": [[166, 168], ["PIL.ImageEnhance.Color().enhance", "PIL.ImageEnhance.Color"], "function", ["None"], ["", "def", "color", "(", "img", ",", "factor", ",", "**", "__", ")", ":", "\n", "    ", "return", "ImageEnhance", ".", "Color", "(", "img", ")", ".", "enhance", "(", "factor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.brightness": [[170, 172], ["PIL.ImageEnhance.Brightness().enhance", "PIL.ImageEnhance.Brightness"], "function", ["None"], ["", "def", "brightness", "(", "img", ",", "factor", ",", "**", "__", ")", ":", "\n", "    ", "return", "ImageEnhance", ".", "Brightness", "(", "img", ")", ".", "enhance", "(", "factor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.sharpness": [[174, 176], ["PIL.ImageEnhance.Sharpness().enhance", "PIL.ImageEnhance.Sharpness"], "function", ["None"], ["", "def", "sharpness", "(", "img", ",", "factor", ",", "**", "__", ")", ":", "\n", "    ", "return", "ImageEnhance", ".", "Sharpness", "(", "img", ")", ".", "enhance", "(", "factor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._randomly_negate": [[178, 181], ["random.random"], "function", ["None"], ["", "def", "_randomly_negate", "(", "v", ")", ":", "\n", "    ", "\"\"\"With 50% prob, negate the value\"\"\"", "\n", "return", "-", "v", "if", "random", ".", "random", "(", ")", ">", "0.5", "else", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._rotate_level_to_arg": [[183, 188], ["auto_augment._randomly_negate"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._randomly_negate"], ["", "def", "_rotate_level_to_arg", "(", "level", ",", "_hparams", ")", ":", "\n", "# range [-30, 30]", "\n", "    ", "level", "=", "(", "level", "/", "_LEVEL_DENOM", ")", "*", "30.", "\n", "level", "=", "_randomly_negate", "(", "level", ")", "\n", "return", "level", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._enhance_level_to_arg": [[190, 193], ["None"], "function", ["None"], ["", "def", "_enhance_level_to_arg", "(", "level", ",", "_hparams", ")", ":", "\n", "# range [0.1, 1.9]", "\n", "    ", "return", "(", "level", "/", "_LEVEL_DENOM", ")", "*", "1.8", "+", "0.1", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._enhance_increasing_level_to_arg": [[195, 201], ["max", "auto_augment._randomly_negate"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._randomly_negate"], ["", "def", "_enhance_increasing_level_to_arg", "(", "level", ",", "_hparams", ")", ":", "\n", "# the 'no change' level is 1.0, moving away from that towards 0. or 2.0 increases the enhancement blend", "\n", "# range [0.1, 1.9] if level <= _LEVEL_DENOM", "\n", "    ", "level", "=", "(", "level", "/", "_LEVEL_DENOM", ")", "*", ".9", "\n", "level", "=", "max", "(", "0.1", ",", "1.0", "+", "_randomly_negate", "(", "level", ")", ")", "# keep it >= 0.1", "\n", "return", "level", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._shear_level_to_arg": [[203, 208], ["auto_augment._randomly_negate"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._randomly_negate"], ["", "def", "_shear_level_to_arg", "(", "level", ",", "_hparams", ")", ":", "\n", "# range [-0.3, 0.3]", "\n", "    ", "level", "=", "(", "level", "/", "_LEVEL_DENOM", ")", "*", "0.3", "\n", "level", "=", "_randomly_negate", "(", "level", ")", "\n", "return", "level", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._translate_abs_level_to_arg": [[210, 215], ["auto_augment._randomly_negate", "float"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._randomly_negate"], ["", "def", "_translate_abs_level_to_arg", "(", "level", ",", "hparams", ")", ":", "\n", "    ", "translate_const", "=", "hparams", "[", "'translate_const'", "]", "\n", "level", "=", "(", "level", "/", "_LEVEL_DENOM", ")", "*", "float", "(", "translate_const", ")", "\n", "level", "=", "_randomly_negate", "(", "level", ")", "\n", "return", "level", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._translate_rel_level_to_arg": [[217, 223], ["hparams.get", "auto_augment._randomly_negate"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.features.FeatureInfo.get", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._randomly_negate"], ["", "def", "_translate_rel_level_to_arg", "(", "level", ",", "hparams", ")", ":", "\n", "# default range [-0.45, 0.45]", "\n", "    ", "translate_pct", "=", "hparams", ".", "get", "(", "'translate_pct'", ",", "0.45", ")", "\n", "level", "=", "(", "level", "/", "_LEVEL_DENOM", ")", "*", "translate_pct", "\n", "level", "=", "_randomly_negate", "(", "level", ")", "\n", "return", "level", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._posterize_level_to_arg": [[225, 230], ["int"], "function", ["None"], ["", "def", "_posterize_level_to_arg", "(", "level", ",", "_hparams", ")", ":", "\n", "# As per Tensorflow TPU EfficientNet impl", "\n", "# range [0, 4], 'keep 0 up to 4 MSB of original image'", "\n", "# intensity/severity of augmentation decreases with level", "\n", "    ", "return", "int", "(", "(", "level", "/", "_LEVEL_DENOM", ")", "*", "4", ")", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._posterize_increasing_level_to_arg": [[232, 237], ["auto_augment._posterize_level_to_arg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._posterize_level_to_arg"], ["", "def", "_posterize_increasing_level_to_arg", "(", "level", ",", "hparams", ")", ":", "\n", "# As per Tensorflow models research and UDA impl", "\n", "# range [4, 0], 'keep 4 down to 0 MSB of original image',", "\n", "# intensity/severity of augmentation increases with level", "\n", "    ", "return", "4", "-", "_posterize_level_to_arg", "(", "level", ",", "hparams", ")", "[", "0", "]", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._posterize_original_level_to_arg": [[239, 244], ["int"], "function", ["None"], ["", "def", "_posterize_original_level_to_arg", "(", "level", ",", "_hparams", ")", ":", "\n", "# As per original AutoAugment paper description", "\n", "# range [4, 8], 'keep 4 up to 8 MSB of image'", "\n", "# intensity/severity of augmentation decreases with level", "\n", "    ", "return", "int", "(", "(", "level", "/", "_LEVEL_DENOM", ")", "*", "4", ")", "+", "4", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._solarize_level_to_arg": [[246, 250], ["int"], "function", ["None"], ["", "def", "_solarize_level_to_arg", "(", "level", ",", "_hparams", ")", ":", "\n", "# range [0, 256]", "\n", "# intensity/severity of augmentation decreases with level", "\n", "    ", "return", "int", "(", "(", "level", "/", "_LEVEL_DENOM", ")", "*", "256", ")", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._solarize_increasing_level_to_arg": [[252, 256], ["auto_augment._solarize_level_to_arg"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._solarize_level_to_arg"], ["", "def", "_solarize_increasing_level_to_arg", "(", "level", ",", "_hparams", ")", ":", "\n", "# range [0, 256]", "\n", "# intensity/severity of augmentation increases with level", "\n", "    ", "return", "256", "-", "_solarize_level_to_arg", "(", "level", ",", "_hparams", ")", "[", "0", "]", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._solarize_add_level_to_arg": [[258, 261], ["int"], "function", ["None"], ["", "def", "_solarize_add_level_to_arg", "(", "level", ",", "_hparams", ")", ":", "\n", "# range [0, 110]", "\n", "    ", "return", "int", "(", "(", "level", "/", "_LEVEL_DENOM", ")", "*", "110", ")", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.auto_augment_policy_v0": [[369, 400], ["auto_augment.AugmentOp"], "function", ["None"], ["", "", "def", "auto_augment_policy_v0", "(", "hparams", ")", ":", "\n", "# ImageNet v0 policy from TPU EfficientNet impl, cannot find a paper reference.", "\n", "    ", "policy", "=", "[", "\n", "[", "(", "'Equalize'", ",", "0.8", ",", "1", ")", ",", "(", "'ShearY'", ",", "0.8", ",", "4", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.4", ",", "9", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "3", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.4", ",", "1", ")", ",", "(", "'Rotate'", ",", "0.6", ",", "8", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.8", ",", "3", ")", ",", "(", "'Equalize'", ",", "0.4", ",", "7", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.4", ",", "2", ")", ",", "(", "'Solarize'", ",", "0.6", ",", "2", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.2", ",", "0", ")", ",", "(", "'Equalize'", ",", "0.8", ",", "8", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.4", ",", "8", ")", ",", "(", "'SolarizeAdd'", ",", "0.8", ",", "3", ")", "]", ",", "\n", "[", "(", "'ShearX'", ",", "0.2", ",", "9", ")", ",", "(", "'Rotate'", ",", "0.6", ",", "8", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.6", ",", "1", ")", ",", "(", "'Equalize'", ",", "1.0", ",", "2", ")", "]", ",", "\n", "[", "(", "'Invert'", ",", "0.4", ",", "9", ")", ",", "(", "'Rotate'", ",", "0.6", ",", "0", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "1.0", ",", "9", ")", ",", "(", "'ShearY'", ",", "0.6", ",", "3", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.4", ",", "7", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "0", ")", "]", ",", "\n", "[", "(", "'Posterize'", ",", "0.4", ",", "6", ")", ",", "(", "'AutoContrast'", ",", "0.4", ",", "7", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.6", ",", "8", ")", ",", "(", "'Color'", ",", "0.6", ",", "9", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.2", ",", "4", ")", ",", "(", "'Rotate'", ",", "0.8", ",", "9", ")", "]", ",", "\n", "[", "(", "'Rotate'", ",", "1.0", ",", "7", ")", ",", "(", "'TranslateYRel'", ",", "0.8", ",", "9", ")", "]", ",", "\n", "[", "(", "'ShearX'", ",", "0.0", ",", "0", ")", ",", "(", "'Solarize'", ",", "0.8", ",", "4", ")", "]", ",", "\n", "[", "(", "'ShearY'", ",", "0.8", ",", "0", ")", ",", "(", "'Color'", ",", "0.6", ",", "4", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "1.0", ",", "0", ")", ",", "(", "'Rotate'", ",", "0.6", ",", "2", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.8", ",", "4", ")", ",", "(", "'Equalize'", ",", "0.0", ",", "8", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "1.0", ",", "4", ")", ",", "(", "'AutoContrast'", ",", "0.6", ",", "2", ")", "]", ",", "\n", "[", "(", "'ShearY'", ",", "0.4", ",", "7", ")", ",", "(", "'SolarizeAdd'", ",", "0.6", ",", "7", ")", "]", ",", "\n", "[", "(", "'Posterize'", ",", "0.8", ",", "2", ")", ",", "(", "'Solarize'", ",", "0.6", ",", "10", ")", "]", ",", "# This results in black image with Tpu posterize", "\n", "[", "(", "'Solarize'", ",", "0.6", ",", "8", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "1", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.8", ",", "6", ")", ",", "(", "'Rotate'", ",", "0.4", ",", "5", ")", "]", ",", "\n", "]", "\n", "pc", "=", "[", "[", "AugmentOp", "(", "*", "a", ",", "hparams", "=", "hparams", ")", "for", "a", "in", "sp", "]", "for", "sp", "in", "policy", "]", "\n", "return", "pc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.auto_augment_policy_v0r": [[402, 434], ["auto_augment.AugmentOp"], "function", ["None"], ["", "def", "auto_augment_policy_v0r", "(", "hparams", ")", ":", "\n", "# ImageNet v0 policy from TPU EfficientNet impl, with variation of Posterize used", "\n", "# in Google research implementation (number of bits discarded increases with magnitude)", "\n", "    ", "policy", "=", "[", "\n", "[", "(", "'Equalize'", ",", "0.8", ",", "1", ")", ",", "(", "'ShearY'", ",", "0.8", ",", "4", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.4", ",", "9", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "3", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.4", ",", "1", ")", ",", "(", "'Rotate'", ",", "0.6", ",", "8", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.8", ",", "3", ")", ",", "(", "'Equalize'", ",", "0.4", ",", "7", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.4", ",", "2", ")", ",", "(", "'Solarize'", ",", "0.6", ",", "2", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.2", ",", "0", ")", ",", "(", "'Equalize'", ",", "0.8", ",", "8", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.4", ",", "8", ")", ",", "(", "'SolarizeAdd'", ",", "0.8", ",", "3", ")", "]", ",", "\n", "[", "(", "'ShearX'", ",", "0.2", ",", "9", ")", ",", "(", "'Rotate'", ",", "0.6", ",", "8", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.6", ",", "1", ")", ",", "(", "'Equalize'", ",", "1.0", ",", "2", ")", "]", ",", "\n", "[", "(", "'Invert'", ",", "0.4", ",", "9", ")", ",", "(", "'Rotate'", ",", "0.6", ",", "0", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "1.0", ",", "9", ")", ",", "(", "'ShearY'", ",", "0.6", ",", "3", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.4", ",", "7", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "0", ")", "]", ",", "\n", "[", "(", "'PosterizeIncreasing'", ",", "0.4", ",", "6", ")", ",", "(", "'AutoContrast'", ",", "0.4", ",", "7", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.6", ",", "8", ")", ",", "(", "'Color'", ",", "0.6", ",", "9", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.2", ",", "4", ")", ",", "(", "'Rotate'", ",", "0.8", ",", "9", ")", "]", ",", "\n", "[", "(", "'Rotate'", ",", "1.0", ",", "7", ")", ",", "(", "'TranslateYRel'", ",", "0.8", ",", "9", ")", "]", ",", "\n", "[", "(", "'ShearX'", ",", "0.0", ",", "0", ")", ",", "(", "'Solarize'", ",", "0.8", ",", "4", ")", "]", ",", "\n", "[", "(", "'ShearY'", ",", "0.8", ",", "0", ")", ",", "(", "'Color'", ",", "0.6", ",", "4", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "1.0", ",", "0", ")", ",", "(", "'Rotate'", ",", "0.6", ",", "2", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.8", ",", "4", ")", ",", "(", "'Equalize'", ",", "0.0", ",", "8", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "1.0", ",", "4", ")", ",", "(", "'AutoContrast'", ",", "0.6", ",", "2", ")", "]", ",", "\n", "[", "(", "'ShearY'", ",", "0.4", ",", "7", ")", ",", "(", "'SolarizeAdd'", ",", "0.6", ",", "7", ")", "]", ",", "\n", "[", "(", "'PosterizeIncreasing'", ",", "0.8", ",", "2", ")", ",", "(", "'Solarize'", ",", "0.6", ",", "10", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.6", ",", "8", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "1", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.8", ",", "6", ")", ",", "(", "'Rotate'", ",", "0.4", ",", "5", ")", "]", ",", "\n", "]", "\n", "pc", "=", "[", "[", "AugmentOp", "(", "*", "a", ",", "hparams", "=", "hparams", ")", "for", "a", "in", "sp", "]", "for", "sp", "in", "policy", "]", "\n", "return", "pc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.auto_augment_policy_original": [[436, 467], ["auto_augment.AugmentOp"], "function", ["None"], ["", "def", "auto_augment_policy_original", "(", "hparams", ")", ":", "\n", "# ImageNet policy from https://arxiv.org/abs/1805.09501", "\n", "    ", "policy", "=", "[", "\n", "[", "(", "'PosterizeOriginal'", ",", "0.4", ",", "8", ")", ",", "(", "'Rotate'", ",", "0.6", ",", "9", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.6", ",", "5", ")", ",", "(", "'AutoContrast'", ",", "0.6", ",", "5", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.8", ",", "8", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "3", ")", "]", ",", "\n", "[", "(", "'PosterizeOriginal'", ",", "0.6", ",", "7", ")", ",", "(", "'PosterizeOriginal'", ",", "0.6", ",", "6", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.4", ",", "7", ")", ",", "(", "'Solarize'", ",", "0.2", ",", "4", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.4", ",", "4", ")", ",", "(", "'Rotate'", ",", "0.8", ",", "8", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.6", ",", "3", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "7", ")", "]", ",", "\n", "[", "(", "'PosterizeOriginal'", ",", "0.8", ",", "5", ")", ",", "(", "'Equalize'", ",", "1.0", ",", "2", ")", "]", ",", "\n", "[", "(", "'Rotate'", ",", "0.2", ",", "3", ")", ",", "(", "'Solarize'", ",", "0.6", ",", "8", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.6", ",", "8", ")", ",", "(", "'PosterizeOriginal'", ",", "0.4", ",", "6", ")", "]", ",", "\n", "[", "(", "'Rotate'", ",", "0.8", ",", "8", ")", ",", "(", "'Color'", ",", "0.4", ",", "0", ")", "]", ",", "\n", "[", "(", "'Rotate'", ",", "0.4", ",", "9", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "2", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.0", ",", "7", ")", ",", "(", "'Equalize'", ",", "0.8", ",", "8", ")", "]", ",", "\n", "[", "(", "'Invert'", ",", "0.6", ",", "4", ")", ",", "(", "'Equalize'", ",", "1.0", ",", "8", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.6", ",", "4", ")", ",", "(", "'Contrast'", ",", "1.0", ",", "8", ")", "]", ",", "\n", "[", "(", "'Rotate'", ",", "0.8", ",", "8", ")", ",", "(", "'Color'", ",", "1.0", ",", "2", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.8", ",", "8", ")", ",", "(", "'Solarize'", ",", "0.8", ",", "7", ")", "]", ",", "\n", "[", "(", "'Sharpness'", ",", "0.4", ",", "7", ")", ",", "(", "'Invert'", ",", "0.6", ",", "8", ")", "]", ",", "\n", "[", "(", "'ShearX'", ",", "0.6", ",", "5", ")", ",", "(", "'Equalize'", ",", "1.0", ",", "9", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.4", ",", "0", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "3", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.4", ",", "7", ")", ",", "(", "'Solarize'", ",", "0.2", ",", "4", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.6", ",", "5", ")", ",", "(", "'AutoContrast'", ",", "0.6", ",", "5", ")", "]", ",", "\n", "[", "(", "'Invert'", ",", "0.6", ",", "4", ")", ",", "(", "'Equalize'", ",", "1.0", ",", "8", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.6", ",", "4", ")", ",", "(", "'Contrast'", ",", "1.0", ",", "8", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.8", ",", "8", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "3", ")", "]", ",", "\n", "]", "\n", "pc", "=", "[", "[", "AugmentOp", "(", "*", "a", ",", "hparams", "=", "hparams", ")", "for", "a", "in", "sp", "]", "for", "sp", "in", "policy", "]", "\n", "return", "pc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.auto_augment_policy_originalr": [[469, 500], ["auto_augment.AugmentOp"], "function", ["None"], ["", "def", "auto_augment_policy_originalr", "(", "hparams", ")", ":", "\n", "# ImageNet policy from https://arxiv.org/abs/1805.09501 with research posterize variation", "\n", "    ", "policy", "=", "[", "\n", "[", "(", "'PosterizeIncreasing'", ",", "0.4", ",", "8", ")", ",", "(", "'Rotate'", ",", "0.6", ",", "9", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.6", ",", "5", ")", ",", "(", "'AutoContrast'", ",", "0.6", ",", "5", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.8", ",", "8", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "3", ")", "]", ",", "\n", "[", "(", "'PosterizeIncreasing'", ",", "0.6", ",", "7", ")", ",", "(", "'PosterizeIncreasing'", ",", "0.6", ",", "6", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.4", ",", "7", ")", ",", "(", "'Solarize'", ",", "0.2", ",", "4", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.4", ",", "4", ")", ",", "(", "'Rotate'", ",", "0.8", ",", "8", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.6", ",", "3", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "7", ")", "]", ",", "\n", "[", "(", "'PosterizeIncreasing'", ",", "0.8", ",", "5", ")", ",", "(", "'Equalize'", ",", "1.0", ",", "2", ")", "]", ",", "\n", "[", "(", "'Rotate'", ",", "0.2", ",", "3", ")", ",", "(", "'Solarize'", ",", "0.6", ",", "8", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.6", ",", "8", ")", ",", "(", "'PosterizeIncreasing'", ",", "0.4", ",", "6", ")", "]", ",", "\n", "[", "(", "'Rotate'", ",", "0.8", ",", "8", ")", ",", "(", "'Color'", ",", "0.4", ",", "0", ")", "]", ",", "\n", "[", "(", "'Rotate'", ",", "0.4", ",", "9", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "2", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.0", ",", "7", ")", ",", "(", "'Equalize'", ",", "0.8", ",", "8", ")", "]", ",", "\n", "[", "(", "'Invert'", ",", "0.6", ",", "4", ")", ",", "(", "'Equalize'", ",", "1.0", ",", "8", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.6", ",", "4", ")", ",", "(", "'Contrast'", ",", "1.0", ",", "8", ")", "]", ",", "\n", "[", "(", "'Rotate'", ",", "0.8", ",", "8", ")", ",", "(", "'Color'", ",", "1.0", ",", "2", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.8", ",", "8", ")", ",", "(", "'Solarize'", ",", "0.8", ",", "7", ")", "]", ",", "\n", "[", "(", "'Sharpness'", ",", "0.4", ",", "7", ")", ",", "(", "'Invert'", ",", "0.6", ",", "8", ")", "]", ",", "\n", "[", "(", "'ShearX'", ",", "0.6", ",", "5", ")", ",", "(", "'Equalize'", ",", "1.0", ",", "9", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.4", ",", "0", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "3", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.4", ",", "7", ")", ",", "(", "'Solarize'", ",", "0.2", ",", "4", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.6", ",", "5", ")", ",", "(", "'AutoContrast'", ",", "0.6", ",", "5", ")", "]", ",", "\n", "[", "(", "'Invert'", ",", "0.6", ",", "4", ")", ",", "(", "'Equalize'", ",", "1.0", ",", "8", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.6", ",", "4", ")", ",", "(", "'Contrast'", ",", "1.0", ",", "8", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.8", ",", "8", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "3", ")", "]", ",", "\n", "]", "\n", "pc", "=", "[", "[", "AugmentOp", "(", "*", "a", ",", "hparams", "=", "hparams", ")", "for", "a", "in", "sp", "]", "for", "sp", "in", "policy", "]", "\n", "return", "pc", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.auto_augment_policy": [[502, 514], ["auto_augment.auto_augment_policy_original", "auto_augment.auto_augment_policy_originalr", "auto_augment.auto_augment_policy_v0", "auto_augment.auto_augment_policy_v0r"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.auto_augment_policy_original", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.auto_augment_policy_originalr", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.auto_augment_policy_v0", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.auto_augment_policy_v0r"], ["", "def", "auto_augment_policy", "(", "name", "=", "'v0'", ",", "hparams", "=", "None", ")", ":", "\n", "    ", "hparams", "=", "hparams", "or", "_HPARAMS_DEFAULT", "\n", "if", "name", "==", "'original'", ":", "\n", "        ", "return", "auto_augment_policy_original", "(", "hparams", ")", "\n", "", "elif", "name", "==", "'originalr'", ":", "\n", "        ", "return", "auto_augment_policy_originalr", "(", "hparams", ")", "\n", "", "elif", "name", "==", "'v0'", ":", "\n", "        ", "return", "auto_augment_policy_v0", "(", "hparams", ")", "\n", "", "elif", "name", "==", "'v0r'", ":", "\n", "        ", "return", "auto_augment_policy_v0r", "(", "hparams", ")", "\n", "", "else", ":", "\n", "        ", "assert", "False", ",", "'Unknown AA policy (%s)'", "%", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.auto_augment_transform": [[537, 566], ["config_str.split", "auto_augment.auto_augment_policy", "auto_augment.AutoAugment", "re.split", "len", "hparams.setdefault", "float"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.auto_augment_policy"], ["", "", "def", "auto_augment_transform", "(", "config_str", ",", "hparams", ")", ":", "\n", "    ", "\"\"\"\n    Create a AutoAugment transform\n\n    :param config_str: String defining configuration of auto augmentation. Consists of multiple sections separated by\n    dashes ('-'). The first section defines the AutoAugment policy (one of 'v0', 'v0r', 'original', 'originalr').\n    The remaining sections, not order sepecific determine\n        'mstd' -  float std deviation of magnitude noise applied\n    Ex 'original-mstd0.5' results in AutoAugment with original policy, magnitude_std 0.5\n\n    :param hparams: Other hparams (kwargs) for the AutoAugmentation scheme\n\n    :return: A PyTorch compatible Transform\n    \"\"\"", "\n", "config", "=", "config_str", ".", "split", "(", "'-'", ")", "\n", "policy_name", "=", "config", "[", "0", "]", "\n", "config", "=", "config", "[", "1", ":", "]", "\n", "for", "c", "in", "config", ":", "\n", "        ", "cs", "=", "re", ".", "split", "(", "r'(\\d.*)'", ",", "c", ")", "\n", "if", "len", "(", "cs", ")", "<", "2", ":", "\n", "            ", "continue", "\n", "", "key", ",", "val", "=", "cs", "[", ":", "2", "]", "\n", "if", "key", "==", "'mstd'", ":", "\n", "# noise param injected via hparams for now", "\n", "            ", "hparams", ".", "setdefault", "(", "'magnitude_std'", ",", "float", "(", "val", ")", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "'Unknown AutoAugment config section'", "\n", "", "", "aa_policy", "=", "auto_augment_policy", "(", "policy_name", ",", "hparams", "=", "hparams", ")", "\n", "return", "AutoAugment", "(", "aa_policy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._select_rand_weights": [[630, 637], ["numpy.sum"], "function", ["None"], ["def", "_select_rand_weights", "(", "weight_idx", "=", "0", ",", "transforms", "=", "None", ")", ":", "\n", "    ", "transforms", "=", "transforms", "or", "_RAND_TRANSFORMS", "\n", "assert", "weight_idx", "==", "0", "# only one set of weights currently", "\n", "rand_weights", "=", "_RAND_CHOICE_WEIGHTS_0", "\n", "probs", "=", "[", "rand_weights", "[", "k", "]", "for", "k", "in", "transforms", "]", "\n", "probs", "/=", "np", ".", "sum", "(", "probs", ")", "\n", "return", "probs", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.rand_augment_ops": [[639, 644], ["auto_augment.AugmentOp"], "function", ["None"], ["", "def", "rand_augment_ops", "(", "magnitude", "=", "10", ",", "hparams", "=", "None", ",", "transforms", "=", "None", ")", ":", "\n", "    ", "hparams", "=", "hparams", "or", "_HPARAMS_DEFAULT", "\n", "transforms", "=", "transforms", "or", "_RAND_TRANSFORMS", "\n", "return", "[", "AugmentOp", "(", "\n", "name", ",", "prob", "=", "0.5", ",", "magnitude", "=", "magnitude", ",", "hparams", "=", "hparams", ")", "for", "name", "in", "transforms", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.rand_augment_transform": [[668, 724], ["config_str.split", "auto_augment.rand_augment_ops", "auto_augment.RandAugment", "re.split", "auto_augment._select_rand_weights", "len", "float", "hparams.setdefault", "float", "hparams.setdefault", "int", "bool", "int", "int", "int"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.rand_augment_ops", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment._select_rand_weights"], ["", "", "def", "rand_augment_transform", "(", "config_str", ",", "hparams", ")", ":", "\n", "    ", "\"\"\"\n    Create a RandAugment transform\n\n    :param config_str: String defining configuration of random augmentation. Consists of multiple sections separated by\n    dashes ('-'). The first section defines the specific variant of rand augment (currently only 'rand'). The remaining\n    sections, not order sepecific determine\n        'm' - integer magnitude of rand augment\n        'n' - integer num layers (number of transform ops selected per image)\n        'w' - integer probabiliy weight index (index of a set of weights to influence choice of op)\n        'mstd' -  float std deviation of magnitude noise applied, or uniform sampling if infinity (or > 100)\n        'mmax' - set upper bound for magnitude to something other than default of  _LEVEL_DENOM (10)\n        'inc' - integer (bool), use augmentations that increase in severity with magnitude (default: 0)\n    Ex 'rand-m9-n3-mstd0.5' results in RandAugment with magnitude 9, num_layers 3, magnitude_std 0.5\n    'rand-mstd1-w0' results in magnitude_std 1.0, weights 0, default magnitude of 10 and num_layers 2\n\n    :param hparams: Other hparams (kwargs) for the RandAugmentation scheme\n\n    :return: A PyTorch compatible Transform\n    \"\"\"", "\n", "magnitude", "=", "_LEVEL_DENOM", "# default to _LEVEL_DENOM for magnitude (currently 10)", "\n", "num_layers", "=", "2", "# default to 2 ops per image", "\n", "weight_idx", "=", "None", "# default to no probability weights for op choice", "\n", "transforms", "=", "_RAND_TRANSFORMS", "\n", "config", "=", "config_str", ".", "split", "(", "'-'", ")", "\n", "assert", "config", "[", "0", "]", "==", "'rand'", "\n", "config", "=", "config", "[", "1", ":", "]", "\n", "for", "c", "in", "config", ":", "\n", "        ", "cs", "=", "re", ".", "split", "(", "r'(\\d.*)'", ",", "c", ")", "\n", "if", "len", "(", "cs", ")", "<", "2", ":", "\n", "            ", "continue", "\n", "", "key", ",", "val", "=", "cs", "[", ":", "2", "]", "\n", "if", "key", "==", "'mstd'", ":", "\n", "# noise param / randomization of magnitude values", "\n", "            ", "mstd", "=", "float", "(", "val", ")", "\n", "if", "mstd", ">", "100", ":", "\n", "# use uniform sampling in 0 to magnitude if mstd is > 100", "\n", "                ", "mstd", "=", "float", "(", "'inf'", ")", "\n", "", "hparams", ".", "setdefault", "(", "'magnitude_std'", ",", "mstd", ")", "\n", "", "elif", "key", "==", "'mmax'", ":", "\n", "# clip magnitude between [0, mmax] instead of default [0, _LEVEL_DENOM]", "\n", "            ", "hparams", ".", "setdefault", "(", "'magnitude_max'", ",", "int", "(", "val", ")", ")", "\n", "", "elif", "key", "==", "'inc'", ":", "\n", "            ", "if", "bool", "(", "val", ")", ":", "\n", "                ", "transforms", "=", "_RAND_INCREASING_TRANSFORMS", "\n", "", "", "elif", "key", "==", "'m'", ":", "\n", "            ", "magnitude", "=", "int", "(", "val", ")", "\n", "", "elif", "key", "==", "'n'", ":", "\n", "            ", "num_layers", "=", "int", "(", "val", ")", "\n", "", "elif", "key", "==", "'w'", ":", "\n", "            ", "weight_idx", "=", "int", "(", "val", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "'Unknown RandAugment config section'", "\n", "", "", "ra_ops", "=", "rand_augment_ops", "(", "magnitude", "=", "magnitude", ",", "hparams", "=", "hparams", ",", "transforms", "=", "transforms", ")", "\n", "choice_weights", "=", "None", "if", "weight_idx", "is", "None", "else", "_select_rand_weights", "(", "weight_idx", ")", "\n", "return", "RandAugment", "(", "ra_ops", ",", "num_layers", ",", "choice_weights", "=", "choice_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.augmix_ops": [[743, 748], ["auto_augment.AugmentOp"], "function", ["None"], ["def", "augmix_ops", "(", "magnitude", "=", "10", ",", "hparams", "=", "None", ",", "transforms", "=", "None", ")", ":", "\n", "    ", "hparams", "=", "hparams", "or", "_HPARAMS_DEFAULT", "\n", "transforms", "=", "transforms", "or", "_AUGMIX_TRANSFORMS", "\n", "return", "[", "AugmentOp", "(", "\n", "name", ",", "prob", "=", "1.0", ",", "magnitude", "=", "magnitude", ",", "hparams", "=", "hparams", ")", "for", "name", "in", "transforms", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.augment_and_mix_transform": [[823, 871], ["config_str.split", "hparams.setdefault", "auto_augment.augmix_ops", "auto_augment.AugMixAugment", "re.split", "float", "len", "hparams.setdefault", "float", "int", "int", "int", "float", "bool"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.auto_augment.augmix_ops"], ["", "", "def", "augment_and_mix_transform", "(", "config_str", ",", "hparams", ")", ":", "\n", "    ", "\"\"\" Create AugMix PyTorch transform\n\n    :param config_str: String defining configuration of random augmentation. Consists of multiple sections separated by\n    dashes ('-'). The first section defines the specific variant of rand augment (currently only 'rand'). The remaining\n    sections, not order sepecific determine\n        'm' - integer magnitude (severity) of augmentation mix (default: 3)\n        'w' - integer width of augmentation chain (default: 3)\n        'd' - integer depth of augmentation chain (-1 is random [1, 3], default: -1)\n        'b' - integer (bool), blend each branch of chain into end result without a final blend, less CPU (default: 0)\n        'mstd' -  float std deviation of magnitude noise applied (default: 0)\n    Ex 'augmix-m5-w4-d2' results in AugMix with severity 5, chain width 4, chain depth 2\n\n    :param hparams: Other hparams (kwargs) for the Augmentation transforms\n\n    :return: A PyTorch compatible Transform\n    \"\"\"", "\n", "magnitude", "=", "3", "\n", "width", "=", "3", "\n", "depth", "=", "-", "1", "\n", "alpha", "=", "1.", "\n", "blended", "=", "False", "\n", "config", "=", "config_str", ".", "split", "(", "'-'", ")", "\n", "assert", "config", "[", "0", "]", "==", "'augmix'", "\n", "config", "=", "config", "[", "1", ":", "]", "\n", "for", "c", "in", "config", ":", "\n", "        ", "cs", "=", "re", ".", "split", "(", "r'(\\d.*)'", ",", "c", ")", "\n", "if", "len", "(", "cs", ")", "<", "2", ":", "\n", "            ", "continue", "\n", "", "key", ",", "val", "=", "cs", "[", ":", "2", "]", "\n", "if", "key", "==", "'mstd'", ":", "\n", "# noise param injected via hparams for now", "\n", "            ", "hparams", ".", "setdefault", "(", "'magnitude_std'", ",", "float", "(", "val", ")", ")", "\n", "", "elif", "key", "==", "'m'", ":", "\n", "            ", "magnitude", "=", "int", "(", "val", ")", "\n", "", "elif", "key", "==", "'w'", ":", "\n", "            ", "width", "=", "int", "(", "val", ")", "\n", "", "elif", "key", "==", "'d'", ":", "\n", "            ", "depth", "=", "int", "(", "val", ")", "\n", "", "elif", "key", "==", "'a'", ":", "\n", "            ", "alpha", "=", "float", "(", "val", ")", "\n", "", "elif", "key", "==", "'b'", ":", "\n", "            ", "blended", "=", "bool", "(", "val", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "'Unknown AugMix config section'", "\n", "", "", "hparams", ".", "setdefault", "(", "'magnitude_std'", ",", "float", "(", "'inf'", ")", ")", "# default to uniform sampling (if not set via mstd arg)", "\n", "ops", "=", "augmix_ops", "(", "magnitude", "=", "magnitude", ",", "hparams", "=", "hparams", ")", "\n", "return", "AugMixAugment", "(", "ops", ",", "alpha", "=", "alpha", ",", "width", "=", "width", ",", "depth", "=", "depth", ",", "blended", "=", "blended", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.ImageDataset.__init__": [[22, 38], ["isinstance", "parsers.create_parser"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_factory.create_parser"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "root", ",", "\n", "parser", "=", "None", ",", "\n", "class_map", "=", "None", ",", "\n", "load_bytes", "=", "False", ",", "\n", "transform", "=", "None", ",", "\n", "target_transform", "=", "None", ",", "\n", ")", ":", "\n", "        ", "if", "parser", "is", "None", "or", "isinstance", "(", "parser", ",", "str", ")", ":", "\n", "            ", "parser", "=", "create_parser", "(", "parser", "or", "''", ",", "root", "=", "root", ",", "class_map", "=", "class_map", ")", "\n", "", "self", ".", "parser", "=", "parser", "\n", "self", ".", "load_bytes", "=", "load_bytes", "\n", "self", ".", "transform", "=", "transform", "\n", "self", ".", "target_transform", "=", "target_transform", "\n", "self", ".", "_consecutive_errors", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.ImageDataset.__getitem__": [[39, 58], ["dataset.ImageDataset.transform", "dataset.ImageDataset.read", "PIL.Image.open().convert", "_logger.warning", "dataset.ImageDataset.target_transform", "dataset.ImageDataset.__getitem__", "PIL.Image.open", "dataset.ImageDataset.parser.filename", "str", "len"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.AugMixDataset.transform", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.convert.convert_from_mxnet.convert", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_tar.ParserImageTar.__getitem__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser.Parser.filename"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "img", ",", "target", "=", "self", ".", "parser", "[", "index", "]", "\n", "try", ":", "\n", "            ", "img", "=", "img", ".", "read", "(", ")", "if", "self", ".", "load_bytes", "else", "Image", ".", "open", "(", "img", ")", ".", "convert", "(", "'RGB'", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "_logger", ".", "warning", "(", "f'Skipped sample (index {index}, file {self.parser.filename(index)}). {str(e)}'", ")", "\n", "self", ".", "_consecutive_errors", "+=", "1", "\n", "if", "self", ".", "_consecutive_errors", "<", "_ERROR_RETRY", ":", "\n", "                ", "return", "self", ".", "__getitem__", "(", "(", "index", "+", "1", ")", "%", "len", "(", "self", ".", "parser", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "e", "\n", "", "", "self", ".", "_consecutive_errors", "=", "0", "\n", "if", "self", ".", "transform", "is", "not", "None", ":", "\n", "            ", "img", "=", "self", ".", "transform", "(", "img", ")", "\n", "", "if", "target", "is", "None", ":", "\n", "            ", "target", "=", "-", "1", "\n", "", "elif", "self", ".", "target_transform", "is", "not", "None", ":", "\n", "            ", "target", "=", "self", ".", "target_transform", "(", "target", ")", "\n", "", "return", "img", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.ImageDataset.__len__": [[59, 61], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "parser", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.ImageDataset.filename": [[62, 64], ["dataset.ImageDataset.parser.filename"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser.Parser.filename"], ["", "def", "filename", "(", "self", ",", "index", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "parser", ".", "filename", "(", "index", ",", "basename", ",", "absolute", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.ImageDataset.filenames": [[65, 67], ["dataset.ImageDataset.parser.filenames"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser.Parser.filenames"], ["", "def", "filenames", "(", "self", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "parser", ".", "filenames", "(", "basename", ",", "absolute", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.IterableImageDataset.__init__": [[71, 93], ["isinstance", "parsers.create_parser"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_factory.create_parser"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "root", ",", "\n", "parser", "=", "None", ",", "\n", "split", "=", "'train'", ",", "\n", "is_training", "=", "False", ",", "\n", "batch_size", "=", "None", ",", "\n", "repeats", "=", "0", ",", "\n", "download", "=", "False", ",", "\n", "transform", "=", "None", ",", "\n", "target_transform", "=", "None", ",", "\n", ")", ":", "\n", "        ", "assert", "parser", "is", "not", "None", "\n", "if", "isinstance", "(", "parser", ",", "str", ")", ":", "\n", "            ", "self", ".", "parser", "=", "create_parser", "(", "\n", "parser", ",", "root", "=", "root", ",", "split", "=", "split", ",", "is_training", "=", "is_training", ",", "\n", "batch_size", "=", "batch_size", ",", "repeats", "=", "repeats", ",", "download", "=", "download", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "parser", "=", "parser", "\n", "", "self", ".", "transform", "=", "transform", "\n", "self", ".", "target_transform", "=", "target_transform", "\n", "self", ".", "_consecutive_errors", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.IterableImageDataset.__iter__": [[94, 101], ["dataset.IterableImageDataset.transform", "dataset.IterableImageDataset.target_transform"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.AugMixDataset.transform"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "img", ",", "target", "in", "self", ".", "parser", ":", "\n", "            ", "if", "self", ".", "transform", "is", "not", "None", ":", "\n", "                ", "img", "=", "self", ".", "transform", "(", "img", ")", "\n", "", "if", "self", ".", "target_transform", "is", "not", "None", ":", "\n", "                ", "target", "=", "self", ".", "target_transform", "(", "target", ")", "\n", "", "yield", "img", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.IterableImageDataset.__len__": [[102, 107], ["hasattr", "len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "if", "hasattr", "(", "self", ".", "parser", ",", "'__len__'", ")", ":", "\n", "            ", "return", "len", "(", "self", ".", "parser", ")", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.IterableImageDataset.filename": [[108, 110], ["None"], "methods", ["None"], ["", "", "def", "filename", "(", "self", ",", "index", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "assert", "False", ",", "'Filename lookup by index not supported, use filenames().'", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.IterableImageDataset.filenames": [[111, 113], ["dataset.IterableImageDataset.parser.filenames"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser.Parser.filenames"], ["", "def", "filenames", "(", "self", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "parser", ".", "filenames", "(", "basename", ",", "absolute", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.AugMixDataset.__init__": [[118, 125], ["dataset.AugMixDataset._set_transforms"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.AugMixDataset._set_transforms"], ["def", "__init__", "(", "self", ",", "dataset", ",", "num_splits", "=", "2", ")", ":", "\n", "        ", "self", ".", "augmentation", "=", "None", "\n", "self", ".", "normalize", "=", "None", "\n", "self", ".", "dataset", "=", "dataset", "\n", "if", "self", ".", "dataset", ".", "transform", "is", "not", "None", ":", "\n", "            ", "self", ".", "_set_transforms", "(", "self", ".", "dataset", ".", "transform", ")", "\n", "", "self", ".", "num_splits", "=", "num_splits", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.AugMixDataset._set_transforms": [[126, 131], ["isinstance", "len"], "methods", ["None"], ["", "def", "_set_transforms", "(", "self", ",", "x", ")", ":", "\n", "        ", "assert", "isinstance", "(", "x", ",", "(", "list", ",", "tuple", ")", ")", "and", "len", "(", "x", ")", "==", "3", ",", "'Expecting a tuple/list of 3 transforms'", "\n", "self", ".", "dataset", ".", "transform", "=", "x", "[", "0", "]", "\n", "self", ".", "augmentation", "=", "x", "[", "1", "]", "\n", "self", ".", "normalize", "=", "x", "[", "2", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.AugMixDataset.transform": [[136, 139], ["dataset.AugMixDataset._set_transforms"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.AugMixDataset._set_transforms"], ["", "@", "transform", ".", "setter", "\n", "def", "transform", "(", "self", ",", "x", ")", ":", "\n", "        ", "self", ".", "_set_transforms", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.AugMixDataset._normalize": [[140, 142], ["dataset.AugMixDataset.normalize"], "methods", ["None"], ["", "def", "_normalize", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", "if", "self", ".", "normalize", "is", "None", "else", "self", ".", "normalize", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.AugMixDataset.__getitem__": [[143, 150], ["range", "dataset.AugMixDataset._normalize", "x_list.append", "tuple", "dataset.AugMixDataset._normalize", "dataset.AugMixDataset.augmentation"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.AugMixDataset._normalize", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.AugMixDataset._normalize"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "x", ",", "y", "=", "self", ".", "dataset", "[", "i", "]", "# all splits share the same dataset base transform", "\n", "x_list", "=", "[", "self", ".", "_normalize", "(", "x", ")", "]", "# first split only normalizes (this is the 'clean' split)", "\n", "# run the full augmentation on the remaining splits", "\n", "for", "_", "in", "range", "(", "self", ".", "num_splits", "-", "1", ")", ":", "\n", "            ", "x_list", ".", "append", "(", "self", ".", "_normalize", "(", "self", ".", "augmentation", "(", "x", ")", ")", ")", "\n", "", "return", "tuple", "(", "x_list", ")", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset.AugMixDataset.__len__": [[151, 153], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "dataset", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.Mixup.__init__": [[104, 120], ["len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "mixup_alpha", "=", "1.", ",", "cutmix_alpha", "=", "0.", ",", "cutmix_minmax", "=", "None", ",", "prob", "=", "1.0", ",", "switch_prob", "=", "0.5", ",", "\n", "mode", "=", "'batch'", ",", "correct_lam", "=", "True", ",", "label_smoothing", "=", "0.1", ",", "num_classes", "=", "1000", ")", ":", "\n", "        ", "self", ".", "mixup_alpha", "=", "mixup_alpha", "\n", "self", ".", "cutmix_alpha", "=", "cutmix_alpha", "\n", "self", ".", "cutmix_minmax", "=", "cutmix_minmax", "\n", "if", "self", ".", "cutmix_minmax", "is", "not", "None", ":", "\n", "            ", "assert", "len", "(", "self", ".", "cutmix_minmax", ")", "==", "2", "\n", "# force cutmix alpha == 1.0 when minmax active to keep logic simple & safe", "\n", "self", ".", "cutmix_alpha", "=", "1.0", "\n", "", "self", ".", "mix_prob", "=", "prob", "\n", "self", ".", "switch_prob", "=", "switch_prob", "\n", "self", ".", "label_smoothing", "=", "label_smoothing", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "correct_lam", "=", "correct_lam", "# correct lambda based on clipped area for cutmix", "\n", "self", ".", "mixup_enabled", "=", "True", "# set to false to disable mixing (intended tp be set by train loop)", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.Mixup._params_per_elem": [[121, 140], ["numpy.ones", "numpy.zeros", "numpy.where", "numpy.where", "numpy.random.beta.astype", "numpy.random.rand", "numpy.random.beta", "numpy.random.beta", "numpy.random.beta", "numpy.random.rand", "numpy.ones", "numpy.random.beta"], "methods", ["None"], ["", "def", "_params_per_elem", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "lam", "=", "np", ".", "ones", "(", "batch_size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "use_cutmix", "=", "np", ".", "zeros", "(", "batch_size", ",", "dtype", "=", "np", ".", "bool", ")", "\n", "if", "self", ".", "mixup_enabled", ":", "\n", "            ", "if", "self", ".", "mixup_alpha", ">", "0.", "and", "self", ".", "cutmix_alpha", ">", "0.", ":", "\n", "                ", "use_cutmix", "=", "np", ".", "random", ".", "rand", "(", "batch_size", ")", "<", "self", ".", "switch_prob", "\n", "lam_mix", "=", "np", ".", "where", "(", "\n", "use_cutmix", ",", "\n", "np", ".", "random", ".", "beta", "(", "self", ".", "cutmix_alpha", ",", "self", ".", "cutmix_alpha", ",", "size", "=", "batch_size", ")", ",", "\n", "np", ".", "random", ".", "beta", "(", "self", ".", "mixup_alpha", ",", "self", ".", "mixup_alpha", ",", "size", "=", "batch_size", ")", ")", "\n", "", "elif", "self", ".", "mixup_alpha", ">", "0.", ":", "\n", "                ", "lam_mix", "=", "np", ".", "random", ".", "beta", "(", "self", ".", "mixup_alpha", ",", "self", ".", "mixup_alpha", ",", "size", "=", "batch_size", ")", "\n", "", "elif", "self", ".", "cutmix_alpha", ">", "0.", ":", "\n", "                ", "use_cutmix", "=", "np", ".", "ones", "(", "batch_size", ",", "dtype", "=", "np", ".", "bool", ")", "\n", "lam_mix", "=", "np", ".", "random", ".", "beta", "(", "self", ".", "cutmix_alpha", ",", "self", ".", "cutmix_alpha", ",", "size", "=", "batch_size", ")", "\n", "", "else", ":", "\n", "                ", "assert", "False", ",", "\"One of mixup_alpha > 0., cutmix_alpha > 0., cutmix_minmax not None should be true.\"", "\n", "", "lam", "=", "np", ".", "where", "(", "np", ".", "random", ".", "rand", "(", "batch_size", ")", "<", "self", ".", "mix_prob", ",", "lam_mix", ".", "astype", "(", "np", ".", "float32", ")", ",", "lam", ")", "\n", "", "return", "lam", ",", "use_cutmix", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.Mixup._params_per_batch": [[141, 158], ["float", "numpy.random.rand", "numpy.random.rand", "numpy.random.beta", "numpy.random.beta", "numpy.random.beta", "numpy.random.beta"], "methods", ["None"], ["", "def", "_params_per_batch", "(", "self", ")", ":", "\n", "        ", "lam", "=", "1.", "\n", "use_cutmix", "=", "False", "\n", "if", "self", ".", "mixup_enabled", "and", "np", ".", "random", ".", "rand", "(", ")", "<", "self", ".", "mix_prob", ":", "\n", "            ", "if", "self", ".", "mixup_alpha", ">", "0.", "and", "self", ".", "cutmix_alpha", ">", "0.", ":", "\n", "                ", "use_cutmix", "=", "np", ".", "random", ".", "rand", "(", ")", "<", "self", ".", "switch_prob", "\n", "lam_mix", "=", "np", ".", "random", ".", "beta", "(", "self", ".", "cutmix_alpha", ",", "self", ".", "cutmix_alpha", ")", "if", "use_cutmix", "else", "np", ".", "random", ".", "beta", "(", "self", ".", "mixup_alpha", ",", "self", ".", "mixup_alpha", ")", "\n", "", "elif", "self", ".", "mixup_alpha", ">", "0.", ":", "\n", "                ", "lam_mix", "=", "np", ".", "random", ".", "beta", "(", "self", ".", "mixup_alpha", ",", "self", ".", "mixup_alpha", ")", "\n", "", "elif", "self", ".", "cutmix_alpha", ">", "0.", ":", "\n", "                ", "use_cutmix", "=", "True", "\n", "lam_mix", "=", "np", ".", "random", ".", "beta", "(", "self", ".", "cutmix_alpha", ",", "self", ".", "cutmix_alpha", ")", "\n", "", "else", ":", "\n", "                ", "assert", "False", ",", "\"One of mixup_alpha > 0., cutmix_alpha > 0., cutmix_minmax not None should be true.\"", "\n", "", "lam", "=", "float", "(", "lam_mix", ")", "\n", "", "return", "lam", ",", "use_cutmix", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.Mixup._mix_elem": [[159, 175], ["len", "mixup.Mixup._params_per_elem", "x.clone", "range", "torch.tensor().unsqueeze", "torch.tensor", "mixup.cutmix_bbox_and_lam"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.Mixup._params_per_elem", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.cutmix_bbox_and_lam"], ["", "def", "_mix_elem", "(", "self", ",", "x", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "x", ")", "\n", "lam_batch", ",", "use_cutmix", "=", "self", ".", "_params_per_elem", "(", "batch_size", ")", "\n", "x_orig", "=", "x", ".", "clone", "(", ")", "# need to keep an unmodified original for mixing source", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "j", "=", "batch_size", "-", "i", "-", "1", "\n", "lam", "=", "lam_batch", "[", "i", "]", "\n", "if", "lam", "!=", "1.", ":", "\n", "                ", "if", "use_cutmix", "[", "i", "]", ":", "\n", "                    ", "(", "yl", ",", "yh", ",", "xl", ",", "xh", ")", ",", "lam", "=", "cutmix_bbox_and_lam", "(", "\n", "x", "[", "i", "]", ".", "shape", ",", "lam", ",", "ratio_minmax", "=", "self", ".", "cutmix_minmax", ",", "correct_lam", "=", "self", ".", "correct_lam", ")", "\n", "x", "[", "i", "]", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "=", "x_orig", "[", "j", "]", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "\n", "lam_batch", "[", "i", "]", "=", "lam", "\n", "", "else", ":", "\n", "                    ", "x", "[", "i", "]", "=", "x", "[", "i", "]", "*", "lam", "+", "x_orig", "[", "j", "]", "*", "(", "1", "-", "lam", ")", "\n", "", "", "", "return", "torch", ".", "tensor", "(", "lam_batch", ",", "device", "=", "x", ".", "device", ",", "dtype", "=", "x", ".", "dtype", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.Mixup._mix_pair": [[176, 195], ["len", "mixup.Mixup._params_per_elem", "x.clone", "range", "numpy.concatenate", "torch.tensor().unsqueeze", "torch.tensor", "mixup.cutmix_bbox_and_lam"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.Mixup._params_per_elem", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.cutmix_bbox_and_lam"], ["", "def", "_mix_pair", "(", "self", ",", "x", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "x", ")", "\n", "lam_batch", ",", "use_cutmix", "=", "self", ".", "_params_per_elem", "(", "batch_size", "//", "2", ")", "\n", "x_orig", "=", "x", ".", "clone", "(", ")", "# need to keep an unmodified original for mixing source", "\n", "for", "i", "in", "range", "(", "batch_size", "//", "2", ")", ":", "\n", "            ", "j", "=", "batch_size", "-", "i", "-", "1", "\n", "lam", "=", "lam_batch", "[", "i", "]", "\n", "if", "lam", "!=", "1.", ":", "\n", "                ", "if", "use_cutmix", "[", "i", "]", ":", "\n", "                    ", "(", "yl", ",", "yh", ",", "xl", ",", "xh", ")", ",", "lam", "=", "cutmix_bbox_and_lam", "(", "\n", "x", "[", "i", "]", ".", "shape", ",", "lam", ",", "ratio_minmax", "=", "self", ".", "cutmix_minmax", ",", "correct_lam", "=", "self", ".", "correct_lam", ")", "\n", "x", "[", "i", "]", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "=", "x_orig", "[", "j", "]", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "\n", "x", "[", "j", "]", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "=", "x_orig", "[", "i", "]", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "\n", "lam_batch", "[", "i", "]", "=", "lam", "\n", "", "else", ":", "\n", "                    ", "x", "[", "i", "]", "=", "x", "[", "i", "]", "*", "lam", "+", "x_orig", "[", "j", "]", "*", "(", "1", "-", "lam", ")", "\n", "x", "[", "j", "]", "=", "x", "[", "j", "]", "*", "lam", "+", "x_orig", "[", "i", "]", "*", "(", "1", "-", "lam", ")", "\n", "", "", "", "lam_batch", "=", "np", ".", "concatenate", "(", "(", "lam_batch", ",", "lam_batch", "[", ":", ":", "-", "1", "]", ")", ")", "\n", "return", "torch", ".", "tensor", "(", "lam_batch", ",", "device", "=", "x", ".", "device", ",", "dtype", "=", "x", ".", "dtype", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.Mixup._mix_batch": [[196, 208], ["mixup.Mixup._params_per_batch", "mixup.cutmix_bbox_and_lam", "x.flip().mul_", "x.mul_().add_", "x.flip", "x.flip", "x.mul_"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.Mixup._params_per_batch", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.cutmix_bbox_and_lam"], ["", "def", "_mix_batch", "(", "self", ",", "x", ")", ":", "\n", "        ", "lam", ",", "use_cutmix", "=", "self", ".", "_params_per_batch", "(", ")", "\n", "if", "lam", "==", "1.", ":", "\n", "            ", "return", "1.", "\n", "", "if", "use_cutmix", ":", "\n", "            ", "(", "yl", ",", "yh", ",", "xl", ",", "xh", ")", ",", "lam", "=", "cutmix_bbox_and_lam", "(", "\n", "x", ".", "shape", ",", "lam", ",", "ratio_minmax", "=", "self", ".", "cutmix_minmax", ",", "correct_lam", "=", "self", ".", "correct_lam", ")", "\n", "x", "[", ":", ",", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "=", "x", ".", "flip", "(", "0", ")", "[", ":", ",", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "\n", "", "else", ":", "\n", "            ", "x_flipped", "=", "x", ".", "flip", "(", "0", ")", ".", "mul_", "(", "1.", "-", "lam", ")", "\n", "x", ".", "mul_", "(", "lam", ")", ".", "add_", "(", "x_flipped", ")", "\n", "", "return", "lam", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.Mixup.__call__": [[209, 219], ["mixup.mixup_target", "mixup.Mixup._mix_elem", "len", "mixup.Mixup._mix_pair", "mixup.Mixup._mix_batch"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.mixup_target", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.Mixup._mix_elem", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.Mixup._mix_pair", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.Mixup._mix_batch"], ["", "def", "__call__", "(", "self", ",", "x", ",", "target", ")", ":", "\n", "        ", "assert", "len", "(", "x", ")", "%", "2", "==", "0", ",", "'Batch size should be even when using this'", "\n", "if", "self", ".", "mode", "==", "'elem'", ":", "\n", "            ", "lam", "=", "self", ".", "_mix_elem", "(", "x", ")", "\n", "", "elif", "self", ".", "mode", "==", "'pair'", ":", "\n", "            ", "lam", "=", "self", ".", "_mix_pair", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "lam", "=", "self", ".", "_mix_batch", "(", "x", ")", "\n", "", "target", "=", "mixup_target", "(", "target", ",", "self", ".", "num_classes", ",", "lam", ",", "self", ".", "label_smoothing", ",", "x", ".", "device", ")", "\n", "return", "x", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.FastCollateMixup._mix_elem_collate": [[227, 251], ["len", "mixup.FastCollateMixup._params_per_elem", "range", "torch.tensor().unsqueeze", "len", "torch.from_numpy", "numpy.concatenate", "mixed.copy.copy.astype", "torch.tensor", "mixup.cutmix_bbox_and_lam", "numpy.rint", "numpy.ones", "mixed.copy.copy.copy", "mixed.copy.copy.astype", "[].astype"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.Mixup._params_per_elem", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.cutmix_bbox_and_lam"], ["def", "_mix_elem_collate", "(", "self", ",", "output", ",", "batch", ",", "half", "=", "False", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "batch", ")", "\n", "num_elem", "=", "batch_size", "//", "2", "if", "half", "else", "batch_size", "\n", "assert", "len", "(", "output", ")", "==", "num_elem", "\n", "lam_batch", ",", "use_cutmix", "=", "self", ".", "_params_per_elem", "(", "num_elem", ")", "\n", "for", "i", "in", "range", "(", "num_elem", ")", ":", "\n", "            ", "j", "=", "batch_size", "-", "i", "-", "1", "\n", "lam", "=", "lam_batch", "[", "i", "]", "\n", "mixed", "=", "batch", "[", "i", "]", "[", "0", "]", "\n", "if", "lam", "!=", "1.", ":", "\n", "                ", "if", "use_cutmix", "[", "i", "]", ":", "\n", "                    ", "if", "not", "half", ":", "\n", "                        ", "mixed", "=", "mixed", ".", "copy", "(", ")", "\n", "", "(", "yl", ",", "yh", ",", "xl", ",", "xh", ")", ",", "lam", "=", "cutmix_bbox_and_lam", "(", "\n", "output", ".", "shape", ",", "lam", ",", "ratio_minmax", "=", "self", ".", "cutmix_minmax", ",", "correct_lam", "=", "self", ".", "correct_lam", ")", "\n", "mixed", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "=", "batch", "[", "j", "]", "[", "0", "]", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "\n", "lam_batch", "[", "i", "]", "=", "lam", "\n", "", "else", ":", "\n", "                    ", "mixed", "=", "mixed", ".", "astype", "(", "np", ".", "float32", ")", "*", "lam", "+", "batch", "[", "j", "]", "[", "0", "]", ".", "astype", "(", "np", ".", "float32", ")", "*", "(", "1", "-", "lam", ")", "\n", "np", ".", "rint", "(", "mixed", ",", "out", "=", "mixed", ")", "\n", "", "", "output", "[", "i", "]", "+=", "torch", ".", "from_numpy", "(", "mixed", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "", "if", "half", ":", "\n", "            ", "lam_batch", "=", "np", ".", "concatenate", "(", "(", "lam_batch", ",", "np", ".", "ones", "(", "num_elem", ")", ")", ")", "\n", "", "return", "torch", ".", "tensor", "(", "lam_batch", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.FastCollateMixup._mix_pair_collate": [[252, 279], ["len", "mixup.FastCollateMixup._params_per_elem", "range", "numpy.concatenate", "torch.tensor().unsqueeze", "torch.from_numpy", "torch.from_numpy", "mixed_i.astype", "mixed_j.astype", "torch.tensor", "mixup.cutmix_bbox_and_lam", "mixed_i[].copy", "numpy.rint", "numpy.rint", "mixed_i.astype", "mixed_j.astype", "mixed_j.astype", "mixed_i.astype"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.Mixup._params_per_elem", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.cutmix_bbox_and_lam"], ["", "def", "_mix_pair_collate", "(", "self", ",", "output", ",", "batch", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "batch", ")", "\n", "lam_batch", ",", "use_cutmix", "=", "self", ".", "_params_per_elem", "(", "batch_size", "//", "2", ")", "\n", "for", "i", "in", "range", "(", "batch_size", "//", "2", ")", ":", "\n", "            ", "j", "=", "batch_size", "-", "i", "-", "1", "\n", "lam", "=", "lam_batch", "[", "i", "]", "\n", "mixed_i", "=", "batch", "[", "i", "]", "[", "0", "]", "\n", "mixed_j", "=", "batch", "[", "j", "]", "[", "0", "]", "\n", "assert", "0", "<=", "lam", "<=", "1.0", "\n", "if", "lam", "<", "1.", ":", "\n", "                ", "if", "use_cutmix", "[", "i", "]", ":", "\n", "                    ", "(", "yl", ",", "yh", ",", "xl", ",", "xh", ")", ",", "lam", "=", "cutmix_bbox_and_lam", "(", "\n", "output", ".", "shape", ",", "lam", ",", "ratio_minmax", "=", "self", ".", "cutmix_minmax", ",", "correct_lam", "=", "self", ".", "correct_lam", ")", "\n", "patch_i", "=", "mixed_i", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", ".", "copy", "(", ")", "\n", "mixed_i", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "=", "mixed_j", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "\n", "mixed_j", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "=", "patch_i", "\n", "lam_batch", "[", "i", "]", "=", "lam", "\n", "", "else", ":", "\n", "                    ", "mixed_temp", "=", "mixed_i", ".", "astype", "(", "np", ".", "float32", ")", "*", "lam", "+", "mixed_j", ".", "astype", "(", "np", ".", "float32", ")", "*", "(", "1", "-", "lam", ")", "\n", "mixed_j", "=", "mixed_j", ".", "astype", "(", "np", ".", "float32", ")", "*", "lam", "+", "mixed_i", ".", "astype", "(", "np", ".", "float32", ")", "*", "(", "1", "-", "lam", ")", "\n", "mixed_i", "=", "mixed_temp", "\n", "np", ".", "rint", "(", "mixed_j", ",", "out", "=", "mixed_j", ")", "\n", "np", ".", "rint", "(", "mixed_i", ",", "out", "=", "mixed_i", ")", "\n", "", "", "output", "[", "i", "]", "+=", "torch", ".", "from_numpy", "(", "mixed_i", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "output", "[", "j", "]", "+=", "torch", ".", "from_numpy", "(", "mixed_j", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "", "lam_batch", "=", "np", ".", "concatenate", "(", "(", "lam_batch", ",", "lam_batch", "[", ":", ":", "-", "1", "]", ")", ")", "\n", "return", "torch", ".", "tensor", "(", "lam_batch", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.FastCollateMixup._mix_batch_collate": [[280, 298], ["len", "mixup.FastCollateMixup._params_per_batch", "range", "mixup.cutmix_bbox_and_lam", "torch.from_numpy", "mixed.copy.copy.astype", "mixed.copy.copy.copy", "numpy.rint", "mixed.copy.copy.astype", "[].astype"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.Mixup._params_per_batch", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.cutmix_bbox_and_lam"], ["", "def", "_mix_batch_collate", "(", "self", ",", "output", ",", "batch", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "batch", ")", "\n", "lam", ",", "use_cutmix", "=", "self", ".", "_params_per_batch", "(", ")", "\n", "if", "use_cutmix", ":", "\n", "            ", "(", "yl", ",", "yh", ",", "xl", ",", "xh", ")", ",", "lam", "=", "cutmix_bbox_and_lam", "(", "\n", "output", ".", "shape", ",", "lam", ",", "ratio_minmax", "=", "self", ".", "cutmix_minmax", ",", "correct_lam", "=", "self", ".", "correct_lam", ")", "\n", "", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "j", "=", "batch_size", "-", "i", "-", "1", "\n", "mixed", "=", "batch", "[", "i", "]", "[", "0", "]", "\n", "if", "lam", "!=", "1.", ":", "\n", "                ", "if", "use_cutmix", ":", "\n", "                    ", "mixed", "=", "mixed", ".", "copy", "(", ")", "# don't want to modify the original while iterating", "\n", "mixed", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "=", "batch", "[", "j", "]", "[", "0", "]", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "\n", "", "else", ":", "\n", "                    ", "mixed", "=", "mixed", ".", "astype", "(", "np", ".", "float32", ")", "*", "lam", "+", "batch", "[", "j", "]", "[", "0", "]", ".", "astype", "(", "np", ".", "float32", ")", "*", "(", "1", "-", "lam", ")", "\n", "np", ".", "rint", "(", "mixed", ",", "out", "=", "mixed", ")", "\n", "", "", "output", "[", "i", "]", "+=", "torch", ".", "from_numpy", "(", "mixed", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "", "return", "lam", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.FastCollateMixup.__call__": [[299, 316], ["len", "torch.zeros", "torch.tensor", "mixup.mixup_target", "mixup.FastCollateMixup._mix_elem_collate", "mixup.FastCollateMixup._mix_pair_collate", "mixup.FastCollateMixup._mix_batch_collate"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.mixup_target", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.FastCollateMixup._mix_elem_collate", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.FastCollateMixup._mix_pair_collate", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.FastCollateMixup._mix_batch_collate"], ["", "def", "__call__", "(", "self", ",", "batch", ",", "_", "=", "None", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "batch", ")", "\n", "assert", "batch_size", "%", "2", "==", "0", ",", "'Batch size should be even when using this'", "\n", "half", "=", "'half'", "in", "self", ".", "mode", "\n", "if", "half", ":", "\n", "            ", "batch_size", "//=", "2", "\n", "", "output", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "*", "batch", "[", "0", "]", "[", "0", "]", ".", "shape", ")", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "if", "self", ".", "mode", "==", "'elem'", "or", "self", ".", "mode", "==", "'half'", ":", "\n", "            ", "lam", "=", "self", ".", "_mix_elem_collate", "(", "output", ",", "batch", ",", "half", "=", "half", ")", "\n", "", "elif", "self", ".", "mode", "==", "'pair'", ":", "\n", "            ", "lam", "=", "self", ".", "_mix_pair_collate", "(", "output", ",", "batch", ")", "\n", "", "else", ":", "\n", "            ", "lam", "=", "self", ".", "_mix_batch_collate", "(", "output", ",", "batch", ")", "\n", "", "target", "=", "torch", ".", "tensor", "(", "[", "b", "[", "1", "]", "for", "b", "in", "batch", "]", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "target", "=", "mixup_target", "(", "target", ",", "self", ".", "num_classes", ",", "lam", ",", "self", ".", "label_smoothing", ",", "device", "=", "'cpu'", ")", "\n", "target", "=", "target", "[", ":", "batch_size", "]", "\n", "return", "output", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.one_hot": [[17, 20], ["x.long().view.long().view", "torch.full().scatter_", "x.long().view.long", "torch.full", "x.long().view.size"], "function", ["None"], ["def", "one_hot", "(", "x", ",", "num_classes", ",", "on_value", "=", "1.", ",", "off_value", "=", "0.", ",", "device", "=", "'cuda'", ")", ":", "\n", "    ", "x", "=", "x", ".", "long", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "return", "torch", ".", "full", "(", "(", "x", ".", "size", "(", ")", "[", "0", "]", ",", "num_classes", ")", ",", "off_value", ",", "device", "=", "device", ")", ".", "scatter_", "(", "1", ",", "x", ",", "on_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.mixup_target": [[22, 28], ["mixup.one_hot", "mixup.one_hot", "target.flip"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.one_hot", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.one_hot"], ["", "def", "mixup_target", "(", "target", ",", "num_classes", ",", "lam", "=", "1.", ",", "smoothing", "=", "0.0", ",", "device", "=", "'cuda'", ")", ":", "\n", "    ", "off_value", "=", "smoothing", "/", "num_classes", "\n", "on_value", "=", "1.", "-", "smoothing", "+", "off_value", "\n", "y1", "=", "one_hot", "(", "target", ",", "num_classes", ",", "on_value", "=", "on_value", ",", "off_value", "=", "off_value", ",", "device", "=", "device", ")", "\n", "y2", "=", "one_hot", "(", "target", ".", "flip", "(", "0", ")", ",", "num_classes", ",", "on_value", "=", "on_value", ",", "off_value", "=", "off_value", ",", "device", "=", "device", ")", "\n", "return", "y1", "*", "lam", "+", "y2", "*", "(", "1.", "-", "lam", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.rand_bbox": [[30, 52], ["numpy.sqrt", "numpy.random.randint", "numpy.random.randint", "numpy.clip", "numpy.clip", "numpy.clip", "numpy.clip", "int", "int", "int", "int"], "function", ["None"], ["", "def", "rand_bbox", "(", "img_shape", ",", "lam", ",", "margin", "=", "0.", ",", "count", "=", "None", ")", ":", "\n", "    ", "\"\"\" Standard CutMix bounding-box\n    Generates a random square bbox based on lambda value. This impl includes\n    support for enforcing a border margin as percent of bbox dimensions.\n\n    Args:\n        img_shape (tuple): Image shape as tuple\n        lam (float): Cutmix lambda value\n        margin (float): Percentage of bbox dimension to enforce as margin (reduce amount of box outside image)\n        count (int): Number of bbox to generate\n    \"\"\"", "\n", "ratio", "=", "np", ".", "sqrt", "(", "1", "-", "lam", ")", "\n", "img_h", ",", "img_w", "=", "img_shape", "[", "-", "2", ":", "]", "\n", "cut_h", ",", "cut_w", "=", "int", "(", "img_h", "*", "ratio", ")", ",", "int", "(", "img_w", "*", "ratio", ")", "\n", "margin_y", ",", "margin_x", "=", "int", "(", "margin", "*", "cut_h", ")", ",", "int", "(", "margin", "*", "cut_w", ")", "\n", "cy", "=", "np", ".", "random", ".", "randint", "(", "0", "+", "margin_y", ",", "img_h", "-", "margin_y", ",", "size", "=", "count", ")", "\n", "cx", "=", "np", ".", "random", ".", "randint", "(", "0", "+", "margin_x", ",", "img_w", "-", "margin_x", ",", "size", "=", "count", ")", "\n", "yl", "=", "np", ".", "clip", "(", "cy", "-", "cut_h", "//", "2", ",", "0", ",", "img_h", ")", "\n", "yh", "=", "np", ".", "clip", "(", "cy", "+", "cut_h", "//", "2", ",", "0", ",", "img_h", ")", "\n", "xl", "=", "np", ".", "clip", "(", "cx", "-", "cut_w", "//", "2", ",", "0", ",", "img_w", ")", "\n", "xh", "=", "np", ".", "clip", "(", "cx", "+", "cut_w", "//", "2", ",", "0", ",", "img_w", ")", "\n", "return", "yl", ",", "yh", ",", "xl", ",", "xh", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.rand_bbox_minmax": [[54, 75], ["numpy.random.randint", "numpy.random.randint", "numpy.random.randint", "numpy.random.randint", "len", "int", "int", "int", "int"], "function", ["None"], ["", "def", "rand_bbox_minmax", "(", "img_shape", ",", "minmax", ",", "count", "=", "None", ")", ":", "\n", "    ", "\"\"\" Min-Max CutMix bounding-box\n    Inspired by Darknet cutmix impl, generates a random rectangular bbox\n    based on min/max percent values applied to each dimension of the input image.\n\n    Typical defaults for minmax are usually in the  .2-.3 for min and .8-.9 range for max.\n\n    Args:\n        img_shape (tuple): Image shape as tuple\n        minmax (tuple or list): Min and max bbox ratios (as percent of image size)\n        count (int): Number of bbox to generate\n    \"\"\"", "\n", "assert", "len", "(", "minmax", ")", "==", "2", "\n", "img_h", ",", "img_w", "=", "img_shape", "[", "-", "2", ":", "]", "\n", "cut_h", "=", "np", ".", "random", ".", "randint", "(", "int", "(", "img_h", "*", "minmax", "[", "0", "]", ")", ",", "int", "(", "img_h", "*", "minmax", "[", "1", "]", ")", ",", "size", "=", "count", ")", "\n", "cut_w", "=", "np", ".", "random", ".", "randint", "(", "int", "(", "img_w", "*", "minmax", "[", "0", "]", ")", ",", "int", "(", "img_w", "*", "minmax", "[", "1", "]", ")", ",", "size", "=", "count", ")", "\n", "yl", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "img_h", "-", "cut_h", ",", "size", "=", "count", ")", "\n", "xl", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "img_w", "-", "cut_w", ",", "size", "=", "count", ")", "\n", "yu", "=", "yl", "+", "cut_h", "\n", "xu", "=", "xl", "+", "cut_w", "\n", "return", "yl", ",", "yu", ",", "xl", ",", "xu", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.cutmix_bbox_and_lam": [[77, 88], ["mixup.rand_bbox_minmax", "mixup.rand_bbox", "float"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.rand_bbox_minmax", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.mixup.rand_bbox"], ["", "def", "cutmix_bbox_and_lam", "(", "img_shape", ",", "lam", ",", "ratio_minmax", "=", "None", ",", "correct_lam", "=", "True", ",", "count", "=", "None", ")", ":", "\n", "    ", "\"\"\" Generate bbox and apply lambda correction.\n    \"\"\"", "\n", "if", "ratio_minmax", "is", "not", "None", ":", "\n", "        ", "yl", ",", "yu", ",", "xl", ",", "xu", "=", "rand_bbox_minmax", "(", "img_shape", ",", "ratio_minmax", ",", "count", "=", "count", ")", "\n", "", "else", ":", "\n", "        ", "yl", ",", "yu", ",", "xl", ",", "xu", "=", "rand_bbox", "(", "img_shape", ",", "lam", ",", "count", "=", "count", ")", "\n", "", "if", "correct_lam", "or", "ratio_minmax", "is", "not", "None", ":", "\n", "        ", "bbox_area", "=", "(", "yu", "-", "yl", ")", "*", "(", "xu", "-", "xl", ")", "\n", "lam", "=", "1.", "-", "bbox_area", "/", "float", "(", "img_shape", "[", "-", "2", "]", "*", "img_shape", "[", "-", "1", "]", ")", "\n", "", "return", "(", "yl", ",", "yu", ",", "xl", ",", "xu", ")", ",", "lam", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing.TfPreprocessTransform.__init__": [[206, 213], ["tf_preprocessing.TfPreprocessTransform._build_tf_graph", "isinstance"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing.TfPreprocessTransform._build_tf_graph"], ["    ", "def", "__init__", "(", "self", ",", "is_training", "=", "False", ",", "size", "=", "224", ",", "interpolation", "=", "'bicubic'", ")", ":", "\n", "        ", "self", ".", "is_training", "=", "is_training", "\n", "self", ".", "size", "=", "size", "[", "0", "]", "if", "isinstance", "(", "size", ",", "tuple", ")", "else", "size", "\n", "self", ".", "interpolation", "=", "interpolation", "\n", "self", ".", "_image_bytes", "=", "None", "\n", "self", ".", "process_image", "=", "self", ".", "_build_tf_graph", "(", ")", "\n", "self", ".", "sess", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing.TfPreprocessTransform._build_tf_graph": [[214, 223], ["tensorflow.device", "tensorflow.placeholder", "tf_preprocessing.preprocess_image"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing.preprocess_image"], ["", "def", "_build_tf_graph", "(", "self", ")", ":", "\n", "        ", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "            ", "self", ".", "_image_bytes", "=", "tf", ".", "placeholder", "(", "\n", "shape", "=", "[", "]", ",", "\n", "dtype", "=", "tf", ".", "string", ",", "\n", ")", "\n", "img", "=", "preprocess_image", "(", "\n", "self", ".", "_image_bytes", ",", "self", ".", "is_training", ",", "False", ",", "self", ".", "size", ",", "self", ".", "interpolation", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing.TfPreprocessTransform.__call__": [[224, 233], ["tf_preprocessing.TfPreprocessTransform.sess.run", "numpy.expand_dims.round().clip().astype", "numpy.rollaxis", "tensorflow.Session", "numpy.expand_dims", "numpy.expand_dims.round().clip", "numpy.expand_dims.round"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.None.benchmark.ProfileRunner.run"], ["", "def", "__call__", "(", "self", ",", "image_bytes", ")", ":", "\n", "        ", "if", "self", ".", "sess", "is", "None", ":", "\n", "            ", "self", ".", "sess", "=", "tf", ".", "Session", "(", ")", "\n", "", "img", "=", "self", ".", "sess", ".", "run", "(", "self", ".", "process_image", ",", "feed_dict", "=", "{", "self", ".", "_image_bytes", ":", "image_bytes", "}", ")", "\n", "img", "=", "img", ".", "round", "(", ")", ".", "clip", "(", "0", ",", "255", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "if", "img", ".", "ndim", "<", "3", ":", "\n", "            ", "img", "=", "np", ".", "expand_dims", "(", "img", ",", "axis", "=", "-", "1", ")", "\n", "", "img", "=", "np", ".", "rollaxis", "(", "img", ",", "2", ")", "# HWC to CHW", "\n", "return", "img", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing.distorted_bounding_box_crop": [[32, 82], ["tensorflow.name_scope", "tensorflow.image.extract_jpeg_shape", "tensorflow.image.sample_distorted_bounding_box", "tensorflow.unstack", "tensorflow.unstack", "tensorflow.stack", "tensorflow.image.decode_and_crop_jpeg"], "function", ["None"], ["def", "distorted_bounding_box_crop", "(", "image_bytes", ",", "\n", "bbox", ",", "\n", "min_object_covered", "=", "0.1", ",", "\n", "aspect_ratio_range", "=", "(", "0.75", ",", "1.33", ")", ",", "\n", "area_range", "=", "(", "0.05", ",", "1.0", ")", ",", "\n", "max_attempts", "=", "100", ",", "\n", "scope", "=", "None", ")", ":", "\n", "    ", "\"\"\"Generates cropped_image using one of the bboxes randomly distorted.\n\n    See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n    Args:\n      image_bytes: `Tensor` of binary image data.\n      bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n          where each coordinate is [0, 1) and the coordinates are arranged\n          as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n          image.\n      min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n          area of the image must contain at least this fraction of any bounding\n          box supplied.\n      aspect_ratio_range: An optional list of `float`s. The cropped area of the\n          image must have an aspect ratio = width / height within this range.\n      area_range: An optional list of `float`s. The cropped area of the image\n          must contain a fraction of the supplied image within in this range.\n      max_attempts: An optional `int`. Number of attempts at generating a cropped\n          region of the image of the specified constraints. After `max_attempts`\n          failures, return the entire image.\n      scope: Optional `str` for name scope.\n    Returns:\n      cropped image `Tensor`\n    \"\"\"", "\n", "with", "tf", ".", "name_scope", "(", "scope", ",", "'distorted_bounding_box_crop'", ",", "[", "image_bytes", ",", "bbox", "]", ")", ":", "\n", "        ", "shape", "=", "tf", ".", "image", ".", "extract_jpeg_shape", "(", "image_bytes", ")", "\n", "sample_distorted_bounding_box", "=", "tf", ".", "image", ".", "sample_distorted_bounding_box", "(", "\n", "shape", ",", "\n", "bounding_boxes", "=", "bbox", ",", "\n", "min_object_covered", "=", "min_object_covered", ",", "\n", "aspect_ratio_range", "=", "aspect_ratio_range", ",", "\n", "area_range", "=", "area_range", ",", "\n", "max_attempts", "=", "max_attempts", ",", "\n", "use_image_if_no_bounding_boxes", "=", "True", ")", "\n", "bbox_begin", ",", "bbox_size", ",", "_", "=", "sample_distorted_bounding_box", "\n", "\n", "# Crop the image to the specified bounding box.", "\n", "offset_y", ",", "offset_x", ",", "_", "=", "tf", ".", "unstack", "(", "bbox_begin", ")", "\n", "target_height", ",", "target_width", ",", "_", "=", "tf", ".", "unstack", "(", "bbox_size", ")", "\n", "crop_window", "=", "tf", ".", "stack", "(", "[", "offset_y", ",", "offset_x", ",", "target_height", ",", "target_width", "]", ")", "\n", "image", "=", "tf", ".", "image", ".", "decode_and_crop_jpeg", "(", "image_bytes", ",", "crop_window", ",", "channels", "=", "3", ")", "\n", "\n", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing._at_least_x_are_equal": [[84, 89], ["tensorflow.equal", "tensorflow.cast", "tensorflow.greater_equal", "tensorflow.reduce_sum"], "function", ["None"], ["", "", "def", "_at_least_x_are_equal", "(", "a", ",", "b", ",", "x", ")", ":", "\n", "    ", "\"\"\"At least `x` of `a` and `b` `Tensors` are equal.\"\"\"", "\n", "match", "=", "tf", ".", "equal", "(", "a", ",", "b", ")", "\n", "match", "=", "tf", ".", "cast", "(", "match", ",", "tf", ".", "int32", ")", "\n", "return", "tf", ".", "greater_equal", "(", "tf", ".", "reduce_sum", "(", "match", ")", ",", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing._decode_and_random_crop": [[91, 111], ["tensorflow.constant", "tf_preprocessing.distorted_bounding_box_crop", "tensorflow.image.extract_jpeg_shape", "tf_preprocessing._at_least_x_are_equal", "tensorflow.cond", "tensorflow.shape", "tf_preprocessing._decode_and_center_crop", "tensorflow.image.resize"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing.distorted_bounding_box_crop", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing._at_least_x_are_equal", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing._decode_and_center_crop"], ["", "def", "_decode_and_random_crop", "(", "image_bytes", ",", "image_size", ",", "resize_method", ")", ":", "\n", "    ", "\"\"\"Make a random crop of image_size.\"\"\"", "\n", "bbox", "=", "tf", ".", "constant", "(", "[", "0.0", ",", "0.0", ",", "1.0", ",", "1.0", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "shape", "=", "[", "1", ",", "1", ",", "4", "]", ")", "\n", "image", "=", "distorted_bounding_box_crop", "(", "\n", "image_bytes", ",", "\n", "bbox", ",", "\n", "min_object_covered", "=", "0.1", ",", "\n", "aspect_ratio_range", "=", "(", "3.", "/", "4", ",", "4.", "/", "3.", ")", ",", "\n", "area_range", "=", "(", "0.08", ",", "1.0", ")", ",", "\n", "max_attempts", "=", "10", ",", "\n", "scope", "=", "None", ")", "\n", "original_shape", "=", "tf", ".", "image", ".", "extract_jpeg_shape", "(", "image_bytes", ")", "\n", "bad", "=", "_at_least_x_are_equal", "(", "original_shape", ",", "tf", ".", "shape", "(", "image", ")", ",", "3", ")", "\n", "\n", "image", "=", "tf", ".", "cond", "(", "\n", "bad", ",", "\n", "lambda", ":", "_decode_and_center_crop", "(", "image_bytes", ",", "image_size", ")", ",", "\n", "lambda", ":", "tf", ".", "image", ".", "resize", "(", "[", "image", "]", ",", "[", "image_size", ",", "image_size", "]", ",", "resize_method", ")", "[", "0", "]", ")", "\n", "\n", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing._decode_and_center_crop": [[113, 132], ["tensorflow.image.extract_jpeg_shape", "tensorflow.cast", "tensorflow.stack", "tensorflow.image.decode_and_crop_jpeg", "tensorflow.image.resize", "tensorflow.cast", "tensorflow.minimum"], "function", ["None"], ["", "def", "_decode_and_center_crop", "(", "image_bytes", ",", "image_size", ",", "resize_method", ")", ":", "\n", "    ", "\"\"\"Crops to center of image with padding then scales image_size.\"\"\"", "\n", "shape", "=", "tf", ".", "image", ".", "extract_jpeg_shape", "(", "image_bytes", ")", "\n", "image_height", "=", "shape", "[", "0", "]", "\n", "image_width", "=", "shape", "[", "1", "]", "\n", "\n", "padded_center_crop_size", "=", "tf", ".", "cast", "(", "\n", "(", "(", "image_size", "/", "(", "image_size", "+", "CROP_PADDING", ")", ")", "*", "\n", "tf", ".", "cast", "(", "tf", ".", "minimum", "(", "image_height", ",", "image_width", ")", ",", "tf", ".", "float32", ")", ")", ",", "\n", "tf", ".", "int32", ")", "\n", "\n", "offset_height", "=", "(", "(", "image_height", "-", "padded_center_crop_size", ")", "+", "1", ")", "//", "2", "\n", "offset_width", "=", "(", "(", "image_width", "-", "padded_center_crop_size", ")", "+", "1", ")", "//", "2", "\n", "crop_window", "=", "tf", ".", "stack", "(", "[", "offset_height", ",", "offset_width", ",", "\n", "padded_center_crop_size", ",", "padded_center_crop_size", "]", ")", "\n", "image", "=", "tf", ".", "image", ".", "decode_and_crop_jpeg", "(", "image_bytes", ",", "crop_window", ",", "channels", "=", "3", ")", "\n", "image", "=", "tf", ".", "image", ".", "resize", "(", "[", "image", "]", ",", "[", "image_size", ",", "image_size", "]", ",", "resize_method", ")", "[", "0", "]", "\n", "\n", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing._flip": [[134, 138], ["tensorflow.image.random_flip_left_right"], "function", ["None"], ["", "def", "_flip", "(", "image", ")", ":", "\n", "    ", "\"\"\"Random horizontal image flip.\"\"\"", "\n", "image", "=", "tf", ".", "image", ".", "random_flip_left_right", "(", "image", ")", "\n", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing.preprocess_for_train": [[140, 159], ["tf_preprocessing._decode_and_random_crop", "tf_preprocessing._flip", "tensorflow.reshape", "tensorflow.image.convert_image_dtype"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing._decode_and_random_crop", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing._flip"], ["", "def", "preprocess_for_train", "(", "image_bytes", ",", "use_bfloat16", ",", "image_size", "=", "IMAGE_SIZE", ",", "interpolation", "=", "'bicubic'", ")", ":", "\n", "    ", "\"\"\"Preprocesses the given image for evaluation.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n\n    Returns:\n      A preprocessed image `Tensor`.\n    \"\"\"", "\n", "resize_method", "=", "tf", ".", "image", ".", "ResizeMethod", ".", "BICUBIC", "if", "interpolation", "==", "'bicubic'", "else", "tf", ".", "image", ".", "ResizeMethod", ".", "BILINEAR", "\n", "image", "=", "_decode_and_random_crop", "(", "image_bytes", ",", "image_size", ",", "resize_method", ")", "\n", "image", "=", "_flip", "(", "image", ")", "\n", "image", "=", "tf", ".", "reshape", "(", "image", ",", "[", "image_size", ",", "image_size", ",", "3", "]", ")", "\n", "image", "=", "tf", ".", "image", ".", "convert_image_dtype", "(", "\n", "image", ",", "dtype", "=", "tf", ".", "bfloat16", "if", "use_bfloat16", "else", "tf", ".", "float32", ")", "\n", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing.preprocess_for_eval": [[161, 179], ["tf_preprocessing._decode_and_center_crop", "tensorflow.reshape", "tensorflow.image.convert_image_dtype"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing._decode_and_center_crop"], ["", "def", "preprocess_for_eval", "(", "image_bytes", ",", "use_bfloat16", ",", "image_size", "=", "IMAGE_SIZE", ",", "interpolation", "=", "'bicubic'", ")", ":", "\n", "    ", "\"\"\"Preprocesses the given image for evaluation.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n\n    Returns:\n      A preprocessed image `Tensor`.\n    \"\"\"", "\n", "resize_method", "=", "tf", ".", "image", ".", "ResizeMethod", ".", "BICUBIC", "if", "interpolation", "==", "'bicubic'", "else", "tf", ".", "image", ".", "ResizeMethod", ".", "BILINEAR", "\n", "image", "=", "_decode_and_center_crop", "(", "image_bytes", ",", "image_size", ",", "resize_method", ")", "\n", "image", "=", "tf", ".", "reshape", "(", "image", ",", "[", "image_size", ",", "image_size", ",", "3", "]", ")", "\n", "image", "=", "tf", ".", "image", ".", "convert_image_dtype", "(", "\n", "image", ",", "dtype", "=", "tf", ".", "bfloat16", "if", "use_bfloat16", "else", "tf", ".", "float32", ")", "\n", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing.preprocess_image": [[181, 202], ["tf_preprocessing.preprocess_for_train", "tf_preprocessing.preprocess_for_eval"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing.preprocess_for_train", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.tf_preprocessing.preprocess_for_eval"], ["", "def", "preprocess_image", "(", "image_bytes", ",", "\n", "is_training", "=", "False", ",", "\n", "use_bfloat16", "=", "False", ",", "\n", "image_size", "=", "IMAGE_SIZE", ",", "\n", "interpolation", "=", "'bicubic'", ")", ":", "\n", "    ", "\"\"\"Preprocesses the given image.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      is_training: `bool` for whether the preprocessing is for training.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n\n    Returns:\n      A preprocessed image `Tensor` with value range of [0, 255].\n    \"\"\"", "\n", "if", "is_training", ":", "\n", "        ", "return", "preprocess_for_train", "(", "image_bytes", ",", "use_bfloat16", ",", "image_size", ",", "interpolation", ")", "\n", "", "else", ":", "\n", "        ", "return", "preprocess_for_eval", "(", "image_bytes", ",", "use_bfloat16", ",", "image_size", ",", "interpolation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset_factory._search_split": [[33, 51], ["os.path.join", "os.path.exists", "split.split", "dataset_factory._search_split._try"], "function", ["None"], ["def", "_search_split", "(", "root", ",", "split", ")", ":", "\n", "# look for sub-folder with name of split in root and use that if it exists", "\n", "    ", "split_name", "=", "split", ".", "split", "(", "'['", ")", "[", "0", "]", "\n", "try_root", "=", "os", ".", "path", ".", "join", "(", "root", ",", "split_name", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "try_root", ")", ":", "\n", "        ", "return", "try_root", "\n", "\n", "", "def", "_try", "(", "syn", ")", ":", "\n", "        ", "for", "s", "in", "syn", ":", "\n", "            ", "try_root", "=", "os", ".", "path", ".", "join", "(", "root", ",", "s", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "try_root", ")", ":", "\n", "                ", "return", "try_root", "\n", "", "", "return", "root", "\n", "", "if", "split_name", "in", "_TRAIN_SYNONYM", ":", "\n", "        ", "root", "=", "_try", "(", "_TRAIN_SYNONYM", ")", "\n", "", "elif", "split_name", "in", "_EVAL_SYNONYM", ":", "\n", "        ", "root", "=", "_try", "(", "_EVAL_SYNONYM", ")", "\n", "", "return", "root", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset_factory.create_dataset": [[53, 144], ["name.lower.lower", "name.lower.startswith", "dict", "name.lower.startswith", "name.lower.split", "ds_class", "dataset.IterableImageDataset", "dataset.ImageDataset", "split.split", "INaturalist", "os.path.isdir", "dataset_factory._search_split", "len", "split_split[].split", "Places365", "len", "torchvision.datasets.ImageNet", "torchvision.datasets.ImageFolder", "os.path.isdir", "dataset_factory._search_split"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset_factory._search_split", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.dataset_factory._search_split"], ["", "def", "create_dataset", "(", "\n", "name", ",", "\n", "root", ",", "\n", "split", "=", "'validation'", ",", "\n", "search_split", "=", "True", ",", "\n", "class_map", "=", "None", ",", "\n", "load_bytes", "=", "False", ",", "\n", "is_training", "=", "False", ",", "\n", "download", "=", "False", ",", "\n", "batch_size", "=", "None", ",", "\n", "repeats", "=", "0", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "    ", "\"\"\" Dataset factory method\n\n    In parenthesis after each arg are the type of dataset supported for each arg, one of:\n      * folder - default, timm folder (or tar) based ImageDataset\n      * torch - torchvision based datasets\n      * TFDS - Tensorflow-datasets wrapper in IterabeDataset interface via IterableImageDataset\n      * all - any of the above\n\n    Args:\n        name: dataset name, empty is okay for folder based datasets\n        root: root folder of dataset (all)\n        split: dataset split (all)\n        search_split: search for split specific child fold from root so one can specify\n            `imagenet/` instead of `/imagenet/val`, etc on cmd line / config. (folder, torch/folder)\n        class_map: specify class -> index mapping via text file or dict (folder)\n        load_bytes: load data, return images as undecoded bytes (folder)\n        download: download dataset if not present and supported (TFDS, torch)\n        is_training: create dataset in train mode, this is different from the split.\n            For Iterable / TDFS it enables shuffle, ignored for other datasets. (TFDS)\n        batch_size: batch size hint for (TFDS)\n        repeats: dataset repeats per iteration i.e. epoch (TFDS)\n        **kwargs: other args to pass to dataset\n\n    Returns:\n        Dataset object\n    \"\"\"", "\n", "name", "=", "name", ".", "lower", "(", ")", "\n", "if", "name", ".", "startswith", "(", "'torch/'", ")", ":", "\n", "        ", "name", "=", "name", ".", "split", "(", "'/'", ",", "2", ")", "[", "-", "1", "]", "\n", "torch_kwargs", "=", "dict", "(", "root", "=", "root", ",", "download", "=", "download", ",", "**", "kwargs", ")", "\n", "if", "name", "in", "_TORCH_BASIC_DS", ":", "\n", "            ", "ds_class", "=", "_TORCH_BASIC_DS", "[", "name", "]", "\n", "use_train", "=", "split", "in", "_TRAIN_SYNONYM", "\n", "ds", "=", "ds_class", "(", "train", "=", "use_train", ",", "**", "torch_kwargs", ")", "\n", "", "elif", "name", "==", "'inaturalist'", "or", "name", "==", "'inat'", ":", "\n", "            ", "assert", "has_inaturalist", ",", "'Please update to PyTorch 1.10, torchvision 0.11+ for Inaturalist'", "\n", "target_type", "=", "'full'", "\n", "split_split", "=", "split", ".", "split", "(", "'/'", ")", "\n", "if", "len", "(", "split_split", ")", ">", "1", ":", "\n", "                ", "target_type", "=", "split_split", "[", "0", "]", ".", "split", "(", "'_'", ")", "\n", "if", "len", "(", "target_type", ")", "==", "1", ":", "\n", "                    ", "target_type", "=", "target_type", "[", "0", "]", "\n", "", "split", "=", "split_split", "[", "-", "1", "]", "\n", "", "if", "split", "in", "_TRAIN_SYNONYM", ":", "\n", "                ", "split", "=", "'2021_train'", "\n", "", "elif", "split", "in", "_EVAL_SYNONYM", ":", "\n", "                ", "split", "=", "'2021_valid'", "\n", "", "ds", "=", "INaturalist", "(", "version", "=", "split", ",", "target_type", "=", "target_type", ",", "**", "torch_kwargs", ")", "\n", "", "elif", "name", "==", "'places365'", ":", "\n", "            ", "assert", "has_places365", ",", "'Please update to a newer PyTorch and torchvision for Places365 dataset.'", "\n", "if", "split", "in", "_TRAIN_SYNONYM", ":", "\n", "                ", "split", "=", "'train-standard'", "\n", "", "elif", "split", "in", "_EVAL_SYNONYM", ":", "\n", "                ", "split", "=", "'val'", "\n", "", "ds", "=", "Places365", "(", "split", "=", "split", ",", "**", "torch_kwargs", ")", "\n", "", "elif", "name", "==", "'imagenet'", ":", "\n", "            ", "if", "split", "in", "_EVAL_SYNONYM", ":", "\n", "                ", "split", "=", "'val'", "\n", "", "ds", "=", "ImageNet", "(", "split", "=", "split", ",", "**", "torch_kwargs", ")", "\n", "", "elif", "name", "==", "'image_folder'", "or", "name", "==", "'folder'", ":", "\n", "# in case torchvision ImageFolder is preferred over timm ImageDataset for some reason", "\n", "            ", "if", "search_split", "and", "os", ".", "path", ".", "isdir", "(", "root", ")", ":", "\n", "# look for split specific sub-folder in root", "\n", "                ", "root", "=", "_search_split", "(", "root", ",", "split", ")", "\n", "", "ds", "=", "ImageFolder", "(", "root", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "f\"Unknown torchvision dataset {name}\"", "\n", "", "", "elif", "name", ".", "startswith", "(", "'tfds/'", ")", ":", "\n", "        ", "ds", "=", "IterableImageDataset", "(", "\n", "root", ",", "parser", "=", "name", ",", "split", "=", "split", ",", "is_training", "=", "is_training", ",", "\n", "download", "=", "download", ",", "batch_size", "=", "batch_size", ",", "repeats", "=", "repeats", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "# FIXME support more advance split cfg for ImageFolder/Tar datasets in the future", "\n", "        ", "if", "search_split", "and", "os", ".", "path", ".", "isdir", "(", "root", ")", ":", "\n", "# look for split specific sub-folder in root", "\n", "            ", "root", "=", "_search_split", "(", "root", ",", "split", ")", "\n", "", "ds", "=", "ImageDataset", "(", "root", ",", "parser", "=", "name", ",", "class_map", "=", "class_map", ",", "load_bytes", "=", "load_bytes", ",", "**", "kwargs", ")", "\n", "", "return", "ds", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader.PrefetchLoader.__init__": [[70, 98], ["loader.expand_to_chs", "loader.expand_to_chs", "torch.tensor().cuda().view", "torch.tensor().cuda().view", "loader.PrefetchLoader.mean.half", "loader.PrefetchLoader.std.half", "random_erasing.RandomErasing", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader.expand_to_chs", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader.expand_to_chs"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "loader", ",", "\n", "mean", "=", "IMAGENET_DEFAULT_MEAN", ",", "\n", "std", "=", "IMAGENET_DEFAULT_STD", ",", "\n", "channels", "=", "3", ",", "\n", "fp16", "=", "False", ",", "\n", "re_prob", "=", "0.", ",", "\n", "re_mode", "=", "'const'", ",", "\n", "re_count", "=", "1", ",", "\n", "re_num_splits", "=", "0", ")", ":", "\n", "\n", "        ", "mean", "=", "expand_to_chs", "(", "mean", ",", "channels", ")", "\n", "std", "=", "expand_to_chs", "(", "std", ",", "channels", ")", "\n", "normalization_shape", "=", "(", "1", ",", "channels", ",", "1", ",", "1", ")", "\n", "\n", "self", ".", "loader", "=", "loader", "\n", "self", ".", "mean", "=", "torch", ".", "tensor", "(", "[", "x", "*", "255", "for", "x", "in", "mean", "]", ")", ".", "cuda", "(", ")", ".", "view", "(", "normalization_shape", ")", "\n", "self", ".", "std", "=", "torch", ".", "tensor", "(", "[", "x", "*", "255", "for", "x", "in", "std", "]", ")", ".", "cuda", "(", ")", ".", "view", "(", "normalization_shape", ")", "\n", "self", ".", "fp16", "=", "fp16", "\n", "if", "fp16", ":", "\n", "            ", "self", ".", "mean", "=", "self", ".", "mean", ".", "half", "(", ")", "\n", "self", ".", "std", "=", "self", ".", "std", ".", "half", "(", ")", "\n", "", "if", "re_prob", ">", "0.", ":", "\n", "            ", "self", ".", "random_erasing", "=", "RandomErasing", "(", "\n", "probability", "=", "re_prob", ",", "mode", "=", "re_mode", ",", "max_count", "=", "re_count", ",", "num_splits", "=", "re_num_splits", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "random_erasing", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader.PrefetchLoader.__iter__": [[99, 124], ["torch.cuda.Stream", "torch.cuda.current_stream().wait_stream", "torch.cuda.stream", "loader.PrefetchLoader.cuda", "next_target.cuda.cuda.cuda", "loader.PrefetchLoader.half().sub_().div_", "loader.PrefetchLoader.float().sub_().div_", "loader.PrefetchLoader.random_erasing", "torch.cuda.current_stream", "loader.PrefetchLoader.half().sub_", "loader.PrefetchLoader.float().sub_", "loader.PrefetchLoader.half", "loader.PrefetchLoader.float"], "methods", ["None"], ["", "", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "stream", "=", "torch", ".", "cuda", ".", "Stream", "(", ")", "\n", "first", "=", "True", "\n", "\n", "for", "next_input", ",", "next_target", "in", "self", ".", "loader", ":", "\n", "            ", "with", "torch", ".", "cuda", ".", "stream", "(", "stream", ")", ":", "\n", "                ", "next_input", "=", "next_input", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "next_target", "=", "next_target", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "if", "self", ".", "fp16", ":", "\n", "                    ", "next_input", "=", "next_input", ".", "half", "(", ")", ".", "sub_", "(", "self", ".", "mean", ")", ".", "div_", "(", "self", ".", "std", ")", "\n", "", "else", ":", "\n", "                    ", "next_input", "=", "next_input", ".", "float", "(", ")", ".", "sub_", "(", "self", ".", "mean", ")", ".", "div_", "(", "self", ".", "std", ")", "\n", "", "if", "self", ".", "random_erasing", "is", "not", "None", ":", "\n", "                    ", "next_input", "=", "self", ".", "random_erasing", "(", "next_input", ")", "\n", "\n", "", "", "if", "not", "first", ":", "\n", "                ", "yield", "input", ",", "target", "\n", "", "else", ":", "\n", "                ", "first", "=", "False", "\n", "\n", "", "torch", ".", "cuda", ".", "current_stream", "(", ")", ".", "wait_stream", "(", "stream", ")", "\n", "input", "=", "next_input", "\n", "target", "=", "next_target", "\n", "\n", "", "yield", "input", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader.PrefetchLoader.__len__": [[125, 127], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "loader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader.PrefetchLoader.sampler": [[128, 131], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "sampler", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "loader", ".", "sampler", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader.PrefetchLoader.dataset": [[132, 135], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "dataset", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "loader", ".", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader.PrefetchLoader.mixup_enabled": [[143, 147], ["isinstance"], "methods", ["None"], ["", "", "@", "mixup_enabled", ".", "setter", "\n", "def", "mixup_enabled", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "isinstance", "(", "self", ".", "loader", ".", "collate_fn", ",", "FastCollateMixup", ")", ":", "\n", "            ", "self", ".", "loader", ".", "collate_fn", ".", "mixup_enabled", "=", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader.MultiEpochsDataLoader.__init__": [[281, 287], ["super().__init__", "loader._RepeatSampler", "super().__iter__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_tfds.ParserTfds.__iter__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "_DataLoader__initialized", "=", "False", "\n", "self", ".", "batch_sampler", "=", "_RepeatSampler", "(", "self", ".", "batch_sampler", ")", "\n", "self", ".", "_DataLoader__initialized", "=", "True", "\n", "self", ".", "iterator", "=", "super", "(", ")", ".", "__iter__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader.MultiEpochsDataLoader.__len__": [[288, 290], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "batch_sampler", ".", "sampler", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader.MultiEpochsDataLoader.__iter__": [[291, 294], ["range", "len", "next"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "self", ")", ")", ":", "\n", "            ", "yield", "next", "(", "self", ".", "iterator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader._RepeatSampler.__init__": [[303, 305], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "sampler", ")", ":", "\n", "        ", "self", ".", "sampler", "=", "sampler", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader._RepeatSampler.__iter__": [[306, 309], ["iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "yield", "from", "iter", "(", "self", ".", "sampler", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader.fast_collate": [[23, 56], ["isinstance", "len", "isinstance", "len", "torch.zeros", "torch.zeros", "range", "isinstance", "range", "torch.tensor", "torch.zeros", "range", "isinstance", "len", "torch.from_numpy", "len", "torch.from_numpy", "torch.tensor", "torch.zeros", "range", "len", "tensor[].copy_"], "function", ["None"], ["def", "fast_collate", "(", "batch", ")", ":", "\n", "    ", "\"\"\" A fast collation function optimized for uint8 images (np array or torch) and int64 targets (labels)\"\"\"", "\n", "assert", "isinstance", "(", "batch", "[", "0", "]", ",", "tuple", ")", "\n", "batch_size", "=", "len", "(", "batch", ")", "\n", "if", "isinstance", "(", "batch", "[", "0", "]", "[", "0", "]", ",", "tuple", ")", ":", "\n", "# This branch 'deinterleaves' and flattens tuples of input tensors into one tensor ordered by position", "\n", "# such that all tuple of position n will end up in a torch.split(tensor, batch_size) in nth position", "\n", "        ", "inner_tuple_size", "=", "len", "(", "batch", "[", "0", "]", "[", "0", "]", ")", "\n", "flattened_batch_size", "=", "batch_size", "*", "inner_tuple_size", "\n", "targets", "=", "torch", ".", "zeros", "(", "flattened_batch_size", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "tensor", "=", "torch", ".", "zeros", "(", "(", "flattened_batch_size", ",", "*", "batch", "[", "0", "]", "[", "0", "]", "[", "0", "]", ".", "shape", ")", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "assert", "len", "(", "batch", "[", "i", "]", "[", "0", "]", ")", "==", "inner_tuple_size", "# all input tensor tuples must be same length", "\n", "for", "j", "in", "range", "(", "inner_tuple_size", ")", ":", "\n", "                ", "targets", "[", "i", "+", "j", "*", "batch_size", "]", "=", "batch", "[", "i", "]", "[", "1", "]", "\n", "tensor", "[", "i", "+", "j", "*", "batch_size", "]", "+=", "torch", ".", "from_numpy", "(", "batch", "[", "i", "]", "[", "0", "]", "[", "j", "]", ")", "\n", "", "", "return", "tensor", ",", "targets", "\n", "", "elif", "isinstance", "(", "batch", "[", "0", "]", "[", "0", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "        ", "targets", "=", "torch", ".", "tensor", "(", "[", "b", "[", "1", "]", "for", "b", "in", "batch", "]", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "assert", "len", "(", "targets", ")", "==", "batch_size", "\n", "tensor", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "*", "batch", "[", "0", "]", "[", "0", "]", ".", "shape", ")", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "tensor", "[", "i", "]", "+=", "torch", ".", "from_numpy", "(", "batch", "[", "i", "]", "[", "0", "]", ")", "\n", "", "return", "tensor", ",", "targets", "\n", "", "elif", "isinstance", "(", "batch", "[", "0", "]", "[", "0", "]", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "targets", "=", "torch", ".", "tensor", "(", "[", "b", "[", "1", "]", "for", "b", "in", "batch", "]", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "assert", "len", "(", "targets", ")", "==", "batch_size", "\n", "tensor", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "*", "batch", "[", "0", "]", "[", "0", "]", ".", "shape", ")", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "tensor", "[", "i", "]", ".", "copy_", "(", "batch", "[", "i", "]", "[", "0", "]", ")", "\n", "", "return", "tensor", ",", "targets", "\n", "", "else", ":", "\n", "        ", "assert", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader.expand_to_chs": [[58, 66], ["isinstance", "tuple", "itertools.repeat", "len", "len"], "function", ["None"], ["", "", "def", "expand_to_chs", "(", "x", ",", "n", ")", ":", "\n", "    ", "if", "not", "isinstance", "(", "x", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "x", "=", "tuple", "(", "repeat", "(", "x", ",", "n", ")", ")", "\n", "", "elif", "len", "(", "x", ")", "==", "1", ":", "\n", "        ", "x", "=", "x", "*", "n", "\n", "", "else", ":", "\n", "        ", "assert", "len", "(", "x", ")", "==", "n", ",", "'normalization stats must match image channels'", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader._worker_init": [[149, 163], ["torch.utils.data.get_worker_info", "isinstance", "worker_seeding", "random.seed", "torch.manual_seed", "numpy.random.seed", "numpy.random.seed"], "function", ["None"], ["", "", "", "def", "_worker_init", "(", "worker_id", ",", "worker_seeding", "=", "'all'", ")", ":", "\n", "    ", "worker_info", "=", "torch", ".", "utils", ".", "data", ".", "get_worker_info", "(", ")", "\n", "assert", "worker_info", ".", "id", "==", "worker_id", "\n", "if", "isinstance", "(", "worker_seeding", ",", "Callable", ")", ":", "\n", "        ", "seed", "=", "worker_seeding", "(", "worker_info", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", "%", "(", "2", "**", "32", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "        ", "assert", "worker_seeding", "in", "(", "'all'", ",", "'part'", ")", "\n", "# random / torch seed already called in dataloader iter class w/ worker_info.seed", "\n", "# to reproduce some old results (same seed + hparam combo), partial seeding is required (skip numpy re-seed)", "\n", "if", "worker_seeding", "==", "'all'", ":", "\n", "            ", "np", ".", "random", ".", "seed", "(", "worker_info", ".", "seed", "%", "(", "2", "**", "32", "-", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.loader.create_loader": [[165, 277], ["transforms_factory.create_transform", "dict", "loader_class", "loader.PrefetchLoader", "isinstance", "distributed_sampler.OrderedDistributedSampler", "functools.partial", "dict.pop", "loader_class", "distributed_sampler.RepeatAugSampler", "torch.utils.data.distributed.DistributedSampler", "isinstance"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.transforms_factory.create_transform"], ["", "", "", "def", "create_loader", "(", "\n", "dataset", ",", "\n", "input_size", ",", "\n", "batch_size", ",", "\n", "is_training", "=", "False", ",", "\n", "use_prefetcher", "=", "True", ",", "\n", "no_aug", "=", "False", ",", "\n", "re_prob", "=", "0.", ",", "\n", "re_mode", "=", "'const'", ",", "\n", "re_count", "=", "1", ",", "\n", "re_split", "=", "False", ",", "\n", "scale", "=", "None", ",", "\n", "ratio", "=", "None", ",", "\n", "hflip", "=", "0.5", ",", "\n", "vflip", "=", "0.", ",", "\n", "color_jitter", "=", "0.4", ",", "\n", "auto_augment", "=", "None", ",", "\n", "num_aug_repeats", "=", "0", ",", "\n", "num_aug_splits", "=", "0", ",", "\n", "interpolation", "=", "'bilinear'", ",", "\n", "mean", "=", "IMAGENET_DEFAULT_MEAN", ",", "\n", "std", "=", "IMAGENET_DEFAULT_STD", ",", "\n", "num_workers", "=", "1", ",", "\n", "distributed", "=", "False", ",", "\n", "crop_pct", "=", "None", ",", "\n", "collate_fn", "=", "None", ",", "\n", "pin_memory", "=", "False", ",", "\n", "fp16", "=", "False", ",", "\n", "tf_preprocessing", "=", "False", ",", "\n", "use_multi_epochs_loader", "=", "False", ",", "\n", "persistent_workers", "=", "True", ",", "\n", "worker_seeding", "=", "'all'", ",", "\n", ")", ":", "\n", "    ", "re_num_splits", "=", "0", "\n", "if", "re_split", ":", "\n", "# apply RE to second half of batch if no aug split otherwise line up with aug split", "\n", "        ", "re_num_splits", "=", "num_aug_splits", "or", "2", "\n", "", "dataset", ".", "transform", "=", "create_transform", "(", "\n", "input_size", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_prefetcher", "=", "use_prefetcher", ",", "\n", "no_aug", "=", "no_aug", ",", "\n", "scale", "=", "scale", ",", "\n", "ratio", "=", "ratio", ",", "\n", "hflip", "=", "hflip", ",", "\n", "vflip", "=", "vflip", ",", "\n", "color_jitter", "=", "color_jitter", ",", "\n", "auto_augment", "=", "auto_augment", ",", "\n", "interpolation", "=", "interpolation", ",", "\n", "mean", "=", "mean", ",", "\n", "std", "=", "std", ",", "\n", "crop_pct", "=", "crop_pct", ",", "\n", "tf_preprocessing", "=", "tf_preprocessing", ",", "\n", "re_prob", "=", "re_prob", ",", "\n", "re_mode", "=", "re_mode", ",", "\n", "re_count", "=", "re_count", ",", "\n", "re_num_splits", "=", "re_num_splits", ",", "\n", "separate", "=", "num_aug_splits", ">", "0", ",", "\n", ")", "\n", "\n", "sampler", "=", "None", "\n", "if", "distributed", "and", "not", "isinstance", "(", "dataset", ",", "torch", ".", "utils", ".", "data", ".", "IterableDataset", ")", ":", "\n", "        ", "if", "is_training", ":", "\n", "            ", "if", "num_aug_repeats", ":", "\n", "                ", "sampler", "=", "RepeatAugSampler", "(", "dataset", ",", "num_repeats", "=", "num_aug_repeats", ")", "\n", "", "else", ":", "\n", "                ", "sampler", "=", "torch", ".", "utils", ".", "data", ".", "distributed", ".", "DistributedSampler", "(", "dataset", ")", "\n", "", "", "else", ":", "\n", "# This will add extra duplicate entries to result in equal num", "\n", "# of samples per-process, will slightly alter validation results", "\n", "            ", "sampler", "=", "OrderedDistributedSampler", "(", "dataset", ")", "\n", "", "", "else", ":", "\n", "        ", "assert", "num_aug_repeats", "==", "0", ",", "\"RepeatAugment not currently supported in non-distributed or IterableDataset use\"", "\n", "\n", "", "if", "collate_fn", "is", "None", ":", "\n", "        ", "collate_fn", "=", "fast_collate", "if", "use_prefetcher", "else", "torch", ".", "utils", ".", "data", ".", "dataloader", ".", "default_collate", "\n", "\n", "", "loader_class", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "\n", "if", "use_multi_epochs_loader", ":", "\n", "        ", "loader_class", "=", "MultiEpochsDataLoader", "\n", "\n", "", "loader_args", "=", "dict", "(", "\n", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "not", "isinstance", "(", "dataset", ",", "torch", ".", "utils", ".", "data", ".", "IterableDataset", ")", "and", "sampler", "is", "None", "and", "is_training", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", "sampler", "=", "sampler", ",", "\n", "collate_fn", "=", "collate_fn", ",", "\n", "pin_memory", "=", "pin_memory", ",", "\n", "drop_last", "=", "is_training", ",", "\n", "worker_init_fn", "=", "partial", "(", "_worker_init", ",", "worker_seeding", "=", "worker_seeding", ")", ",", "\n", "persistent_workers", "=", "persistent_workers", "\n", ")", "\n", "try", ":", "\n", "        ", "loader", "=", "loader_class", "(", "dataset", ",", "**", "loader_args", ")", "\n", "", "except", "TypeError", "as", "e", ":", "\n", "        ", "loader_args", ".", "pop", "(", "'persistent_workers'", ")", "# only in Pytorch 1.7+", "\n", "loader", "=", "loader_class", "(", "dataset", ",", "**", "loader_args", ")", "\n", "", "if", "use_prefetcher", ":", "\n", "        ", "prefetch_re_prob", "=", "re_prob", "if", "is_training", "and", "not", "no_aug", "else", "0.", "\n", "loader", "=", "PrefetchLoader", "(", "\n", "loader", ",", "\n", "mean", "=", "mean", ",", "\n", "std", "=", "std", ",", "\n", "channels", "=", "input_size", "[", "0", "]", ",", "\n", "fp16", "=", "fp16", ",", "\n", "re_prob", "=", "prefetch_re_prob", ",", "\n", "re_mode", "=", "re_mode", ",", "\n", "re_count", "=", "re_count", ",", "\n", "re_num_splits", "=", "re_num_splits", "\n", ")", "\n", "\n", "", "return", "loader", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.distributed_sampler.OrderedDistributedSampler.__init__": [[22, 36], ["int", "torch.get_world_size", "torch.get_world_size", "torch.get_rank", "torch.get_rank", "math.ceil", "torch.is_available", "torch.is_available", "RuntimeError", "torch.is_available", "torch.is_available", "RuntimeError", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataset", ",", "num_replicas", "=", "None", ",", "rank", "=", "None", ")", ":", "\n", "        ", "if", "num_replicas", "is", "None", ":", "\n", "            ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Requires distributed package to be available\"", ")", "\n", "", "num_replicas", "=", "dist", ".", "get_world_size", "(", ")", "\n", "", "if", "rank", "is", "None", ":", "\n", "            ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Requires distributed package to be available\"", ")", "\n", "", "rank", "=", "dist", ".", "get_rank", "(", ")", "\n", "", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "num_replicas", "=", "num_replicas", "\n", "self", ".", "rank", "=", "rank", "\n", "self", ".", "num_samples", "=", "int", "(", "math", ".", "ceil", "(", "len", "(", "self", ".", "dataset", ")", "*", "1.0", "/", "self", ".", "num_replicas", ")", ")", "\n", "self", ".", "total_size", "=", "self", ".", "num_samples", "*", "self", ".", "num_replicas", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.distributed_sampler.OrderedDistributedSampler.__iter__": [[37, 49], ["list", "iter", "range", "len", "len", "len", "len"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "indices", "=", "list", "(", "range", "(", "len", "(", "self", ".", "dataset", ")", ")", ")", "\n", "\n", "# add extra samples to make it evenly divisible", "\n", "indices", "+=", "indices", "[", ":", "(", "self", ".", "total_size", "-", "len", "(", "indices", ")", ")", "]", "\n", "assert", "len", "(", "indices", ")", "==", "self", ".", "total_size", "\n", "\n", "# subsample", "\n", "indices", "=", "indices", "[", "self", ".", "rank", ":", "self", ".", "total_size", ":", "self", ".", "num_replicas", "]", "\n", "assert", "len", "(", "indices", ")", "==", "self", ".", "num_samples", "\n", "\n", "return", "iter", "(", "indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.distributed_sampler.OrderedDistributedSampler.__len__": [[50, 52], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.distributed_sampler.RepeatAugSampler.__init__": [[65, 100], ["int", "torch.get_world_size", "torch.get_world_size", "torch.get_rank", "torch.get_rank", "math.ceil", "int", "int", "torch.is_available", "torch.is_available", "RuntimeError", "torch.is_available", "torch.is_available", "RuntimeError", "math.floor", "math.ceil", "len", "len", "len"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dataset", ",", "\n", "num_replicas", "=", "None", ",", "\n", "rank", "=", "None", ",", "\n", "shuffle", "=", "True", ",", "\n", "num_repeats", "=", "3", ",", "\n", "selected_round", "=", "256", ",", "\n", "selected_ratio", "=", "0", ",", "\n", ")", ":", "\n", "        ", "if", "num_replicas", "is", "None", ":", "\n", "            ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Requires distributed package to be available\"", ")", "\n", "", "num_replicas", "=", "dist", ".", "get_world_size", "(", ")", "\n", "", "if", "rank", "is", "None", ":", "\n", "            ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Requires distributed package to be available\"", ")", "\n", "", "rank", "=", "dist", ".", "get_rank", "(", ")", "\n", "", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "num_replicas", "=", "num_replicas", "\n", "self", ".", "rank", "=", "rank", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "self", ".", "num_repeats", "=", "num_repeats", "\n", "self", ".", "epoch", "=", "0", "\n", "self", ".", "num_samples", "=", "int", "(", "math", ".", "ceil", "(", "len", "(", "self", ".", "dataset", ")", "*", "num_repeats", "/", "self", ".", "num_replicas", ")", ")", "\n", "self", ".", "total_size", "=", "self", ".", "num_samples", "*", "self", ".", "num_replicas", "\n", "# Determine the number of samples to select per epoch for each rank.", "\n", "# num_selected logic defaults to be the same as original RASampler impl, but this one can be tweaked", "\n", "# via selected_ratio and selected_round args.", "\n", "selected_ratio", "=", "selected_ratio", "or", "num_replicas", "# ratio to reduce selected samples by, num_replicas if 0", "\n", "if", "selected_round", ":", "\n", "            ", "self", ".", "num_selected_samples", "=", "int", "(", "math", ".", "floor", "(", "\n", "len", "(", "self", ".", "dataset", ")", "//", "selected_round", "*", "selected_round", "/", "selected_ratio", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_selected_samples", "=", "int", "(", "math", ".", "ceil", "(", "len", "(", "self", ".", "dataset", ")", "/", "selected_ratio", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.distributed_sampler.RepeatAugSampler.__iter__": [[101, 130], ["torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator.manual_seed", "torch.Generator.manual_seed", "torch.repeat_interleave.tolist", "torch.repeat_interleave.tolist", "iter", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "isinstance", "math.ceil", "torch.repeat_interleave", "torch.repeat_interleave", "torch.repeat_interleave", "torch.repeat_interleave", "len", "len", "len", "len", "distributed_sampler.RepeatAugSampler.num_repeats.is_integer", "len", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "int", "int", "range"], "methods", ["None"], ["", "", "def", "__iter__", "(", "self", ")", ":", "\n", "# deterministically shuffle based on epoch", "\n", "        ", "g", "=", "torch", ".", "Generator", "(", ")", "\n", "g", ".", "manual_seed", "(", "self", ".", "epoch", ")", "\n", "if", "self", ".", "shuffle", ":", "\n", "            ", "indices", "=", "torch", ".", "randperm", "(", "len", "(", "self", ".", "dataset", ")", ",", "generator", "=", "g", ")", "\n", "", "else", ":", "\n", "            ", "indices", "=", "torch", ".", "arange", "(", "start", "=", "0", ",", "end", "=", "len", "(", "self", ".", "dataset", ")", ")", "\n", "\n", "# produce repeats e.g. [0, 0, 0, 1, 1, 1, 2, 2, 2....]", "\n", "", "if", "isinstance", "(", "self", ".", "num_repeats", ",", "float", ")", "and", "not", "self", ".", "num_repeats", ".", "is_integer", "(", ")", ":", "\n", "# resample for repeats w/ non-integer ratio", "\n", "            ", "repeat_size", "=", "math", ".", "ceil", "(", "self", ".", "num_repeats", "*", "len", "(", "self", ".", "dataset", ")", ")", "\n", "indices", "=", "indices", "[", "torch", ".", "tensor", "(", "[", "int", "(", "i", "//", "self", ".", "num_repeats", ")", "for", "i", "in", "range", "(", "repeat_size", ")", "]", ")", "]", "\n", "", "else", ":", "\n", "            ", "indices", "=", "torch", ".", "repeat_interleave", "(", "indices", ",", "repeats", "=", "int", "(", "self", ".", "num_repeats", ")", ",", "dim", "=", "0", ")", "\n", "", "indices", "=", "indices", ".", "tolist", "(", ")", "# leaving as tensor thrashes dataloader memory", "\n", "# add extra samples to make it evenly divisible", "\n", "padding_size", "=", "self", ".", "total_size", "-", "len", "(", "indices", ")", "\n", "if", "padding_size", ">", "0", ":", "\n", "            ", "indices", "+=", "indices", "[", ":", "padding_size", "]", "\n", "", "assert", "len", "(", "indices", ")", "==", "self", ".", "total_size", "\n", "\n", "# subsample per rank", "\n", "indices", "=", "indices", "[", "self", ".", "rank", ":", "self", ".", "total_size", ":", "self", ".", "num_replicas", "]", "\n", "assert", "len", "(", "indices", ")", "==", "self", ".", "num_samples", "\n", "\n", "# return up to num selected samples", "\n", "return", "iter", "(", "indices", "[", ":", "self", ".", "num_selected_samples", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.distributed_sampler.RepeatAugSampler.__len__": [[131, 133], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_selected_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.data.distributed_sampler.RepeatAugSampler.set_epoch": [[134, 136], ["None"], "methods", ["None"], ["", "def", "set_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "self", ".", "epoch", "=", "epoch", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_folder.ParserImageFolder.__init__": [[41, 55], ["parser.Parser.__init__", "parser_image_folder.find_images_and_targets", "class_map.load_class_map", "len", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_folder.find_images_and_targets", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.class_map.load_class_map"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "root", ",", "\n", "class_map", "=", "''", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "root", "=", "root", "\n", "class_to_idx", "=", "None", "\n", "if", "class_map", ":", "\n", "            ", "class_to_idx", "=", "load_class_map", "(", "class_map", ",", "root", ")", "\n", "", "self", ".", "samples", ",", "self", ".", "class_to_idx", "=", "find_images_and_targets", "(", "root", ",", "class_to_idx", "=", "class_to_idx", ")", "\n", "if", "len", "(", "self", ".", "samples", ")", "==", "0", ":", "\n", "            ", "raise", "RuntimeError", "(", "\n", "f'Found 0 images in subfolders of {root}. Supported image extensions are {\", \".join(IMG_EXTENSIONS)}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_folder.ParserImageFolder.__getitem__": [[56, 59], ["open"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "path", ",", "target", "=", "self", ".", "samples", "[", "index", "]", "\n", "return", "open", "(", "path", ",", "'rb'", ")", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_folder.ParserImageFolder.__len__": [[60, 62], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_folder.ParserImageFolder._filename": [[63, 70], ["os.path.basename", "os.path.relpath"], "methods", ["None"], ["", "def", "_filename", "(", "self", ",", "index", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "filename", "=", "self", ".", "samples", "[", "index", "]", "[", "0", "]", "\n", "if", "basename", ":", "\n", "            ", "filename", "=", "os", ".", "path", ".", "basename", "(", "filename", ")", "\n", "", "elif", "not", "absolute", ":", "\n", "            ", "filename", "=", "os", ".", "path", ".", "relpath", "(", "filename", ",", "self", ".", "root", ")", "\n", "", "return", "filename", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_folder.find_images_and_targets": [[17, 37], ["os.walk", "set", "list", "sorted", "os.path.relpath", "os.path.basename", "rel_path.replace", "os.path.splitext", "sorted", "zip", "ext.lower", "filenames.append", "labels.append", "enumerate", "os.path.join", "timm.utils.misc.natural_key"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.misc.natural_key"], ["def", "find_images_and_targets", "(", "folder", ",", "types", "=", "IMG_EXTENSIONS", ",", "class_to_idx", "=", "None", ",", "leaf_name_only", "=", "True", ",", "sort", "=", "True", ")", ":", "\n", "    ", "labels", "=", "[", "]", "\n", "filenames", "=", "[", "]", "\n", "for", "root", ",", "subdirs", ",", "files", "in", "os", ".", "walk", "(", "folder", ",", "topdown", "=", "False", ",", "followlinks", "=", "True", ")", ":", "\n", "        ", "rel_path", "=", "os", ".", "path", ".", "relpath", "(", "root", ",", "folder", ")", "if", "(", "root", "!=", "folder", ")", "else", "''", "\n", "label", "=", "os", ".", "path", ".", "basename", "(", "rel_path", ")", "if", "leaf_name_only", "else", "rel_path", ".", "replace", "(", "os", ".", "path", ".", "sep", ",", "'_'", ")", "\n", "for", "f", "in", "files", ":", "\n", "            ", "base", ",", "ext", "=", "os", ".", "path", ".", "splitext", "(", "f", ")", "\n", "if", "ext", ".", "lower", "(", ")", "in", "types", ":", "\n", "                ", "filenames", ".", "append", "(", "os", ".", "path", ".", "join", "(", "root", ",", "f", ")", ")", "\n", "labels", ".", "append", "(", "label", ")", "\n", "", "", "", "if", "class_to_idx", "is", "None", ":", "\n", "# building class index", "\n", "        ", "unique_labels", "=", "set", "(", "labels", ")", "\n", "sorted_labels", "=", "list", "(", "sorted", "(", "unique_labels", ",", "key", "=", "natural_key", ")", ")", "\n", "class_to_idx", "=", "{", "c", ":", "idx", "for", "idx", ",", "c", "in", "enumerate", "(", "sorted_labels", ")", "}", "\n", "", "images_and_targets", "=", "[", "(", "f", ",", "class_to_idx", "[", "l", "]", ")", "for", "f", ",", "l", "in", "zip", "(", "filenames", ",", "labels", ")", "if", "l", "in", "class_to_idx", "]", "\n", "if", "sort", ":", "\n", "        ", "images_and_targets", "=", "sorted", "(", "images_and_targets", ",", "key", "=", "lambda", "k", ":", "natural_key", "(", "k", "[", "0", "]", ")", ")", "\n", "", "return", "images_and_targets", ",", "class_to_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_tfds.ParserTfds.__init__": [[78, 157], ["parser.Parser.__init__", "tfds.builder", "parser_tfds.ParserTfds.builder.download_and_prepare", "parser_tfds.get_class_labels", "torch.is_available", "torch.is_available", "torch.is_initialized", "torch.is_initialized", "torch.get_rank", "torch.get_rank", "torch.get_world_size", "torch.get_world_size", "torch.get_world_size", "torch.get_world_size"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_tfds.get_class_labels"], ["def", "__init__", "(", "\n", "self", ",", "\n", "root", ",", "\n", "name", ",", "\n", "split", "=", "'train'", ",", "\n", "is_training", "=", "False", ",", "\n", "batch_size", "=", "None", ",", "\n", "download", "=", "False", ",", "\n", "repeats", "=", "0", ",", "\n", "seed", "=", "42", ",", "\n", "input_name", "=", "'image'", ",", "\n", "input_image", "=", "'RGB'", ",", "\n", "target_name", "=", "'label'", ",", "\n", "target_image", "=", "''", ",", "\n", "prefetch_size", "=", "None", ",", "\n", "shuffle_size", "=", "None", ",", "\n", "max_threadpool_size", "=", "None", "\n", ")", ":", "\n", "        ", "\"\"\" Tensorflow-datasets Wrapper\n\n        Args:\n            root: root data dir (ie your TFDS_DATA_DIR. not dataset specific sub-dir)\n            name: tfds dataset name (eg `imagenet2012`)\n            split: tfds dataset split (can use all TFDS split strings eg `train[:10%]`)\n            is_training: training mode, shuffle enabled, dataset len rounded by batch_size\n            batch_size: batch_size to use to unsure total examples % batch_size == 0 in training across all dis nodes\n            download: download and build TFDS dataset if set, otherwise must use tfds CLI\n            repeats: iterate through (repeat) the dataset this many times per iteration (once if 0 or 1)\n            seed: common seed for shard shuffle across all distributed/worker instances\n            input_name: name of Feature to return as data (input)\n            input_image: image mode if input is an image (currently PIL mode string)\n            target_name: name of Feature to return as target (label)\n            target_image: image mode if target is an image (currently PIL mode string)\n            prefetch_size: override default tf.data prefetch buffer size\n            shuffle_size: override default tf.data shuffle buffer size\n            max_threadpool_size: override default threadpool size for tf.data\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "root", "=", "root", "\n", "self", ".", "split", "=", "split", "\n", "self", ".", "is_training", "=", "is_training", "\n", "if", "self", ".", "is_training", ":", "\n", "            ", "assert", "batch_size", "is", "not", "None", ",", "\"Must specify batch_size in training mode for reasonable behaviour w/ TFDS wrapper\"", "\n", "", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "repeats", "=", "repeats", "\n", "self", ".", "common_seed", "=", "seed", "# a seed that's fixed across all worker / distributed instances", "\n", "\n", "# performance settings", "\n", "self", ".", "prefetch_size", "=", "prefetch_size", "or", "PREFETCH_SIZE", "\n", "self", ".", "shuffle_size", "=", "shuffle_size", "or", "SHUFFLE_SIZE", "\n", "self", ".", "max_threadpool_size", "=", "max_threadpool_size", "or", "MAX_TP_SIZE", "\n", "\n", "# TFDS builder and split information", "\n", "self", ".", "input_name", "=", "input_name", "# FIXME support tuples / lists of inputs and targets and full range of Feature", "\n", "self", ".", "input_image", "=", "input_image", "\n", "self", ".", "target_name", "=", "target_name", "\n", "self", ".", "target_image", "=", "target_image", "\n", "self", ".", "builder", "=", "tfds", ".", "builder", "(", "name", ",", "data_dir", "=", "root", ")", "\n", "# NOTE: the tfds command line app can be used download & prepare datasets if you don't enable download flag", "\n", "if", "download", ":", "\n", "            ", "self", ".", "builder", ".", "download_and_prepare", "(", ")", "\n", "", "self", ".", "class_to_idx", "=", "get_class_labels", "(", "self", ".", "builder", ".", "info", ")", "if", "self", ".", "target_name", "==", "'label'", "else", "{", "}", "\n", "self", ".", "split_info", "=", "self", ".", "builder", ".", "info", ".", "splits", "[", "split", "]", "\n", "self", ".", "num_examples", "=", "self", ".", "split_info", ".", "num_examples", "\n", "\n", "# Distributed world state", "\n", "self", ".", "dist_rank", "=", "0", "\n", "self", ".", "dist_num_replicas", "=", "1", "\n", "if", "dist", ".", "is_available", "(", ")", "and", "dist", ".", "is_initialized", "(", ")", "and", "dist", ".", "get_world_size", "(", ")", ">", "1", ":", "\n", "            ", "self", ".", "dist_rank", "=", "dist", ".", "get_rank", "(", ")", "\n", "self", ".", "dist_num_replicas", "=", "dist", ".", "get_world_size", "(", ")", "\n", "\n", "# Attributes that are updated in _lazy_init, including the tf.data pipeline itself", "\n", "", "self", ".", "global_num_workers", "=", "1", "\n", "self", ".", "worker_info", "=", "None", "\n", "self", ".", "worker_seed", "=", "0", "# seed unique to each work instance", "\n", "self", ".", "subsplit", "=", "None", "# set when data is distributed across workers using sub-splits", "\n", "self", ".", "ds", "=", "None", "# initialized lazily on each dataloader worker process", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_tfds.ParserTfds._lazy_init": [[158, 233], ["torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "tfds.ReadConfig", "parser_tfds.ParserTfds.builder.as_dataset", "tf.data.Options", "max", "ds.shuffle.shuffle.with_options", "ds.shuffle.shuffle.prefetch", "tfds.as_numpy", "tf.distribute.InputContext", "hasattr", "getattr", "getattr", "ds.shuffle.shuffle.repeat", "ds.shuffle.shuffle.shuffle", "min", "tfds.even_splits", "min", "isinstance", "parser_tfds.even_split_indices"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_tfds.even_split_indices"], ["", "def", "_lazy_init", "(", "self", ")", ":", "\n", "        ", "\"\"\" Lazily initialize the dataset.\n\n        This is necessary to init the Tensorflow dataset pipeline in the (dataloader) process that\n        will be using the dataset instance. The __init__ method is called on the main process,\n        this will be called in a dataloader worker process.\n\n        NOTE: There will be problems if you try to re-use this dataset across different loader/worker\n        instances once it has been initialized. Do not call any dataset methods that can call _lazy_init\n        before it is passed to dataloader.\n        \"\"\"", "\n", "worker_info", "=", "torch", ".", "utils", ".", "data", ".", "get_worker_info", "(", ")", "\n", "\n", "# setup input context to split dataset across distributed processes", "\n", "num_workers", "=", "1", "\n", "global_worker_id", "=", "0", "\n", "if", "worker_info", "is", "not", "None", ":", "\n", "            ", "self", ".", "worker_info", "=", "worker_info", "\n", "self", ".", "worker_seed", "=", "worker_info", ".", "seed", "\n", "num_workers", "=", "worker_info", ".", "num_workers", "\n", "self", ".", "global_num_workers", "=", "self", ".", "dist_num_replicas", "*", "num_workers", "\n", "global_worker_id", "=", "self", ".", "dist_rank", "*", "num_workers", "+", "worker_info", ".", "id", "\n", "\n", "\"\"\" Data sharding\n            InputContext will assign subset of underlying TFRecord files to each 'pipeline' if used.\n            My understanding is that using split, the underling TFRecord files will shuffle (shuffle_files=True)\n            between the splits each iteration, but that understanding could be wrong.\n\n            I am currently using a mix of InputContext shard assignment and fine-grained sub-splits for distributing\n            the data across workers. For training InputContext is used to assign shards to nodes unless num_shards\n            in dataset < total number of workers. Otherwise sub-split API is used for datasets without enough shards or\n            for validation where we can't drop examples and need to avoid minimize uneven splits to avoid padding.\n            \"\"\"", "\n", "should_subsplit", "=", "self", ".", "global_num_workers", ">", "1", "and", "(", "\n", "self", ".", "split_info", ".", "num_shards", "<", "self", ".", "global_num_workers", "or", "not", "self", ".", "is_training", ")", "\n", "if", "should_subsplit", ":", "\n", "# split the dataset w/o using sharding for more even examples / worker, can result in less optimal", "\n", "# read patterns for distributed training (overlap across shards) so better to use InputContext there", "\n", "                ", "if", "has_buggy_even_splits", ":", "\n", "# my even_split workaround doesn't work on subsplits, upgrade tfds!", "\n", "                    ", "if", "not", "isinstance", "(", "self", ".", "split_info", ",", "tfds", ".", "core", ".", "splits", ".", "SubSplitInfo", ")", ":", "\n", "                        ", "subsplits", "=", "even_split_indices", "(", "self", ".", "split", ",", "self", ".", "global_num_workers", ",", "self", ".", "num_examples", ")", "\n", "self", ".", "subsplit", "=", "subsplits", "[", "global_worker_id", "]", "\n", "", "", "else", ":", "\n", "                    ", "subsplits", "=", "tfds", ".", "even_splits", "(", "self", ".", "split", ",", "self", ".", "global_num_workers", ")", "\n", "self", ".", "subsplit", "=", "subsplits", "[", "global_worker_id", "]", "\n", "\n", "", "", "", "input_context", "=", "None", "\n", "if", "self", ".", "global_num_workers", ">", "1", "and", "self", ".", "subsplit", "is", "None", ":", "\n", "# set input context to divide shards among distributed replicas", "\n", "            ", "input_context", "=", "tf", ".", "distribute", ".", "InputContext", "(", "\n", "num_input_pipelines", "=", "self", ".", "global_num_workers", ",", "\n", "input_pipeline_id", "=", "global_worker_id", ",", "\n", "num_replicas_in_sync", "=", "self", ".", "dist_num_replicas", "# FIXME does this arg have any impact?", "\n", ")", "\n", "", "read_config", "=", "tfds", ".", "ReadConfig", "(", "\n", "shuffle_seed", "=", "self", ".", "common_seed", ",", "\n", "shuffle_reshuffle_each_iteration", "=", "True", ",", "\n", "input_context", "=", "input_context", ")", "\n", "ds", "=", "self", ".", "builder", ".", "as_dataset", "(", "\n", "split", "=", "self", ".", "subsplit", "or", "self", ".", "split", ",", "shuffle_files", "=", "self", ".", "is_training", ",", "read_config", "=", "read_config", ")", "\n", "# avoid overloading threading w/ combo of TF ds threads + PyTorch workers", "\n", "options", "=", "tf", ".", "data", ".", "Options", "(", ")", "\n", "thread_member", "=", "'threading'", "if", "hasattr", "(", "options", ",", "'threading'", ")", "else", "'experimental_threading'", "\n", "getattr", "(", "options", ",", "thread_member", ")", ".", "private_threadpool_size", "=", "max", "(", "1", ",", "self", ".", "max_threadpool_size", "//", "num_workers", ")", "\n", "getattr", "(", "options", ",", "thread_member", ")", ".", "max_intra_op_parallelism", "=", "1", "\n", "ds", "=", "ds", ".", "with_options", "(", "options", ")", "\n", "if", "self", ".", "is_training", "or", "self", ".", "repeats", ">", "1", ":", "\n", "# to prevent excessive drop_last batch behaviour w/ IterableDatasets", "\n", "# see warnings at https://pytorch.org/docs/stable/data.html#multi-process-data-loading", "\n", "            ", "ds", "=", "ds", ".", "repeat", "(", ")", "# allow wrap around and break iteration manually", "\n", "", "if", "self", ".", "is_training", ":", "\n", "            ", "ds", "=", "ds", ".", "shuffle", "(", "min", "(", "self", ".", "num_examples", ",", "self", ".", "shuffle_size", ")", "//", "self", ".", "global_num_workers", ",", "seed", "=", "self", ".", "worker_seed", ")", "\n", "", "ds", "=", "ds", ".", "prefetch", "(", "min", "(", "self", ".", "num_examples", "//", "self", ".", "global_num_workers", ",", "self", ".", "prefetch_size", ")", ")", "\n", "self", ".", "ds", "=", "tfds", ".", "as_numpy", "(", "ds", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_tfds.ParserTfds.__iter__": [[234, 275], ["math.ceil", "parser_tfds.ParserTfds._lazy_init", "math.ceil", "PIL.Image.fromarray", "PIL.Image.fromarray", "max"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_tfds.ParserTfds._lazy_init"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "ds", "is", "None", ":", "\n", "            ", "self", ".", "_lazy_init", "(", ")", "\n", "\n", "# Compute a rounded up sample count that is used to:", "\n", "#   1. make batches even cross workers & replicas in distributed validation.", "\n", "#     This adds extra examples and will slightly alter validation results.", "\n", "#   2. determine loop ending condition in training w/ repeat enabled so that only full batch_size", "\n", "#     batches are produced (underlying tfds iter wraps around)", "\n", "", "target_example_count", "=", "math", ".", "ceil", "(", "max", "(", "1", ",", "self", ".", "repeats", ")", "*", "self", ".", "num_examples", "/", "self", ".", "global_num_workers", ")", "\n", "if", "self", ".", "is_training", ":", "\n", "# round up to nearest batch_size per worker-replica", "\n", "            ", "target_example_count", "=", "math", ".", "ceil", "(", "target_example_count", "/", "self", ".", "batch_size", ")", "*", "self", ".", "batch_size", "\n", "\n", "# Iterate until exhausted or sample count hits target when training (ds.repeat enabled)", "\n", "", "example_count", "=", "0", "\n", "for", "example", "in", "self", ".", "ds", ":", "\n", "            ", "input_data", "=", "example", "[", "self", ".", "input_name", "]", "\n", "if", "self", ".", "input_image", ":", "\n", "                ", "input_data", "=", "Image", ".", "fromarray", "(", "input_data", ",", "mode", "=", "self", ".", "input_image", ")", "\n", "", "target_data", "=", "example", "[", "self", ".", "target_name", "]", "\n", "if", "self", ".", "target_image", ":", "\n", "                ", "target_data", "=", "Image", ".", "fromarray", "(", "target_data", ",", "mode", "=", "self", ".", "target_image", ")", "\n", "", "yield", "input_data", ",", "target_data", "\n", "example_count", "+=", "1", "\n", "if", "self", ".", "is_training", "and", "example_count", ">=", "target_example_count", ":", "\n", "# Need to break out of loop when repeat() is enabled for training w/ oversampling", "\n", "# this results in extra examples per epoch but seems more desirable than dropping", "\n", "# up to N*J batches per epoch (where N = num distributed processes, and J = num worker processes)", "\n", "                ", "break", "\n", "\n", "# Pad across distributed nodes (make counts equal by adding examples)", "\n", "", "", "if", "not", "self", ".", "is_training", "and", "self", ".", "dist_num_replicas", ">", "1", "and", "self", ".", "subsplit", "is", "not", "None", "and", "0", "<", "example_count", "<", "target_example_count", ":", "\n", "# Validation batch padding only done for distributed training where results are reduced across nodes.", "\n", "# For single process case, it won't matter if workers return different batch sizes.", "\n", "# If using input_context or % based splits, sample count can vary significantly across workers and this", "\n", "# approach should not be used (hence disabled if self.subsplit isn't set).", "\n", "            ", "while", "example_count", "<", "target_example_count", ":", "\n", "                ", "yield", "input_data", ",", "target_data", "# yield prev sample again", "\n", "example_count", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_tfds.ParserTfds.__len__": [[276, 280], ["math.ceil", "max"], "methods", ["None"], ["", "", "", "def", "__len__", "(", "self", ")", ":", "\n", "# this is just an estimate and does not factor in extra examples added to pad batches based on", "\n", "# complete worker & replica info (not available until init in dataloader).", "\n", "        ", "return", "math", ".", "ceil", "(", "max", "(", "1", ",", "self", ".", "repeats", ")", "*", "self", ".", "num_examples", "/", "self", ".", "dist_num_replicas", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_tfds.ParserTfds._filename": [[281, 283], ["None"], "methods", ["None"], ["", "def", "_filename", "(", "self", ",", "index", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "assert", "False", ",", "\"Not supported\"", "# no random access to examples", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_tfds.ParserTfds.filenames": [[284, 302], ["parser_tfds.ParserTfds._lazy_init", "names.append", "len"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_tfds.ParserTfds._lazy_init"], ["", "def", "filenames", "(", "self", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "\"\"\" Return all filenames in dataset, overrides base\"\"\"", "\n", "if", "self", ".", "ds", "is", "None", ":", "\n", "            ", "self", ".", "_lazy_init", "(", ")", "\n", "", "names", "=", "[", "]", "\n", "for", "sample", "in", "self", ".", "ds", ":", "\n", "            ", "if", "len", "(", "names", ")", ">", "self", ".", "num_examples", ":", "\n", "                ", "break", "# safety for ds.repeat() case", "\n", "", "if", "'file_name'", "in", "sample", ":", "\n", "                ", "name", "=", "sample", "[", "'file_name'", "]", "\n", "", "elif", "'filename'", "in", "sample", ":", "\n", "                ", "name", "=", "sample", "[", "'filename'", "]", "\n", "", "elif", "'id'", "in", "sample", ":", "\n", "                ", "name", "=", "sample", "[", "'id'", "]", "\n", "", "else", ":", "\n", "                ", "assert", "False", ",", "\"No supported name field present\"", "\n", "", "names", ".", "append", "(", "name", ")", "\n", "", "return", "names", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_tfds.even_split_indices": [[41, 44], ["round", "range", "range"], "function", ["None"], ["def", "even_split_indices", "(", "split", ",", "n", ",", "num_examples", ")", ":", "\n", "    ", "partitions", "=", "[", "round", "(", "i", "*", "num_examples", "/", "n", ")", "for", "i", "in", "range", "(", "n", "+", "1", ")", "]", "\n", "return", "[", "f\"{split}[{partitions[i]}:{partitions[i + 1]}]\"", "for", "i", "in", "range", "(", "n", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_tfds.get_class_labels": [[46, 52], ["class_label.str2int"], "function", ["None"], ["", "def", "get_class_labels", "(", "info", ")", ":", "\n", "    ", "if", "'label'", "not", "in", "info", ".", "features", ":", "\n", "        ", "return", "{", "}", "\n", "", "class_label", "=", "info", ".", "features", "[", "'label'", "]", "\n", "class_to_idx", "=", "{", "n", ":", "class_label", ".", "str2int", "(", "n", ")", "for", "n", "in", "class_label", ".", "names", "}", "\n", "return", "class_to_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_in_tar.TarState.__init__": [[33, 37], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "tf", ":", "tarfile", ".", "TarFile", "=", "None", ",", "ti", ":", "tarfile", ".", "TarInfo", "=", "None", ")", ":", "\n", "        ", "self", ".", "tf", ":", "tarfile", ".", "TarFile", "=", "tf", "\n", "self", ".", "ti", ":", "tarfile", ".", "TarInfo", "=", "ti", "\n", "self", ".", "children", ":", "Dict", "[", "str", ",", "TarState", "]", "=", "{", "}", "# child states (tars within tars)", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_in_tar.TarState.reset": [[38, 40], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "tf", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_in_tar.ParserImageInTar.__init__": [[169, 189], ["parser.Parser.__init__", "parser_image_in_tar.extract_tarinfos", "class_map.load_class_map", "dict", "parser_image_in_tar.ParserImageInTar.class_name_to_idx.items", "len"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_in_tar.extract_tarinfos", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.class_map.load_class_map"], ["def", "__init__", "(", "self", ",", "root", ",", "class_map", "=", "''", ",", "cache_tarfiles", "=", "True", ",", "cache_tarinfo", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "class_name_to_idx", "=", "None", "\n", "if", "class_map", ":", "\n", "            ", "class_name_to_idx", "=", "load_class_map", "(", "class_map", ",", "root", ")", "\n", "", "self", ".", "root", "=", "root", "\n", "self", ".", "samples", ",", "self", ".", "targets", ",", "self", ".", "class_name_to_idx", ",", "tarfiles", "=", "extract_tarinfos", "(", "\n", "self", ".", "root", ",", "\n", "class_name_to_idx", "=", "class_name_to_idx", ",", "\n", "cache_tarinfo", "=", "cache_tarinfo", ",", "\n", "extensions", "=", "IMG_EXTENSIONS", ")", "\n", "self", ".", "class_idx_to_name", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "class_name_to_idx", ".", "items", "(", ")", "}", "\n", "if", "len", "(", "tarfiles", ")", "==", "1", "and", "tarfiles", "[", "0", "]", "[", "0", "]", "is", "None", ":", "\n", "            ", "self", ".", "root_is_tar", "=", "True", "\n", "self", ".", "tar_state", "=", "tarfiles", "[", "0", "]", "[", "1", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "root_is_tar", "=", "False", "\n", "self", ".", "tar_state", "=", "dict", "(", "tarfiles", ")", "\n", "", "self", ".", "cache_tarfiles", "=", "cache_tarfiles", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_in_tar.ParserImageInTar.__len__": [[190, 192], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_in_tar.ParserImageInTar.__getitem__": [[193, 217], ["os.path.join", "tarfile.open", "tarfile.open.extractfile", "tarfile.open", "tarfile.open.extractfile"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "sample", "=", "self", ".", "samples", "[", "index", "]", "\n", "target", "=", "self", ".", "targets", "[", "index", "]", "\n", "sample_ti", ",", "parent_fn", ",", "child_ti", "=", "sample", "\n", "parent_abs", "=", "os", ".", "path", ".", "join", "(", "self", ".", "root", ",", "parent_fn", ")", "if", "parent_fn", "else", "self", ".", "root", "\n", "\n", "tf", "=", "None", "\n", "cache_state", "=", "None", "\n", "if", "self", ".", "cache_tarfiles", ":", "\n", "            ", "cache_state", "=", "self", ".", "tar_state", "if", "self", ".", "root_is_tar", "else", "self", ".", "tar_state", "[", "parent_fn", "]", "\n", "tf", "=", "cache_state", ".", "tf", "\n", "", "if", "tf", "is", "None", ":", "\n", "            ", "tf", "=", "tarfile", ".", "open", "(", "parent_abs", ")", "\n", "if", "self", ".", "cache_tarfiles", ":", "\n", "                ", "cache_state", ".", "tf", "=", "tf", "\n", "", "", "if", "child_ti", "is", "not", "None", ":", "\n", "            ", "ctf", "=", "cache_state", ".", "children", "[", "child_ti", ".", "name", "]", ".", "tf", "if", "self", ".", "cache_tarfiles", "else", "None", "\n", "if", "ctf", "is", "None", ":", "\n", "                ", "ctf", "=", "tarfile", ".", "open", "(", "fileobj", "=", "tf", ".", "extractfile", "(", "child_ti", ")", ")", "\n", "if", "self", ".", "cache_tarfiles", ":", "\n", "                    ", "cache_state", ".", "children", "[", "child_ti", ".", "name", "]", ".", "tf", "=", "ctf", "\n", "", "", "tf", "=", "ctf", "\n", "\n", "", "return", "tf", ".", "extractfile", "(", "sample_ti", ")", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_in_tar.ParserImageInTar._filename": [[218, 223], ["os.path.basename"], "methods", ["None"], ["", "def", "_filename", "(", "self", ",", "index", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "filename", "=", "self", ".", "samples", "[", "index", "]", "[", "0", "]", ".", "name", "\n", "if", "basename", ":", "\n", "            ", "filename", "=", "os", ".", "path", ".", "basename", "(", "filename", ")", "\n", "", "return", "filename", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_in_tar._extract_tarinfo": [[42, 61], ["enumerate", "os.path.split", "os.path.splitext", "ext.lower.lower", "ti.isfile", "tarfile.open", "dict", "parser_image_in_tar._extract_tarinfo", "_logger.debug", "parent_info[].append", "parent_info[].append", "tf.extractfile", "os.path.join", "len"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_in_tar._extract_tarinfo"], ["", "", "def", "_extract_tarinfo", "(", "tf", ":", "tarfile", ".", "TarFile", ",", "parent_info", ":", "Dict", ",", "extensions", "=", "IMG_EXTENSIONS", ")", ":", "\n", "    ", "sample_count", "=", "0", "\n", "for", "i", ",", "ti", "in", "enumerate", "(", "tf", ")", ":", "\n", "        ", "if", "not", "ti", ".", "isfile", "(", ")", ":", "\n", "            ", "continue", "\n", "", "dirname", ",", "basename", "=", "os", ".", "path", ".", "split", "(", "ti", ".", "path", ")", "\n", "name", ",", "ext", "=", "os", ".", "path", ".", "splitext", "(", "basename", ")", "\n", "ext", "=", "ext", ".", "lower", "(", ")", "\n", "if", "ext", "==", "'.tar'", ":", "\n", "            ", "with", "tarfile", ".", "open", "(", "fileobj", "=", "tf", ".", "extractfile", "(", "ti", ")", ",", "mode", "=", "'r|'", ")", "as", "ctf", ":", "\n", "                ", "child_info", "=", "dict", "(", "\n", "name", "=", "ti", ".", "name", ",", "path", "=", "os", ".", "path", ".", "join", "(", "parent_info", "[", "'path'", "]", ",", "name", ")", ",", "ti", "=", "ti", ",", "children", "=", "[", "]", ",", "samples", "=", "[", "]", ")", "\n", "sample_count", "+=", "_extract_tarinfo", "(", "ctf", ",", "child_info", ",", "extensions", "=", "extensions", ")", "\n", "_logger", ".", "debug", "(", "f'{i}/?. Extracted child tarinfos from {ti.name}. {len(child_info[\"samples\"])} images.'", ")", "\n", "parent_info", "[", "'children'", "]", ".", "append", "(", "child_info", ")", "\n", "", "", "elif", "ext", "in", "extensions", ":", "\n", "            ", "parent_info", "[", "'samples'", "]", ".", "append", "(", "ti", ")", "\n", "sample_count", "+=", "1", "\n", "", "", "return", "sample_count", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_in_tar.extract_tarinfos": [[63, 163], ["os.path.isfile", "len", "sum", "_logger.info", "dict", "os.path.exists", "_logger.info", "_logger.info", "zip", "numpy.array", "numpy.array", "_logger.info", "os.path.split", "glob.glob", "os.path.join", "_logger.info", "enumerate", "os.path.join().strip", "parser_image_in_tar.TarState", "parser_image_in_tar.extract_tarinfos._add_samples"], "function", ["None"], ["", "def", "extract_tarinfos", "(", "root", ",", "class_name_to_idx", "=", "None", ",", "cache_tarinfo", "=", "None", ",", "extensions", "=", "IMG_EXTENSIONS", ",", "sort", "=", "True", ")", ":", "\n", "    ", "root_is_tar", "=", "False", "\n", "if", "os", ".", "path", ".", "isfile", "(", "root", ")", ":", "\n", "        ", "assert", "os", ".", "path", ".", "splitext", "(", "root", ")", "[", "-", "1", "]", ".", "lower", "(", ")", "==", "'.tar'", "\n", "tar_filenames", "=", "[", "root", "]", "\n", "root", ",", "root_name", "=", "os", ".", "path", ".", "split", "(", "root", ")", "\n", "root_name", "=", "os", ".", "path", ".", "splitext", "(", "root_name", ")", "[", "0", "]", "\n", "root_is_tar", "=", "True", "\n", "", "else", ":", "\n", "        ", "root_name", "=", "root", ".", "strip", "(", "os", ".", "path", ".", "sep", ")", ".", "split", "(", "os", ".", "path", ".", "sep", ")", "[", "-", "1", "]", "\n", "tar_filenames", "=", "glob", "(", "os", ".", "path", ".", "join", "(", "root", ",", "'*.tar'", ")", ",", "recursive", "=", "True", ")", "\n", "", "num_tars", "=", "len", "(", "tar_filenames", ")", "\n", "tar_bytes", "=", "sum", "(", "[", "os", ".", "path", ".", "getsize", "(", "f", ")", "for", "f", "in", "tar_filenames", "]", ")", "\n", "assert", "num_tars", ",", "f'No .tar files found at specified path ({root}).'", "\n", "\n", "_logger", ".", "info", "(", "f'Scanning {tar_bytes/1024**2:.2f}MB of tar files...'", ")", "\n", "info", "=", "dict", "(", "tartrees", "=", "[", "]", ")", "\n", "cache_path", "=", "''", "\n", "if", "cache_tarinfo", "is", "None", ":", "\n", "        ", "cache_tarinfo", "=", "True", "if", "tar_bytes", ">", "10", "*", "1024", "**", "3", "else", "False", "# FIXME magic number, 10GB", "\n", "", "if", "cache_tarinfo", ":", "\n", "        ", "cache_filename", "=", "'_'", "+", "root_name", "+", "CACHE_FILENAME_SUFFIX", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "cache_filename", ")", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "_logger", ".", "info", "(", "f'Reading tar info from cache file {cache_path}.'", ")", "\n", "with", "open", "(", "cache_path", ",", "'rb'", ")", "as", "pf", ":", "\n", "            ", "info", "=", "pickle", ".", "load", "(", "pf", ")", "\n", "", "assert", "len", "(", "info", "[", "'tartrees'", "]", ")", "==", "num_tars", ",", "\"Cached tartree len doesn't match number of tarfiles\"", "\n", "", "else", ":", "\n", "        ", "for", "i", ",", "fn", "in", "enumerate", "(", "tar_filenames", ")", ":", "\n", "            ", "path", "=", "''", "if", "root_is_tar", "else", "os", ".", "path", ".", "splitext", "(", "os", ".", "path", ".", "basename", "(", "fn", ")", ")", "[", "0", "]", "\n", "with", "tarfile", ".", "open", "(", "fn", ",", "mode", "=", "'r|'", ")", "as", "tf", ":", "# tarinfo scans done in streaming mode", "\n", "                ", "parent_info", "=", "dict", "(", "name", "=", "os", ".", "path", ".", "relpath", "(", "fn", ",", "root", ")", ",", "path", "=", "path", ",", "ti", "=", "None", ",", "children", "=", "[", "]", ",", "samples", "=", "[", "]", ")", "\n", "num_samples", "=", "_extract_tarinfo", "(", "tf", ",", "parent_info", ",", "extensions", "=", "extensions", ")", "\n", "num_children", "=", "len", "(", "parent_info", "[", "\"children\"", "]", ")", "\n", "_logger", ".", "debug", "(", "\n", "f'{i}/{num_tars}. Extracted tarinfos from {fn}. {num_children} children, {num_samples} samples.'", ")", "\n", "", "info", "[", "'tartrees'", "]", ".", "append", "(", "parent_info", ")", "\n", "", "if", "cache_path", ":", "\n", "            ", "_logger", ".", "info", "(", "f'Writing tar info to cache file {cache_path}.'", ")", "\n", "with", "open", "(", "cache_path", ",", "'wb'", ")", "as", "pf", ":", "\n", "                ", "pickle", ".", "dump", "(", "info", ",", "pf", ")", "\n", "\n", "", "", "", "samples", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "build_class_map", "=", "False", "\n", "if", "class_name_to_idx", "is", "None", ":", "\n", "        ", "build_class_map", "=", "True", "\n", "\n", "# Flatten tartree info into lists of samples and targets w/ targets based on label id via", "\n", "# class map arg or from unique paths.", "\n", "# NOTE: currently only flattening up to two-levels, filesystem .tars and then one level of sub-tar children", "\n", "# this covers my current use cases and keeps things a little easier to test for now.", "\n", "", "tarfiles", "=", "[", "]", "\n", "\n", "def", "_label_from_paths", "(", "*", "path", ",", "leaf_only", "=", "True", ")", ":", "\n", "        ", "path", "=", "os", ".", "path", ".", "join", "(", "*", "path", ")", ".", "strip", "(", "os", ".", "path", ".", "sep", ")", "\n", "return", "path", ".", "split", "(", "os", ".", "path", ".", "sep", ")", "[", "-", "1", "]", "if", "leaf_only", "else", "path", ".", "replace", "(", "os", ".", "path", ".", "sep", ",", "'_'", ")", "\n", "\n", "", "def", "_add_samples", "(", "info", ",", "fn", ")", ":", "\n", "        ", "added", "=", "0", "\n", "for", "s", "in", "info", "[", "'samples'", "]", ":", "\n", "            ", "label", "=", "_label_from_paths", "(", "info", "[", "'path'", "]", ",", "os", ".", "path", ".", "dirname", "(", "s", ".", "path", ")", ")", "\n", "if", "not", "build_class_map", "and", "label", "not", "in", "class_name_to_idx", ":", "\n", "                ", "continue", "\n", "", "samples", ".", "append", "(", "(", "s", ",", "fn", ",", "info", "[", "'ti'", "]", ")", ")", "\n", "labels", ".", "append", "(", "label", ")", "\n", "added", "+=", "1", "\n", "", "return", "added", "\n", "\n", "", "_logger", ".", "info", "(", "f'Collecting samples and building tar states.'", ")", "\n", "for", "parent_info", "in", "info", "[", "'tartrees'", "]", ":", "\n", "# if tartree has children, we assume all samples are at the child level", "\n", "        ", "tar_name", "=", "None", "if", "root_is_tar", "else", "parent_info", "[", "'name'", "]", "\n", "tar_state", "=", "TarState", "(", ")", "\n", "parent_added", "=", "0", "\n", "for", "child_info", "in", "parent_info", "[", "'children'", "]", ":", "\n", "            ", "child_added", "=", "_add_samples", "(", "child_info", ",", "fn", "=", "tar_name", ")", "\n", "if", "child_added", ":", "\n", "                ", "tar_state", ".", "children", "[", "child_info", "[", "'name'", "]", "]", "=", "TarState", "(", "ti", "=", "child_info", "[", "'ti'", "]", ")", "\n", "", "parent_added", "+=", "child_added", "\n", "", "parent_added", "+=", "_add_samples", "(", "parent_info", ",", "fn", "=", "tar_name", ")", "\n", "if", "parent_added", ":", "\n", "            ", "tarfiles", ".", "append", "(", "(", "tar_name", ",", "tar_state", ")", ")", "\n", "", "", "del", "info", "\n", "\n", "if", "build_class_map", ":", "\n", "# build class index", "\n", "        ", "sorted_labels", "=", "list", "(", "sorted", "(", "set", "(", "labels", ")", ",", "key", "=", "natural_key", ")", ")", "\n", "class_name_to_idx", "=", "{", "c", ":", "idx", "for", "idx", ",", "c", "in", "enumerate", "(", "sorted_labels", ")", "}", "\n", "\n", "", "_logger", ".", "info", "(", "f'Mapping targets and sorting samples.'", ")", "\n", "samples_and_targets", "=", "[", "(", "s", ",", "class_name_to_idx", "[", "l", "]", ")", "for", "s", ",", "l", "in", "zip", "(", "samples", ",", "labels", ")", "if", "l", "in", "class_name_to_idx", "]", "\n", "if", "sort", ":", "\n", "        ", "samples_and_targets", "=", "sorted", "(", "samples_and_targets", ",", "key", "=", "lambda", "k", ":", "natural_key", "(", "k", "[", "0", "]", "[", "0", "]", ".", "path", ")", ")", "\n", "", "samples", ",", "targets", "=", "zip", "(", "*", "samples_and_targets", ")", "\n", "samples", "=", "np", ".", "array", "(", "samples", ")", "\n", "targets", "=", "np", ".", "array", "(", "targets", ")", "\n", "_logger", ".", "info", "(", "f'Finished processing {len(samples)} samples across {len(tarfiles)} tar files.'", ")", "\n", "return", "samples", ",", "targets", ",", "class_name_to_idx", ",", "tarfiles", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_factory.create_parser": [[8, 30], ["name.split.lower", "name.split.split", "len", "ParserTfds", "os.path.exists", "os.path.isfile", "parser_image_in_tar.ParserImageInTar", "parser_image_folder.ParserImageFolder", "os.path.splitext"], "function", ["None"], ["def", "create_parser", "(", "name", ",", "root", ",", "split", "=", "'train'", ",", "**", "kwargs", ")", ":", "\n", "    ", "name", "=", "name", ".", "lower", "(", ")", "\n", "name", "=", "name", ".", "split", "(", "'/'", ",", "2", ")", "\n", "prefix", "=", "''", "\n", "if", "len", "(", "name", ")", ">", "1", ":", "\n", "        ", "prefix", "=", "name", "[", "0", "]", "\n", "", "name", "=", "name", "[", "-", "1", "]", "\n", "\n", "# FIXME improve the selection right now just tfds prefix or fallback path, will need options to", "\n", "# explicitly select other options shortly", "\n", "if", "prefix", "==", "'tfds'", ":", "\n", "        ", "from", ".", "parser_tfds", "import", "ParserTfds", "# defer tensorflow import", "\n", "parser", "=", "ParserTfds", "(", "root", ",", "name", ",", "split", "=", "split", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "        ", "assert", "os", ".", "path", ".", "exists", "(", "root", ")", "\n", "# default fallback path (backwards compat), use image tar if root is a .tar file, otherwise image folder", "\n", "# FIXME support split here, in parser?", "\n", "if", "os", ".", "path", ".", "isfile", "(", "root", ")", "and", "os", ".", "path", ".", "splitext", "(", "root", ")", "[", "1", "]", "==", "'.tar'", ":", "\n", "            ", "parser", "=", "ParserImageInTar", "(", "root", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "            ", "parser", "=", "ParserImageFolder", "(", "root", ",", "**", "kwargs", ")", "\n", "", "", "return", "parser", "\n", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser.Parser.__init__": [[5, 7], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser.Parser._filename": [[8, 11], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "_filename", "(", "self", ",", "index", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser.Parser.filename": [[12, 14], ["parser.Parser._filename"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_tar.ParserImageTar._filename"], ["", "def", "filename", "(", "self", ",", "index", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "_filename", "(", "index", ",", "basename", "=", "basename", ",", "absolute", "=", "absolute", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser.Parser.filenames": [[15, 17], ["parser.Parser._filename", "range", "len"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_tar.ParserImageTar._filename"], ["", "def", "filenames", "(", "self", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "return", "[", "self", ".", "_filename", "(", "index", ",", "basename", "=", "basename", ",", "absolute", "=", "absolute", ")", "for", "index", "in", "range", "(", "len", "(", "self", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_tar.ParserImageTar.__init__": [[44, 57], ["parser.Parser.__init__", "os.path.isfile", "class_map.load_class_map", "tarfile.open", "parser_image_tar.extract_tarinfo"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.class_map.load_class_map", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_tar.extract_tarinfo"], ["def", "__init__", "(", "self", ",", "root", ",", "class_map", "=", "''", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "class_to_idx", "=", "None", "\n", "if", "class_map", ":", "\n", "            ", "class_to_idx", "=", "load_class_map", "(", "class_map", ",", "root", ")", "\n", "", "assert", "os", ".", "path", ".", "isfile", "(", "root", ")", "\n", "self", ".", "root", "=", "root", "\n", "\n", "with", "tarfile", ".", "open", "(", "root", ")", "as", "tf", ":", "# cannot keep this open across processes, reopen later", "\n", "            ", "self", ".", "samples", ",", "self", ".", "class_to_idx", "=", "extract_tarinfo", "(", "tf", ",", "class_to_idx", ")", "\n", "", "self", ".", "imgs", "=", "self", ".", "samples", "\n", "self", ".", "tarfile", "=", "None", "# lazy init in __getitem__", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_tar.ParserImageTar.__getitem__": [[58, 64], ["parser_image_tar.ParserImageTar.tarfile.extractfile", "tarfile.open"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "if", "self", ".", "tarfile", "is", "None", ":", "\n", "            ", "self", ".", "tarfile", "=", "tarfile", ".", "open", "(", "self", ".", "root", ")", "\n", "", "tarinfo", ",", "target", "=", "self", ".", "samples", "[", "index", "]", "\n", "fileobj", "=", "self", ".", "tarfile", ".", "extractfile", "(", "tarinfo", ")", "\n", "return", "fileobj", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_tar.ParserImageTar.__len__": [[65, 67], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_tar.ParserImageTar._filename": [[68, 73], ["os.path.basename"], "methods", ["None"], ["", "def", "_filename", "(", "self", ",", "index", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "filename", "=", "self", ".", "samples", "[", "index", "]", "[", "0", "]", ".", "name", "\n", "if", "basename", ":", "\n", "            ", "filename", "=", "os", ".", "path", ".", "basename", "(", "filename", ")", "\n", "", "return", "filename", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.parser_image_tar.extract_tarinfo": [[17, 37], ["tarfile.getmembers", "os.path.split", "os.path.basename", "set", "list", "sorted", "ti.isfile", "os.path.splitext", "ext.lower", "files.append", "labels.append", "sorted", "zip", "enumerate", "timm.utils.misc.natural_key"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.utils.misc.natural_key"], ["def", "extract_tarinfo", "(", "tarfile", ",", "class_to_idx", "=", "None", ",", "sort", "=", "True", ")", ":", "\n", "    ", "files", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "for", "ti", "in", "tarfile", ".", "getmembers", "(", ")", ":", "\n", "        ", "if", "not", "ti", ".", "isfile", "(", ")", ":", "\n", "            ", "continue", "\n", "", "dirname", ",", "basename", "=", "os", ".", "path", ".", "split", "(", "ti", ".", "path", ")", "\n", "label", "=", "os", ".", "path", ".", "basename", "(", "dirname", ")", "\n", "ext", "=", "os", ".", "path", ".", "splitext", "(", "basename", ")", "[", "1", "]", "\n", "if", "ext", ".", "lower", "(", ")", "in", "IMG_EXTENSIONS", ":", "\n", "            ", "files", ".", "append", "(", "ti", ")", "\n", "labels", ".", "append", "(", "label", ")", "\n", "", "", "if", "class_to_idx", "is", "None", ":", "\n", "        ", "unique_labels", "=", "set", "(", "labels", ")", "\n", "sorted_labels", "=", "list", "(", "sorted", "(", "unique_labels", ",", "key", "=", "natural_key", ")", ")", "\n", "class_to_idx", "=", "{", "c", ":", "idx", "for", "idx", ",", "c", "in", "enumerate", "(", "sorted_labels", ")", "}", "\n", "", "tarinfo_and_targets", "=", "[", "(", "f", ",", "class_to_idx", "[", "l", "]", ")", "for", "f", ",", "l", "in", "zip", "(", "files", ",", "labels", ")", "if", "l", "in", "class_to_idx", "]", "\n", "if", "sort", ":", "\n", "        ", "tarinfo_and_targets", "=", "sorted", "(", "tarinfo_and_targets", ",", "key", "=", "lambda", "k", ":", "natural_key", "(", "k", "[", "0", "]", ".", "path", ")", ")", "\n", "", "return", "tarinfo_and_targets", ",", "class_to_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.parsers.class_map.load_class_map": [[4, 19], ["isinstance", "[].lower", "os.path.exists", "os.path.join", "os.path.exists", "open", "os.path.splitext", "v.strip", "enumerate"], "function", ["None"], ["def", "load_class_map", "(", "map_or_filename", ",", "root", "=", "''", ")", ":", "\n", "    ", "if", "isinstance", "(", "map_or_filename", ",", "dict", ")", ":", "\n", "        ", "assert", "dict", ",", "'class_map dict must be non-empty'", "\n", "return", "map_or_filename", "\n", "", "class_map_path", "=", "map_or_filename", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "class_map_path", ")", ":", "\n", "        ", "class_map_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "class_map_path", ")", "\n", "assert", "os", ".", "path", ".", "exists", "(", "class_map_path", ")", ",", "'Cannot locate specified class map file (%s)'", "%", "map_or_filename", "\n", "", "class_map_ext", "=", "os", ".", "path", ".", "splitext", "(", "map_or_filename", ")", "[", "-", "1", "]", ".", "lower", "(", ")", "\n", "if", "class_map_ext", "==", "'.txt'", ":", "\n", "        ", "with", "open", "(", "class_map_path", ")", "as", "f", ":", "\n", "            ", "class_to_idx", "=", "{", "v", ".", "strip", "(", ")", ":", "k", "for", "k", ",", "v", "in", "enumerate", "(", "f", ")", "}", "\n", "", "", "else", ":", "\n", "        ", "assert", "False", ",", "f'Unsupported class map file extension ({class_map_ext}).'", "\n", "", "return", "class_to_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.jsd.JsdCrossEntropy.__init__": [[17, 25], ["torch.Module.__init__", "cross_entropy.LabelSmoothingCrossEntropy", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "num_splits", "=", "3", ",", "alpha", "=", "12", ",", "smoothing", "=", "0.1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_splits", "=", "num_splits", "\n", "self", ".", "alpha", "=", "alpha", "\n", "if", "smoothing", "is", "not", "None", "and", "smoothing", ">", "0", ":", "\n", "            ", "self", ".", "cross_entropy_loss", "=", "LabelSmoothingCrossEntropy", "(", "smoothing", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "cross_entropy_loss", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.jsd.JsdCrossEntropy.__call__": [[26, 40], ["torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "jsd.JsdCrossEntropy.cross_entropy_loss", "torch.clamp().log", "torch.clamp().log", "torch.clamp().log", "torch.clamp().log", "torch.clamp().log", "torch.clamp().log", "torch.clamp().log", "torch.clamp().log", "torch.clamp().log", "torch.softmax", "torch.softmax", "torch.softmax", "len", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "sum", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.kl_div", "torch.kl_div", "torch.kl_div", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["None"], ["", "", "def", "__call__", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "split_size", "=", "output", ".", "shape", "[", "0", "]", "//", "self", ".", "num_splits", "\n", "assert", "split_size", "*", "self", ".", "num_splits", "==", "output", ".", "shape", "[", "0", "]", "\n", "logits_split", "=", "torch", ".", "split", "(", "output", ",", "split_size", ")", "\n", "\n", "# Cross-entropy is only computed on clean images", "\n", "loss", "=", "self", ".", "cross_entropy_loss", "(", "logits_split", "[", "0", "]", ",", "target", "[", ":", "split_size", "]", ")", "\n", "probs", "=", "[", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "1", ")", "for", "logits", "in", "logits_split", "]", "\n", "\n", "# Clamp mixture distribution to avoid exploding KL divergence", "\n", "logp_mixture", "=", "torch", ".", "clamp", "(", "torch", ".", "stack", "(", "probs", ")", ".", "mean", "(", "axis", "=", "0", ")", ",", "1e-7", ",", "1", ")", ".", "log", "(", ")", "\n", "loss", "+=", "self", ".", "alpha", "*", "sum", "(", "[", "F", ".", "kl_div", "(", "\n", "logp_mixture", ",", "p_split", ",", "reduction", "=", "'batchmean'", ")", "for", "p_split", "in", "probs", "]", ")", "/", "len", "(", "probs", ")", "\n", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.cross_entropy.LabelSmoothingCrossEntropy.__init__": [[14, 19], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "self", ",", "smoothing", "=", "0.1", ")", ":", "\n", "        ", "super", "(", "LabelSmoothingCrossEntropy", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "smoothing", "<", "1.0", "\n", "self", ".", "smoothing", "=", "smoothing", "\n", "self", ".", "confidence", "=", "1.", "-", "smoothing", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.cross_entropy.LabelSmoothingCrossEntropy.forward": [[20, 27], ["torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "nll_loss.squeeze.squeeze.squeeze", "loss.mean", "torch.log_softmax.gather", "torch.log_softmax.mean", "target.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ",", "target", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "logprobs", "=", "F", ".", "log_softmax", "(", "x", ",", "dim", "=", "-", "1", ")", "\n", "nll_loss", "=", "-", "logprobs", ".", "gather", "(", "dim", "=", "-", "1", ",", "index", "=", "target", ".", "unsqueeze", "(", "1", ")", ")", "\n", "nll_loss", "=", "nll_loss", ".", "squeeze", "(", "1", ")", "\n", "smooth_loss", "=", "-", "logprobs", ".", "mean", "(", "dim", "=", "-", "1", ")", "\n", "loss", "=", "self", ".", "confidence", "*", "nll_loss", "+", "self", ".", "smoothing", "*", "smooth_loss", "\n", "return", "loss", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.cross_entropy.SoftTargetCrossEntropy.__init__": [[31, 33], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "SoftTargetCrossEntropy", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.cross_entropy.SoftTargetCrossEntropy.forward": [[34, 37], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum.mean", "torch.sum.mean", "torch.sum.mean", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ",", "target", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "loss", "=", "torch", ".", "sum", "(", "-", "target", "*", "F", ".", "log_softmax", "(", "x", ",", "dim", "=", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "return", "loss", ".", "mean", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.asymmetric_loss.AsymmetricLossMultiLabel.__init__": [[6, 14], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "gamma_neg", "=", "4", ",", "gamma_pos", "=", "1", ",", "clip", "=", "0.05", ",", "eps", "=", "1e-8", ",", "disable_torch_grad_focal_loss", "=", "False", ")", ":", "\n", "        ", "super", "(", "AsymmetricLossMultiLabel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "gamma_neg", "=", "gamma_neg", "\n", "self", ".", "gamma_pos", "=", "gamma_pos", "\n", "self", ".", "clip", "=", "clip", "\n", "self", ".", "disable_torch_grad_focal_loss", "=", "disable_torch_grad_focal_loss", "\n", "self", ".", "eps", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.asymmetric_loss.AsymmetricLossMultiLabel.forward": [[15, 51], ["torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "loss.sum", "xs_pos.clamp", "xs_neg.clamp", "torch._C.set_grad_enabled", "torch._C.set_grad_enabled", "torch._C.set_grad_enabled", "torch._C.set_grad_enabled", "torch._C.set_grad_enabled", "torch._C.set_grad_enabled", "torch._C.set_grad_enabled", "torch._C.set_grad_enabled"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.layers.activations.sigmoid"], ["", "def", "forward", "(", "self", ",", "x", ",", "y", ")", ":", "\n", "        ", "\"\"\"\"\n        Parameters\n        ----------\n        x: input logits\n        y: targets (multi-label binarized vector)\n        \"\"\"", "\n", "\n", "# Calculating Probabilities", "\n", "x_sigmoid", "=", "torch", ".", "sigmoid", "(", "x", ")", "\n", "xs_pos", "=", "x_sigmoid", "\n", "xs_neg", "=", "1", "-", "x_sigmoid", "\n", "\n", "# Asymmetric Clipping", "\n", "if", "self", ".", "clip", "is", "not", "None", "and", "self", ".", "clip", ">", "0", ":", "\n", "            ", "xs_neg", "=", "(", "xs_neg", "+", "self", ".", "clip", ")", ".", "clamp", "(", "max", "=", "1", ")", "\n", "\n", "# Basic CE calculation", "\n", "", "los_pos", "=", "y", "*", "torch", ".", "log", "(", "xs_pos", ".", "clamp", "(", "min", "=", "self", ".", "eps", ")", ")", "\n", "los_neg", "=", "(", "1", "-", "y", ")", "*", "torch", ".", "log", "(", "xs_neg", ".", "clamp", "(", "min", "=", "self", ".", "eps", ")", ")", "\n", "loss", "=", "los_pos", "+", "los_neg", "\n", "\n", "# Asymmetric Focusing", "\n", "if", "self", ".", "gamma_neg", ">", "0", "or", "self", ".", "gamma_pos", ">", "0", ":", "\n", "            ", "if", "self", ".", "disable_torch_grad_focal_loss", ":", "\n", "                ", "torch", ".", "_C", ".", "set_grad_enabled", "(", "False", ")", "\n", "", "pt0", "=", "xs_pos", "*", "y", "\n", "pt1", "=", "xs_neg", "*", "(", "1", "-", "y", ")", "# pt = p if t > 0 else 1-p", "\n", "pt", "=", "pt0", "+", "pt1", "\n", "one_sided_gamma", "=", "self", ".", "gamma_pos", "*", "y", "+", "self", ".", "gamma_neg", "*", "(", "1", "-", "y", ")", "\n", "one_sided_w", "=", "torch", ".", "pow", "(", "1", "-", "pt", ",", "one_sided_gamma", ")", "\n", "if", "self", ".", "disable_torch_grad_focal_loss", ":", "\n", "                ", "torch", ".", "_C", ".", "set_grad_enabled", "(", "True", ")", "\n", "", "loss", "*=", "one_sided_w", "\n", "\n", "", "return", "-", "loss", ".", "sum", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.asymmetric_loss.AsymmetricLossSingleLabel.__init__": [[54, 63], ["torch.Module.__init__", "torch.LogSoftmax", "torch.LogSoftmax"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["    ", "def", "__init__", "(", "self", ",", "gamma_pos", "=", "1", ",", "gamma_neg", "=", "4", ",", "eps", ":", "float", "=", "0.1", ",", "reduction", "=", "'mean'", ")", ":", "\n", "        ", "super", "(", "AsymmetricLossSingleLabel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "logsoftmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "-", "1", ")", "\n", "self", ".", "targets_classes", "=", "[", "]", "# prevent gpu repeated memory allocation", "\n", "self", ".", "gamma_pos", "=", "gamma_pos", "\n", "self", ".", "gamma_neg", "=", "gamma_neg", "\n", "self", ".", "reduction", "=", "reduction", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.asymmetric_loss.AsymmetricLossSingleLabel.forward": [[64, 98], ["asymmetric_loss.AsymmetricLossSingleLabel.logsoftmax", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "loss.mean.mean.sum", "inputs.size", "target.long().unsqueeze", "asymmetric_loss.AsymmetricLossSingleLabel.targets_classes.mul_().add_", "asymmetric_loss.AsymmetricLossSingleLabel.targets_classes.mul", "loss.mean.mean.mean", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "target.long", "asymmetric_loss.AsymmetricLossSingleLabel.targets_classes.mul_"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "target", ",", "reduction", "=", "None", ")", ":", "\n", "        ", "\"\"\"\"\n        Parameters\n        ----------\n        x: input logits\n        y: targets (1-hot vector)\n        \"\"\"", "\n", "\n", "num_classes", "=", "inputs", ".", "size", "(", ")", "[", "-", "1", "]", "\n", "log_preds", "=", "self", ".", "logsoftmax", "(", "inputs", ")", "\n", "self", ".", "targets_classes", "=", "torch", ".", "zeros_like", "(", "inputs", ")", ".", "scatter_", "(", "1", ",", "target", ".", "long", "(", ")", ".", "unsqueeze", "(", "1", ")", ",", "1", ")", "\n", "\n", "# ASL weights", "\n", "targets", "=", "self", ".", "targets_classes", "\n", "anti_targets", "=", "1", "-", "targets", "\n", "xs_pos", "=", "torch", ".", "exp", "(", "log_preds", ")", "\n", "xs_neg", "=", "1", "-", "xs_pos", "\n", "xs_pos", "=", "xs_pos", "*", "targets", "\n", "xs_neg", "=", "xs_neg", "*", "anti_targets", "\n", "asymmetric_w", "=", "torch", ".", "pow", "(", "1", "-", "xs_pos", "-", "xs_neg", ",", "\n", "self", ".", "gamma_pos", "*", "targets", "+", "self", ".", "gamma_neg", "*", "anti_targets", ")", "\n", "log_preds", "=", "log_preds", "*", "asymmetric_w", "\n", "\n", "if", "self", ".", "eps", ">", "0", ":", "# label smoothing", "\n", "            ", "self", ".", "targets_classes", ".", "mul_", "(", "1", "-", "self", ".", "eps", ")", ".", "add_", "(", "self", ".", "eps", "/", "num_classes", ")", "\n", "\n", "# loss calculation", "\n", "", "loss", "=", "-", "self", ".", "targets_classes", ".", "mul", "(", "log_preds", ")", "\n", "\n", "loss", "=", "loss", ".", "sum", "(", "dim", "=", "-", "1", ")", "\n", "if", "self", ".", "reduction", "==", "'mean'", ":", "\n", "            ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "\n", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__": [[16, 26], ["torch.Module.__init__", "binary_cross_entropy.BinaryCrossEntropy.register_buffer", "binary_cross_entropy.BinaryCrossEntropy.register_buffer"], "methods", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.__init__"], ["def", "__init__", "(", "\n", "self", ",", "smoothing", "=", "0.1", ",", "target_threshold", ":", "Optional", "[", "float", "]", "=", "None", ",", "weight", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "reduction", ":", "str", "=", "'mean'", ",", "pos_weight", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ")", ":", "\n", "        ", "super", "(", "BinaryCrossEntropy", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "0.", "<=", "smoothing", "<", "1.0", "\n", "self", ".", "smoothing", "=", "smoothing", "\n", "self", ".", "target_threshold", "=", "target_threshold", "\n", "self", ".", "reduction", "=", "reduction", "\n", "self", ".", "register_buffer", "(", "'weight'", ",", "weight", ")", "\n", "self", ".", "register_buffer", "(", "'pos_weight'", ",", "pos_weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.loss.binary_cross_entropy.BinaryCrossEntropy.forward": [[27, 48], ["torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "target.gt().to.gt().to.long().view", "torch.full().scatter_", "torch.full().scatter_", "torch.full().scatter_", "torch.full().scatter_", "torch.full().scatter_", "torch.full().scatter_", "torch.full().scatter_", "torch.full().scatter_", "torch.full().scatter_", "target.gt().to.gt().to.gt().to", "target.gt().to.gt().to.long", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "target.gt().to.gt().to.gt", "target.gt().to.gt().to.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ",", "target", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "assert", "x", ".", "shape", "[", "0", "]", "==", "target", ".", "shape", "[", "0", "]", "\n", "if", "target", ".", "shape", "!=", "x", ".", "shape", ":", "\n", "# NOTE currently assume smoothing or other label softening is applied upstream if targets are already sparse", "\n", "            ", "num_classes", "=", "x", ".", "shape", "[", "-", "1", "]", "\n", "# FIXME should off/on be different for smoothing w/ BCE? Other impl out there differ", "\n", "off_value", "=", "self", ".", "smoothing", "/", "num_classes", "\n", "on_value", "=", "1.", "-", "self", ".", "smoothing", "+", "off_value", "\n", "target", "=", "target", ".", "long", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "target", "=", "torch", ".", "full", "(", "\n", "(", "target", ".", "size", "(", ")", "[", "0", "]", ",", "num_classes", ")", ",", "\n", "off_value", ",", "\n", "device", "=", "x", ".", "device", ",", "dtype", "=", "x", ".", "dtype", ")", ".", "scatter_", "(", "1", ",", "target", ",", "on_value", ")", "\n", "", "if", "self", ".", "target_threshold", "is", "not", "None", ":", "\n", "# Make target 0, or 1 if threshold set", "\n", "            ", "target", "=", "target", ".", "gt", "(", "self", ".", "target_threshold", ")", ".", "to", "(", "dtype", "=", "target", ".", "dtype", ")", "\n", "", "return", "F", ".", "binary_cross_entropy_with_logits", "(", "\n", "x", ",", "target", ",", "\n", "self", ".", "weight", ",", "\n", "pos_weight", "=", "self", ".", "pos_weight", ",", "\n", "reduction", "=", "self", ".", "reduction", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.convert.convert_from_mxnet.convert": [[15, 72], ["gluoncv.model_zoo.get_model", "timm.create_model", "list", "zip", "zip", "timm.create_model.load_state_dict", "torch.save", "os.rename", "print", "timm.create_model.named_parameters", "mn.split", "tn.split", "print", "print", "all", "torch.from_numpy", "print", "print", "torch.from_numpy", "timm.create_model.state_dict", "open", "hashlib.sha256().hexdigest", "gluoncv.model_zoo.get_model.collect_params().items", "mv.data().asnumpy", "gluoncv.model_zoo.get_model.collect_params().items", "any", "timm.create_model.named_buffers", "mv.data().asnumpy", "hashlib.sha256", "gluoncv.model_zoo.get_model.collect_params", "zip", "mv.data", "gluoncv.model_zoo.get_model.collect_params", "mv.data", "f.read", "os.path.splitext"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.factory.create_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.optim.lookahead.Lookahead.state_dict"], ["def", "convert", "(", "mxnet_name", ",", "torch_name", ")", ":", "\n", "# download and load the pre-trained model", "\n", "    ", "net", "=", "gluoncv", ".", "model_zoo", ".", "get_model", "(", "mxnet_name", ",", "pretrained", "=", "True", ")", "\n", "\n", "# create corresponding torch model", "\n", "torch_net", "=", "create_model", "(", "torch_name", ")", "\n", "\n", "mxp", "=", "[", "(", "k", ",", "v", ")", "for", "k", ",", "v", "in", "net", ".", "collect_params", "(", ")", ".", "items", "(", ")", "if", "'running'", "not", "in", "k", "]", "\n", "torchp", "=", "list", "(", "torch_net", ".", "named_parameters", "(", ")", ")", "\n", "torch_params", "=", "{", "}", "\n", "\n", "# convert parameters", "\n", "# NOTE: we are relying on the fact that the order of parameters", "\n", "# are usually exactly the same between these models, thus no key name mapping", "\n", "# is necessary. Asserts will trip if this is not the case.", "\n", "for", "(", "tn", ",", "tv", ")", ",", "(", "mn", ",", "mv", ")", "in", "zip", "(", "torchp", ",", "mxp", ")", ":", "\n", "        ", "m_split", "=", "mn", ".", "split", "(", "'_'", ")", "\n", "t_split", "=", "tn", ".", "split", "(", "'.'", ")", "\n", "print", "(", "t_split", ",", "m_split", ")", "\n", "print", "(", "tv", ".", "shape", ",", "mv", ".", "shape", ")", "\n", "\n", "# ensure ordering of BN params match since their sizes are not specific", "\n", "if", "m_split", "[", "-", "1", "]", "==", "'gamma'", ":", "\n", "            ", "assert", "t_split", "[", "-", "1", "]", "==", "'weight'", "\n", "", "if", "m_split", "[", "-", "1", "]", "==", "'beta'", ":", "\n", "            ", "assert", "t_split", "[", "-", "1", "]", "==", "'bias'", "\n", "\n", "# ensure shapes match", "\n", "", "assert", "all", "(", "t", "==", "m", "for", "t", ",", "m", "in", "zip", "(", "tv", ".", "shape", ",", "mv", ".", "shape", ")", ")", "\n", "\n", "torch_tensor", "=", "torch", ".", "from_numpy", "(", "mv", ".", "data", "(", ")", ".", "asnumpy", "(", ")", ")", "\n", "torch_params", "[", "tn", "]", "=", "torch_tensor", "\n", "\n", "# convert buffers (batch norm running stats)", "\n", "", "mxb", "=", "[", "(", "k", ",", "v", ")", "for", "k", ",", "v", "in", "net", ".", "collect_params", "(", ")", ".", "items", "(", ")", "if", "any", "(", "x", "in", "k", "for", "x", "in", "[", "'running_mean'", ",", "'running_var'", "]", ")", "]", "\n", "torchb", "=", "[", "(", "k", ",", "v", ")", "for", "k", ",", "v", "in", "torch_net", ".", "named_buffers", "(", ")", "if", "'num_batches'", "not", "in", "k", "]", "\n", "for", "(", "tn", ",", "tv", ")", ",", "(", "mn", ",", "mv", ")", "in", "zip", "(", "torchb", ",", "mxb", ")", ":", "\n", "        ", "print", "(", "tn", ",", "mn", ")", "\n", "print", "(", "tv", ".", "shape", ",", "mv", ".", "shape", ")", "\n", "\n", "# ensure ordering of BN params match since their sizes are not specific", "\n", "if", "'running_var'", "in", "tn", ":", "\n", "            ", "assert", "'running_var'", "in", "mn", "\n", "", "if", "'running_mean'", "in", "tn", ":", "\n", "            ", "assert", "'running_mean'", "in", "mn", "\n", "\n", "", "torch_tensor", "=", "torch", ".", "from_numpy", "(", "mv", ".", "data", "(", ")", ".", "asnumpy", "(", ")", ")", "\n", "torch_params", "[", "tn", "]", "=", "torch_tensor", "\n", "\n", "", "torch_net", ".", "load_state_dict", "(", "torch_params", ")", "\n", "torch_filename", "=", "'./%s.pth'", "%", "torch_name", "\n", "torch", ".", "save", "(", "torch_net", ".", "state_dict", "(", ")", ",", "torch_filename", ")", "\n", "with", "open", "(", "torch_filename", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "sha_hash", "=", "hashlib", ".", "sha256", "(", "f", ".", "read", "(", ")", ")", ".", "hexdigest", "(", ")", "\n", "", "final_filename", "=", "os", ".", "path", ".", "splitext", "(", "torch_filename", ")", "[", "0", "]", "+", "'-'", "+", "sha_hash", "[", ":", "8", "]", "+", "'.pth'", "\n", "os", ".", "rename", "(", "torch_filename", ",", "final_filename", ")", "\n", "print", "(", "\"=> Saved converted model to '{}, SHA256: {}'\"", ".", "format", "(", "final_filename", ",", "sha_hash", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.convert.convert_from_mxnet.map_mx_to_torch_model": [[74, 84], ["mx_name.lower", "torch_name.replace.startswith", "torch_name.replace.replace", "torch_name.replace.startswith", "torch_name.replace.replace", "torch_name.replace.startswith", "torch_name.replace.replace"], "function", ["None"], ["", "def", "map_mx_to_torch_model", "(", "mx_name", ")", ":", "\n", "    ", "torch_name", "=", "mx_name", ".", "lower", "(", ")", "\n", "if", "torch_name", ".", "startswith", "(", "'se_'", ")", ":", "\n", "        ", "torch_name", "=", "torch_name", ".", "replace", "(", "'se_'", ",", "'se'", ")", "\n", "", "elif", "torch_name", ".", "startswith", "(", "'senet_'", ")", ":", "\n", "        ", "torch_name", "=", "torch_name", ".", "replace", "(", "'senet_'", ",", "'senet'", ")", "\n", "", "elif", "torch_name", ".", "startswith", "(", "'inceptionv3'", ")", ":", "\n", "        ", "torch_name", "=", "torch_name", ".", "replace", "(", "'inceptionv3'", ",", "'inception_v3'", ")", "\n", "", "torch_name", "=", "'gluon_'", "+", "torch_name", "\n", "return", "torch_name", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.convert.convert_from_mxnet.main": [[93, 104], ["parser.parse_args", "convert_from_mxnet.map_mx_to_torch_model", "convert_from_mxnet.convert", "convert_from_mxnet.map_mx_to_torch_model", "convert_from_mxnet.convert"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.convert.convert_from_mxnet.map_mx_to_torch_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.convert.convert_from_mxnet.convert", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.convert.convert_from_mxnet.map_mx_to_torch_model", "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.convert.convert_from_mxnet.convert"], ["def", "main", "(", ")", ":", "\n", "    ", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "not", "args", ".", "model", "or", "args", ".", "model", "==", "'all'", ":", "\n", "        ", "for", "mx_model", "in", "ALL", ":", "\n", "            ", "torch_model", "=", "map_mx_to_torch_model", "(", "mx_model", ")", "\n", "convert", "(", "mx_model", ",", "torch_model", ")", "\n", "", "", "else", ":", "\n", "        ", "mx_model", "=", "args", ".", "model", "\n", "torch_model", "=", "map_mx_to_torch_model", "(", "mx_model", ")", "\n", "convert", "(", "mx_model", ",", "torch_model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rwightman_pytorch-image-models.convert.convert_nest_flax.convert_nest": [[21, 104], ["torch.tensor().permute", "torch.tensor", "enumerate", "range", "range", "torch.tensor", "torch.tensor", "torch.tensor().permute", "torch.tensor", "torch.tensor", "len", "range", "len", "torch.tensor().permute", "torch.tensor", "torch.tensor", "torch.tensor", "clu.checkpoint.load_state_dict", "torch.tensor", "flax_dict.keys", "k.startswith", "range", "numpy.concatenate", "numpy.concatenate", "torch.tensor().flatten().permute", "numpy.concatenate", "numpy.concatenate", "torch.tensor().reshape", "torch.tensor().permute().flatten", "torch.tensor", "range", "torch.tensor", "sum", "torch.tensor", "torch.tensor", "numpy.split", "numpy.split", "torch.tensor().permute", "torch.tensor", "torch.tensor", "torch.tensor().flatten", "torch.tensor", "torch.tensor().permute", "torch.tensor", "torch.tensor", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.rwightman_pytorch-image-models.models.helpers.load_state_dict"], ["def", "convert_nest", "(", "checkpoint_path", ",", "arch", ")", ":", "\n", "    ", "\"\"\"\n    Expects path to checkpoint which is a dir containing 4 files like in each of these folders\n        - https://console.cloud.google.com/storage/browser/gresearch/nest-checkpoints\n    `arch` is needed to \n    Returns a state dict that can be used with `torch.nn.Module.load_state_dict`\n    Hint: Follow timm.models.nest.Nest.__init__ and \n    https://github.com/google-research/nested-transformer/blob/main/models/nest_net.py\n    \"\"\"", "\n", "assert", "arch", "in", "[", "'nest_base'", ",", "'nest_small'", ",", "'nest_tiny'", "]", ",", "\"Your `arch` is not supported\"", "\n", "\n", "flax_dict", "=", "checkpoint", ".", "load_state_dict", "(", "checkpoint_path", ")", "[", "'optimizer'", "]", "[", "'target'", "]", "\n", "state_dict", "=", "{", "}", "\n", "\n", "# Patch embedding", "\n", "state_dict", "[", "'patch_embed.proj.weight'", "]", "=", "torch", ".", "tensor", "(", "\n", "flax_dict", "[", "'PatchEmbedding_0'", "]", "[", "'Conv_0'", "]", "[", "'kernel'", "]", ")", ".", "permute", "(", "3", ",", "2", ",", "0", ",", "1", ")", "\n", "state_dict", "[", "'patch_embed.proj.bias'", "]", "=", "torch", ".", "tensor", "(", "flax_dict", "[", "'PatchEmbedding_0'", "]", "[", "'Conv_0'", "]", "[", "'bias'", "]", ")", "\n", "\n", "# Positional embeddings", "\n", "posemb_keys", "=", "[", "k", "for", "k", "in", "flax_dict", ".", "keys", "(", ")", "if", "k", ".", "startswith", "(", "'PositionEmbedding'", ")", "]", "\n", "for", "i", ",", "k", "in", "enumerate", "(", "posemb_keys", ")", ":", "\n", "        ", "state_dict", "[", "f'levels.{i}.pos_embed'", "]", "=", "torch", ".", "tensor", "(", "flax_dict", "[", "k", "]", "[", "'pos_embedding'", "]", ")", "\n", "\n", "# Transformer encoders", "\n", "", "depths", "=", "arch_depths", "[", "arch", "]", "\n", "for", "level", "in", "range", "(", "len", "(", "depths", ")", ")", ":", "\n", "        ", "for", "layer", "in", "range", "(", "depths", "[", "level", "]", ")", ":", "\n", "            ", "global_layer_ix", "=", "sum", "(", "depths", "[", ":", "level", "]", ")", "+", "layer", "\n", "# Norms", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "                ", "state_dict", "[", "f'levels.{level}.transformer_encoder.{layer}.norm{i+1}.weight'", "]", "=", "torch", ".", "tensor", "(", "\n", "flax_dict", "[", "f'EncoderNDBlock_{global_layer_ix}'", "]", "[", "f'LayerNorm_{i}'", "]", "[", "'scale'", "]", ")", "\n", "state_dict", "[", "f'levels.{level}.transformer_encoder.{layer}.norm{i+1}.bias'", "]", "=", "torch", ".", "tensor", "(", "\n", "flax_dict", "[", "f'EncoderNDBlock_{global_layer_ix}'", "]", "[", "f'LayerNorm_{i}'", "]", "[", "'bias'", "]", ")", "\n", "# Attention qkv", "\n", "", "w_q", "=", "flax_dict", "[", "f'EncoderNDBlock_{global_layer_ix}'", "]", "[", "'MultiHeadAttention_0'", "]", "[", "'DenseGeneral_0'", "]", "[", "'kernel'", "]", "\n", "w_kv", "=", "flax_dict", "[", "f'EncoderNDBlock_{global_layer_ix}'", "]", "[", "'MultiHeadAttention_0'", "]", "[", "'DenseGeneral_1'", "]", "[", "'kernel'", "]", "\n", "# Pay attention to dims here (maybe get pen and paper)", "\n", "w_kv", "=", "np", ".", "concatenate", "(", "np", ".", "split", "(", "w_kv", ",", "2", ",", "-", "1", ")", ",", "1", ")", "\n", "w_qkv", "=", "np", ".", "concatenate", "(", "[", "w_q", ",", "w_kv", "]", ",", "1", ")", "\n", "state_dict", "[", "f'levels.{level}.transformer_encoder.{layer}.attn.qkv.weight'", "]", "=", "torch", ".", "tensor", "(", "w_qkv", ")", ".", "flatten", "(", "1", ")", ".", "permute", "(", "1", ",", "0", ")", "\n", "b_q", "=", "flax_dict", "[", "f'EncoderNDBlock_{global_layer_ix}'", "]", "[", "'MultiHeadAttention_0'", "]", "[", "'DenseGeneral_0'", "]", "[", "'bias'", "]", "\n", "b_kv", "=", "flax_dict", "[", "f'EncoderNDBlock_{global_layer_ix}'", "]", "[", "'MultiHeadAttention_0'", "]", "[", "'DenseGeneral_1'", "]", "[", "'bias'", "]", "\n", "# Pay attention to dims here (maybe get pen and paper)", "\n", "b_kv", "=", "np", ".", "concatenate", "(", "np", ".", "split", "(", "b_kv", ",", "2", ",", "-", "1", ")", ",", "0", ")", "\n", "b_qkv", "=", "np", ".", "concatenate", "(", "[", "b_q", ",", "b_kv", "]", ",", "0", ")", "\n", "state_dict", "[", "f'levels.{level}.transformer_encoder.{layer}.attn.qkv.bias'", "]", "=", "torch", ".", "tensor", "(", "b_qkv", ")", ".", "reshape", "(", "-", "1", ")", "\n", "# Attention proj", "\n", "w_proj", "=", "flax_dict", "[", "f'EncoderNDBlock_{global_layer_ix}'", "]", "[", "'MultiHeadAttention_0'", "]", "[", "'proj_kernel'", "]", "\n", "w_proj", "=", "torch", ".", "tensor", "(", "w_proj", ")", ".", "permute", "(", "2", ",", "1", ",", "0", ")", ".", "flatten", "(", "1", ")", "\n", "state_dict", "[", "f'levels.{level}.transformer_encoder.{layer}.attn.proj.weight'", "]", "=", "w_proj", "\n", "state_dict", "[", "f'levels.{level}.transformer_encoder.{layer}.attn.proj.bias'", "]", "=", "torch", ".", "tensor", "(", "\n", "flax_dict", "[", "f'EncoderNDBlock_{global_layer_ix}'", "]", "[", "'MultiHeadAttention_0'", "]", "[", "'bias'", "]", ")", "\n", "# MLP", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "                ", "state_dict", "[", "f'levels.{level}.transformer_encoder.{layer}.mlp.fc{i+1}.weight'", "]", "=", "torch", ".", "tensor", "(", "\n", "flax_dict", "[", "f'EncoderNDBlock_{global_layer_ix}'", "]", "[", "'MlpBlock_0'", "]", "[", "f'Dense_{i}'", "]", "[", "'kernel'", "]", ")", ".", "permute", "(", "1", ",", "0", ")", "\n", "state_dict", "[", "f'levels.{level}.transformer_encoder.{layer}.mlp.fc{i+1}.bias'", "]", "=", "torch", ".", "tensor", "(", "\n", "flax_dict", "[", "f'EncoderNDBlock_{global_layer_ix}'", "]", "[", "'MlpBlock_0'", "]", "[", "f'Dense_{i}'", "]", "[", "'bias'", "]", ")", "\n", "\n", "# Block aggregations (ConvPool)", "\n", "", "", "", "for", "level", "in", "range", "(", "1", ",", "len", "(", "depths", ")", ")", ":", "\n", "# Convs", "\n", "        ", "state_dict", "[", "f'levels.{level}.pool.conv.weight'", "]", "=", "torch", ".", "tensor", "(", "\n", "flax_dict", "[", "f'ConvPool_{level-1}'", "]", "[", "'Conv_0'", "]", "[", "'kernel'", "]", ")", ".", "permute", "(", "3", ",", "2", ",", "0", ",", "1", ")", "\n", "state_dict", "[", "f'levels.{level}.pool.conv.bias'", "]", "=", "torch", ".", "tensor", "(", "\n", "flax_dict", "[", "f'ConvPool_{level-1}'", "]", "[", "'Conv_0'", "]", "[", "'bias'", "]", ")", "\n", "# Norms", "\n", "state_dict", "[", "f'levels.{level}.pool.norm.weight'", "]", "=", "torch", ".", "tensor", "(", "\n", "flax_dict", "[", "f'ConvPool_{level-1}'", "]", "[", "'LayerNorm_0'", "]", "[", "'scale'", "]", ")", "\n", "state_dict", "[", "f'levels.{level}.pool.norm.bias'", "]", "=", "torch", ".", "tensor", "(", "\n", "flax_dict", "[", "f'ConvPool_{level-1}'", "]", "[", "'LayerNorm_0'", "]", "[", "'bias'", "]", ")", "\n", "\n", "# Final norm", "\n", "", "state_dict", "[", "f'norm.weight'", "]", "=", "torch", ".", "tensor", "(", "flax_dict", "[", "'LayerNorm_0'", "]", "[", "'scale'", "]", ")", "\n", "state_dict", "[", "f'norm.bias'", "]", "=", "torch", ".", "tensor", "(", "flax_dict", "[", "'LayerNorm_0'", "]", "[", "'bias'", "]", ")", "\n", "\n", "# Classifier", "\n", "state_dict", "[", "'head.weight'", "]", "=", "torch", ".", "tensor", "(", "flax_dict", "[", "'Dense_0'", "]", "[", "'kernel'", "]", ")", ".", "permute", "(", "1", ",", "0", ")", "\n", "state_dict", "[", "'head.bias'", "]", "=", "torch", ".", "tensor", "(", "flax_dict", "[", "'Dense_0'", "]", "[", "'bias'", "]", ")", "\n", "\n", "return", "state_dict", "\n", "\n"]]}