{"home.repos.pwc.inspect_result.rabeehk_hyperformer.None.setup.setup_package": [[4, 30], ["setuptools.setup", "open", "f.read().splitlines", "setuptools.find_packages", "f.read"], "function", ["None"], ["def", "setup_package", "(", ")", ":", "\n", "  ", "long_description", "=", "\"hyperformer\"", "\n", "with", "open", "(", "'requirements.txt'", ")", "as", "f", ":", "\n", "      ", "required", "=", "f", ".", "read", "(", ")", ".", "splitlines", "(", ")", "\n", "", "setuptools", ".", "setup", "(", "\n", "name", "=", "'hyperformer'", ",", "\n", "version", "=", "'0.0.1'", ",", "\n", "description", "=", "'HyperFormer'", ",", "\n", "long_description", "=", "long_description", ",", "\n", "long_description_content_type", "=", "'text/markdown'", ",", "\n", "license", "=", "'MIT License'", ",", "\n", "packages", "=", "setuptools", ".", "find_packages", "(", "\n", "exclude", "=", "[", "'docs'", ",", "'tests'", ",", "'scripts'", ",", "'examples'", "]", ")", ",", "\n", "dependency_links", "=", "[", "\n", "'https://download.pytorch.org/whl/torch_stable.html'", ",", "\n", "]", ",", "\n", "install_requires", "=", "required", ",", "\n", "classifiers", "=", "[", "\n", "'Intended Audience :: Developers'", ",", "\n", "'Intended Audience :: Science/Research'", ",", "\n", "'License :: OSI Approved :: MIT License'", ",", "\n", "'Topic :: Scientific/Engineering :: Artificial Intelligence'", ",", "\n", "'Programming Language :: Python :: 3'", ",", "\n", "'Programming Language :: Python :: 3.7'", ",", "\n", "]", ",", "\n", "keywords", "=", "'text nlp machinelearning'", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.hyperformer.finetune_t5_trainer.remove_rank_info_from_argv": [[26, 32], ["args[].startswith", "extra_parameters.update", "int", "args[].split", "sys.argv"], "function", ["None"], ["def", "remove_rank_info_from_argv", "(", "args", ")", ":", "\n", "    ", "extra_parameters", "=", "{", "}", "\n", "if", "args", "[", "1", "]", ".", "startswith", "(", "\"--local_rank\"", ")", ":", "\n", "        ", "extra_parameters", ".", "update", "(", "{", "'local_rank'", ":", "int", "(", "args", "[", "1", "]", ".", "split", "(", "'='", ")", "[", "-", "1", "]", ")", "}", ")", "\n", "del", "args", "[", "1", "]", "\n", "", "return", "extra_parameters", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.hyperformer.finetune_t5_trainer.main": [[33, 299], ["transformers.HfArgumentParser", "hyperformer.third_party.utils.check_output_dir", "logging.basicConfig", "logger.warning", "logger.info", "transformers.set_seed", "hyperformer.third_party.models.T5Config.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "hyperformer.third_party.trainers.T5Trainer", "hyperformer.third_party.trainers.T5Trainer.is_world_process_zero", "sys.argv[].startswith", "sys.argv[].endswith", "finetune_t5_trainer.remove_rank_info_from_argv", "json.loads", "json.loads.update", "transformers.HfArgumentParser.parse_dict", "bool", "getattr", "hyperformer.adapters.AutoAdapterConfig.get", "hyperformer.third_party.models.T5ForConditionalGeneration", "logger.warning", "hyperformer.third_party.models.T5ForConditionalGeneration.from_pretrained", "hyperformer.utils.freezing_params", "logger.info", "T5ForConditionalGeneration.from_pretrained.named_parameters", "sum", "sum", "logger.info", "logger.info", "datasets.concatenate_datasets", "hyperformer.metrics.build_compute_metrics_fn", "hyperformer.utils.get_training_args", "hyperformer.utils.handle_metrics", "hyperformer.third_party.trainers.T5Trainer.is_world_process_zero", "hyperformer.third_party.trainers.T5Trainer.train", "hyperformer.third_party.trainers.T5Trainer.save_model", "hyperformer.third_party.trainers.T5Trainer.is_world_process_zero", "hyperformer.third_party.trainers.T5Trainer.is_world_process_zero", "hyperformer.third_party.trainers.T5Trainer.evaluate", "hyperformer.third_party.trainers.T5Trainer.is_world_process_zero", "hyperformer.third_party.trainers.T5Trainer.evaluate", "hyperformer.third_party.trainers.T5Trainer.is_world_process_zero", "torch.cuda.is_available", "print", "len", "pathlib.Path().read_text", "sys.argv[].endswith", "logger.warning", "transformers.HfArgumentParser.parse_json_file", "transformers.HfArgumentParser.parse_args_into_dataclasses", "hasattr", "setattr", "dataset_class.get().get_dataset", "len", "dataset_class.get().get_dataset", "dataset_class.get().get_dataset", "hyperformer.third_party.utils.TaskCollator", "torch.cuda.synchronize", "torch.cuda.Event", "torch.cuda.Event", "torch.cuda.Event.record", "torch.cuda.synchronize", "torch.cuda.Event.record", "print", "hyperformer.third_party.trainers.T5Trainer.state.save_to_json", "AutoTokenizer.from_pretrained.save_pretrained", "hyperformer.utils.get_last_checkpoint_path", "hyperformer.third_party.models.T5Config.from_pretrained", "hyperformer.third_party.models.T5ForConditionalGeneration.from_pretrained", "hyperformer.third_party.trainers.T5Trainer", "hyperformer.utils.handle_metrics", "all_metrics.update", "hyperformer.utils.handle_metrics", "all_metrics.update", "torch.cuda.max_memory_allocated", "len", "getattr", "hasattr", "hasattr", "setattr", "logger.warning", "logger.info", "p.numel", "p.numel", "torch.cuda.Event.elapsed_time", "os.path.join", "T5ForConditionalGeneration.from_pretrained.named_modules", "pathlib.Path", "os.path.abspath", "zip", "zip", "getattr", "os.path.exists", "T5ForConditionalGeneration.from_pretrained.parameters", "T5ForConditionalGeneration.from_pretrained.parameters", "dataset_class.get", "dataset_class.get", "dataset_class.get", "hyperformer.third_party.utils.TaskCollator", "isinstance", "os.path.join", "os.path.exists", "os.path.exists", "sub_module.set_task_to_adapter_map", "os.path.join", "zip"], "function", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.check_output_dir", "home.repos.pwc.inspect_result.rabeehk_hyperformer.hyperformer.finetune_t5_trainer.remove_rank_info_from_argv", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AutoTask.get", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.freezing_params", "home.repos.pwc.inspect_result.rabeehk_hyperformer.metrics.metrics.build_compute_metrics_fn", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.get_training_args", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.handle_metrics", "home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer.train", "home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer.evaluate", "home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer.evaluate", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.get_dataset", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.get_dataset", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.get_dataset", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.get_last_checkpoint_path", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.handle_metrics", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.handle_metrics", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AutoTask.get", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AutoTask.get", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AutoTask.get", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AdapterController.set_task_to_adapter_map"], ["", "def", "main", "(", ")", ":", "\n", "# See all possible arguments in src/transformers/training_args.py or by passing", "\n", "# the --help flag to this script. We now keep distinct sets of args, for a cleaner", "\n", "# separation of concerns.", "\n", "    ", "parser", "=", "HfArgumentParser", "(", "(", "ModelArguments", ",", "DataTrainingArguments", ",", "Seq2SeqTrainingArguments", ",", "AdapterTrainingArguments", ")", ")", "\n", "\n", "# For running on multiple gpus with torch.distributed.launch, it adds a local_rank paramter, to allow the parser", "\n", "# still use the config file, we add the local_rank to the config file.", "\n", "if", "len", "(", "sys", ".", "argv", ")", ">", "2", "and", "sys", ".", "argv", "[", "1", "]", ".", "startswith", "(", "\"--local_rank\"", ")", "and", "(", "sys", ".", "argv", "[", "2", "]", ".", "endswith", "(", "\".json\"", ")", ")", ":", "\n", "        ", "rank_info", "=", "remove_rank_info_from_argv", "(", "sys", ".", "argv", ")", "\n", "args_dict", "=", "json", ".", "loads", "(", "Path", "(", "sys", ".", "argv", "[", "1", "]", ")", ".", "read_text", "(", ")", ")", "\n", "args_dict", ".", "update", "(", "rank_info", ")", "\n", "model_args", ",", "data_args", ",", "training_args", ",", "adapter_args", "=", "parser", ".", "parse_dict", "(", "args_dict", ")", "\n", "", "elif", "len", "(", "sys", ".", "argv", ")", "==", "2", "and", "sys", ".", "argv", "[", "1", "]", ".", "endswith", "(", "\".json\"", ")", ":", "\n", "        ", "logger", ".", "warning", "(", "\"config path: %s\"", ",", "sys", ".", "argv", "[", "1", "]", ")", "\n", "# If we pass only one argument to the script and it's the path to a json file,", "\n", "# let's parse it to get our arguments.", "\n", "model_args", ",", "data_args", ",", "training_args", ",", "adapter_args", "=", "parser", ".", "parse_json_file", "(", "\n", "json_file", "=", "os", ".", "path", ".", "abspath", "(", "sys", ".", "argv", "[", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "        ", "model_args", ",", "data_args", ",", "training_args", ",", "adapter_args", "=", "parser", ".", "parse_args_into_dataclasses", "(", ")", "\n", "", "check_output_dir", "(", "training_args", ")", "\n", "\n", "# Setup logging", "\n", "logging", ".", "basicConfig", "(", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d/%Y %H:%M:%S\"", ",", "\n", "level", "=", "logging", ".", "INFO", "if", "training_args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", "else", "logging", ".", "WARN", ",", "\n", ")", "\n", "logger", ".", "warning", "(", "\n", "\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\"", ",", "\n", "training_args", ".", "local_rank", ",", "\n", "training_args", ".", "device", ",", "\n", "training_args", ".", "n_gpu", ",", "\n", "bool", "(", "training_args", ".", "local_rank", "!=", "-", "1", ")", ",", "\n", "training_args", ".", "fp16", ",", "\n", ")", "\n", "logger", ".", "info", "(", "\"Training/evaluation parameters %s\"", ",", "training_args", ")", "\n", "\n", "# Set seed", "\n", "set_seed", "(", "training_args", ".", "seed", ")", "\n", "\n", "# Load pretrained model and tokenizer", "\n", "#", "\n", "# Distributed training:", "\n", "# The .from_pretrained methods guarantee that only one local process can concurrently", "\n", "# download model & vocab.", "\n", "config", "=", "T5Config", ".", "from_pretrained", "(", "\n", "model_args", ".", "config_name", "if", "model_args", ".", "config_name", "else", "model_args", ".", "model_name_or_path", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "extra_model_params", "=", "(", "\"encoder_layerdrop\"", ",", "\"decoder_layerdrop\"", ",", "\"dropout\"", ",", "\n", "\"attention_dropout\"", ",", "\"train_adapters\"", ")", "\n", "for", "p", "in", "extra_model_params", ":", "\n", "        ", "if", "getattr", "(", "training_args", ",", "p", ",", "None", ")", ":", "\n", "            ", "assert", "hasattr", "(", "config", ",", "p", ")", ",", "f\"({config.__class__.__name__}) doesn't have a `{p}` attribute\"", "\n", "setattr", "(", "config", ",", "p", ",", "getattr", "(", "training_args", ",", "p", ")", ")", "\n", "\n", "# Gets the adapter config and updates the specified parameters.", "\n", "", "", "if", "training_args", ".", "train_adapters", ":", "\n", "        ", "adapter_config", "=", "AutoAdapterConfig", ".", "get", "(", "adapter_args", ".", "adapter_config_name", ")", "\n", "adapter_config", ".", "input_dim", "=", "config", ".", "d_model", "\n", "adapter_config", ".", "tasks", "=", "data_args", ".", "tasks", "\n", "adapter_config", ".", "task_to_adapter", "=", "{", "task", ":", "adapter", "for", "task", ",", "adapter", "in", "zip", "(", "data_args", ".", "tasks", ",", "data_args", ".", "adapters", ")", "}", "if", "data_args", ".", "adapters", "is", "not", "None", "else", "None", "\n", "# If this is a parametric task embedding this mapping makes sense, but in case we use any task embeddings,", "\n", "# then, we do not need any mapping as we use the pretrained task embeddings.", "\n", "adapter_config", ".", "task_to_embeddings", "=", "{", "task", ":", "embedding", "for", "task", ",", "embedding", "in", "zip", "(", "data_args", ".", "tasks", ",", "data_args", ".", "task_embeddings", ")", "}", "if", "(", "data_args", ".", "task_embeddings", "is", "not", "None", ")", "else", "None", "\n", "extra_adapter_params", "=", "(", "\"task_embedding_dim\"", ",", "\n", "\"add_layer_norm_before_adapter\"", ",", "\n", "\"add_layer_norm_after_adapter\"", ",", "\n", "\"reduction_factor\"", ",", "\n", "\"hidden_dim\"", ",", "\n", "\"non_linearity\"", ",", "\n", "\"train_task_embeddings\"", ",", "\n", "\"projected_task_embedding_dim\"", ",", "\n", "\"task_hidden_dim\"", ",", "\n", "\"conditional_layer_norm\"", ",", "\n", "\"train_adapters_blocks\"", ",", "\n", "\"unique_hyper_net\"", ",", "\n", "\"unique_hyper_net_layer_norm\"", ",", "\n", "\"efficient_unique_hyper_net\"", ")", "\n", "for", "p", "in", "extra_adapter_params", ":", "\n", "            ", "if", "hasattr", "(", "adapter_args", ",", "p", ")", "and", "hasattr", "(", "adapter_config", ",", "p", ")", ":", "\n", "                ", "setattr", "(", "adapter_config", ",", "p", ",", "getattr", "(", "adapter_args", ",", "p", ")", ")", "\n", "", "else", ":", "\n", "                ", "logger", ".", "warning", "(", "f\"({adapter_config.__class__.__name__}) doesn't have a `{p}` attribute\"", ")", "\n", "", "", "adapter_config", ".", "device", "=", "training_args", ".", "device", "\n", "", "else", ":", "\n", "        ", "adapter_config", "=", "None", "\n", "\n", "", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\n", "model_args", ".", "tokenizer_name", "if", "model_args", ".", "tokenizer_name", "else", "model_args", ".", "model_name_or_path", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "if", "model_args", ".", "not_load_t5_checkpoint", ":", "\n", "        ", "model", "=", "T5ForConditionalGeneration", "(", "config", "=", "config", ",", "adapter_config", "=", "adapter_config", ")", "\n", "", "else", ":", "\n", "        ", "last_checkpoint_path", "=", "training_args", ".", "output_dir", "\n", "model_path", "=", "model_args", ".", "model_name_or_path", "if", "(", "(", "training_args", ".", "optimize_from_scratch", "and", "not", "training_args", ".", "optimize_from_scratch_with_loading_model", ")", "or", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "last_checkpoint_path", ",", "'pytorch_model.bin'", ")", ")", ")", "else", "last_checkpoint_path", "\n", "logger", ".", "warning", "(", "\"model path loaded from : %s\"", ",", "model_path", ")", "\n", "model", "=", "T5ForConditionalGeneration", ".", "from_pretrained", "(", "\n", "model_path", ",", "\n", "from_tf", "=", "\".ckpt\"", "in", "model_args", ".", "model_name_or_path", ",", "\n", "config", "=", "config", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", "adapter_config", "=", "adapter_config", "\n", ")", "\n", "\n", "# set num_beams for evaluation", "\n", "", "if", "data_args", ".", "eval_beams", "is", "None", ":", "\n", "        ", "data_args", ".", "eval_beams", "=", "model", ".", "config", ".", "num_beams", "\n", "\n", "# freezing the parameters.", "\n", "", "if", "training_args", ".", "do_train", ":", "\n", "        ", "freezing_params", "(", "model", ",", "training_args", ",", "model_args", ",", "adapter_args", ")", "\n", "\n", "", "if", "training_args", ".", "print_num_parameters", ":", "\n", "        ", "logger", ".", "info", "(", "model", ")", "\n", "for", "name", ",", "param", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "param", ".", "requires_grad", ":", "\n", "                ", "logger", ".", "info", "(", "\"Parameter name %s\"", ",", "name", ")", "\n", "", "", "total_trainable_params", "=", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", ")", "\n", "total_params", "=", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", ")", "\n", "logger", ".", "info", "(", "\"Total trainable parameters %s\"", ",", "total_trainable_params", ")", "\n", "logger", ".", "info", "(", "\"Total parameters %s\"", ",", "total_params", ")", "\n", "# Gets the training/test/validation datasets.", "\n", "", "dataset_class", "=", "AutoTask", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "train_datasets", "=", "[", "dataset_class", ".", "get", "(", "task", ",", "seed", "=", "data_args", ".", "data_seed", ")", ".", "get_dataset", "(", "\n", "split", "=", "\"train\"", ",", "n_obs", "=", "data_args", ".", "n_train", ",", "add_prefix", "=", "False", "if", "training_args", ".", "train_adapters", "else", "True", ")", "\n", "for", "task", "in", "data_args", ".", "tasks", "]", "\n", "dataset_sizes", "=", "[", "len", "(", "train_dataset", ")", "for", "train_dataset", "in", "train_datasets", "]", "\n", "train_dataset", "=", "datasets", ".", "concatenate_datasets", "(", "train_datasets", ")", "\n", "", "training_args", ".", "remove_unused_columns", "=", "False", "\n", "eval_datasets", "=", "(", "{", "task", ":", "dataset_class", ".", "get", "(", "task", ",", "seed", "=", "data_args", ".", "data_seed", ")", ".", "get_dataset", "(", "\n", "split", "=", "\"validation\"", ",", "n_obs", "=", "data_args", ".", "n_val", ",", "\n", "add_prefix", "=", "False", "if", "training_args", ".", "train_adapters", "else", "True", ",", "\n", "split_validation_test", "=", "training_args", ".", "split_validation_test", ")", "\n", "for", "task", "in", "data_args", ".", "eval_tasks", "}", "\n", "if", "training_args", ".", "do_eval", "or", "training_args", ".", "evaluation_strategy", "!=", "EvaluationStrategy", ".", "NO", "\n", "else", "None", ")", "\n", "test_dataset", "=", "(", "\n", "{", "task", ":", "dataset_class", ".", "get", "(", "task", ",", "seed", "=", "data_args", ".", "data_seed", ")", ".", "get_dataset", "(", "\n", "split", "=", "\"test\"", ",", "n_obs", "=", "data_args", ".", "n_test", ",", "\n", "add_prefix", "=", "False", "if", "training_args", ".", "train_adapters", "else", "True", ",", "\n", "split_validation_test", "=", "training_args", ".", "split_validation_test", ")", "\n", "for", "task", "in", "data_args", ".", "eval_tasks", "}", "if", "training_args", ".", "do_test", "else", "None", "\n", ")", "\n", "# Defines the metrics for evaluation.", "\n", "compute_metrics_fn", "=", "(", "\n", "build_compute_metrics_fn", "(", "data_args", ".", "eval_tasks", ",", "tokenizer", ")", "if", "training_args", ".", "predict_with_generate", "else", "None", "\n", ")", "\n", "# Defines the trainer.", "\n", "trainer", "=", "T5Trainer", "(", "\n", "model", "=", "model", ",", "\n", "config", "=", "config", ",", "\n", "args", "=", "training_args", ",", "\n", "train_dataset", "=", "train_dataset", "if", "training_args", ".", "do_train", "else", "None", ",", "\n", "eval_dataset", "=", "eval_datasets", ",", "\n", "data_collator", "=", "TaskCollator", "(", "tokenizer", ",", "data_args", ",", "tpu_num_cores", "=", "training_args", ".", "tpu_num_cores", ")", ",", "\n", "compute_metrics", "=", "None", ",", "\n", "multi_task_compute_metrics", "=", "compute_metrics_fn", ",", "\n", "data_args", "=", "data_args", ",", "\n", "dataset_sizes", "=", "dataset_sizes", "if", "training_args", ".", "do_train", "else", "None", ",", "\n", "adapter_config", "=", "adapter_config", "\n", ")", "\n", "if", "trainer", ".", "is_world_process_zero", "(", ")", ":", "\n", "        ", "arguments", "=", "get_training_args", "(", "[", "model_args", ",", "data_args", ",", "training_args", ",", "adapter_args", "]", ")", "\n", "handle_metrics", "(", "\"arguments\"", ",", "arguments", ",", "training_args", ".", "output_dir", ")", "\n", "\n", "# Trains the model.", "\n", "", "if", "training_args", ".", "do_train", ":", "\n", "        ", "if", "trainer", ".", "is_world_process_zero", "(", ")", ":", "\n", "           ", "last_checkpoint_path", "=", "training_args", ".", "output_dir", "\n", "model_path", "=", "model_args", ".", "model_name_or_path", "if", "(", "training_args", ".", "optimize_from_scratch", "or", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "last_checkpoint_path", ",", "'pytorch_model.bin'", ")", ")", ")", "else", "last_checkpoint_path", "\n", "", "if", "training_args", ".", "compute_time", ":", "\n", "           ", "torch", ".", "cuda", ".", "synchronize", "(", ")", "# wait for move to complete", "\n", "start", "=", "torch", ".", "cuda", ".", "Event", "(", "enable_timing", "=", "True", ")", "\n", "end", "=", "torch", ".", "cuda", ".", "Event", "(", "enable_timing", "=", "True", ")", "\n", "start", ".", "record", "(", ")", "\n", "", "trainer", ".", "train", "(", "\n", "#get_last_checkpoint_path(training_args.output_dir) \\", "\n", "model_path", "=", "model_path", "if", "(", "os", ".", "path", ".", "exists", "(", "training_args", ".", "output_dir", ")", "and", "not", "training_args", ".", "optimize_from_scratch", ")", "else", "None", ",", "\n", ")", "\n", "if", "training_args", ".", "compute_time", ":", "\n", "           ", "torch", ".", "cuda", ".", "synchronize", "(", ")", "# wait for all_reduce to complete", "\n", "end", ".", "record", "(", ")", "\n", "total_time", "=", "{", "\"total_time\"", ":", "start", ".", "elapsed_time", "(", "end", ")", "}", "\n", "print", "(", "\"###### total_time \"", ",", "total_time", ")", "\n", "", "trainer", ".", "save_model", "(", ")", "\n", "# For convenience, we also re-save the tokenizer to the same directory,", "\n", "# so that you can share your model easily on huggingface.co/models =)", "\n", "if", "trainer", ".", "is_world_process_zero", "(", ")", ":", "\n", "            ", "trainer", ".", "state", ".", "save_to_json", "(", "os", ".", "path", ".", "join", "(", "training_args", ".", "output_dir", ",", "\"trainer_state.json\"", ")", ")", "\n", "tokenizer", ".", "save_pretrained", "(", "training_args", ".", "output_dir", ")", "\n", "\n", "# Evaluation", "\n", "", "", "all_metrics", "=", "{", "}", "\n", "if", "training_args", ".", "do_eval", "or", "training_args", ".", "do_test", ":", "\n", "        ", "if", "trainer", ".", "is_world_process_zero", "(", ")", ":", "\n", "# By default we load  the model from last checkpoint path,", "\n", "# in case of saving the model with the best metrics, make sure to", "\n", "# set save_total = 1 so the best model is loaded here.", "\n", "# if not exists returns the path to the output_dir.", "\n", "            ", "last_checkpoint_path", "=", "get_last_checkpoint_path", "(", "training_args", ".", "output_dir", ")", "\n", "config", "=", "T5Config", ".", "from_pretrained", "(", "\n", "last_checkpoint_path", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "\n", "model", "=", "T5ForConditionalGeneration", ".", "from_pretrained", "(", "\n", "last_checkpoint_path", ",", "\n", "from_tf", "=", "\".ckpt\"", "in", "training_args", ".", "output_dir", ",", "\n", "config", "=", "config", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", "adapter_config", "=", "adapter_config", "\n", ")", "\n", "# NOTE: if trainer is not re-defined, there is a bug in the codes, that making", "\n", "# huggingface codes does not using the best checkpoint.", "\n", "trainer", "=", "T5Trainer", "(", "\n", "model", "=", "model", ",", "\n", "config", "=", "config", ",", "\n", "args", "=", "training_args", ",", "\n", "train_dataset", "=", "train_dataset", "if", "training_args", ".", "do_train", "else", "None", ",", "\n", "eval_dataset", "=", "eval_datasets", ",", "\n", "data_collator", "=", "TaskCollator", "(", "tokenizer", ",", "data_args", ",", "tpu_num_cores", "=", "training_args", ".", "tpu_num_cores", ")", ",", "\n", "compute_metrics", "=", "None", ",", "\n", "multi_task_compute_metrics", "=", "compute_metrics_fn", ",", "\n", "data_args", "=", "data_args", ",", "\n", "dataset_sizes", "=", "dataset_sizes", "if", "training_args", ".", "do_train", "else", "None", ",", "\n", "adapter_config", "=", "adapter_config", "\n", ")", "\n", "\n", "", "if", "training_args", ".", "train_adapters", ":", "\n", "            ", "if", "adapter_args", ".", "adapter_config_name", "==", "\"adapter\"", "and", "data_args", ".", "adapters", "is", "not", "None", ":", "\n", "                ", "for", "name", ",", "sub_module", "in", "model", ".", "named_modules", "(", ")", ":", "\n", "                    ", "task_to_adapter", "=", "{", "eval_task", ":", "adapter", "for", "eval_task", ",", "adapter", "in", "\n", "zip", "(", "data_args", ".", "eval_tasks", ",", "data_args", ".", "adapters", ")", "}", "\n", "if", "isinstance", "(", "sub_module", ",", "AdapterController", ")", ":", "\n", "                        ", "sub_module", ".", "set_task_to_adapter_map", "(", "task_to_adapter", ")", "\n", "\n", "", "", "", "", "", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "metrics", "=", "trainer", ".", "evaluate", "(", ")", "\n", "if", "trainer", ".", "is_world_process_zero", "(", ")", ":", "\n", "            ", "handle_metrics", "(", "\"val\"", ",", "metrics", ",", "training_args", ".", "output_dir", ")", "\n", "all_metrics", ".", "update", "(", "metrics", ")", "\n", "\n", "", "", "if", "training_args", ".", "do_test", ":", "\n", "        ", "metrics", "=", "trainer", ".", "evaluate", "(", "test_dataset", ")", "\n", "if", "trainer", ".", "is_world_process_zero", "(", ")", ":", "\n", "            ", "handle_metrics", "(", "\"test\"", ",", "metrics", ",", "training_args", ".", "output_dir", ")", "\n", "all_metrics", ".", "update", "(", "metrics", ")", "\n", "\n", "", "", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "training_args", ".", "compute_memory", ":", "\n", "        ", "peak_memory", "=", "torch", ".", "cuda", ".", "max_memory_allocated", "(", ")", "/", "1024", "**", "2", "\n", "print", "(", "\n", "\"Memory utilization\"", ",", "\n", "peak_memory", ",", "\n", "\"MB\"", "\n", ")", "\n", "memory_usage", "=", "{", "\"peak_memory\"", ":", "peak_memory", "}", "\n", "", "return", "all_metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.hyperformer.finetune_t5_trainer._mp_fn": [[301, 304], ["finetune_t5_trainer.main"], "function", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.hyperformer.finetune_t5_trainer.main"], ["", "def", "_mp_fn", "(", "index", ")", ":", "\n", "# For xla_spawn (TPUs)", "\n", "    ", "main", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.create_dir": [[19, 27], ["os.path.exists", "os.makedirs"], "function", ["None"], ["def", "create_dir", "(", "output_dir", ")", ":", "\n", "    ", "\"\"\"\n    Checks whether to the output_dir already exists and creates it if not.\n    Args:\n      output_dir: path to the output_dir\n    \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "output_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.handle_metrics": [[29, 43], ["logger.info", "sorted", "utils.save_json_file", "metrics.keys", "logger.info"], "function", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.save_json_file"], ["", "", "def", "handle_metrics", "(", "split", ",", "metrics", ",", "output_dir", ")", ":", "#, gcs_bucket=None):", "\n", "    ", "\"\"\"\n    Prints and saves metrics or a general dictionary of results.\n\n    Args:\n        split: one of train, val, test, or training arguments.\n        metrics: metrics dict\n        output_dir: where to save the metrics, if gcs_bucket is given\n        we save the results also on the given bucket.\n    \"\"\"", "\n", "logger", ".", "info", "(", "f\"***** {split} metrics *****\"", ")", "\n", "for", "key", "in", "sorted", "(", "metrics", ".", "keys", "(", ")", ")", ":", "\n", "        ", "logger", ".", "info", "(", "f\"  {key} = {metrics[key]}\"", ")", "\n", "", "save_json_file", "(", "metrics", ",", "f\"{split}_results.json\"", ",", "output_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.save_json_file": [[45, 51], ["hyperformer.third_party.utils.save_json", "os.path.join"], "function", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.save_json"], ["", "def", "save_json_file", "(", "json_dict", ",", "outfile_name", ",", "output_dir", ")", ":", "\n", "    ", "\"\"\"\n    Saves the given dictionary as a json file to output_dir and also\n    the given bucket if given.\n    \"\"\"", "\n", "save_json", "(", "json_dict", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "outfile_name", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.get_training_args": [[53, 67], ["all_arguments.pop", "all_arguments.update", "dataclasses.asdict"], "function", ["None"], ["", "def", "get_training_args", "(", "arguments_list", ")", ":", "\n", "    ", "\"\"\"\n    Concatenate all training arguments except evaluation strategy which\n    is not Json serializable.\n    Args:\n        arguments_list: list of dataclasses.\n    Return:\n        arguments: concatenated arguments.\n    \"\"\"", "\n", "all_arguments", "=", "{", "}", "\n", "for", "arguments", "in", "arguments_list", ":", "\n", "        ", "all_arguments", ".", "update", "(", "asdict", "(", "arguments", ")", ")", "\n", "", "all_arguments", ".", "pop", "(", "\"evaluation_strategy\"", ")", "\n", "return", "all_arguments", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.get_last_checkpoint_path": [[69, 84], ["glob.glob", "os.path.join", "len", "max", "os.path.join", "int", "str", "checkpoint.split"], "function", ["None"], ["", "def", "get_last_checkpoint_path", "(", "output_dir", ")", ":", "\n", "    ", "\"\"\"\n    Finds the path for the last checkpoint saved in the output_dir\n    Args:\n        output_dir:  output_dir\n    Returns:\n        path to the last checkpoint saved in the output dir.\n    \"\"\"", "\n", "paths", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"checkpoint-*\"", ")", ")", "\n", "if", "len", "(", "paths", ")", "==", "0", ":", "\n", "        ", "return", "output_dir", "\n", "", "else", ":", "\n", "        ", "checkpoints", "=", "[", "int", "(", "checkpoint", ".", "split", "(", "'-'", ")", "[", "-", "1", "]", ")", "for", "checkpoint", "in", "paths", "]", "\n", "max_checkpoint", "=", "max", "(", "checkpoints", ")", "\n", "return", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"checkpoint-\"", "+", "str", "(", "max_checkpoint", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.use_task_specific_params": [[432, 440], ["task_specific_params.get", "logger.info", "model.config.update"], "function", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AutoTask.get"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.reset_config": [[95, 99], ["logger.info"], "function", ["None"], ["", "", "def", "reset_config", "(", "model", ",", "config", ")", ":", "\n", "    ", "\"\"\"Resets the config file to the one provided.\"\"\"", "\n", "model", ".", "config", "=", "config", "\n", "logger", ".", "info", "(", "f\"config is reset to the initial values.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.freezing_params": [[101, 171], ["hyperformer.third_party.utils.freeze_params", "model.named_modules", "hyperformer.third_party.utils.freeze_params", "hyperformer.third_party.utils.freeze_params", "model.lm_head.parameters", "hyperformer.third_party.utils.freeze_embeds", "hyperformer.third_party.utils.freeze_params", "hyperformer.third_party.utils.assert_all_frozen", "hyperformer.third_party.utils.freeze_params", "model.lm_head.parameters", "model.named_modules", "model.parameters", "isinstance", "model.task_embedding_controller.parameters", "model.named_modules", "model.named_modules", "model.get_encoder", "model.get_encoder", "model.task_embedding_controller.task_to_embeddings.parameters", "isinstance", "sub_module.named_parameters", "isinstance", "isinstance", "isinstance", "sub_module.named_parameters", "sub_module.named_parameters", "sub_module.named_parameters"], "function", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.freeze_params", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.freeze_params", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.freeze_params", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.freeze_embeds", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.freeze_params", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.assert_all_frozen", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.freeze_params", "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5ForConditionalGeneration.get_encoder", "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5ForConditionalGeneration.get_encoder"], ["", "def", "freezing_params", "(", "model", ",", "training_args", ",", "model_args", ",", "adapter_args", ")", ":", "\n", "    ", "\"\"\"\n    Freezes the model parameters based on the given setting in the arguments.\n    Args:\n      model: the given model.\n      training_args: defines the training arguments.\n      model_args: defines the model arguments.\n      adapter_args: defines the adapters arguments.\n    \"\"\"", "\n", "# If we are training adapters, we freeze all parameters except the", "\n", "# parameters of computing task embeddings and adapter controllers.", "\n", "if", "training_args", ".", "train_adapters", ":", "\n", "        ", "freeze_params", "(", "model", ")", "\n", "for", "name", ",", "sub_module", "in", "model", ".", "named_modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "sub_module", ",", "(", "MetaAdapterController", ",", "AdapterController", ")", ")", ":", "\n", "                ", "for", "param_name", ",", "param", "in", "sub_module", ".", "named_parameters", "(", ")", ":", "\n", "                    ", "param", ".", "requires_grad", "=", "True", "\n", "", "", "", "if", "adapter_args", ".", "adapter_config_name", "==", "\"meta-adapter\"", ":", "\n", "            ", "for", "param", "in", "model", ".", "task_embedding_controller", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "True", "\n", "", "", "if", "adapter_args", ".", "unique_hyper_net", ":", "\n", "            ", "for", "name", ",", "sub_module", "in", "model", ".", "named_modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "sub_module", ",", "(", "AdapterLayersHyperNetController", ",", "AdapterController", ")", ")", ":", "\n", "                    ", "for", "param_name", ",", "param", "in", "sub_module", ".", "named_parameters", "(", ")", ":", "\n", "                        ", "param", ".", "requires_grad", "=", "True", "\n", "", "", "", "", "if", "adapter_args", ".", "efficient_unique_hyper_net", ":", "\n", "            ", "for", "name", ",", "sub_module", "in", "model", ".", "named_modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "sub_module", ",", "(", "AdapterLayersOneHyperNetController", ")", ")", ":", "\n", "                    ", "for", "param_name", ",", "param", "in", "sub_module", ".", "named_parameters", "(", ")", ":", "\n", "                        ", "param", ".", "requires_grad", "=", "True", "\n", "", "", "", "", "", "if", "model_args", ".", "freeze_model", ":", "\n", "        ", "freeze_params", "(", "model", ")", "\n", "\n", "# Freezes all models parameters except last linear layer of decoder.", "\n", "", "if", "model_args", ".", "freeze_model_but_lm_head", ":", "\n", "        ", "freeze_params", "(", "model", ")", "\n", "for", "param", "in", "model", ".", "lm_head", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "True", "\n", "\n", "", "", "if", "model_args", ".", "freeze_embeds", ":", "\n", "        ", "freeze_embeds", "(", "model", ")", "\n", "\n", "", "if", "model_args", ".", "freeze_encoder", ":", "\n", "        ", "freeze_params", "(", "model", ".", "get_encoder", "(", ")", ")", "\n", "assert_all_frozen", "(", "model", ".", "get_encoder", "(", ")", ")", "\n", "\n", "# In case of meta-adapters and if task-embeddings are paramteric,", "\n", "# freezes all parameters except task-embedding parameters.", "\n", "", "if", "model_args", ".", "freeze_model_but_task_embeddings", ":", "\n", "        ", "freeze_params", "(", "model", ")", "\n", "if", "adapter_args", ".", "adapter_config_name", "==", "\"meta-adapter\"", "and", "not", "isinstance", "(", "model", ".", "task_embedding_controller", ".", "task_to_embeddings", ",", "dict", ")", ":", "\n", "            ", "for", "param", "in", "model", ".", "task_embedding_controller", ".", "task_to_embeddings", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "True", "\n", "\n", "# Unfreezes last linear layer of decoder.", "\n", "", "", "", "if", "model_args", ".", "unfreeze_lm_head", ":", "\n", "        ", "for", "param", "in", "model", ".", "lm_head", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "True", "\n", "\n", "# Unfreezes layer norms.", "\n", "", "", "if", "model_args", ".", "unfreeze_layer_norms", ":", "\n", "        ", "for", "name", ",", "sub_module", "in", "model", ".", "named_modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "sub_module", ",", "T5LayerNorm", ")", ":", "\n", "                ", "for", "param_name", ",", "param", "in", "sub_module", ".", "named_parameters", "(", ")", ":", "\n", "                    ", "param", ".", "requires_grad", "=", "True", "\n", "\n", "", "", "", "", "if", "model_args", ".", "unfreeze_model", ":", "\n", "        ", "for", "param", "in", "model", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "True", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.sentence_splitter.add_newline_to_end_of_each_sentence": [[30, 35], ["re.sub", "nltk.sent_tokenize"], "function", ["None"], ["", "", "def", "add_newline_to_end_of_each_sentence", "(", "x", ":", "str", ")", "->", "str", ":", "\n", "    ", "\"\"\"This was added to get rougeLsum scores matching published rougeL scores for BART and PEGASUS.\"\"\"", "\n", "re", ".", "sub", "(", "\"<n>\"", ",", "\"\"", ",", "x", ")", "# remove pegasus newline char", "\n", "assert", "NLTK_AVAILABLE", ",", "\"nltk must be installed to separate newlines between sentences. (pip install nltk)\"", "\n", "return", "\"\\n\"", ".", "join", "(", "nltk", ".", "sent_tokenize", "(", "x", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.AbstractSeq2SeqDataset.__init__": [[124, 156], ["torch.utils.data.Dataset.__init__", "pathlib.Path().joinpath", "pathlib.Path().joinpath", "pathlib.Path().joinpath", "os.path.exists", "dataset_kwargs.update", "utils.pickle_load", "utils.AbstractSeq2SeqDataset.get_char_lens", "min", "pathlib.Path", "pathlib.Path", "pathlib.Path", "isinstance"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.pickle_load", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.AbstractSeq2SeqDataset.get_char_lens"], ["                    ", "for", "param_name", ",", "param", "in", "sub_module", ".", "named_parameters", "(", ")", ":", "\n", "                        ", "param", ".", "requires_grad", "=", "True", "\n", "", "", "", "", "if", "adapter_args", ".", "efficient_unique_hyper_net", ":", "\n", "            ", "for", "name", ",", "sub_module", "in", "model", ".", "named_modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "sub_module", ",", "(", "AdapterLayersOneHyperNetController", ")", ")", ":", "\n", "                    ", "for", "param_name", ",", "param", "in", "sub_module", ".", "named_parameters", "(", ")", ":", "\n", "                        ", "param", ".", "requires_grad", "=", "True", "\n", "", "", "", "", "", "if", "model_args", ".", "freeze_model", ":", "\n", "        ", "freeze_params", "(", "model", ")", "\n", "\n", "# Freezes all models parameters except last linear layer of decoder.", "\n", "", "if", "model_args", ".", "freeze_model_but_lm_head", ":", "\n", "        ", "freeze_params", "(", "model", ")", "\n", "for", "param", "in", "model", ".", "lm_head", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "True", "\n", "\n", "", "", "if", "model_args", ".", "freeze_embeds", ":", "\n", "        ", "freeze_embeds", "(", "model", ")", "\n", "\n", "", "if", "model_args", ".", "freeze_encoder", ":", "\n", "        ", "freeze_params", "(", "model", ".", "get_encoder", "(", ")", ")", "\n", "assert_all_frozen", "(", "model", ".", "get_encoder", "(", ")", ")", "\n", "\n", "# In case of meta-adapters and if task-embeddings are paramteric,", "\n", "# freezes all parameters except task-embedding parameters.", "\n", "", "if", "model_args", ".", "freeze_model_but_task_embeddings", ":", "\n", "        ", "freeze_params", "(", "model", ")", "\n", "if", "adapter_args", ".", "adapter_config_name", "==", "\"meta-adapter\"", "and", "not", "isinstance", "(", "model", ".", "task_embedding_controller", ".", "task_to_embeddings", ",", "dict", ")", ":", "\n", "            ", "for", "param", "in", "model", ".", "task_embedding_controller", ".", "task_to_embeddings", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "True", "\n", "\n", "# Unfreezes last linear layer of decoder.", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.AbstractSeq2SeqDataset.__len__": [[157, 159], ["len"], "methods", ["None"], ["", "", "", "if", "model_args", ".", "unfreeze_lm_head", ":", "\n", "        ", "for", "param", "in", "model", ".", "lm_head", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "True", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.AbstractSeq2SeqDataset.get_char_lens": [[160, 163], ["len", "pathlib.Path().open().readlines", "pathlib.Path().open", "pathlib.Path"], "methods", ["None"], ["\n", "# Unfreezes layer norms.", "\n", "", "", "if", "model_args", ".", "unfreeze_layer_norms", ":", "\n", "        ", "for", "name", ",", "sub_module", "in", "model", ".", "named_modules", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.AbstractSeq2SeqDataset.tgt_lens": [[164, 168], ["utils.AbstractSeq2SeqDataset.get_char_lens"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.AbstractSeq2SeqDataset.get_char_lens"], ["            ", "if", "isinstance", "(", "sub_module", ",", "T5LayerNorm", ")", ":", "\n", "                ", "for", "param_name", ",", "param", "in", "sub_module", ".", "named_parameters", "(", ")", ":", "\n", "                    ", "param", ".", "requires_grad", "=", "True", "\n", "\n", "", "", "", "", "if", "model_args", ".", "unfreeze_model", ":", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.AbstractSeq2SeqDataset.make_sortish_sampler": [[169, 174], ["utils.DistributedSortishSampler", "utils.SortishSampler"], "methods", ["None"], ["        ", "for", "param", "in", "model", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "True", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.AbstractSeq2SeqDataset.make_dynamic_sampler": [[175, 199], ["list", "batch_by_size", "numpy.argmax", "utils.AbstractSeq2SeqDataset.make_sortish_sampler", "min", "numpy.random.permutation", "max", "len", "range", "len"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.AbstractSeq2SeqDataset.make_sortish_sampler"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.AbstractSeq2SeqDataset.__getitem__": [[200, 202], ["NotImplementedError"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.AbstractSeq2SeqDataset.collate_fn": [[203, 205], ["NotImplementedError"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.LegacySeq2SeqDataset.__getitem__": [[208, 225], ["linecache.getline().rstrip", "utils.LegacySeq2SeqDataset.encode_line", "utils.LegacySeq2SeqDataset.encode_line", "source_inputs[].squeeze", "target_inputs[].squeeze", "source_inputs[].squeeze", "linecache.getline().rstrip", "linecache.getline", "linecache.getline", "str", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.LegacySeq2SeqDataset.encode_line", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.LegacySeq2SeqDataset.encode_line"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.LegacySeq2SeqDataset.encode_line": [[227, 236], ["tokenizer"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.LegacySeq2SeqDataset.collate_fn": [[238, 251], ["torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "utils.trim_batch", "utils.trim_batch"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.trim_batch", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.trim_batch"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.Seq2SeqDataset.__getitem__": [[256, 263], ["linecache.getline().rstrip", "linecache.getline().rstrip", "linecache.getline", "linecache.getline", "str", "str"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.Seq2SeqDataset.collate_fn": [[264, 276], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "utils.Seq2SeqDataset.tokenizer.prepare_seq2seq_batch"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.Seq2SeqDataCollator.__init__": [[279, 292], ["isinstance"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.Seq2SeqDataCollator.__call__": [[293, 321], ["hasattr", "isinstance", "utils.Seq2SeqDataCollator._encode", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "utils.trim_batch", "utils.trim_batch", "utils.Seq2SeqDataCollator._shift_right_t5", "transformers.modeling_bart.shift_tokens_right"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.TaskCollator._encode", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.trim_batch", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.trim_batch", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.TaskCollator._shift_right_t5"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.Seq2SeqDataCollator._shift_right_t5": [[322, 328], ["input_ids.new_zeros", "input_ids[].clone"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.Seq2SeqDataCollator._encode": [[329, 340], ["utils.Seq2SeqDataCollator.tokenizer.prepare_seq2seq_batch"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.SortishSampler.__init__": [[345, 347], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.SortishSampler.__len__": [[348, 350], ["len"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.SortishSampler.__iter__": [[351, 353], ["iter", "utils.sortish_sampler_indices"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.sortish_sampler_indices"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.DistributedSortishSampler.__init__": [[379, 401], ["torch.get_world_size", "torch.get_world_size", "torch.get_rank", "torch.get_rank", "int", "len", "len", "torch.is_available", "torch.is_available", "RuntimeError", "torch.is_available", "torch.is_available", "RuntimeError", "math.ceil", "len"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.DistributedSortishSampler.__iter__": [[402, 411], ["torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator.manual_seed", "torch.Generator.manual_seed", "utils.sortish_sampler_indices", "iter", "len"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.sortish_sampler_indices"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.DistributedSortishSampler.available_indices": [[412, 421], ["list", "range", "len", "len", "len"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.DistributedSortishSampler.__len__": [[422, 424], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.DistributedSortishSampler.set_epoch": [[425, 427], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.TaskCollator.__init__": [[667, 675], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.TaskCollator.__call__": [[676, 693], ["utils.TaskCollator._encode", "utils.TaskCollator._shift_right_t5"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.TaskCollator._encode", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.TaskCollator._shift_right_t5"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.TaskCollator._shift_right_t5": [[694, 700], ["input_ids.new_zeros", "input_ids[].clone"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.TaskCollator._encode": [[701, 715], ["utils.TaskCollator.tokenizer.prepare_seq2seq_batch", "len", "set"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.label_smoothed_nll_loss": [[50, 69], ["nll_loss.squeeze.sum", "smooth_loss.squeeze.sum", "target.unsqueeze.dim", "target.unsqueeze.unsqueeze", "lprobs.gather", "lprobs.sum", "target.unsqueeze.eq", "nll_loss.squeeze.masked_fill_", "smooth_loss.squeeze.masked_fill_", "nll_loss.squeeze.squeeze", "smooth_loss.squeeze.squeeze", "lprobs.size", "lprobs.dim"], "function", ["None"], ["save_json", "(", "json_dict", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "outfile_name", ")", ")", "\n", "\n", "\n", "", "def", "get_training_args", "(", "arguments_list", ")", ":", "\n", "    ", "\"\"\"\n    Concatenate all training arguments except evaluation strategy which\n    is not Json serializable.\n    Args:\n        arguments_list: list of dataclasses.\n    Return:\n        arguments: concatenated arguments.\n    \"\"\"", "\n", "all_arguments", "=", "{", "}", "\n", "for", "arguments", "in", "arguments_list", ":", "\n", "        ", "all_arguments", ".", "update", "(", "asdict", "(", "arguments", ")", ")", "\n", "", "all_arguments", ".", "pop", "(", "\"evaluation_strategy\"", ")", "\n", "return", "all_arguments", "\n", "\n", "\n", "", "def", "get_last_checkpoint_path", "(", "output_dir", ")", ":", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.lmap": [[71, 74], ["list", "map"], "function", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.calculate_bleu": [[76, 79], ["round", "sacrebleu.corpus_bleu"], "function", ["None"], ["\n", "paths", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"checkpoint-*\"", ")", ")", "\n", "if", "len", "(", "paths", ")", "==", "0", ":", "\n", "        ", "return", "output_dir", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.build_compute_metrics_fn": [[81, 108], ["numpy.count_nonzero", "tokenizer.batch_decode", "tokenizer.batch_decode", "utils.lmap", "utils.lmap", "utils.build_compute_metrics_fn.decode_pred"], "function", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.lmap", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.lmap"], ["        ", "checkpoints", "=", "[", "int", "(", "checkpoint", ".", "split", "(", "'-'", ")", "[", "-", "1", "]", ")", "for", "checkpoint", "in", "paths", "]", "\n", "max_checkpoint", "=", "max", "(", "checkpoints", ")", "\n", "return", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"checkpoint-\"", "+", "str", "(", "max_checkpoint", ")", ")", "\n", "\n", "\n", "", "", "def", "use_task_specific_params", "(", "model", ",", "task", ")", ":", "\n", "    ", "\"\"\"Update config with task specific params during evaluation.\"\"\"", "\n", "task_dataset", "=", "TASK_MAPPING", "[", "task", "]", "\n", "task_specific_config", "=", "task_dataset", ".", "task_specific_config", "\n", "if", "task_specific_config", "is", "not", "None", ":", "\n", "        ", "logger", ".", "info", "(", "f\"using task specific params for {task}: {task_specific_config}\"", ")", "\n", "model", ".", "config", ".", "update", "(", "task_specific_config", ")", "\n", "\n", "\n", "", "", "def", "reset_config", "(", "model", ",", "config", ")", ":", "\n", "    ", "\"\"\"Resets the config file to the one provided.\"\"\"", "\n", "model", ".", "config", "=", "config", "\n", "logger", ".", "info", "(", "f\"config is reset to the initial values.\"", ")", "\n", "\n", "\n", "", "def", "freezing_params", "(", "model", ",", "training_args", ",", "model_args", ",", "adapter_args", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.trim_batch": [[110, 121], ["input_ids.ne().any", "input_ids.ne"], "function", ["None"], ["# If we are training adapters, we freeze all parameters except the", "\n", "# parameters of computing task embeddings and adapter controllers.", "\n", "if", "training_args", ".", "train_adapters", ":", "\n", "        ", "freeze_params", "(", "model", ")", "\n", "for", "name", ",", "sub_module", "in", "model", ".", "named_modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "sub_module", ",", "(", "MetaAdapterController", ",", "AdapterController", ")", ")", ":", "\n", "                ", "for", "param_name", ",", "param", "in", "sub_module", ".", "named_parameters", "(", ")", ":", "\n", "                    ", "param", ".", "requires_grad", "=", "True", "\n", "", "", "", "if", "adapter_args", ".", "adapter_config_name", "==", "\"meta-adapter\"", ":", "\n", "            ", "for", "param", "in", "model", ".", "task_embedding_controller", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "True", "\n", "", "", "if", "adapter_args", ".", "unique_hyper_net", ":", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.sortish_sampler_indices": [[355, 374], ["numpy.random.permutation", "numpy.concatenate", "numpy.argmax", "numpy.concatenate", "numpy.argsort", "len", "numpy.concatenate", "numpy.array", "range", "sorted", "range", "utils.sortish_sampler_indices.key_fn"], "function", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.pickle_load": [[442, 446], ["open", "pickle.load"], "function", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.pickle_save": [[448, 452], ["open", "pickle.dump"], "function", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.flatten_list": [[454, 456], ["itertools.chain.from_iterable"], "function", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.save_git_info": [[458, 462], ["utils.get_git_info", "utils.save_json", "os.path.join"], "function", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.get_git_info", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.save_json"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.save_json": [[464, 467], ["open", "json.dump"], "function", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.load_json": [[469, 472], ["open", "json.load"], "function", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.get_git_info": [[474, 490], ["git.Repo", "str", "str", "str", "str", "socket.gethostname"], "function", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.extract_rouge_mid_statistics": [[496, 502], ["dct.items", "round", "getattr"], "function", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.calculate_rouge": [[504, 550], ["rouge_score.rouge_scorer.RougeScorer", "rouge_score.scoring.BootstrapAggregator", "zip", "rouge_scorer.RougeScorer.score", "scoring.BootstrapAggregator.add_scores", "scoring.BootstrapAggregator.aggregate", "sentence_splitter.add_newline_to_end_of_each_sentence", "sentence_splitter.add_newline_to_end_of_each_sentence", "utils.extract_rouge_mid_statistics", "round", "aggregator.aggregate.items"], "function", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.sentence_splitter.add_newline_to_end_of_each_sentence", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.sentence_splitter.add_newline_to_end_of_each_sentence", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.extract_rouge_mid_statistics"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.freeze_params": [[555, 559], ["model.parameters"], "function", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.freeze_embeds": [[561, 578], ["utils.freeze_params", "utils.freeze_params", "utils.freeze_params", "utils.freeze_params", "utils.freeze_params", "utils.freeze_params", "utils.freeze_params"], "function", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.freeze_params", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.freeze_params", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.freeze_params", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.freeze_params", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.freeze_params", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.freeze_params", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.freeze_params"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.grad_status": [[580, 582], ["model.parameters"], "function", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.any_requires_grad": [[584, 586], ["any", "utils.grad_status"], "function", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.grad_status"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.assert_all_frozen": [[588, 593], ["list", "sum", "len", "utils.grad_status", "utils.lmap", "any"], "function", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.grad_status", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.lmap"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.assert_not_all_frozen": [[595, 599], ["list", "len", "any", "utils.grad_status"], "function", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.grad_status"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.parse_numeric_n_bool_cl_kwargs": [[601, 624], ["range", "len", "unparsed_args[].startswith", "len", "unparsed_args[].lower", "unparsed_args[].lower", "int", "float"], "function", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.write_txt_file": [[626, 631], ["pathlib.Path().open", "Path().open.write", "Path().open.flush", "pathlib.Path"], "function", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.chunks": [[633, 637], ["range", "len"], "function", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.check_output_dir": [[639, 658], ["os.path.exists", "ValueError", "len", "os.listdir", "len", "os.listdir"], "function", ["None"], []], "home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer.__init__": [[76, 111], ["transformers.Trainer.__init__", "isinstance", "isinstance", "logger.warn", "torch.nn.CrossEntropyLoss", "t5_trainer.T5Trainer._actual_model"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", "=", "None", ",", "data_args", "=", "None", ",", "dataset_sizes", "=", "None", ",", "adapter_config", "=", "None", ",", "\n", "multi_task_compute_metrics", "=", "None", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "if", "config", "is", "None", ":", "\n", "            ", "assert", "isinstance", "(", "\n", "self", ".", "model", ",", "PreTrainedModel", "\n", ")", ",", "f\"If no `config` is passed the model to be trained has to be of type `PreTrainedModel`, but is {self.model.__class__}\"", "\n", "self", ".", "config", "=", "self", ".", "_actual_model", "(", "self", ".", "model", ")", ".", "config", "\n", "", "else", ":", "\n", "            ", "self", ".", "config", "=", "config", "\n", "\n", "", "self", ".", "adapter_config", "=", "adapter_config", "\n", "self", ".", "multi_task_compute_metrics", "=", "multi_task_compute_metrics", "\n", "self", ".", "dataset_sizes", "=", "dataset_sizes", "\n", "self", ".", "data_args", "=", "data_args", "\n", "self", ".", "vocab_size", "=", "self", ".", "config", ".", "tgt_vocab_size", "if", "isinstance", "(", "self", ".", "config", ",", "FSMTConfig", ")", "else", "self", ".", "config", ".", "vocab_size", "\n", "\n", "if", "self", ".", "args", ".", "label_smoothing", "!=", "0", "or", "(", "self", ".", "data_args", "is", "not", "None", "and", "self", ".", "data_args", ".", "ignore_pad_token_for_loss", ")", ":", "\n", "            ", "assert", "(", "\n", "self", ".", "config", ".", "pad_token_id", "is", "not", "None", "\n", ")", ",", "\"Make sure that `config.pad_token_id` is correcly defined when ignoring `pad_token` for loss calculation or doing label smoothing.\"", "\n", "\n", "", "if", "self", ".", "config", ".", "pad_token_id", "is", "None", "and", "self", ".", "config", ".", "eos_token_id", "is", "not", "None", ":", "\n", "            ", "logger", ".", "warn", "(", "\n", "f\"The `config.pad_token_id` is `None`. Using `config.eos_token_id` = {self.config.eos_token_id} for padding..\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "args", ".", "label_smoothing", "==", "0", ":", "\n", "            ", "self", ".", "loss_fn", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", "ignore_index", "=", "self", ".", "config", ".", "pad_token_id", ")", "\n", "", "else", ":", "\n", "# dynamically import label_smoothed_nll_loss", "\n", "            ", "from", "hyperformer", ".", "third_party", ".", "utils", "import", "label_smoothed_nll_loss", "\n", "\n", "self", ".", "loss_fn", "=", "label_smoothed_nll_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer.create_optimizer_and_scheduler": [[112, 149], ["t5_trainer.T5Trainer._get_lr_scheduler", "logger.warn", "transformers.optimization.Adafactor", "transformers.optimization.AdamW", "t5_trainer.T5Trainer.model.named_parameters", "t5_trainer.T5Trainer.model.named_parameters", "any", "any"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer._get_lr_scheduler"], ["", "", "def", "create_optimizer_and_scheduler", "(", "self", ",", "num_training_steps", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Setup the optimizer and the learning rate scheduler.\n\n        We provide a reasonable default that works well. If you want to use\n        something else, you can pass a tuple in the Trainer's init through\n        :obj:`optimizers`, or subclass and override this method in a subclass.\n        \"\"\"", "\n", "if", "self", ".", "optimizer", "is", "None", ":", "\n", "            ", "no_decay", "=", "[", "\"bias\"", ",", "\"LayerNorm.weight\"", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "self", ".", "model", ".", "named_parameters", "(", ")", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "\"weight_decay\"", ":", "self", ".", "args", ".", "weight_decay", ",", "\n", "}", ",", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "self", ".", "model", ".", "named_parameters", "(", ")", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "\"weight_decay\"", ":", "0.0", ",", "\n", "}", ",", "\n", "]", "\n", "if", "self", ".", "args", ".", "adafactor", ":", "\n", "                ", "self", ".", "optimizer", "=", "Adafactor", "(", "\n", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "self", ".", "args", ".", "learning_rate", ",", "\n", "scale_parameter", "=", "False", ",", "\n", "relative_step", "=", "False", ",", "\n", ")", "\n", "\n", "", "else", ":", "\n", "                ", "self", ".", "optimizer", "=", "AdamW", "(", "\n", "optimizer_grouped_parameters", ",", "lr", "=", "self", ".", "args", ".", "learning_rate", ",", "eps", "=", "self", ".", "args", ".", "adam_epsilon", "\n", ")", "\n", "\n", "", "", "if", "self", ".", "lr_scheduler", "is", "None", ":", "\n", "            ", "self", ".", "lr_scheduler", "=", "self", ".", "_get_lr_scheduler", "(", "num_training_steps", ")", "\n", "", "else", ":", "# ignoring --lr_scheduler", "\n", "            ", "logger", ".", "warn", "(", "\"scheduler is passed to `Seq2SeqTrainer`, `--lr_scheduler` arg is ignored.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer._get_lr_scheduler": [[150, 161], ["schedule_func", "schedule_func", "schedule_func"], "methods", ["None"], ["", "", "def", "_get_lr_scheduler", "(", "self", ",", "num_training_steps", ")", ":", "\n", "        ", "schedule_func", "=", "arg_to_scheduler", "[", "self", ".", "args", ".", "lr_scheduler", "]", "\n", "if", "self", ".", "args", ".", "lr_scheduler", "==", "\"constant\"", ":", "\n", "            ", "scheduler", "=", "schedule_func", "(", "self", ".", "optimizer", ")", "\n", "", "elif", "self", ".", "args", ".", "lr_scheduler", "==", "\"constant_w_warmup\"", ":", "\n", "            ", "scheduler", "=", "schedule_func", "(", "self", ".", "optimizer", ",", "num_warmup_steps", "=", "self", ".", "args", ".", "warmup_steps", ")", "\n", "", "else", ":", "\n", "            ", "scheduler", "=", "schedule_func", "(", "\n", "self", ".", "optimizer", ",", "num_warmup_steps", "=", "self", ".", "args", ".", "warmup_steps", ",", "num_training_steps", "=", "num_training_steps", "\n", ")", "\n", "", "return", "scheduler", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer._get_train_sampler": [[162, 175], ["hyperformer.data.MultiTaskBatchSampler", "transformers.file_utils.is_torch_tpu_available", "xm.xrt_world_size", "xm.get_ordinal", "xm.xrt_world_size", "torch.distributed.get_world_size", "torch.distributed.get_rank"], "methods", ["None"], ["", "def", "_get_train_sampler", "(", "self", ")", "->", "Optional", "[", "torch", ".", "utils", ".", "data", ".", "sampler", ".", "Sampler", "]", ":", "\n", "        ", "if", "is_torch_tpu_available", "(", ")", "and", "xm", ".", "xrt_world_size", "(", ")", ">", "1", ":", "\n", "            ", "num_replicas", "=", "xm", ".", "xrt_world_size", "(", ")", "\n", "rank", "=", "xm", ".", "get_ordinal", "(", ")", "\n", "", "elif", "self", ".", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "            ", "num_replicas", "=", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", ")", "\n", "", "else", ":", "\n", "            ", "num_replicas", "=", "1", "\n", "rank", "=", "0", "\n", "", "return", "MultiTaskBatchSampler", "(", "self", ".", "dataset_sizes", ",", "self", ".", "args", ".", "train_batch_size", ",", "\n", "self", ".", "args", ".", "temperature", ",", "rank", "=", "rank", ",", "\n", "num_replicas", "=", "num_replicas", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer._compute_loss": [[176, 191], ["torch.nn.functional.log_softmax", "t5_trainer.T5Trainer.loss_fn", "t5_trainer.T5Trainer.loss_fn", "model", "model", "logits.view", "labels.view", "model"], "methods", ["None"], ["", "def", "_compute_loss", "(", "self", ",", "model", ",", "inputs", ",", "labels", ")", ":", "\n", "        ", "if", "self", ".", "args", ".", "label_smoothing", "==", "0", ":", "\n", "            ", "if", "self", ".", "data_args", "is", "not", "None", "and", "self", ".", "data_args", ".", "ignore_pad_token_for_loss", ":", "\n", "# force training to ignore pad token", "\n", "                ", "logits", "=", "model", "(", "**", "inputs", ",", "use_cache", "=", "False", ")", "[", "0", "]", "\n", "loss", "=", "self", ".", "loss_fn", "(", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "# compute usual loss via models", "\n", "                ", "loss", ",", "logits", "=", "model", "(", "**", "inputs", ",", "labels", "=", "labels", ",", "use_cache", "=", "False", ")", "[", ":", "2", "]", "\n", "", "", "else", ":", "\n", "# compute label smoothed loss", "\n", "            ", "logits", "=", "model", "(", "**", "inputs", ",", "use_cache", "=", "False", ")", "[", "0", "]", "\n", "lprobs", "=", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "loss", ",", "_", "=", "self", ".", "loss_fn", "(", "lprobs", ",", "labels", ",", "self", ".", "args", ".", "label_smoothing", ",", "ignore_index", "=", "self", ".", "config", ".", "pad_token_id", ")", "\n", "", "return", "loss", ",", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer.get_train_dataloader": [[192, 204], ["t5_trainer.T5Trainer._get_train_sampler", "torch.utils.data.dataloader.DataLoader"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer._get_train_sampler"], ["", "def", "get_train_dataloader", "(", "self", ")", "->", "DataLoader", ":", "\n", "        ", "\"\"\"\n        Returns the training :class:`~torch.utils.data.DataLoader`.\n\n        Will use no sampler if :obj:`self.train_dataset` does not implement :obj:`__len__`, a random sampler (adapted\n        to distributed training if necessary) otherwise.\n\n        Subclass and override this method if you want to inject some custom behavior.\n        \"\"\"", "\n", "multitask_sampler", "=", "self", ".", "_get_train_sampler", "(", ")", "\n", "return", "DataLoader", "(", "self", ".", "train_dataset", ",", "batch_sampler", "=", "multitask_sampler", ",", "\n", "collate_fn", "=", "self", ".", "data_collator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer.compute_loss": [[205, 209], ["inputs.pop", "t5_trainer.T5Trainer._compute_loss"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer._compute_loss"], ["", "def", "compute_loss", "(", "self", ",", "model", ",", "inputs", ")", ":", "\n", "        ", "labels", "=", "inputs", ".", "pop", "(", "\"labels\"", ")", "\n", "loss", ",", "_", "=", "self", ".", "_compute_loss", "(", "model", ",", "inputs", ",", "labels", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer.evaluate": [[210, 266], ["eval_datasets.items", "numpy.mean", "t5_trainer.T5Trainer.callback_handler.on_evaluate", "hyperformer.utils.use_task_specific_params", "t5_trainer.T5Trainer.get_eval_dataloader", "t5_trainer.T5Trainer.prediction_loop", "sorted", "results.update", "hyperformer.utils.reset_config", "ValueError", "xm.master_print", "tasks_metric.keys", "logger.info", "results.keys", "isinstance", "met.metrics_report", "t5_trainer.T5Trainer.metrics.items"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.use_task_specific_params", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.reset_config"], ["", "def", "evaluate", "(", "self", ",", "eval_datasets", ":", "Optional", "[", "Dict", "[", "str", ",", "Dataset", "]", "]", "=", "None", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"\n        Run evaluation and returns metrics.\n\n        The calling script will be responsible for providing a method to compute metrics, as they are task-dependent\n        (pass it to the init :obj:`compute_metrics` argument).\n\n        You can also subclass and override this method to inject custom behavior.\n\n        Args:\n            eval_dataset (:obj:`Dataset`, `optional`):\n                Pass a dataset if you wish to override :obj:`self.eval_dataset`. If it is an :obj:`datasets.Dataset`,\n                columns not accepted by the ``model.forward()`` method are automatically removed. It must implement the\n                :obj:`__len__` method.\n\n        Returns:\n            A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The\n            dictionary also contains the epoch number which comes from the training state.\n        \"\"\"", "\n", "results", "=", "{", "}", "\n", "if", "eval_datasets", "is", "None", ":", "\n", "            ", "eval_datasets", "=", "self", ".", "eval_dataset", "\n", "\n", "", "for", "eval_task", ",", "eval_dataset", "in", "eval_datasets", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "compute_metrics", "=", "self", ".", "multi_task_compute_metrics", "[", "eval_task", "]", "\n", "model_config", "=", "self", ".", "model", ".", "config", "\n", "\n", "use_task_specific_params", "(", "self", ".", "model", ",", "eval_task", ")", "\n", "\n", "if", "eval_dataset", "is", "not", "None", "and", "not", "isinstance", "(", "eval_dataset", ",", "collections", ".", "abc", ".", "Sized", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"eval_dataset must implement __len__\"", ")", "\n", "\n", "", "eval_dataloader", "=", "self", ".", "get_eval_dataloader", "(", "eval_dataset", ")", "\n", "\n", "output", "=", "self", ".", "prediction_loop", "(", "\n", "eval_dataloader", ",", "\n", "description", "=", "\"Evaluation\"", ",", "\n", "# No point gathering the predictions if there are no metrics, otherwise we defer to", "\n", "# self.args.prediction_loss_only", "\n", "prediction_loss_only", "=", "True", "if", "self", ".", "compute_metrics", "is", "None", "else", "None", "\n", ")", "\n", "if", "self", ".", "args", ".", "tpu_metrics_debug", "or", "self", ".", "args", ".", "debug", ":", "\n", "# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)", "\n", "                ", "xm", ".", "master_print", "(", "met", ".", "metrics_report", "(", ")", ")", "\n", "\n", "", "tasks_metric", "=", "{", "eval_task", "+", "\"_\"", "+", "k", ":", "v", "for", "k", ",", "v", "in", "output", ".", "metrics", ".", "items", "(", ")", "}", "\n", "for", "key", "in", "sorted", "(", "tasks_metric", ".", "keys", "(", ")", ")", ":", "\n", "                ", "logger", ".", "info", "(", "f\"  {key} = {tasks_metric[key]}\"", ")", "\n", "", "results", ".", "update", "(", "tasks_metric", ")", "\n", "reset_config", "(", "self", ".", "model", ",", "model_config", ")", "\n", "\n", "# Computes the average metrics across all the tasks without their corresponding losses.", "\n", "", "metrics", "=", "[", "results", "[", "key", "]", "for", "key", "in", "results", ".", "keys", "(", ")", "if", "\"loss\"", "not", "in", "key", "]", "\n", "results", "[", "'eval_average_metrics'", "]", "=", "np", ".", "mean", "(", "metrics", ")", "\n", "self", ".", "control", "=", "self", ".", "callback_handler", ".", "on_evaluate", "(", "self", ".", "args", ",", "self", ".", "state", ",", "self", ".", "control", ",", "results", ")", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer.train": [[267, 532], ["t5_trainer.T5Trainer._hp_search_setup", "isinstance", "t5_trainer.T5Trainer.get_train_dataloader", "t5_trainer.T5Trainer.create_optimizer_and_scheduler", "transformers.trainer_callback.TrainerState", "t5_trainer.T5Trainer._load_optimizer_and_scheduler", "transformers.file_utils.is_torch_tpu_available", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "t5_trainer.T5Trainer.is_local_process_zero", "t5_trainer.T5Trainer.is_world_process_zero", "torch.tensor().to", "torch.nn.parallel.DistributedDataParallel.zero_grad", "t5_trainer.T5Trainer.callback_handler.on_train_begin", "range", "logger.info", "t5_trainer.T5Trainer.callback_handler.on_train_end", "transformers.trainer_utils.TrainOutput", "transformers.trainer_utils.set_seed", "t5_trainer.T5Trainer.call_model_init", "torch.nn.parallel.DistributedDataParallel.to", "max", "amp.initialize", "torch.nn.DataParallel", "torch.nn.parallel.DistributedDataParallel", "t5_trainer.T5Trainer.num_examples", "os.path.isfile", "transformers.trainer_callback.TrainerState.load_from_json", "logger.info", "logger.info", "logger.info", "logger.info", "t5_trainer.T5Trainer.hp_name", "transformers.integrations.hp_params", "transformers.file_utils.is_torch_tpu_available", "t5_trainer.T5Trainer.callback_handler.on_epoch_begin", "enumerate", "t5_trainer.T5Trainer.callback_handler.on_epoch_end", "t5_trainer.T5Trainer._maybe_log_save_evaluate", "hasattr", "delattr", "t5_trainer.T5Trainer.store_flos", "t5_trainer.T5Trainer.log", "len", "math.ceil", "math.ceil", "is_apex_available", "ImportError", "xm.xrt_world_size", "os.path.join", "os.path.join", "torch.tensor", "isinstance", "isinstance", "pl.ParallelLoader().per_device_loader", "len", "t5_trainer.T5Trainer.floating_point_ops", "transformers.file_utils.is_torch_tpu_available", "torch.tensor().to.item", "int", "torch.distributed.get_world_size", "isinstance", "isinstance", "t5_trainer.T5Trainer.sampler.set_epoch", "t5_trainer.T5Trainer.batch_sampler.set_epoch", "t5_trainer.T5Trainer.callback_handler.on_step_begin", "t5_trainer.T5Trainer.training_step", "transformers.file_utils.is_torch_tpu_available", "t5_trainer.T5Trainer.lr_scheduler.step", "torch.nn.parallel.DistributedDataParallel.zero_grad", "t5_trainer.T5Trainer.callback_handler.on_step_end", "t5_trainer.T5Trainer._maybe_log_save_evaluate", "xm.master_print", "logger.warning", "isinstance", "pl.ParallelLoader", "torch.nn.parallel.DistributedDataParallel.no_sync", "t5_trainer.T5Trainer.training_step", "t5_trainer.T5Trainer.scaler.unscale_", "torch.nn.utils.clip_grad_norm_", "xm.optimizer_step", "met.metrics_report", "getattr", "torch.nn.parallel.DistributedDataParallel.parameters", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "t5_trainer.T5Trainer.scaler.step", "t5_trainer.T5Trainer.scaler.update", "t5_trainer.T5Trainer.optimizer.step", "amp.master_params", "torch.nn.parallel.DistributedDataParallel.parameters"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer.get_train_dataloader", "home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer.create_optimizer_and_scheduler", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.set_epoch", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.set_epoch"], ["", "def", "train", "(", "self", ",", "model_path", ":", "Optional", "[", "str", "]", "=", "None", ",", "trial", ":", "Union", "[", "\"optuna.Trial\"", ",", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Main training entry point.\n\n        Args:\n            model_path (:obj:`str`, `optional`):\n                Local path to the model if the model to train has been instantiated from a local path. If present,\n                training will resume from the optimizer/scheduler states loaded here.\n            trial (:obj:`optuna.Trial` or :obj:`Dict[str, Any]`, `optional`):\n                The trial run or the hyperparameter dictionary for hyperparameter search.\n        \"\"\"", "\n", "# This might change the seed so needs to run first.", "\n", "self", ".", "_hp_search_setup", "(", "trial", ")", "\n", "\n", "# Model re-init", "\n", "if", "self", ".", "model_init", "is", "not", "None", ":", "\n", "# Seed must be set before instantiating the model when using model_init.", "\n", "            ", "set_seed", "(", "self", ".", "args", ".", "seed", ")", "\n", "\n", "model", "=", "self", ".", "call_model_init", "(", "trial", ")", "\n", "\n", "self", ".", "model", "=", "model", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "\n", "# Reinitializes optimizer and scheduler", "\n", "self", ".", "optimizer", ",", "self", ".", "lr_scheduler", "=", "None", ",", "None", "\n", "\n", "# Keeping track whether we can can len() on the dataset or not", "\n", "", "train_dataset_is_sized", "=", "isinstance", "(", "self", ".", "train_dataset", ",", "collections", ".", "abc", ".", "Sized", ")", "\n", "\n", "# Data loader and number of training steps", "\n", "train_dataloader", "=", "self", ".", "get_train_dataloader", "(", ")", "\n", "\n", "# Setting up training control variables:", "\n", "# number of training epochs: num_train_epochs", "\n", "# number of training steps per epoch: num_update_steps_per_epoch", "\n", "# total number of training steps to execute: max_steps", "\n", "if", "train_dataset_is_sized", ":", "\n", "            ", "num_update_steps_per_epoch", "=", "len", "(", "train_dataloader", ")", "//", "self", ".", "args", ".", "gradient_accumulation_steps", "\n", "num_update_steps_per_epoch", "=", "max", "(", "num_update_steps_per_epoch", ",", "1", ")", "\n", "if", "self", ".", "args", ".", "max_steps", ">", "0", ":", "\n", "                ", "max_steps", "=", "self", ".", "args", ".", "max_steps", "\n", "num_train_epochs", "=", "self", ".", "args", ".", "max_steps", "//", "num_update_steps_per_epoch", "+", "int", "(", "\n", "self", ".", "args", ".", "max_steps", "%", "num_update_steps_per_epoch", ">", "0", "\n", ")", "\n", "", "else", ":", "\n", "                ", "max_steps", "=", "math", ".", "ceil", "(", "self", ".", "args", ".", "num_train_epochs", "*", "num_update_steps_per_epoch", ")", "\n", "num_train_epochs", "=", "math", ".", "ceil", "(", "self", ".", "args", ".", "num_train_epochs", ")", "\n", "", "", "else", ":", "\n", "# see __init__. max_steps is set when the dataset has no __len__", "\n", "            ", "max_steps", "=", "self", ".", "args", ".", "max_steps", "\n", "num_train_epochs", "=", "1", "\n", "num_update_steps_per_epoch", "=", "max_steps", "\n", "\n", "", "self", ".", "create_optimizer_and_scheduler", "(", "num_training_steps", "=", "max_steps", ")", "\n", "self", ".", "state", "=", "TrainerState", "(", ")", "\n", "self", ".", "state", ".", "is_hyper_param_search", "=", "trial", "is", "not", "None", "\n", "\n", "# Check if saved optimizer or scheduler states exist", "\n", "self", ".", "_load_optimizer_and_scheduler", "(", "model_path", ")", "\n", "\n", "# Mixed precision training with apex (torch < 1.6)", "\n", "model", "=", "self", ".", "model", "\n", "if", "self", ".", "args", ".", "fp16", "and", "_use_apex", ":", "\n", "            ", "if", "not", "is_apex_available", "(", ")", ":", "\n", "                ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"", ")", "\n", "", "model", ",", "self", ".", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "self", ".", "optimizer", ",", "opt_level", "=", "self", ".", "args", ".", "fp16_opt_level", ")", "\n", "\n", "# Multi-gpu training (should be after apex fp16 initialization)", "\n", "", "if", "self", ".", "args", ".", "n_gpu", ">", "1", ":", "\n", "            ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "# Distributed training (should be after apex fp16 initialization)", "\n", "", "if", "self", ".", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "            ", "model", "=", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "\n", "model", ",", "\n", "device_ids", "=", "[", "self", ".", "args", ".", "local_rank", "]", ",", "\n", "output_device", "=", "self", ".", "args", ".", "local_rank", ",", "\n", "find_unused_parameters", "=", "(", "\n", "not", "getattr", "(", "model", ".", "config", ",", "\"gradient_checkpointing\"", ",", "False", ")", "\n", "if", "isinstance", "(", "model", ",", "PreTrainedModel", ")", "\n", "else", "True", "\n", ")", ",", "\n", ")", "\n", "# find_unused_parameters breaks checkpointing as per", "\n", "# https://github.com/huggingface/transformers/pull/4659#issuecomment-643356021", "\n", "\n", "# Train!", "\n", "", "if", "is_torch_tpu_available", "(", ")", ":", "\n", "            ", "total_train_batch_size", "=", "self", ".", "args", ".", "train_batch_size", "*", "xm", ".", "xrt_world_size", "(", ")", "\n", "", "else", ":", "\n", "            ", "total_train_batch_size", "=", "(", "\n", "self", ".", "args", ".", "train_batch_size", "\n", "*", "self", ".", "args", ".", "gradient_accumulation_steps", "\n", "*", "(", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "if", "self", ".", "args", ".", "local_rank", "!=", "-", "1", "else", "1", ")", "\n", ")", "\n", "\n", "", "num_examples", "=", "(", "\n", "self", ".", "num_examples", "(", "train_dataloader", ")", "\n", "if", "train_dataset_is_sized", "\n", "else", "total_train_batch_size", "*", "self", ".", "args", ".", "max_steps", "\n", ")", "\n", "\n", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "num_examples", ")", "\n", "logger", ".", "info", "(", "\"  Num Epochs = %d\"", ",", "num_train_epochs", ")", "\n", "logger", ".", "info", "(", "\"  Instantaneous batch size per device = %d\"", ",", "self", ".", "args", ".", "per_device_train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Total train batch size (w. parallel, distributed & accumulation) = %d\"", ",", "total_train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Gradient Accumulation steps = %d\"", ",", "self", ".", "args", ".", "gradient_accumulation_steps", ")", "\n", "logger", ".", "info", "(", "\"  Total optimization steps = %d\"", ",", "max_steps", ")", "\n", "\n", "self", ".", "state", ".", "epoch", "=", "0", "\n", "epochs_trained", "=", "0", "\n", "steps_trained_in_current_epoch", "=", "0", "\n", "\n", "# Check if continuing training from a checkpoint", "\n", "if", "model_path", "and", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "model_path", ",", "\"trainer_state.json\"", ")", ")", ":", "\n", "            ", "self", ".", "state", "=", "TrainerState", ".", "load_from_json", "(", "os", ".", "path", ".", "join", "(", "model_path", ",", "\"trainer_state.json\"", ")", ")", "\n", "epochs_trained", "=", "self", ".", "state", ".", "global_step", "//", "num_update_steps_per_epoch", "\n", "steps_trained_in_current_epoch", "=", "self", ".", "state", ".", "global_step", "%", "(", "num_update_steps_per_epoch", ")", "\n", "\n", "logger", ".", "info", "(", "\"  Continuing training from checkpoint, will skip to saved global_step\"", ")", "\n", "logger", ".", "info", "(", "\"  Continuing training from epoch %d\"", ",", "epochs_trained", ")", "\n", "logger", ".", "info", "(", "\"  Continuing training from global step %d\"", ",", "self", ".", "state", ".", "global_step", ")", "\n", "logger", ".", "info", "(", "\"  Will skip the first %d steps in the first epoch\"", ",", "steps_trained_in_current_epoch", ")", "\n", "\n", "# Update the references", "\n", "", "self", ".", "callback_handler", ".", "model", "=", "self", ".", "model", "\n", "self", ".", "callback_handler", ".", "optimizer", "=", "self", ".", "optimizer", "\n", "self", ".", "callback_handler", ".", "lr_scheduler", "=", "self", ".", "lr_scheduler", "\n", "self", ".", "callback_handler", ".", "train_dataloader", "=", "train_dataloader", "\n", "self", ".", "state", ".", "trial_name", "=", "self", ".", "hp_name", "(", "trial", ")", "if", "self", ".", "hp_name", "is", "not", "None", "else", "None", "\n", "self", ".", "state", ".", "trial_params", "=", "hp_params", "(", "trial", ")", "if", "trial", "is", "not", "None", "else", "None", "\n", "# This should be the same if the state has been saved but in case the training arguments changed, it's safer", "\n", "# to set this after the load.", "\n", "self", ".", "state", ".", "max_steps", "=", "max_steps", "\n", "self", ".", "state", ".", "num_train_epochs", "=", "num_train_epochs", "\n", "self", ".", "state", ".", "is_local_process_zero", "=", "self", ".", "is_local_process_zero", "(", ")", "\n", "self", ".", "state", ".", "is_world_process_zero", "=", "self", ".", "is_world_process_zero", "(", ")", "\n", "\n", "tr_loss", "=", "torch", ".", "tensor", "(", "0.0", ")", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "self", ".", "_logging_loss_scalar", "=", "0", "\n", "self", ".", "_globalstep_last_logged", "=", "0", "\n", "self", ".", "_total_flos", "=", "self", ".", "state", ".", "total_flos", "\n", "model", ".", "zero_grad", "(", ")", "\n", "\n", "self", ".", "control", "=", "self", ".", "callback_handler", ".", "on_train_begin", "(", "self", ".", "args", ",", "self", ".", "state", ",", "self", ".", "control", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "epochs_trained", ",", "num_train_epochs", ")", ":", "\n", "            ", "if", "isinstance", "(", "train_dataloader", ",", "DataLoader", ")", "and", "(", "\n", "isinstance", "(", "train_dataloader", ".", "sampler", ",", "DistributedSampler", ")", "\n", "or", "isinstance", "(", "train_dataloader", ".", "batch_sampler", ",", "MultiTaskBatchSampler", ")", ")", ":", "\n", "                ", "if", "isinstance", "(", "train_dataloader", ".", "sampler", ",", "DistributedSampler", ")", ":", "\n", "                    ", "train_dataloader", ".", "sampler", ".", "set_epoch", "(", "epoch", ")", "\n", "", "else", ":", "\n", "                    ", "train_dataloader", ".", "batch_sampler", ".", "set_epoch", "(", "epoch", ")", "\n", "\n", "", "", "if", "is_torch_tpu_available", "(", ")", ":", "\n", "                ", "parallel_loader", "=", "pl", ".", "ParallelLoader", "(", "train_dataloader", ",", "[", "self", ".", "args", ".", "device", "]", ")", ".", "per_device_loader", "(", "\n", "self", ".", "args", ".", "device", "\n", ")", "\n", "epoch_iterator", "=", "parallel_loader", "\n", "", "else", ":", "\n", "                ", "epoch_iterator", "=", "train_dataloader", "\n", "\n", "# Reset the past mems state at the beginning of each epoch if necessary.", "\n", "", "if", "self", ".", "args", ".", "past_index", ">=", "0", ":", "\n", "                ", "self", ".", "_past", "=", "None", "\n", "\n", "", "steps_in_epoch", "=", "len", "(", "epoch_iterator", ")", "if", "train_dataset_is_sized", "else", "self", ".", "args", ".", "max_steps", "\n", "self", ".", "control", "=", "self", ".", "callback_handler", ".", "on_epoch_begin", "(", "self", ".", "args", ",", "self", ".", "state", ",", "self", ".", "control", ")", "\n", "\n", "for", "step", ",", "inputs", "in", "enumerate", "(", "epoch_iterator", ")", ":", "\n", "\n", "# Skip past any already trained steps if resuming training", "\n", "                ", "if", "steps_trained_in_current_epoch", ">", "0", ":", "\n", "                    ", "steps_trained_in_current_epoch", "-=", "1", "\n", "continue", "\n", "\n", "", "if", "(", "step", "+", "1", ")", "%", "self", ".", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                    ", "self", ".", "control", "=", "self", ".", "callback_handler", ".", "on_step_begin", "(", "self", ".", "args", ",", "self", ".", "state", ",", "self", ".", "control", ")", "\n", "\n", "", "if", "(", "\n", "(", "(", "step", "+", "1", ")", "%", "self", ".", "args", ".", "gradient_accumulation_steps", "!=", "0", ")", "\n", "and", "self", ".", "args", ".", "local_rank", "!=", "-", "1", "\n", "and", "_use_ddp_no_sync", "\n", ")", ":", "\n", "                    ", "with", "model", ".", "no_sync", "(", ")", ":", "\n", "                        ", "tr_loss", "+=", "self", ".", "training_step", "(", "model", ",", "inputs", ")", "\n", "", "", "else", ":", "\n", "                    ", "tr_loss", "+=", "self", ".", "training_step", "(", "model", ",", "inputs", ")", "\n", "", "self", ".", "_total_flos", "+=", "self", ".", "floating_point_ops", "(", "inputs", ")", "\n", "\n", "if", "(", "step", "+", "1", ")", "%", "self", ".", "args", ".", "gradient_accumulation_steps", "==", "0", "or", "(", "\n", "# last step in epoch but step is always smaller than gradient_accumulation_steps", "\n", "steps_in_epoch", "<=", "self", ".", "args", ".", "gradient_accumulation_steps", "\n", "and", "(", "step", "+", "1", ")", "==", "steps_in_epoch", "\n", ")", ":", "\n", "                    ", "if", "self", ".", "args", ".", "fp16", "and", "_use_native_amp", ":", "\n", "                        ", "self", ".", "scaler", ".", "unscale_", "(", "self", ".", "optimizer", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "self", ".", "args", ".", "max_grad_norm", ")", "\n", "", "elif", "self", ".", "args", ".", "fp16", "and", "_use_apex", ":", "\n", "                        ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "amp", ".", "master_params", "(", "self", ".", "optimizer", ")", ",", "self", ".", "args", ".", "max_grad_norm", ")", "\n", "", "else", ":", "\n", "                        ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "self", ".", "args", ".", "max_grad_norm", ")", "\n", "\n", "", "if", "is_torch_tpu_available", "(", ")", ":", "\n", "                        ", "xm", ".", "optimizer_step", "(", "self", ".", "optimizer", ")", "\n", "", "elif", "self", ".", "args", ".", "fp16", "and", "_use_native_amp", ":", "\n", "                        ", "self", ".", "scaler", ".", "step", "(", "self", ".", "optimizer", ")", "\n", "self", ".", "scaler", ".", "update", "(", ")", "\n", "", "else", ":", "\n", "                        ", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "self", ".", "lr_scheduler", ".", "step", "(", ")", "\n", "model", ".", "zero_grad", "(", ")", "\n", "self", ".", "state", ".", "global_step", "+=", "1", "\n", "self", ".", "state", ".", "epoch", "=", "epoch", "+", "(", "step", "+", "1", ")", "/", "steps_in_epoch", "\n", "self", ".", "control", "=", "self", ".", "callback_handler", ".", "on_step_end", "(", "self", ".", "args", ",", "self", ".", "state", ",", "self", ".", "control", ")", "\n", "\n", "self", ".", "_maybe_log_save_evaluate", "(", "tr_loss", ",", "model", ",", "trial", ",", "epoch", ")", "\n", "\n", "", "if", "self", ".", "control", ".", "should_epoch_stop", "or", "self", ".", "control", ".", "should_training_stop", ":", "\n", "                    ", "break", "\n", "\n", "", "", "self", ".", "control", "=", "self", ".", "callback_handler", ".", "on_epoch_end", "(", "self", ".", "args", ",", "self", ".", "state", ",", "self", ".", "control", ")", "\n", "self", ".", "_maybe_log_save_evaluate", "(", "tr_loss", ",", "model", ",", "trial", ",", "epoch", ")", "\n", "\n", "if", "self", ".", "args", ".", "tpu_metrics_debug", "or", "self", ".", "args", ".", "debug", ":", "\n", "                ", "if", "is_torch_tpu_available", "(", ")", ":", "\n", "# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)", "\n", "                    ", "xm", ".", "master_print", "(", "met", ".", "metrics_report", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "logger", ".", "warning", "(", "\n", "\"You enabled PyTorch/XLA debug metrics but you don't have a TPU \"", "\n", "\"configured. Check your training configuration if this is unexpected.\"", "\n", ")", "\n", "", "", "if", "self", ".", "control", ".", "should_training_stop", ":", "\n", "                ", "break", "\n", "\n", "", "", "if", "self", ".", "args", ".", "past_index", "and", "hasattr", "(", "self", ",", "\"_past\"", ")", ":", "\n", "# Clean the state at the end of training", "\n", "            ", "delattr", "(", "self", ",", "\"_past\"", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\"", ")", "\n", "\n", "\"\"\"\n        if self.args.load_best_model_at_end and self.state.best_model_checkpoint is not None:\n            logger.info(\n                f\"Loading best model from {self.state.best_model_checkpoint} (score: {self.state.best_metric}).\"\n            )\n            if isinstance(model, PreTrainedModel):\n                self.model = model.from_pretrained(self.state.best_model_checkpoint, adapter_config=self.adapter_config)\n                self.model = self.model.to(self.args.device)\n            else:\n                state_dict = torch.load(os.path.join(self.state.best_model_checkpoint, WEIGHTS_NAME))\n                self.model.load_state_dict(state_dict)\n        \"\"\"", "\n", "\n", "if", "self", ".", "_total_flos", "is", "not", "None", ":", "\n", "            ", "self", ".", "store_flos", "(", ")", "\n", "self", ".", "log", "(", "{", "\"total_flos\"", ":", "self", ".", "state", ".", "total_flos", "}", ")", "\n", "\n", "", "self", ".", "control", "=", "self", ".", "callback_handler", ".", "on_train_end", "(", "self", ".", "args", ",", "self", ".", "state", ",", "self", ".", "control", ")", "\n", "\n", "return", "TrainOutput", "(", "self", ".", "state", ".", "global_step", ",", "tr_loss", ".", "item", "(", ")", "/", "self", ".", "state", ".", "global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer.prediction_step": [[533, 591], ["t5_trainer.T5Trainer._prepare_inputs", "t5_trainer.T5Trainer.pop", "loss.mean().detach.mean().detach.mean().detach", "model.task_embedding_controller", "t5_trainer.T5Trainer.model.generate", "torch.no_grad", "t5_trainer.T5Trainer._compute_loss", "t5_trainer.T5Trainer._pad_tensors_to_max_len", "isinstance", "t5_trainer.T5Trainer._pad_tensors_to_max_len", "loss.mean().detach.mean().detach.mean"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer._compute_loss", "home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer._pad_tensors_to_max_len", "home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer._pad_tensors_to_max_len"], ["", "def", "prediction_step", "(", "\n", "self", ",", "model", ":", "nn", ".", "Module", ",", "inputs", ":", "Dict", "[", "str", ",", "Union", "[", "torch", ".", "Tensor", ",", "Any", "]", "]", ",", "\n", "prediction_loss_only", ":", "bool", "\n", ")", "->", "Tuple", "[", "Optional", "[", "float", "]", ",", "Optional", "[", "torch", ".", "Tensor", "]", ",", "Optional", "[", "torch", ".", "Tensor", "]", "]", ":", "\n", "        ", "\"\"\"\n        Perform an evaluation step on :obj:`model` using obj:`inputs`.\n\n        Subclass and override to inject custom behavior.\n\n        Args:\n            model (:obj:`nn.Module`):\n                The model to evaluate.\n            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.\n\n                The dictionary will be unpacked before being fed to the model.\n                Most models expect the targets under the argument :obj:`labels`.\n                Check your model's documentation for all accepted arguments.\n            prediction_loss_only (:obj:`bool`):\n                Whether or not to return the loss only.\n\n        Return:\n            Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n            A tuple with the loss, logits and labels (each being optional).\n        \"\"\"", "\n", "inputs", "=", "self", ".", "_prepare_inputs", "(", "inputs", ")", "\n", "gen_kwargs", "=", "{", "\n", "\"max_length\"", ":", "self", ".", "config", ".", "max_length", ",", "\n", "\"num_beams\"", ":", "self", ".", "config", ".", "num_beams", "\n", "}", "\n", "gen_kwargs", "[", "\"task\"", "]", "=", "inputs", "[", "\"task\"", "]", "\n", "gen_kwargs", "[", "\"task_embedding\"", "]", "=", "model", ".", "task_embedding_controller", "(", "inputs", "[", "\"task\"", "]", ")", "if", "(", "self", ".", "config", ".", "train_adapters", "and", "isinstance", "(", "self", ".", "adapter_config", ",", "MetaAdapterConfig", ")", ")", "else", "None", "\n", "if", "self", ".", "args", ".", "predict_with_generate", "and", "not", "self", ".", "args", ".", "prediction_loss_only", ":", "\n", "            ", "generated_tokens", "=", "self", ".", "model", ".", "generate", "(", "\n", "inputs", "[", "\"input_ids\"", "]", ",", "\n", "attention_mask", "=", "inputs", "[", "\"attention_mask\"", "]", ",", "\n", "**", "gen_kwargs", ",", "\n", ")", "\n", "# in case the batch is shorter than max length, the output should be padded", "\n", "if", "generated_tokens", ".", "shape", "[", "-", "1", "]", "<", "gen_kwargs", "[", "\"max_length\"", "]", ":", "\n", "                ", "generated_tokens", "=", "self", ".", "_pad_tensors_to_max_len", "(", "generated_tokens", ",", "gen_kwargs", "[", "\"max_length\"", "]", ")", "\n", "\n", "", "", "labels", "=", "inputs", ".", "pop", "(", "\"labels\"", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# compute loss on predict data", "\n", "            ", "loss", ",", "logits", "=", "self", ".", "_compute_loss", "(", "model", ",", "inputs", ",", "labels", ")", "\n", "\n", "", "loss", "=", "loss", ".", "mean", "(", ")", ".", "detach", "(", ")", "\n", "if", "self", ".", "args", ".", "prediction_loss_only", ":", "\n", "            ", "return", "(", "loss", ",", "None", ",", "None", ")", "\n", "\n", "", "logits", "=", "generated_tokens", "if", "self", ".", "args", ".", "predict_with_generate", "else", "logits", "\n", "\n", "if", "labels", ".", "shape", "[", "-", "1", "]", "<", "gen_kwargs", "[", "\"max_length\"", "]", ":", "\n", "            ", "labels", "=", "self", ".", "_pad_tensors_to_max_len", "(", "labels", ",", "gen_kwargs", "[", "\"max_length\"", "]", ")", "\n", "\n", "", "return", "(", "loss", ",", "logits", ",", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.trainers.t5_trainer.T5Trainer._pad_tensors_to_max_len": [[592, 607], ["ValueError", "torch.ones"], "methods", ["None"], ["", "def", "_pad_tensors_to_max_len", "(", "self", ",", "tensor", ",", "max_length", ")", ":", "\n", "# If PAD token is not defined at least EOS token has to be defined", "\n", "        ", "pad_token_id", "=", "self", ".", "config", ".", "pad_token_id", "if", "self", ".", "config", ".", "pad_token_id", "is", "not", "None", "else", "self", ".", "config", ".", "eos_token_id", "\n", "\n", "if", "pad_token_id", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"Make sure that either `config.pad_token_id` or `config.eos_token_id`\"", "\n", "f\" is defined if tensor has to be padded to `max_length`={max_length}\"", "\n", ")", "\n", "\n", "", "padded_tensor", "=", "pad_token_id", "*", "torch", ".", "ones", "(", "\n", "(", "tensor", ".", "shape", "[", "0", "]", ",", "max_length", ")", ",", "dtype", "=", "tensor", ".", "dtype", ",", "device", "=", "tensor", ".", "device", "\n", ")", "\n", "padded_tensor", "[", ":", ",", ":", "tensor", ".", "shape", "[", "-", "1", "]", "]", "=", "tensor", "\n", "return", "padded_tensor", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.configuration_t5.T5Config.__init__": [[7, 10], ["configuration_t5.T5Config.__init__"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "train_adapters", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "train_adapters", "=", "train_adapters", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5LayerFF.__init__": [[27, 43], ["torch.nn.Module.__init__", "transformers.modeling_t5.T5DenseReluDense", "transformers.modeling_t5.T5LayerNorm", "torch.nn.Dropout", "hyperformer.adapters.AutoAdapterController.get", "isinstance", "isinstance", "hyperformer.adapters.MetaLayersAdapterController"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AutoTask.get"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "adapter_config", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "DenseReluDense", "=", "T5DenseReluDense", "(", "config", ")", "\n", "self", ".", "train_adapters", "=", "config", ".", "train_adapters", "\n", "if", "self", ".", "train_adapters", ":", "\n", "            ", "self", ".", "unique_hyper_net", "=", "True", "if", "isinstance", "(", "adapter_config", ",", "MetaAdapterConfig", ")", "and", "(", "adapter_config", ".", "unique_hyper_net", "\n", "or", "adapter_config", ".", "efficient_unique_hyper_net", ")", "else", "False", "\n", "self", ".", "train_adapters_blocks", "=", "adapter_config", ".", "train_adapters_blocks", "and", "not", "self", ".", "unique_hyper_net", "\n", "if", "self", ".", "train_adapters_blocks", ":", "\n", "                ", "self", ".", "adapter_controller", "=", "AutoAdapterController", ".", "get", "(", "adapter_config", ")", "\n", "self", ".", "is_meta_adapter", "=", "True", "if", "isinstance", "(", "adapter_config", ",", "MetaAdapterConfig", ")", "else", "False", "\n", "", "elif", "self", ".", "unique_hyper_net", ":", "\n", "                ", "self", ".", "layer_hyper_net", "=", "MetaLayersAdapterController", "(", "adapter_config", ")", "\n", "", "", "self", ".", "layer_norm", "=", "T5LayerNorm", "(", "config", ".", "d_model", ",", "eps", "=", "config", ".", "layer_norm_epsilon", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "dropout_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5LayerFF.forward": [[44, 53], ["modeling_t5.T5LayerFF.layer_norm", "modeling_t5.T5LayerFF.DenseReluDense", "modeling_t5.T5LayerFF.adapter_controller", "modeling_t5.T5LayerFF.dropout", "modeling_t5.T5LayerFF.layer_hyper_net"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "task", "=", "None", ",", "task_embedding", "=", "None", ",", "t5_block_adapters", "=", "None", ")", ":", "\n", "        ", "norm_x", "=", "self", ".", "layer_norm", "(", "hidden_states", ")", "\n", "y", "=", "self", ".", "DenseReluDense", "(", "norm_x", ")", "\n", "if", "self", ".", "train_adapters", "and", "self", ".", "train_adapters_blocks", ":", "\n", "            ", "y", "=", "self", ".", "adapter_controller", "(", "task", "if", "not", "self", ".", "is_meta_adapter", "else", "task_embedding", ",", "y", ")", "\n", "", "elif", "self", ".", "train_adapters", "and", "self", ".", "unique_hyper_net", ":", "\n", "            ", "y", "=", "self", ".", "layer_hyper_net", "(", "y", ",", "t5_block_adapters", ".", "feed_forward", ")", "\n", "", "layer_output", "=", "hidden_states", "+", "self", ".", "dropout", "(", "y", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5LayerSelfAttention.__init__": [[56, 75], ["torch.nn.Module.__init__", "transformers.modeling_t5.T5Attention", "transformers.modeling_t5.T5LayerNorm", "torch.nn.Dropout", "hyperformer.adapters.AutoAdapterController.get", "isinstance", "isinstance", "hyperformer.adapters.MetaLayersAdapterController"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AutoTask.get"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "has_relative_attention_bias", "=", "False", ",", "adapter_config", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "SelfAttention", "=", "T5Attention", "(", "\n", "config", ",", "has_relative_attention_bias", "=", "has_relative_attention_bias", ",", "\n", "is_bidirectional", "=", "not", "config", ".", "is_decoder", "\n", ")", "\n", "self", ".", "train_adapters", "=", "config", ".", "train_adapters", "\n", "if", "self", ".", "train_adapters", ":", "\n", "            ", "self", ".", "unique_hyper_net", "=", "True", "if", "isinstance", "(", "adapter_config", ",", "MetaAdapterConfig", ")", "and", "(", "adapter_config", ".", "unique_hyper_net", "or", "\n", "adapter_config", ".", "efficient_unique_hyper_net", ")", "else", "False", "\n", "self", ".", "train_adapter_blocks", "=", "adapter_config", ".", "train_adapters_blocks", "and", "not", "self", ".", "unique_hyper_net", "\n", "if", "self", ".", "train_adapter_blocks", ":", "\n", "                ", "self", ".", "adapter_controller", "=", "AutoAdapterController", ".", "get", "(", "adapter_config", ")", "\n", "self", ".", "is_meta_adapter", "=", "True", "if", "isinstance", "(", "adapter_config", ",", "MetaAdapterConfig", ")", "else", "False", "\n", "", "elif", "self", ".", "unique_hyper_net", ":", "\n", "                ", "self", ".", "layer_hyper_net", "=", "MetaLayersAdapterController", "(", "adapter_config", ")", "\n", "", "", "self", ".", "layer_norm", "=", "T5LayerNorm", "(", "config", ".", "d_model", ",", "eps", "=", "config", ".", "layer_norm_epsilon", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "dropout_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5LayerSelfAttention.forward": [[76, 107], ["modeling_t5.T5LayerSelfAttention.layer_norm", "modeling_t5.T5LayerSelfAttention.SelfAttention", "modeling_t5.T5LayerSelfAttention.adapter_controller", "modeling_t5.T5LayerSelfAttention.dropout", "modeling_t5.T5LayerSelfAttention.layer_hyper_net"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "position_bias", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "use_cache", "=", "False", ",", "\n", "output_attentions", "=", "False", ",", "\n", "task", "=", "None", ",", "\n", "task_embedding", "=", "None", ",", "\n", "t5_block_adapters", "=", "None", "\n", ")", ":", "\n", "        ", "norm_x", "=", "self", ".", "layer_norm", "(", "hidden_states", ")", "\n", "attention_output", "=", "self", ".", "SelfAttention", "(", "\n", "norm_x", ",", "\n", "mask", "=", "attention_mask", ",", "\n", "position_bias", "=", "position_bias", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "past_key_value", "=", "past_key_value", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", ")", "\n", "y", "=", "attention_output", "[", "0", "]", "\n", "if", "self", ".", "train_adapters", "and", "self", ".", "train_adapter_blocks", ":", "\n", "            ", "y", "=", "self", ".", "adapter_controller", "(", "task", "if", "not", "self", ".", "is_meta_adapter", "else", "task_embedding", ",", "y", ")", "\n", "", "elif", "self", ".", "train_adapters", "and", "self", ".", "unique_hyper_net", ":", "\n", "            ", "y", "=", "self", ".", "layer_hyper_net", "(", "y", ",", "t5_block_adapters", ".", "self_attention", ")", "\n", "", "layer_output", "=", "hidden_states", "+", "self", ".", "dropout", "(", "y", ")", "\n", "outputs", "=", "(", "layer_output", ",", ")", "+", "attention_output", "[", "1", ":", "]", "# add attentions if we output them", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5Block.__init__": [[110, 122], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "modeling_t5.T5Block.layer.append", "modeling_t5.T5Block.layer.append", "modeling_t5.T5LayerSelfAttention", "modeling_t5.T5Block.layer.append", "modeling_t5.T5LayerFF", "transformers.modeling_t5.T5LayerCrossAttention"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "has_relative_attention_bias", "=", "False", ",", "adapter_config", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "adapter_config", "=", "adapter_config", "\n", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "layer", ".", "append", "(", "T5LayerSelfAttention", "(", "config", ",", "has_relative_attention_bias", "=", "has_relative_attention_bias", ",", "\n", "adapter_config", "=", "self", ".", "adapter_config", ")", ")", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "self", ".", "layer", ".", "append", "(", "T5LayerCrossAttention", "(", "config", ",", "has_relative_attention_bias", "=", "has_relative_attention_bias", ")", ")", "\n", "", "self", ".", "layer", ".", "append", "(", "T5LayerFF", "(", "config", ",", "self", ".", "adapter_config", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5Block.forward": [[123, 211], ["len", "len"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "position_bias", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "encoder_decoder_position_bias", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "use_cache", "=", "False", ",", "\n", "output_attentions", "=", "False", ",", "\n", "return_dict", "=", "False", ",", "\n", "task", "=", "None", ",", "\n", "task_embedding", "=", "None", ",", "\n", "t5_block_adapters", "=", "None", "\n", ")", ":", "\n", "        ", "if", "past_key_value", "is", "not", "None", ":", "\n", "            ", "assert", "self", ".", "is_decoder", ",", "\"Only decoder can use `past_key_values`\"", "\n", "expected_num_past_key_values", "=", "2", "if", "encoder_hidden_states", "is", "None", "else", "4", "\n", "\n", "error_message", "=", "\"There should be {} past states. 2 (past / key)\\\n            for self attention.{} Got {} past key / value states\"", ".", "format", "(", "\n", "expected_num_past_key_values", ",", "\"2 (past / key) for cross \\\n                attention\"", "if", "expected_num_past_key_values", "==", "4", "else", "\"\"", ",", "len", "(", "past_key_value", ")", ",", "\n", ")", "\n", "assert", "len", "(", "past_key_value", ")", "==", "expected_num_past_key_values", ",", "error_message", "\n", "\n", "self_attn_past_key_value", "=", "past_key_value", "[", ":", "2", "]", "\n", "cross_attn_past_key_value", "=", "past_key_value", "[", "2", ":", "]", "\n", "", "else", ":", "\n", "            ", "self_attn_past_key_value", ",", "cross_attn_past_key_value", "=", "None", ",", "None", "\n", "\n", "", "self_attention_outputs", "=", "self", ".", "layer", "[", "0", "]", "(", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "position_bias", "=", "position_bias", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "past_key_value", "=", "self_attn_past_key_value", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "task", "=", "task", ",", "\n", "task_embedding", "=", "task_embedding", ",", "\n", "t5_block_adapters", "=", "t5_block_adapters", "\n", ")", "\n", "hidden_states", ",", "present_key_value_state", "=", "self_attention_outputs", "[", ":", "2", "]", "\n", "# Keep self-attention outputs and relative position weights", "\n", "attention_outputs", "=", "self_attention_outputs", "[", "2", ":", "]", "\n", "\n", "do_cross_attention", "=", "self", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", "\n", "if", "do_cross_attention", ":", "\n", "# the actual query length is unknown for cross attention", "\n", "# if using past key value states. Need to inject it here", "\n", "            ", "if", "present_key_value_state", "is", "not", "None", ":", "\n", "                ", "query_length", "=", "present_key_value_state", "[", "0", "]", ".", "shape", "[", "2", "]", "\n", "", "else", ":", "\n", "                ", "query_length", "=", "None", "\n", "\n", "", "cross_attention_outputs", "=", "self", ".", "layer", "[", "1", "]", "(", "\n", "hidden_states", ",", "\n", "kv", "=", "encoder_hidden_states", ",", "\n", "attention_mask", "=", "encoder_attention_mask", ",", "\n", "position_bias", "=", "encoder_decoder_position_bias", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "past_key_value", "=", "cross_attn_past_key_value", ",", "\n", "query_length", "=", "query_length", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", "\n", ")", "\n", "hidden_states", "=", "cross_attention_outputs", "[", "0", "]", "\n", "# Combine self attn and cross attn key value states", "\n", "if", "present_key_value_state", "is", "not", "None", ":", "\n", "                ", "present_key_value_state", "=", "present_key_value_state", "+", "cross_attention_outputs", "[", "1", "]", "\n", "\n", "# Keep cross-attention outputs and relative position weights", "\n", "", "attention_outputs", "=", "attention_outputs", "+", "cross_attention_outputs", "[", "2", ":", "]", "\n", "\n", "# Apply Feed Forward layer", "\n", "", "hidden_states", "=", "self", ".", "layer", "[", "-", "1", "]", "(", "hidden_states", ",", "task", "=", "task", ",", "\n", "task_embedding", "=", "task_embedding", ",", "\n", "t5_block_adapters", "=", "t5_block_adapters", ")", "\n", "outputs", "=", "(", "hidden_states", ",", ")", "\n", "\n", "outputs", "=", "outputs", "+", "(", "present_key_value_state", ",", ")", "+", "attention_outputs", "\n", "return", "outputs", "# hidden-states, present_key_value_states,", "\n", "# (self-attention weights), (self-attention position bias),", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5Stack.__init__": [[216, 238], ["transformers.modeling_t5.T5PreTrainedModel.__init__", "torch.nn.ModuleList", "transformers.modeling_t5.T5LayerNorm", "torch.nn.Dropout", "modeling_t5.T5Stack.init_weights", "modeling_t5.T5Block", "isinstance", "isinstance", "hyperformer.adapters.AdapterLayersHyperNetController", "hyperformer.adapters.AdapterLayersOneHyperNetController", "range", "bool"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "embed_tokens", "=", "None", ",", "adapter_config", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "adapter_config", "=", "adapter_config", "\n", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "self", ".", "block", "=", "nn", ".", "ModuleList", "(", "\n", "[", "T5Block", "(", "config", ",", "has_relative_attention_bias", "=", "bool", "(", "i", "==", "0", ")", ",", "adapter_config", "=", "self", ".", "adapter_config", ")", "\n", "for", "i", "in", "range", "(", "config", ".", "num_layers", ")", "]", "\n", ")", "\n", "self", ".", "train_adapters", "=", "config", ".", "train_adapters", "\n", "if", "self", ".", "train_adapters", ":", "\n", "            ", "self", ".", "unique_hyper_net", "=", "isinstance", "(", "adapter_config", ",", "MetaAdapterConfig", ")", "and", "adapter_config", ".", "unique_hyper_net", "\n", "self", ".", "efficient_unique_hyper_net", "=", "isinstance", "(", "adapter_config", ",", "MetaAdapterConfig", ")", "and", "adapter_config", ".", "efficient_unique_hyper_net", "\n", "if", "self", ".", "unique_hyper_net", ":", "\n", "                ", "self", ".", "adapter_layers_hyper_net", "=", "AdapterLayersHyperNetController", "(", "adapter_config", ",", "config", ".", "num_layers", ")", "\n", "", "if", "self", ".", "efficient_unique_hyper_net", ":", "\n", "                ", "self", ".", "adapter_layers_hyper_net", "=", "AdapterLayersOneHyperNetController", "(", "adapter_config", ",", "config", ".", "num_layers", ")", "\n", "", "", "self", ".", "final_layer_norm", "=", "T5LayerNorm", "(", "config", ".", "d_model", ",", "eps", "=", "config", ".", "layer_norm_epsilon", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "dropout_rate", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5Stack.get_input_embeddings": [[239, 241], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "embed_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5Stack.get_output_embeddings": [[242, 244], ["None"], "methods", ["None"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "embed_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5Stack.set_input_embeddings": [[245, 247], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "new_embeddings", ")", ":", "\n", "        ", "self", ".", "embed_tokens", "=", "new_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5Stack.forward": [[248, 399], ["modeling_t5.T5Stack.get_extended_attention_mask", "modeling_t5.T5Stack.get_head_mask", "modeling_t5.T5Stack.dropout", "enumerate", "modeling_t5.T5Stack.final_layer_norm", "modeling_t5.T5Stack.dropout", "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions", "ValueError", "modeling_t5.T5Stack.embed_tokens", "torch.ones().to", "torch.ones", "modeling_t5.T5Stack.invert_attention_mask", "zip", "layer_module", "tuple", "input_ids.view.view.size", "input_ids.view.view.view", "len", "modeling_t5.T5Stack.adapter_layers_hyper_net", "ValueError", "torch.ones", "modeling_t5.T5Stack.size"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "task", "=", "None", ",", "\n", "task_embedding", "=", "None", "\n", ")", ":", "\n", "        ", "use_cache", "=", "use_cache", "if", "use_cache", "is", "not", "None", "else", "self", ".", "config", ".", "use_cache", "\n", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "err_msg_prefix", "=", "\"decoder_\"", "if", "self", ".", "is_decoder", "else", "\"\"", "\n", "raise", "ValueError", "(", "\n", "f\"You cannot specify both {err_msg_prefix}inputs and {err_msg_prefix}inputs_embeds at the same time\"", "\n", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_shape", "[", "-", "1", "]", ")", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "err_msg_prefix", "=", "\"decoder_\"", "if", "self", ".", "is_decoder", "else", "\"\"", "\n", "raise", "ValueError", "(", "f\"You have to specify either {err_msg_prefix}inputs or {err_msg_prefix}inputs_embeds\"", ")", "\n", "\n", "", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "assert", "self", ".", "embed_tokens", "is", "not", "None", ",", "\"You have to initialize the model with valid token embeddings\"", "\n", "inputs_embeds", "=", "self", ".", "embed_tokens", "(", "input_ids", ")", "\n", "\n", "", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "\n", "# required mask seq length can be calculated via length of past", "\n", "mask_seq_length", "=", "past_key_values", "[", "0", "]", "[", "0", "]", ".", "shape", "[", "2", "]", "+", "seq_length", "if", "past_key_values", "is", "not", "None", "else", "seq_length", "\n", "\n", "if", "use_cache", "is", "True", ":", "\n", "            ", "assert", "self", ".", "is_decoder", ",", "\":obj:`use_cache` can only be set to `True` if {} is used as a decoder\"", ".", "format", "(", "\n", "self", "\n", ")", "\n", "\n", "", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones", "(", "batch_size", ",", "mask_seq_length", ")", ".", "to", "(", "inputs_embeds", ".", "device", ")", "\n", "", "if", "self", ".", "is_decoder", "and", "encoder_attention_mask", "is", "None", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "encoder_seq_length", "=", "encoder_hidden_states", ".", "shape", "[", "1", "]", "\n", "encoder_attention_mask", "=", "torch", ".", "ones", "(", "\n", "batch_size", ",", "encoder_seq_length", ",", "device", "=", "inputs_embeds", ".", "device", ",", "dtype", "=", "torch", ".", "long", "\n", ")", "\n", "\n", "# initialize past_key_values with `None` if past does not exist", "\n", "", "if", "past_key_values", "is", "None", ":", "\n", "            ", "past_key_values", "=", "[", "None", "]", "*", "len", "(", "self", ".", "block", ")", "\n", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "", "extended_attention_mask", "=", "self", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "input_shape", ",", "inputs_embeds", ".", "device", ")", "\n", "\n", "if", "self", ".", "is_decoder", "and", "encoder_attention_mask", "is", "not", "None", ":", "\n", "            ", "encoder_extended_attention_mask", "=", "self", ".", "invert_attention_mask", "(", "encoder_attention_mask", ")", "\n", "", "else", ":", "\n", "            ", "encoder_extended_attention_mask", "=", "None", "\n", "\n", "# Prepare head mask if needed", "\n", "", "head_mask", "=", "self", ".", "get_head_mask", "(", "head_mask", ",", "self", ".", "config", ".", "num_layers", ")", "\n", "present_key_value_states", "=", "(", ")", "if", "use_cache", "else", "None", "\n", "all_hidden_states", "=", "(", ")", "if", "output_hidden_states", "else", "None", "\n", "all_attentions", "=", "(", ")", "if", "output_attentions", "else", "None", "\n", "all_cross_attentions", "=", "(", ")", "if", "(", "output_attentions", "and", "self", ".", "is_decoder", ")", "else", "None", "\n", "position_bias", "=", "None", "\n", "encoder_decoder_position_bias", "=", "None", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "inputs_embeds", ")", "\n", "\n", "for", "i", ",", "(", "layer_module", ",", "past_key_value", ")", "in", "enumerate", "(", "zip", "(", "self", ".", "block", ",", "past_key_values", ")", ")", ":", "\n", "# Computes the adapter weights for the T5 Block.", "\n", "            ", "t5_block_adapters", "=", "None", "\n", "if", "self", ".", "train_adapters", "and", "(", "self", ".", "unique_hyper_net", "or", "self", ".", "efficient_unique_hyper_net", ")", ":", "\n", "                ", "t5_block_adapters", "=", "self", ".", "adapter_layers_hyper_net", "(", "task_embedding", ",", "i", ")", "\n", "\n", "", "if", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "layer_outputs", "=", "layer_module", "(", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "position_bias", "=", "position_bias", ",", "\n", "encoder_hidden_states", "=", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", "=", "encoder_extended_attention_mask", ",", "\n", "encoder_decoder_position_bias", "=", "encoder_decoder_position_bias", ",", "\n", "head_mask", "=", "head_mask", "[", "i", "]", ",", "\n", "past_key_value", "=", "past_key_value", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "task", "=", "task", ",", "\n", "task_embedding", "=", "task_embedding", ",", "\n", "t5_block_adapters", "=", "t5_block_adapters", "\n", ")", "\n", "# layer_outputs is a tuple with:", "\n", "# hidden-states, key-value-states, (self-attention weights),", "\n", "# (self-attention position bias), (cross-attention weights),", "\n", "# (cross-attention position bias)", "\n", "hidden_states", ",", "present_key_value_state", "=", "layer_outputs", "[", ":", "2", "]", "\n", "\n", "if", "i", "==", "0", ":", "\n", "# We share the position biases between the layers - the first layer store them", "\n", "# layer_outputs = hidden-states, key-value-states (self-attention weights),", "\n", "# (self-attention position bias), (cross-attention weights), (cross-attention position bias)", "\n", "                ", "position_bias", "=", "layer_outputs", "[", "3", "if", "output_attentions", "else", "2", "]", "\n", "if", "self", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "                    ", "encoder_decoder_position_bias", "=", "layer_outputs", "[", "5", "if", "output_attentions", "else", "3", "]", "\n", "# append next layer key value states", "\n", "", "", "if", "use_cache", ":", "\n", "                ", "present_key_value_states", "=", "present_key_value_states", "+", "(", "present_key_value_state", ",", ")", "\n", "\n", "", "if", "output_attentions", ":", "\n", "                ", "all_attentions", "=", "all_attentions", "+", "(", "layer_outputs", "[", "2", "]", ",", ")", "\n", "if", "self", ".", "is_decoder", ":", "\n", "                    ", "all_cross_attentions", "=", "all_cross_attentions", "+", "(", "layer_outputs", "[", "4", "if", "i", "==", "0", "else", "3", "]", ",", ")", "\n", "", "", "", "hidden_states", "=", "self", ".", "final_layer_norm", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "\n", "# Add last layer", "\n", "if", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "return", "tuple", "(", "\n", "v", "\n", "for", "v", "in", "[", "\n", "hidden_states", ",", "\n", "present_key_value_states", ",", "\n", "all_hidden_states", ",", "\n", "all_attentions", ",", "\n", "all_cross_attentions", ",", "\n", "]", "\n", "if", "v", "is", "not", "None", "\n", ")", "\n", "", "return", "BaseModelOutputWithPastAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "hidden_states", ",", "\n", "past_key_values", "=", "present_key_value_states", ",", "\n", "hidden_states", "=", "all_hidden_states", ",", "\n", "attentions", "=", "all_attentions", ",", "\n", "cross_attentions", "=", "all_cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5ForConditionalGeneration.__init__": [[406, 429], ["transformers.modeling_t5.T5PreTrainedModel.__init__", "torch.nn.Embedding", "copy.deepcopy", "modeling_t5.T5Stack", "copy.deepcopy", "modeling_t5.T5Stack", "torch.nn.Linear", "modeling_t5.T5ForConditionalGeneration.init_weights", "isinstance", "hyperformer.adapters.TaskEmbeddingController"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "adapter_config", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "# Computes the task-embeddings.", "\n", "self", ".", "train_adapters", "=", "config", ".", "train_adapters", "\n", "if", "config", ".", "train_adapters", "and", "isinstance", "(", "adapter_config", ",", "MetaAdapterConfig", ")", ":", "\n", "            ", "self", ".", "task_embedding_controller", "=", "TaskEmbeddingController", "(", "adapter_config", ")", "\n", "", "self", ".", "adapter_config", "=", "adapter_config", "\n", "self", ".", "model_dim", "=", "config", ".", "d_model", "\n", "self", ".", "shared", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "d_model", ")", "\n", "encoder_config", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "encoder_config", ".", "use_cache", "=", "False", "\n", "encoder_config", ".", "is_encoder_decoder", "=", "False", "\n", "if", "config", ".", "train_adapters", ":", "\n", "            ", "encoder_config", ".", "train_adapters", "=", "True", "\n", "", "self", ".", "encoder", "=", "T5Stack", "(", "encoder_config", ",", "self", ".", "shared", ",", "adapter_config", "=", "adapter_config", ")", "\n", "decoder_config", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "decoder_config", ".", "is_decoder", "=", "True", "\n", "decoder_config", ".", "is_encoder_decoder", "=", "False", "\n", "decoder_config", ".", "num_layers", "=", "config", ".", "num_decoder_layers", "\n", "self", ".", "decoder", "=", "T5Stack", "(", "decoder_config", ",", "self", ".", "shared", ",", "adapter_config", "=", "adapter_config", ")", "\n", "self", ".", "lm_head", "=", "nn", ".", "Linear", "(", "config", ".", "d_model", ",", "config", ".", "vocab_size", ",", "bias", "=", "False", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5ForConditionalGeneration.get_input_embeddings": [[430, 432], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "shared", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5ForConditionalGeneration.set_input_embeddings": [[433, 437], ["modeling_t5.T5ForConditionalGeneration.encoder.set_input_embeddings", "modeling_t5.T5ForConditionalGeneration.decoder.set_input_embeddings"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5ForConditionalGeneration.set_input_embeddings", "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5ForConditionalGeneration.set_input_embeddings"], ["", "def", "set_input_embeddings", "(", "self", ",", "new_embeddings", ")", ":", "\n", "        ", "self", ".", "shared", "=", "new_embeddings", "\n", "self", ".", "encoder", ".", "set_input_embeddings", "(", "new_embeddings", ")", "\n", "self", ".", "decoder", ".", "set_input_embeddings", "(", "new_embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5ForConditionalGeneration.get_output_embeddings": [[438, 440], ["None"], "methods", ["None"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "lm_head", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5ForConditionalGeneration.get_encoder": [[441, 443], ["None"], "methods", ["None"], ["", "def", "get_encoder", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5ForConditionalGeneration.get_decoder": [[444, 446], ["None"], "methods", ["None"], ["", "def", "get_decoder", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5ForConditionalGeneration.forward": [[447, 603], ["modeling_t5.T5ForConditionalGeneration.decoder", "modeling_t5.T5ForConditionalGeneration.lm_head", "transformers.modeling_outputs.Seq2SeqLMOutput", "warnings.warn", "kwargs.pop", "warnings.warn", "kwargs.pop", "warnings.warn", "kwargs.pop", "modeling_t5.T5ForConditionalGeneration.encoder", "modeling_t5.T5ForConditionalGeneration._shift_right", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "list", "transformers.modeling_outputs.BaseModelOutput", "modeling_t5.T5ForConditionalGeneration.view", "kwargs.pop.view", "kwargs.keys", "isinstance", "modeling_t5.T5ForConditionalGeneration.task_embedding_controller", "modeling_t5.T5ForConditionalGeneration.size", "modeling_t5.T5ForConditionalGeneration.task_embedding_controller", "isinstance", "isinstance", "len", "len"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "decoder_input_ids", "=", "None", ",", "\n", "decoder_attention_mask", "=", "None", ",", "\n", "encoder_outputs", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "decoder_inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "task", "=", "None", ",", "\n", "task_embedding", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`,\n        `optional`):\n            Labels for computing the sequence classification/regression loss.\n            Indices should be in :obj:`[-100, 0, ...,\n            config.vocab_size - 1]`. All labels set to ``-100`` are ignored\n            (masked), the loss is only computed for\n            labels in ``[0, ..., config.vocab_size]``\n        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n            Used to hide legacy arguments that have been deprecated.\n\n        Returns:\n\n        Examples::\n\n            >>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n            >>> tokenizer = T5Tokenizer.from_pretrained('t5-small')\n            >>> model = T5ForConditionalGeneration.from_pretrained('t5-small',\n            return_dict=True)\n\n            >>> input_ids = tokenizer('The <extra_id_0> walks in <extra_id_1>\n            park', return_tensors='pt').input_ids\n            >>> labels = tokenizer('<extra_id_0> cute dog <extra_id_1> the\n            <extra_id_2> </s>', return_tensors='pt').input_ids\n            >>> outputs = model(input_ids=input_ids, labels=labels)\n            >>> loss = outputs.loss\n            >>> logits = outputs.logits\n\n            >>> input_ids = tokenizer(\"summarize: studies have shown that owning\n            a dog is good for you \", return_tensors=\"pt\").input_ids# Batch size 1\n            >>> outputs = model.generate(input_ids)\n        \"\"\"", "\n", "if", "\"lm_labels\"", "in", "kwargs", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "\"The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\"", ",", "\n", "FutureWarning", ",", "\n", ")", "\n", "labels", "=", "kwargs", ".", "pop", "(", "\"lm_labels\"", ")", "\n", "", "if", "\"decoder_past_key_value_states\"", "in", "kwargs", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "\"The `decoder_past_key_value_states` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\"", ",", "\n", "FutureWarning", ",", "\n", ")", "\n", "past_key_values", "=", "kwargs", ".", "pop", "(", "\"decoder_past_key_value_states\"", ")", "\n", "", "if", "\"decoder_past_key_values\"", "in", "kwargs", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "\"The `decoder_past_key_values` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\"", ",", "\n", "FutureWarning", ",", "\n", ")", "\n", "past_key_values", "=", "kwargs", ".", "pop", "(", "\"decoder_past_key_values\"", ")", "\n", "", "assert", "kwargs", "==", "{", "}", ",", "f\"Unexpected keyword arguments: {list(kwargs.keys())}.\"", "\n", "\n", "use_cache", "=", "use_cache", "if", "use_cache", "is", "not", "None", "else", "self", ".", "config", ".", "use_cache", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "# Encode if needed (training, first prediction pass)", "\n", "if", "encoder_outputs", "is", "None", ":", "\n", "# Convert encoder inputs in embeddings if needed", "\n", "            ", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "task", "=", "task", ",", "\n", "task_embedding", "=", "self", ".", "task_embedding_controller", "(", "task", ")", "if", "self", ".", "train_adapters", "and", "isinstance", "(", "self", ".", "adapter_config", ",", "\n", "MetaAdapterConfig", ")", "else", "None", "\n", ")", "\n", "", "elif", "return_dict", "and", "not", "isinstance", "(", "encoder_outputs", ",", "BaseModelOutput", ")", ":", "\n", "            ", "encoder_outputs", "=", "BaseModelOutput", "(", "\n", "last_hidden_state", "=", "encoder_outputs", "[", "0", "]", ",", "\n", "hidden_states", "=", "encoder_outputs", "[", "1", "]", "if", "len", "(", "encoder_outputs", ")", ">", "1", "else", "None", ",", "\n", "attentions", "=", "encoder_outputs", "[", "2", "]", "if", "len", "(", "encoder_outputs", ")", ">", "2", "else", "None", ",", "\n", ")", "\n", "", "hidden_states", "=", "encoder_outputs", "[", "0", "]", "\n", "if", "labels", "is", "not", "None", "and", "decoder_input_ids", "is", "None", "and", "decoder_inputs_embeds", "is", "None", ":", "\n", "# get decoder inputs from shifting lm labels to the right", "\n", "            ", "decoder_input_ids", "=", "self", ".", "_shift_right", "(", "labels", ")", "\n", "\n", "# If decoding with past key value states, only the last tokens", "\n", "# should be given as an input", "\n", "", "if", "past_key_values", "is", "not", "None", ":", "\n", "            ", "assert", "labels", "is", "None", ",", "\"Decoder should not use cached key value states when training.\"", "\n", "if", "decoder_input_ids", "is", "not", "None", ":", "\n", "                ", "decoder_input_ids", "=", "decoder_input_ids", "[", ":", ",", "-", "1", ":", "]", "\n", "", "if", "decoder_inputs_embeds", "is", "not", "None", ":", "\n", "                ", "decoder_inputs_embeds", "=", "decoder_inputs_embeds", "[", ":", ",", "-", "1", ":", "]", "\n", "\n", "# Decode", "\n", "", "", "decoder_outputs", "=", "self", ".", "decoder", "(", "\n", "input_ids", "=", "decoder_input_ids", ",", "\n", "attention_mask", "=", "decoder_attention_mask", ",", "\n", "inputs_embeds", "=", "decoder_inputs_embeds", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", "encoder_hidden_states", "=", "hidden_states", ",", "\n", "encoder_attention_mask", "=", "attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "task", "=", "task", ",", "\n", "task_embedding", "=", "self", ".", "task_embedding_controller", "(", "task", ")", "if", "(", "self", ".", "train_adapters", "and", "isinstance", "(", "self", ".", "adapter_config", ",", "MetaAdapterConfig", ")", ")", "else", "None", "\n", ")", "\n", "\n", "sequence_output", "=", "decoder_outputs", "[", "0", "]", "\n", "# Rescale output before projecting on vocab", "\n", "# See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586", "\n", "sequence_output", "=", "sequence_output", "*", "(", "self", ".", "model_dim", "**", "-", "0.5", ")", "\n", "lm_logits", "=", "self", ".", "lm_head", "(", "sequence_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "100", ")", "\n", "loss", "=", "loss_fct", "(", "lm_logits", ".", "view", "(", "-", "1", ",", "lm_logits", ".", "size", "(", "-", "1", ")", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "# TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "lm_logits", ",", ")", "+", "decoder_outputs", "[", "1", ":", "]", "+", "encoder_outputs", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "Seq2SeqLMOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "lm_logits", ",", "\n", "past_key_values", "=", "decoder_outputs", ".", "past_key_values", ",", "\n", "decoder_hidden_states", "=", "decoder_outputs", ".", "hidden_states", ",", "\n", "decoder_attentions", "=", "decoder_outputs", ".", "attentions", ",", "\n", "cross_attentions", "=", "decoder_outputs", ".", "cross_attentions", ",", "\n", "encoder_last_hidden_state", "=", "encoder_outputs", ".", "last_hidden_state", ",", "\n", "encoder_hidden_states", "=", "encoder_outputs", ".", "hidden_states", ",", "\n", "encoder_attentions", "=", "encoder_outputs", ".", "attentions", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5ForConditionalGeneration.prepare_inputs_for_generation": [[605, 619], ["None"], "methods", ["None"], ["", "def", "prepare_inputs_for_generation", "(", "\n", "self", ",", "input_ids", ",", "past", "=", "None", ",", "attention_mask", "=", "None", ",", "use_cache", "=", "None", ",", "encoder_outputs", "=", "None", ",", "**", "kwargs", "\n", ")", ":", "\n", "# cut decoder_input_ids if past is used", "\n", "        ", "if", "past", "is", "not", "None", ":", "\n", "            ", "input_ids", "=", "input_ids", "[", ":", ",", "-", "1", ":", "]", "\n", "", "return", "{", "\n", "\"decoder_input_ids\"", ":", "input_ids", ",", "\n", "\"past_key_values\"", ":", "past", ",", "\n", "\"encoder_outputs\"", ":", "encoder_outputs", ",", "\n", "\"attention_mask\"", ":", "attention_mask", ",", "\n", "\"use_cache\"", ":", "use_cache", ",", "\n", "\"task\"", ":", "kwargs", "[", "\"task\"", "]", ",", "\n", "\"task_embedding\"", ":", "kwargs", "[", "\"task_embedding\"", "]", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.models.modeling_t5.T5ForConditionalGeneration._reorder_cache": [[621, 644], ["logger.warning", "len", "len", "layer_past_state.index_select"], "methods", ["None"], ["", "def", "_reorder_cache", "(", "self", ",", "past", ",", "beam_idx", ")", ":", "\n", "# if decoder past is not included in output", "\n", "# speedy decoding is disabled and no need to reorder", "\n", "        ", "if", "past", "is", "None", ":", "\n", "            ", "logger", ".", "warning", "(", "\"You might want to consider setting `use_cache=True` to speed up decoding\"", ")", "\n", "return", "past", "\n", "\n", "", "reordered_decoder_past", "=", "(", ")", "\n", "for", "layer_past_states", "in", "past", ":", "\n", "# get the correct batch idx from layer past batch dim", "\n", "# batch dim of `past` is at 2nd position", "\n", "            ", "reordered_layer_past_states", "=", "(", ")", "\n", "for", "layer_past_state", "in", "layer_past_states", ":", "\n", "# need to set correct `past` for each of the four key / value states", "\n", "                ", "reordered_layer_past_states", "=", "reordered_layer_past_states", "+", "(", "\n", "layer_past_state", ".", "index_select", "(", "0", ",", "beam_idx", ")", ",", "\n", ")", "\n", "\n", "", "assert", "reordered_layer_past_states", "[", "0", "]", ".", "shape", "==", "layer_past_states", "[", "0", "]", ".", "shape", "\n", "assert", "len", "(", "reordered_layer_past_states", ")", "==", "len", "(", "layer_past_states", ")", "\n", "\n", "reordered_decoder_past", "=", "reordered_decoder_past", "+", "(", "reordered_layer_past_states", ",", ")", "\n", "", "return", "reordered_decoder_past", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.Activations.__init__": [[9, 12], ["torch.Module.__init__", "transformers.activations.get_activation"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "activation_type", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "f", "=", "get_activation", "(", "activation_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.Activations.forward": [[13, 15], ["adapter_utils.Activations.f"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "f", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.TaskHyperNet.__init__": [[33, 41], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "adapter_utils.linear_layer", "torch.ReLU", "torch.ReLU", "adapter_utils.linear_layer"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.linear_layer", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.linear_layer"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "TaskHyperNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "task_hidden_dim", "=", "config", ".", "task_hidden_dim", "\n", "self", ".", "projected_task_embedding_dim", "=", "config", ".", "projected_task_embedding_dim", "\n", "self", ".", "task_embeding_generator", "=", "nn", ".", "Sequential", "(", "\n", "linear_layer", "(", "config", ".", "task_embedding_dim", ",", "self", ".", "task_hidden_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "linear_layer", "(", "self", ".", "task_hidden_dim", ",", "self", ".", "projected_task_embedding_dim", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.TaskHyperNet.forward": [[42, 45], ["task_embedding.view.view.view", "adapter_utils.TaskHyperNet.task_embeding_generator().view", "adapter_utils.TaskHyperNet.task_embeding_generator"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "task_embedding", ")", ":", "\n", "        ", "task_embedding", "=", "task_embedding", ".", "view", "(", "-", "1", ")", "\n", "return", "self", ".", "task_embeding_generator", "(", "task_embedding", ")", ".", "view", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.LayerNormHyperNet.__init__": [[50, 56], ["torch.Module.__init__", "adapter_utils.linear_layer", "adapter_utils.linear_layer"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.linear_layer", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.linear_layer"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "LayerNormHyperNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "task_embedding_dim", "=", "config", ".", "projected_task_embedding_dim", "if", "config", ".", "train_task_embeddings", "else", "config", ".", "task_embedding_dim", "\n", "self", ".", "weight_generator", "=", "linear_layer", "(", "self", ".", "task_embedding_dim", ",", "config", ".", "input_dim", ")", "\n", "self", ".", "bias_generator", "=", "linear_layer", "(", "self", ".", "task_embedding_dim", ",", "config", ".", "input_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.LayerNormHyperNet.forward": [[57, 59], ["adapter_utils.LayerNormHyperNet.weight_generator", "adapter_utils.LayerNormHyperNet.bias_generator"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "return", "self", ".", "weight_generator", "(", "input", ")", ",", "self", ".", "bias_generator", "(", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.TaskEmbeddingController.__init__": [[64, 77], ["torch.Module.__init__", "adapter_utils.TaskEmbeddingController.set_task_embeddings", "adapter_utils.TaskEmbeddingController.task_to_task_embeddings.values", "adapter_utils.TaskHyperNet"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.TaskEmbeddingController.set_task_embeddings"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "TaskEmbeddingController", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "device", "=", "config", ".", "device", "\n", "self", ".", "task_embedding_dim", "=", "config", ".", "task_embedding_dim", "\n", "self", ".", "tasks", "=", "config", ".", "tasks", "\n", "self", ".", "task_to_task_embeddings", "=", "{", "task", ":", "task", "for", "task", "in", "self", ".", "tasks", "}", "\n", "if", "config", ".", "task_to_embeddings", "is", "not", "None", ":", "\n", "            ", "self", ".", "task_to_task_embeddings", "=", "config", ".", "task_to_embeddings", "\n", "self", ".", "tasks", "=", "self", ".", "task_to_task_embeddings", ".", "values", "(", ")", "\n", "", "self", ".", "set_task_embeddings", "(", "self", ".", "tasks", ")", "\n", "self", ".", "train_task_embeddings", "=", "config", ".", "train_task_embeddings", "\n", "if", "self", ".", "train_task_embeddings", ":", "\n", "            ", "self", ".", "task_hyper_net", "=", "TaskHyperNet", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.TaskEmbeddingController.get_task": [[78, 80], ["None"], "methods", ["None"], ["", "", "def", "get_task", "(", "self", ",", "task", ")", ":", "\n", "        ", "return", "self", ".", "task_to_task_embeddings", "[", "task", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.TaskEmbeddingController.set_task_embeddings": [[81, 86], ["torch.ParameterDict", "torch.ParameterDict", "dict", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Parameter", "torch.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["None"], ["", "def", "set_task_embeddings", "(", "self", ",", "tasks", ")", ":", "\n", "        ", "self", ".", "task_to_embeddings", "=", "nn", ".", "ParameterDict", "(", "dict", "(", ")", ")", "\n", "for", "task", "in", "tasks", ":", "\n", "            ", "task_embedding", "=", "torch", ".", "Tensor", "(", "torch", ".", "randn", "(", "self", ".", "task_embedding_dim", ")", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "task_to_embeddings", "[", "task", "]", "=", "nn", ".", "Parameter", "(", "task_embedding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.TaskEmbeddingController.forward": [[87, 93], ["adapter_utils.TaskEmbeddingController.get_task", "adapter_utils.TaskEmbeddingController.task_hyper_net"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AdapterController.get_task"], ["", "", "def", "forward", "(", "self", ",", "task", ")", ":", "\n", "        ", "task_mapped", "=", "self", ".", "get_task", "(", "task", ")", "\n", "task_embedding", "=", "self", ".", "task_to_embeddings", "[", "task_mapped", "]", "\n", "if", "self", ".", "train_task_embeddings", ":", "\n", "            ", "return", "self", ".", "task_hyper_net", "(", "task_embedding", ")", "\n", "", "return", "task_embedding", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.init_linear_layer": [[17, 21], ["torch.init.normal_", "torch.init.zeros_"], "function", ["None"], ["", "", "def", "init_linear_layer", "(", "linear_layer", ",", "std", "=", "1e-2", ")", ":", "\n", "    ", "\"\"\"Initializes the given linear module as explained in adapter paper.\"\"\"", "\n", "nn", ".", "init", ".", "normal_", "(", "linear_layer", ".", "weight", ",", "std", "=", "std", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "linear_layer", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.linear_layer": [[23, 28], ["torch.Linear", "adapter_utils.init_linear_layer"], "function", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.init_linear_layer"], ["", "def", "linear_layer", "(", "input_dim", ",", "output_dim", ",", "std", "=", "1e-2", ")", ":", "\n", "    ", "\"\"\"Generates a linear module and initializes it.\"\"\"", "\n", "linear", "=", "nn", ".", "Linear", "(", "input_dim", ",", "output_dim", ")", "\n", "init_linear_layer", "(", "linear", ",", "std", "=", "std", ")", "\n", "return", "linear", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AdapterController.__init__": [[17, 34], ["torch.Module.__init__", "torch.ModuleDict", "torch.ModuleDict", "torch.ModuleDict", "adapter_controller.AdapterController.construct_adapters", "dict", "adapter_controller.AdapterController.task_to_adapter.values", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AdapterController.construct_adapters"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "adapters", "=", "nn", ".", "ModuleDict", "(", "dict", "(", ")", ")", "\n", "self", ".", "tasks", "=", "config", ".", "tasks", "\n", "self", ".", "task_to_adapter", "=", "{", "task", ":", "task", "for", "task", "in", "self", ".", "tasks", "}", "\n", "# If a dictionary from task to adapter is given, the task is over-written by the given adapters.", "\n", "if", "config", ".", "task_to_adapter", "is", "not", "None", ":", "\n", "            ", "self", ".", "task_to_adapter", "=", "config", ".", "task_to_adapter", "\n", "self", ".", "tasks", "=", "self", ".", "task_to_adapter", ".", "values", "(", ")", "\n", "", "self", ".", "adapters", "=", "self", ".", "construct_adapters", "(", "self", ".", "tasks", ")", "\n", "self", ".", "add_layer_norm_before_adapter", "=", "config", ".", "add_layer_norm_before_adapter", "\n", "self", ".", "add_layer_norm_after_adapter", "=", "config", ".", "add_layer_norm_after_adapter", "\n", "if", "self", ".", "add_layer_norm_before_adapter", ":", "\n", "            ", "self", ".", "pre_layer_norm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "input_dim", ")", "\n", "", "if", "self", ".", "add_layer_norm_after_adapter", ":", "\n", "            ", "self", ".", "post_layer_norm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "input_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AdapterController.set_task_to_adapter_map": [[35, 37], ["None"], "methods", ["None"], ["", "", "def", "set_task_to_adapter_map", "(", "self", ",", "mapping", ")", ":", "\n", "        ", "self", ".", "task_to_adapter", "=", "mapping", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AdapterController.get_task": [[38, 40], ["None"], "methods", ["None"], ["", "def", "get_task", "(", "self", ",", "task", ")", ":", "\n", "        ", "return", "self", ".", "task_to_adapter", "[", "task", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AdapterController.construct_adapters": [[41, 51], ["adapter_modeling.Adapter"], "methods", ["None"], ["", "def", "construct_adapters", "(", "self", ",", "tasks", ")", ":", "\n", "        ", "\"\"\"\n        Constructs adapter layers and adds them to a dictionary for the given\n        tasks.\n        Args:\n            tasks: A list of string containing the task names.\n        \"\"\"", "\n", "for", "task", "in", "tasks", ":", "\n", "            ", "self", ".", "adapters", "[", "task", "]", "=", "Adapter", "(", "self", ".", "config", ")", "\n", "", "return", "self", ".", "adapters", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AdapterController.disable_adapters": [[52, 64], ["adapter_controller.AdapterController.convert_to_list", "adapter_controller.AdapterController.get_adapter", "adapter_controller.AdapterController.parameters"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AdapterController.convert_to_list", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AdapterController.get_adapter"], ["", "def", "disable_adapters", "(", "self", ",", "tasks", ")", ":", "\n", "        ", "\"\"\"\n        Given a list of tasks, it freezes their corresponding adapter layers'\n        parameters.\n        Args:\n           tasks: List of tasks.\n        \"\"\"", "\n", "tasks", "=", "self", ".", "convert_to_list", "(", "tasks", ")", "\n", "for", "task", "in", "tasks", ":", "\n", "            ", "adapter", "=", "self", ".", "get_adapter", "(", "task", ")", "\n", "for", "param", "in", "adapter", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AdapterController.convert_to_list": [[65, 69], ["isinstance"], "methods", ["None"], ["", "", "", "def", "convert_to_list", "(", "self", ",", "tasks", ")", ":", "\n", "        ", "if", "isinstance", "(", "tasks", ",", "list", ")", ":", "\n", "            ", "return", "tasks", "\n", "", "return", "[", "tasks", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AdapterController.enable_adapters": [[70, 81], ["adapter_controller.AdapterController.convert_to_list", "adapter_controller.AdapterController.get_adapter", "adapter_controller.AdapterController.parameters"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AdapterController.convert_to_list", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AdapterController.get_adapter"], ["", "def", "enable_adapters", "(", "self", ",", "tasks", ")", ":", "\n", "        ", "\"\"\"\n        Given a list of tasks, it unfreezes their corresponding adapter layers.\n        Args:\n            tasks: Given list of tasks.\n        \"\"\"", "\n", "tasks", "=", "self", ".", "convert_to_list", "(", "tasks", ")", "\n", "for", "task", "in", "tasks", ":", "\n", "            ", "adapter", "=", "self", ".", "get_adapter", "(", "task", ")", "\n", "for", "param", "in", "adapter", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AdapterController.get_adapter": [[82, 90], ["None"], "methods", ["None"], ["", "", "", "def", "get_adapter", "(", "self", ",", "task", ")", ":", "\n", "        ", "\"\"\"Given a task returns its corresponding adapter layer.\n        Args:\n            task: Input task name.\n        Returns:\n            Adapter layer corresponding to the given task.\n        \"\"\"", "\n", "return", "self", ".", "adapters", "[", "task", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AdapterController.forward": [[91, 113], ["adapter_controller.AdapterController.get_task", "adapter_controller.AdapterController.enable_adapters", "adapter_controller.AdapterController.disable_adapters", "adapter_controller.AdapterController.get_adapter", "adapter_controller.AdapterController.", "adapter_controller.AdapterController.pre_layer_norm", "adapter_controller.AdapterController.post_layer_norm"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AdapterController.get_task", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AdapterController.enable_adapters", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AdapterController.disable_adapters", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AdapterController.get_adapter"], ["", "def", "forward", "(", "self", ",", "task", ",", "inputs", ")", ":", "\n", "        ", "\"\"\"Retrieves the adapter layer corresponding to the given\n        task. It freezes the adapter layers for all the other tasks\n        and call the selected adapter layer.\n        Args:\n            task: the name of the current task.\n            inputs: the inputs to feed in in the adapter layer.\n        Returns:\n            outputs of the adapter layer.\"\"\"", "\n", "task", "=", "self", ".", "get_task", "(", "task", ")", "\n", "# Enables the adapter layer for the given task.", "\n", "self", ".", "enable_adapters", "(", "task", ")", "\n", "# Disable other adapters.", "\n", "other_tasks", "=", "[", "x", "for", "x", "in", "self", ".", "tasks", "if", "x", "!=", "task", "]", "\n", "self", ".", "disable_adapters", "(", "other_tasks", ")", "\n", "adapter", "=", "self", ".", "get_adapter", "(", "task", ")", "\n", "z", "=", "self", ".", "pre_layer_norm", "(", "inputs", ")", "if", "self", ".", "add_layer_norm_before_adapter", "else", "inputs", "\n", "outputs", "=", "adapter", "(", "z", ")", "\n", "if", "self", ".", "add_layer_norm_after_adapter", ":", "\n", "            ", "outputs", "=", "self", ".", "post_layer_norm", "(", "outputs", ")", "\n", "", "outputs", "=", "outputs", "+", "inputs", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.MetaAdapterController.__init__": [[121, 144], ["torch.Module.__init__", "torch.ModuleDict", "torch.ModuleDict", "torch.ModuleDict", "adapter_modeling.AdapterHyperNet", "adapter_modeling.AdapterHyperNet", "config.non_linearity.lower", "dict", "adapter_utils.LayerNormHyperNet", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "adapter_utils.LayerNormHyperNet", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "device", "=", "config", ".", "device", "\n", "self", ".", "adapters", "=", "nn", ".", "ModuleDict", "(", "dict", "(", ")", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "input_dim", "=", "config", ".", "input_dim", "\n", "self", ".", "down_sample_size", "=", "self", ".", "input_dim", "//", "config", ".", "reduction_factor", "\n", "self", ".", "meta_up_sampler", "=", "AdapterHyperNet", "(", "config", ",", "self", ".", "input_dim", ",", "self", ".", "down_sample_size", ")", "\n", "self", ".", "meta_down_sampler", "=", "AdapterHyperNet", "(", "config", ",", "self", ".", "down_sample_size", ",", "self", ".", "input_dim", ")", "\n", "self", ".", "activation_type", "=", "config", ".", "non_linearity", ".", "lower", "(", ")", "\n", "self", ".", "add_layer_norm_before_adapter", "=", "config", ".", "add_layer_norm_before_adapter", "\n", "self", ".", "add_layer_norm_after_adapter", "=", "config", ".", "add_layer_norm_after_adapter", "\n", "self", ".", "conditional_layer_norm", "=", "config", ".", "conditional_layer_norm", "\n", "if", "self", ".", "add_layer_norm_after_adapter", ":", "\n", "            ", "if", "self", ".", "conditional_layer_norm", ":", "\n", "                ", "self", ".", "post_layernorm_hypernet", "=", "LayerNormHyperNet", "(", "config", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "post_layer_norm", "=", "nn", ".", "LayerNorm", "(", "self", ".", "input_dim", ")", "\n", "", "", "if", "self", ".", "add_layer_norm_before_adapter", ":", "\n", "            ", "if", "self", ".", "conditional_layer_norm", ":", "\n", "                ", "self", ".", "pre_layernorm_hypernet", "=", "LayerNormHyperNet", "(", "config", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "pre_layer_norm", "=", "nn", ".", "LayerNorm", "(", "self", ".", "input_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.MetaAdapterController.call_adapter": [[145, 152], ["adapter_controller.MetaAdapterController.meta_up_sampler", "adapter_controller.MetaAdapterController.meta_down_sampler", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "transformers.activations.get_activation"], "methods", ["None"], ["", "", "", "def", "call_adapter", "(", "self", ",", "inputs", ",", "task_embedding", ")", ":", "\n", "        ", "weight_up", ",", "bias_up", "=", "self", ".", "meta_up_sampler", "(", "task_embedding", ")", "\n", "weight_down", ",", "bias_down", "=", "self", ".", "meta_down_sampler", "(", "task_embedding", ")", "\n", "down", "=", "F", ".", "linear", "(", "inputs", ",", "weight", "=", "weight_down", ",", "bias", "=", "bias_down", ")", "\n", "middle", "=", "get_activation", "(", "self", ".", "activation_type", ")", "(", "down", ")", "\n", "output", "=", "F", ".", "linear", "(", "middle", ",", "weight", "=", "weight_up", ",", "bias", "=", "bias_up", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.MetaAdapterController.apply_pre_layer_norm": [[153, 160], ["adapter_controller.MetaAdapterController.pre_layernorm_hypernet", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "adapter_controller.MetaAdapterController.pre_layer_norm"], "methods", ["None"], ["", "def", "apply_pre_layer_norm", "(", "self", ",", "inputs", ",", "task_embeddings", ")", ":", "\n", "        ", "\"\"\"Applies pre layer norm to the inputs.\"\"\"", "\n", "if", "self", ".", "conditional_layer_norm", ":", "\n", "            ", "weight", ",", "bias", "=", "self", ".", "pre_layernorm_hypernet", "(", "task_embeddings", ")", "\n", "return", "torch", ".", "nn", ".", "functional", ".", "layer_norm", "(", "inputs", ",", "(", "self", ".", "input_dim", ",", ")", ",", "weight", "=", "weight", ",", "bias", "=", "bias", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "pre_layer_norm", "(", "inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.MetaAdapterController.apply_post_layer_norm": [[161, 168], ["adapter_controller.MetaAdapterController.post_layernorm_hypernet", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "adapter_controller.MetaAdapterController.post_layer_norm"], "methods", ["None"], ["", "", "def", "apply_post_layer_norm", "(", "self", ",", "inputs", ",", "task_embeddings", ")", ":", "\n", "        ", "\"\"\"Applies post layer norm to the inputs.\"\"\"", "\n", "if", "self", ".", "conditional_layer_norm", ":", "\n", "            ", "weight", ",", "bias", "=", "self", ".", "post_layernorm_hypernet", "(", "task_embeddings", ")", "\n", "return", "torch", ".", "nn", ".", "functional", ".", "layer_norm", "(", "inputs", ",", "(", "self", ".", "input_dim", ",", ")", ",", "weight", "=", "weight", ",", "bias", "=", "bias", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "post_layer_norm", "(", "inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.MetaAdapterController.forward": [[169, 184], ["adapter_controller.MetaAdapterController.call_adapter", "adapter_controller.MetaAdapterController.apply_pre_layer_norm", "adapter_controller.MetaAdapterController.apply_post_layer_norm"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.MetaLayersAdapterController.call_adapter", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.MetaAdapterController.apply_pre_layer_norm", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.MetaAdapterController.apply_post_layer_norm"], ["", "", "def", "forward", "(", "self", ",", "task_embedding", ",", "inputs", ")", ":", "\n", "        ", "\"\"\"Retrieves the adapter layer corresponding to the given\n        task. It freezes the adapter layers for all the other tasks\n        and call the selected adapter layer.\n        Args:\n            task: the name of the current task.\n            inputs: the inputs to feed in in the adapter layer.\n        Returns:\n            outputs of the adapter layer.\"\"\"", "\n", "z", "=", "self", ".", "apply_pre_layer_norm", "(", "inputs", ",", "task_embedding", ")", "if", "self", ".", "add_layer_norm_before_adapter", "else", "inputs", "\n", "outputs", "=", "self", ".", "call_adapter", "(", "z", ",", "task_embedding", ")", "\n", "if", "self", ".", "add_layer_norm_after_adapter", ":", "\n", "            ", "outputs", "=", "self", ".", "apply_post_layer_norm", "(", "outputs", ",", "task_embedding", ")", "\n", "", "outputs", "=", "outputs", "+", "inputs", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.MetaLayersAdapterController.__init__": [[190, 196], ["torch.Module.__init__", "config.non_linearity.lower"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "activation_type", "=", "config", ".", "non_linearity", ".", "lower", "(", ")", "\n", "self", ".", "input_dim", "=", "config", ".", "input_dim", "\n", "self", ".", "add_layer_norm_before_adapter", "=", "config", ".", "add_layer_norm_before_adapter", "\n", "self", ".", "add_layer_norm_after_adapter", "=", "config", ".", "add_layer_norm_after_adapter", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.MetaLayersAdapterController.apply_layer_norm": [[197, 202], ["torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm", "torch.nn.functional.layer_norm"], "methods", ["None"], ["", "def", "apply_layer_norm", "(", "self", ",", "inputs", ",", "layer_norm_weights", ")", ":", "\n", "        ", "\"\"\"Applies layer norm to the inputs.\"\"\"", "\n", "return", "torch", ".", "nn", ".", "functional", ".", "layer_norm", "(", "inputs", ",", "(", "self", ".", "input_dim", ",", ")", ",", "\n", "weight", "=", "layer_norm_weights", ".", "weight", ",", "\n", "bias", "=", "layer_norm_weights", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.MetaLayersAdapterController.call_adapter": [[203, 211], ["torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "transformers.activations.get_activation"], "methods", ["None"], ["", "def", "call_adapter", "(", "self", ",", "inputs", ",", "adapter_weights", ")", ":", "\n", "        ", "\"\"\"Computes the output of the adapter layers.\"\"\"", "\n", "down", "=", "F", ".", "linear", "(", "inputs", ",", "weight", "=", "adapter_weights", ".", "down", ".", "weight", ",", "\n", "bias", "=", "adapter_weights", ".", "down", ".", "bias", ")", "\n", "middle", "=", "get_activation", "(", "self", ".", "activation_type", ")", "(", "down", ")", "\n", "output", "=", "F", ".", "linear", "(", "middle", ",", "weight", "=", "adapter_weights", ".", "up", ".", "weight", ",", "\n", "bias", "=", "adapter_weights", ".", "up", ".", "bias", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.MetaLayersAdapterController.forward": [[212, 219], ["adapter_controller.MetaLayersAdapterController.call_adapter", "adapter_controller.MetaLayersAdapterController.apply_layer_norm", "adapter_controller.MetaLayersAdapterController.apply_layer_norm"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.MetaLayersAdapterController.call_adapter", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.MetaLayersAdapterController.apply_layer_norm", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.MetaLayersAdapterController.apply_layer_norm"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "adapter_weights", ")", ":", "\n", "        ", "z", "=", "self", ".", "apply_layer_norm", "(", "inputs", ",", "adapter_weights", ".", "pre_norm", ")", "if", "self", ".", "add_layer_norm_before_adapter", "else", "inputs", "\n", "outputs", "=", "self", ".", "call_adapter", "(", "z", ",", "adapter_weights", ")", "\n", "if", "self", ".", "add_layer_norm_after_adapter", ":", "\n", "            ", "outputs", "=", "self", ".", "apply_layer_norm", "(", "outputs", ",", "adapter_weights", ".", "post_norm", ")", "\n", "", "outputs", "=", "outputs", "+", "inputs", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_controller.AutoAdapterController.get": [[225, 232], ["isinstance", "ValueError", "adapter_controller.MetaAdapterController", "isinstance", "adapter_controller.AdapterController"], "methods", ["None"], ["@", "classmethod", "\n", "def", "get", "(", "cls", ",", "config", ")", ":", "\n", "        ", "if", "isinstance", "(", "config", ",", "MetaAdapterConfig", ")", ":", "\n", "            ", "return", "MetaAdapterController", "(", "config", ")", "\n", "", "elif", "isinstance", "(", "config", ",", "AdapterConfig", ")", ":", "\n", "            ", "return", "AdapterController", "(", "config", ")", "\n", "", "raise", "ValueError", "(", "\"Unrecognized adapter config\"", ",", "config", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_modeling.Adapter.__init__": [[14, 23], ["torch.Module.__init__", "adapter_utils.Activations", "adapter_utils.linear_layer", "adapter_utils.linear_layer", "config.non_linearity.lower"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.linear_layer", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.linear_layer"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "input_dim", "=", "config", ".", "input_dim", "\n", "self", ".", "weight_init_range", "=", "config", ".", "weight_init_range", "\n", "self", ".", "down_sample_size", "=", "self", ".", "input_dim", "//", "config", ".", "reduction_factor", "\n", "self", ".", "activation", "=", "Activations", "(", "config", ".", "non_linearity", ".", "lower", "(", ")", ")", "\n", "self", ".", "down_sampler", "=", "linear_layer", "(", "self", ".", "input_dim", ",", "self", ".", "down_sample_size", ",", "std", "=", "self", ".", "weight_init_range", ")", "\n", "self", ".", "up_sampler", "=", "linear_layer", "(", "self", ".", "down_sample_size", ",", "self", ".", "input_dim", ",", "std", "=", "self", ".", "weight_init_range", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_modeling.Adapter.forward": [[24, 28], ["adapter_modeling.Adapter.down_sampler", "adapter_modeling.Adapter.activation", "adapter_modeling.Adapter.up_sampler"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "z", "=", "self", ".", "down_sampler", "(", "x", ")", "\n", "z", "=", "self", ".", "activation", "(", "z", ")", "\n", "return", "self", ".", "up_sampler", "(", "z", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_modeling.AdapterHyperNet.__init__": [[33, 46], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "adapter_utils.linear_layer", "adapter_utils.linear_layer"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.linear_layer", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.linear_layer"], ["def", "__init__", "(", "self", ",", "config", ",", "input_dim", ",", "output_dim", ")", ":", "\n", "        ", "super", "(", "AdapterHyperNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hidden_dim", "=", "config", ".", "hidden_dim", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "output_dim", "=", "output_dim", "\n", "self", ".", "train_task_embeddings", "=", "config", ".", "train_task_embeddings", "\n", "self", ".", "task_embedding_dim", "=", "config", ".", "projected_task_embedding_dim", "if", "config", ".", "train_task_embeddings", "else", "config", ".", "task_embedding_dim", "\n", "# Considers weight and bias parameters for generating adapter weights.", "\n", "self", ".", "weight_generator", "=", "nn", ".", "Sequential", "(", "\n", "linear_layer", "(", "self", ".", "task_embedding_dim", ",", "self", ".", "input_dim", "*", "self", ".", "output_dim", ")", ")", "\n", "self", ".", "bias_generator", "=", "nn", ".", "Sequential", "(", "\n", "linear_layer", "(", "self", ".", "task_embedding_dim", ",", "self", ".", "input_dim", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_modeling.AdapterHyperNet.forward": [[47, 52], ["task_embedding.view.view.view", "adapter_modeling.AdapterHyperNet.weight_generator().view", "adapter_modeling.AdapterHyperNet.bias_generator().view", "adapter_modeling.AdapterHyperNet.weight_generator", "adapter_modeling.AdapterHyperNet.bias_generator"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "task_embedding", ")", ":", "\n", "        ", "task_embedding", "=", "task_embedding", ".", "view", "(", "-", "1", ")", "\n", "weight", "=", "self", ".", "weight_generator", "(", "task_embedding", ")", ".", "view", "(", "self", ".", "input_dim", ",", "self", ".", "output_dim", ")", "\n", "bias", "=", "self", ".", "bias_generator", "(", "task_embedding", ")", ".", "view", "(", "-", "1", ")", "\n", "return", "weight", ",", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_modeling.AdapterLayersHyperNet.__init__": [[58, 66], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "adapter_utils.linear_layer", "adapter_utils.linear_layer"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.linear_layer", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_utils.linear_layer"], ["def", "__init__", "(", "self", ",", "config", ",", "input_dim", ",", "output_dim", ")", ":", "\n", "        ", "super", "(", "AdapterLayersHyperNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "output_dim", "=", "output_dim", "\n", "self", ".", "weight_generator", "=", "nn", ".", "Sequential", "(", "\n", "linear_layer", "(", "config", ".", "projected_task_embedding_dim", ",", "self", ".", "input_dim", "*", "self", ".", "output_dim", ")", ")", "\n", "self", ".", "bias_generator", "=", "nn", ".", "Sequential", "(", "\n", "linear_layer", "(", "config", ".", "projected_task_embedding_dim", ",", "self", ".", "input_dim", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_modeling.AdapterLayersHyperNet.forward": [[67, 71], ["adapter_modeling.AdapterLayersHyperNet.weight_generator().view", "adapter_modeling.AdapterLayersHyperNet.bias_generator().view", "adapter_outputs.SamplerOutput", "adapter_modeling.AdapterLayersHyperNet.weight_generator", "adapter_modeling.AdapterLayersHyperNet.bias_generator"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "embeddings", ")", ":", "\n", "        ", "weight", "=", "self", ".", "weight_generator", "(", "embeddings", ")", ".", "view", "(", "self", ".", "input_dim", ",", "self", ".", "output_dim", ")", "\n", "bias", "=", "self", ".", "bias_generator", "(", "embeddings", ")", ".", "view", "(", "-", "1", ")", "\n", "return", "SamplerOutput", "(", "weight", "=", "weight", ",", "bias", "=", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_modeling.AdapterLayersHyperNetController.__init__": [[78, 118], ["torch.Module.__init__", "torch.Embedding().to", "torch.Embedding().to", "adapter_utils.TaskHyperNet", "adapter_modeling.AdapterLayersHyperNet", "adapter_modeling.AdapterLayersHyperNet", "adapter_modeling.AdapterLayersHyperNet", "adapter_modeling.AdapterLayersHyperNet", "torch.LayerNorm", "torch.LayerNorm", "adapter_utils.LayerNormHyperNet", "adapter_utils.LayerNormHyperNet", "adapter_utils.LayerNormHyperNet", "adapter_utils.LayerNormHyperNet", "torch.Embedding", "torch.Embedding"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_layers", "=", "6", ")", ":", "\n", "        ", "super", "(", "AdapterLayersHyperNetController", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "layer_norm_epsilon", "=", "1e-6", "\n", "self", ".", "max_position_embeddings", "=", "2", "\n", "self", ".", "device", "=", "config", ".", "device", "\n", "self", ".", "task_embedding_dim", "=", "config", ".", "task_embedding_dim", "\n", "self", ".", "layer_id_embeddings", "=", "nn", ".", "Embedding", "(", "self", ".", "num_layers", ",", "\n", "self", ".", "task_embedding_dim", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "# self.token_type_embeddings = nn.Embedding(self.max_position_embeddings,", "\n", "#                                          self.task_embedding_dim).to(self.device)", "\n", "config", ".", "task_embedding_dim", "=", "self", ".", "task_embedding_dim", "*", "2", "\n", "self", ".", "task_hypernet", "=", "TaskHyperNet", "(", "config", ")", "\n", "config", ".", "task_embedding_dim", "=", "self", ".", "task_embedding_dim", "\n", "self", ".", "unique_hyper_net_layer_norm", "=", "config", ".", "unique_hyper_net_layer_norm", "\n", "if", "self", ".", "unique_hyper_net_layer_norm", ":", "\n", "            ", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "projected_task_embedding_dim", ",", "eps", "=", "self", ".", "layer_norm_epsilon", ")", "\n", "", "self", ".", "input_dim", "=", "config", ".", "input_dim", "\n", "self", ".", "down_sample_size", "=", "self", ".", "input_dim", "//", "config", ".", "reduction_factor", "\n", "# Defines the adapters hyper-nets.", "\n", "self", ".", "feed_forward_up_sampler_hyper_net", "=", "AdapterLayersHyperNet", "(", "config", ",", "\n", "self", ".", "input_dim", ",", "self", ".", "down_sample_size", ")", "\n", "self", ".", "feed_forward_down_sampler_hyper_net", "=", "AdapterLayersHyperNet", "(", "config", ",", "\n", "self", ".", "down_sample_size", ",", "self", ".", "input_dim", ")", "\n", "self", ".", "self_attention_up_sampler_hyper_net", "=", "AdapterLayersHyperNet", "(", "config", ",", "\n", "self", ".", "input_dim", ",", "self", ".", "down_sample_size", ")", "\n", "self", ".", "self_attention_down_sampler_hyper_net", "=", "AdapterLayersHyperNet", "(", "config", ",", "\n", "self", ".", "down_sample_size", ",", "self", ".", "input_dim", ")", "\n", "# Defines the layer norms' hyper net.", "\n", "self", ".", "add_layer_norm_before_adapter", "=", "config", ".", "add_layer_norm_before_adapter", "\n", "self", ".", "add_layer_norm_after_adapter", "=", "config", ".", "add_layer_norm_after_adapter", "\n", "self", ".", "train_task_embeddings", "=", "config", ".", "train_task_embeddings", "\n", "config", ".", "train_task_embeddings", "=", "True", "\n", "if", "self", ".", "add_layer_norm_before_adapter", ":", "\n", "            ", "self", ".", "feed_forward_pre_layernorm_hypernet", "=", "LayerNormHyperNet", "(", "config", ")", "\n", "self", ".", "self_attention_pre_layernorm_hypernet", "=", "LayerNormHyperNet", "(", "config", ")", "\n", "", "if", "self", ".", "add_layer_norm_after_adapter", ":", "\n", "            ", "self", ".", "feed_forward_post_layernorm_hypernet", "=", "LayerNormHyperNet", "(", "config", ")", "\n", "self", ".", "self_attention_post_layernorm_hypernet", "=", "LayerNormHyperNet", "(", "config", ")", "\n", "", "config", ".", "train_task_embeddings", "=", "self", ".", "train_task_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_modeling.AdapterLayersHyperNetController.get_embedding": [[119, 130], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "adapter_modeling.AdapterLayersHyperNetController.layer_id_embeddings", "layer_embedding.view.view.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "adapter_modeling.AdapterLayersHyperNetController.task_hypernet", "adapter_modeling.AdapterLayersHyperNetController.view", "adapter_modeling.AdapterLayersHyperNetController.LayerNorm", "task_embedding.view", "layer_embedding.view.view.view"], "methods", ["None"], ["", "def", "get_embedding", "(", "self", ",", "task_embedding", ",", "layer_id", ")", ":", "\n", "        ", "\"\"\"Concatenates the task embedding with the embedding for the layer id and\n        returns the final joint embedding.\"\"\"", "\n", "layer_id_tensor", "=", "torch", ".", "tensor", "(", "[", "layer_id", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "device", ")", "\n", "layer_embedding", "=", "self", ".", "layer_id_embeddings", "(", "layer_id_tensor", ")", "\n", "layer_embedding", "=", "layer_embedding", ".", "view", "(", "-", "1", ")", "\n", "embeddings", "=", "torch", ".", "cat", "(", "[", "task_embedding", ".", "view", "(", "1", ",", "-", "1", ")", ",", "layer_embedding", ".", "view", "(", "1", ",", "-", "1", ")", "]", ",", "axis", "=", "0", ")", "\n", "embeddings", "=", "self", ".", "task_hypernet", "(", "embeddings", ".", "view", "(", "-", "1", ")", ")", "\n", "if", "self", ".", "unique_hyper_net_layer_norm", ":", "\n", "            ", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_modeling.AdapterLayersHyperNetController.forward": [[131, 153], ["adapter_modeling.AdapterLayersHyperNetController.get_embedding", "adapter_modeling.AdapterLayersHyperNetController.feed_forward_down_sampler_hyper_net", "adapter_modeling.AdapterLayersHyperNetController.feed_forward_up_sampler_hyper_net", "adapter_modeling.AdapterLayersHyperNetController.self_attention_down_sampler_hyper_net", "adapter_modeling.AdapterLayersHyperNetController.self_attention_up_sampler_hyper_net", "adapter_outputs.AdapterOutput", "adapter_outputs.AdapterOutput", "adapter_outputs.AdapterT5BlockOutput", "adapter_modeling.AdapterLayersHyperNetController.feed_forward_pre_layernorm_hypernet", "adapter_outputs.LayerNormOutput", "adapter_modeling.AdapterLayersHyperNetController.self_attention_pre_layernorm_hypernet", "adapter_outputs.LayerNormOutput", "adapter_modeling.AdapterLayersHyperNetController.feed_forward_post_layernorm_hypernet", "adapter_outputs.LayerNormOutput", "adapter_modeling.AdapterLayersHyperNetController.self_attention_post_layernorm_hypernet", "adapter_outputs.LayerNormOutput"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_modeling.AdapterLayersOneHyperNetController.get_embedding"], ["", "def", "forward", "(", "self", ",", "task_embedding", ",", "layer_id", ")", ":", "\n", "        ", "embeddings", "=", "self", ".", "get_embedding", "(", "task_embedding", ",", "layer_id", ")", "\n", "# Generates the adapters weights in feed-forward and self-attention modules.", "\n", "feed_forward_down", "=", "self", ".", "feed_forward_down_sampler_hyper_net", "(", "embeddings", ")", "\n", "feed_forward_up", "=", "self", ".", "feed_forward_up_sampler_hyper_net", "(", "embeddings", ")", "\n", "self_attention_down", "=", "self", ".", "self_attention_down_sampler_hyper_net", "(", "embeddings", ")", "\n", "self_attention_up", "=", "self", ".", "self_attention_up_sampler_hyper_net", "(", "embeddings", ")", "\n", "feed_forward_output", "=", "AdapterOutput", "(", "up", "=", "feed_forward_up", ",", "down", "=", "feed_forward_down", ")", "\n", "self_attention_output", "=", "AdapterOutput", "(", "up", "=", "self_attention_up", ",", "down", "=", "self_attention_down", ")", "\n", "# Generates the weights and baises for pre and post layer norms.", "\n", "if", "self", ".", "add_layer_norm_before_adapter", ":", "\n", "            ", "weight", ",", "bias", "=", "self", ".", "feed_forward_pre_layernorm_hypernet", "(", "embeddings", ")", "\n", "feed_forward_output", ".", "pre_norm", "=", "LayerNormOutput", "(", "weight", "=", "weight", ",", "bias", "=", "bias", ")", "\n", "weight", ",", "bias", "=", "self", ".", "self_attention_pre_layernorm_hypernet", "(", "embeddings", ")", "\n", "self_attention_output", ".", "pre_norm", "=", "LayerNormOutput", "(", "weight", "=", "weight", ",", "bias", "=", "bias", ")", "\n", "", "if", "self", ".", "add_layer_norm_after_adapter", ":", "\n", "            ", "weight", ",", "bias", "=", "self", ".", "feed_forward_post_layernorm_hypernet", "(", "embeddings", ")", "\n", "feed_forward_output", ".", "post_norm", "=", "LayerNormOutput", "(", "weight", "=", "weight", ",", "bias", "=", "bias", ")", "\n", "weight", ",", "bias", "=", "self", ".", "self_attention_post_layernorm_hypernet", "(", "embeddings", ")", "\n", "self_attention_output", ".", "post_norm", "=", "LayerNormOutput", "(", "weight", "=", "weight", ",", "bias", "=", "bias", ")", "\n", "", "return", "AdapterT5BlockOutput", "(", "feed_forward", "=", "feed_forward_output", ",", "\n", "self_attention", "=", "self_attention_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_modeling.AdapterLayersOneHyperNetController.__init__": [[160, 195], ["torch.Module.__init__", "torch.Embedding().to", "torch.Embedding().to", "torch.Embedding().to", "torch.Embedding().to", "adapter_utils.TaskHyperNet", "adapter_modeling.AdapterLayersHyperNet", "adapter_modeling.AdapterLayersHyperNet", "torch.LayerNorm", "torch.LayerNorm", "adapter_utils.LayerNormHyperNet", "adapter_utils.LayerNormHyperNet", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_layers", "=", "6", ")", ":", "\n", "        ", "super", "(", "AdapterLayersOneHyperNetController", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "layer_norm_epsilon", "=", "1e-6", "\n", "self", ".", "max_position_embeddings", "=", "2", "\n", "self", ".", "device", "=", "config", ".", "device", "\n", "self", ".", "task_embedding_dim", "=", "config", ".", "task_embedding_dim", "\n", "self", ".", "layer_id_embeddings", "=", "nn", ".", "Embedding", "(", "self", ".", "num_layers", ",", "\n", "self", ".", "task_embedding_dim", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "# This is 2 types of adapters for feed-forward, and self-attention.", "\n", "self", ".", "adapters_block_type", "=", "nn", ".", "Embedding", "(", "2", ",", "self", ".", "task_embedding_dim", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "config", ".", "task_embedding_dim", "=", "self", ".", "task_embedding_dim", "*", "3", "\n", "self", ".", "task_hypernet", "=", "TaskHyperNet", "(", "config", ")", "\n", "config", ".", "task_embedding_dim", "=", "self", ".", "task_embedding_dim", "\n", "self", ".", "unique_hyper_net_layer_norm", "=", "config", ".", "unique_hyper_net_layer_norm", "\n", "if", "self", ".", "unique_hyper_net_layer_norm", ":", "\n", "            ", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "projected_task_embedding_dim", ",", "eps", "=", "self", ".", "layer_norm_epsilon", ")", "\n", "", "self", ".", "input_dim", "=", "config", ".", "input_dim", "\n", "self", ".", "down_sample_size", "=", "self", ".", "input_dim", "//", "config", ".", "reduction_factor", "\n", "\n", "# Defines the adapters hyper-nets.", "\n", "self", ".", "up_sampler_hyper_net", "=", "AdapterLayersHyperNet", "(", "config", ",", "self", ".", "input_dim", ",", "self", ".", "down_sample_size", ")", "\n", "self", ".", "down_sampler_hyper_net", "=", "AdapterLayersHyperNet", "(", "config", ",", "self", ".", "down_sample_size", ",", "self", ".", "input_dim", ")", "\n", "\n", "# Defines the layer norms' hyper net.", "\n", "self", ".", "add_layer_norm_before_adapter", "=", "config", ".", "add_layer_norm_before_adapter", "\n", "self", ".", "add_layer_norm_after_adapter", "=", "config", ".", "add_layer_norm_after_adapter", "\n", "self", ".", "train_task_embeddings", "=", "config", ".", "train_task_embeddings", "\n", "config", ".", "train_task_embeddings", "=", "True", "\n", "if", "self", ".", "add_layer_norm_before_adapter", ":", "\n", "            ", "self", ".", "pre_layernorm_hypernet", "=", "LayerNormHyperNet", "(", "config", ")", "\n", "", "if", "self", ".", "add_layer_norm_after_adapter", ":", "\n", "            ", "self", ".", "post_layernorm_hypernet", "=", "LayerNormHyperNet", "(", "config", ")", "\n", "", "config", ".", "train_task_embeddings", "=", "self", ".", "train_task_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_modeling.AdapterLayersOneHyperNetController.get_embedding": [[196, 211], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "adapter_modeling.AdapterLayersOneHyperNetController.layer_id_embeddings", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "adapter_modeling.AdapterLayersOneHyperNetController.adapters_block_type", "layer_embedding.view.view.view", "type_embedding.view.view.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "adapter_modeling.AdapterLayersOneHyperNetController.task_hypernet", "adapter_modeling.AdapterLayersOneHyperNetController.view", "adapter_modeling.AdapterLayersOneHyperNetController.LayerNorm", "task_embedding.view", "layer_embedding.view.view.view", "type_embedding.view.view.view"], "methods", ["None"], ["", "def", "get_embedding", "(", "self", ",", "task_embedding", ",", "layer_id", ",", "block_type", ")", ":", "\n", "        ", "\"\"\"Concatenates the task embedding with the embedding for the layer id and\n        returns the final joint embedding.\"\"\"", "\n", "layer_id_tensor", "=", "torch", ".", "tensor", "(", "[", "layer_id", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "device", ")", "\n", "layer_embedding", "=", "self", ".", "layer_id_embeddings", "(", "layer_id_tensor", ")", "\n", "type_id_tensor", "=", "torch", ".", "tensor", "(", "[", "block_type", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "device", ")", "\n", "type_embedding", "=", "self", ".", "adapters_block_type", "(", "type_id_tensor", ")", "\n", "layer_embedding", "=", "layer_embedding", ".", "view", "(", "-", "1", ")", "\n", "type_embedding", "=", "type_embedding", ".", "view", "(", "-", "1", ")", "\n", "embeddings", "=", "torch", ".", "cat", "(", "[", "task_embedding", ".", "view", "(", "1", ",", "-", "1", ")", ",", "layer_embedding", ".", "view", "(", "1", ",", "-", "1", ")", ",", "type_embedding", ".", "view", "(", "1", ",", "-", "1", ")", "]", ",", "\n", "axis", "=", "0", ")", "\n", "embeddings", "=", "self", ".", "task_hypernet", "(", "embeddings", ".", "view", "(", "-", "1", ")", ")", "\n", "if", "self", ".", "unique_hyper_net_layer_norm", ":", "\n", "            ", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_modeling.AdapterLayersOneHyperNetController.forward": [[212, 242], ["adapter_modeling.AdapterLayersOneHyperNetController.get_embedding", "adapter_modeling.AdapterLayersOneHyperNetController.get_embedding", "adapter_modeling.AdapterLayersOneHyperNetController.down_sampler_hyper_net", "adapter_modeling.AdapterLayersOneHyperNetController.up_sampler_hyper_net", "adapter_modeling.AdapterLayersOneHyperNetController.down_sampler_hyper_net", "adapter_modeling.AdapterLayersOneHyperNetController.up_sampler_hyper_net", "adapter_outputs.AdapterOutput", "adapter_outputs.AdapterOutput", "adapter_outputs.AdapterT5BlockOutput", "adapter_modeling.AdapterLayersOneHyperNetController.pre_layernorm_hypernet", "adapter_outputs.LayerNormOutput", "adapter_modeling.AdapterLayersOneHyperNetController.pre_layernorm_hypernet", "adapter_outputs.LayerNormOutput", "adapter_modeling.AdapterLayersOneHyperNetController.post_layernorm_hypernet", "adapter_outputs.LayerNormOutput", "adapter_modeling.AdapterLayersOneHyperNetController.post_layernorm_hypernet", "adapter_outputs.LayerNormOutput"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_modeling.AdapterLayersOneHyperNetController.get_embedding", "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_modeling.AdapterLayersOneHyperNetController.get_embedding"], ["", "def", "forward", "(", "self", ",", "task_embedding", ",", "layer_id", ")", ":", "\n", "        ", "feed_forward_embeddings", "=", "self", ".", "get_embedding", "(", "task_embedding", ",", "layer_id", ",", "0", ")", "\n", "self_attention_embeddings", "=", "self", ".", "get_embedding", "(", "task_embedding", ",", "layer_id", ",", "1", ")", "\n", "\n", "# Generates the adapters weights in feed-forward.", "\n", "feed_forward_down", "=", "self", ".", "down_sampler_hyper_net", "(", "feed_forward_embeddings", ")", "\n", "feed_forward_up", "=", "self", ".", "up_sampler_hyper_net", "(", "feed_forward_embeddings", ")", "\n", "\n", "# Generates the adapter weights in self-attention.", "\n", "self_attention_down", "=", "self", ".", "down_sampler_hyper_net", "(", "self_attention_embeddings", ")", "\n", "self_attention_up", "=", "self", ".", "up_sampler_hyper_net", "(", "self_attention_embeddings", ")", "\n", "\n", "feed_forward_output", "=", "AdapterOutput", "(", "up", "=", "feed_forward_up", ",", "down", "=", "feed_forward_down", ")", "\n", "self_attention_output", "=", "AdapterOutput", "(", "up", "=", "self_attention_up", ",", "down", "=", "self_attention_down", ")", "\n", "\n", "# Generates the weights and baises for pre and post layer norms.", "\n", "if", "self", ".", "add_layer_norm_before_adapter", ":", "\n", "            ", "weight", ",", "bias", "=", "self", ".", "pre_layernorm_hypernet", "(", "feed_forward_embeddings", ")", "\n", "feed_forward_output", ".", "pre_norm", "=", "LayerNormOutput", "(", "weight", "=", "weight", ",", "bias", "=", "bias", ")", "\n", "weight", ",", "bias", "=", "self", ".", "pre_layernorm_hypernet", "(", "self_attention_embeddings", ")", "\n", "self_attention_output", ".", "pre_norm", "=", "LayerNormOutput", "(", "weight", "=", "weight", ",", "bias", "=", "bias", ")", "\n", "\n", "", "if", "self", ".", "add_layer_norm_after_adapter", ":", "\n", "            ", "weight", ",", "bias", "=", "self", ".", "post_layernorm_hypernet", "(", "feed_forward_embeddings", ")", "\n", "feed_forward_output", ".", "post_norm", "=", "LayerNormOutput", "(", "weight", "=", "weight", ",", "bias", "=", "bias", ")", "\n", "weight", ",", "bias", "=", "self", ".", "post_layernorm_hypernet", "(", "self_attention_embeddings", ")", "\n", "self_attention_output", ".", "post_norm", "=", "LayerNormOutput", "(", "weight", "=", "weight", ",", "bias", "=", "bias", ")", "\n", "\n", "", "return", "AdapterT5BlockOutput", "(", "feed_forward", "=", "feed_forward_output", ",", "\n", "self_attention", "=", "self_attention_output", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.adapters.adapter_configuration.AutoAdapterConfig.get": [[52, 59], ["ValueError", "ADAPTER_CONFIG_MAPPING.keys"], "methods", ["None"], ["@", "classmethod", "\n", "def", "get", "(", "cls", ",", "config_name", ":", "str", ")", ":", "\n", "        ", "if", "config_name", "in", "ADAPTER_CONFIG_MAPPING", ":", "\n", "            ", "return", "ADAPTER_CONFIG_MAPPING", "[", "config_name", "]", "(", ")", "\n", "", "raise", "ValueError", "(", "\n", "\"Unrecognized adapter config type identifier: {}. Should contain one of {}\"", "\n", ".", "format", "(", "config_name", ",", "\", \"", ".", "join", "(", "ADAPTER_CONFIG_MAPPING", ".", "keys", "(", ")", ")", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.metrics.metrics.rouge": [[15, 18], ["hyperformer.third_party.utils.calculate_rouge"], "function", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.calculate_rouge"], ["def", "rouge", "(", "predictions", ",", "targets", ")", "->", "dict", ":", "\n", "    ", "\"\"\"Computes rouge score.\"\"\"", "\n", "return", "calculate_rouge", "(", "predictions", ",", "targets", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.metrics.metrics.bleu": [[20, 23], ["hyperformer.third_party.utils.calculate_bleu"], "function", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.calculate_bleu"], ["", "def", "bleu", "(", "predictions", ",", "targets", ")", "->", "dict", ":", "\n", "    ", "\"\"\"Computes bleu score.\"\"\"", "\n", "return", "calculate_bleu", "(", "predictions", ",", "targets", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.metrics.metrics.accuracy": [[25, 28], ["numpy.array", "numpy.array"], "function", ["None"], ["", "def", "accuracy", "(", "predictions", ",", "targets", ")", "->", "dict", ":", "\n", "    ", "\"\"\"Computes the average accuracy.\"\"\"", "\n", "return", "{", "\"acc\"", ":", "100", "*", "(", "(", "np", ".", "array", "(", "predictions", ")", "==", "np", ".", "array", "(", "targets", ")", ")", ".", "mean", "(", ")", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.metrics.metrics.pearson_corrcoef": [[30, 40], ["math.isnan", "scipy.stats.pearsonr"], "function", ["None"], ["", "def", "pearson_corrcoef", "(", "predictions", ",", "targets", ")", "->", "dict", ":", "\n", "    ", "\"\"\"Computes Pearson correlation coefficient.\"\"\"", "\n", "pearson_corrcoef", "=", "100", "*", "scipy", ".", "stats", ".", "pearsonr", "(", "targets", ",", "predictions", ")", "[", "0", "]", "\n", "\n", "# Note that if all the predictions will be the same, spearman", "\n", "# correlation is nan, to gaurad against this, we check the output", "\n", "# and return 0 in this case.", "\n", "if", "math", ".", "isnan", "(", "pearson_corrcoef", ")", ":", "\n", "        ", "pearson_corrcoef", "=", "0", "\n", "", "return", "{", "\"pearson_corrcoef\"", ":", "pearson_corrcoef", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.metrics.metrics.spearman_corrcoef": [[42, 52], ["math.isnan", "scipy.stats.spearmanr"], "function", ["None"], ["", "def", "spearman_corrcoef", "(", "predictions", ",", "targets", ")", "->", "dict", ":", "\n", "    ", "\"\"\"Computes Spearman correlation coefficient.\"\"\"", "\n", "spearman_corrcoef", "=", "100", "*", "scipy", ".", "stats", ".", "spearmanr", "(", "targets", ",", "predictions", ")", "[", "0", "]", "\n", "\n", "# Note that if all the predictions will be the same, spearman", "\n", "# correlation is nan, to gaurad against this, we check the output", "\n", "# and return 0 in this case.", "\n", "if", "math", ".", "isnan", "(", "spearman_corrcoef", ")", ":", "\n", "        ", "spearman_corrcoef", "=", "0", "\n", "", "return", "{", "\"spearman_corrcoef\"", ":", "spearman_corrcoef", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.metrics.metrics.f1_score_with_invalid": [[54, 68], ["numpy.logical_and", "numpy.asarray", "numpy.asarray", "sklearn.metrics.f1_score"], "function", ["None"], ["", "def", "f1_score_with_invalid", "(", "predictions", ",", "targets", ")", "->", "dict", ":", "\n", "    ", "\"\"\"Computes F1 score,  with any prediction != 0 or 1 is counted as incorrect.\n    Args:\n      targets: list of targets, either 0 or 1\n      predictions: list of predictions, any integer value\n    Returns:\n      F1 score, where any prediction != 0 or 1 is counted as wrong.\n    \"\"\"", "\n", "targets", ",", "predictions", "=", "np", ".", "asarray", "(", "targets", ")", ",", "np", ".", "asarray", "(", "predictions", ")", "\n", "# Get indices of invalid predictions.", "\n", "invalid_idx_mask", "=", "np", ".", "logical_and", "(", "predictions", "!=", "0", ",", "predictions", "!=", "1", ")", "\n", "# For any prediction != 0 or 1, we set the prediction to the opposite of its corresponding target.", "\n", "predictions", "[", "invalid_idx_mask", "]", "=", "1", "-", "targets", "[", "invalid_idx_mask", "]", "\n", "return", "{", "\"f1\"", ":", "100", "*", "sklearn", ".", "metrics", ".", "f1_score", "(", "targets", ",", "predictions", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.metrics.metrics.matthews_corrcoef": [[71, 74], ["sklearn.metrics.matthews_corrcoef"], "function", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.metrics.metrics.matthews_corrcoef"], ["", "def", "matthews_corrcoef", "(", "predictions", ",", "targets", ")", "->", "dict", ":", "\n", "    ", "\"\"\"Computes the Matthews correlation coefficient.\"\"\"", "\n", "return", "{", "\"mcc\"", ":", "100", "*", "sklearn", ".", "metrics", ".", "matthews_corrcoef", "(", "targets", ",", "predictions", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.metrics.metrics.build_compute_metrics_fn": [[76, 113], ["numpy.count_nonzero", "tokenizer.batch_decode", "tokenizer.batch_decode", "hyperformer.third_party.utils.lmap", "hyperformer.third_party.utils.lmap", "metrics.build_compute_metrics_fn.decode_pred"], "function", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.lmap", "home.repos.pwc.inspect_result.rabeehk_hyperformer.utils.utils.lmap"], ["", "def", "build_compute_metrics_fn", "(", "task_names", ":", "List", "[", "str", "]", ",", "\n", "tokenizer", ":", "PreTrainedTokenizer", ")", "->", "Callable", "[", "[", "EvalPrediction", "]", ",", "Dict", "]", ":", "\n", "    ", "\"\"\"Builds a dictionary from each task to the task metric.\"\"\"", "\n", "\n", "def", "non_pad_len", "(", "tokens", ":", "np", ".", "ndarray", ")", "->", "int", ":", "\n", "        ", "return", "np", ".", "count_nonzero", "(", "tokens", "!=", "tokenizer", ".", "pad_token_id", ")", "\n", "\n", "", "def", "decode_pred", "(", "pred", ":", "EvalPrediction", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "pred_str", "=", "tokenizer", ".", "batch_decode", "(", "pred", ".", "predictions", ",", "skip_special_tokens", "=", "True", ")", "\n", "label_str", "=", "tokenizer", ".", "batch_decode", "(", "pred", ".", "label_ids", ",", "skip_special_tokens", "=", "True", ")", "\n", "pred_str", "=", "lmap", "(", "str", ".", "strip", ",", "pred_str", ")", "\n", "label_str", "=", "lmap", "(", "str", ".", "strip", ",", "label_str", ")", "\n", "return", "pred_str", ",", "label_str", "\n", "\n", "", "def", "compute_metrics", "(", "pred", ":", "EvalPrediction", ",", "metrics", ",", "post_processor", "=", "None", ")", "->", "Dict", ":", "\n", "        ", "pred_str", ",", "label_str", "=", "decode_pred", "(", "pred", ")", "\n", "\n", "# Applies task post-processor.", "\n", "if", "post_processor", "is", "not", "None", ":", "\n", "            ", "pred_str", "=", "[", "post_processor", "(", "pred", ")", "for", "pred", "in", "pred_str", "]", "\n", "label_str", "=", "[", "post_processor", "(", "label", ")", "for", "label", "in", "label_str", "]", "\n", "\n", "", "eval_results", "=", "{", "}", "\n", "for", "metric", "in", "metrics", ":", "\n", "            ", "eval_results", ".", "update", "(", "metric", "(", "pred_str", ",", "label_str", ")", ")", "\n", "if", "metric", ".", "__name__", "in", "[", "'bleu'", ",", "'rouge'", "]", ":", "\n", "                ", "gen_len", "=", "np", ".", "round", "(", "np", ".", "mean", "(", "lmap", "(", "non_pad_len", ",", "pred", ".", "predictions", ")", ")", ",", "1", ")", "\n", "eval_results", ".", "update", "(", "{", "\"gen_len\"", ":", "gen_len", "}", ")", "\n", "", "", "return", "eval_results", "\n", "\n", "", "def", "tasks_metrics", "(", "task", ")", "->", "Dict", ":", "\n", "        ", "from", "data", ".", "tasks", "import", "TASK_MAPPING", "\n", "from", "data", ".", "postprocessors", "import", "get_post_processor", "\n", "return", "functools", ".", "partial", "(", "compute_metrics", ",", "metrics", "=", "TASK_MAPPING", "[", "task", "]", ".", "metrics", ",", "\n", "post_processor", "=", "get_post_processor", "(", "task", ")", ")", "\n", "\n", "", "return", "{", "task", ":", "tasks_metrics", "(", "task", ")", "for", "task", "in", "task_names", "}", "\n", "", ""]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.postprocessors.string_to_float": [[1, 7], ["float"], "function", ["None"], ["def", "string_to_float", "(", "string", ",", "default", "=", "-", "1.", ")", ":", "\n", "    ", "\"\"\"Converts string to float, using default when conversion not possible.\"\"\"", "\n", "try", ":", "\n", "        ", "return", "float", "(", "string", ")", "\n", "", "except", "ValueError", ":", "\n", "        ", "return", "default", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.postprocessors.string_to_int": [[9, 15], ["int"], "function", ["None"], ["", "", "def", "string_to_int", "(", "string", ",", "default", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\"Converts string to int, using default when conversion not possible.\"\"\"", "\n", "try", ":", "\n", "        ", "return", "int", "(", "string", ")", "\n", "", "except", "ValueError", ":", "\n", "        ", "return", "default", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.postprocessors.get_post_processor": [[17, 26], ["None"], "function", ["None"], ["", "", "def", "get_post_processor", "(", "task", ")", ":", "\n", "    ", "\"\"\"Returns post processor required to apply on the predictions/targets\n    before computing metrics for each task.\"\"\"", "\n", "if", "task", "==", "\"stsb\"", ":", "\n", "        ", "return", "string_to_float", "\n", "", "elif", "task", "in", "[", "\"qqp\"", ",", "\"cola\"", ",", "\"mrpc\"", "]", ":", "\n", "        ", "return", "string_to_int", "\n", "", "else", ":", "\n", "        ", "return", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.__init__": [[51, 53], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "seed", "=", "42", ")", ":", "\n", "        ", "self", ".", "seed", "=", "seed", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.get_sampled_split": [[54, 64], ["tasks.AbstractTaskDataset.load_dataset", "len", "tasks.AbstractTaskDataset.check_n_obs"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.check_n_obs"], ["", "def", "get_sampled_split", "(", "self", ",", "split", ":", "int", ",", "n_obs", ":", "int", "=", "None", ")", ":", "\n", "# If the requested number of observation is more than dataset", "\n", "# size we reset it to the maximum available.", "\n", "        ", "split", "=", "self", ".", "split_to_data_split", "[", "split", "]", "\n", "dataset", "=", "self", ".", "load_dataset", "(", "split", ")", "\n", "total_size", "=", "len", "(", "dataset", ")", "\n", "n_obs", "=", "self", ".", "check_n_obs", "(", "n_obs", ",", "total_size", ")", "\n", "if", "n_obs", "is", "not", "None", ":", "\n", "            ", "split", "=", "split", "+", "\"[:{}]\"", ".", "format", "(", "n_obs", ")", "\n", "", "return", "split", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.get_shuffled_sampled_split": [[65, 78], ["torch.Generator", "torch.Generator.manual_seed", "tasks.AbstractTaskDataset.load_dataset", "len", "torch.randperm().tolist", "tasks.AbstractTaskDataset.select_dataset_samples", "torch.randperm"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.select_dataset_samples"], ["", "def", "get_shuffled_sampled_split", "(", "self", ",", "split", ":", "int", ",", "n_obs", ":", "int", "=", "None", ")", ":", "\n", "# Defines the random generator.", "\n", "        ", "generator", "=", "torch", ".", "Generator", "(", ")", "\n", "generator", ".", "manual_seed", "(", "self", ".", "seed", ")", "\n", "# If the requested number of observation is more than dataset", "\n", "# size we reset it to the maximum available.", "\n", "mapped_split", "=", "self", ".", "split_to_data_split", "[", "split", "]", "\n", "dataset", "=", "self", ".", "load_dataset", "(", "mapped_split", ")", "\n", "# shuffle the dataset and get the random samples.", "\n", "train_size", "=", "len", "(", "dataset", ")", "\n", "indices", "=", "torch", ".", "randperm", "(", "train_size", ",", "generator", "=", "generator", ")", ".", "tolist", "(", ")", "\n", "dataset", "=", "self", ".", "select_dataset_samples", "(", "indices", ",", "dataset", ",", "n_obs", "=", "n_obs", ")", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.check_n_obs": [[79, 84], ["logger.warning"], "methods", ["None"], ["", "def", "check_n_obs", "(", "self", ",", "n_obs", ",", "total_size", ")", ":", "\n", "        ", "if", "n_obs", "is", "not", "None", "and", "n_obs", ">", "total_size", ":", "\n", "            ", "n_obs", "=", "total_size", "\n", "logger", ".", "warning", "(", "\"n_obs is set to %s\"", ",", "n_obs", ")", "\n", "", "return", "n_obs", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.select_dataset_samples": [[85, 96], ["tasks.AbstractTaskDataset.check_n_obs", "dataset.select", "len"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.check_n_obs"], ["", "def", "select_dataset_samples", "(", "self", ",", "indices", ",", "dataset", ",", "n_obs", ":", "int", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Given a dataset for the split, obtains the sample indices for this split\n        and returns the subsampled dataset.\n        :param indices: the selected indices.\n        :param dataset: dataset corresponding to this split.\n        :return: subsampled dataset.\n        \"\"\"", "\n", "n_obs", "=", "self", ".", "check_n_obs", "(", "n_obs", ",", "len", "(", "indices", ")", ")", "\n", "indices", "=", "indices", "[", ":", "n_obs", "]", "if", "n_obs", "is", "not", "None", "else", "indices", "\n", "return", "dataset", ".", "select", "(", "indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.load_dataset": [[97, 99], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["", "def", "load_dataset", "(", "self", ",", "split", ":", "int", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "self", ".", "name", ",", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.get_train_split_indices": [[100, 112], ["torch.Generator", "torch.Generator.manual_seed", "tasks.AbstractTaskDataset.load_dataset", "len", "torch.randperm().tolist", "torch.randperm"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["", "def", "get_train_split_indices", "(", "self", ",", "split", ")", ":", "\n", "        ", "generator", "=", "torch", ".", "Generator", "(", ")", "\n", "generator", ".", "manual_seed", "(", "self", ".", "seed", ")", "\n", "mapped_split", "=", "self", ".", "split_to_data_split", "[", "\"train\"", "]", "\n", "dataset", "=", "self", ".", "load_dataset", "(", "mapped_split", ")", "\n", "train_size", "=", "len", "(", "dataset", ")", "\n", "indices", "=", "torch", ".", "randperm", "(", "train_size", ",", "generator", "=", "generator", ")", ".", "tolist", "(", ")", "\n", "validation_size", "=", "1000", "\n", "if", "split", "==", "\"validation\"", ":", "\n", "            ", "return", "indices", "[", ":", "validation_size", "]", "\n", "", "else", ":", "\n", "            ", "return", "indices", "[", "validation_size", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.get_half_validation_indices": [[113, 124], ["torch.Generator", "torch.Generator.manual_seed", "tasks.AbstractTaskDataset.load_dataset", "len", "torch.randperm().tolist", "torch.randperm"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["", "", "def", "get_half_validation_indices", "(", "self", ",", "split", ")", ":", "\n", "        ", "generator", "=", "torch", ".", "Generator", "(", ")", "\n", "generator", ".", "manual_seed", "(", "self", ".", "seed", ")", "\n", "mapped_split", "=", "self", ".", "split_to_data_split", "[", "\"validation\"", "]", "\n", "dataset", "=", "self", ".", "load_dataset", "(", "mapped_split", ")", "\n", "validation_size", "=", "len", "(", "dataset", ")", "\n", "indices", "=", "torch", ".", "randperm", "(", "validation_size", ",", "generator", "=", "generator", ")", ".", "tolist", "(", ")", "\n", "if", "split", "==", "\"validation\"", ":", "\n", "            ", "return", "indices", "[", ":", "(", "validation_size", "//", "2", ")", "]", "\n", "", "else", ":", "\n", "            ", "return", "indices", "[", "validation_size", "//", "2", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.get_dataset": [[125, 152], ["tasks.AbstractTaskDataset.map", "tasks.AbstractTaskDataset.load_dataset", "tasks.AbstractTaskDataset.get_half_validation_indices", "tasks.AbstractTaskDataset.select_dataset_samples", "functools.partial", "tasks.AbstractTaskDataset.load_dataset", "tasks.AbstractTaskDataset.get_train_split_indices", "tasks.AbstractTaskDataset.select_dataset_samples", "tasks.AbstractTaskDataset.get_sampled_split", "tasks.AbstractTaskDataset.load_dataset", "tasks.AbstractTaskDataset.get_shuffled_sampled_split"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.get_half_validation_indices", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.select_dataset_samples", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.get_train_split_indices", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.select_dataset_samples", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.get_sampled_split", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.get_shuffled_sampled_split"], ["", "", "def", "get_dataset", "(", "self", ",", "split", ",", "n_obs", "=", "None", ",", "add_prefix", "=", "True", ",", "split_validation_test", "=", "False", ")", ":", "\n", "# For small datasets (n_samples < 10K) without test set, we divide validation set to", "\n", "# half, use one half as test set and one half as validation set.", "\n", "        ", "if", "split_validation_test", "and", "self", ".", "name", "in", "self", ".", "small_datasets_without_all_splits", "and", "split", "!=", "\"train\"", ":", "\n", "            ", "mapped_split", "=", "self", ".", "split_to_data_split", "[", "\"validation\"", "]", "\n", "dataset", "=", "self", ".", "load_dataset", "(", "split", "=", "mapped_split", ")", "\n", "indices", "=", "self", ".", "get_half_validation_indices", "(", "split", ")", "\n", "dataset", "=", "self", ".", "select_dataset_samples", "(", "indices", ",", "dataset", ",", "n_obs", ")", "\n", "# For larger datasets (n_samples > 10K), we divide training set into 1K as", "\n", "# validation and the rest as training set, keeping the original validation", "\n", "# set as the test set.", "\n", "", "elif", "split_validation_test", "and", "self", ".", "name", "in", "self", ".", "large_data_without_all_splits", "and", "split", "!=", "\"test\"", ":", "\n", "            ", "dataset", "=", "self", ".", "load_dataset", "(", "split", "=", "\"train\"", ")", "\n", "indices", "=", "self", ".", "get_train_split_indices", "(", "split", ")", "\n", "dataset", "=", "self", ".", "select_dataset_samples", "(", "indices", ",", "dataset", ",", "n_obs", ")", "\n", "", "else", ":", "\n", "# TODO: later we can join these as one.", "\n", "            ", "if", "n_obs", "==", "-", "1", ":", "\n", "                ", "split", "=", "self", ".", "get_sampled_split", "(", "split", ",", "n_obs", ")", "\n", "dataset", "=", "self", ".", "load_dataset", "(", "split", "=", "split", ")", "\n", "", "else", ":", "\n", "# shuffles the data and samples it.", "\n", "                ", "dataset", "=", "self", ".", "get_shuffled_sampled_split", "(", "split", ",", "n_obs", ")", "\n", "", "", "return", "dataset", ".", "map", "(", "functools", ".", "partial", "(", "self", ".", "preprocessor", ",", "add_prefix", "=", "add_prefix", ")", ",", "\n", "remove_columns", "=", "dataset", ".", "column_names", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format": [[153, 160], ["None"], "methods", ["None"], ["", "def", "seq2seq_format", "(", "self", ",", "src_strs", ":", "List", "[", "str", "]", ",", "tgt_strs", ":", "List", "[", "str", "]", ",", "\n", "add_prefix", ":", "bool", "=", "False", ",", "prefix", ":", "str", "=", "None", ")", ":", "\n", "        ", "src_prefix", "=", "self", ".", "name", "if", "prefix", "is", "None", "else", "prefix", "\n", "src_strs", "=", "[", "src_prefix", "]", "+", "src_strs", "if", "add_prefix", "else", "src_strs", "\n", "return", "{", "\"src_texts\"", ":", "' '", ".", "join", "(", "src_strs", ")", ",", "\n", "\"tgt_texts\"", ":", "' '", ".", "join", "(", "tgt_strs", ")", ",", "\n", "\"task\"", ":", "self", ".", "name", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.IMDBTaskDataset.preprocessor": [[171, 175], ["tasks.IMDBTaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "example", "[", "\"text\"", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "example", "[", "\"label\"", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.SickTaskDataset.load_dataset": [[187, 189], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ":", "int", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "\"csv\"", ",", "data_files", "=", "{", "split", ":", "f\"sick/{split}_clean.csv\"", "}", ")", "[", "split", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.SickTaskDataset.preprocessor": [[190, 194], ["tasks.SickTaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"premise:\"", ",", "example", "[", "\"premise\"", "]", ",", "\"hypothesis:\"", ",", "example", "[", "\"hypothesis\"", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "self", ".", "label_to_target", "[", "example", "[", "\"label\"", "]", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.PawsTaskDataset.load_dataset": [[205, 207], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ":", "int", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "self", ".", "name", ",", "'labeled_final'", ",", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.PawsTaskDataset.preprocessor": [[208, 212], ["tasks.PawsTaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"sentence1:\"", ",", "example", "[", "\"sentence1\"", "]", ",", "\"sentence2:\"", ",", "example", "[", "\"sentence2\"", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "example", "[", "\"label\"", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.SuperGLUEBoolQTaskDataset.load_dataset": [[223, 225], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "'super_glue'", ",", "'boolq'", ",", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.SuperGLUEBoolQTaskDataset.preprocessor": [[226, 230], ["tasks.SuperGLUEBoolQTaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"question:\"", ",", "example", "[", "\"question\"", "]", ",", "\"passage:\"", ",", "example", "[", "\"passage\"", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "example", "[", "\"label\"", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.SuperGLUERTETaskDataset.load_dataset": [[241, 243], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "'super_glue'", ",", "'rte'", ",", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.SuperGLUERTETaskDataset.preprocessor": [[244, 249], ["tasks.SuperGLUERTETaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"premise:\"", ",", "example", "[", "\"premise\"", "]", ",", "\n", "\"hypothesis:\"", ",", "example", "[", "\"hypothesis\"", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "example", "[", "\"label\"", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.SuperGLUECBTaskDataset.load_dataset": [[260, 262], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "'super_glue'", ",", "'cb'", ",", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.SuperGLUECBTaskDataset.preprocessor": [[263, 267], ["tasks.SuperGLUECBTaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"premise:\"", ",", "example", "[", "\"premise\"", "]", ",", "\"hypothesis:\"", ",", "example", "[", "\"hypothesis\"", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "example", "[", "\"label\"", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.SNLITaskDataset.preprocessor": [[275, 279], ["tasks.SNLITaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"premise:\"", ",", "example", "[", "\"premise\"", "]", ",", "\"hypothesis:\"", ",", "example", "[", "\"hypothesis\"", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "example", "[", "\"label\"", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.IWSLT2017RONL.load_dataset": [[287, 290], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "\"iwslt2017\"", ",", "'iwslt2017-ro-nl'", ",", "\n", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.IWSLT2017RONL.preprocessor": [[291, 296], ["tasks.IWSLT2017RONL.seq2seq_format"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "example", "[", "'translation'", "]", "[", "\"ro\"", "]", "]", "\n", "tgt_texts", "=", "[", "example", "[", "'translation'", "]", "[", "\"nl\"", "]", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ",", "\n", "prefix", "=", "\"Translate Romanian to Dutch\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.IWSLT2017ENNL.load_dataset": [[304, 307], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "\"iwslt2017\"", ",", "'iwslt2017-en-nl'", ",", "\n", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.IWSLT2017ENNL.preprocessor": [[308, 313], ["tasks.IWSLT2017ENNL.seq2seq_format"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "example", "[", "'translation'", "]", "[", "\"en\"", "]", "]", "\n", "tgt_texts", "=", "[", "example", "[", "'translation'", "]", "[", "\"nl\"", "]", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ",", "\n", "prefix", "=", "\"Translate English to Dutch\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WMT16ENROTaskDataset.load_dataset": [[321, 324], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "\"wmt16\"", ",", "self", ".", "pair", ",", "\n", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WMT16ENROTaskDataset.preprocessor": [[325, 330], ["tasks.WMT16ENROTaskDataset.seq2seq_format"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "example", "[", "'translation'", "]", "[", "\"en\"", "]", "]", "\n", "tgt_texts", "=", "[", "example", "[", "'translation'", "]", "[", "\"ro\"", "]", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ",", "\n", "prefix", "=", "\"Translate English to Romanian\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WMT16ROENTaskDataset.load_dataset": [[338, 341], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "\"wmt16\"", ",", "self", ".", "pair", ",", "\n", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WMT16ROENTaskDataset.preprocessor": [[342, 347], ["tasks.WMT16ROENTaskDataset.seq2seq_format"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "example", "[", "'translation'", "]", "[", "\"ro\"", "]", "]", "\n", "tgt_texts", "=", "[", "example", "[", "'translation'", "]", "[", "\"en\"", "]", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ",", "\n", "prefix", "=", "\"Translate Romanian to English\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WMT16ENCSTaskDataset.load_dataset": [[355, 358], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "\"wmt16\"", ",", "self", ".", "pair", ",", "\n", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WMT16ENCSTaskDataset.preprocessor": [[359, 364], ["tasks.WMT16ENCSTaskDataset.seq2seq_format"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "example", "[", "'translation'", "]", "[", "\"en\"", "]", "]", "\n", "tgt_texts", "=", "[", "example", "[", "'translation'", "]", "[", "\"cs\"", "]", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ",", "\n", "prefix", "=", "\"Translate English to Czech\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WMT16ENFITaskDataset.load_dataset": [[372, 375], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "\"wmt16\"", ",", "self", ".", "pair", ",", "\n", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WMT16ENFITaskDataset.preprocessor": [[376, 381], ["tasks.WMT16ENFITaskDataset.seq2seq_format"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "example", "[", "'translation'", "]", "[", "\"en\"", "]", "]", "\n", "tgt_texts", "=", "[", "example", "[", "'translation'", "]", "[", "\"fi\"", "]", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ",", "\n", "prefix", "=", "\"Translate English to Finnish\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WMT14HIENTaskDataset.load_dataset": [[389, 392], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "\"wmt14\"", ",", "self", ".", "pair", ",", "\n", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WMT14HIENTaskDataset.preprocessor": [[393, 398], ["tasks.WMT14HIENTaskDataset.seq2seq_format"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "example", "[", "'translation'", "]", "[", "\"en\"", "]", "]", "\n", "tgt_texts", "=", "[", "example", "[", "'translation'", "]", "[", "\"hi\"", "]", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ",", "\n", "prefix", "=", "\"Translate English to Hindi\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.TRECTaskDataset.load_dataset": [[409, 411], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "\"trec\"", ",", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.TRECTaskDataset.preprocessor": [[412, 416], ["tasks.TRECTaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"sentence:\"", ",", "example", "[", "'text'", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "example", "[", "'label-coarse'", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.YelpPolarityTaskDataset.load_dataset": [[427, 430], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "\"yelp_polarity\"", ",", "\n", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.YelpPolarityTaskDataset.preprocessor": [[431, 435], ["tasks.YelpPolarityTaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"sentence:\"", ",", "example", "[", "'text'", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "example", "[", "'label'", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.ScitailTaskDataset.map_label": [[444, 446], ["None"], "methods", ["None"], ["def", "map_label", "(", "self", ",", "label", ")", ":", "\n", "        ", "return", "self", ".", "label_map", "[", "label", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.ScitailTaskDataset.load_dataset": [[447, 450], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["", "def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "\"scitail\"", ",", "\"snli_format\"", ",", "\n", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.ScitailTaskDataset.preprocessor": [[451, 456], ["tasks.ScitailTaskDataset.seq2seq_format", "str", "tasks.ScitailTaskDataset.map_label"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.ScitailTaskDataset.map_label"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"sentence1:\"", ",", "example", "[", "'sentence1'", "]", ",", "\"sentence2:\"", ",", "example", "[", "\"sentence2\"", "]", "]", "\n", "# To increase the transfer performance, we modified the targets to be similar to other datasets.", "\n", "tgt_texts", "=", "[", "str", "(", "self", ".", "map_label", "(", "example", "[", "'gold_label'", "]", ")", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.MRPCTaskDataset.load_dataset": [[467, 470], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "'glue'", ",", "'mrpc'", ",", "\n", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.MRPCTaskDataset.preprocessor": [[471, 476], ["tasks.MRPCTaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"sentence1:\"", ",", "example", "[", "'sentence1'", "]", ",", "\n", "\"sentence2:\"", ",", "example", "[", "\"sentence2\"", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "example", "[", "'label'", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.COLATaskDataset.load_dataset": [[487, 490], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "'glue'", ",", "'cola'", ",", "\n", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.COLATaskDataset.preprocessor": [[491, 495], ["tasks.COLATaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"sentence:\"", ",", "example", "[", "'sentence'", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "example", "[", "'label'", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.SST2TaskDataset.load_dataset": [[506, 509], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "'glue'", ",", "'sst2'", ",", "\n", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.SST2TaskDataset.preprocessor": [[510, 514], ["tasks.SST2TaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"sentence:\"", ",", "example", "[", "'sentence'", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "example", "[", "'label'", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.STSBTaskDataset.load_dataset": [[525, 528], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "'glue'", ",", "'stsb'", ",", "\n", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.STSBTaskDataset.preprocessor": [[529, 534], ["tasks.STSBTaskDataset.seq2seq_format", "str", "utils.round_stsb_target"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format", "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.utils.round_stsb_target"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"sentence1:\"", ",", "example", "[", "'sentence1'", "]", ",", "\n", "\"sentence2:\"", ",", "example", "[", "\"sentence2\"", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "round_stsb_target", "(", "example", "[", "'label'", "]", ")", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.QQPTaskDataset.load_dataset": [[545, 548], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "'glue'", ",", "'qqp'", ",", "\n", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.QQPTaskDataset.preprocessor": [[549, 554], ["tasks.QQPTaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"question1:\"", ",", "example", "[", "'question1'", "]", ",", "\n", "\"question2:\"", ",", "example", "[", "\"question2\"", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "example", "[", "'label'", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.MNLITaskDataset.load_dataset": [[565, 567], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "'glue'", ",", "'mnli'", ",", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.MNLITaskDataset.preprocessor": [[568, 573], ["tasks.MNLITaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"premise:\"", ",", "example", "[", "'premise'", "]", ",", "\n", "\"hypothesis\"", ",", "example", "[", "\"hypothesis\"", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "example", "[", "'label'", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.QNLITaskDataset.load_dataset": [[584, 586], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "'glue'", ",", "'qnli'", ",", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.QNLITaskDataset.preprocessor": [[587, 592], ["tasks.QNLITaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"question:\"", ",", "example", "[", "'question'", "]", ",", "\n", "\"sentence:\"", ",", "example", "[", "\"sentence\"", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "example", "[", "'label'", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.RTETaskDataset.load_dataset": [[603, 606], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "'glue'", ",", "'rte'", ",", "\n", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.RTETaskDataset.preprocessor": [[607, 612], ["tasks.RTETaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"sentence1:\"", ",", "example", "[", "'sentence1'", "]", ",", "\n", "\"sentence2:\"", ",", "example", "[", "\"sentence2\"", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "example", "[", "'label'", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WNLITaskDataset.load_dataset": [[623, 625], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "'glue'", ",", "'wnli'", ",", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WNLITaskDataset.preprocessor": [[626, 631], ["tasks.WNLITaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"sentence1:\"", ",", "example", "[", "'sentence1'", "]", ",", "\n", "\"sentence2:\"", ",", "example", "[", "\"sentence2\"", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "example", "[", "'label'", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.SocialIQaTaskDataset.preprocessor": [[643, 651], ["tasks.SocialIQaTaskDataset.seq2seq_format", "example[].rstrip"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"question:\"", ",", "example", "[", "\"question\"", "]", ",", "\n", "\"context:\"", ",", "example", "[", "\"context\"", "]", ",", "\n", "\"answerA:\"", ",", "example", "[", "\"answerA\"", "]", ",", "\n", "\"answerB:\"", ",", "example", "[", "\"answerB\"", "]", ",", "\n", "\"answerC:\"", ",", "example", "[", "\"answerC\"", "]", "]", "\n", "tgt_texts", "=", "[", "self", ".", "label_map", "[", "example", "[", "'label'", "]", ".", "rstrip", "(", ")", "]", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.CosmosQaTaskDataset.preprocessor": [[662, 671], ["tasks.CosmosQaTaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"question:\"", ",", "example", "[", "\"question\"", "]", ",", "\n", "\"context:\"", ",", "example", "[", "\"context\"", "]", ",", "\n", "\"answer0:\"", ",", "example", "[", "\"answer0\"", "]", ",", "\n", "\"answer1:\"", ",", "example", "[", "\"answer1\"", "]", ",", "\n", "\"answer2:\"", ",", "example", "[", "\"answer2\"", "]", ",", "\n", "\"answer3:\"", ",", "example", "[", "\"answer3\"", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "example", "[", "'label'", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset": [[682, 685], ["datasets.load_dataset"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.load_dataset"], ["def", "load_dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "return", "datasets", ".", "load_dataset", "(", "'winogrande'", ",", "'winogrande_l'", ",", "\n", "split", "=", "split", ",", "script_version", "=", "\"master\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.WinograndeTaskDataset.preprocessor": [[686, 692], ["tasks.WinograndeTaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["", "def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"sentence:\"", ",", "example", "[", "\"sentence\"", "]", ",", "\n", "\"option1:\"", ",", "example", "[", "\"option1\"", "]", ",", "\n", "\"option2:\"", ",", "example", "[", "\"option2\"", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "example", "[", "'answer'", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.HellaSwagTaskDataset.preprocessor": [[703, 711], ["tasks.HellaSwagTaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"ctx:\"", ",", "example", "[", "\"ctx\"", "]", ",", "\n", "\"ending0:\"", ",", "example", "[", "\"endings\"", "]", "[", "0", "]", ",", "\n", "\"ending1:\"", ",", "example", "[", "\"endings\"", "]", "[", "1", "]", ",", "\n", "\"ending2:\"", ",", "example", "[", "\"endings\"", "]", "[", "2", "]", ",", "\n", "\"ending3:\"", ",", "example", "[", "\"endings\"", "]", "[", "3", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "example", "[", "'label'", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.CommonsenseQaTaskDataset.preprocessor": [[723, 732], ["tasks.CommonsenseQaTaskDataset.seq2seq_format", "str"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AbstractTaskDataset.seq2seq_format"], ["def", "preprocessor", "(", "self", ",", "example", ",", "add_prefix", "=", "True", ")", ":", "\n", "        ", "src_texts", "=", "[", "\"question:\"", ",", "example", "[", "\"question\"", "]", ",", "\n", "\"A:\"", ",", "example", "[", "\"choices\"", "]", "[", "\"text\"", "]", "[", "0", "]", ",", "\n", "\"B:\"", ",", "example", "[", "\"choices\"", "]", "[", "\"text\"", "]", "[", "1", "]", ",", "\n", "\"C:\"", ",", "example", "[", "\"choices\"", "]", "[", "\"text\"", "]", "[", "2", "]", ",", "\n", "\"D:\"", ",", "example", "[", "\"choices\"", "]", "[", "\"text\"", "]", "[", "3", "]", ",", "\n", "\"E:\"", ",", "example", "[", "\"choices\"", "]", "[", "\"text\"", "]", "[", "4", "]", "]", "\n", "tgt_texts", "=", "[", "str", "(", "self", ".", "label_map", "[", "example", "[", "'answerKey'", "]", "]", ")", "]", "\n", "return", "self", ".", "seq2seq_format", "(", "src_texts", ",", "tgt_texts", ",", "add_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.tasks.AutoTask.get": [[771, 779], ["ValueError", "TASK_MAPPING.keys"], "methods", ["None"], ["    ", "@", "classmethod", "\n", "def", "get", "(", "self", ",", "task_name", ",", "seed", "=", "42", ")", ":", "\n", "        ", "if", "task_name", "in", "TASK_MAPPING", ":", "\n", "            ", "return", "TASK_MAPPING", "[", "task_name", "]", "(", "seed", ")", "\n", "", "raise", "ValueError", "(", "\n", "\"Unrecognized task {} for AutoTask Model: {}.\\n\"", "\n", "\"Task name should be one of {}.\"", ".", "format", "(", "\n", "\", \"", ".", "join", "(", "c", "for", "c", "in", "TASK_MAPPING", ".", "keys", "(", ")", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__init__": [[17, 60], ["torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.get_world_size", "torch.get_world_size", "torch.get_rank", "torch.get_rank", "ValueError", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.is_available", "torch.is_available", "RuntimeError", "torch.is_available", "torch.is_available", "RuntimeError", "numpy.sum"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataset_sizes", ":", "List", "[", "int", "]", ",", "batch_size", ":", "int", ",", "temperature", ":", "float", ",", "\n", "num_replicas", ":", "Optional", "[", "int", "]", "=", "None", ",", "rank", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "seed", ":", "int", "=", "0", ",", "shuffle", ":", "bool", "=", "True", ")", "->", "None", ":", "\n", "        ", "\"\"\"Constructor for MultiTaskBatchSampler.\n        Args:\n            dataset_sizes: a list of integers, specifies the number of samples in\n                each dataset.\n            batch_size: integer, specifies the batch size.\n            temperature: float, temperature used for temperature sampling. The larger\n                the value, the datasets are sampled equally, and for value of 0, the datasets\n                will be sampled according to their number of samples.\n            num_replicas: integer, specifies the number of processes.\n            rank: integer, specifies the rank of the current process/\n            seed: integer, random seed.\n            shuffle: bool, if set to true, the datasets will be shuffled in each epoch.\n        \"\"\"", "\n", "if", "num_replicas", "is", "None", ":", "\n", "            ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Requires distributed package to be available\"", ")", "\n", "", "num_replicas", "=", "dist", ".", "get_world_size", "(", ")", "\n", "", "if", "rank", "is", "None", ":", "\n", "            ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Requires distributed package to be available\"", ")", "\n", "", "rank", "=", "dist", ".", "get_rank", "(", ")", "\n", "", "if", "rank", ">=", "num_replicas", "or", "rank", "<", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Invalid rank {}, rank should be in the interval\"", "\n", "\" [0, {}]\"", ".", "format", "(", "rank", ",", "num_replicas", "-", "1", ")", ")", "\n", "", "self", ".", "num_replicas", "=", "num_replicas", "\n", "self", ".", "rank", "=", "rank", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "dataset_sizes", "=", "dataset_sizes", "\n", "# By default we drop the last elements if dataset is not divisible by the number of ranks.", "\n", "self", ".", "rank_dataset_sizes", "=", "[", "dataset_size", "//", "self", ".", "num_replicas", "for", "dataset_size", "in", "self", ".", "dataset_sizes", "]", "\n", "self", ".", "dataset_offsets", "=", "torch", ".", "cumsum", "(", "torch", ".", "LongTensor", "(", "[", "0", "]", "+", "dataset_sizes", ")", ",", "0", ")", "\n", "self", ".", "total_sizes", "=", "[", "(", "dataset_size", "//", "self", ".", "num_replicas", ")", "*", "self", ".", "num_replicas", "for", "dataset_size", "in", "\n", "self", ".", "dataset_sizes", "]", "\n", "self", ".", "temperature", "=", "temperature", "\n", "self", ".", "seed", "=", "seed", "\n", "self", ".", "epoch", "=", "0", "\n", "self", ".", "num_batches_per_epoch", "=", "(", "np", ".", "sum", "(", "\n", "dataset_sizes", ")", "+", "self", ".", "batch_size", "-", "1", ")", "//", "self", ".", "batch_size", "//", "self", ".", "num_replicas", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.generate_tasks_distribution": [[61, 68], ["sum", "numpy.array", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "numpy.sum"], "methods", ["None"], ["", "def", "generate_tasks_distribution", "(", "self", ")", ":", "\n", "        ", "\"\"\"Given the dataset sizes computes the weights to sample each dataset\n        according to the temperature sampling.\"\"\"", "\n", "total_size", "=", "sum", "(", "self", ".", "dataset_sizes", ")", "\n", "weights", "=", "np", ".", "array", "(", "[", "(", "size", "/", "total_size", ")", "**", "(", "1.0", "/", "self", ".", "temperature", ")", "for", "size", "in", "self", ".", "dataset_sizes", "]", ")", "\n", "weights", "=", "weights", "/", "np", ".", "sum", "(", "weights", ")", "\n", "return", "torch", ".", "as_tensor", "(", "weights", ",", "dtype", "=", "torch", ".", "double", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__iter__": [[69, 108], ["torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator.manual_seed", "torch.Generator.manual_seed", "range", "multitask_sampler.MultiTaskBatchSampler.generate_tasks_distribution", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "len", "multitask_sampler.MultiTaskBatchSampler.rank_indices.append", "torch.randint().tolist", "torch.randint().tolist", "torch.randint().tolist", "torch.randint().tolist", "torch.randint().tolist.append", "torch.randint().tolist.append", "torch.randint().tolist.append", "torch.randint().tolist.append", "torch.randperm().tolist", "torch.randperm().tolist", "torch.randperm().tolist", "torch.randperm().tolist", "list", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "range", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.generate_tasks_distribution"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "# Defines torch generator, to make random choices consistent across cores in", "\n", "# different epochs, the seed needs to be set based on seed and epoch.", "\n", "        ", "generator", "=", "torch", ".", "Generator", "(", ")", "\n", "generator", ".", "manual_seed", "(", "self", ".", "seed", "+", "self", ".", "epoch", ")", "\n", "\n", "# Shuffles the datasets if shuffle is set to true.", "\n", "indices", "=", "[", "]", "\n", "for", "dataset_size", "in", "self", ".", "dataset_sizes", ":", "\n", "            ", "if", "self", ".", "shuffle", ":", "\n", "                ", "indices", ".", "append", "(", "torch", ".", "randperm", "(", "dataset_size", ",", "generator", "=", "generator", ")", ".", "tolist", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "indices", ".", "append", "(", "list", "(", "range", "(", "dataset_size", ")", ")", ")", "\n", "\n", "# Shards the datasets across the all processes.", "\n", "", "", "self", ".", "rank_indices", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "dataset_sizes", ")", ")", ":", "\n", "            ", "self", ".", "rank_indices", ".", "append", "(", "indices", "[", "i", "]", "[", "self", ".", "rank", ":", "self", ".", "total_sizes", "[", "i", "]", ":", "self", ".", "num_replicas", "]", ")", "\n", "\n", "# To make the model consistent across different processes, since the", "\n", "# model is based on tasks, we need to make sure the same task is selected", "\n", "# across different processes.", "\n", "", "tasks_distribution", ":", "torch", ".", "Tensor", "=", "self", ".", "generate_tasks_distribution", "(", ")", "\n", "\n", "# Chooses the tasks which will be used in each batch in one epoch.", "\n", "# With passing generator, we make sure this choice is consistent across", "\n", "# different processes.", "\n", "batch_task_assignments", "=", "torch", ".", "multinomial", "(", "tasks_distribution", ",", "\n", "self", ".", "num_batches_per_epoch", ",", "replacement", "=", "True", ",", "generator", "=", "generator", ")", "\n", "\n", "for", "batch_task", "in", "batch_task_assignments", ":", "\n", "# Gets the number of samples of the selected datasets available for the", "\n", "# current rank.", "\n", "            ", "num_task_samples", "=", "self", ".", "rank_dataset_sizes", "[", "batch_task", "]", "\n", "# Computes the random samples from the chosen dataset.", "\n", "indices", "=", "torch", ".", "randint", "(", "low", "=", "0", ",", "high", "=", "num_task_samples", ",", "size", "=", "(", "self", ".", "batch_size", ",", ")", ",", "generator", "=", "generator", ")", ".", "tolist", "(", ")", "\n", "# Converts the selected indices to the global indices on the given dataset.", "\n", "results", "=", "(", "self", ".", "dataset_offsets", "[", "batch_task", "]", "+", "torch", ".", "tensor", "(", "self", ".", "rank_indices", "[", "batch_task", "]", ")", "[", "indices", "]", ")", ".", "tolist", "(", ")", "\n", "yield", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.__len__": [[109, 111], ["None"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_batches_per_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.multitask_sampler.MultiTaskBatchSampler.set_epoch": [[112, 114], ["None"], "methods", ["None"], ["", "def", "set_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "self", ".", "epoch", "=", "epoch", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.utils.round_stsb_target": [[7, 21], ["numpy.round"], "function", ["None"], ["freeze_embeds", ",", "\n", "freeze_params", ",", "\n", "save_json", ")", "\n", "from", "transformers", ".", "modeling_t5", "import", "T5LayerNorm", "\n", "\n", "from", "hyperformer", ".", "adapters", "import", "(", "AdapterController", ",", "MetaAdapterController", ",", "\n", "AdapterLayersHyperNetController", ",", "AdapterLayersOneHyperNetController", ")", "\n", "from", "hyperformer", ".", "data", "import", "TASK_MAPPING", "\n", "\n", "logger", "=", "getLogger", "(", "__name__", ")", "\n", "\n", "\n", "def", "create_dir", "(", "output_dir", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.rabeehk_hyperformer.data.utils.compute_task_max_decoding_length": [[23, 36], ["transformers.T5Tokenizer.from_pretrained", "T5Tokenizer.from_pretrained.encode", "max", "len"], "function", ["None"], ["\n", "if", "not", "os", ".", "path", ".", "exists", "(", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "output_dir", ")", "\n", "\n", "\n", "", "", "def", "handle_metrics", "(", "split", ",", "metrics", ",", "output_dir", ")", ":", "#, gcs_bucket=None):", "\n", "    "]]}