{"home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.BertConfig.__init__": [[19, 66], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "16", ",", "\n", "initializer_range", "=", "0.02", ")", ":", "\n", "    ", "\"\"\"Constructs BertConfig.\n\n    Args:\n      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n      hidden_size: Size of the encoder layers and the pooler layer.\n      num_hidden_layers: Number of hidden layers in the Transformer encoder.\n      num_attention_heads: Number of attention heads for each attention layer in\n        the Transformer encoder.\n      intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n        layer in the Transformer encoder.\n      hidden_act: The non-linear activation function (function or string) in the\n        encoder and pooler.\n      hidden_dropout_prob: The dropout probability for all fully connected\n        layers in the embeddings, encoder, and pooler.\n      attention_probs_dropout_prob: The dropout ratio for the attention\n        probabilities.\n      max_position_embeddings: The maximum sequence length that this model might\n        ever be used with. Typically set this to something large just in case\n        (e.g., 512 or 1024 or 2048).\n      type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n        `BertModel`.\n      initializer_range: The stdev of the truncated_normal_initializer for\n        initializing all weight matrices.\n    \"\"\"", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.BertConfig.from_dict": [[67, 74], ["modeling.BertConfig", "six.iteritems"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "    ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size", "=", "None", ")", "\n", "for", "(", "key", ",", "value", ")", "in", "six", ".", "iteritems", "(", "json_object", ")", ":", "\n", "      ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.BertConfig.from_json_file": [[75, 81], ["cls.from_dict", "tensorflow.gfile.GFile", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.BertConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "    ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "json_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "      ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.BertConfig.to_dict": [[82, 86], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "    ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.BertConfig.to_json_string": [[87, 90], ["json.dumps", "modeling.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.BertConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "    ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.BertModel.__init__": [[116, 219], ["copy.deepcopy", "modeling.get_shape_list", "tensorflow.ones", "tensorflow.zeros", "tensorflow.variable_scope", "tensorflow.variable_scope", "modeling.embedding_lookup", "modeling.embedding_postprocessor", "tensorflow.variable_scope", "modeling.create_attention_mask_from_input_mask", "modeling.transformer_model", "tensorflow.variable_scope", "tensorflow.squeeze", "tensorflow.layers.dense", "modeling.get_activation", "modeling.create_initializer"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_shape_list", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.embedding_lookup", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.embedding_postprocessor", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.create_attention_mask_from_input_mask", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.transformer_model", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_activation", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.create_initializer"], ["def", "__init__", "(", "self", ",", "\n", "config", ",", "\n", "is_training", ",", "\n", "input_ids", ",", "\n", "input_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "use_one_hot_embeddings", "=", "False", ",", "\n", "scope", "=", "None", ",", "\n", "reuse", "=", "False", ")", ":", "\n", "    ", "\"\"\"Constructor for BertModel.\n\n    Args:\n      config: `BertConfig` instance.\n      is_training: bool. true for training model, false for eval model. Controls\n        whether dropout will be applied.\n      input_ids: int32 Tensor of shape [batch_size, seq_length].\n      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n        embeddings or tf.embedding_lookup() for the word embeddings.\n      scope: (optional) variable scope. Defaults to \"bert\".\n\n    Raises:\n      ValueError: The config is invalid or one of the input tensor shapes\n        is invalid.\n    \"\"\"", "\n", "config", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "if", "not", "is_training", ":", "\n", "      ", "config", ".", "hidden_dropout_prob", "=", "0.0", "\n", "config", ".", "attention_probs_dropout_prob", "=", "0.0", "\n", "\n", "", "input_shape", "=", "get_shape_list", "(", "input_ids", ",", "expected_rank", "=", "2", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "\n", "if", "input_mask", "is", "None", ":", "\n", "      ", "input_mask", "=", "tf", ".", "ones", "(", "shape", "=", "[", "batch_size", ",", "seq_length", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "      ", "token_type_ids", "=", "tf", ".", "zeros", "(", "shape", "=", "[", "batch_size", ",", "seq_length", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"bert\"", ",", "reuse", "=", "reuse", ")", ":", "\n", "      ", "with", "tf", ".", "variable_scope", "(", "\"embeddings\"", ")", ":", "\n", "# Perform embedding lookup on the word ids.", "\n", "        ", "(", "self", ".", "embedding_output", ",", "self", ".", "embedding_table", ")", "=", "embedding_lookup", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "vocab_size", "=", "config", ".", "vocab_size", ",", "\n", "embedding_size", "=", "config", ".", "hidden_size", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "word_embedding_name", "=", "\"word_embeddings\"", ",", "\n", "use_one_hot_embeddings", "=", "use_one_hot_embeddings", ")", "\n", "\n", "# Add positional embeddings and token type embeddings, then layer", "\n", "# normalize and perform dropout.", "\n", "self", ".", "embedding_output", "=", "embedding_postprocessor", "(", "\n", "input_tensor", "=", "self", ".", "embedding_output", ",", "\n", "use_token_type", "=", "True", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "token_type_vocab_size", "=", "config", ".", "type_vocab_size", ",", "\n", "token_type_embedding_name", "=", "\"token_type_embeddings\"", ",", "\n", "use_position_embeddings", "=", "True", ",", "\n", "position_embedding_name", "=", "\"position_embeddings\"", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "max_position_embeddings", "=", "config", ".", "max_position_embeddings", ",", "\n", "dropout_prob", "=", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"encoder\"", ")", ":", "\n", "# This converts a 2D mask of shape [batch_size, seq_length] to a 3D", "\n", "# mask of shape [batch_size, seq_length, seq_length] which is used", "\n", "# for the attention scores.", "\n", "        ", "attention_mask", "=", "create_attention_mask_from_input_mask", "(", "\n", "input_ids", ",", "input_mask", ")", "\n", "\n", "# Run the stacked transformer.", "\n", "# `sequence_output` shape = [batch_size, seq_length, hidden_size].", "\n", "self", ".", "all_encoder_layers", "=", "transformer_model", "(", "\n", "input_tensor", "=", "self", ".", "embedding_output", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "hidden_size", "=", "config", ".", "hidden_size", ",", "\n", "num_hidden_layers", "=", "config", ".", "num_hidden_layers", ",", "\n", "num_attention_heads", "=", "config", ".", "num_attention_heads", ",", "\n", "intermediate_size", "=", "config", ".", "intermediate_size", ",", "\n", "intermediate_act_fn", "=", "get_activation", "(", "config", ".", "hidden_act", ")", ",", "\n", "hidden_dropout_prob", "=", "config", ".", "hidden_dropout_prob", ",", "\n", "attention_probs_dropout_prob", "=", "config", ".", "attention_probs_dropout_prob", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "do_return_all_layers", "=", "True", ")", "\n", "\n", "", "self", ".", "sequence_output", "=", "self", ".", "all_encoder_layers", "[", "-", "1", "]", "\n", "# The \"pooler\" converts the encoded sequence tensor of shape", "\n", "# [batch_size, seq_length, hidden_size] to a tensor of shape", "\n", "# [batch_size, hidden_size]. This is necessary for segment-level", "\n", "# (or segment-pair-level) classification tasks where we need a fixed", "\n", "# dimensional representation of the segment.", "\n", "with", "tf", ".", "variable_scope", "(", "\"pooler\"", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token. We assume that this has been pre-trained", "\n", "        ", "first_token_tensor", "=", "tf", ".", "squeeze", "(", "self", ".", "sequence_output", "[", ":", ",", "0", ":", "1", ",", ":", "]", ",", "axis", "=", "1", ")", "\n", "self", ".", "pooled_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "first_token_tensor", ",", "\n", "config", ".", "hidden_size", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "config", ".", "initializer_range", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.BertModel.get_pooled_output": [[220, 222], ["None"], "methods", ["None"], ["", "", "", "def", "get_pooled_output", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.BertModel.get_sequence_output": [[223, 231], ["None"], "methods", ["None"], ["", "def", "get_sequence_output", "(", "self", ")", ":", "\n", "    ", "\"\"\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \"\"\"", "\n", "return", "self", ".", "sequence_output", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.BertModel.get_all_encoder_layers": [[232, 234], ["None"], "methods", ["None"], ["", "def", "get_all_encoder_layers", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "all_encoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.BertModel.get_embedding_output": [[235, 245], ["None"], "methods", ["None"], ["", "def", "get_embedding_output", "(", "self", ")", ":", "\n", "    ", "\"\"\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \"\"\"", "\n", "return", "self", ".", "embedding_output", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.BertModel.get_embedding_table": [[246, 248], ["None"], "methods", ["None"], ["", "def", "get_embedding_table", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "embedding_table", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.gelu": [[250, 264], ["tensorflow.erf", "tensorflow.sqrt"], "function", ["None"], ["", "", "def", "gelu", "(", "input_tensor", ")", ":", "\n", "  ", "\"\"\"Gaussian Error Linear Unit.\n\n  This is a smoother version of the RELU.\n  Original paper: https://arxiv.org/abs/1606.08415\n\n  Args:\n    input_tensor: float Tensor to perform activation.\n\n  Returns:\n    `input_tensor` with the GELU activation applied.\n  \"\"\"", "\n", "cdf", "=", "0.5", "*", "(", "1.0", "+", "tf", ".", "erf", "(", "input_tensor", "/", "tf", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "return", "input_tensor", "*", "cdf", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_activation": [[266, 301], ["activation_string.lower", "isinstance", "ValueError"], "function", ["None"], ["", "def", "get_activation", "(", "activation_string", ")", ":", "\n", "  ", "\"\"\"Maps a string to a Python function, e.g., \"relu\" => `tf.nn.relu`.\n\n  Args:\n    activation_string: String name of the activation function.\n\n  Returns:\n    A Python function corresponding to the activation function. If\n    `activation_string` is None, empty, or \"linear\", this will return None.\n    If `activation_string` is not a string, it will return `activation_string`.\n\n  Raises:\n    ValueError: The `activation_string` does not correspond to a known\n      activation.\n  \"\"\"", "\n", "\n", "# We assume that anything that\"s not a string is already an activation", "\n", "# function, so we just return it.", "\n", "if", "not", "isinstance", "(", "activation_string", ",", "six", ".", "string_types", ")", ":", "\n", "    ", "return", "activation_string", "\n", "\n", "", "if", "not", "activation_string", ":", "\n", "    ", "return", "None", "\n", "\n", "", "act", "=", "activation_string", ".", "lower", "(", ")", "\n", "if", "act", "==", "\"linear\"", ":", "\n", "    ", "return", "None", "\n", "", "elif", "act", "==", "\"relu\"", ":", "\n", "    ", "return", "tf", ".", "nn", ".", "relu", "\n", "", "elif", "act", "==", "\"gelu\"", ":", "\n", "    ", "return", "gelu", "\n", "", "elif", "act", "==", "\"tanh\"", ":", "\n", "    ", "return", "tf", ".", "tanh", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Unsupported activation: %s\"", "%", "act", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_assignment_map_from_checkpoint": [[303, 334], ["collections.OrderedDict", "tensorflow.train.list_variables", "collections.OrderedDict", "re.match", "re.match.group", "tensorflow.logging.info"], "function", ["None"], ["", "", "def", "get_assignment_map_from_checkpoint", "(", "tvars", ",", "init_checkpoint", ",", "scope", "=", "None", ")", ":", "\n", "  ", "\"\"\"Compute the union of the current variables and checkpoint variables.\"\"\"", "\n", "assignment_map", "=", "{", "}", "\n", "initialized_variable_names", "=", "{", "}", "\n", "\n", "name_to_variable", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "var", "in", "tvars", ":", "\n", "    ", "name", "=", "var", ".", "name", "\n", "m", "=", "re", ".", "match", "(", "\"^(.*):\\\\d+$\"", ",", "name", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "      ", "name", "=", "m", ".", "group", "(", "1", ")", "\n", "", "name_to_variable", "[", "name", "]", "=", "var", "\n", "\n", "", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "init_checkpoint", ")", "\n", "\n", "assignment_map", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "x", "in", "init_vars", ":", "\n", "    ", "(", "original_name", ",", "var", ")", "=", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", "\n", "name", "=", "original_name", "\n", "if", "scope", ":", "\n", "      ", "name", "=", "scope", "+", "name", "\n", "", "if", "name", "not", "in", "name_to_variable", ":", "\n", "      ", "if", "'adam'", "not", "in", "name", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"var %s not found\"", ",", "name", ")", "\n", "", "continue", "\n", "# 'checkpoint_scope_name/': 'scope_name/' - will load all variables in current scope_name from checkpoint_scope_name with matching tensor names.", "\n", "", "assignment_map", "[", "original_name", "]", "=", "name", "\n", "initialized_variable_names", "[", "name", "]", "=", "1", "\n", "initialized_variable_names", "[", "name", "+", "\":0\"", "]", "=", "1", "\n", "\n", "", "return", "(", "assignment_map", ",", "initialized_variable_names", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.dropout": [[336, 352], ["tensorflow.nn.dropout"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.dropout"], ["", "def", "dropout", "(", "input_tensor", ",", "dropout_prob", ")", ":", "\n", "  ", "\"\"\"Perform dropout.\n\n  Args:\n    input_tensor: float Tensor.\n    dropout_prob: Python float. The probability of dropping out a value (NOT of\n      *keeping* a dimension as in `tf.nn.dropout`).\n\n  Returns:\n    A version of `input_tensor` with dropout applied.\n  \"\"\"", "\n", "if", "dropout_prob", "is", "None", "or", "dropout_prob", "==", "0.0", ":", "\n", "    ", "return", "input_tensor", "\n", "\n", "", "output", "=", "tf", ".", "nn", ".", "dropout", "(", "input_tensor", ",", "1.0", "-", "dropout_prob", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.layer_norm": [[354, 358], ["tensorflow.contrib.layers.layer_norm"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.layer_norm"], ["", "def", "layer_norm", "(", "input_tensor", ",", "name", "=", "None", ")", ":", "\n", "  ", "\"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"", "\n", "return", "tf", ".", "contrib", ".", "layers", ".", "layer_norm", "(", "\n", "inputs", "=", "input_tensor", ",", "begin_norm_axis", "=", "-", "1", ",", "begin_params_axis", "=", "-", "1", ",", "scope", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.layer_norm_and_dropout": [[360, 365], ["modeling.layer_norm", "modeling.dropout"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.layer_norm", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.dropout"], ["", "def", "layer_norm_and_dropout", "(", "input_tensor", ",", "dropout_prob", ",", "name", "=", "None", ")", ":", "\n", "  ", "\"\"\"Runs layer normalization followed by dropout.\"\"\"", "\n", "output_tensor", "=", "layer_norm", "(", "input_tensor", ",", "name", ")", "\n", "output_tensor", "=", "dropout", "(", "output_tensor", ",", "dropout_prob", ")", "\n", "return", "output_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.create_initializer": [[367, 370], ["tensorflow.truncated_normal_initializer"], "function", ["None"], ["", "def", "create_initializer", "(", "initializer_range", "=", "0.02", ")", ":", "\n", "  ", "\"\"\"Creates a `truncated_normal_initializer` with the given range.\"\"\"", "\n", "return", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "initializer_range", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.embedding_lookup": [[372, 418], ["tensorflow.get_variable", "modeling.get_shape_list", "tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.reshape", "tensorflow.one_hot", "tensorflow.matmul", "tensorflow.nn.embedding_lookup", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_shape_list", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.embedding_lookup", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.create_initializer"], ["", "def", "embedding_lookup", "(", "input_ids", ",", "\n", "vocab_size", ",", "\n", "embedding_size", "=", "128", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "word_embedding_name", "=", "\"word_embeddings\"", ",", "\n", "use_one_hot_embeddings", "=", "False", ")", ":", "\n", "  ", "\"\"\"Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.nn.embedding_lookup()`.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  \"\"\"", "\n", "# This function assumes that the input is of shape [batch_size, seq_length,", "\n", "# num_inputs].", "\n", "#", "\n", "# If the input is a 2D tensor of shape [batch_size, seq_length], we", "\n", "# reshape to [batch_size, seq_length, 1].", "\n", "if", "input_ids", ".", "shape", ".", "ndims", "==", "2", ":", "\n", "    ", "input_ids", "=", "tf", ".", "expand_dims", "(", "input_ids", ",", "axis", "=", "[", "-", "1", "]", ")", "\n", "\n", "", "embedding_table", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "word_embedding_name", ",", "\n", "shape", "=", "[", "vocab_size", ",", "embedding_size", "]", ",", "\n", "initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "if", "use_one_hot_embeddings", ":", "\n", "    ", "flat_input_ids", "=", "tf", ".", "reshape", "(", "input_ids", ",", "[", "-", "1", "]", ")", "\n", "one_hot_input_ids", "=", "tf", ".", "one_hot", "(", "flat_input_ids", ",", "depth", "=", "vocab_size", ")", "\n", "output", "=", "tf", ".", "matmul", "(", "one_hot_input_ids", ",", "embedding_table", ")", "\n", "", "else", ":", "\n", "    ", "output", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embedding_table", ",", "input_ids", ")", "\n", "\n", "", "input_shape", "=", "get_shape_list", "(", "input_ids", ")", "\n", "\n", "output", "=", "tf", ".", "reshape", "(", "output", ",", "\n", "input_shape", "[", "0", ":", "-", "1", "]", "+", "[", "input_shape", "[", "-", "1", "]", "*", "embedding_size", "]", ")", "\n", "return", "(", "output", ",", "embedding_table", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.embedding_postprocessor": [[420, 514], ["modeling.get_shape_list", "modeling.layer_norm_and_dropout", "tensorflow.get_variable", "tensorflow.reshape", "tensorflow.one_hot", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.assert_less_equal", "ValueError", "tensorflow.control_dependencies", "tensorflow.get_variable", "tensorflow.slice", "len", "range", "position_broadcast_shape.extend", "tensorflow.reshape", "modeling.create_initializer", "layer_norm_and_dropout.shape.as_list", "position_broadcast_shape.append", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_shape_list", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.layer_norm_and_dropout", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.create_initializer", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.create_initializer"], ["", "def", "embedding_postprocessor", "(", "input_tensor", ",", "\n", "use_token_type", "=", "False", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "token_type_vocab_size", "=", "16", ",", "\n", "token_type_embedding_name", "=", "\"token_type_embeddings\"", ",", "\n", "use_position_embeddings", "=", "True", ",", "\n", "position_embedding_name", "=", "\"position_embeddings\"", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "dropout_prob", "=", "0.1", ")", ":", "\n", "  ", "\"\"\"Performs various post-processing on a word embedding tensor.\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length,\n      embedding_size].\n    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      Must be specified if `use_token_type` is True.\n    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n    token_type_embedding_name: string. The name of the embedding table variable\n      for token type ids.\n    use_position_embeddings: bool. Whether to add position embeddings for the\n      position of each token in the sequence.\n    position_embedding_name: string. The name of the embedding table variable\n      for positional embeddings.\n    initializer_range: float. Range of the weight initialization.\n    max_position_embeddings: int. Maximum sequence length that might ever be\n      used with this model. This can be longer than the sequence length of\n      input_tensor, but cannot be shorter.\n    dropout_prob: float. Dropout probability applied to the final output tensor.\n\n  Returns:\n    float tensor with same shape as `input_tensor`.\n\n  Raises:\n    ValueError: One of the tensor shapes or input values is invalid.\n  \"\"\"", "\n", "input_shape", "=", "get_shape_list", "(", "input_tensor", ",", "expected_rank", "=", "3", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "width", "=", "input_shape", "[", "2", "]", "\n", "\n", "output", "=", "input_tensor", "\n", "\n", "if", "use_token_type", ":", "\n", "    ", "if", "token_type_ids", "is", "None", ":", "\n", "      ", "raise", "ValueError", "(", "\"`token_type_ids` must be specified if\"", "\n", "\"`use_token_type` is True.\"", ")", "\n", "", "token_type_table", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "token_type_embedding_name", ",", "\n", "shape", "=", "[", "token_type_vocab_size", ",", "width", "]", ",", "\n", "initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "# This vocab will be small so we always do one-hot here, since it is always", "\n", "# faster for a small vocabulary.", "\n", "flat_token_type_ids", "=", "tf", ".", "reshape", "(", "token_type_ids", ",", "[", "-", "1", "]", ")", "\n", "one_hot_ids", "=", "tf", ".", "one_hot", "(", "flat_token_type_ids", ",", "depth", "=", "token_type_vocab_size", ")", "\n", "token_type_embeddings", "=", "tf", ".", "matmul", "(", "one_hot_ids", ",", "token_type_table", ")", "\n", "token_type_embeddings", "=", "tf", ".", "reshape", "(", "token_type_embeddings", ",", "\n", "[", "batch_size", ",", "seq_length", ",", "width", "]", ")", "\n", "output", "+=", "token_type_embeddings", "\n", "\n", "", "if", "use_position_embeddings", ":", "\n", "    ", "assert_op", "=", "tf", ".", "assert_less_equal", "(", "seq_length", ",", "max_position_embeddings", ")", "\n", "with", "tf", ".", "control_dependencies", "(", "[", "assert_op", "]", ")", ":", "\n", "      ", "full_position_embeddings", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "position_embedding_name", ",", "\n", "shape", "=", "[", "max_position_embeddings", ",", "width", "]", ",", "\n", "initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "# Since the position embedding table is a learned variable, we create it", "\n", "# using a (long) sequence length `max_position_embeddings`. The actual", "\n", "# sequence length might be shorter than this, for faster training of", "\n", "# tasks that do not have long sequences.", "\n", "#", "\n", "# So `full_position_embeddings` is effectively an embedding table", "\n", "# for position [0, 1, 2, ..., max_position_embeddings-1], and the current", "\n", "# sequence has positions [0, 1, 2, ... seq_length-1], so we can just", "\n", "# perform a slice.", "\n", "position_embeddings", "=", "tf", ".", "slice", "(", "full_position_embeddings", ",", "[", "0", ",", "0", "]", ",", "\n", "[", "seq_length", ",", "-", "1", "]", ")", "\n", "num_dims", "=", "len", "(", "output", ".", "shape", ".", "as_list", "(", ")", ")", "\n", "\n", "# Only the last two dimensions are relevant (`seq_length` and `width`), so", "\n", "# we broadcast among the first dimensions, which is typically just", "\n", "# the batch size.", "\n", "position_broadcast_shape", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "num_dims", "-", "2", ")", ":", "\n", "        ", "position_broadcast_shape", ".", "append", "(", "1", ")", "\n", "", "position_broadcast_shape", ".", "extend", "(", "[", "seq_length", ",", "width", "]", ")", "\n", "position_embeddings", "=", "tf", ".", "reshape", "(", "position_embeddings", ",", "\n", "position_broadcast_shape", ")", "\n", "output", "+=", "position_embeddings", "\n", "\n", "", "", "output", "=", "layer_norm_and_dropout", "(", "output", ",", "dropout_prob", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.create_attention_mask_from_input_mask": [[516, 548], ["modeling.get_shape_list", "modeling.get_shape_list", "tensorflow.cast", "tensorflow.ones", "tensorflow.reshape"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_shape_list", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_shape_list"], ["", "def", "create_attention_mask_from_input_mask", "(", "from_tensor", ",", "to_mask", ")", ":", "\n", "  ", "\"\"\"Create 3D attention mask from a 2D tensor mask.\n\n  Args:\n    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n  \"\"\"", "\n", "from_shape", "=", "get_shape_list", "(", "from_tensor", ",", "expected_rank", "=", "[", "2", ",", "3", "]", ")", "\n", "batch_size", "=", "from_shape", "[", "0", "]", "\n", "from_seq_length", "=", "from_shape", "[", "1", "]", "\n", "\n", "to_shape", "=", "get_shape_list", "(", "to_mask", ",", "expected_rank", "=", "2", ")", "\n", "to_seq_length", "=", "to_shape", "[", "1", "]", "\n", "\n", "to_mask", "=", "tf", ".", "cast", "(", "\n", "tf", ".", "reshape", "(", "to_mask", ",", "[", "batch_size", ",", "1", ",", "to_seq_length", "]", ")", ",", "tf", ".", "float32", ")", "\n", "\n", "# We don't assume that `from_tensor` is a mask (although it could be). We", "\n", "# don't actually care if we attend *from* padding tokens (only *to* padding)", "\n", "# tokens so we create a tensor of all ones.", "\n", "#", "\n", "# `broadcast_ones` = [batch_size, from_seq_length, 1]", "\n", "broadcast_ones", "=", "tf", ".", "ones", "(", "\n", "shape", "=", "[", "batch_size", ",", "from_seq_length", ",", "1", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "# Here we broadcast along two dimensions to create the mask.", "\n", "mask", "=", "broadcast_ones", "*", "to_mask", "\n", "\n", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.attention_layer": [[550, 744], ["modeling.get_shape_list", "modeling.get_shape_list", "modeling.reshape_to_matrix", "modeling.reshape_to_matrix", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.layers.dense", "modeling.attention_layer.transpose_for_scores"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_shape_list", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_shape_list", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.reshape_to_matrix", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.reshape_to_matrix"], ["", "def", "attention_layer", "(", "from_tensor", ",", "\n", "to_tensor", ",", "\n", "attention_mask", "=", "None", ",", "\n", "num_attention_heads", "=", "1", ",", "\n", "size_per_head", "=", "512", ",", "\n", "query_act", "=", "None", ",", "\n", "key_act", "=", "None", ",", "\n", "value_act", "=", "None", ",", "\n", "attention_probs_dropout_prob", "=", "0.0", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "do_return_2d_tensor", "=", "False", ",", "\n", "batch_size", "=", "None", ",", "\n", "from_seq_length", "=", "None", ",", "\n", "to_seq_length", "=", "None", ")", ":", "\n", "  ", "\"\"\"Performs multi-headed attention from `from_tensor` to `to_tensor`.\n\n  This is an implementation of multi-headed attention based on \"Attention\n  is all you Need\". If `from_tensor` and `to_tensor` are the same, then\n  this is self-attention. Each timestep in `from_tensor` attends to the\n  corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n\n  This function first projects `from_tensor` into a \"query\" tensor and\n  `to_tensor` into \"key\" and \"value\" tensors. These are (effectively) a list\n  of tensors of length `num_attention_heads`, where each tensor is of shape\n  [batch_size, seq_length, size_per_head].\n\n  Then, the query and key tensors are dot-producted and scaled. These are\n  softmaxed to obtain attention probabilities. The value tensors are then\n  interpolated by these probabilities, then concatenated back to a single\n  tensor and returned.\n\n  In practice, the multi-headed attention are done with transposes and\n  reshapes rather than actual separate tensors.\n\n  Args:\n    from_tensor: float Tensor of shape [batch_size, from_seq_length,\n      from_width].\n    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n    attention_mask: (optional) int32 Tensor of shape [batch_size,\n      from_seq_length, to_seq_length]. The values should be 1 or 0. The\n      attention scores will effectively be set to -infinity for any positions in\n      the mask that are 0, and will be unchanged for positions that are 1.\n    num_attention_heads: int. Number of attention heads.\n    size_per_head: int. Size of each attention head.\n    query_act: (optional) Activation function for the query transform.\n    key_act: (optional) Activation function for the key transform.\n    value_act: (optional) Activation function for the value transform.\n    attention_probs_dropout_prob: (optional) float. Dropout probability of the\n      attention probabilities.\n    initializer_range: float. Range of the weight initializer.\n    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n      * from_seq_length, num_attention_heads * size_per_head]. If False, the\n      output will be of shape [batch_size, from_seq_length, num_attention_heads\n      * size_per_head].\n    batch_size: (Optional) int. If the input is 2D, this might be the batch size\n      of the 3D version of the `from_tensor` and `to_tensor`.\n    from_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `from_tensor`.\n    to_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `to_tensor`.\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length,\n      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n      true, this will be of shape [batch_size * from_seq_length,\n      num_attention_heads * size_per_head]).\n\n  Raises:\n    ValueError: Any of the arguments or tensor shapes are invalid.\n  \"\"\"", "\n", "\n", "def", "transpose_for_scores", "(", "input_tensor", ",", "batch_size", ",", "num_attention_heads", ",", "\n", "seq_length", ",", "width", ")", ":", "\n", "    ", "output_tensor", "=", "tf", ".", "reshape", "(", "\n", "input_tensor", ",", "[", "batch_size", ",", "seq_length", ",", "num_attention_heads", ",", "width", "]", ")", "\n", "\n", "output_tensor", "=", "tf", ".", "transpose", "(", "output_tensor", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "return", "output_tensor", "\n", "\n", "", "from_shape", "=", "get_shape_list", "(", "from_tensor", ",", "expected_rank", "=", "[", "2", ",", "3", "]", ")", "\n", "to_shape", "=", "get_shape_list", "(", "to_tensor", ",", "expected_rank", "=", "[", "2", ",", "3", "]", ")", "\n", "\n", "if", "len", "(", "from_shape", ")", "!=", "len", "(", "to_shape", ")", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"The rank of `from_tensor` must match the rank of `to_tensor`.\"", ")", "\n", "\n", "", "if", "len", "(", "from_shape", ")", "==", "3", ":", "\n", "    ", "batch_size", "=", "from_shape", "[", "0", "]", "\n", "from_seq_length", "=", "from_shape", "[", "1", "]", "\n", "to_seq_length", "=", "to_shape", "[", "1", "]", "\n", "", "elif", "len", "(", "from_shape", ")", "==", "2", ":", "\n", "    ", "if", "(", "batch_size", "is", "None", "or", "from_seq_length", "is", "None", "or", "to_seq_length", "is", "None", ")", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "\"When passing in rank 2 tensors to attention_layer, the values \"", "\n", "\"for `batch_size`, `from_seq_length`, and `to_seq_length` \"", "\n", "\"must all be specified.\"", ")", "\n", "\n", "# Scalar dimensions referenced here:", "\n", "#   B = batch size (number of sequences)", "\n", "#   F = `from_tensor` sequence length", "\n", "#   T = `to_tensor` sequence length", "\n", "#   N = `num_attention_heads`", "\n", "#   H = `size_per_head`", "\n", "\n", "", "", "from_tensor_2d", "=", "reshape_to_matrix", "(", "from_tensor", ")", "\n", "to_tensor_2d", "=", "reshape_to_matrix", "(", "to_tensor", ")", "\n", "\n", "# `query_layer` = [B*F, N*H]", "\n", "query_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "from_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation", "=", "query_act", ",", "\n", "name", "=", "\"query\"", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# `key_layer` = [B*T, N*H]", "\n", "key_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "to_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation", "=", "key_act", ",", "\n", "name", "=", "\"key\"", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# `value_layer` = [B*T, N*H]", "\n", "value_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "to_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation", "=", "value_act", ",", "\n", "name", "=", "\"value\"", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# `query_layer` = [B, N, F, H]", "\n", "query_layer", "=", "transpose_for_scores", "(", "query_layer", ",", "batch_size", ",", "\n", "num_attention_heads", ",", "from_seq_length", ",", "\n", "size_per_head", ")", "\n", "\n", "# `key_layer` = [B, N, T, H]", "\n", "key_layer", "=", "transpose_for_scores", "(", "key_layer", ",", "batch_size", ",", "num_attention_heads", ",", "\n", "to_seq_length", ",", "size_per_head", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw", "\n", "# attention scores.", "\n", "# `attention_scores` = [B, N, F, T]", "\n", "attention_scores", "=", "tf", ".", "matmul", "(", "query_layer", ",", "key_layer", ",", "transpose_b", "=", "True", ")", "\n", "attention_scores", "=", "tf", ".", "multiply", "(", "attention_scores", ",", "\n", "1.0", "/", "math", ".", "sqrt", "(", "float", "(", "size_per_head", ")", ")", ")", "\n", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "# `attention_mask` = [B, 1, F, T]", "\n", "    ", "attention_mask", "=", "tf", ".", "expand_dims", "(", "attention_mask", ",", "axis", "=", "[", "1", "]", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "adder", "=", "(", "1.0", "-", "tf", ".", "cast", "(", "attention_mask", ",", "tf", ".", "float32", ")", ")", "*", "-", "10000.0", "\n", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "attention_scores", "+=", "adder", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "# `attention_probs` = [B, N, F, T]", "\n", "", "attention_probs", "=", "tf", ".", "nn", ".", "softmax", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "dropout", "(", "attention_probs", ",", "attention_probs_dropout_prob", ")", "\n", "\n", "# `value_layer` = [B, T, N, H]", "\n", "value_layer", "=", "tf", ".", "reshape", "(", "\n", "value_layer", ",", "\n", "[", "batch_size", ",", "to_seq_length", ",", "num_attention_heads", ",", "size_per_head", "]", ")", "\n", "\n", "# `value_layer` = [B, N, T, H]", "\n", "value_layer", "=", "tf", ".", "transpose", "(", "value_layer", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n", "# `context_layer` = [B, N, F, H]", "\n", "context_layer", "=", "tf", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "\n", "# `context_layer` = [B, F, N, H]", "\n", "context_layer", "=", "tf", ".", "transpose", "(", "context_layer", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n", "if", "do_return_2d_tensor", ":", "\n", "# `context_layer` = [B*F, N*H]", "\n", "    ", "context_layer", "=", "tf", ".", "reshape", "(", "\n", "context_layer", ",", "\n", "[", "batch_size", "*", "from_seq_length", ",", "num_attention_heads", "*", "size_per_head", "]", ")", "\n", "", "else", ":", "\n", "# `context_layer` = [B, F, N*H]", "\n", "    ", "context_layer", "=", "tf", ".", "reshape", "(", "\n", "context_layer", ",", "\n", "[", "batch_size", ",", "from_seq_length", ",", "num_attention_heads", "*", "size_per_head", "]", ")", "\n", "\n", "", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.transformer_model": [[747, 886], ["int", "modeling.get_shape_list", "modeling.reshape_to_matrix", "range", "ValueError", "ValueError", "modeling.reshape_from_matrix", "tensorflow.variable_scope", "modeling.reshape_from_matrix", "final_outputs.append", "tensorflow.variable_scope", "tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.variable_scope", "tensorflow.layers.dense", "modeling.dropout", "modeling.layer_norm", "all_layer_outputs.append", "tensorflow.variable_scope", "modeling.attention_layer", "attention_heads.append", "len", "tensorflow.concat", "tensorflow.variable_scope", "tensorflow.layers.dense", "modeling.dropout", "modeling.layer_norm", "modeling.create_initializer", "modeling.create_initializer", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_shape_list", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.reshape_to_matrix", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.reshape_from_matrix", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.reshape_from_matrix", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.dropout", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.layer_norm", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.attention_layer", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.dropout", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.layer_norm", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.create_initializer", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.create_initializer", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.create_initializer"], ["", "def", "transformer_model", "(", "input_tensor", ",", "\n", "attention_mask", "=", "None", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "intermediate_act_fn", "=", "gelu", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "do_return_all_layers", "=", "False", ")", ":", "\n", "  ", "\"\"\"Multi-headed, multi-layer Transformer from \"Attention is All You Need\".\n\n  This is almost an exact implementation of the original Transformer encoder.\n\n  See the original paper:\n  https://arxiv.org/abs/1706.03762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the \"intermediate\" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  \"\"\"", "\n", "if", "hidden_size", "%", "num_attention_heads", "!=", "0", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "hidden_size", ",", "num_attention_heads", ")", ")", "\n", "\n", "", "attention_head_size", "=", "int", "(", "hidden_size", "/", "num_attention_heads", ")", "\n", "input_shape", "=", "get_shape_list", "(", "input_tensor", ",", "expected_rank", "=", "3", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "input_width", "=", "input_shape", "[", "2", "]", "\n", "\n", "# The Transformer performs sum residuals on all layers so the input needs", "\n", "# to be the same as the hidden size.", "\n", "if", "input_width", "!=", "hidden_size", ":", "\n", "    ", "raise", "ValueError", "(", "\"The width of the input tensor (%d) != hidden size (%d)\"", "%", "\n", "(", "input_width", ",", "hidden_size", ")", ")", "\n", "\n", "# We keep the representation as a 2D tensor to avoid re-shaping it back and", "\n", "# forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on", "\n", "# the GPU/CPU but may not be free on the TPU, so we want to minimize them to", "\n", "# help the optimizer.", "\n", "", "prev_output", "=", "reshape_to_matrix", "(", "input_tensor", ")", "\n", "\n", "all_layer_outputs", "=", "[", "]", "\n", "for", "layer_idx", "in", "range", "(", "num_hidden_layers", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "\"layer_%d\"", "%", "layer_idx", ")", ":", "\n", "      ", "layer_input", "=", "prev_output", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"attention\"", ")", ":", "\n", "        ", "attention_heads", "=", "[", "]", "\n", "with", "tf", ".", "variable_scope", "(", "\"self\"", ")", ":", "\n", "          ", "attention_head", "=", "attention_layer", "(", "\n", "from_tensor", "=", "layer_input", ",", "\n", "to_tensor", "=", "layer_input", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "num_attention_heads", "=", "num_attention_heads", ",", "\n", "size_per_head", "=", "attention_head_size", ",", "\n", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", ",", "\n", "initializer_range", "=", "initializer_range", ",", "\n", "do_return_2d_tensor", "=", "True", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "from_seq_length", "=", "seq_length", ",", "\n", "to_seq_length", "=", "seq_length", ")", "\n", "attention_heads", ".", "append", "(", "attention_head", ")", "\n", "\n", "", "attention_output", "=", "None", "\n", "if", "len", "(", "attention_heads", ")", "==", "1", ":", "\n", "          ", "attention_output", "=", "attention_heads", "[", "0", "]", "\n", "", "else", ":", "\n", "# In the case where we have other sequences, we just concatenate", "\n", "# them to the self-attention head before the projection.", "\n", "          ", "attention_output", "=", "tf", ".", "concat", "(", "attention_heads", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# Run a linear projection of `hidden_size` then add a residual", "\n", "# with `layer_input`.", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"output\"", ")", ":", "\n", "          ", "attention_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "attention_output", ",", "\n", "hidden_size", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "attention_output", "=", "dropout", "(", "attention_output", ",", "hidden_dropout_prob", ")", "\n", "attention_output", "=", "layer_norm", "(", "attention_output", "+", "layer_input", ")", "\n", "\n", "# The activation is only applied to the \"intermediate\" hidden layer.", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"intermediate\"", ")", ":", "\n", "        ", "intermediate_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "attention_output", ",", "\n", "intermediate_size", ",", "\n", "activation", "=", "intermediate_act_fn", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# Down-project back to `hidden_size` then add the residual.", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"output\"", ")", ":", "\n", "        ", "layer_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "intermediate_output", ",", "\n", "hidden_size", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "layer_output", "=", "dropout", "(", "layer_output", ",", "hidden_dropout_prob", ")", "\n", "layer_output", "=", "layer_norm", "(", "layer_output", "+", "attention_output", ")", "\n", "prev_output", "=", "layer_output", "\n", "all_layer_outputs", ".", "append", "(", "layer_output", ")", "\n", "\n", "", "", "", "if", "do_return_all_layers", ":", "\n", "    ", "final_outputs", "=", "[", "]", "\n", "for", "layer_output", "in", "all_layer_outputs", ":", "\n", "      ", "final_output", "=", "reshape_from_matrix", "(", "layer_output", ",", "input_shape", ")", "\n", "final_outputs", ".", "append", "(", "final_output", ")", "\n", "", "return", "final_outputs", "\n", "", "else", ":", "\n", "    ", "final_output", "=", "reshape_from_matrix", "(", "prev_output", ",", "input_shape", ")", "\n", "return", "final_output", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_shape_list": [[888, 923], ["tensor.shape.as_list", "enumerate", "tensorflow.shape", "modeling.assert_rank", "non_static_indexes.append"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.assert_rank"], ["", "", "def", "get_shape_list", "(", "tensor", ",", "expected_rank", "=", "None", ",", "name", "=", "None", ")", ":", "\n", "  ", "\"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n\n  Args:\n    tensor: A tf.Tensor object to find the shape of.\n    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n      specified and the `tensor` has a different rank, and exception will be\n      thrown.\n    name: Optional name of the tensor for the error message.\n\n  Returns:\n    A list of dimensions of the shape of tensor. All static dimensions will\n    be returned as python integers, and dynamic dimensions will be returned\n    as tf.Tensor scalars.\n  \"\"\"", "\n", "if", "name", "is", "None", ":", "\n", "    ", "name", "=", "tensor", ".", "name", "\n", "\n", "", "if", "expected_rank", "is", "not", "None", ":", "\n", "    ", "assert_rank", "(", "tensor", ",", "expected_rank", ",", "name", ")", "\n", "\n", "", "shape", "=", "tensor", ".", "shape", ".", "as_list", "(", ")", "\n", "\n", "non_static_indexes", "=", "[", "]", "\n", "for", "(", "index", ",", "dim", ")", "in", "enumerate", "(", "shape", ")", ":", "\n", "    ", "if", "dim", "is", "None", ":", "\n", "      ", "non_static_indexes", ".", "append", "(", "index", ")", "\n", "\n", "", "", "if", "not", "non_static_indexes", ":", "\n", "    ", "return", "shape", "\n", "\n", "", "dyn_shape", "=", "tf", ".", "shape", "(", "tensor", ")", "\n", "for", "index", "in", "non_static_indexes", ":", "\n", "    ", "shape", "[", "index", "]", "=", "dyn_shape", "[", "index", "]", "\n", "", "return", "shape", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.reshape_to_matrix": [[925, 937], ["tensorflow.reshape", "ValueError"], "function", ["None"], ["", "def", "reshape_to_matrix", "(", "input_tensor", ")", ":", "\n", "  ", "\"\"\"Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).\"\"\"", "\n", "ndims", "=", "input_tensor", ".", "shape", ".", "ndims", "\n", "if", "ndims", "<", "2", ":", "\n", "    ", "raise", "ValueError", "(", "\"Input tensor must have at least rank 2. Shape = %s\"", "%", "\n", "(", "input_tensor", ".", "shape", ")", ")", "\n", "", "if", "ndims", "==", "2", ":", "\n", "    ", "return", "input_tensor", "\n", "\n", "", "width", "=", "input_tensor", ".", "shape", "[", "-", "1", "]", "\n", "output_tensor", "=", "tf", ".", "reshape", "(", "input_tensor", ",", "[", "-", "1", ",", "width", "]", ")", "\n", "return", "output_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.reshape_from_matrix": [[939, 950], ["modeling.get_shape_list", "tensorflow.reshape", "len"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_shape_list"], ["", "def", "reshape_from_matrix", "(", "output_tensor", ",", "orig_shape_list", ")", ":", "\n", "  ", "\"\"\"Reshapes a rank 2 tensor back to its original rank >= 2 tensor.\"\"\"", "\n", "if", "len", "(", "orig_shape_list", ")", "==", "2", ":", "\n", "    ", "return", "output_tensor", "\n", "\n", "", "output_shape", "=", "get_shape_list", "(", "output_tensor", ")", "\n", "\n", "orig_dims", "=", "orig_shape_list", "[", "0", ":", "-", "1", "]", "\n", "width", "=", "output_shape", "[", "-", "1", "]", "\n", "\n", "return", "tf", ".", "reshape", "(", "output_tensor", ",", "orig_dims", "+", "[", "width", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.assert_rank": [[952, 980], ["isinstance", "ValueError", "tensorflow.get_variable_scope", "str", "str"], "function", ["None"], ["", "def", "assert_rank", "(", "tensor", ",", "expected_rank", ",", "name", "=", "None", ")", ":", "\n", "  ", "\"\"\"Raises an exception if the tensor rank is not of the expected rank.\n\n  Args:\n    tensor: A tf.Tensor to check the rank of.\n    expected_rank: Python integer or list of integers, expected rank.\n    name: Optional name of the tensor for the error message.\n\n  Raises:\n    ValueError: If the expected shape doesn't match the actual shape.\n  \"\"\"", "\n", "if", "name", "is", "None", ":", "\n", "    ", "name", "=", "tensor", ".", "name", "\n", "\n", "", "expected_rank_dict", "=", "{", "}", "\n", "if", "isinstance", "(", "expected_rank", ",", "six", ".", "integer_types", ")", ":", "\n", "    ", "expected_rank_dict", "[", "expected_rank", "]", "=", "True", "\n", "", "else", ":", "\n", "    ", "for", "x", "in", "expected_rank", ":", "\n", "      ", "expected_rank_dict", "[", "x", "]", "=", "True", "\n", "\n", "", "", "actual_rank", "=", "tensor", ".", "shape", ".", "ndims", "\n", "if", "actual_rank", "not", "in", "expected_rank_dict", ":", "\n", "    ", "scope_name", "=", "tf", ".", "get_variable_scope", "(", ")", ".", "name", "\n", "raise", "ValueError", "(", "\n", "\"For the tensor `%s` in scope `%s`, the actual rank \"", "\n", "\"`%d` (shape = %s) is not equal to the expected rank `%s`\"", "%", "\n", "(", "name", ",", "scope_name", ",", "actual_rank", ",", "str", "(", "tensor", ".", "shape", ")", ",", "str", "(", "expected_rank", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.optimization.AdamWeightDecayOptimizer.__init__": [[104, 121], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.FeatureWriter.__init__"], ["def", "__init__", "(", "self", ",", "\n", "learning_rate", ",", "\n", "weight_decay_rate", "=", "0.0", ",", "\n", "beta_1", "=", "0.9", ",", "\n", "beta_2", "=", "0.999", ",", "\n", "epsilon", "=", "1e-6", ",", "\n", "exclude_from_weight_decay", "=", "None", ",", "\n", "name", "=", "\"AdamWeightDecayOptimizer\"", ")", ":", "\n", "    ", "\"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"", "\n", "super", "(", "AdamWeightDecayOptimizer", ",", "self", ")", ".", "__init__", "(", "False", ",", "name", ")", "\n", "\n", "self", ".", "learning_rate", "=", "learning_rate", "\n", "self", ".", "weight_decay_rate", "=", "weight_decay_rate", "\n", "self", ".", "beta_1", "=", "beta_1", "\n", "self", ".", "beta_2", "=", "beta_2", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "exclude_from_weight_decay", "=", "exclude_from_weight_decay", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.optimization.AdamWeightDecayOptimizer.apply_gradients": [[122, 172], ["tensorflow.group", "optimization.AdamWeightDecayOptimizer._get_variable_name", "tensorflow.get_variable", "tensorflow.get_variable", "optimization.AdamWeightDecayOptimizer._do_use_weight_decay", "assignments.extend", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.multiply", "param.shape.as_list", "tensorflow.zeros_initializer", "param.shape.as_list", "tensorflow.zeros_initializer", "tensorflow.square", "tensorflow.sqrt", "param.assign", "tensorflow.get_variable.assign", "tensorflow.get_variable.assign"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.optimization.AdamWeightDecayOptimizer._get_variable_name", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.optimization.AdamWeightDecayOptimizer._do_use_weight_decay"], ["", "def", "apply_gradients", "(", "self", ",", "grads_and_vars", ",", "global_step", "=", "None", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "assignments", "=", "[", "]", "\n", "for", "(", "grad", ",", "param", ")", "in", "grads_and_vars", ":", "\n", "      ", "if", "grad", "is", "None", "or", "param", "is", "None", ":", "\n", "        ", "continue", "\n", "\n", "", "param_name", "=", "self", ".", "_get_variable_name", "(", "param", ".", "name", ")", "\n", "\n", "m", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "param_name", "+", "\"/adam_m\"", ",", "\n", "shape", "=", "param", ".", "shape", ".", "as_list", "(", ")", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "trainable", "=", "False", ",", "\n", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "v", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "param_name", "+", "\"/adam_v\"", ",", "\n", "shape", "=", "param", ".", "shape", ".", "as_list", "(", ")", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "trainable", "=", "False", ",", "\n", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "\n", "# Standard Adam update.", "\n", "next_m", "=", "(", "\n", "tf", ".", "multiply", "(", "self", ".", "beta_1", ",", "m", ")", "+", "tf", ".", "multiply", "(", "1.0", "-", "self", ".", "beta_1", ",", "grad", ")", ")", "\n", "next_v", "=", "(", "\n", "tf", ".", "multiply", "(", "self", ".", "beta_2", ",", "v", ")", "+", "tf", ".", "multiply", "(", "1.0", "-", "self", ".", "beta_2", ",", "\n", "tf", ".", "square", "(", "grad", ")", ")", ")", "\n", "\n", "update", "=", "next_m", "/", "(", "tf", ".", "sqrt", "(", "next_v", ")", "+", "self", ".", "epsilon", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want ot decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "if", "self", ".", "_do_use_weight_decay", "(", "param_name", ")", ":", "\n", "        ", "update", "+=", "self", ".", "weight_decay_rate", "*", "param", "\n", "\n", "", "update_with_lr", "=", "self", ".", "learning_rate", "*", "update", "\n", "\n", "next_param", "=", "param", "-", "update_with_lr", "\n", "\n", "assignments", ".", "extend", "(", "\n", "[", "param", ".", "assign", "(", "next_param", ")", ",", "\n", "m", ".", "assign", "(", "next_m", ")", ",", "\n", "v", ".", "assign", "(", "next_v", ")", "]", ")", "\n", "", "return", "tf", ".", "group", "(", "*", "assignments", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.optimization.AdamWeightDecayOptimizer._do_use_weight_decay": [[173, 182], ["re.search"], "methods", ["None"], ["", "def", "_do_use_weight_decay", "(", "self", ",", "param_name", ")", ":", "\n", "    ", "\"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"", "\n", "if", "not", "self", ".", "weight_decay_rate", ":", "\n", "      ", "return", "False", "\n", "", "if", "self", ".", "exclude_from_weight_decay", ":", "\n", "      ", "for", "r", "in", "self", ".", "exclude_from_weight_decay", ":", "\n", "        ", "if", "re", ".", "search", "(", "r", ",", "param_name", ")", "is", "not", "None", ":", "\n", "          ", "return", "False", "\n", "", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.optimization.AdamWeightDecayOptimizer._get_variable_name": [[183, 189], ["re.match", "re.match.group"], "methods", ["None"], ["", "def", "_get_variable_name", "(", "self", ",", "param_name", ")", ":", "\n", "    ", "\"\"\"Get the variable name from the tensor name.\"\"\"", "\n", "m", "=", "re", ".", "match", "(", "\"^(.*):\\\\d+$\"", ",", "param_name", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "      ", "param_name", "=", "m", ".", "group", "(", "1", ")", "\n", "", "return", "param_name", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.optimization.create_optimizer": [[26, 100], ["tensorflow.train.get_or_create_global_step", "tensorflow.constant", "optimization.AdamWeightDecayOptimizer", "tensorflow.trainable_variables", "tensorflow.gradients", "tensorflow.clip_by_global_norm", "tf.contrib.tpu.CrossShardOptimizer.apply_gradients", "tensorflow.group", "tensorflow.train.polynomial_decay", "tensorflow.cast", "tensorflow.constant", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.contrib.tpu.CrossShardOptimizer", "zip", "tensorflow.train.cosine_decay", "tf.train.get_or_create_global_step.assign", "tensorflow.train.exponential_decay", "ValueError"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.optimization.AdamWeightDecayOptimizer.apply_gradients"], ["def", "create_optimizer", "(", "loss", ",", "init_lr", ",", "num_train_steps", ",", "num_warmup_steps", ",", "use_tpu", ",", "decay_option", "=", "'cosine'", ")", ":", "\n", "  ", "\"\"\"Creates an optimizer training op.\"\"\"", "\n", "global_step", "=", "tf", ".", "train", ".", "get_or_create_global_step", "(", ")", "\n", "\n", "learning_rate", "=", "tf", ".", "constant", "(", "value", "=", "init_lr", ",", "shape", "=", "[", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "# Implements linear decay of the learning rate.", "\n", "if", "decay_option", "==", "'polynomial'", ":", "\n", "    ", "learning_rate", "=", "tf", ".", "train", ".", "polynomial_decay", "(", "\n", "learning_rate", ",", "\n", "global_step", ",", "\n", "num_train_steps", ",", "\n", "end_learning_rate", "=", "0.0", ",", "\n", "power", "=", "1.0", ",", "\n", "cycle", "=", "False", ")", "\n", "", "elif", "decay_option", "==", "'cosine'", ":", "\n", "    ", "learning_rate", "=", "tf", ".", "train", ".", "cosine_decay", "(", "\n", "learning_rate", ",", "\n", "global_step", ",", "\n", "num_train_steps", ")", "\n", "", "elif", "decay_option", "==", "'exponential'", ":", "\n", "    ", "learning_rate", "=", "tf", ".", "train", ".", "exponential_decay", "(", "\n", "learning_rate", ",", "\n", "global_step", ",", "\n", "num_train_steps", ",", "\n", "0.95", ")", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Error decay_option\"", ")", "\n", "\n", "# Implements linear warmup. I.e., if global_step < num_warmup_steps, the", "\n", "# learning rate will be `global_step/num_warmup_steps * init_lr`.", "\n", "", "if", "num_warmup_steps", ":", "\n", "    ", "global_steps_int", "=", "tf", ".", "cast", "(", "global_step", ",", "tf", ".", "int32", ")", "\n", "warmup_steps_int", "=", "tf", ".", "constant", "(", "num_warmup_steps", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "global_steps_float", "=", "tf", ".", "cast", "(", "global_steps_int", ",", "tf", ".", "float32", ")", "\n", "warmup_steps_float", "=", "tf", ".", "cast", "(", "warmup_steps_int", ",", "tf", ".", "float32", ")", "\n", "\n", "warmup_percent_done", "=", "global_steps_float", "/", "warmup_steps_float", "\n", "warmup_learning_rate", "=", "init_lr", "*", "warmup_percent_done", "\n", "\n", "is_warmup", "=", "tf", ".", "cast", "(", "global_steps_int", "<", "warmup_steps_int", ",", "tf", ".", "float32", ")", "\n", "learning_rate", "=", "(", "\n", "(", "1.0", "-", "is_warmup", ")", "*", "learning_rate", "+", "is_warmup", "*", "warmup_learning_rate", ")", "\n", "\n", "# It is recommended that you use this optimizer for fine tuning, since this", "\n", "# is how the model was trained (note that the Adam m/v variables are NOT", "\n", "# loaded from init_checkpoint.)", "\n", "", "optimizer", "=", "AdamWeightDecayOptimizer", "(", "\n", "learning_rate", "=", "learning_rate", ",", "\n", "weight_decay_rate", "=", "0.01", ",", "\n", "beta_1", "=", "0.9", ",", "\n", "beta_2", "=", "0.999", ",", "\n", "epsilon", "=", "1e-6", ",", "\n", "exclude_from_weight_decay", "=", "[", "\"LayerNorm\"", ",", "\"layer_norm\"", ",", "\"bias\"", "]", ")", "\n", "\n", "if", "use_tpu", ":", "\n", "    ", "optimizer", "=", "tf", ".", "contrib", ".", "tpu", ".", "CrossShardOptimizer", "(", "optimizer", ")", "#, reduction=tf.losses.Reduction.SUM)", "\n", "\n", "", "tvars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "grads", "=", "tf", ".", "gradients", "(", "loss", ",", "tvars", ")", "\n", "\n", "# This is how the model was pre-trained.", "\n", "(", "grads", ",", "_", ")", "=", "tf", ".", "clip_by_global_norm", "(", "grads", ",", "clip_norm", "=", "1.0", ")", "\n", "\n", "train_op", "=", "optimizer", ".", "apply_gradients", "(", "\n", "zip", "(", "grads", ",", "tvars", ")", ",", "global_step", "=", "global_step", ")", "\n", "\n", "# Normally the global step update is done inside of `apply_gradients`.", "\n", "# However, `AdamWeightDecayOptimizer` doesn't do this. But if you use", "\n", "# a different optimizer, you should probably take this line out.", "\n", "new_global_step", "=", "global_step", "+", "1", "\n", "train_op", "=", "tf", ".", "group", "(", "train_op", ",", "[", "global_step", ".", "assign", "(", "new_global_step", ")", "]", ")", "\n", "return", "train_op", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.FullTokenizer.__init__": [[164, 169], ["tokenization.load_vocab", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer", "tokenization.FullTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ")", ":", "\n", "    ", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "inv_vocab", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "vocab", ".", "items", "(", ")", "}", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.FullTokenizer.tokenize": [[170, 177], ["tokenization.FullTokenizer.basic_tokenizer.tokenize", "tokenization.FullTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.ChineseFullTokenizer.tokenize", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.ChineseFullTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "      ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "        ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.FullTokenizer.convert_tokens_to_ids": [[178, 180], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.convert_by_vocab"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.FullTokenizer.convert_ids_to_tokens": [[181, 183], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.convert_by_vocab"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "self", ".", "inv_vocab", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.BasicTokenizer.__init__": [[188, 195], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ")", ":", "\n", "    ", "\"\"\"Constructs a BasicTokenizer.\n\n    Args:\n      do_lower_case: Whether to lower case the input.\n    \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.BasicTokenizer.tokenize": [[196, 219], ["tokenization.convert_to_unicode", "tokenization.BasicTokenizer._clean_text", "tokenization.BasicTokenizer._tokenize_chinese_chars", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.BasicTokenizer._tokenize_chinese_chars", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "      ", "if", "self", ".", "do_lower_case", ":", "\n", "        ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.BasicTokenizer._run_strip_accents": [[220, 230], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "        ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.BasicTokenizer._run_split_on_punc": [[231, 250], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "      ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "        ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "        ", "if", "start_new_word", ":", "\n", "          ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.BasicTokenizer._tokenize_chinese_chars": [[251, 263], ["ord", "tokenization.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "        ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "        ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.BasicTokenizer._is_chinese_char": [[264, 285], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "    ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "      ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.BasicTokenizer._clean_text": [[286, 298], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization._is_whitespace", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "        ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "        ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "        ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.WordpieceTokenizer.__init__": [[303, 307], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "200", ")", ":", "\n", "    ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.WordpieceTokenizer.tokenize": [[308, 360], ["tokenization.convert_to_unicode", "tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n    This uses a greedy longest-match-first algorithm to perform tokenization\n    using the given vocabulary.\n\n    For example:\n      input = \"unaffable\"\n      output = [\"un\", \"##aff\", \"##able\"]\n\n    Args:\n      text: A single token or whitespace separated tokens. This should have\n        already been passed through `BasicTokenizer.\n\n    Returns:\n      A list of wordpiece tokens.\n    \"\"\"", "\n", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "      ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "        ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "        ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "          ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "            ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "            ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "          ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "        ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "        ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.validate_case_matches_checkpoint": [[28, 76], ["re.match", "re.match.group", "ValueError"], "function", ["None"], ["def", "validate_case_matches_checkpoint", "(", "do_lower_case", ",", "init_checkpoint", ")", ":", "\n", "  ", "\"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"", "\n", "\n", "# The casing has to be passed in by the user and there is no explicit check", "\n", "# as to whether it matches the checkpoint. The casing information probably", "\n", "# should have been stored in the bert_config.json file, but it's not, so", "\n", "# we have to heuristically detect it to validate.", "\n", "\n", "if", "not", "init_checkpoint", ":", "\n", "    ", "return", "\n", "\n", "", "m", "=", "re", ".", "match", "(", "\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\"", ",", "init_checkpoint", ")", "\n", "if", "m", "is", "None", ":", "\n", "    ", "return", "\n", "\n", "", "model_name", "=", "m", ".", "group", "(", "1", ")", "\n", "\n", "lower_models", "=", "[", "\n", "\"uncased_L-24_H-1024_A-16\"", ",", "\"uncased_L-12_H-768_A-12\"", ",", "\n", "\"multilingual_L-12_H-768_A-12\"", ",", "\"chinese_L-12_H-768_A-12\"", "\n", "]", "\n", "\n", "cased_models", "=", "[", "\n", "\"cased_L-12_H-768_A-12\"", ",", "\"cased_L-24_H-1024_A-16\"", ",", "\n", "\"multi_cased_L-12_H-768_A-12\"", "\n", "]", "\n", "\n", "is_bad_config", "=", "False", "\n", "if", "model_name", "in", "lower_models", "and", "not", "do_lower_case", ":", "\n", "    ", "is_bad_config", "=", "True", "\n", "actual_flag", "=", "\"False\"", "\n", "case_name", "=", "\"lowercased\"", "\n", "opposite_flag", "=", "\"True\"", "\n", "\n", "", "if", "model_name", "in", "cased_models", "and", "do_lower_case", ":", "\n", "    ", "is_bad_config", "=", "True", "\n", "actual_flag", "=", "\"True\"", "\n", "case_name", "=", "\"cased\"", "\n", "opposite_flag", "=", "\"False\"", "\n", "\n", "", "if", "is_bad_config", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"", "\n", "\"However, `%s` seems to be a %s model, so you \"", "\n", "\"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"", "\n", "\"how the model was pre-training. If this error is wrong, please \"", "\n", "\"just comment out this check.\"", "%", "(", "actual_flag", ",", "init_checkpoint", ",", "\n", "model_name", ",", "case_name", ",", "opposite_flag", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.convert_to_unicode": [[78, 96], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "text.decode", "isinstance", "ValueError", "type", "type"], "function", ["None"], ["", "", "def", "convert_to_unicode", "(", "text", ")", ":", "\n", "  ", "\"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"", "\n", "if", "six", ".", "PY3", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "      ", "return", "text", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.printable_text": [[98, 119], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "isinstance", "text.encode", "ValueError", "type", "type"], "function", ["None"], ["", "", "def", "printable_text", "(", "text", ")", ":", "\n", "  ", "\"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"", "\n", "\n", "# These functions want `str` for both Python2 and Python3, but in one case", "\n", "# it's a Unicode string and in the other it's a byte string.", "\n", "if", "six", ".", "PY3", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "      ", "return", "text", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.load_vocab": [[121, 134], ["collections.OrderedDict", "tensorflow.gfile.GFile", "tokenization.convert_to_unicode", "token.strip.strip", "reader.readline"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.convert_to_unicode"], ["", "", "def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "  ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "vocab_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "    ", "while", "True", ":", "\n", "      ", "token", "=", "convert_to_unicode", "(", "reader", ".", "readline", "(", ")", ")", "\n", "if", "not", "token", ":", "\n", "        ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.convert_by_vocab": [[136, 142], ["output.append"], "function", ["None"], ["", "def", "convert_by_vocab", "(", "vocab", ",", "items", ")", ":", "\n", "  ", "\"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "item", "in", "items", ":", "\n", "    ", "output", ".", "append", "(", "vocab", "[", "item", "]", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.convert_tokens_to_ids": [[144, 146], ["tokenization.convert_by_vocab"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.convert_by_vocab"], ["", "def", "convert_tokens_to_ids", "(", "vocab", ",", "tokens", ")", ":", "\n", "  ", "return", "convert_by_vocab", "(", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.convert_ids_to_tokens": [[148, 150], ["tokenization.convert_by_vocab"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.convert_by_vocab"], ["", "def", "convert_ids_to_tokens", "(", "inv_vocab", ",", "ids", ")", ":", "\n", "  ", "return", "convert_by_vocab", "(", "inv_vocab", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.whitespace_tokenize": [[152, 159], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "  ", "\"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "    ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization._is_whitespace": [[362, 372], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "    ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization._is_control": [[374, 384], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "    ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization._is_punctuation": [[386, 400], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "    ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.layers.self_attention_layer": [[21, 234], ["modeling.get_shape_list", "modeling.get_shape_list", "modeling.reshape_to_matrix", "modeling.reshape_to_matrix", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.layers.dense", "layers.self_attention_layer.transpose_for_scores"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_shape_list", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_shape_list", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.reshape_to_matrix", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.reshape_to_matrix"], ["def", "self_attention_layer", "(", "from_tensor", ",", "\n", "to_tensor", ",", "\n", "self_adaptive", "=", "True", ",", "\n", "attention_mask", "=", "None", ",", "\n", "num_attention_heads", "=", "1", ",", "\n", "size_per_head", "=", "512", ",", "\n", "query_act", "=", "None", ",", "\n", "key_act", "=", "None", ",", "\n", "value_act", "=", "None", ",", "\n", "attention_probs_dropout_prob", "=", "0.0", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "do_return_2d_tensor", "=", "False", ",", "\n", "batch_size", "=", "None", ",", "\n", "from_seq_length", "=", "None", ",", "\n", "to_seq_length", "=", "None", ")", ":", "\n", "  ", "\"\"\"Performs multi-headed attention from `from_tensor` to `to_tensor`.\n\n  This is an implementation of multi-headed attention based on \"Attention\n  is all you Need\". If `from_tensor` and `to_tensor` are the same, then\n  this is self-attention. Each timestep in `from_tensor` attends to the\n  corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n\n  This function first projects `from_tensor` into a \"query\" tensor and\n  `to_tensor` into \"key\" and \"value\" tensors. These are (effectively) a list\n  of tensors of length `num_attention_heads`, where each tensor is of shape\n  [batch_size, seq_length, size_per_head].\n\n  Then, the query and key tensors are dot-producted and scaled. These are\n  softmaxed to obtain attention probabilities. The value tensors are then\n  interpolated by these probabilities, then concatenated back to a single\n  tensor and returned.\n\n  In practice, the multi-headed attention are done with transposes and\n  reshapes rather than actual separate tensors.\n\n  Args:\n    from_tensor: float Tensor of shape [batch_size, from_seq_length,\n      from_width].\n    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n    attention_mask: (optional) int32 Tensor of shape [batch_size,\n      from_seq_length, to_seq_length]. The values should be 1 or 0. The\n      attention scores will effectively be set to -infinity for any positions in\n      the mask that are 0, and will be unchanged for positions that are 1.\n    num_attention_heads: int. Number of attention heads.\n    size_per_head: int. Size of each attention head.\n    query_act: (optional) Activation function for the query transform.\n    key_act: (optional) Activation function for the key transform.\n    value_act: (optional) Activation function for the value transform.\n    attention_probs_dropout_prob: (optional) float. Dropout probability of the\n      attention probabilities.\n    initializer_range: float. Range of the weight initializer.\n    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n      * from_seq_length, num_attention_heads * size_per_head]. If False, the\n      output will be of shape [batch_size, from_seq_length, num_attention_heads\n      * size_per_head].\n    batch_size: (Optional) int. If the input is 2D, this might be the batch size\n      of the 3D version of the `from_tensor` and `to_tensor`.\n    from_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `from_tensor`.\n    to_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `to_tensor`.\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length,\n      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n      true, this will be of shape [batch_size * from_seq_length,\n      num_attention_heads * size_per_head]).\n\n  Raises:\n    ValueError: Any of the arguments or tensor shapes are invalid.\n  \"\"\"", "\n", "\n", "def", "transpose_for_scores", "(", "input_tensor", ",", "batch_size", ",", "num_attention_heads", ",", "\n", "seq_length", ",", "width", ")", ":", "\n", "    ", "output_tensor", "=", "tf", ".", "reshape", "(", "\n", "input_tensor", ",", "[", "batch_size", ",", "seq_length", ",", "num_attention_heads", ",", "width", "]", ")", "\n", "\n", "output_tensor", "=", "tf", ".", "transpose", "(", "output_tensor", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "return", "output_tensor", "\n", "\n", "", "from_shape", "=", "modeling", ".", "get_shape_list", "(", "from_tensor", ",", "expected_rank", "=", "[", "2", ",", "3", "]", ")", "\n", "to_shape", "=", "modeling", ".", "get_shape_list", "(", "to_tensor", ",", "expected_rank", "=", "[", "2", ",", "3", "]", ")", "\n", "\n", "if", "len", "(", "from_shape", ")", "!=", "len", "(", "to_shape", ")", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"The rank of `from_tensor` must match the rank of `to_tensor`.\"", ")", "\n", "\n", "", "if", "len", "(", "from_shape", ")", "==", "3", ":", "\n", "    ", "batch_size", "=", "from_shape", "[", "0", "]", "\n", "from_seq_length", "=", "from_shape", "[", "1", "]", "\n", "to_seq_length", "=", "to_shape", "[", "1", "]", "\n", "", "elif", "len", "(", "from_shape", ")", "==", "2", ":", "\n", "    ", "if", "(", "batch_size", "is", "None", "or", "from_seq_length", "is", "None", "or", "to_seq_length", "is", "None", ")", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "\"When passing in rank 2 tensors to attention_layer, the values \"", "\n", "\"for `batch_size`, `from_seq_length`, and `to_seq_length` \"", "\n", "\"must all be specified.\"", ")", "\n", "\n", "# Scalar dimensions referenced here:", "\n", "#   B = batch size (number of sequences)", "\n", "#   F = `from_tensor` sequence length", "\n", "#   T = `to_tensor` sequence length", "\n", "#   N = `num_attention_heads`", "\n", "#   H = `size_per_head`", "\n", "\n", "", "", "from_tensor_2d", "=", "modeling", ".", "reshape_to_matrix", "(", "from_tensor", ")", "\n", "to_tensor_2d", "=", "modeling", ".", "reshape_to_matrix", "(", "to_tensor", ")", "\n", "\n", "# `query_layer` = [B*F, N*H]", "\n", "raw_query_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "from_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation", "=", "query_act", ",", "\n", "name", "=", "\"query\"", ",", "\n", "kernel_initializer", "=", "modeling", ".", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# `key_layer` = [B*T, N*H]", "\n", "raw_key_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "to_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation", "=", "key_act", ",", "\n", "name", "=", "\"key\"", ",", "\n", "kernel_initializer", "=", "modeling", ".", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# `value_layer` = [B*T, N*H]", "\n", "raw_value_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "to_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation", "=", "value_act", ",", "\n", "name", "=", "\"value\"", ",", "\n", "kernel_initializer", "=", "modeling", ".", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# `query_layer` = [B, N, F, H]", "\n", "query_layer", "=", "transpose_for_scores", "(", "raw_query_layer", ",", "batch_size", ",", "\n", "num_attention_heads", ",", "from_seq_length", ",", "\n", "size_per_head", ")", "\n", "\n", "# `key_layer` = [B, N, T, H]", "\n", "key_layer", "=", "transpose_for_scores", "(", "raw_key_layer", ",", "batch_size", ",", "num_attention_heads", ",", "\n", "to_seq_length", ",", "size_per_head", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw", "\n", "# attention scores.", "\n", "# `attention_scores` = [B, N, F, T]", "\n", "attention_scores", "=", "tf", ".", "matmul", "(", "query_layer", ",", "key_layer", ",", "transpose_b", "=", "True", ")", "\n", "\n", "# self-interactive attention (alpha version)", "\n", "# [F, F_sm] x [F, T] x [T_sm, T] => [F, T]", "\n", "if", "self_adaptive", ":", "\n", "# `left_matrix` = [B, N, F, F_sm]", "\n", "    ", "left_matrix", "=", "tf", ".", "matmul", "(", "query_layer", ",", "query_layer", ",", "transpose_b", "=", "True", ")", "\n", "left_matrix", "=", "tf", ".", "nn", ".", "softmax", "(", "left_matrix", ")", "\n", "\n", "# `right_matrix` = [B, N, T_sm, T]", "\n", "right_matrix", "=", "tf", ".", "matmul", "(", "key_layer", ",", "key_layer", ",", "transpose_b", "=", "True", ")", "\n", "right_matrix", "=", "tf", ".", "nn", ".", "softmax", "(", "right_matrix", ")", "\n", "right_matrix", "=", "tf", ".", "transpose", "(", "right_matrix", ",", "[", "0", ",", "1", ",", "3", ",", "2", "]", ")", "\n", "\n", "# `left_product` = [B, N, F, F_sm] x [B, N, F, T]", "\n", "left_product", "=", "tf", ".", "matmul", "(", "left_matrix", ",", "attention_scores", ")", "\n", "\n", "# `attention_scores` = [B, N, F, T] x [B, N, T_sm, T]", "\n", "attention_scores", "=", "tf", ".", "matmul", "(", "left_product", ",", "right_matrix", ")", "\n", "\n", "", "attention_scores", "=", "tf", ".", "multiply", "(", "attention_scores", ",", "1.0", "/", "math", ".", "sqrt", "(", "float", "(", "size_per_head", ")", ")", ")", "\n", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "# `attention_mask` = [B, 1, F, T]", "\n", "    ", "attention_mask", "=", "tf", ".", "expand_dims", "(", "attention_mask", ",", "axis", "=", "[", "1", "]", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "adder", "=", "(", "1.0", "-", "tf", ".", "cast", "(", "attention_mask", ",", "tf", ".", "float32", ")", ")", "*", "-", "10000.0", "\n", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "attention_scores", "+=", "adder", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "# `attention_probs` = [B, N, F, T]", "\n", "", "attention_probs", "=", "tf", ".", "nn", ".", "softmax", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "modeling", ".", "dropout", "(", "attention_probs", ",", "attention_probs_dropout_prob", ")", "\n", "\n", "# `value_layer` = [B, T, N, H]", "\n", "value_layer", "=", "tf", ".", "reshape", "(", "\n", "raw_value_layer", ",", "\n", "[", "batch_size", ",", "to_seq_length", ",", "num_attention_heads", ",", "size_per_head", "]", ")", "\n", "\n", "# `value_layer` = [B, N, T, H]", "\n", "value_layer", "=", "tf", ".", "transpose", "(", "value_layer", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n", "# `context_layer` = [B, N, F, H]", "\n", "context_layer", "=", "tf", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "\n", "# `context_layer` = [B, F, N, H]", "\n", "context_layer", "=", "tf", ".", "transpose", "(", "context_layer", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n", "if", "do_return_2d_tensor", ":", "\n", "# `context_layer` = [B*F, N*H]", "\n", "    ", "context_layer", "=", "tf", ".", "reshape", "(", "\n", "context_layer", ",", "\n", "[", "batch_size", "*", "from_seq_length", ",", "num_attention_heads", "*", "size_per_head", "]", ")", "\n", "", "else", ":", "\n", "# `context_layer` = [B, F, N*H]", "\n", "    ", "context_layer", "=", "tf", ".", "reshape", "(", "\n", "context_layer", ",", "\n", "[", "batch_size", ",", "from_seq_length", ",", "num_attention_heads", "*", "size_per_head", "]", ")", "\n", "\n", "", "return", "context_layer", "#, raw_query_layer, raw_value_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.layers.gather_indexes": [[237, 254], ["modeling.get_shape_list", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.gather", "tensorflow.range"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_shape_list"], ["", "def", "gather_indexes", "(", "sequence_tensor", ",", "positions", ")", ":", "\n", "  ", "\"\"\"\n  Gathers the vectors at the specific positions over a minibatch.\n  sequence_tensor: [batch, seq_length, width]\n  positions: [batch, n]\n  \"\"\"", "\n", "sequence_shape", "=", "modeling", ".", "get_shape_list", "(", "sequence_tensor", ",", "expected_rank", "=", "3", ")", "\n", "batch_size", "=", "sequence_shape", "[", "0", "]", "\n", "seq_length", "=", "sequence_shape", "[", "1", "]", "\n", "width", "=", "sequence_shape", "[", "2", "]", "\n", "\n", "flat_offsets", "=", "tf", ".", "reshape", "(", "tf", ".", "range", "(", "0", ",", "batch_size", ",", "dtype", "=", "tf", ".", "int32", ")", "*", "seq_length", ",", "[", "-", "1", ",", "1", "]", ")", "\n", "flat_positions", "=", "tf", ".", "reshape", "(", "positions", "+", "flat_offsets", ",", "[", "-", "1", "]", ")", "\n", "flat_sequence_tensor", "=", "tf", ".", "reshape", "(", "sequence_tensor", ",", "[", "batch_size", "*", "seq_length", ",", "width", "]", ")", "\n", "output_tensor", "=", "tf", ".", "gather", "(", "flat_sequence_tensor", ",", "flat_positions", ")", "\n", "\n", "return", "output_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.layers.simple_attention": [[257, 305], ["modeling.get_shape_list", "tensorflow.reduce_mean", "tensorflow.nn.softmax", "tensorflow.matmul", "tensorflow.squeeze", "tensorflow.matmul", "tensorflow.nn.softmax", "tensorflow.matmul", "tensorflow.nn.softmax", "tensorflow.transpose", "tensorflow.matmul", "tensorflow.matmul", "tensorflow.matmul", "tensorflow.matmul", "tensorflow.expand_dims", "tensorflow.cast"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_shape_list"], ["", "def", "simple_attention", "(", "tensor_a", ",", "tensor_b", ",", "attention_mask", ",", "self_adaptive", "=", "False", ")", ":", "\n", "  ", "'''\n  input tensor_a/b: [B, L, H]\n\n  '''", "\n", "sequence_shape", "=", "modeling", ".", "get_shape_list", "(", "tensor_a", ",", "expected_rank", "=", "3", ")", "\n", "batch_size", "=", "sequence_shape", "[", "0", "]", "\n", "seq_length", "=", "sequence_shape", "[", "1", "]", "\n", "width", "=", "sequence_shape", "[", "2", "]", "\n", "\n", "# self-interactive attention (alpha version)", "\n", "# [F, F_sm] x [F, T] x [T_sm, T] => [F, T]", "\n", "if", "self_adaptive", ":", "\n", "# `left_matrix` = [B, F, F_sm]", "\n", "    ", "left_matrix", "=", "tf", ".", "matmul", "(", "tensor_a", ",", "tensor_a", ",", "transpose_b", "=", "True", ")", "\n", "left_matrix", "=", "tf", ".", "nn", ".", "softmax", "(", "left_matrix", ")", "\n", "\n", "# `right_matrix` = [B, T_sm, T]", "\n", "right_matrix", "=", "tf", ".", "matmul", "(", "tensor_b", ",", "tensor_b", ",", "transpose_b", "=", "True", ")", "\n", "right_matrix", "=", "tf", ".", "nn", ".", "softmax", "(", "right_matrix", ")", "\n", "right_matrix", "=", "tf", ".", "transpose", "(", "right_matrix", ",", "[", "0", ",", "2", ",", "1", "]", ")", "\n", "\n", "# `left_product` = [B, F, F_sm] x [B, F, T]", "\n", "attention_matrix", "=", "tf", ".", "matmul", "(", "tensor_a", ",", "tensor_b", ",", "transpose_b", "=", "True", ")", "\n", "left_product", "=", "tf", ".", "matmul", "(", "left_matrix", ",", "attention_matrix", ")", "\n", "\n", "# `attention_matrix` = [B, F, T] x [B, T_sm, T]", "\n", "attention_matrix", "=", "tf", ".", "matmul", "(", "left_product", ",", "right_matrix", ")", "\n", "", "else", ":", "\n", "# attention_matrix = [B, L, L] = [B, L, H] * [B, H, L]", "\n", "    ", "attention_matrix", "=", "tf", ".", "matmul", "(", "tensor_a", ",", "tensor_b", ",", "transpose_b", "=", "True", ")", "\n", "\n", "# attention_raw_value = [B, L]", "\n", "", "attention_raw_value", "=", "tf", ".", "reduce_mean", "(", "attention_matrix", ",", "axis", "=", "1", ")", "\n", "\n", "# apply mask", "\n", "adder", "=", "(", "1.0", "-", "tf", ".", "cast", "(", "attention_mask", ",", "tf", ".", "float32", ")", ")", "*", "-", "10000.0", "\n", "attention_raw_value", "+=", "adder", "\n", "\n", "# apply softmax", "\n", "# attention_scores = [B, L_sm]", "\n", "attention_scores", "=", "tf", ".", "nn", ".", "softmax", "(", "attention_raw_value", ")", "\n", "\n", "# get attended representation", "\n", "attended_repr", "=", "tf", ".", "matmul", "(", "tf", ".", "expand_dims", "(", "attention_scores", ",", "1", ")", ",", "tensor_a", ")", "\n", "attended_repr", "=", "tf", ".", "squeeze", "(", "attended_repr", ",", "1", ")", "\n", "\n", "return", "attended_repr", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.layers.extract_span_tensor": [[308, 324], ["modeling.get_shape_list", "tensorflow.cast", "tensorflow.expand_dims", "tensorflow.variable_scope", "layers.gather_indexes", "layers.gather_indexes", "layers.simple_attention", "tensorflow.concat", "tensorflow.expand_dims", "tensorflow.expand_dims"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_shape_list", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.layers.gather_indexes", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.layers.gather_indexes", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.layers.simple_attention"], ["", "def", "extract_span_tensor", "(", "bert_config", ",", "sequence_tensor", ",", "output_span_mask", ",", "\n", "start_positions", ",", "end_positions", ",", "\n", "scope", "=", "None", ")", ":", "\n", "  ", "sequence_shape", "=", "modeling", ".", "get_shape_list", "(", "sequence_tensor", ",", "expected_rank", "=", "3", ")", "\n", "seq_length", "=", "sequence_shape", "[", "1", "]", "\n", "\n", "output_span_mask", "=", "tf", ".", "cast", "(", "output_span_mask", ",", "tf", ".", "float32", ")", "\n", "filtered_sequence_tensor", "=", "sequence_tensor", "*", "tf", ".", "expand_dims", "(", "output_span_mask", ",", "-", "1", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"span_loss\"", ")", ":", "\n", "    ", "fw_output_tensor", "=", "gather_indexes", "(", "filtered_sequence_tensor", ",", "tf", ".", "expand_dims", "(", "start_positions", ",", "-", "1", ")", ")", "\n", "bw_output_tensor", "=", "gather_indexes", "(", "filtered_sequence_tensor", ",", "tf", ".", "expand_dims", "(", "end_positions", ",", "-", "1", ")", ")", "\n", "att_output_tensor", "=", "simple_attention", "(", "filtered_sequence_tensor", ",", "filtered_sequence_tensor", ",", "output_span_mask", ",", "self_adaptive", "=", "True", ")", "\n", "output_tensor", "=", "tf", ".", "concat", "(", "[", "fw_output_tensor", ",", "bw_output_tensor", ",", "att_output_tensor", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "", "return", "output_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.layers.attention_fusion_layer": [[327, 379], ["modeling.get_shape_list", "modeling.get_shape_list", "tensorflow.concat", "tensorflow.variable_scope", "int", "tensorflow.variable_scope", "layers..", "tensorflow.variable_scope", "tensorflow.layers.dense", "modeling.dropout", "modeling.layer_norm", "modeling.create_attention_mask_from_input_mask", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_shape_list", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_shape_list", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.dropout", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.layer_norm", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.create_attention_mask_from_input_mask", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.create_initializer"], ["", "def", "attention_fusion_layer", "(", "bert_config", ",", "\n", "input_tensor", ",", "input_ids", ",", "input_mask", ",", "\n", "source_input_tensor", ",", "source_input_ids", ",", "source_input_mask", ",", "\n", "is_training", "=", "True", ",", "scope", "=", "None", ")", ":", "\n", "  ", "'''\n  Attention Fusion Layer for merging source representation and target representation.\n  '''", "\n", "# universal shapes", "\n", "input_tensor_shape", "=", "modeling", ".", "get_shape_list", "(", "input_tensor", ",", "expected_rank", "=", "3", ")", "\n", "batch_size", "=", "input_tensor_shape", "[", "0", "]", "\n", "seq_length", "=", "input_tensor_shape", "[", "1", "]", "\n", "hidden_size", "=", "input_tensor_shape", "[", "2", "]", "\n", "source_input_tensor_shape", "=", "modeling", ".", "get_shape_list", "(", "source_input_tensor", ",", "expected_rank", "=", "3", ")", "\n", "source_seq_length", "=", "source_input_tensor_shape", "[", "1", "]", "\n", "source_hidden_size", "=", "source_input_tensor_shape", "[", "2", "]", "\n", "\n", "# universal parameters", "\n", "UNIVERSAL_DROPOUT_RATE", "=", "0.1", "\n", "if", "not", "is_training", ":", "\n", "    ", "UNIVERSAL_DROPOUT_RATE", "=", "0", "# we disable dropout when predicting", "\n", "", "UNIVERSAL_INIT_RANGE", "=", "bert_config", ".", "initializer_range", "\n", "NUM_ATTENTION_HEAD", "=", "bert_config", ".", "num_attention_heads", "\n", "\n", "# attention fusion module", "\n", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"attention_fusion\"", ")", ":", "\n", "    ", "ATTENTION_HEAD_SIZE", "=", "int", "(", "source_hidden_size", "/", "NUM_ATTENTION_HEAD", ")", "\n", "with", "tf", ".", "variable_scope", "(", "\"attention\"", ")", ":", "\n", "      ", "source_attended_repr", "=", "self_attention_layer", "(", "\n", "from_tensor", "=", "input_tensor", ",", "\n", "to_tensor", "=", "source_input_tensor", ",", "\n", "attention_mask", "=", "modeling", ".", "create_attention_mask_from_input_mask", "(", "input_ids", ",", "source_input_mask", ")", ",", "\n", "num_attention_heads", "=", "NUM_ATTENTION_HEAD", ",", "\n", "size_per_head", "=", "ATTENTION_HEAD_SIZE", ",", "\n", "attention_probs_dropout_prob", "=", "UNIVERSAL_DROPOUT_RATE", ",", "\n", "initializer_range", "=", "UNIVERSAL_INIT_RANGE", ",", "\n", "do_return_2d_tensor", "=", "False", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "from_seq_length", "=", "seq_length", ",", "\n", "to_seq_length", "=", "source_seq_length", ",", "\n", "self_adaptive", "=", "True", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"transform\"", ")", ":", "\n", "      ", "source_attended_repr", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "source_attended_repr", ",", "\n", "source_hidden_size", ",", "\n", "kernel_initializer", "=", "modeling", ".", "create_initializer", "(", "UNIVERSAL_INIT_RANGE", ")", ")", "\n", "source_attended_repr", "=", "modeling", ".", "dropout", "(", "source_attended_repr", ",", "UNIVERSAL_DROPOUT_RATE", ")", "\n", "source_attended_repr", "=", "modeling", ".", "layer_norm", "(", "source_attended_repr", "+", "source_input_tensor", ")", "\n", "\n", "", "", "final_output", "=", "tf", ".", "concat", "(", "[", "input_tensor", ",", "source_attended_repr", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "return", "final_output", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.layers.span_output_layer": [[382, 412], ["modeling.get_shape_list", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.nn.bias_add", "tensorflow.reshape", "tensorflow.transpose", "tensorflow.unstack", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.truncated_normal_initializer", "tensorflow.zeros_initializer", "tensorflow.cast"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_shape_list"], ["", "def", "span_output_layer", "(", "bert_config", ",", "input_tensor", ",", "input_span_mask", "=", "None", ",", "scope", "=", "None", ")", ":", "\n", "  ", "input_tensor_shape", "=", "modeling", ".", "get_shape_list", "(", "input_tensor", ",", "expected_rank", "=", "3", ")", "\n", "batch_size", "=", "input_tensor_shape", "[", "0", "]", "\n", "seq_length", "=", "input_tensor_shape", "[", "1", "]", "\n", "hidden_size", "=", "input_tensor_shape", "[", "2", "]", "\n", "\n", "# output layers", "\n", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"cls/squad\"", ")", ":", "\n", "    ", "output_weights", "=", "tf", ".", "get_variable", "(", "\"output_weights\"", ",", "[", "2", ",", "hidden_size", "]", ",", "\n", "initializer", "=", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "0.02", ")", ")", "\n", "output_bias", "=", "tf", ".", "get_variable", "(", "\"output_bias\"", ",", "[", "2", "]", ",", "\n", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "\n", "", "final_hidden_matrix", "=", "tf", ".", "reshape", "(", "input_tensor", ",", "[", "batch_size", "*", "seq_length", ",", "hidden_size", "]", ")", "\n", "logits", "=", "tf", ".", "matmul", "(", "final_hidden_matrix", ",", "output_weights", ",", "transpose_b", "=", "True", ")", "\n", "logits", "=", "tf", ".", "nn", ".", "bias_add", "(", "logits", ",", "output_bias", ")", "\n", "\n", "logits", "=", "tf", ".", "reshape", "(", "logits", ",", "[", "batch_size", ",", "seq_length", ",", "2", "]", ")", "\n", "logits", "=", "tf", ".", "transpose", "(", "logits", ",", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "\n", "unstacked_logits", "=", "tf", ".", "unstack", "(", "logits", ",", "axis", "=", "0", ")", "\n", "\n", "(", "start_logits", ",", "end_logits", ")", "=", "(", "unstacked_logits", "[", "0", "]", ",", "unstacked_logits", "[", "1", "]", ")", "\n", "\n", "if", "input_span_mask", "is", "not", "None", ":", "\n", "    ", "adder", "=", "(", "1.0", "-", "tf", ".", "cast", "(", "input_span_mask", ",", "tf", ".", "float32", ")", ")", "*", "-", "10000.0", "\n", "start_logits", "+=", "adder", "\n", "end_logits", "+=", "adder", "\n", "\n", "", "return", "(", "start_logits", ",", "end_logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.layers.create_model": [[415, 466], ["modeling.BertModel", "modeling.BertModel", "modeling.BertModel.get_sequence_output", "modeling.BertModel.get_sequence_output", "layers.span_output_layer", "layers.attention_fusion_layer", "layers.span_output_layer", "layers.extract_span_tensor", "layers.extract_span_tensor"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.BertModel.get_sequence_output", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.BertModel.get_sequence_output", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.layers.span_output_layer", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.layers.attention_fusion_layer", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.layers.span_output_layer", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.layers.extract_span_tensor", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.layers.extract_span_tensor"], ["", "def", "create_model", "(", "bert_config", ",", "is_training", ",", "\n", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "input_span_mask", ",", "output_span_mask", ",", "\n", "source_input_ids", ",", "source_input_mask", ",", "source_segment_ids", ",", "source_input_span_mask", ",", "source_output_span_mask", ",", "\n", "start_positions", ",", "end_positions", ",", "source_start_positions", ",", "source_end_positions", ",", "\n", "use_one_hot_embeddings", ")", ":", "\n", "\n", "  ", "model", "=", "modeling", ".", "BertModel", "(", "\n", "config", "=", "bert_config", ",", "\n", "is_training", "=", "is_training", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "token_type_ids", "=", "segment_ids", ",", "\n", "use_one_hot_embeddings", "=", "use_one_hot_embeddings", ",", "\n", "scope", "=", "'bert'", ")", "\n", "\n", "source_model", "=", "modeling", ".", "BertModel", "(", "\n", "config", "=", "bert_config", ",", "\n", "is_training", "=", "is_training", ",", "\n", "input_ids", "=", "source_input_ids", ",", "\n", "input_mask", "=", "source_input_mask", ",", "\n", "token_type_ids", "=", "source_segment_ids", ",", "\n", "use_one_hot_embeddings", "=", "use_one_hot_embeddings", ",", "\n", "scope", "=", "'bert'", ",", "\n", "reuse", "=", "True", ")", "\n", "\n", "# get BERT outputs", "\n", "target_final_hidden", "=", "model", ".", "get_sequence_output", "(", ")", "\n", "source_final_hidden", "=", "source_model", ".", "get_sequence_output", "(", ")", "\n", "\n", "# source BERT predictions", "\n", "(", "source_raw_start_logits", ",", "source_raw_end_logits", ")", "=", "span_output_layer", "(", "bert_config", ",", "\n", "source_final_hidden", ",", "source_input_span_mask", ",", "\n", "scope", "=", "'source/cls/squad'", ")", "\n", "\n", "# target BERT predictions", "\n", "source_attended_repr", "=", "attention_fusion_layer", "(", "bert_config", ",", "\n", "target_final_hidden", ",", "input_ids", ",", "input_mask", ",", "\n", "source_final_hidden", ",", "source_input_ids", ",", "source_input_mask", ",", "\n", "is_training", "=", "is_training", ",", "scope", "=", "'target/attention_fusion'", ")", "\n", "(", "target_start_logits", ",", "target_end_logits", ")", "=", "span_output_layer", "(", "bert_config", ",", "\n", "source_attended_repr", ",", "input_span_mask", ",", "\n", "scope", "=", "'target/cls/squad'", ")", "\n", "# ", "\n", "source_span_gt_tensor", "=", "extract_span_tensor", "(", "bert_config", ",", "source_final_hidden", ",", "source_output_span_mask", ",", "\n", "source_start_positions", ",", "source_end_positions", ",", "\n", "scope", "=", "'source_gt'", ")", "\n", "target_span_gt_tensor", "=", "extract_span_tensor", "(", "bert_config", ",", "target_final_hidden", ",", "output_span_mask", ",", "\n", "start_positions", ",", "end_positions", ",", "\n", "scope", "=", "'target_gt'", ")", "\n", "\n", "return", "(", "target_start_logits", ",", "target_end_logits", ",", "source_raw_start_logits", ",", "source_raw_end_logits", ",", "target_span_gt_tensor", ",", "source_span_gt_tensor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.layers.model_fn_builder": [[469, 593], ["tensorflow.logging.info", "sorted", "layers.create_model", "tensorflow.trainable_variables", "tensorflow.logging.info", "features.keys", "tensorflow.logging.info", "modeling.get_assignment_map_from_checkpoint", "tensorflow.logging.info", "compute_loss"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.layers.create_model", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.get_assignment_map_from_checkpoint"], ["", "def", "model_fn_builder", "(", "bert_config", ",", "init_checkpoint", ",", "learning_rate", ",", "\n", "num_train_steps", ",", "num_warmup_steps", ",", "use_tpu", ",", "\n", "use_one_hot_embeddings", ")", ":", "\n", "  ", "\"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"", "\n", "\n", "def", "model_fn", "(", "features", ",", "labels", ",", "mode", ",", "params", ")", ":", "# pylint: disable=unused-argument", "\n", "    ", "\"\"\"The `model_fn` for TPUEstimator.\"\"\"", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"*** Features ***\"", ")", "\n", "for", "name", "in", "sorted", "(", "features", ".", "keys", "(", ")", ")", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"  name = %s, shape = %s\"", "%", "(", "name", ",", "features", "[", "name", "]", ".", "shape", ")", ")", "\n", "\n", "", "unique_ids", "=", "features", "[", "\"unique_ids\"", "]", "\n", "input_ids", "=", "features", "[", "\"input_ids\"", "]", "\n", "input_mask", "=", "features", "[", "\"input_mask\"", "]", "\n", "segment_ids", "=", "features", "[", "\"segment_ids\"", "]", "\n", "input_span_mask", "=", "features", "[", "\"input_span_mask\"", "]", "\n", "output_span_mask", "=", "features", "[", "\"output_span_mask\"", "]", "\n", "\n", "source_input_ids", "=", "features", "[", "\"source_input_ids\"", "]", "\n", "source_input_mask", "=", "features", "[", "\"source_input_mask\"", "]", "\n", "source_segment_ids", "=", "features", "[", "\"source_segment_ids\"", "]", "\n", "source_input_span_mask", "=", "features", "[", "\"source_input_span_mask\"", "]", "\n", "source_output_span_mask", "=", "features", "[", "\"source_output_span_mask\"", "]", "\n", "\n", "start_positions", "=", "features", "[", "\"start_positions\"", "]", "\n", "end_positions", "=", "features", "[", "\"end_positions\"", "]", "\n", "source_start_positions", "=", "features", "[", "\"source_start_positions\"", "]", "\n", "source_end_positions", "=", "features", "[", "\"source_end_positions\"", "]", "\n", "\n", "is_training", "=", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ")", "\n", "\n", "(", "start_logits", ",", "end_logits", ",", "source_raw_start_logits", ",", "source_raw_end_logits", ",", "target_span_gt_tensor", ",", "source_span_gt_tensor", ")", "=", "create_model", "(", "\n", "bert_config", "=", "bert_config", ",", "\n", "is_training", "=", "is_training", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "input_span_mask", "=", "input_span_mask", ",", "\n", "output_span_mask", "=", "output_span_mask", ",", "\n", "source_input_ids", "=", "source_input_ids", ",", "\n", "source_input_mask", "=", "source_input_mask", ",", "\n", "source_segment_ids", "=", "source_segment_ids", ",", "\n", "source_input_span_mask", "=", "source_input_span_mask", ",", "\n", "source_output_span_mask", "=", "source_output_span_mask", ",", "\n", "start_positions", "=", "start_positions", ",", "\n", "end_positions", "=", "end_positions", ",", "\n", "source_start_positions", "=", "source_start_positions", ",", "\n", "source_end_positions", "=", "source_end_positions", ",", "\n", "use_one_hot_embeddings", "=", "use_one_hot_embeddings", ")", "\n", "\n", "tvars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "\n", "initialized_variable_names", "=", "{", "}", "\n", "scaffold_fn", "=", "None", "\n", "if", "init_checkpoint", ":", "\n", "      ", "(", "assignment_map", ",", "initialized_variable_names", "\n", ")", "=", "modeling", ".", "get_assignment_map_from_checkpoint", "(", "tvars", ",", "init_checkpoint", ")", "\n", "if", "use_tpu", ":", "\n", "\n", "        ", "def", "tpu_scaffold", "(", ")", ":", "\n", "          ", "tf", ".", "train", ".", "init_from_checkpoint", "(", "init_checkpoint", ",", "assignment_map", ")", "\n", "return", "tf", ".", "train", ".", "Scaffold", "(", ")", "\n", "\n", "", "scaffold_fn", "=", "tpu_scaffold", "\n", "", "else", ":", "\n", "        ", "tf", ".", "train", ".", "init_from_checkpoint", "(", "init_checkpoint", ",", "assignment_map", ")", "\n", "\n", "# print info", "\n", "", "", "tf", ".", "logging", ".", "info", "(", "\"**** Trainable Variables ****\"", ")", "\n", "for", "var", "in", "tvars", ":", "\n", "      ", "init_string", "=", "\"\"", "\n", "if", "var", ".", "name", "in", "initialized_variable_names", ":", "\n", "        ", "init_string", "=", "\", *INIT_FROM_CKPT*\"", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"  name = %s, shape = %s%s\"", ",", "var", ".", "name", ",", "var", ".", "shape", ",", "init_string", ")", "\n", "\n", "", "output_spec", "=", "None", "\n", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "      ", "seq_length", "=", "modeling", ".", "get_shape_list", "(", "input_ids", ")", "[", "1", "]", "\n", "\n", "def", "compute_loss", "(", "logits", ",", "positions", ")", ":", "\n", "        ", "on_hot_pos", "=", "tf", ".", "one_hot", "(", "positions", ",", "depth", "=", "seq_length", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "log_probs", "=", "tf", ".", "nn", ".", "log_softmax", "(", "logits", ",", "axis", "=", "-", "1", ")", "\n", "loss", "=", "-", "tf", ".", "reduce_mean", "(", "tf", ".", "reduce_sum", "(", "on_hot_pos", "*", "log_probs", ",", "axis", "=", "-", "1", ")", ")", "\n", "return", "loss", "\n", "\n", "", "def", "cosine_similarity", "(", "tensor1", ",", "tensor2", ")", ":", "\n", "        ", "cosine_val", "=", "1", "-", "tf", ".", "losses", ".", "cosine_distance", "(", "tensor1", ",", "tensor2", ",", "axis", "=", "0", ")", "\n", "return", "cosine_val", "\n", "\n", "", "start_loss", "=", "compute_loss", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "compute_loss", "(", "end_logits", ",", "end_positions", ")", "\n", "main_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2.0", "\n", "\n", "aux_lambda", "=", "cosine_similarity", "(", "target_span_gt_tensor", ",", "source_span_gt_tensor", ")", "\n", "source_start_loss", "=", "compute_loss", "(", "source_raw_start_logits", ",", "source_start_positions", ")", "\n", "source_end_loss", "=", "compute_loss", "(", "source_raw_end_logits", ",", "source_end_positions", ")", "\n", "aux_loss", "=", "tf", ".", "maximum", "(", "0.0", ",", "aux_lambda", ")", "*", "(", "source_start_loss", "+", "source_end_loss", ")", "/", "2.0", "\n", "\n", "total_loss", "=", "main_loss", "+", "aux_loss", "\n", "\n", "train_op", "=", "optimization", ".", "create_optimizer", "(", "total_loss", ",", "learning_rate", ",", "num_train_steps", ",", "num_warmup_steps", ",", "use_tpu", ")", "\n", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "\n", "loss", "=", "total_loss", ",", "\n", "train_op", "=", "train_op", ",", "\n", "scaffold_fn", "=", "scaffold_fn", ")", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "PREDICT", ":", "\n", "      ", "start_logits", "=", "tf", ".", "nn", ".", "log_softmax", "(", "start_logits", ",", "axis", "=", "-", "1", ")", "\n", "end_logits", "=", "tf", ".", "nn", ".", "log_softmax", "(", "end_logits", ",", "axis", "=", "-", "1", ")", "\n", "predictions", "=", "{", "\n", "\"unique_ids\"", ":", "unique_ids", ",", "\n", "\"start_logits\"", ":", "start_logits", ",", "\n", "\"end_logits\"", ":", "end_logits", ",", "\n", "}", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "predictions", "=", "predictions", ",", "scaffold_fn", "=", "scaffold_fn", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Only TRAIN and PREDICT modes are supported: %s\"", "%", "(", "mode", ")", ")", "\n", "\n", "", "return", "output_spec", "\n", "\n", "", "return", "model_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.SquadExample.__init__": [[175, 198], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "qas_id", ",", "\n", "question_text", ",", "\n", "doc_tokens", ",", "\n", "source_question_text", ",", "\n", "source_doc_tokens", ",", "\n", "orig_answer_text", "=", "None", ",", "\n", "source_orig_answer_text", "=", "None", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ",", "\n", "source_start_position", "=", "None", ",", "\n", "source_end_position", "=", "None", ")", ":", "\n", "    ", "self", ".", "qas_id", "=", "qas_id", "\n", "self", ".", "question_text", "=", "question_text", "\n", "self", ".", "doc_tokens", "=", "doc_tokens", "\n", "self", ".", "source_question_text", "=", "source_question_text", "\n", "self", ".", "source_doc_tokens", "=", "source_doc_tokens", "\n", "self", ".", "orig_answer_text", "=", "orig_answer_text", "\n", "self", ".", "source_orig_answer_text", "=", "source_orig_answer_text", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "self", ".", "source_start_position", "=", "source_start_position", "\n", "self", ".", "source_end_position", "=", "source_end_position", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.SquadExample.__str__": [[200, 202], ["run_clmrc.SquadExample.__repr__"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.SquadExample.__repr__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "__repr__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.SquadExample.__repr__": [[203, 214], ["tokenization.printable_text", "tokenization.printable_text"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.printable_text", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.printable_text"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "    ", "s", "=", "\"\"", "\n", "s", "+=", "\"qas_id: %s\"", "%", "(", "tokenization", ".", "printable_text", "(", "self", ".", "qas_id", ")", ")", "\n", "s", "+=", "\", question_text: %s\"", "%", "(", "\n", "tokenization", ".", "printable_text", "(", "self", ".", "question_text", ")", ")", "\n", "s", "+=", "\", doc_tokens: [%s]\"", "%", "(", "\" \"", ".", "join", "(", "self", ".", "doc_tokens", ")", ")", "\n", "if", "self", ".", "start_position", ":", "\n", "      ", "s", "+=", "\", start_position: %d\"", "%", "(", "self", ".", "start_position", ")", "\n", "", "if", "self", ".", "start_position", ":", "\n", "      ", "s", "+=", "\", end_position: %d\"", "%", "(", "self", ".", "end_position", ")", "\n", "", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.InputFeatures.__init__": [[219, 266], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "unique_id", ",", "\n", "example_index", ",", "\n", "doc_span_index", ",", "\n", "tokens", ",", "\n", "token_to_orig_map", ",", "\n", "token_is_max_context", ",", "\n", "input_ids", ",", "\n", "input_mask", ",", "\n", "segment_ids", ",", "\n", "source_tokens", ",", "\n", "source_token_to_orig_map", ",", "\n", "source_token_is_max_context", ",", "\n", "source_input_ids", ",", "\n", "source_input_mask", ",", "\n", "source_segment_ids", ",", "\n", "input_span_mask", ",", "\n", "source_input_span_mask", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ",", "\n", "source_start_position", "=", "None", ",", "\n", "source_end_position", "=", "None", ",", "\n", "output_span_mask", "=", "None", ",", "\n", "source_output_span_mask", "=", "None", ")", ":", "\n", "    ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "example_index", "=", "example_index", "\n", "self", ".", "doc_span_index", "=", "doc_span_index", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "token_to_orig_map", "=", "token_to_orig_map", "\n", "self", ".", "token_is_max_context", "=", "token_is_max_context", "\n", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "source_tokens", "=", "source_tokens", "\n", "self", ".", "source_token_to_orig_map", "=", "source_token_to_orig_map", "\n", "self", ".", "source_token_is_max_context", "=", "source_token_is_max_context", "\n", "self", ".", "source_input_ids", "=", "source_input_ids", "\n", "self", ".", "source_input_mask", "=", "source_input_mask", "\n", "self", ".", "source_segment_ids", "=", "source_segment_ids", "\n", "self", ".", "input_span_mask", "=", "input_span_mask", "\n", "self", ".", "source_input_span_mask", "=", "source_input_span_mask", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "self", ".", "source_start_position", "=", "source_start_position", "\n", "self", ".", "source_end_position", "=", "source_end_position", "\n", "self", ".", "output_span_mask", "=", "output_span_mask", "\n", "self", ".", "source_output_span_mask", "=", "source_output_span_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.ChineseFullTokenizer.__init__": [[286, 291], ["tokenization.load_vocab", "tokenization.WordpieceTokenizer", "run_clmrc.ChineseFullTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "False", ")", ":", "\n", "    ", "self", ".", "vocab", "=", "tokenization", ".", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "inv_vocab", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "vocab", ".", "items", "(", ")", "}", "\n", "self", ".", "wordpiece_tokenizer", "=", "tokenization", ".", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.ChineseFullTokenizer.tokenize": [[292, 299], ["run_clmrc.customize_tokenizer", "run_clmrc.ChineseFullTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.customize_tokenizer", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.ChineseFullTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "customize_tokenizer", "(", "text", ",", "do_lower_case", "=", "self", ".", "do_lower_case", ")", ":", "\n", "      ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "        ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.ChineseFullTokenizer.convert_tokens_to_ids": [[300, 302], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.convert_by_vocab"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "    ", "return", "tokenization", ".", "convert_by_vocab", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.ChineseFullTokenizer.convert_ids_to_tokens": [[303, 305], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.convert_by_vocab"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "    ", "return", "tokenization", ".", "convert_by_vocab", "(", "self", ".", "inv_vocab", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.FeatureWriter.__init__": [[1269, 1274], ["tensorflow.python_io.TFRecordWriter"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "filename", ",", "is_training", ")", ":", "\n", "    ", "self", ".", "filename", "=", "filename", "\n", "self", ".", "is_training", "=", "is_training", "\n", "self", ".", "num_features", "=", "0", "\n", "self", ".", "_writer", "=", "tf", ".", "python_io", ".", "TFRecordWriter", "(", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.FeatureWriter.process_feature": [[1275, 1305], ["collections.OrderedDict", "run_clmrc.FeatureWriter.process_feature.create_int_feature"], "methods", ["None"], ["", "def", "process_feature", "(", "self", ",", "feature", ")", ":", "\n", "    ", "\"\"\"Write a InputFeature to the TFRecordWriter as a tf.train.Example.\"\"\"", "\n", "self", ".", "num_features", "+=", "1", "\n", "\n", "def", "create_int_feature", "(", "values", ")", ":", "\n", "      ", "feature", "=", "tf", ".", "train", ".", "Feature", "(", "\n", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "list", "(", "values", ")", ")", ")", "\n", "return", "feature", "\n", "\n", "", "features", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "features", "[", "\"unique_ids\"", "]", "=", "create_int_feature", "(", "[", "feature", ".", "unique_id", "]", ")", "\n", "features", "[", "\"input_ids\"", "]", "=", "create_int_feature", "(", "feature", ".", "input_ids", ")", "\n", "features", "[", "\"input_mask\"", "]", "=", "create_int_feature", "(", "feature", ".", "input_mask", ")", "\n", "features", "[", "\"segment_ids\"", "]", "=", "create_int_feature", "(", "feature", ".", "segment_ids", ")", "\n", "features", "[", "\"input_span_mask\"", "]", "=", "create_int_feature", "(", "feature", ".", "input_span_mask", ")", "\n", "features", "[", "\"source_input_span_mask\"", "]", "=", "create_int_feature", "(", "feature", ".", "source_input_span_mask", ")", "\n", "features", "[", "\"source_input_ids\"", "]", "=", "create_int_feature", "(", "feature", ".", "source_input_ids", ")", "\n", "features", "[", "\"source_input_mask\"", "]", "=", "create_int_feature", "(", "feature", ".", "source_input_mask", ")", "\n", "features", "[", "\"source_segment_ids\"", "]", "=", "create_int_feature", "(", "feature", ".", "source_segment_ids", ")", "\n", "\n", "#if self.is_training:", "\n", "features", "[", "\"start_positions\"", "]", "=", "create_int_feature", "(", "[", "feature", ".", "start_position", "]", ")", "\n", "features", "[", "\"end_positions\"", "]", "=", "create_int_feature", "(", "[", "feature", ".", "end_position", "]", ")", "\n", "features", "[", "\"source_start_positions\"", "]", "=", "create_int_feature", "(", "[", "feature", ".", "source_start_position", "]", ")", "\n", "features", "[", "\"source_end_positions\"", "]", "=", "create_int_feature", "(", "[", "feature", ".", "source_end_position", "]", ")", "\n", "features", "[", "\"output_span_mask\"", "]", "=", "create_int_feature", "(", "feature", ".", "output_span_mask", ")", "\n", "features", "[", "\"source_output_span_mask\"", "]", "=", "create_int_feature", "(", "feature", ".", "source_output_span_mask", ")", "\n", "\n", "tf_example", "=", "tf", ".", "train", ".", "Example", "(", "features", "=", "tf", ".", "train", ".", "Features", "(", "feature", "=", "features", ")", ")", "\n", "self", ".", "_writer", ".", "write", "(", "tf_example", ".", "SerializeToString", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.FeatureWriter.close": [[1306, 1308], ["run_clmrc.FeatureWriter._writer.close"], "methods", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.FeatureWriter.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "    ", "self", ".", "_writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.customize_tokenizer": [[269, 281], ["tokenization.BasicTokenizer", "tokenization.convert_to_unicode", "temp_x.lower.split", "temp_x.lower.lower", "tokenization.BasicTokenizer._is_chinese_char", "tokenization._is_punctuation", "tokenization._is_whitespace", "tokenization._is_control", "ord"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.BasicTokenizer._is_chinese_char", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization._is_punctuation", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization._is_whitespace", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization._is_control"], ["", "", "def", "customize_tokenizer", "(", "text", ",", "do_lower_case", "=", "False", ")", ":", "\n", "  ", "tokenizer", "=", "tokenization", ".", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "temp_x", "=", "\"\"", "\n", "text", "=", "tokenization", ".", "convert_to_unicode", "(", "text", ")", "\n", "for", "c", "in", "text", ":", "\n", "    ", "if", "tokenizer", ".", "_is_chinese_char", "(", "ord", "(", "c", ")", ")", "or", "tokenization", ".", "_is_punctuation", "(", "c", ")", "or", "tokenization", ".", "_is_whitespace", "(", "c", ")", "or", "tokenization", ".", "_is_control", "(", "c", ")", ":", "\n", "      ", "temp_x", "+=", "\" \"", "+", "c", "+", "\" \"", "\n", "", "else", ":", "\n", "      ", "temp_x", "+=", "c", "\n", "", "", "if", "do_lower_case", ":", "\n", "    ", "temp_x", "=", "temp_x", ".", "lower", "(", ")", "\n", "", "return", "temp_x", ".", "split", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.read_squad_examples": [[308, 448], ["tensorflow.logging.info", "tensorflow.gfile.Open", "json.load", "run_clmrc.customize_tokenizer", "tokenization._is_whitespace", "len", "run_clmrc.read_squad_examples.is_whitespace"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.customize_tokenizer", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization._is_whitespace"], ["", "", "def", "read_squad_examples", "(", "input_file", ",", "is_training", ")", ":", "\n", "  ", "\"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"", "\n", "is_training", "=", "True", "\n", "with", "tf", ".", "gfile", ".", "Open", "(", "input_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "    ", "input_data", "=", "json", ".", "load", "(", "reader", ")", "[", "\"data\"", "]", "\n", "\n", "#", "\n", "", "examples", "=", "[", "]", "\n", "for", "entry", "in", "input_data", ":", "\n", "    ", "for", "paragraph", "in", "entry", "[", "\"paragraphs\"", "]", ":", "\n", "      ", "paragraph_text", "=", "paragraph", "[", "\"context\"", "]", "\n", "raw_doc_tokens", "=", "customize_tokenizer", "(", "paragraph_text", ",", "do_lower_case", "=", "FLAGS", ".", "do_lower_case", ")", "\n", "doc_tokens", "=", "[", "]", "\n", "char_to_word_offset", "=", "[", "]", "\n", "prev_is_whitespace", "=", "True", "\n", "\n", "k", "=", "0", "\n", "temp_word", "=", "\"\"", "\n", "for", "c", "in", "paragraph_text", ":", "\n", "        ", "if", "tokenization", ".", "_is_whitespace", "(", "c", ")", ":", "\n", "          ", "char_to_word_offset", ".", "append", "(", "k", "-", "1", ")", "\n", "continue", "\n", "", "else", ":", "\n", "          ", "temp_word", "+=", "c", "\n", "char_to_word_offset", ".", "append", "(", "k", ")", "\n", "\n", "", "if", "FLAGS", ".", "do_lower_case", ":", "\n", "          ", "temp_word", "=", "temp_word", ".", "lower", "(", ")", "\n", "\n", "", "if", "temp_word", "==", "raw_doc_tokens", "[", "k", "]", ":", "\n", "          ", "doc_tokens", ".", "append", "(", "temp_word", ")", "\n", "temp_word", "=", "\"\"", "\n", "k", "+=", "1", "\n", "\n", "", "", "assert", "k", "==", "len", "(", "raw_doc_tokens", ")", "\n", "\n", "# process pivot example", "\n", "def", "is_whitespace", "(", "c", ")", ":", "\n", "        ", "if", "c", "==", "\" \"", "or", "c", "==", "\"\\t\"", "or", "c", "==", "\"\\r\"", "or", "c", "==", "\"\\n\"", "or", "ord", "(", "c", ")", "==", "0x202F", ":", "\n", "          ", "return", "True", "\n", "", "return", "False", "\n", "", "source_paragraph_text", "=", "paragraph", "[", "\"trans_context\"", "]", "\n", "source_doc_tokens", "=", "[", "]", "\n", "source_char_to_word_offset", "=", "[", "]", "\n", "source_prev_is_whitespace", "=", "True", "\n", "for", "c", "in", "source_paragraph_text", ":", "\n", "        ", "if", "is_whitespace", "(", "c", ")", ":", "\n", "          ", "source_prev_is_whitespace", "=", "True", "\n", "", "else", ":", "\n", "          ", "if", "source_prev_is_whitespace", ":", "\n", "            ", "source_doc_tokens", ".", "append", "(", "c", ")", "\n", "", "else", ":", "\n", "            ", "source_doc_tokens", "[", "-", "1", "]", "+=", "c", "\n", "", "source_prev_is_whitespace", "=", "False", "\n", "", "source_char_to_word_offset", ".", "append", "(", "len", "(", "source_doc_tokens", ")", "-", "1", ")", "\n", "\n", "\n", "", "for", "qa", "in", "paragraph", "[", "\"qas\"", "]", ":", "\n", "        ", "qas_id", "=", "qa", "[", "\"id\"", "]", "\n", "question_text", "=", "qa", "[", "\"question\"", "]", "\n", "source_question_text", "=", "qa", "[", "\"trans_question\"", "]", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "orig_answer_text", "=", "None", "\n", "\n", "source_start_position", "=", "None", "\n", "source_end_position", "=", "None", "\n", "source_orig_answer_text", "=", "None", "\n", "\n", "if", "is_training", ":", "\n", "          ", "answer", "=", "qa", "[", "\"answers\"", "]", "[", "0", "]", "\n", "orig_answer_text", "=", "answer", "[", "\"text\"", "]", "\n", "\n", "if", "orig_answer_text", "not", "in", "paragraph_text", ":", "\n", "            ", "tf", ".", "logging", ".", "warning", "(", "\"Could not find answer\"", ")", "\n", "start_position", "=", "-", "1", "\n", "end_position", "=", "-", "1", "\n", "orig_answer_text", "=", "\"\"", "\n", "", "else", ":", "\n", "            ", "answer_offset", "=", "paragraph_text", ".", "index", "(", "orig_answer_text", ")", "\n", "answer_length", "=", "len", "(", "orig_answer_text", ")", "\n", "start_position", "=", "char_to_word_offset", "[", "answer_offset", "]", "\n", "end_position", "=", "char_to_word_offset", "[", "answer_offset", "+", "answer_length", "-", "1", "]", "\n", "\n", "# Only add answers where the text can be exactly recovered from the", "\n", "# document. If this CAN'T happen it's likely due to weird Unicode", "\n", "# stuff so we will just skip the example.", "\n", "#", "\n", "# Note that this means for training mode, every example is NOT", "\n", "# guaranteed to be preserved.", "\n", "actual_text", "=", "\"\"", ".", "join", "(", "\n", "doc_tokens", "[", "start_position", ":", "(", "end_position", "+", "1", ")", "]", ")", "\n", "cleaned_answer_text", "=", "\"\"", ".", "join", "(", "\n", "tokenization", ".", "whitespace_tokenize", "(", "orig_answer_text", ")", ")", "\n", "if", "actual_text", ".", "find", "(", "cleaned_answer_text", ")", "==", "-", "1", ":", "\n", "              ", "pdb", ".", "set_trace", "(", ")", "\n", "tf", ".", "logging", ".", "warning", "(", "\"Could not find answer: '%s' vs. '%s'\"", ",", "actual_text", ",", "cleaned_answer_text", ")", "\n", "continue", "\n", "\n", "#", "\n", "", "", "source_answer", "=", "qa", "[", "\"answers\"", "]", "[", "0", "]", "\n", "source_orig_answer_text", "=", "source_answer", "[", "\"trans_aligned_text\"", "]", "\n", "\n", "if", "source_orig_answer_text", "not", "in", "source_paragraph_text", ":", "\n", "            ", "tf", ".", "logging", ".", "warning", "(", "\"Could not find pivot answer: %s\"", ",", "source_orig_answer_text", ")", "\n", "source_start_position", "=", "-", "1", "\n", "source_end_position", "=", "-", "1", "\n", "", "else", ":", "\n", "            ", "source_answer_offset", "=", "source_paragraph_text", ".", "index", "(", "source_orig_answer_text", ")", "\n", "source_answer_length", "=", "len", "(", "source_orig_answer_text", ")", "\n", "source_start_position", "=", "source_char_to_word_offset", "[", "source_answer_offset", "]", "\n", "source_end_position", "=", "source_char_to_word_offset", "[", "source_answer_offset", "+", "source_answer_length", "-", "1", "]", "\n", "\n", "#", "\n", "source_actual_text", "=", "\"\"", ".", "join", "(", "\n", "source_doc_tokens", "[", "source_start_position", ":", "(", "source_end_position", "+", "1", ")", "]", ")", "\n", "source_cleaned_answer_text", "=", "\"\"", ".", "join", "(", "\n", "tokenization", ".", "whitespace_tokenize", "(", "source_orig_answer_text", ")", ")", "\n", "if", "source_actual_text", ".", "find", "(", "source_cleaned_answer_text", ")", "==", "-", "1", ":", "\n", "              ", "pdb", ".", "set_trace", "(", ")", "\n", "tf", ".", "logging", ".", "warning", "(", "\"Could not find pivot answer: '%s' vs. '%s'\"", ",", "source_actual_text", ",", "source_cleaned_answer_text", ")", "\n", "continue", "\n", "\n", "", "", "", "example", "=", "SquadExample", "(", "\n", "qas_id", "=", "qas_id", ",", "\n", "question_text", "=", "question_text", ",", "\n", "doc_tokens", "=", "doc_tokens", ",", "\n", "orig_answer_text", "=", "orig_answer_text", ",", "\n", "start_position", "=", "start_position", ",", "\n", "end_position", "=", "end_position", ",", "\n", "source_question_text", "=", "source_question_text", ",", "\n", "source_doc_tokens", "=", "source_doc_tokens", ",", "\n", "source_orig_answer_text", "=", "source_orig_answer_text", ",", "\n", "source_start_position", "=", "source_start_position", ",", "\n", "source_end_position", "=", "source_end_position", ")", "\n", "examples", ".", "append", "(", "example", ")", "\n", "\n", "", "", "", "tf", ".", "logging", ".", "info", "(", "\"**********read_squad_examples complete!**********\"", ")", "\n", "\n", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.convert_source_examples_to_features": [[451, 605], ["tokenizer.tokenize", "enumerate", "collections.namedtuple", "enumerate", "len", "orig_to_tok_index.append", "tokenizer.tokenize", "len", "doc_spans.append", "min", "tokens.append", "segment_ids.append", "input_span_mask.append", "tokens.append", "segment_ids.append", "input_span_mask.append", "range", "tokens.append", "segment_ids.append", "input_span_mask.append", "tokenizer.convert_tokens_to_ids", "INPUT_IDS.append", "INPUT_MASK.append", "SEGMENT_IDS.append", "INPUT_SPAN_MASK.append", "OUTPUT_SPAN_MASK.append", "START_POSITION.append", "END_POSITION.append", "TOKENS.append", "TOKEN_TO_ORIG_MAP.append", "TOKEN_IS_MAX_CONTEXT.append", "len", "tok_to_orig_index.append", "all_doc_tokens.append", "run_clmrc._improve_answer_span", "len", "len", "collections.namedtuple.", "len", "tokens.append", "segment_ids.append", "input_span_mask.append", "run_clmrc._check_is_max_context", "tokens.append", "segment_ids.append", "input_span_mask.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "input_span_mask.append", "len", "len", "len", "len", "range", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.ChineseFullTokenizer.tokenize", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.ChineseFullTokenizer.tokenize", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.ChineseFullTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc._improve_answer_span", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc._check_is_max_context"], ["", "def", "convert_source_examples_to_features", "(", "tokenizer", ",", "example", ",", "is_training", ")", ":", "\n", "  ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "is_training", "=", "True", "\n", "query_tokens", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "source_question_text", ")", "\n", "\n", "if", "len", "(", "query_tokens", ")", ">", "FLAGS", ".", "max_query_length", ":", "\n", "    ", "query_tokens", "=", "query_tokens", "[", "0", ":", "FLAGS", ".", "max_query_length", "]", "\n", "\n", "", "tok_to_orig_index", "=", "[", "]", "\n", "orig_to_tok_index", "=", "[", "]", "\n", "all_doc_tokens", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "example", ".", "source_doc_tokens", ")", ":", "\n", "    ", "orig_to_tok_index", ".", "append", "(", "len", "(", "all_doc_tokens", ")", ")", "\n", "sub_tokens", "=", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "for", "sub_token", "in", "sub_tokens", ":", "\n", "      ", "tok_to_orig_index", ".", "append", "(", "i", ")", "\n", "all_doc_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "if", "is_training", ":", "\n", "    ", "if", "example", ".", "source_start_position", "==", "-", "1", "or", "example", ".", "source_end_position", "==", "-", "1", ":", "\n", "      ", "tok_start_position", "=", "-", "1", "\n", "tok_end_position", "=", "-", "1", "\n", "", "else", ":", "\n", "      ", "tok_start_position", "=", "orig_to_tok_index", "[", "example", ".", "source_start_position", "]", "\n", "if", "example", ".", "source_end_position", "<", "len", "(", "example", ".", "source_doc_tokens", ")", "-", "1", ":", "\n", "        ", "tok_end_position", "=", "orig_to_tok_index", "[", "example", ".", "source_end_position", "+", "1", "]", "-", "1", "\n", "", "else", ":", "\n", "        ", "tok_end_position", "=", "len", "(", "all_doc_tokens", ")", "-", "1", "\n", "", "(", "tok_start_position", ",", "tok_end_position", ")", "=", "_improve_answer_span", "(", "\n", "all_doc_tokens", ",", "tok_start_position", ",", "tok_end_position", ",", "tokenizer", ",", "\n", "example", ".", "source_orig_answer_text", ")", "\n", "\n", "# The -3 accounts for [CLS], [SEP] and [SEP]", "\n", "", "", "max_tokens_for_doc", "=", "FLAGS", ".", "max_seq_length", "-", "len", "(", "query_tokens", ")", "-", "3", "\n", "\n", "# We can have documents that are longer than the maximum sequence length.", "\n", "# To deal with this we do a sliding window approach, where we take chunks", "\n", "# of the up to our max length with a stride of `doc_stride`.", "\n", "_DocSpan", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"DocSpan\"", ",", "[", "\"start\"", ",", "\"length\"", "]", ")", "\n", "doc_spans", "=", "[", "]", "\n", "start_offset", "=", "0", "\n", "while", "start_offset", "<", "len", "(", "all_doc_tokens", ")", ":", "\n", "    ", "length", "=", "len", "(", "all_doc_tokens", ")", "-", "start_offset", "\n", "if", "length", ">", "max_tokens_for_doc", ":", "\n", "      ", "length", "=", "max_tokens_for_doc", "\n", "", "doc_spans", ".", "append", "(", "_DocSpan", "(", "start", "=", "start_offset", ",", "length", "=", "length", ")", ")", "\n", "if", "start_offset", "+", "length", "==", "len", "(", "all_doc_tokens", ")", ":", "\n", "      ", "break", "\n", "", "start_offset", "+=", "min", "(", "length", ",", "FLAGS", ".", "doc_stride", ")", "\n", "\n", "#", "\n", "", "INPUT_IDS", "=", "[", "]", "\n", "INPUT_MASK", "=", "[", "]", "\n", "SEGMENT_IDS", "=", "[", "]", "\n", "INPUT_SPAN_MASK", "=", "[", "]", "\n", "OUTPUT_SPAN_MASK", "=", "[", "]", "\n", "START_POSITION", "=", "[", "]", "\n", "END_POSITION", "=", "[", "]", "\n", "TOKENS", "=", "[", "]", "\n", "TOKEN_TO_ORIG_MAP", "=", "[", "]", "\n", "TOKEN_IS_MAX_CONTEXT", "=", "[", "]", "\n", "\n", "for", "(", "doc_span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "    ", "tokens", "=", "[", "]", "\n", "token_to_orig_map", "=", "{", "}", "\n", "token_is_max_context", "=", "{", "}", "\n", "segment_ids", "=", "[", "]", "\n", "input_span_mask", "=", "[", "]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "input_span_mask", ".", "append", "(", "1", ")", "\n", "for", "token", "in", "query_tokens", ":", "\n", "      ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "input_span_mask", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "input_span_mask", ".", "append", "(", "0", ")", "\n", "\n", "for", "i", "in", "range", "(", "doc_span", ".", "length", ")", ":", "\n", "      ", "split_token_index", "=", "doc_span", ".", "start", "+", "i", "\n", "token_to_orig_map", "[", "len", "(", "tokens", ")", "]", "=", "tok_to_orig_index", "[", "split_token_index", "]", "\n", "\n", "is_max_context", "=", "_check_is_max_context", "(", "doc_spans", ",", "doc_span_index", ",", "\n", "split_token_index", ")", "\n", "token_is_max_context", "[", "len", "(", "tokens", ")", "]", "=", "is_max_context", "\n", "tokens", ".", "append", "(", "all_doc_tokens", "[", "split_token_index", "]", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "input_span_mask", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "input_span_mask", ".", "append", "(", "0", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "FLAGS", ".", "max_seq_length", ":", "\n", "      ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "input_span_mask", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "FLAGS", ".", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "FLAGS", ".", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "FLAGS", ".", "max_seq_length", "\n", "assert", "len", "(", "input_span_mask", ")", "==", "FLAGS", ".", "max_seq_length", "\n", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "output_span_mask", "=", "None", "\n", "if", "is_training", ":", "\n", "# For training, if our document chunk does not contain an annotation", "\n", "# we throw it out, since there is nothing to predict.", "\n", "      ", "doc_start", "=", "doc_span", ".", "start", "\n", "doc_end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "out_of_span", "=", "False", "\n", "if", "not", "(", "tok_start_position", ">=", "doc_start", "and", "\n", "tok_end_position", "<=", "doc_end", ")", ":", "\n", "        ", "out_of_span", "=", "True", "\n", "", "if", "out_of_span", ":", "\n", "        ", "start_position", "=", "0", "\n", "end_position", "=", "0", "\n", "", "else", ":", "\n", "        ", "doc_offset", "=", "len", "(", "query_tokens", ")", "+", "2", "\n", "start_position", "=", "tok_start_position", "-", "doc_start", "+", "doc_offset", "\n", "end_position", "=", "tok_end_position", "-", "doc_start", "+", "doc_offset", "\n", "\n", "", "output_span_mask", "=", "[", "0", "]", "*", "FLAGS", ".", "max_seq_length", "\n", "for", "osm_idx", "in", "range", "(", "start_position", ",", "end_position", "+", "1", ")", ":", "\n", "        ", "output_span_mask", "[", "osm_idx", "]", "=", "1", "\n", "\n", "\n", "", "", "INPUT_IDS", ".", "append", "(", "input_ids", ")", "\n", "INPUT_MASK", ".", "append", "(", "input_mask", ")", "\n", "SEGMENT_IDS", ".", "append", "(", "segment_ids", ")", "\n", "INPUT_SPAN_MASK", ".", "append", "(", "input_span_mask", ")", "\n", "OUTPUT_SPAN_MASK", ".", "append", "(", "output_span_mask", ")", "\n", "START_POSITION", ".", "append", "(", "start_position", ")", "\n", "END_POSITION", ".", "append", "(", "end_position", ")", "\n", "TOKENS", ".", "append", "(", "tokens", ")", "\n", "TOKEN_TO_ORIG_MAP", ".", "append", "(", "token_to_orig_map", ")", "\n", "TOKEN_IS_MAX_CONTEXT", ".", "append", "(", "token_is_max_context", ")", "\n", "\n", "", "ret_array", "=", "(", "INPUT_IDS", ",", "INPUT_MASK", ",", "SEGMENT_IDS", ",", "\n", "INPUT_SPAN_MASK", ",", "OUTPUT_SPAN_MASK", ",", "\n", "START_POSITION", ",", "END_POSITION", ",", "\n", "TOKENS", ",", "TOKEN_TO_ORIG_MAP", ",", "TOKEN_IS_MAX_CONTEXT", ")", "\n", "\n", "return", "ret_array", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.convert_token_to_ids": [[608, 616], ["output.append", "output.append"], "function", ["None"], ["", "def", "convert_token_to_ids", "(", "vocab", ",", "items", ")", ":", "\n", "  ", "output", "=", "[", "]", "\n", "for", "item", "in", "items", ":", "\n", "    ", "if", "item", "in", "vocab", ":", "\n", "      ", "output", ".", "append", "(", "vocab", "[", "item", "]", ")", "\n", "", "else", ":", "\n", "      ", "output", ".", "append", "(", "vocab", "[", "'[UNK]'", "]", ")", "\n", "", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.convert_examples_to_features": [[618, 847], ["run_clmrc.ChineseFullTokenizer", "enumerate", "run_clmrc.ChineseFullTokenizer.tokenize", "enumerate", "collections.namedtuple", "run_clmrc.convert_source_examples_to_features", "len", "enumerate", "len", "orig_to_tok_index.append", "run_clmrc.ChineseFullTokenizer.tokenize", "run_clmrc._improve_answer_span", "len", "doc_spans.append", "min", "tokens.append", "segment_ids.append", "input_span_mask.append", "tokens.append", "segment_ids.append", "input_span_mask.append", "range", "tokens.append", "segment_ids.append", "input_span_mask.append", "run_clmrc.ChineseFullTokenizer.convert_tokens_to_ids", "run_clmrc.InputFeatures", "output_fn", "len", "tok_to_orig_index.append", "all_doc_tokens.append", "len", "len", "collections.namedtuple.", "len", "tokens.append", "segment_ids.append", "input_span_mask.append", "run_clmrc._check_is_max_context", "tokens.append", "segment_ids.append", "input_span_mask.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "input_span_mask.append", "len", "len", "len", "len", "range", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "len", "len", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "len", "len", "len", "tokenization.printable_text", "tokenization.printable_text", "tokenization.printable_text", "str", "str", "str", "str", "str", "str", "str", "six.iteritems", "six.iteritems", "str"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.ChineseFullTokenizer.tokenize", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.convert_source_examples_to_features", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.ChineseFullTokenizer.tokenize", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc._improve_answer_span", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.ChineseFullTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc._check_is_max_context", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.printable_text", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.printable_text", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.printable_text"], ["", "def", "convert_examples_to_features", "(", "examples", ",", "tokenizer", ",", "max_seq_length", ",", "\n", "doc_stride", ",", "max_query_length", ",", "is_training", ",", "\n", "output_fn", ")", ":", "\n", "  ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "is_training", "=", "True", "\n", "unique_id", "=", "1000000000", "\n", "tokenizer", "=", "ChineseFullTokenizer", "(", "vocab_file", "=", "FLAGS", ".", "vocab_file", ",", "do_lower_case", "=", "FLAGS", ".", "do_lower_case", ")", "\n", "#source_tokenizer = tokenization.FullTokenizer(vocab_file=FLAGS.source_vocab_file, do_lower_case=FLAGS.source_do_lower_case)", "\n", "\n", "for", "(", "example_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "    ", "query_tokens", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "question_text", ")", "\n", "\n", "if", "len", "(", "query_tokens", ")", ">", "max_query_length", ":", "\n", "      ", "query_tokens", "=", "query_tokens", "[", "0", ":", "max_query_length", "]", "\n", "\n", "", "tok_to_orig_index", "=", "[", "]", "\n", "orig_to_tok_index", "=", "[", "]", "\n", "all_doc_tokens", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "example", ".", "doc_tokens", ")", ":", "\n", "      ", "orig_to_tok_index", ".", "append", "(", "len", "(", "all_doc_tokens", ")", ")", "\n", "sub_tokens", "=", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "for", "sub_token", "in", "sub_tokens", ":", "\n", "        ", "tok_to_orig_index", ".", "append", "(", "i", ")", "\n", "all_doc_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "tok_start_position", "=", "None", "\n", "tok_end_position", "=", "None", "\n", "if", "is_training", ":", "\n", "      ", "tok_start_position", "=", "orig_to_tok_index", "[", "example", ".", "start_position", "]", "\n", "if", "example", ".", "end_position", "<", "len", "(", "example", ".", "doc_tokens", ")", "-", "1", ":", "\n", "        ", "tok_end_position", "=", "orig_to_tok_index", "[", "example", ".", "end_position", "+", "1", "]", "-", "1", "\n", "", "else", ":", "\n", "        ", "tok_end_position", "=", "len", "(", "all_doc_tokens", ")", "-", "1", "\n", "", "(", "tok_start_position", ",", "tok_end_position", ")", "=", "_improve_answer_span", "(", "\n", "all_doc_tokens", ",", "tok_start_position", ",", "tok_end_position", ",", "tokenizer", ",", "\n", "example", ".", "orig_answer_text", ")", "\n", "\n", "# The -3 accounts for [CLS], [SEP] and [SEP]", "\n", "", "max_tokens_for_doc", "=", "max_seq_length", "-", "len", "(", "query_tokens", ")", "-", "3", "\n", "\n", "# We can have documents that are longer than the maximum sequence length.", "\n", "# To deal with this we do a sliding window approach, where we take chunks", "\n", "# of the up to our max length with a stride of `doc_stride`.", "\n", "_DocSpan", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"DocSpan\"", ",", "[", "\"start\"", ",", "\"length\"", "]", ")", "\n", "doc_spans", "=", "[", "]", "\n", "start_offset", "=", "0", "\n", "while", "start_offset", "<", "len", "(", "all_doc_tokens", ")", ":", "\n", "      ", "length", "=", "len", "(", "all_doc_tokens", ")", "-", "start_offset", "\n", "if", "length", ">", "max_tokens_for_doc", ":", "\n", "        ", "length", "=", "max_tokens_for_doc", "\n", "", "doc_spans", ".", "append", "(", "_DocSpan", "(", "start", "=", "start_offset", ",", "length", "=", "length", ")", ")", "\n", "if", "start_offset", "+", "length", "==", "len", "(", "all_doc_tokens", ")", ":", "\n", "        ", "break", "\n", "", "start_offset", "+=", "min", "(", "length", ",", "doc_stride", ")", "\n", "\n", "# here we use a approximated span for pivot language", "\n", "", "(", "source_INPUT_IDS", ",", "source_INPUT_MASK", ",", "source_SEGMENT_IDS", ",", "source_INPUT_SPAN_MASK", ",", "source_OUTPUT_SPAN_MASK", ",", "source_START_POSITION", ",", "source_END_POSITION", ",", "source_TOKENS", ",", "source_TOKEN_TO_ORIG_MAP", ",", "source_TOKEN_IS_MAX_CONTEXT", ")", "=", "convert_source_examples_to_features", "(", "tokenizer", ",", "example", ",", "is_training", ")", "\n", "source_DOC_SPAN_LEN", "=", "len", "(", "source_INPUT_IDS", ")", "\n", "\n", "# process target sample", "\n", "for", "(", "doc_span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "      ", "tokens", "=", "[", "]", "\n", "token_to_orig_map", "=", "{", "}", "\n", "token_is_max_context", "=", "{", "}", "\n", "segment_ids", "=", "[", "]", "\n", "input_span_mask", "=", "[", "]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "input_span_mask", ".", "append", "(", "1", ")", "\n", "for", "token", "in", "query_tokens", ":", "\n", "        ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "input_span_mask", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "input_span_mask", ".", "append", "(", "0", ")", "\n", "\n", "for", "i", "in", "range", "(", "doc_span", ".", "length", ")", ":", "\n", "        ", "split_token_index", "=", "doc_span", ".", "start", "+", "i", "\n", "token_to_orig_map", "[", "len", "(", "tokens", ")", "]", "=", "tok_to_orig_index", "[", "split_token_index", "]", "\n", "\n", "is_max_context", "=", "_check_is_max_context", "(", "doc_spans", ",", "doc_span_index", ",", "\n", "split_token_index", ")", "\n", "token_is_max_context", "[", "len", "(", "tokens", ")", "]", "=", "is_max_context", "\n", "tokens", ".", "append", "(", "all_doc_tokens", "[", "split_token_index", "]", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "input_span_mask", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "input_span_mask", ".", "append", "(", "0", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "        ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "input_span_mask", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_span_mask", ")", "==", "max_seq_length", "\n", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "output_span_mask", "=", "None", "\n", "if", "is_training", ":", "\n", "# For training, if our document chunk does not contain an annotation", "\n", "# we throw it out, since there is nothing to predict.", "\n", "        ", "doc_start", "=", "doc_span", ".", "start", "\n", "doc_end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "out_of_span", "=", "False", "\n", "if", "not", "(", "tok_start_position", ">=", "doc_start", "and", "\n", "tok_end_position", "<=", "doc_end", ")", ":", "\n", "          ", "out_of_span", "=", "True", "\n", "", "if", "out_of_span", ":", "\n", "          ", "start_position", "=", "0", "\n", "end_position", "=", "0", "\n", "", "else", ":", "\n", "          ", "doc_offset", "=", "len", "(", "query_tokens", ")", "+", "2", "\n", "start_position", "=", "tok_start_position", "-", "doc_start", "+", "doc_offset", "\n", "end_position", "=", "tok_end_position", "-", "doc_start", "+", "doc_offset", "\n", "\n", "", "output_span_mask", "=", "[", "0", "]", "*", "max_seq_length", "\n", "for", "osm_idx", "in", "range", "(", "start_position", ",", "end_position", "+", "1", ")", ":", "\n", "          ", "output_span_mask", "[", "osm_idx", "]", "=", "1", "\n", "\n", "# as target/source doc_span may differ, we use heuristic here to synchronize them", "\n", "", "", "if", "doc_span_index", ">=", "source_DOC_SPAN_LEN", ":", "\n", "        ", "source_tokens", "=", "source_TOKENS", "[", "-", "1", "]", "\n", "source_token_to_orig_map", "=", "source_TOKEN_TO_ORIG_MAP", "[", "-", "1", "]", "\n", "source_token_is_max_context", "=", "source_TOKEN_IS_MAX_CONTEXT", "[", "-", "1", "]", "\n", "source_input_ids", "=", "source_INPUT_IDS", "[", "-", "1", "]", "\n", "source_input_mask", "=", "source_INPUT_MASK", "[", "-", "1", "]", "\n", "source_segment_ids", "=", "source_SEGMENT_IDS", "[", "-", "1", "]", "\n", "source_input_span_mask", "=", "source_INPUT_SPAN_MASK", "[", "-", "1", "]", "\n", "source_output_span_mask", "=", "source_OUTPUT_SPAN_MASK", "[", "-", "1", "]", "\n", "source_start_position", "=", "source_START_POSITION", "[", "-", "1", "]", "\n", "source_end_position", "=", "source_END_POSITION", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "        ", "source_tokens", "=", "source_TOKENS", "[", "doc_span_index", "]", "\n", "source_token_to_orig_map", "=", "source_TOKEN_TO_ORIG_MAP", "[", "doc_span_index", "]", "\n", "source_token_is_max_context", "=", "source_TOKEN_IS_MAX_CONTEXT", "[", "doc_span_index", "]", "\n", "source_input_ids", "=", "source_INPUT_IDS", "[", "doc_span_index", "]", "\n", "source_input_mask", "=", "source_INPUT_MASK", "[", "doc_span_index", "]", "\n", "source_segment_ids", "=", "source_SEGMENT_IDS", "[", "doc_span_index", "]", "\n", "source_input_span_mask", "=", "source_INPUT_SPAN_MASK", "[", "doc_span_index", "]", "\n", "source_output_span_mask", "=", "source_OUTPUT_SPAN_MASK", "[", "doc_span_index", "]", "\n", "source_start_position", "=", "source_START_POSITION", "[", "doc_span_index", "]", "\n", "source_end_position", "=", "source_END_POSITION", "[", "doc_span_index", "]", "\n", "\n", "\n", "", "if", "example_index", "<", "3", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"unique_id: %s\"", "%", "(", "unique_id", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"example_index: %s\"", "%", "(", "example_index", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"doc_span_index: %s\"", "%", "(", "doc_span_index", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "tokenization", ".", "printable_text", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"token_to_orig_map: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "\"%d:%d\"", "%", "(", "x", ",", "y", ")", "for", "(", "x", ",", "y", ")", "in", "six", ".", "iteritems", "(", "token_to_orig_map", ")", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"token_is_max_context: %s\"", "%", "\" \"", ".", "join", "(", "[", "\n", "\"%d:%s\"", "%", "(", "x", ",", "y", ")", "for", "(", "x", ",", "y", ")", "in", "six", ".", "iteritems", "(", "token_is_max_context", ")", "\n", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "\"input_span_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_span_mask", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"source_input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "source_input_ids", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "\"source_input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "source_input_mask", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "\"source_segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "source_segment_ids", "]", ")", ")", "\n", "if", "is_training", ":", "\n", "          ", "answer_text", "=", "\" \"", ".", "join", "(", "tokens", "[", "start_position", ":", "(", "end_position", "+", "1", ")", "]", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"start_position: %d\"", "%", "(", "start_position", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"end_position: %d\"", "%", "(", "end_position", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "\"answer: %s\"", "%", "(", "tokenization", ".", "printable_text", "(", "answer_text", ")", ")", ")", "\n", "\n", "source_answer_text", "=", "\" \"", ".", "join", "(", "source_tokens", "[", "source_start_position", ":", "(", "source_end_position", "+", "1", ")", "]", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"source_start_position: %d\"", "%", "(", "source_start_position", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"source_end_position: %d\"", "%", "(", "source_end_position", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"answer: %s\"", "%", "(", "tokenization", ".", "printable_text", "(", "source_answer_text", ")", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"source_output_span_mask: %s\"", "%", "' '", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "source_output_span_mask", "]", ")", ")", "\n", "\n", "#else:", "\n", "#  if example_index % 100 ==0:", "\n", "#    tf.logging.info(\"%d processed\", example_index)", "\n", "\n", "", "", "feature", "=", "InputFeatures", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "example_index", "=", "example_index", ",", "\n", "doc_span_index", "=", "doc_span_index", ",", "\n", "tokens", "=", "tokens", ",", "\n", "token_to_orig_map", "=", "token_to_orig_map", ",", "\n", "token_is_max_context", "=", "token_is_max_context", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "input_span_mask", "=", "input_span_mask", ",", "\n", "start_position", "=", "start_position", ",", "\n", "end_position", "=", "end_position", ",", "\n", "source_tokens", "=", "source_tokens", ",", "\n", "source_token_to_orig_map", "=", "source_token_to_orig_map", ",", "\n", "source_token_is_max_context", "=", "source_token_is_max_context", ",", "\n", "source_input_ids", "=", "source_input_ids", ",", "\n", "source_input_mask", "=", "source_input_mask", ",", "\n", "source_segment_ids", "=", "source_segment_ids", ",", "\n", "source_input_span_mask", "=", "source_input_span_mask", ",", "\n", "source_start_position", "=", "source_start_position", ",", "\n", "source_end_position", "=", "source_end_position", ",", "\n", "output_span_mask", "=", "output_span_mask", ",", "\n", "source_output_span_mask", "=", "source_output_span_mask", ")", "\n", "\n", "# Run callback", "\n", "output_fn", "(", "feature", ")", "\n", "\n", "unique_id", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc._improve_answer_span": [[849, 884], ["range", "tokenizer.tokenize", "range"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.ChineseFullTokenizer.tokenize"], ["", "", "", "def", "_improve_answer_span", "(", "doc_tokens", ",", "input_start", ",", "input_end", ",", "tokenizer", ",", "\n", "orig_answer_text", ")", ":", "\n", "  ", "\"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"", "\n", "\n", "# The SQuAD annotations are character based. We first project them to", "\n", "# whitespace-tokenized words. But then after WordPiece tokenization, we can", "\n", "# often find a \"better match\". For example:", "\n", "#", "\n", "#   Question: What year was John Smith born?", "\n", "#   Context: The leader was John Smith (1895-1943).", "\n", "#   Answer: 1895", "\n", "#", "\n", "# The original whitespace-tokenized answer will be \"(1895-1943).\". However", "\n", "# after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match", "\n", "# the exact answer, 1895.", "\n", "#", "\n", "# However, this is not always possible. Consider the following:", "\n", "#", "\n", "#   Question: What country is the top exporter of electornics?", "\n", "#   Context: The Japanese electronics industry is the lagest in the world.", "\n", "#   Answer: Japan", "\n", "#", "\n", "# In this case, the annotator chose \"Japan\" as a character sub-span of", "\n", "# the word \"Japanese\". Since our WordPiece tokenizer does not split", "\n", "# \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare", "\n", "# in SQuAD, but does happen.", "\n", "tok_answer_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "orig_answer_text", ")", ")", "\n", "\n", "for", "new_start", "in", "range", "(", "input_start", ",", "input_end", "+", "1", ")", ":", "\n", "    ", "for", "new_end", "in", "range", "(", "input_end", ",", "new_start", "-", "1", ",", "-", "1", ")", ":", "\n", "      ", "text_span", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "new_start", ":", "(", "new_end", "+", "1", ")", "]", ")", "\n", "if", "text_span", "==", "tok_answer_text", ":", "\n", "        ", "return", "(", "new_start", ",", "new_end", ")", "\n", "\n", "", "", "", "return", "(", "input_start", ",", "input_end", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc._check_is_max_context": [[886, 921], ["enumerate", "min"], "function", ["None"], ["", "def", "_check_is_max_context", "(", "doc_spans", ",", "cur_span_index", ",", "position", ")", ":", "\n", "  ", "\"\"\"Check if this is the 'max context' doc span for the token.\"\"\"", "\n", "\n", "# Because of the sliding window approach taken to scoring documents, a single", "\n", "# token can appear in multiple documents. E.g.", "\n", "#  Doc: the man went to the store and bought a gallon of milk", "\n", "#  Span A: the man went to the", "\n", "#  Span B: to the store and bought", "\n", "#  Span C: and bought a gallon of", "\n", "#  ...", "\n", "#", "\n", "# Now the word 'bought' will have two scores from spans B and C. We only", "\n", "# want to consider the score with \"maximum context\", which we define as", "\n", "# the *minimum* of its left and right context (the *sum* of left and", "\n", "# right context will always be the same, of course).", "\n", "#", "\n", "# In the example the maximum context for 'bought' would be span C since", "\n", "# it has 1 left context and 3 right context, while span B has 4 left context", "\n", "# and 0 right context.", "\n", "best_score", "=", "None", "\n", "best_span_index", "=", "None", "\n", "for", "(", "span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "    ", "end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "if", "position", "<", "doc_span", ".", "start", ":", "\n", "      ", "continue", "\n", "", "if", "position", ">", "end", ":", "\n", "      ", "continue", "\n", "", "num_left_context", "=", "position", "-", "doc_span", ".", "start", "\n", "num_right_context", "=", "end", "-", "position", "\n", "score", "=", "min", "(", "num_left_context", ",", "num_right_context", ")", "+", "0.01", "*", "doc_span", ".", "length", "\n", "if", "best_score", "is", "None", "or", "score", ">", "best_score", ":", "\n", "      ", "best_score", "=", "score", "\n", "best_span_index", "=", "span_index", "\n", "\n", "", "", "return", "cur_span_index", "==", "best_span_index", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.input_fn_builder": [[923, 980], ["tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.parse_single_example", "list", "tensorflow.data.TFRecordDataset", "d.shuffle.apply", "tf.parse_single_example.keys", "d.shuffle.repeat", "d.shuffle.shuffle", "tensorflow.contrib.data.map_and_batch", "tensorflow.to_int32", "run_clmrc.input_fn_builder._decode_record"], "function", ["None"], ["", "def", "input_fn_builder", "(", "input_file", ",", "seq_length", ",", "is_training", ",", "drop_remainder", ")", ":", "\n", "  ", "\"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"", "\n", "name_to_features", "=", "{", "\n", "\"unique_ids\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"input_ids\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"input_mask\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"segment_ids\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"input_span_mask\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"source_input_ids\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"source_input_mask\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"source_segment_ids\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"source_input_span_mask\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"output_span_mask\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"source_output_span_mask\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "}", "\n", "\n", "#if is_training:", "\n", "name_to_features", "[", "\"start_positions\"", "]", "=", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "tf", ".", "int64", ")", "\n", "name_to_features", "[", "\"end_positions\"", "]", "=", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "tf", ".", "int64", ")", "\n", "name_to_features", "[", "\"source_start_positions\"", "]", "=", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "tf", ".", "int64", ")", "\n", "name_to_features", "[", "\"source_end_positions\"", "]", "=", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "tf", ".", "int64", ")", "\n", "\n", "\n", "def", "_decode_record", "(", "record", ",", "name_to_features", ")", ":", "\n", "    ", "\"\"\"Decodes a record to a TensorFlow example.\"\"\"", "\n", "example", "=", "tf", ".", "parse_single_example", "(", "record", ",", "name_to_features", ")", "\n", "\n", "# tf.Example only supports tf.int64, but the TPU only supports tf.int32.", "\n", "# So cast all int64 to int32.", "\n", "for", "name", "in", "list", "(", "example", ".", "keys", "(", ")", ")", ":", "\n", "      ", "t", "=", "example", "[", "name", "]", "\n", "if", "t", ".", "dtype", "==", "tf", ".", "int64", ":", "\n", "        ", "t", "=", "tf", ".", "to_int32", "(", "t", ")", "\n", "", "example", "[", "name", "]", "=", "t", "\n", "\n", "", "return", "example", "\n", "\n", "", "def", "input_fn", "(", "params", ")", ":", "\n", "    ", "\"\"\"The actual input function.\"\"\"", "\n", "batch_size", "=", "params", "[", "\"batch_size\"", "]", "\n", "\n", "# For training, we want a lot of parallel reading and shuffling.", "\n", "# For eval, we want no shuffling and parallel reading doesn't matter.", "\n", "d", "=", "tf", ".", "data", ".", "TFRecordDataset", "(", "input_file", ")", "\n", "if", "is_training", ":", "\n", "      ", "d", "=", "d", ".", "repeat", "(", ")", "\n", "d", "=", "d", ".", "shuffle", "(", "buffer_size", "=", "100", ")", "\n", "\n", "", "d", "=", "d", ".", "apply", "(", "\n", "tf", ".", "contrib", ".", "data", ".", "map_and_batch", "(", "\n", "lambda", "record", ":", "_decode_record", "(", "record", ",", "name_to_features", ")", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "drop_remainder", "=", "drop_remainder", ")", ")", "\n", "\n", "return", "d", "\n", "\n", "", "return", "input_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.write_predictions": [[985, 1133], ["tensorflow.logging.info", "tensorflow.logging.info", "collections.defaultdict", "collections.namedtuple", "collections.OrderedDict", "collections.OrderedDict", "enumerate", "example_index_to_features[].append", "enumerate", "sorted", "collections.namedtuple", "run_clmrc._compute_softmax", "enumerate", "tensorflow.gfile.GFile", "writer.write", "tensorflow.gfile.GFile", "writer.write", "tensorflow.gfile.GFile", "writer.write", "run_clmrc._get_best_indexes", "run_clmrc._get_best_indexes", "nbest.append", "nbest.append", "len", "total_scores.append", "collections.OrderedDict", "nbest_json.append", "len", "len", "tok_text.strip.replace", "tok_text.strip.replace", "tok_text.strip.strip", "run_clmrc.get_final_text", "final_text.replace.replace", "collections.namedtuple.", "collections.namedtuple.", "json.dumps", "json.dumps", "json.dumps", "sorted.append", "tok_text.strip.split", "len", "len", "feature.token_is_max_context.get", "collections.namedtuple."], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc._compute_softmax", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc._get_best_indexes", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc._get_best_indexes", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.get_final_text"], ["def", "write_predictions", "(", "all_examples", ",", "all_features", ",", "all_results", ",", "n_best_size", ",", "\n", "max_answer_length", ",", "do_lower_case", ",", "output_prediction_file", ",", "\n", "output_nbest_file", ")", ":", "\n", "  ", "\"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"", "\n", "tf", ".", "logging", ".", "info", "(", "\"Writing predictions to: %s\"", "%", "(", "output_prediction_file", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Writing nbest to: %s\"", "%", "(", "output_nbest_file", ")", ")", "\n", "\n", "example_index_to_features", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "feature", "in", "all_features", ":", "\n", "    ", "example_index_to_features", "[", "feature", ".", "example_index", "]", ".", "append", "(", "feature", ")", "\n", "\n", "", "unique_id_to_result", "=", "{", "}", "\n", "for", "result", "in", "all_results", ":", "\n", "    ", "unique_id_to_result", "[", "result", ".", "unique_id", "]", "=", "result", "\n", "\n", "", "_PrelimPrediction", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"PrelimPrediction\"", ",", "\n", "[", "\"feature_index\"", ",", "\"start_index\"", ",", "\"end_index\"", ",", "\"start_logit\"", ",", "\"end_logit\"", "]", ")", "\n", "\n", "all_predictions", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "all_nbest_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "\n", "for", "(", "example_index", ",", "example", ")", "in", "enumerate", "(", "all_examples", ")", ":", "\n", "    ", "features", "=", "example_index_to_features", "[", "example_index", "]", "\n", "prelim_predictions", "=", "[", "]", "\n", "\n", "for", "(", "feature_index", ",", "feature", ")", "in", "enumerate", "(", "features", ")", ":", "# multi-trunk", "\n", "      ", "result", "=", "unique_id_to_result", "[", "feature", ".", "unique_id", "]", "\n", "start_indexes", "=", "_get_best_indexes", "(", "result", ".", "start_logits", ",", "n_best_size", ")", "\n", "end_indexes", "=", "_get_best_indexes", "(", "result", ".", "end_logits", ",", "n_best_size", ")", "\n", "for", "start_index", "in", "start_indexes", ":", "\n", "        ", "for", "end_index", "in", "end_indexes", ":", "\n", "# We could hypothetically create invalid predictions, e.g., predict", "\n", "# that the start of the span is in the question. We throw out all", "\n", "# invalid predictions.", "\n", "          ", "if", "start_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "            ", "continue", "\n", "", "if", "end_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "            ", "continue", "\n", "", "if", "start_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "            ", "continue", "\n", "", "if", "end_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "            ", "continue", "\n", "", "if", "not", "feature", ".", "token_is_max_context", ".", "get", "(", "start_index", ",", "False", ")", ":", "\n", "            ", "continue", "\n", "", "if", "end_index", "<", "start_index", ":", "\n", "            ", "continue", "\n", "", "length", "=", "end_index", "-", "start_index", "+", "1", "\n", "if", "length", ">", "max_answer_length", ":", "\n", "            ", "continue", "\n", "", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "\n", "feature_index", "=", "feature_index", ",", "\n", "start_index", "=", "start_index", ",", "\n", "end_index", "=", "end_index", ",", "\n", "start_logit", "=", "result", ".", "start_logits", "[", "start_index", "]", ",", "\n", "end_logit", "=", "result", ".", "end_logits", "[", "end_index", "]", ")", ")", "\n", "\n", "", "", "", "prelim_predictions", "=", "sorted", "(", "\n", "prelim_predictions", ",", "\n", "key", "=", "lambda", "x", ":", "(", "x", ".", "start_logit", "+", "x", ".", "end_logit", ")", ",", "\n", "reverse", "=", "True", ")", "\n", "\n", "_NbestPrediction", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"NbestPrediction\"", ",", "[", "\"text\"", ",", "\"start_logit\"", ",", "\"end_logit\"", ",", "\"start_index\"", ",", "\"end_index\"", "]", ")", "\n", "\n", "seen_predictions", "=", "{", "}", "\n", "nbest", "=", "[", "]", "\n", "for", "pred", "in", "prelim_predictions", ":", "\n", "      ", "if", "len", "(", "nbest", ")", ">=", "n_best_size", ":", "\n", "        ", "break", "\n", "", "feature", "=", "features", "[", "pred", ".", "feature_index", "]", "\n", "if", "pred", ".", "start_index", ">", "0", ":", "# this is a non-null prediction", "\n", "        ", "tok_tokens", "=", "feature", ".", "tokens", "[", "pred", ".", "start_index", ":", "(", "pred", ".", "end_index", "+", "1", ")", "]", "\n", "orig_doc_start", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "start_index", "]", "\n", "orig_doc_end", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "end_index", "]", "\n", "orig_tokens", "=", "example", ".", "doc_tokens", "[", "orig_doc_start", ":", "(", "orig_doc_end", "+", "1", ")", "]", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tok_tokens", ")", "\n", "\n", "# De-tokenize WordPieces that have been split off.", "\n", "tok_text", "=", "tok_text", ".", "replace", "(", "\" ##\"", ",", "\"\"", ")", "\n", "tok_text", "=", "tok_text", ".", "replace", "(", "\"##\"", ",", "\"\"", ")", "\n", "\n", "# Clean whitespace", "\n", "tok_text", "=", "tok_text", ".", "strip", "(", ")", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tok_text", ".", "split", "(", ")", ")", "\n", "orig_text", "=", "\" \"", ".", "join", "(", "orig_tokens", ")", "\n", "\n", "final_text", "=", "get_final_text", "(", "tok_text", ",", "orig_text", ",", "do_lower_case", ")", "\n", "final_text", "=", "final_text", ".", "replace", "(", "' '", ",", "''", ")", "\n", "if", "final_text", "in", "seen_predictions", ":", "\n", "          ", "continue", "\n", "\n", "", "seen_predictions", "[", "final_text", "]", "=", "True", "\n", "", "else", ":", "\n", "        ", "final_text", "=", "\"\"", "\n", "seen_predictions", "[", "final_text", "]", "=", "True", "\n", "\n", "", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "\n", "text", "=", "final_text", ",", "\n", "start_logit", "=", "pred", ".", "start_logit", ",", "\n", "end_logit", "=", "pred", ".", "end_logit", ",", "\n", "start_index", "=", "pred", ".", "start_index", ",", "\n", "end_index", "=", "pred", ".", "end_index", ")", ")", "\n", "\n", "# In very rare edge cases we could have no valid predictions. So we", "\n", "# just create a nonce prediction in this case to avoid failure.", "\n", "", "if", "not", "nbest", ":", "\n", "      ", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "text", "=", "\"empty\"", ",", "start_logit", "=", "0.0", ",", "end_logit", "=", "0.0", ",", "start_index", "=", "-", "1", ",", "end_index", "=", "-", "1", ")", ")", "\n", "\n", "", "assert", "len", "(", "nbest", ")", ">=", "1", "\n", "\n", "total_scores", "=", "[", "]", "\n", "best_non_null_entry", "=", "None", "\n", "for", "entry", "in", "nbest", ":", "\n", "      ", "total_scores", ".", "append", "(", "entry", ".", "start_logit", "+", "entry", ".", "end_logit", ")", "\n", "if", "not", "best_non_null_entry", ":", "\n", "        ", "if", "entry", ".", "text", ":", "\n", "          ", "best_non_null_entry", "=", "entry", "\n", "\n", "", "", "", "probs", "=", "_compute_softmax", "(", "total_scores", ")", "\n", "\n", "nbest_json", "=", "[", "]", "\n", "for", "(", "i", ",", "entry", ")", "in", "enumerate", "(", "nbest", ")", ":", "\n", "      ", "output", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "output", "[", "\"text\"", "]", "=", "entry", ".", "text", "\n", "output", "[", "\"probability\"", "]", "=", "probs", "[", "i", "]", "\n", "output", "[", "\"start_logit\"", "]", "=", "entry", ".", "start_logit", "\n", "output", "[", "\"end_logit\"", "]", "=", "entry", ".", "end_logit", "\n", "output", "[", "\"start_index\"", "]", "=", "entry", ".", "start_index", "\n", "output", "[", "\"end_index\"", "]", "=", "entry", ".", "end_index", "\n", "nbest_json", ".", "append", "(", "output", ")", "\n", "\n", "", "assert", "len", "(", "nbest_json", ")", ">=", "1", "\n", "\n", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "best_non_null_entry", ".", "text", "#nbest_json[0][\"text\"]", "\n", "all_nbest_json", "[", "example", ".", "qas_id", "]", "=", "nbest_json", "\n", "\n", "", "with", "tf", ".", "gfile", ".", "GFile", "(", "output_prediction_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "    ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "all_predictions", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "with", "tf", ".", "gfile", ".", "GFile", "(", "output_prediction_file", "+", "\"2\"", ",", "\"w\"", ")", "as", "writer", ":", "\n", "    ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "all_predictions", ",", "indent", "=", "4", ",", "ensure_ascii", "=", "False", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "with", "tf", ".", "gfile", ".", "GFile", "(", "output_nbest_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "    ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "all_nbest_json", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.get_final_text": [[1135, 1229], ["tokenization.BasicTokenizer", "tok_text.find", "run_clmrc.get_final_text._strip_spaces"], "function", ["None"], ["", "", "def", "get_final_text", "(", "pred_text", ",", "orig_text", ",", "do_lower_case", ")", ":", "\n", "  ", "\"\"\"Project the tokenized prediction back to the original text.\"\"\"", "\n", "\n", "# When we created the data, we kept track of the alignment between original", "\n", "# (whitespace tokenized) tokens and our WordPiece tokenized tokens. So", "\n", "# now `orig_text` contains the span of our original text corresponding to the", "\n", "# span that we predicted.", "\n", "#", "\n", "# However, `orig_text` may contain extra characters that we don't want in", "\n", "# our prediction.", "\n", "#", "\n", "# For example, let's say:", "\n", "#   pred_text = steve smith", "\n", "#   orig_text = Steve Smith's", "\n", "#", "\n", "# We don't want to return `orig_text` because it contains the extra \"'s\".", "\n", "#", "\n", "# We don't want to return `pred_text` because it's already been normalized", "\n", "# (the SQuAD eval script also does punctuation stripping/lower casing but", "\n", "# our tokenizer does additional normalization like stripping accent", "\n", "# characters).", "\n", "#", "\n", "# What we really want to return is \"Steve Smith\".", "\n", "#", "\n", "# Therefore, we have to apply a semi-complicated alignment heruistic between", "\n", "# `pred_text` and `orig_text` to get a character-to-charcter alignment. This", "\n", "# can fail in certain cases in which case we just return `orig_text`.", "\n", "\n", "def", "_strip_spaces", "(", "text", ")", ":", "\n", "    ", "ns_chars", "=", "[", "]", "\n", "ns_to_s_map", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "(", "i", ",", "c", ")", "in", "enumerate", "(", "text", ")", ":", "\n", "      ", "if", "c", "==", "\" \"", ":", "\n", "        ", "continue", "\n", "", "ns_to_s_map", "[", "len", "(", "ns_chars", ")", "]", "=", "i", "\n", "ns_chars", ".", "append", "(", "c", ")", "\n", "", "ns_text", "=", "\"\"", ".", "join", "(", "ns_chars", ")", "\n", "return", "(", "ns_text", ",", "ns_to_s_map", ")", "\n", "\n", "# We first tokenize `orig_text`, strip whitespace from the result", "\n", "# and `pred_text`, and check if they are the same length. If they are", "\n", "# NOT the same length, the heuristic has failed. If they are the same", "\n", "# length, we assume the characters are one-to-one aligned.", "\n", "", "tokenizer", "=", "tokenization", ".", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "orig_text", ")", ")", "\n", "\n", "start_position", "=", "tok_text", ".", "find", "(", "pred_text", ")", "\n", "if", "start_position", "==", "-", "1", ":", "\n", "    ", "if", "FLAGS", ".", "verbose_logging", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\n", "\"Unable to find text: '%s' in '%s'\"", "%", "(", "pred_text", ",", "orig_text", ")", ")", "\n", "", "return", "orig_text", "\n", "", "end_position", "=", "start_position", "+", "len", "(", "pred_text", ")", "-", "1", "\n", "\n", "(", "orig_ns_text", ",", "orig_ns_to_s_map", ")", "=", "_strip_spaces", "(", "orig_text", ")", "\n", "(", "tok_ns_text", ",", "tok_ns_to_s_map", ")", "=", "_strip_spaces", "(", "tok_text", ")", "\n", "\n", "if", "len", "(", "orig_ns_text", ")", "!=", "len", "(", "tok_ns_text", ")", ":", "\n", "    ", "if", "FLAGS", ".", "verbose_logging", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"Length not equal after stripping spaces: '%s' vs '%s'\"", ",", "\n", "orig_ns_text", ",", "tok_ns_text", ")", "\n", "", "return", "orig_text", "\n", "\n", "# We then project the characters in `pred_text` back to `orig_text` using", "\n", "# the character-to-character alignment.", "\n", "", "tok_s_to_ns_map", "=", "{", "}", "\n", "for", "(", "i", ",", "tok_index", ")", "in", "six", ".", "iteritems", "(", "tok_ns_to_s_map", ")", ":", "\n", "    ", "tok_s_to_ns_map", "[", "tok_index", "]", "=", "i", "\n", "\n", "", "orig_start_position", "=", "None", "\n", "if", "start_position", "in", "tok_s_to_ns_map", ":", "\n", "    ", "ns_start_position", "=", "tok_s_to_ns_map", "[", "start_position", "]", "\n", "if", "ns_start_position", "in", "orig_ns_to_s_map", ":", "\n", "      ", "orig_start_position", "=", "orig_ns_to_s_map", "[", "ns_start_position", "]", "\n", "\n", "", "", "if", "orig_start_position", "is", "None", ":", "\n", "    ", "if", "FLAGS", ".", "verbose_logging", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"Couldn't map start position\"", ")", "\n", "", "return", "orig_text", "\n", "\n", "", "orig_end_position", "=", "None", "\n", "if", "end_position", "in", "tok_s_to_ns_map", ":", "\n", "    ", "ns_end_position", "=", "tok_s_to_ns_map", "[", "end_position", "]", "\n", "if", "ns_end_position", "in", "orig_ns_to_s_map", ":", "\n", "      ", "orig_end_position", "=", "orig_ns_to_s_map", "[", "ns_end_position", "]", "\n", "\n", "", "", "if", "orig_end_position", "is", "None", ":", "\n", "    ", "if", "FLAGS", ".", "verbose_logging", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"Couldn't map end position\"", ")", "\n", "", "return", "orig_text", "\n", "\n", "", "output_text", "=", "orig_text", "[", "orig_start_position", ":", "(", "orig_end_position", "+", "1", ")", "]", "\n", "return", "output_text", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc._get_best_indexes": [[1231, 1241], ["sorted", "range", "enumerate", "len", "best_indexes.append"], "function", ["None"], ["", "def", "_get_best_indexes", "(", "logits", ",", "n_best_size", ")", ":", "\n", "  ", "\"\"\"Get the n-best logits from a list.\"\"\"", "\n", "index_and_score", "=", "sorted", "(", "enumerate", "(", "logits", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "best_indexes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "index_and_score", ")", ")", ":", "\n", "    ", "if", "i", ">=", "n_best_size", ":", "\n", "      ", "break", "\n", "", "best_indexes", ".", "append", "(", "index_and_score", "[", "i", "]", "[", "0", "]", ")", "\n", "", "return", "best_indexes", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc._compute_softmax": [[1243, 1264], ["math.exp", "exp_scores.append", "probs.append"], "function", ["None"], ["", "def", "_compute_softmax", "(", "scores", ")", ":", "\n", "  ", "\"\"\"Compute softmax probability over raw logits.\"\"\"", "\n", "if", "not", "scores", ":", "\n", "    ", "return", "[", "]", "\n", "\n", "", "max_score", "=", "None", "\n", "for", "score", "in", "scores", ":", "\n", "    ", "if", "max_score", "is", "None", "or", "score", ">", "max_score", ":", "\n", "      ", "max_score", "=", "score", "\n", "\n", "", "", "exp_scores", "=", "[", "]", "\n", "total_sum", "=", "0.0", "\n", "for", "score", "in", "scores", ":", "\n", "    ", "x", "=", "math", ".", "exp", "(", "score", "-", "max_score", ")", "\n", "exp_scores", ".", "append", "(", "x", ")", "\n", "total_sum", "+=", "x", "\n", "\n", "", "probs", "=", "[", "]", "\n", "for", "score", "in", "exp_scores", ":", "\n", "    ", "probs", ".", "append", "(", "score", "/", "total_sum", ")", "\n", "", "return", "probs", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.validate_flags_or_throw": [[1310, 1341], ["tokenization.validate_case_matches_checkpoint", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.tokenization.validate_case_matches_checkpoint"], ["", "", "def", "validate_flags_or_throw", "(", "bert_config", ")", ":", "\n", "  ", "\"\"\"Validate the input FLAGS or throw an exception.\"\"\"", "\n", "tokenization", ".", "validate_case_matches_checkpoint", "(", "FLAGS", ".", "do_lower_case", ",", "\n", "FLAGS", ".", "init_checkpoint", ")", "\n", "\n", "if", "not", "FLAGS", ".", "do_train", "and", "not", "FLAGS", ".", "do_predict", "and", "not", "FLAGS", ".", "do_eval", ":", "\n", "    ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_predict` must be True.\"", ")", "\n", "\n", "", "if", "FLAGS", ".", "do_train", ":", "\n", "    ", "if", "not", "FLAGS", ".", "train_file", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "\"If `do_train` is True, then `train_file` must be specified.\"", ")", "\n", "", "", "if", "FLAGS", ".", "do_predict", ":", "\n", "    ", "if", "not", "FLAGS", ".", "predict_file", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "\"If `do_predict` is True, then `predict_file` must be specified.\"", ")", "\n", "", "", "if", "FLAGS", ".", "do_eval", ":", "\n", "    ", "if", "not", "FLAGS", ".", "eval_file", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "\"If `do_eval` is True, then `eval_file` must be specified.\"", ")", "\n", "\n", "", "", "if", "FLAGS", ".", "max_seq_length", ">", "bert_config", ".", "max_position_embeddings", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"Cannot use sequence length %d because the BERT model \"", "\n", "\"was only trained up to sequence length %d\"", "%", "\n", "(", "FLAGS", ".", "max_seq_length", ",", "bert_config", ".", "max_position_embeddings", ")", ")", "\n", "\n", "", "if", "FLAGS", ".", "max_seq_length", "<=", "FLAGS", ".", "max_query_length", "+", "3", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"The max_seq_length (%d) must be greater than max_query_length \"", "\n", "\"(%d) + 3\"", "%", "(", "FLAGS", ".", "max_seq_length", ",", "FLAGS", ".", "max_query_length", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.main": [[1344, 1503], ["tensorflow.logging.set_verbosity", "modeling.BertConfig.from_json_file", "run_clmrc.validate_flags_or_throw", "tensorflow.gfile.MakeDirs", "tokenization.FullTokenizer", "tensorflow.contrib.tpu.RunConfig", "layers.model_fn_builder", "tensorflow.contrib.tpu.TPUEstimator", "tensorflow.contrib.cluster_resolver.TPUClusterResolver", "run_clmrc.read_squad_examples", "random.Random", "random.Random.shuffle", "run_clmrc.FeatureWriter", "run_clmrc.convert_examples_to_features", "run_clmrc.FeatureWriter.close", "len", "int", "int", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "run_clmrc.input_fn_builder", "tf.contrib.tpu.TPUEstimator.train", "run_clmrc.read_squad_examples", "run_clmrc.FeatureWriter", "run_clmrc.convert_examples_to_features", "run_clmrc.FeatureWriter.close", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "run_clmrc.input_fn_builder", "tf.contrib.tpu.TPUEstimator.predict", "os.path.join", "os.path.join", "run_clmrc.write_predictions", "tensorflow.contrib.tpu.TPUConfig", "int", "eval_features.append", "run_clmrc.FeatureWriter.process_feature", "len", "len", "int", "all_results.append", "os.path.join", "os.path.join", "tensorflow.logging.info", "float", "float", "RawResult", "len", "len"], "function", ["home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.modeling.BertConfig.from_json_file", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.validate_flags_or_throw", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.layers.model_fn_builder", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.read_squad_examples", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.convert_examples_to_features", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.FeatureWriter.close", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.input_fn_builder", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.read_squad_examples", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.convert_examples_to_features", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.FeatureWriter.close", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.input_fn_builder", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.write_predictions", "home.repos.pwc.inspect_result.ymcui_Cross-Lingual-MRC.src.run_clmrc.FeatureWriter.process_feature"], ["", "", "def", "main", "(", "_", ")", ":", "\n", "  ", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "\n", "bert_config", "=", "modeling", ".", "BertConfig", ".", "from_json_file", "(", "FLAGS", ".", "bert_config_file", ")", "\n", "\n", "validate_flags_or_throw", "(", "bert_config", ")", "\n", "\n", "tf", ".", "gfile", ".", "MakeDirs", "(", "FLAGS", ".", "output_dir", ")", "\n", "\n", "tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "vocab_file", "=", "FLAGS", ".", "vocab_file", ",", "do_lower_case", "=", "FLAGS", ".", "do_lower_case", ")", "\n", "\n", "tpu_cluster_resolver", "=", "None", "\n", "if", "FLAGS", ".", "use_tpu", "and", "FLAGS", ".", "tpu_name", ":", "\n", "    ", "tpu_cluster_resolver", "=", "tf", ".", "contrib", ".", "cluster_resolver", ".", "TPUClusterResolver", "(", "\n", "FLAGS", ".", "tpu_name", ",", "zone", "=", "FLAGS", ".", "tpu_zone", ",", "project", "=", "FLAGS", ".", "gcp_project", ")", "\n", "\n", "", "is_per_host", "=", "tf", ".", "contrib", ".", "tpu", ".", "InputPipelineConfig", ".", "PER_HOST_V2", "\n", "run_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "RunConfig", "(", "\n", "cluster", "=", "tpu_cluster_resolver", ",", "\n", "master", "=", "FLAGS", ".", "master", ",", "\n", "model_dir", "=", "FLAGS", ".", "output_dir", ",", "\n", "save_checkpoints_steps", "=", "FLAGS", ".", "save_checkpoints_steps", ",", "\n", "keep_checkpoint_max", "=", "2", ",", "\n", "tpu_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUConfig", "(", "\n", "iterations_per_loop", "=", "FLAGS", ".", "iterations_per_loop", ",", "\n", "num_shards", "=", "FLAGS", ".", "num_tpu_cores", ",", "\n", "per_host_input_for_training", "=", "is_per_host", ")", ")", "\n", "\n", "train_examples", "=", "None", "\n", "num_train_steps", "=", "None", "\n", "num_warmup_steps", "=", "None", "\n", "if", "FLAGS", ".", "do_train", ":", "\n", "    ", "train_examples", "=", "read_squad_examples", "(", "input_file", "=", "FLAGS", ".", "train_file", ",", "is_training", "=", "True", ")", "\n", "\n", "# Pre-shuffle the input to avoid having to make a very large shuffle", "\n", "# buffer in in the `input_fn`.", "\n", "rng", "=", "random", ".", "Random", "(", "int", "(", "FLAGS", ".", "rand_seed", ")", ")", "\n", "rng", ".", "shuffle", "(", "train_examples", ")", "\n", "\n", "# We write to a temporary file to avoid storing very large constant tensors", "\n", "# in memory.", "\n", "train_writer", "=", "FeatureWriter", "(", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"train.tf_record\"", ")", ",", "\n", "is_training", "=", "True", ")", "\n", "convert_examples_to_features", "(", "\n", "examples", "=", "train_examples", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "max_seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "doc_stride", "=", "FLAGS", ".", "doc_stride", ",", "\n", "max_query_length", "=", "FLAGS", ".", "max_query_length", ",", "\n", "is_training", "=", "True", ",", "\n", "output_fn", "=", "train_writer", ".", "process_feature", ")", "\n", "train_writer", ".", "close", "(", ")", "\n", "num_features", "=", "train_writer", ".", "num_features", "\n", "train_examples_len", "=", "len", "(", "train_examples", ")", "\n", "del", "train_examples", "\n", "\n", "num_train_steps", "=", "int", "(", "num_features", "/", "FLAGS", ".", "train_batch_size", "*", "FLAGS", ".", "num_train_epochs", ")", "\n", "num_warmup_steps", "=", "int", "(", "num_train_steps", "*", "FLAGS", ".", "warmup_proportion", ")", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num orig examples = %d\"", ",", "train_examples_len", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num split examples = %d\"", ",", "num_features", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Batch size = %d\"", ",", "FLAGS", ".", "train_batch_size", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_steps", ")", "\n", "\n", "", "model_fn", "=", "model_fn_builder", "(", "\n", "bert_config", "=", "bert_config", ",", "\n", "init_checkpoint", "=", "FLAGS", ".", "init_checkpoint", ",", "\n", "learning_rate", "=", "FLAGS", ".", "learning_rate", ",", "\n", "num_train_steps", "=", "num_train_steps", ",", "\n", "num_warmup_steps", "=", "num_warmup_steps", ",", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "use_one_hot_embeddings", "=", "FLAGS", ".", "use_tpu", ")", "\n", "\n", "# If TPU is not available, this will fall back to normal Estimator on CPU", "\n", "# or GPU.", "\n", "estimator", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimator", "(", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "model_fn", "=", "model_fn", ",", "\n", "config", "=", "run_config", ",", "\n", "train_batch_size", "=", "FLAGS", ".", "train_batch_size", ",", "\n", "predict_batch_size", "=", "FLAGS", ".", "predict_batch_size", ")", "\n", "\n", "# do training", "\n", "if", "FLAGS", ".", "do_train", ":", "\n", "    ", "train_writer_filename", "=", "train_writer", ".", "filename", "\n", "\n", "train_input_fn", "=", "input_fn_builder", "(", "\n", "input_file", "=", "train_writer_filename", ",", "\n", "seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "is_training", "=", "True", ",", "\n", "drop_remainder", "=", "True", ")", "\n", "estimator", ".", "train", "(", "input_fn", "=", "train_input_fn", ",", "max_steps", "=", "num_train_steps", ")", "\n", "\n", "# do predictions", "\n", "", "if", "FLAGS", ".", "do_predict", ":", "\n", "    ", "eval_examples", "=", "read_squad_examples", "(", "\n", "input_file", "=", "FLAGS", ".", "predict_file", ",", "is_training", "=", "False", ")", "\n", "\n", "eval_writer", "=", "FeatureWriter", "(", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"predict.tf_record\"", ")", ",", "\n", "is_training", "=", "False", ")", "\n", "eval_features", "=", "[", "]", "\n", "\n", "def", "append_feature", "(", "feature", ")", ":", "\n", "      ", "eval_features", ".", "append", "(", "feature", ")", "\n", "eval_writer", ".", "process_feature", "(", "feature", ")", "\n", "\n", "", "convert_examples_to_features", "(", "\n", "examples", "=", "eval_examples", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "max_seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "doc_stride", "=", "FLAGS", ".", "doc_stride", ",", "\n", "max_query_length", "=", "FLAGS", ".", "max_query_length", ",", "\n", "is_training", "=", "False", ",", "\n", "output_fn", "=", "append_feature", ")", "\n", "eval_writer", ".", "close", "(", ")", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** Running predictions *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num orig examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num split examples = %d\"", ",", "len", "(", "eval_features", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Batch size = %d\"", ",", "FLAGS", ".", "predict_batch_size", ")", "\n", "\n", "all_results", "=", "[", "]", "\n", "\n", "predict_input_fn", "=", "input_fn_builder", "(", "\n", "input_file", "=", "eval_writer", ".", "filename", ",", "\n", "seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "is_training", "=", "False", ",", "\n", "drop_remainder", "=", "False", ")", "\n", "\n", "# If running eval on the TPU, you will need to specify the number of", "\n", "# steps.", "\n", "all_results", "=", "[", "]", "\n", "for", "result", "in", "estimator", ".", "predict", "(", "\n", "predict_input_fn", ",", "yield_single_examples", "=", "True", ")", ":", "\n", "      ", "if", "len", "(", "all_results", ")", "%", "1000", "==", "0", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"Processing example: %d\"", "%", "(", "len", "(", "all_results", ")", ")", ")", "\n", "", "unique_id", "=", "int", "(", "result", "[", "\"unique_ids\"", "]", ")", "\n", "start_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "result", "[", "\"start_logits\"", "]", ".", "flat", "]", "\n", "end_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "result", "[", "\"end_logits\"", "]", ".", "flat", "]", "\n", "all_results", ".", "append", "(", "\n", "RawResult", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "start_logits", "=", "start_logits", ",", "\n", "end_logits", "=", "end_logits", ")", ")", "\n", "\n", "\n", "", "output_json_name", "=", "\"dev_predictions.json\"", "\n", "output_nbest_name", "=", "\"dev_nbest_predictions.json\"", "\n", "\n", "output_prediction_file", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "output_json_name", ")", "\n", "output_nbest_file", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "output_nbest_name", ")", "\n", "\n", "write_predictions", "(", "eval_examples", ",", "eval_features", ",", "all_results", ",", "\n", "FLAGS", ".", "n_best_size", ",", "FLAGS", ".", "max_answer_length", ",", "\n", "FLAGS", ".", "do_lower_case", ",", "output_prediction_file", ",", "\n", "output_nbest_file", ")", "\n", "\n"]]}