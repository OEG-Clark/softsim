{"home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.inference.combine_result": [[10, 39], ["open", "len", "range", "open", "g.readlines", "open", "p.readlines", "len", "len", "gold_l.strip().split", "pred_l.strip().split", "pred_label_str.split", "len", "range", "o.writelines", "gold_label_str.split", "len", "len", "text.split", "o.writelines", "gold_l.strip", "pred_l.strip", "len"], "function", ["None"], ["def", "combine_result", "(", "gold_path", ",", "pred_path", ",", "out_path", ")", ":", "\n", "    ", "with", "open", "(", "out_path", ",", "'w'", ",", "encoding", "=", "'utf8'", ")", "as", "o", ":", "\n", "        ", "with", "open", "(", "gold_path", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "g", ":", "\n", "            ", "gold_lines", "=", "g", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "pred_path", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "p", ":", "\n", "            ", "pred_lines", "=", "p", ".", "readlines", "(", ")", "\n", "", "assert", "len", "(", "gold_lines", ")", "==", "len", "(", "pred_lines", ")", "\n", "data_num", "=", "len", "(", "gold_lines", ")", "\n", "for", "i", "in", "range", "(", "data_num", ")", ":", "\n", "            ", "gold_l", "=", "gold_lines", "[", "i", "]", "\n", "pred_l", "=", "pred_lines", "[", "i", "]", "\n", "gold_content_list", "=", "gold_l", ".", "strip", "(", "'\\n'", ")", ".", "split", "(", "'\\t'", ")", "\n", "text", "=", "gold_content_list", "[", "0", "]", "\n", "gold_label_str", "=", "gold_content_list", "[", "1", "]", "\n", "\n", "pred_l", "=", "pred_lines", "[", "i", "]", "\n", "pred_content_list", "=", "pred_l", ".", "strip", "(", "'\\n'", ")", ".", "split", "(", "'\\t'", ")", "\n", "pred_label_str", "=", "pred_content_list", "[", "1", "]", "\n", "\n", "pred_label_list", "=", "pred_label_str", ".", "split", "(", ")", "\n", "gold_label_list", "=", "gold_label_str", ".", "split", "(", ")", "[", ":", "len", "(", "pred_label_list", ")", "]", "# result truncation", "\n", "assert", "len", "(", "gold_label_list", ")", "==", "len", "(", "pred_label_list", ")", "\n", "\n", "instance_len", "=", "len", "(", "gold_label_list", ")", "\n", "text_list", "=", "text", ".", "split", "(", ")", "[", ":", "instance_len", "]", "\n", "for", "j", "in", "range", "(", "instance_len", ")", ":", "\n", "                ", "out_str", "=", "text_list", "[", "j", "]", "+", "' '", "+", "gold_label_list", "[", "j", "]", "+", "' '", "+", "pred_label_list", "[", "j", "]", "\n", "o", ".", "writelines", "(", "out_str", "+", "'\\n'", ")", "\n", "", "o", ".", "writelines", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.inference.evaluate_model": [[40, 90], ["os.path.exists", "torch.cuda.is_available", "torch.cuda.is_available", "torch.device", "torch.device", "inference.combine_result", "metric_py3.fmeasure_from_singlefile", "os.remove", "os.remove", "os.makedirs", "torch.no_grad", "torch.no_grad", "model.eval", "range", "open", "int", "Exception", "data.get_next_validation_batch", "model.decode", "data.parse_result", "data.parse_result", "len", "range", "o.writelines", "int", "src_tensor.cuda.cuda", "src_attn_mask.cuda.cuda", "tgt_mask.cuda.cuda", "len", "len", "predictions[].split", "ref_predictions[].split"], "function", ["home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.train.combine_result", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.fmeasure_from_singlefile", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.get_next_validation_batch", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.model.NERModel.decode", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.parse_result", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.parse_result"], ["", "", "", "def", "evaluate_model", "(", "args", ",", "data", ",", "model", ",", "save_path", ",", "mode", ")", ":", "\n", "    ", "import", "os", "\n", "if", "os", ".", "path", ".", "exists", "(", "save_path", ")", ":", "\n", "        ", "pass", "\n", "", "else", ":", "# recursively construct directory", "\n", "        ", "os", ".", "makedirs", "(", "save_path", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "cuda_available", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "\n", "\n", "device", "=", "torch", ".", "device", "(", "'cuda'", ")", "\n", "if", "mode", "==", "'dev'", ":", "\n", "        ", "eval_step_num", "=", "int", "(", "data", ".", "dev_num", "/", "args", ".", "batch_size", ")", "+", "1", "\n", "instance_num", "=", "data", ".", "dev_num", "\n", "gold_path", "=", "data", ".", "dev_path", "\n", "", "elif", "mode", "==", "'test'", ":", "\n", "        ", "eval_step_num", "=", "int", "(", "data", ".", "test_num", "/", "args", ".", "batch_size", ")", "+", "1", "\n", "instance_num", "=", "data", ".", "test_num", "\n", "gold_path", "=", "data", ".", "test_path", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "'Wrong Mode!!!'", ")", "\n", "\n", "", "res_list", "=", "[", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "for", "_", "in", "range", "(", "eval_step_num", ")", ":", "\n", "            ", "src_tensor", ",", "src_attn_mask", ",", "_", ",", "tgt_mask", ",", "tgt_ref_id_list", "=", "data", ".", "get_next_validation_batch", "(", "args", ".", "batch_size", ",", "mode", ")", "\n", "if", "cuda_available", ":", "\n", "                ", "src_tensor", "=", "src_tensor", ".", "cuda", "(", "device", ")", "\n", "src_attn_mask", "=", "src_attn_mask", ".", "cuda", "(", "device", ")", "\n", "tgt_mask", "=", "tgt_mask", ".", "cuda", "(", "device", ")", "\n", "", "predictions", "=", "model", ".", "decode", "(", "src_tensor", ",", "src_attn_mask", ",", "tgt_mask", ")", "\n", "predictions", "=", "data", ".", "parse_result", "(", "predictions", ")", "\n", "ref_predictions", "=", "data", ".", "parse_result", "(", "tgt_ref_id_list", ")", "\n", "bsz", "=", "len", "(", "tgt_ref_id_list", ")", "\n", "for", "idx", "in", "range", "(", "bsz", ")", ":", "\n", "                ", "assert", "len", "(", "predictions", "[", "idx", "]", ".", "split", "(", ")", ")", "==", "len", "(", "ref_predictions", "[", "idx", "]", ".", "split", "(", ")", ")", "\n", "", "res_list", "+=", "predictions", "\n", "", "res_list", "=", "res_list", "[", ":", "instance_num", "]", "\n", "", "eval_path", "=", "save_path", "+", "'/eval.txt'", "\n", "with", "open", "(", "eval_path", ",", "'w'", ",", "encoding", "=", "'utf8'", ")", "as", "o", ":", "\n", "        ", "for", "res", "in", "res_list", ":", "\n", "            ", "o", ".", "writelines", "(", "res", "+", "'\\t'", "+", "res", "+", "'\\n'", ")", "\n", "", "", "combine_path", "=", "save_path", "+", "'/'", "+", "mode", "+", "'_gold_eval_combine.txt'", "\n", "combine_result", "(", "gold_path", ",", "eval_path", ",", "combine_path", ")", "\n", "precision", ",", "recall", ",", "f1", "=", "fmeasure_from_singlefile", "(", "combine_path", ",", "args", ".", "evaluation_mode", ")", "\n", "os", ".", "remove", "(", "combine_path", ")", "\n", "os", ".", "remove", "(", "eval_path", ")", "\n", "return", "precision", ",", "recall", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.inference.parse_config": [[92, 105], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["", "def", "parse_config", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_name\"", ",", "type", "=", "str", ",", "default", "=", "\"cambridgeltl/clbert-base-chinese\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--saved_ckpt_path\"", ",", "type", "=", "str", ",", "help", "=", "\"the path of the trained model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_path\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--dev_path\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--test_path\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--label_path\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_len\"", ",", "type", "=", "str", ",", "default", "=", "128", ")", "\n", "# learning configuration", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--evaluation_mode\"", ",", "type", "=", "str", ",", "default", "=", "\"BMES\"", ",", "help", "=", "\"BMES or BIO\"", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.__init__": [[19, 56], ["dataclass.load_label_dict", "len", "print", "dataclass.Data.tokenizer.convert_tokens_to_ids", "dataclass.Data.process_file", "dataclass.Data.process_file", "dataclass.Data.process_file", "print", "random.shuffle", "print", "len", "len", "len", "max", "max", "max", "range", "range", "range", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.load_label_dict", "home.repos.pwc.inspect_result.yxuansu_tacl.english.tokenize_data.process_file", "home.repos.pwc.inspect_result.yxuansu_tacl.english.tokenize_data.process_file", "home.repos.pwc.inspect_result.yxuansu_tacl.english.tokenize_data.process_file"], ["    ", "def", "__init__", "(", "self", ",", "tokenizer", ",", "train_path", ",", "dev_path", ",", "test_path", ",", "label_path", ",", "max_len", ")", ":", "\n", "        ", "self", ".", "train_path", ",", "self", ".", "dev_path", ",", "self", ".", "test_path", ",", "self", ".", "label_path", "=", "train_path", ",", "dev_path", ",", "test_path", ",", "label_path", "\n", "self", ".", "label_dict", ",", "self", ".", "id2label_dict", "=", "load_label_dict", "(", "label_path", ")", "\n", "self", ".", "num_class", "=", "len", "(", "self", ".", "label_dict", ")", "\n", "print", "(", "'number of tags is {}'", ".", "format", "(", "self", ".", "num_class", ")", ")", "\n", "self", ".", "max_len", "=", "max_len", "\n", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "unk_idx", ",", "self", ".", "sep_idx", ",", "self", ".", "pad_idx", ",", "self", ".", "cls_idx", ",", "self", ".", "mask_idx", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "UNK", ",", "SEP", ",", "PAD", ",", "CLS", ",", "MASK", "]", ")", "\n", "\n", "self", ".", "train_token_id_list", ",", "self", ".", "train_tag_id_list", "=", "self", ".", "process_file", "(", "train_path", ")", "\n", "self", ".", "dev_token_id_list", ",", "self", ".", "dev_tag_id_list", "=", "self", ".", "process_file", "(", "dev_path", ")", "\n", "self", ".", "test_token_id_list", ",", "self", ".", "test_tag_id_list", "=", "self", ".", "process_file", "(", "test_path", ")", "\n", "\n", "self", ".", "train_num", ",", "self", ".", "dev_num", ",", "self", ".", "test_num", "=", "len", "(", "self", ".", "train_token_id_list", ")", ",", "len", "(", "self", ".", "dev_token_id_list", ")", ",", "len", "(", "self", ".", "test_token_id_list", ")", "\n", "print", "(", "'training number is {}, dev number is {}, test_num is {}'", ".", "format", "(", "self", ".", "train_num", ",", "\n", "self", ".", "dev_num", ",", "self", ".", "test_num", ")", ")", "\n", "self", ".", "train_idx_list", "=", "[", "i", "for", "i", "in", "range", "(", "self", ".", "train_num", ")", "]", "\n", "random", ".", "shuffle", "(", "self", ".", "train_idx_list", ")", "\n", "self", ".", "dev_idx_list", "=", "[", "j", "for", "j", "in", "range", "(", "self", ".", "dev_num", ")", "]", "\n", "self", ".", "test_idx_list", "=", "[", "j", "for", "j", "in", "range", "(", "self", ".", "test_num", ")", "]", "\n", "self", ".", "dev_current_idx", ",", "self", ".", "test_current_idx", "=", "0", ",", "0", "\n", "\n", "max_train_seq_len", "=", "0", "\n", "for", "item", "in", "self", ".", "train_token_id_list", ":", "\n", "            ", "max_train_seq_len", "=", "max", "(", "len", "(", "item", ")", ",", "max_train_seq_len", ")", "\n", "", "max_dev_seq_len", "=", "0", "\n", "for", "item", "in", "self", ".", "dev_token_id_list", ":", "\n", "            ", "max_dev_seq_len", "=", "max", "(", "len", "(", "item", ")", ",", "max_dev_seq_len", ")", "\n", "", "max_test_seq_len", "=", "0", "\n", "for", "item", "in", "self", ".", "test_token_id_list", ":", "\n", "            ", "max_test_seq_len", "=", "max", "(", "len", "(", "item", ")", ",", "max_test_seq_len", ")", "\n", "", "print", "(", "'Maximum train sequence length: %d, dev sequence length: %d, test sequence length: %d'", "%", "(", "max_train_seq_len", ",", "max_dev_seq_len", ",", "max_test_seq_len", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.process_instance": [[57, 70], ["line.strip().split", "dataclass.Data.tokenizer.convert_tokens_to_ids", "len", "content_list[].split", "content_list[].split", "len", "len", "len", "len", "line.strip"], "methods", ["None"], ["", "def", "process_instance", "(", "self", ",", "line", ")", ":", "\n", "        ", "content_list", "=", "line", ".", "strip", "(", "'\\n'", ")", ".", "split", "(", "'\\t'", ")", "\n", "assert", "len", "(", "content_list", ")", "==", "2", "\n", "token_list", ",", "tag_name_list", "=", "content_list", "[", "0", "]", ".", "split", "(", ")", ",", "content_list", "[", "1", "]", ".", "split", "(", ")", "\n", "token_list", "=", "token_list", "[", ":", "self", ".", "max_len", "]", "\n", "tag_name_list", "=", "tag_name_list", "[", ":", "self", ".", "max_len", "]", "\n", "assert", "len", "(", "token_list", ")", "==", "len", "(", "tag_name_list", ")", "\n", "token_list", "=", "[", "CLS", "]", "+", "token_list", "+", "[", "SEP", "]", "\n", "tag_name_list", "=", "[", "'O'", "]", "+", "tag_name_list", "+", "[", "'O'", "]", "\n", "token_id_list", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "token_list", ")", "\n", "tag_list", "=", "[", "self", ".", "label_dict", "[", "token", "]", "for", "token", "in", "tag_name_list", "]", "\n", "assert", "len", "(", "token_id_list", ")", "==", "len", "(", "tag_list", ")", "\n", "return", "token_id_list", ",", "tag_list", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.process_file": [[71, 80], ["open", "i.readlines", "dataclass.Data.process_instance", "all_token_id.append", "all_tag_id.append"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.process_instance"], ["", "def", "process_file", "(", "self", ",", "path", ")", ":", "\n", "        ", "all_token_id", ",", "all_tag_id", "=", "[", "]", ",", "[", "]", "\n", "with", "open", "(", "path", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "i", ":", "\n", "            ", "lines", "=", "i", ".", "readlines", "(", ")", "\n", "for", "l", "in", "lines", ":", "\n", "                ", "one_token_id_list", ",", "one_tag_id_list", "=", "self", ".", "process_instance", "(", "l", ")", "\n", "all_token_id", ".", "append", "(", "one_token_id_list", ")", "\n", "all_tag_id", ".", "append", "(", "one_tag_id_list", ")", "\n", "", "", "return", "all_token_id", ",", "all_tag_id", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.process_input": [[81, 90], ["max", "torch.LongTensor().contiguous", "xs.append", "torch.LongTensor().contiguous.eq", "len", "torch.LongTensor", "len"], "methods", ["None"], ["", "def", "process_input", "(", "self", ",", "batch_inp", ")", ":", "\n", "        ", "max_len", "=", "max", "(", "[", "len", "(", "item", ")", "for", "item", "in", "batch_inp", "]", ")", "\n", "xs", "=", "[", "]", "\n", "for", "item", "in", "batch_inp", ":", "\n", "            ", "x", "=", "item", "+", "[", "self", ".", "pad_idx", "]", "*", "(", "max_len", "-", "len", "(", "item", ")", ")", "\n", "xs", ".", "append", "(", "x", ")", "\n", "", "src_tensor", "=", "torch", ".", "LongTensor", "(", "xs", ")", ".", "contiguous", "(", ")", "\n", "attn_mask", "=", "~", "src_tensor", ".", "eq", "(", "self", ".", "pad_idx", ")", "\n", "return", "src_tensor", ",", "attn_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.process_output": [[91, 103], ["max", "torch.LongTensor().contiguous", "torch.tensor().contiguous", "ys.append", "masks.append", "len", "torch.LongTensor", "torch.tensor", "len", "len", "len"], "methods", ["None"], ["", "def", "process_output", "(", "self", ",", "batch_out", ")", ":", "\n", "        ", "o_tag_id", "=", "self", ".", "label_dict", "[", "'O'", "]", "\n", "max_len", "=", "max", "(", "[", "len", "(", "item", ")", "for", "item", "in", "batch_out", "]", ")", "\n", "ys", ",", "masks", "=", "[", "]", ",", "[", "]", "\n", "for", "item", "in", "batch_out", ":", "\n", "            ", "y", "=", "item", "+", "[", "o_tag_id", "]", "*", "(", "max_len", "-", "len", "(", "item", ")", ")", "\n", "msk", "=", "[", "1.0", "]", "*", "len", "(", "item", ")", "+", "[", "0.0", "]", "*", "(", "max_len", "-", "len", "(", "item", ")", ")", "\n", "ys", ".", "append", "(", "y", ")", "\n", "masks", ".", "append", "(", "msk", ")", "\n", "", "tgt_tensor", "=", "torch", ".", "LongTensor", "(", "ys", ")", ".", "contiguous", "(", ")", "\n", "tgt_mask", "=", "torch", ".", "tensor", "(", "masks", ",", "dtype", "=", "torch", ".", "uint8", ")", ".", "contiguous", "(", ")", "\n", "return", "tgt_tensor", ",", "tgt_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.process_batch_data": [[104, 108], ["dataclass.Data.process_input", "dataclass.Data.process_output"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.process_input", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.process_output"], ["", "def", "process_batch_data", "(", "self", ",", "batch_inp", ",", "batch_out", ")", ":", "\n", "        ", "src_tensor", ",", "src_attn_mask", "=", "self", ".", "process_input", "(", "batch_inp", ")", "\n", "tgt_tensor", ",", "tgt_mask", "=", "self", ".", "process_output", "(", "batch_out", ")", "\n", "return", "src_tensor", ",", "src_attn_mask", ",", "tgt_tensor", ",", "tgt_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.get_next_train_batch": [[109, 118], ["random.sample", "dataclass.Data.process_batch_data", "batch_token_id_list.append", "batch_tag_id_list.append"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.process_batch_data"], ["", "def", "get_next_train_batch", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "batch_idx_list", "=", "random", ".", "sample", "(", "self", ".", "train_idx_list", ",", "batch_size", ")", "\n", "batch_token_id_list", ",", "batch_tag_id_list", "=", "[", "]", ",", "[", "]", "\n", "for", "idx", "in", "batch_idx_list", ":", "\n", "            ", "batch_token_id_list", ".", "append", "(", "self", ".", "train_token_id_list", "[", "idx", "]", ")", "\n", "batch_tag_id_list", ".", "append", "(", "self", ".", "train_tag_id_list", "[", "idx", "]", ")", "\n", "", "src_tensor", ",", "src_attn_mask", ",", "tgt_tensor", ",", "tgt_mask", "=", "self", ".", "process_batch_data", "(", "batch_token_id_list", ",", "batch_tag_id_list", ")", "\n", "return", "src_tensor", ",", "src_attn_mask", ",", "tgt_tensor", ",", "tgt_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.get_next_validation_batch": [[119, 159], ["dataclass.Data.process_batch_data", "range", "range", "Exception", "batch_token_id_list.append", "batch_tag_id_list.append", "batch_ref_id_list.append", "batch_token_id_list.append", "batch_tag_id_list.append", "batch_ref_id_list.append"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.process_batch_data"], ["", "def", "get_next_validation_batch", "(", "self", ",", "batch_size", ",", "mode", ")", ":", "\n", "        ", "batch_token_id_list", ",", "batch_tag_id_list", ",", "batch_ref_id_list", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "if", "mode", "==", "'dev'", ":", "\n", "            ", "curr_select_idx", ",", "instance_num", "=", "self", ".", "dev_current_idx", ",", "self", ".", "dev_num", "\n", "token_id_list", ",", "tag_id_list", "=", "self", ".", "dev_token_id_list", ",", "self", ".", "dev_tag_id_list", "\n", "", "elif", "mode", "==", "'test'", ":", "\n", "            ", "curr_select_idx", ",", "instance_num", "=", "self", ".", "test_current_idx", ",", "self", ".", "test_num", "\n", "token_id_list", ",", "tag_id_list", "=", "self", ".", "test_token_id_list", ",", "self", ".", "test_tag_id_list", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "'Wrong Validation Mode!!!'", ")", "\n", "\n", "", "if", "curr_select_idx", "+", "batch_size", "<", "instance_num", ":", "\n", "            ", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "curr_idx", "=", "curr_select_idx", "+", "i", "\n", "batch_token_id_list", ".", "append", "(", "token_id_list", "[", "curr_idx", "]", ")", "\n", "batch_tag_id_list", ".", "append", "(", "tag_id_list", "[", "curr_idx", "]", ")", "\n", "batch_ref_id_list", ".", "append", "(", "tag_id_list", "[", "curr_idx", "]", ")", "\n", "", "if", "mode", "==", "'dev'", ":", "\n", "                ", "self", ".", "dev_current_idx", "+=", "batch_size", "\n", "", "else", ":", "\n", "                ", "self", ".", "test_current_idx", "+=", "batch_size", "\n", "", "", "else", ":", "\n", "            ", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "curr_idx", "=", "curr_select_idx", "+", "i", "\n", "if", "curr_idx", ">", "instance_num", "-", "1", ":", "# \u5bf9dev_current_idx\u91cd\u65b0\u8d4b\u503c", "\n", "                    ", "curr_idx", "=", "0", "\n", "if", "mode", "==", "'dev'", ":", "\n", "                        ", "self", ".", "dev_current_idx", "=", "0", "\n", "", "else", ":", "\n", "                        ", "self", ".", "test_current_idx", "=", "0", "\n", "", "", "batch_token_id_list", ".", "append", "(", "token_id_list", "[", "curr_idx", "]", ")", "\n", "batch_tag_id_list", ".", "append", "(", "tag_id_list", "[", "curr_idx", "]", ")", "\n", "batch_ref_id_list", ".", "append", "(", "tag_id_list", "[", "curr_idx", "]", ")", "\n", "", "if", "mode", "==", "'dev'", ":", "\n", "                ", "self", ".", "dev_current_idx", "=", "0", "\n", "", "else", ":", "\n", "                ", "self", ".", "test_current_idx", "=", "0", "\n", "", "", "src_tensor", ",", "src_attn_mask", ",", "tgt_tensor", ",", "tgt_mask", "=", "self", ".", "process_batch_data", "(", "batch_token_id_list", ",", "batch_tag_id_list", ")", "\n", "return", "src_tensor", ",", "src_attn_mask", ",", "tgt_tensor", ",", "tgt_mask", ",", "batch_ref_id_list", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.convert_tag_id_to_name": [[160, 162], ["None"], "methods", ["None"], ["", "def", "convert_tag_id_to_name", "(", "self", ",", "tag_id_list", ")", ":", "\n", "        ", "return", "[", "self", ".", "id2label_dict", "[", "idx", "]", "for", "idx", "in", "tag_id_list", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.parse_result": [[163, 168], ["res.append", "dataclass.Data.convert_tag_id_to_name"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.convert_tag_id_to_name"], ["", "def", "parse_result", "(", "self", ",", "decode_list", ")", ":", "\n", "        ", "res", "=", "[", "]", "\n", "for", "y", "in", "decode_list", ":", "\n", "            ", "res", ".", "append", "(", "' '", ".", "join", "(", "self", ".", "convert_tag_id_to_name", "(", "y", "[", "1", ":", "-", "1", "]", ")", ")", ")", "# remove CLS and SEP", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.load_label_dict": [[6, 17], ["open", "i.readlines", "l.strip().split", "int", "l.strip"], "function", ["None"], ["def", "load_label_dict", "(", "label_path", ")", ":", "\n", "    ", "label_dict", ",", "id2label_dict", "=", "{", "}", ",", "{", "}", "\n", "with", "open", "(", "label_path", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "i", ":", "\n", "        ", "lines", "=", "i", ".", "readlines", "(", ")", "\n", "for", "l", "in", "lines", ":", "\n", "            ", "content_list", "=", "l", ".", "strip", "(", "'\\n'", ")", ".", "split", "(", ")", "\n", "label", "=", "content_list", "[", "0", "]", "\n", "label_id", "=", "int", "(", "content_list", "[", "1", "]", ")", "\n", "label_dict", "[", "label", "]", "=", "label_id", "\n", "id2label_dict", "[", "label_id", "]", "=", "label", "\n", "", "", "return", "label_dict", ",", "id2label_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.train.combine_result": [[10, 39], ["open", "len", "range", "open", "g.readlines", "open", "p.readlines", "len", "len", "gold_l.strip().split", "pred_l.strip().split", "pred_label_str.split", "len", "range", "o.writelines", "gold_label_str.split", "len", "len", "text.split", "o.writelines", "gold_l.strip", "pred_l.strip", "len"], "function", ["None"], ["def", "combine_result", "(", "gold_path", ",", "pred_path", ",", "out_path", ")", ":", "\n", "    ", "with", "open", "(", "out_path", ",", "'w'", ",", "encoding", "=", "'utf8'", ")", "as", "o", ":", "\n", "        ", "with", "open", "(", "gold_path", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "g", ":", "\n", "            ", "gold_lines", "=", "g", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "pred_path", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "p", ":", "\n", "            ", "pred_lines", "=", "p", ".", "readlines", "(", ")", "\n", "", "assert", "len", "(", "gold_lines", ")", "==", "len", "(", "pred_lines", ")", "\n", "data_num", "=", "len", "(", "gold_lines", ")", "\n", "for", "i", "in", "range", "(", "data_num", ")", ":", "\n", "            ", "gold_l", "=", "gold_lines", "[", "i", "]", "\n", "pred_l", "=", "pred_lines", "[", "i", "]", "\n", "gold_content_list", "=", "gold_l", ".", "strip", "(", "'\\n'", ")", ".", "split", "(", "'\\t'", ")", "\n", "text", "=", "gold_content_list", "[", "0", "]", "\n", "gold_label_str", "=", "gold_content_list", "[", "1", "]", "\n", "\n", "pred_l", "=", "pred_lines", "[", "i", "]", "\n", "pred_content_list", "=", "pred_l", ".", "strip", "(", "'\\n'", ")", ".", "split", "(", "'\\t'", ")", "\n", "pred_label_str", "=", "pred_content_list", "[", "1", "]", "\n", "\n", "pred_label_list", "=", "pred_label_str", ".", "split", "(", ")", "\n", "gold_label_list", "=", "gold_label_str", ".", "split", "(", ")", "[", ":", "len", "(", "pred_label_list", ")", "]", "# result truncation", "\n", "assert", "len", "(", "gold_label_list", ")", "==", "len", "(", "pred_label_list", ")", "\n", "\n", "instance_len", "=", "len", "(", "gold_label_list", ")", "\n", "text_list", "=", "text", ".", "split", "(", ")", "[", ":", "instance_len", "]", "\n", "for", "j", "in", "range", "(", "instance_len", ")", ":", "\n", "                ", "out_str", "=", "text_list", "[", "j", "]", "+", "' '", "+", "gold_label_list", "[", "j", "]", "+", "' '", "+", "pred_label_list", "[", "j", "]", "\n", "o", ".", "writelines", "(", "out_str", "+", "'\\n'", ")", "\n", "", "o", ".", "writelines", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.train.save_model": [[40, 64], ["torch.save", "torch.save", "os.listdir", "sorted", "os.path.exists", "os.mkdir", "torch.cuda.device_count", "torch.cuda.device_count", "fname.startswith", "fileData.items", "len", "range", "model.state_dict", "itemgetter", "len", "os.remove", "os.stat"], "function", ["None"], ["", "", "", "def", "save_model", "(", "model", ",", "save_path", ",", "save_name", ")", ":", "\n", "    ", "from", "operator", "import", "itemgetter", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "save_path", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "save_path", ")", "\n", "\n", "", "if", "torch", ".", "cuda", ".", "device_count", "(", ")", ">", "1", ":", "# multi-gpu training", "\n", "        ", "model", "=", "model", ".", "module", "\n", "\n", "", "model_save_path", "=", "save_path", "+", "'/'", "+", "save_name", "\n", "torch", ".", "save", "(", "{", "'model'", ":", "model", ".", "state_dict", "(", ")", "}", ",", "model_save_path", ")", "\n", "\n", "fileData", "=", "{", "}", "\n", "for", "fname", "in", "os", ".", "listdir", "(", "save_path", ")", ":", "\n", "        ", "if", "fname", ".", "startswith", "(", "'epoch'", ")", ":", "\n", "            ", "fileData", "[", "fname", "]", "=", "os", ".", "stat", "(", "save_path", "+", "'/'", "+", "fname", ")", ".", "st_mtime", "\n", "", "else", ":", "\n", "            ", "pass", "\n", "", "", "sortedFiles", "=", "sorted", "(", "fileData", ".", "items", "(", ")", ",", "key", "=", "itemgetter", "(", "1", ")", ")", "\n", "if", "len", "(", "sortedFiles", ")", "<", "1", ":", "\n", "        ", "pass", "\n", "", "else", ":", "\n", "        ", "delete", "=", "len", "(", "sortedFiles", ")", "-", "1", "\n", "for", "x", "in", "range", "(", "0", ",", "delete", ")", ":", "\n", "            ", "os", ".", "remove", "(", "save_path", "+", "'/'", "+", "sortedFiles", "[", "x", "]", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.train.evaluate_model": [[65, 115], ["torch.cuda.is_available", "torch.cuda.is_available", "torch.device", "torch.device", "train.combine_result", "metric_py3.fmeasure_from_singlefile", "os.remove", "os.remove", "torch.no_grad", "torch.no_grad", "model.eval", "range", "open", "torch.cuda.device_count", "torch.cuda.device_count", "int", "Exception", "data.get_next_validation_batch", "model.decode", "data.parse_result", "data.parse_result", "len", "range", "o.writelines", "int", "src_tensor.cuda.cuda", "src_attn_mask.cuda.cuda", "tgt_mask.cuda.cuda", "len", "len", "predictions[].split", "ref_predictions[].split"], "function", ["home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.train.combine_result", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.fmeasure_from_singlefile", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.get_next_validation_batch", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.model.NERModel.decode", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.parse_result", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.parse_result"], ["", "", "", "def", "evaluate_model", "(", "args", ",", "data", ",", "model", ",", "save_path", ",", "mode", ")", ":", "\n", "    ", "cuda_available", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "if", "cuda_available", ":", "\n", "        ", "if", "torch", ".", "cuda", ".", "device_count", "(", ")", ">", "1", ":", "# multi-gpu training ", "\n", "            ", "model", "=", "model", ".", "module", "\n", "", "else", ":", "# single gpu training", "\n", "            ", "pass", "\n", "", "", "else", ":", "\n", "        ", "pass", "\n", "\n", "", "device", "=", "torch", ".", "device", "(", "'cuda'", ")", "\n", "if", "mode", "==", "'dev'", ":", "\n", "        ", "eval_step_num", "=", "int", "(", "data", ".", "dev_num", "/", "args", ".", "batch_size", ")", "+", "1", "\n", "instance_num", "=", "data", ".", "dev_num", "\n", "gold_path", "=", "data", ".", "dev_path", "\n", "", "elif", "mode", "==", "'test'", ":", "\n", "        ", "eval_step_num", "=", "int", "(", "data", ".", "test_num", "/", "args", ".", "batch_size", ")", "+", "1", "\n", "instance_num", "=", "data", ".", "test_num", "\n", "gold_path", "=", "data", ".", "test_path", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "'Wrong Mode!!!'", ")", "\n", "\n", "", "res_list", "=", "[", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "for", "_", "in", "range", "(", "eval_step_num", ")", ":", "\n", "            ", "src_tensor", ",", "src_attn_mask", ",", "_", ",", "tgt_mask", ",", "tgt_ref_id_list", "=", "data", ".", "get_next_validation_batch", "(", "args", ".", "batch_size", ",", "mode", ")", "\n", "if", "cuda_available", ":", "\n", "                ", "src_tensor", "=", "src_tensor", ".", "cuda", "(", "device", ")", "\n", "src_attn_mask", "=", "src_attn_mask", ".", "cuda", "(", "device", ")", "\n", "tgt_mask", "=", "tgt_mask", ".", "cuda", "(", "device", ")", "\n", "", "predictions", "=", "model", ".", "decode", "(", "src_tensor", ",", "src_attn_mask", ",", "tgt_mask", ")", "\n", "predictions", "=", "data", ".", "parse_result", "(", "predictions", ")", "\n", "ref_predictions", "=", "data", ".", "parse_result", "(", "tgt_ref_id_list", ")", "\n", "bsz", "=", "len", "(", "tgt_ref_id_list", ")", "\n", "for", "idx", "in", "range", "(", "bsz", ")", ":", "\n", "                ", "assert", "len", "(", "predictions", "[", "idx", "]", ".", "split", "(", ")", ")", "==", "len", "(", "ref_predictions", "[", "idx", "]", ".", "split", "(", ")", ")", "\n", "", "res_list", "+=", "predictions", "\n", "", "res_list", "=", "res_list", "[", ":", "instance_num", "]", "\n", "", "eval_path", "=", "save_path", "+", "'/eval.txt'", "\n", "with", "open", "(", "eval_path", ",", "'w'", ",", "encoding", "=", "'utf8'", ")", "as", "o", ":", "\n", "        ", "for", "res", "in", "res_list", ":", "\n", "            ", "o", ".", "writelines", "(", "res", "+", "'\\t'", "+", "res", "+", "'\\n'", ")", "\n", "", "", "combine_path", "=", "save_path", "+", "'/'", "+", "mode", "+", "'_gold_eval_combine.txt'", "\n", "combine_result", "(", "gold_path", ",", "eval_path", ",", "combine_path", ")", "\n", "precision", ",", "recall", ",", "f1", "=", "fmeasure_from_singlefile", "(", "combine_path", ",", "args", ".", "evaluation_mode", ")", "\n", "os", ".", "remove", "(", "combine_path", ")", "\n", "os", ".", "remove", "(", "eval_path", ")", "\n", "return", "precision", ",", "recall", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.train.train_one_model": [[116, 233], ["os.path.exists", "torch.cuda.is_available", "torch.cuda.is_available", "torch.device", "torch.device", "BertTokenizer.from_pretrained", "print", "Data", "print", "print", "NERModel", "print", "nn.DataParallel.train", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam.zero_grad", "int", "range", "os.makedirs", "nn.DataParallel.to", "nn.DataParallel.parameters", "print", "print", "nn.DataParallel.train", "progressbar.ProgressBar", "progressbar.ProgressBar.start", "range", "progressbar.ProgressBar.finish", "nn.DataParallel.eval", "nn.DataParallel.train", "dev_f1_list.append", "test_f1_list.append", "print", "print", "print", "print", "max", "max", "torch.cuda.device_count", "torch.cuda.device_count", "print", "torch.nn.DataParallel", "int", "int", "int", "progressbar.ProgressBar.update", "Data.get_next_train_batch", "nn.DataParallel.", "loss.mean.mean", "loss.mean.item", "loss.mean.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.no_grad", "torch.no_grad", "train.evaluate_model", "train.evaluate_model", "round", "round", "train.save_model", "train_src_tensor.cuda.cuda", "train_src_attn_mask.cuda.cuda", "train_tgt_tensor.cuda.cuda", "train_tgt_mask.cuda.cuda", "nn.DataParallel.parameters", "torch.optim.Adam.step", "torch.optim.Adam.zero_grad", "round", "print", "print", "max", "max", "max", "max"], "function", ["home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.dataclass.Data.get_next_train_batch", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.train.evaluate_model", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.train.evaluate_model", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.bert_contrastive.BERTContrastivePretraining.save_model"], ["", "def", "train_one_model", "(", "args", ",", "model_name", ",", "run_number", ")", ":", "\n", "    ", "save_path", "=", "args", ".", "save_path_prefix", "+", "'/run_{}'", ".", "format", "(", "run_number", ")", "+", "'/'", "\n", "import", "os", "\n", "if", "os", ".", "path", ".", "exists", "(", "save_path", ")", ":", "\n", "        ", "pass", "\n", "", "else", ":", "# recursively construct directory", "\n", "        ", "os", ".", "makedirs", "(", "save_path", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "cuda_available", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "device", "=", "torch", ".", "device", "(", "'cuda'", ")", "\n", "\n", "from", "transformers", "import", "BertTokenizer", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "model_name", ")", "\n", "\n", "print", "(", "'Loading data...'", ")", "\n", "from", "dataclass", "import", "Data", "\n", "train_path", ",", "dev_path", ",", "test_path", ",", "label_path", "=", "args", ".", "train_path", ",", "args", ".", "dev_path", ",", "args", ".", "test_path", ",", "args", ".", "label_path", "\n", "data", "=", "Data", "(", "tokenizer", ",", "train_path", ",", "dev_path", ",", "test_path", ",", "label_path", ",", "args", ".", "max_len", ")", "\n", "print", "(", "'Data loaded.'", ")", "\n", "\n", "print", "(", "'Loading model...'", ")", "\n", "from", "model", "import", "NERModel", "\n", "model", "=", "NERModel", "(", "model_name", ",", "data", ".", "num_class", ")", "\n", "#if cuda_available:", "\n", "#    model = model.cuda(device)", "\n", "if", "cuda_available", ":", "\n", "        ", "if", "torch", ".", "cuda", ".", "device_count", "(", ")", ">", "1", ":", "# multi-gpu training ", "\n", "            ", "print", "(", "'Multi-GPU training...'", ")", "\n", "model", "=", "nn", ".", "DataParallel", "(", "model", ")", "\n", "", "else", ":", "# single gpu training", "\n", "            ", "pass", "\n", "", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "", "else", ":", "\n", "        ", "pass", "\n", "\n", "", "print", "(", "'Model loaded.'", ")", "\n", "model", ".", "train", "(", ")", "\n", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "args", ".", "learning_rate", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "batch_size", ",", "gradient_accumulation_steps", "=", "args", ".", "batch_size", ",", "args", ".", "gradient_accumulation_steps", "\n", "train_num", ",", "dev_num", ",", "test_num", "=", "data", ".", "train_num", ",", "data", ".", "dev_num", ",", "data", ".", "test_num", "\n", "train_step_num", ",", "dev_step_num", ",", "test_step_num", "=", "int", "(", "train_num", "/", "batch_size", ")", "+", "1", ",", "int", "(", "dev_num", "/", "batch_size", ")", "+", "1", ",", "int", "(", "test_num", "/", "batch_size", ")", "+", "1", "\n", "\n", "print_every", "=", "int", "(", "train_step_num", "/", "4", ")", "\n", "\n", "batches_processed", "=", "0", "\n", "loss_acm", "=", "0.", "\n", "max_combine_score", ",", "best_combine_str", "=", "0.", ",", "'best combine dev f1: {}, test f1: {}'", ".", "format", "(", "0.", ",", "0.", ")", "\n", "dev_f1_list", ",", "test_f1_list", "=", "[", "0.", "]", ",", "[", "0.", "]", "\n", "best_combined_score_dict", "=", "{", "'dev'", ":", "0.", ",", "'test'", ":", "0.", "}", "\n", "max_test_f1_score", "=", "0.", "\n", "for", "epoch_num", "in", "range", "(", "args", ".", "total_epochs", ")", ":", "\n", "        ", "print", "(", "'------------------------------------------------------------------'", ")", "\n", "print", "(", "'Start epoch {} training...'", ".", "format", "(", "epoch_num", ")", ")", "\n", "model", ".", "train", "(", ")", "\n", "p", "=", "progressbar", ".", "ProgressBar", "(", "train_step_num", ")", "\n", "p", ".", "start", "(", ")", "\n", "for", "train_step", "in", "range", "(", "train_step_num", ")", ":", "\n", "            ", "p", ".", "update", "(", "train_step", ")", "\n", "batches_processed", "+=", "1", "\n", "train_src_tensor", ",", "train_src_attn_mask", ",", "train_tgt_tensor", ",", "train_tgt_mask", "=", "data", ".", "get_next_train_batch", "(", "batch_size", ")", "\n", "if", "cuda_available", ":", "\n", "                ", "train_src_tensor", "=", "train_src_tensor", ".", "cuda", "(", "device", ")", "\n", "train_src_attn_mask", "=", "train_src_attn_mask", ".", "cuda", "(", "device", ")", "\n", "train_tgt_tensor", "=", "train_tgt_tensor", ".", "cuda", "(", "device", ")", "\n", "train_tgt_mask", "=", "train_tgt_mask", ".", "cuda", "(", "device", ")", "\n", "", "loss", "=", "model", "(", "train_src_tensor", ",", "train_src_attn_mask", ",", "train_tgt_tensor", ",", "train_tgt_mask", ")", "\n", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "loss_acm", "+=", "loss", ".", "item", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "\n", "if", "batches_processed", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "", "if", "batches_processed", "%", "print_every", "==", "0", ":", "\n", "                ", "one_loss", "=", "loss_acm", "/", "print_every", "\n", "one_loss", "=", "round", "(", "one_loss", ",", "3", ")", "\n", "print", "(", "\"epoch {}, batch {}, loss is {}\"", ".", "format", "(", "epoch_num", ",", "batches_processed", ",", "one_loss", ")", ")", "\n", "print", "(", "\"Batch %d, loss %.5f\"", "%", "(", "batches_processed", ",", "loss_acm", "/", "batches_processed", ")", ")", "\n", "loss_acm", "=", "0.", "\n", "", "", "p", ".", "finish", "(", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "_", ",", "_", ",", "dev_f1", "=", "evaluate_model", "(", "args", ",", "data", ",", "model", ",", "save_path", ",", "mode", "=", "'dev'", ")", "\n", "_", ",", "_", ",", "test_f1", "=", "evaluate_model", "(", "args", ",", "data", ",", "model", ",", "save_path", ",", "mode", "=", "'test'", ")", "\n", "", "model", ".", "train", "(", ")", "\n", "\n", "dev_f1", ",", "test_f1", "=", "dev_f1", "*", "100", ",", "test_f1", "*", "100", "\n", "dev_f1", ",", "test_f1", "=", "round", "(", "dev_f1", ",", "3", ")", ",", "round", "(", "test_f1", ",", "3", ")", "\n", "dev_f1_list", ".", "append", "(", "dev_f1", ")", "\n", "test_f1_list", ".", "append", "(", "test_f1", ")", "\n", "print", "(", "'At epoch {}, dev f1: {}, test f1: {}'", ".", "format", "(", "epoch_num", ",", "dev_f1", ",", "test_f1", ")", ")", "\n", "one_combine_score", "=", "dev_f1", "+", "test_f1", "\n", "if", "test_f1", ">", "max_test_f1_score", ":", "\n", "            ", "best_combine_str", "=", "'dev f1: {}, test f1: {}'", ".", "format", "(", "dev_f1", ",", "test_f1", ")", "\n", "best_combined_score_dict", "[", "'dev'", "]", "=", "dev_f1", "\n", "best_combined_score_dict", "[", "'test'", "]", "=", "test_f1", "\n", "max_dev_f1", ",", "max_test_f1", "=", "max", "(", "dev_f1_list", ")", ",", "max", "(", "test_f1_list", ")", "\n", "save_name", "=", "'epoch_{}_dev_f1_{}_test_f1_{}_max_dev_f1_{}_max_test_f1_{}'", ".", "format", "(", "epoch_num", ",", "\n", "dev_f1", ",", "test_f1", ",", "max_dev_f1", ",", "max_test_f1", ")", "\n", "save_model", "(", "model", ",", "save_path", ",", "save_name", ")", "\n", "max_combine_score", "=", "one_combine_score", "\n", "max_test_f1_score", "=", "test_f1", "\n", "\n", "", "print", "(", "'Current best combine result is '", "+", "best_combine_str", ")", "\n", "print", "(", "'Best dev f1: {}, test f1: {}'", ".", "format", "(", "max", "(", "dev_f1_list", ")", ",", "max", "(", "test_f1_list", ")", ")", ")", "\n", "print", "(", "'Epoch {} finished.'", ".", "format", "(", "epoch_num", ")", ")", "\n", "\n", "", "best_dev_f1", ",", "best_test_f1", "=", "max", "(", "dev_f1_list", ")", ",", "max", "(", "test_f1_list", ")", "\n", "best_combine_dev_f1", ",", "best_combine_test_f1", "=", "best_combined_score_dict", "[", "'dev'", "]", ",", "best_combined_score_dict", "[", "'test'", "]", "\n", "return", "best_combine_dev_f1", ",", "best_combine_test_f1", ",", "best_dev_f1", ",", "best_test_f1", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.train.compute_mean_std": [[235, 237], ["round", "round", "numpy.mean", "numpy.std"], "function", ["None"], ["def", "compute_mean_std", "(", "num_list", ")", ":", "\n", "    ", "return", "round", "(", "np", ".", "mean", "(", "num_list", ")", ",", "2", ")", ",", "round", "(", "np", ".", "std", "(", "num_list", ")", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.train.multiple_runs": [[238, 260], ["print", "print", "range", "train.compute_mean_std", "train.compute_mean_std", "train.compute_mean_std", "train.compute_mean_std", "print", "print", "train.train_one_model", "combine_dev_f1_list.append", "combine_test_f1_list.append", "best_dev_f1_list.append", "best_test_f1_list.append"], "function", ["home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.train.compute_mean_std", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.train.compute_mean_std", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.train.compute_mean_std", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.train.compute_mean_std", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.train.train_one_model"], ["", "def", "multiple_runs", "(", "args", ")", ":", "\n", "    ", "model_path", "=", "args", ".", "model_name", "\n", "print", "(", "'------------------------------------------'", ")", "\n", "print", "(", "'Evaluatiing model {}'", ".", "format", "(", "args", ".", "model_name", ")", ")", "\n", "combine_dev_f1_list", ",", "combine_test_f1_list", ",", "best_dev_f1_list", ",", "best_test_f1_list", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "run", "in", "range", "(", "args", ".", "number_of_runs", ")", ":", "\n", "        ", "print", "(", "'######'", ")", "\n", "print", "(", "'start run {}'", ".", "format", "(", "run", ")", ")", "\n", "one_best_combine_dev_f1", ",", "one_best_combine_test_f1", ",", "one_best_dev_f1", ",", "one_best_test_f1", "=", "train_one_model", "(", "args", ",", "model_path", ",", "run", ")", "\n", "combine_dev_f1_list", ".", "append", "(", "one_best_combine_dev_f1", ")", "\n", "combine_test_f1_list", ".", "append", "(", "one_best_combine_test_f1", ")", "\n", "best_dev_f1_list", ".", "append", "(", "one_best_dev_f1", ")", "\n", "best_test_f1_list", ".", "append", "(", "one_best_test_f1", ")", "\n", "\n", "", "combine_dev_f1_mean", ",", "combine_dev_f1_std", "=", "compute_mean_std", "(", "combine_dev_f1_list", ")", "\n", "combine_test_f1_mean", ",", "combine_test_f1_std", "=", "compute_mean_std", "(", "combine_test_f1_list", ")", "\n", "best_dev_f1_mean", ",", "best_dev_f1_std", "=", "compute_mean_std", "(", "best_dev_f1_list", ")", "\n", "best_test_f1_mean", ",", "best_test_f1_std", "=", "compute_mean_std", "(", "best_test_f1_list", ")", "\n", "overall_save_name", "=", "'overall_combine_dev_f1_mean_{}_std_{}_test_f1_mean_{}_std_{}_best_dev_f1_mean_{}_std_{}_test_f1_mean_{}_std_{}'", ".", "format", "(", "\n", "combine_dev_f1_mean", ",", "combine_dev_f1_std", ",", "combine_test_f1_mean", ",", "combine_test_f1_std", ",", "best_dev_f1_mean", ",", "best_dev_f1_std", ",", "\n", "best_test_f1_mean", ",", "best_test_f1_std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.train.parse_config": [[261, 281], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["", "def", "parse_config", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_name\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_path\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--dev_path\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--test_path\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--label_path\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_len\"", ",", "type", "=", "str", ",", "default", "=", "128", ")", "\n", "# learning configuration", "\n", "parser", ".", "add_argument", "(", "\"--number_of_runs\"", ",", "type", "=", "int", ",", "default", "=", "5", ",", "help", "=", "\"number of different experiment runs\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "type", "=", "float", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size_per_gpu\"", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--number_of_gpu\"", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--gradient_accumulation_steps\"", ",", "type", "=", "int", ",", "help", "=", "\"gradient accumulation step.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_grad_norm\"", ",", "default", "=", "1.0", ",", "type", "=", "float", ",", "help", "=", "\"Max gradient norm.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--total_epochs\"", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_path_prefix\"", ",", "type", "=", "str", ",", "help", "=", "\"directory to save the model evaluation results.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--evaluation_mode\"", ",", "type", "=", "str", ",", "default", "=", "\"BMES\"", ",", "help", "=", "\"BMES or BIO\"", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.model.NERModel.__init__": [[15, 24], ["torch.nn.Module.__init__", "TorchCRF.CRF", "transformers.BertModel.from_pretrained", "transformers.BertTokenizer.from_pretrained", "transformers.BertConfig", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "model_name", ",", "num_class", ")", ":", "\n", "        ", "super", "(", "NERModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_class", "=", "num_class", "\n", "self", ".", "crf", "=", "CRF", "(", "self", ".", "num_class", ")", "\n", "self", ".", "bert", "=", "BertModel", ".", "from_pretrained", "(", "model_name", ")", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "model_name", ")", "\n", "self", ".", "config", "=", "BertConfig", "(", "model_name", ")", "\n", "self", ".", "embed_dim", "=", "self", ".", "config", ".", "hidden_size", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "num_class", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.model.NERModel.compute_logits": [[25, 30], ["representation.size", "model.NERModel.fc().view", "model.NERModel.bert", "model.NERModel.fc", "representation.view"], "methods", ["None"], ["", "def", "compute_logits", "(", "self", ",", "src_tensor", ",", "src_attn_mask", ")", ":", "\n", "        ", "representation", "=", "self", ".", "bert", "(", "input_ids", "=", "src_tensor", ",", "attention_mask", "=", "src_attn_mask", ")", "[", "0", "]", "# bsz x seqlen x hidden_size", "\n", "bsz", ",", "seqlen", ",", "_", "=", "representation", ".", "size", "(", ")", "\n", "logits", "=", "self", ".", "fc", "(", "representation", ".", "view", "(", "bsz", "*", "seqlen", ",", "self", ".", "embed_dim", ")", ")", ".", "view", "(", "bsz", ",", "seqlen", ",", "self", ".", "num_class", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.model.NERModel.forward": [[31, 36], ["model.NERModel.compute_logits", "model.NERModel.crf.forward().mean", "model.NERModel.crf.forward"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.model.NERModel.compute_logits", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.bert_contrastive.BERTContrastivePretraining.forward"], ["", "def", "forward", "(", "self", ",", "src_tensor", ",", "src_attn_mask", ",", "tgt_tensor", ",", "tgt_mask", ")", ":", "\n", "        ", "logits", "=", "self", ".", "compute_logits", "(", "src_tensor", ",", "src_attn_mask", ")", "\n", "loglikelihood", "=", "self", ".", "crf", ".", "forward", "(", "logits", ",", "tgt_tensor", ",", "tgt_mask", ")", ".", "mean", "(", ")", "\n", "loss", "=", "-", "1", "*", "loglikelihood", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.model.NERModel.decode": [[37, 41], ["model.NERModel.compute_logits", "model.NERModel.crf.viterbi_decode"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.model.NERModel.compute_logits"], ["", "def", "decode", "(", "self", ",", "src_tensor", ",", "src_attn_mask", ",", "tgt_mask", ")", ":", "\n", "        ", "logits", "=", "self", ".", "compute_logits", "(", "src_tensor", ",", "src_attn_mask", ")", "\n", "res", "=", "self", ".", "crf", ".", "viterbi_decode", "(", "logits", ",", "tgt_mask", ")", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.get_ner_fmeasure": [[7, 56], ["len", "range", "len", "len", "len", "print", "print", "range", "len", "list", "len", "metric_py3.get_ner_BMES", "metric_py3.get_ner_BMES", "set().intersection", "metric_py3.get_ner_BIO", "metric_py3.get_ner_BIO", "Exception", "set", "set"], "function", ["home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.get_ner_BMES", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.get_ner_BMES", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.get_ner_BIO", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.get_ner_BIO"], ["def", "get_ner_fmeasure", "(", "golden_lists", ",", "predict_lists", ",", "label_type", "=", "\"BMES\"", ")", ":", "\n", "    ", "sent_num", "=", "len", "(", "golden_lists", ")", "\n", "golden_full", "=", "[", "]", "\n", "predict_full", "=", "[", "]", "\n", "right_full", "=", "[", "]", "\n", "right_tag", "=", "0", "\n", "all_tag", "=", "0", "\n", "for", "idx", "in", "range", "(", "0", ",", "sent_num", ")", ":", "\n", "# word_list = sentence_lists[idx]", "\n", "        ", "golden_list", "=", "golden_lists", "[", "idx", "]", "\n", "predict_list", "=", "predict_lists", "[", "idx", "]", "\n", "for", "idy", "in", "range", "(", "len", "(", "golden_list", ")", ")", ":", "\n", "            ", "if", "golden_list", "[", "idy", "]", "==", "predict_list", "[", "idy", "]", ":", "\n", "                ", "right_tag", "+=", "1", "\n", "", "", "all_tag", "+=", "len", "(", "golden_list", ")", "\n", "if", "label_type", "==", "\"BMES\"", ":", "\n", "            ", "gold_matrix", "=", "get_ner_BMES", "(", "golden_list", ")", "\n", "pred_matrix", "=", "get_ner_BMES", "(", "predict_list", ")", "\n", "", "elif", "label_type", "==", "\"BIO\"", ":", "\n", "            ", "gold_matrix", "=", "get_ner_BIO", "(", "golden_list", ")", "\n", "pred_matrix", "=", "get_ner_BIO", "(", "predict_list", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "'Wrong NER evaluation mode!!!'", ")", "\n", "# print \"gold\", gold_matrix", "\n", "# print \"pred\", pred_matrix", "\n", "", "right_ner", "=", "list", "(", "set", "(", "gold_matrix", ")", ".", "intersection", "(", "set", "(", "pred_matrix", ")", ")", ")", "\n", "golden_full", "+=", "gold_matrix", "\n", "predict_full", "+=", "pred_matrix", "\n", "right_full", "+=", "right_ner", "\n", "", "right_num", "=", "len", "(", "right_full", ")", "\n", "golden_num", "=", "len", "(", "golden_full", ")", "\n", "predict_num", "=", "len", "(", "predict_full", ")", "\n", "if", "predict_num", "==", "0", ":", "\n", "        ", "precision", "=", "-", "1", "\n", "", "else", ":", "\n", "        ", "precision", "=", "(", "right_num", "+", "0.0", ")", "/", "predict_num", "\n", "", "if", "golden_num", "==", "0", ":", "\n", "        ", "recall", "=", "-", "1", "\n", "", "else", ":", "\n", "        ", "recall", "=", "(", "right_num", "+", "0.0", ")", "/", "golden_num", "\n", "", "if", "(", "precision", "==", "-", "1", ")", "or", "(", "recall", "==", "-", "1", ")", "or", "(", "precision", "+", "recall", ")", "<=", "0.", ":", "\n", "        ", "f_measure", "=", "-", "1", "\n", "", "else", ":", "\n", "        ", "f_measure", "=", "2", "*", "precision", "*", "recall", "/", "(", "precision", "+", "recall", ")", "\n", "", "accuracy", "=", "(", "right_tag", "+", "0.0", ")", "/", "all_tag", "\n", "# print \"Accuracy: \", right_tag,\"/\",all_tag,\"=\",accuracy", "\n", "print", "(", "\"gold_num = \"", ",", "golden_num", ",", "\" pred_num = \"", ",", "predict_num", ",", "\" right_num = \"", ",", "right_num", ")", "\n", "print", "(", "precision", ",", "recall", ",", "f_measure", ")", "\n", "return", "accuracy", ",", "precision", ",", "recall", ",", "f_measure", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.reverse_style": [[58, 63], ["input_string.index", "len"], "function", ["None"], ["", "def", "reverse_style", "(", "input_string", ")", ":", "\n", "    ", "target_position", "=", "input_string", ".", "index", "(", "'['", ")", "\n", "input_len", "=", "len", "(", "input_string", ")", "\n", "output_string", "=", "input_string", "[", "target_position", ":", "input_len", "]", "+", "input_string", "[", "0", ":", "target_position", "]", "\n", "return", "output_string", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.get_ner_BMES": [[65, 110], ["len", "range", "len", "range", "label_list[].upper", "tag_list.append", "label_list[].upper.replace", "len", "metric_py3.reverse_style", "stand_matrix.append", "tag_list.append", "str", "tag_list.append", "label_list[].upper.replace", "tag_list.append", "str", "str", "label_list[].upper.replace", "tag_list.append", "str", "str"], "function", ["home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.reverse_style"], ["", "def", "get_ner_BMES", "(", "label_list", ")", ":", "\n", "# list_len = len(word_list)", "\n", "# assert(list_len == len(label_list)), \"word list size unmatch with label list\"", "\n", "    ", "list_len", "=", "len", "(", "label_list", ")", "\n", "begin_label", "=", "'B-'", "\n", "end_label", "=", "'E-'", "\n", "single_label", "=", "'S-'", "\n", "whole_tag", "=", "''", "\n", "index_tag", "=", "''", "\n", "tag_list", "=", "[", "]", "\n", "stand_matrix", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "list_len", ")", ":", "\n", "# wordlabel = word_list[i]", "\n", "        ", "current_label", "=", "label_list", "[", "i", "]", ".", "upper", "(", ")", "\n", "if", "begin_label", "in", "current_label", ":", "\n", "            ", "if", "index_tag", "!=", "''", ":", "\n", "                ", "tag_list", ".", "append", "(", "whole_tag", "+", "','", "+", "str", "(", "i", "-", "1", ")", ")", "\n", "", "whole_tag", "=", "current_label", ".", "replace", "(", "begin_label", ",", "\"\"", ",", "1", ")", "+", "'['", "+", "str", "(", "i", ")", "\n", "index_tag", "=", "current_label", ".", "replace", "(", "begin_label", ",", "\"\"", ",", "1", ")", "\n", "\n", "", "elif", "single_label", "in", "current_label", ":", "\n", "            ", "if", "index_tag", "!=", "''", ":", "\n", "                ", "tag_list", ".", "append", "(", "whole_tag", "+", "','", "+", "str", "(", "i", "-", "1", ")", ")", "\n", "", "whole_tag", "=", "current_label", ".", "replace", "(", "single_label", ",", "\"\"", ",", "1", ")", "+", "'['", "+", "str", "(", "i", ")", "\n", "tag_list", ".", "append", "(", "whole_tag", ")", "\n", "whole_tag", "=", "\"\"", "\n", "index_tag", "=", "\"\"", "\n", "", "elif", "end_label", "in", "current_label", ":", "\n", "            ", "if", "index_tag", "!=", "''", ":", "\n", "                ", "tag_list", ".", "append", "(", "whole_tag", "+", "','", "+", "str", "(", "i", ")", ")", "\n", "", "whole_tag", "=", "''", "\n", "index_tag", "=", "''", "\n", "", "else", ":", "\n", "            ", "continue", "\n", "", "", "if", "(", "whole_tag", "!=", "''", ")", "&", "(", "index_tag", "!=", "''", ")", ":", "\n", "        ", "tag_list", ".", "append", "(", "whole_tag", ")", "\n", "", "tag_list_len", "=", "len", "(", "tag_list", ")", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "tag_list_len", ")", ":", "\n", "        ", "if", "len", "(", "tag_list", "[", "i", "]", ")", ">", "0", ":", "\n", "            ", "tag_list", "[", "i", "]", "=", "tag_list", "[", "i", "]", "+", "']'", "\n", "insert_list", "=", "reverse_style", "(", "tag_list", "[", "i", "]", ")", "\n", "stand_matrix", ".", "append", "(", "insert_list", ")", "\n", "# print stand_matrix", "\n", "", "", "return", "stand_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.get_ner_BIO": [[112, 158], ["len", "range", "len", "range", "label_list[].upper", "tag_list.append", "len", "metric_py3.reverse_style", "stand_matrix.append", "label_list[].upper.replace", "tag_list.append", "label_list[].upper.replace", "str", "str", "label_list[].upper.replace", "tag_list.append", "label_list[].upper.replace", "str", "label_list[].upper.replace", "tag_list.append", "str", "str"], "function", ["home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.reverse_style"], ["", "def", "get_ner_BIO", "(", "label_list", ")", ":", "\n", "# list_len = len(word_list)", "\n", "# assert(list_len == len(label_list)), \"word list size unmatch with label list\"", "\n", "    ", "list_len", "=", "len", "(", "label_list", ")", "\n", "begin_label", "=", "'B-'", "\n", "inside_label", "=", "'I-'", "\n", "whole_tag", "=", "''", "\n", "index_tag", "=", "''", "\n", "tag_list", "=", "[", "]", "\n", "stand_matrix", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "list_len", ")", ":", "\n", "# wordlabel = word_list[i]", "\n", "        ", "current_label", "=", "label_list", "[", "i", "]", ".", "upper", "(", ")", "\n", "if", "begin_label", "in", "current_label", ":", "\n", "            ", "if", "index_tag", "==", "''", ":", "\n", "                ", "whole_tag", "=", "current_label", ".", "replace", "(", "begin_label", ",", "\"\"", ",", "1", ")", "+", "'['", "+", "str", "(", "i", ")", "\n", "index_tag", "=", "current_label", ".", "replace", "(", "begin_label", ",", "\"\"", ",", "1", ")", "\n", "", "else", ":", "\n", "                ", "tag_list", ".", "append", "(", "whole_tag", "+", "','", "+", "str", "(", "i", "-", "1", ")", ")", "\n", "whole_tag", "=", "current_label", ".", "replace", "(", "begin_label", ",", "\"\"", ",", "1", ")", "+", "'['", "+", "str", "(", "i", ")", "\n", "index_tag", "=", "current_label", ".", "replace", "(", "begin_label", ",", "\"\"", ",", "1", ")", "\n", "\n", "", "", "elif", "inside_label", "in", "current_label", ":", "\n", "            ", "if", "current_label", ".", "replace", "(", "inside_label", ",", "\"\"", ",", "1", ")", "==", "index_tag", ":", "\n", "                ", "whole_tag", "=", "whole_tag", "\n", "", "else", ":", "\n", "                ", "if", "(", "whole_tag", "!=", "''", ")", "&", "(", "index_tag", "!=", "''", ")", ":", "\n", "                    ", "tag_list", ".", "append", "(", "whole_tag", "+", "','", "+", "str", "(", "i", "-", "1", ")", ")", "\n", "", "whole_tag", "=", "''", "\n", "index_tag", "=", "''", "\n", "", "", "else", ":", "\n", "            ", "if", "(", "whole_tag", "!=", "''", ")", "&", "(", "index_tag", "!=", "''", ")", ":", "\n", "                ", "tag_list", ".", "append", "(", "whole_tag", "+", "','", "+", "str", "(", "i", "-", "1", ")", ")", "\n", "", "whole_tag", "=", "''", "\n", "index_tag", "=", "''", "\n", "\n", "", "", "if", "(", "whole_tag", "!=", "''", ")", "&", "(", "index_tag", "!=", "''", ")", ":", "\n", "        ", "tag_list", ".", "append", "(", "whole_tag", ")", "\n", "", "tag_list_len", "=", "len", "(", "tag_list", ")", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "tag_list_len", ")", ":", "\n", "        ", "if", "len", "(", "tag_list", "[", "i", "]", ")", ">", "0", ":", "\n", "            ", "tag_list", "[", "i", "]", "=", "tag_list", "[", "i", "]", "+", "']'", "\n", "insert_list", "=", "reverse_style", "(", "tag_list", "[", "i", "]", ")", "\n", "stand_matrix", ".", "append", "(", "insert_list", ")", "\n", "", "", "return", "stand_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.readSentence": [[161, 179], ["open", "i.readlines", "len", "sentences.append", "labels.append", "line.strip().split", "sentence.append", "label.append", "line.strip"], "function", ["None"], ["", "def", "readSentence", "(", "input_file", ")", ":", "\n", "    ", "with", "open", "(", "input_file", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "i", ":", "\n", "        ", "in_lines", "=", "i", ".", "readlines", "(", ")", "\n", "", "sentences", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "sentence", "=", "[", "]", "\n", "label", "=", "[", "]", "\n", "for", "line", "in", "in_lines", ":", "\n", "        ", "if", "len", "(", "line", ")", "<", "2", ":", "\n", "            ", "sentences", ".", "append", "(", "sentence", ")", "\n", "labels", ".", "append", "(", "label", ")", "\n", "sentence", "=", "[", "]", "\n", "label", "=", "[", "]", "\n", "", "else", ":", "\n", "            ", "pair", "=", "line", ".", "strip", "(", "'\\n'", ")", ".", "split", "(", "' '", ")", "\n", "sentence", ".", "append", "(", "pair", "[", "0", "]", ")", "\n", "label", ".", "append", "(", "pair", "[", "-", "1", "]", ")", "\n", "", "", "return", "sentences", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.readTwoLabelSentence": [[181, 207], ["open", "i.readlines", "len", "sentences.append", "golden_labels.append", "predict_labels.append", "line.strip().split", "sentence.append", "golden_label.append", "predict_label.append", "line.strip"], "function", ["None"], ["", "def", "readTwoLabelSentence", "(", "input_file", ",", "pred_col", "=", "-", "1", ")", ":", "\n", "    ", "with", "open", "(", "input_file", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "i", ":", "\n", "        ", "in_lines", "=", "i", ".", "readlines", "(", ")", "\n", "", "sentences", "=", "[", "]", "\n", "predict_labels", "=", "[", "]", "\n", "golden_labels", "=", "[", "]", "\n", "sentence", "=", "[", "]", "\n", "predict_label", "=", "[", "]", "\n", "golden_label", "=", "[", "]", "\n", "for", "line", "in", "in_lines", ":", "\n", "        ", "if", "\"##score##\"", "in", "line", ":", "\n", "            ", "continue", "\n", "", "if", "len", "(", "line", ")", "<", "2", ":", "\n", "            ", "sentences", ".", "append", "(", "sentence", ")", "\n", "golden_labels", ".", "append", "(", "golden_label", ")", "\n", "predict_labels", ".", "append", "(", "predict_label", ")", "\n", "sentence", "=", "[", "]", "\n", "golden_label", "=", "[", "]", "\n", "predict_label", "=", "[", "]", "\n", "", "else", ":", "\n", "            ", "pair", "=", "line", ".", "strip", "(", "'\\n'", ")", ".", "split", "(", "' '", ")", "\n", "sentence", ".", "append", "(", "pair", "[", "0", "]", ")", "\n", "golden_label", ".", "append", "(", "pair", "[", "1", "]", ")", "\n", "predict_label", ".", "append", "(", "pair", "[", "pred_col", "]", ")", "\n", "\n", "", "", "return", "sentences", ",", "golden_labels", ",", "predict_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.fmeasure_from_file": [[209, 216], ["print", "print", "metric_py3.readSentence", "metric_py3.readSentence", "metric_py3.get_ner_fmeasure", "print"], "function", ["home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.readSentence", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.readSentence", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.get_ner_fmeasure"], ["", "def", "fmeasure_from_file", "(", "golden_file", ",", "predict_file", ",", "label_type", "=", "\"BMES\"", ")", ":", "\n", "    ", "print", "(", "\"Get f measure from file:\"", ",", "golden_file", ",", "predict_file", ")", "\n", "print", "(", "\"Label format:\"", ",", "label_type", ")", "\n", "golden_sent", ",", "golden_labels", "=", "readSentence", "(", "golden_file", ")", "\n", "predict_sent", ",", "predict_labels", "=", "readSentence", "(", "predict_file", ")", "\n", "acc", ",", "P", ",", "R", ",", "F", "=", "get_ner_fmeasure", "(", "golden_labels", ",", "predict_labels", ",", "label_type", ")", "\n", "print", "(", "\"Acc:%s, P:%s R:%s, F:%s\"", "%", "(", "acc", ",", "P", ",", "R", ",", "F", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.fmeasure_from_singlefile": [[219, 224], ["metric_py3.readTwoLabelSentence", "metric_py3.get_ner_fmeasure"], "function", ["home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.readTwoLabelSentence", "home.repos.pwc.inspect_result.yxuansu_tacl.chinese_benchmark.metric_py3.get_ner_fmeasure"], ["", "def", "fmeasure_from_singlefile", "(", "twolabel_file", ",", "label_type", "=", "\"BMES\"", ",", "pred_col", "=", "-", "1", ")", ":", "\n", "    ", "sent", ",", "golden_labels", ",", "predict_labels", "=", "readTwoLabelSentence", "(", "twolabel_file", ",", "pred_col", ")", "\n", "A", ",", "P", ",", "R", ",", "F", "=", "get_ner_fmeasure", "(", "golden_labels", ",", "predict_labels", ",", "label_type", ")", "\n", "#print (\"P:%s, R:%s, F:%s\"%(P,R,F))", "\n", "return", "P", ",", "R", ",", "F", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.analysis.plot_result.process_result": [[2, 26], ["print", "range", "list", "range", "open", "json.load", "len", "json.load.keys", "range", "res_list.append", "str", "round"], "function", ["None"], ["def", "process_result", "(", "in_f", ")", ":", "\n", "    ", "with", "open", "(", "in_f", ")", "as", "f", ":", "\n", "        ", "res_dict", "=", "json", ".", "load", "(", "f", ")", "\n", "", "print", "(", "len", "(", "res_dict", ")", ")", "\n", "\n", "layer_res", "=", "{", "}", "\n", "for", "idx", "in", "range", "(", "1", ",", "13", ")", ":", "\n", "        ", "layer_res", "[", "idx", "]", "=", "{", "'cosine_sum'", ":", "0.", ",", "'token_sum'", ":", "0", "}", "\n", "\n", "", "key_list", "=", "list", "(", "res_dict", ".", "keys", "(", ")", ")", "\n", "for", "key", "in", "key_list", ":", "\n", "        ", "instance", "=", "res_dict", "[", "key", "]", "\n", "for", "idx", "in", "range", "(", "1", ",", "13", ")", ":", "\n", "            ", "key", "=", "str", "(", "idx", ")", "\n", "one_cosine_sum", "=", "instance", "[", "key", "]", "[", "'cosine_sum'", "]", "\n", "one_token_sum", "=", "instance", "[", "key", "]", "[", "'token_sum'", "]", "\n", "layer_res", "[", "idx", "]", "[", "'cosine_sum'", "]", "+=", "one_cosine_sum", "\n", "layer_res", "[", "idx", "]", "[", "'token_sum'", "]", "+=", "one_token_sum", "\n", "\n", "", "", "res_list", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "1", ",", "13", ")", ":", "\n", "        ", "one_cross_similarity", "=", "layer_res", "[", "idx", "]", "[", "'cosine_sum'", "]", "/", "layer_res", "[", "idx", "]", "[", "'token_sum'", "]", "\n", "res_list", ".", "append", "(", "round", "(", "one_cross_similarity", ",", "3", ")", ")", "\n", "", "return", "res_list", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.analysis.layerwise_intra_sentence_similarity.text_to_id": [[2, 10], ["tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "torch.LongTensor().view", "torch.LongTensor().view", "tokens_id.cuda.cuda", "torch.LongTensor", "torch.LongTensor"], "function", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.BasicTokenizer.tokenize"], ["def", "text_to_id", "(", "text", ",", "tokenizer", ",", "is_cuda", ",", "device", ")", ":", "\n", "    ", "text", "=", "'[CLS] '", "+", "text", "+", "' [SEP]'", "\n", "tokens", "=", "tokenizer", ".", "tokenize", "(", "text", ")", "\n", "tokens_id", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "tokens_id", "=", "torch", ".", "LongTensor", "(", "tokens_id", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "if", "is_cuda", ":", "\n", "        ", "tokens_id", "=", "tokens_id", ".", "cuda", "(", "device", ")", "\n", "", "return", "tokens_id", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.analysis.layerwise_intra_sentence_similarity.get_diag_mask_matrix": [[12, 16], ["numpy.zeros", "numpy.fill_diagonal"], "function", ["None"], ["def", "get_diag_mask_matrix", "(", "seqlen", ")", ":", "\n", "    ", "a", "=", "np", ".", "zeros", "(", "(", "seqlen", ",", "seqlen", ")", ",", "float", ")", "\n", "np", ".", "fill_diagonal", "(", "a", ",", "1.0", ")", "\n", "return", "1.0", "-", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.analysis.layerwise_intra_sentence_similarity.compute_cosine_correlation_matrix": [[18, 25], ["hidden.size", "torch.matmul().view", "torch.matmul().view", "correlation_matrix.detach().cpu().numpy.detach().cpu().numpy", "hidden.norm", "torch.matmul", "torch.matmul", "correlation_matrix.detach().cpu().numpy.detach().cpu", "norm_hidden.transpose", "correlation_matrix.detach().cpu().numpy.detach"], "function", ["None"], ["def", "compute_cosine_correlation_matrix", "(", "hidden", ")", ":", "\n", "# hidden: 1 x seqlen x embed_dim", "\n", "    ", "_", ",", "seq_len", ",", "_", "=", "hidden", ".", "size", "(", ")", "\n", "norm_hidden", "=", "hidden", "/", "hidden", ".", "norm", "(", "dim", "=", "2", ",", "keepdim", "=", "True", ")", "# normalize vectors to unit norm", "\n", "correlation_matrix", "=", "torch", ".", "matmul", "(", "norm_hidden", ",", "norm_hidden", ".", "transpose", "(", "1", ",", "2", ")", ")", ".", "view", "(", "seq_len", ",", "seq_len", ")", "\n", "correlation_matrix", "=", "correlation_matrix", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "return", "correlation_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.analysis.layerwise_intra_sentence_similarity.compute_stat": [[26, 35], ["layerwise_intra_sentence_similarity.compute_cosine_correlation_matrix", "layerwise_intra_sentence_similarity.get_diag_mask_matrix", "numpy.sum"], "function", ["home.repos.pwc.inspect_result.yxuansu_tacl.analysis.layerwise_intra_sentence_similarity.compute_cosine_correlation_matrix", "home.repos.pwc.inspect_result.yxuansu_tacl.analysis.layerwise_intra_sentence_similarity.get_diag_mask_matrix"], ["", "def", "compute_stat", "(", "hidden", ")", ":", "\n", "    ", "cosine_correlation_matrix", "=", "compute_cosine_correlation_matrix", "(", "hidden", ")", "\n", "seq_len", "=", "cosine_correlation_matrix", ".", "shape", "[", "0", "]", "\n", "mask_matrix", "=", "get_diag_mask_matrix", "(", "seq_len", ")", "\n", "assert", "mask_matrix", ".", "shape", "==", "cosine_correlation_matrix", ".", "shape", "\n", "cosine_masked_matrix", "=", "cosine_correlation_matrix", "*", "mask_matrix", "# mask out the diagonal elements", "\n", "sum_cosine", "=", "np", ".", "sum", "(", "cosine_masked_matrix", ")", "/", "2", "\n", "element_num", "=", "seq_len", "*", "(", "seq_len", "-", "1", ")", "/", "2", "\n", "return", "sum_cosine", ",", "element_num", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.analysis.layerwise_intra_sentence_similarity.process_one_instance": [[36, 49], ["range", "layerwise_intra_sentence_similarity.text_to_id", "model", "range", "layerwise_intra_sentence_similarity.compute_stat"], "function", ["home.repos.pwc.inspect_result.yxuansu_tacl.analysis.layerwise_intra_sentence_similarity.text_to_id", "home.repos.pwc.inspect_result.yxuansu_tacl.analysis.layerwise_intra_sentence_similarity.compute_stat"], ["", "def", "process_one_instance", "(", "model", ",", "tokenizer", ",", "text", ",", "is_cuda", ",", "device", ")", ":", "\n", "    ", "res_dict", "=", "{", "}", "\n", "for", "idx", "in", "range", "(", "1", ",", "13", ")", ":", "\n", "        ", "res_dict", "[", "idx", "]", "=", "{", "}", "\n", "\n", "", "input_id", "=", "text_to_id", "(", "text", ",", "tokenizer", ",", "is_cuda", ",", "device", ")", "\n", "outputs", "=", "model", "(", "input_id", ",", "output_hidden_states", "=", "True", ")", "\n", "attention_hidden_states", "=", "outputs", "[", "-", "1", "]", "\n", "for", "idx", "in", "range", "(", "1", ",", "13", ")", ":", "\n", "        ", "one_cosine_sum", ",", "one_token_num", "=", "compute_stat", "(", "attention_hidden_states", "[", "idx", "]", ")", "\n", "res_dict", "[", "idx", "]", "[", "'cosine_sum'", "]", "=", "one_cosine_sum", "\n", "res_dict", "[", "idx", "]", "[", "'token_sum'", "]", "=", "one_token_num", "\n", "", "return", "res_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.analysis.layerwise_intra_sentence_similarity.load_model_tokenizer": [[51, 55], ["transformers.AutoModel.from_pretrained", "transformers.AutoTokenizer.from_pretrained"], "function", ["None"], ["def", "load_model_tokenizer", "(", "model_name", ")", ":", "\n", "    ", "model", "=", "AutoModel", ".", "from_pretrained", "(", "model_name", ")", "\n", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_name", ")", "\n", "return", "model", ",", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.analysis.layerwise_intra_sentence_similarity.parse_config": [[57, 63], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "parse_config", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_name\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--file_path\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_path\"", ",", "type", "=", "str", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.analysis.plot_self_similarity_matrix.compute_correlation_matrix": [[7, 16], ["tokenizer.tokenize", "torch.LongTensor().view", "torch.LongTensor().view", "torch.LongTensor().view.size", "torch.matmul().view", "torch.matmul().view", "model", "hidden.norm", "torch.matmul().view.detach().numpy", "text.strip", "torch.LongTensor", "torch.LongTensor", "torch.matmul", "torch.matmul", "tokenizer.convert_tokens_to_ids", "norm_hidden.transpose", "torch.matmul().view.detach"], "function", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.BasicTokenizer.tokenize"], ["def", "compute_correlation_matrix", "(", "model", ",", "tokenizer", ",", "text", ")", ":", "\n", "    ", "text", "=", "'[CLS] '", "+", "text", ".", "strip", "(", "'\\n'", ")", "+", "' [SEP]'", "\n", "token_list", "=", "tokenizer", ".", "tokenize", "(", "text", ")", "\n", "input_ids", "=", "torch", ".", "LongTensor", "(", "tokenizer", ".", "convert_tokens_to_ids", "(", "token_list", ")", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "_", ",", "seq_len", "=", "input_ids", ".", "size", "(", ")", "\n", "hidden", "=", "model", "(", "input_ids", ")", ".", "last_hidden_state", "\n", "norm_hidden", "=", "hidden", "/", "hidden", ".", "norm", "(", "dim", "=", "2", ",", "keepdim", "=", "True", ")", "\n", "correlation_matrix", "=", "torch", ".", "matmul", "(", "norm_hidden", ",", "norm_hidden", ".", "transpose", "(", "1", ",", "2", ")", ")", ".", "view", "(", "seq_len", ",", "seq_len", ")", "\n", "return", "correlation_matrix", ".", "detach", "(", ")", ".", "numpy", "(", ")", ",", "token_list", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.analysis.plot_self_similarity_matrix.load_model": [[18, 23], ["transformers.AutoModel.from_pretrained", "AutoModel.from_pretrained.eval", "transformers.AutoTokenizer.from_pretrained"], "function", ["None"], ["def", "load_model", "(", "model_name", ")", ":", "\n", "    ", "model", "=", "AutoModel", ".", "from_pretrained", "(", "model_name", ")", "\n", "model", ".", "eval", "(", ")", "\n", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_name", ")", "\n", "return", "model", ",", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.bert_contrastive.BERTContrastivePretraining.__init__": [[37, 72], ["torch.nn.Module.__init__", "transformers.BertTokenizer.from_pretrained", "transformers.BertForPreTraining.from_pretrained", "transformers.BertConfig.from_pretrained", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "print", "print", "print", "transformers.BertModel.from_pretrained", "bert_contrastive.BERTContrastivePretraining.teacher_bert.parameters", "print", "print", "Exception"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.__init__"], ["    ", "def", "__init__", "(", "self", ",", "model_name", ",", "sim", "=", "'cosine'", ",", "temperature", "=", "0.01", ",", "use_contrastive_loss", "=", "'True'", ")", ":", "\n", "        ", "super", "(", "BERTContrastivePretraining", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "model_name", ")", "\n", "self", ".", "vocab_size", "=", "self", ".", "tokenizer", ".", "vocab_size", "\n", "self", ".", "model", "=", "BertForPreTraining", ".", "from_pretrained", "(", "model_name", ")", "\n", "self", ".", "bert", "=", "self", ".", "model", ".", "bert", "\n", "self", ".", "cls", "=", "self", ".", "model", ".", "cls", "\n", "self", ".", "config", "=", "BertConfig", ".", "from_pretrained", "(", "model_name", ")", "\n", "embed_dim", "=", "self", ".", "config", ".", "hidden_size", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "100", ")", "\n", "assert", "sim", "in", "[", "'dot_product'", ",", "'cosine'", "]", "\n", "self", ".", "sim", "=", "sim", "\n", "if", "self", ".", "sim", "==", "'dot_product'", ":", "\n", "            ", "print", "(", "'use dot product similarity'", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "'use cosine similarity'", ")", "\n", "", "self", ".", "temperature", "=", "temperature", "\n", "\n", "if", "use_contrastive_loss", "==", "'True'", ":", "\n", "            ", "use_contrastive_loss", "=", "True", "\n", "", "elif", "use_contrastive_loss", "==", "'False'", ":", "\n", "            ", "use_contrastive_loss", "=", "False", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "'Wrong contrastive loss setting!'", ")", "\n", "\n", "", "self", ".", "use_contrastive_loss", "=", "use_contrastive_loss", "\n", "if", "self", ".", "use_contrastive_loss", ":", "\n", "            ", "print", "(", "'Initializing teacher BERT.'", ")", "\n", "self", ".", "teacher_bert", "=", "BertModel", ".", "from_pretrained", "(", "model_name", ")", "\n", "for", "param", "in", "self", ".", "teacher_bert", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "", "print", "(", "'Teacher BERT initialized.'", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "'Train BERT with vanilla MLM loss.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.bert_contrastive.BERTContrastivePretraining.save_model": [[73, 84], ["os.path.exists", "bert_contrastive.BERTContrastivePretraining.bert.save_pretrained", "bert_contrastive.BERTContrastivePretraining.tokenizer.save_pretrained", "os.makedirs"], "methods", ["None"], ["", "", "def", "save_model", "(", "self", ",", "ckpt_save_path", ")", ":", "\n", "        ", "import", "os", "\n", "if", "os", ".", "path", ".", "exists", "(", "ckpt_save_path", ")", ":", "\n", "            ", "pass", "\n", "", "else", ":", "# recursively construct directory", "\n", "            ", "os", ".", "makedirs", "(", "ckpt_save_path", ",", "exist_ok", "=", "True", ")", "\n", "\n", "# save model", "\n", "", "self", ".", "bert", ".", "save_pretrained", "(", "ckpt_save_path", ")", "\n", "# save tokenizer", "\n", "self", ".", "tokenizer", ".", "save_pretrained", "(", "ckpt_save_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.bert_contrastive.BERTContrastivePretraining.compute_teacher_representations": [[85, 93], ["input_ids.size", "bert_contrastive.BERTContrastivePretraining.teacher_bert", "rep.view.view.view", "bert_contrastive.BERTContrastivePretraining.cls"], "methods", ["None"], ["", "def", "compute_teacher_representations", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", ":", "\n", "        ", "bsz", ",", "seqlen", "=", "input_ids", ".", "size", "(", ")", "\n", "outputs", "=", "self", ".", "teacher_bert", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ",", "attention_mask", "=", "attention_mask", ")", "\n", "rep", ",", "pooled_output", "=", "outputs", "[", "0", "]", ",", "outputs", "[", "1", "]", "\n", "# rep: bsz x seqlen x embed_dim", "\n", "rep", "=", "rep", ".", "view", "(", "bsz", ",", "seqlen", ",", "self", ".", "embed_dim", ")", "\n", "logits", ",", "sen_relation_scores", "=", "self", ".", "cls", "(", "rep", ",", "pooled_output", ")", "# bsz x seqlen x vocab_size", "\n", "return", "rep", ",", "logits", ",", "sen_relation_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.bert_contrastive.BERTContrastivePretraining.compute_representations": [[94, 102], ["input_ids.size", "bert_contrastive.BERTContrastivePretraining.bert", "rep.view.view.view", "bert_contrastive.BERTContrastivePretraining.cls"], "methods", ["None"], ["", "def", "compute_representations", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", ":", "\n", "        ", "bsz", ",", "seqlen", "=", "input_ids", ".", "size", "(", ")", "\n", "outputs", "=", "self", ".", "bert", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ",", "attention_mask", "=", "attention_mask", ")", "\n", "rep", ",", "pooled_output", "=", "outputs", "[", "0", "]", ",", "outputs", "[", "1", "]", "\n", "# rep: bsz x seqlen x embed_dim", "\n", "rep", "=", "rep", ".", "view", "(", "bsz", ",", "seqlen", ",", "self", ".", "embed_dim", ")", "\n", "logits", ",", "sen_relation_scores", "=", "self", ".", "cls", "(", "rep", ",", "pooled_output", ")", "# bsz x seqlen x vocab_size", "\n", "return", "rep", ",", "logits", ",", "sen_relation_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.bert_contrastive.BERTContrastivePretraining.compute_mlm_loss": [[103, 116], ["truth.transpose.transpose.transpose", "msk.transpose.transpose.transpose", "torch.sum().float().item", "torch.sum().float().item", "torch.sum().float().item", "torch.sum().float().item", "logits.transpose().masked_select", "y_mlm.view.view.view", "truth.transpose.transpose.masked_select", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.nll_loss", "torch.nll_loss", "torch.log_softmax.max", "torch.log_softmax.max", "torch.eq().float().sum().item", "torch.eq().float().sum().item", "torch.eq().float().sum().item", "torch.eq().float().sum().item", "msk.transpose.transpose.unsqueeze().to", "msk.transpose.transpose.to", "torch.sum().float", "torch.sum().float", "torch.sum().float", "torch.sum().float", "logits.transpose", "torch.eq().float().sum", "torch.eq().float().sum", "torch.eq().float().sum", "torch.eq().float().sum", "msk.transpose.transpose.unsqueeze", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.eq().float", "torch.eq().float", "torch.eq().float", "torch.eq().float", "torch.eq", "torch.eq", "torch.eq", "torch.eq"], "methods", ["None"], ["", "def", "compute_mlm_loss", "(", "self", ",", "truth", ",", "msk", ",", "logits", ")", ":", "\n", "        ", "truth", "=", "truth", ".", "transpose", "(", "0", ",", "1", ")", "\n", "msk", "=", "msk", ".", "transpose", "(", "0", ",", "1", ")", "\n", "msk_token_num", "=", "torch", ".", "sum", "(", "msk", ")", ".", "float", "(", ")", ".", "item", "(", ")", "\n", "# center", "\n", "y_mlm", "=", "logits", ".", "transpose", "(", "0", ",", "1", ")", ".", "masked_select", "(", "msk", ".", "unsqueeze", "(", "-", "1", ")", ".", "to", "(", "torch", ".", "bool", ")", ")", "\n", "y_mlm", "=", "y_mlm", ".", "view", "(", "-", "1", ",", "self", ".", "vocab_size", ")", "\n", "gold", "=", "truth", ".", "masked_select", "(", "msk", ".", "to", "(", "torch", ".", "bool", ")", ")", "\n", "log_probs_mlm", "=", "torch", ".", "log_softmax", "(", "y_mlm", ",", "-", "1", ")", "\n", "mlm_loss", "=", "F", ".", "nll_loss", "(", "log_probs_mlm", ",", "gold", ",", "reduction", "=", "'mean'", ")", "\n", "_", ",", "pred_mlm", "=", "log_probs_mlm", ".", "max", "(", "-", "1", ")", "\n", "mlm_correct_num", "=", "torch", ".", "eq", "(", "pred_mlm", ",", "gold", ")", ".", "float", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "return", "mlm_loss", ",", "mlm_correct_num", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.bert_contrastive.BERTContrastivePretraining.forward": [[117, 186], ["truth.size", "bert_contrastive.BERTContrastivePretraining.compute_representations", "bert_contrastive.BERTContrastivePretraining.compute_mlm_loss", "bert_contrastive.BERTContrastivePretraining.loss_fct", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.eq().float().sum().item", "torch.eq().float().sum().item", "torch.eq().float().sum().item", "torch.eq().float().sum().item", "msk.float().sum().item", "truth.get_device", "bert_contrastive.BERTContrastivePretraining.compute_teacher_representations", "bert_contrastive.label_smoothed_nll_loss", "nxt_snt_flag.type.type.type().cuda", "nxt_snt_flag.type.type.type", "sen_relation_scores.view", "nxt_snt_flag.type.type.view", "torch.max", "torch.max", "torch.max", "torch.max", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul.size", "torch.matmul.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.eq().float().sum", "torch.eq().float().sum", "torch.eq().float().sum", "torch.eq().float().sum", "msk.float().sum", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor().cuda", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "truth_rep.transpose", "Exception", "nxt_snt_flag.type.type.type", "masked_rep.norm", "truth_rep.norm", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.eq().float", "torch.eq().float", "torch.eq().float", "torch.eq().float", "msk.float", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "truth_rep.transpose", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "nxt_snt_flag.type.type.view"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.bert_contrastive.BERTContrastivePretraining.compute_representations", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.bert_contrastive.BERTContrastivePretraining.compute_mlm_loss", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.bert_contrastive.BERTContrastivePretraining.compute_teacher_representations", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.bert_contrastive.label_smoothed_nll_loss"], ["", "def", "forward", "(", "self", ",", "truth", ",", "inp", ",", "seg", ",", "msk", ",", "attn_msk", ",", "labels", ",", "contrastive_labels", ",", "nxt_snt_flag", ")", ":", "\n", "        ", "'''\n           truth: bsz x seqlen\n           inp: bsz x seqlen\n           seg: bsz x seqlen\n           msk: bsz x seqlen\n           attn_msk: bsz x seqlen\n           labels: bsz x seqlen; masked positions are filled with -100\n           contrastive_labels: bsz x seqlen; masked position with 0., otherwise 1.\n        '''", "\n", "if", "truth", ".", "is_cuda", ":", "\n", "            ", "is_cuda", "=", "True", "\n", "device", "=", "truth", ".", "get_device", "(", ")", "\n", "", "else", ":", "\n", "            ", "is_cuda", "=", "False", "\n", "\n", "", "bsz", ",", "seqlen", "=", "truth", ".", "size", "(", ")", "\n", "masked_rep", ",", "logits", ",", "sen_relation_scores", "=", "self", ".", "compute_representations", "(", "input_ids", "=", "inp", ",", "token_type_ids", "=", "seg", ",", "attention_mask", "=", "attn_msk", ")", "\n", "\n", "# compute masked language model loss", "\n", "# --------------------------------------------------------------------------------------- #", "\n", "mlm_loss", ",", "mlm_correct_num", "=", "self", ".", "compute_mlm_loss", "(", "truth", ",", "msk", ",", "logits", ")", "\n", "# --------------------------------------------------------------------------------------- #", "\n", "\n", "# compute contrastive loss", "\n", "if", "self", ".", "use_contrastive_loss", ":", "\n", "            ", "truth_rep", ",", "truth_logits", ",", "_", "=", "self", ".", "compute_teacher_representations", "(", "input_ids", "=", "truth", ",", "token_type_ids", "=", "seg", ",", "attention_mask", "=", "attn_msk", ")", "\n", "''' \n                mask_rep, truth_rep : hidden_size\n                rep, left_rep, right_rep: bsz x seqlen x embed_dim\n            '''", "\n", "if", "self", ".", "sim", "==", "'dot_product'", ":", "\n", "                ", "contrastive_scores", "=", "torch", ".", "matmul", "(", "masked_rep", ",", "truth_rep", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "\n", "", "elif", "self", ".", "sim", "==", "'cosine'", ":", "# 'cosine'", "\n", "                ", "masked_rep", "=", "masked_rep", "/", "masked_rep", ".", "norm", "(", "dim", "=", "2", ",", "keepdim", "=", "True", ")", "\n", "truth_rep", "=", "truth_rep", "/", "truth_rep", ".", "norm", "(", "dim", "=", "2", ",", "keepdim", "=", "True", ")", "\n", "contrastive_scores", "=", "torch", ".", "matmul", "(", "masked_rep", ",", "truth_rep", ".", "transpose", "(", "1", ",", "2", ")", ")", "/", "self", ".", "temperature", "# bsz x seqlen x seqlen", "\n", "", "else", ":", "\n", "                ", "raise", "Exception", "(", "'Wrong similarity mode!!!'", ")", "\n", "\n", "", "assert", "contrastive_scores", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "bsz", ",", "seqlen", ",", "seqlen", "]", ")", "\n", "contrastive_loss", ",", "correct_contrastive_num", ",", "total_contrastive_num", "=", "label_smoothed_nll_loss", "(", "contrastive_scores", ",", "contrastive_labels", ")", "\n", "", "else", ":", "\n", "            ", "correct_contrastive_num", ",", "total_contrastive_num", "=", "0.", ",", "1.", "\n", "\n", "", "if", "is_cuda", ":", "\n", "            ", "nxt_snt_flag", "=", "nxt_snt_flag", ".", "type", "(", "torch", ".", "LongTensor", ")", ".", "cuda", "(", "device", ")", "\n", "", "else", ":", "\n", "            ", "nxt_snt_flag", "=", "nxt_snt_flag", ".", "type", "(", "torch", ".", "LongTensor", ")", "\n", "", "next_sentence_loss", "=", "self", ".", "loss_fct", "(", "sen_relation_scores", ".", "view", "(", "-", "1", ",", "2", ")", ",", "nxt_snt_flag", ".", "view", "(", "-", "1", ")", ")", "\n", "if", "self", ".", "use_contrastive_loss", ":", "\n", "            ", "tot_loss", "=", "mlm_loss", "+", "next_sentence_loss", "+", "contrastive_loss", "\n", "", "else", ":", "\n", "            ", "tot_loss", "=", "mlm_loss", "+", "next_sentence_loss", "\n", "\n", "", "next_sentence_logprob", "=", "torch", ".", "log_softmax", "(", "sen_relation_scores", ",", "-", "1", ")", "\n", "next_sentence_predictions", "=", "torch", ".", "max", "(", "next_sentence_logprob", ",", "dim", "=", "-", "1", ")", "[", "-", "1", "]", "\n", "nxt_snt_correct_num", "=", "torch", ".", "eq", "(", "next_sentence_predictions", ",", "nxt_snt_flag", ".", "view", "(", "-", "1", ")", ")", ".", "float", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "tot_tokens", "=", "msk", ".", "float", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "if", "is_cuda", ":", "\n", "            ", "return", "tot_loss", ",", "torch", ".", "Tensor", "(", "[", "mlm_correct_num", "]", ")", ".", "cuda", "(", "device", ")", ",", "torch", ".", "Tensor", "(", "[", "tot_tokens", "]", ")", ".", "cuda", "(", "device", ")", ",", "torch", ".", "Tensor", "(", "[", "nxt_snt_correct_num", "]", ")", ".", "cuda", "(", "device", ")", ",", "torch", ".", "Tensor", "(", "[", "correct_contrastive_num", "]", ")", ".", "cuda", "(", "device", ")", ",", "torch", ".", "Tensor", "(", "[", "total_contrastive_num", "]", ")", ".", "cuda", "(", "device", ")", "\n", "", "else", ":", "\n", "            ", "return", "tot_loss", ",", "torch", ".", "Tensor", "(", "[", "mlm_correct_num", "]", ")", ",", "torch", ".", "Tensor", "(", "[", "tot_tokens", "]", ")", ",", "torch", ".", "Tensor", "(", "[", "nxt_snt_correct_num", "]", ")", ",", "torch", ".", "Tensor", "(", "[", "correct_contrastive_num", "]", ")", ",", "torch", ".", "Tensor", "(", "[", "total_contrastive_num", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.bert_contrastive.label_smoothed_nll_loss": [[15, 35], ["contrastive_scores.size", "torch.log_softmax", "torch.arange().view", "torch.arange().view", "gold.cuda.expand().contiguous().view", "torch.max", "torch.max", "torch.eq().float().view", "torch.eq().float().view", "torch.sum", "torch.sum", "contrastive_labels.sum", "contrastive_scores.view", "gold.cuda.cuda", "F.log_softmax.gather().squeeze", "loss.view", "torch.sum", "torch.sum", "contrastive_labels.sum", "torch.arange", "torch.arange", "gold.cuda.expand().contiguous", "contrastive_scores.get_device", "torch.eq().float", "torch.eq().float", "F.log_softmax.gather", "gold.cuda.expand", "torch.eq", "torch.eq", "gold.cuda.unsqueeze"], "function", ["None"], ["def", "label_smoothed_nll_loss", "(", "contrastive_scores", ",", "contrastive_labels", ",", "eps", "=", "0.0", ")", ":", "\n", "    ", "'''\n        contrasive_scores: bsz x seqlen x seqlen\n        contrasive_labels: bsz x seqlen; masked positions with 0., otherwise 1.\n    '''", "\n", "bsz", ",", "seqlen", ",", "_", "=", "contrastive_scores", ".", "size", "(", ")", "\n", "logprobs", "=", "F", ".", "log_softmax", "(", "contrastive_scores", ".", "view", "(", "-", "1", ",", "seqlen", ")", ",", "dim", "=", "-", "1", ")", "\n", "gold", "=", "torch", ".", "arange", "(", "seqlen", ")", ".", "view", "(", "-", "1", ",", ")", "\n", "gold", "=", "gold", ".", "expand", "(", "bsz", ",", "seqlen", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "if", "contrastive_scores", ".", "is_cuda", ":", "\n", "        ", "gold", "=", "gold", ".", "cuda", "(", "contrastive_scores", ".", "get_device", "(", ")", ")", "\n", "", "loss", "=", "-", "logprobs", ".", "gather", "(", "dim", "=", "-", "1", ",", "index", "=", "gold", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "loss", "=", "loss", ".", "view", "(", "bsz", ",", "seqlen", ")", "*", "contrastive_labels", "\n", "loss", "=", "torch", ".", "sum", "(", "loss", ")", "/", "contrastive_labels", ".", "sum", "(", ")", "\n", "\n", "_", ",", "pred", "=", "torch", ".", "max", "(", "logprobs", ",", "-", "1", ")", "\n", "correct_num", "=", "torch", ".", "eq", "(", "gold", ",", "pred", ")", ".", "float", "(", ")", ".", "view", "(", "bsz", ",", "seqlen", ")", "\n", "correct_num", "=", "torch", ".", "sum", "(", "correct_num", "*", "contrastive_labels", ")", "\n", "total_num", "=", "contrastive_labels", ".", "sum", "(", ")", "\n", "return", "loss", ",", "correct_num", ",", "total_num", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_english.PretrainCorpus.__init__": [[9, 25], ["tokenizer.convert_tokens_to_ids", "open", "dataclass_english.PretrainCorpus.load_lines"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.load_lines"], ["    ", "def", "__init__", "(", "self", ",", "tokenizer", ",", "filename", ",", "max_len", ",", "whole_word_masking", "=", "False", ")", ":", "\n", "        ", "'''\n            tokenizer: BERT tokenizer\n            filename: pretraining corpus\n            max_len: maximum length for each sentence\n        '''", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "vocab_size", "=", "self", ".", "tokenizer", ".", "vocab_size", "\n", "self", ".", "special_token_id_list", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "UNK", ",", "SEP", ",", "PAD", ",", "CLS", ",", "MASK", "]", ")", "\n", "self", ".", "unk_id", ",", "self", ".", "sep_id", ",", "self", ".", "pad_id", ",", "self", ".", "cls_id", ",", "self", ".", "mask_id", "=", "self", ".", "special_token_id_list", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "filename", "=", "filename", "\n", "self", ".", "stream", "=", "open", "(", "self", ".", "filename", ",", "encoding", "=", "'utf8'", ")", "\n", "self", ".", "epoch_id", "=", "0", "\n", "self", ".", "whole_word_masking", "=", "whole_word_masking", "\n", "self", ".", "load_lines", "(", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_english.PretrainCorpus.load_lines": [[26, 49], ["print", "random.shuffle", "enumerate", "print", "open", "i.readlines", "line.strip().strip().split", "data.extend", "len", "docs[].append", "docs.append", "google_bert.create_instances_from_document", "len", "range", "line.strip().strip", "len", "line.strip"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.create_instances_from_document"], ["", "def", "load_lines", "(", "self", ",", "filename", ")", ":", "\n", "        ", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "i", ":", "\n", "            ", "lines", "=", "i", ".", "readlines", "(", ")", "\n", "", "print", "(", "'Number of lines is {}'", ".", "format", "(", "len", "(", "lines", ")", ")", ")", "\n", "docs", "=", "[", "[", "]", "]", "\n", "for", "line", "in", "lines", ":", "\n", "            ", "tokens", "=", "line", ".", "strip", "(", "'\\n'", ")", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "if", "tokens", ":", "\n", "                ", "docs", "[", "-", "1", "]", ".", "append", "(", "tokens", ")", "\n", "", "else", ":", "\n", "                ", "docs", ".", "append", "(", "[", "]", ")", "\n", "\n", "", "", "docs", "=", "[", "x", "for", "x", "in", "docs", "if", "x", "]", "# filter out empty lines", "\n", "\n", "self", ".", "docs", "=", "docs", "\n", "random", ".", "shuffle", "(", "docs", ")", "\n", "\n", "data", "=", "[", "]", "\n", "for", "idx", ",", "doc", "in", "enumerate", "(", "docs", ")", ":", "\n", "            ", "data", ".", "extend", "(", "create_instances_from_document", "(", "docs", ",", "idx", ",", "self", ".", "max_len", ")", ")", "\n", "", "self", ".", "data", "=", "data", "\n", "print", "(", "'number of sentence pairs is {}'", ".", "format", "(", "len", "(", "self", ".", "data", ")", ")", ")", "\n", "self", ".", "train_idx_list", "=", "[", "i", "for", "i", "in", "range", "(", "len", "(", "self", ".", "data", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_english.PretrainCorpus.get_batch_data": [[50, 56], ["random.sample", "dataclass_english.PretrainCorpus.batchify", "batch_data.append"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.batchify"], ["", "def", "get_batch_data", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "batch_idx_list", "=", "random", ".", "sample", "(", "self", ".", "train_idx_list", ",", "batch_size", ")", "\n", "batch_data", "=", "[", "]", "\n", "for", "idx", "in", "batch_idx_list", ":", "\n", "            ", "batch_data", ".", "append", "(", "self", ".", "data", "[", "idx", "]", ")", "\n", "", "return", "self", ".", "batchify", "(", "batch_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_english.PretrainCorpus.random_token": [[57, 63], ["numpy.random.randint", "dataclass_english.PretrainCorpus.tokenizer.convert_ids_to_tokens", "numpy.random.randint"], "methods", ["None"], ["", "def", "random_token", "(", "self", ")", ":", "\n", "        ", "rand_idx", "=", "1", "+", "np", ".", "random", ".", "randint", "(", "self", ".", "vocab_size", "-", "1", ")", "\n", "while", "rand_idx", "in", "self", ".", "special_token_id_list", ":", "\n", "            ", "rand_idx", "=", "1", "+", "np", ".", "random", ".", "randint", "(", "self", ".", "vocab_size", "-", "1", ")", "\n", "", "random_token", "=", "self", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "[", "rand_idx", "]", ")", "[", "0", "]", "\n", "return", "random_token", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_english.PretrainCorpus.random_mask": [[64, 92], ["min", "enumerate", "random.shuffle", "set", "enumerate", "max", "set.append", "int", "mask.append", "tgt.append", "masked_tokens.append", "mask.append", "tgt.append", "round", "random.random", "masked_tokens.append", "random.random", "masked_tokens.append", "masked_tokens.append", "len", "dataclass_english.PretrainCorpus.random_token"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.random_token"], ["", "def", "random_mask", "(", "self", ",", "tokens", ",", "masked_lm_prob", ",", "max_predictions_per_seq", ")", ":", "\n", "        ", "num_to_predict", "=", "min", "(", "max_predictions_per_seq", ",", "max", "(", "1", ",", "int", "(", "round", "(", "len", "(", "tokens", ")", "*", "masked_lm_prob", ")", ")", ")", ")", "\n", "masked_tokens", ",", "mask", "=", "[", "]", ",", "[", "]", "\n", "cand", "=", "[", "]", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "if", "token", "in", "[", "UNK", ",", "SEP", ",", "PAD", ",", "CLS", ",", "MASK", "]", ":", "# do not learn to predict special tokens", "\n", "                ", "continue", "\n", "", "cand", ".", "append", "(", "i", ")", "\n", "", "random", ".", "shuffle", "(", "cand", ")", "\n", "cand", "=", "set", "(", "cand", "[", ":", "num_to_predict", "]", ")", "\n", "\n", "masked_tokens", ",", "mask", ",", "tgt", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "if", "i", "in", "cand", ":", "\n", "                ", "if", "random", ".", "random", "(", ")", "<", "0.8", ":", "\n", "                    ", "masked_tokens", ".", "append", "(", "MASK", ")", "\n", "", "else", ":", "\n", "                    ", "if", "random", ".", "random", "(", ")", "<", "0.5", ":", "\n", "                        ", "masked_tokens", ".", "append", "(", "token", ")", "\n", "", "else", ":", "\n", "                        ", "masked_tokens", ".", "append", "(", "self", ".", "random_token", "(", ")", ")", "\n", "", "", "mask", ".", "append", "(", "1", ")", "\n", "tgt", ".", "append", "(", "token", ")", "\n", "", "else", ":", "\n", "                ", "masked_tokens", ".", "append", "(", "token", ")", "\n", "mask", ".", "append", "(", "0", ")", "\n", "tgt", ".", "append", "(", "PAD", ")", "\n", "", "", "return", "masked_tokens", ",", "mask", ",", "tgt", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_english.PretrainCorpus.whole_word_random_mask": [[93, 137], ["range", "len", "min", "random.shuffle", "set", "enumerate", "token.startswith", "len", "int", "max", "joined_word_list[].append", "joined_word_list.append", "cand_idx_list.append", "int", "len", "len", "round", "mask.append", "tgt.append", "masked_tokens.append", "mask.append", "tgt.append", "random.random", "masked_tokens.append", "random.random", "masked_tokens.append", "masked_tokens.append", "dataclass_english.PretrainCorpus.random_token"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.random_token"], ["", "def", "whole_word_random_mask", "(", "self", ",", "tokens", ",", "masked_lm_prob", ")", ":", "\n", "# tokens is a list of words, some tokens are partial words", "\n", "# e.g. [token_1 token_2_part_1 ##token_2_part_2 token_3]", "\n", "\n", "        ", "joined_word_list", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "token", ".", "startswith", "(", "'##'", ")", ":", "# it must be a subword", "\n", "                ", "assert", "len", "(", "joined_word_list", ")", "!=", "0", "\n", "joined_word_list", "[", "-", "1", "]", ".", "append", "(", "token", ")", "\n", "", "else", ":", "\n", "                ", "joined_word_list", ".", "append", "(", "[", "token", "]", ")", "\n", "\n", "", "", "cand_idx_list", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "len", "(", "joined_word_list", ")", ")", ":", "\n", "            ", "item", "=", "joined_word_list", "[", "idx", "]", "\n", "if", "len", "(", "item", ")", "==", "1", "and", "item", "[", "0", "]", "in", "[", "UNK", ",", "SEP", ",", "PAD", ",", "CLS", ",", "MASK", "]", ":", "# we do not mask special tokens", "\n", "                ", "pass", "\n", "", "else", ":", "\n", "                ", "cand_idx_list", ".", "append", "(", "idx", ")", "\n", "\n", "", "", "num_words", "=", "len", "(", "cand_idx_list", ")", "\n", "num_to_predict", "=", "min", "(", "int", "(", "num_words", "*", "0.25", ")", ",", "max", "(", "1", ",", "int", "(", "round", "(", "num_words", "*", "masked_lm_prob", ")", ")", ")", ")", "\n", "random", ".", "shuffle", "(", "cand_idx_list", ")", "\n", "cand", "=", "set", "(", "cand_idx_list", "[", ":", "num_to_predict", "]", ")", "\n", "\n", "masked_tokens", ",", "mask", ",", "tgt", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "i", ",", "one_word_list", "in", "enumerate", "(", "joined_word_list", ")", ":", "\n", "            ", "if", "i", "in", "cand", ":", "\n", "                ", "for", "token", "in", "one_word_list", ":", "\n", "                    ", "if", "random", ".", "random", "(", ")", "<", "0.8", ":", "\n", "                        ", "masked_tokens", ".", "append", "(", "MASK", ")", "\n", "", "else", ":", "\n", "                        ", "if", "random", ".", "random", "(", ")", "<", "0.5", ":", "\n", "                            ", "masked_tokens", ".", "append", "(", "token", ")", "\n", "", "else", ":", "\n", "                            ", "masked_tokens", ".", "append", "(", "self", ".", "random_token", "(", ")", ")", "\n", "", "", "mask", ".", "append", "(", "1", ")", "\n", "tgt", ".", "append", "(", "token", ")", "\n", "", "", "else", ":", "\n", "                ", "for", "token", "in", "one_word_list", ":", "\n", "                    ", "masked_tokens", ".", "append", "(", "token", ")", "\n", "mask", ".", "append", "(", "0", ")", "\n", "tgt", ".", "append", "(", "PAD", ")", "\n", "", "", "", "return", "masked_tokens", ",", "mask", ",", "tgt", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_english.PretrainCorpus.ListsToTensor": [[138, 149], ["max", "torch.LongTensor().contiguous", "ys.append", "len", "torch.LongTensor", "dataclass_english.PretrainCorpus.tokenizer.convert_tokens_to_ids", "len", "len"], "methods", ["None"], ["", "def", "ListsToTensor", "(", "self", ",", "xs", ",", "tokenize", "=", "False", ")", ":", "\n", "        ", "max_len", "=", "max", "(", "len", "(", "x", ")", "for", "x", "in", "xs", ")", "\n", "ys", "=", "[", "]", "\n", "for", "x", "in", "xs", ":", "\n", "            ", "if", "tokenize", ":", "\n", "                ", "y", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "x", ")", "+", "[", "self", ".", "pad_id", "]", "*", "(", "self", ".", "max_len", "-", "len", "(", "x", ")", ")", "\n", "", "else", ":", "\n", "                ", "y", "=", "x", "+", "[", "0", "]", "*", "(", "self", ".", "max_len", "-", "len", "(", "x", ")", ")", "\n", "", "ys", ".", "append", "(", "y", ")", "\n", "", "data", "=", "torch", ".", "LongTensor", "(", "ys", ")", ".", "contiguous", "(", ")", "# bsz x seqlen", "\n", "return", "data", "# bsz x seqlen", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_english.PretrainCorpus.process_tgt": [[150, 163], ["max", "torch.LongTensor().contiguous", "torch.LongTensor().contiguous.clone", "torch.LongTensor().contiguous.clone", "torch.LongTensor().contiguous.append", "torch.LongTensor().contiguous.clone.type", "len", "dataclass_english.PretrainCorpus.tokenizer.convert_tokens_to_ids", "torch.LongTensor", "len"], "methods", ["None"], ["", "def", "process_tgt", "(", "self", ",", "tgt_matrix", ")", ":", "\n", "        ", "max_len", "=", "max", "(", "len", "(", "x", ")", "for", "x", "in", "tgt_matrix", ")", "\n", "ys", "=", "[", "]", "\n", "for", "x", "in", "tgt_matrix", ":", "\n", "            ", "y", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "x", ")", "+", "[", "self", ".", "pad_id", "]", "*", "(", "self", ".", "max_len", "-", "len", "(", "x", ")", ")", "\n", "ys", ".", "append", "(", "y", ")", "\n", "", "ys", "=", "torch", ".", "LongTensor", "(", "ys", ")", ".", "contiguous", "(", ")", "# bsz x seqlen", "\n", "labels", "=", "ys", ".", "clone", "(", ")", "\n", "labels", "[", "labels", "[", ":", ",", ":", "]", "==", "self", ".", "pad_id", "]", "=", "-", "100", "\n", "contrastive_labels", "=", "ys", ".", "clone", "(", ")", "\n", "contrastive_labels", "[", "contrastive_labels", "[", ":", ",", ":", "]", "==", "self", ".", "pad_id", "]", "=", "0", "\n", "contrastive_labels", "[", "contrastive_labels", "[", ":", ",", ":", "]", "!=", "self", ".", "pad_id", "]", "=", "1", "\n", "return", "labels", ",", "contrastive_labels", ".", "type", "(", "torch", ".", "FloatTensor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_english.PretrainCorpus.batchify": [[164, 192], ["dataclass_english.PretrainCorpus.ListsToTensor", "dataclass_english.PretrainCorpus.ListsToTensor", "dataclass_english.PretrainCorpus.ListsToTensor", "dataclass_english.PretrainCorpus.ListsToTensor().to", "torch.ByteTensor", "dataclass_english.PretrainCorpus.process_tgt", "dataclass_english.PretrainCorpus.append", "dataclass_english.PretrainCorpus.append", "dataclass_english.PretrainCorpus.append", "dataclass_english.PretrainCorpus.append", "tgt_matrix.append", "dataclass_english.PretrainCorpus.eq", "dataclass_english.PretrainCorpus.whole_word_random_mask", "dataclass_english.PretrainCorpus.random_mask", "torch.ByteTensor.append", "torch.ByteTensor.append", "dataclass_english.PretrainCorpus.ListsToTensor", "len", "len"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.ListsToTensor", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.ListsToTensor", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.ListsToTensor", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.process_tgt", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.whole_word_random_mask", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.random_mask", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.ListsToTensor"], ["", "def", "batchify", "(", "self", ",", "data", ")", ":", "\n", "        ", "truth", ",", "inp", ",", "seg", ",", "msk", ",", "tgt_matrix", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "nxt_snt_flag", "=", "[", "]", "\n", "for", "a", ",", "b", ",", "r", "in", "data", ":", "\n", "            ", "x", "=", "[", "CLS", "]", "+", "a", "+", "[", "SEP", "]", "+", "b", "+", "[", "SEP", "]", "\n", "truth", ".", "append", "(", "x", ")", "\n", "seg", ".", "append", "(", "[", "0", "]", "*", "(", "len", "(", "a", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "b", ")", "+", "1", ")", ")", "\n", "if", "self", ".", "whole_word_masking", ":", "\n", "                ", "masked_x", ",", "mask", ",", "tgt", "=", "self", ".", "whole_word_random_mask", "(", "x", ",", "0.15", ")", "\n", "", "else", ":", "\n", "                ", "masked_x", ",", "mask", ",", "tgt", "=", "self", ".", "random_mask", "(", "x", ",", "0.15", ",", "self", ".", "max_len", "*", "0.25", ")", "\n", "\n", "", "inp", ".", "append", "(", "masked_x", ")", "\n", "msk", ".", "append", "(", "mask", ")", "\n", "tgt_matrix", ".", "append", "(", "tgt", ")", "\n", "if", "r", ":", "# r stands for is_random_text", "\n", "                ", "nxt_snt_flag", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "nxt_snt_flag", ".", "append", "(", "0", ")", "\n", "\n", "", "", "truth", "=", "self", ".", "ListsToTensor", "(", "truth", ",", "tokenize", "=", "True", ")", "\n", "inp", "=", "self", ".", "ListsToTensor", "(", "inp", ",", "tokenize", "=", "True", ")", "\n", "seg", "=", "self", ".", "ListsToTensor", "(", "seg", ",", "tokenize", "=", "False", ")", "\n", "msk", "=", "self", ".", "ListsToTensor", "(", "msk", ",", "tokenize", "=", "False", ")", ".", "to", "(", "torch", ".", "uint8", ")", "# bsz x seqlen", "\n", "attn_msk", "=", "~", "inp", ".", "eq", "(", "self", ".", "pad_id", ")", "\n", "nxt_snt_flag", "=", "torch", ".", "ByteTensor", "(", "nxt_snt_flag", ")", "\n", "labels", ",", "contrastive_labels", "=", "self", ".", "process_tgt", "(", "tgt_matrix", ")", "\n", "return", "truth", ",", "inp", ",", "seg", ",", "msk", ",", "attn_msk", ",", "labels", ",", "contrastive_labels", ",", "nxt_snt_flag", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.train.parse_config": [[13, 43], ["argparse.ArgumentParser", "argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["            ", "gold_lines", "=", "g", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "pred_path", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "p", ":", "\n", "            ", "pred_lines", "=", "p", ".", "readlines", "(", ")", "\n", "", "assert", "len", "(", "gold_lines", ")", "==", "len", "(", "pred_lines", ")", "\n", "data_num", "=", "len", "(", "gold_lines", ")", "\n", "for", "i", "in", "range", "(", "data_num", ")", ":", "\n", "            ", "gold_l", "=", "gold_lines", "[", "i", "]", "\n", "pred_l", "=", "pred_lines", "[", "i", "]", "\n", "gold_content_list", "=", "gold_l", ".", "strip", "(", "'\\n'", ")", ".", "split", "(", "'\\t'", ")", "\n", "text", "=", "gold_content_list", "[", "0", "]", "\n", "gold_label_str", "=", "gold_content_list", "[", "1", "]", "\n", "\n", "pred_l", "=", "pred_lines", "[", "i", "]", "\n", "pred_content_list", "=", "pred_l", ".", "strip", "(", "'\\n'", ")", ".", "split", "(", "'\\t'", ")", "\n", "pred_label_str", "=", "pred_content_list", "[", "1", "]", "\n", "\n", "pred_label_list", "=", "pred_label_str", ".", "split", "(", ")", "\n", "gold_label_list", "=", "gold_label_str", ".", "split", "(", ")", "[", ":", "len", "(", "pred_label_list", ")", "]", "# result truncation", "\n", "assert", "len", "(", "gold_label_list", ")", "==", "len", "(", "pred_label_list", ")", "\n", "\n", "instance_len", "=", "len", "(", "gold_label_list", ")", "\n", "text_list", "=", "text", ".", "split", "(", ")", "[", ":", "instance_len", "]", "\n", "for", "j", "in", "range", "(", "instance_len", ")", ":", "\n", "                ", "out_str", "=", "text_list", "[", "j", "]", "+", "' '", "+", "gold_label_list", "[", "j", "]", "+", "' '", "+", "pred_label_list", "[", "j", "]", "\n", "o", ".", "writelines", "(", "out_str", "+", "'\\n'", ")", "\n", "", "o", ".", "writelines", "(", "'\\n'", ")", "\n", "\n", "", "", "", "def", "save_model", "(", "model", ",", "save_path", ",", "save_name", ")", ":", "\n", "    ", "from", "operator", "import", "itemgetter", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "save_path", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "save_path", ")", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.train.update_lr": [[44, 47], ["None"], "function", ["None"], ["\n", "", "if", "torch", ".", "cuda", ".", "device_count", "(", ")", ">", "1", ":", "# multi-gpu training", "\n", "        ", "model", "=", "model", ".", "module", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.train.train_model": [[48, 188], ["torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "int", "print", "AdamW", "get_linear_schedule_with_warmup", "AdamW.zero_grad", "model.train", "print", "PretrainCorpus", "print", "print", "print", "model.parameters", "print", "PretrainCorpus.get_batch_data", "model", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "loss.mean.mean", "loss.mean.backward", "loss.mean.item", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "print", "Exception", "train.update_lr", "truth.cuda.cuda", "inp.cuda.cuda", "seg.cuda.cuda", "msk.cuda.cuda", "attn_mask.cuda.cuda", "nxt_snt_flag.cuda.cuda", "labels.cuda.cuda", "contrastive_labels.cuda.cuda", "truth.cuda.size", "model.parameters", "AdamW.step", "get_linear_schedule_with_warmup.step", "AdamW.zero_grad", "round", "round", "round", "print", "round", "round", "round", "print", "print", "os.path.exists", "print", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "os.makedirs", "model.module.save_model", "model.save_model", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count"], "function", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.get_batch_data", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.train.update_lr", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.bert_contrastive.BERTContrastivePretraining.save_model", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.bert_contrastive.BERTContrastivePretraining.save_model"], ["", "model_save_path", "=", "save_path", "+", "'/'", "+", "save_name", "\n", "torch", ".", "save", "(", "{", "'model'", ":", "model", ".", "state_dict", "(", ")", "}", ",", "model_save_path", ")", "\n", "\n", "fileData", "=", "{", "}", "\n", "for", "fname", "in", "os", ".", "listdir", "(", "save_path", ")", ":", "\n", "        ", "if", "fname", ".", "startswith", "(", "'epoch'", ")", ":", "\n", "            ", "fileData", "[", "fname", "]", "=", "os", ".", "stat", "(", "save_path", "+", "'/'", "+", "fname", ")", ".", "st_mtime", "\n", "", "else", ":", "\n", "            ", "pass", "\n", "", "", "sortedFiles", "=", "sorted", "(", "fileData", ".", "items", "(", ")", ",", "key", "=", "itemgetter", "(", "1", ")", ")", "\n", "if", "len", "(", "sortedFiles", ")", "<", "1", ":", "\n", "        ", "pass", "\n", "", "else", ":", "\n", "        ", "delete", "=", "len", "(", "sortedFiles", ")", "-", "1", "\n", "for", "x", "in", "range", "(", "0", ",", "delete", ")", ":", "\n", "            ", "os", ".", "remove", "(", "save_path", "+", "'/'", "+", "sortedFiles", "[", "x", "]", "[", "0", "]", ")", "\n", "\n", "", "", "", "def", "evaluate_model", "(", "args", ",", "data", ",", "model", ",", "save_path", ",", "mode", ")", ":", "\n", "    ", "cuda_available", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "if", "cuda_available", ":", "\n", "        ", "if", "torch", ".", "cuda", ".", "device_count", "(", ")", ">", "1", ":", "# multi-gpu training ", "\n", "            ", "model", "=", "model", ".", "module", "\n", "", "else", ":", "# single gpu training", "\n", "            ", "pass", "\n", "", "", "else", ":", "\n", "        ", "pass", "\n", "\n", "", "device", "=", "torch", ".", "device", "(", "'cuda'", ")", "\n", "if", "mode", "==", "'dev'", ":", "\n", "        ", "eval_step_num", "=", "int", "(", "data", ".", "dev_num", "/", "args", ".", "batch_size", ")", "+", "1", "\n", "instance_num", "=", "data", ".", "dev_num", "\n", "gold_path", "=", "data", ".", "dev_path", "\n", "", "elif", "mode", "==", "'test'", ":", "\n", "        ", "eval_step_num", "=", "int", "(", "data", ".", "test_num", "/", "args", ".", "batch_size", ")", "+", "1", "\n", "instance_num", "=", "data", ".", "test_num", "\n", "gold_path", "=", "data", ".", "test_path", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "'Wrong Mode!!!'", ")", "\n", "\n", "", "res_list", "=", "[", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "for", "_", "in", "range", "(", "eval_step_num", ")", ":", "\n", "            ", "src_tensor", ",", "src_attn_mask", ",", "_", ",", "tgt_mask", ",", "tgt_ref_id_list", "=", "data", ".", "get_next_validation_batch", "(", "args", ".", "batch_size", ",", "mode", ")", "\n", "if", "cuda_available", ":", "\n", "                ", "src_tensor", "=", "src_tensor", ".", "cuda", "(", "device", ")", "\n", "src_attn_mask", "=", "src_attn_mask", ".", "cuda", "(", "device", ")", "\n", "tgt_mask", "=", "tgt_mask", ".", "cuda", "(", "device", ")", "\n", "", "predictions", "=", "model", ".", "decode", "(", "src_tensor", ",", "src_attn_mask", ",", "tgt_mask", ")", "\n", "predictions", "=", "data", ".", "parse_result", "(", "predictions", ")", "\n", "ref_predictions", "=", "data", ".", "parse_result", "(", "tgt_ref_id_list", ")", "\n", "bsz", "=", "len", "(", "tgt_ref_id_list", ")", "\n", "for", "idx", "in", "range", "(", "bsz", ")", ":", "\n", "                ", "assert", "len", "(", "predictions", "[", "idx", "]", ".", "split", "(", ")", ")", "==", "len", "(", "ref_predictions", "[", "idx", "]", ".", "split", "(", ")", ")", "\n", "", "res_list", "+=", "predictions", "\n", "", "res_list", "=", "res_list", "[", ":", "instance_num", "]", "\n", "", "eval_path", "=", "save_path", "+", "'/eval.txt'", "\n", "with", "open", "(", "eval_path", ",", "'w'", ",", "encoding", "=", "'utf8'", ")", "as", "o", ":", "\n", "        ", "for", "res", "in", "res_list", ":", "\n", "            ", "o", ".", "writelines", "(", "res", "+", "'\\t'", "+", "res", "+", "'\\n'", ")", "\n", "", "", "combine_path", "=", "save_path", "+", "'/'", "+", "mode", "+", "'_gold_eval_combine.txt'", "\n", "combine_result", "(", "gold_path", ",", "eval_path", ",", "combine_path", ")", "\n", "precision", ",", "recall", ",", "f1", "=", "fmeasure_from_singlefile", "(", "combine_path", ",", "args", ".", "evaluation_mode", ")", "\n", "os", ".", "remove", "(", "combine_path", ")", "\n", "os", ".", "remove", "(", "eval_path", ")", "\n", "return", "precision", ",", "recall", ",", "f1", "\n", "\n", "", "def", "train_one_model", "(", "args", ",", "model_name", ",", "run_number", ")", ":", "\n", "    ", "save_path", "=", "args", ".", "save_path_prefix", "+", "'/run_{}'", ".", "format", "(", "run_number", ")", "+", "'/'", "\n", "import", "os", "\n", "if", "os", ".", "path", ".", "exists", "(", "save_path", ")", ":", "\n", "        ", "pass", "\n", "", "else", ":", "# recursively construct directory", "\n", "        ", "os", ".", "makedirs", "(", "save_path", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "cuda_available", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "device", "=", "torch", ".", "device", "(", "'cuda'", ")", "\n", "\n", "from", "transformers", "import", "BertTokenizer", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "model_name", ")", "\n", "\n", "print", "(", "'Loading data...'", ")", "\n", "from", "dataclass", "import", "Data", "\n", "train_path", ",", "dev_path", ",", "test_path", ",", "label_path", "=", "args", ".", "train_path", ",", "args", ".", "dev_path", ",", "args", ".", "test_path", ",", "args", ".", "label_path", "\n", "data", "=", "Data", "(", "tokenizer", ",", "train_path", ",", "dev_path", ",", "test_path", ",", "label_path", ",", "args", ".", "max_len", ")", "\n", "print", "(", "'Data loaded.'", ")", "\n", "\n", "print", "(", "'Loading model...'", ")", "\n", "from", "model", "import", "NERModel", "\n", "model", "=", "NERModel", "(", "model_name", ",", "data", ".", "num_class", ")", "\n", "#if cuda_available:", "\n", "#    model = model.cuda(device)", "\n", "if", "cuda_available", ":", "\n", "        ", "if", "torch", ".", "cuda", ".", "device_count", "(", ")", ">", "1", ":", "# multi-gpu training ", "\n", "            ", "print", "(", "'Multi-GPU training...'", ")", "\n", "model", "=", "nn", ".", "DataParallel", "(", "model", ")", "\n", "", "else", ":", "# single gpu training", "\n", "            ", "pass", "\n", "", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "", "else", ":", "\n", "        ", "pass", "\n", "\n", "", "print", "(", "'Model loaded.'", ")", "\n", "model", ".", "train", "(", ")", "\n", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "args", ".", "learning_rate", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "batch_size", ",", "gradient_accumulation_steps", "=", "args", ".", "batch_size", ",", "args", ".", "gradient_accumulation_steps", "\n", "train_num", ",", "dev_num", ",", "test_num", "=", "data", ".", "train_num", ",", "data", ".", "dev_num", ",", "data", ".", "test_num", "\n", "train_step_num", ",", "dev_step_num", ",", "test_step_num", "=", "int", "(", "train_num", "/", "batch_size", ")", "+", "1", ",", "int", "(", "dev_num", "/", "batch_size", ")", "+", "1", ",", "int", "(", "test_num", "/", "batch_size", ")", "+", "1", "\n", "\n", "print_every", "=", "int", "(", "train_step_num", "/", "4", ")", "\n", "\n", "batches_processed", "=", "0", "\n", "loss_acm", "=", "0.", "\n", "max_combine_score", ",", "best_combine_str", "=", "0.", ",", "'best combine dev f1: {}, test f1: {}'", ".", "format", "(", "0.", ",", "0.", ")", "\n", "dev_f1_list", ",", "test_f1_list", "=", "[", "0.", "]", ",", "[", "0.", "]", "\n", "best_combined_score_dict", "=", "{", "'dev'", ":", "0.", ",", "'test'", ":", "0.", "}", "\n", "max_test_f1_score", "=", "0.", "\n", "for", "epoch_num", "in", "range", "(", "args", ".", "total_epochs", ")", ":", "\n", "        ", "print", "(", "'------------------------------------------------------------------'", ")", "\n", "print", "(", "'Start epoch {} training...'", ".", "format", "(", "epoch_num", ")", ")", "\n", "model", ".", "train", "(", ")", "\n", "p", "=", "progressbar", ".", "ProgressBar", "(", "train_step_num", ")", "\n", "p", ".", "start", "(", ")", "\n", "for", "train_step", "in", "range", "(", "train_step_num", ")", ":", "\n", "            ", "p", ".", "update", "(", "train_step", ")", "\n", "batches_processed", "+=", "1", "\n", "train_src_tensor", ",", "train_src_attn_mask", ",", "train_tgt_tensor", ",", "train_tgt_mask", "=", "data", ".", "get_next_train_batch", "(", "batch_size", ")", "\n", "if", "cuda_available", ":", "\n", "                ", "train_src_tensor", "=", "train_src_tensor", ".", "cuda", "(", "device", ")", "\n", "train_src_attn_mask", "=", "train_src_attn_mask", ".", "cuda", "(", "device", ")", "\n", "train_tgt_tensor", "=", "train_tgt_tensor", ".", "cuda", "(", "device", ")", "\n", "train_tgt_mask", "=", "train_tgt_mask", ".", "cuda", "(", "device", ")", "\n", "", "loss", "=", "model", "(", "train_src_tensor", ",", "train_src_attn_mask", ",", "train_tgt_tensor", ",", "train_tgt_mask", ")", "\n", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "loss_acm", "+=", "loss", ".", "item", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.BasicTokenizer.__init__": [[6, 12], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ")", ":", "\n", "    ", "\"\"\"Constructs a BasicTokenizer.\n    Args:\n      do_lower_case: Whether to lower case the input.\n    \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.BasicTokenizer.tokenize": [[13, 29], ["google_bert.BasicTokenizer._clean_text", "google_bert.BasicTokenizer._tokenize_chinese_chars", "google_bert.whitespace_tokenize", "google_bert.whitespace_tokenize", "split_tokens.extend", "google_bert.BasicTokenizer.lower", "google_bert.BasicTokenizer._run_strip_accents", "google_bert.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.BasicTokenizer._tokenize_chinese_chars", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.whitespace_tokenize", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.whitespace_tokenize", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "      ", "if", "self", ".", "do_lower_case", ":", "\n", "        ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.BasicTokenizer._run_strip_accents": [[30, 40], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "        ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.BasicTokenizer._run_split_on_punc": [[41, 60], ["list", "len", "google_bert._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "      ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "        ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "        ", "if", "start_new_word", ":", "\n", "          ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.BasicTokenizer._tokenize_chinese_chars": [[61, 73], ["ord", "google_bert.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "        ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "        ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.BasicTokenizer._is_chinese_char": [[74, 86], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "    ", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "      ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.BasicTokenizer._clean_text": [[87, 99], ["ord", "google_bert._is_whitespace", "google_bert._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert._is_whitespace", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "        ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "        ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "        ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.whitespace_tokenize": [[100, 107], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "  ", "\"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "    ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert._is_whitespace": [[108, 118], ["unicodedata.category"], "function", ["None"], ["", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "    ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert._is_control": [[120, 130], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "    ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert._is_punctuation": [[132, 146], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "    ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.truncate_seq_pair": [[147, 163], ["len", "len", "len", "random.random", "trunc_tokens.pop", "len", "len"], "function", ["None"], ["", "def", "truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_num_tokens", ")", ":", "\n", "  ", "\"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"", "\n", "while", "True", ":", "\n", "    ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_num_tokens", ":", "\n", "      ", "break", "\n", "\n", "", "trunc_tokens", "=", "tokens_a", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", "else", "tokens_b", "\n", "assert", "len", "(", "trunc_tokens", ")", ">=", "1", "\n", "\n", "# We want to sometimes truncate from the front and sometimes from the", "\n", "# back to add more randomness and avoid biases.", "\n", "if", "random", ".", "random", "(", ")", "<", "0.5", ":", "\n", "      ", "del", "trunc_tokens", "[", "0", "]", "\n", "", "else", ":", "\n", "      ", "trunc_tokens", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.create_instances_from_document": [[164, 251], ["random.random", "random.randint", "len", "current_chunk.append", "len", "range", "google_bert.truncate_seq_pair", "instances.append", "len", "len", "random.randint", "tokens_a.extend", "range", "random.randint", "range", "range", "len", "len", "len", "random.random", "len", "random.randint", "len", "tokens_b.extend", "len", "len", "tokens_b.extend", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.truncate_seq_pair"], ["", "", "", "def", "create_instances_from_document", "(", "\n", "all_documents", ",", "document_index", ",", "max_seq_length", ",", "short_seq_prob", "=", "0.1", ")", ":", "\n", "  ", "\"\"\"Creates `TrainingInstance`s for a single document.\"\"\"", "\n", "document", "=", "all_documents", "[", "document_index", "]", "\n", "\n", "# Account for [CLS], [SEP], [SEP]", "\n", "max_num_tokens", "=", "max_seq_length", "-", "3", "\n", "\n", "# We *usually* want to fill up the entire sequence since we are padding", "\n", "# to `max_seq_length` anyways, so short sequences are generally wasted", "\n", "# computation. However, we *sometimes*", "\n", "# (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter", "\n", "# sequences to minimize the mismatch between pre-training and fine-tuning.", "\n", "# The `target_seq_length` is just a rough target however, whereas", "\n", "# `max_seq_length` is a hard limit.", "\n", "target_seq_length", "=", "max_num_tokens", "\n", "if", "random", ".", "random", "(", ")", "<", "short_seq_prob", ":", "\n", "    ", "target_seq_length", "=", "random", ".", "randint", "(", "2", ",", "max_num_tokens", ")", "\n", "\n", "# We DON'T just concatenate all of the tokens from a document into a long", "\n", "# sequence and choose an arbitrary split point because this would make the", "\n", "# next sentence prediction task too easy. Instead, we split the input into", "\n", "# segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user", "\n", "# input.", "\n", "", "instances", "=", "[", "]", "\n", "current_chunk", "=", "[", "]", "\n", "current_length", "=", "0", "\n", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "document", ")", ":", "\n", "    ", "segment", "=", "document", "[", "i", "]", "\n", "current_chunk", ".", "append", "(", "segment", ")", "\n", "current_length", "+=", "len", "(", "segment", ")", "\n", "if", "i", "==", "len", "(", "document", ")", "-", "1", "or", "current_length", ">=", "target_seq_length", ":", "\n", "      ", "if", "current_chunk", ":", "\n", "# `a_end` is how many segments from `current_chunk` go into the `A`", "\n", "# (first) sentence.", "\n", "        ", "a_end", "=", "1", "\n", "if", "len", "(", "current_chunk", ")", ">=", "2", ":", "\n", "          ", "a_end", "=", "random", ".", "randint", "(", "1", ",", "len", "(", "current_chunk", ")", "-", "1", ")", "\n", "\n", "", "tokens_a", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "a_end", ")", ":", "\n", "          ", "tokens_a", ".", "extend", "(", "current_chunk", "[", "j", "]", ")", "\n", "\n", "", "tokens_b", "=", "[", "]", "\n", "# Random next", "\n", "is_random_next", "=", "False", "\n", "if", "len", "(", "current_chunk", ")", "==", "1", "or", "random", ".", "random", "(", ")", "<", "0.5", ":", "\n", "          ", "is_random_next", "=", "True", "\n", "target_b_length", "=", "target_seq_length", "-", "len", "(", "tokens_a", ")", "\n", "\n", "# This should rarely go for more than one iteration for large", "\n", "# corpora. However, just to be careful, we try to make sure that", "\n", "# the random document is not the same as the document", "\n", "# we're processing.", "\n", "for", "_", "in", "range", "(", "10", ")", ":", "\n", "            ", "random_document_index", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "all_documents", ")", "-", "1", ")", "\n", "if", "random_document_index", "!=", "document_index", ":", "\n", "              ", "break", "\n", "\n", "", "", "random_document", "=", "all_documents", "[", "random_document_index", "]", "\n", "random_start", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "random_document", ")", "-", "1", ")", "\n", "for", "j", "in", "range", "(", "random_start", ",", "len", "(", "random_document", ")", ")", ":", "\n", "            ", "tokens_b", ".", "extend", "(", "random_document", "[", "j", "]", ")", "\n", "if", "len", "(", "tokens_b", ")", ">=", "target_b_length", ":", "\n", "              ", "break", "\n", "# We didn't actually use these segments so we \"put them back\" so", "\n", "# they don't go to waste.", "\n", "", "", "num_unused_segments", "=", "len", "(", "current_chunk", ")", "-", "a_end", "\n", "i", "-=", "num_unused_segments", "\n", "# Actual next", "\n", "", "else", ":", "\n", "          ", "is_random_next", "=", "False", "\n", "for", "j", "in", "range", "(", "a_end", ",", "len", "(", "current_chunk", ")", ")", ":", "\n", "            ", "tokens_b", ".", "extend", "(", "current_chunk", "[", "j", "]", ")", "\n", "\n", "", "", "truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_num_tokens", ")", "\n", "\n", "assert", "len", "(", "tokens_a", ")", ">=", "1", "\n", "assert", "len", "(", "tokens_b", ")", ">=", "1", "\n", "instance", "=", "(", "tokens_a", ",", "tokens_b", ",", "is_random_next", ")", "\n", "instances", ".", "append", "(", "instance", ")", "\n", "", "current_chunk", "=", "[", "]", "\n", "current_length", "=", "0", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "instances", "\n", "", ""]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.__init__": [[9, 25], ["tokenizer.convert_tokens_to_ids", "open", "dataclass_chinese.PretrainCorpus.load_lines"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.load_lines"], ["    ", "def", "__init__", "(", "self", ",", "tokenizer", ",", "filename", ",", "max_len", ",", "whole_word_masking", "=", "False", ")", ":", "\n", "        ", "'''\n            tokenizer: BERT tokenizer\n            filename: pretraining corpus\n            max_len: maximum length for each sentence\n        '''", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "vocab_size", "=", "self", ".", "tokenizer", ".", "vocab_size", "\n", "self", ".", "special_token_id_list", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "UNK", ",", "SEP", ",", "PAD", ",", "CLS", ",", "MASK", "]", ")", "\n", "self", ".", "unk_id", ",", "self", ".", "sep_id", ",", "self", ".", "pad_id", ",", "self", ".", "cls_id", ",", "self", ".", "mask_id", "=", "self", ".", "special_token_id_list", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "filename", "=", "filename", "\n", "self", ".", "stream", "=", "open", "(", "self", ".", "filename", ",", "encoding", "=", "'utf8'", ")", "\n", "self", ".", "epoch_id", "=", "0", "\n", "self", ".", "whole_word_masking", "=", "whole_word_masking", "\n", "self", ".", "load_lines", "(", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.load_lines": [[26, 48], ["print", "random.shuffle", "enumerate", "print", "open", "i.readlines", "line.strip().strip().split", "data.extend", "len", "docs[].append", "docs.append", "google_bert.create_instances_from_document", "len", "range", "line.strip().strip", "len", "line.strip"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.create_instances_from_document"], ["", "def", "load_lines", "(", "self", ",", "filename", ")", ":", "\n", "        ", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "i", ":", "\n", "            ", "lines", "=", "i", ".", "readlines", "(", ")", "\n", "", "print", "(", "'Number of lines is {}'", ".", "format", "(", "len", "(", "lines", ")", ")", ")", "\n", "docs", "=", "[", "[", "]", "]", "\n", "for", "line", "in", "lines", ":", "\n", "            ", "tokens", "=", "line", ".", "strip", "(", "'\\n'", ")", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "if", "tokens", ":", "\n", "                ", "docs", "[", "-", "1", "]", ".", "append", "(", "tokens", ")", "\n", "", "else", ":", "\n", "                ", "docs", ".", "append", "(", "[", "]", ")", "\n", "\n", "", "", "docs", "=", "[", "x", "for", "x", "in", "docs", "if", "x", "]", "# filter out empty lines", "\n", "self", ".", "docs", "=", "docs", "\n", "random", ".", "shuffle", "(", "docs", ")", "\n", "\n", "data", "=", "[", "]", "\n", "for", "idx", ",", "doc", "in", "enumerate", "(", "docs", ")", ":", "\n", "            ", "data", ".", "extend", "(", "create_instances_from_document", "(", "docs", ",", "idx", ",", "self", ".", "max_len", ")", ")", "\n", "", "self", ".", "data", "=", "data", "\n", "print", "(", "'number of sentence pairs is {}'", ".", "format", "(", "len", "(", "self", ".", "data", ")", ")", ")", "\n", "self", ".", "train_idx_list", "=", "[", "i", "for", "i", "in", "range", "(", "len", "(", "self", ".", "data", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.get_batch_data": [[49, 55], ["random.sample", "dataclass_chinese.PretrainCorpus.batchify", "batch_data.append"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.batchify"], ["", "def", "get_batch_data", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "batch_idx_list", "=", "random", ".", "sample", "(", "self", ".", "train_idx_list", ",", "batch_size", ")", "\n", "batch_data", "=", "[", "]", "\n", "for", "idx", "in", "batch_idx_list", ":", "\n", "            ", "batch_data", ".", "append", "(", "self", ".", "data", "[", "idx", "]", ")", "\n", "", "return", "self", ".", "batchify", "(", "batch_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.random_token": [[56, 62], ["numpy.random.randint", "dataclass_chinese.PretrainCorpus.tokenizer.convert_ids_to_tokens", "numpy.random.randint"], "methods", ["None"], ["", "def", "random_token", "(", "self", ")", ":", "\n", "        ", "rand_idx", "=", "1", "+", "np", ".", "random", ".", "randint", "(", "self", ".", "vocab_size", "-", "1", ")", "\n", "while", "rand_idx", "in", "self", ".", "special_token_id_list", ":", "\n", "            ", "rand_idx", "=", "1", "+", "np", ".", "random", ".", "randint", "(", "self", ".", "vocab_size", "-", "1", ")", "\n", "", "random_token", "=", "self", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "[", "rand_idx", "]", ")", "[", "0", "]", "\n", "return", "random_token", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.random_mask": [[63, 92], ["min", "enumerate", "random.shuffle", "set", "enumerate", "max", "set.append", "int", "mask.append", "tgt.append", "masked_tokens.append", "mask.append", "tgt.append", "round", "random.random", "masked_tokens.append", "random.random", "masked_tokens.append", "masked_tokens.append", "len", "dataclass_chinese.PretrainCorpus.random_token"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.random_token"], ["", "def", "random_mask", "(", "self", ",", "tokens", ",", "masked_lm_prob", ",", "max_predictions_per_seq", ")", ":", "\n", "        ", "num_to_predict", "=", "min", "(", "max_predictions_per_seq", ",", "max", "(", "1", ",", "int", "(", "round", "(", "len", "(", "tokens", ")", "*", "masked_lm_prob", ")", ")", ")", ")", "\n", "masked_tokens", ",", "mask", "=", "[", "]", ",", "[", "]", "\n", "cand", "=", "[", "]", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "#if token == CLS or token == SEP:", "\n", "            ", "if", "token", "in", "[", "UNK", ",", "SEP", ",", "PAD", ",", "CLS", ",", "MASK", "]", ":", "# do not learn to predict special tokens", "\n", "                ", "continue", "\n", "", "cand", ".", "append", "(", "i", ")", "\n", "", "random", ".", "shuffle", "(", "cand", ")", "\n", "cand", "=", "set", "(", "cand", "[", ":", "num_to_predict", "]", ")", "\n", "\n", "masked_tokens", ",", "mask", ",", "tgt", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "if", "i", "in", "cand", ":", "\n", "                ", "if", "random", ".", "random", "(", ")", "<", "0.8", ":", "\n", "                    ", "masked_tokens", ".", "append", "(", "MASK", ")", "\n", "", "else", ":", "\n", "                    ", "if", "random", ".", "random", "(", ")", "<", "0.5", ":", "\n", "                        ", "masked_tokens", ".", "append", "(", "token", ")", "\n", "", "else", ":", "\n", "                        ", "masked_tokens", ".", "append", "(", "self", ".", "random_token", "(", ")", ")", "\n", "", "", "mask", ".", "append", "(", "1", ")", "\n", "tgt", ".", "append", "(", "token", ")", "\n", "", "else", ":", "\n", "                ", "masked_tokens", ".", "append", "(", "token", ")", "\n", "mask", ".", "append", "(", "0", ")", "\n", "tgt", ".", "append", "(", "PAD", ")", "\n", "", "", "return", "masked_tokens", ",", "mask", ",", "tgt", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.whole_word_random_mask": [[93, 137], ["len", "min", "enumerate", "random.shuffle", "set", "enumerate", "int", "max", "set.append", "len", "len", "len", "len", "int", "round", "mask.append", "tgt.append", "masked_tokens.append", "mask.append", "tgt.append", "random.random", "masked_tokens.append", "masked_tokens.append", "mask.append", "tgt.append", "len", "random.random", "masked_tokens.append", "masked_tokens.append", "dataclass_chinese.PretrainCorpus.random_token"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.random_token"], ["", "def", "whole_word_random_mask", "(", "self", ",", "tokens", ",", "masked_lm_prob", ")", ":", "\n", "# tokens is a list of words, each work could contain multiple characters", "\n", "        ", "num_tokens", "=", "len", "(", "tokens", ")", "\n", "num_to_predict", "=", "min", "(", "int", "(", "num_tokens", "*", "0.25", ")", ",", "max", "(", "1", ",", "int", "(", "round", "(", "len", "(", "tokens", ")", "*", "masked_lm_prob", ")", ")", ")", ")", "\n", "masked_tokens", ",", "mask", "=", "[", "]", ",", "[", "]", "\n", "cand", "=", "[", "]", "# token index ", "\n", "total_char_num", "=", "0", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "#if token == CLS or token == SEP:", "\n", "            ", "if", "token", "in", "[", "UNK", ",", "SEP", ",", "PAD", ",", "CLS", ",", "MASK", "]", ":", "# do not learn to predict special tokens", "\n", "                ", "continue", "\n", "", "cand", ".", "append", "(", "i", ")", "\n", "", "random", ".", "shuffle", "(", "cand", ")", "\n", "cand", "=", "set", "(", "cand", "[", ":", "num_to_predict", "]", ")", "\n", "\n", "masked_tokens", ",", "mask", ",", "tgt", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "if", "i", "in", "cand", ":", "\n", "                ", "for", "char", "in", "token", ":", "\n", "                    ", "if", "random", ".", "random", "(", ")", "<", "0.8", ":", "\n", "                        ", "masked_tokens", ".", "append", "(", "MASK", ")", "\n", "", "else", ":", "\n", "                        ", "if", "random", ".", "random", "(", ")", "<", "0.5", ":", "\n", "                            ", "masked_tokens", ".", "append", "(", "char", ")", "\n", "", "else", ":", "\n", "                            ", "masked_tokens", ".", "append", "(", "self", ".", "random_token", "(", ")", ")", "\n", "", "", "mask", ".", "append", "(", "1", ")", "\n", "tgt", ".", "append", "(", "char", ")", "\n", "", "", "else", ":", "\n", "                ", "if", "token", "in", "[", "UNK", ",", "SEP", ",", "PAD", ",", "CLS", ",", "MASK", "]", ":", "# special tokens", "\n", "                    ", "masked_tokens", ".", "append", "(", "token", ")", "\n", "mask", ".", "append", "(", "0", ")", "\n", "tgt", ".", "append", "(", "PAD", ")", "\n", "", "else", ":", "\n", "                    ", "for", "char", "in", "token", ":", "\n", "                        ", "masked_tokens", ".", "append", "(", "char", ")", "\n", "mask", ".", "append", "(", "0", ")", "\n", "tgt", ".", "append", "(", "PAD", ")", "\n", "", "", "", "", "masked_tokens", "=", "masked_tokens", "[", ":", "-", "1", "]", "[", ":", "self", ".", "max_len", "-", "1", "]", "+", "[", "masked_tokens", "[", "-", "1", "]", "]", "\n", "mask", "=", "mask", "[", ":", "-", "1", "]", "[", ":", "self", ".", "max_len", "-", "1", "]", "+", "[", "mask", "[", "-", "1", "]", "]", "# keep number of max_len characters", "\n", "tgt", "=", "tgt", "[", ":", "-", "1", "]", "[", ":", "self", ".", "max_len", "-", "1", "]", "+", "[", "tgt", "[", "-", "1", "]", "]", "\n", "assert", "len", "(", "masked_tokens", ")", "==", "len", "(", "mask", ")", "\n", "assert", "len", "(", "mask", ")", "==", "len", "(", "tgt", ")", "\n", "return", "masked_tokens", ",", "mask", ",", "tgt", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.ListsToTensor": [[138, 149], ["max", "torch.LongTensor().contiguous", "ys.append", "len", "torch.LongTensor", "dataclass_chinese.PretrainCorpus.tokenizer.convert_tokens_to_ids", "len", "len"], "methods", ["None"], ["", "def", "ListsToTensor", "(", "self", ",", "xs", ",", "tokenize", "=", "False", ")", ":", "\n", "        ", "max_len", "=", "max", "(", "len", "(", "x", ")", "for", "x", "in", "xs", ")", "\n", "ys", "=", "[", "]", "\n", "for", "x", "in", "xs", ":", "\n", "            ", "if", "tokenize", ":", "\n", "                ", "y", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "x", ")", "+", "[", "self", ".", "pad_id", "]", "*", "(", "self", ".", "max_len", "-", "len", "(", "x", ")", ")", "\n", "", "else", ":", "\n", "                ", "y", "=", "x", "+", "[", "0", "]", "*", "(", "self", ".", "max_len", "-", "len", "(", "x", ")", ")", "\n", "", "ys", ".", "append", "(", "y", ")", "\n", "", "data", "=", "torch", ".", "LongTensor", "(", "ys", ")", ".", "contiguous", "(", ")", "# bsz x seqlen", "\n", "return", "data", "# bsz x seqlen", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.process_tgt": [[150, 163], ["max", "torch.LongTensor().contiguous", "torch.LongTensor().contiguous.clone", "torch.LongTensor().contiguous.clone", "torch.LongTensor().contiguous.append", "torch.LongTensor().contiguous.clone.type", "len", "dataclass_chinese.PretrainCorpus.tokenizer.convert_tokens_to_ids", "torch.LongTensor", "len"], "methods", ["None"], ["", "def", "process_tgt", "(", "self", ",", "tgt_matrix", ")", ":", "\n", "        ", "max_len", "=", "max", "(", "len", "(", "x", ")", "for", "x", "in", "tgt_matrix", ")", "\n", "ys", "=", "[", "]", "\n", "for", "x", "in", "tgt_matrix", ":", "\n", "            ", "y", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "x", ")", "+", "[", "self", ".", "pad_id", "]", "*", "(", "self", ".", "max_len", "-", "len", "(", "x", ")", ")", "\n", "ys", ".", "append", "(", "y", ")", "\n", "", "ys", "=", "torch", ".", "LongTensor", "(", "ys", ")", ".", "contiguous", "(", ")", "# bsz x seqlen", "\n", "labels", "=", "ys", ".", "clone", "(", ")", "\n", "labels", "[", "labels", "[", ":", ",", ":", "]", "==", "self", ".", "pad_id", "]", "=", "-", "100", "\n", "contrastive_labels", "=", "ys", ".", "clone", "(", ")", "\n", "contrastive_labels", "[", "contrastive_labels", "[", ":", ",", ":", "]", "==", "self", ".", "pad_id", "]", "=", "0", "\n", "contrastive_labels", "[", "contrastive_labels", "[", ":", ",", ":", "]", "!=", "self", ".", "pad_id", "]", "=", "1", "\n", "return", "labels", ",", "contrastive_labels", ".", "type", "(", "torch", ".", "FloatTensor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.batchify": [[164, 221], ["dataclass_chinese.PretrainCorpus.ListsToTensor", "dataclass_chinese.PretrainCorpus.ListsToTensor", "dataclass_chinese.PretrainCorpus.ListsToTensor", "dataclass_chinese.PretrainCorpus.ListsToTensor().to", "torch.ByteTensor", "dataclass_chinese.PretrainCorpus.process_tgt", "dataclass_chinese.PretrainCorpus.append", "dataclass_chinese.PretrainCorpus.append", "tgt_matrix.append", "dataclass_chinese.PretrainCorpus.eq", "dataclass_chinese.PretrainCorpus.whole_word_random_mask", "truth_char.append", "truth_char.append", "seg_char.append", "dataclass_chinese.PretrainCorpus.append", "dataclass_chinese.PretrainCorpus.append", "dataclass_chinese.PretrainCorpus.append", "dataclass_chinese.PretrainCorpus.append", "dataclass_chinese.PretrainCorpus.random_mask", "torch.ByteTensor.append", "torch.ByteTensor.append", "dataclass_chinese.PretrainCorpus.ListsToTensor", "len", "len", "len", "len", "len", "len", "len", "len", "truth_char.append", "truth_char.append", "seg_char.append", "len", "len"], "methods", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.ListsToTensor", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.ListsToTensor", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.ListsToTensor", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.process_tgt", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.whole_word_random_mask", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.random_mask", "home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.dataclass_chinese.PretrainCorpus.ListsToTensor"], ["", "def", "batchify", "(", "self", ",", "data", ")", ":", "\n", "        ", "truth", ",", "inp", ",", "seg", ",", "msk", ",", "tgt_matrix", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "nxt_snt_flag", "=", "[", "]", "\n", "for", "a", ",", "b", ",", "r", "in", "data", ":", "\n", "#print (a, b, r)", "\n", "#raise Exception()", "\n", "            ", "'''\n                a, b are two tokenized sequence\n                r indicates whether two sentences are continuation, True or False\n            '''", "\n", "\n", "if", "self", ".", "whole_word_masking", ":", "\n", "                ", "x", "=", "[", "CLS", "]", "+", "a", "+", "[", "SEP", "]", "+", "b", "+", "[", "SEP", "]", "\n", "masked_x", ",", "mask", ",", "tgt", "=", "self", ".", "whole_word_random_mask", "(", "x", ",", "0.15", ")", "\n", "truth_char", "=", "[", "CLS", "]", "\n", "seg_char", "=", "[", "]", "\n", "for", "word", "in", "a", ":", "\n", "                    ", "for", "char", "in", "word", ":", "\n", "                        ", "truth_char", ".", "append", "(", "char", ")", "\n", "", "", "truth_char", ".", "append", "(", "SEP", ")", "\n", "seg_char", "+=", "[", "0", "for", "_", "in", "truth_char", "]", "\n", "for", "word", "in", "b", ":", "\n", "                    ", "for", "char", "in", "word", ":", "\n", "                        ", "truth_char", ".", "append", "(", "char", ")", "\n", "seg_char", ".", "append", "(", "1", ")", "\n", "", "", "truth_char", ".", "append", "(", "SEP", ")", "\n", "seg_char", ".", "append", "(", "1", ")", "\n", "assert", "len", "(", "truth_char", ")", "==", "len", "(", "seg_char", ")", "\n", "truth_char", "=", "truth_char", "[", ":", "-", "1", "]", "[", ":", "self", ".", "max_len", "-", "1", "]", "+", "[", "truth_char", "[", "-", "1", "]", "]", "\n", "seg_char", "=", "seg_char", "[", ":", "-", "1", "]", "[", ":", "self", ".", "max_len", "-", "1", "]", "+", "[", "seg_char", "[", "-", "1", "]", "]", "\n", "truth", ".", "append", "(", "truth_char", ")", "\n", "seg", ".", "append", "(", "seg_char", ")", "\n", "assert", "len", "(", "masked_x", ")", "==", "len", "(", "mask", ")", "\n", "assert", "len", "(", "mask", ")", "==", "len", "(", "truth_char", ")", "\n", "assert", "len", "(", "truth_char", ")", "==", "len", "(", "seg_char", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "[", "CLS", "]", "+", "a", "+", "[", "SEP", "]", "+", "b", "+", "[", "SEP", "]", "\n", "truth", ".", "append", "(", "x", ")", "\n", "seg", ".", "append", "(", "[", "0", "]", "*", "(", "len", "(", "a", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "b", ")", "+", "1", ")", ")", "\n", "masked_x", ",", "mask", ",", "tgt", "=", "self", ".", "random_mask", "(", "x", ",", "0.15", ",", "self", ".", "max_len", "*", "0.25", ")", "\n", "\n", "", "inp", ".", "append", "(", "masked_x", ")", "\n", "msk", ".", "append", "(", "mask", ")", "\n", "tgt_matrix", ".", "append", "(", "tgt", ")", "\n", "if", "r", ":", "# r stands for is_random_text", "\n", "                ", "nxt_snt_flag", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "nxt_snt_flag", ".", "append", "(", "0", ")", "\n", "\n", "", "", "truth", "=", "self", ".", "ListsToTensor", "(", "truth", ",", "tokenize", "=", "True", ")", "\n", "inp", "=", "self", ".", "ListsToTensor", "(", "inp", ",", "tokenize", "=", "True", ")", "\n", "seg", "=", "self", ".", "ListsToTensor", "(", "seg", ",", "tokenize", "=", "False", ")", "\n", "msk", "=", "self", ".", "ListsToTensor", "(", "msk", ",", "tokenize", "=", "False", ")", ".", "to", "(", "torch", ".", "uint8", ")", "# bsz x seqlen", "\n", "attn_msk", "=", "~", "inp", ".", "eq", "(", "self", ".", "pad_id", ")", "\n", "nxt_snt_flag", "=", "torch", ".", "ByteTensor", "(", "nxt_snt_flag", ")", "\n", "labels", ",", "contrastive_labels", "=", "self", ".", "process_tgt", "(", "tgt_matrix", ")", "\n", "return", "truth", ",", "inp", ",", "seg", ",", "msk", ",", "attn_msk", ",", "labels", ",", "contrastive_labels", ",", "nxt_snt_flag", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yxuansu_tacl.english.funcs.split_into_sentences": [[11, 36], ["text.replace.replace", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "text.replace.replace", "text.replace.replace", "text.replace.replace", "text.replace.replace", "text.replace.split", "text.replace.replace", "text.replace.replace", "text.replace.replace", "text.replace.replace", "text.replace.replace", "s.strip"], "function", ["None"], ["def", "split_into_sentences", "(", "text", ")", ":", "\n", "    ", "text", "=", "\" \"", "+", "text", "+", "\"  \"", "\n", "text", "=", "text", ".", "replace", "(", "\"\\n\"", ",", "\" \"", ")", "\n", "text", "=", "re", ".", "sub", "(", "prefixes", ",", "\"\\\\1<prd>\"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "websites", ",", "\"<prd>\\\\1\"", ",", "text", ")", "\n", "if", "\"Ph.D\"", "in", "text", ":", "text", "=", "text", ".", "replace", "(", "\"Ph.D.\"", ",", "\"Ph<prd>D<prd>\"", ")", "\n", "text", "=", "re", ".", "sub", "(", "\"\\s\"", "+", "alphabets", "+", "\"[.] \"", ",", "\" \\\\1<prd> \"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "acronyms", "+", "\" \"", "+", "starters", ",", "\"\\\\1<stop> \\\\2\"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "alphabets", "+", "\"[.]\"", "+", "alphabets", "+", "\"[.]\"", "+", "alphabets", "+", "\"[.]\"", ",", "\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "alphabets", "+", "\"[.]\"", "+", "alphabets", "+", "\"[.]\"", ",", "\"\\\\1<prd>\\\\2<prd>\"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "\" \"", "+", "suffixes", "+", "\"[.] \"", "+", "starters", ",", "\" \\\\1<stop> \\\\2\"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "\" \"", "+", "suffixes", "+", "\"[.]\"", ",", "\" \\\\1<prd>\"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "\" \"", "+", "alphabets", "+", "\"[.]\"", ",", "\" \\\\1<prd>\"", ",", "text", ")", "\n", "if", "\"\u201d\"", "in", "text", ":", "text", "=", "text", ".", "replace", "(", "\".\u201d\"", ",", "\"\u201d.\"", ")", "\n", "if", "\"\\\"\"", "in", "text", ":", "text", "=", "text", ".", "replace", "(", "\".\\\"\"", ",", "\"\\\".\"", ")", "\n", "if", "\"!\"", "in", "text", ":", "text", "=", "text", ".", "replace", "(", "\"!\\\"\"", ",", "\"\\\"!\"", ")", "\n", "if", "\"?\"", "in", "text", ":", "text", "=", "text", ".", "replace", "(", "\"?\\\"\"", ",", "\"\\\"?\"", ")", "\n", "text", "=", "text", ".", "replace", "(", "\".\"", ",", "\".<stop>\"", ")", "\n", "text", "=", "text", ".", "replace", "(", "\"?\"", ",", "\"?<stop>\"", ")", "\n", "text", "=", "text", ".", "replace", "(", "\"!\"", ",", "\"!<stop>\"", ")", "\n", "text", "=", "text", ".", "replace", "(", "\"<prd>\"", ",", "\".\"", ")", "\n", "sentences", "=", "text", ".", "split", "(", "\"<stop>\"", ")", "\n", "sentences", "=", "sentences", "[", ":", "-", "1", "]", "\n", "sentences", "=", "[", "s", ".", "strip", "(", ")", "for", "s", "in", "sentences", "]", "\n", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.english.funcs.process_chunk_data": [[37, 63], ["article.split", "funcs.split_into_sentences", "text.startswith", "len", "res_list.append", "text.split", "len", "sentence_list.append"], "function", ["home.repos.pwc.inspect_result.yxuansu_tacl.english.funcs.split_into_sentences"], ["", "def", "process_chunk_data", "(", "item", ",", "stop_prefix_list", ")", ":", "\n", "    ", "article", "=", "item", "[", "'text'", "]", "\n", "article_list", "=", "article", ".", "split", "(", "'\\n'", ")", "\n", "res_list", "=", "[", "]", "\n", "break_flag", "=", "False", "\n", "for", "text", "in", "article_list", ":", "\n", "        ", "for", "prefix", "in", "stop_prefix_list", ":", "\n", "            ", "if", "text", ".", "startswith", "(", "prefix", ")", ":", "\n", "                ", "break_flag", "=", "True", "\n", "\n", "", "", "if", "len", "(", "text", ".", "split", "(", ")", ")", "<", "3", ":", "\n", "            ", "pass", "\n", "", "else", ":", "\n", "            ", "res_list", ".", "append", "(", "text", ")", "\n", "", "if", "break_flag", ":", "\n", "            ", "break", "\n", "\n", "", "", "sentence_list", "=", "[", "]", "\n", "for", "text", "in", "res_list", ":", "\n", "        ", "one_sen_list", "=", "split_into_sentences", "(", "text", ")", "\n", "for", "sen", "in", "one_sen_list", ":", "\n", "            ", "if", "len", "(", "sen", ")", "<", "3", ":", "\n", "                ", "pass", "\n", "", "else", ":", "\n", "                ", "sentence_list", ".", "append", "(", "sen", ")", "\n", "", "", "", "return", "sentence_list", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.english.funcs.process_corpus": [[64, 78], ["len", "progressbar.ProgressBar", "print", "progressbar.ProgressBar.start", "range", "progressbar.ProgressBar.finish", "progressbar.ProgressBar.update", "funcs.process_chunk_data", "len"], "function", ["home.repos.pwc.inspect_result.yxuansu_tacl.english.funcs.process_chunk_data"], ["", "def", "process_corpus", "(", "dataset", ",", "stop_prefix_list", ")", ":", "\n", "    ", "doc_num", "=", "len", "(", "dataset", ")", "\n", "p", "=", "progressbar", ".", "ProgressBar", "(", "doc_num", ")", "\n", "print", "(", "'Start processing data...'", ")", "\n", "p", ".", "start", "(", ")", "\n", "all_doc_list", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "doc_num", ")", ":", "\n", "        ", "p", ".", "update", "(", "idx", ")", "\n", "sentence_list", "=", "process_chunk_data", "(", "dataset", "[", "idx", "]", ",", "stop_prefix_list", ")", "\n", "if", "len", "(", "sentence_list", ")", "<", "2", ":", "\n", "            ", "continue", "\n", "", "all_doc_list", "+=", "[", "sentence_list", "]", "\n", "", "p", ".", "finish", "(", ")", "\n", "return", "all_doc_list", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.english.tokenize_data.tokenize_line": [[4, 7], ["tokenizer.tokenize", "text.strip"], "function", ["home.repos.pwc.inspect_result.yxuansu_tacl.pretraining.google_bert.BasicTokenizer.tokenize"], ["def", "tokenize_line", "(", "text", ",", "tokenizer", ")", ":", "\n", "    ", "token_list", "=", "tokenizer", ".", "tokenize", "(", "text", ".", "strip", "(", "'\\n'", ")", ",", "max_length", "=", "512", ",", "truncation", "=", "True", ")", "\n", "return", "' '", ".", "join", "(", "token_list", ")", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.english.tokenize_data.process_file": [[8, 25], ["len", "open", "i.readlines", "open", "progressbar.ProgressBar", "progressbar.ProgressBar.start", "range", "progressbar.ProgressBar.finish", "progressbar.ProgressBar.update", "tokenize_data.tokenize_line", "line.strip", "len", "o.writelines", "o.writelines"], "function", ["home.repos.pwc.inspect_result.yxuansu_tacl.english.tokenize_data.tokenize_line"], ["", "def", "process_file", "(", "in_f", ",", "out_f", ",", "tokenizer", ")", ":", "\n", "    ", "with", "open", "(", "in_f", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "i", ":", "\n", "        ", "lines", "=", "i", ".", "readlines", "(", ")", "\n", "", "line_num", "=", "len", "(", "lines", ")", "\n", "\n", "with", "open", "(", "out_f", ",", "'w'", ",", "encoding", "=", "'utf8'", ")", "as", "o", ":", "\n", "        ", "p", "=", "progressbar", ".", "ProgressBar", "(", "line_num", ")", "\n", "p", ".", "start", "(", ")", "\n", "for", "idx", "in", "range", "(", "line_num", ")", ":", "\n", "            ", "p", ".", "update", "(", "idx", ")", "\n", "line", "=", "lines", "[", "idx", "]", "\n", "tokenized_text", "=", "tokenize_line", "(", "line", ".", "strip", "(", "'\\n'", ")", ",", "tokenizer", ")", "\n", "if", "len", "(", "tokenized_text", ")", "==", "0", ":", "\n", "                ", "o", ".", "writelines", "(", "'\\n'", ")", "\n", "", "else", ":", "\n", "                ", "o", ".", "writelines", "(", "tokenized_text", "+", "'\\n'", ")", "\n", "", "", "p", ".", "finish", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yxuansu_tacl.english.tokenize_data.parse_config": [[26, 33], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["", "", "def", "parse_config", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_name\"", ",", "type", "=", "str", ",", "help", "=", "\"roberta-* or bert-*\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--raw_data_path\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_name\"", ",", "type", "=", "str", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]]}