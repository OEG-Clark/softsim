{"home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.feature_extraction_functions.train_test_corpus": [[19, 104], ["database_functions.database_connect", "database_functions.db_to_df", "all_data[].set_index().sort_values", "all_data[].set_index().sort_index", "all_data[].set_index().sort_values", "pickle.dump", "pickle.dump", "web_corpus_train[].append", "pm_corpus_train[].append", "pickle.dump", "web_corpus_train[].append.append", "pickle.dump", "pandas.to_pickle", "print", "print", "print", "open", "open", "open", "len", "len", "len", "len", "len", "len", "open", "all_data[].set_index", "all_data[].set_index", "all_data[].set_index", "all_data[].set_index().sort_index.index.isin", "all_data[].set_index().sort_values.index.isin", "all_data[].set_index().sort_index.index.isin", "all_data[].set_index().sort_values.index.isin", "len", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.database_functions.database_connect", "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.database_functions.db_to_df"], ["def", "train_test_corpus", "(", ")", ":", "\n", "\n", "# DATABASE DOWNLOAD", "\n", "# Established database connection", "\n", "    ", "cnxn", "=", "db_fnc", ".", "database_connect", "(", "db_fnc", ".", "db", ",", "db_fnc", ".", "host", ",", "db_fnc", ".", "user", ",", "db_fnc", ".", "password", ")", "\n", "all_data", "=", "db_fnc", ".", "db_to_df", "(", "db_fnc", ".", "final_corpora_scripts", ",", "cnxn", ")", "\n", "\n", "# Extracts each dataset (database table) from list", "\n", "pm_corpus_1", "=", "all_data", "[", "0", "]", ".", "set_index", "(", "'pmid'", ")", ".", "sort_values", "(", "'web_id'", ")", "\n", "web_corpus_1", "=", "all_data", "[", "1", "]", ".", "set_index", "(", "'web_id'", ")", ".", "sort_index", "(", ")", "\n", "corpora_links_1", "=", "all_data", "[", "2", "]", ".", "set_index", "(", "'web_id'", ")", ".", "sort_values", "(", "'web_id'", ")", "\n", "\n", "# Select web-PubMed links in test datasetsv (training set only used for fitting CCA model)", "\n", "# NOTE: PubMed test set contains both linked and unlinked articles [n = len(pm_corpus_1) - len(pm_corpus_train)]", "\n", "corpora_links_train", "=", "corpora_links_1", ".", "loc", "[", "corpora_links_1", "[", "'train_test'", "]", "==", "'Train'", "]", "\n", "web_corpus_train", "=", "web_corpus_1", ".", "loc", "[", "web_corpus_1", ".", "index", ".", "isin", "(", "corpora_links_train", ".", "index", ")", "]", "\n", "pm_corpus_train", "=", "pm_corpus_1", ".", "loc", "[", "pm_corpus_1", ".", "index", ".", "isin", "(", "corpora_links_train", "[", "'pmid'", "]", ")", "]", "\n", "\n", "all_train_datasets", "=", "{", "'corpora_links_train'", ":", "corpora_links_train", ",", "\n", "'web_corpus_train'", ":", "web_corpus_train", ",", "\n", "'pm_corpus_train'", ":", "pm_corpus_train", ",", "\n", "}", "\n", "pickle", ".", "dump", "(", "all_train_datasets", ",", "\n", "open", "(", "'all_train_datasets.pkl'", ",", "'wb'", ")", ",", "\n", "protocol", "=", "2", ",", "\n", ")", "\n", "\n", "corpora_links_test", "=", "corpora_links_1", ".", "loc", "[", "corpora_links_1", "[", "'train_test'", "]", "==", "'Test'", "]", "\n", "web_corpus_test", "=", "web_corpus_1", ".", "loc", "[", "~", "web_corpus_1", ".", "index", ".", "isin", "(", "corpora_links_train", ".", "index", ")", "]", "\n", "pm_corpus_test", "=", "pm_corpus_1", ".", "loc", "[", "~", "pm_corpus_1", ".", "index", ".", "isin", "(", "corpora_links_train", "[", "'pmid'", "]", ")", "]", "\n", "all_test_datasets", "=", "{", "'corpora_links_test'", ":", "corpora_links_test", ",", "\n", "'web_corpus_test'", ":", "web_corpus_test", ",", "\n", "'pm_corpus_test'", ":", "pm_corpus_test", ",", "\n", "}", "\n", "pickle", ".", "dump", "(", "all_test_datasets", ",", "\n", "open", "(", "'all_test_datasets.pkl'", ",", "'wb'", ")", ",", "\n", "protocol", "=", "2", ",", "\n", ")", "\n", "\n", "# Generates full combined corpus appending all sample web articles to all PubMed articles", "\n", "# Specific order to allow for later separation into individual web/PubMed training and test datasets", "\n", "# full_corpus_train = web_corpus_train['corpus_text'].append(pm_corpus_train['corpus_text'])", "\n", "# full_corpus_test = web_corpus_test['corpus_text'].append(pm_corpus_test['corpus_text'])", "\n", "\n", "full_web", "=", "web_corpus_train", "[", "'corpus_text'", "]", ".", "append", "(", "web_corpus_test", "[", "'corpus_text'", "]", ")", "\n", "full_pm", "=", "pm_corpus_train", "[", "'corpus_text'", "]", ".", "append", "(", "pm_corpus_test", "[", "'corpus_text'", "]", ")", "\n", "pickle", ".", "dump", "(", "{", "'full_web'", ":", "full_web", ",", "\n", "'full_pm'", ":", "full_pm", ",", "\n", "}", ",", "\n", "open", "(", "'full_web_pm_matrix_order.pkl'", ",", "'wb'", ")", ",", "\n", "protocol", "=", "2", ",", "\n", ")", "\n", "\n", "full_corpus", "=", "full_web", ".", "append", "(", "full_pm", ")", "\n", "\n", "corpus_lengths", "=", "{", "'web_corpus_train'", ":", "len", "(", "web_corpus_train", ")", ",", "\n", "'web_corpus_test'", ":", "len", "(", "web_corpus_test", ")", ",", "\n", "'pm_corpus_train'", ":", "len", "(", "pm_corpus_train", ")", ",", "\n", "'pm_corpus_test'", ":", "len", "(", "pm_corpus_test", ")", ",", "\n", "'full_web'", ":", "len", "(", "full_web", ")", ",", "\n", "'full_pm'", ":", "len", "(", "full_pm", ",", ")", "\n", "}", "\n", "pickle", ".", "dump", "(", "corpus_lengths", ",", "\n", "open", "(", "'test_train_corpus_lengths.pkl'", ",", "'wb'", ")", ",", "\n", "protocol", "=", "2", ",", "\n", ")", "\n", "\n", "pd", ".", "to_pickle", "(", "full_corpus", ",", "\n", "'full_web_pm_corpus_text.pkl'", ",", "\n", "protocol", "=", "2", ",", "\n", ")", "\n", "\n", "print", "(", "'%s web documents + %s PubMed documents in training set'", "%", "(", "len", "(", "web_corpus_train", ")", ",", "\n", "len", "(", "pm_corpus_train", ")", ")", ")", "\n", "print", "(", "'%s web documents + %s PubMed documents in test set'", "%", "(", "len", "(", "web_corpus_test", ")", ",", "\n", "len", "(", "pm_corpus_test", ")", ")", ")", "\n", "print", "(", "'There are %s known links in the training set and %s known links in the test set'", "%", "(", "len", "(", "corpora_links_train", ")", ",", "\n", "len", "(", "corpora_links_test", ")", ")", ")", "\n", "return_vals", "=", "(", "pm_corpus_1", ",", "\n", "web_corpus_1", ",", "\n", "full_corpus", ",", "\n", "corpora_links_1", ",", "\n", ")", "\n", "\n", "return", "return_vals", "\n", "\n"]], "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.feature_extraction_functions.vocab_gen": [[106, 173], ["sklearn.feature_extraction.text.CountVectorizer", "sklearn.feature_extraction.text.CountVectorizer.fit_transform", "pandas.Series", "print", "sklearn.feature_extraction.text.CountVectorizer", "sklearn.feature_extraction.text.CountVectorizer.fit_transform", "pandas.Series", "print", "shared_vocab_no_threshold[].reset_index", "print", "pandas.to_pickle", "sklearn.feature_extraction.text.CountVectorizer.get_feature_names", "sklearn.feature_extraction.text.CountVectorizer.get_feature_names", "sklearn.feature_extraction.text.CountVectorizer", "sklearn.feature_extraction.text.CountVectorizer.fit_transform", "pandas.Series", "print", "sklearn.feature_extraction.text.CountVectorizer", "sklearn.feature_extraction.text.CountVectorizer.fit_transform", "pandas.Series", "print", "str", "str", "sklearn.feature_extraction.text.CountVectorizer.get_feature_names", "sklearn.feature_extraction.text.CountVectorizer.get_feature_names", "set", "word.isdigit", "str", "len", "len", "str", "str", "pd.Series.isin", "pd.Series.isin", "len", "len", "len", "shared_vocab_no_threshold.isin"], "function", ["None"], ["", "def", "vocab_gen", "(", "pm_corpus", ",", "\n", "web_corpus", ",", "\n", "full_corpus", ",", "\n", "params_dict", ",", "\n", ")", ":", "\n", "\n", "    ", "dimensionality_reduction", "=", "params_dict", "[", "'Dimensionality Reduction'", "]", "\n", "min_df", "=", "params_dict", "[", "'Min DF'", "]", "\n", "max_df", "=", "params_dict", "[", "'Max DF'", "]", "\n", "\n", "# Generates web article vocabulary count matrix & vocabulary list", "\n", "vectorizer", "=", "CountVectorizer", "(", "lowercase", "=", "True", ",", "\n", ")", "\n", "web_countx", "=", "vectorizer", ".", "fit_transform", "(", "web_corpus", "[", "'corpus_text'", "]", ")", "\n", "web_vocab", "=", "pd", ".", "Series", "(", "vectorizer", ".", "get_feature_names", "(", ")", ")", "\n", "print", "(", "'Web vocabulary size: '", "+", "str", "(", "len", "(", "web_vocab", ")", ")", ")", "\n", "\n", "# Generates PubMed article vocabulary count matrix & vocabulary list", "\n", "vectorizer", "=", "CountVectorizer", "(", "lowercase", "=", "True", ",", "\n", ")", "\n", "pubmed_countx", "=", "vectorizer", ".", "fit_transform", "(", "pm_corpus", "[", "'corpus_text'", "]", ")", "\n", "pubmed_vocab", "=", "pd", ".", "Series", "(", "vectorizer", ".", "get_feature_names", "(", ")", ")", "\n", "print", "(", "'PubMed vocabulary size: '", "+", "str", "(", "len", "(", "pubmed_vocab", ")", ")", ")", "\n", "\n", "if", "dimensionality_reduction", "==", "'Thresholds'", ":", "\n", "# Generates full, combined corpus vocabulary with NO THRESHOLD PARAMETERS", "\n", "        ", "vectorizer", "=", "CountVectorizer", "(", "lowercase", "=", "True", ",", "\n", "min_df", "=", "min_df", ",", "# Must appear in >= x articles (web or PubMed)", "\n", "max_df", "=", "max_df", ",", "# Must appear in <= yy% of articles (web or PubMed)", "\n", ")", "\n", "web_pm_countx", "=", "vectorizer", ".", "fit_transform", "(", "full_corpus", ")", "\n", "web_pm_vocab", "=", "pd", ".", "Series", "(", "vectorizer", ".", "get_feature_names", "(", ")", ")", "\n", "print", "(", "'Full combined corpus vocabulary size (threshold values): '", "+", "str", "(", "len", "(", "web_pm_vocab", ")", ")", ")", "\n", "\n", "filename", "=", "'shared_vocab_no_thresholds.pkl'", "\n", "\n", "", "else", ":", "\n", "# Generates full, combined corpus vocabulary with NO THRESHOLD PARAMETERS", "\n", "# Used in experimental groups with dimensionality reduction using truncated singular value decomposition", "\n", "        ", "vectorizer", "=", "CountVectorizer", "(", "lowercase", "=", "True", ",", "\n", ")", "\n", "web_pm_countx", "=", "vectorizer", ".", "fit_transform", "(", "full_corpus", ")", "\n", "web_pm_vocab", "=", "pd", ".", "Series", "(", "vectorizer", ".", "get_feature_names", "(", ")", ")", "\n", "print", "(", "'Full combined corpus vocabulary size (no threshold values): '", "+", "str", "(", "len", "(", "web_pm_vocab", ")", ")", ")", "\n", "\n", "filename", "=", "'shared_vocab_thresholds_%s_%s.pkl'", "%", "(", "min_df", ",", "min_df", ")", "\n", "\n", "# Identifies words appearing in both corpora, and those appearing in only one corpus (web or PubMed)", "\n", "", "shared_vocab_no_threshold", "=", "web_pm_vocab", "[", "web_pm_vocab", ".", "isin", "(", "web_vocab", ")", "&", "web_pm_vocab", ".", "isin", "(", "pubmed_vocab", ")", "]", "\n", "# web_only_vocab_no_threshold = web_vocab[~web_vocab.isin(pubmed_vocab)]", "\n", "# pubmed_only_vocab_no_threshold = pubmed_vocab[~pubmed_vocab.isin(web_vocab)]", "\n", "\n", "# Identifies and removes all 'numeric-only' words in vocabulary", "\n", "numeric_words", "=", "[", "word", "for", "word", "in", "set", "(", "shared_vocab_no_threshold", ")", "if", "word", ".", "isdigit", "(", ")", "]", "\n", "shared_vocab", "=", "shared_vocab_no_threshold", "[", "~", "shared_vocab_no_threshold", ".", "isin", "(", "numeric_words", ")", "]", ".", "reset_index", "(", "drop", "=", "True", ",", "\n", ")", "\n", "print", "(", "'Final vocabulary size: '", "+", "str", "(", "len", "(", "shared_vocab_no_threshold", ")", ")", ")", "\n", "\n", "# Saves NO THRESHOLD vocabulary to file for use in analyses", "\n", "pd", ".", "to_pickle", "(", "shared_vocab", ",", "\n", "filename", ",", "\n", "protocol", "=", "2", ",", "\n", ")", "\n", "\n", "return_vals", "=", "shared_vocab", "\n", "\n", "return", "return_vals", "\n", "\n"]], "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.feature_extraction_functions.feature_representation": [[175, 260], ["print", "print", "sklearn.feature_extraction.text.CountVectorizer", "print", "sklearn.feature_extraction.text.TfidfVectorizer.fit_transform", "print", "scipy.sparse.save_npz", "scipy.sparse.save_npz", "pickle.dump", "open", "print", "sklearn.feature_extraction.text.CountVectorizer", "print", "sklearn.feature_extraction.text.TfidfVectorizer.fit_transform", "print", "scipy.sparse.save_npz", "scipy.sparse.save_npz", "pickle.dump", "print", "sklearn.feature_extraction.text.TfidfVectorizer", "print", "sklearn.feature_extraction.text.TfidfVectorizer.fit_transform", "print", "scipy.sparse.save_npz", "scipy.sparse.save_npz", "pickle.dump", "print", "str", "open", "open", "str", "str", "str"], "function", ["None"], ["", "def", "feature_representation", "(", "vocab", ",", "\n", "full_corpus", ",", "\n", "params_dict", ",", "\n", ")", ":", "\n", "\n", "    ", "feature_extraction", "=", "params_dict", "[", "'Feature Extraction'", "]", "\n", "dimensionality_reduction", "=", "params_dict", "[", "'Dimensionality Reduction'", "]", "\n", "min_df", "=", "params_dict", "[", "'Min DF'", "]", "\n", "max_df", "=", "params_dict", "[", "'Max DF'", "]", "\n", "tsvd_components", "=", "params_dict", "[", "'T-SVD Components'", "]", "\n", "\n", "if", "dimensionality_reduction", "==", "'Thresholds'", ":", "\n", "        ", "matrix_filename", "=", "'%s_matrix_%s_%s_%s.npz'", "%", "(", "feature_extraction", ",", "\n", "dimensionality_reduction", ",", "\n", "min_df", ",", "\n", "max_df", ",", "\n", ")", "\n", "vectorizer_filename", "=", "'%s_vectorizer_%s_%s_%s.pkl'", "%", "(", "feature_extraction", ",", "\n", "dimensionality_reduction", ",", "\n", "min_df", ",", "\n", "max_df", ",", "\n", ")", "\n", "\n", "", "else", ":", "\n", "        ", "matrix_filename", "=", "'%s_matrix_%s.npz'", "%", "(", "feature_extraction", ",", "\n", "dimensionality_reduction", ",", "\n", ")", "\n", "vectorizer_filename", "=", "'%s_vectorizer_%s.pkl'", "%", "(", "feature_extraction", ",", "\n", "dimensionality_reduction", ",", "\n", ")", "\n", "\n", "", "if", "feature_extraction", "==", "'Binary'", ":", "\n", "# Vectorization of full, combined corpus (PubMed + web)", "\n", "        ", "print", "(", "'Beginning binary vectorization (NO THRESHOLDS)...'", ")", "\n", "vectorizer", "=", "CountVectorizer", "(", "vocabulary", "=", "vocab", ".", "values", ",", "\n", "binary", "=", "True", ",", "\n", ")", "\n", "print", "(", "vectorizer", ")", "\n", "binary_x", "=", "vectorizer", ".", "fit_transform", "(", "full_corpus", ")", "\n", "print", "(", "'Binary vectorization complete '", "\n", "+", "str", "(", "binary_x", ".", "shape", ")", ")", "\n", "# Saves sparse matrix and vectorizer objects to files", "\n", "scipy", ".", "sparse", ".", "save_npz", "(", "matrix_filename", ",", "binary_x", ")", "\n", "pickle", ".", "dump", "(", "vectorizer", ",", "\n", "open", "(", "vectorizer_filename", ",", "'wb'", ")", ",", "\n", "protocol", "=", "2", ",", "\n", ")", "\n", "\n", "", "elif", "feature_extraction", "==", "'TF'", ":", "\n", "# Term frequency vectorization  of full, combined corpus (PubMed + web)", "\n", "        ", "print", "(", "'Beginning term frequency vectorization (NO THRESHOLDS)...'", ")", "\n", "vectorizer", "=", "CountVectorizer", "(", "vocabulary", "=", "vocab", ".", "values", ",", "\n", ")", "\n", "print", "(", "vectorizer", ")", "\n", "tf_x", "=", "vectorizer", ".", "fit_transform", "(", "full_corpus", ")", "\n", "print", "(", "'Term frequency vectorization complete '", "\n", "+", "str", "(", "tf_x", ".", "shape", ")", ")", "\n", "\n", "# Saves sparse matrix and vectorizer objects to files", "\n", "scipy", ".", "sparse", ".", "save_npz", "(", "matrix_filename", ",", "tf_x", ")", "\n", "pickle", ".", "dump", "(", "vectorizer", ",", "\n", "open", "(", "vectorizer_filename", ",", "'wb'", ")", ",", "\n", "protocol", "=", "2", ",", "\n", ")", "\n", "\n", "", "else", ":", "\n", "# Term frequency vectorization  of full, combined corpus (PubMed + web)", "\n", "        ", "print", "(", "'Beginning term frequency vectorization (NO THRESHOLDS)...'", ")", "\n", "vectorizer", "=", "TfidfVectorizer", "(", "vocabulary", "=", "vocab", ".", "values", ",", "\n", ")", "\n", "print", "(", "vectorizer", ")", "\n", "tfidf_x", "=", "vectorizer", ".", "fit_transform", "(", "full_corpus", ")", "\n", "print", "(", "'Term frequency vectorization complete '", "\n", "+", "str", "(", "tfidf_x", ".", "shape", ")", ")", "\n", "\n", "# Saves sparse matrix and vectorizer objects to files", "\n", "scipy", ".", "sparse", ".", "save_npz", "(", "matrix_filename", ",", "tfidf_x", ")", "\n", "pickle", ".", "dump", "(", "vectorizer", ",", "\n", "open", "(", "vectorizer_filename", ",", "'wb'", ")", ",", "\n", "protocol", "=", "2", ",", "\n", ")", "\n", "print", "(", "'TF-IDF vectorization complete '", "\n", "+", "str", "(", "tfidf_x", ".", "shape", ")", ")", "\n", "\n", "", "print", "(", "'Feature matrix saved to file'", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.retrieval_cleansing_functions.eutils_from_df": [[21, 149], ["eutils.Client", "pandas.DataFrame.from_records", "print", "datafile.close", "open", "csv.DictWriter", "csv.DictWriter.writeheader", "zip", "range", "range", "len", "len", "list", "list", "print", "iter", "enumerate", "eutils.Client.efetch", "print", "print", "str", "next", "dict", "print", "print", "pm_article_list.append", "print", "csv.DictWriter.writerow", "print", "print", "print", "time.sleep", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str"], "function", ["None"], ["def", "eutils_from_df", "(", "input_df", ",", "chunksize", ",", "output_csv", ")", ":", "\n", "    ", "\"\"\"\n    Retrieves and saves PubMed article content from PubMed via E-Utilities API to CSV file\n    for set of PMIDs contained within Pandas Dataframe.\n\n    Args:\n        input_df: object name for Dataframe containing PMIDs of interest\n        chunksize: number of PMIDs to pass to API\n        output_csv: filename for CSV file to which article content will be saved\n\n    Returns:\n        CSV file with rows pertaining to article content for each PMID in input_csv.\n        Columns correspond to fields retrieved via efetch client:\n            'PMID', 'Year', 'Title', 'Abstract', 'Authors', 'Journal', 'Volume', 'Issue',\n            'Pages', 'DOI', 'PMC'\n        List and dataframe containing all PubMed article data successfully retrieved from database\n    \"\"\"", "\n", "\n", "# Specifies names for output csv column headers", "\n", "fieldnames", "=", "[", "'PMID'", ",", "\n", "'Year'", ",", "\n", "'Title'", ",", "\n", "'Abstract'", ",", "\n", "'Authors'", ",", "\n", "'Journal'", ",", "\n", "'Volume'", ",", "\n", "'Issue'", ",", "\n", "'Pages'", ",", "\n", "'DOI'", ",", "\n", "'PMC'", ",", "\n", "]", "\n", "\n", "# Creates generator object containing each row in the input dataframe", "\n", "pm_chunks_gen", "=", "(", "input_df", "[", "i", ":", "i", "+", "chunksize", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "input_df", ")", ",", "chunksize", ")", ")", "\n", "\n", "# Initialises empty list for compilation of article dictionaries into single container", "\n", "pm_article_list", "=", "[", "]", "\n", "\n", "# Initialise eutils client to access NCBI E-Utilities API", "\n", "ec", "=", "eutils", ".", "Client", "(", ")", "\n", "\n", "# Open CSV file to which each PubMed IDs downloaded data appended as a new row with specified column names", "\n", "with", "open", "(", "output_csv", ",", "'a'", ")", "as", "datafile", ":", "\n", "        ", "writer", "=", "csv", ".", "DictWriter", "(", "datafile", ",", "\n", "fieldnames", "=", "fieldnames", ",", "\n", ")", "\n", "writer", ".", "writeheader", "(", ")", "\n", "\n", "# Converts each chunk of PubMed IDs from dataframe to list", "\n", "for", "chunk_count", ",", "chunk", "in", "zip", "(", "range", "(", "0", ",", "len", "(", "input_df", ")", ")", ",", "pm_chunks_gen", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "index_list", "=", "list", "(", "chunk", ".", "index", ".", "values", ")", "\n", "chunk_list", "=", "list", "(", "chunk", "[", "'PMID'", "]", ")", "\n", "print", "(", "'Chunk No. '", "+", "str", "(", "chunk_count", ")", ")", "\n", "\n", "# Passes chunk of PubMed IDs to E-Utilities API", "\n", "# Returns iterator object containing key data for each PubMed ID", "\n", "pm_article_set", "=", "iter", "(", "ec", ".", "efetch", "(", "db", "=", "'pubmed'", ",", "\n", "id", "=", "chunk_list", ",", "\n", ")", "\n", ")", "\n", "\n", "# Assigns each PubMed ID an index value", "\n", "# Iterates over pm_article_set to access data for each individual PubMed ID", "\n", "for", "id_index", ",", "id_value", "in", "enumerate", "(", "chunk_list", ")", ":", "\n", "                    ", "print", "(", "index_list", "[", "id_index", "]", ",", "id_value", ")", "\n", "try", ":", "\n", "# For each PMID index/value pair, iterates through article set", "\n", "# Aggregates key article attributes for each PubMed ID into dictionary", "\n", "                        ", "pm_article", "=", "next", "(", "pm_article_set", ")", "\n", "pm_article_content", "=", "dict", "(", "\n", "PMID", "=", "str", "(", "pm_article", ".", "pmid", ")", ",", "\n", "Year", "=", "str", "(", "pm_article", ".", "year", ")", ",", "\n", "Title", "=", "str", "(", "pm_article", ".", "title", ")", ",", "\n", "Abstract", "=", "str", "(", "pm_article", ".", "abstract", ")", ",", "\n", "Authors", "=", "str", "(", "pm_article", ".", "authors", ")", ",", "\n", "Journal", "=", "str", "(", "pm_article", ".", "jrnl", ")", ",", "\n", "Volume", "=", "str", "(", "pm_article", ".", "volume", ")", ",", "\n", "Issue", "=", "str", "(", "pm_article", ".", "issue", ")", ",", "\n", "Pages", "=", "str", "(", "pm_article", ".", "pages", ")", ",", "\n", "DOI", "=", "str", "(", "pm_article", ".", "doi", ")", ",", "\n", "PMC", "=", "str", "(", "pm_article", ".", "pmc", ")", ",", "\n", ")", "\n", "\n", "print", "(", "pm_article_content", ")", "\n", "print", "(", "pm_article", ".", "pmid", "+", "' - Download from Enterez complete'", ")", "\n", "\n", "# Saves dictionary as new item in list for later construction of dataframe", "\n", "pm_article_list", ".", "append", "(", "pm_article_content", ")", "\n", "print", "(", "pm_article", ".", "pmid", "+", "' - Save to list complete'", ")", "\n", "\n", "# Writes dictionary to new row of csv file for future reference", "\n", "writer", ".", "writerow", "(", "pm_article_content", ")", "\n", "print", "(", "pm_article", ".", "pmid", "+", "' - Write Data to CSV Complete'", ")", "\n", "\n", "# Except statements for content errors", "\n", "", "except", "(", "StopIteration", ",", "\n", "TypeError", ",", "\n", "NameError", ",", "\n", "ValueError", ",", "\n", "lxml", ".", "etree", ".", "XMLSyntaxError", ",", "\n", "eutils", ".", "exceptions", ".", "EutilsNCBIError", ",", "\n", ")", "as", "e1", ":", "\n", "                        ", "print", "(", "'Error: '", "+", "str", "(", "e1", ")", ")", "\n", "continue", "\n", "# Except statements for network/connection errors", "\n", "", "except", "(", "TimeoutError", ",", "\n", "RuntimeError", ",", "\n", "ConnectionError", ",", "\n", "ConnectionResetError", ",", "\n", "eutils", ".", "exceptions", ".", "EutilsRequestError", ",", "\n", "requests", ".", "exceptions", ".", "ConnectionError", ",", "\n", ")", "as", "e2", ":", "\n", "                        ", "print", "(", "'Error: '", "+", "str", "(", "e2", ")", ")", "\n", "time", ".", "sleep", "(", "10", ")", "\n", "continue", "\n", "\n", "", "", "", "except", "StopIteration", ":", "\n", "                ", "print", "(", "'All downloads complete'", ")", "\n", "break", "\n", "\n", "# Save list of dictionaries to dataframe & write to CSV file", "\n", "", "", "", "pm_article_df", "=", "pd", ".", "DataFrame", ".", "from_records", "(", "pm_article_list", ",", "\n", "columns", "=", "fieldnames", ",", "\n", ")", "\n", "print", "(", "'Save to DataFrame complete'", ")", "\n", "datafile", ".", "close", "(", ")", "\n", "return", "pm_article_df", "\n", "\n"]], "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.retrieval_cleansing_functions.lcs_algorithm": [[152, 207], ["re.sub", "re.sub", "tuple", "tuple", "len", "len", "set", "range", "range", "range", "re.sub.lower().split", "re.sub.lower().split", "set", "set.add", "re.sub.lower", "re.sub.lower"], "function", ["None"], ["", "def", "lcs_algorithm", "(", "str1", ",", "str2", ")", ":", "\n", "    ", "\"\"\"\n    Extracts the longest common substring (words) between two strings.\n\n    Args:\n        str1: Input string 1\n        str2: Input string 2\n\n    Returns:\n        lcs_set: The longest common substring shared between the two input strings.\n\n\n   SOURCE: Code adapted from\n        https://www.bogotobogo.com/python/python_longest_common_substring_lcs_algorithm_generalized_suffix_tree.php\n    NOTE: Commented out sections correspond to code that allows for saving of more than one\n        longest common substring (where len(lcs) is the same).\n    \"\"\"", "\n", "# str1_words = ''.join(str1.split())", "\n", "# str2_words = ''.join(str2.split())", "\n", "\n", "# Removes punctuation from string to prevent premature termination of longest common substring", "\n", "str1", "=", "re", ".", "sub", "(", "r'[^\\w\\s]'", ",", "''", ",", "str1", ")", "\n", "str2", "=", "re", ".", "sub", "(", "r'[^\\w\\s]'", ",", "''", ",", "str2", ")", "\n", "\n", "# Splits string into tuple of words, to compute lcs by word (vs character)", "\n", "str1_words", "=", "tuple", "(", "word", "for", "word", "in", "str1", ".", "lower", "(", ")", ".", "split", "(", ")", ")", "\n", "str2_words", "=", "tuple", "(", "word", "for", "word", "in", "str2", ".", "lower", "(", ")", ".", "split", "(", ")", ")", "\n", "\n", "m", "=", "len", "(", "str1_words", ")", "\n", "n", "=", "len", "(", "str2_words", ")", "\n", "\n", "matrix", "=", "[", "[", "0", "]", "*", "(", "n", "+", "1", ")", "for", "i", "in", "range", "(", "m", "+", "1", ")", "]", "\n", "\n", "longest", "=", "0", "\n", "lcs_set", "=", "set", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "m", ")", ":", "\n", "\n", "        ", "for", "j", "in", "range", "(", "n", ")", ":", "\n", "\n", "            ", "if", "str1_words", "[", "i", "]", "==", "str2_words", "[", "j", "]", ":", "\n", "                ", "x", "=", "matrix", "[", "i", "]", "[", "j", "]", "+", "1", "\n", "matrix", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", "=", "x", "\n", "\n", "if", "x", ">", "longest", ":", "\n", "                    ", "longest", "=", "x", "\n", "lcs_set", "=", "set", "(", ")", "\n", "lcs_set", ".", "add", "(", "str1_words", "[", "i", "-", "x", "+", "1", ":", "i", "+", "1", "]", ")", "\n", "\n", "", "else", ":", "\n", "                    ", "pass", "\n", "\n", "", "", "", "", "lcs", "=", "[", "' '", ".", "join", "(", "tup", ")", "for", "tup", "in", "lcs_set", "]", "\n", "\n", "return", "lcs", "\n", "\n"]], "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.retrieval_cleansing_functions.lcs_analysis": [[209, 270], ["time.time", "print", "itertools.combinations", "print", "pandas.DataFrame().transpose", "time.time", "round", "print", "retrieval_cleansing_functions.lcs_algorithm", "pandas.DataFrame", "len", "print", "lcs_list[].append", "lcs_list[].append", "lcs_list[].append", "lcs_list[].append", "len", "max", "len", "len"], "function", ["home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.retrieval_cleansing_functions.lcs_algorithm"], ["", "def", "lcs_analysis", "(", "series", ",", "min_similarity", ")", ":", "\n", "    ", "\"\"\"\n    Calculates and extracts longest common substring (lcs) between pairs of strings.\n\n    Args:\n        series:\n        min_similarity:\n\n    Returns:\n        lcs_df:\n\n    NOTE: Commented out sections correspond to code that allows for saving of more than one\n        longest common substring (where len(lcs) is the same).\n    \"\"\"", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "'Beginning Longest Common Substring analysis...'", ")", "\n", "\n", "# Removes all whitespace from series values to reduce size", "\n", "# series = series.apply(lambda x: ''.join(x.split()))  # This is stopping it from working on the title dataset", "\n", "\n", "# Generates all distinct pairs of series values", "\n", "article_pairs", "=", "itertools", ".", "combinations", "(", "series", ".", "index", ",", "2", ")", "\n", "\n", "# Initialises empty list in which indices, lcs and %age match can be stored", "\n", "lcs_list", "=", "[", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "]", "\n", "\n", "for", "pair", "in", "article_pairs", ":", "\n", "        ", "index_1", ",", "index_2", "=", "pair", "\n", "\n", "if", "index_1", "!=", "index_2", ":", "\n", "            ", "str1", "=", "series", "[", "index_1", "]", "\n", "str2", "=", "series", "[", "index_2", "]", "\n", "lcs", "=", "lcs_algorithm", "(", "str1", ",", "str2", ")", "\n", "\n", "if", "len", "(", "lcs", ")", ">", "0", ":", "\n", "                ", "pct", "=", "(", "len", "(", "lcs", "[", "0", "]", ")", "/", "max", "(", "len", "(", "str1", ")", ",", "len", "(", "str2", ")", ")", ")", "*", "100", "\n", "# pct = len(lcs[0]) / max(len(str1), len(str2)) * 100", "\n", "\n", "if", "pct", ">", "min_similarity", ":", "\n", "                    ", "print", "(", "'%s - %s Longest common substring > min threshold'", "%", "(", "index_1", ",", "index_2", ")", ")", "\n", "lcs_list", "[", "0", "]", ".", "append", "(", "index_1", ")", "\n", "lcs_list", "[", "1", "]", ".", "append", "(", "index_2", ")", "\n", "lcs_list", "[", "2", "]", ".", "append", "(", "lcs", ")", "\n", "lcs_list", "[", "3", "]", ".", "append", "(", "pct", ")", "\n", "\n", "", "else", ":", "\n", "                    ", "pass", "\n", "", "", "else", ":", "\n", "                ", "pass", "\n", "", "", "else", ":", "\n", "            ", "pass", "\n", "\n", "", "", "print", "(", "'Longest common substring analysis of all records complete'", ")", "\n", "lcs_df", "=", "pd", ".", "DataFrame", "(", "lcs_list", ")", ".", "transpose", "(", ")", "\n", "lcs_df", ".", "columns", "=", "[", "'article id 1'", ",", "'article id 2'", ",", "'lcs'", ",", "'%age common'", "]", "\n", "\n", "t1", "=", "time", ".", "time", "(", ")", "\n", "elapsed", "=", "round", "(", "t1", "-", "t0", ",", "2", ")", "\n", "print", "(", "'%s seconds / %s minutes elapsed/n'", "%", "(", "elapsed", ",", "elapsed", "/", "60", ")", ")", "\n", "\n", "return", "lcs_df", "\n", "\n"]], "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.retrieval_cleansing_functions.select_web_records": [[273, 318], ["subset.loc[].unique", "full_corpus.append", "len", "len", "text_lengths.index", "selected_web_ids.append", "selected_urls.append", "print", "for_selection[].isin", "for_selection[].isin", "max", "len", "len"], "function", ["None"], ["", "def", "select_web_records", "(", "for_selection", ",", "full_corpus", ")", ":", "\n", "    ", "subset", "=", "for_selection", ".", "loc", "[", "(", "~", "for_selection", "[", "'pmid'", "]", ".", "isin", "(", "full_corpus", "[", "'pmid'", "]", ".", "values", ")", ")", "\n", "&", "(", "~", "for_selection", "[", "'url'", "]", ".", "isin", "(", "full_corpus", "[", "'url'", "]", ".", "values", ")", ")", "]", "\n", "\n", "selected_web_ids", "=", "[", "]", "\n", "selected_urls", "=", "[", "]", "\n", "\n", "for", "pmid", "in", "subset", ".", "loc", "[", ":", ",", "'pmid'", "]", ".", "unique", "(", ")", ":", "\n", "        ", "web_ids", "=", "subset", ".", "loc", "[", "subset", "[", "'pmid'", "]", "==", "pmid", "]", ".", "index", ".", "values", "\n", "urls", "=", "subset", ".", "loc", "[", "subset", "[", "'pmid'", "]", "==", "pmid", ",", "'url'", "]", ".", "values", "\n", "corpus_text", "=", "subset", ".", "loc", "[", "subset", "[", "'pmid'", "]", "==", "pmid", ",", "'corpus_text'", "]", ".", "values", "\n", "text_lengths", "=", "[", "len", "(", "text", ")", "for", "text", "in", "corpus_text", "]", "\n", "not_in_corpus", "=", "True", "\n", "\n", "while", "not_in_corpus", "is", "True", ":", "\n", "            ", "if", "len", "(", "web_ids", ")", ">", "1", ":", "\n", "                ", "i", "=", "text_lengths", ".", "index", "(", "max", "(", "text_lengths", ")", ")", "# for selecting longest web article", "\n", "# i = random.randint(0, len(web_ids) - 1)  # for selecting random url", "\n", "", "elif", "len", "(", "web_ids", ")", "==", "1", ":", "\n", "                ", "i", "=", "0", "\n", "", "else", ":", "\n", "                ", "not_in_corpus", "=", "False", "\n", "break", "\n", "\n", "", "selected_id", "=", "web_ids", "[", "i", "]", "\n", "selected_url", "=", "urls", "[", "i", "]", "\n", "selected_corpus_text", "=", "corpus_text", "[", "i", "]", "\n", "\n", "if", "selected_url", "in", "selected_urls", "or", "selected_url", "in", "full_corpus", "[", "'url'", "]", ".", "values", ":", "\n", "                ", "web_ids", "=", "[", "x", "for", "x", "in", "web_ids", "if", "x", "!=", "selected_id", "]", "\n", "urls", "=", "[", "x", "for", "x", "in", "urls", "if", "x", "!=", "selected_url", "]", "\n", "corpus_text", "=", "[", "x", "for", "x", "in", "corpus_text", "if", "x", "!=", "selected_corpus_text", "]", "\n", "text_lengths", "=", "[", "len", "(", "text", ")", "for", "text", "in", "corpus_text", "]", "\n", "not_in_corpus", "=", "True", "\n", "\n", "", "else", ":", "\n", "                ", "selected_web_ids", ".", "append", "(", "selected_id", ")", "\n", "selected_urls", ".", "append", "(", "selected_url", ")", "\n", "not_in_corpus", "=", "False", "\n", "print", "(", "'Web record %s added to final corpus (%s)'", "%", "(", "selected_id", ",", "selected_url", ")", ")", "\n", "\n", "", "", "", "full_corpus_updated", "=", "full_corpus", ".", "append", "(", "subset", ".", "loc", "[", "selected_web_ids", "]", ")", "\n", "\n", "return", "full_corpus_updated", "\n", "\n"]], "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.train_transform_cca.write_to_file": [[16, 19], ["open", "oF.write"], "function", ["None"], ["def", "write_to_file", "(", "outFile", ",", "text", ",", "mode", ")", ":", "\n", "    ", "with", "open", "(", "outFile", ",", "mode", ")", "as", "oF", ":", "\n", "        ", "oF", ".", "write", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.train_transform_cca.dump_var": [[21, 23], ["_pickle.dump", "open"], "function", ["None"], ["", "", "def", "dump_var", "(", "data", ",", "outFile", ")", ":", "\n", "    ", "pickle", ".", "dump", "(", "data", ",", "open", "(", "outFile", ",", "\"wb\"", ")", ",", "protocol", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.train_transform_cca.load_var": [[25, 27], ["_pickle.load", "open"], "function", ["None"], ["", "def", "load_var", "(", "inFile", ")", ":", "\n", "    ", "return", "pickle", ".", "load", "(", "open", "(", "inFile", ",", "\"rb\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.train_transform_cca.run_cca": [[29, 88], ["print", "train_transform_cca.write_to_file", "sklearn.cross_decomposition.CCA", "time.time", "sklearn.cross_decomposition.CCA.fit", "print", "time.time", "train_transform_cca.dump_var", "print", "time.time", "sklearn.cross_decomposition.CCA.transform", "print", "time.time", "sklearn.cross_decomposition.CCA.transform", "print", "time.time", "scipy.sparse.save_npz", "scipy.sparse.save_npz", "print", "time.time", "scipy.sparse.save_npz", "scipy.sparse.save_npz", "print", "train_transform_cca.write_to_file", "print", "X_train.toarray", "Y_train.toarray", "time.time", "print", "time.time", "print", "X_train.toarray", "Y_train.toarray", "time.time", "print", "X_test.toarray", "Y_test.toarray", "time.time", "print", "scipy.sparse.csr_matrix", "scipy.sparse.csr_matrix", "time.time", "print", "scipy.sparse.csr_matrix", "scipy.sparse.csr_matrix", "time.time", "print", "train_transform_cca.write_to_file", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.train_transform_cca.write_to_file", "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.train_transform_cca.dump_var", "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.train_transform_cca.write_to_file", "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.train_transform_cca.write_to_file"], ["", "def", "run_cca", "(", "l_cca_components", ",", "X_train", ",", "Y_train", ",", "X_test", ",", "Y_test", ",", "fname", ",", "tsvddim", ",", "dataFolder", ",", "logFile", ")", ":", "\n", "    ", "for", "cca_components", "in", "l_cca_components", ":", "\n", "        ", "try", ":", "\n", "            ", "print", "(", "'\\nCCA components: {0}'", ".", "format", "(", "cca_components", ")", ")", "\n", "\n", "write_to_file", "(", "logFile", ",", "'CCA components: '", "+", "str", "(", "cca_components", ")", "+", "'...'", ",", "'a'", ")", "\n", "\n", "cca", "=", "CCA", "(", "n_components", "=", "cca_components", ",", "max_iter", "=", "5000", ")", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "'\\tFit...'", ")", ",", "\n", "cca", ".", "fit", "(", "X_train", ".", "toarray", "(", ")", ",", "Y_train", ".", "toarray", "(", ")", ")", "\n", "partTime", "=", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", "\n", "print", "(", "'done in {0} secs'", ".", "format", "(", "partTime", ")", ")", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "'\\tDump model...'", ")", ",", "\n", "dump_var", "(", "cca", ",", "'cca_'", "+", "str", "(", "cca_components", ")", "+", "'_fortsvddim_'", "+", "str", "(", "tsvddim", ")", "+", "'.cpickle'", ")", "\n", "partTime", "=", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", "\n", "print", "(", "'done in {0} secs'", ".", "format", "(", "partTime", ")", ")", "\n", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "'\\tTransform train...'", ")", ",", "\n", "X_train_transform", ",", "Y_train_transform", "=", "cca", ".", "transform", "(", "X_train", ".", "toarray", "(", ")", ",", "Y_train", ".", "toarray", "(", ")", ")", "\n", "partTime", "=", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", "\n", "print", "(", "'done in {0} secs'", ".", "format", "(", "partTime", ")", ")", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "'\\tTransform test...'", ")", ",", "\n", "X_test_transform", ",", "Y_test_transform", "=", "cca", ".", "transform", "(", "X_test", ".", "toarray", "(", ")", ",", "Y_test", ".", "toarray", "(", ")", ")", "\n", "partTime", "=", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", "\n", "print", "(", "'done in {0} secs'", ".", "format", "(", "partTime", ")", ")", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "'\\tDump transformed train...'", ")", ",", "\n", "scipy", ".", "sparse", ".", "save_npz", "(", "dataFolder", "+", "fname", "+", "str", "(", "tsvddim", ")", "+", "'_cca_'", "+", "str", "(", "cca_components", ")", "+", "'_web_train.npz'", ",", "\n", "scipy", ".", "sparse", ".", "csr_matrix", "(", "X_train_transform", ")", ")", "\n", "scipy", ".", "sparse", ".", "save_npz", "(", "dataFolder", "+", "fname", "+", "str", "(", "tsvddim", ")", "+", "'_cca_'", "+", "str", "(", "cca_components", ")", "+", "'_pm_train.npz'", ",", "\n", "scipy", ".", "sparse", ".", "csr_matrix", "(", "Y_train_transform", ")", ")", "\n", "partTime", "=", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", "\n", "print", "(", "'done in {0} secs'", ".", "format", "(", "partTime", ")", ")", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "'\\tDump transformed test...'", ")", ",", "\n", "scipy", ".", "sparse", ".", "save_npz", "(", "dataFolder", "+", "fname", "+", "str", "(", "tsvddim", ")", "+", "'_cca_'", "+", "str", "(", "cca_components", ")", "+", "'_web_test.npz'", ",", "\n", "scipy", ".", "sparse", ".", "csr_matrix", "(", "X_test_transform", ")", ")", "\n", "scipy", ".", "sparse", ".", "save_npz", "(", "dataFolder", "+", "fname", "+", "str", "(", "tsvddim", ")", "+", "'_cca_'", "+", "str", "(", "cca_components", ")", "+", "'_pm_test.npz'", ",", "\n", "scipy", ".", "sparse", ".", "csr_matrix", "(", "Y_test_transform", ")", ")", "\n", "partTime", "=", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", "\n", "print", "(", "'done in {0} secs'", ".", "format", "(", "partTime", ")", ")", "\n", "\n", "write_to_file", "(", "logFile", ",", "'done\\n'", ",", "'a'", ")", "\n", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "print", "(", "'\\n[ERROR] Component {0}. Error: {1}'", ".", "format", "(", "cca_components", ",", "e", ")", ")", "\n", "\n", "write_to_file", "(", "logFile", ",", "'Error: '", "+", "str", "(", "e", ")", "+", "'\\n'", ",", "'a'", ")", "\n", "continue", "\n", "\n"]], "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.database_functions.database_connect": [[19, 46], ["print", "psycopg2.connect", "print", "print"], "function", ["None"], ["def", "database_connect", "(", "db_name", ",", "host_name", ",", "user_name", ",", "user_password", ")", ":", "\n", "    ", "\"\"\"\n    Connects to database via specific user profile\n    Args:\n        db_name: Database to which connection should be established\n        host_name: Host name for database\n        user_name: Username for login\n        user_password: Password for user profile\n\n    Returns:\n        connection: Database connection\n\n    \"\"\"", "\n", "# Establishes database connection", "\n", "print", "(", "'Connecting to database...'", ")", "\n", "try", ":", "\n", "        ", "connection", "=", "psycopg2", ".", "connect", "(", "dbname", "=", "db_name", ",", "\n", "host", "=", "host_name", ",", "\n", "user", "=", "user_name", ",", "\n", "password", "=", "user_password", ",", "\n", ")", "\n", "print", "(", "'\\nConnection successful'", ")", "\n", "\n", "", "except", "Exception", "as", "e", ":", "\n", "        ", "print", "(", "'Connection unsuccessful - please check connection and try again'", ")", "\n", "\n", "", "return", "connection", "\n", "\n"]], "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.database_functions.db_to_df": [[50, 85], ["enumerate", "print", "connection.close", "print", "pandas.read_sql", "print", "len", "result.append"], "function", ["None"], ["", "def", "db_to_df", "(", "script_list", ",", "connection", ")", ":", "\n", "    ", "\"\"\"\n    Accesses database using pre-existing connection and executes provided list of SQL queries.\n    Saves each resulting report to a Dataframe.\n\n    Args:\n        script_list (lst): List of SQL scripts to be executed\n        connection: Database connection variable name\n    Returns:\n        all_reports: List of DataFrames retrieved by each SQL script\n    \"\"\"", "\n", "# Initialises empty dictionary for compilation of report DataFrames", "\n", "result", "=", "[", "]", "\n", "\n", "# Iterates through each SQL script in list and assigns integer index value", "\n", "for", "script_num", ",", "script", "in", "enumerate", "(", "script_list", ")", ":", "\n", "        ", "print", "(", "script_num", ",", "\n", "script", "\n", ")", "\n", "\n", "# Queries the database and saves retrieved report to a DataFrame", "\n", "report_df", "=", "pd", ".", "read_sql", "(", "script", ",", "connection", ")", "\n", "print", "(", "report_df", ".", "columns", ",", "\n", "report_df", ".", "iloc", "[", "0", ":", "5", "]", "\n", ")", "\n", "\n", "if", "len", "(", "script_list", ")", "==", "1", ":", "\n", "            ", "result", "=", "report_df", "\n", "", "else", ":", "\n", "# Appends each DataFrame to list of DataFrames", "\n", "            ", "result", ".", "append", "(", "report_df", ")", "\n", "", "", "print", "(", "'Download complete'", ")", "\n", "connection", ".", "close", "(", ")", "\n", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.document_similarity_functions.matrix_split": [[11, 36], ["scipy.sparse.csr_matrix", "scipy.sparse.csr_matrix", "scipy.sparse.csr_matrix", "scipy.sparse.csr_matrix"], "function", ["None"], ["def", "matrix_split", "(", "feature_matrix", ",", "part_1_n", ")", ":", "\n", "    ", "\"\"\"\n    Splits a single sparse matrix along the row axis into two parts of\n    size n and matrix_len - n.\n\n    Args:\n        feature_matrix: Sparse matrix\n            Full matrix to be broken into parts of n-size\n        part_1_n: int\n            Size of first part (row at which matrix is split)\n\n    Returns:\n        return_vals: Tuple containing newly generated parts of original matrix\n\n    \"\"\"", "\n", "\n", "# Selects first part_1_n rows in the feature matrix", "\n", "part_1_x", "=", "scipy", ".", "sparse", ".", "csr_matrix", "(", "feature_matrix", "[", ":", "part_1_n", "]", ")", "\n", "\n", "# Selects the remaining rows in feature matrix", "\n", "part_2_x", "=", "scipy", ".", "sparse", ".", "csr_matrix", "(", "feature_matrix", "[", "part_1_n", ":", "]", ")", "\n", "\n", "return_vals", "=", "(", "part_1_x", ",", "part_2_x", ")", "\n", "\n", "return", "return_vals", "\n", "\n"]], "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.document_similarity_functions.tsvd_function": [[38, 62], ["sklearn.decomposition.TruncatedSVD", "sklearn.decomposition.TruncatedSVD.fit_transform", "print", "print", "scipy.sparse.csr_matrix", "scipy.sparse.csr_matrix", "str", "sklearn.decomposition.TruncatedSVD.explained_variance_ratio_.sum"], "function", ["None"], ["", "def", "tsvd_function", "(", "sparse_matrix", ",", "n_components", ")", ":", "\n", "    ", "\"\"\"\n    Performs Truncated singular value decomposition of a sparse matrix, reducing\n    dimensionality to a specified number of components\n    Args:\n        sparse_matrix: Sparse matrix\n        Sparse matrix on which to perform T-SVD\n        n_components: int\n            Number of singular values to retain in final TSVD matrix\n\n    Returns:\n        return_vals: Tuple containing TSVD function object and TSVD matrix\n\n    \"\"\"", "\n", "tsvd_obj", "=", "TruncatedSVD", "(", "n_components", "=", "n_components", ")", "\n", "tsvd_x", "=", "tsvd_obj", ".", "fit_transform", "(", "sparse_matrix", ")", "\n", "return_vals", "=", "(", "tsvd_obj", ",", "\n", "scipy", ".", "sparse", ".", "csr_matrix", "(", "tsvd_x", ")", ",", "\n", ")", "\n", "\n", "print", "(", "'Sum of explained variance ratio = '", "+", "str", "(", "tsvd_obj", ".", "explained_variance_ratio_", ".", "sum", "(", ")", ")", ")", "\n", "print", "(", "tsvd_obj", ".", "explained_variance_ratio_", ")", "\n", "\n", "return", "return_vals", "\n", "\n"]], "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.document_similarity_functions.experimental_groups_gen": [[67, 148], ["len", "len", "len", "len", "list", "min_df_all.extend", "max_df_all.extend", "dimensionality_reduction_all.extend", "dimensionality_reduction_all.sort", "tsvd_components_all.sort", "min_df_all.extend", "max_df_all.extend", "zip", "all_experimental_groups.append", "print", "print", "tsvd_cca.sort", "print", "cca_components_all.extend", "dimensionality_reduction_all.extend", "tsvd_components_all.extend", "len", "len", "dict", "len", "len", "zip", "len", "len"], "function", ["None"], ["", "def", "experimental_groups_gen", "(", "feature_extraction_methods", ",", "\n", "min_dfs", ",", "\n", "max_dfs", ",", "\n", "tsvd_components", ",", "\n", "cca_components", ",", "\n", ")", ":", "\n", "    ", "feature_reps_counts", "=", "len", "(", "feature_extraction_methods", ")", "\n", "threshold_counts", "=", "len", "(", "min_dfs", ")", "\n", "tsvd_counts", "=", "len", "(", "tsvd_components", ")", "\n", "cca_counts", "=", "len", "(", "cca_components", ")", "\n", "\n", "if", "cca_counts", "==", "0", ":", "\n", "        ", "all_groups_count", "=", "(", "threshold_counts", "+", "tsvd_counts", ")", "*", "feature_reps_counts", "\n", "\n", "feature_extraction_all", "=", "[", "feature_xt", "for", "feature_xt", "in", "feature_extraction_methods", "]", "*", "(", "threshold_counts", "+", "\n", "tsvd_counts", "\n", ")", "\n", "min_df_all", "=", "[", "min_df", "for", "min_df", "in", "min_dfs", "]", "*", "feature_reps_counts", "\n", "max_df_all", "=", "[", "max_df", "for", "max_df", "in", "max_dfs", "]", "*", "feature_reps_counts", "\n", "min_df_all", ".", "extend", "(", "[", "2", "]", "*", "(", "tsvd_counts", "*", "feature_reps_counts", ")", ")", "\n", "max_df_all", ".", "extend", "(", "[", "1", "]", "*", "(", "tsvd_counts", "*", "feature_reps_counts", ")", ")", "\n", "\n", "dimensionality_reduction_all", "=", "[", "'Thresholds'", "]", "*", "(", "threshold_counts", "*", "feature_reps_counts", ")", "\n", "dimensionality_reduction_all", ".", "extend", "(", "[", "'T-SVD'", "]", "*", "(", "(", "tsvd_counts", "+", "cca_counts", ")", "*", "feature_reps_counts", ")", ")", "\n", "dimensionality_reduction_all", ".", "sort", "(", "reverse", "=", "True", ")", "\n", "\n", "tsvd_components_all", "=", "[", "0", "]", "*", "(", "threshold_counts", "*", "feature_reps_counts", ")", "+", "[", "components", "for", "components", "in", "tsvd_components", "]", "*", "feature_reps_counts", "\n", "tsvd_components_all", ".", "sort", "(", ")", "\n", "\n", "cca_components_all", "=", "[", "0", "]", "*", "all_groups_count", "\n", "cca_all", "=", "[", "False", "]", "*", "all_groups_count", "\n", "\n", "", "else", ":", "\n", "        ", "cca_components_all", "=", "[", "]", "\n", "dimensionality_reduction_all", "=", "[", "]", "\n", "tsvd_components_all", "=", "[", "]", "\n", "\n", "for", "tsvd", "in", "tsvd_components", ":", "\n", "            ", "print", "(", "tsvd", ")", "\n", "tsvd_cca", "=", "[", "cca", "for", "cca", "in", "cca_components", "if", "cca", "<=", "tsvd", "]", "\n", "tsvd_cca", ".", "sort", "(", ")", "\n", "print", "(", "tsvd_cca", ")", "\n", "cca_components_all", ".", "extend", "(", "tsvd_cca", ")", "\n", "dimensionality_reduction_all", ".", "extend", "(", "[", "'T-SVD'", "]", "*", "len", "(", "tsvd_cca", ")", ")", "\n", "tsvd_components_all", ".", "extend", "(", "[", "tsvd", "]", "*", "len", "(", "tsvd_cca", ")", ")", "\n", "\n", "", "feature_extraction_all", "=", "feature_extraction_methods", "*", "len", "(", "cca_components_all", ")", "\n", "\n", "cca_all", "=", "[", "True", "]", "*", "len", "(", "cca_components_all", ")", "\n", "\n", "min_df_all", "=", "[", "min_df", "for", "min_df", "in", "min_dfs", "]", "*", "feature_reps_counts", "\n", "max_df_all", "=", "[", "max_df", "for", "max_df", "in", "max_dfs", "]", "*", "feature_reps_counts", "\n", "min_df_all", ".", "extend", "(", "[", "2", "]", "*", "len", "(", "cca_components_all", ")", ")", "\n", "max_df_all", ".", "extend", "(", "[", "1", "]", "*", "len", "(", "cca_components_all", ")", ")", "\n", "\n", "", "all_param_combos", "=", "list", "(", "zip", "(", "feature_extraction_all", ",", "\n", "dimensionality_reduction_all", ",", "\n", "min_df_all", ",", "\n", "max_df_all", ",", "\n", "tsvd_components_all", ",", "\n", "cca_all", ",", "\n", "cca_components_all", ",", "\n", ")", ")", "\n", "\n", "all_experimental_groups", "=", "[", "]", "\n", "exp_groups_params_list", "=", "[", "'Feature Extraction'", ",", "\n", "'Dimensionality Reduction'", ",", "\n", "'Min DF'", ",", "\n", "'Max DF'", ",", "\n", "'T-SVD Components'", ",", "\n", "'CCA'", ",", "\n", "'CCA Components'", ",", "\n", "]", "\n", "\n", "for", "params", "in", "all_param_combos", ":", "\n", "        ", "all_experimental_groups", ".", "append", "(", "dict", "(", "zip", "(", "exp_groups_params_list", ",", "params", ")", ")", ")", "\n", "\n", "", "[", "print", "(", "x", ")", "for", "x", "in", "all_experimental_groups", "]", "\n", "\n", "return", "all_experimental_groups", "\n", "\n"]], "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.document_similarity_functions.train_test_matrix": [[150, 227], ["print", "print", "document_similarity_functions.matrix_split", "document_similarity_functions.matrix_split", "scipy.sparse.save_npz", "scipy.sparse.save_npz", "scipy.sparse.save_npz", "scipy.sparse.save_npz", "document_similarity_functions.matrix_split", "scipy.sparse.save_npz", "scipy.sparse.save_npz", "scipy.sparse.save_npz", "scipy.sparse.save_npz", "print", "os.getcwd", "scipy.sparse.load_npz", "scipy.sparse.load_npz", "scipy.sparse.load_npz", "scipy.sparse.load_npz", "print", "print", "document_similarity_functions.tsvd_function", "scipy.sparse.save_npz", "scipy.sparse.save_npz", "print"], "function", ["home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.document_similarity_functions.matrix_split", "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.document_similarity_functions.matrix_split", "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.document_similarity_functions.matrix_split", "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.document_similarity_functions.tsvd_function"], ["", "def", "train_test_matrix", "(", "corpus_lengths", ",", "\n", "params_dict", ",", "\n", ")", ":", "\n", "    ", "print", "(", "os", ".", "getcwd", "(", ")", ")", "\n", "\n", "feature_extraction", "=", "params_dict", "[", "'Feature Extraction'", "]", "\n", "dimensionality_reduction", "=", "params_dict", "[", "'Dimensionality Reduction'", "]", "\n", "apply_cca", "=", "params_dict", "[", "'CCA'", "]", "\n", "tsvd_components", "=", "params_dict", "[", "'T-SVD Components'", "]", "\n", "cca_components", "=", "params_dict", "[", "'CCA Components'", "]", "\n", "min_df", "=", "params_dict", "[", "'Min DF'", "]", "\n", "max_df", "=", "params_dict", "[", "'Max DF'", "]", "\n", "\n", "if", "dimensionality_reduction", "==", "'Thresholds'", ":", "\n", "        ", "matrix_filename", "=", "'%s_matrix_%s_%s_%s.npz'", "%", "(", "feature_extraction", ",", "\n", "dimensionality_reduction", ",", "\n", "min_df", ",", "\n", "max_df", ",", "\n", ")", "\n", "\n", "# Loads full feature matrices from file depending on feature extraction and dimensionality reduction methods", "\n", "feature_x_full", "=", "scipy", ".", "sparse", ".", "load_npz", "(", "matrix_filename", ")", "\n", "feature_matrix", "=", "feature_x_full", "\n", "\n", "", "else", ":", "\n", "        ", "matrix_filename", "=", "'%s_matrix_%s.npz'", "%", "(", "feature_extraction", ",", "\n", "dimensionality_reduction", ",", "\n", ")", "\n", "# Loads full feature matrices from file depending on feature extraction and dimensionality reduction methods", "\n", "feature_x_full", "=", "scipy", ".", "sparse", ".", "load_npz", "(", "matrix_filename", ")", "\n", "print", "(", "'%s feature matrix loaded from file '", "%", "feature_extraction", ")", "\n", "\n", "print", "(", "'Beginning T-SVD...'", ")", "\n", "tsvd_object", ",", "feature_matrix", "=", "tsvd_function", "(", "feature_x_full", ",", "tsvd_components", ")", "\n", "scipy", ".", "sparse", ".", "save_npz", "(", "'%s_T-SVD_Applied_%s_matrix.npz'", "%", "(", "feature_extraction", ",", "\n", "tsvd_components", ",", "\n", ")", ",", "\n", "feature_matrix", ",", "\n", ")", "\n", "print", "(", "'T-SVD dimensionality reduction complete'", ")", "\n", "\n", "#####", "\n", "\n", "# TRAIN & TEST DATASETS #", "\n", "", "print", "(", "'Beginning generation of TRAIN and TEST datasets...'", ")", "\n", "\n", "# Splits feature matrix into parts cointaining web and PubMed vectors", "\n", "web_feature_matrix", ",", "pm_feature_matrix", "=", "matrix_split", "(", "feature_matrix", ",", "corpus_lengths", "[", "'full_web'", "]", ")", "\n", "\n", "# Splits web matrix in to train and test datasets", "\n", "web_train_matrix", ",", "web_test_matrix", "=", "matrix_split", "(", "web_feature_matrix", ",", "corpus_lengths", "[", "'web_corpus_train'", "]", ")", "\n", "\n", "scipy", ".", "sparse", ".", "save_npz", "(", "matrix_filename", "+", "'_web_train.npz'", ",", "\n", "web_train_matrix", ",", "\n", ")", "\n", "scipy", ".", "sparse", ".", "save_npz", "(", "matrix_filename", "+", "'_web_test.npz'", ",", "\n", "web_test_matrix", ",", "\n", ")", "\n", "\n", "# Splits PubMed matrix in to train and test datasets", "\n", "pm_train_matrix", ",", "pm_test_matrix", "=", "matrix_split", "(", "pm_feature_matrix", ",", "corpus_lengths", "[", "'pm_corpus_train'", "]", ")", "\n", "\n", "scipy", ".", "sparse", ".", "save_npz", "(", "matrix_filename", "+", "'_pm_train.npz'", ",", "\n", "pm_train_matrix", ",", "\n", ")", "\n", "scipy", ".", "sparse", ".", "save_npz", "(", "matrix_filename", "+", "'_pm_test.npz'", ",", "\n", "pm_test_matrix", ",", "\n", ")", "\n", "\n", "print", "(", "'TRAIN and TEST datasets generated'", ")", "\n", "all_test_train_data", "=", "(", "web_train_matrix", ",", "\n", "web_test_matrix", ",", "\n", "pm_train_matrix", ",", "\n", "pm_test_matrix", ",", "\n", ")", "\n", "\n", "return", "all_test_train_data", "\n", "\n"]], "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.document_similarity_functions.cosine_sim": [[229, 313], ["print", "sklearn.metrics.pairwise.cosine_similarity", "print", "pandas.DataFrame", "print", "pandas.Series", "print", "pd.DataFrame.loc[].sort_values().rank", "correct_ranks_all.append", "str", "pd.DataFrame.loc[].sort_values"], "function", ["None"], ["", "def", "cosine_sim", "(", "web_test_matrix", ",", "\n", "pm_test_matrix", ",", "\n", "web_test_corpus", ",", "\n", "pm_test_corpus", ",", "\n", "known_links_corpus", ",", "\n", "params_dict", ",", "\n", ")", ":", "\n", "    ", "feature_extraction", "=", "params_dict", "[", "'Feature Extraction'", "]", "\n", "dimensionality_reduction", "=", "params_dict", "[", "'Dimensionality Reduction'", "]", "\n", "apply_cca", "=", "params_dict", "[", "'CCA'", "]", "\n", "tsvd_components", "=", "params_dict", "[", "'T-SVD Components'", "]", "\n", "cca_components", "=", "params_dict", "[", "'CCA Components'", "]", "\n", "\n", "#####", "\n", "\n", "# COSINE SIMILARITY #", "\n", "print", "(", "'Beginning cosine similarity calculations...'", ")", "\n", "\n", "# Cosine distance between web and PubMed vectors in tf-idf representation following transformation using CCA", "\n", "cosine_matrix", "=", "cosine_similarity", "(", "web_test_matrix", ",", "pm_test_matrix", ")", "\n", "print", "(", "'Cosine similarity complete:\\n'", "\n", "+", "str", "(", "cosine_matrix", ".", "shape", ")", ")", "\n", "\n", "# Convert to dataframe", "\n", "cosine_df", "=", "pd", ".", "DataFrame", "(", "cosine_matrix", ",", "\n", "index", "=", "web_test_corpus", ".", "index", ",", "\n", "columns", "=", "pm_test_corpus", ".", "index", ",", "\n", ")", "\n", "\n", "#####", "\n", "\n", "# RANKING OF CORRECT LINKS #", "\n", "print", "(", "'Beginning ranking...'", ")", "\n", "correct_ranks_all", "=", "[", "]", "\n", "\n", "for", "index", "in", "known_links_corpus", ".", "index", ":", "\n", "# Saves web_id and PMID for each link to objects", "\n", "        ", "web_id", "=", "index", "\n", "pmid", "=", "known_links_corpus", ".", "loc", "[", "index", ",", "'pmid'", "]", "\n", "\n", "# Ranks all cosine distances between vectors in BINARY representation", "\n", "ranks", "=", "cosine_df", ".", "loc", "[", "web_id", "]", ".", "sort_values", "(", ")", ".", "rank", "(", "axis", "=", "0", ",", "\n", "method", "=", "'min'", ",", "\n", "ascending", "=", "False", ",", "\n", ")", "\n", "correct_link_rank", "=", "ranks", "[", "pmid", "]", "\n", "correct_ranks_all", ".", "append", "(", "correct_link_rank", ")", "\n", "\n", "", "cos_col_name", "=", "'%s_%s_%s'", "%", "(", "feature_extraction", ",", "\n", "dimensionality_reduction", ",", "\n", "tsvd_components", ",", "\n", ")", "\n", "correct_links_srs", "=", "pd", ".", "Series", "(", "correct_ranks_all", ",", "\n", "index", "=", "known_links_corpus", ".", "index", ",", "\n", "name", "=", "cos_col_name", ",", "\n", ")", "\n", "\n", "'''\n    if apply_cca is False:\n        pickle.dump(known_links_corpus,\n                    open('%s_%s_%s_%s-tsvd_ranks.pkl' % (feature_extraction,\n                                                         dimensionality_reduction,\n                                                         tsvd_components,\n                                                         cca_components,\n                                                         ), 'wb'),\n                    protocol=2,\n                    )\n    else:\n        pickle.dump(known_links_corpus,\n                    open('%s_%s_%s_%s-tsvd_%s-cca_ranks.pkl' % (feature_extraction,\n                                                                dimensionality_reduction,\n                                                                apply_cca,\n                                                                tsvd_components,\n                                                                cca_components,\n                                                                ), 'wb'),\n                    protocol=2,\n                    )\n\n        print('Final results saved to file')\n    '''", "\n", "\n", "print", "(", "'All cosine similarity scores ranked'", ")", "\n", "\n", "return", "correct_links_srs", "\n", "\n"]], "home.repos.pwc.inspect_result.lizaharrison_web2pubmed.None.document_similarity_functions.measures": [[315, 392], ["correct_links_srs.describe", "round", "round", "round", "print", "print", "print", "print", "print", "pickle.dump", "open", "pickle.dump", "pickle.dump", "len", "len", "len", "len", "len", "len", "open", "open"], "function", ["None"], ["", "def", "measures", "(", "correct_links_srs", ",", "\n", "params_dict", ",", "\n", ")", ":", "\n", "    ", "feature_extraction", "=", "params_dict", "[", "'Feature Extraction'", "]", "\n", "dimensionality_reduction", "=", "params_dict", "[", "'Dimensionality Reduction'", "]", "\n", "apply_cca", "=", "params_dict", "[", "'CCA'", "]", "\n", "tsvd_components", "=", "params_dict", "[", "'T-SVD Components'", "]", "\n", "cca_components", "=", "params_dict", "[", "'CCA Components'", "]", "\n", "\n", "# METRIC 1) MEDIAN RANK", "\n", "cosine_metrics", "=", "correct_links_srs", ".", "describe", "(", ")", "\n", "\n", "median", "=", "cosine_metrics", "[", "'50%'", "]", "\n", "iqr_25", "=", "cosine_metrics", "[", "'25%'", "]", "\n", "iqr_75", "=", "cosine_metrics", "[", "'75%'", "]", "\n", "\n", "# METRIC 2) PERCENTAGE CORRECT", "\n", "cosine_correct", "=", "round", "(", "len", "(", "correct_links_srs", ".", "loc", "[", "correct_links_srs", "==", "1", "]", ")", "/", "\n", "len", "(", "correct_links_srs", ")", "*", "100", ",", "2", ")", "\n", "\n", "# METRIC 3) PERCENTAGE IN TOP 50", "\n", "cosine_top_50", "=", "round", "(", "(", "len", "(", "correct_links_srs", ".", "loc", "[", "correct_links_srs", "<=", "50", "]", ")", "/", "\n", "len", "(", "correct_links_srs", ")", ")", "*", "100", ",", "2", ")", "\n", "\n", "# METRIC 3) PERCENTAGE IN TOP 100", "\n", "cosine_top_250", "=", "round", "(", "(", "len", "(", "correct_links_srs", ".", "loc", "[", "correct_links_srs", "<=", "250", "]", ")", "/", "\n", "len", "(", "correct_links_srs", ")", ")", "*", "100", ",", "2", ")", "\n", "\n", "print", "(", "'Median rank (IQR): %s (%s - %s)'", "%", "(", "median", ",", "iqr_25", ",", "iqr_75", ")", ")", "\n", "print", "(", "'Percentage of links ranked correctly: %s'", "%", "cosine_correct", ")", "\n", "print", "(", "'Percentage of links ranked in top 50: %s'", "%", "cosine_top_50", ")", "\n", "print", "(", "'Percentage of links ranked in top 250: %s'", "%", "cosine_top_250", ")", "\n", "\n", "results_data", "=", "{", "'Feature Extraction'", ":", "feature_extraction", ",", "\n", "'Dimensionality Reduction'", ":", "dimensionality_reduction", ",", "\n", "'Min DF'", ":", "params_dict", "[", "'Min DF'", "]", ",", "\n", "'Max DF'", ":", "params_dict", "[", "'Max DF'", "]", ",", "\n", "'T-SVD Components'", ":", "tsvd_components", ",", "\n", "'CCA'", ":", "apply_cca", ",", "\n", "'CCA Components'", ":", "cca_components", ",", "\n", "'Median Rank'", ":", "median", ",", "\n", "'IQR_25'", ":", "iqr_25", ",", "\n", "'IQR_75'", ":", "iqr_75", ",", "\n", "'Percentage correct'", ":", "cosine_correct", ",", "\n", "'Percentage in Top 50'", ":", "cosine_top_50", ",", "\n", "'Percentage in Top 250'", ":", "cosine_top_250", ",", "\n", "}", "\n", "if", "dimensionality_reduction", "==", "'Thresholds'", ":", "\n", "        ", "pickle", ".", "dump", "(", "results_data", ",", "\n", "open", "(", "'%s_%s_%s_%s_results.pkl'", "%", "(", "feature_extraction", ",", "\n", "dimensionality_reduction", ",", "\n", "params_dict", "[", "'Min DF'", "]", ",", "\n", "params_dict", "[", "'Max DF'", "]", ",", "\n", ")", ",", "'wb'", ")", ",", "\n", "protocol", "=", "2", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "if", "apply_cca", "is", "False", ":", "\n", "            ", "pickle", ".", "dump", "(", "results_data", ",", "\n", "open", "(", "'%s_%s_%s_results.pkl'", "%", "(", "feature_extraction", ",", "\n", "dimensionality_reduction", ",", "\n", "tsvd_components", ",", "\n", ")", ",", "'wb'", ")", ",", "\n", "protocol", "=", "2", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "pickle", ".", "dump", "(", "results_data", ",", "\n", "open", "(", "'%s_%s_%s_%s-cca_results.pkl'", "%", "(", "feature_extraction", ",", "\n", "dimensionality_reduction", ",", "\n", "tsvd_components", ",", "\n", "cca_components", ",", "\n", ")", ",", "'wb'", ")", ",", "\n", "protocol", "=", "2", ",", "\n", ")", "\n", "", "", "print", "(", "'Final results saved to file'", ")", "\n", "\n", "return", "results_data", "\n", "\n"]]}