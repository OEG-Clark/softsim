{"home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.SensesVSM.__init__": [[30, 47], ["numpy.array", "vectorspace.SensesVSM.vecs_path.endswith", "vectorspace.SensesVSM.load_aux_senses", "vectorspace.SensesVSM.load_txt", "vectorspace.SensesVSM.vecs_path.endswith", "vectorspace.SensesVSM.normalize", "vectorspace.SensesVSM.load_npz"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.SensesVSM.load_aux_senses", "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.load_txt", "home.repos.pwc.inspect_result.danlou_LMMS.scripts.merge_cat.normalize", "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.SensesVSM.load_npz"], ["    ", "def", "__init__", "(", "self", ",", "vecs_path", ",", "normalize", "=", "True", ")", ":", "\n", "        ", "self", ".", "vecs_path", "=", "vecs_path", "\n", "self", ".", "labels", "=", "[", "]", "\n", "self", ".", "vectors", "=", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "indices", "=", "{", "}", "\n", "self", ".", "ndims", "=", "0", "\n", "\n", "if", "self", ".", "vecs_path", ".", "endswith", "(", "'.txt'", ")", ":", "\n", "            ", "self", ".", "load_txt", "(", "self", ".", "vecs_path", ")", "\n", "\n", "", "elif", "self", ".", "vecs_path", ".", "endswith", "(", "'.npz'", ")", ":", "\n", "            ", "self", ".", "load_npz", "(", "self", ".", "vecs_path", ")", "\n", "\n", "", "self", ".", "load_aux_senses", "(", ")", "\n", "\n", "if", "normalize", ":", "\n", "            ", "self", ".", "normalize", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.SensesVSM.load_txt": [[48, 60], ["numpy.vstack", "set", "open", "enumerate", "line.split", "vectorspace.SensesVSM.labels.append", "vectorspace.SensesVSM.vectors.append", "enumerate", "numpy.array", "list", "map"], "methods", ["None"], ["", "", "def", "load_txt", "(", "self", ",", "txt_vecs_path", ")", ":", "\n", "        ", "self", ".", "vectors", "=", "[", "]", "\n", "with", "open", "(", "txt_vecs_path", ",", "encoding", "=", "'utf-8'", ")", "as", "vecs_f", ":", "\n", "            ", "for", "line_idx", ",", "line", "in", "enumerate", "(", "vecs_f", ")", ":", "\n", "                ", "elems", "=", "line", ".", "split", "(", ")", "\n", "self", ".", "labels", ".", "append", "(", "elems", "[", "0", "]", ")", "\n", "self", ".", "vectors", ".", "append", "(", "np", ".", "array", "(", "list", "(", "map", "(", "float", ",", "elems", "[", "1", ":", "]", ")", ")", ",", "dtype", "=", "np", ".", "float32", ")", ")", "\n", "", "", "self", ".", "vectors", "=", "np", ".", "vstack", "(", "self", ".", "vectors", ")", "\n", "\n", "self", ".", "labels_set", "=", "set", "(", "self", ".", "labels", ")", "\n", "self", ".", "indices", "=", "{", "l", ":", "i", "for", "i", ",", "l", "in", "enumerate", "(", "self", ".", "labels", ")", "}", "\n", "self", ".", "ndims", "=", "self", ".", "vectors", ".", "shape", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.SensesVSM.load_npz": [[61, 69], ["numpy.load", "loader[].tolist", "set", "enumerate"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.load"], ["", "def", "load_npz", "(", "self", ",", "npz_vecs_path", ")", ":", "\n", "        ", "loader", "=", "np", ".", "load", "(", "npz_vecs_path", ")", "\n", "self", ".", "labels", "=", "loader", "[", "'labels'", "]", ".", "tolist", "(", ")", "\n", "self", ".", "vectors", "=", "loader", "[", "'vectors'", "]", "\n", "\n", "self", ".", "labels_set", "=", "set", "(", "self", ".", "labels", ")", "\n", "self", ".", "indices", "=", "{", "l", ":", "i", "for", "i", ",", "l", "in", "enumerate", "(", "self", ".", "labels", ")", "}", "\n", "self", ".", "ndims", "=", "self", ".", "vectors", ".", "shape", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.SensesVSM.load_aux_senses": [[70, 83], ["collections.defaultdict", "vectorspace.SensesVSM.sk_lemmas.items", "set", "collections.defaultdict", "set", "vectorspace.get_sk_lemma", "vectorspace.get_sk_pos", "vectorspace.SensesVSM.lemma_sks[].append", "vectorspace.SensesVSM.lemma_sks.keys", "vectorspace.SensesVSM.sks_by_pos[].append", "vectorspace.SensesVSM.sks_by_pos.keys"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.get_sk_lemma", "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.get_sk_pos"], ["", "def", "load_aux_senses", "(", "self", ")", ":", "\n", "        ", "self", ".", "sk_lemmas", "=", "{", "sk", ":", "get_sk_lemma", "(", "sk", ")", "for", "sk", "in", "self", ".", "labels", "}", "\n", "self", ".", "sk_postags", "=", "{", "sk", ":", "get_sk_pos", "(", "sk", ")", "for", "sk", "in", "self", ".", "labels", "}", "\n", "\n", "self", ".", "lemma_sks", "=", "defaultdict", "(", "list", ")", "\n", "for", "sk", ",", "lemma", "in", "self", ".", "sk_lemmas", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "lemma_sks", "[", "lemma", "]", ".", "append", "(", "sk", ")", "\n", "", "self", ".", "known_lemmas", "=", "set", "(", "self", ".", "lemma_sks", ".", "keys", "(", ")", ")", "\n", "\n", "self", ".", "sks_by_pos", "=", "defaultdict", "(", "list", ")", "\n", "for", "s", "in", "self", ".", "labels", ":", "\n", "            ", "self", ".", "sks_by_pos", "[", "self", ".", "sk_postags", "[", "s", "]", "]", ".", "append", "(", "s", ")", "\n", "", "self", ".", "known_postags", "=", "set", "(", "self", ".", "sks_by_pos", ".", "keys", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.SensesVSM.save_npz": [[84, 89], ["vectorspace.SensesVSM.vecs_path.replace", "numpy.savez_compressed"], "methods", ["None"], ["", "def", "save_npz", "(", "self", ")", ":", "\n", "        ", "npz_path", "=", "self", ".", "vecs_path", ".", "replace", "(", "'.txt'", ",", "'.npz'", ")", "\n", "np", ".", "savez_compressed", "(", "npz_path", ",", "\n", "labels", "=", "self", ".", "labels", ",", "\n", "vectors", "=", "self", ".", "vectors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.SensesVSM.normalize": [[90, 93], ["numpy.linalg.norm"], "methods", ["None"], ["", "def", "normalize", "(", "self", ",", "norm", "=", "'l2'", ")", ":", "\n", "        ", "norms", "=", "np", ".", "linalg", ".", "norm", "(", "self", ".", "vectors", ",", "axis", "=", "1", ")", "\n", "self", ".", "vectors", "=", "(", "self", ".", "vectors", ".", "T", "/", "norms", ")", ".", "T", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.SensesVSM.get_vec": [[94, 96], ["None"], "methods", ["None"], ["", "def", "get_vec", "(", "self", ",", "label", ")", ":", "\n", "        ", "return", "self", ".", "vectors", "[", "self", ".", "indices", "[", "label", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.SensesVSM.similarity": [[97, 101], ["vectorspace.SensesVSM.get_vec", "vectorspace.SensesVSM.get_vec", "numpy.dot().tolist", "numpy.dot"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.get_vec", "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.get_vec"], ["", "def", "similarity", "(", "self", ",", "label1", ",", "label2", ")", ":", "\n", "        ", "v1", "=", "self", ".", "get_vec", "(", "label1", ")", "\n", "v2", "=", "self", ".", "get_vec", "(", "label2", ")", "\n", "return", "np", ".", "dot", "(", "v1", ",", "v2", ")", ".", "tolist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.SensesVSM.match_senses": [[102, 116], ["numpy.dot", "list", "sorted", "numpy.array", "zip", "relevant_sks.append"], "methods", ["None"], ["", "def", "match_senses", "(", "self", ",", "vec", ",", "lemma", "=", "None", ",", "postag", "=", "None", ",", "topn", "=", "100", ")", ":", "\n", "        ", "relevant_sks", "=", "[", "]", "\n", "for", "sk", "in", "self", ".", "labels", ":", "\n", "            ", "if", "(", "lemma", "is", "None", ")", "or", "(", "self", ".", "sk_lemmas", "[", "sk", "]", "==", "lemma", ")", ":", "\n", "                ", "if", "(", "postag", "is", "None", ")", "or", "(", "self", ".", "sk_postags", "[", "sk", "]", "==", "postag", ")", ":", "\n", "                    ", "relevant_sks", ".", "append", "(", "sk", ")", "\n", "\n", "", "", "", "relevant_sks_idxs", "=", "[", "self", ".", "indices", "[", "sk", "]", "for", "sk", "in", "relevant_sks", "]", "\n", "sims", "=", "np", ".", "dot", "(", "self", ".", "vectors", "[", "relevant_sks_idxs", "]", ",", "np", ".", "array", "(", "vec", ")", ")", "\n", "matches", "=", "list", "(", "zip", "(", "relevant_sks", ",", "sims", ")", ")", "\n", "\n", "matches", "=", "sorted", "(", "matches", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "return", "matches", "[", ":", "topn", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.SensesVSM.most_similar_vec": [[117, 124], ["numpy.dot().astype", "numpy.dot().astype.tolist", "r.append", "numpy.dot", "numpy.dot().astype.argsort().tolist", "numpy.dot().astype.argsort"], "methods", ["None"], ["", "def", "most_similar_vec", "(", "self", ",", "vec", ",", "topn", "=", "10", ")", ":", "\n", "        ", "sims", "=", "np", ".", "dot", "(", "self", ".", "vectors", ",", "vec", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "sims_", "=", "sims", ".", "tolist", "(", ")", "\n", "r", "=", "[", "]", "\n", "for", "top_i", "in", "sims", ".", "argsort", "(", ")", ".", "tolist", "(", ")", "[", ":", ":", "-", "1", "]", "[", ":", "topn", "]", ":", "\n", "            ", "r", ".", "append", "(", "(", "self", ".", "labels", "[", "top_i", "]", ",", "sims_", "[", "top_i", "]", ")", ")", "\n", "", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.SensesVSM.sims": [[125, 127], ["numpy.dot().tolist", "numpy.dot", "numpy.array"], "methods", ["None"], ["", "def", "sims", "(", "self", ",", "vec", ")", ":", "\n", "        ", "return", "np", ".", "dot", "(", "self", ".", "vectors", ",", "np", ".", "array", "(", "vec", ")", ")", ".", "tolist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.__init__": [[132, 137], ["numpy.array"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "labels", "=", "[", "]", "\n", "self", ".", "vectors", "=", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "indices", "=", "{", "}", "\n", "self", ".", "ndims", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.load": [[143, 148], ["enumerate"], "methods", ["None"], ["", "def", "load", "(", "self", ",", "vectors", ",", "labels", ")", ":", "\n", "        ", "self", ".", "vectors", "=", "vectors", "\n", "self", ".", "labels", "=", "labels", "\n", "self", ".", "indices", "=", "{", "l", ":", "i", "for", "i", ",", "l", "in", "enumerate", "(", "self", ".", "labels", ")", "}", "\n", "self", ".", "ndims", "=", "self", ".", "vectors", ".", "shape", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.load_txt": [[149, 161], ["set", "numpy.vstack", "open", "enumerate", "line.split", "vectorspace.VSM.labels.append", "vectorspace.VSM.vectors.append", "enumerate", "numpy.array", "list", "map"], "methods", ["None"], ["", "def", "load_txt", "(", "self", ",", "vecs_path", ")", ":", "\n", "        ", "self", ".", "vectors", "=", "[", "]", "\n", "with", "open", "(", "vecs_path", ",", "encoding", "=", "'utf-8'", ")", "as", "vecs_f", ":", "\n", "            ", "for", "line_idx", ",", "line", "in", "enumerate", "(", "vecs_f", ")", ":", "\n", "                ", "elems", "=", "line", ".", "split", "(", ")", "\n", "self", ".", "labels", ".", "append", "(", "elems", "[", "0", "]", ")", "\n", "self", ".", "vectors", ".", "append", "(", "np", ".", "array", "(", "list", "(", "map", "(", "float", ",", "elems", "[", "1", ":", "]", ")", ")", ",", "dtype", "=", "np", ".", "float32", ")", ")", "\n", "\n", "", "", "self", ".", "labels_set", "=", "set", "(", "self", ".", "labels", ")", "\n", "self", ".", "vectors", "=", "np", ".", "vstack", "(", "self", ".", "vectors", ")", "\n", "self", ".", "indices", "=", "{", "l", ":", "i", "for", "i", ",", "l", "in", "enumerate", "(", "self", ".", "labels", ")", "}", "\n", "self", ".", "ndims", "=", "self", ".", "vectors", ".", "shape", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.normalize": [[162, 164], ["numpy.linalg.norm"], "methods", ["None"], ["", "def", "normalize", "(", "self", ",", "norm", "=", "'l2'", ")", ":", "\n", "        ", "self", ".", "vectors", "=", "(", "self", ".", "vectors", ".", "T", "/", "np", ".", "linalg", ".", "norm", "(", "self", ".", "vectors", ",", "axis", "=", "1", ")", ")", ".", "T", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.get_vec": [[165, 167], ["None"], "methods", ["None"], ["", "def", "get_vec", "(", "self", ",", "label", ")", ":", "\n", "        ", "return", "self", ".", "vectors", "[", "self", ".", "indices", "[", "label", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.similarity": [[168, 172], ["vectorspace.VSM.get_vec", "vectorspace.VSM.get_vec", "numpy.dot().tolist", "numpy.dot"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.get_vec", "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.get_vec"], ["", "def", "similarity", "(", "self", ",", "label1", ",", "label2", ")", ":", "\n", "        ", "v1", "=", "self", ".", "get_vec", "(", "label1", ")", "\n", "v2", "=", "self", ".", "get_vec", "(", "label2", ")", "\n", "return", "np", ".", "dot", "(", "v1", ",", "v2", ")", ".", "tolist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.most_similar_vec": [[173, 180], ["numpy.dot().astype", "numpy.dot().astype.tolist", "r.append", "numpy.dot", "numpy.dot().astype.argsort().tolist", "numpy.dot().astype.argsort"], "methods", ["None"], ["", "def", "most_similar_vec", "(", "self", ",", "vec", ",", "topn", "=", "10", ")", ":", "\n", "        ", "sims", "=", "np", ".", "dot", "(", "self", ".", "vectors", ",", "vec", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "sims_", "=", "sims", ".", "tolist", "(", ")", "\n", "r", "=", "[", "]", "\n", "for", "top_i", "in", "sims", ".", "argsort", "(", ")", ".", "tolist", "(", ")", "[", ":", ":", "-", "1", "]", "[", ":", "topn", "]", ":", "\n", "            ", "r", ".", "append", "(", "(", "self", ".", "labels", "[", "top_i", "]", ",", "sims_", "[", "top_i", "]", ")", ")", "\n", "", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.most_similar": [[181, 183], ["vectorspace.VSM.most_similar_vec", "vectorspace.VSM.get_vec"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.most_similar_vec", "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.get_vec"], ["", "def", "most_similar", "(", "self", ",", "label", ",", "topn", "=", "10", ")", ":", "\n", "        ", "return", "self", ".", "most_similar_vec", "(", "self", ".", "get_vec", "(", "label", ")", ",", "topn", "=", "topn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.sims": [[184, 186], ["numpy.dot().tolist", "numpy.dot", "numpy.array"], "methods", ["None"], ["", "def", "sims", "(", "self", ",", "vec", ")", ":", "\n", "        ", "return", "np", ".", "dot", "(", "self", ".", "vectors", ",", "np", ".", "array", "(", "vec", ")", ")", ".", "tolist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.get_sk_type": [[8, 10], ["int", "[].split", "sensekey.split"], "function", ["None"], ["def", "get_sk_type", "(", "sensekey", ")", ":", "\n", "    ", "return", "int", "(", "sensekey", ".", "split", "(", "'%'", ")", "[", "1", "]", ".", "split", "(", "':'", ")", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.get_sk_pos": [[12, 22], ["vectorspace.get_sk_type", "vectorspace.get_sk_type"], "function", ["home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.get_sk_type", "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.get_sk_type"], ["", "def", "get_sk_pos", "(", "sk", ",", "tagtype", "=", "'long'", ")", ":", "\n", "# merges ADJ with ADJ_SAT", "\n", "\n", "    ", "if", "tagtype", "==", "'long'", ":", "\n", "        ", "type2pos", "=", "{", "1", ":", "'NOUN'", ",", "2", ":", "'VERB'", ",", "3", ":", "'ADJ'", ",", "4", ":", "'ADV'", ",", "5", ":", "'ADJ'", "}", "\n", "return", "type2pos", "[", "get_sk_type", "(", "sk", ")", "]", "\n", "\n", "", "elif", "tagtype", "==", "'short'", ":", "\n", "        ", "type2pos", "=", "{", "1", ":", "'n'", ",", "2", ":", "'v'", ",", "3", ":", "'s'", ",", "4", ":", "'r'", ",", "5", ":", "'s'", "}", "\n", "return", "type2pos", "[", "get_sk_type", "(", "sk", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.get_sk_lemma": [[24, 26], ["sensekey.split"], "function", ["None"], ["", "", "def", "get_sk_lemma", "(", "sensekey", ")", ":", "\n", "    ", "return", "sensekey", ".", "split", "(", "'%'", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.__init__": [[19, 35], ["transformers_encoder.TransformersEncoder.load_nlm", "transformers_encoder.TransformersEncoder.load_layer_weights"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.load_nlm", "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.load_layer_weights"], ["    ", "def", "__init__", "(", "self", ",", "nlm_config", ")", ":", "\n", "        ", "self", ".", "nlm_config", "=", "nlm_config", "\n", "self", ".", "nlm_model", "=", "None", "\n", "self", ".", "nlm_tokenizer", "=", "None", "\n", "self", ".", "nlm_weights", "=", "[", "]", "\n", "\n", "if", "'weights_path'", "not", "in", "self", ".", "nlm_config", ":", "\n", "            ", "self", ".", "nlm_config", "[", "'weights_path'", "]", "=", "''", "\n", "\n", "", "if", "'subword_op'", "not", "in", "self", ".", "nlm_config", ":", "\n", "            ", "self", ".", "nlm_config", "[", "'subword_op'", "]", "=", "'mean'", "\n", "\n", "", "self", ".", "load_nlm", "(", "nlm_config", "[", "'model_name_or_path'", "]", ")", "\n", "\n", "if", "nlm_config", "[", "'weights_path'", "]", "!=", "''", ":", "\n", "            ", "self", ".", "load_layer_weights", "(", "nlm_config", "[", "'weights_path'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.load_layer_weights": [[36, 43], ["numpy.array", "open", "transformers_encoder.TransformersEncoder.nlm_weights.append", "float", "line.strip"], "methods", ["None"], ["", "", "def", "load_layer_weights", "(", "self", ",", "weights_path", ")", ":", "\n", "        ", "self", ".", "nlm_weights", "=", "[", "]", "\n", "with", "open", "(", "weights_path", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "self", ".", "nlm_weights", ".", "append", "(", "float", "(", "line", ".", "strip", "(", ")", ")", ")", "\n", "# self.nlm_weights = th.tensor(self.nlm_weights).to('cuda')", "\n", "", "", "self", ".", "nlm_weights", "=", "np", ".", "array", "(", "self", ".", "nlm_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.load_nlm": [[44, 92], ["model_name_or_path.startswith", "transformers_encoder.TransformersEncoder.nlm_model.eval", "transformers_encoder.TransformersEncoder.nlm_model.to", "transformers.BertModel.from_pretrained", "transformers.BertTokenizer.from_pretrained", "model_name_or_path.startswith", "transformers_encoder.TransformersEncoder.nlm_tokenizer.encode", "transformers_encoder.TransformersEncoder.nlm_tokenizer.encode", "transformers_encoder.TransformersEncoder.nlm_tokenizer.encode", "transformers.XLNetModel.from_pretrained", "transformers.XLNetTokenizer.from_pretrained", "transformers_encoder.TransformersEncoder.nlm_tokenizer.encode", "transformers_encoder.TransformersEncoder.nlm_tokenizer.encode", "transformers_encoder.TransformersEncoder.nlm_tokenizer.encode", "model_name_or_path.startswith", "model_name_or_path.startswith", "transformers.RobertaModel.from_pretrained", "transformers.RobertaTokenizer.from_pretrained", "model_name_or_path.startswith", "transformers_encoder.TransformersEncoder.nlm_tokenizer.encode", "transformers_encoder.TransformersEncoder.nlm_tokenizer.encode", "transformers_encoder.TransformersEncoder.nlm_tokenizer.encode", "transformers.AlbertModel.from_pretrained", "transformers.AlbertTokenizer.from_pretrained", "model_name_or_path.startswith", "transformers_encoder.TransformersEncoder.nlm_tokenizer.encode", "transformers_encoder.TransformersEncoder.nlm_tokenizer.encode", "transformers_encoder.TransformersEncoder.nlm_tokenizer.encode", "transformers.DebertaV2Model.from_pretrained", "transformers.DebertaV2Tokenizer.from_pretrained", "BaseException", "transformers_encoder.TransformersEncoder.nlm_tokenizer.encode", "transformers_encoder.TransformersEncoder.nlm_tokenizer.encode", "transformers_encoder.TransformersEncoder.nlm_tokenizer.encode"], "methods", ["None"], ["", "def", "load_nlm", "(", "self", ",", "model_name_or_path", ")", ":", "\n", "\n", "        ", "if", "model_name_or_path", ".", "startswith", "(", "'bert-'", ")", ":", "\n", "            ", "self", ".", "nlm_model", "=", "BertModel", ".", "from_pretrained", "(", "model_name_or_path", ",", "output_hidden_states", "=", "True", ")", "\n", "self", ".", "nlm_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "model_name_or_path", ")", "\n", "\n", "self", ".", "cls_encoding", "=", "self", ".", "nlm_tokenizer", ".", "encode", "(", "self", ".", "nlm_tokenizer", ".", "cls_token", ",", "add_special_tokens", "=", "False", ")", "[", "0", "]", "\n", "self", ".", "sep_encoding", "=", "self", ".", "nlm_tokenizer", ".", "encode", "(", "self", ".", "nlm_tokenizer", ".", "sep_token", ",", "add_special_tokens", "=", "False", ")", "[", "0", "]", "\n", "self", ".", "pad_encoding", "=", "self", ".", "nlm_tokenizer", ".", "encode", "(", "self", ".", "nlm_tokenizer", ".", "pad_token", ",", "add_special_tokens", "=", "False", ")", "[", "0", "]", "\n", "\n", "", "elif", "model_name_or_path", ".", "startswith", "(", "'xlnet-'", ")", ":", "\n", "            ", "self", ".", "nlm_model", "=", "XLNetModel", ".", "from_pretrained", "(", "model_name_or_path", ",", "output_hidden_states", "=", "True", ")", "\n", "self", ".", "nlm_tokenizer", "=", "XLNetTokenizer", ".", "from_pretrained", "(", "model_name_or_path", ")", "\n", "\n", "self", ".", "cls_encoding", "=", "self", ".", "nlm_tokenizer", ".", "encode", "(", "self", ".", "nlm_tokenizer", ".", "cls_token", ",", "add_special_tokens", "=", "False", ")", "[", "0", "]", "\n", "self", ".", "sep_encoding", "=", "self", ".", "nlm_tokenizer", ".", "encode", "(", "self", ".", "nlm_tokenizer", ".", "sep_token", ",", "add_special_tokens", "=", "False", ")", "[", "0", "]", "\n", "self", ".", "pad_encoding", "=", "self", ".", "nlm_tokenizer", ".", "encode", "(", "self", ".", "nlm_tokenizer", ".", "pad_token", ",", "add_special_tokens", "=", "False", ")", "[", "0", "]", "\n", "\n", "", "elif", "model_name_or_path", ".", "startswith", "(", "'roberta-'", ")", "or", "model_name_or_path", ".", "startswith", "(", "'cardiffnlp/twitter-roberta-'", ")", ":", "\n", "            ", "self", ".", "nlm_model", "=", "RobertaModel", ".", "from_pretrained", "(", "model_name_or_path", ",", "output_hidden_states", "=", "True", ")", "\n", "self", ".", "nlm_tokenizer", "=", "RobertaTokenizer", ".", "from_pretrained", "(", "model_name_or_path", ")", "\n", "\n", "self", ".", "bos_encoding", "=", "self", ".", "nlm_tokenizer", ".", "encode", "(", "self", ".", "nlm_tokenizer", ".", "bos_token", ",", "add_special_tokens", "=", "False", ")", "[", "0", "]", "\n", "self", ".", "eos_encoding", "=", "self", ".", "nlm_tokenizer", ".", "encode", "(", "self", ".", "nlm_tokenizer", ".", "eos_token", ",", "add_special_tokens", "=", "False", ")", "[", "0", "]", "\n", "self", ".", "pad_encoding", "=", "self", ".", "nlm_tokenizer", ".", "encode", "(", "self", ".", "nlm_tokenizer", ".", "pad_token", ",", "add_special_tokens", "=", "False", ")", "[", "0", "]", "\n", "\n", "", "elif", "model_name_or_path", ".", "startswith", "(", "'albert-'", ")", ":", "\n", "            ", "self", ".", "nlm_model", "=", "AlbertModel", ".", "from_pretrained", "(", "model_name_or_path", ",", "output_hidden_states", "=", "True", ")", "\n", "self", ".", "nlm_tokenizer", "=", "AlbertTokenizer", ".", "from_pretrained", "(", "model_name_or_path", ")", "\n", "\n", "self", ".", "cls_encoding", "=", "self", ".", "nlm_tokenizer", ".", "encode", "(", "self", ".", "nlm_tokenizer", ".", "cls_token", ",", "add_special_tokens", "=", "False", ")", "[", "0", "]", "\n", "self", ".", "sep_encoding", "=", "self", ".", "nlm_tokenizer", ".", "encode", "(", "self", ".", "nlm_tokenizer", ".", "sep_token", ",", "add_special_tokens", "=", "False", ")", "[", "0", "]", "\n", "self", ".", "pad_encoding", "=", "self", ".", "nlm_tokenizer", ".", "encode", "(", "self", ".", "nlm_tokenizer", ".", "pad_token", ",", "add_special_tokens", "=", "False", ")", "[", "0", "]", "\n", "\n", "", "elif", "model_name_or_path", ".", "startswith", "(", "'microsoft/deberta-v2-'", ")", ":", "\n", "            ", "self", ".", "nlm_model", "=", "DebertaV2Model", ".", "from_pretrained", "(", "model_name_or_path", ",", "output_hidden_states", "=", "True", ")", "\n", "self", ".", "nlm_tokenizer", "=", "DebertaV2Tokenizer", ".", "from_pretrained", "(", "model_name_or_path", ")", "\n", "\n", "self", ".", "cls_encoding", "=", "self", ".", "nlm_tokenizer", ".", "encode", "(", "self", ".", "nlm_tokenizer", ".", "cls_token", ",", "add_special_tokens", "=", "False", ")", "[", "0", "]", "\n", "self", ".", "sep_encoding", "=", "self", ".", "nlm_tokenizer", ".", "encode", "(", "self", ".", "nlm_tokenizer", ".", "sep_token", ",", "add_special_tokens", "=", "False", ")", "[", "0", "]", "\n", "self", ".", "pad_encoding", "=", "self", ".", "nlm_tokenizer", ".", "encode", "(", "self", ".", "nlm_tokenizer", ".", "pad_token", ",", "add_special_tokens", "=", "False", ")", "[", "0", "]", "\n", "\n", "", "else", ":", "\n", "# TODO: support paths", "\n", "            ", "raise", "(", "BaseException", "(", "'Invalid model_name - %s'", "%", "model_name_or_path", ")", ")", "\n", "\n", "", "self", ".", "nlm_model", ".", "eval", "(", ")", "\n", "self", ".", "nlm_model", ".", "to", "(", "'cuda'", ")", "# TODO: make cuda optional", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.encode_token": [[93, 96], ["transformers_encoder.TransformersEncoder.nlm_tokenizer.encode"], "methods", ["None"], ["", "def", "encode_token", "(", "self", ",", "token", ")", ":", "\n", "# returns list of subtokens", "\n", "        ", "return", "self", ".", "nlm_tokenizer", ".", "encode", "(", "token", ",", "add_special_tokens", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.get_encodings": [[97, 122], ["model_name_or_path.startswith", "model_name_or_path.startswith", "transformers_encoder.TransformersEncoder.nlm_tokenizer.encode", "zip", "len", "m.append", "transformers_encoder.TransformersEncoder.encode_token", "m.append", "group.append"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.encode_token"], ["", "def", "get_encodings", "(", "self", ",", "tokens", ")", ":", "\n", "\n", "        ", "model_name_or_path", "=", "self", ".", "nlm_config", "[", "'model_name_or_path'", "]", "\n", "\n", "if", "model_name_or_path", ".", "startswith", "(", "'roberta-'", ")", "or", "model_name_or_path", ".", "startswith", "(", "'cardiffnlp/twitter-roberta-'", ")", ":", "\n", "\n", "            ", "encodings", "=", "self", ".", "nlm_tokenizer", ".", "encode", "(", "' '", ".", "join", "(", "tokens", ")", ",", "add_special_tokens", "=", "False", ")", "\n", "decodings", "=", "[", "self", ".", "nlm_tokenizer", ".", "decoder", "[", "e", "]", "for", "e", "in", "encodings", "]", "\n", "\n", "m", "=", "[", "]", "\n", "group", "=", "[", "]", "\n", "for", "encoded_id", ",", "decoded_token", "in", "zip", "(", "encodings", ",", "decodings", ")", ":", "\n", "                ", "if", "decoded_token", "[", "0", "]", "==", "'\u0120'", ":", "\n", "                    ", "m", ".", "append", "(", "group", ")", "\n", "group", "=", "[", "encoded_id", "]", "\n", "", "else", ":", "\n", "                    ", "group", ".", "append", "(", "encoded_id", ")", "\n", "\n", "", "", "if", "len", "(", "group", ")", ">", "0", ":", "\n", "                ", "m", ".", "append", "(", "group", ")", "\n", "\n", "", "return", "m", "\n", "\n", "", "else", ":", "\n", "            ", "return", "[", "self", ".", "encode_token", "(", "t", ")", "for", "t", "in", "tokens", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.flatten_encodings": [[123, 125], ["sum"], "methods", ["None"], ["", "", "def", "flatten_encodings", "(", "self", ",", "encodings", ")", ":", "\n", "        ", "return", "sum", "(", "encodings", ",", "[", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.add_special_encodings": [[126, 144], ["model_name_or_path.startswith", "model_name_or_path.startswith", "model_name_or_path.startswith", "model_name_or_path.startswith", "model_name_or_path.startswith", "model_name_or_path.startswith"], "methods", ["None"], ["", "def", "add_special_encodings", "(", "self", ",", "encodings", ")", ":", "\n", "\n", "        ", "model_name_or_path", "=", "self", ".", "nlm_config", "[", "'model_name_or_path'", "]", "\n", "\n", "if", "model_name_or_path", ".", "startswith", "(", "'bert-'", ")", ":", "\n", "            ", "return", "[", "self", ".", "cls_encoding", "]", "+", "encodings", "+", "[", "self", ".", "sep_encoding", "]", "\n", "\n", "", "elif", "model_name_or_path", ".", "startswith", "(", "'xlnet-'", ")", ":", "\n", "            ", "return", "encodings", "+", "[", "self", ".", "sep_encoding", ",", "self", ".", "cls_encoding", "]", "\n", "\n", "", "elif", "model_name_or_path", ".", "startswith", "(", "'roberta-'", ")", "or", "model_name_or_path", ".", "startswith", "(", "'cardiffnlp/twitter-roberta-'", ")", ":", "\n", "            ", "return", "[", "self", ".", "bos_encoding", "]", "+", "encodings", "+", "[", "self", ".", "eos_encoding", "]", "\n", "\n", "", "elif", "model_name_or_path", ".", "startswith", "(", "'albert-'", ")", ":", "\n", "            ", "return", "[", "self", ".", "cls_encoding", "]", "+", "encodings", "+", "[", "self", ".", "sep_encoding", "]", "\n", "\n", "", "elif", "model_name_or_path", ".", "startswith", "(", "'microsoft/deberta-v2-'", ")", ":", "\n", "            ", "return", "[", "self", ".", "cls_encoding", "]", "+", "encodings", "+", "[", "self", ".", "sep_encoding", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.add_padding_encodings": [[145, 148], ["len"], "methods", ["None"], ["", "", "def", "add_padding_encodings", "(", "self", ",", "encodings", ",", "max_len", ")", ":", "\n", "        ", "encodings", "+=", "[", "self", ".", "pad_encoding", "]", "*", "(", "max_len", "-", "len", "(", "encodings", ")", ")", "\n", "return", "encodings", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.get_attention_mask": [[149, 157], ["att_mask.append", "att_mask.append"], "methods", ["None"], ["", "def", "get_attention_mask", "(", "self", ",", "encodings", ")", ":", "\n", "        ", "att_mask", "=", "[", "]", "\n", "for", "enc", "in", "encodings", ":", "\n", "            ", "if", "enc", "==", "self", ".", "pad_encoding", ":", "\n", "                ", "att_mask", ".", "append", "(", "0", ")", "\n", "", "else", ":", "\n", "                ", "att_mask", ".", "append", "(", "1", ")", "\n", "", "", "return", "att_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.merge_subword_embeddings": [[158, 187], ["zip", "torch.zeros().to.detach().cpu().numpy", "torch.zeros().to", "tok_embeddings.append", "tok_embeddings.append", "len", "BaseException", "torch.zeros().to.detach().cpu", "torch.zeros", "torch.zeros().to.detach"], "methods", ["None"], ["", "def", "merge_subword_embeddings", "(", "self", ",", "tokens", ",", "encodings", ",", "embeddings", ",", "return_tokens", "=", "True", ")", ":", "\n", "# align and merge subword embeddings", "\n", "        ", "tok_embeddings", "=", "[", "]", "\n", "encoding_idx", "=", "0", "\n", "for", "tok", ",", "tok_encodings", "in", "zip", "(", "tokens", ",", "encodings", ")", ":", "\n", "\n", "            ", "if", "self", ".", "nlm_config", "[", "'subword_op'", "]", "==", "'mean'", ":", "\n", "                ", "tok_embedding", "=", "th", ".", "zeros", "(", "embeddings", ".", "shape", "[", "-", "1", "]", ")", ".", "to", "(", "'cuda'", ")", "\n", "for", "_", "in", "tok_encodings", ":", "\n", "                    ", "tok_embedding", "+=", "embeddings", "[", "encoding_idx", "]", "\n", "encoding_idx", "+=", "1", "\n", "", "tok_embedding", "=", "tok_embedding", "/", "len", "(", "tok_encodings", ")", "# avg of subword embs", "\n", "\n", "", "elif", "self", ".", "nlm_config", "[", "'subword_op'", "]", "==", "'first'", ":", "\n", "                ", "tok_embedding", "=", "embeddings", "[", "encoding_idx", "]", "\n", "for", "_", "in", "tok_encodings", ":", "\n", "                    ", "encoding_idx", "+=", "1", "# just move idx", "\n", "\n", "", "", "else", ":", "\n", "                ", "raise", "(", "BaseException", "(", "'Invalid subword_op - %s'", "%", "self", ".", "nlm_config", "[", "'subword_op'", "]", ")", ")", "\n", "\n", "", "tok_embedding", "=", "tok_embedding", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "if", "return_tokens", ":", "\n", "                ", "tok_embeddings", ".", "append", "(", "(", "tok", ",", "tok_embedding", ")", ")", "\n", "", "else", ":", "\n", "                ", "tok_embeddings", ".", "append", "(", "tok_embedding", ")", "\n", "\n", "", "", "return", "tok_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.get_num_features": [[188, 190], ["len", "transformers_encoder.TransformersEncoder.get_encodings"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.get_encodings"], ["", "def", "get_num_features", "(", "self", ",", "tokens", ",", "n_special_toks", "=", "2", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "get_encodings", "(", "tokens", ")", ")", "+", "n_special_toks", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.get_num_subtokens": [[191, 193], ["len", "transformers_encoder.TransformersEncoder.get_encodings"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.get_encodings"], ["", "def", "get_num_subtokens", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "get_encodings", "(", "tokens", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.get_token_embeddings_batch": [[194, 287], ["zip", "torch.tensor().to", "torch.tensor().to", "enumerate", "transformers_encoder.TransformersEncoder.get_encodings", "max", "transformers_encoder.TransformersEncoder.flatten_encodings", "transformers_encoder.TransformersEncoder.add_special_encodings", "transformers_encoder.TransformersEncoder.add_padding_encodings", "torch.tensor().to.append", "transformers_encoder.TransformersEncoder.get_attention_mask", "torch.tensor().to.append", "torch.no_grad", "enumerate", "merged_batch_hidden_states.append", "range", "combined_batch_embeddings.append", "len", "len", "torch.tensor", "torch.tensor", "transformers_encoder.TransformersEncoder.nlm_model", "transformers_encoder.TransformersEncoder.nlm_config[].startswith", "transformers_encoder.TransformersEncoder.merge_subword_embeddings", "merged_layer_hidden_states.append", "len", "range", "combined_sent_embeddings.append", "len", "len", "numpy.array.append", "len", "numpy.array", "transformers_encoder.TransformersEncoder.flatten_encodings", "numpy.array.sum", "numpy.array.mean", "numpy.stack", "tok_combined_vec.sum.sum.sum", "zip"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.get_encodings", "home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.flatten_encodings", "home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.add_special_encodings", "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.add_padding_encodings", "home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.get_attention_mask", "home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.merge_subword_embeddings", "home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.flatten_encodings"], ["", "def", "get_token_embeddings_batch", "(", "self", ",", "batch_sent_tokens", ",", "return_tokens", "=", "True", ")", ":", "\n", "\n", "        ", "batch_sent_encodings", "=", "[", "self", ".", "get_encodings", "(", "sent_tokens", ")", "for", "sent_tokens", "in", "batch_sent_tokens", "]", "\n", "\n", "batch_max_len", "=", "max", "(", "[", "len", "(", "self", ".", "flatten_encodings", "(", "e", ")", ")", "for", "e", "in", "batch_sent_encodings", "]", ")", "+", "2", "\n", "\n", "# prepare nlm input", "\n", "input_ids", ",", "input_mask", "=", "[", "]", ",", "[", "]", "\n", "for", "sent_tokens", ",", "sent_encodings", "in", "zip", "(", "batch_sent_tokens", ",", "batch_sent_encodings", ")", ":", "\n", "\n", "            ", "sent_encodings", "=", "self", ".", "flatten_encodings", "(", "sent_encodings", ")", "\n", "sent_encodings", "=", "self", ".", "add_special_encodings", "(", "sent_encodings", ")", "\n", "sent_encodings", "=", "self", ".", "add_padding_encodings", "(", "sent_encodings", ",", "batch_max_len", ")", "\n", "input_ids", ".", "append", "(", "sent_encodings", ")", "\n", "\n", "sent_attention", "=", "self", ".", "get_attention_mask", "(", "sent_encodings", ")", "\n", "input_mask", ".", "append", "(", "sent_attention", ")", "\n", "\n", "assert", "len", "(", "sent_encodings", ")", "==", "len", "(", "sent_attention", ")", "\n", "\n", "\n", "", "input_ids", "=", "th", ".", "tensor", "(", "input_ids", ")", ".", "to", "(", "'cuda'", ")", "\n", "input_mask", "=", "th", ".", "tensor", "(", "input_mask", ")", ".", "to", "(", "'cuda'", ")", "\n", "with", "th", ".", "no_grad", "(", ")", ":", "\n", "\n", "# if self.nlm_config['model_name_or_path'].startswith('xlnet-'):", "\n", "#     pooled, batch_hidden_states = self.nlm_model(input_ids, attention_mask=input_mask)", "\n", "#     last_layer = batch_hidden_states[-1]", "\n", "# ", "\n", "# else:", "\n", "#     # last_layer, pooled, batch_hidden_states = self.nlm_model(input_ids, attention_mask=input_mask)", "\n", "\n", "            ", "batch_hidden_states", "=", "self", ".", "nlm_model", "(", "input_ids", ",", "attention_mask", "=", "input_mask", ")", "[", "'hidden_states'", "]", "\n", "\n", "# select layers of interest", "\n", "", "sel_hidden_states", "=", "[", "batch_hidden_states", "[", "i", "]", "for", "i", "in", "self", ".", "nlm_config", "[", "'layers'", "]", "]", "\n", "\n", "# align embeddings (merge subword embeddings)", "\n", "merged_batch_hidden_states", "=", "[", "]", "\n", "for", "layer_hidden_states", "in", "sel_hidden_states", ":", "\n", "            ", "merged_layer_hidden_states", "=", "[", "]", "\n", "for", "sent_idx", ",", "sent_embeddings", "in", "enumerate", "(", "layer_hidden_states", ")", ":", "\n", "\n", "# ignoring special tokens, depends where models position them", "\n", "                ", "if", "self", ".", "nlm_config", "[", "'model_name_or_path'", "]", ".", "startswith", "(", "'xlnet-'", ")", ":", "\n", "                    ", "sent_embeddings", "=", "sent_embeddings", "[", ":", "-", "2", "]", "\n", "", "else", ":", "\n", "                    ", "sent_embeddings", "=", "sent_embeddings", "[", "1", ":", "-", "1", "]", "\n", "\n", "", "sent_tokens", "=", "batch_sent_tokens", "[", "sent_idx", "]", "# ['Mr.', 'Keo', 'agreed', '.']", "\n", "sent_encodings", "=", "batch_sent_encodings", "[", "sent_idx", "]", "# [[1828, 119], [26835, 1186], [2675], [119]]  (bert-l)", "\n", "\n", "sent_embeddings", "=", "self", ".", "merge_subword_embeddings", "(", "sent_tokens", ",", "sent_encodings", ",", "sent_embeddings", ",", "return_tokens", "=", "return_tokens", ")", "\n", "merged_layer_hidden_states", ".", "append", "(", "sent_embeddings", ")", "\n", "", "merged_batch_hidden_states", ".", "append", "(", "merged_layer_hidden_states", ")", "\n", "\n", "\n", "# combine layers", "\n", "", "combined_batch_embeddings", "=", "[", "]", "\n", "for", "sent_idx", ",", "sent_tokens", "in", "enumerate", "(", "batch_sent_tokens", ")", ":", "\n", "\n", "            ", "combined_sent_embeddings", "=", "[", "]", "\n", "for", "tok_idx", "in", "range", "(", "len", "(", "sent_tokens", ")", ")", ":", "\n", "                ", "tok_layer_vecs", "=", "[", "]", "\n", "for", "layer_idx", "in", "range", "(", "len", "(", "merged_batch_hidden_states", ")", ")", ":", "\n", "                    ", "tok_layer_vecs", ".", "append", "(", "merged_batch_hidden_states", "[", "layer_idx", "]", "[", "sent_idx", "]", "[", "tok_idx", "]", "[", "1", "]", ")", "\n", "\n", "", "if", "len", "(", "tok_layer_vecs", ")", "==", "1", ":", "\n", "                    ", "tok_combined_vec", "=", "tok_layer_vecs", "[", "0", "]", "\n", "\n", "", "else", ":", "\n", "# tok_layer_vecs = th.stack(tok_layer_vecs)", "\n", "                    ", "tok_layer_vecs", "=", "np", ".", "array", "(", "tok_layer_vecs", ")", "\n", "\n", "if", "self", ".", "nlm_config", "[", "'layer_op'", "]", "==", "'sum'", ":", "\n", "                        ", "tok_combined_vec", "=", "tok_layer_vecs", ".", "sum", "(", "axis", "=", "0", ")", "\n", "", "elif", "self", ".", "nlm_config", "[", "'layer_op'", "]", "==", "'mean'", ":", "\n", "                        ", "tok_combined_vec", "=", "tok_layer_vecs", ".", "mean", "(", "axis", "=", "0", ")", "\n", "", "elif", "self", ".", "nlm_config", "[", "'layer_op'", "]", "==", "'ws'", ":", "\n", "                        ", "tok_combined_vec", "=", "[", "w", "*", "m", "for", "w", ",", "m", "in", "zip", "(", "self", ".", "nlm_weights", ",", "tok_layer_vecs", ")", "]", "\n", "tok_combined_vec", "=", "np", ".", "stack", "(", "tok_combined_vec", ")", "\n", "tok_combined_vec", "=", "tok_combined_vec", ".", "sum", "(", "axis", "=", "0", ")", "\n", "# sel_hidden_states = self.nlm_weights.dot(sel_hidden_states)", "\n", "", "else", ":", "\n", "                        ", "tok_combined_vec", "=", "tok_layer_vecs", "\n", "\n", "", "", "tok", "=", "merged_batch_hidden_states", "[", "layer_idx", "]", "[", "sent_idx", "]", "[", "tok_idx", "]", "[", "0", "]", "\n", "combined_sent_embeddings", ".", "append", "(", "(", "tok", ",", "tok_combined_vec", ")", ")", "\n", "\n", "", "combined_batch_embeddings", ".", "append", "(", "combined_sent_embeddings", ")", "\n", "\n", "# return [combined_batch_embeddings]", "\n", "", "return", "combined_batch_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.token_embeddings": [[288, 290], ["transformers_encoder.TransformersEncoder.get_token_embeddings_batch"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.get_token_embeddings_batch"], ["", "def", "token_embeddings", "(", "self", ",", "batch_sent_tokens", ",", "return_tokens", "=", "True", ")", ":", "\n", "        ", "return", "self", ".", "get_token_embeddings_batch", "(", "batch_sent_tokens", ",", "return_tokens", "=", "return_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.is_valid": [[291, 297], ["transformers_encoder.TransformersEncoder.flatten_encodings", "transformers_encoder.TransformersEncoder.get_encodings", "len"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.transformers_encoder.TransformersEncoder.flatten_encodings", "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.get_encodings"], ["", "def", "is_valid", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "encodings", "=", "self", ".", "flatten_encodings", "(", "self", ".", "get_encodings", "(", "tokens", ")", ")", "\n", "if", "(", "len", "(", "encodings", ")", "+", "2", ")", ">", "self", ".", "nlm_config", "[", "'max_seq_len'", "]", ":", "\n", "            ", "return", "False", "\n", "", "else", ":", "\n", "            ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.__init__": [[104, 114], ["fairseq_encoder.FairSeqEncoder.load_nlm", "fairseq_encoder.FairSeqEncoder.load_layer_weights"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.load_nlm", "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.load_layer_weights"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "self", ".", "config", "=", "config", "\n", "self", ".", "nlm_model", "=", "None", "\n", "self", ".", "pad_idx", "=", "None", "\n", "self", ".", "nlm_weights", "=", "[", "]", "\n", "\n", "self", ".", "load_nlm", "(", "self", ".", "config", "[", "'model_name_or_path'", "]", ")", "\n", "\n", "if", "config", "[", "'weights_path'", "]", "!=", "''", ":", "\n", "            ", "self", ".", "load_layer_weights", "(", "config", "[", "'weights_path'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.load_layer_weights": [[115, 121], ["torch.tensor().to", "open", "fairseq_encoder.FairSeqEncoder.nlm_weights.append", "torch.tensor", "float", "line.strip"], "methods", ["None"], ["", "", "def", "load_layer_weights", "(", "self", ",", "weights_path", ")", ":", "\n", "        ", "self", ".", "nlm_weights", "=", "[", "]", "\n", "with", "open", "(", "weights_path", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "self", ".", "nlm_weights", ".", "append", "(", "float", "(", "line", ".", "strip", "(", ")", ")", ")", "\n", "", "", "self", ".", "nlm_weights", "=", "th", ".", "tensor", "(", "self", ".", "nlm_weights", ")", ".", "to", "(", "'cuda'", ")", "\n", "# self.nlm_weights = np.array(self.nlm_weights)", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.load_nlm": [[123, 130], ["model_name.replace.replace.replace", "torch.hub.load", "fairseq_encoder.FairSeqEncoder.nlm_model.eval", "fairseq_encoder.FairSeqEncoder.nlm_model.cuda", "fairseq_encoder.FairSeqEncoder.nlm_model.task.source_dictionary.pad"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.load"], ["", "def", "load_nlm", "(", "self", ",", "model_name", ")", ":", "\n", "        ", "model_name", "=", "model_name", ".", "replace", "(", "'-'", ",", "'.'", ")", "\n", "self", ".", "nlm_model", "=", "th", ".", "hub", ".", "load", "(", "'pytorch/fairseq'", ",", "model_name", ")", "\n", "\n", "self", ".", "nlm_model", ".", "eval", "(", ")", "\n", "self", ".", "nlm_model", ".", "cuda", "(", ")", "\n", "self", ".", "pad_idx", "=", "self", ".", "nlm_model", ".", "task", ".", "source_dictionary", ".", "pad", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.add_padding_encodings": [[131, 134], ["torch.tensor", "torch.cat", "len"], "methods", ["None"], ["", "def", "add_padding_encodings", "(", "self", ",", "encodings", ",", "max_len", ")", ":", "\n", "        ", "padding", "=", "th", ".", "tensor", "(", "[", "self", ".", "pad_idx", "]", "*", "(", "max_len", "-", "len", "(", "encodings", ")", ")", ",", "dtype", "=", "th", ".", "long", ")", "\n", "return", "th", ".", "cat", "(", "(", "encodings", ",", "padding", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.align_toks": [[135, 137], ["fairseq_encoder.align_bpe_to_words"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.align_bpe_to_words"], ["", "def", "align_toks", "(", "self", ",", "bpe", ",", "toks", ")", ":", "\n", "        ", "return", "align_bpe_to_words", "(", "self", ".", "nlm_model", ",", "bpe", ",", "toks", ",", "model_name", "=", "self", ".", "config", "[", "'model_name_or_path'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.align_feats": [[138, 140], ["fairseq_encoder.align_features_to_words"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.align_features_to_words"], ["", "def", "align_feats", "(", "self", ",", "feats", ",", "alignment", ")", ":", "\n", "        ", "return", "align_features_to_words", "(", "self", ".", "nlm_model", ",", "feats", ",", "alignment", ",", "model_name", "=", "self", ".", "config", "[", "'model_name_or_path'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.get_encodings": [[141, 143], ["fairseq_encoder.FairSeqEncoder.nlm_model.encode"], "methods", ["None"], ["", "def", "get_encodings", "(", "self", ",", "toks", ")", ":", "\n", "        ", "return", "self", ".", "nlm_model", ".", "encode", "(", "' '", ".", "join", "(", "toks", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.is_valid": [[144, 150], ["fairseq_encoder.FairSeqEncoder.get_encodings", "len"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.get_encodings"], ["", "def", "is_valid", "(", "self", ",", "toks", ")", ":", "\n", "        ", "encodings", "=", "self", ".", "get_encodings", "(", "toks", ")", "\n", "if", "len", "(", "encodings", ")", ">", "self", ".", "config", "[", "'max_seq_len'", "]", ":", "\n", "            ", "return", "False", "\n", "", "else", ":", "\n", "            ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.get_num_subtokens": [[151, 153], ["len", "fairseq_encoder.FairSeqEncoder.get_encodings"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.get_encodings"], ["", "", "def", "get_num_subtokens", "(", "self", ",", "toks", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "get_encodings", "(", "toks", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.token_embeddings": [[154, 206], ["max", "torch.stack", "enumerate", "fairseq_encoder.FairSeqEncoder.get_encodings", "fairseq_encoder.FairSeqEncoder.align_toks", "torch.stack.append", "torch.no_grad", "fairseq_encoder.FairSeqEncoder.nlm_model.extract_features", "len", "torch.stack", "fairseq_encoder.FairSeqEncoder.align_feats", "zip", "batch_embeddings.append", "zip", "len", "fairseq_encoder.FairSeqEncoder.add_padding_encodings", "sel_hidden_states.sum.sum.sum", "emb.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "paired_embeddings.append", "sel_hidden_states.sum.sum.mean", "emb.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "sel_hidden_states.sum.sum.max", "sel_hidden_states.sum.sum.reshape", "torch.stack", "sel_hidden_states.sum.sum.sum", "emb.detach().cpu().numpy.detach().cpu().numpy.detach", "zip"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.get_encodings", "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.align_toks", "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.align_feats", "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.add_padding_encodings"], ["", "def", "token_embeddings", "(", "self", ",", "batch_toks", ",", "return_tokens", "=", "True", ")", ":", "\n", "\n", "        ", "batch_bpe", "=", "[", "self", ".", "get_encodings", "(", "toks", ")", "for", "toks", "in", "batch_toks", "]", "\n", "batch_aln", "=", "[", "self", ".", "align_toks", "(", "bpe", ",", "toks", ")", "for", "bpe", ",", "toks", "in", "zip", "(", "batch_bpe", ",", "batch_toks", ")", "]", "\n", "\n", "# prepare for model", "\n", "input_ids", "=", "[", "]", "\n", "batch_max_len", "=", "max", "(", "[", "len", "(", "e", ")", "for", "e", "in", "batch_bpe", "]", ")", "\n", "for", "enc", "in", "batch_bpe", ":", "\n", "            ", "input_ids", ".", "append", "(", "self", ".", "add_padding_encodings", "(", "enc", ",", "batch_max_len", ")", ")", "\n", "", "input_ids", "=", "th", ".", "stack", "(", "input_ids", ")", "\n", "\n", "# get features", "\n", "with", "th", ".", "no_grad", "(", ")", ":", "\n", "            ", "batch_hidden_states", "=", "self", ".", "nlm_model", ".", "extract_features", "(", "input_ids", ",", "return_all_hiddens", "=", "True", ")", "\n", "", "sel_hidden_states", "=", "[", "batch_hidden_states", "[", "i", "]", "for", "i", "in", "self", ".", "config", "[", "'layers'", "]", "]", "\n", "\n", "# combine layers", "\n", "if", "len", "(", "sel_hidden_states", ")", ">", "1", ":", "\n", "            ", "sel_hidden_states", "=", "th", ".", "stack", "(", "sel_hidden_states", ")", "\n", "\n", "if", "self", ".", "config", "[", "'layer_op'", "]", "==", "'sum'", ":", "\n", "                ", "sel_hidden_states", "=", "sel_hidden_states", ".", "sum", "(", "axis", "=", "0", ")", "\n", "", "elif", "self", ".", "config", "[", "'layer_op'", "]", "==", "'mean'", ":", "\n", "                ", "sel_hidden_states", "=", "sel_hidden_states", ".", "mean", "(", "axis", "=", "0", ")", "\n", "", "elif", "self", ".", "config", "[", "'layer_op'", "]", "==", "'max'", ":", "\n", "                ", "sel_hidden_states", "=", "sel_hidden_states", ".", "max", "(", "axis", "=", "0", ")", ".", "values", "\n", "", "elif", "self", ".", "config", "[", "'layer_op'", "]", "==", "'concat'", ":", "\n", "                ", "sel_hidden_states", "=", "sel_hidden_states", ".", "reshape", "(", "(", "sel_hidden_states", ".", "shape", "[", "1", "]", ",", "-", "1", ")", ")", "\n", "", "elif", "self", ".", "config", "[", "'layer_op'", "]", "==", "'ws'", ":", "\n", "                ", "sel_hidden_states", "=", "[", "w", "*", "m", "for", "w", ",", "m", "in", "zip", "(", "self", ".", "nlm_weights", ",", "sel_hidden_states", ")", "]", "\n", "sel_hidden_states", "=", "th", ".", "stack", "(", "sel_hidden_states", ")", "\n", "sel_hidden_states", "=", "sel_hidden_states", ".", "sum", "(", "axis", "=", "0", ")", "\n", "# sel_hidden_states = self.nlm_weights.dot(sel_hidden_states)", "\n", "", "", "else", ":", "\n", "            ", "sel_hidden_states", "=", "sel_hidden_states", "[", "0", "]", "\n", "\n", "# align layers", "\n", "", "batch_embeddings", "=", "[", "]", "\n", "for", "sent_idx", ",", "sent_embeddings", "in", "enumerate", "(", "sel_hidden_states", ")", ":", "\n", "            ", "sent_embeddings", "=", "self", ".", "align_feats", "(", "sent_embeddings", ",", "batch_aln", "[", "sent_idx", "]", ")", "\n", "sent_embeddings", "=", "sent_embeddings", "[", "1", ":", "-", "1", "]", "# ignoring special tokens", "\n", "sent_tokens", "=", "batch_toks", "[", "sent_idx", "]", "\n", "\n", "paired_embeddings", "=", "[", "]", "\n", "for", "tok", ",", "emb", "in", "zip", "(", "sent_tokens", ",", "sent_embeddings", ")", ":", "\n", "                ", "emb", "=", "emb", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "paired_embeddings", ".", "append", "(", "(", "tok", ",", "emb", ")", ")", "\n", "\n", "", "batch_embeddings", ".", "append", "(", "paired_embeddings", ")", "\n", "\n", "", "return", "batch_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.align_features_to_words": [[15, 42], ["collections.Counter", "features.new", "range", "torch.stack", "features.dim", "features.new.unsqueeze", "th.stack.append", "max", "len", "th.stack.append", "collections.Counter.get", "weighted_features[].sum", "range", "len"], "function", ["None"], ["def", "align_features_to_words", "(", "roberta", ",", "features", ",", "alignment", ",", "model_name", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Align given features to words.\n\n    Args:\n        roberta (RobertaHubInterface): RoBERTa instance\n        features (torch.Tensor): features to align of shape `(T_bpe x C)`\n        alignment: alignment between BPE tokens and words returned by\n            func:`align_bpe_to_words`.\n    \"\"\"", "\n", "assert", "features", ".", "dim", "(", ")", "==", "2", "\n", "\n", "bpe_counts", "=", "Counter", "(", "j", "for", "bpe_indices", "in", "alignment", "for", "j", "in", "bpe_indices", ")", "\n", "denom", "=", "features", ".", "new", "(", "[", "bpe_counts", ".", "get", "(", "j", ",", "1", ")", "for", "j", "in", "range", "(", "len", "(", "features", ")", ")", "]", ")", "\n", "weighted_features", "=", "features", "/", "denom", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "output", "=", "[", "weighted_features", "[", "0", "]", "]", "\n", "largest_j", "=", "-", "1", "\n", "for", "bpe_indices", "in", "alignment", ":", "\n", "        ", "output", ".", "append", "(", "weighted_features", "[", "bpe_indices", "]", ".", "sum", "(", "dim", "=", "0", ")", ")", "\n", "largest_j", "=", "max", "(", "largest_j", ",", "*", "bpe_indices", ")", "\n", "", "for", "j", "in", "range", "(", "largest_j", "+", "1", ",", "len", "(", "features", ")", ")", ":", "\n", "        ", "output", ".", "append", "(", "weighted_features", "[", "j", "]", ")", "\n", "", "output", "=", "th", ".", "stack", "(", "output", ")", "\n", "# assert th.all(th.abs(output.sum(dim=0) - features.sum(dim=0)) < 1e-4)", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.align_bpe_to_words": [[44, 100], ["filter", "next", "bpe_tokens.dim", "text.strip", "roberta.task.source_dictionary.string", "fairseq_encoder.align_bpe_to_words.clean"], "function", ["None"], ["", "def", "align_bpe_to_words", "(", "roberta", ",", "bpe_tokens", ":", "th", ".", "LongTensor", ",", "other_tokens", ":", "List", "[", "str", "]", ",", "model_name", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Helper to align GPT-2 BPE to other tokenization formats\n\n    Args:\n        roberta (RobertaHubInterface): RoBERTa instance\n        bpe_tokens (torch.LongTensor): GPT-2 BPE tokens of shape `(T_bpe)`\n        other_tokens (List[str]): other tokens of shape `(T_words)`\n\n    Returns:\n        List[str]: mapping from *other_tokens* to corresponding *bpe_tokens*.\n    \"\"\"", "\n", "assert", "bpe_tokens", ".", "dim", "(", ")", "==", "1", "\n", "assert", "bpe_tokens", "[", "0", "]", "==", "0", "\n", "\n", "def", "clean", "(", "text", ")", ":", "\n", "        ", "return", "text", ".", "strip", "(", ")", "\n", "\n", "# remove whitespaces to simplify alignment", "\n", "", "bpe_tokens", "=", "[", "roberta", ".", "task", ".", "source_dictionary", ".", "string", "(", "[", "x", "]", ")", "for", "x", "in", "bpe_tokens", "]", "\n", "bpe_tokens", "=", "[", "clean", "(", "roberta", ".", "bpe", ".", "decode", "(", "x", ")", "if", "x", "not", "in", "{", "'<s>'", ",", "''", "}", "else", "x", ")", "for", "x", "in", "bpe_tokens", "]", "\n", "\n", "# strip leading <s>", "\n", "bpe_tokens", "=", "bpe_tokens", "[", "1", ":", "]", "\n", "\n", "other_tokens", "=", "[", "clean", "(", "str", "(", "o", ")", ")", "for", "o", "in", "other_tokens", "]", "\n", "assert", "''", ".", "join", "(", "bpe_tokens", ")", "==", "''", ".", "join", "(", "other_tokens", ")", "\n", "\n", "# create alignment from every word to a list of BPE tokens", "\n", "alignment", "=", "[", "]", "\n", "bpe_toks", "=", "filter", "(", "lambda", "item", ":", "item", "[", "1", "]", "!=", "''", ",", "enumerate", "(", "bpe_tokens", ",", "start", "=", "1", ")", ")", "\n", "j", ",", "bpe_tok", "=", "next", "(", "bpe_toks", ")", "\n", "for", "other_tok", "in", "other_tokens", ":", "\n", "        ", "bpe_indices", "=", "[", "]", "\n", "while", "True", ":", "\n", "            ", "if", "other_tok", ".", "startswith", "(", "bpe_tok", ")", ":", "\n", "                ", "bpe_indices", ".", "append", "(", "j", ")", "\n", "other_tok", "=", "other_tok", "[", "len", "(", "bpe_tok", ")", ":", "]", "\n", "try", ":", "\n", "                    ", "j", ",", "bpe_tok", "=", "next", "(", "bpe_toks", ")", "\n", "", "except", "StopIteration", ":", "\n", "                    ", "j", ",", "bpe_tok", "=", "None", ",", "None", "\n", "", "", "elif", "bpe_tok", ".", "startswith", "(", "other_tok", ")", ":", "\n", "# other_tok spans multiple BPE tokens", "\n", "                ", "bpe_indices", ".", "append", "(", "j", ")", "\n", "bpe_tok", "=", "bpe_tok", "[", "len", "(", "other_tok", ")", ":", "]", "\n", "other_tok", "=", "''", "\n", "", "else", ":", "\n", "                ", "raise", "Exception", "(", "'Cannot align \"{}\" and \"{}\"'", ".", "format", "(", "other_tok", ",", "bpe_tok", ")", ")", "\n", "", "if", "other_tok", "==", "''", ":", "\n", "                ", "break", "\n", "", "", "assert", "len", "(", "bpe_indices", ")", ">", "0", "\n", "alignment", ".", "append", "(", "bpe_indices", ")", "\n", "", "assert", "len", "(", "alignment", ")", "==", "len", "(", "other_tokens", ")", "\n", "\n", "return", "alignment", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.__init__": [[18, 21], ["wn_utils.WN_Utils.load_sk2syn"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.load_sk2syn"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "map_sk2syn", "=", "{", "}", "\n", "self", ".", "load_sk2syn", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.load_sk2syn": [[22, 26], ["nltk.corpus.wordnet.all_synsets", "synset.lemmas", "lemma.key"], "methods", ["None"], ["", "def", "load_sk2syn", "(", "self", ")", ":", "\n", "        ", "for", "synset", "in", "wn", ".", "all_synsets", "(", ")", ":", "\n", "            ", "for", "lemma", "in", "synset", ".", "lemmas", "(", ")", ":", "\n", "                ", "self", ".", "map_sk2syn", "[", "lemma", ".", "key", "(", ")", "]", "=", "synset", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.syn2sks": [[27, 32], ["functools.lru_cache", "isinstance", "list", "nltk.corpus.wordnet.synset", "set", "lemma.key", "nltk.corpus.wordnet.synset.lemmas"], "methods", ["None"], ["", "", "", "@", "lru_cache", "(", ")", "\n", "def", "syn2sks", "(", "self", ",", "synset", ")", ":", "\n", "        ", "if", "isinstance", "(", "synset", ",", "str", ")", ":", "\n", "            ", "synset", "=", "wn", ".", "synset", "(", "synset", ")", "\n", "", "return", "list", "(", "set", "(", "[", "lemma", ".", "key", "(", ")", "for", "lemma", "in", "synset", ".", "lemmas", "(", ")", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.syn2pos": [[33, 38], ["functools.lru_cache", "isinstance", "nltk.corpus.wordnet.synset.pos", "nltk.corpus.wordnet.synset"], "methods", ["None"], ["", "@", "lru_cache", "(", ")", "\n", "def", "syn2pos", "(", "self", ",", "synset", ")", ":", "\n", "        ", "if", "isinstance", "(", "synset", ",", "str", ")", ":", "\n", "            ", "synset", "=", "wn", ".", "synset", "(", "synset", ")", "\n", "", "return", "synset", ".", "pos", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.syn2lemmas": [[39, 48], ["functools.lru_cache", "isinstance", "nltk.corpus.wordnet.synset.lemma_names", "nltk.corpus.wordnet.synset", "nltk.corpus.wordnet.synset.pos"], "methods", ["None"], ["", "@", "lru_cache", "(", ")", "\n", "def", "syn2lemmas", "(", "self", ",", "synset", ",", "include_pos", "=", "False", ")", ":", "\n", "        ", "if", "isinstance", "(", "synset", ",", "str", ")", ":", "\n", "            ", "synset", "=", "wn", ".", "synset", "(", "synset", ")", "\n", "\n", "", "lemmas", "=", "synset", ".", "lemma_names", "(", ")", "\n", "if", "include_pos", ":", "\n", "            ", "lemmas", "=", "[", "'%s|%s'", "%", "(", "lem", ",", "synset", ".", "pos", "(", ")", ")", "for", "lem", "in", "lemmas", "]", "\n", "", "return", "lemmas", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.syn2lexname": [[49, 55], ["functools.lru_cache", "isinstance", "nltk.corpus.wordnet.synset.lexname", "nltk.corpus.wordnet.synset"], "methods", ["None"], ["", "@", "lru_cache", "(", ")", "\n", "def", "syn2lexname", "(", "self", ",", "synset", ")", ":", "\n", "        ", "if", "isinstance", "(", "synset", ",", "str", ")", ":", "\n", "            ", "synset", "=", "wn", ".", "synset", "(", "synset", ")", "\n", "\n", "", "return", "synset", ".", "lexname", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.syn2offset": [[56, 59], ["functools.lru_cache", "synset.offset"], "methods", ["None"], ["", "@", "lru_cache", "(", ")", "\n", "def", "syn2offset", "(", "self", ",", "synset", ")", ":", "\n", "        ", "return", "synset", ".", "offset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.sk2syn": [[60, 63], ["functools.lru_cache"], "methods", ["None"], ["", "@", "lru_cache", "(", ")", "\n", "def", "sk2syn", "(", "self", ",", "sk", ")", ":", "\n", "        ", "return", "self", ".", "map_sk2syn", "[", "sk", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.sk2lemma": [[64, 74], ["functools.lru_cache", "nltk.corpus.wordnet.lemma_from_key().name", "lemma_name.replace.replace.replace", "nltk.corpus.wordnet.lemma_from_key", "sk.split"], "methods", ["None"], ["", "@", "lru_cache", "(", ")", "\n", "def", "sk2lemma", "(", "self", ",", "sk", ",", "use_ws", "=", "False", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "lemma_name", "=", "wn", ".", "lemma_from_key", "(", "sk", ")", ".", "name", "(", ")", "\n", "", "except", ":", "\n", "            ", "lemma_name", "=", "sk", ".", "split", "(", "'%'", ")", "[", "0", "]", "\n", "\n", "", "if", "use_ws", ":", "\n", "            ", "lemma_name", "=", "lemma_name", ".", "replace", "(", "'_'", ",", "' '", ")", "\n", "", "return", "lemma_name", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.sk2pos": [[75, 81], ["functools.lru_cache", "int", "[].split", "sk.split"], "methods", ["None"], ["", "@", "lru_cache", "(", ")", "\n", "def", "sk2pos", "(", "self", ",", "sk", ")", ":", "\n", "# merging ADJ with ADJ_SAT", "\n", "        ", "sk_types_map", "=", "{", "1", ":", "'n'", ",", "2", ":", "'v'", ",", "3", ":", "'a'", ",", "4", ":", "'r'", ",", "5", ":", "'a'", "}", "\n", "sk_type", "=", "int", "(", "sk", ".", "split", "(", "'%'", ")", "[", "1", "]", ".", "split", "(", "':'", ")", "[", "0", "]", ")", "\n", "return", "sk_types_map", "[", "sk_type", "]", "\n", "# syn = self.sk2syn(sk)", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.sk2lexname": [[84, 88], ["functools.lru_cache", "wn_utils.WN_Utils.sk2syn", "wn_utils.WN_Utils.syn2lexname"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.sk2syn", "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.syn2lexname"], ["", "@", "lru_cache", "(", ")", "\n", "def", "sk2lexname", "(", "self", ",", "sk", ")", ":", "\n", "        ", "syn", "=", "self", ".", "sk2syn", "(", "sk", ")", "\n", "return", "self", ".", "syn2lexname", "(", "syn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.lemma2syns": [[89, 107], ["functools.lru_cache", "lemma.replace.replace.replace", "lemma.replace.replace.split", "nltk.corpus.wordnet.synsets", "len", "wn_utils.NoSynset", "nltk.corpus.wordnet.synsets", "nltk.corpus.wordnet.synsets"], "methods", ["None"], ["", "@", "lru_cache", "(", ")", "\n", "def", "lemma2syns", "(", "self", ",", "lemma", ",", "pos", "=", "None", ")", ":", "\n", "\n", "        ", "if", "'|'", "in", "lemma", ":", "# custom format, overrides arg", "\n", "            ", "lemma", ",", "pos", "=", "lemma", ".", "split", "(", "'|'", ")", "\n", "\n", "", "lemma", "=", "lemma", ".", "replace", "(", "' '", ",", "'_'", ")", "\n", "\n", "# merging ADJ with ADJ_SAT", "\n", "if", "pos", "in", "[", "'a'", ",", "'s'", "]", ":", "\n", "            ", "syns", "=", "wn", ".", "synsets", "(", "lemma", ",", "pos", "=", "'a'", ")", "+", "wn", ".", "synsets", "(", "lemma", ",", "pos", "=", "'s'", ")", "\n", "", "else", ":", "\n", "            ", "syns", "=", "wn", ".", "synsets", "(", "lemma", ",", "pos", "=", "pos", ")", "\n", "\n", "", "if", "len", "(", "syns", ")", ">", "0", ":", "\n", "            ", "return", "syns", "\n", "", "else", ":", "\n", "            ", "raise", "NoSynset", "(", "'No synset for lemma=\\'%s\\', pos=\\'%s\\'.'", "%", "(", "lemma", ",", "pos", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.lemma2sks": [[108, 126], ["functools.lru_cache", "set", "lemma.replace.replace.replace", "wn_utils.WN_Utils.lemma2syns", "list", "lemma.replace.replace.split", "wn_utils.WN_Utils.syn2sks", "wn_utils.WN_Utils.sk2lemma", "set.add"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.lemma2syns", "home.repos.pwc.inspect_result.danlou_LMMS.scripts.convert_vecs_sks_to_syns.syn2sks", "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.sk2lemma"], ["", "", "@", "lru_cache", "(", ")", "\n", "def", "lemma2sks", "(", "self", ",", "lemma", ",", "pos", "=", "None", ")", ":", "\n", "        ", "sks", "=", "set", "(", ")", "\n", "\n", "if", "'|'", "in", "lemma", ":", "# custom format, overrides arg", "\n", "            ", "lemma", ",", "pos", "=", "lemma", ".", "split", "(", "'|'", ")", "\n", "", "lemma", "=", "lemma", ".", "replace", "(", "' '", ",", "'_'", ")", "\n", "\n", "# for sk in self.get_all_sks():", "\n", "#     if lemma == self.sk2lemma(sk) and pos == self.sk2pos(sk):", "\n", "#         sks.add(sk)", "\n", "\n", "for", "syn", "in", "self", ".", "lemma2syns", "(", "lemma", ",", "pos", "=", "pos", ")", ":", "\n", "            ", "for", "sk", "in", "self", ".", "syn2sks", "(", "syn", ")", ":", "\n", "                ", "if", "self", ".", "sk2lemma", "(", "sk", ",", "use_ws", "=", "False", ")", "==", "lemma", ":", "\n", "                    ", "sks", ".", "add", "(", "sk", ")", "\n", "\n", "", "", "", "return", "list", "(", "sks", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.lemma2lexnames": [[127, 133], ["functools.lru_cache", "set", "wn_utils.WN_Utils.lemma2syns", "list", "set.add", "wn_utils.WN_Utils.syn2lexname"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.lemma2syns", "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.syn2lexname"], ["", "@", "lru_cache", "(", ")", "\n", "def", "lemma2lexnames", "(", "self", ",", "lemma", ",", "pos", "=", "None", ")", ":", "\n", "        ", "lexnames", "=", "set", "(", ")", "\n", "for", "syn", "in", "self", ".", "lemma2syns", "(", "lemma", ",", "pos", "=", "pos", ")", ":", "\n", "            ", "lexnames", ".", "add", "(", "self", ".", "syn2lexname", "(", "syn", ")", ")", "\n", "", "return", "list", "(", "lexnames", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.synid2syn": [[134, 137], ["functools.lru_cache", "nltk.corpus.wordnet.of2ss"], "methods", ["None"], ["", "@", "lru_cache", "(", ")", "\n", "def", "synid2syn", "(", "self", ",", "synid", ")", ":", "\n", "        ", "return", "wn", ".", "of2ss", "(", "synid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.synname2syn": [[138, 141], ["functools.lru_cache", "nltk.corpus.wordnet.synset"], "methods", ["None"], ["", "@", "lru_cache", "(", ")", "\n", "def", "synname2syn", "(", "self", ",", "synname", ")", ":", "\n", "        ", "return", "wn", ".", "synset", "(", "synname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.get_all_syns": [[142, 144], ["list", "nltk.corpus.wordnet.all_synsets"], "methods", ["None"], ["", "def", "get_all_syns", "(", "self", ")", ":", "\n", "        ", "return", "list", "(", "wn", ".", "all_synsets", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.get_all_lemmas": [[145, 150], ["list", "nltk.corpus.wordnet.all_lemma_names", "lemma.replace"], "methods", ["None"], ["", "def", "get_all_lemmas", "(", "self", ",", "replace_ws", "=", "True", ")", ":", "\n", "        ", "all_wn_lemmas", "=", "list", "(", "wn", ".", "all_lemma_names", "(", ")", ")", "\n", "if", "replace_ws", ":", "\n", "            ", "all_wn_lemmas", "=", "[", "lemma", ".", "replace", "(", "'_'", ",", "' '", ")", "for", "lemma", "in", "all_wn_lemmas", "]", "\n", "", "return", "all_wn_lemmas", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.get_all_sks": [[151, 154], ["wn_utils.WN_Utils.map_sk2syn.keys"], "methods", ["None"], ["", "def", "get_all_sks", "(", "self", ")", ":", "\n", "# return list(self.map_sk2syn.keys())", "\n", "        ", "return", "self", ".", "map_sk2syn", ".", "keys", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.get_all_lexnames": [[155, 160], ["set", "wn_utils.WN_Utils.get_all_syns", "list", "set.add", "wn_utils.WN_Utils.syn2lexname"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.get_all_syns", "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.syn2lexname"], ["", "def", "get_all_lexnames", "(", "self", ")", ":", "\n", "        ", "lexnames", "=", "set", "(", ")", "\n", "for", "syn", "in", "self", ".", "get_all_syns", "(", ")", ":", "\n", "            ", "lexnames", ".", "add", "(", "self", ".", "syn2lexname", "(", "syn", ")", ")", "\n", "", "return", "list", "(", "lexnames", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.get_wn_first_sk": [[161, 167], ["first_syn.lemmas", "wn_utils.WN_Utils.lemma2syns", "lem.key", "lem.key.startswith"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.lemma2syns"], ["", "def", "get_wn_first_sk", "(", "self", ",", "lemma", ",", "postag", ")", ":", "\n", "        ", "first_syn", "=", "self", ".", "lemma2syns", "(", "lemma", ",", "postag", ")", "[", "0", "]", "\n", "for", "lem", "in", "first_syn", ".", "lemmas", "(", ")", ":", "\n", "            ", "key", "=", "lem", ".", "key", "(", ")", "\n", "if", "key", ".", "startswith", "(", "'{}%'", ".", "format", "(", "lemma", ")", ")", ":", "\n", "                ", "return", "key", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.get_syn_antonyms": [[168, 174], ["syn.lemmas", "list", "lemma.antonyms", "set", "syn_antonyms.append", "ant.synset"], "methods", ["None"], ["", "", "", "def", "get_syn_antonyms", "(", "self", ",", "syn", ")", ":", "\n", "        ", "syn_antonyms", "=", "[", "]", "\n", "for", "lemma", "in", "syn", ".", "lemmas", "(", ")", ":", "\n", "            ", "for", "ant", "in", "lemma", ".", "antonyms", "(", ")", ":", "\n", "                ", "syn_antonyms", ".", "append", "(", "ant", ".", "synset", "(", ")", ")", "\n", "", "", "return", "list", "(", "set", "(", "syn_antonyms", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.get_sk_antonyms": [[175, 181], ["wn_utils.WN_Utils.get_syn_antonyms", "list", "wn_utils.WN_Utils.sk2syn", "wn_utils.WN_Utils.syn2sks", "set"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.get_syn_antonyms", "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.sk2syn", "home.repos.pwc.inspect_result.danlou_LMMS.scripts.convert_vecs_sks_to_syns.syn2sks"], ["", "def", "get_sk_antonyms", "(", "self", ",", "sk", ")", ":", "\n", "        ", "syn_antonyms", "=", "self", ".", "get_syn_antonyms", "(", "self", ".", "sk2syn", "(", "sk", ")", ")", "\n", "sk_antonyms", "=", "[", "]", "\n", "for", "syn", "in", "syn_antonyms", ":", "\n", "            ", "sk_antonyms", "+=", "self", ".", "syn2sks", "(", "syn", ")", "\n", "", "return", "list", "(", "set", "(", "sk_antonyms", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.check_lemma_amb": [[182, 184], ["len", "wn_utils.WN_Utils.lemma2syns"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.lemma2syns"], ["", "def", "check_lemma_amb", "(", "self", ",", "lemma", ",", "postag", "=", "None", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "lemma2syns", "(", "lemma", ",", "postag", ")", ")", ">", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.check_sk_amb": [[185, 189], ["wn_utils.WN_Utils.sk2lemma", "wn_utils.WN_Utils.sk2pos", "wn_utils.WN_Utils.check_lemma_amb"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.sk2lemma", "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.sk2pos", "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.check_lemma_amb"], ["", "def", "check_sk_amb", "(", "self", ",", "sk", ")", ":", "\n", "        ", "sk_lemma", "=", "self", ".", "sk2lemma", "(", "sk", ")", "\n", "sk_postag", "=", "self", ".", "sk2pos", "(", "sk", ")", "\n", "return", "self", ".", "check_lemma_amb", "(", "sk_lemma", ",", "sk_postag", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.check_sk_1st_sense": [[190, 194], ["wn_utils.WN_Utils.sk2lemma", "wn_utils.WN_Utils.sk2pos", "wn_utils.WN_Utils.get_wn_first_sk"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.sk2lemma", "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.sk2pos", "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.get_wn_first_sk"], ["", "def", "check_sk_1st_sense", "(", "self", ",", "sk", ")", ":", "\n", "        ", "sk_lemma", "=", "self", ".", "sk2lemma", "(", "sk", ")", "\n", "sk_postag", "=", "self", ".", "sk2pos", "(", "sk", ")", "\n", "return", "sk", "==", "self", ".", "get_wn_first_sk", "(", "sk_lemma", ",", "sk_postag", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.get_all_hypernyms": [[195, 200], ["float", "list", "list", "syn.closure", "syn.closure", "s.hypernyms", "s.hypernyms"], "methods", ["None"], ["", "def", "get_all_hypernyms", "(", "self", ",", "syn", ",", "depth", "=", "float", "(", "'inf'", ")", ",", "include_self", "=", "True", ")", ":", "\n", "        ", "if", "include_self", ":", "\n", "            ", "return", "[", "syn", "]", "+", "list", "(", "syn", ".", "closure", "(", "lambda", "s", ":", "s", ".", "hypernyms", "(", ")", ",", "depth", "=", "depth", ")", ")", "\n", "", "else", ":", "\n", "            ", "return", "list", "(", "syn", ".", "closure", "(", "lambda", "s", ":", "s", ".", "hypernyms", "(", ")", ",", "depth", "=", "depth", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.get_all_ambiguous_sks": [[201, 207], ["wn_utils.WN_Utils.get_all_sks", "wn_utils.WN_Utils.check_sk_amb", "ambiguous_sks.append"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.get_all_sks", "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.check_sk_amb"], ["", "", "def", "get_all_ambiguous_sks", "(", "self", ")", ":", "\n", "        ", "ambiguous_sks", "=", "[", "]", "\n", "for", "sk", "in", "self", ".", "get_all_sks", "(", ")", ":", "\n", "            ", "if", "self", ".", "check_sk_amb", "(", "sk", ")", ":", "\n", "                ", "ambiguous_sks", ".", "append", "(", "sk", ")", "\n", "", "", "return", "ambiguous_sks", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.get_disambiguating_sks": [[208, 239], ["wn_utils.WN_Utils.lemma2sks", "collections.Counter", "sk_ancestors.items", "sk_ancestors.items", "wn_utils.WN_Utils.sk2syn", "set", "wn_utils.WN_Utils.get_all_hypernyms", "list", "collections.Counter.update", "wn_utils.WN_Utils.syn2sks", "set.update"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.lemma2sks", "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.sk2syn", "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.get_all_hypernyms", "home.repos.pwc.inspect_result.danlou_LMMS.scripts.convert_vecs_sks_to_syns.syn2sks"], ["", "def", "get_disambiguating_sks", "(", "self", ",", "lemma", ",", "pos", ")", ":", "\n", "\n", "        ", "lemma_sks", "=", "self", ".", "lemma2sks", "(", "lemma", ",", "pos", ")", "\n", "\n", "sk_ancestors", "=", "{", "}", "\n", "ancestor_counter", "=", "Counter", "(", ")", "\n", "for", "sk", "in", "lemma_sks", ":", "\n", "            ", "syn", "=", "self", ".", "sk2syn", "(", "sk", ")", "\n", "\n", "syn_hypernyms_sks", "=", "set", "(", ")", "\n", "for", "hypernym", "in", "self", ".", "get_all_hypernyms", "(", "syn", ")", ":", "\n", "                ", "hypernym_sks", "=", "self", ".", "syn2sks", "(", "hypernym", ")", "\n", "syn_hypernyms_sks", ".", "update", "(", "hypernym_sks", ")", "\n", "\n", "", "sk_ancestors", "[", "sk", "]", "=", "list", "(", "syn_hypernyms_sks", ")", "\n", "ancestor_counter", ".", "update", "(", "sk_ancestors", "[", "sk", "]", ")", "\n", "\n", "# keep only unique ancestors", "\n", "", "for", "sk", ",", "ancestors", "in", "sk_ancestors", ".", "items", "(", ")", ":", "\n", "            ", "sk_ancestors", "[", "sk", "]", "=", "[", "sk_", "for", "sk_", "in", "ancestors", "if", "ancestor_counter", "[", "sk_", "]", "==", "1", "]", "\n", "\n", "# invert ancestors to lemma sks", "\n", "", "disambiguating_sks", "=", "{", "}", "\n", "for", "sk", ",", "ancestors", "in", "sk_ancestors", ".", "items", "(", ")", ":", "\n", "            ", "for", "sk_", "in", "ancestors", ":", "\n", "                ", "disambiguating_sks", "[", "sk_", "]", "=", "sk", "\n", "\n", "", "", "for", "sk", "in", "lemma_sks", ":", "\n", "            ", "disambiguating_sks", "[", "sk", "]", "=", "sk", "\n", "\n", "", "return", "disambiguating_sks", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.convert_postag": [[240, 250], ["postags_map.values"], "methods", ["None"], ["", "def", "convert_postag", "(", "self", ",", "postag", ")", ":", "\n", "# merges ADJ with ADJ_SAT", "\n", "        ", "postags_map", "=", "{", "'NOUN'", ":", "'n'", ",", "'VERB'", ":", "'v'", ",", "'ADJ'", ":", "'a'", ",", "'ADV'", ":", "'r'", ",", "'ADJ_SAT'", ":", "'a'", "}", "\n", "if", "postag", "in", "postags_map", ".", "values", "(", ")", ":", "\n", "            ", "return", "postag", "\n", "", "elif", "postag", "in", "postags_map", ":", "\n", "            ", "return", "postags_map", "[", "postag", "]", "\n", "", "else", ":", "\n", "# raise exception", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.wup_similarity": [[251, 253], ["syn1.wup_similarity"], "methods", ["home.repos.pwc.inspect_result.danlou_LMMS.None.wn_utils.WN_Utils.wup_similarity"], ["", "", "def", "wup_similarity", "(", "self", ",", "syn1", ",", "syn2", ")", ":", "\n", "        ", "return", "syn1", ".", "wup_similarity", "(", "syn2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.extend_sensekeys.get_sensekey2synset_map": [[19, 25], ["nltk.corpus.wordnet.all_synsets", "synset.lemmas", "lemma.key"], "function", ["None"], ["def", "get_sensekey2synset_map", "(", ")", ":", "\n", "    ", "sensekey2synset_map", "=", "{", "}", "\n", "for", "synset", "in", "wn", ".", "all_synsets", "(", ")", ":", "\n", "        ", "for", "lemma", "in", "synset", ".", "lemmas", "(", ")", ":", "\n", "            ", "sensekey2synset_map", "[", "lemma", ".", "key", "(", ")", "]", "=", "synset", "\n", "", "", "return", "sensekey2synset_map", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.extend_sensekeys.wn_sensekey2synset": [[31, 33], ["None"], "function", ["None"], ["def", "wn_sensekey2synset", "(", "sensekey", ")", ":", "\n", "    ", "return", "sensekey2synset_map", "[", "sensekey", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.extend_sensekeys.wn_synset2sensekeys": [[35, 41], ["functools.lru_cache", "synset.lemmas", "sensekeys.append", "lemma.key"], "function", ["None"], ["", "@", "lru_cache", "(", ")", "\n", "def", "wn_synset2sensekeys", "(", "synset", ")", ":", "\n", "    ", "sensekeys", "=", "[", "]", "\n", "for", "lemma", "in", "synset", ".", "lemmas", "(", ")", ":", "\n", "        ", "sensekeys", ".", "append", "(", "lemma", ".", "key", "(", ")", ")", "\n", "", "return", "sensekeys", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.extend_sensekeys.get_synset_vec": [[43, 55], ["extend_sensekeys.wn_synset2sensekeys", "len", "numpy.mean", "sk_vecs.append", "senses_vsm.get_vec", "sk_vecs.append"], "function", ["home.repos.pwc.inspect_result.danlou_LMMS.scripts.extend_sensekeys.wn_synset2sensekeys", "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.get_vec"], ["", "def", "get_synset_vec", "(", "senses_vsm", ",", "synset", ",", "additional_vecs", "=", "{", "}", ")", ":", "\n", "    ", "sk_vecs", "=", "[", "]", "\n", "for", "synset_sensekey", "in", "wn_synset2sensekeys", "(", "synset", ")", ":", "\n", "        ", "if", "synset_sensekey", "in", "senses_vsm", ".", "labels_set", ":", "\n", "            ", "sk_vecs", ".", "append", "(", "senses_vsm", ".", "get_vec", "(", "synset_sensekey", ")", ")", "\n", "", "elif", "synset_sensekey", "in", "additional_vecs", ":", "\n", "            ", "sk_vecs", ".", "append", "(", "additional_vecs", "[", "synset_sensekey", "]", ")", "\n", "\n", "", "", "if", "len", "(", "sk_vecs", ")", ">", "0", ":", "\n", "        ", "return", "np", ".", "mean", "(", "sk_vecs", ",", "axis", "=", "0", ")", "\n", "", "else", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.extend_sensekeys.wn_all_lexnames": [[57, 62], ["set", "nltk.corpus.wordnet.all_synsets", "set.add", "s.lexname"], "function", ["None"], ["", "", "def", "wn_all_lexnames", "(", ")", ":", "\n", "    ", "all_lexs", "=", "set", "(", ")", "\n", "for", "s", "in", "wn", ".", "all_synsets", "(", ")", ":", "\n", "        ", "all_lexs", ".", "add", "(", "s", ".", "lexname", "(", ")", ")", "\n", "", "return", "all_lexs", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.extend_sensekeys.wn_all_lexnames_groups": [[64, 69], ["collections.defaultdict", "nltk.corpus.wordnet.all_synsets", "dict", "groups[].append", "synset.lexname"], "function", ["None"], ["", "def", "wn_all_lexnames_groups", "(", ")", ":", "\n", "    ", "groups", "=", "defaultdict", "(", "list", ")", "\n", "for", "synset", "in", "wn", ".", "all_synsets", "(", ")", ":", "\n", "        ", "groups", "[", "synset", ".", "lexname", "(", ")", "]", ".", "append", "(", "synset", ")", "\n", "", "return", "dict", "(", "groups", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.merge_avg.normalize": [[12, 14], ["numpy.linalg.norm"], "function", ["None"], ["def", "normalize", "(", "vec", ")", ":", "\n", "    ", "return", "vec", "/", "np", ".", "linalg", ".", "norm", "(", "vec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.merge_avg.avg": [[16, 73], ["logging.info", "logging.info", "logging.info", "orderedset.OrderedSet", "logging.info", "logging.info", "open", "open", "logging.info", "txt1_vecs.keys", "open", "line.split", "numpy.array", "line.split", "numpy.array", "open", "txt3_vecs.get", "merged_f.write", "merge_avg.normalize", "merge_avg.normalize", "line.split", "numpy.array", "str", "float", "float", "merge_avg.normalize", "round", "float"], "function", ["home.repos.pwc.inspect_result.danlou_LMMS.scripts.merge_cat.normalize", "home.repos.pwc.inspect_result.danlou_LMMS.scripts.merge_cat.normalize", "home.repos.pwc.inspect_result.danlou_LMMS.scripts.merge_cat.normalize"], ["", "def", "avg", "(", "v1_path", ",", "v2_path", ",", "v3_path", ",", "out_path", ",", "norm", "=", "True", ")", ":", "\n", "\n", "    ", "logging", ".", "info", "(", "'Loading %s ...'", "%", "v1_path", ")", "# e.g., fastText tokens", "\n", "txt1_vecs", "=", "{", "}", "\n", "with", "open", "(", "v1_path", ")", "as", "txt1_f", ":", "\n", "        ", "for", "line", "in", "txt1_f", ":", "\n", "            ", "info", "=", "line", ".", "split", "(", ")", "\n", "label", ",", "vec_str", "=", "info", "[", "0", "]", ",", "info", "[", "1", ":", "]", "\n", "vec", "=", "np", ".", "array", "(", "[", "float", "(", "v", ")", "for", "v", "in", "vec_str", "]", ")", "\n", "if", "norm", ":", "\n", "                ", "vec", "=", "normalize", "(", "vec", ")", "\n", "", "txt1_vecs", "[", "label", "]", "=", "vec", "\n", "\n", "", "", "logging", ".", "info", "(", "'Loading %s ...'", "%", "v2_path", ")", "# e.g., BERT sentences", "\n", "txt2_vecs", "=", "{", "}", "\n", "with", "open", "(", "v2_path", ")", "as", "txt2_f", ":", "\n", "        ", "for", "line", "in", "txt2_f", ":", "\n", "            ", "info", "=", "line", ".", "split", "(", ")", "\n", "label", ",", "vec_str", "=", "info", "[", "0", "]", ",", "info", "[", "1", ":", "]", "\n", "vec", "=", "np", ".", "array", "(", "[", "float", "(", "v", ")", "for", "v", "in", "vec_str", "]", ")", "\n", "if", "norm", ":", "\n", "                ", "vec", "=", "normalize", "(", "vec", ")", "\n", "", "txt2_vecs", "[", "label", "]", "=", "vec", "\n", "\n", "", "", "if", "v3_path", "is", "not", "None", ":", "\n", "        ", "logging", ".", "info", "(", "'Loading %s ...'", "%", "v3_path", ")", "# e.g., BERT tokens", "\n", "txt3_vecs", "=", "{", "}", "\n", "with", "open", "(", "v3_path", ")", "as", "txt3_f", ":", "\n", "            ", "for", "line", "in", "txt3_f", ":", "\n", "                ", "info", "=", "line", ".", "split", "(", ")", "\n", "label", ",", "vec_str", "=", "info", "[", "0", "]", ",", "info", "[", "1", ":", "]", "\n", "vec", "=", "np", ".", "array", "(", "[", "float", "(", "v", ")", "for", "v", "in", "vec_str", "]", ")", "\n", "if", "norm", ":", "\n", "                    ", "vec", "=", "normalize", "(", "vec", ")", "\n", "", "txt3_vecs", "[", "label", "]", "=", "vec", "\n", "\n", "", "", "", "logging", ".", "info", "(", "'Combining vecs (avg) ...'", ")", "\n", "txt1_labels", "=", "OrderedSet", "(", "txt1_vecs", ".", "keys", "(", ")", ")", "# first sets the order", "\n", "for", "label1", "in", "txt1_labels", ":", "\n", "        ", "v1", "=", "txt1_vecs", "[", "label1", "]", "\n", "v2", "=", "txt2_vecs", "[", "label1", "]", "\n", "\n", "if", "v3_path", "is", "not", "None", ":", "\n", "            ", "v3", "=", "txt3_vecs", ".", "get", "(", "label1", ",", "v2", ")", "# takes from txt2 if missing", "\n", "txt1_vecs", "[", "label1", "]", "=", "(", "v1", "+", "v2", "+", "v3", ")", "/", "2", "\n", "\n", "", "else", ":", "\n", "            ", "txt1_vecs", "[", "label1", "]", "=", "(", "v1", "+", "v2", ")", "/", "2", "\n", "\n", "", "", "logging", ".", "info", "(", "'Writing %s ...'", "%", "out_path", ")", "\n", "with", "open", "(", "out_path", ",", "'w'", ")", "as", "merged_f", ":", "\n", "        ", "for", "label", "in", "txt1_labels", ":", "\n", "            ", "vec", "=", "txt1_vecs", "[", "label", "]", "\n", "vec_str", "=", "[", "str", "(", "round", "(", "v", ",", "6", ")", ")", "for", "v", "in", "vec", "]", "\n", "merged_f", ".", "write", "(", "'%s %s\\n'", "%", "(", "label", ",", "' '", ".", "join", "(", "vec_str", ")", ")", ")", "\n", "\n", "", "", "logging", ".", "info", "(", "'Done'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.embed_glosses.chunks": [[22, 26], ["range", "len"], "function", ["None"], ["def", "chunks", "(", "l", ",", "n", ")", ":", "\n", "    ", "\"\"\"Yield successive n-sized chunks from l.\"\"\"", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "l", ")", ",", "n", ")", ":", "\n", "        ", "yield", "l", "[", "i", ":", "i", "+", "n", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.embed_glosses.wn_synset2keys": [[28, 32], ["isinstance", "list", "nltk.corpus.wordnet.synset", "set", "lemma.key", "wn.synset.lemmas"], "function", ["None"], ["", "", "def", "wn_synset2keys", "(", "synset", ")", ":", "\n", "    ", "if", "isinstance", "(", "synset", ",", "str", ")", ":", "\n", "        ", "synset", "=", "wn", ".", "synset", "(", "synset", ")", "\n", "", "return", "list", "(", "set", "(", "[", "lemma", ".", "key", "(", ")", "for", "lemma", "in", "synset", ".", "lemmas", "(", ")", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.embed_glosses.fix_lemma": [[34, 36], ["lemma.replace"], "function", ["None"], ["", "def", "fix_lemma", "(", "lemma", ")", ":", "\n", "    ", "return", "lemma", ".", "replace", "(", "'_'", ",", "' '", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.embed_glosses.get_sense_data": [[38, 51], ["nltk.corpus.wordnet.all_synsets", "sorted", "synset.lemmas", "embed_glosses.fix_lemma", "nltk.word_tokenize", "embed_glosses.fix_lemma", "sorted.append", "lemma.name", "synset.lemmas", "synset.definition", "lemma.name", "lemma.key"], "function", ["home.repos.pwc.inspect_result.danlou_LMMS.scripts.embed_glosses.fix_lemma", "home.repos.pwc.inspect_result.danlou_LMMS.scripts.embed_glosses.fix_lemma"], ["", "def", "get_sense_data", "(", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "\n", "for", "synset", "in", "wn", ".", "all_synsets", "(", ")", ":", "\n", "        ", "all_lemmas", "=", "[", "fix_lemma", "(", "lemma", ".", "name", "(", ")", ")", "for", "lemma", "in", "synset", ".", "lemmas", "(", ")", "]", "\n", "gloss", "=", "' '", ".", "join", "(", "word_tokenize", "(", "synset", ".", "definition", "(", ")", ")", ")", "\n", "for", "lemma", "in", "synset", ".", "lemmas", "(", ")", ":", "\n", "            ", "lemma_name", "=", "fix_lemma", "(", "lemma", ".", "name", "(", ")", ")", "\n", "d_str", "=", "lemma_name", "+", "' - '", "+", "' , '", ".", "join", "(", "all_lemmas", ")", "+", "' - '", "+", "gloss", "\n", "data", ".", "append", "(", "(", "synset", ",", "lemma", ".", "key", "(", ")", ",", "d_str", ")", ")", "\n", "\n", "", "", "data", "=", "sorted", "(", "data", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.merge_cat.normalize": [[12, 14], ["numpy.linalg.norm"], "function", ["None"], ["def", "normalize", "(", "vec", ")", ":", "\n", "    ", "return", "vec", "/", "np", ".", "linalg", ".", "norm", "(", "vec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.merge_cat.concat": [[16, 78], ["logging.info", "logging.info", "logging.info", "orderedset.OrderedSet", "logging.info", "logging.info", "open", "open", "logging.info", "txt1_vecs.keys", "open", "line.split", "numpy.array", "line.split", "numpy.array", "open", "txt3_vecs.get", "numpy.hstack", "numpy.hstack", "merged_f.write", "merge_cat.normalize", "merge_cat.normalize", "line.split", "numpy.array", "str", "float", "float", "merge_cat.normalize", "round", "float"], "function", ["home.repos.pwc.inspect_result.danlou_LMMS.scripts.merge_cat.normalize", "home.repos.pwc.inspect_result.danlou_LMMS.scripts.merge_cat.normalize", "home.repos.pwc.inspect_result.danlou_LMMS.scripts.merge_cat.normalize"], ["", "def", "concat", "(", "v1_path", ",", "v2_path", ",", "v3_path", ",", "out_path", ",", "norm", "=", "True", ")", ":", "\n", "\n", "    ", "logging", ".", "info", "(", "'Loading %s ...'", "%", "v1_path", ")", "# i.e. fastText tokens", "\n", "txt1_vecs", "=", "{", "}", "\n", "with", "open", "(", "v1_path", ")", "as", "txt1_f", ":", "\n", "        ", "for", "line", "in", "txt1_f", ":", "\n", "            ", "info", "=", "line", ".", "split", "(", ")", "\n", "label", ",", "vec_str", "=", "info", "[", "0", "]", ",", "info", "[", "1", ":", "]", "\n", "# vec = [float(v) for v in vec_str]", "\n", "vec", "=", "np", ".", "array", "(", "[", "float", "(", "v", ")", "for", "v", "in", "vec_str", "]", ")", "\n", "if", "norm", ":", "\n", "                ", "vec", "=", "normalize", "(", "vec", ")", "\n", "", "txt1_vecs", "[", "label", "]", "=", "vec", "\n", "\n", "", "", "logging", ".", "info", "(", "'Loading %s ...'", "%", "v2_path", ")", "# i.e. BERT sentences", "\n", "txt2_vecs", "=", "{", "}", "\n", "with", "open", "(", "v2_path", ")", "as", "txt2_f", ":", "\n", "        ", "for", "line", "in", "txt2_f", ":", "\n", "            ", "info", "=", "line", ".", "split", "(", ")", "\n", "label", ",", "vec_str", "=", "info", "[", "0", "]", ",", "info", "[", "1", ":", "]", "\n", "# vec = [float(v) for v in vec_str]", "\n", "vec", "=", "np", ".", "array", "(", "[", "float", "(", "v", ")", "for", "v", "in", "vec_str", "]", ")", "\n", "if", "norm", ":", "\n", "                ", "vec", "=", "normalize", "(", "vec", ")", "\n", "", "txt2_vecs", "[", "label", "]", "=", "vec", "\n", "\n", "", "", "if", "v3_path", "is", "not", "None", ":", "\n", "        ", "logging", ".", "info", "(", "'Loading %s ...'", "%", "v3_path", ")", "# i.e. BERT tokens", "\n", "txt3_vecs", "=", "{", "}", "\n", "with", "open", "(", "v3_path", ")", "as", "txt3_f", ":", "\n", "            ", "for", "line", "in", "txt3_f", ":", "\n", "                ", "info", "=", "line", ".", "split", "(", ")", "\n", "label", ",", "vec_str", "=", "info", "[", "0", "]", ",", "info", "[", "1", ":", "]", "\n", "# vec = [float(v) for v in vec_str]", "\n", "vec", "=", "np", ".", "array", "(", "[", "float", "(", "v", ")", "for", "v", "in", "vec_str", "]", ")", "\n", "if", "norm", ":", "\n", "                    ", "vec", "=", "normalize", "(", "vec", ")", "\n", "", "txt3_vecs", "[", "label", "]", "=", "vec", "\n", "\n", "", "", "", "logging", ".", "info", "(", "'Combining vecs (concat) ...'", ")", "\n", "txt1_labels", "=", "OrderedSet", "(", "txt1_vecs", ".", "keys", "(", ")", ")", "# first sets the order", "\n", "for", "label1", "in", "txt1_labels", ":", "\n", "        ", "v1", "=", "txt1_vecs", "[", "label1", "]", "\n", "v2", "=", "txt2_vecs", "[", "label1", "]", "\n", "\n", "if", "v3_path", "is", "not", "None", ":", "\n", "            ", "v3", "=", "txt3_vecs", ".", "get", "(", "label1", ",", "v2", ")", "# takes from txt2 if missing", "\n", "# txt1_vecs[label1] = v1 + v2 + v3  # concatenation, not sum", "\n", "txt1_vecs", "[", "label1", "]", "=", "np", ".", "hstack", "(", "(", "v1", ",", "v2", ",", "v3", ")", ")", "# concatenation, not sum", "\n", "\n", "", "else", ":", "\n", "# txt1_vecs[label1] = v1 + v2  # concatenation, not sum", "\n", "            ", "txt1_vecs", "[", "label1", "]", "=", "np", ".", "hstack", "(", "(", "v1", ",", "v2", ")", ")", "# concatenation, not sum", "\n", "\n", "", "", "logging", ".", "info", "(", "'Writing %s ...'", "%", "out_path", ")", "\n", "with", "open", "(", "out_path", ",", "'w'", ")", "as", "merged_f", ":", "\n", "        ", "for", "label", "in", "txt1_labels", ":", "\n", "            ", "vec", "=", "txt1_vecs", "[", "label", "]", "\n", "vec_str", "=", "[", "str", "(", "round", "(", "v", ",", "6", ")", ")", "for", "v", "in", "vec", "]", "\n", "merged_f", ".", "write", "(", "'%s %s\\n'", "%", "(", "label", ",", "' '", ".", "join", "(", "vec_str", ")", ")", ")", "\n", "\n", "", "", "logging", ".", "info", "(", "'Done'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.extend_synsets.wn_all_lexnames": [[21, 26], ["set", "nltk.corpus.wordnet.all_synsets", "set.add", "s.lexname"], "function", ["None"], ["def", "wn_all_lexnames", "(", ")", ":", "\n", "    ", "all_lexs", "=", "set", "(", ")", "\n", "for", "s", "in", "wn", ".", "all_synsets", "(", ")", ":", "\n", "        ", "all_lexs", ".", "add", "(", "s", ".", "lexname", "(", ")", ")", "\n", "", "return", "all_lexs", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.extend_synsets.wn_all_lexnames_groups": [[28, 33], ["collections.defaultdict", "nltk.corpus.wordnet.all_synsets", "dict", "groups[].append", "synset.lexname"], "function", ["None"], ["", "def", "wn_all_lexnames_groups", "(", ")", ":", "\n", "    ", "groups", "=", "defaultdict", "(", "list", ")", "\n", "for", "synset", "in", "wn", ".", "all_synsets", "(", ")", ":", "\n", "        ", "groups", "[", "synset", ".", "lexname", "(", ")", "]", ".", "append", "(", "synset", ")", "\n", "", "return", "dict", "(", "groups", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.embed_annotations.chunks": [[21, 25], ["range", "len"], "function", ["None"], ["def", "chunks", "(", "l", ",", "n", ")", ":", "\n", "    ", "\"\"\"Yield successive n-sized chunks from given list.\"\"\"", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "l", ")", ",", "n", ")", ":", "\n", "        ", "yield", "l", "[", "i", ":", "i", "+", "n", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.embed_annotations.get_sense_mapping": [[27, 35], ["open", "line.split", "line.split"], "function", ["None"], ["", "", "def", "get_sense_mapping", "(", "eval_path", ")", ":", "\n", "    ", "sensekey_mapping", "=", "{", "}", "\n", "with", "open", "(", "eval_path", ")", "as", "keys_f", ":", "\n", "        ", "for", "line", "in", "keys_f", ":", "\n", "            ", "id_", "=", "line", ".", "split", "(", ")", "[", "0", "]", "\n", "keys", "=", "line", ".", "split", "(", ")", "[", "1", ":", "]", "\n", "sensekey_mapping", "[", "id_", "]", "=", "keys", "\n", "", "", "return", "sensekey_mapping", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.embed_annotations.read_xml_sents": [[37, 52], ["open", "enumerate", "line.strip.strip", "line.strip.startswith", "line.strip.startswith", "line.strip.startswith", "sent_elems.append", "line.strip.startswith", "sent_elems.append", "lxml.etree.fromstring", "logging.fatal", "input"], "function", ["None"], ["", "def", "read_xml_sents", "(", "xml_path", ")", ":", "\n", "    ", "with", "open", "(", "xml_path", ")", "as", "f", ":", "\n", "        ", "for", "line_idx", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", ".", "startswith", "(", "'<sentence '", ")", "or", "line", "==", "'<sentence>'", ":", "\n", "                ", "sent_elems", "=", "[", "line", "]", "\n", "", "elif", "line", ".", "startswith", "(", "'<wf '", ")", "or", "line", ".", "startswith", "(", "'<instance '", ")", ":", "\n", "                ", "sent_elems", ".", "append", "(", "line", ")", "\n", "", "elif", "line", ".", "startswith", "(", "'</sentence>'", ")", ":", "\n", "                ", "sent_elems", ".", "append", "(", "line", ")", "\n", "try", ":", "\n", "                    ", "yield", "lxml", ".", "etree", ".", "fromstring", "(", "''", ".", "join", "(", "sent_elems", ")", ")", "\n", "", "except", "lxml", ".", "etree", ".", "XMLSyntaxError", ":", "\n", "                    ", "logging", ".", "fatal", "(", "'XML Parsing Error: %d'", "%", "line_idx", ")", "\n", "input", "(", "'...'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.embed_annotations.process_et_sent": [[54, 84], ["sent_et.getchildren", "ch.items", "len", "idx_map_abs.append", "entry[].append", "entry[].append", "ch.attrib.keys", "entry[].append", "entry[].append", "list", "enumerate", "entry[].append", "len", "range", "len", "t.split"], "function", ["None"], ["", "", "", "", "", "def", "process_et_sent", "(", "sent_et", ",", "sense_mapping", ")", ":", "\n", "\n", "    ", "entry", "=", "{", "f", ":", "[", "]", "for", "f", "in", "[", "'token_mw'", ",", "'senses'", "]", "}", "\n", "for", "ch", "in", "sent_et", ".", "getchildren", "(", ")", ":", "\n", "        ", "for", "k", ",", "v", "in", "ch", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", "in", "{", "'token_mw'", ",", "'senses'", "}", ":", "\n", "                ", "entry", "[", "k", "]", ".", "append", "(", "v", ")", "\n", "\n", "", "", "if", "(", "ch", ".", "text", "is", "not", "None", ")", "and", "len", "(", "ch", ".", "text", ")", "<", "32", ":", "\n", "            ", "entry", "[", "'token_mw'", "]", ".", "append", "(", "ch", ".", "text", ")", "\n", "", "else", ":", "\n", "            ", "entry", "[", "'token_mw'", "]", ".", "append", "(", "'UNK'", ")", "\n", "\n", "", "if", "'id'", "in", "ch", ".", "attrib", ".", "keys", "(", ")", ":", "\n", "            ", "entry", "[", "'senses'", "]", ".", "append", "(", "sense_mapping", "[", "ch", ".", "attrib", "[", "'id'", "]", "]", ")", "\n", "", "else", ":", "\n", "            ", "entry", "[", "'senses'", "]", ".", "append", "(", "None", ")", "\n", "\n", "# handling multi-word expressions, mapping allows matching tokens with mw features", "\n", "", "", "idx_map_abs", "=", "[", "]", "\n", "idx_map_rel", "=", "[", "(", "i", ",", "list", "(", "range", "(", "len", "(", "t", ".", "split", "(", ")", ")", ")", ")", ")", "for", "i", ",", "t", "in", "enumerate", "(", "entry", "[", "'token_mw'", "]", ")", "]", "\n", "token_counter", "=", "0", "\n", "for", "idx_group", ",", "idx_tokens", "in", "idx_map_rel", ":", "# converting relative token positions to absolute", "\n", "        ", "idx_tokens", "=", "[", "i", "+", "token_counter", "for", "i", "in", "idx_tokens", "]", "\n", "token_counter", "+=", "len", "(", "idx_tokens", ")", "\n", "idx_map_abs", ".", "append", "(", "[", "idx_group", ",", "idx_tokens", "]", ")", "\n", "", "entry", "[", "'idx_map'", "]", "=", "idx_map_abs", "\n", "entry", "[", "'n_toks'", "]", "=", "token_counter", "\n", "\n", "return", "entry", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.embed_annotations.gen_vecs": [[86, 149], ["embed_annotations.get_sense_mapping", "logging.info", "enumerate", "sorted", "logging.info", "embed_annotations.chunks", "logging.info", "logging.info", "logging.info", "logging.info", "embed_annotations.read_xml_sents", "embed_annotations.process_et_sent", "sorted.append", "time.time", "encoder.token_embeddings", "zip", "open", "sense_vecs.items", "sum", "logging.info", "len", "vecs_f.write", "numpy.array().mean", "time.time", "t.split", "len", "str", "numpy.array", "len", "len", "round", "np.array().mean.tolist", "len"], "function", ["home.repos.pwc.inspect_result.danlou_LMMS.scripts.embed_annotations.get_sense_mapping", "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wsd.chunks", "home.repos.pwc.inspect_result.danlou_LMMS.scripts.embed_annotations.read_xml_sents", "home.repos.pwc.inspect_result.danlou_LMMS.scripts.embed_annotations.process_et_sent", "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.token_embeddings"], ["", "def", "gen_vecs", "(", "args", ",", "encoder", ",", "train_path", ",", "eval_path", ")", ":", "\n", "\n", "    ", "sense_vecs", "=", "{", "}", "\n", "sense_mapping", "=", "get_sense_mapping", "(", "eval_path", ")", "\n", "\n", "logging", ".", "info", "(", "'Preparing docs ...'", ")", "\n", "docs", "=", "[", "]", "\n", "for", "sent_idx", ",", "sent_et", "in", "enumerate", "(", "read_xml_sents", "(", "train_path", ")", ")", ":", "\n", "        ", "entry", "=", "process_et_sent", "(", "sent_et", ",", "sense_mapping", ")", "\n", "docs", ".", "append", "(", "entry", ")", "\n", "\n", "# if sent_idx % 100000 == 0:", "\n", "#     logging.info('sent_idx: %d' % sent_idx)", "\n", "\n", "", "docs", "=", "sorted", "(", "docs", ",", "key", "=", "lambda", "x", ":", "x", "[", "'n_toks'", "]", ")", "\n", "\n", "logging", ".", "info", "(", "'Processing docs ...'", ")", "\n", "sent_idx", "=", "0", "\n", "n_failed", "=", "0", "\n", "for", "batch", "in", "chunks", "(", "docs", ",", "args", ".", "batch_size", ")", ":", "\n", "        ", "batch_t0", "=", "time", "(", ")", "\n", "\n", "batch_sent_toks_mw", "=", "[", "e", "[", "'token_mw'", "]", "for", "e", "in", "batch", "]", "\n", "batch_sent_toks", "=", "[", "sum", "(", "[", "t", ".", "split", "(", ")", "for", "t", "in", "toks_mw", "]", ",", "[", "]", ")", "for", "toks_mw", "in", "batch_sent_toks_mw", "]", "\n", "batch_embs", "=", "encoder", ".", "token_embeddings", "(", "batch_sent_toks", ")", "\n", "\n", "for", "sent_info", ",", "sent_embs", "in", "zip", "(", "batch", ",", "batch_embs", ")", ":", "\n", "            ", "sent_idx", "+=", "1", "\n", "\n", "for", "mw_idx", ",", "tok_idxs", "in", "sent_info", "[", "'idx_map'", "]", ":", "\n", "                ", "if", "sent_info", "[", "'senses'", "]", "[", "mw_idx", "]", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "vec", "=", "np", ".", "array", "(", "[", "sent_embs", "[", "i", "]", "[", "1", "]", "for", "i", "in", "tok_idxs", "]", ",", "dtype", "=", "np", ".", "float64", ")", ".", "mean", "(", "axis", "=", "0", ")", "\n", "\n", "for", "sk", "in", "sent_info", "[", "'senses'", "]", "[", "mw_idx", "]", ":", "\n", "\n", "                    ", "if", "args", ".", "sense_level", "==", "'sensekey'", ":", "\n", "                        ", "sense_id", "=", "sk", "\n", "", "elif", "args", ".", "sense_level", "==", "'synset'", ":", "\n", "                        ", "sense_id", "=", "map_sk2syn", "[", "sk", "]", "\n", "\n", "", "if", "sense_id", "not", "in", "sense_vecs", ":", "\n", "                        ", "sense_vecs", "[", "sense_id", "]", "=", "{", "'n'", ":", "1", ",", "'vec'", ":", "vec", "}", "\n", "\n", "", "elif", "len", "(", "sense_vecs", "[", "sense_id", "]", ")", "<", "args", ".", "max_instances", ":", "\n", "                        ", "sense_vecs", "[", "sense_id", "]", "[", "'n'", "]", "+=", "1", "\n", "sense_vecs", "[", "sense_id", "]", "[", "'vec'", "]", "+=", "vec", "\n", "\n", "", "", "", "batch_tspan", "=", "time", "(", ")", "-", "batch_t0", "\n", "progress", "=", "sent_idx", "/", "len", "(", "docs", ")", "*", "100", "\n", "logging", ".", "info", "(", "'PROGRESS: %.3f - %.3f sents/sec - %d/%d sents, %d sks'", "%", "(", "progress", ",", "args", ".", "batch_size", "/", "batch_tspan", ",", "sent_idx", ",", "len", "(", "docs", ")", ",", "len", "(", "sense_vecs", ")", ")", ")", "\n", "\n", "", "", "logging", ".", "info", "(", "'#sents final: %d'", "%", "sent_idx", ")", "\n", "logging", ".", "info", "(", "'#vecs: %d'", "%", "len", "(", "sense_vecs", ")", ")", "\n", "logging", ".", "info", "(", "'#failed batches: %d'", "%", "n_failed", ")", "\n", "\n", "logging", ".", "info", "(", "'Writing sense vecs to %s ...'", "%", "args", ".", "out_path", ")", "\n", "with", "open", "(", "args", ".", "out_path", ",", "'w'", ")", "as", "vecs_f", ":", "\n", "        ", "for", "sense_id", ",", "vec_info", "in", "sense_vecs", ".", "items", "(", ")", ":", "\n", "            ", "vec", "=", "vec_info", "[", "'vec'", "]", "/", "vec_info", "[", "'n'", "]", "\n", "vec_str", "=", "' '", ".", "join", "(", "[", "str", "(", "round", "(", "v", ",", "6", ")", ")", "for", "v", "in", "vec", ".", "tolist", "(", ")", "]", ")", "\n", "vecs_f", ".", "write", "(", "'%s %s\\n'", "%", "(", "sense_id", ",", "vec_str", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.scripts.convert_vecs_sks_to_syns.syn2sks": [[10, 12], ["list", "set", "lemma.key", "synset.lemmas"], "function", ["None"], ["def", "syn2sks", "(", "synset", ")", ":", "\n", "    ", "return", "list", "(", "set", "(", "[", "lemma", ".", "key", "(", ")", "for", "lemma", "in", "synset", ".", "lemmas", "(", ")", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_sid.get_correlation": [[16, 55], ["ranked_gold.sort", "ranked_pred.sort", "list", "scipy.stats.pearsonr", "open", "enumerate", "range", "gold_pairs.index", "print", "line.strip().split", "ranked_pred.append", "ranked_gold.append", "len", "print", "line.strip", "wns1.split", "wns2.split", "len", "len", "max", "float", "vsm.similarity", "line.strip"], "function", ["home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.similarity"], ["def", "get_correlation", "(", "vsm", ",", "sid_path", ")", ":", "\n", "\n", "    ", "n_skipped", "=", "0", "\n", "ranked_gold", ",", "ranked_pred", "=", "[", "]", ",", "[", "]", "\n", "with", "open", "(", "sid_path", ")", "as", "SID_f", ":", "\n", "        ", "for", "line_idx", ",", "line", "in", "enumerate", "(", "SID_f", ")", ":", "\n", "            ", "w1", ",", "w2", ",", "score", ",", "bns1", ",", "bns2", ",", "wns1", ",", "wns2", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "\n", "wns1", "=", "[", "wn1", "for", "wn1", "in", "wns1", ".", "split", "(", "';'", ")", "if", "wn1", "in", "vsm", ".", "labels_set", "]", "\n", "wns2", "=", "[", "wn2", "for", "wn2", "in", "wns2", ".", "split", "(", "';'", ")", "if", "wn2", "in", "vsm", ".", "labels_set", "]", "\n", "if", "len", "(", "wns1", ")", "==", "0", "or", "len", "(", "wns2", ")", "==", "0", ":", "\n", "                ", "print", "(", "'L:%d contains OOV - %s'", "%", "(", "line_idx", "+", "1", ",", "line", ".", "strip", "(", ")", ")", ")", "\n", "n_skipped", "+=", "1", "\n", "continue", "\n", "\n", "", "pair_sim", "=", "-", "1", "\n", "for", "wn1", "in", "wns1", ":", "\n", "                ", "for", "wn2", "in", "wns2", ":", "\n", "                    ", "pair_sim", "=", "max", "(", "pair_sim", ",", "vsm", ".", "similarity", "(", "wn1", ",", "wn2", ")", ")", "\n", "\n", "", "", "ranked_pred", ".", "append", "(", "(", "pair_sim", ",", "line_idx", ",", "w1", ",", "w2", ")", ")", "\n", "ranked_gold", ".", "append", "(", "(", "float", "(", "score", ")", ",", "line_idx", ",", "w1", ",", "w2", ")", ")", "\n", "# print(line_idx, wn1, wn2, pred_score)", "\n", "\n", "", "", "ranked_gold", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "ranked_pred", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "\n", "gold_pairs", "=", "[", "(", "w1", ",", "w2", ")", "for", "(", "score", ",", "idx", ",", "w1", ",", "w2", ")", "in", "ranked_gold", "]", "\n", "pred_pairs", "=", "[", "(", "w1", ",", "w2", ")", "for", "(", "score", ",", "idx", ",", "w1", ",", "w2", ")", "in", "ranked_pred", "]", "\n", "gold_ranks", "=", "list", "(", "range", "(", "len", "(", "gold_pairs", ")", ")", ")", "\n", "pred_ranks", "=", "[", "gold_pairs", ".", "index", "(", "pred_pair", ")", "for", "pred_pair", "in", "pred_pairs", "]", "\n", "# rho, pval = spearmanr(gold_ranks, pred_ranks)", "\n", "corr", ",", "pval", "=", "pearsonr", "(", "gold_ranks", ",", "pred_ranks", ")", "\n", "\n", "# print('n=%d, r=%f, pval=%f' % (len(gold_ranks), corr, pval))", "\n", "if", "n_skipped", ">", "0", ":", "\n", "        ", "print", "(", "'%d pairs skipped'", "%", "n_skipped", ")", "\n", "\n", "", "return", "corr", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_gwcs.unbolded": [[64, 67], ["str.replace", "str.replace"], "function", ["None"], ["", "def", "unbolded", "(", "context", ")", ":", "\n", "    ", "context", "=", "str", ".", "replace", "(", "context", ",", "'<strong>'", ",", "''", ")", "\n", "return", "str", ".", "replace", "(", "context", ",", "'</strong>'", ",", "''", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_gwcs.tokenize": [[69, 79], ["eval_gwcs.unbolded", "unbolded.split", "unidecode.unidecode", "en_nlp"], "function", ["home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_gwcs.unbolded"], ["", "def", "tokenize", "(", "context", ",", "remove_bold", "=", "True", ",", "lang", "=", "'en'", ")", ":", "\n", "\n", "    ", "if", "remove_bold", ":", "\n", "        ", "context", "=", "unbolded", "(", "context", ")", "\n", "\n", "", "context", "=", "' '", ".", "join", "(", "context", ".", "split", "(", ")", ")", "# normalize spacing", "\n", "tokens", "=", "[", "tok", ".", "text", "for", "tok", "in", "en_nlp", "(", "context", ")", "]", "\n", "tokens", "=", "[", "unidecode", "(", "tok", ")", "for", "tok", "in", "tokens", "]", "\n", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_gwcs.get_word_embedding": [[81, 91], ["tok.split"], "function", ["None"], ["", "def", "get_word_embedding", "(", "ctx_embeddings", ",", "tgt_word", ")", ":", "\n", "    ", "for", "(", "tok", ",", "emb", ")", "in", "ctx_embeddings", ":", "\n", "# if unidecode(tok) == unidecode(row['word1_context1'].lower()):", "\n", "        ", "if", "tok", "==", "tgt_word", ":", "\n", "            ", "return", "emb", "\n", "\n", "# try a hotfix on tokenization", "\n", "", "", "for", "(", "tok", ",", "emb", ")", "in", "ctx_embeddings", ":", "\n", "        ", "if", "tgt_word", "in", "tok", ".", "split", "(", "'-'", ")", ":", "\n", "            ", "return", "emb", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wic.wn_sensekey2synset": [[30, 39], ["functools.lru_cache", "nltk.corpus.wordnet.synsets", "sensekey.split", "synset.lemmas", "lemma.key"], "function", ["None"], ["@", "lru_cache", "(", ")", "\n", "def", "wn_sensekey2synset", "(", "sensekey", ")", ":", "\n", "    ", "\"\"\" Convert sensekey to synset. \"\"\"", "\n", "lemma", "=", "sensekey", ".", "split", "(", "'%'", ")", "[", "0", "]", "\n", "for", "synset", "in", "wn", ".", "synsets", "(", "lemma", ")", ":", "\n", "        ", "for", "lemma", "in", "synset", ".", "lemmas", "(", ")", ":", "\n", "            ", "if", "lemma", ".", "key", "(", ")", "==", "sensekey", ":", "\n", "                ", "return", "synset", "\n", "", "", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wic.wn_lemmatize": [[41, 48], ["functools.lru_cache", "w.lower.lower", "wn_lemmatizer.lemmatize", "wn_lemmatizer.lemmatize", "postag[].lower"], "function", ["None"], ["", "@", "lru_cache", "(", ")", "\n", "def", "wn_lemmatize", "(", "w", ",", "postag", "=", "None", ")", ":", "\n", "    ", "w", "=", "w", ".", "lower", "(", ")", "\n", "if", "postag", "is", "not", "None", ":", "\n", "        ", "return", "wn_lemmatizer", ".", "lemmatize", "(", "w", ",", "pos", "=", "postag", "[", "0", "]", ".", "lower", "(", ")", ")", "\n", "", "else", ":", "\n", "        ", "return", "wn_lemmatizer", ".", "lemmatize", "(", "w", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wic.load_wic": [[50, 73], ["open", "open", "line.strip().split", "list", "data_entries.append", "line.strip", "len", "len", "map", "gold_entries.append", "enumerate", "line.strip", "idxs.split", "gold_entries.append"], "function", ["None"], ["", "", "def", "load_wic", "(", "setname", "=", "'dev'", ",", "wic_path", "=", "'external/wic'", ")", ":", "\n", "    ", "data_entries", "=", "[", "]", "\n", "pos_map", "=", "{", "'N'", ":", "'NOUN'", ",", "'V'", ":", "'VERB'", "}", "\n", "data_path", "=", "'%s/%s/%s.data.txt'", "%", "(", "wic_path", ",", "setname", ",", "setname", ")", "\n", "for", "line", "in", "open", "(", "data_path", ")", ":", "\n", "        ", "word", ",", "pos", ",", "idxs", ",", "ex1", ",", "ex2", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "idx1", ",", "idx2", "=", "list", "(", "map", "(", "int", ",", "idxs", ".", "split", "(", "'-'", ")", ")", ")", "\n", "data_entries", ".", "append", "(", "[", "word", ",", "pos_map", "[", "pos", "]", ",", "idx1", ",", "idx2", ",", "ex1", ",", "ex2", "]", ")", "\n", "\n", "", "if", "setname", "==", "'test'", ":", "# no gold", "\n", "        ", "return", "[", "e", "+", "[", "None", "]", "for", "e", "in", "data_entries", "]", "\n", "\n", "", "gold_entries", "=", "[", "]", "\n", "gold_path", "=", "'%s/%s/%s.gold.txt'", "%", "(", "wic_path", ",", "setname", ",", "setname", ")", "\n", "for", "line", "in", "open", "(", "gold_path", ")", ":", "\n", "        ", "gold", "=", "line", ".", "strip", "(", ")", "\n", "if", "gold", "==", "'T'", ":", "\n", "            ", "gold_entries", ".", "append", "(", "True", ")", "\n", "", "elif", "gold", "==", "'F'", ":", "\n", "            ", "gold_entries", ".", "append", "(", "False", ")", "\n", "\n", "", "", "assert", "len", "(", "data_entries", ")", "==", "len", "(", "gold_entries", ")", "\n", "return", "[", "e", "+", "[", "gold_entries", "[", "i", "]", "]", "for", "i", ",", "e", "in", "enumerate", "(", "data_entries", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wsd_mfs.chunks": [[16, 20], ["range", "len"], "function", ["None"], ["def", "chunks", "(", "l", ",", "n", ")", ":", "\n", "    ", "\"\"\"Yield successive n-sized chunks from l.\"\"\"", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "l", ")", ",", "n", ")", ":", "\n", "        ", "yield", "l", "[", "i", ":", "i", "+", "n", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wsd_mfs.wn_sensekey2synset": [[22, 31], ["functools.lru_cache", "nltk.corpus.wordnet.synsets", "sensekey.split", "synset.lemmas", "lemma.key"], "function", ["None"], ["", "", "@", "lru_cache", "(", ")", "\n", "def", "wn_sensekey2synset", "(", "sensekey", ")", ":", "\n", "    ", "\"\"\"Convert sensekey to synset.\"\"\"", "\n", "lemma", "=", "sensekey", ".", "split", "(", "'%'", ")", "[", "0", "]", "\n", "for", "synset", "in", "wn", ".", "synsets", "(", "lemma", ")", ":", "\n", "        ", "for", "lemma", "in", "synset", ".", "lemmas", "(", ")", ":", "\n", "            ", "if", "lemma", ".", "key", "(", ")", "==", "sensekey", ":", "\n", "                ", "return", "synset", "\n", "", "", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wsd_mfs.wn_first_sense": [[33, 45], ["functools.lru_cache", "first_synset.lemmas", "nltk.corpus.wordnet.synsets", "lem.key", "lem.key.startswith"], "function", ["None"], ["", "@", "lru_cache", "(", ")", "\n", "def", "wn_first_sense", "(", "lemma", ",", "postag", "=", "None", ")", ":", "\n", "    ", "pos_map", "=", "{", "'VERB'", ":", "'v'", ",", "'NOUN'", ":", "'n'", ",", "'ADJ'", ":", "'a'", ",", "'ADV'", ":", "'r'", "}", "\n", "first_synset", "=", "wn", ".", "synsets", "(", "lemma", ",", "pos", "=", "pos_map", "[", "postag", "]", ")", "[", "0", "]", "\n", "found", "=", "False", "\n", "for", "lem", "in", "first_synset", ".", "lemmas", "(", ")", ":", "\n", "        ", "key", "=", "lem", ".", "key", "(", ")", "\n", "if", "key", ".", "startswith", "(", "'{}%'", ".", "format", "(", "lemma", ")", ")", ":", "\n", "            ", "found", "=", "True", "\n", "break", "\n", "", "", "assert", "found", "\n", "return", "key", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wsd_mfs.load_wsd_fw_set": [[47, 79], ["xml.parse", "ET.parse.getroot", "enumerate", "sum", "eval_entries.append", "sent_info[].append", "sent_info[].append", "sent_info[].append", "sent_info[].append", "len", "idx_map_abs.append", "e.get", "e.get", "e.get", "t.split", "list", "enumerate", "range", "len", "t.split"], "function", ["None"], ["", "def", "load_wsd_fw_set", "(", "eval_path", ")", ":", "\n", "    ", "\"\"\"Parse XML of split set and return list of instances (dict).\"\"\"", "\n", "eval_entries", "=", "[", "]", "\n", "tree", "=", "ET", ".", "parse", "(", "eval_path", ")", "\n", "for", "text", "in", "tree", ".", "getroot", "(", ")", ":", "\n", "        ", "for", "sent_idx", ",", "sentence", "in", "enumerate", "(", "text", ")", ":", "\n", "            ", "sent_info", "=", "{", "'tokens'", ":", "[", "]", ",", "'tokens_mw'", ":", "[", "]", ",", "'lemmas'", ":", "[", "]", ",", "'senses'", ":", "[", "]", ",", "'pos'", ":", "[", "]", "}", "\n", "for", "e", "in", "sentence", ":", "\n", "                ", "sent_info", "[", "'tokens_mw'", "]", ".", "append", "(", "e", ".", "text", ")", "\n", "sent_info", "[", "'lemmas'", "]", ".", "append", "(", "e", ".", "get", "(", "'lemma'", ")", ")", "\n", "sent_info", "[", "'senses'", "]", ".", "append", "(", "e", ".", "get", "(", "'id'", ")", ")", "\n", "sent_info", "[", "'pos'", "]", ".", "append", "(", "e", ".", "get", "(", "'pos'", ")", ")", "\n", "\n", "", "sent_info", "[", "'tokens'", "]", "=", "sum", "(", "[", "t", ".", "split", "(", ")", "for", "t", "in", "sent_info", "[", "'tokens_mw'", "]", "]", ",", "[", "]", ")", "\n", "\n", "# handling multi-word expressions, mapping allows matching tokens with mw features", "\n", "idx_map_abs", "=", "[", "]", "\n", "idx_map_rel", "=", "[", "(", "i", ",", "list", "(", "range", "(", "len", "(", "t", ".", "split", "(", ")", ")", ")", ")", ")", "\n", "for", "i", ",", "t", "in", "enumerate", "(", "sent_info", "[", "'tokens_mw'", "]", ")", "]", "\n", "token_counter", "=", "0", "\n", "for", "idx_group", ",", "idx_tokens", "in", "idx_map_rel", ":", "# converting relative token positions to absolute", "\n", "                ", "idx_tokens", "=", "[", "i", "+", "token_counter", "for", "i", "in", "idx_tokens", "]", "\n", "token_counter", "+=", "len", "(", "idx_tokens", ")", "\n", "idx_map_abs", ".", "append", "(", "[", "idx_group", ",", "idx_tokens", "]", ")", "\n", "\n", "", "sent_info", "[", "'tokenized_sentence'", "]", "=", "' '", ".", "join", "(", "sent_info", "[", "'tokens'", "]", ")", "\n", "sent_info", "[", "'idx_map_abs'", "]", "=", "idx_map_abs", "\n", "sent_info", "[", "'idx'", "]", "=", "sent_idx", "\n", "\n", "eval_entries", ".", "append", "(", "sent_info", ")", "\n", "\n", "", "", "return", "eval_entries", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wsd_mfs.get_id2senses": [[81, 90], ["open", "line.split", "line.split"], "function", ["None"], ["", "def", "get_id2senses", "(", "wsd_eval_keys", ")", ":", "\n", "    ", "\"\"\"Maps ids of split set to sensekeys, just for in-code evaluation.\"\"\"", "\n", "id2senses", "=", "{", "}", "\n", "with", "open", "(", "wsd_eval_keys", ")", "as", "keys_f", ":", "\n", "        ", "for", "line", "in", "keys_f", ":", "\n", "            ", "id_", "=", "line", ".", "split", "(", ")", "[", "0", "]", "\n", "keys", "=", "line", ".", "split", "(", ")", "[", "1", ":", "]", "\n", "id2senses", "[", "id_", "]", "=", "keys", "\n", "", "", "return", "id2senses", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wsd_mfs.run_scorer": [[92, 99], ["print", "os.system"], "function", ["None"], ["", "def", "run_scorer", "(", "eval_framework_path", ",", "eval_set", ",", "results_path", ")", ":", "\n", "    ", "\"\"\"Runs the official java-based scorer of the WSD Evaluation Framework.\"\"\"", "\n", "cmd", "=", "'cd %s && java Scorer %s %s'", "%", "(", "eval_framework_path", "+", "'Evaluation_Datasets/'", ",", "\n", "'%s/%s.gold.key.txt'", "%", "(", "eval_set", ",", "eval_set", ")", ",", "\n", "'../../../../'", "+", "results_path", ")", "\n", "print", "(", "cmd", ")", "\n", "os", ".", "system", "(", "cmd", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_usm.load_wsd_fw_set": [[29, 72], ["xml.parse", "ET.parse.getroot", "enumerate", "sum", "eval_instances.append", "inst[].append", "inst[].append", "inst[].append", "inst[].append", "set", "len", "idx_map_abs.append", "e.get", "e.get", "eval_usm.load_wsd_fw_set.convert_pos"], "function", ["None"], ["def", "load_wsd_fw_set", "(", "wsd_fw_set_path", ")", ":", "\n", "    ", "\"\"\"Parse XML of split set and return list of instances (dict).\"\"\"", "\n", "\n", "def", "convert_pos", "(", "postag", ")", ":", "\n", "        ", "short2long_map", "=", "{", "'VB'", ":", "'VERB'", ",", "'NN'", ":", "'NOUN'", ",", "'JJ'", ":", "'ADJ'", ",", "'RB'", ":", "'ADV'", "}", "\n", "if", "postag", "[", ":", "2", "]", "in", "short2long_map", ":", "\n", "            ", "return", "short2long_map", "[", "postag", "[", ":", "2", "]", "]", "\n", "", "else", ":", "\n", "            ", "return", "postag", "\n", "\n", "", "", "eval_instances", "=", "[", "]", "\n", "tree", "=", "ET", ".", "parse", "(", "wsd_fw_set_path", ")", "\n", "for", "text", "in", "tree", ".", "getroot", "(", ")", ":", "\n", "        ", "for", "sent_idx", ",", "sentence", "in", "enumerate", "(", "text", ")", ":", "\n", "            ", "inst", "=", "{", "'tokens'", ":", "[", "]", ",", "'tokens_mw'", ":", "[", "]", ",", "'lemmas'", ":", "[", "]", ",", "'senses'", ":", "[", "]", ",", "'pos'", ":", "[", "]", "}", "\n", "for", "e", "in", "sentence", ":", "\n", "                ", "inst", "[", "'tokens_mw'", "]", ".", "append", "(", "e", ".", "text", ")", "\n", "inst", "[", "'lemmas'", "]", ".", "append", "(", "e", ".", "get", "(", "'lemma'", ")", ")", "\n", "inst", "[", "'senses'", "]", ".", "append", "(", "e", ".", "get", "(", "'id'", ")", ")", "\n", "inst", "[", "'pos'", "]", ".", "append", "(", "convert_pos", "(", "e", ".", "get", "(", "'pos'", ")", ")", ")", "\n", "\n", "", "inst", "[", "'tokens'", "]", "=", "sum", "(", "[", "t", ".", "split", "(", ")", "for", "t", "in", "inst", "[", "'tokens_mw'", "]", "]", ",", "[", "]", ")", "\n", "\n", "if", "set", "(", "inst", "[", "'senses'", "]", ")", "==", "{", "None", "}", ":", "\n", "                ", "continue", "\n", "\n", "# handling multi-word expressions, mapping allows matching tokens with mw features", "\n", "", "idx_map_abs", "=", "[", "]", "\n", "idx_map_rel", "=", "[", "(", "i", ",", "list", "(", "range", "(", "len", "(", "t", ".", "split", "(", ")", ")", ")", ")", ")", "\n", "for", "i", ",", "t", "in", "enumerate", "(", "inst", "[", "'tokens_mw'", "]", ")", "]", "\n", "token_counter", "=", "0", "\n", "for", "idx_group", ",", "idx_tokens", "in", "idx_map_rel", ":", "# converting relative token positions to absolute", "\n", "                ", "idx_tokens", "=", "[", "i", "+", "token_counter", "for", "i", "in", "idx_tokens", "]", "\n", "token_counter", "+=", "len", "(", "idx_tokens", ")", "\n", "idx_map_abs", ".", "append", "(", "[", "idx_group", ",", "idx_tokens", "]", ")", "\n", "\n", "", "inst", "[", "'tokenized_sentence'", "]", "=", "' '", ".", "join", "(", "inst", "[", "'tokens'", "]", ")", "\n", "inst", "[", "'idx_map_abs'", "]", "=", "idx_map_abs", "\n", "inst", "[", "'idx'", "]", "=", "sent_idx", "\n", "\n", "eval_instances", ".", "append", "(", "inst", ")", "\n", "\n", "", "", "return", "eval_instances", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_usm.load_ground_truth": [[74, 85], ["open", "line.split", "line.split"], "function", ["None"], ["", "def", "load_ground_truth", "(", "eval_keys", ",", "sense_level", "=", "'sensekey'", ")", ":", "\n", "    ", "\"\"\" Maps ids of split set to sensekeys, just for in-code evaluation. \"\"\"", "\n", "id2sks", "=", "{", "}", "\n", "with", "open", "(", "eval_keys", ")", "as", "keys_f", ":", "\n", "        ", "for", "line", "in", "keys_f", ":", "\n", "            ", "id_", "=", "line", ".", "split", "(", ")", "[", "0", "]", "\n", "keys", "=", "line", ".", "split", "(", ")", "[", "1", ":", "]", "\n", "if", "sense_level", "==", "'synset'", ":", "\n", "                ", "keys", "=", "[", "map_sk2syn", "[", "k", "]", "for", "k", "in", "keys", "]", "\n", "", "id2sks", "[", "id_", "]", "=", "keys", "\n", "", "", "return", "id2sks", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_usm.run_scorer": [[88, 103], ["print", "subprocess.check_output", "cmd_output.decode.decode"], "function", ["None"], ["", "def", "run_scorer", "(", "eval_fw_path", ",", "test_set", ",", "results_path", ")", ":", "\n", "    ", "\"\"\" Runs the official java-based scorer of the WSD Evaluation Framework. \"\"\"", "\n", "\n", "if", "test_set", "in", "[", "'NOUN'", ",", "'VERB'", ",", "'ADJ'", ",", "'ADV'", "]", ":", "\n", "        ", "gold_fn", "=", "'ALL/ALL.gold.%s.key.txt'", "%", "test_set", "\n", "", "else", ":", "\n", "        ", "gold_fn", "=", "'%s/%s.gold.key.txt'", "%", "(", "test_set", ",", "test_set", ")", "\n", "\n", "", "cmd", "=", "'cd %s && java Scorer %s %s'", "%", "(", "eval_fw_path", "+", "'Evaluation_Datasets/'", ",", "\n", "gold_fn", ",", "\n", "'../../../../'", "+", "results_path", ")", "\n", "print", "(", "cmd", ")", "\n", "cmd_output", "=", "subprocess", ".", "check_output", "(", "cmd", ",", "shell", "=", "True", ")", "\n", "cmd_output", "=", "cmd_output", ".", "decode", "(", "'utf8'", ")", "\n", "return", "cmd_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_usm.chunks": [[105, 109], ["range", "len"], "function", ["None"], ["", "def", "chunks", "(", "l", ",", "n", ")", ":", "\n", "    ", "\"\"\"Yield successive n-sized chunks from given list.\"\"\"", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "l", ")", ",", "n", ")", ":", "\n", "        ", "yield", "l", "[", "i", ":", "i", "+", "n", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_usm.str_configuration": [[111, 127], ["[].split", "len", "args.lmms_path.split"], "function", ["None"], ["", "", "def", "str_configuration", "(", "args", ")", ":", "\n", "\n", "# str_conf = '%d' % int(time())", "\n", "    ", "str_conf", "=", "args", ".", "lmms_path", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "str_conf", "+=", "'.%s'", "%", "args", ".", "test_set", "\n", "str_conf", "+=", "'.%s'", "%", "args", ".", "nlm_id", "\n", "\n", "str_conf", "+=", "'.%s'", "%", "args", ".", "layer_op", "\n", "if", "len", "(", "args", ".", "layers", ")", ">", "1", ":", "\n", "        ", "str_conf", "+=", "'.%d,%d'", "%", "(", "args", ".", "layers", "[", "0", "]", ",", "args", ".", "layers", "[", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "        ", "str_conf", "+=", "'.%d'", "%", "args", ".", "layers", "[", "0", "]", "\n", "\n", "", "str_conf", "+=", "'.USM'", "\n", "\n", "return", "str_conf", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_usm.eval_usm": [[129, 267], ["eval_usm.str_configuration", "open", "eval_usm.chunks", "open.close", "logging.info", "eval_usm.run_scorer", "print", "encoder.token_embeddings", "zip", "logging.info", "numpy.mean", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "correct_at_5.count", "len", "numpy.array().mean", "senses_vsm.most_similar_vec", "num_options.append", "enumerate", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.linalg.norm", "numpy.hstack", "numpy.linalg.norm", "len", "len", "open.write", "len", "len", "correct_at_5.append", "correct_at_5.append", "numpy.mean", "logging.info", "len", "numpy.array", "numpy.array", "numpy.array", "numpy.hstack", "set().intersection", "set().intersection", "correct_idxs.append", "reciprocal_ranks.append", "correct_at_5.count", "len", "set", "set", "set", "set", "len"], "function", ["home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wsd.str_configuration", "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wsd.chunks", "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wsd.run_scorer", "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.token_embeddings", "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.VSM.most_similar_vec"], ["", "def", "eval_usm", "(", "args", ",", "encoder", ",", "eval_instances", ")", ":", "\n", "\n", "    ", "\"\"\"\n    Initialize various counters for calculating supplementary metrics.\n    \"\"\"", "\n", "n_instances", ",", "n_sents", ",", "n_correct", ",", "n_unk_lemmas", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "correct_idxs", "=", "[", "]", "\n", "num_options", "=", "[", "]", "\n", "reciprocal_ranks", "=", "[", "]", "\n", "correct_at_5", "=", "[", "]", "\n", "\n", "pos_confusion", "=", "{", "}", "\n", "for", "pos", "in", "[", "'NOUN'", ",", "'VERB'", ",", "'ADJ'", ",", "'ADV'", "]", ":", "\n", "        ", "pos_confusion", "[", "pos", "]", "=", "{", "'NOUN'", ":", "0", ",", "'VERB'", ":", "0", ",", "'ADJ'", ":", "0", ",", "'ADV'", ":", "0", "}", "\n", "\n", "", "\"\"\"\n    Iterate over evaluation instances and write predictions in WSD_Evaluation_Framework's format.\n    File with predictions is processed by the official scorer after iterating over all instances.\n    \"\"\"", "\n", "\n", "str_conf", "=", "str_configuration", "(", "args", ")", "\n", "\n", "results_path", "=", "'results/%s.key'", "%", "str_conf", "\n", "results_f", "=", "open", "(", "results_path", ",", "'w'", ")", "\n", "\n", "# matches_path = 'matches/%s.tsv' % str_conf", "\n", "# matches_f = open(matches_path, 'w')", "\n", "\n", "for", "batch", "in", "chunks", "(", "eval_instances", ",", "args", ".", "batch_size", ")", ":", "\n", "\n", "        ", "batch_tokens", "=", "[", "e", "[", "'tokens'", "]", "for", "e", "in", "batch", "]", "\n", "batch_embs", "=", "encoder", ".", "token_embeddings", "(", "batch_tokens", ")", "\n", "\n", "for", "sent_info", ",", "sent_embs", "in", "zip", "(", "batch", ",", "batch_embs", ")", ":", "\n", "\n", "            ", "n_sents", "+=", "1", "\n", "idx_map_abs", "=", "sent_info", "[", "'idx_map_abs'", "]", "\n", "\n", "for", "mw_idx", ",", "tok_idxs", "in", "idx_map_abs", ":", "\n", "                ", "curr_sense", "=", "sent_info", "[", "'senses'", "]", "[", "mw_idx", "]", "\n", "\n", "if", "curr_sense", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "n_instances", "+=", "1", "\n", "\n", "curr_lemma", "=", "sent_info", "[", "'lemmas'", "]", "[", "mw_idx", "]", "\n", "if", "curr_lemma", "not", "in", "senses_vsm", ".", "known_lemmas", ":", "\n", "                    ", "n_unk_lemmas", "+=", "1", "\n", "\n", "", "curr_postag", "=", "sent_info", "[", "'pos'", "]", "[", "mw_idx", "]", "\n", "curr_tokens", "=", "[", "sent_info", "[", "'tokens'", "]", "[", "i", "]", "for", "i", "in", "tok_idxs", "]", "\n", "curr_vector", "=", "np", ".", "array", "(", "[", "sent_embs", "[", "i", "]", "[", "1", "]", "for", "i", "in", "tok_idxs", "]", ")", ".", "mean", "(", "axis", "=", "0", ")", "\n", "curr_vector", "=", "curr_vector", "/", "np", ".", "linalg", ".", "norm", "(", "curr_vector", ")", "\n", "\n", "if", "args", ".", "test_set", "in", "[", "'NOUN'", ",", "'VERB'", ",", "'ADJ'", ",", "'ADV'", "]", "and", "curr_postag", "!=", "args", ".", "test_set", ":", "\n", "                    ", "continue", "\n", "\n", "", "\"\"\"\n                Compose test-time embedding for matching with sense embeddings in SensesVSM.\n                Test-time embedding corresponds to stack of contextual embeddings.\n                Stacking composition performed according to dimensionality of sense embeddings.\n                \"\"\"", "\n", "\n", "# duplicating contextual feature for cos similarity against features from", "\n", "# sense annotations and glosses that belong to the same NLM", "\n", "if", "senses_vsm", ".", "ndims", "==", "1024", "+", "1024", ":", "# TODO: get dims from loaded model", "\n", "                    ", "curr_vector", "=", "np", ".", "hstack", "(", "(", "curr_vector", ",", "curr_vector", ")", ")", "\n", "\n", "", "elif", "senses_vsm", ".", "ndims", "==", "4096", "+", "4096", ":", "\n", "                    ", "curr_vector", "=", "np", ".", "hstack", "(", "(", "curr_vector", ",", "curr_vector", ")", ")", "\n", "\n", "", "curr_vector", "=", "curr_vector", "/", "np", ".", "linalg", ".", "norm", "(", "curr_vector", ")", "\n", "\n", "\"\"\"\n                Matches test-time embedding against sense embeddings in SensesVSM.\n                Matching is actually cosine similarity (most similar), or 1-NN.\n                \"\"\"", "\n", "\n", "matches", "=", "senses_vsm", ".", "most_similar_vec", "(", "curr_vector", ",", "topn", "=", "None", ")", "\n", "# matches = senses_vsm.match_senses(curr_vector, lemma=None, postag=curr_postag, topn=None)", "\n", "preds", "=", "[", "sk", "for", "sk", ",", "_", "in", "matches", "]", "\n", "\n", "num_options", ".", "append", "(", "len", "(", "matches", ")", ")", "\n", "\n", "if", "len", "(", "preds", ")", ">", "0", ":", "\n", "                    ", "results_f", ".", "write", "(", "'%s %s\\n'", "%", "(", "curr_sense", ",", "preds", "[", "0", "]", ")", ")", "\n", "\n", "# matches_str = '\\t'.join(['%s|%.5f' % (sk, sim) for sk, sim in matches[:100]])", "\n", "# matches_f.write('%s\\t%s\\t%s\\n' % (curr_sense, ','.join(id2gold[curr_sense]), matches_str))", "\n", "\n", "", "\"\"\"\n                Processing additional performance metrics.\n                \"\"\"", "\n", "\n", "# check if our prediction(s) was correct", "\n", "gold_sensekeys", "=", "id2gold", "[", "curr_sense", "]", "\n", "if", "len", "(", "set", "(", "preds", "[", ":", "1", "]", ")", ".", "intersection", "(", "set", "(", "gold_sensekeys", ")", ")", ")", ">", "0", ":", "\n", "                    ", "n_correct", "+=", "1", "\n", "\n", "", "if", "len", "(", "set", "(", "preds", "[", ":", "5", "]", ")", ".", "intersection", "(", "set", "(", "gold_sensekeys", ")", ")", ")", ">", "0", ":", "\n", "                    ", "correct_at_5", ".", "append", "(", "True", ")", "\n", "", "else", ":", "\n", "                    ", "correct_at_5", ".", "append", "(", "False", ")", "\n", "\n", "# register how far the correct prediction was from the top of our matches", "\n", "", "for", "idx", ",", "(", "matched_sensekey", ",", "_", ")", "in", "enumerate", "(", "matches", ")", ":", "\n", "                    ", "if", "matched_sensekey", "in", "gold_sensekeys", ":", "\n", "                        ", "correct_idx", "=", "idx", "\n", "correct_idxs", ".", "append", "(", "idx", ")", "\n", "reciprocal_ranks", ".", "append", "(", "1", "/", "(", "1", "+", "correct_idx", ")", ")", "\n", "break", "\n", "\n", "", "", "if", "args", ".", "debug", "and", "(", "n_instances", "%", "100", "==", "0", ")", ":", "\n", "                    ", "acc", "=", "n_correct", "/", "n_instances", "\n", "mrr", "=", "np", ".", "mean", "(", "reciprocal_ranks", ")", "\n", "p_at_5", "=", "correct_at_5", ".", "count", "(", "True", ")", "/", "len", "(", "correct_at_5", ")", "\n", "logging", ".", "info", "(", "'ACC: %.3f MRR: %.3f P@5: %.3f (inst_idx=%d sent_idx=%d/%d)'", "%", "(", "acc", ",", "mrr", ",", "p_at_5", ",", "n_instances", ",", "n_sents", ",", "len", "(", "eval_instances", ")", ")", ")", "\n", "\n", "", "", "", "", "results_f", ".", "close", "(", ")", "\n", "# matches_f.close()", "\n", "\n", "if", "args", ".", "debug", ":", "\n", "\n", "        ", "logging", ".", "info", "(", "'Final Results:'", ")", "\n", "acc", "=", "n_correct", "/", "n_instances", "\n", "mrr", "=", "np", ".", "mean", "(", "reciprocal_ranks", ")", "\n", "p_at_5", "=", "correct_at_5", ".", "count", "(", "True", ")", "/", "len", "(", "correct_at_5", ")", "\n", "logging", ".", "info", "(", "'ACC: %.3f MRR: %.3f P@5: %.3f (inst_idx=%d sent_idx=%d/%d)'", "%", "(", "acc", ",", "mrr", ",", "p_at_5", ",", "n_instances", ",", "n_sents", ",", "len", "(", "eval_instances", ")", ")", ")", "\n", "logging", ".", "info", "(", "'Avg. correct idx: %.6f'", "%", "np", ".", "mean", "(", "np", ".", "array", "(", "correct_idxs", ")", ")", ")", "\n", "logging", ".", "info", "(", "'Avg. correct idx (failed): %.6f'", "%", "np", ".", "mean", "(", "np", ".", "array", "(", "[", "i", "for", "i", "in", "correct_idxs", "if", "i", ">", "0", "]", ")", ")", ")", "\n", "logging", ".", "info", "(", "'Avg. num options: %.6f'", "%", "np", ".", "mean", "(", "num_options", ")", ")", "\n", "logging", ".", "info", "(", "'Num. unknown lemmas: %d'", "%", "n_unk_lemmas", ")", "\n", "logging", ".", "info", "(", "'Num. instances: %d'", "%", "n_instances", ")", "\n", "\n", "", "logging", ".", "info", "(", "'Running official scorer ...'", ")", "\n", "scores_str", "=", "run_scorer", "(", "args", ".", "eval_fw_path", ",", "args", ".", "test_set", ",", "results_path", ")", "\n", "print", "(", "scores_str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wsd.load_wsd_fw_set": [[29, 72], ["xml.parse", "ET.parse.getroot", "enumerate", "sum", "eval_instances.append", "inst[].append", "inst[].append", "inst[].append", "inst[].append", "set", "len", "idx_map_abs.append", "e.get", "e.get", "eval_wsd.load_wsd_fw_set.convert_pos"], "function", ["None"], ["def", "load_wsd_fw_set", "(", "wsd_fw_set_path", ")", ":", "\n", "    ", "\"\"\"Parse XML of split set and return list of instances (dict).\"\"\"", "\n", "\n", "def", "convert_pos", "(", "postag", ")", ":", "\n", "        ", "short2long_map", "=", "{", "'VB'", ":", "'VERB'", ",", "'NN'", ":", "'NOUN'", ",", "'JJ'", ":", "'ADJ'", ",", "'RB'", ":", "'ADV'", "}", "\n", "if", "postag", "[", ":", "2", "]", "in", "short2long_map", ":", "\n", "            ", "return", "short2long_map", "[", "postag", "[", ":", "2", "]", "]", "\n", "", "else", ":", "\n", "            ", "return", "postag", "\n", "\n", "", "", "eval_instances", "=", "[", "]", "\n", "tree", "=", "ET", ".", "parse", "(", "wsd_fw_set_path", ")", "\n", "for", "text", "in", "tree", ".", "getroot", "(", ")", ":", "\n", "        ", "for", "sent_idx", ",", "sentence", "in", "enumerate", "(", "text", ")", ":", "\n", "            ", "inst", "=", "{", "'tokens'", ":", "[", "]", ",", "'tokens_mw'", ":", "[", "]", ",", "'lemmas'", ":", "[", "]", ",", "'senses'", ":", "[", "]", ",", "'pos'", ":", "[", "]", "}", "\n", "for", "e", "in", "sentence", ":", "\n", "                ", "inst", "[", "'tokens_mw'", "]", ".", "append", "(", "e", ".", "text", ")", "\n", "inst", "[", "'lemmas'", "]", ".", "append", "(", "e", ".", "get", "(", "'lemma'", ")", ")", "\n", "inst", "[", "'senses'", "]", ".", "append", "(", "e", ".", "get", "(", "'id'", ")", ")", "\n", "inst", "[", "'pos'", "]", ".", "append", "(", "convert_pos", "(", "e", ".", "get", "(", "'pos'", ")", ")", ")", "\n", "\n", "", "inst", "[", "'tokens'", "]", "=", "sum", "(", "[", "t", ".", "split", "(", ")", "for", "t", "in", "inst", "[", "'tokens_mw'", "]", "]", ",", "[", "]", ")", "\n", "\n", "if", "set", "(", "inst", "[", "'senses'", "]", ")", "==", "{", "None", "}", ":", "\n", "                ", "continue", "\n", "\n", "# handling multi-word expressions, mapping allows matching tokens with mw features", "\n", "", "idx_map_abs", "=", "[", "]", "\n", "idx_map_rel", "=", "[", "(", "i", ",", "list", "(", "range", "(", "len", "(", "t", ".", "split", "(", ")", ")", ")", ")", ")", "\n", "for", "i", ",", "t", "in", "enumerate", "(", "inst", "[", "'tokens_mw'", "]", ")", "]", "\n", "token_counter", "=", "0", "\n", "for", "idx_group", ",", "idx_tokens", "in", "idx_map_rel", ":", "# converting relative token positions to absolute", "\n", "                ", "idx_tokens", "=", "[", "i", "+", "token_counter", "for", "i", "in", "idx_tokens", "]", "\n", "token_counter", "+=", "len", "(", "idx_tokens", ")", "\n", "idx_map_abs", ".", "append", "(", "[", "idx_group", ",", "idx_tokens", "]", ")", "\n", "\n", "", "inst", "[", "'tokenized_sentence'", "]", "=", "' '", ".", "join", "(", "inst", "[", "'tokens'", "]", ")", "\n", "inst", "[", "'idx_map_abs'", "]", "=", "idx_map_abs", "\n", "inst", "[", "'idx'", "]", "=", "sent_idx", "\n", "\n", "eval_instances", ".", "append", "(", "inst", ")", "\n", "\n", "", "", "return", "eval_instances", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wsd.load_ground_truth": [[74, 85], ["open", "line.split", "line.split"], "function", ["None"], ["", "def", "load_ground_truth", "(", "eval_keys", ",", "sense_level", "=", "'sensekey'", ")", ":", "\n", "    ", "\"\"\" Maps ids of split set to sensekeys, just for in-code evaluation. \"\"\"", "\n", "id2sks", "=", "{", "}", "\n", "with", "open", "(", "eval_keys", ")", "as", "keys_f", ":", "\n", "        ", "for", "line", "in", "keys_f", ":", "\n", "            ", "id_", "=", "line", ".", "split", "(", ")", "[", "0", "]", "\n", "keys", "=", "line", ".", "split", "(", ")", "[", "1", ":", "]", "\n", "if", "sense_level", "==", "'synset'", ":", "\n", "                ", "keys", "=", "[", "map_sk2syn", "[", "k", "]", "for", "k", "in", "keys", "]", "\n", "", "id2sks", "[", "id_", "]", "=", "keys", "\n", "", "", "return", "id2sks", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wsd.run_scorer": [[87, 102], ["print", "subprocess.check_output", "cmd_output.decode.decode"], "function", ["None"], ["", "def", "run_scorer", "(", "eval_fw_path", ",", "test_set", ",", "results_path", ")", ":", "\n", "    ", "\"\"\" Runs the official java-based scorer of the WSD Evaluation Framework. \"\"\"", "\n", "\n", "if", "test_set", "in", "[", "'NOUN'", ",", "'VERB'", ",", "'ADJ'", ",", "'ADV'", "]", ":", "\n", "        ", "gold_fn", "=", "'ALL/ALL.gold.%s.key.txt'", "%", "test_set", "\n", "", "else", ":", "\n", "        ", "gold_fn", "=", "'%s/%s.gold.key.txt'", "%", "(", "test_set", ",", "test_set", ")", "\n", "\n", "", "cmd", "=", "'cd %s && java Scorer %s %s'", "%", "(", "eval_fw_path", "+", "'Evaluation_Datasets/'", ",", "\n", "gold_fn", ",", "\n", "'../../../../'", "+", "results_path", ")", "\n", "print", "(", "cmd", ")", "\n", "cmd_output", "=", "subprocess", ".", "check_output", "(", "cmd", ",", "shell", "=", "True", ")", "\n", "cmd_output", "=", "cmd_output", ".", "decode", "(", "'utf8'", ")", "\n", "return", "cmd_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wsd.chunks": [[104, 108], ["range", "len"], "function", ["None"], ["", "def", "chunks", "(", "l", ",", "n", ")", ":", "\n", "    ", "\"\"\"Yield successive n-sized chunks from given list.\"\"\"", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "l", ")", ",", "n", ")", ":", "\n", "        ", "yield", "l", "[", "i", ":", "i", "+", "n", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wsd.str_configuration": [[110, 123], ["[].split", "len", "args.lmms_path.split"], "function", ["None"], ["", "", "def", "str_configuration", "(", "args", ")", ":", "\n", "\n", "    ", "str_conf", "=", "args", ".", "lmms_path", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "str_conf", "+=", "'.%s'", "%", "args", ".", "test_set", "\n", "str_conf", "+=", "'.%s'", "%", "args", ".", "nlm_id", "\n", "\n", "str_conf", "+=", "'.%s'", "%", "args", ".", "layer_op", "\n", "if", "len", "(", "args", ".", "layers", ")", ">", "1", ":", "\n", "        ", "str_conf", "+=", "'.%d,%d'", "%", "(", "args", ".", "layers", "[", "0", "]", ",", "args", ".", "layers", "[", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "        ", "str_conf", "+=", "'.%d'", "%", "args", ".", "layers", "[", "0", "]", "\n", "\n", "", "return", "str_conf", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wsd.eval_wsd": [[125, 222], ["eval_wsd.str_configuration", "open", "eval_wsd.chunks", "open.close", "logging.info", "eval_wsd.run_scorer", "print", "encoder.token_embeddings", "zip", "logging.info", "logging.info", "logging.info", "numpy.array().mean", "senses_vsm.match_senses", "set", "numpy.linalg.norm", "numpy.hstack", "numpy.linalg.norm", "len", "open.write", "logging.info", "len", "numpy.array", "numpy.hstack", "len"], "function", ["home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wsd.str_configuration", "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wsd.chunks", "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_wsd.run_scorer", "home.repos.pwc.inspect_result.danlou_LMMS.None.fairseq_encoder.FairSeqEncoder.token_embeddings", "home.repos.pwc.inspect_result.danlou_LMMS.None.vectorspace.SensesVSM.match_senses"], ["", "def", "eval_wsd", "(", "args", ",", "encoder", ",", "eval_instances", ")", ":", "\n", "\n", "    ", "\"\"\"\n    Initialize various counters for calculating supplementary metrics.\n    \"\"\"", "\n", "n_instances", ",", "n_sents", ",", "n_correct", ",", "n_unk_lemmas", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "\n", "\"\"\"\n    Iterate over evaluation instances and write predictions in WSD_Evaluation_Framework's format.\n    File with predictions is processed by the official scorer after iterating over all instances.\n    \"\"\"", "\n", "\n", "str_conf", "=", "str_configuration", "(", "args", ")", "\n", "\n", "results_path", "=", "'results/%s.key'", "%", "str_conf", "\n", "results_f", "=", "open", "(", "results_path", ",", "'w'", ")", "\n", "\n", "# matches_path = 'matches/%s.tsv' % str_conf", "\n", "# matches_f = open(matches_path, 'w')", "\n", "\n", "for", "batch", "in", "chunks", "(", "eval_instances", ",", "args", ".", "batch_size", ")", ":", "\n", "\n", "        ", "batch_tokens", "=", "[", "e", "[", "'tokens'", "]", "for", "e", "in", "batch", "]", "\n", "batch_embs", "=", "encoder", ".", "token_embeddings", "(", "batch_tokens", ")", "\n", "\n", "for", "sent_info", ",", "sent_embs", "in", "zip", "(", "batch", ",", "batch_embs", ")", ":", "\n", "\n", "            ", "n_sents", "+=", "1", "\n", "idx_map_abs", "=", "sent_info", "[", "'idx_map_abs'", "]", "\n", "\n", "for", "mw_idx", ",", "tok_idxs", "in", "idx_map_abs", ":", "\n", "                ", "curr_sense", "=", "sent_info", "[", "'senses'", "]", "[", "mw_idx", "]", "\n", "\n", "if", "curr_sense", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "n_instances", "+=", "1", "\n", "\n", "curr_lemma", "=", "sent_info", "[", "'lemmas'", "]", "[", "mw_idx", "]", "\n", "if", "curr_lemma", "not", "in", "senses_vsm", ".", "known_lemmas", ":", "\n", "                    ", "n_unk_lemmas", "+=", "1", "\n", "continue", "# skips hurt performance in official scorer", "\n", "\n", "", "curr_postag", "=", "sent_info", "[", "'pos'", "]", "[", "mw_idx", "]", "\n", "curr_tokens", "=", "[", "sent_info", "[", "'tokens'", "]", "[", "i", "]", "for", "i", "in", "tok_idxs", "]", "\n", "curr_vector", "=", "np", ".", "array", "(", "[", "sent_embs", "[", "i", "]", "[", "1", "]", "for", "i", "in", "tok_idxs", "]", ")", ".", "mean", "(", "axis", "=", "0", ")", "\n", "curr_vector", "=", "curr_vector", "/", "np", ".", "linalg", ".", "norm", "(", "curr_vector", ")", "\n", "\n", "\"\"\"\n                Compose test-time embedding for matching with sense embeddings in SensesVSM.\n                Test-time embedding may correspond to stack of contextual embeddings.\n                Stacking composition performed according to dimensionality of sense embeddings.\n                \"\"\"", "\n", "\n", "# duplicating contextual feature for cos similarity against features from", "\n", "# sense annotations and glosses that belong to the same NLM", "\n", "if", "senses_vsm", ".", "ndims", "==", "1024", "+", "1024", ":", "# TODO: get dims from loaded model", "\n", "                    ", "curr_vector", "=", "np", ".", "hstack", "(", "(", "curr_vector", ",", "curr_vector", ")", ")", "\n", "\n", "", "elif", "senses_vsm", ".", "ndims", "==", "4096", "+", "4096", ":", "\n", "                    ", "curr_vector", "=", "np", ".", "hstack", "(", "(", "curr_vector", ",", "curr_vector", ")", ")", "\n", "\n", "", "curr_vector", "=", "curr_vector", "/", "np", ".", "linalg", ".", "norm", "(", "curr_vector", ")", "\n", "\n", "\"\"\"\n                Matches test-time embedding against sense embeddings in SensesVSM.\n                Matching is actually cosine similarity (most similar), or 1-NN.\n                \"\"\"", "\n", "matches", "=", "senses_vsm", ".", "match_senses", "(", "curr_vector", ",", "curr_lemma", ",", "curr_postag", ",", "topn", "=", "None", ")", "\n", "preds", "=", "[", "sk", "for", "sk", ",", "_", "in", "matches", "]", "\n", "\n", "if", "len", "(", "preds", ")", ">", "0", ":", "\n", "                    ", "results_f", ".", "write", "(", "'%s %s\\n'", "%", "(", "curr_sense", ",", "preds", "[", "0", "]", ")", ")", "\n", "\n", "# matches_str = '\\t'.join(['%s|%.5f' % (sk, sim) for sk, sim in matches[:100]])", "\n", "# matches_f.write('%s\\t%s\\t%s\\n' % (curr_sense, ','.join(id2gold[curr_sense]), matches_str))", "\n", "\n", "", "gold_sensekeys", "=", "set", "(", "id2gold", "[", "curr_sense", "]", ")", "\n", "if", "preds", "[", "0", "]", "in", "gold_sensekeys", ":", "\n", "                    ", "n_correct", "+=", "1", "\n", "\n", "", "if", "args", ".", "debug", "and", "(", "n_instances", "%", "100", "==", "0", ")", ":", "\n", "                    ", "acc", "=", "n_correct", "/", "n_instances", "\n", "logging", ".", "info", "(", "'ACC: %.3f (inst_idx=%d sent_idx=%d/%d)'", "%", "(", "acc", ",", "n_instances", ",", "n_sents", ",", "len", "(", "eval_instances", ")", ")", ")", "\n", "\n", "", "", "", "", "results_f", ".", "close", "(", ")", "\n", "# matches_f.close()", "\n", "\n", "if", "args", ".", "debug", ":", "\n", "        ", "logging", ".", "info", "(", "'Final Results:'", ")", "\n", "acc", "=", "n_correct", "/", "n_instances", "\n", "logging", ".", "info", "(", "'ACC: %.3f (inst_idx=%d sent_idx=%d/%d)'", "%", "(", "acc", ",", "n_instances", ",", "n_sents", ",", "len", "(", "eval_instances", ")", ")", ")", "\n", "logging", ".", "info", "(", "'# unknown lemmas: %d'", "%", "n_unk_lemmas", ")", "\n", "\n", "", "logging", ".", "info", "(", "'Running official scorer ...'", ")", "\n", "scores_str", "=", "run_scorer", "(", "args", ".", "eval_fw_path", ",", "args", ".", "test_set", ",", "results_path", ")", "\n", "print", "(", "scores_str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_scws.load_scws": [[24, 65], ["str.replace", "str.replace", "open", "tokens.index", "line.strip().split", "eval_scws.load_scws.unbolded"], "function", ["home.repos.pwc.inspect_result.danlou_LMMS.evaluation.eval_gwcs.unbolded"], ["def", "load_scws", "(", "path", "=", "'external/scws/ratings.txt'", ",", "restr_pos_pair", "=", "None", ")", ":", "\n", "\n", "    ", "def", "unbolded", "(", "context", ")", ":", "\n", "        ", "context", "=", "str", ".", "replace", "(", "context", ",", "'<b>'", ",", "''", ")", "\n", "return", "str", ".", "replace", "(", "context", ",", "'</b>'", ",", "''", ")", "\n", "\n", "", "def", "get_word_idx", "(", "tokens", ",", "word", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "tokens", ".", "index", "(", "word", ")", "\n", "", "except", "ValueError", ":", "\n", "            ", "tokens", "=", "[", "t", ".", "lower", "(", ")", "for", "t", "in", "tokens", "]", "\n", "return", "tokens", ".", "index", "(", "word", ".", "lower", "(", ")", ")", "\n", "\n", "", "", "instances", "=", "[", "]", "\n", "ratings", "=", "[", "]", "\n", "\n", "with", "open", "(", "path", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "elems", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "inst_id", ",", "word1", ",", "word1_pos", ",", "word2", ",", "word2_pos", ",", "ctx1", ",", "ctx2", "=", "elems", "[", ":", "7", "]", "\n", "avg_rating", ",", "all_ratings", "=", "float", "(", "elems", "[", "7", "]", ")", ",", "elems", "[", "8", ":", "]", "\n", "\n", "# restrict to specific pairs of POS tags (unordered)", "\n", "if", "restr_pos_pair", "!=", "None", ":", "\n", "                ", "if", "set", "(", "[", "word1_pos", ",", "word2_pos", "]", ")", "!=", "set", "(", "restr_pos_pair", ")", ":", "\n", "                    ", "continue", "\n", "\n", "", "", "ctx1", "=", "unbolded", "(", "ctx1", ")", "\n", "ctx2", "=", "unbolded", "(", "ctx2", ")", "\n", "ctx1_tokens", "=", "ctx1", ".", "split", "(", "' '", ")", "\n", "ctx2_tokens", "=", "ctx2", ".", "split", "(", "' '", ")", "\n", "word1_idx", "=", "get_word_idx", "(", "ctx1_tokens", ",", "word1", ")", "\n", "word2_idx", "=", "get_word_idx", "(", "ctx2_tokens", ",", "word2", ")", "\n", "\n", "e", "=", "{", "'ctx1_tokens'", ":", "ctx1_tokens", ",", "'ctx2_tokens'", ":", "ctx2_tokens", ",", "\n", "'word1_idx'", ":", "word1_idx", ",", "'word2_idx'", ":", "word2_idx", ",", "\n", "'word1_pos'", ":", "word1_pos", ",", "'word2_pos'", ":", "word2_pos", "}", "\n", "instances", ".", "append", "(", "e", ")", "\n", "ratings", ".", "append", "(", "avg_rating", ")", "\n", "\n", "", "", "return", "instances", ",", "ratings", "\n", "\n"]]}