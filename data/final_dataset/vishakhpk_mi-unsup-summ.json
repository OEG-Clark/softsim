{"home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.None.get_matrices.get_probabilities": [[44, 86], ["model.get_probabilities", "enumerate", "article.split", "enumerate", "range", "print", "len", "res[].append", "len"], "function", ["home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.gpt2.GPT2.get_probabilities"], ["", "def", "get_probabilities", "(", "articles", ")", ":", "\n", "    ", "\"\"\"\n    Given a batch of articles (can be any strings) run a forward pass on GPT2 and obtain word probabilities for the same\n    \"\"\"", "\n", "article_splits", "=", "[", "article", ".", "split", "(", "\" \"", ")", "for", "article", "in", "articles", "]", "\n", "payload", "=", "model", ".", "get_probabilities", "(", "articles", ",", "topk", "=", "20", ")", "\n", "res", "=", "[", "[", "]", "for", "i", "in", "range", "(", "len", "(", "articles", ")", ")", "]", "\n", "for", "t", ",", "article", "in", "enumerate", "(", "articles", ")", ":", "\n", "        ", "context", "=", "\"\"", "\n", "idx", "=", "0", "\n", "chain", "=", "False", "\n", "next_word", "=", "\"\"", "\n", "article_words", "=", "article_splits", "[", "t", "]", "\n", "#print(article, article_words)", "\n", "word_probability", "=", "1.0", "\n", "gt_count", "=", "0", "\n", "idx", "+=", "1", "\n", "found_words", "=", "[", "]", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "payload", "[", "\"context_strings\"", "]", "[", "t", "]", "[", ":", "-", "1", "]", ")", ":", "\n", "            ", "context", "=", "context", "+", "\" \"", "+", "word", "\n", "probability", "=", "payload", "[", "'real_probs'", "]", "[", "t", "]", "[", "i", "]", "#[1]", "\n", "next_word_fragment", "=", "payload", "[", "\"context_strings\"", "]", "[", "t", "]", "[", "i", "+", "1", "]", "\n", "\n", "next_word", "+=", "next_word_fragment", "\n", "#print(next_word, article_words[gt_count])", "\n", "if", "next_word", "==", "article_words", "[", "gt_count", "]", ":", "\n", "                ", "chain", "=", "False", "\n", "gt_count", "+=", "1", "\n", "", "else", ":", "\n", "                ", "chain", "=", "True", "\n", "\n", "", "word_probability", "*=", "probability", "\n", "assert", "word_probability", "<=", "1.0", ",", "print", "(", "word_probability", ",", "context", ")", "\n", "if", "chain", "==", "False", ":", "\n", "#print(\"Word Probability: \", word_probability, next_word)", "\n", "                ", "res", "[", "t", "]", ".", "append", "(", "word_probability", ")", "\n", "word_probability", "=", "1.0", "\n", "next_word", "=", "\"\"", "\n", "#print(gt_count, len(article_words))", "\n", "", "if", "gt_count", "==", "len", "(", "article_words", ")", ":", "\n", "                ", "break", "\n", "", "", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.None.get_matrices.get_npmi_matrix": [[88, 153], ["numpy.zeros", "numpy.zeros", "range", "range", "len", "get_matrices.get_probabilities", "len", "range", "len", "len", "len", "len", "p.append", "len", "batch.append", "sum", "print", "get_matrices.get_probabilities", "batch_indices.keys", "math.log", "str", "int", "sum", "print", "len", "len", "len", "key.split", "print", "print", "str", "sentences[].split", "math.log", "str"], "function", ["home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.gpt2.GPT2.get_probabilities", "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.gpt2.GPT2.get_probabilities"], ["", "def", "get_npmi_matrix", "(", "sentences", ",", "method", "=", "1", ",", "batch_size", "=", "1", ")", ":", "\n", "    ", "\"\"\"\n    Accepts a list of sentences of length n and returns 3 objects:\n    - Normalised PMI nxn matrix - temp\n    - PMI nxn matrix - temp2\n    - List of length n indicating sentence-wise surprisal i.e. p(sentence) - p \n\n    To optimize performance, we do the forward pass batchwise by assembling the batch and maintaining batch indices\n    For each batch we call get_probabilities\n    \"\"\"", "\n", "temp", "=", "np", ".", "zeros", "(", "(", "len", "(", "sentences", ")", ",", "len", "(", "sentences", ")", ")", ")", "\n", "temp2", "=", "np", ".", "zeros", "(", "(", "len", "(", "sentences", ")", ",", "len", "(", "sentences", ")", ")", ")", "\n", "batch_indices", "=", "{", "}", "\n", "batch", "=", "[", "]", "\n", "batchCount", "=", "0", "\n", "batchSize", "=", "batch_size", "\n", "#print(len(sentences))", "\n", "c", "=", "0", "\n", "p", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "sentences", ")", ")", ":", "\n", "        ", "result", "=", "get_probabilities", "(", "[", "sentences", "[", "i", "]", "]", ")", "\n", "try", ":", "\n", "            ", "p", ".", "append", "(", "sum", "(", "[", "math", ".", "log", "(", "i", ")", "for", "i", "in", "result", "[", "0", "]", "]", ")", ")", "\n", "", "except", ":", "\n", "            ", "print", "(", "\"Math domain error surprise\"", ",", "i", ")", "\n", "return", "temp", ",", "temp2", ",", "p", "\n", "", "", "for", "i", "in", "range", "(", "len", "(", "sentences", ")", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "len", "(", "sentences", ")", ")", ":", "\n", "            ", "if", "i", "==", "j", ":", "\n", "                ", "temp", "[", "i", "]", "[", "j", "]", "=", "-", "1", "\n", "temp2", "[", "i", "]", "[", "j", "]", "=", "-", "1", "\n", "continue", "\n", "", "article", "=", "sentences", "[", "i", "]", "+", "\" \"", "+", "sentences", "[", "j", "]", "\n", "#print(article)", "\n", "batch_indices", "[", "str", "(", "i", ")", "+", "\"-\"", "+", "str", "(", "j", ")", "+", "\"-\"", "+", "str", "(", "len", "(", "sentences", "[", "i", "]", ".", "split", "(", ")", ")", ")", "]", "=", "batchCount", "\n", "batch", ".", "append", "(", "article", ")", "\n", "batchCount", "+=", "1", "\n", "\n", "if", "batchCount", "==", "batchSize", "or", "(", "i", "==", "len", "(", "sentences", ")", "-", "1", "and", "j", "==", "len", "(", "sentences", ")", "-", "1", ")", ":", "\n", "#print(batch)", "\n", "                ", "c", "+=", "1", "\n", "result", "=", "get_probabilities", "(", "batch", ")", "\n", "for", "key", "in", "batch_indices", ".", "keys", "(", ")", ":", "\n", "#print(key)", "\n", "#print(key.split(\"-\"))", "\n", "                    ", "idx_i", ",", "idx_j", ",", "idx_l", "=", "[", "int", "(", "idx", ")", "for", "idx", "in", "key", ".", "split", "(", "\"-\"", ")", "]", "\n", "try", ":", "\n", "                        ", "pxy", "=", "sum", "(", "[", "math", ".", "log", "(", "q", ")", "for", "q", "in", "result", "[", "batch_indices", "[", "key", "]", "]", "[", "idx_l", ":", "]", "]", ")", "\n", "py", "=", "p", "[", "idx_j", "]", "\n", "px", "=", "p", "[", "idx_i", "]", "\n", "\n", "temp", "[", "idx_i", "]", "[", "idx_j", "]", "=", "(", "pxy", "-", "py", ")", "/", "(", "-", "1", "*", "(", "pxy", "+", "px", ")", ")", "\n", "temp2", "[", "idx_i", "]", "[", "idx_j", "]", "=", "(", "pxy", "-", "py", ")", "\n", "", "except", "ZeroDivisionError", ":", "\n", "                        ", "print", "(", "\"Zero division error \"", ",", "idx_i", ",", "idx_j", ")", "\n", "temp", "[", "idx_i", "]", "[", "idx_j", "]", "=", "-", "1", "\n", "temp2", "[", "idx_i", "]", "[", "idx_j", "]", "=", "-", "1", "\n", "", "except", ":", "\n", "                        ", "print", "(", "\"Math Domain Error\"", ",", "i", ",", "j", ")", "\n", "", "if", "temp", "[", "idx_i", "]", "[", "idx_j", "]", ">", "1", "or", "temp", "[", "idx_i", "]", "[", "idx_j", "]", "<", "-", "1", ":", "\n", "                        ", "print", "(", "\"Normalise assert \"", ",", "temp", "[", "idx_i", "]", "[", "idx_j", "]", ",", "idx_i", ",", "idx_j", ")", "\n", "", "", "batchCount", "=", "0", "\n", "batch", "=", "[", "]", "\n", "batch_indices", "=", "{", "}", "\n", "", "", "", "return", "temp", ",", "temp2", ",", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.None.get_matrices.remove_unicode": [[154, 156], ["ord"], "function", ["None"], ["", "def", "remove_unicode", "(", "text", ")", ":", "\n", "    ", "return", "''", ".", "join", "(", "[", "i", "if", "ord", "(", "i", ")", "<", "128", "else", "' '", "for", "i", "in", "text", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.None.get_matrices.get_article": [[157, 175], ["print", "nlp", "get_matrices.get_npmi_matrix", "get_matrices.remove_unicode"], "function", ["home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.None.get_matrices.get_npmi_matrix", "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.None.get_matrices.remove_unicode"], ["", "def", "get_article", "(", "idx", ")", ":", "\n", "    ", "\"\"\"\n    For each document in the dataset, split it into sentences and call get_npmi_matrix to create the matrices\n    \"\"\"", "\n", "print", "(", "idx", ")", "\n", "article", ",", "abstract", "=", "data", "[", "idx", "]", "\n", "#sentences = tokenize.sent_tokenize(article)", "\n", "doc", "=", "nlp", "(", "article", ")", "\n", "sentences", "=", "[", "remove_unicode", "(", "sentence", ".", "text", ")", "for", "sentence", "in", "doc", ".", "sents", "]", "\n", "normalised", ",", "vanilla", ",", "surprise", "=", "get_npmi_matrix", "(", "sentences", ",", "batch_size", "=", "10", ")", "\n", "#avg = get_pmi_matrix(sentences, method = 1)", "\n", "output", "[", "idx", "]", "=", "{", "}", "\n", "output", "[", "idx", "]", "[", "\"vanilla\"", "]", "=", "vanilla", "\n", "output", "[", "idx", "]", "[", "\"normalised\"", "]", "=", "normalised", "\n", "output", "[", "idx", "]", "[", "\"surprise\"", "]", "=", "surprise", "\n", "#output[idx][\"averaging\"] = avg", "\n", "#pickle.dump(output, open(\"full_set_1.pkl\", \"wb\"))", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.gpt2.GPT2.__init__": [[11, 25], ["torch.device", "gpt2.GPT2.model.eval", "gpt2.GPT2.model.to", "transformers.GPT2Tokenizer.from_pretrained", "transformers.GPT2LMHeadModel.from_pretrained", "transformers.GPT2Tokenizer.from_pretrained", "transformers.GPT2LMHeadModel.from_pretrained"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "device", "=", "\"cpu\"", ",", "location", "=", "\"\"", ")", ":", "\n", "        ", "if", "location", "==", "\"\"", ":", "\n", "            ", "self", ".", "enc", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "\"gpt2-large\"", ")", "\n", "self", ".", "model", "=", "GPT2LMHeadModel", ".", "from_pretrained", "(", "\"gpt2-large\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "enc", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "location", ")", "\n", "self", ".", "model", "=", "GPT2LMHeadModel", ".", "from_pretrained", "(", "location", ")", "\n", "", "self", ".", "device", "=", "torch", ".", "device", "(", "device", ")", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "self", ".", "start_tok", "=", "\"<|endoftext|>\"", "\n", "#SPECIAL_TOKENS = [\"<pad>\"]", "\n", "#self.enc.add_special_tokens(SPECIAL_TOKENS)", "\n", "#self.model.set_num_special_tokens(len(SPECIAL_TOKENS))", "\n", "self", ".", "model", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.gpt2.GPT2.pad": [[26, 39], ["max", "range", "len", "range", "len", "context[].append", "len"], "methods", ["None"], ["", "def", "pad", "(", "self", ",", "context", ")", ":", "\n", "        ", "max_len", "=", "max", "(", "[", "len", "(", "sentence", ")", "for", "sentence", "in", "context", "]", ")", "\n", "#print(\"Maximum Length: \", max_len)", "\n", "for", "i", "in", "range", "(", "len", "(", "context", ")", ")", ":", "\n", "#print(len(context[i]), max_len - len(context[i]))", "\n", "            ", "for", "j", "in", "range", "(", "max_len", "-", "len", "(", "context", "[", "i", "]", ")", ")", ":", "\n", "                ", "context", "[", "i", "]", ".", "append", "(", "context", "[", "i", "]", "[", "0", "]", ")", "\n", "#print(max_len - len(sentences[i].split()))", "\n", "#print(\"i: \", sentences[i])", "\n", "#print([[self.enc.encode(\"<pad>\") for idx in range(max_len - len(in_text[i].split()))]  for i in range(len(in_text))])", "\n", "#print(sentences)", "\n", "#print([len(context[i]) for i in range(len(context))])", "\n", "", "", "return", "context", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.gpt2.GPT2.get_probabilities": [[40, 88], ["torch.no_grad", "torch.full", "gpt2.GPT2.pad", "torch.tensor", "gpt2.GPT2.model", "torch.softmax", "torch.cuda.empty_cache", "gpt2.GPT2.enc.encode", "[].data.cpu().numpy().tolist", "list", "list", "range", "range", "range", "map", "range", "range", "range", "gpt2.GPT2.postprocess", "range", "len", "len", "[].data.cpu().numpy", "len", "len", "len", "len", "round", "s.item", "[].data.cpu", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.gpt2.GPT2.pad", "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.gpt2.GPT2.postprocess"], ["", "def", "get_probabilities", "(", "self", ",", "in_text", ",", "topk", "=", "40", ")", ":", "\n", "        ", "\"\"\"\n        Take in a sequence of text tokens, make predictions on each word given past context and \n        return topk\n        \n        Returns:\n            Dictionary \"payload\" containing:\n            real_probs\n                - List of tuples, one for each token in sequence\n                - Probability of the actual words in the sequence\n                - Each tuple of the form (position of next word in prediction, predicted probability)\n            \n            context_strings:\n                - Strings in the sequence along with start token\n        \"\"\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "start_tok", "=", "torch", ".", "full", "(", "(", "1", ",", "1", ")", ",", "self", ".", "enc", ".", "encoder", "[", "self", ".", "start_tok", "]", ",", "\n", "device", "=", "self", ".", "device", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "context", "=", "[", "self", ".", "start_tok", "+", "\" \"", "+", "in_text", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "in_text", ")", ")", "]", "\n", "context", "=", "[", "self", ".", "enc", ".", "encode", "(", "context", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "context", ")", ")", "]", "\n", "context", "=", "self", ".", "pad", "(", "context", ")", "\n", "context", "=", "torch", ".", "tensor", "(", "context", ",", "device", "=", "self", ".", "device", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "logits", ",", "_", "=", "self", ".", "model", "(", "context", ")", "\n", "yhat", "=", "torch", ".", "softmax", "(", "logits", "[", ":", ",", ":", "-", "1", "]", ",", "dim", "=", "-", "1", ")", "\n", "y", "=", "context", "[", ":", ",", "1", ":", "]", "\n", "real_topk_probs", "=", "[", "yhat", "[", "t", "]", "[", "np", ".", "arange", "(", "0", ",", "y", "[", "t", "]", ".", "shape", "[", "0", "]", ",", "1", ")", ",", "y", "[", "t", "]", "]", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "for", "t", "in", "range", "(", "yhat", ".", "shape", "[", "0", "]", ")", "]", "\n", "real_topk_probs", "=", "[", "list", "(", "map", "(", "lambda", "x", ":", "round", "(", "x", ",", "15", ")", ",", "real_topk_probs", "[", "t", "]", ")", ")", "for", "t", "in", "range", "(", "len", "(", "real_topk_probs", ")", ")", "]", "\n", "\n", "real_topk", "=", "[", "list", "(", "real_topk_probs", "[", "t", "]", ")", "for", "t", "in", "range", "(", "len", "(", "real_topk_probs", ")", ")", "]", "\n", "\n", "context_strings", "=", "[", "[", "self", ".", "enc", ".", "decoder", "[", "s", ".", "item", "(", ")", "]", "for", "s", "in", "context", "[", "t", "]", "]", "for", "t", "in", "range", "(", "len", "(", "context", ")", ")", "]", "\n", "context_strings", "=", "[", "[", "self", ".", "postprocess", "(", "s", ")", "for", "s", "in", "context_strings", "[", "t", "]", "]", "for", "t", "in", "range", "(", "len", "(", "context_strings", ")", ")", "]", "\n", "del", "context", ",", "logits", ",", "y", ",", "yhat", ",", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "", "\"\"\" \n        pred_topk = [[list(zip([self.enc.decoder[p] for p in sorted_preds[t][i][:topk]],\n            list(map(lambda x: round(x, 5),yhat[t][i][sorted_preds[t][i][\n                :topk]].data.cpu().numpy().tolist()))))\n                    for i in range(y[t].shape[0])] for t in range(y.shape[0])]\n        pred_topk = [[[(self.postprocess(t[0]), t[1]) for t in pred] for pred in pred_topk[t]] for t in range(len(pred_topk))]\n        \"\"\"", "\n", "payload", "=", "{", "'context_strings'", ":", "context_strings", ",", "\n", "'real_probs'", ":", "real_topk", "}", "#, 'pred_topk': pred_topk}", "\n", "\n", "#del context, logits, y, yhat, ", "\n", "#torch.cuda.empty_cache()", "\n", "#code.interact(local=locals())", "\n", "return", "payload", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.gpt2.GPT2.postprocess": [[89, 114], ["token.startswith", "token.startswith", "token.startswith", "token.startswith", "token.startswith", "token.startswith", "len"], "methods", ["None"], ["", "def", "postprocess", "(", "self", ",", "token", ")", ":", "\n", "        ", "with_space", "=", "False", "\n", "with_break", "=", "False", "\n", "#print(token, token[0], token[1:]),", "\n", "if", "token", "[", "0", "]", "==", "'\u0120'", ":", "\n", "            ", "with_space", "=", "True", "\n", "token", "=", "token", "[", "1", ":", "]", "\n", "", "elif", "token", ".", "startswith", "(", "'\u00e2'", ")", ":", "\n", "            ", "token", "=", "' '", "\n", "", "elif", "token", ".", "startswith", "(", "'\u010a'", ")", ":", "\n", "            ", "token", "=", "' '", "\n", "with_break", "=", "True", "\n", "\n", "", "if", "len", "(", "token", ")", ">", "0", "and", "token", "[", "0", "]", "==", "\"\u00c2\"", ":", "\n", "            ", "token", "=", "token", "[", "1", ":", "]", "\n", "", "token", "=", "'-'", "if", "token", ".", "startswith", "(", "'\u00e2'", ")", "else", "token", "\n", "token", "=", "'\u201c'", "if", "token", ".", "startswith", "(", "'\u013e'", ")", "else", "token", "\n", "token", "=", "'\u201d'", "if", "token", ".", "startswith", "(", "'\u013f'", ")", "else", "token", "\n", "token", "=", "\"'\"", "if", "token", ".", "startswith", "(", "'\u013b'", ")", "else", "token", "\n", "#if with_space:", "\n", "#    token = '\\u0120' + token", "\n", "#if with_break:", "\n", "#    token = '\\u010A' + token", "\n", "#print(token)", "\n", "return", "token", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.TextDataset.__init__": [[67, 99], ["os.path.isfile", "os.path.split", "os.path.join", "os.path.exists", "logger.info", "logger.info", "tokenizer.convert_tokens_to_ids", "range", "logger.info", "open", "pickle.load", "open", "f.read", "tokenizer.tokenize", "run_language_modeling.TextDataset.examples.append", "open", "pickle.dump", "tokenizer.build_inputs_with_special_tokens", "str", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "tokenizer", ":", "PreTrainedTokenizer", ",", "args", ",", "file_path", ":", "str", ",", "block_size", "=", "512", ")", ":", "\n", "        ", "assert", "os", ".", "path", ".", "isfile", "(", "file_path", ")", "\n", "\n", "block_size", "=", "block_size", "-", "(", "tokenizer", ".", "max_len", "-", "tokenizer", ".", "max_len_single_sentence", ")", "\n", "\n", "directory", ",", "filename", "=", "os", ".", "path", ".", "split", "(", "file_path", ")", "\n", "cached_features_file", "=", "os", ".", "path", ".", "join", "(", "\n", "directory", ",", "args", ".", "model_type", "+", "\"_cached_lm_\"", "+", "str", "(", "block_size", ")", "+", "\"_\"", "+", "filename", "\n", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "cached_features_file", ")", "and", "not", "args", ".", "overwrite_cache", ":", "\n", "            ", "logger", ".", "info", "(", "\"Loading features from cached file %s\"", ",", "cached_features_file", ")", "\n", "with", "open", "(", "cached_features_file", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "                ", "self", ".", "examples", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"Creating features from dataset file at %s\"", ",", "directory", ")", "\n", "\n", "self", ".", "examples", "=", "[", "]", "\n", "with", "open", "(", "file_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "text", "=", "f", ".", "read", "(", ")", "\n", "\n", "", "tokenized_text", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenizer", ".", "tokenize", "(", "text", ")", ")", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "tokenized_text", ")", "-", "block_size", "+", "1", ",", "block_size", ")", ":", "# Truncate in block of block_size", "\n", "                ", "self", ".", "examples", ".", "append", "(", "tokenizer", ".", "build_inputs_with_special_tokens", "(", "tokenized_text", "[", "i", ":", "i", "+", "block_size", "]", ")", ")", "\n", "# Note that we are loosing the last truncated example here for the sake of simplicity (no padding)", "\n", "# If your dataset is small, first you should loook for a bigger one :-) and second you", "\n", "# can change this behavior by adding (model specific) padding.", "\n", "\n", "", "logger", ".", "info", "(", "\"Saving features into cached file %s\"", ",", "cached_features_file", ")", "\n", "with", "open", "(", "cached_features_file", ",", "\"wb\"", ")", "as", "handle", ":", "\n", "                ", "pickle", ".", "dump", "(", "self", ".", "examples", ",", "handle", ",", "protocol", "=", "pickle", ".", "HIGHEST_PROTOCOL", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.TextDataset.__len__": [[100, 102], ["len"], "methods", ["None"], ["", "", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "examples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.TextDataset.__getitem__": [[103, 105], ["torch.tensor"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "        ", "return", "torch", ".", "tensor", "(", "self", ".", "examples", "[", "item", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.LineByLineTextDataset.__init__": [[108, 119], ["os.path.isfile", "logger.info", "open", "tokenizer.batch_encode_plus", "f.read().splitlines", "f.read", "len", "line.isspace"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "tokenizer", ":", "PreTrainedTokenizer", ",", "args", ",", "file_path", ":", "str", ",", "block_size", "=", "512", ")", ":", "\n", "        ", "assert", "os", ".", "path", ".", "isfile", "(", "file_path", ")", "\n", "# Here, we do not cache the features, operating under the assumption", "\n", "# that we will soon use fast multithreaded tokenizers from the", "\n", "# `tokenizers` repo everywhere =)", "\n", "logger", ".", "info", "(", "\"Creating features from dataset file at %s\"", ",", "file_path", ")", "\n", "\n", "with", "open", "(", "file_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "lines", "=", "[", "line", "for", "line", "in", "f", ".", "read", "(", ")", ".", "splitlines", "(", ")", "if", "(", "len", "(", "line", ")", ">", "0", "and", "not", "line", ".", "isspace", "(", ")", ")", "]", "\n", "\n", "", "self", ".", "examples", "=", "tokenizer", ".", "batch_encode_plus", "(", "lines", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "block_size", ")", "[", "\"input_ids\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.LineByLineTextDataset.__len__": [[120, 122], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "examples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.LineByLineTextDataset.__getitem__": [[123, 125], ["torch.tensor"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "torch", ".", "tensor", "(", "self", ".", "examples", "[", "i", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.load_and_cache_examples": [[127, 133], ["run_language_modeling.LineByLineTextDataset", "run_language_modeling.TextDataset"], "function", ["None"], ["", "", "def", "load_and_cache_examples", "(", "args", ",", "tokenizer", ",", "evaluate", "=", "False", ")", ":", "\n", "    ", "file_path", "=", "args", ".", "eval_data_file", "if", "evaluate", "else", "args", ".", "train_data_file", "\n", "if", "args", ".", "line_by_line", ":", "\n", "        ", "return", "LineByLineTextDataset", "(", "tokenizer", ",", "args", ",", "file_path", "=", "file_path", ",", "block_size", "=", "args", ".", "block_size", ")", "\n", "", "else", ":", "\n", "        ", "return", "TextDataset", "(", "tokenizer", ",", "args", ",", "file_path", "=", "file_path", ",", "block_size", "=", "args", ".", "block_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.set_seed": [[135, 141], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all"], "function", ["None"], ["", "", "def", "set_seed", "(", "args", ")", ":", "\n", "    ", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "args", ".", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling._sorted_checkpoints": [[143, 159], ["glob.glob", "sorted", "os.path.join", "ordering_and_checkpoint_path.append", "re.match", "re.match.groups", "ordering_and_checkpoint_path.append", "os.path.getmtime", "int", "re.match.groups"], "function", ["None"], ["", "", "def", "_sorted_checkpoints", "(", "args", ",", "checkpoint_prefix", "=", "\"checkpoint\"", ",", "use_mtime", "=", "False", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "ordering_and_checkpoint_path", "=", "[", "]", "\n", "\n", "glob_checkpoints", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"{}-*\"", ".", "format", "(", "checkpoint_prefix", ")", ")", ")", "\n", "\n", "for", "path", "in", "glob_checkpoints", ":", "\n", "        ", "if", "use_mtime", ":", "\n", "            ", "ordering_and_checkpoint_path", ".", "append", "(", "(", "os", ".", "path", ".", "getmtime", "(", "path", ")", ",", "path", ")", ")", "\n", "", "else", ":", "\n", "            ", "regex_match", "=", "re", ".", "match", "(", "\".*{}-([0-9]+)\"", ".", "format", "(", "checkpoint_prefix", ")", ",", "path", ")", "\n", "if", "regex_match", "and", "regex_match", ".", "groups", "(", ")", ":", "\n", "                ", "ordering_and_checkpoint_path", ".", "append", "(", "(", "int", "(", "regex_match", ".", "groups", "(", ")", "[", "0", "]", ")", ",", "path", ")", ")", "\n", "\n", "", "", "", "checkpoints_sorted", "=", "sorted", "(", "ordering_and_checkpoint_path", ")", "\n", "checkpoints_sorted", "=", "[", "checkpoint", "[", "1", "]", "for", "checkpoint", "in", "checkpoints_sorted", "]", "\n", "return", "checkpoints_sorted", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling._rotate_checkpoints": [[161, 177], ["run_language_modeling._sorted_checkpoints", "max", "len", "logger.info", "shutil.rmtree", "len"], "function", ["home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling._sorted_checkpoints"], ["", "def", "_rotate_checkpoints", "(", "args", ",", "checkpoint_prefix", "=", "\"checkpoint\"", ",", "use_mtime", "=", "False", ")", "->", "None", ":", "\n", "    ", "if", "not", "args", ".", "save_total_limit", ":", "\n", "        ", "return", "\n", "", "if", "args", ".", "save_total_limit", "<=", "0", ":", "\n", "        ", "return", "\n", "\n", "# Check if we should delete older checkpoint(s)", "\n", "", "checkpoints_sorted", "=", "_sorted_checkpoints", "(", "args", ",", "checkpoint_prefix", ",", "use_mtime", ")", "\n", "if", "len", "(", "checkpoints_sorted", ")", "<=", "args", ".", "save_total_limit", ":", "\n", "        ", "return", "\n", "\n", "", "number_of_checkpoints_to_delete", "=", "max", "(", "0", ",", "len", "(", "checkpoints_sorted", ")", "-", "args", ".", "save_total_limit", ")", "\n", "checkpoints_to_be_deleted", "=", "checkpoints_sorted", "[", ":", "number_of_checkpoints_to_delete", "]", "\n", "for", "checkpoint", "in", "checkpoints_to_be_deleted", ":", "\n", "        ", "logger", ".", "info", "(", "\"Deleting older checkpoint [{}] due to args.save_total_limit\"", ".", "format", "(", "checkpoint", ")", ")", "\n", "shutil", ".", "rmtree", "(", "checkpoint", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.mask_tokens": [[179, 211], ["inputs.clone", "torch.full", "torch.full.masked_fill_", "torch.bernoulli().bool", "tokenizer.convert_tokens_to_ids", "torch.randint", "ValueError", "tokenizer.get_special_tokens_mask", "torch.tensor", "inputs.clone.eq", "torch.full.masked_fill_", "torch.bernoulli().bool", "len", "inputs.clone.tolist", "torch.bernoulli", "torch.bernoulli().bool", "torch.bernoulli", "torch.full", "torch.bernoulli", "torch.full"], "function", ["None"], ["", "", "def", "mask_tokens", "(", "inputs", ":", "torch", ".", "Tensor", ",", "tokenizer", ":", "PreTrainedTokenizer", ",", "args", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "    ", "\"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"", "\n", "\n", "if", "tokenizer", ".", "mask_token", "is", "None", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.\"", "\n", ")", "\n", "\n", "", "labels", "=", "inputs", ".", "clone", "(", ")", "\n", "# We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)", "\n", "probability_matrix", "=", "torch", ".", "full", "(", "labels", ".", "shape", ",", "args", ".", "mlm_probability", ")", "\n", "special_tokens_mask", "=", "[", "\n", "tokenizer", ".", "get_special_tokens_mask", "(", "val", ",", "already_has_special_tokens", "=", "True", ")", "for", "val", "in", "labels", ".", "tolist", "(", ")", "\n", "]", "\n", "probability_matrix", ".", "masked_fill_", "(", "torch", ".", "tensor", "(", "special_tokens_mask", ",", "dtype", "=", "torch", ".", "bool", ")", ",", "value", "=", "0.0", ")", "\n", "if", "tokenizer", ".", "_pad_token", "is", "not", "None", ":", "\n", "        ", "padding_mask", "=", "labels", ".", "eq", "(", "tokenizer", ".", "pad_token_id", ")", "\n", "probability_matrix", ".", "masked_fill_", "(", "padding_mask", ",", "value", "=", "0.0", ")", "\n", "", "masked_indices", "=", "torch", ".", "bernoulli", "(", "probability_matrix", ")", ".", "bool", "(", ")", "\n", "labels", "[", "~", "masked_indices", "]", "=", "-", "100", "# We only compute loss on masked tokens", "\n", "\n", "# 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])", "\n", "indices_replaced", "=", "torch", ".", "bernoulli", "(", "torch", ".", "full", "(", "labels", ".", "shape", ",", "0.8", ")", ")", ".", "bool", "(", ")", "&", "masked_indices", "\n", "inputs", "[", "indices_replaced", "]", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenizer", ".", "mask_token", ")", "\n", "\n", "# 10% of the time, we replace masked input tokens with random word", "\n", "indices_random", "=", "torch", ".", "bernoulli", "(", "torch", ".", "full", "(", "labels", ".", "shape", ",", "0.5", ")", ")", ".", "bool", "(", ")", "&", "masked_indices", "&", "~", "indices_replaced", "\n", "random_words", "=", "torch", ".", "randint", "(", "len", "(", "tokenizer", ")", ",", "labels", ".", "shape", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "inputs", "[", "indices_random", "]", "=", "random_words", "[", "indices_random", "]", "\n", "\n", "# The rest of the time (10% of the time) we keep the masked input tokens unchanged", "\n", "return", "inputs", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.train": [[213, 405], ["torch.utils.data.DataLoader", "torch.nn.parallel.DistributedDataParallel.resize_token_embeddings", "transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "torch.nn.parallel.DistributedDataParallel.zero_grad", "tqdm.trange", "run_language_modeling.set_seed", "SummaryWriter", "max", "torch.nn.utils.rnn.pad_sequence", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "hasattr", "len", "os.path.isfile", "os.path.isfile", "transformers.AdamW.load_state_dict", "transformers.get_linear_schedule_with_warmup.load_state_dict", "amp.initialize", "torch.nn.DataParallel", "torch.nn.parallel.DistributedDataParallel", "len", "os.path.exists", "int", "tqdm.tqdm", "enumerate", "SummaryWriter.close", "torch.nn.utils.rnn.pad_sequence", "os.path.join", "os.path.join", "torch.load", "torch.load", "int", "logger.info", "logger.info", "logger.info", "logger.info", "train_sampler.set_epoch", "inputs.to.to", "labels.to.to", "torch.nn.parallel.DistributedDataParallel.train", "loss.mean.item", "tqdm.trange.close", "len", "os.path.join", "os.path.join", "ImportError", "torch.distributed.get_world_size", "[].split", "logger.info", "run_language_modeling.mask_tokens", "torch.nn.parallel.DistributedDataParallel.", "torch.nn.parallel.DistributedDataParallel.", "loss.mean.mean", "loss.mean.backward", "transformers.AdamW.step", "transformers.get_linear_schedule_with_warmup.step", "torch.nn.parallel.DistributedDataParallel.zero_grad", "tqdm.tqdm.close", "len", "torch.nn.parallel.DistributedDataParallel.named_parameters", "torch.nn.parallel.DistributedDataParallel.named_parameters", "any", "len", "len", "amp.scale_loss", "scaled_loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "SummaryWriter.add_scalar", "SummaryWriter.add_scalar", "os.path.join", "os.makedirs", "model_to_save.save_pretrained", "tokenizer.save_pretrained", "torch.save", "logger.info", "run_language_modeling._rotate_checkpoints", "torch.save", "torch.save", "logger.info", "any", "amp.master_params", "torch.nn.parallel.DistributedDataParallel.parameters", "run_language_modeling.evaluate", "evaluate.items", "hasattr", "os.path.join", "transformers.AdamW.state_dict", "os.path.join", "transformers.get_linear_schedule_with_warmup.state_dict", "os.path.join", "args.model_name_or_path.split", "SummaryWriter.add_scalar", "transformers.get_linear_schedule_with_warmup.get_lr"], "function", ["home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.set_seed", "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.train", "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.mask_tokens", "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling._rotate_checkpoints", "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.evaluate"], ["", "def", "train", "(", "args", ",", "train_dataset", ",", "model", ":", "PreTrainedModel", ",", "tokenizer", ":", "PreTrainedTokenizer", ")", "->", "Tuple", "[", "int", ",", "float", "]", ":", "\n", "    ", "\"\"\" Train the model \"\"\"", "\n", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "        ", "tb_writer", "=", "SummaryWriter", "(", ")", "\n", "\n", "", "args", ".", "train_batch_size", "=", "args", ".", "per_gpu_train_batch_size", "*", "max", "(", "1", ",", "args", ".", "n_gpu", ")", "\n", "\n", "def", "collate", "(", "examples", ":", "List", "[", "torch", ".", "Tensor", "]", ")", ":", "\n", "        ", "if", "tokenizer", ".", "_pad_token", "is", "None", ":", "\n", "            ", "return", "pad_sequence", "(", "examples", ",", "batch_first", "=", "True", ")", "\n", "", "return", "pad_sequence", "(", "examples", ",", "batch_first", "=", "True", ",", "padding_value", "=", "tokenizer", ".", "pad_token_id", ")", "\n", "\n", "", "train_sampler", "=", "RandomSampler", "(", "train_dataset", ")", "if", "args", ".", "local_rank", "==", "-", "1", "else", "DistributedSampler", "(", "train_dataset", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "\n", "train_dataset", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "args", ".", "train_batch_size", ",", "collate_fn", "=", "collate", "\n", ")", "\n", "\n", "if", "args", ".", "max_steps", ">", "0", ":", "\n", "        ", "t_total", "=", "args", ".", "max_steps", "\n", "args", ".", "num_train_epochs", "=", "args", ".", "max_steps", "//", "(", "len", "(", "train_dataloader", ")", "//", "args", ".", "gradient_accumulation_steps", ")", "+", "1", "\n", "", "else", ":", "\n", "        ", "t_total", "=", "len", "(", "train_dataloader", ")", "//", "args", ".", "gradient_accumulation_steps", "*", "args", ".", "num_train_epochs", "\n", "\n", "", "model", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "\"module\"", ")", "else", "model", "# Take care of distributed/parallel training", "\n", "model", ".", "resize_token_embeddings", "(", "len", "(", "tokenizer", ")", ")", "\n", "\n", "# Prepare optimizer and schedule (linear warmup and decay)", "\n", "no_decay", "=", "[", "\"bias\"", ",", "\"LayerNorm.weight\"", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "\"weight_decay\"", ":", "args", ".", "weight_decay", ",", "\n", "}", ",", "\n", "{", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\"weight_decay\"", ":", "0.0", "}", ",", "\n", "]", "\n", "optimizer", "=", "AdamW", "(", "optimizer_grouped_parameters", ",", "lr", "=", "args", ".", "learning_rate", ",", "eps", "=", "args", ".", "adam_epsilon", ")", "\n", "scheduler", "=", "get_linear_schedule_with_warmup", "(", "\n", "optimizer", ",", "num_warmup_steps", "=", "args", ".", "warmup_steps", ",", "num_training_steps", "=", "t_total", "\n", ")", "\n", "\n", "# Check if saved optimizer or scheduler states exist", "\n", "if", "(", "\n", "args", ".", "model_name_or_path", "\n", "and", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_name_or_path", ",", "\"optimizer.pt\"", ")", ")", "\n", "and", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_name_or_path", ",", "\"scheduler.pt\"", ")", ")", "\n", ")", ":", "\n", "# Load in optimizer and scheduler states", "\n", "        ", "optimizer", ".", "load_state_dict", "(", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_name_or_path", ",", "\"optimizer.pt\"", ")", ")", ")", "\n", "scheduler", ".", "load_state_dict", "(", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_name_or_path", ",", "\"scheduler.pt\"", ")", ")", ")", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", "import", "amp", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"", ")", "\n", "", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "optimizer", ",", "opt_level", "=", "args", ".", "fp16_opt_level", ")", "\n", "\n", "# multi-gpu training (should be after apex fp16 initialization)", "\n", "", "if", "args", ".", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "# Distributed training (should be after apex fp16 initialization)", "\n", "", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "\n", "model", ",", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "output_device", "=", "args", ".", "local_rank", ",", "find_unused_parameters", "=", "True", "\n", ")", "\n", "\n", "# Train!", "\n", "", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_dataset", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num Epochs = %d\"", ",", "args", ".", "num_train_epochs", ")", "\n", "logger", ".", "info", "(", "\"  Instantaneous batch size per GPU = %d\"", ",", "args", ".", "per_gpu_train_batch_size", ")", "\n", "logger", ".", "info", "(", "\n", "\"  Total train batch size (w. parallel, distributed & accumulation) = %d\"", ",", "\n", "args", ".", "train_batch_size", "\n", "*", "args", ".", "gradient_accumulation_steps", "\n", "*", "(", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "if", "args", ".", "local_rank", "!=", "-", "1", "else", "1", ")", ",", "\n", ")", "\n", "logger", ".", "info", "(", "\"  Gradient Accumulation steps = %d\"", ",", "args", ".", "gradient_accumulation_steps", ")", "\n", "logger", ".", "info", "(", "\"  Total optimization steps = %d\"", ",", "t_total", ")", "\n", "\n", "global_step", "=", "0", "\n", "epochs_trained", "=", "0", "\n", "steps_trained_in_current_epoch", "=", "0", "\n", "# Check if continuing training from a checkpoint", "\n", "if", "args", ".", "model_name_or_path", "and", "os", ".", "path", ".", "exists", "(", "args", ".", "model_name_or_path", ")", ":", "\n", "        ", "try", ":", "\n", "# set global_step to gobal_step of last saved checkpoint from model path", "\n", "            ", "checkpoint_suffix", "=", "args", ".", "model_name_or_path", ".", "split", "(", "\"-\"", ")", "[", "-", "1", "]", ".", "split", "(", "\"/\"", ")", "[", "0", "]", "\n", "global_step", "=", "int", "(", "checkpoint_suffix", ")", "\n", "epochs_trained", "=", "global_step", "//", "(", "len", "(", "train_dataloader", ")", "//", "args", ".", "gradient_accumulation_steps", ")", "\n", "steps_trained_in_current_epoch", "=", "global_step", "%", "(", "len", "(", "train_dataloader", ")", "//", "args", ".", "gradient_accumulation_steps", ")", "\n", "\n", "logger", ".", "info", "(", "\"  Continuing training from checkpoint, will skip to saved global_step\"", ")", "\n", "logger", ".", "info", "(", "\"  Continuing training from epoch %d\"", ",", "epochs_trained", ")", "\n", "logger", ".", "info", "(", "\"  Continuing training from global step %d\"", ",", "global_step", ")", "\n", "logger", ".", "info", "(", "\"  Will skip the first %d steps in the first epoch\"", ",", "steps_trained_in_current_epoch", ")", "\n", "", "except", "ValueError", ":", "\n", "            ", "logger", ".", "info", "(", "\"  Starting fine-tuning.\"", ")", "\n", "\n", "", "", "tr_loss", ",", "logging_loss", "=", "0.0", ",", "0.0", "\n", "\n", "model", ".", "zero_grad", "(", ")", "\n", "train_iterator", "=", "trange", "(", "\n", "epochs_trained", ",", "int", "(", "args", ".", "num_train_epochs", ")", ",", "desc", "=", "\"Epoch\"", ",", "disable", "=", "args", ".", "local_rank", "not", "in", "[", "-", "1", ",", "0", "]", "\n", ")", "\n", "set_seed", "(", "args", ")", "# Added here for reproducibility", "\n", "for", "epoch", "in", "train_iterator", ":", "\n", "        ", "epoch_iterator", "=", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Iteration\"", ",", "disable", "=", "args", ".", "local_rank", "not", "in", "[", "-", "1", ",", "0", "]", ")", "\n", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "            ", "train_sampler", ".", "set_epoch", "(", "epoch", ")", "\n", "\n", "", "for", "step", ",", "batch", "in", "enumerate", "(", "epoch_iterator", ")", ":", "\n", "\n", "# Skip past any already trained steps if resuming training", "\n", "            ", "if", "steps_trained_in_current_epoch", ">", "0", ":", "\n", "                ", "steps_trained_in_current_epoch", "-=", "1", "\n", "continue", "\n", "\n", "", "inputs", ",", "labels", "=", "mask_tokens", "(", "batch", ",", "tokenizer", ",", "args", ")", "if", "args", ".", "mlm", "else", "(", "batch", ",", "batch", ")", "\n", "inputs", "=", "inputs", ".", "to", "(", "args", ".", "device", ")", "\n", "labels", "=", "labels", ".", "to", "(", "args", ".", "device", ")", "\n", "model", ".", "train", "(", ")", "\n", "outputs", "=", "model", "(", "inputs", ",", "masked_lm_labels", "=", "labels", ")", "if", "args", ".", "mlm", "else", "model", "(", "inputs", ",", "labels", "=", "labels", ")", "\n", "loss", "=", "outputs", "[", "0", "]", "# model outputs are always tuple in transformers (see doc)", "\n", "\n", "if", "args", ".", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu parallel training", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                ", "with", "amp", ".", "scale_loss", "(", "loss", ",", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                    ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "", "else", ":", "\n", "                ", "loss", ".", "backward", "(", ")", "\n", "\n", "", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "if", "args", ".", "fp16", ":", "\n", "                    ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "amp", ".", "master_params", "(", "optimizer", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "", "else", ":", "\n", "                    ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", ")", "# Update learning rate schedule", "\n", "model", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", "and", "args", ".", "logging_steps", ">", "0", "and", "global_step", "%", "args", ".", "logging_steps", "==", "0", ":", "\n", "# Log metrics", "\n", "                    ", "if", "(", "\n", "args", ".", "local_rank", "==", "-", "1", "and", "args", ".", "evaluate_during_training", "\n", ")", ":", "# Only evaluate when single GPU otherwise metrics may not average well", "\n", "                        ", "results", "=", "evaluate", "(", "args", ",", "model", ",", "tokenizer", ")", "\n", "for", "key", ",", "value", "in", "results", ".", "items", "(", ")", ":", "\n", "                            ", "tb_writer", ".", "add_scalar", "(", "\"eval_{}\"", ".", "format", "(", "key", ")", ",", "value", ",", "global_step", ")", "\n", "", "", "tb_writer", ".", "add_scalar", "(", "\"lr\"", ",", "scheduler", ".", "get_lr", "(", ")", "[", "0", "]", ",", "global_step", ")", "\n", "tb_writer", ".", "add_scalar", "(", "\"loss\"", ",", "(", "tr_loss", "-", "logging_loss", ")", "/", "args", ".", "logging_steps", ",", "global_step", ")", "\n", "logging_loss", "=", "tr_loss", "\n", "\n", "", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", "and", "args", ".", "save_steps", ">", "0", "and", "global_step", "%", "args", ".", "save_steps", "==", "0", ":", "\n", "                    ", "checkpoint_prefix", "=", "\"checkpoint\"", "\n", "# Save model checkpoint", "\n", "output_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"{}-{}\"", ".", "format", "(", "checkpoint_prefix", ",", "global_step", ")", ")", "\n", "os", ".", "makedirs", "(", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "model_to_save", "=", "(", "\n", "model", ".", "module", "if", "hasattr", "(", "model", ",", "\"module\"", ")", "else", "model", "\n", ")", "# Take care of distributed/parallel training", "\n", "model_to_save", ".", "save_pretrained", "(", "output_dir", ")", "\n", "tokenizer", ".", "save_pretrained", "(", "output_dir", ")", "\n", "\n", "torch", ".", "save", "(", "args", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"training_args.bin\"", ")", ")", "\n", "logger", ".", "info", "(", "\"Saving model checkpoint to %s\"", ",", "output_dir", ")", "\n", "\n", "_rotate_checkpoints", "(", "args", ",", "checkpoint_prefix", ")", "\n", "\n", "torch", ".", "save", "(", "optimizer", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"optimizer.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "scheduler", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"scheduler.pt\"", ")", ")", "\n", "logger", ".", "info", "(", "\"Saving optimizer and scheduler states to %s\"", ",", "output_dir", ")", "\n", "\n", "", "", "if", "args", ".", "max_steps", ">", "0", "and", "global_step", ">", "args", ".", "max_steps", ":", "\n", "                ", "epoch_iterator", ".", "close", "(", ")", "\n", "break", "\n", "", "", "if", "args", ".", "max_steps", ">", "0", "and", "global_step", ">", "args", ".", "max_steps", ":", "\n", "            ", "train_iterator", ".", "close", "(", ")", "\n", "break", "\n", "\n", "", "", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "        ", "tb_writer", ".", "close", "(", ")", "\n", "\n", "", "return", "global_step", ",", "tr_loss", "/", "global_step", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.evaluate": [[407, 465], ["run_language_modeling.load_and_cache_examples", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "logger.info", "logger.info", "logger.info", "torch.nn.DataParallel.eval", "tqdm.tqdm", "torch.exp", "os.path.join", "os.makedirs", "max", "torch.nn.utils.rnn.pad_sequence", "torch.nn.DataParallel", "len", "inputs.to.to", "labels.to.to", "torch.tensor", "open", "logger.info", "sorted", "torch.nn.utils.rnn.pad_sequence", "run_language_modeling.mask_tokens", "torch.no_grad", "lm_loss.mean().item", "result.keys", "logger.info", "writer.write", "torch.nn.DataParallel.", "torch.nn.DataParallel.", "str", "lm_loss.mean", "str"], "function", ["home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.load_and_cache_examples", "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.mask_tokens"], ["", "def", "evaluate", "(", "args", ",", "model", ":", "PreTrainedModel", ",", "tokenizer", ":", "PreTrainedTokenizer", ",", "prefix", "=", "\"\"", ")", "->", "Dict", ":", "\n", "# Loop to handle MNLI double evaluation (matched, mis-matched)", "\n", "    ", "eval_output_dir", "=", "args", ".", "output_dir", "\n", "\n", "eval_dataset", "=", "load_and_cache_examples", "(", "args", ",", "tokenizer", ",", "evaluate", "=", "True", ")", "\n", "\n", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "        ", "os", ".", "makedirs", "(", "eval_output_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "args", ".", "eval_batch_size", "=", "args", ".", "per_gpu_eval_batch_size", "*", "max", "(", "1", ",", "args", ".", "n_gpu", ")", "\n", "# Note that DistributedSampler samples randomly", "\n", "\n", "def", "collate", "(", "examples", ":", "List", "[", "torch", ".", "Tensor", "]", ")", ":", "\n", "        ", "if", "tokenizer", ".", "_pad_token", "is", "None", ":", "\n", "            ", "return", "pad_sequence", "(", "examples", ",", "batch_first", "=", "True", ")", "\n", "", "return", "pad_sequence", "(", "examples", ",", "batch_first", "=", "True", ",", "padding_value", "=", "tokenizer", ".", "pad_token_id", ")", "\n", "\n", "", "eval_sampler", "=", "SequentialSampler", "(", "eval_dataset", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "\n", "eval_dataset", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "args", ".", "eval_batch_size", ",", "collate_fn", "=", "collate", "\n", ")", "\n", "\n", "# multi-gpu evaluate", "\n", "if", "args", ".", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "# Eval!", "\n", "", "logger", ".", "info", "(", "\"***** Running evaluation {} *****\"", ".", "format", "(", "prefix", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "eval_dataset", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "eval_loss", "=", "0.0", "\n", "nb_eval_steps", "=", "0", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "for", "batch", "in", "tqdm", "(", "eval_dataloader", ",", "desc", "=", "\"Evaluating\"", ")", ":", "\n", "        ", "inputs", ",", "labels", "=", "mask_tokens", "(", "batch", ",", "tokenizer", ",", "args", ")", "if", "args", ".", "mlm", "else", "(", "batch", ",", "batch", ")", "\n", "inputs", "=", "inputs", ".", "to", "(", "args", ".", "device", ")", "\n", "labels", "=", "labels", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "outputs", "=", "model", "(", "inputs", ",", "masked_lm_labels", "=", "labels", ")", "if", "args", ".", "mlm", "else", "model", "(", "inputs", ",", "labels", "=", "labels", ")", "\n", "lm_loss", "=", "outputs", "[", "0", "]", "\n", "eval_loss", "+=", "lm_loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "", "nb_eval_steps", "+=", "1", "\n", "\n", "", "eval_loss", "=", "eval_loss", "/", "nb_eval_steps", "\n", "perplexity", "=", "torch", ".", "exp", "(", "torch", ".", "tensor", "(", "eval_loss", ")", ")", "\n", "\n", "result", "=", "{", "\"perplexity\"", ":", "perplexity", "}", "\n", "\n", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "eval_output_dir", ",", "prefix", ",", "\"eval_results.txt\"", ")", "\n", "with", "open", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "        ", "logger", ".", "info", "(", "\"***** Eval results {} *****\"", ".", "format", "(", "prefix", ")", ")", "\n", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "\n", "", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.main": [[467, 784], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logging.basicConfig", "logger.warning", "run_language_modeling.set_seed", "AutoModelWithLMHead.from_pretrained.to", "logger.info", "ValueError", "ValueError", "run_language_modeling._sorted_checkpoints", "os.path.exists", "os.listdir", "ValueError", "print", "ptvsd.enable_attach", "ptvsd.wait_for_attach", "torch.device", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "bool", "torch.distributed.barrier", "transformers.AutoConfig.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "min", "transformers.AutoModelWithLMHead.from_pretrained", "logger.info", "transformers.AutoModelWithLMHead.from_config", "torch.distributed.barrier", "run_language_modeling.load_and_cache_examples", "run_language_modeling.train", "logger.info", "logger.info", "model_to_save.save_pretrained", "AutoTokenizer.from_pretrained.save_pretrained", "torch.save", "transformers.AutoModelWithLMHead.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "AutoModelWithLMHead.from_pretrained.to", "logger.info", "len", "ValueError", "torch.cuda.device_count", "transformers.AutoConfig.from_pretrained", "ValueError", "transformers.AutoTokenizer.from_pretrained", "ValueError", "torch.distributed.barrier", "torch.distributed.barrier", "os.makedirs", "hasattr", "os.path.join", "list", "logging.getLogger().setLevel", "transformers.AutoModelWithLMHead.from_pretrained", "AutoModelWithLMHead.from_pretrained.to", "run_language_modeling.evaluate", "dict", "results.update", "bool", "torch.distributed.get_rank", "torch.cuda.is_available", "os.path.dirname", "logging.getLogger", "len", "checkpoint.split", "checkpoint.find", "checkpoint.split", "sorted", "dict.items", "glob.glob"], "function", ["home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.set_seed", "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling._sorted_checkpoints", "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.load_and_cache_examples", "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.train", "home.repos.pwc.inspect_result.vishakhpk_mi-unsup-summ.models.run_language_modeling.evaluate"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "# Required parameters", "\n", "parser", ".", "add_argument", "(", "\n", "\"--train_data_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "help", "=", "\"The input training data file (a text file).\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--output_dir\"", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--model_type\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "help", "=", "\"The model architecture to be trained or fine-tuned.\"", ",", "\n", ")", "\n", "\n", "# Other parameters", "\n", "parser", ".", "add_argument", "(", "\n", "\"--eval_data_file\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"An optional input evaluation data file to evaluate the perplexity on (a text file).\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--line_by_line\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--should_continue\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Whether to continue from latest checkpoint in output_dir\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--model_name_or_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--mlm\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Train with masked-language modeling loss instead of language modeling.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--mlm_probability\"", ",", "type", "=", "float", ",", "default", "=", "0.15", ",", "help", "=", "\"Ratio of tokens to mask for masked language modeling loss\"", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--config_name\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Optional pretrained config name or path if not the same as model_name_or_path. If both are None, initialize a new config.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--tokenizer_name\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Optional pretrained tokenizer name or path if not the same as model_name_or_path. If both are None, initialize a new tokenizer.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--cache_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Optional directory to store the pre-trained models downloaded from s3 (instead of the default one)\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--block_size\"", ",", "\n", "default", "=", "-", "1", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Optional input sequence length after tokenization.\"", "\n", "\"The training dataset will be truncated in block of this size for training.\"", "\n", "\"Default to the model max input length for single sentence inputs (take into account special tokens).\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--evaluate_during_training\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Run evaluation during training at each logging step.\"", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--per_gpu_train_batch_size\"", ",", "default", "=", "1", ",", "type", "=", "int", ",", "help", "=", "\"Batch size per GPU/CPU for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--per_gpu_eval_batch_size\"", ",", "default", "=", "1", ",", "type", "=", "int", ",", "help", "=", "\"Batch size per GPU/CPU for evaluation.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--gradient_accumulation_steps\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "5e-5", ",", "type", "=", "float", ",", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "help", "=", "\"Weight decay if we apply some.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--adam_epsilon\"", ",", "default", "=", "1e-8", ",", "type", "=", "float", ",", "help", "=", "\"Epsilon for Adam optimizer.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_grad_norm\"", ",", "default", "=", "1.0", ",", "type", "=", "float", ",", "help", "=", "\"Max gradient norm.\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--num_train_epochs\"", ",", "default", "=", "1.0", ",", "type", "=", "float", ",", "help", "=", "\"Total number of training epochs to perform.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--max_steps\"", ",", "\n", "default", "=", "-", "1", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"If > 0: set total number of training steps to perform. Override num_train_epochs.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_steps\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "help", "=", "\"Linear warmup over warmup_steps.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--logging_steps\"", ",", "type", "=", "int", ",", "default", "=", "500", ",", "help", "=", "\"Log every X updates steps.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_steps\"", ",", "type", "=", "int", ",", "default", "=", "500", ",", "help", "=", "\"Save checkpoint every X updates steps.\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--save_total_limit\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "None", ",", "\n", "help", "=", "\"Limit the total amount of checkpoints, delete the older checkpoints in the output_dir, does not delete by default\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--eval_all_checkpoints\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Avoid using CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--overwrite_output_dir\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Overwrite the content of the output directory\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--overwrite_cache\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Overwrite the cached training and evaluation sets\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "42", ",", "help", "=", "\"random seed for initialization\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--fp16\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--fp16_opt_level\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"O1\"", ",", "\n", "help", "=", "\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"", "\n", "\"See details at https://nvidia.github.io/apex/amp.html\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "help", "=", "\"For distributed training: local_rank\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--server_ip\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"For distant debugging.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--server_port\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"For distant debugging.\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "model_type", "in", "[", "\"bert\"", ",", "\"roberta\"", ",", "\"distilbert\"", ",", "\"camembert\"", "]", "and", "not", "args", ".", "mlm", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"BERT and RoBERTa-like models do not have LM heads but masked LM heads. They must be run using the --mlm \"", "\n", "\"flag (masked language modeling).\"", "\n", ")", "\n", "", "if", "args", ".", "eval_data_file", "is", "None", "and", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"", "\n", "\"or remove the --do_eval argument.\"", "\n", ")", "\n", "", "if", "args", ".", "should_continue", ":", "\n", "        ", "sorted_checkpoints", "=", "_sorted_checkpoints", "(", "args", ")", "\n", "if", "len", "(", "sorted_checkpoints", ")", "==", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Used --should_continue but no checkpoint was found in --output_dir.\"", ")", "\n", "", "else", ":", "\n", "            ", "args", ".", "model_name_or_path", "=", "sorted_checkpoints", "[", "-", "1", "]", "\n", "\n", "", "", "if", "(", "\n", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", "\n", "and", "os", ".", "listdir", "(", "args", ".", "output_dir", ")", "\n", "and", "args", ".", "do_train", "\n", "and", "not", "args", ".", "overwrite_output_dir", "\n", "and", "not", "args", ".", "should_continue", "\n", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"", ".", "format", "(", "\n", "args", ".", "output_dir", "\n", ")", "\n", ")", "\n", "\n", "# Setup distant debugging if needed", "\n", "", "if", "args", ".", "server_ip", "and", "args", ".", "server_port", ":", "\n", "# Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script", "\n", "        ", "import", "ptvsd", "\n", "\n", "print", "(", "\"Waiting for debugger attach\"", ")", "\n", "ptvsd", ".", "enable_attach", "(", "address", "=", "(", "args", ".", "server_ip", ",", "args", ".", "server_port", ")", ",", "redirect_output", "=", "True", ")", "\n", "ptvsd", ".", "wait_for_attach", "(", ")", "\n", "\n", "# Setup CUDA, GPU & distributed training", "\n", "", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "args", ".", "n_gpu", "=", "0", "if", "args", ".", "no_cuda", "else", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "\"nccl\"", ")", "\n", "args", ".", "n_gpu", "=", "1", "\n", "", "args", ".", "device", "=", "device", "\n", "\n", "# Setup logging", "\n", "logging", ".", "basicConfig", "(", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d/%Y %H:%M:%S\"", ",", "\n", "level", "=", "logging", ".", "INFO", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", "else", "logging", ".", "WARN", ",", "\n", ")", "\n", "logger", ".", "warning", "(", "\n", "\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\"", ",", "\n", "args", ".", "local_rank", ",", "\n", "device", ",", "\n", "args", ".", "n_gpu", ",", "\n", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "\n", "args", ".", "fp16", ",", "\n", ")", "\n", "\n", "# Set seed", "\n", "set_seed", "(", "args", ")", "\n", "\n", "# Load pretrained model and tokenizer", "\n", "if", "args", ".", "local_rank", "not", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "        ", "torch", ".", "distributed", ".", "barrier", "(", ")", "# Barrier to make sure only the first process in distributed training download model & vocab", "\n", "\n", "", "if", "args", ".", "config_name", ":", "\n", "        ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "args", ".", "config_name", ",", "cache_dir", "=", "args", ".", "cache_dir", ")", "\n", "", "elif", "args", ".", "model_name_or_path", ":", "\n", "        ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "args", ".", "model_name_or_path", ",", "cache_dir", "=", "args", ".", "cache_dir", ")", "\n", "", "else", ":", "\n", "# When we release a pip version exposing CONFIG_MAPPING,", "\n", "# we can do `config = CONFIG_MAPPING[args.model_type]()`.", "\n", "        ", "raise", "ValueError", "(", "\n", "\"You are instantiating a new config instance from scratch. This is not supported, but you can do it from another script, save it,\"", "\n", "\"and load it from here, using --config_name\"", "\n", ")", "\n", "\n", "", "if", "args", ".", "tokenizer_name", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "args", ".", "tokenizer_name", ",", "cache_dir", "=", "args", ".", "cache_dir", ")", "\n", "", "elif", "args", ".", "model_name_or_path", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "args", ".", "model_name_or_path", ",", "cache_dir", "=", "args", ".", "cache_dir", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"You are instantiating a new tokenizer from scratch. This is not supported, but you can do it from another script, save it,\"", "\n", "\"and load it from here, using --tokenizer_name\"", "\n", ")", "\n", "\n", "", "if", "args", ".", "block_size", "<=", "0", ":", "\n", "        ", "args", ".", "block_size", "=", "512", "#tokenizer.max_len", "\n", "# Our input block size will be the max possible for the model", "\n", "", "else", ":", "\n", "        ", "args", ".", "block_size", "=", "min", "(", "args", ".", "block_size", ",", "tokenizer", ".", "max_len", ")", "\n", "#print(\"Block_size\", args.block_size)", "\n", "", "if", "args", ".", "model_name_or_path", ":", "\n", "        ", "model", "=", "AutoModelWithLMHead", ".", "from_pretrained", "(", "\n", "args", ".", "model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "args", ".", "model_name_or_path", ")", ",", "\n", "config", "=", "config", ",", "\n", "cache_dir", "=", "args", ".", "cache_dir", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"Training new model from scratch\"", ")", "\n", "model", "=", "AutoModelWithLMHead", ".", "from_config", "(", "config", ")", "\n", "\n", "", "model", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "        ", "torch", ".", "distributed", ".", "barrier", "(", ")", "# End of barrier to make sure only the first process in distributed training download model & vocab", "\n", "\n", "", "logger", ".", "info", "(", "\"Training/evaluation parameters %s\"", ",", "args", ")", "\n", "\n", "# Training", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "if", "args", ".", "local_rank", "not", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "            ", "torch", ".", "distributed", ".", "barrier", "(", ")", "# Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache", "\n", "\n", "", "train_dataset", "=", "load_and_cache_examples", "(", "args", ",", "tokenizer", ",", "evaluate", "=", "False", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "            ", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "\n", "", "global_step", ",", "tr_loss", "=", "train", "(", "args", ",", "train_dataset", ",", "model", ",", "tokenizer", ")", "\n", "logger", ".", "info", "(", "\" global_step = %s, average loss = %s\"", ",", "global_step", ",", "tr_loss", ")", "\n", "\n", "# Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()", "\n", "", "if", "args", ".", "do_train", "and", "(", "args", ".", "local_rank", "==", "-", "1", "or", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ")", ":", "\n", "# Create output directory if needed", "\n", "        ", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "            ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Saving model checkpoint to %s\"", ",", "args", ".", "output_dir", ")", "\n", "# Save a trained model, configuration and tokenizer using `save_pretrained()`.", "\n", "# They can then be reloaded using `from_pretrained()`", "\n", "model_to_save", "=", "(", "\n", "model", ".", "module", "if", "hasattr", "(", "model", ",", "\"module\"", ")", "else", "model", "\n", ")", "# Take care of distributed/parallel training", "\n", "model_to_save", ".", "save_pretrained", "(", "args", ".", "output_dir", ")", "\n", "tokenizer", ".", "save_pretrained", "(", "args", ".", "output_dir", ")", "\n", "\n", "# Good practice: save your training arguments together with the trained model", "\n", "torch", ".", "save", "(", "args", ",", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"training_args.bin\"", ")", ")", "\n", "\n", "# Load a trained model and vocabulary that you have fine-tuned", "\n", "model", "=", "AutoModelWithLMHead", ".", "from_pretrained", "(", "args", ".", "output_dir", ")", "\n", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "args", ".", "output_dir", ")", "\n", "model", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "# Evaluation", "\n", "", "results", "=", "{", "}", "\n", "if", "args", ".", "do_eval", "and", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "        ", "checkpoints", "=", "[", "args", ".", "output_dir", "]", "\n", "if", "args", ".", "eval_all_checkpoints", ":", "\n", "            ", "checkpoints", "=", "list", "(", "\n", "os", ".", "path", ".", "dirname", "(", "c", ")", "for", "c", "in", "sorted", "(", "glob", ".", "glob", "(", "args", ".", "output_dir", "+", "\"/**/\"", "+", "WEIGHTS_NAME", ",", "recursive", "=", "True", ")", ")", "\n", ")", "\n", "logging", ".", "getLogger", "(", "\"transformers.modeling_utils\"", ")", ".", "setLevel", "(", "logging", ".", "WARN", ")", "# Reduce logging", "\n", "", "logger", ".", "info", "(", "\"Evaluate the following checkpoints: %s\"", ",", "checkpoints", ")", "\n", "for", "checkpoint", "in", "checkpoints", ":", "\n", "            ", "global_step", "=", "checkpoint", ".", "split", "(", "\"-\"", ")", "[", "-", "1", "]", "if", "len", "(", "checkpoints", ")", ">", "1", "else", "\"\"", "\n", "prefix", "=", "checkpoint", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "if", "checkpoint", ".", "find", "(", "\"checkpoint\"", ")", "!=", "-", "1", "else", "\"\"", "\n", "\n", "model", "=", "AutoModelWithLMHead", ".", "from_pretrained", "(", "checkpoint", ")", "\n", "model", ".", "to", "(", "args", ".", "device", ")", "\n", "result", "=", "evaluate", "(", "args", ",", "model", ",", "tokenizer", ",", "prefix", "=", "prefix", ")", "\n", "result", "=", "dict", "(", "(", "k", "+", "\"_{}\"", ".", "format", "(", "global_step", ")", ",", "v", ")", "for", "k", ",", "v", "in", "result", ".", "items", "(", ")", ")", "\n", "results", ".", "update", "(", "result", ")", "\n", "\n", "", "", "return", "results", "\n", "\n"]]}