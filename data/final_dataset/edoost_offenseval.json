{"home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader_emotion_feature.DataLoader.__init__": [[16, 68], ["nltk.tokenize.TweetTokenizer", "open", "open", "open", "open", "pickle.load", "gensim.models.KeyedVectors.load_word2vec_format", "line.strip().split", "line.strip().split", "line.strip().split", "open", "pickle.load", "data_loader_emotion_feature.DataLoader._data_reader", "chars_list.extend", "enumerate", "open", "pickle.dump", "open", "pickle.dump", "line.strip", "line.strip", "line.strip", "tweet.replace", "collections.Counter().most_common", "collections.Counter"], "methods", ["home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._data_reader"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "# loading word embedding model", "\n", "        ", "try", ":", "\n", "            ", "with", "open", "(", "cfg", ".", "we_pickled_model_dir", ",", "'rb'", ")", "as", "handle", ":", "\n", "                ", "self", ".", "word_embedding_model", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "", "except", "FileNotFoundError", ":", "\n", "            ", "self", ".", "word_embedding_model", "=", "KeyedVectors", ".", "load_word2vec_format", "(", "cfg", ".", "we_model_dir", ",", "binary", "=", "False", ")", "\n", "#self.word_embedding_model = KeyedVectors.load_word2vec_format('/mnt/hdd2/ehsan/word2vec.txt', binary=False)", "\n", "with", "open", "(", "cfg", ".", "we_pickled_model_dir", ",", "'wb'", ")", "as", "handle", ":", "\n", "                ", "pickle", ".", "dump", "(", "self", ".", "word_embedding_model", ",", "handle", ",", "protocol", "=", "pickle", ".", "HIGHEST_PROTOCOL", ")", "\n", "\n", "", "", "self", ".", "tokenizer", "=", "TweetTokenizer", "(", ")", "\n", "\n", "# loading distorted black words", "\n", "self", ".", "distorted_black_words_dict", "=", "{", "}", "\n", "with", "open", "(", "cfg", ".", "distorted_black_words_dir", ")", "as", "file", ":", "\n", "            ", "for", "line", "in", "file", ":", "\n", "                ", "source", ",", "target", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "self", ".", "distorted_black_words_dict", "[", "source", "]", "=", "target", "\n", "\n", "# loading abbreviations", "\n", "", "", "self", ".", "abbr_dict", "=", "{", "}", "\n", "with", "open", "(", "cfg", ".", "abbreviations_dir", ")", "as", "file", ":", "\n", "            ", "for", "line", "in", "file", ":", "\n", "                ", "source", ",", "target", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "self", ".", "abbr_dict", "[", "source", "]", "=", "target", "\n", "\n", "# loading contractions", "\n", "", "", "self", ".", "contractions_dict", "=", "{", "}", "\n", "with", "open", "(", "cfg", ".", "contractions_dir", ")", "as", "file", ":", "\n", "            ", "for", "line", "in", "file", ":", "\n", "                ", "source", ",", "target", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "self", ".", "contractions_dict", "[", "source", "]", "=", "target", "\n", "\n", "# loading/builing chracter indices", "\n", "", "", "try", ":", "\n", "            ", "with", "open", "(", "cfg", ".", "char_indices_dir", ",", "'rb'", ")", "as", "handle", ":", "\n", "                ", "self", ".", "char_to_index", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "", "except", "FileNotFoundError", ":", "\n", "            ", "tweets", ",", "_", ",", "_", ",", "_", "=", "self", ".", "_data_reader", "(", "cfg", ".", "train_data_dir", ")", "\n", "chars_list", "=", "[", "]", "\n", "chars_list", ".", "extend", "(", "[", "char", "for", "tweet", "in", "tweets", "for", "char", "in", "tweet", ".", "replace", "(", "' '", ",", "' '", ")", "]", ")", "\n", "char_vocab_list", "=", "[", "x", "[", "0", "]", "for", "x", "in", "[", "(", "'<PAD>'", ",", "0", ")", ",", "(", "'<UNK>'", ",", "1", ")", "]", "+", "Counter", "(", "chars_list", ")", ".", "most_common", "(", "cfg", ".", "num_chars", ")", "]", "\n", "\n", "self", ".", "char_to_index", "=", "{", "}", "\n", "for", "i", ",", "char", "in", "enumerate", "(", "char_vocab_list", ")", ":", "\n", "                ", "self", ".", "char_to_index", "[", "char", "]", "=", "i", "\n", "\n", "", "with", "open", "(", "cfg", ".", "char_indices_dir", ",", "'wb'", ")", "as", "handle", ":", "\n", "                ", "pickle", ".", "dump", "(", "self", ".", "char_to_index", ",", "handle", ",", "protocol", "=", "pickle", ".", "HIGHEST_PROTOCOL", ")", "\n", "\n", "", "", "self", ".", "a_tag_to_index", "=", "{", "'NOT'", ":", "0", ",", "'OFF'", ":", "1", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader_emotion_feature.DataLoader._data_reader": [[69, 86], ["open", "tweets.append", "a_tags.append", "b_tags.append", "c_tags.append", "line.strip().split", "line.strip().split", "line.strip", "line.strip"], "methods", ["None"], ["", "def", "_data_reader", "(", "self", ",", "file_path", ")", ":", "\n", "        ", "tweets", ",", "a_tags", ",", "b_tags", ",", "c_tags", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "with", "open", "(", "file_path", ")", "as", "file", ":", "\n", "            ", "for", "line", "in", "file", ":", "\n", "                ", "try", ":", "\n", "                    ", "_", ",", "tweet", ",", "a_tag", ",", "b_tag", ",", "c_tag", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "", "except", "ValueError", ":", "\n", "#continue", "\n", "                    ", "tweet", ",", "a_tag", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "b_tag", ",", "c_tag", "=", "'NA'", ",", "'NA'", "\n", "\n", "", "tweets", ".", "append", "(", "tweet", ")", "\n", "a_tags", ".", "append", "(", "a_tag", ")", "\n", "b_tags", ".", "append", "(", "b_tag", ")", "\n", "c_tags", ".", "append", "(", "c_tag", ")", "\n", "\n", "", "", "return", "tweets", ",", "a_tags", ",", "b_tags", ",", "c_tags", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader_emotion_feature.DataLoader._preprocessor": [[87, 101], ["re.sub", "tweet.replace.replace.strip().replace", "tweet.replace.replace.replace", "tweet.replace.replace.strip"], "methods", ["None"], ["", "def", "_preprocessor", "(", "self", ",", "tweet", ")", ":", "\n", "        ", "for", "distorted_black_word", "in", "self", ".", "distorted_black_words_dict", ":", "\n", "            ", "tweet", "=", "tweet", ".", "replace", "(", "distorted_black_word", ",", "self", ".", "distorted_black_words_dict", "[", "distorted_black_word", "]", ")", "\n", "\n", "# for word in self.abbr_dict:", "\n", "#     tweet = re.sub(' ' + word.lower() + ' ', self.abbr_dict.get(word.lower()), tweet)", "\n", "\n", "# for word in self.contractions_dict:", "\n", "#     tweet = re.sub(word, self.contractions_dict[word], tweet)", "\n", "\n", "# tweet = tweet.replace('@user', '').replace('@USER', '')", "\n", "", "tweet", "=", "re", ".", "sub", "(", "r' +'", ",", "' '", ",", "tweet", ")", "\n", "\n", "return", "tweet", ".", "strip", "(", ")", ".", "replace", "(", "'&amp;'", ",", "'and'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader_emotion_feature.DataLoader._tweet_to_embeddings": [[109, 117], ["data_loader_emotion_feature.DataLoader.tokenizer.tokenize", "numpy.array", "embedded_tweet.append", "embedded_tweet.append", "word.lower", "range"], "methods", ["None"], ["", "def", "_tweet_to_embeddings", "(", "self", ",", "tweet", ")", ":", "\n", "        ", "embedded_tweet", "=", "[", "]", "\n", "for", "word", "in", "self", ".", "tokenizer", ".", "tokenize", "(", "tweet", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "embedded_tweet", ".", "append", "(", "self", ".", "word_embedding_model", "[", "word", ".", "lower", "(", ")", "]", ")", "\n", "", "except", "KeyError", ":", "\n", "                ", "embedded_tweet", ".", "append", "(", "[", "0", "for", "_", "in", "range", "(", "cfg", ".", "word_embed_dim", ")", "]", ")", "\n", "", "", "return", "np", ".", "array", "(", "embedded_tweet", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader_emotion_feature.DataLoader._pad": [[118, 122], ["range", "word.append", "len"], "methods", ["None"], ["", "def", "_pad", "(", "self", ",", "word", ")", ":", "\n", "        ", "for", "_", "in", "range", "(", "cfg", ".", "word_max_len", "-", "len", "(", "word", ")", ")", ":", "\n", "            ", "word", ".", "append", "(", "0", ")", "\n", "", "return", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader_emotion_feature.DataLoader._tweet_to_indices": [[123, 132], ["data_loader_emotion_feature.DataLoader.tokenizer.tokenize", "numpy.array", "indexed_tweet.append", "indexed_word.append", "data_loader_emotion_feature.DataLoader._pad", "data_loader_emotion_feature.DataLoader.char_to_index.get"], "methods", ["home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._pad"], ["", "def", "_tweet_to_indices", "(", "self", ",", "tweet", ")", ":", "\n", "        ", "indexed_tweet", "=", "[", "]", "\n", "for", "word", "in", "self", ".", "tokenizer", ".", "tokenize", "(", "tweet", ")", ":", "\n", "            ", "indexed_word", "=", "[", "]", "\n", "for", "char", "in", "word", ":", "\n", "                ", "indexed_word", ".", "append", "(", "self", ".", "char_to_index", ".", "get", "(", "char", ",", "1", ")", ")", "\n", "", "indexed_tweet", ".", "append", "(", "self", ".", "_pad", "(", "indexed_word", ")", ")", "\n", "\n", "", "return", "np", ".", "array", "(", "indexed_tweet", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader_emotion_feature.DataLoader._tags_to_one_hot": [[133, 138], ["numpy.eye", "numpy.eye"], "methods", ["None"], ["", "def", "_tags_to_one_hot", "(", "self", ",", "tag", ",", "mode", "=", "None", ")", ":", "\n", "        ", "if", "mode", "is", "'a'", ":", "\n", "            ", "return", "np", ".", "eye", "(", "2", ")", "[", "self", ".", "a_tag_to_index", "[", "tag", "]", "]", "\n", "", "elif", "mode", "is", "'b'", ":", "\n", "            ", "return", "np", ".", "eye", "(", "2", ")", "[", "self", ".", "b_tag_to_index", "[", "tag", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader_emotion_feature.DataLoader.data_generator": [[139, 160], ["zip", "data_loader_emotion_feature.DataLoader._data_reader", "data_loader_emotion_feature.DataLoader._preprocessor", "data_loader_emotion_feature.DataLoader._tweet_to_embeddings", "data_loader_emotion_feature.DataLoader._tweet_to_indices", "data_loader_emotion_feature.DataLoader._tags_to_one_hot", "open", "pickle.load", "data_loader_emotion_feature.DataLoader._data_reader", "open", "pickle.load"], "methods", ["home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._data_reader", "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._preprocessor", "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._tweet_to_embeddings", "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._tweet_to_indices", "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._tags_to_one_hot", "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._data_reader"], ["", "", "def", "data_generator", "(", "self", ",", "mode", "=", "None", ")", ":", "\n", "        ", "if", "mode", "is", "'train'", ":", "\n", "            ", "with", "open", "(", "cfg", ".", "train_emotion_features_dir", ",", "'rb'", ")", "as", "handle", ":", "\n", "                ", "emotion_features", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "\n", "", "tweets", ",", "a_tags", ",", "b_tags", ",", "c_tags", "=", "self", ".", "_data_reader", "(", "cfg", ".", "train_data_dir", ")", "\n", "", "elif", "mode", "is", "'valid'", ":", "\n", "            ", "with", "open", "(", "cfg", ".", "valid_emotion_features_dir", ",", "'rb'", ")", "as", "handle", ":", "\n", "                ", "emotion_features", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "\n", "", "tweets", ",", "a_tags", ",", "b_tags", ",", "c_tags", "=", "self", ".", "_data_reader", "(", "cfg", ".", "valid_data_dir", ")", "\n", "\n", "", "for", "tweet", ",", "a_tag", ",", "emotion_feature", "in", "zip", "(", "tweets", ",", "a_tags", ",", "emotion_features", ")", ":", "\n", "            ", "preprocessed_tweet", "=", "self", ".", "_preprocessor", "(", "tweet", ")", "\n", "\n", "embedded_tweet", "=", "self", ".", "_tweet_to_embeddings", "(", "preprocessed_tweet", ")", "\n", "indexed_tweet", "=", "self", ".", "_tweet_to_indices", "(", "preprocessed_tweet", ")", "\n", "\n", "indexed_a_tag", "=", "self", ".", "_tags_to_one_hot", "(", "a_tag", ",", "'a'", ")", "\n", "\n", "yield", "(", "embedded_tweet", ",", "indexed_tweet", ",", "emotion_feature", ")", ",", "indexed_a_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader_emotion_feature.DataLoader.data_generator_pred": [[161, 171], ["data_loader_emotion_feature.DataLoader._data_reader", "data_loader_emotion_feature.DataLoader._preprocessor", "data_loader_emotion_feature.DataLoader._tweet_to_embeddings", "data_loader_emotion_feature.DataLoader._tweet_to_indices"], "methods", ["home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._data_reader", "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._preprocessor", "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._tweet_to_embeddings", "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._tweet_to_indices"], ["", "", "def", "data_generator_pred", "(", "self", ")", ":", "\n", "        ", "tweets", ",", "a_tags", ",", "b_tags", ",", "c_tags", "=", "self", ".", "_data_reader", "(", "cfg", ".", "valid_data_dir", ")", "\n", "\n", "for", "tweet", "in", "tweets", ":", "\n", "            ", "preprocessed_tweet", "=", "self", ".", "_preprocessor", "(", "tweet", ")", "\n", "\n", "embedded_tweet", "=", "self", ".", "_tweet_to_embeddings", "(", "preprocessed_tweet", ")", "\n", "indexed_tweet", "=", "self", ".", "_tweet_to_indices", "(", "preprocessed_tweet", ")", "\n", "\n", "yield", "embedded_tweet", ",", "indexed_tweet", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_gru.model_fn": [[20, 94], ["pathlib.Path().mkdir", "tensorflow.logging.set_verbosity", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.reshape", "tensorflow.layers.conv1d", "tensorflow.layers.max_pooling1d", "tensorflow.layers.conv1d", "tensorflow.layers.max_pooling1d", "tensorflow.layers.dropout", "tensorflow.layers.flatten", "tensorflow.reshape", "tensorflow.concat", "tensorflow.contrib.rnn.GRUCell", "tensorflow.nn.dynamic_rnn", "tensorflow.layers.dropout", "tensorflow.layers.dense", "tensorflow.layers.dropout", "tensorflow.layers.dense", "tensorflow.reduce_mean", "metrics.items", "logging.FileHandler", "logging.StreamHandler", "logging.getLogger", "tensorflow.nn.softmax_cross_entropy_with_logits", "tensorflow.metrics.accuracy", "tensorflow.metrics.precision", "tensorflow.metrics.recall", "tensorflow.contrib.metrics.f1_score", "tensorflow.summary.scalar", "tensorflow.estimator.EstimatorSpec", "pathlib.Path", "tensorflow.argmax", "tensorflow.argmax", "tensorflow.argmax", "tensorflow.argmax", "tensorflow.argmax", "tensorflow.argmax", "tensorflow.argmax", "tensorflow.argmax", "tensorflow.train.AdamOptimizer().minimize", "tensorflow.estimator.EstimatorSpec", "tensorflow.shape", "tensorflow.shape", "tensorflow.train.AdamOptimizer", "tensorflow.train.get_or_create_global_step"], "function", ["None"], ["def", "model_fn", "(", "mode", ",", "features", ",", "labels", ")", ":", "\n", "# Logging", "\n", "    ", "Path", "(", "'results'", ")", ".", "mkdir", "(", "exist_ok", "=", "True", ")", "\n", "tf", ".", "logging", ".", "set_verbosity", "(", "logging", ".", "INFO", ")", "\n", "handlers", "=", "[", "logging", ".", "FileHandler", "(", "'./results/main.log'", ")", ",", "\n", "logging", ".", "StreamHandler", "(", "sys", ".", "stdout", ")", "]", "\n", "logging", ".", "getLogger", "(", "'tensorflow'", ")", ".", "handlers", "=", "handlers", "\n", "\n", "word_inputs", ",", "char_inputs", "=", "features", "\n", "\n", "training", "=", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ")", "\n", "\n", "# Embeddings", "\n", "embeddings", "=", "tf", ".", "get_variable", "(", "'embeddings'", ",", "[", "cfg", ".", "num_chars", "+", "2", ",", "cfg", ".", "char_embed_dim", "]", ")", "\n", "char_input_emb", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embeddings", ",", "char_inputs", ")", "\n", "\n", "# Reshaping for CNN", "\n", "output", "=", "tf", ".", "reshape", "(", "char_input_emb", ",", "[", "-", "1", ",", "tf", ".", "shape", "(", "char_inputs", ")", "[", "2", "]", ",", "cfg", ".", "char_embed_dim", "]", ")", "\n", "\n", "# CNN", "\n", "output", "=", "tf", ".", "layers", ".", "conv1d", "(", "output", ",", "filters", "=", "64", ",", "kernel_size", "=", "2", ",", "strides", "=", "1", ",", "padding", "=", "\"same\"", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "output", "=", "tf", ".", "layers", ".", "max_pooling1d", "(", "output", ",", "pool_size", "=", "2", ",", "strides", "=", "2", ")", "\n", "output", "=", "tf", ".", "layers", ".", "conv1d", "(", "output", ",", "filters", "=", "128", ",", "kernel_size", "=", "2", ",", "strides", "=", "1", ",", "padding", "=", "\"same\"", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "output", "=", "tf", ".", "layers", ".", "max_pooling1d", "(", "output", ",", "pool_size", "=", "2", ",", "strides", "=", "2", ")", "\n", "\n", "cnn_output", "=", "tf", ".", "layers", ".", "dropout", "(", "output", ",", "rate", "=", "cfg", ".", "dropout", ",", "training", "=", "training", ")", "\n", "cnn_output", "=", "tf", ".", "layers", ".", "flatten", "(", "cnn_output", ")", "\n", "\n", "#cell = tf.contrib.rnn.GRUCell(num_units=64)", "\n", "#encoder_outputs, cnn_output = tf.nn.dynamic_rnn(cell, word_inputs, dtype=tf.float32)", "\n", "\n", "# Reshaping CNN and concatenating for LSTM", "\n", "cnn_output", "=", "tf", ".", "reshape", "(", "cnn_output", ",", "[", "-", "1", ",", "tf", ".", "shape", "(", "char_inputs", ")", "[", "1", "]", ",", "128", "*", "15", "]", ")", "\n", "word_inputs", "=", "tf", ".", "concat", "(", "[", "word_inputs", ",", "cnn_output", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# LSTM", "\n", "cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "GRUCell", "(", "num_units", "=", "cfg", ".", "lstm_units", ")", "\n", "encoder_outputs", ",", "encoder_final_state", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "cell", ",", "word_inputs", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "#output = tf.concat(encoder_final_state, axis=-1)", "\n", "lstm_output", "=", "tf", ".", "layers", ".", "dropout", "(", "encoder_final_state", ",", "rate", "=", "cfg", ".", "dropout", ",", "training", "=", "training", ")", "\n", "\n", "# Dense", "\n", "output", "=", "tf", ".", "layers", ".", "dense", "(", "lstm_output", ",", "128", ")", "\n", "output", "=", "tf", ".", "layers", ".", "dropout", "(", "output", ",", "rate", "=", ".5", ",", "training", "=", "training", ")", "\n", "logits", "=", "tf", ".", "layers", ".", "dense", "(", "output", ",", "2", ")", "\n", "\n", "# Loss", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits", "(", "labels", "=", "labels", ",", "logits", "=", "logits", ")", ")", "\n", "\n", "# Metrics", "\n", "indices", "=", "[", "0", ",", "1", "]", "\n", "metrics", "=", "{", "\n", "'acc'", ":", "tf", ".", "metrics", ".", "accuracy", "(", "tf", ".", "argmax", "(", "labels", ",", "1", ")", ",", "tf", ".", "argmax", "(", "logits", ",", "1", ")", ")", ",", "\n", "#'precision': precision(labels, pred_ids, 2, indices, weights),", "\n", "#'recall': recall(labels, pred_ids, 2, indices, weights),", "\n", "#'f1': f1(labels, pred_ids, 2, indices, weights)", "\n", "'precision'", ":", "tf", ".", "metrics", ".", "precision", "(", "tf", ".", "argmax", "(", "labels", ",", "1", ")", ",", "tf", ".", "argmax", "(", "logits", ",", "1", ")", ")", ",", "\n", "'recall'", ":", "tf", ".", "metrics", ".", "recall", "(", "tf", ".", "argmax", "(", "labels", ",", "1", ")", ",", "tf", ".", "argmax", "(", "logits", ",", "1", ")", ")", ",", "\n", "'f1'", ":", "tf", ".", "contrib", ".", "metrics", ".", "f1_score", "(", "tf", ".", "argmax", "(", "labels", ",", "1", ")", ",", "tf", ".", "argmax", "(", "logits", ",", "1", ")", ")", "\n", "}", "\n", "\n", "for", "metric_name", ",", "op", "in", "metrics", ".", "items", "(", ")", ":", "\n", "        ", "tf", ".", "summary", ".", "scalar", "(", "metric_name", ",", "op", "[", "1", "]", ")", "\n", "\n", "", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "        ", "return", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "mode", ",", "loss", "=", "loss", ",", "\n", "eval_metric_ops", "=", "metrics", ")", "\n", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "        ", "train_op", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "cfg", ".", "learning_rate", ")", ".", "minimize", "(", "loss", ",", "global_step", "=", "tf", ".", "train", ".", "get_or_create_global_step", "(", ")", ")", "\n", "\n", "return", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "mode", ",", "loss", "=", "loss", ",", "\n", "train_op", "=", "train_op", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_gru.input_fn": [[96, 109], ["tensorflow.data.Dataset.from_generator", "dataset.shuffle().repeat.padded_batch", "data_loader.data_generator", "dataset.shuffle().repeat.shuffle().repeat", "dataset.shuffle().repeat.shuffle"], "function", ["home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader.data_generator"], ["", "", "def", "input_fn", "(", "mode", "=", "None", ")", ":", "\n", "        ", "data_generator", "=", "lambda", ":", "data_loader", ".", "data_generator", "(", "mode", "=", "mode", ")", "\n", "\n", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "from_generator", "(", "data_generator", ",", "\n", "output_types", "=", "(", "(", "tf", ".", "float32", ",", "tf", ".", "int32", ")", ",", "tf", ".", "int32", ")", ",", "\n", "output_shapes", "=", "(", "(", "[", "None", ",", "300", "]", ",", "[", "None", ",", "None", "]", ")", ",", "[", "None", "]", ")", ")", "\n", "\n", "if", "mode", "is", "'train'", ":", "\n", "            ", "dataset", "=", "dataset", ".", "shuffle", "(", "cfg", ".", "shuffle_buffer", ")", ".", "repeat", "(", "cfg", ".", "num_epochs", ")", "\n", "\n", "", "dataset", "=", "dataset", ".", "padded_batch", "(", "cfg", ".", "batch_size", ",", "padded_shapes", "=", "(", "(", "[", "None", ",", "300", "]", ",", "[", "None", ",", "None", "]", ")", ",", "[", "None", "]", ")", ")", "\n", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_gru.train": [[111, 122], ["tensorflow.estimator.RunConfig", "tensorflow.estimator.Estimator", "tensorflow.estimator.TrainSpec", "tensorflow.estimator.EvalSpec", "tensorflow.estimator.train_and_evaluate", "char_cnn_word_gru.input_fn", "char_cnn_word_gru.input_fn"], "function", ["home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_blstm_attn.input_fn", "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_blstm_attn.input_fn"], ["", "def", "train", "(", ")", ":", "\n", "    ", "train_input_func", "=", "lambda", ":", "input_fn", "(", "mode", "=", "'train'", ")", "\n", "eval_input_func", "=", "lambda", ":", "input_fn", "(", "mode", "=", "'valid'", ")", "\n", "\n", "est_conf", "=", "tf", ".", "estimator", ".", "RunConfig", "(", "cfg", ".", "model_dir", ",", "save_checkpoints_secs", "=", "60", ")", "\n", "est", "=", "tf", ".", "estimator", ".", "Estimator", "(", "model_fn", ",", "cfg", ".", "model_dir", ",", "est_conf", ")", "\n", "\n", "train_spec", "=", "tf", ".", "estimator", ".", "TrainSpec", "(", "input_fn", "=", "train_input_func", ")", "\n", "eval_spec", "=", "tf", ".", "estimator", ".", "EvalSpec", "(", "input_fn", "=", "eval_input_func", ",", "throttle_secs", "=", "60", ")", "\n", "\n", "tf", ".", "estimator", ".", "train_and_evaluate", "(", "est", ",", "train_spec", ",", "eval_spec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader.__init__": [[16, 69], ["nltk.tokenize.TweetTokenizer", "open", "open", "open", "open", "pickle.load", "gensim.models.KeyedVectors.load_word2vec_format", "line.strip().split", "line.strip().split", "line.strip().split", "open", "pickle.load", "data_loader.DataLoader._data_reader", "chars_list.extend", "enumerate", "open", "pickle.dump", "open", "pickle.dump", "line.strip", "line.strip", "line.strip", "tweet.replace", "collections.Counter().most_common", "collections.Counter"], "methods", ["home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._data_reader"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "# loading word embedding model", "\n", "        ", "try", ":", "\n", "            ", "with", "open", "(", "cfg", ".", "we_pickled_model_dir", ",", "'rb'", ")", "as", "handle", ":", "\n", "                ", "self", ".", "word_embedding_model", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "", "except", "FileNotFoundError", ":", "\n", "            ", "self", ".", "word_embedding_model", "=", "KeyedVectors", ".", "load_word2vec_format", "(", "cfg", ".", "we_model_dir", ",", "binary", "=", "False", ")", "\n", "#self.word_embedding_model = KeyedVectors.load_word2vec_format('/mnt/hdd2/ehsan/word2vec.txt', binary=False)", "\n", "with", "open", "(", "cfg", ".", "we_pickled_model_dir", ",", "'wb'", ")", "as", "handle", ":", "\n", "                ", "pickle", ".", "dump", "(", "self", ".", "word_embedding_model", ",", "handle", ",", "protocol", "=", "pickle", ".", "HIGHEST_PROTOCOL", ")", "\n", "\n", "", "", "self", ".", "tokenizer", "=", "TweetTokenizer", "(", ")", "\n", "\n", "# loading distorted black words", "\n", "self", ".", "distorted_black_words_dict", "=", "{", "}", "\n", "with", "open", "(", "cfg", ".", "distorted_black_words_dir", ")", "as", "file", ":", "\n", "            ", "for", "line", "in", "file", ":", "\n", "                ", "source", ",", "target", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "self", ".", "distorted_black_words_dict", "[", "source", "]", "=", "target", "\n", "\n", "# loading abbreviations", "\n", "", "", "self", ".", "abbr_dict", "=", "{", "}", "\n", "with", "open", "(", "cfg", ".", "abbreviations_dir", ")", "as", "file", ":", "\n", "            ", "for", "line", "in", "file", ":", "\n", "                ", "source", ",", "target", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "self", ".", "abbr_dict", "[", "source", "]", "=", "target", "\n", "\n", "# loading contractions", "\n", "", "", "self", ".", "contractions_dict", "=", "{", "}", "\n", "with", "open", "(", "cfg", ".", "contractions_dir", ")", "as", "file", ":", "\n", "            ", "for", "line", "in", "file", ":", "\n", "                ", "source", ",", "target", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "self", ".", "contractions_dict", "[", "source", "]", "=", "target", "\n", "\n", "# loading/builing chracter indices", "\n", "", "", "try", ":", "\n", "            ", "with", "open", "(", "cfg", ".", "char_indices_dir", ",", "'rb'", ")", "as", "handle", ":", "\n", "                ", "self", ".", "char_to_index", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "", "except", "FileNotFoundError", ":", "\n", "            ", "tweets", ",", "_", ",", "_", ",", "_", "=", "self", ".", "_data_reader", "(", "cfg", ".", "train_data_dir", ")", "\n", "chars_list", "=", "[", "]", "\n", "chars_list", ".", "extend", "(", "[", "char", "for", "tweet", "in", "tweets", "for", "char", "in", "tweet", ".", "replace", "(", "' '", ",", "' '", ")", "]", ")", "\n", "char_vocab_list", "=", "[", "x", "[", "0", "]", "for", "x", "in", "[", "(", "'<PAD>'", ",", "0", ")", ",", "(", "'<UNK>'", ",", "1", ")", "]", "+", "Counter", "(", "chars_list", ")", ".", "most_common", "(", "cfg", ".", "num_chars", ")", "]", "\n", "\n", "self", ".", "char_to_index", "=", "{", "}", "\n", "for", "i", ",", "char", "in", "enumerate", "(", "char_vocab_list", ")", ":", "\n", "                ", "self", ".", "char_to_index", "[", "char", "]", "=", "i", "\n", "\n", "", "with", "open", "(", "cfg", ".", "char_indices_dir", ",", "'wb'", ")", "as", "handle", ":", "\n", "                ", "pickle", ".", "dump", "(", "self", ".", "char_to_index", ",", "handle", ",", "protocol", "=", "pickle", ".", "HIGHEST_PROTOCOL", ")", "\n", "\n", "", "", "self", ".", "a_tag_to_index", "=", "{", "'NOT'", ":", "0", ",", "'OFF'", ":", "1", "}", "\n", "self", ".", "a_tag_to_index", "=", "{", "'TIN'", ":", "0", ",", "'UNT'", ":", "1", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._data_reader": [[70, 89], ["open", "tweets.append", "a_tags.append", "b_tags.append", "c_tags.append", "line.strip().split", "line.strip().split", "line.strip", "line.strip"], "methods", ["None"], ["", "def", "_data_reader", "(", "self", ",", "file_path", ")", ":", "\n", "        ", "tweets", ",", "a_tags", ",", "b_tags", ",", "c_tags", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "with", "open", "(", "file_path", ")", "as", "file", ":", "\n", "            ", "for", "line", "in", "file", ":", "\n", "                ", "try", ":", "\n", "                    ", "_", ",", "tweet", ",", "a_tag", ",", "b_tag", ",", "c_tag", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "b_tag", "==", "'NULL'", ":", "\n", "                        ", "continue", "\n", "", "", "except", "ValueError", ":", "\n", "                    ", "continue", "\n", "tweet", ",", "a_tag", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "b_tag", ",", "c_tag", "=", "'NA'", ",", "'NA'", "\n", "\n", "", "tweets", ".", "append", "(", "tweet", ")", "\n", "a_tags", ".", "append", "(", "a_tag", ")", "\n", "b_tags", ".", "append", "(", "b_tag", ")", "\n", "c_tags", ".", "append", "(", "c_tag", ")", "\n", "\n", "", "", "return", "tweets", ",", "a_tags", ",", "b_tags", ",", "c_tags", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._preprocessor": [[90, 104], ["re.sub", "tweet.replace.replace.strip().replace", "tweet.replace.replace.replace", "tweet.replace.replace.strip"], "methods", ["None"], ["", "def", "_preprocessor", "(", "self", ",", "tweet", ")", ":", "\n", "        ", "for", "distorted_black_word", "in", "self", ".", "distorted_black_words_dict", ":", "\n", "            ", "tweet", "=", "tweet", ".", "replace", "(", "distorted_black_word", ",", "self", ".", "distorted_black_words_dict", "[", "distorted_black_word", "]", ")", "\n", "\n", "# for word in self.abbr_dict:", "\n", "#     tweet = re.sub(' ' + word.lower() + ' ', self.abbr_dict.get(word.lower()), tweet)", "\n", "\n", "# for word in self.contractions_dict:", "\n", "#     tweet = re.sub(word, self.contractions_dict[word], tweet)", "\n", "\n", "# tweet = tweet.replace('@user', '').replace('@USER', '')", "\n", "", "tweet", "=", "re", ".", "sub", "(", "r' +'", ",", "' '", ",", "tweet", ")", "\n", "\n", "return", "tweet", ".", "strip", "(", ")", ".", "replace", "(", "'&amp;'", ",", "'and'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._tweet_to_embeddings": [[112, 121], ["data_loader.DataLoader.tokenizer.tokenize", "numpy.array", "embedded_tweet.append", "embedded_tweet.append", "word.lower", "range"], "methods", ["None"], ["", "def", "_tweet_to_embeddings", "(", "self", ",", "tweet", ")", ":", "\n", "        ", "embedded_tweet", "=", "[", "]", "\n", "for", "word", "in", "self", ".", "tokenizer", ".", "tokenize", "(", "tweet", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "embedded_tweet", ".", "append", "(", "self", ".", "word_embedding_model", "[", "word", ".", "lower", "(", ")", "]", ")", "\n", "", "except", "KeyError", ":", "\n", "                ", "embedded_tweet", ".", "append", "(", "[", "0", "for", "_", "in", "range", "(", "cfg", ".", "word_embed_dim", ")", "]", ")", "\n", "\n", "", "", "return", "np", ".", "array", "(", "embedded_tweet", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._pad": [[122, 127], ["range", "word.append", "len"], "methods", ["None"], ["", "def", "_pad", "(", "self", ",", "word", ")", ":", "\n", "        ", "for", "_", "in", "range", "(", "cfg", ".", "word_max_len", "-", "len", "(", "word", ")", ")", ":", "\n", "            ", "word", ".", "append", "(", "0", ")", "\n", "\n", "", "return", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._tweet_to_indices": [[128, 137], ["data_loader.DataLoader.tokenizer.tokenize", "numpy.array", "indexed_tweet.append", "indexed_word.append", "data_loader.DataLoader._pad", "data_loader.DataLoader.char_to_index.get"], "methods", ["home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._pad"], ["", "def", "_tweet_to_indices", "(", "self", ",", "tweet", ")", ":", "\n", "        ", "indexed_tweet", "=", "[", "]", "\n", "for", "word", "in", "self", ".", "tokenizer", ".", "tokenize", "(", "tweet", ")", ":", "\n", "            ", "indexed_word", "=", "[", "]", "\n", "for", "char", "in", "word", ":", "\n", "                ", "indexed_word", ".", "append", "(", "self", ".", "char_to_index", ".", "get", "(", "char", ",", "1", ")", ")", "\n", "", "indexed_tweet", ".", "append", "(", "self", ".", "_pad", "(", "indexed_word", ")", ")", "\n", "\n", "", "return", "np", ".", "array", "(", "indexed_tweet", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._tags_to_one_hot": [[138, 143], ["numpy.eye", "numpy.eye"], "methods", ["None"], ["", "def", "_tags_to_one_hot", "(", "self", ",", "tag", ",", "mode", "=", "None", ")", ":", "\n", "        ", "if", "mode", "is", "'a'", ":", "\n", "            ", "return", "np", ".", "eye", "(", "2", ")", "[", "self", ".", "a_tag_to_index", "[", "tag", "]", "]", "\n", "", "elif", "mode", "is", "'b'", ":", "\n", "            ", "return", "np", ".", "eye", "(", "2", ")", "[", "self", ".", "b_tag_to_index", "[", "tag", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader.data_generator": [[144, 159], ["zip", "data_loader.DataLoader._data_reader", "data_loader.DataLoader._preprocessor", "data_loader.DataLoader._tags_to_one_hot", "data_loader.DataLoader._tweet_to_indices", "data_loader.DataLoader._tweet_to_embeddings", "data_loader.DataLoader._data_reader"], "methods", ["home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._data_reader", "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._preprocessor", "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._tags_to_one_hot", "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._tweet_to_indices", "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._tweet_to_embeddings", "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._data_reader"], ["", "", "def", "data_generator", "(", "self", ",", "mode", "=", "None", ")", ":", "\n", "        ", "if", "mode", "is", "'train'", ":", "\n", "            ", "tweets", ",", "a_tags", ",", "b_tags", ",", "c_tags", "=", "self", ".", "_data_reader", "(", "cfg", ".", "train_data_dir", ")", "\n", "", "elif", "mode", "is", "'valid'", ":", "\n", "            ", "tweets", ",", "a_tags", ",", "b_tags", ",", "c_tags", "=", "self", ".", "_data_reader", "(", "cfg", ".", "valid_data_dir", ")", "\n", "\n", "", "for", "tweet", ",", "b_tag", "in", "zip", "(", "tweets", ",", "b_tags", ")", ":", "\n", "            ", "preprocessed_tweet", "=", "self", ".", "_preprocessor", "(", "tweet", ")", "\n", "\n", "indexed_a_tag", "=", "self", ".", "_tags_to_one_hot", "(", "b_tag", ",", "'a'", ")", "\n", "indexed_tweet", "=", "self", ".", "_tweet_to_indices", "(", "preprocessed_tweet", ")", "\n", "\n", "embedded_tweet", "=", "self", ".", "_tweet_to_embeddings", "(", "preprocessed_tweet", ")", "\n", "\n", "yield", "(", "embedded_tweet", ",", "indexed_tweet", ")", ",", "indexed_a_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader.bert_data_generator": [[160, 175], ["zip", "data_loader.DataLoader._data_reader", "data_loader.DataLoader._tags_to_one_hot", "open", "pickle.load", "data_loader.DataLoader._data_reader", "open", "pickle.load"], "methods", ["home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._data_reader", "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._tags_to_one_hot", "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._data_reader"], ["", "", "def", "bert_data_generator", "(", "self", ",", "mode", "=", "None", ")", ":", "\n", "        ", "if", "mode", "is", "'train'", ":", "\n", "            ", "tweets", ",", "a_tags", ",", "b_tags", ",", "c_tags", "=", "self", ".", "_data_reader", "(", "cfg", ".", "train_data_dir", ")", "\n", "with", "open", "(", "'bert.pickle'", ",", "'rb'", ")", "as", "handle", ":", "\n", "                ", "encoded_comments", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "\n", "", "", "elif", "mode", "is", "'valid'", ":", "\n", "            ", "tweets", ",", "a_tags", ",", "b_tags", ",", "c_tags", "=", "self", ".", "_data_reader", "(", "cfg", ".", "valid_data_dir", ")", "\n", "with", "open", "(", "'test_bert.pickle'", ",", "'rb'", ")", "as", "handle", ":", "\n", "                ", "encoded_comments", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "\n", "", "", "for", "tweet", ",", "a_tag", "in", "zip", "(", "encoded_comments", ",", "a_tags", ")", ":", "\n", "            ", "indexed_a_tag", "=", "self", ".", "_tags_to_one_hot", "(", "a_tag", ",", "'a'", ")", "\n", "\n", "yield", "tweet", ",", "indexed_a_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader.data_generator_pred": [[176, 187], ["data_loader.DataLoader._data_reader", "print", "data_loader.DataLoader._preprocessor", "data_loader.DataLoader._tweet_to_embeddings", "data_loader.DataLoader._tweet_to_indices", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._data_reader", "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._preprocessor", "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._tweet_to_embeddings", "home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader._tweet_to_indices"], ["", "", "def", "data_generator_pred", "(", "self", ")", ":", "\n", "        ", "tweet_ids", ",", "tweets", "=", "self", ".", "_data_reader", "(", "'subtask_b_test.tsv'", ")", "\n", "\n", "for", "tweet", "in", "tweets", ":", "\n", "            ", "print", "(", "tweet", ")", "\n", "preprocessed_tweet", "=", "self", ".", "_preprocessor", "(", "tweet", ")", "\n", "\n", "embedded_tweet", "=", "self", ".", "_tweet_to_embeddings", "(", "preprocessed_tweet", ")", "\n", "indexed_tweet", "=", "self", ".", "_tweet_to_indices", "(", "preprocessed_tweet", ")", "\n", "\n", "yield", "(", "embedded_tweet", ",", "indexed_tweet", ")", ",", "np", ".", "array", "(", "[", "1", ",", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_lstm_emotion.model_fn": [[21, 94], ["pathlib.Path().mkdir", "tensorflow.logging.set_verbosity", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.reshape", "tensorflow.layers.conv1d", "tensorflow.layers.max_pooling1d", "tensorflow.layers.conv1d", "tensorflow.layers.max_pooling1d", "tensorflow.layers.dropout", "tensorflow.layers.flatten", "tensorflow.reshape", "tensorflow.layers.dropout", "tensorflow.concat", "tensorflow.contrib.rnn.LSTMCell", "tensorflow.nn.dynamic_rnn", "tensorflow.concat", "tensorflow.concat", "tensorflow.layers.dropout", "tensorflow.layers.dense", "tensorflow.layers.dropout", "tensorflow.layers.dense", "tensorflow.reduce_mean", "tensorflow.argmax", "tensorflow.argmax", "metrics.items", "logging.FileHandler", "logging.StreamHandler", "logging.getLogger", "tensorflow.nn.softmax_cross_entropy_with_logits", "tensorflow.metrics.accuracy", "tf_metrics.precision", "tf_metrics.recall", "tf_metrics.f1", "tensorflow.summary.scalar", "tensorflow.estimator.EstimatorSpec", "pathlib.Path", "tensorflow.train.AdamOptimizer().minimize", "tensorflow.estimator.EstimatorSpec", "tensorflow.shape", "tensorflow.shape", "int", "tensorflow.train.AdamOptimizer", "tensorflow.train.get_or_create_global_step"], "function", ["None"], ["def", "model_fn", "(", "mode", ",", "features", ",", "labels", ")", ":", "\n", "# Logging", "\n", "    ", "Path", "(", "'results'", ")", ".", "mkdir", "(", "exist_ok", "=", "True", ")", "\n", "tf", ".", "logging", ".", "set_verbosity", "(", "logging", ".", "INFO", ")", "\n", "handlers", "=", "[", "logging", ".", "FileHandler", "(", "'./results/main.log'", ")", ",", "\n", "logging", ".", "StreamHandler", "(", "sys", ".", "stdout", ")", "]", "\n", "logging", ".", "getLogger", "(", "'tensorflow'", ")", ".", "handlers", "=", "handlers", "\n", "\n", "word_inputs", ",", "char_inputs", ",", "emo_features", "=", "features", "\n", "\n", "training", "=", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ")", "\n", "\n", "# Embeddings", "\n", "embeddings", "=", "tf", ".", "get_variable", "(", "'embeddings'", ",", "[", "cfg", ".", "num_chars", "+", "2", ",", "cfg", ".", "char_embed_dim", "]", ")", "\n", "char_input_emb", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embeddings", ",", "char_inputs", ")", "\n", "\n", "# Reshaping for CNN", "\n", "output", "=", "tf", ".", "reshape", "(", "char_input_emb", ",", "[", "-", "1", ",", "tf", ".", "shape", "(", "char_inputs", ")", "[", "2", "]", ",", "cfg", ".", "char_embed_dim", "]", ")", "\n", "\n", "# CNN", "\n", "output", "=", "tf", ".", "layers", ".", "conv1d", "(", "output", ",", "filters", "=", "64", ",", "kernel_size", "=", "2", ",", "strides", "=", "1", ",", "padding", "=", "\"same\"", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "output", "=", "tf", ".", "layers", ".", "max_pooling1d", "(", "output", ",", "pool_size", "=", "2", ",", "strides", "=", "2", ")", "\n", "output", "=", "tf", ".", "layers", ".", "conv1d", "(", "output", ",", "filters", "=", "128", ",", "kernel_size", "=", "2", ",", "strides", "=", "1", ",", "padding", "=", "\"same\"", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "output", "=", "tf", ".", "layers", ".", "max_pooling1d", "(", "output", ",", "pool_size", "=", "2", ",", "strides", "=", "2", ")", "\n", "\n", "cnn_output", "=", "tf", ".", "layers", ".", "dropout", "(", "output", ",", "rate", "=", ".5", ",", "training", "=", "training", ")", "\n", "cnn_output", "=", "tf", ".", "layers", ".", "flatten", "(", "cnn_output", ")", "\n", "\n", "# Reshaping CNN and concatenating for LSTM", "\n", "cnn_output", "=", "tf", ".", "reshape", "(", "cnn_output", ",", "[", "-", "1", ",", "tf", ".", "shape", "(", "char_inputs", ")", "[", "1", "]", ",", "128", "*", "int", "(", "cfg", ".", "word_max_len", "/", "4", ")", "]", ")", "\n", "word_inputs", "=", "tf", ".", "layers", ".", "dropout", "(", "word_inputs", ",", "rate", "=", ".5", ",", "training", "=", "training", ")", "\n", "lstm_inputs", "=", "tf", ".", "concat", "(", "[", "word_inputs", ",", "cnn_output", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# LSTM", "\n", "cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMCell", "(", "num_units", "=", "cfg", ".", "lstm_units", ")", "\n", "_", ",", "final_state", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "cell", ",", "lstm_inputs", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "lstm_output", "=", "tf", ".", "concat", "(", "final_state", ",", "axis", "=", "-", "1", ")", "\n", "lstm_output", "=", "tf", ".", "concat", "(", "[", "lstm_output", ",", "emo_features", "]", ",", "axis", "=", "-", "1", ")", "\n", "lstm_output", "=", "tf", ".", "layers", ".", "dropout", "(", "lstm_output", ",", "rate", "=", ".5", ",", "training", "=", "training", ")", "\n", "\n", "# Dense", "\n", "output", "=", "tf", ".", "layers", ".", "dense", "(", "lstm_output", ",", "128", ")", "\n", "output", "=", "tf", ".", "layers", ".", "dropout", "(", "output", ",", "rate", "=", ".5", ",", "training", "=", "training", ")", "\n", "logits", "=", "tf", ".", "layers", ".", "dense", "(", "output", ",", "2", ")", "\n", "\n", "# Loss", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits", "(", "labels", "=", "labels", ",", "logits", "=", "logits", ")", ")", "\n", "\n", "# Metrics", "\n", "indices", "=", "[", "0", ",", "1", "]", "\n", "labels", "=", "tf", ".", "argmax", "(", "labels", ",", "1", ")", "\n", "pred_ids", "=", "tf", ".", "argmax", "(", "logits", ",", "1", ")", "\n", "\n", "metrics", "=", "{", "\n", "'acc'", ":", "tf", ".", "metrics", ".", "accuracy", "(", "labels", ",", "pred_ids", ")", ",", "\n", "'precision'", ":", "precision", "(", "labels", ",", "pred_ids", ",", "2", ",", "indices", ",", "None", ",", "average", "=", "'macro'", ")", ",", "\n", "'recall'", ":", "recall", "(", "labels", ",", "pred_ids", ",", "2", ",", "indices", ",", "None", ",", "average", "=", "'macro'", ")", ",", "\n", "'f1'", ":", "f1", "(", "labels", ",", "pred_ids", ",", "2", ",", "indices", ",", "None", ",", "average", "=", "'macro'", ")", "\n", "}", "\n", "\n", "for", "metric_name", ",", "op", "in", "metrics", ".", "items", "(", ")", ":", "\n", "        ", "tf", ".", "summary", ".", "scalar", "(", "metric_name", ",", "op", "[", "1", "]", ")", "\n", "\n", "", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "        ", "return", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "mode", ",", "loss", "=", "loss", ",", "\n", "eval_metric_ops", "=", "metrics", ")", "\n", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "        ", "train_op", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "cfg", ".", "learning_rate", ")", ".", "minimize", "(", "loss", ",", "global_step", "=", "tf", ".", "train", ".", "get_or_create_global_step", "(", ")", ")", "\n", "\n", "return", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "mode", ",", "loss", "=", "loss", ",", "\n", "train_op", "=", "train_op", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_lstm_emotion.input_fn": [[96, 109], ["tensorflow.data.Dataset.from_generator", "dataset.shuffle().repeat.padded_batch", "data_loader.data_generator", "dataset.shuffle().repeat.shuffle().repeat", "dataset.shuffle().repeat.shuffle"], "function", ["home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader.data_generator"], ["", "", "def", "input_fn", "(", "mode", "=", "None", ")", ":", "\n", "    ", "data_generator", "=", "lambda", ":", "data_loader", ".", "data_generator", "(", "mode", "=", "mode", ")", "\n", "\n", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "from_generator", "(", "data_generator", ",", "\n", "output_types", "=", "(", "(", "tf", ".", "float32", ",", "tf", ".", "int32", ",", "tf", ".", "float32", ")", ",", "tf", ".", "int32", ")", ",", "\n", "output_shapes", "=", "(", "(", "[", "None", ",", "cfg", ".", "word_embed_dim", "]", ",", "[", "None", ",", "None", "]", ",", "[", "128", "]", ")", ",", "[", "None", "]", ")", ")", "\n", "\n", "if", "mode", "is", "'train'", ":", "\n", "        ", "dataset", "=", "dataset", ".", "shuffle", "(", "cfg", ".", "shuffle_buffer", ")", ".", "repeat", "(", "cfg", ".", "num_epochs", ")", "\n", "\n", "", "dataset", "=", "dataset", ".", "padded_batch", "(", "cfg", ".", "batch_size", ",", "padded_shapes", "=", "(", "(", "[", "None", ",", "cfg", ".", "word_embed_dim", "]", ",", "[", "None", ",", "None", "]", ",", "[", "128", "]", ")", ",", "[", "None", "]", ")", ")", "\n", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_lstm_emotion.train": [[111, 122], ["tensorflow.estimator.RunConfig", "tensorflow.estimator.Estimator", "tensorflow.estimator.TrainSpec", "tensorflow.estimator.EvalSpec", "tensorflow.estimator.train_and_evaluate", "char_cnn_word_lstm_emotion.input_fn", "char_cnn_word_lstm_emotion.input_fn"], "function", ["home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_blstm_attn.input_fn", "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_blstm_attn.input_fn"], ["", "def", "train", "(", ")", ":", "\n", "    ", "train_input_func", "=", "lambda", ":", "input_fn", "(", "mode", "=", "'train'", ")", "\n", "eval_input_func", "=", "lambda", ":", "input_fn", "(", "mode", "=", "'valid'", ")", "\n", "\n", "est_conf", "=", "tf", ".", "estimator", ".", "RunConfig", "(", "cfg", ".", "model_dir", ",", "save_checkpoints_secs", "=", "30", ",", "keep_checkpoint_max", "=", "500", ")", "\n", "est", "=", "tf", ".", "estimator", ".", "Estimator", "(", "model_fn", ",", "cfg", ".", "model_dir", ",", "est_conf", ")", "\n", "\n", "train_spec", "=", "tf", ".", "estimator", ".", "TrainSpec", "(", "input_fn", "=", "train_input_func", ")", "\n", "eval_spec", "=", "tf", ".", "estimator", ".", "EvalSpec", "(", "input_fn", "=", "eval_input_func", ",", "throttle_secs", "=", "30", ")", "\n", "\n", "tf", ".", "estimator", ".", "train_and_evaluate", "(", "est", ",", "train_spec", ",", "eval_spec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_lstm_attn.model_fn": [[20, 124], ["pathlib.Path().mkdir", "tensorflow.logging.set_verbosity", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.reshape", "tensorflow.layers.conv1d", "tensorflow.layers.max_pooling1d", "tensorflow.layers.conv1d", "tensorflow.layers.max_pooling1d", "tensorflow.layers.dropout", "tensorflow.layers.flatten", "tensorflow.reshape", "tensorflow.layers.dropout", "tensorflow.concat", "tensorflow.contrib.rnn.LSTMCell", "tensorflow.nn.dynamic_rnn", "tensorflow.layers.dropout", "tensorflow.Variable", "tensorflow.tanh", "tensorflow.nn.softmax", "tensorflow.matmul", "tensorflow.squeeze", "tensorflow.tanh", "tensorflow.nn.dropout", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.nn.xw_plus_b", "tensorflow.reduce_mean", "tensorflow.argmax", "tensorflow.argmax", "metrics.items", "logging.FileHandler", "logging.StreamHandler", "logging.getLogger", "tensorflow.random_normal", "tensorflow.reshape", "tensorflow.transpose", "tensorflow.reshape", "tensorflow.truncated_normal", "tensorflow.constant", "tensorflow.nn.softmax_cross_entropy_with_logits", "tensorflow.metrics.accuracy", "tf_metrics.precision", "tf_metrics.recall", "tf_metrics.f1", "tensorflow.summary.scalar", "tensorflow.estimator.EstimatorSpec", "pathlib.Path", "tensorflow.matmul", "tensorflow.train.AdamOptimizer().minimize", "tensorflow.estimator.EstimatorSpec", "tensorflow.shape", "tensorflow.shape", "int", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.shape", "tensorflow.shape", "tensorflow.train.AdamOptimizer", "tensorflow.train.get_or_create_global_step"], "function", ["None"], ["def", "model_fn", "(", "mode", ",", "features", ",", "labels", ")", ":", "\n", "# Logging", "\n", "    ", "Path", "(", "'results'", ")", ".", "mkdir", "(", "exist_ok", "=", "True", ")", "\n", "tf", ".", "logging", ".", "set_verbosity", "(", "logging", ".", "INFO", ")", "\n", "handlers", "=", "[", "logging", ".", "FileHandler", "(", "'./results/main.log'", ")", ",", "\n", "logging", ".", "StreamHandler", "(", "sys", ".", "stdout", ")", "]", "\n", "logging", ".", "getLogger", "(", "'tensorflow'", ")", ".", "handlers", "=", "handlers", "\n", "\n", "word_inputs", ",", "char_inputs", "=", "features", "\n", "\n", "training", "=", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ")", "\n", "\n", "# Embeddings", "\n", "embeddings", "=", "tf", ".", "get_variable", "(", "'embeddings'", ",", "[", "cfg", ".", "num_chars", "+", "2", ",", "cfg", ".", "char_embed_dim", "]", ")", "\n", "char_input_emb", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embeddings", ",", "char_inputs", ")", "\n", "\n", "# Reshaping for CNN", "\n", "output", "=", "tf", ".", "reshape", "(", "char_input_emb", ",", "[", "-", "1", ",", "tf", ".", "shape", "(", "char_inputs", ")", "[", "2", "]", ",", "cfg", ".", "char_embed_dim", "]", ")", "\n", "\n", "# CNN", "\n", "output", "=", "tf", ".", "layers", ".", "conv1d", "(", "output", ",", "filters", "=", "64", ",", "kernel_size", "=", "2", ",", "strides", "=", "1", ",", "padding", "=", "\"same\"", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "output", "=", "tf", ".", "layers", ".", "max_pooling1d", "(", "output", ",", "pool_size", "=", "2", ",", "strides", "=", "2", ")", "\n", "output", "=", "tf", ".", "layers", ".", "conv1d", "(", "output", ",", "filters", "=", "128", ",", "kernel_size", "=", "2", ",", "strides", "=", "1", ",", "padding", "=", "\"same\"", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "output", "=", "tf", ".", "layers", ".", "max_pooling1d", "(", "output", ",", "pool_size", "=", "2", ",", "strides", "=", "2", ")", "\n", "\n", "cnn_output", "=", "tf", ".", "layers", ".", "dropout", "(", "output", ",", "rate", "=", ".5", ",", "training", "=", "training", ")", "\n", "cnn_output", "=", "tf", ".", "layers", ".", "flatten", "(", "cnn_output", ")", "\n", "\n", "# Reshaping CNN and concatenating for LSTM", "\n", "cnn_output", "=", "tf", ".", "reshape", "(", "cnn_output", ",", "[", "-", "1", ",", "tf", ".", "shape", "(", "char_inputs", ")", "[", "1", "]", ",", "128", "*", "int", "(", "cfg", ".", "word_max_len", "/", "4", ")", "]", ")", "\n", "word_inputs", "=", "tf", ".", "layers", ".", "dropout", "(", "word_inputs", ",", "rate", "=", ".5", ",", "training", "=", "training", ")", "\n", "lstm_inputs", "=", "tf", ".", "concat", "(", "[", "word_inputs", ",", "cnn_output", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# LSTM", "\n", "#with tf.variable_scope('lstm_1'):", "\n", "#    cell = tf.contrib.rnn.LSTMCell(num_units=cfg.lstm_units)", "\n", "#    lstm_inputs, _ = tf.nn.dynamic_rnn(cell, lstm_inputs, dtype=tf.float32)", "\n", "\n", "#with tf.variable_scope('lstm_2'):", "\n", "cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMCell", "(", "num_units", "=", "cfg", ".", "lstm_units", ")", "\n", "outputs", ",", "state", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "cell", ",", "lstm_inputs", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "outputs", "=", "tf", ".", "layers", ".", "dropout", "(", "outputs", ",", "rate", "=", ".5", ",", "training", "=", "training", ")", "\n", "W", "=", "tf", ".", "Variable", "(", "tf", ".", "random_normal", "(", "[", "cfg", ".", "lstm_units", "]", ",", "stddev", "=", "0.1", ")", ")", "\n", "#H = fw_outputs + bw_outputs", "\n", "H", "=", "outputs", "\n", "M", "=", "tf", ".", "tanh", "(", "H", ")", "\n", "\n", "#alpha = tf.nn.softmax(tf.reshape(tf.matmul(tf.reshape(M, [-1, cfg.lstm_units]), tf.reshape(W, [-1, 1])), (-1, cfg.word_max_len)))", "\n", "alpha", "=", "tf", ".", "nn", ".", "softmax", "(", "tf", ".", "reshape", "(", "tf", ".", "matmul", "(", "tf", ".", "reshape", "(", "M", ",", "[", "-", "1", ",", "cfg", ".", "lstm_units", "]", ")", ",", "tf", ".", "reshape", "(", "W", ",", "[", "-", "1", ",", "1", "]", ")", ")", ",", "(", "-", "1", ",", "tf", ".", "shape", "(", "word_inputs", ")", "[", "1", "]", ")", ")", ")", "\n", "r", "=", "tf", ".", "matmul", "(", "tf", ".", "transpose", "(", "H", ",", "[", "0", ",", "2", ",", "1", "]", ")", ",", "tf", ".", "reshape", "(", "alpha", ",", "[", "-", "1", ",", "tf", ".", "shape", "(", "word_inputs", ")", "[", "1", "]", ",", "1", "]", ")", ")", "\n", "r", "=", "tf", ".", "squeeze", "(", "r", ")", "\n", "\n", "h_star", "=", "tf", ".", "tanh", "(", "r", ")", "\n", "h_drop", "=", "tf", ".", "nn", ".", "dropout", "(", "h_star", ",", ".5", ")", "\n", "\n", "#c_output = tf.concat([bw_state.c, fw_state.c], axis=-1)", "\n", "#h_output = tf.concat([bw_state.h, fw_state.h], axis=-1)", "\n", "#output = tf.contrib.rnn.LSTMStateTuple(c=c_output, h=h_output)", "\n", "\n", "#output = tf.concat([bw_state.c, bw_state.h, fw_state.c, fw_state.h], axis=-1)", "\n", "#lstm_output = tf.layers.dropout(output, rate=.5, training=training)", "\n", "\n", "# Dense", "\n", "FC_W", "=", "tf", ".", "Variable", "(", "tf", ".", "truncated_normal", "(", "[", "cfg", ".", "lstm_units", ",", "2", "]", ",", "stddev", "=", "0.1", ")", ")", "\n", "FC_b", "=", "tf", ".", "Variable", "(", "tf", ".", "constant", "(", "0.", ",", "shape", "=", "[", "2", "]", ")", ")", "\n", "logits", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "h_drop", ",", "FC_W", ",", "FC_b", ")", "\n", "\n", "#output = tf.layers.dense(h_drop, 128)", "\n", "#output = tf.layers.dropout(output, rate=.5, training=training)", "\n", "#logits = tf.layers.dense(output, 2)", "\n", "\n", "# Loss", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits", "(", "labels", "=", "labels", ",", "logits", "=", "logits", ")", ")", "\n", "\n", "#optimizer = tf.train.AdamOptimizer(1e-4)", "\n", "#gradients, variables = zip(*optimizer.compute_gradients(loss))", "\n", "#gradients, _ = tf.clip_by_global_norm(gradients, .1)", "\n", "#train_op = optimizer.apply_gradients(zip(gradients, variables), tf.train.get_global_step())  ", "\n", "\n", "# Metrics", "\n", "indices", "=", "[", "0", ",", "1", "]", "\n", "labels", "=", "tf", ".", "argmax", "(", "labels", ",", "1", ")", "\n", "pred_ids", "=", "tf", ".", "argmax", "(", "logits", ",", "1", ")", "\n", "\n", "metrics", "=", "{", "\n", "'acc'", ":", "tf", ".", "metrics", ".", "accuracy", "(", "labels", ",", "pred_ids", ")", ",", "\n", "'precision'", ":", "precision", "(", "labels", ",", "pred_ids", ",", "2", ",", "indices", ",", "None", ",", "average", "=", "'macro'", ")", ",", "\n", "'recall'", ":", "recall", "(", "labels", ",", "pred_ids", ",", "2", ",", "indices", ",", "None", ",", "average", "=", "'macro'", ")", ",", "\n", "'f1'", ":", "f1", "(", "labels", ",", "pred_ids", ",", "2", ",", "indices", ",", "None", ",", "average", "=", "'macro'", ")", "\n", "}", "\n", "\n", "for", "metric_name", ",", "op", "in", "metrics", ".", "items", "(", ")", ":", "\n", "        ", "tf", ".", "summary", ".", "scalar", "(", "metric_name", ",", "op", "[", "1", "]", ")", "\n", "\n", "", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "        ", "return", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "mode", ",", "loss", "=", "loss", ",", "\n", "eval_metric_ops", "=", "metrics", ")", "\n", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "        ", "train_op", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "cfg", ".", "learning_rate", ")", ".", "minimize", "(", "loss", ",", "global_step", "=", "tf", ".", "train", ".", "get_or_create_global_step", "(", ")", ")", "\n", "\n", "return", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "mode", ",", "loss", "=", "loss", ",", "\n", "train_op", "=", "train_op", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_lstm_attn.input_fn": [[126, 139], ["tensorflow.data.Dataset.from_generator", "dataset.shuffle().repeat.padded_batch", "data_loader.data_generator", "dataset.shuffle().repeat.shuffle().repeat", "dataset.shuffle().repeat.shuffle"], "function", ["home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader.data_generator"], ["", "", "def", "input_fn", "(", "mode", "=", "None", ")", ":", "\n", "    ", "data_generator", "=", "lambda", ":", "data_loader", ".", "data_generator", "(", "mode", "=", "mode", ")", "\n", "\n", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "from_generator", "(", "data_generator", ",", "\n", "output_types", "=", "(", "(", "tf", ".", "float32", ",", "tf", ".", "int32", ")", ",", "tf", ".", "int32", ")", ",", "\n", "output_shapes", "=", "(", "(", "[", "None", ",", "cfg", ".", "word_embed_dim", "]", ",", "[", "None", ",", "None", "]", ")", ",", "[", "None", "]", ")", ")", "\n", "\n", "if", "mode", "is", "'train'", ":", "\n", "        ", "dataset", "=", "dataset", ".", "shuffle", "(", "cfg", ".", "shuffle_buffer", ")", ".", "repeat", "(", "cfg", ".", "num_epochs", ")", "\n", "\n", "", "dataset", "=", "dataset", ".", "padded_batch", "(", "cfg", ".", "batch_size", ",", "padded_shapes", "=", "(", "(", "[", "None", ",", "cfg", ".", "word_embed_dim", "]", ",", "[", "None", ",", "None", "]", ")", ",", "[", "None", "]", ")", ")", "\n", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_lstm_attn.train": [[141, 152], ["tensorflow.estimator.RunConfig", "tensorflow.estimator.Estimator", "tensorflow.estimator.TrainSpec", "tensorflow.estimator.EvalSpec", "tensorflow.estimator.train_and_evaluate", "char_cnn_word_lstm_attn.input_fn", "char_cnn_word_lstm_attn.input_fn"], "function", ["home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_blstm_attn.input_fn", "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_blstm_attn.input_fn"], ["", "def", "train", "(", ")", ":", "\n", "    ", "train_input_func", "=", "lambda", ":", "input_fn", "(", "mode", "=", "'train'", ")", "\n", "eval_input_func", "=", "lambda", ":", "input_fn", "(", "mode", "=", "'valid'", ")", "\n", "\n", "est_conf", "=", "tf", ".", "estimator", ".", "RunConfig", "(", "cfg", ".", "model_dir", ",", "save_checkpoints_secs", "=", "30", ",", "keep_checkpoint_max", "=", "500", ")", "\n", "est", "=", "tf", ".", "estimator", ".", "Estimator", "(", "model_fn", ",", "cfg", ".", "model_dir", ",", "est_conf", ")", "\n", "\n", "train_spec", "=", "tf", ".", "estimator", ".", "TrainSpec", "(", "input_fn", "=", "train_input_func", ")", "\n", "eval_spec", "=", "tf", ".", "estimator", ".", "EvalSpec", "(", "input_fn", "=", "eval_input_func", ",", "throttle_secs", "=", "30", ")", "\n", "\n", "tf", ".", "estimator", ".", "train_and_evaluate", "(", "est", ",", "train_spec", ",", "eval_spec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_lstm.model_fn": [[21, 99], ["pathlib.Path().mkdir", "tensorflow.logging.set_verbosity", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.reshape", "tensorflow.layers.conv1d", "tensorflow.layers.max_pooling1d", "tensorflow.layers.conv1d", "tensorflow.layers.max_pooling1d", "tensorflow.layers.dropout", "tensorflow.layers.flatten", "tensorflow.reshape", "tensorflow.layers.dropout", "tensorflow.concat", "tensorflow.contrib.rnn.LSTMCell", "tensorflow.nn.dynamic_rnn", "tensorflow.concat", "tensorflow.layers.dropout", "tensorflow.layers.dense", "tensorflow.layers.dropout", "tensorflow.layers.dense", "tensorflow.reduce_mean", "tensorflow.argmax", "tensorflow.argmax", "metrics.items", "logging.FileHandler", "logging.StreamHandler", "logging.getLogger", "tensorflow.nn.softmax_cross_entropy_with_logits", "tensorflow.metrics.accuracy", "tf_metrics.precision", "tf_metrics.recall", "tf_metrics.f1", "tensorflow.summary.scalar", "tensorflow.estimator.EstimatorSpec", "pathlib.Path", "tensorflow.train.AdamOptimizer().minimize", "tensorflow.estimator.EstimatorSpec", "tensorflow.shape", "tensorflow.shape", "int", "tensorflow.train.AdamOptimizer", "tensorflow.train.get_or_create_global_step"], "function", ["None"], ["def", "model_fn", "(", "mode", ",", "features", ",", "labels", ")", ":", "\n", "# Logging", "\n", "    ", "Path", "(", "'results'", ")", ".", "mkdir", "(", "exist_ok", "=", "True", ")", "\n", "tf", ".", "logging", ".", "set_verbosity", "(", "logging", ".", "INFO", ")", "\n", "handlers", "=", "[", "logging", ".", "FileHandler", "(", "'./results/main.log'", ")", ",", "\n", "logging", ".", "StreamHandler", "(", "sys", ".", "stdout", ")", "]", "\n", "logging", ".", "getLogger", "(", "'tensorflow'", ")", ".", "handlers", "=", "handlers", "\n", "\n", "word_inputs", ",", "char_inputs", "=", "features", "\n", "\n", "training", "=", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ")", "\n", "\n", "# Embeddings", "\n", "embeddings", "=", "tf", ".", "get_variable", "(", "'embeddings'", ",", "[", "cfg", ".", "num_chars", "+", "2", ",", "cfg", ".", "char_embed_dim", "]", ")", "\n", "char_input_emb", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embeddings", ",", "char_inputs", ")", "\n", "\n", "# Reshaping for CNN", "\n", "output", "=", "tf", ".", "reshape", "(", "char_input_emb", ",", "[", "-", "1", ",", "tf", ".", "shape", "(", "char_inputs", ")", "[", "2", "]", ",", "cfg", ".", "char_embed_dim", "]", ")", "\n", "\n", "# CNN", "\n", "output", "=", "tf", ".", "layers", ".", "conv1d", "(", "output", ",", "filters", "=", "64", ",", "kernel_size", "=", "2", ",", "strides", "=", "1", ",", "padding", "=", "\"same\"", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "output", "=", "tf", ".", "layers", ".", "max_pooling1d", "(", "output", ",", "pool_size", "=", "2", ",", "strides", "=", "2", ")", "\n", "output", "=", "tf", ".", "layers", ".", "conv1d", "(", "output", ",", "filters", "=", "128", ",", "kernel_size", "=", "2", ",", "strides", "=", "1", ",", "padding", "=", "\"same\"", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "output", "=", "tf", ".", "layers", ".", "max_pooling1d", "(", "output", ",", "pool_size", "=", "2", ",", "strides", "=", "2", ")", "\n", "\n", "cnn_output", "=", "tf", ".", "layers", ".", "dropout", "(", "output", ",", "rate", "=", ".5", ",", "training", "=", "training", ")", "\n", "cnn_output", "=", "tf", ".", "layers", ".", "flatten", "(", "cnn_output", ")", "\n", "\n", "# Reshaping CNN and concatenating for LSTM", "\n", "cnn_output", "=", "tf", ".", "reshape", "(", "cnn_output", ",", "[", "-", "1", ",", "tf", ".", "shape", "(", "char_inputs", ")", "[", "1", "]", ",", "128", "*", "int", "(", "cfg", ".", "word_max_len", "/", "4", ")", "]", ")", "\n", "word_inputs", "=", "tf", ".", "layers", ".", "dropout", "(", "word_inputs", ",", "rate", "=", ".5", ",", "training", "=", "training", ")", "\n", "lstm_inputs", "=", "tf", ".", "concat", "(", "[", "word_inputs", ",", "cnn_output", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# LSTM", "\n", "cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMCell", "(", "num_units", "=", "cfg", ".", "lstm_units", ")", "\n", "_", ",", "final_state", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "cell", ",", "lstm_inputs", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "output", "=", "tf", ".", "concat", "(", "final_state", ",", "axis", "=", "-", "1", ")", "\n", "lstm_output", "=", "tf", ".", "layers", ".", "dropout", "(", "output", ",", "rate", "=", ".5", ",", "training", "=", "training", ")", "\n", "\n", "# Dense", "\n", "output", "=", "tf", ".", "layers", ".", "dense", "(", "lstm_output", ",", "128", ")", "\n", "output", "=", "tf", ".", "layers", ".", "dropout", "(", "output", ",", "rate", "=", ".5", ",", "training", "=", "training", ")", "\n", "logits", "=", "tf", ".", "layers", ".", "dense", "(", "output", ",", "2", ")", "\n", "\n", "# Loss", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits", "(", "labels", "=", "labels", ",", "logits", "=", "logits", ")", ")", "\n", "\n", "# Gradient clipping", "\n", "# optimizer = tf.train.AdamOptimizer(1e-4)", "\n", "# gradients, variables = zip(*optimizer.compute_gradients(loss))", "\n", "# gradients, _ = tf.clip_by_global_norm(gradients, .05)", "\n", "# train_op = optimizer.apply_gradients(zip(gradients, variables), tf.train.get_global_step())  ", "\n", "\n", "# Metrics", "\n", "indices", "=", "[", "0", ",", "1", "]", "\n", "labels", "=", "tf", ".", "argmax", "(", "labels", ",", "1", ")", "\n", "pred_ids", "=", "tf", ".", "argmax", "(", "logits", ",", "1", ")", "\n", "\n", "metrics", "=", "{", "\n", "'acc'", ":", "tf", ".", "metrics", ".", "accuracy", "(", "labels", ",", "pred_ids", ")", ",", "\n", "'precision'", ":", "precision", "(", "labels", ",", "pred_ids", ",", "2", ",", "indices", ",", "None", ",", "average", "=", "'macro'", ")", ",", "\n", "'recall'", ":", "recall", "(", "labels", ",", "pred_ids", ",", "2", ",", "indices", ",", "None", ",", "average", "=", "'macro'", ")", ",", "\n", "'f1'", ":", "f1", "(", "labels", ",", "pred_ids", ",", "2", ",", "indices", ",", "None", ",", "average", "=", "'macro'", ")", "\n", "}", "\n", "\n", "for", "metric_name", ",", "op", "in", "metrics", ".", "items", "(", ")", ":", "\n", "        ", "tf", ".", "summary", ".", "scalar", "(", "metric_name", ",", "op", "[", "1", "]", ")", "\n", "\n", "", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "        ", "return", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "mode", ",", "loss", "=", "loss", ",", "\n", "eval_metric_ops", "=", "metrics", ")", "\n", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "        ", "train_op", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "cfg", ".", "learning_rate", ")", ".", "minimize", "(", "loss", ",", "global_step", "=", "tf", ".", "train", ".", "get_or_create_global_step", "(", ")", ")", "\n", "\n", "return", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "mode", ",", "loss", "=", "loss", ",", "\n", "train_op", "=", "train_op", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_lstm.input_fn": [[101, 114], ["tensorflow.data.Dataset.from_generator", "dataset.shuffle().repeat.padded_batch", "data_loader.data_generator", "dataset.shuffle().repeat.shuffle().repeat", "dataset.shuffle().repeat.shuffle"], "function", ["home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader.data_generator"], ["", "", "def", "input_fn", "(", "mode", "=", "None", ")", ":", "\n", "    ", "data_generator", "=", "lambda", ":", "data_loader", ".", "data_generator", "(", "mode", "=", "mode", ")", "\n", "\n", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "from_generator", "(", "data_generator", ",", "\n", "output_types", "=", "(", "(", "tf", ".", "float32", ",", "tf", ".", "int32", ")", ",", "tf", ".", "int32", ")", ",", "\n", "output_shapes", "=", "(", "(", "[", "None", ",", "cfg", ".", "word_embed_dim", "]", ",", "[", "None", ",", "None", "]", ")", ",", "[", "None", "]", ")", ")", "\n", "\n", "if", "mode", "is", "'train'", ":", "\n", "        ", "dataset", "=", "dataset", ".", "shuffle", "(", "cfg", ".", "shuffle_buffer", ")", ".", "repeat", "(", "cfg", ".", "num_epochs", ")", "\n", "\n", "", "dataset", "=", "dataset", ".", "padded_batch", "(", "cfg", ".", "batch_size", ",", "padded_shapes", "=", "(", "(", "[", "None", ",", "cfg", ".", "word_embed_dim", "]", ",", "[", "None", ",", "None", "]", ")", ",", "[", "None", "]", ")", ")", "\n", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_lstm.train": [[116, 127], ["tensorflow.estimator.RunConfig", "tensorflow.estimator.Estimator", "tensorflow.estimator.TrainSpec", "tensorflow.estimator.EvalSpec", "tensorflow.estimator.train_and_evaluate", "char_cnn_word_lstm.input_fn", "char_cnn_word_lstm.input_fn"], "function", ["home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_blstm_attn.input_fn", "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_blstm_attn.input_fn"], ["", "def", "train", "(", ")", ":", "\n", "    ", "train_input_func", "=", "lambda", ":", "input_fn", "(", "mode", "=", "'train'", ")", "\n", "eval_input_func", "=", "lambda", ":", "input_fn", "(", "mode", "=", "'valid'", ")", "\n", "\n", "est_conf", "=", "tf", ".", "estimator", ".", "RunConfig", "(", "cfg", ".", "model_dir", ",", "save_checkpoints_secs", "=", "30", ",", "keep_checkpoint_max", "=", "500", ")", "\n", "est", "=", "tf", ".", "estimator", ".", "Estimator", "(", "model_fn", ",", "cfg", ".", "model_dir", ",", "est_conf", ")", "\n", "\n", "train_spec", "=", "tf", ".", "estimator", ".", "TrainSpec", "(", "input_fn", "=", "train_input_func", ")", "\n", "eval_spec", "=", "tf", ".", "estimator", ".", "EvalSpec", "(", "input_fn", "=", "eval_input_func", ",", "throttle_secs", "=", "30", ")", "\n", "\n", "tf", ".", "estimator", ".", "train_and_evaluate", "(", "est", ",", "train_spec", ",", "eval_spec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_blstm.model_fn": [[21, 100], ["pathlib.Path().mkdir", "tensorflow.logging.set_verbosity", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.reshape", "tensorflow.layers.conv1d", "tensorflow.layers.max_pooling1d", "tensorflow.layers.conv1d", "tensorflow.layers.max_pooling1d", "tensorflow.layers.dropout", "tensorflow.layers.flatten", "tensorflow.reshape", "tensorflow.layers.dropout", "tensorflow.concat", "tensorflow.contrib.rnn.LSTMCell", "tensorflow.contrib.rnn.LSTMCell", "tensorflow.nn.bidirectional_dynamic_rnn", "tensorflow.concat", "tensorflow.layers.dropout", "tensorflow.layers.dense", "tensorflow.layers.dropout", "tensorflow.layers.dense", "tensorflow.reduce_mean", "tensorflow.argmax", "tensorflow.argmax", "metrics.items", "logging.FileHandler", "logging.StreamHandler", "logging.getLogger", "tensorflow.nn.softmax_cross_entropy_with_logits", "tensorflow.metrics.accuracy", "tf_metrics.precision", "tf_metrics.recall", "tf_metrics.f1", "tensorflow.summary.scalar", "tensorflow.estimator.EstimatorSpec", "pathlib.Path", "tensorflow.train.AdamOptimizer().minimize", "tensorflow.estimator.EstimatorSpec", "tensorflow.shape", "tensorflow.shape", "int", "tensorflow.train.AdamOptimizer", "tensorflow.train.get_or_create_global_step"], "function", ["None"], ["def", "model_fn", "(", "mode", ",", "features", ",", "labels", ")", ":", "\n", "# Logging", "\n", "    ", "Path", "(", "'results'", ")", ".", "mkdir", "(", "exist_ok", "=", "True", ")", "\n", "tf", ".", "logging", ".", "set_verbosity", "(", "logging", ".", "INFO", ")", "\n", "handlers", "=", "[", "logging", ".", "FileHandler", "(", "'./results/main.log'", ")", ",", "\n", "logging", ".", "StreamHandler", "(", "sys", ".", "stdout", ")", "]", "\n", "logging", ".", "getLogger", "(", "'tensorflow'", ")", ".", "handlers", "=", "handlers", "\n", "\n", "word_inputs", ",", "char_inputs", "=", "features", "\n", "\n", "training", "=", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ")", "\n", "\n", "# Embeddings", "\n", "embeddings", "=", "tf", ".", "get_variable", "(", "'embeddings'", ",", "[", "cfg", ".", "num_chars", "+", "2", ",", "cfg", ".", "char_embed_dim", "]", ")", "\n", "char_input_emb", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embeddings", ",", "char_inputs", ")", "\n", "\n", "# Reshaping for CNN", "\n", "output", "=", "tf", ".", "reshape", "(", "char_input_emb", ",", "[", "-", "1", ",", "tf", ".", "shape", "(", "char_inputs", ")", "[", "2", "]", ",", "cfg", ".", "char_embed_dim", "]", ")", "\n", "\n", "# CNN", "\n", "output", "=", "tf", ".", "layers", ".", "conv1d", "(", "output", ",", "filters", "=", "64", ",", "kernel_size", "=", "2", ",", "strides", "=", "1", ",", "padding", "=", "\"same\"", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "output", "=", "tf", ".", "layers", ".", "max_pooling1d", "(", "output", ",", "pool_size", "=", "2", ",", "strides", "=", "2", ")", "\n", "output", "=", "tf", ".", "layers", ".", "conv1d", "(", "output", ",", "filters", "=", "128", ",", "kernel_size", "=", "2", ",", "strides", "=", "1", ",", "padding", "=", "\"same\"", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "output", "=", "tf", ".", "layers", ".", "max_pooling1d", "(", "output", ",", "pool_size", "=", "2", ",", "strides", "=", "2", ")", "\n", "\n", "cnn_output", "=", "tf", ".", "layers", ".", "dropout", "(", "output", ",", "rate", "=", ".5", ",", "training", "=", "training", ")", "\n", "cnn_output", "=", "tf", ".", "layers", ".", "flatten", "(", "cnn_output", ")", "\n", "\n", "# Reshaping CNN and concatenating for LSTM", "\n", "cnn_output", "=", "tf", ".", "reshape", "(", "cnn_output", ",", "[", "-", "1", ",", "tf", ".", "shape", "(", "char_inputs", ")", "[", "1", "]", ",", "128", "*", "int", "(", "cfg", ".", "word_max_len", "/", "4", ")", "]", ")", "\n", "word_inputs", "=", "tf", ".", "layers", ".", "dropout", "(", "word_inputs", ",", "rate", "=", ".5", ",", "training", "=", "training", ")", "\n", "lstm_inputs", "=", "tf", ".", "concat", "(", "[", "word_inputs", ",", "cnn_output", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# LSTM", "\n", "fw_cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMCell", "(", "num_units", "=", "cfg", ".", "lstm_units", ")", "\n", "bw_cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMCell", "(", "num_units", "=", "cfg", ".", "lstm_units", ")", "\n", "_", ",", "(", "fw_state", ",", "bw_state", ")", "=", "tf", ".", "nn", ".", "bidirectional_dynamic_rnn", "(", "fw_cell", ",", "bw_cell", ",", "lstm_inputs", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "output", "=", "tf", ".", "concat", "(", "[", "bw_state", ".", "c", ",", "bw_state", ".", "h", ",", "fw_state", ".", "c", ",", "fw_state", ".", "h", "]", ",", "axis", "=", "-", "1", ")", "\n", "lstm_output", "=", "tf", ".", "layers", ".", "dropout", "(", "output", ",", "rate", "=", ".5", ",", "training", "=", "training", ")", "\n", "\n", "# Dense", "\n", "output", "=", "tf", ".", "layers", ".", "dense", "(", "lstm_output", ",", "128", ")", "\n", "output", "=", "tf", ".", "layers", ".", "dropout", "(", "output", ",", "rate", "=", ".5", ",", "training", "=", "training", ")", "\n", "logits", "=", "tf", ".", "layers", ".", "dense", "(", "output", ",", "2", ")", "\n", "\n", "# Loss", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits", "(", "labels", "=", "labels", ",", "logits", "=", "logits", ")", ")", "\n", "\n", "# Gradient clipping", "\n", "# optimizer = tf.train.AdamOptimizer(1e-4)", "\n", "# gradients, variables = zip(*optimizer.compute_gradients(loss))", "\n", "# gradients, _ = tf.clip_by_global_norm(gradients, .1)", "\n", "# train_op = optimizer.apply_gradients(zip(gradients, variables), tf.train.get_global_step())  ", "\n", "\n", "# Metrics", "\n", "indices", "=", "[", "0", ",", "1", "]", "\n", "labels", "=", "tf", ".", "argmax", "(", "labels", ",", "1", ")", "\n", "pred_ids", "=", "tf", ".", "argmax", "(", "logits", ",", "1", ")", "\n", "\n", "metrics", "=", "{", "\n", "'acc'", ":", "tf", ".", "metrics", ".", "accuracy", "(", "labels", ",", "pred_ids", ")", ",", "\n", "'precision'", ":", "precision", "(", "labels", ",", "pred_ids", ",", "2", ",", "indices", ",", "None", ",", "average", "=", "'macro'", ")", ",", "\n", "'recall'", ":", "recall", "(", "labels", ",", "pred_ids", ",", "2", ",", "indices", ",", "None", ",", "average", "=", "'macro'", ")", ",", "\n", "'f1'", ":", "f1", "(", "labels", ",", "pred_ids", ",", "2", ",", "indices", ",", "None", ",", "average", "=", "'macro'", ")", "\n", "}", "\n", "\n", "for", "metric_name", ",", "op", "in", "metrics", ".", "items", "(", ")", ":", "\n", "        ", "tf", ".", "summary", ".", "scalar", "(", "metric_name", ",", "op", "[", "1", "]", ")", "\n", "\n", "", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "        ", "return", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "mode", ",", "loss", "=", "loss", ",", "\n", "eval_metric_ops", "=", "metrics", ")", "\n", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "        ", "train_op", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "cfg", ".", "learning_rate", ")", ".", "minimize", "(", "loss", ",", "global_step", "=", "tf", ".", "train", ".", "get_or_create_global_step", "(", ")", ")", "\n", "\n", "return", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "mode", ",", "loss", "=", "loss", ",", "\n", "train_op", "=", "train_op", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_blstm.input_fn": [[102, 115], ["tensorflow.data.Dataset.from_generator", "dataset.shuffle().repeat.padded_batch", "data_loader.data_generator", "dataset.shuffle().repeat.shuffle().repeat", "dataset.shuffle().repeat.shuffle"], "function", ["home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader.data_generator"], ["", "", "def", "input_fn", "(", "mode", "=", "None", ")", ":", "\n", "    ", "data_generator", "=", "lambda", ":", "data_loader", ".", "data_generator", "(", "mode", "=", "mode", ")", "\n", "\n", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "from_generator", "(", "data_generator", ",", "\n", "output_types", "=", "(", "(", "tf", ".", "float32", ",", "tf", ".", "int32", ")", ",", "tf", ".", "int32", ")", ",", "\n", "output_shapes", "=", "(", "(", "[", "None", ",", "cfg", ".", "word_embed_dim", "]", ",", "[", "None", ",", "None", "]", ")", ",", "[", "None", "]", ")", ")", "\n", "\n", "if", "mode", "is", "'train'", ":", "\n", "        ", "dataset", "=", "dataset", ".", "shuffle", "(", "cfg", ".", "shuffle_buffer", ")", ".", "repeat", "(", "cfg", ".", "num_epochs", ")", "\n", "\n", "", "dataset", "=", "dataset", ".", "padded_batch", "(", "cfg", ".", "batch_size", ",", "padded_shapes", "=", "(", "(", "[", "None", ",", "cfg", ".", "word_embed_dim", "]", ",", "[", "None", ",", "None", "]", ")", ",", "[", "None", "]", ")", ")", "\n", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_blstm.train": [[117, 128], ["tensorflow.estimator.RunConfig", "tensorflow.estimator.Estimator", "tensorflow.estimator.TrainSpec", "tensorflow.estimator.EvalSpec", "tensorflow.estimator.train_and_evaluate", "char_cnn_word_blstm.input_fn", "char_cnn_word_blstm.input_fn"], "function", ["home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_blstm_attn.input_fn", "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_blstm_attn.input_fn"], ["", "def", "train", "(", ")", ":", "\n", "    ", "train_input_func", "=", "lambda", ":", "input_fn", "(", "mode", "=", "'train'", ")", "\n", "eval_input_func", "=", "lambda", ":", "input_fn", "(", "mode", "=", "'valid'", ")", "\n", "\n", "est_conf", "=", "tf", ".", "estimator", ".", "RunConfig", "(", "cfg", ".", "model_dir", ",", "save_checkpoints_secs", "=", "30", ",", "keep_checkpoint_max", "=", "500", ")", "\n", "est", "=", "tf", ".", "estimator", ".", "Estimator", "(", "model_fn", ",", "cfg", ".", "model_dir", ",", "est_conf", ")", "\n", "\n", "train_spec", "=", "tf", ".", "estimator", ".", "TrainSpec", "(", "input_fn", "=", "train_input_func", ")", "\n", "eval_spec", "=", "tf", ".", "estimator", ".", "EvalSpec", "(", "input_fn", "=", "eval_input_func", ",", "throttle_secs", "=", "30", ")", "\n", "\n", "tf", ".", "estimator", ".", "train_and_evaluate", "(", "est", ",", "train_spec", ",", "eval_spec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_blstm_attn.model_fn": [[21, 107], ["pathlib.Path().mkdir", "tensorflow.logging.set_verbosity", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.reshape", "tensorflow.layers.conv1d", "tensorflow.layers.max_pooling1d", "tensorflow.layers.conv1d", "tensorflow.layers.max_pooling1d", "tensorflow.layers.dropout", "tensorflow.layers.flatten", "tensorflow.reshape", "tensorflow.layers.dropout", "tensorflow.concat", "tensorflow.contrib.rnn.LSTMCell", "tensorflow.contrib.rnn.LSTMCell", "tensorflow.nn.bidirectional_dynamic_rnn", "tensorflow.Variable", "tensorflow.tanh", "tensorflow.nn.softmax", "tensorflow.matmul", "tensorflow.squeeze", "tensorflow.tanh", "tensorflow.nn.dropout", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.nn.xw_plus_b", "tensorflow.reduce_mean", "tensorflow.argmax", "tensorflow.argmax", "metrics.items", "logging.FileHandler", "logging.StreamHandler", "logging.getLogger", "tensorflow.random_normal", "tensorflow.reshape", "tensorflow.transpose", "tensorflow.reshape", "tensorflow.truncated_normal", "tensorflow.constant", "tensorflow.nn.softmax_cross_entropy_with_logits", "tensorflow.metrics.accuracy", "tf_metrics.precision", "tf_metrics.recall", "tf_metrics.f1", "tensorflow.summary.scalar", "tensorflow.estimator.EstimatorSpec", "pathlib.Path", "tensorflow.matmul", "tensorflow.train.AdamOptimizer().minimize", "tensorflow.estimator.EstimatorSpec", "tensorflow.shape", "tensorflow.shape", "int", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.shape", "tensorflow.shape", "tensorflow.train.AdamOptimizer", "tensorflow.train.get_or_create_global_step"], "function", ["None"], ["def", "model_fn", "(", "mode", ",", "features", ",", "labels", ")", ":", "\n", "# Logging", "\n", "    ", "Path", "(", "'results'", ")", ".", "mkdir", "(", "exist_ok", "=", "True", ")", "\n", "tf", ".", "logging", ".", "set_verbosity", "(", "logging", ".", "INFO", ")", "\n", "handlers", "=", "[", "logging", ".", "FileHandler", "(", "'./results/main.log'", ")", ",", "\n", "logging", ".", "StreamHandler", "(", "sys", ".", "stdout", ")", "]", "\n", "logging", ".", "getLogger", "(", "'tensorflow'", ")", ".", "handlers", "=", "handlers", "\n", "\n", "word_inputs", ",", "char_inputs", "=", "features", "\n", "\n", "training", "=", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ")", "\n", "\n", "# Embeddings", "\n", "embeddings", "=", "tf", ".", "get_variable", "(", "'embeddings'", ",", "[", "cfg", ".", "num_chars", "+", "2", ",", "cfg", ".", "char_embed_dim", "]", ")", "\n", "char_input_emb", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embeddings", ",", "char_inputs", ")", "\n", "\n", "# Reshaping for CNN", "\n", "output", "=", "tf", ".", "reshape", "(", "char_input_emb", ",", "[", "-", "1", ",", "tf", ".", "shape", "(", "char_inputs", ")", "[", "2", "]", ",", "cfg", ".", "char_embed_dim", "]", ")", "\n", "\n", "# CNN", "\n", "output", "=", "tf", ".", "layers", ".", "conv1d", "(", "output", ",", "filters", "=", "64", ",", "kernel_size", "=", "2", ",", "strides", "=", "1", ",", "padding", "=", "\"same\"", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "output", "=", "tf", ".", "layers", ".", "max_pooling1d", "(", "output", ",", "pool_size", "=", "2", ",", "strides", "=", "2", ")", "\n", "output", "=", "tf", ".", "layers", ".", "conv1d", "(", "output", ",", "filters", "=", "128", ",", "kernel_size", "=", "2", ",", "strides", "=", "1", ",", "padding", "=", "\"same\"", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "output", "=", "tf", ".", "layers", ".", "max_pooling1d", "(", "output", ",", "pool_size", "=", "2", ",", "strides", "=", "2", ")", "\n", "\n", "cnn_output", "=", "tf", ".", "layers", ".", "dropout", "(", "output", ",", "rate", "=", ".5", ",", "training", "=", "training", ")", "\n", "cnn_output", "=", "tf", ".", "layers", ".", "flatten", "(", "cnn_output", ")", "\n", "\n", "# Reshaping CNN and concatenating for LSTM", "\n", "cnn_output", "=", "tf", ".", "reshape", "(", "cnn_output", ",", "[", "-", "1", ",", "tf", ".", "shape", "(", "char_inputs", ")", "[", "1", "]", ",", "128", "*", "int", "(", "cfg", ".", "word_max_len", "/", "4", ")", "]", ")", "\n", "word_inputs", "=", "tf", ".", "layers", ".", "dropout", "(", "word_inputs", ",", "rate", "=", ".5", ",", "training", "=", "training", ")", "\n", "lstm_inputs", "=", "tf", ".", "concat", "(", "[", "word_inputs", ",", "cnn_output", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# LSTM", "\n", "fw_cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMCell", "(", "num_units", "=", "cfg", ".", "lstm_units", ")", "\n", "bw_cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMCell", "(", "num_units", "=", "cfg", ".", "lstm_units", ")", "\n", "(", "fw_outputs", ",", "bw_outputs", ")", ",", "(", "fw_state", ",", "bw_state", ")", "=", "tf", ".", "nn", ".", "bidirectional_dynamic_rnn", "(", "fw_cell", ",", "bw_cell", ",", "lstm_inputs", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "# Attention", "\n", "W", "=", "tf", ".", "Variable", "(", "tf", ".", "random_normal", "(", "[", "cfg", ".", "lstm_units", "]", ",", "stddev", "=", "0.1", ")", ")", "\n", "H", "=", "fw_outputs", "+", "bw_outputs", "\n", "M", "=", "tf", ".", "tanh", "(", "H", ")", "\n", "alpha", "=", "tf", ".", "nn", ".", "softmax", "(", "tf", ".", "reshape", "(", "tf", ".", "matmul", "(", "tf", ".", "reshape", "(", "M", ",", "[", "-", "1", ",", "cfg", ".", "lstm_units", "]", ")", ",", "tf", ".", "reshape", "(", "W", ",", "[", "-", "1", ",", "1", "]", ")", ")", ",", "(", "-", "1", ",", "tf", ".", "shape", "(", "word_inputs", ")", "[", "1", "]", ")", ")", ")", "\n", "r", "=", "tf", ".", "matmul", "(", "tf", ".", "transpose", "(", "H", ",", "[", "0", ",", "2", ",", "1", "]", ")", ",", "tf", ".", "reshape", "(", "alpha", ",", "[", "-", "1", ",", "tf", ".", "shape", "(", "word_inputs", ")", "[", "1", "]", ",", "1", "]", ")", ")", "\n", "r", "=", "tf", ".", "squeeze", "(", "r", ")", "\n", "h_star", "=", "tf", ".", "tanh", "(", "r", ")", "\n", "h_drop", "=", "tf", ".", "nn", ".", "dropout", "(", "h_star", ",", ".5", ")", "\n", "\n", "# Dense", "\n", "FC_W", "=", "tf", ".", "Variable", "(", "tf", ".", "truncated_normal", "(", "[", "cfg", ".", "lstm_units", ",", "2", "]", ",", "stddev", "=", "0.1", ")", ")", "\n", "FC_b", "=", "tf", ".", "Variable", "(", "tf", ".", "constant", "(", "0.", ",", "shape", "=", "[", "2", "]", ")", ")", "\n", "logits", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "h_drop", ",", "FC_W", ",", "FC_b", ")", "\n", "\n", "# Loss", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits", "(", "labels", "=", "labels", ",", "logits", "=", "logits", ")", ")", "\n", "\n", "# Gradient clipping", "\n", "# optimizer = tf.train.AdamOptimizer(1e-4)", "\n", "# gradients, variables = zip(*optimizer.compute_gradients(loss))", "\n", "# gradients, _ = tf.clip_by_global_norm(gradients, .1)", "\n", "# train_op = optimizer.apply_gradients(zip(gradients, variables), tf.train.get_global_step())  ", "\n", "\n", "# Metrics", "\n", "indices", "=", "[", "0", ",", "1", "]", "\n", "labels", "=", "tf", ".", "argmax", "(", "labels", ",", "1", ")", "\n", "pred_ids", "=", "tf", ".", "argmax", "(", "logits", ",", "1", ")", "\n", "\n", "metrics", "=", "{", "\n", "'acc'", ":", "tf", ".", "metrics", ".", "accuracy", "(", "labels", ",", "pred_ids", ")", ",", "\n", "'precision'", ":", "precision", "(", "labels", ",", "pred_ids", ",", "2", ",", "indices", ",", "None", ",", "average", "=", "'macro'", ")", ",", "\n", "'recall'", ":", "recall", "(", "labels", ",", "pred_ids", ",", "2", ",", "indices", ",", "None", ",", "average", "=", "'macro'", ")", ",", "\n", "'f1'", ":", "f1", "(", "labels", ",", "pred_ids", ",", "2", ",", "indices", ",", "None", ",", "average", "=", "'macro'", ")", "\n", "}", "\n", "\n", "for", "metric_name", ",", "op", "in", "metrics", ".", "items", "(", ")", ":", "\n", "        ", "tf", ".", "summary", ".", "scalar", "(", "metric_name", ",", "op", "[", "1", "]", ")", "\n", "\n", "", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "        ", "return", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "mode", ",", "loss", "=", "loss", ",", "\n", "eval_metric_ops", "=", "metrics", ")", "\n", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "        ", "train_op", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "cfg", ".", "learning_rate", ")", ".", "minimize", "(", "loss", ",", "global_step", "=", "tf", ".", "train", ".", "get_or_create_global_step", "(", ")", ")", "\n", "\n", "return", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "mode", ",", "loss", "=", "loss", ",", "\n", "train_op", "=", "train_op", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_blstm_attn.input_fn": [[109, 122], ["tensorflow.data.Dataset.from_generator", "dataset.shuffle().repeat.padded_batch", "data_loader.data_generator", "dataset.shuffle().repeat.shuffle().repeat", "dataset.shuffle().repeat.shuffle"], "function", ["home.repos.pwc.inspect_result.edoost_offenseval.None.data_loader.DataLoader.data_generator"], ["", "", "def", "input_fn", "(", "mode", "=", "None", ")", ":", "\n", "    ", "data_generator", "=", "lambda", ":", "data_loader", ".", "data_generator", "(", "mode", "=", "mode", ")", "\n", "\n", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "from_generator", "(", "data_generator", ",", "\n", "output_types", "=", "(", "(", "tf", ".", "float32", ",", "tf", ".", "int32", ")", ",", "tf", ".", "int32", ")", ",", "\n", "output_shapes", "=", "(", "(", "[", "None", ",", "cfg", ".", "word_embed_dim", "]", ",", "[", "None", ",", "None", "]", ")", ",", "[", "None", "]", ")", ")", "\n", "\n", "if", "mode", "is", "'train'", ":", "\n", "        ", "dataset", "=", "dataset", ".", "shuffle", "(", "cfg", ".", "shuffle_buffer", ")", ".", "repeat", "(", "cfg", ".", "num_epochs", ")", "\n", "\n", "", "dataset", "=", "dataset", ".", "padded_batch", "(", "cfg", ".", "batch_size", ",", "padded_shapes", "=", "(", "(", "[", "None", ",", "cfg", ".", "word_embed_dim", "]", ",", "[", "None", ",", "None", "]", ")", ",", "[", "None", "]", ")", ")", "\n", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_blstm_attn.train": [[124, 135], ["tensorflow.estimator.RunConfig", "tensorflow.estimator.Estimator", "tensorflow.estimator.TrainSpec", "tensorflow.estimator.EvalSpec", "tensorflow.estimator.train_and_evaluate", "char_cnn_word_blstm_attn.input_fn", "char_cnn_word_blstm_attn.input_fn"], "function", ["home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_blstm_attn.input_fn", "home.repos.pwc.inspect_result.edoost_offenseval.None.char_cnn_word_blstm_attn.input_fn"], ["", "def", "train", "(", ")", ":", "\n", "    ", "train_input_func", "=", "lambda", ":", "input_fn", "(", "mode", "=", "'train'", ")", "\n", "eval_input_func", "=", "lambda", ":", "input_fn", "(", "mode", "=", "'valid'", ")", "\n", "\n", "est_conf", "=", "tf", ".", "estimator", ".", "RunConfig", "(", "cfg", ".", "model_dir", ",", "save_checkpoints_secs", "=", "30", ",", "keep_checkpoint_max", "=", "500", ")", "\n", "est", "=", "tf", ".", "estimator", ".", "Estimator", "(", "model_fn", ",", "cfg", ".", "model_dir", ",", "est_conf", ")", "\n", "\n", "train_spec", "=", "tf", ".", "estimator", ".", "TrainSpec", "(", "input_fn", "=", "train_input_func", ")", "\n", "eval_spec", "=", "tf", ".", "estimator", ".", "EvalSpec", "(", "input_fn", "=", "eval_input_func", ",", "throttle_secs", "=", "30", ")", "\n", "\n", "tf", ".", "estimator", ".", "train_and_evaluate", "(", "est", ",", "train_spec", ",", "eval_spec", ")", "\n", "\n"]]}